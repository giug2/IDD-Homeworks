{
    "PAPER'S NUMBER OF TABLES": 2,
    "S2.T1": {
        "caption": "Table 1: Hyper-parameter tuning on centralized setting. Learning rate and pre-processing.",
        "table": "<table id=\"S2.T1.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S2.T1.2.1.1\" class=\"ltx_tr\">\n<th id=\"S2.T1.2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">lr</th>\n<th id=\"S2.T1.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">0.2</th>\n<th id=\"S2.T1.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">0.15</th>\n<th id=\"S2.T1.2.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">0.1</th>\n<th id=\"S2.T1.2.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">0.08</th>\n<th id=\"S2.T1.2.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">0.06</th>\n<th id=\"S2.T1.2.1.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">0.05</th>\n<th id=\"S2.T1.2.1.1.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">0.04</th>\n<th id=\"S2.T1.2.1.1.9\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">0.02</th>\n<th id=\"S2.T1.2.1.1.10\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">0.01</th>\n<th id=\"S2.T1.2.1.1.11\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">0.005</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S2.T1.2.2.1\" class=\"ltx_tr\">\n<th id=\"S2.T1.2.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">raw</th>\n<td id=\"S2.T1.2.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">47.67</td>\n<td id=\"S2.T1.2.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">44.7</td>\n<td id=\"S2.T1.2.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">49.26</td>\n<td id=\"S2.T1.2.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">47.75</td>\n<td id=\"S2.T1.2.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">49.98</td>\n<td id=\"S2.T1.2.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">48.61</td>\n<td id=\"S2.T1.2.2.1.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">47.46</td>\n<td id=\"S2.T1.2.2.1.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">44.13</td>\n<td id=\"S2.T1.2.2.1.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">43.83</td>\n<td id=\"S2.T1.2.2.1.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">39.92</td>\n</tr>\n<tr id=\"S2.T1.2.3.2\" class=\"ltx_tr\">\n<th id=\"S2.T1.2.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">scaled</th>\n<td id=\"S2.T1.2.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">51.08</td>\n<td id=\"S2.T1.2.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">49.88</td>\n<td id=\"S2.T1.2.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\">53.33</td>\n<td id=\"S2.T1.2.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\">52.29</td>\n<td id=\"S2.T1.2.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_r\">48.09</td>\n<td id=\"S2.T1.2.3.2.7\" class=\"ltx_td ltx_align_center ltx_border_r\">50.77</td>\n<td id=\"S2.T1.2.3.2.8\" class=\"ltx_td ltx_align_center ltx_border_r\">51.27</td>\n<td id=\"S2.T1.2.3.2.9\" class=\"ltx_td ltx_align_center ltx_border_r\">43.80</td>\n<td id=\"S2.T1.2.3.2.10\" class=\"ltx_td ltx_align_center ltx_border_r\">37.21</td>\n<td id=\"S2.T1.2.3.2.11\" class=\"ltx_td ltx_align_center ltx_border_r\">28.71</td>\n</tr>\n<tr id=\"S2.T1.2.4.3\" class=\"ltx_tr\">\n<th id=\"S2.T1.2.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\">standardized</th>\n<td id=\"S2.T1.2.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">53.42</td>\n<td id=\"S2.T1.2.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">54.07</td>\n<td id=\"S2.T1.2.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">54.27</td>\n<td id=\"S2.T1.2.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">53.53</td>\n<td id=\"S2.T1.2.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">53.85</td>\n<td id=\"S2.T1.2.4.3.7\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">52.32</td>\n<td id=\"S2.T1.2.4.3.8\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">52.74</td>\n<td id=\"S2.T1.2.4.3.9\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">47.54</td>\n<td id=\"S2.T1.2.4.3.10\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">40.56</td>\n<td id=\"S2.T1.2.4.3.11\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">34.81</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "FedProx.",
                " There are two key challenges in Federated Learning that differentiate it from traditional distributed optimization: (1) significant variability in terms of the system’s characteristics on each device in the network (systems heterogeneity), and (2) non-identically distributed data across the network (statistical heterogeneity). FedProx (Tian Li ",
                "et al",
                ".",
                " ",
                "[",
                "12",
                "]",
                ") can be viewed as a generalization and re-parametrization of FedAvg. In this work, the authors have theoretically proven convergence guarantees when learning over data from non-identical distributions (satistical heterogeneity), while also adhering to the constraints each device-level system has by allowing each participating device to perform a variable amount of work (system heterogeneity). They demonstrate that in highly heterogeneous settings, FedProx has significantly more stable and accurate convergence behavior relative to FedAvg, improving absolute test accuracy by 22% on average.",
                "FedIR.",
                " As well as ",
                "[",
                "12",
                "]",
                ", the authors of this paper (Hsu ",
                "et al",
                ".",
                " ",
                "[",
                "6",
                "]",
                ") recognized that one of the challenges in terms of data diversity relies on the fact that data at the source is far from independent and identically distributed (IID). In addition, they also found that differing quantities of data are typically available at each device (imbalance). In their work, they characterized the effect these real-world data distributions have on distributed learning, using FedAvg as a benchmark. They proposed two new algorithms, FedVC and FedIR. The latter one addresses the issue of the non-identical class distribution shift present in the federated clients. If we consider a target distribution ",
                "p",
                "​",
                "(",
                "x",
                ",",
                "y",
                ")",
                "𝑝",
                "𝑥",
                "𝑦",
                "p(x,y)",
                " of images ",
                "x",
                "𝑥",
                "x",
                " and class labels ",
                "y",
                "𝑦",
                "y",
                " on which a model is supposed to perform well, and a predefined loss function ",
                "l",
                "​",
                "(",
                "x",
                ",",
                "y",
                ")",
                "𝑙",
                "𝑥",
                "𝑦",
                "l(x,y)",
                ", the objective of learning is to minimize the expected loss ",
                "E",
                "p",
                "​",
                "[",
                "l",
                "​",
                "(",
                "x",
                ",",
                "y",
                ")",
                "]",
                "subscript",
                "𝐸",
                "𝑝",
                "delimited-[]",
                "𝑙",
                "𝑥",
                "𝑦",
                "E_{p}[l(x,y)]",
                " with respect to the target distribution ",
                "p",
                "𝑝",
                "p",
                ". Instead, training examples on a federated client ",
                "k",
                "𝑘",
                "k",
                " are sampled from a client-specific distribution ",
                "q",
                "k",
                "​",
                "(",
                "x",
                ",",
                "y",
                ")",
                "subscript",
                "𝑞",
                "𝑘",
                "𝑥",
                "𝑦",
                "q_{k}(x,y)",
                ". This implies that the empirical loss being optimized on every client is a ",
                "biased",
                " estimator of the loss with respect to the target distribution, since ",
                "E",
                "q",
                "k",
                "​",
                "[",
                "l",
                "​",
                "(",
                "x",
                ",",
                "y",
                ")",
                "]",
                "≠",
                "E",
                "p",
                "​",
                "[",
                "l",
                "​",
                "(",
                "x",
                ",",
                "y",
                ")",
                "]",
                "subscript",
                "𝐸",
                "subscript",
                "𝑞",
                "𝑘",
                "delimited-[]",
                "𝑙",
                "𝑥",
                "𝑦",
                "subscript",
                "𝐸",
                "𝑝",
                "delimited-[]",
                "𝑙",
                "𝑥",
                "𝑦",
                "E_{q_{k}}[l(x,y)]\\neq E_{p}[l(x,y)]",
                ". Therefore, this algorithm presents an importance reweighting scheme that applies importance weights ",
                "w",
                "k",
                "​",
                "(",
                "x",
                ",",
                "y",
                ")",
                "subscript",
                "𝑤",
                "𝑘",
                "𝑥",
                "𝑦",
                "w_{k}(x,y)",
                " to every client’s local objective. With those in place, an unbiased estimator of loss with respect to the target distribution can be obtained using training examples from the client distribution. The paper shows a consistent improvement versus the experiments run over the FedAvg baseline."
            ]
        ]
    },
    "S6.T2": {
        "caption": "Table 2: Performance comparison of different methods",
        "table": "<table id=\"S6.T2.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T2.2.1.1\" class=\"ltx_tr\">\n<td id=\"S6.T2.2.1.1.1\" class=\"ltx_td\"></td>\n<td id=\"S6.T2.2.1.1.2\" class=\"ltx_td ltx_align_center\" colspan=\"8\"><span id=\"S6.T2.2.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Relative accuracy @N rounds</span></td>\n</tr>\n<tr id=\"S6.T2.2.2.2\" class=\"ltx_tr\">\n<td id=\"S6.T2.2.2.2.1\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"S6.T2.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span id=\"S6.T2.2.2.2.2.1\" class=\"ltx_text ltx_font_bold\">Alpha=1</span></td>\n<td id=\"S6.T2.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span id=\"S6.T2.2.2.2.3.1\" class=\"ltx_text ltx_font_bold\">Alpha=0.01</span></td>\n</tr>\n<tr id=\"S6.T2.2.3.3\" class=\"ltx_tr\">\n<td id=\"S6.T2.2.3.3.1\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T2.2.3.3.1.1\" class=\"ltx_text ltx_font_bold\">Methods</span></td>\n<td id=\"S6.T2.2.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S6.T2.2.3.3.2.1\" class=\"ltx_text ltx_font_bold\">@125</span></td>\n<td id=\"S6.T2.2.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S6.T2.2.3.3.3.1\" class=\"ltx_text ltx_font_bold\">@250</span></td>\n<td id=\"S6.T2.2.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S6.T2.2.3.3.4.1\" class=\"ltx_text ltx_font_bold\">@375</span></td>\n<td id=\"S6.T2.2.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S6.T2.2.3.3.5.1\" class=\"ltx_text ltx_font_bold\">@500</span></td>\n<td id=\"S6.T2.2.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S6.T2.2.3.3.6.1\" class=\"ltx_text ltx_font_bold\">@125</span></td>\n<td id=\"S6.T2.2.3.3.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S6.T2.2.3.3.7.1\" class=\"ltx_text ltx_font_bold\">@250</span></td>\n<td id=\"S6.T2.2.3.3.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S6.T2.2.3.3.8.1\" class=\"ltx_text ltx_font_bold\">@375</span></td>\n<td id=\"S6.T2.2.3.3.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S6.T2.2.3.3.9.1\" class=\"ltx_text ltx_font_bold\">@500</span></td>\n</tr>\n<tr id=\"S6.T2.2.4.4\" class=\"ltx_tr\">\n<td id=\"S6.T2.2.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_t\">FedAvg</td>\n<td id=\"S6.T2.2.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\">65.94</td>\n<td id=\"S6.T2.2.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\">82.17</td>\n<td id=\"S6.T2.2.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\">83.03</td>\n<td id=\"S6.T2.2.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\">88.34</td>\n<td id=\"S6.T2.2.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\">40.41</td>\n<td id=\"S6.T2.2.4.4.7\" class=\"ltx_td ltx_align_center ltx_border_t\">41.92</td>\n<td id=\"S6.T2.2.4.4.8\" class=\"ltx_td ltx_align_center ltx_border_t\">51.19</td>\n<td id=\"S6.T2.2.4.4.9\" class=\"ltx_td ltx_align_center ltx_border_t\">60.75</td>\n</tr>\n<tr id=\"S6.T2.2.5.5\" class=\"ltx_tr\">\n<td id=\"S6.T2.2.5.5.1\" class=\"ltx_td ltx_align_center\">FedIR</td>\n<td id=\"S6.T2.2.5.5.2\" class=\"ltx_td ltx_align_center\">77.66</td>\n<td id=\"S6.T2.2.5.5.3\" class=\"ltx_td ltx_align_center\">82.98</td>\n<td id=\"S6.T2.2.5.5.4\" class=\"ltx_td ltx_align_center\">89.03</td>\n<td id=\"S6.T2.2.5.5.5\" class=\"ltx_td ltx_align_center\">89.36</td>\n<td id=\"S6.T2.2.5.5.6\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T2.2.5.5.6.1\" class=\"ltx_text ltx_font_bold\">51.75</span></td>\n<td id=\"S6.T2.2.5.5.7\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T2.2.5.5.7.1\" class=\"ltx_text ltx_font_bold\">59.56</span></td>\n<td id=\"S6.T2.2.5.5.8\" class=\"ltx_td ltx_align_center\">64</td>\n<td id=\"S6.T2.2.5.5.9\" class=\"ltx_td ltx_align_center\">69.03</td>\n</tr>\n<tr id=\"S6.T2.2.6.6\" class=\"ltx_tr\">\n<td id=\"S6.T2.2.6.6.1\" class=\"ltx_td ltx_align_center\">FedProx</td>\n<td id=\"S6.T2.2.6.6.2\" class=\"ltx_td ltx_align_center\">63.85</td>\n<td id=\"S6.T2.2.6.6.3\" class=\"ltx_td ltx_align_center\">82.07</td>\n<td id=\"S6.T2.2.6.6.4\" class=\"ltx_td ltx_align_center\">86.51</td>\n<td id=\"S6.T2.2.6.6.5\" class=\"ltx_td ltx_align_center\">91.4</td>\n<td id=\"S6.T2.2.6.6.6\" class=\"ltx_td ltx_align_center\">28.81</td>\n<td id=\"S6.T2.2.6.6.7\" class=\"ltx_td ltx_align_center\">37.64</td>\n<td id=\"S6.T2.2.6.6.8\" class=\"ltx_td ltx_align_center\">48.58</td>\n<td id=\"S6.T2.2.6.6.9\" class=\"ltx_td ltx_align_center\">48.58</td>\n</tr>\n<tr id=\"S6.T2.2.7.7\" class=\"ltx_tr\">\n<td id=\"S6.T2.2.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">FedOS</td>\n<td id=\"S6.T2.2.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S6.T2.2.7.7.2.1\" class=\"ltx_text ltx_font_bold\">77.92</span></td>\n<td id=\"S6.T2.2.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S6.T2.2.7.7.3.1\" class=\"ltx_text ltx_font_bold\">84.09</span></td>\n<td id=\"S6.T2.2.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S6.T2.2.7.7.4.1\" class=\"ltx_text ltx_font_bold\">94.98</span></td>\n<td id=\"S6.T2.2.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S6.T2.2.7.7.5.1\" class=\"ltx_text ltx_font_bold\">95.85</span></td>\n<td id=\"S6.T2.2.7.7.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">46.6</td>\n<td id=\"S6.T2.2.7.7.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">56.13</td>\n<td id=\"S6.T2.2.7.7.8\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S6.T2.2.7.7.8.1\" class=\"ltx_text ltx_font_bold\">64.7</span></td>\n<td id=\"S6.T2.2.7.7.9\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S6.T2.2.7.7.9.1\" class=\"ltx_text ltx_font_bold\">69.11</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "First, the trained GAN is sent to the client-side, this step accounts only for half a round since the adversarial network doesn’t need to be sent back to the server-side and its size is comparable to the model used for classification. The clients generate a set of random samples proportional to the amount of data they have locally. This is controlled by our second hyper parameter ",
                "ℱ",
                "𝒰",
                "subscript",
                "ℱ",
                "𝒰",
                "\\mathcal{F_{U}}",
                ". Suppose ",
                "𝒩",
                "𝒩",
                "\\mathcal{N}",
                " is the total number of local samples, and the quantity of unknown samples generated is ",
                "ℱ",
                "𝒰",
                "∗",
                "𝒩",
                "subscript",
                "ℱ",
                "𝒰",
                "𝒩",
                "\\mathcal{F_{U}*N}",
                ".\nWe add an extra output class to classify ”unknown” samples (see Figure ",
                "3(a)",
                ") and the training proceeds with the normal FedAvg scheme. At the end of training and before inference, we disable the extra output forcing the model to output one of the known classes since our final objective is a closed set classification task."
            ]
        ]
    }
}