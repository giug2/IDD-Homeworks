{
    "PAPER'S NUMBER OF TABLES": 1,
    "S6.T1": {
        "caption": "TABLE I: Experimental parameters setting",
        "table": "<table id=\"S6.T1.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S6.T1.1.1\" class=\"ltx_tr\">\n<td id=\"S6.T1.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Parameter</td>\n<td id=\"S6.T1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">Numerical value</td>\n</tr>\n<tr id=\"S6.T1.1.2\" class=\"ltx_tr\">\n<td id=\"S6.T1.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Number of nodes:</td>\n<td id=\"S6.T1.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">10</td>\n</tr>\n<tr id=\"S6.T1.1.3\" class=\"ltx_tr\">\n<td id=\"S6.T1.1.3.1\" class=\"ltx_td ltx_align_left\">Training rounds:</td>\n<td id=\"S6.T1.1.3.2\" class=\"ltx_td ltx_align_center\">100</td>\n</tr>\n<tr id=\"S6.T1.1.4\" class=\"ltx_tr\">\n<td id=\"S6.T1.1.4.1\" class=\"ltx_td ltx_align_left\">Local batch size:</td>\n<td id=\"S6.T1.1.4.2\" class=\"ltx_td ltx_align_center\">20</td>\n</tr>\n<tr id=\"S6.T1.1.5\" class=\"ltx_tr\">\n<td id=\"S6.T1.1.5.1\" class=\"ltx_td ltx_align_left\">Local epoch:</td>\n<td id=\"S6.T1.1.5.2\" class=\"ltx_td ltx_align_center\">1</td>\n</tr>\n<tr id=\"S6.T1.1.6\" class=\"ltx_tr\">\n<td id=\"S6.T1.1.6.1\" class=\"ltx_td ltx_align_left\">Decaying for learning rate:</td>\n<td id=\"S6.T1.1.6.2\" class=\"ltx_td ltx_align_center\">0.995</td>\n</tr>\n<tr id=\"S6.T1.1.7\" class=\"ltx_tr\">\n<td id=\"S6.T1.1.7.1\" class=\"ltx_td ltx_align_left\">Loss function:</td>\n<td id=\"S6.T1.1.7.2\" class=\"ltx_td ltx_align_center\">Cross Entropy</td>\n</tr>\n<tr id=\"S6.T1.1.8\" class=\"ltx_tr\">\n<td id=\"S6.T1.1.8.1\" class=\"ltx_td ltx_align_left ltx_border_b\">Learning rate:</td>\n<td id=\"S6.T1.1.8.2\" class=\"ltx_td ltx_align_center ltx_border_b\">MNIST/FMNIST: 0.001, CIFAR: 0.005</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Fig. 4 and 5 respectively shows the performances of different algorithms with i.i.d data allocation under time-invariant and time-varying topologies.\nThe hyper-parameters in this experiment follow Table I.",
            "The section 6.3 has shown the result of DACFL on i.i.d data and declared its practicality under both time-invariant and time-varying topology.\nIn this section, we test the performance of DACFL on non-i.i.d data and show the result in Fig. 6 and Fig. 7.\nThe hyper-parameters in this experiment follow Table I.",
            "To figure out how the learning rate and network topology size affect our solution, we also log the average test accuracy and average training loss on i.i.d MNIST with different learning rates and topology sizes. Fig. 8 shows the numerical result. Note that there are no decaying on learning rate and all topologies are dense in this part of experiments.\nExcept for the learning rate and topology size, other hyper-parameters in this experiment follow Table I."
        ]
    }
}