{
    "id_table_1": {
        "caption": "Table 1:  Visual Question Answering benchmarks of  W2C  on LLaVA1.5 and LLaVA-NeXT under different combination of IT datasets. The best results are  bold  and the second results are  underlined .   : our reproduction of LLaVA-1.5 and LLaVA-Next, which achieves comparable performance with the original papers.   - - : LLaVA-1.5 does not support benchmarks that requires high input resolution. Abbreviations: SQA I (ScienceQA), MMS.(MMStar), MMT.(MMT-Bench), Text.(TextVQA), Doc.(DocVQA), Chart.(ChartQA).",
        "table": "S4.T1.5",
        "footnotes": [],
        "references": [
            "Recent progress shows that generated results of LLMs  Wang et al. ( 2022 ); Li et al. ( 2023c )  and VLMs  Zhang et al. ( 2024b )  for prompts with similar meanings should be alike and we can help filter out noisy generated texts and captions by consistency checking among multiple prompt instructed results. In light of the above evidence, we present a self-instructed data construction pipeline, coined  W2C  .  W2C  autonomously extracts and articulates specific content from images, and enhances the reliability of the generated image captions by employing consistency filtering by assessing the outputs through multiple instructed prompt consistencies. The overall pipeline reduces requested specialists and frees off expensive human feedback as shown in Figure  1 . In addition, we leverage the idea from human-machine interaction and organize the model-generated responses into a Python code format, following Eureka  Ma et al. ( 2023 )  and Text2Reward  Xie et al. ( 2023 ) . Experiments have shown that our proposed  W2C  can improve VLMs on various visual question-answering benchmarks. To be specific,  W2C  performs the best in 7 out of 9 VQA benchmarks on LLaVA-NeXT-7B, and 6 out of 9 VQA benchmarks on LLaVA-NeXT-13B. Furthermore,  W2C  also improves few-shot evaluations on two widely used VQA benchmarks including GQA and MME. Especially, on the 2-shot evaluation of GQA, the method achieves over 5 accuracy gains across different VLMs.",
            "(1) Visual Concepts Extraction in Section  3.1 , (2) Self-Instructed Information Extraction in Section  3.2 , (3) Information Filtering via Self Consistency in Section  3.3 , (4) Structured formatting in Section  3.4 . The overview of our construction pipeline is shown in Figure  2  and all the used instruct prompts are shown in Appendix  7 .",
            "In conclusion, by denoting the final dataset as  D W  2  C subscript D W 2 C D_{W2C} italic_D start_POSTSUBSCRIPT italic_W 2 italic_C end_POSTSUBSCRIPT , the whole data construction pipeline is depicted in Algorithm  1 .",
            "In this paper, we employ two types of leading methods: LLaVA-1.5  Liu et al. ( 2023a )  uses a CLIP-pretrained ViT-L/14  Radford et al. ( 2021 )  as a vision encoder, a projector and an LLM, and LLaVA-NeXT  Liu et al. ( 2024a )  increases the input image resolution by applying an adaptive image cropping strategy to concatenate all vision tokens. To ensure a fair and comprehensive comparison Table  1  and Table  2  present results both excluding and including the ShareGPT4V dataset, as well as results from the incorporation of our dataset. Table  3  We have reproduced LLaVA-NeXT with a learning rate of ViT to 1/10 of the base learning rate for the reason that LLaVA-NeXT only publishes their evaluation code. The learning rate for the PT stage is set to  1  e  3 1 superscript e 3 1e^{-3} 1 italic_e start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT  and the IT stage is set to  2  e  5 2 superscript e 5 2e^{-5} 2 italic_e start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT  for both Vicuna-7B and Vicuna-13B backbone LLM. We use 16 A100 for experiments on VLM training. We freeze the vision encoder during training on the LLaVA-1.5 and only freeze the vision encoder on the PT stage during training on the LLaVA-NEXT following the original paper. We show more training details in the Appendix  C.1",
            "During the data construction pipeline, we employ NLTK  Bird ( 2006 )  tool to extract noun phrases from the captions, and the resulting set of phrases is then post-processed using WordNet  Miller ( 1995 )  to remove duplicates and filter out inaccurately named entities. The total amount of final data after consistency filtering will not be completely consistent for different VLMs and we show the details in Appendix  C.1 . The checkpoints of the VLM we used in our data processing are the original checkpoints of the official release. For LLaVA-1.5, which is not trained with the ShareGPT4V dataset, LLaVA-NEXT is trained with part of the ShareGPT4V dataset. The detailed GPU hours can be found in Appendix  C.2  and we show the visualization of our  W2C  samples in Appendix  4 .",
            "We show a quantitative comparison results of the trained VLMs with and without the ShareGPT4V dataset, as well as  W2C  for replacement of the ShareGPT4V during the IT training stage in Table  1 .  W2C  consistently improves the performance on different settings in both LLaVA-1.5 and LLaVA-NeXT. Especially, in the high resolution setting, our  W2C  presents impressive performance improvement on multi-modal visual understanding benchmarks such as MMT Bench, MMStar, and MME. Specifically,  W2C  can bring improvement in 7 out of 9 benchmarks on LLaVA-NeXT-7B and 6 out of 9 on LLaVA-NeXT-13B. Especially, on LLaVA-NeXT-13B,  W2C  improves DocVQA by 0.7 ANLS, ChartQA by 1.8 accuracy, MMT Bench by 0.8 accuracy and MME by 23 points compared to the reproduction results of LLaVA-NeXT. More benchmarks results are shown in  8 .",
            "Our results show advantageous performance in Table  1  and Table  2 , but our analysis of these results shows the limitations of the base models OCR capability on LLaVA-1.5. We proceed with further ablation studies on LLaVA-Next-7B for the constraints on resources, which optimally demonstrate the full benefits of our pipeline and consistency filtering in a comprehensive manner."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Grounding benchmarks of  W2C  on LLaVA1.5 and LLaVA-NeXT under different combination of IT datasets. The best results are  bold  and the second results are  underlined .",
        "table": "S4.T2.1",
        "footnotes": [],
        "references": [
            "(1) Visual Concepts Extraction in Section  3.1 , (2) Self-Instructed Information Extraction in Section  3.2 , (3) Information Filtering via Self Consistency in Section  3.3 , (4) Structured formatting in Section  3.4 . The overview of our construction pipeline is shown in Figure  2  and all the used instruct prompts are shown in Appendix  7 .",
            "As shown in Figure  2 , we organize the structured information into code format to fully represent the region-level information of an image. Inspired by Eureka  Ma et al. ( 2023 )  and Text2Reward  Xie et al. ( 2023 ) , we organize the information as a structured representation into the Python format due to its generality and conciseness. The organization is achieved by the following three rules.",
            "In this paper, we employ two types of leading methods: LLaVA-1.5  Liu et al. ( 2023a )  uses a CLIP-pretrained ViT-L/14  Radford et al. ( 2021 )  as a vision encoder, a projector and an LLM, and LLaVA-NeXT  Liu et al. ( 2024a )  increases the input image resolution by applying an adaptive image cropping strategy to concatenate all vision tokens. To ensure a fair and comprehensive comparison Table  1  and Table  2  present results both excluding and including the ShareGPT4V dataset, as well as results from the incorporation of our dataset. Table  3  We have reproduced LLaVA-NeXT with a learning rate of ViT to 1/10 of the base learning rate for the reason that LLaVA-NeXT only publishes their evaluation code. The learning rate for the PT stage is set to  1  e  3 1 superscript e 3 1e^{-3} 1 italic_e start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT  and the IT stage is set to  2  e  5 2 superscript e 5 2e^{-5} 2 italic_e start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT  for both Vicuna-7B and Vicuna-13B backbone LLM. We use 16 A100 for experiments on VLM training. We freeze the vision encoder during training on the LLaVA-1.5 and only freeze the vision encoder on the PT stage during training on the LLaVA-NEXT following the original paper. We show more training details in the Appendix  C.1",
            "During the data construction pipeline, we employ NLTK  Bird ( 2006 )  tool to extract noun phrases from the captions, and the resulting set of phrases is then post-processed using WordNet  Miller ( 1995 )  to remove duplicates and filter out inaccurately named entities. The total amount of final data after consistency filtering will not be completely consistent for different VLMs and we show the details in Appendix  C.1 . The checkpoints of the VLM we used in our data processing are the original checkpoints of the official release. For LLaVA-1.5, which is not trained with the ShareGPT4V dataset, LLaVA-NEXT is trained with part of the ShareGPT4V dataset. The detailed GPU hours can be found in Appendix  C.2  and we show the visualization of our  W2C  samples in Appendix  4 .",
            "We present the performance of the VLMs on Grounding benchmarks in Table  2 . The task of referential expression comprehension necessitates that the model accurately identifies and localizes the object described. Our models demonstrate their exceptional capability for detailed image recognition and localization by undergoing evaluation across various referential expression comprehension benchmarks, including RefCOCO, RefCOCO+, and RefCOCOg. Benefit from the entity-enteric generation of local captions and the presence of local bounding box information, our model achieved an average improvement of 1.5/1.6 average IoU on LLaVA-1.5 7B/13B and 3.5/1.3 average IoU on LLaVA-NeXT-7B/13B.",
            "Our results show advantageous performance in Table  1  and Table  2 , but our analysis of these results shows the limitations of the base models OCR capability on LLaVA-1.5. We proceed with further ablation studies on LLaVA-Next-7B for the constraints on resources, which optimally demonstrate the full benefits of our pipeline and consistency filtering in a comprehensive manner.",
            "We discussed in Section  3.2  the strengths of choosing the code format for the representation of structured data. In Table  4 , we quantitatively compare our data format with a single-round dialogue format and a multi-round dialogue format. By using the python code as data construction format, we observe improved performance in both visual grounding benchmarks and visual question answer benchmarks on LLaVA-NeXT-7B. Especially, we improved the MMT-Bench by 0.9/1.3 accuracy and DocVQA by 1.1/4.5 ANLS compared to the  single/multi  data format."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Visual Question Answering benchmarks and Grouding benchmarks on LLaVA-NeXT-7B under more combination of SOTA IT dataset methods. The best results are  bold  and the second results are  underlined .   : our reproduction of LLaVA-Next, which achieves comparable performance with the original papers. To ensure a fair comparison, we randomly selected an equal amount of corresponding data from each dataset for this analysis.",
        "table": "S4.T3.4",
        "footnotes": [],
        "references": [
            "(1) Visual Concepts Extraction in Section  3.1 , (2) Self-Instructed Information Extraction in Section  3.2 , (3) Information Filtering via Self Consistency in Section  3.3 , (4) Structured formatting in Section  3.4 . The overview of our construction pipeline is shown in Figure  2  and all the used instruct prompts are shown in Appendix  7 .",
            "In this paper, we employ two types of leading methods: LLaVA-1.5  Liu et al. ( 2023a )  uses a CLIP-pretrained ViT-L/14  Radford et al. ( 2021 )  as a vision encoder, a projector and an LLM, and LLaVA-NeXT  Liu et al. ( 2024a )  increases the input image resolution by applying an adaptive image cropping strategy to concatenate all vision tokens. To ensure a fair and comprehensive comparison Table  1  and Table  2  present results both excluding and including the ShareGPT4V dataset, as well as results from the incorporation of our dataset. Table  3  We have reproduced LLaVA-NeXT with a learning rate of ViT to 1/10 of the base learning rate for the reason that LLaVA-NeXT only publishes their evaluation code. The learning rate for the PT stage is set to  1  e  3 1 superscript e 3 1e^{-3} 1 italic_e start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT  and the IT stage is set to  2  e  5 2 superscript e 5 2e^{-5} 2 italic_e start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT  for both Vicuna-7B and Vicuna-13B backbone LLM. We use 16 A100 for experiments on VLM training. We freeze the vision encoder during training on the LLaVA-1.5 and only freeze the vision encoder on the PT stage during training on the LLaVA-NEXT following the original paper. We show more training details in the Appendix  C.1",
            "We discussed in Section  3.2  the strengths of choosing the code format for the representation of structured data. In Table  4 , we quantitatively compare our data format with a single-round dialogue format and a multi-round dialogue format. By using the python code as data construction format, we observe improved performance in both visual grounding benchmarks and visual question answer benchmarks on LLaVA-NeXT-7B. Especially, we improved the MMT-Bench by 0.9/1.3 accuracy and DocVQA by 1.1/4.5 ANLS compared to the  single/multi  data format.",
            "In Figure  3  and Figure  4 , we present images from the ShareGPT4V dataset alongside the corresponding annotations we constructed by  W2C  . As shown in these images, the annotations generated entirely by the VLMs accurately describe both the global captions and the detailed captions of local entities within specific areas. Additionally, the OCR text is also encapsulated within the corresponding frames. For multiple entities present in the images, a display of group merging is also conducted."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Ablation study of  W2C  on using different data organization format.  single/multi/code : constructed data are organized in single-round conversations/multi-round conversations/python code format.",
        "table": "S4.T4.3",
        "footnotes": [],
        "references": [
            "(1) Visual Concepts Extraction in Section  3.1 , (2) Self-Instructed Information Extraction in Section  3.2 , (3) Information Filtering via Self Consistency in Section  3.3 , (4) Structured formatting in Section  3.4 . The overview of our construction pipeline is shown in Figure  2  and all the used instruct prompts are shown in Appendix  7 .",
            "During the data construction pipeline, we employ NLTK  Bird ( 2006 )  tool to extract noun phrases from the captions, and the resulting set of phrases is then post-processed using WordNet  Miller ( 1995 )  to remove duplicates and filter out inaccurately named entities. The total amount of final data after consistency filtering will not be completely consistent for different VLMs and we show the details in Appendix  C.1 . The checkpoints of the VLM we used in our data processing are the original checkpoints of the official release. For LLaVA-1.5, which is not trained with the ShareGPT4V dataset, LLaVA-NEXT is trained with part of the ShareGPT4V dataset. The detailed GPU hours can be found in Appendix  C.2  and we show the visualization of our  W2C  samples in Appendix  4 .",
            "We discussed in Section  3.2  the strengths of choosing the code format for the representation of structured data. In Table  4 , we quantitatively compare our data format with a single-round dialogue format and a multi-round dialogue format. By using the python code as data construction format, we observe improved performance in both visual grounding benchmarks and visual question answer benchmarks on LLaVA-NeXT-7B. Especially, we improved the MMT-Bench by 0.9/1.3 accuracy and DocVQA by 1.1/4.5 ANLS compared to the  single/multi  data format.",
            "In Figure  3  and Figure  4 , we present images from the ShareGPT4V dataset alongside the corresponding annotations we constructed by  W2C  . As shown in these images, the annotations generated entirely by the VLMs accurately describe both the global captions and the detailed captions of local entities within specific areas. Additionally, the OCR text is also encapsulated within the corresponding frames. For multiple entities present in the images, a display of group merging is also conducted."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Ablation study of  W2C  when combined the different consistency filtering strategy.  re-ranking : caption re-ranking.  counting : counting filtering.",
        "table": "S4.T5.3",
        "footnotes": [],
        "references": [
            "To provide better region-level captions for a given visual concept, we use beam search to bootstrap multiple caption candidates. To select the best candidate, we again leverage the generator-validator consistency. Specifically, for each given visual concept  c i subscript c i c_{i} italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , we get a list of caption candidate  [ o desc 1  ( c i ) , o desc 2  ( c i ) , ... , o desc b  ( c i ) ] subscript superscript o 1 desc subscript c i subscript superscript o 2 desc subscript c i ... subscript superscript o b desc subscript c i [o^{1}_{\\text{desc}}(c_{i}),o^{2}_{\\text{desc}}(c_{i}),...,o^{b}_{\\text{desc}}% (c_{i})] [ italic_o start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT desc end_POSTSUBSCRIPT ( italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) , italic_o start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT desc end_POSTSUBSCRIPT ( italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) , ... , italic_o start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT start_POSTSUBSCRIPT desc end_POSTSUBSCRIPT ( italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ] . We use NLTK to parse these captions and collect all the visual concepts that are contained in these captions. Taking  n n n italic_n  as the total number of extracted concepts in the captions of  c i subscript c i c_{i} italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , we get a new visual concept list denoted as  c i k  C rank subscript superscript c k i subscript C rank c^{k}_{i}\\in\\textbf{C}_{\\text{rank}} italic_c start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  C start_POSTSUBSCRIPT rank end_POSTSUBSCRIPT . Following Equation  5 , we prompt VLMs to check the existence of each extracted visual concept  c i k subscript superscript c k i c^{k}_{i} italic_c start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  via instruct prompt  p valid-c  ( c i k ) subscript p valid-c subscript superscript c k i p_{\\text{valid-c}}(c^{k}_{i}) italic_p start_POSTSUBSCRIPT valid-c end_POSTSUBSCRIPT ( italic_c start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) :",
            "We show the ablation of different consistency filtering choices in Table  5 . Similarly, the performance of LLaVA-NeXT-7B on the both visual grounding benchmarks and visual question answering benchmarks highlights the effectiveness and necessity of our consistency filtering approaches. When two filtering strategies are combined, we achieve the best performance by improving DocVQA with 1.0 ANLS, TextVQA with 1.0 accuracy, RefCOCO+ val  with 0.5 IOU and RefCOCOg val  with 0.8 IOU. We also achieve comparable results on MMT-Bench and RefCOCO val  with little performance degradation."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Comparison between detail caption and code parsing ability in few-shot evaluations on MME and GQA without referring to the image.",
        "table": "S4.T6.1",
        "footnotes": [],
        "references": [
            "From Table  6 , the code parsing output shows significant improvement when compared with using the detail caption output. On the binary classification task for the visual perception subset of MME, the code parsing ability achieves comparable or better performance in various settings. On the free generation VQA task, GQA, using the code parsing output can bring clear accuracy gain across different model size and architectures. Especially, on the 2-shot evaluation of GQA on LLaVA-NEXT-13B, the code parsing output by model trained with  W2C  achieves 8.2 accuracy improvement compared to baseline, indicating that the code-parsing ability present improved performance in presenting the details of one image. More benchmarks results are shown in  9 ."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Prompt for  W2C  data construction pipeline.",
        "table": "A1.T7.6",
        "footnotes": [],
        "references": [
            "(1) Visual Concepts Extraction in Section  3.1 , (2) Self-Instructed Information Extraction in Section  3.2 , (3) Information Filtering via Self Consistency in Section  3.3 , (4) Structured formatting in Section  3.4 . The overview of our construction pipeline is shown in Figure  2  and all the used instruct prompts are shown in Appendix  7 .",
            "W2C  data construction pipeline calls the VLMs repeatedly by using different prompts. We guide the VLMs to accurately answer questions by designing universal prompt templates, thus ensuring better compliance with instruction. All the prompts are shown in Table  7 ."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  More Visual Question Answering benchmarks of  W2C  on LLaVA-NeXT-7B/13B under different combination of IT datasets. The best results are  bold .",
        "table": "A2.T8.1",
        "footnotes": [],
        "references": [
            "We show a quantitative comparison results of the trained VLMs with and without the ShareGPT4V dataset, as well as  W2C  for replacement of the ShareGPT4V during the IT training stage in Table  1 .  W2C  consistently improves the performance on different settings in both LLaVA-1.5 and LLaVA-NeXT. Especially, in the high resolution setting, our  W2C  presents impressive performance improvement on multi-modal visual understanding benchmarks such as MMT Bench, MMStar, and MME. Specifically,  W2C  can bring improvement in 7 out of 9 benchmarks on LLaVA-NeXT-7B and 6 out of 9 on LLaVA-NeXT-13B. Especially, on LLaVA-NeXT-13B,  W2C  improves DocVQA by 0.7 ANLS, ChartQA by 1.8 accuracy, MMT Bench by 0.8 accuracy and MME by 23 points compared to the reproduction results of LLaVA-NeXT. More benchmarks results are shown in  8 .",
            "We show more Visual Question Answering benchmarks of  W2C  on LLaVA-NeXT-7B/13B under different combination of IT datasets in Table  8 . The  W2C  method consistently demonstrates superior experimental results."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Comparison between detail caption and code parsing ability in few-shot evaluations on MMStar and RefCOCOg without referring to the image on LLaVA-NeXT-7B.",
        "table": "A2.T9.1",
        "footnotes": [],
        "references": [
            "From Table  6 , the code parsing output shows significant improvement when compared with using the detail caption output. On the binary classification task for the visual perception subset of MME, the code parsing ability achieves comparable or better performance in various settings. On the free generation VQA task, GQA, using the code parsing output can bring clear accuracy gain across different model size and architectures. Especially, on the 2-shot evaluation of GQA on LLaVA-NEXT-13B, the code parsing output by model trained with  W2C  achieves 8.2 accuracy improvement compared to baseline, indicating that the code-parsing ability present improved performance in presenting the details of one image. More benchmarks results are shown in  9 .",
            "We have added an analysis of in-context learning for two representative datasets in Table  9 : MMStar and RefCOCOg. Its important to note that although we report the in-context learning results on RefCOCOg val set under the same settings, comparing these two types of outputs for grounding tasks is not practically meaningful. This is because when we instruct the  W2C  -trained model to output in detailed caption format, the captions do not usually contain specific box information like [x1,y1,x2,y2]. This leads to a low IoU score for in-context learning with 2/4 shot detailed captions. However, when outputting in code format, the model does predict box information, which accounts for the significant difference in results on RefCOCOg."
        ]
    }
}