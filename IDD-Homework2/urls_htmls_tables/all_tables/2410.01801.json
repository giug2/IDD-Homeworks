{
    "id_table_1": {
        "caption": "Table 1.  Quantitative comparison on image-to-garment clothing texture transfer. Performances evaluated on synthetic testing data. Our method succeeds at faithfully extracting and transferring textures from images, whereas Material Palette  (Lopes et al . ,  2024 )  exhibits significant artifacts, resulting in suboptimal performance, particularly on FID.",
        "table": "S4.T1.6",
        "footnotes": [
            ""
        ],
        "references": [
            "There is an increasing interest to experience apparel in 3D for virtual try-on applications and e-commerce as well as an increasing demand for 3D clothing assets for games, virtual reality and augmented reality applications. While there is an abundance of 2D images of fashion items online, and recent generative AI algorithms democratize the creative generation of such images, the creation of high-quality 3D clothing assets remains a significant challenge. In this work we explore how to transfer the appearance of clothing items from 2D images onto 3D assets, as shown in Figure  1 .",
            "We propose FabricDiffusion to extract normalized, tileable texture images and materials from a real-world clothing image, and then apply them to the target 3D garment. The overall framework is illustrated in Figure  2 . We first introduce the problem statement in Section  3.1 , followed by procedures for constructing synthetic training examples in Section  3.2 . In Section  3.3 , we detail our specific approach of texture map generation. Finally, we describe PBR materials generation and garment rendering in Section  3.4 .",
            "As mentioned in Section  1 , we formulate the generation of normalized texture maps from a real-life clothing patch as a distribution mapping problem. Specifically, the mapping function  g g g italic_g  can be modeled by a generative process:",
            "Collecting paired training examples with real clothing poses significant challenges. In contrast, we found that PBR textures  the fundamental unit for appearance modeling in 3D apparel creation  are much more accessible from public sources (see Section  4.1  for details on dataset collection). Given these observations, we propose to build synthetic environments for constructing distorted and flat rendered training pairs using the PBR material model  (McAuley et al . ,  2012 ) . Figure  3  illustrates the overall pipeline.",
            "In this work, we are able to collect a BRDF dataset comprises 3.8k assets in total (see Section  4.1  for details), covering a broad spectrum of fabric materials. However, the texture patterns in this dataset exhibit limited diversity because it is not large enough to model the appearance of fabric textures in our real life, given the vast range of colors, patterns, and materials. To address this, we augmented the dataset by gathering 100k textile color images featuring a wide array of patterns and designs, which are then used to generate pseudo-BRDF 2 2 2 Since the normal, roughness, and metallic maps of the 100k textile images are sampled instead of ground truth, they are referred to as pseudo-BRDF data.  materials. Specifically, the color image served as the albedo map, while the roughness map was assigned a uniform value    \\alpha italic_  sampled from the distribution  N  ( 0.708 , 0.193 2 ) N 0.708 superscript 0.193 2 \\mathcal{N}(0.708,0.193^{2}) caligraphic_N ( 0.708 , 0.193 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) , with 0.708 and 0.193 representing the population mean and standard deviation of the mean roughness values of the real BRDF dataset, respectively. The metallic map was assigned a uniform value  max  (  , 0 )  0 \\max(\\beta,0) roman_max ( italic_ , 0 ) , where    U  (  0.05 , 0.05 ) similar-to  U 0.05 0.05 \\beta\\sim\\mathcal{U}(-0.05,0.05) italic_  caligraphic_U ( - 0.05 , 0.05 ) , and the normal map was kept flat.",
            "Recalling Equation  1 , the above formulation incorporates input-specific information (i.e., the captured patch  x x x italic_x ) into the training process for generating normalized textures. As will be shown in the experimental results in Section  4.2 , this design is the key to producing faithful texture maps that differs from existing per-example optimization-based texture extraction approaches  (Lopes et al . ,  2024 ; Richardson et al . ,  2023 ) .",
            "where  x ~ ~ x \\tilde{x} over~ start_ARG italic_x end_ARG  is the generated texture (Equation  1 ). The alpha channel value at each pixel  ( i , j ) i j (i,j) ( italic_i , italic_j )  is thus determined by the following criteria:",
            "We validate FabricDiffusion with both synthetic data and real-world images across various scenarios. We begin by introducing the experimental setup in Section  4.1 , followed by detailing the experimental results in Section  4.2 . Finally, we conduct ablation studies and show several real-world applications in Section  4.3 .",
            "In Figure  6 , we compare our method with Material Palette  (Lopes et al . ,  2024 )  and TEXTure  (Richardson et al . ,  2023 )  for image-to-garment texture transfer. We present the results on real-world clothing images featuring a variety of textures, ranging from micro to macro patterns and prints. Our observations indicate that FabricDiffusion not only recovers repetitive patterns, such as scattered stars or camouflage, but also maintains the regularity of structured patterns, like the plaid on a skirt. Please refer to Table  1  for quantitative results.",
            "Since FabricDiffusion works on patches, it can be applied to multi-material garments as well as evidenced in Figure  10 . This suggests that FabricDiffusion can serve as a basic building block for multi-material garment texture transfer.",
            "In this paper, we introduce FabricDiffusion, a new method for transferring fabric textures and prints from a single real-world clothing image onto 3D garments with arbitrary shapes. Our method, trained entirely using synthetic rendered images, is able to generate undistorted texture and prints from in-the-wild clothing images. While our method demonstrates strong generalization abilities with real photos and diverse texture patterns, it faces challenges with certain inputs, as shown in Figure  11 . Specifically, FabricDiffusion may produce errors when reconstructing non-repetitive patterns and struggles to accurately capture fine details in complex prints or logos, especially since our focus is on prints with uniform backgrounds, moderate complexity, and moderate distortion. In the future, we plan to address these challenges by enhancing texture transfer for more complex scenarios and improving performance on difficult fabric categories, such as leather. Additionally, we plan to expand our method to handle a broader range of material maps, including transmittance, to further extend its applicability.",
            "where  L L L italic_L  is the rendered pixel color along the direction   o subscript  o \\omega_{o} italic_ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT  from the surface point  p p p italic_p ,   = {  i :  i  n p  0 }  conditional-set subscript  i  subscript  i subscript n p 0 \\Omega=\\{\\omega_{i}:\\omega_{i}\\cdot n_{p}\\geq 0\\} roman_ = { italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT : italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  italic_n start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT  0 }  denotes a hemisphere with the incident direction   i subscript  i \\omega_{i} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and surface normal  n p subscript n p n_{p} italic_n start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT  at point  p p p italic_p ,  L i subscript L i L_{i} italic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is the incident light that is represented by the environment map, and  f r subscript f r f_{r} italic_f start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT  is known as the BRDF that scales or weighs the incoming radiance based the material parameters  ( k d , k n , k r , k m ) subscript k d subscript k n subscript k r subscript k m (k_{d},k_{n},k_{r},k_{m}) ( italic_k start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_k start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_k start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , italic_k start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT )  of the garment surface. By aggregating the rendered pixel colors along the direction   o subscript  o \\omega_{o} italic_ start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT  (i.e., camera pose), we are able to obtain the rendered image of the input patch (image  x x x italic_x  in Equation   1  of the main paper).",
            "Generating a normalized texture image plays a crucial intermediate step to ensure reliable texture transfer. Figure  7  (in the main paper) shows some cases of the generated normalized textures. In Table  S1 , we provide a quantitative analysis using synthetic data, for which we have ground-truth textures, and compare our method with state-of-the-are methods. As we observe, our method consistently outperforms Material Palette  (Lopes et al . ,  2024 )  across various evaluation metrics. As discussed in Section  2  and Section  4.2  of the main paper, personalization-based methods struggle at capturing fine-grained texture details, or disentangling the effects of distortion.",
            "We also validate our method using synthetic data and show the qualitative results in Figure  S1 . We test on textured garments with ground-truth BRDF materials, enabling controlled evaluation of geometric distortions and illumination variations. Our method reliably generates normalized textures and PBR materials. As our focus is on clothing fabrics with minimal metallic properties, we omit metallic map results for simplicity in the following experiments. Quantitative results are shown in Table  1  of the main paper."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.  Quantitative comparison with state-of-the-art methods on PBR material extraction. Results are evaluated on the real PBR test examples. By fine-tuning MatFusion with additional fabric PBR training data, our method achieves superior performance across most material maps. Material Palette performs subpar, particularly in estimating the diffuse and roughness maps, due the differences in physical properties between fabric materials and general objects. Please see Table  3  for quantitative evaluation on rendered images and Figure  7  for a qualitative comparison between FabricDiffusion and Material Palette.",
        "table": "S4.T2.2",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "We propose FabricDiffusion to extract normalized, tileable texture images and materials from a real-world clothing image, and then apply them to the target 3D garment. The overall framework is illustrated in Figure  2 . We first introduce the problem statement in Section  3.1 , followed by procedures for constructing synthetic training examples in Section  3.2 . In Section  3.3 , we detail our specific approach of texture map generation. Finally, we describe PBR materials generation and garment rendering in Section  3.4 .",
            "Recalling Equation  1 , the above formulation incorporates input-specific information (i.e., the captured patch  x x x italic_x ) into the training process for generating normalized textures. As will be shown in the experimental results in Section  4.2 , this design is the key to producing faithful texture maps that differs from existing per-example optimization-based texture extraction approaches  (Lopes et al . ,  2024 ; Richardson et al . ,  2023 ) .",
            "This approach assigns full opacity (alpha value of 1) to pixels where the initial value exceeds a certain threshold, and scales down the alpha value for other pixels, designating them as transparent background. As will be shown in Section  4.2  and Figure  5 , our method can handle complex prints and logos and output RGBA print images that can be overlaid onto the fabric texture.",
            "We validate FabricDiffusion with both synthetic data and real-world images across various scenarios. We begin by introducing the experimental setup in Section  4.1 , followed by detailing the experimental results in Section  4.2 . Finally, we conduct ablation studies and show several real-world applications in Section  4.3 .",
            "We detail the process of collecting BRDF texture, print, and garment datasets. (1) Fabric BRDF dataset. This dataset includes 3.8k real fabric materials and 100k pseudo-BRDF textures (RGB only). We reserved 200 real BRDF materials for testing the PBR generator and 800 pseudo-BRDF materials (combined with the 200 real materials) for testing the texture generator. (2) 3D garment dataset. We collected 22 3D garment meshes for training and 5 for testing. Using the method in Section  3.2 , we created 220k flat and distorted rendered image pairs for training and 5k pairs for testing. (3) Logos and prints dataset. This dataset contains 7k prints and logos in PNG format. We generated pseudo-BRDF materials with specific roughness and metallic values and a flat normal map. Dark prints were converted to white if necessary. By compositing these onto 3D garments, we produced 82k warped print images.",
            "We compare our method to Material Palette  (Lopes et al . ,  2024 )  and MatFusion  (Sartor and Peers,  2023 )  on PBR materials extraction. In Table  2 , we present a comparison of pixel-level MSE and SSIM between the generated material maps and the ground-truths. Our FabricDiffusion material generator, fine-tuned from the base MatFusion model with additional fabric BRDF training examples, demonstrates superior performance. Additionally, Figure  7  shows visual comparisons between FabricDiffusion and Material Palette. While Material Palette  (Lopes et al . ,  2024 )  struggles to accurately capture fabric materials, our FabricDiffusion model excels in recovering the physical properties for fabric textures, particularly in roughness and diffuse maps. We also evaluate different methods on the rendered images and show the results in Table  3 . Particularly, we use render-aware metrics like FLIP  (Andersson et al . ,  2020 )  and perceptual metrics like LPIPS and DISTS. FabricDiffusion consistently achieve better performance over other approaches.",
            "We collect a dataset of 7k prints and logos in PNG format with CC0 license. Their corresponding pseudo-BRDF materials are generated by assigning a uniform roughness value sampled from  U  ( 0.4 , 0.7 ) U 0.4 0.7 \\mathcal{U}(0.4,0.7) caligraphic_U ( 0.4 , 0.7 ) , a uniform metallic value sampled from  U  ( 0 , 0.3 ) U 0 0.3 \\mathcal{U}(0,0.3) caligraphic_U ( 0 , 0.3 ) , and a default flat normal map. In cases where a print was uniformly black, we converted it to white if the background texture was also dark. By compositing the logo prints onto the 3D garments, we obtain a total of 82k warped print images, following the method outlined in Section   3.2  of the main paper.",
            "Generating a normalized texture image plays a crucial intermediate step to ensure reliable texture transfer. Figure  7  (in the main paper) shows some cases of the generated normalized textures. In Table  S1 , we provide a quantitative analysis using synthetic data, for which we have ground-truth textures, and compare our method with state-of-the-are methods. As we observe, our method consistently outperforms Material Palette  (Lopes et al . ,  2024 )  across various evaluation metrics. As discussed in Section  2  and Section  4.2  of the main paper, personalization-based methods struggle at capturing fine-grained texture details, or disentangling the effects of distortion."
        ]
    },
    "id_table_3": {
        "caption": "Table 3.  Quantitative comparison on rendered materials. We adopt render-aware and perceptual metrics and compare the quality of rendered generated texture. FabricDiffusion outperforms other methods.",
        "table": "S4.T3.5",
        "footnotes": [],
        "references": [
            "However, acquiring such paired training data from real clothing at scale is infeasible. To address this issue, we develop a large-scale synthetic dataset comprising over  100 100 100 100 k textile color images,  3.8 3.8 3.8 3.8 k material PBR texture maps,  7 7 7 7 k prints (e.g., logos), and  22 22 22 22  raw 3D garment meshes. These PBR textures and prints are carefully applied to the raw 3D garment meshes and then rendered using PBR techniques under diverse lighting and environmental conditions, simulating real-world scenarios. For each fabric captures from the textured 3D garment, we render a corresponding image using ground-truth PBR textures, which are applied to a flat mesh under a controlled illumination condition, i.e., orthogonal close-up views with a pointed lighting from above. The captured texture inputs along with their ground-truth flat mesh render are used to train our diffusion model. Figure  3  illustrates the pipeline of training data construction.",
            "We propose FabricDiffusion to extract normalized, tileable texture images and materials from a real-world clothing image, and then apply them to the target 3D garment. The overall framework is illustrated in Figure  2 . We first introduce the problem statement in Section  3.1 , followed by procedures for constructing synthetic training examples in Section  3.2 . In Section  3.3 , we detail our specific approach of texture map generation. Finally, we describe PBR materials generation and garment rendering in Section  3.4 .",
            "Collecting paired training examples with real clothing poses significant challenges. In contrast, we found that PBR textures  the fundamental unit for appearance modeling in 3D apparel creation  are much more accessible from public sources (see Section  4.1  for details on dataset collection). Given these observations, we propose to build synthetic environments for constructing distorted and flat rendered training pairs using the PBR material model  (McAuley et al . ,  2012 ) . Figure  3  illustrates the overall pipeline.",
            "Separately, we render the same texture material on a plane mesh to create flat rendered images as ground-truths (image  x 0 subscript x 0 x_{0} italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  in Figure  3 ). For illumination, we use a fixed point light above the surface center and a fixed orthogonal camera for rendering. This method is highly beneficial as it provides supervision to align the distorted rendered images on the 3D garment to a canonical space of normalized, flat images with a unified lighting condition.",
            "In fact, our flat image rendering and capturing approach may be reminiscent of the input format used in well-known SVBRDF material estimation methods  (Sartor and Peers,  2023 ; Zhou and Kalantari,  2021 ; Zhou et al . ,  2022 ,  2023b ) , which require orthogonal close-up views of the materials and/or a flashing image as input. As will be described in Section  3.4 , the output normalized textures from our method can be effectively integrated with SVBRDF material estimation models to generate high-quality PBR material maps.",
            "We use a combination of real (3.8k) and pseudo-BRDF (100k) materials to create paired rendered images for training our texture generation model. During paired training examples construction, both real and pseudo-BRDF have  x x x italic_x  and  x 0 subscript x 0 x_{0} italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  (as illustrated in Figure  3 ), representing distorted and flat textures, respectively. Intuitively, the primary goal of our texture generator is to eliminate geometric distortions, and our generated pseudo rendered images, serve this purpose effectively.",
            "Any diffusion-based architecture for conditional image generation can realize Equation  3 . Specifically, we use Stable Diffusion  (Rombach et al . ,  2022 ) , a popular open-source text-conditioned image generative model pre-trained on large-scale text and image pairs. To support image conditioning, we use additional input channels to the first convolutional layer, where the latent noise  x t subscript x t x_{t} italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  is concatenated with the conditioned image latent  E  ( x ) E x \\mathcal{E}(x) caligraphic_E ( italic_x ) . The models initial weights come from the pre-trained Stable Diffusion v1.5, while the newly added channels are initialized to zero, speeding up training and convergence. We eliminate text conditioning, focusing solely on using a single image as the prompt. This approach addresses the challenge of generating normalized texture maps, which text prompts struggle to describe accurately  (Deschaintre et al . ,  2023 ) .",
            "We validate FabricDiffusion with both synthetic data and real-world images across various scenarios. We begin by introducing the experimental setup in Section  4.1 , followed by detailing the experimental results in Section  4.2 . Finally, we conduct ablation studies and show several real-world applications in Section  4.3 .",
            "We detail the process of collecting BRDF texture, print, and garment datasets. (1) Fabric BRDF dataset. This dataset includes 3.8k real fabric materials and 100k pseudo-BRDF textures (RGB only). We reserved 200 real BRDF materials for testing the PBR generator and 800 pseudo-BRDF materials (combined with the 200 real materials) for testing the texture generator. (2) 3D garment dataset. We collected 22 3D garment meshes for training and 5 for testing. Using the method in Section  3.2 , we created 220k flat and distorted rendered image pairs for training and 5k pairs for testing. (3) Logos and prints dataset. This dataset contains 7k prints and logos in PNG format. We generated pseudo-BRDF materials with specific roughness and metallic values and a flat normal map. Dark prints were converted to white if necessary. By compositing these onto 3D garments, we produced 82k warped print images.",
            "In addition to texture patterns and material properties, our FabricDiffusion model can transfer detailed prints and logos. Figure  5  shows some examples. We highlight two key advantages of our design that benefit the recovery of prints and logos. First, our conditional generative model corrects geometry distortion caused by human pose or camera perspective. Second, as detailed in Section  3.3 , our method can generate prints with a transparent background, enabling practical usage in garment appearance modeling.",
            "We compare our method to Material Palette  (Lopes et al . ,  2024 )  and MatFusion  (Sartor and Peers,  2023 )  on PBR materials extraction. In Table  2 , we present a comparison of pixel-level MSE and SSIM between the generated material maps and the ground-truths. Our FabricDiffusion material generator, fine-tuned from the base MatFusion model with additional fabric BRDF training examples, demonstrates superior performance. Additionally, Figure  7  shows visual comparisons between FabricDiffusion and Material Palette. While Material Palette  (Lopes et al . ,  2024 )  struggles to accurately capture fabric materials, our FabricDiffusion model excels in recovering the physical properties for fabric textures, particularly in roughness and diffuse maps. We also evaluate different methods on the rendered images and show the results in Table  3 . Particularly, we use render-aware metrics like FLIP  (Andersson et al . ,  2020 )  and perceptual metrics like LPIPS and DISTS. FabricDiffusion consistently achieve better performance over other approaches.",
            "In Section  3.4 , we explored how FabricDiffusion can be integrated into an end-to-end framework for 3D garment design. To assess whether the generated texture remains consistent with the input, Figure  8 -(a) shows the results of varying the location of a fixed-size capture region. The results indicate that FabricDiffusion consistently produces similar texture patterns, regardless of the location of the captured region.",
            "We collect a dataset of 7k prints and logos in PNG format with CC0 license. Their corresponding pseudo-BRDF materials are generated by assigning a uniform roughness value sampled from  U  ( 0.4 , 0.7 ) U 0.4 0.7 \\mathcal{U}(0.4,0.7) caligraphic_U ( 0.4 , 0.7 ) , a uniform metallic value sampled from  U  ( 0 , 0.3 ) U 0 0.3 \\mathcal{U}(0,0.3) caligraphic_U ( 0 , 0.3 ) , and a default flat normal map. In cases where a print was uniformly black, we converted it to white if the background texture was also dark. By compositing the logo prints onto the 3D garments, we obtain a total of 82k warped print images, following the method outlined in Section   3.2  of the main paper."
        ]
    },
    "id_table_4": {
        "caption": "Table 4.  Ablation study on pseudo-BRDF data. We compare the performance of using combined versus only real-BRDF data. Combined data effectively improve the performance.",
        "table": "S4.T4.4",
        "footnotes": [],
        "references": [
            "We propose FabricDiffusion to extract normalized, tileable texture images and materials from a real-world clothing image, and then apply them to the target 3D garment. The overall framework is illustrated in Figure  2 . We first introduce the problem statement in Section  3.1 , followed by procedures for constructing synthetic training examples in Section  3.2 . In Section  3.3 , we detail our specific approach of texture map generation. Finally, we describe PBR materials generation and garment rendering in Section  3.4 .",
            "Collecting paired training examples with real clothing poses significant challenges. In contrast, we found that PBR textures  the fundamental unit for appearance modeling in 3D apparel creation  are much more accessible from public sources (see Section  4.1  for details on dataset collection). Given these observations, we propose to build synthetic environments for constructing distorted and flat rendered training pairs using the PBR material model  (McAuley et al . ,  2012 ) . Figure  3  illustrates the overall pipeline.",
            "In fact, our flat image rendering and capturing approach may be reminiscent of the input format used in well-known SVBRDF material estimation methods  (Sartor and Peers,  2023 ; Zhou and Kalantari,  2021 ; Zhou et al . ,  2022 ,  2023b ) , which require orthogonal close-up views of the materials and/or a flashing image as input. As will be described in Section  3.4 , the output normalized textures from our method can be effectively integrated with SVBRDF material estimation models to generate high-quality PBR material maps.",
            "In this work, we are able to collect a BRDF dataset comprises 3.8k assets in total (see Section  4.1  for details), covering a broad spectrum of fabric materials. However, the texture patterns in this dataset exhibit limited diversity because it is not large enough to model the appearance of fabric textures in our real life, given the vast range of colors, patterns, and materials. To address this, we augmented the dataset by gathering 100k textile color images featuring a wide array of patterns and designs, which are then used to generate pseudo-BRDF 2 2 2 Since the normal, roughness, and metallic maps of the 100k textile images are sampled instead of ground truth, they are referred to as pseudo-BRDF data.  materials. Specifically, the color image served as the albedo map, while the roughness map was assigned a uniform value    \\alpha italic_  sampled from the distribution  N  ( 0.708 , 0.193 2 ) N 0.708 superscript 0.193 2 \\mathcal{N}(0.708,0.193^{2}) caligraphic_N ( 0.708 , 0.193 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) , with 0.708 and 0.193 representing the population mean and standard deviation of the mean roughness values of the real BRDF dataset, respectively. The metallic map was assigned a uniform value  max  (  , 0 )  0 \\max(\\beta,0) roman_max ( italic_ , 0 ) , where    U  (  0.05 , 0.05 ) similar-to  U 0.05 0.05 \\beta\\sim\\mathcal{U}(-0.05,0.05) italic_  caligraphic_U ( - 0.05 , 0.05 ) , and the normal map was kept flat.",
            "Recalling Equation  1 , the above formulation incorporates input-specific information (i.e., the captured patch  x x x italic_x ) into the training process for generating normalized textures. As will be shown in the experimental results in Section  4.2 , this design is the key to producing faithful texture maps that differs from existing per-example optimization-based texture extraction approaches  (Lopes et al . ,  2024 ; Richardson et al . ,  2023 ) .",
            "This approach assigns full opacity (alpha value of 1) to pixels where the initial value exceeds a certain threshold, and scales down the alpha value for other pixels, designating them as transparent background. As will be shown in Section  4.2  and Figure  5 , our method can handle complex prints and logos and output RGBA print images that can be overlaid onto the fabric texture.",
            "We validate FabricDiffusion with both synthetic data and real-world images across various scenarios. We begin by introducing the experimental setup in Section  4.1 , followed by detailing the experimental results in Section  4.2 . Finally, we conduct ablation studies and show several real-world applications in Section  4.3 .",
            "We first show the results of our method on real-world images in Figure  4 . Our method effectively transfers both texture patterns and material properties from various types of clothing to the target 3D garment. Notably, our method is capable of recovering challenging materials such as knit, translucent fabric, and leather. We attribute this success to our construction of paired training examples that seamlessly couples the PBR generator with the upstream texture generator. Since we focus on non-metallic fabrics, the metallic map is omitted in the visualizations in the section. Please be referred to Appendix for more details and results.",
            "We compare the performance of using combined real-BRDF and pseudo-BRDF data versus using only real-BRDF data. The results, summarized in Table  4 , demonstrate that the inclusion of pseudo-BRDF data alongside real-BRDF data improves performance across all metrics.",
            "In Section  3.4 , we explored how FabricDiffusion can be integrated into an end-to-end framework for 3D garment design. To assess whether the generated texture remains consistent with the input, Figure  8 -(a) shows the results of varying the location of a fixed-size capture region. The results indicate that FabricDiffusion consistently produces similar texture patterns, regardless of the location of the captured region.",
            "Generating a normalized texture image plays a crucial intermediate step to ensure reliable texture transfer. Figure  7  (in the main paper) shows some cases of the generated normalized textures. In Table  S1 , we provide a quantitative analysis using synthetic data, for which we have ground-truth textures, and compare our method with state-of-the-are methods. As we observe, our method consistently outperforms Material Palette  (Lopes et al . ,  2024 )  across various evaluation metrics. As discussed in Section  2  and Section  4.2  of the main paper, personalization-based methods struggle at capturing fine-grained texture details, or disentangling the effects of distortion."
        ]
    },
    "id_table_5": {
        "caption": "Table S1.  Quantitative comparison on texture images extraction from 3D garments. Results are evaluated on synthetic testing data. The ground-truths are normalized texture images that are flat and with a unified lighting condition. Our method outperforms Material Palette  (Lopes et al . ,  2024 )  across different evaluation metrics.",
        "table": "A4.T1.5",
        "footnotes": [
            ""
        ],
        "references": [
            "This approach assigns full opacity (alpha value of 1) to pixels where the initial value exceeds a certain threshold, and scales down the alpha value for other pixels, designating them as transparent background. As will be shown in Section  4.2  and Figure  5 , our method can handle complex prints and logos and output RGBA print images that can be overlaid onto the fabric texture.",
            "In addition to texture patterns and material properties, our FabricDiffusion model can transfer detailed prints and logos. Figure  5  shows some examples. We highlight two key advantages of our design that benefit the recovery of prints and logos. First, our conditional generative model corrects geometry distortion caused by human pose or camera perspective. Second, as detailed in Section  3.3 , our method can generate prints with a transparent background, enabling practical usage in garment appearance modeling."
        ]
    }
}