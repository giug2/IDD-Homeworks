{
    "id_table_1": {
        "caption": "Table 1 :  List of evaluated CWEs. Nine of the eleven CWEs are in the top 25 list. The description is from  [ 51 ] .",
        "table": "S3.T1.4",
        "footnotes": [
            ""
        ],
        "references": [
            "During the data synthesis phase, the LLMs repair the code by modifying or extending the included libraries and the main code. As a result, our synthesized data includes the required security-relevant libraries. This suggests that writing secure code may require the use of additional libraries in specific scenarios. Based on this observation, we propose a two-step generation approach to give the models the opportunity to include libraries that can potentially be used to generate secure codes. In our two-step generation approach, we complete the given codes during inference by first providing only the included libraries as input to the models to generate all other potential libraries. In the second code generation step, we provide the updated libraries together with the rest of the code as input to the models. Figure  1  illustrates how our approach automatically synthesizes the secure code and fine-tunes the CodeLMs to enhance the models capabilities in generating secure codes.",
            "In this work, we focus on 11 CWEs that can be identified using static analysis tools. Notably, 9 of these 11 CWEs are included in MITREs list of the 25 most dangerous software weaknesses published in 2023  [ 51 ] . The list of these CWEs, including a brief description, is provided in Table  1 . In our analysis, we decided against using fuzzing for vulnerability detection, as this may require significant computational resources and extensive manual effort for classifying vulnerability types and root cause analysis. In addition, the code sequences generated by LLMs are typically not suitable for a fuzzing campaign because they do not represent a full program and would require a program-specific testing harness.",
            "Following the state-of-the-art work  [ 35 ] , we generate for each scenario up to 200 new tokens with a temperature of  0.4 0.4 0.4 0.4  to complete the provided input. We then use CodeQL  [ 39 ]  to evaluate the security issues in the generated code. CodeQL offers queries to identify 29 different CWEs for Python code and 35 CWEs for C/C++ code. Although we focus on 11 specific CWEs, as listed in Table  1  , we analyze the generated code for all CWEs supported by CodeQL and report all identified security vulnerabilities in both Python and C/C++ code. In our results, CWEs not listed in Table  1  are categorized as  Other .",
            "In our data synthesis pipeline, we generate a set of vulnerable and fixed codes. To generate the vulnerable set, we employ the few-shot prompting approach proposed in  [ 32 ] . To minimize unnecessary computing usage, we use a set of 2,042 vulnerable code samples generated by this work  [ 32 ] . This set contains 1,519 Python codes and 523 C/C++ codes. Each of these code samples contains at least one vulnerability of a CWE type listed in Table  1 . Note that, using CodeQL  [ 39 ] , we validate whether the code contains the targeted vulnerabilities. Only the samples that pass this validation are included in the vulnerable set.",
            "To fix the vulnerabilities in each code, as described in Subsection  4.1 , we consider each vulnerable code, along with its corresponding security report, as the input of GPT-4  [ 54 ] . Given the input, we generate up to 1,000 tokens using GPT-4  [ 54 ]  with a temperature of  0.1 0.1 0.1 0.1 . In an initial study, we found that these two parameters provide the best results considering the budget and the models performance. Given the provided input to the GPT-4 model, we generate one sample and extract the fixed code from the provided sample. We then check the generated code again with CodeQL  [ 54 ] , and if the code does not contain any vulnerability, we consider it as an instance of our secure codes. Out of 2,042 vulnerable code samples, our approach successfully fixed the vulnerabilities in 1,776 of them, which we refer to as the  secure code set . This secure set includes 1,414 Python codes and 362 C/C++ codes. Detailed results of the synthesized code data are provided in Table  2 . In this table, the first column lists the type of CWE, and the second column shows the number of synthesized secure Python and C/C++ codes. The overall results for the synthesized secure codes are presented in the last row.",
            "To evaluate each model, we sample  q q q italic_q  outputs for each given prompt. Following previous work  [ 32 ] , we set  q = 5 q 5 q=5 italic_q = 5  for the CodeLMSec prompts, and since the benchmark provided by Pearce et al.  [ 56 ]  contains fewer prompts, we set  q = 15 q 15 q=15 italic_q = 15 . We use nucleus sampling to sample  q q q italic_q  programs for each given prompt. As mentioned in Section  5.1 , we generate up to 200 new tokens per prompt for both the CodeLMSec  [ 32 ]  and Pearce et al.  [ 56 ]  benchmarks. To ensure fairness, in our two-step generation approach, we set the maximum number of tokens to 20 for the first step and 180 for the second step. In the first step, we set the maximum token number to 20 to provide the model with sufficient capacity to include the necessary libraries. We consider a code vulnerable if CodeQL identifies a vulnerability in it. We report the number of vulnerable codes within the top- x x x italic_x  codes, where top- x x x italic_x  refers to the most probable sampled codes out of all the sampled codes. For example, top- 1 1 1 1  represents the most probable sampled code among  q q q italic_q  sampled codes.",
            "Figure  12  presents the overall performance results of CodeGen-2B-multi  [ 52 ]  in terms of code security and functional correctness. The figure compares the results for the pre-trained CodeGen-2B-multi  [ 52 ]  ( Base ), the fine-tuned version using the SVEN method  [ 35 ] , and the fine-tuned version using our HexaCoder approach. Figures  11(a)  and  11(b)  provide the number of generated vulnerable codes using Python and C/C++ prompts of the CodeLMSec  [ 32 ]  and Pearce et al.  [ 56 ]  benchmarks, respectively. These figures illustrate the number of vulnerable codes generated among the top- 1 1 1 1  and top- 5 5 5 5  most probable sampled codes for CodeLMSec  [ 32 ]  (Figure  11(a) ) and the top- 1 1 1 1  and top- 15 15 15 15  most probable sampled codes for Pearce et al. [ 56 ]  (Figure 11(b) ). Notably, both Figures  11(a)  and  11(b)  demonstrate that our HexaCoder approach produces the fewest vulnerable codes among the models. For example, Figure  11(a)  shows that using our HexaCoder, the model generates  65 65 65 65  vulnerable codes among the top- 5 5 5 5  sampled codes, while SVEN  [ 35 ]  and the pre-trained CodeGen-2B-multi  [ 52 ]  generate 368 and 456 vulnerable codes, respectively. This highlights the effectiveness of the synthesized secure data and our two-step generation approach. In Figure  12 , we also compare the effectiveness of the models in generating functionally correct programs. Figure  11(c)  provides the results of pass@ 1 1 1 1  and pass@ 10 10 10 10  scores for the models on the HumanEval  [ 16 ]  benchmark. In Figure  11(c) , we observe our approach achieves functional correctness accuracy comparable to the base model and even slightly outperforms SVENs model  [ 35 ] . Overall, the results indicate that HexaCoder significantly enhances the CodeGen-2B-multi  [ 52 ]  models ability to generate secure code while maintaining its utility in generating functionally correct programs.",
            "Our HexaCoder approach consists of fine-tuning the pre-trained models using the synthesized secure code data and generating the codes using the two-step generation approach. Here, we investigate the effectiveness of the two-step generation approach on the fine-tuned models with HexaCoder, as well as on models fine-tuned with SVEN  [ 35 ]  and the original pre-trained model. Table  5.3.2  shows the results of the number of vulnerable codes generated using different approaches, focusing on the top- 1 1 1 1  and top- 5 5 5 5  most probable samples. In this table, the CodeGen-350M-multi model serves as the base model in each case, with results provided both with and without the two-step generation approach. In Table  5.3.2 , we compare the results for the pre-trained CodeGen-350M-multi  [ 52 ]  ( Base ), the fine-tuned version of the model using SVEN  [ 35 ] , and the fine-tuned model using our HexaCoder approach. In Table  5.3.2 ,  Two  refers to our two-step approach. We provide the detailed results per each CWE in Appendix  15 .",
            "Figure  13  provides the full version of the input prompts template that we demonstrated in Figure  5 . Here, we provide the full list of instructions that we used in our synthesis pipeline.",
            "In Section  5.2 , we compared three variations of the secure code synthesis approach. In one of these variations, we do not use any security report. In Figure  14 , we provide the input prompt that we used for this specific case.",
            "Tables  10 ,  11 , 12 ,  13 ,  14 , and  15  provide the detailed results of evaluating different variations of the models using CodeLMSec  [ 32 ]  and Pearce et al.  [ 56 ]  benchmarks. These tables provide the number of generated vulnerable codes per CWE for Python and C/C++ codes.",
            "Tables  10  and  11  demonstrate the number of vulnerable codes generated by different variations of CodeGen-350M-multi  [ 52 ] . In Table   10 , we observe that the CodeGen-350M-multi model fine-tuned with our HexaCoder produces fewer vulnerable Python codes compared to the other variations of the model. For example, using our approach, the model generates no code containing CWE-078 vulnerabilities, whereas the pre-trained model generates 12 such instances, and SVEN  [ 35 ]  generates 10. Table  11  also shows that in 9 out of 13 cases, our approach generates the same or fewer number of vulnerable codes than the other approaches.",
            "In Tables  12  and  13  we provide the number of vulnerable codes generated by different variations of InCoder-6B  [ 27 ] . These tables also demonstrate that fine-tuning InCoder-6B  [ 27 ]  using our approach significantly reduces the number of vulnerable codes that can be generated using CodeLMSec  [ 32 ]  and Pearce et al.  [ 56 ]  benchmarks.",
            "Tables  14  and  15  demonstrate the number of vulnerable codes generated by different variations of DeepSeek-Coder-V2-16B  [ 71 ] . In these two tables, as the fine-tuned version of DeepSeek-Coder-V2-16B  [ 71 ]  using SVEN  [ 35 ]  was not provided in the original work, we only report the results for the pre-trained model and our approach. In Table  14 , we observe that our HexaCoder generates fewer vulnerable codes than the pre-trained model in most cases, except for CWE-020. This may be due to the fact that our dataset contains only 21 samples relevant to this CWE, which are not representative enough. In contrast, for other CWEs, we have up to 298 samples. This data imbalance might explain why our approach generated a higher number of vulnerable codes of type CWE-020 compared to the pre-trained model.",
            "Table  16  demonstrates the detailed results of the different approaches with and without using the two-step generation approach. This table provides the number of generated vulnerable codes among the top- 5 5 5 5  most probable samples using the Python prompts of the CodeLMSec  [ 32 ]  benchmark. In Table  16 , we present the results of the pre-trained CodeGen-350M-multi  [ 52 ]  ( Base ), the fine-tuned version of CodeGen-350M-multi using SVEN  [ 35 ] , and the fine-tuned model using our HexaCoder approach. In this table,  Two  refers to our two-step approach.",
            "In Table  16 , we observe that the two-step generation method achieves better performance when used with our approach compared to the others.\". For example, using the two-step generation approach, our HexaCoder generates no vulnerable codes for CWE-078, CWE-094, and CWE-502. In contrast, without the two-step approach, it generates at least 14 vulnerable codes for each of these CWEs. However, for the Base model, the two-step generation approach increases the number of vulnerable codes for CWE-094 and CWE-502."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Statistics of our synthesized secure code data.",
        "table": "S5.T2.6.1",
        "footnotes": [],
        "references": [
            "To address this challenge, we propose an oracle-guided code synthesis pipeline in which LLMs are used to synthesize pairs of vulnerable and fixed code samples. These samples are then used to fine-tune the LLM and guide the model to generate secure codes. Figure  2  provides an overview of our secure code synthesizing procedure. To synthesize vulnerable codes, we employ the few-shot prompting approach proposed by Hajipour et al.  [ 32 ] . This approach employs a few examples of codes with targeted vulnerabilities to generate a diverse set of vulnerable code samples at scale. A security oracle then validates whether the generated code contains the targeted vulnerabilities; only the validated samples are included in the set of vulnerable codes. Each vulnerable code sample, along with its corresponding security report, serves as part of the model input. This security report contains the security oracle report together with a security hint. Based on the input, the model is then prompted to fix the security issues in the vulnerable code. The output of the model is checked again with the security oracle and if the oracle finds no vulnerability, the code is considered fixed and is used as a secure version of the code. The secured codes, together with their corresponding vulnerable versions, form our fine-tuning dataset. Note that in this work we use the GPT-4 model  [ 54 ]  to fix the given codes.",
            "We use this masked negative log-likelihood loss from the previous works  [ 35 ,  36 ] . In Equation  2 ,  x = [ x 1 , ... , x n ] x subscript x 1 ... subscript x n \\mathbf{x}=[x_{1},\\dots,x_{n}] bold_x = [ italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ]  is an example of the synthesized secure code and  m t subscript m t m_{t} italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  is an element of the binary mask  m m \\mathbf{m} bold_m . The mask  m m \\mathbf{m} bold_m  has the sample length as  x x \\mathbf{x} bold_x  and represents the modifications made in the secured version of the synthesized code. Specifically, each element  m t subscript m t m_{t} italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  is set to 1 if the token  x t subscript x t x_{t} italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  is inserted or replaced compared to the corresponding synthesized vulnerable code; otherwise, it is 0. We use the Python library  difflib   [ 21 ]  to extract the token-level differences between the pair of secure and vulnerable codes. This mask forces the model to focus only on the security-related context by zeroing the gradient signal on the other parts of the codes.",
            "To fix the vulnerabilities in each code, as described in Subsection  4.1 , we consider each vulnerable code, along with its corresponding security report, as the input of GPT-4  [ 54 ] . Given the input, we generate up to 1,000 tokens using GPT-4  [ 54 ]  with a temperature of  0.1 0.1 0.1 0.1 . In an initial study, we found that these two parameters provide the best results considering the budget and the models performance. Given the provided input to the GPT-4 model, we generate one sample and extract the fixed code from the provided sample. We then check the generated code again with CodeQL  [ 54 ] , and if the code does not contain any vulnerability, we consider it as an instance of our secure codes. Out of 2,042 vulnerable code samples, our approach successfully fixed the vulnerabilities in 1,776 of them, which we refer to as the  secure code set . This secure set includes 1,414 Python codes and 362 C/C++ codes. Detailed results of the synthesized code data are provided in Table  2 . In this table, the first column lists the type of CWE, and the second column shows the number of synthesized secure Python and C/C++ codes. The overall results for the synthesized secure codes are presented in the last row.",
            "In Listing  5.2.2 , we provide an example of a vulnerable C code with its corresponding fixed code. Note that we only provide part of the generated code, and we chose this example for illustration purposes. The other samples in our dataset have higher complexity. Listing  5.2.2  contains an integer overflow (CWE-190) vulnerability at line 17. This vulnerability arises if the user inputs a value that, when multiplied by 2, exceeds the maximum limit for an integer variable, leading to an integer overflow. In Listing  5.2.2 , we provide the fixed version of the code generated using our code synthesis pipeline. The model fixed the code by including the limits.h library, which provides access to the INT_MAX and INT_MIN macros, representing the maximum and minimum integer values. Since the code in Listing  5.2.2  involves multiplication by 2, the model added a validation at line 21 to check if the input falls within the integer bounds, thereby preventing the overflow. Additionally, the model inserted a check at line 16 to ensure that scanf successfully reads the expected input. More code examples are provided in Appendix  E",
            "We use the synthesized codes (with data statistics provided in Table  2 ) to fine-tune the targeted CodeLMs. For each secure code, we have a corresponding vulnerable code. From this pair, we extract a mask for each data item and use the secure code along with the extracted mask to fine-tune the CodeLMs. Out of the 1,776 data items, 1,421 are used for training, while 335 (  \\approx   20% of the data) are used for validation. To ensure that there is no overlap between the initial vulnerable codes and the prompts used in CodeLMSec  [ 32 ]  and Pearce et al. [ 56 ] , we carefully check for any similarities before including them in our dataset pipeline. To this end, we remove any code in which any prompts of the benchmarks have more than 75% token overlap or share the same function name. Note that the prompts also contain included libraries that can be written in any code, and we also consider them when calculating the tokens overlaps.",
            "Figure  12  presents the overall performance results of CodeGen-2B-multi  [ 52 ]  in terms of code security and functional correctness. The figure compares the results for the pre-trained CodeGen-2B-multi  [ 52 ]  ( Base ), the fine-tuned version using the SVEN method  [ 35 ] , and the fine-tuned version using our HexaCoder approach. Figures  11(a)  and  11(b)  provide the number of generated vulnerable codes using Python and C/C++ prompts of the CodeLMSec  [ 32 ]  and Pearce et al.  [ 56 ]  benchmarks, respectively. These figures illustrate the number of vulnerable codes generated among the top- 1 1 1 1  and top- 5 5 5 5  most probable sampled codes for CodeLMSec  [ 32 ]  (Figure  11(a) ) and the top- 1 1 1 1  and top- 15 15 15 15  most probable sampled codes for Pearce et al. [ 56 ]  (Figure 11(b) ). Notably, both Figures  11(a)  and  11(b)  demonstrate that our HexaCoder approach produces the fewest vulnerable codes among the models. For example, Figure  11(a)  shows that using our HexaCoder, the model generates  65 65 65 65  vulnerable codes among the top- 5 5 5 5  sampled codes, while SVEN  [ 35 ]  and the pre-trained CodeGen-2B-multi  [ 52 ]  generate 368 and 456 vulnerable codes, respectively. This highlights the effectiveness of the synthesized secure data and our two-step generation approach. In Figure  12 , we also compare the effectiveness of the models in generating functionally correct programs. Figure  11(c)  provides the results of pass@ 1 1 1 1  and pass@ 10 10 10 10  scores for the models on the HumanEval  [ 16 ]  benchmark. In Figure  11(c) , we observe our approach achieves functional correctness accuracy comparable to the base model and even slightly outperforms SVENs model  [ 35 ] . Overall, the results indicate that HexaCoder significantly enhances the CodeGen-2B-multi  [ 52 ]  models ability to generate secure code while maintaining its utility in generating functionally correct programs.",
            "Our HexaCoder approach consists of fine-tuning the pre-trained models using the synthesized secure code data and generating the codes using the two-step generation approach. Here, we investigate the effectiveness of the two-step generation approach on the fine-tuned models with HexaCoder, as well as on models fine-tuned with SVEN  [ 35 ]  and the original pre-trained model. Table  5.3.2  shows the results of the number of vulnerable codes generated using different approaches, focusing on the top- 1 1 1 1  and top- 5 5 5 5  most probable samples. In this table, the CodeGen-350M-multi model serves as the base model in each case, with results provided both with and without the two-step generation approach. In Table  5.3.2 , we compare the results for the pre-trained CodeGen-350M-multi  [ 52 ]  ( Base ), the fine-tuned version of the model using SVEN  [ 35 ] , and the fine-tuned model using our HexaCoder approach. In Table  5.3.2 ,  Two  refers to our two-step approach. We provide the detailed results per each CWE in Appendix  15 .",
            "In Table  5.3.2 , we observe that our two-step generation for the pre-trained model ( Base ) can even increase the number of vulnerable codes while for the fine-tuned model with SVEN  [ 35 ] , it reduces the number of vulnerable codes from 258 to 199 for top-5 most probable sample. This indicates that the number of vulnerable codes generated by SVEN with our two-step generation reduced by 22.8% compared to the original SVEN approach. Table  5.3.2  demonstrates that, among the top-5 samples, the fine-tuned model using HexaCoder generates 174 vulnerable codes without the two-step generation, whereas it only produces 81 vulnerable codes when our two-step generation approach is applied. This reflects a 53.4% reduction in the number of vulnerable codes when using HexaCoder with the two-step generation compared to using HexaCoder without it.",
            "Table  5.3.2  demonstrates that the two-step generation approach most effectively reduces the number of vulnerable codes in our method compared to the other approaches. This indicates that our synthesized dataset provides a better representation than SVENs dataset. This is mainly due to the fact that our data synthesis pipeline includes the necessary libraries, while many samples in the SVEN dataset lack these essential components.",
            "In Section  5.2 , we compared three variations of the secure code synthesis approach. In one of these variations, we do not use any security report. In Figure  14 , we provide the input prompt that we used for this specific case.",
            "Tables  10 ,  11 , 12 ,  13 ,  14 , and  15  provide the detailed results of evaluating different variations of the models using CodeLMSec  [ 32 ]  and Pearce et al.  [ 56 ]  benchmarks. These tables provide the number of generated vulnerable codes per CWE for Python and C/C++ codes.",
            "In Tables  12  and  13  we provide the number of vulnerable codes generated by different variations of InCoder-6B  [ 27 ] . These tables also demonstrate that fine-tuning InCoder-6B  [ 27 ]  using our approach significantly reduces the number of vulnerable codes that can be generated using CodeLMSec  [ 32 ]  and Pearce et al.  [ 56 ]  benchmarks."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Impact of each security report component on repair rates for different CWE types. The results provide the percentage of the repaired codes. CodeQL refers to the CodeQL report, and Hint denotes the provided security hint. The analysis covers five specific CWE types, with the final column showing the average repair rate across all CWEs.",
        "table": "S5.T3.6",
        "footnotes": [],
        "references": [
            "In the process of fixing the security vulnerabilities in a given code, we guide the CodeLM using the security reports, which include both the report provided by CodeQL  [ 39 ]  as a security oracle and an additional security hint. More specifically, we analyze the security issue of each generated vulnerable code using CodeQL security queries. CodeQL then generates a report detailing the identified vulnerabilities in the code. We guide the model using the description and line number of each identified vulnerability. In Figure  3 , we provide an example of the CodeQL report for CWE-079, which serves as part of the input for the model.",
            "Figure  5  shows a summarized version of the prompt template that we use as input for the model. In this template, each variable ( {variable} ) is replaced with the corresponding content for the given vulnerable code. Specifically,  {prog_lang}  indicates the programming language of the code,  {num_vuls}  denotes the total number of vulnerabilities,  {vul_count}  is used to enumerate the vulnerabilities,  {line_num}  specifies the line of code in which the corresponding vulnerability was found,  {cwe_type}  describes the type of vulnerability according to the CWE classification,  {cwe_explanation}  provides an explanation of the CWE type that is provided by the CodeQL  [ 39 ]  report (Figure  3 ),  {hint}  represent the security hint (Figure  4 ), and  {vul_code}  contains the actual vulnerable code that needs to be fixed.",
            "In the first step, we condition the CodeLM on the included libraries  [ x 1 , ... , x l ] subscript x 1 ... subscript x l [x_{1},\\dots,x_{l}] [ italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_x start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ]  and generate up to  m  superscript m  m^{\\prime} italic_m start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  additional tokens. We then update the input context to  x  = [ x 1 , ... , x l , ... , x l  , ... , x n ] superscript x  subscript x 1 ... subscript x l ... subscript x superscript l  ... subscript x n \\mathbf{x^{\\prime}}=[x_{1},\\dots,x_{l},\\dots,x_{l^{\\prime}},\\dots,x_{n}] bold_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = [ italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_x start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT , ... , italic_x start_POSTSUBSCRIPT italic_l start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT end_POSTSUBSCRIPT , ... , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ]  by incorporating the newly included libraries and modules. Note that, in the first step, we only consider the newly added libraries and discard the other types of the generated token. In the second step, we generate up to  m m m italic_m  tokens  y = [ y 1 , ... , y m ] y subscript y 1 ... subscript y m \\mathbf{y}=[y_{1},\\dots,y_{m}] bold_y = [ italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_y start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ]  given the updated input context  x  superscript x  \\mathbf{x^{\\prime}} bold_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT . Listing  4.3  provides an example of updating the given input context. Listing  4.3  shows the original given prompt from the CodeLMSec  [ 32 ]  benchmark, and Listing  4.3  shows the updated prompt after conditioning our fine-tuned model on the included libraries of the given input context.",
            "To perform this experiment, we randomly selected 30 vulnerable codes for each CWE to evaluate our secure code synthesis pipeline using three variations of the security report. We limited the selection to 30 programs per CWE to maintain a reasonable compute budget. Table  3  shows the repair rate results using different security report components. We consider a code fixed if CodeQL  [ 39 ]  does not detect any vulnerabilities in it. The first row of this table provides results for the baseline case, in which we do not provide any security report about the vulnerability in the code. In this case, we simply ask the model to identify any vulnerabilities in the code and attempt to repair them directly. We provide the input prompt for this case in Appendix  A . The second row of Table  3  presents the results for the scenario where only the CodeQL report is included as the security report in the input prompt. Comparing the first two rows of Table  3 , we observe that for all of the CWEs, employing the CodeQL report provides helpful guidance for the model to fix the vulnerabilities. For example, for CWE-094, using the CodeQL report, we were able to repair 80.0% of codes, compared to only 43.33% when we didnt use any security report. The last row of Table  3  shows the results for scenarios where we use both the CodeQL report and the security hint to guide the model. These results demonstrate that by using the CodeQL report along with the security hint, we gained the highest repair rate compared to other approaches. On average, with the full security report, we were able to repair 83.99% of the code, whereas using only CodeQL, we repaired just 63.33% of the code.",
            "Our HexaCoder approach consists of fine-tuning the pre-trained models using the synthesized secure code data and generating the codes using the two-step generation approach. Here, we investigate the effectiveness of the two-step generation approach on the fine-tuned models with HexaCoder, as well as on models fine-tuned with SVEN  [ 35 ]  and the original pre-trained model. Table  5.3.2  shows the results of the number of vulnerable codes generated using different approaches, focusing on the top- 1 1 1 1  and top- 5 5 5 5  most probable samples. In this table, the CodeGen-350M-multi model serves as the base model in each case, with results provided both with and without the two-step generation approach. In Table  5.3.2 , we compare the results for the pre-trained CodeGen-350M-multi  [ 52 ]  ( Base ), the fine-tuned version of the model using SVEN  [ 35 ] , and the fine-tuned model using our HexaCoder approach. In Table  5.3.2 ,  Two  refers to our two-step approach. We provide the detailed results per each CWE in Appendix  15 .",
            "In Table  5.3.2 , we observe that our two-step generation for the pre-trained model ( Base ) can even increase the number of vulnerable codes while for the fine-tuned model with SVEN  [ 35 ] , it reduces the number of vulnerable codes from 258 to 199 for top-5 most probable sample. This indicates that the number of vulnerable codes generated by SVEN with our two-step generation reduced by 22.8% compared to the original SVEN approach. Table  5.3.2  demonstrates that, among the top-5 samples, the fine-tuned model using HexaCoder generates 174 vulnerable codes without the two-step generation, whereas it only produces 81 vulnerable codes when our two-step generation approach is applied. This reflects a 53.4% reduction in the number of vulnerable codes when using HexaCoder with the two-step generation compared to using HexaCoder without it.",
            "Table  5.3.2  demonstrates that the two-step generation approach most effectively reduces the number of vulnerable codes in our method compared to the other approaches. This indicates that our synthesized dataset provides a better representation than SVENs dataset. This is mainly due to the fact that our data synthesis pipeline includes the necessary libraries, while many samples in the SVEN dataset lack these essential components.",
            "Figure  13  provides the full version of the input prompts template that we demonstrated in Figure  5 . Here, we provide the full list of instructions that we used in our synthesis pipeline.",
            "Tables  10 ,  11 , 12 ,  13 ,  14 , and  15  provide the detailed results of evaluating different variations of the models using CodeLMSec  [ 32 ]  and Pearce et al.  [ 56 ]  benchmarks. These tables provide the number of generated vulnerable codes per CWE for Python and C/C++ codes.",
            "In Tables  12  and  13  we provide the number of vulnerable codes generated by different variations of InCoder-6B  [ 27 ] . These tables also demonstrate that fine-tuning InCoder-6B  [ 27 ]  using our approach significantly reduces the number of vulnerable codes that can be generated using CodeLMSec  [ 32 ]  and Pearce et al.  [ 56 ]  benchmarks."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  Number of vulnerable code samples generated by the  CodeGen-2B-multi  model as evaluated using the  CodeLMSec benchmark .  Base  represents the original model, while  SVEN   [ 35 ]  and  HexaCoder  refer to the CodeGen-2B-multi model fine-tuned by each respective approach. The table presents the number of vulnerable codes among the top- 5 5 5 5  samples for each evaluated CWE, with separate columns for Python (left) and C/C++ (right). The  Other  column refers to the rest of the CWEs that are identified by CodeQL. The  Total  column shows the sum of vulnerable samples.",
        "table": "S5.SS3.SSS2.6.6",
        "footnotes": [
            ""
        ],
        "references": [
            "The CodeQL report provides a comprehensive overview of the identified vulnerabilities; however, it does not describe potential mitigation strategies. To address this shortcoming, we guide the model by providing security hints that describe possible mitigation implementations for the corresponding CWE. These mitigation descriptions are adapted from the Potential Mitigations section on each CWE page provided by MITRE  [ 51 ]  and Semgrep documentation  [ 40 ] . Figure  4  provides an example of a security hint used for CWE-079. We provide a complete list of hints in Appendix  B .",
            "Figure  5  shows a summarized version of the prompt template that we use as input for the model. In this template, each variable ( {variable} ) is replaced with the corresponding content for the given vulnerable code. Specifically,  {prog_lang}  indicates the programming language of the code,  {num_vuls}  denotes the total number of vulnerabilities,  {vul_count}  is used to enumerate the vulnerabilities,  {line_num}  specifies the line of code in which the corresponding vulnerability was found,  {cwe_type}  describes the type of vulnerability according to the CWE classification,  {cwe_explanation}  provides an explanation of the CWE type that is provided by the CodeQL  [ 39 ]  report (Figure  3 ),  {hint}  represent the security hint (Figure  4 ), and  {vul_code}  contains the actual vulnerable code that needs to be fixed.",
            "In the first step, we condition the CodeLM on the included libraries  [ x 1 , ... , x l ] subscript x 1 ... subscript x l [x_{1},\\dots,x_{l}] [ italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_x start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ]  and generate up to  m  superscript m  m^{\\prime} italic_m start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  additional tokens. We then update the input context to  x  = [ x 1 , ... , x l , ... , x l  , ... , x n ] superscript x  subscript x 1 ... subscript x l ... subscript x superscript l  ... subscript x n \\mathbf{x^{\\prime}}=[x_{1},\\dots,x_{l},\\dots,x_{l^{\\prime}},\\dots,x_{n}] bold_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = [ italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_x start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT , ... , italic_x start_POSTSUBSCRIPT italic_l start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT end_POSTSUBSCRIPT , ... , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ]  by incorporating the newly included libraries and modules. Note that, in the first step, we only consider the newly added libraries and discard the other types of the generated token. In the second step, we generate up to  m m m italic_m  tokens  y = [ y 1 , ... , y m ] y subscript y 1 ... subscript y m \\mathbf{y}=[y_{1},\\dots,y_{m}] bold_y = [ italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_y start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ]  given the updated input context  x  superscript x  \\mathbf{x^{\\prime}} bold_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT . Listing  4.3  provides an example of updating the given input context. Listing  4.3  shows the original given prompt from the CodeLMSec  [ 32 ]  benchmark, and Listing  4.3  shows the updated prompt after conditioning our fine-tuned model on the included libraries of the given input context.",
            "To fix the vulnerabilities in each code, as described in Subsection  4.1 , we consider each vulnerable code, along with its corresponding security report, as the input of GPT-4  [ 54 ] . Given the input, we generate up to 1,000 tokens using GPT-4  [ 54 ]  with a temperature of  0.1 0.1 0.1 0.1 . In an initial study, we found that these two parameters provide the best results considering the budget and the models performance. Given the provided input to the GPT-4 model, we generate one sample and extract the fixed code from the provided sample. We then check the generated code again with CodeQL  [ 54 ] , and if the code does not contain any vulnerability, we consider it as an instance of our secure codes. Out of 2,042 vulnerable code samples, our approach successfully fixed the vulnerabilities in 1,776 of them, which we refer to as the  secure code set . This secure set includes 1,414 Python codes and 362 C/C++ codes. Detailed results of the synthesized code data are provided in Table  2 . In this table, the first column lists the type of CWE, and the second column shows the number of synthesized secure Python and C/C++ codes. The overall results for the synthesized secure codes are presented in the last row.",
            "Tables  4  and   5  provide the detailed results of the number of vulnerable codes generated by different variations of CodeGen-2B-multi  [ 52 ]  for each CWE. These tables also show the total number of generated codes vulnerable for each programming language. In Table  4 , we present the results of evaluating the models using CodeLMSec benchmark  [ 32 ] . Our HexaCoder approach consistently reduces or maintains the number of generated vulnerable codes compared to both the pre-trained CodeGen-2B-multi  [ 52 ]  and SVEN  [ 35 ] , as shown in Table  4 . For example, for CWE-094 in Python codes, using our approach, the model only generates 4 vulnerable codes, while the pre-trained model and SVEN  [ 35 ]  generated 48 and 28 vulnerable codes, respectively. Table  5  provides the results of evaluating the model using the Pearce et al. benchmark  [ 56 ] . In this table, we also observe that our approach reduces or maintains the number of vulnerable codes for nearly all CWEs compared to the other approaches. This table shows that HexaCoder generates no vulnerable code for CWE-022, CWE-502, and CWE-611 in Python, as well as for CWE-022, CWE-190, and CWE-476 in C/C++. In contrast, other models generate at least two or more vulnerable codes for each of these CWEs. The results shown in Tables  4  and  5  indicate that our approach significantly reduces the generation of vulnerable code for various CWEs compared to the other methods. This demonstrates the effectiveness and generalizability of our method in producing secure code across different scenarios.",
            "In Section  5.2 , we compared three variations of the secure code synthesis approach. In one of these variations, we do not use any security report. In Figure  14 , we provide the input prompt that we used for this specific case.",
            "Tables  10 ,  11 , 12 ,  13 ,  14 , and  15  provide the detailed results of evaluating different variations of the models using CodeLMSec  [ 32 ]  and Pearce et al.  [ 56 ]  benchmarks. These tables provide the number of generated vulnerable codes per CWE for Python and C/C++ codes.",
            "Tables  14  and  15  demonstrate the number of vulnerable codes generated by different variations of DeepSeek-Coder-V2-16B  [ 71 ] . In these two tables, as the fine-tuned version of DeepSeek-Coder-V2-16B  [ 71 ]  using SVEN  [ 35 ]  was not provided in the original work, we only report the results for the pre-trained model and our approach. In Table  14 , we observe that our HexaCoder generates fewer vulnerable codes than the pre-trained model in most cases, except for CWE-020. This may be due to the fact that our dataset contains only 21 samples relevant to this CWE, which are not representative enough. In contrast, for other CWEs, we have up to 298 samples. This data imbalance might explain why our approach generated a higher number of vulnerable codes of type CWE-020 compared to the pre-trained model."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 :  Number of vulnerable code samples generated by the  CodeGen-2B-multi  model as evaluated using the  Pearce et al. benchmark   [ 56 ]  .  Base  represents the original model, while  SVEN   [ 35 ]  and  HexaCoder  refer to the CodeGen-2B-multi model fine-tuned by each respective approach. The table presents the number of vulnerable codes among the top- 15 15 15 15  samples for each evaluated CWE, with separate columns for Python (left) and C/C++ (right). The  Other  column refers to the rest of the CWEs that are identified by CodeQL. The  Total  column shows the sum of vulnerable samples.",
        "table": "A2.T8.6",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "Figure  5  shows a summarized version of the prompt template that we use as input for the model. In this template, each variable ( {variable} ) is replaced with the corresponding content for the given vulnerable code. Specifically,  {prog_lang}  indicates the programming language of the code,  {num_vuls}  denotes the total number of vulnerabilities,  {vul_count}  is used to enumerate the vulnerabilities,  {line_num}  specifies the line of code in which the corresponding vulnerability was found,  {cwe_type}  describes the type of vulnerability according to the CWE classification,  {cwe_explanation}  provides an explanation of the CWE type that is provided by the CodeQL  [ 39 ]  report (Figure  3 ),  {hint}  represent the security hint (Figure  4 ), and  {vul_code}  contains the actual vulnerable code that needs to be fixed.",
            "In Listing  5.2.2 , we provide an example of a vulnerable C code with its corresponding fixed code. Note that we only provide part of the generated code, and we chose this example for illustration purposes. The other samples in our dataset have higher complexity. Listing  5.2.2  contains an integer overflow (CWE-190) vulnerability at line 17. This vulnerability arises if the user inputs a value that, when multiplied by 2, exceeds the maximum limit for an integer variable, leading to an integer overflow. In Listing  5.2.2 , we provide the fixed version of the code generated using our code synthesis pipeline. The model fixed the code by including the limits.h library, which provides access to the INT_MAX and INT_MIN macros, representing the maximum and minimum integer values. Since the code in Listing  5.2.2  involves multiplication by 2, the model added a validation at line 21 to check if the input falls within the integer bounds, thereby preventing the overflow. Additionally, the model inserted a check at line 16 to ensure that scanf successfully reads the expected input. More code examples are provided in Appendix  E",
            "To evaluate each model, we sample  q q q italic_q  outputs for each given prompt. Following previous work  [ 32 ] , we set  q = 5 q 5 q=5 italic_q = 5  for the CodeLMSec prompts, and since the benchmark provided by Pearce et al.  [ 56 ]  contains fewer prompts, we set  q = 15 q 15 q=15 italic_q = 15 . We use nucleus sampling to sample  q q q italic_q  programs for each given prompt. As mentioned in Section  5.1 , we generate up to 200 new tokens per prompt for both the CodeLMSec  [ 32 ]  and Pearce et al.  [ 56 ]  benchmarks. To ensure fairness, in our two-step generation approach, we set the maximum number of tokens to 20 for the first step and 180 for the second step. In the first step, we set the maximum token number to 20 to provide the model with sufficient capacity to include the necessary libraries. We consider a code vulnerable if CodeQL identifies a vulnerability in it. We report the number of vulnerable codes within the top- x x x italic_x  codes, where top- x x x italic_x  refers to the most probable sampled codes out of all the sampled codes. For example, top- 1 1 1 1  represents the most probable sampled code among  q q q italic_q  sampled codes.",
            "Tables  4  and   5  provide the detailed results of the number of vulnerable codes generated by different variations of CodeGen-2B-multi  [ 52 ]  for each CWE. These tables also show the total number of generated codes vulnerable for each programming language. In Table  4 , we present the results of evaluating the models using CodeLMSec benchmark  [ 32 ] . Our HexaCoder approach consistently reduces or maintains the number of generated vulnerable codes compared to both the pre-trained CodeGen-2B-multi  [ 52 ]  and SVEN  [ 35 ] , as shown in Table  4 . For example, for CWE-094 in Python codes, using our approach, the model only generates 4 vulnerable codes, while the pre-trained model and SVEN  [ 35 ]  generated 48 and 28 vulnerable codes, respectively. Table  5  provides the results of evaluating the model using the Pearce et al. benchmark  [ 56 ] . In this table, we also observe that our approach reduces or maintains the number of vulnerable codes for nearly all CWEs compared to the other approaches. This table shows that HexaCoder generates no vulnerable code for CWE-022, CWE-502, and CWE-611 in Python, as well as for CWE-022, CWE-190, and CWE-476 in C/C++. In contrast, other models generate at least two or more vulnerable codes for each of these CWEs. The results shown in Tables  4  and  5  indicate that our approach significantly reduces the generation of vulnerable code for various CWEs compared to the other methods. This demonstrates the effectiveness and generalizability of our method in producing secure code across different scenarios.",
            "Our HexaCoder approach consists of fine-tuning the pre-trained models using the synthesized secure code data and generating the codes using the two-step generation approach. Here, we investigate the effectiveness of the two-step generation approach on the fine-tuned models with HexaCoder, as well as on models fine-tuned with SVEN  [ 35 ]  and the original pre-trained model. Table  5.3.2  shows the results of the number of vulnerable codes generated using different approaches, focusing on the top- 1 1 1 1  and top- 5 5 5 5  most probable samples. In this table, the CodeGen-350M-multi model serves as the base model in each case, with results provided both with and without the two-step generation approach. In Table  5.3.2 , we compare the results for the pre-trained CodeGen-350M-multi  [ 52 ]  ( Base ), the fine-tuned version of the model using SVEN  [ 35 ] , and the fine-tuned model using our HexaCoder approach. In Table  5.3.2 ,  Two  refers to our two-step approach. We provide the detailed results per each CWE in Appendix  15 .",
            "In Table  5.3.2 , we observe that our two-step generation for the pre-trained model ( Base ) can even increase the number of vulnerable codes while for the fine-tuned model with SVEN  [ 35 ] , it reduces the number of vulnerable codes from 258 to 199 for top-5 most probable sample. This indicates that the number of vulnerable codes generated by SVEN with our two-step generation reduced by 22.8% compared to the original SVEN approach. Table  5.3.2  demonstrates that, among the top-5 samples, the fine-tuned model using HexaCoder generates 174 vulnerable codes without the two-step generation, whereas it only produces 81 vulnerable codes when our two-step generation approach is applied. This reflects a 53.4% reduction in the number of vulnerable codes when using HexaCoder with the two-step generation compared to using HexaCoder without it.",
            "Table  5.3.2  demonstrates that the two-step generation approach most effectively reduces the number of vulnerable codes in our method compared to the other approaches. This indicates that our synthesized dataset provides a better representation than SVENs dataset. This is mainly due to the fact that our data synthesis pipeline includes the necessary libraries, while many samples in the SVEN dataset lack these essential components.",
            "Figure  13  provides the full version of the input prompts template that we demonstrated in Figure  5 . Here, we provide the full list of instructions that we used in our synthesis pipeline.",
            "In Section  5.2 , we compared three variations of the secure code synthesis approach. In one of these variations, we do not use any security report. In Figure  14 , we provide the input prompt that we used for this specific case.",
            "Tables  10 ,  11 , 12 ,  13 ,  14 , and  15  provide the detailed results of evaluating different variations of the models using CodeLMSec  [ 32 ]  and Pearce et al.  [ 56 ]  benchmarks. These tables provide the number of generated vulnerable codes per CWE for Python and C/C++ codes.",
            "Tables  14  and  15  demonstrate the number of vulnerable codes generated by different variations of DeepSeek-Coder-V2-16B  [ 71 ] . In these two tables, as the fine-tuned version of DeepSeek-Coder-V2-16B  [ 71 ]  using SVEN  [ 35 ]  was not provided in the original work, we only report the results for the pre-trained model and our approach. In Table  14 , we observe that our HexaCoder generates fewer vulnerable codes than the pre-trained model in most cases, except for CWE-020. This may be due to the fact that our dataset contains only 21 samples relevant to this CWE, which are not representative enough. In contrast, for other CWEs, we have up to 298 samples. This data imbalance might explain why our approach generated a higher number of vulnerable codes of type CWE-020 compared to the pre-trained model."
        ]
    },
    "id_table_6": {
        "caption": "Table 6 :  The overall result of different models in generating vulnerable codes and generating functionally correct codes.  Base  represents the original model, while  SVEN   [ 35 ]  and  HexaCoder  refer to the models fine-tuned by each respective approach. For the CodeLMSec  [ 32 ]  and Pearce et al.  [ 56 ]  benchmarks, the number of generated vulnerable codes is reported. For the HumanEval  [ 16 ] , the pass@ 10 10 10 10  score is reported.",
        "table": "A3.T9.5.5",
        "footnotes": [
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Table  6  presents the overall results for three additional models. It shows the number of vulnerable codes generated by each model, as well as their performance in generating functionally correct codes. In this table, we present the results for three models: CodeGen-350M-multi  [ 52 ] , InCoder-6B  [ 27 ] , and DeepSeek-Coder-V2-16B  [ 71 ] . For each model, we include the results of the original pre-trained version ( Base ) and the fine-tuned versions using SVEN  [ 35 ]  ( SVEN ) and our HexaCoder ( HexaCoder ). Note that since the SVEN fine-tuned version of DeepSeek-Coder-V2-16B  [ 71 ]  was not provided by the authors, we only report the results for the original model and HexaCoder for this case. For each of these models and their different variations, we provide the number of generated Python and C/C++ vulnerable codes using the CodeLMSec  [ 32 ]  and Pearce et al.  [ 56 ]  benchmarks. Furthermore, we also present the performance of these models on the HumanEval  [ 16 ]  benchmark. Specifically, for CodeLMSec  [ 32 ]  and Pearce et al.  [ 56 ] , we provide the number of generated vulnerable codes among the top-5 and top-15 most probable samples, while for HumanEval, we report the pass@10 score. Detailed results per each CWE are available in Appendix  D .",
            "Table  6  demonstrates that HexaCoder consistently generates a lower number of vulnerable codes compared to the other approach for various models. This pattern holds for the number of generated vulnerable codes using both the benchmarks. For example, the fine-tuned version of InCoder-6B  [ 27 ]  using HexaCoder generates a total number of  215 215 215 215  vulnerable codes using the CodeLMSec benchmark, while the SVEN version  [ 35 ]  of this model produces  457 457 457 457  vulnerable codes. Additionally, as shown in Table  6 , HexaCoder demonstrates functional correctness accuracy that is comparable to, or even exceeds, that of the original pre-trained models. For instance, our approach achieved a pass@ 10 10 10 10  score of  72.0 72.0 72.0 72.0  for the DeepSeek-Coder-V2-16B  [ 71 ] , surpassing the pre-trained models score of  70.5 70.5 70.5 70.5 . The results in Table  6  demonstrate the effectiveness of the proposed approach in enhancing the ability of various models to generate secure code, while also maintaining their performance in generating functionally correct code.",
            "Table  16  demonstrates the detailed results of the different approaches with and without using the two-step generation approach. This table provides the number of generated vulnerable codes among the top- 5 5 5 5  most probable samples using the Python prompts of the CodeLMSec  [ 32 ]  benchmark. In Table  16 , we present the results of the pre-trained CodeGen-350M-multi  [ 52 ]  ( Base ), the fine-tuned version of CodeGen-350M-multi using SVEN  [ 35 ] , and the fine-tuned model using our HexaCoder approach. In this table,  Two  refers to our two-step approach.",
            "In Table  16 , we observe that the two-step generation method achieves better performance when used with our approach compared to the others.\". For example, using the two-step generation approach, our HexaCoder generates no vulnerable codes for CWE-078, CWE-094, and CWE-502. In contrast, without the two-step approach, it generates at least 14 vulnerable codes for each of these CWEs. However, for the Base model, the two-step generation approach increases the number of vulnerable codes for CWE-094 and CWE-502."
        ]
    }
}