{
    "PAPER'S NUMBER OF TABLES": 4,
    "S3.SS2.7": {
        "caption": "Table 1: Average benefit of collaboration and FL. The lowest accuracy and highest fairness gap are bold. The standard deviation across five runs is indicated between parentheses. The green arrows (red arrows) represent the positive (negative) impacts of FL or centralized learning compared to standalone training (i.e., FL or centralized training increases the accuracy or decreases the fairness gap compared to standalone training). ",
        "table": "<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<div id=\"S3.SS2.2.2\" class=\"ltx_inline-block ltx_figure_panel ltx_transformed_outer\" style=\"width:433.6pt;height:50pt;vertical-align:-40.5pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(17.5pt,-0.4pt) scale(1.08790103394876,1.08790103394876) ;\"><span id=\"S3.SS2.2.2.3\" class=\"ltx_ERROR undefined\">\\csvreader</span>\n<p id=\"S3.SS2.2.2.2\" class=\"ltx_p\">[tabular=c*3a*3e*3d,table head=   <span id=\"S3.SS2.2.2.2.2\" class=\"ltx_text\" style=\"background-color:#FFF6F6;\"> Accuracy    <span id=\"S3.SS2.2.2.2.2.2\" class=\"ltx_text\" style=\"background-color:#F6FCF2;\"> <math id=\"S3.SS2.1.1.1.1.1.m1.1\" class=\"ltx_Math\" style=\"background-color:#F6FCF2;\" alttext=\"\\Delta^{EO}\" display=\"inline\"><semantics id=\"S3.SS2.1.1.1.1.1.m1.1a\"><msup id=\"S3.SS2.1.1.1.1.1.m1.1.1\" xref=\"S3.SS2.1.1.1.1.1.m1.1.1.cmml\"><mi mathbackground=\"#F6FCF2\" mathvariant=\"normal\" id=\"S3.SS2.1.1.1.1.1.m1.1.1.2\" xref=\"S3.SS2.1.1.1.1.1.m1.1.1.2.cmml\">Î”</mi><mrow id=\"S3.SS2.1.1.1.1.1.m1.1.1.3\" xref=\"S3.SS2.1.1.1.1.1.m1.1.1.3.cmml\"><mi mathbackground=\"#F6FCF2\" id=\"S3.SS2.1.1.1.1.1.m1.1.1.3.2\" xref=\"S3.SS2.1.1.1.1.1.m1.1.1.3.2.cmml\">E</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.SS2.1.1.1.1.1.m1.1.1.3.1\" xref=\"S3.SS2.1.1.1.1.1.m1.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#F6FCF2\" id=\"S3.SS2.1.1.1.1.1.m1.1.1.3.3\" xref=\"S3.SS2.1.1.1.1.1.m1.1.1.3.3.cmml\">O</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS2.1.1.1.1.1.m1.1b\"><apply id=\"S3.SS2.1.1.1.1.1.m1.1.1.cmml\" xref=\"S3.SS2.1.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS2.1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S3.SS2.1.1.1.1.1.m1.1.1\">superscript</csymbol><ci id=\"S3.SS2.1.1.1.1.1.m1.1.1.2.cmml\" xref=\"S3.SS2.1.1.1.1.1.m1.1.1.2\">Î”</ci><apply id=\"S3.SS2.1.1.1.1.1.m1.1.1.3.cmml\" xref=\"S3.SS2.1.1.1.1.1.m1.1.1.3\"><times id=\"S3.SS2.1.1.1.1.1.m1.1.1.3.1.cmml\" xref=\"S3.SS2.1.1.1.1.1.m1.1.1.3.1\"></times><ci id=\"S3.SS2.1.1.1.1.1.m1.1.1.3.2.cmml\" xref=\"S3.SS2.1.1.1.1.1.m1.1.1.3.2\">ğ¸</ci><ci id=\"S3.SS2.1.1.1.1.1.m1.1.1.3.3.cmml\" xref=\"S3.SS2.1.1.1.1.1.m1.1.1.3.3\">ğ‘‚</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS2.1.1.1.1.1.m1.1c\">\\Delta^{EO}</annotation></semantics></math>    <math id=\"S3.SS2.2.2.2.2.2.m2.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{DP}\" display=\"inline\"><semantics id=\"S3.SS2.2.2.2.2.2.m2.1a\"><msup id=\"S3.SS2.2.2.2.2.2.m2.1.1\" xref=\"S3.SS2.2.2.2.2.2.m2.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"S3.SS2.2.2.2.2.2.m2.1.1.2\" xref=\"S3.SS2.2.2.2.2.2.m2.1.1.2.cmml\">Î”</mi><mrow id=\"S3.SS2.2.2.2.2.2.m2.1.1.3\" xref=\"S3.SS2.2.2.2.2.2.m2.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS2.2.2.2.2.2.m2.1.1.3.2\" xref=\"S3.SS2.2.2.2.2.2.m2.1.1.3.2.cmml\">D</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.SS2.2.2.2.2.2.m2.1.1.3.1\" xref=\"S3.SS2.2.2.2.2.2.m2.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#F5FAFC\" id=\"S3.SS2.2.2.2.2.2.m2.1.1.3.3\" xref=\"S3.SS2.2.2.2.2.2.m2.1.1.3.3.cmml\">P</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS2.2.2.2.2.2.m2.1b\"><apply id=\"S3.SS2.2.2.2.2.2.m2.1.1.cmml\" xref=\"S3.SS2.2.2.2.2.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS2.2.2.2.2.2.m2.1.1.1.cmml\" xref=\"S3.SS2.2.2.2.2.2.m2.1.1\">superscript</csymbol><ci id=\"S3.SS2.2.2.2.2.2.m2.1.1.2.cmml\" xref=\"S3.SS2.2.2.2.2.2.m2.1.1.2\">Î”</ci><apply id=\"S3.SS2.2.2.2.2.2.m2.1.1.3.cmml\" xref=\"S3.SS2.2.2.2.2.2.m2.1.1.3\"><times id=\"S3.SS2.2.2.2.2.2.m2.1.1.3.1.cmml\" xref=\"S3.SS2.2.2.2.2.2.m2.1.1.3.1\"></times><ci id=\"S3.SS2.2.2.2.2.2.m2.1.1.3.2.cmml\" xref=\"S3.SS2.2.2.2.2.2.m2.1.1.3.2\">ğ·</ci><ci id=\"S3.SS2.2.2.2.2.2.m2.1.1.3.3.cmml\" xref=\"S3.SS2.2.2.2.2.2.m2.1.1.3.3\">ğ‘ƒ</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS2.2.2.2.2.2.m2.1c\">\\Delta^{DP}</annotation></semantics></math><span id=\"S3.SS2.2.2.2.2.2.1\" class=\"ltx_text\" style=\"background-color:#F5FAFC;\">  \n<br class=\"ltx_break\">Dataset  Standalone  Centralized  FedAvg  Standalone  Centralized  FedAvg  Standalone  Centralized  FedAvg \n<br class=\"ltx_break\">, table foot=]data/data_benefit.csv\n<span id=\"S3.SS2.2.2.2.2.2.1.1\" class=\"ltx_ERROR undefined\">\\csvcoli</span>- <span id=\"S3.SS2.2.2.2.2.2.1.2\" class=\"ltx_ERROR undefined\">\\csvcolii</span></span></span></span></p>\n</span></div>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S3.SS2.7.7\" class=\"ltx_p ltx_figure_panel\"><span id=\"S3.SS2.7.7.5\" class=\"ltx_text\" style=\"background-color:#F5FAFC;\">TableÂ <a href=\"#S3.SS2\" title=\"3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a> presents the average accuracy and fairness gaps of the centralized model, FL model, and standalone models on local datasets. Centralized training is observed to improve accuracy and fairness at the same time for parties. For instance, on the Income dataset, centralized training improves the accuracy by <math id=\"S3.SS2.3.3.1.m1.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"6\\%\" display=\"inline\"><semantics id=\"S3.SS2.3.3.1.m1.1a\"><mrow id=\"S3.SS2.3.3.1.m1.1.1\" xref=\"S3.SS2.3.3.1.m1.1.1.cmml\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS2.3.3.1.m1.1.1.2\" xref=\"S3.SS2.3.3.1.m1.1.1.2.cmml\">6</mn><mo mathbackground=\"#F5FAFC\" id=\"S3.SS2.3.3.1.m1.1.1.1\" xref=\"S3.SS2.3.3.1.m1.1.1.1.cmml\">%</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS2.3.3.1.m1.1b\"><apply id=\"S3.SS2.3.3.1.m1.1.1.cmml\" xref=\"S3.SS2.3.3.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS2.3.3.1.m1.1.1.1.cmml\" xref=\"S3.SS2.3.3.1.m1.1.1.1\">percent</csymbol><cn type=\"integer\" id=\"S3.SS2.3.3.1.m1.1.1.2.cmml\" xref=\"S3.SS2.3.3.1.m1.1.1.2\">6</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS2.3.3.1.m1.1c\">6\\%</annotation></semantics></math> and reduces the fairness gap across racial groups by <math id=\"S3.SS2.4.4.2.m2.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"5.5\\%\" display=\"inline\"><semantics id=\"S3.SS2.4.4.2.m2.1a\"><mrow id=\"S3.SS2.4.4.2.m2.1.1\" xref=\"S3.SS2.4.4.2.m2.1.1.cmml\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS2.4.4.2.m2.1.1.2\" xref=\"S3.SS2.4.4.2.m2.1.1.2.cmml\">5.5</mn><mo mathbackground=\"#F5FAFC\" id=\"S3.SS2.4.4.2.m2.1.1.1\" xref=\"S3.SS2.4.4.2.m2.1.1.1.cmml\">%</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS2.4.4.2.m2.1b\"><apply id=\"S3.SS2.4.4.2.m2.1.1.cmml\" xref=\"S3.SS2.4.4.2.m2.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS2.4.4.2.m2.1.1.1.cmml\" xref=\"S3.SS2.4.4.2.m2.1.1.1\">percent</csymbol><cn type=\"float\" id=\"S3.SS2.4.4.2.m2.1.1.2.cmml\" xref=\"S3.SS2.4.4.2.m2.1.1.2\">5.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS2.4.4.2.m2.1c\">5.5\\%</annotation></semantics></math> with respect to equalized odds and by <math id=\"S3.SS2.5.5.3.m3.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"5.5\\%\" display=\"inline\"><semantics id=\"S3.SS2.5.5.3.m3.1a\"><mrow id=\"S3.SS2.5.5.3.m3.1.1\" xref=\"S3.SS2.5.5.3.m3.1.1.cmml\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS2.5.5.3.m3.1.1.2\" xref=\"S3.SS2.5.5.3.m3.1.1.2.cmml\">5.5</mn><mo mathbackground=\"#F5FAFC\" id=\"S3.SS2.5.5.3.m3.1.1.1\" xref=\"S3.SS2.5.5.3.m3.1.1.1.cmml\">%</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS2.5.5.3.m3.1b\"><apply id=\"S3.SS2.5.5.3.m3.1.1.cmml\" xref=\"S3.SS2.5.5.3.m3.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS2.5.5.3.m3.1.1.1.cmml\" xref=\"S3.SS2.5.5.3.m3.1.1.1\">percent</csymbol><cn type=\"float\" id=\"S3.SS2.5.5.3.m3.1.1.2.cmml\" xref=\"S3.SS2.5.5.3.m3.1.1.2\">5.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS2.5.5.3.m3.1c\">5.5\\%</annotation></semantics></math> with respect to demographic parity. However, FL may not achieve the same benefit as centralized training in terms of fairness and can even exacerbate the fairness issue for parties. For example, on the Income dataset, the EO fairness gap of the FL model for the sex groups increases by <math id=\"S3.SS2.6.6.4.m4.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"35\\%\" display=\"inline\"><semantics id=\"S3.SS2.6.6.4.m4.1a\"><mrow id=\"S3.SS2.6.6.4.m4.1.1\" xref=\"S3.SS2.6.6.4.m4.1.1.cmml\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS2.6.6.4.m4.1.1.2\" xref=\"S3.SS2.6.6.4.m4.1.1.2.cmml\">35</mn><mo mathbackground=\"#F5FAFC\" id=\"S3.SS2.6.6.4.m4.1.1.1\" xref=\"S3.SS2.6.6.4.m4.1.1.1.cmml\">%</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS2.6.6.4.m4.1b\"><apply id=\"S3.SS2.6.6.4.m4.1.1.cmml\" xref=\"S3.SS2.6.6.4.m4.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS2.6.6.4.m4.1.1.1.cmml\" xref=\"S3.SS2.6.6.4.m4.1.1.1\">percent</csymbol><cn type=\"integer\" id=\"S3.SS2.6.6.4.m4.1.1.2.cmml\" xref=\"S3.SS2.6.6.4.m4.1.1.2\">35</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS2.6.6.4.m4.1c\">35\\%</annotation></semantics></math>, and the DP fairness gap increases by <math id=\"S3.SS2.7.7.5.m5.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"29.9\\%\" display=\"inline\"><semantics id=\"S3.SS2.7.7.5.m5.1a\"><mrow id=\"S3.SS2.7.7.5.m5.1.1\" xref=\"S3.SS2.7.7.5.m5.1.1.cmml\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS2.7.7.5.m5.1.1.2\" xref=\"S3.SS2.7.7.5.m5.1.1.2.cmml\">29.9</mn><mo mathbackground=\"#F5FAFC\" id=\"S3.SS2.7.7.5.m5.1.1.1\" xref=\"S3.SS2.7.7.5.m5.1.1.1.cmml\">%</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS2.7.7.5.m5.1b\"><apply id=\"S3.SS2.7.7.5.m5.1.1.cmml\" xref=\"S3.SS2.7.7.5.m5.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS2.7.7.5.m5.1.1.1.cmml\" xref=\"S3.SS2.7.7.5.m5.1.1.1\">percent</csymbol><cn type=\"float\" id=\"S3.SS2.7.7.5.m5.1.1.2.cmml\" xref=\"S3.SS2.7.7.5.m5.1.1.2\">29.9</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS2.7.7.5.m5.1c\">29.9\\%</annotation></semantics></math>, compared to standalone models. We include results for other popular FL algorithms, including FedNovaÂ <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang etÂ al., <a href=\"#bib.bib46\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>, ScaffoldÂ <cite class=\"ltx_cite ltx_citemacro_cite\">Karimireddy etÂ al. (<a href=\"#bib.bib24\" title=\"\" class=\"ltx_ref\">2020b</a>)</cite>, FedOptÂ <cite class=\"ltx_cite ltx_citemacro_citep\">(Reddi etÂ al., <a href=\"#bib.bib40\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>, FedProxÂ <cite class=\"ltx_cite ltx_citemacro_cite\">Li etÂ al. (<a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>, and MimeÂ <cite class=\"ltx_cite ltx_citemacro_citep\">(Karimireddy etÂ al., <a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">2020a</a>)</cite>, in TableÂ <a href=\"#A2.T2\" title=\"Table 2 â€£ B.5 Effect of FL algorithm â€£ Appendix B Additional Experimental Results â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> in AppendixÂ <a href=\"#A2\" title=\"Appendix B Additional Experimental Results â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>, which show a similar pattern to that of the FedAvg algorithm. AppendixÂ <a href=\"#A2\" title=\"Appendix B Additional Experimental Results â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> provides more detailed results on other FL algorithms.\nThe centralized and FL models are trained on the same dataset. The difference between the FL model and the centralized model in terms of fairness suggests FL algorithm can introduce more bias compared to standard training. Therefore, the explanation of how bias is introduced during standard training in the centralized setting may not fully explain how FL introduces bias. In the following, we further explore how FL impacts fairness from the party level.</span></p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<section id=\"S3.SS3\" class=\"ltx_subsection ltx_figure_panel\" style=\"background-color:#F5FAFC;\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.3 </span>FL propagates bias among parties</h3>\n\n<div id=\"S3.SS3.p1\" class=\"ltx_para\">\n<p id=\"S3.SS3.p1.1\" class=\"ltx_p\"><span id=\"S3.SS3.p1.1.1\" class=\"ltx_text ltx_font_bold\">Disparate impact of FL on group fairness across parties. </span>\nFigureÂ <a href=\"#S3.F1\" title=\"Figure 1 â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates the benefit of FL on fairness and accuracy for parties. We observe that FL improves accuracy for almost all parties, and the variance in accuracy improvement across parties is small. On the other hand, the fairness benefit of FL is negative for most parties. It implies that most parties obtain a more biased model in FL compared to standalone training. Furthermore, we notice that the variance in the fairness benefits across parties is large, suggesting that although all parties have the same global model in FL, they do not benefit from the FL model equally (each party evaluates the model performance on their local test dataset).</p>\n</div>\n<figure id=\"S3.F1\" class=\"ltx_figure ltx_align_floatright\"><img src=\"/html/2309.02160/assets/images/main_png/income_benefit_box_all.png\" id=\"S3.F1.1.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"209\" height=\"130\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">Figure 1: </span><span id=\"S3.F1.3.1\" class=\"ltx_text ltx_font_bold\">Accuracy and Fairness Benefit of FL - Income (Sex).</span> The benefit of FL is the increase in accuracy or reduction in the fairness gap of the FL model compared to standalone training.\n</figcaption>\n</figure>\n<div id=\"S3.SS3.p2\" class=\"ltx_para ltx_noindent\">\n<p id=\"S3.SS3.p2.1\" class=\"ltx_p\">To explore the impact of FL on fairness at the party level, FigureÂ <a href=\"#S3.F2\" title=\"Figure 2 â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows strong correlations between the fairness benefit a party obtains in FL and the fairness gap of the standalone model for the party, i.e., the bias level of the party. This finding highlights the disparate impact of FL on fairness: FL can improve fairness for more biased parties but at the cost of worsening the issue for less biased parties.</p>\n</div>\n<figure id=\"S3.F2\" class=\"ltx_figure\"><img src=\"/html/2309.02160/assets/images/main_png/income_bias_propagation.png\" id=\"S3.F2.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"129\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 2: </span><span id=\"S3.F2.4.1\" class=\"ltx_text ltx_font_bold\">Correlation between the fairness gap of the standalone model and the benefit of FL - Income.</span> The x-axis shows the fairness gap of the standalone model, and the y-axis shows the fairness benefit of FL, which is the fairness gap of the standalone model subtracted by that of the FL model (defined in SectionÂ <a href=\"#S2\" title=\"2 Background â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). The Pearson correlation coefficients between the fairness gap of the partiesâ€™ standalone models and the fairness benefit they obtain from FL are presented. The p-value for all settings is smaller than <math id=\"S3.F2.2.m1.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"0.0001\" display=\"inline\"><semantics id=\"S3.F2.2.m1.1b\"><mn mathbackground=\"#F5FAFC\" id=\"S3.F2.2.m1.1.1\" xref=\"S3.F2.2.m1.1.1.cmml\">0.0001</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.F2.2.m1.1c\"><cn type=\"float\" id=\"S3.F2.2.m1.1.1.cmml\" xref=\"S3.F2.2.m1.1.1\">0.0001</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.F2.2.m1.1d\">0.0001</annotation></semantics></math>.\n</figcaption>\n</figure>\n<figure id=\"S3.F3\" class=\"ltx_figure\"><img src=\"/html/2309.02160/assets/images/main_png/income_effect_local_agg.png\" id=\"S3.F3.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"479\" height=\"198\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 3: </span><span id=\"S3.F3.2.1\" class=\"ltx_text ltx_font_bold\">Dynamic of fairness gap during the training - Income (Sex, DP). </span> Figure shows the fairness gap of the global model and locally updated model for the most biased party and least biased party in the first 30 rounds (FigureÂ <a href=\"#A2.F16\" title=\"Figure 16 â€£ B.2 Aggregation contradicts with local update â€£ Appendix B Additional Experimental Results â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> in AppendixÂ <a href=\"#A2\" title=\"Appendix B Additional Experimental Results â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> shows the results for all training rounds.). The most (least) biased party has the highest (lowest) fairness gap in the standalone setting. The fairness gap in the standalone setting for those parties is shown in the title.\n</figcaption>\n</figure>\n<div id=\"S3.SS3.p3\" class=\"ltx_para ltx_noindent\">\n<p id=\"S3.SS3.p3.1\" class=\"ltx_p\"><span id=\"S3.SS3.p3.1.1\" class=\"ltx_text ltx_font_bold\">Contradiction between aggregation and local update.</span>\nDuring the training process of FL, we observe that aggregation and local update contradict each other, shown in FigureÂ <a href=\"#S3.F3\" title=\"Figure 3 â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Local update from the least biased party, whose standalone model has the lowest fairness gap, reduces the fairness gap of the model. However, this reduction is eliminated by the aggregation step. Conversely, the local update from the most biased party increases the fairness gap of the model, which is then reduced by aggregation. This finding implies that the aggregation contributes to the disparate impact of FL on fairness, improving fairness for more biased parties but worsening the fairness for less biased parties compared to the standalone setting.</p>\n</div>\n<figure id=\"S3.F4\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\"><img src=\"/html/2309.02160/assets/images/income/propagation_path/overall_attr_1_metric_d_dp.png\" id=\"S3.F4.g1\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" width=\"240\" height=\"120\" alt=\"Refer to caption\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\"><img src=\"/html/2309.02160/assets/images/income/propagation_path/overall_attr_0_metric_d_dp.png\" id=\"S3.F4.g2\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" width=\"240\" height=\"122\" alt=\"Refer to caption\"></div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 4: </span><span id=\"S3.F4.2.1\" class=\"ltx_text ltx_font_bold\">Influence graphs - Income (DP). </span> The figure shows the most influential pairs of parties (with respect to demographic parity), with nodes on the top representing parties that influence other parties and the nodes on the bottom representing parties that are influenced by others. The number inside the node represents the party index, and the color of the node represents the fairness gap in the standalone setting. Green edges (resp. red edges) connect pairs of parties where the top party positively (negatively) influences the bottom party. We show the top 5 pairs with maximal positive influence and the top 5 pairs with maximal negative influence. </figcaption>\n</figure>\n<figure id=\"S3.F6\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure id=\"S3.F6.1\" class=\"ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle\" style=\"width:208.1pt;\"><img src=\"/html/2309.02160/assets/images/main_png/income_influence_standalone_dp.png\" id=\"S3.F6.1.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"598\" height=\"247\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">Figure 5: </span><span id=\"S3.F6.1.2.1\" class=\"ltx_text ltx_font_bold\">Correlation between the influence and standalone bias - Income (DP).</span> The y-axis represents the average influence of each party. The Pearson correlation coefficient between the fairness gap a party obtains in the standalone setting and the impact she has on local fairness for parties in FL are shown in the figure.\n</figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure id=\"S3.F6.2\" class=\"ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle\" style=\"width:208.1pt;\"><img src=\"/html/2309.02160/assets/images/main_png/income_cum_influence.png\" id=\"S3.F6.2.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"598\" height=\"241\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 6: </span><span id=\"S3.F6.2.2.1\" class=\"ltx_text ltx_font_bold\">Cumulative influence - Income (DP). </span> The y-axis shows the cumulative influence of each party on all parties up until the current round. The results for the top five most biased parties (parties with the highest fairness gap) and the top five least biased parties (parties with the lowest fairness gap) are presented. </figcaption>\n</figure>\n</div>\n</div>\n</figure>\n<div id=\"S3.SS3.p4\" class=\"ltx_para ltx_noindent\">\n<p id=\"S3.SS3.p4.12\" class=\"ltx_p\"><span id=\"S3.SS3.p4.12.1\" class=\"ltx_text ltx_font_bold\">Biased parties negatively influence other parties via aggregation throughout the training. </span>\nWhy does aggregation have disparate impacts on local fairness for parties? Specifically, we ask which partyâ€™s local update causes the increase (or decrease) of the fairness gap for other parties via aggregation. To answer this question, we look into the influence of a partyâ€™s local update on other partiesâ€™ fairness via aggregation. More precisely, we compute the influence of party <math id=\"S3.SS3.p4.1.m1.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"i\" display=\"inline\"><semantics id=\"S3.SS3.p4.1.m1.1a\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.1.m1.1.1\" xref=\"S3.SS3.p4.1.m1.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p4.1.m1.1b\"><ci id=\"S3.SS3.p4.1.m1.1.1.cmml\" xref=\"S3.SS3.p4.1.m1.1.1\">ğ‘–</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p4.1.m1.1c\">i</annotation></semantics></math> on party <math id=\"S3.SS3.p4.2.m2.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"j\" display=\"inline\"><semantics id=\"S3.SS3.p4.2.m2.1a\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.2.m2.1.1\" xref=\"S3.SS3.p4.2.m2.1.1.cmml\">j</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p4.2.m2.1b\"><ci id=\"S3.SS3.p4.2.m2.1.1.cmml\" xref=\"S3.SS3.p4.2.m2.1.1\">ğ‘—</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p4.2.m2.1c\">j</annotation></semantics></math> as the fairness gap increase when party <math id=\"S3.SS3.p4.3.m3.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"i\" display=\"inline\"><semantics id=\"S3.SS3.p4.3.m3.1a\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.3.m3.1.1\" xref=\"S3.SS3.p4.3.m3.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p4.3.m3.1b\"><ci id=\"S3.SS3.p4.3.m3.1.1.cmml\" xref=\"S3.SS3.p4.3.m3.1.1\">ğ‘–</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p4.3.m3.1c\">i</annotation></semantics></math>â€™s local update is removed from the aggregation in each round <math id=\"S3.SS3.p4.4.m4.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"t\" display=\"inline\"><semantics id=\"S3.SS3.p4.4.m4.1a\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.4.m4.1.1\" xref=\"S3.SS3.p4.4.m4.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p4.4.m4.1b\"><ci id=\"S3.SS3.p4.4.m4.1.1.cmml\" xref=\"S3.SS3.p4.4.m4.1.1\">ğ‘¡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p4.4.m4.1c\">t</annotation></semantics></math> and sum over all the training rounds. Formally, we define the influence of party <math id=\"S3.SS3.p4.5.m5.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"i\" display=\"inline\"><semantics id=\"S3.SS3.p4.5.m5.1a\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.5.m5.1.1\" xref=\"S3.SS3.p4.5.m5.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p4.5.m5.1b\"><ci id=\"S3.SS3.p4.5.m5.1.1.cmml\" xref=\"S3.SS3.p4.5.m5.1.1\">ğ‘–</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p4.5.m5.1c\">i</annotation></semantics></math> on party <math id=\"S3.SS3.p4.6.m6.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"j\" display=\"inline\"><semantics id=\"S3.SS3.p4.6.m6.1a\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.6.m6.1.1\" xref=\"S3.SS3.p4.6.m6.1.1.cmml\">j</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p4.6.m6.1b\"><ci id=\"S3.SS3.p4.6.m6.1.1.cmml\" xref=\"S3.SS3.p4.6.m6.1.1\">ğ‘—</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p4.6.m6.1c\">j</annotation></semantics></math> as <math id=\"S3.SS3.p4.7.m7.8\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"I_{i,j}=\\sum_{t=1}^{T}\\Delta(\\theta_{t,-i},D_{j})-\\Delta(\\theta_{t},D_{j})\" display=\"inline\"><semantics id=\"S3.SS3.p4.7.m7.8a\"><mrow id=\"S3.SS3.p4.7.m7.8.8\" xref=\"S3.SS3.p4.7.m7.8.8.cmml\"><msub id=\"S3.SS3.p4.7.m7.8.8.6\" xref=\"S3.SS3.p4.7.m7.8.8.6.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.8.8.6.2\" xref=\"S3.SS3.p4.7.m7.8.8.6.2.cmml\">I</mi><mrow id=\"S3.SS3.p4.7.m7.2.2.2.4\" xref=\"S3.SS3.p4.7.m7.2.2.2.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.1.1.1.1\" xref=\"S3.SS3.p4.7.m7.1.1.1.1.cmml\">i</mi><mo mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.2.2.2.4.1\" xref=\"S3.SS3.p4.7.m7.2.2.2.3.cmml\">,</mo><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.2.2.2.2\" xref=\"S3.SS3.p4.7.m7.2.2.2.2.cmml\">j</mi></mrow></msub><mo mathbackground=\"#F5FAFC\" rspace=\"0.111em\" id=\"S3.SS3.p4.7.m7.8.8.5\" xref=\"S3.SS3.p4.7.m7.8.8.5.cmml\">=</mo><mrow id=\"S3.SS3.p4.7.m7.8.8.4\" xref=\"S3.SS3.p4.7.m7.8.8.4.cmml\"><mrow id=\"S3.SS3.p4.7.m7.6.6.2.2\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.cmml\"><msubsup id=\"S3.SS3.p4.7.m7.6.6.2.2.3\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3.cmml\"><mo mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.2\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.2.cmml\">âˆ‘</mo><mrow id=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3.2\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3.2.cmml\">t</mi><mo mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3.1\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3.1.cmml\">=</mo><mn mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3.3\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3.3.cmml\">1</mn></mrow><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.6.6.2.2.3.3\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3.3.cmml\">T</mi></msubsup><mrow id=\"S3.SS3.p4.7.m7.6.6.2.2.2\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"S3.SS3.p4.7.m7.6.6.2.2.2.4\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.4.cmml\">Î”</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.SS3.p4.7.m7.6.6.2.2.2.3\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.3.cmml\">â€‹</mo><mrow id=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.3.cmml\"><mo mathbackground=\"#F5FAFC\" stretchy=\"false\" id=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.3\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.3.cmml\">(</mo><msub id=\"S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1\" xref=\"S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1.2\" xref=\"S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1.2.cmml\">Î¸</mi><mrow id=\"S3.SS3.p4.7.m7.4.4.2.2\" xref=\"S3.SS3.p4.7.m7.4.4.2.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.3.3.1.1\" xref=\"S3.SS3.p4.7.m7.3.3.1.1.cmml\">t</mi><mo mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.4.4.2.2.2\" xref=\"S3.SS3.p4.7.m7.4.4.2.3.cmml\">,</mo><mrow id=\"S3.SS3.p4.7.m7.4.4.2.2.1\" xref=\"S3.SS3.p4.7.m7.4.4.2.2.1.cmml\"><mo mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.4.4.2.2.1a\" xref=\"S3.SS3.p4.7.m7.4.4.2.2.1.cmml\">âˆ’</mo><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.4.4.2.2.1.2\" xref=\"S3.SS3.p4.7.m7.4.4.2.2.1.2.cmml\">i</mi></mrow></mrow></msub><mo mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.4\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.3.cmml\">,</mo><msub id=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.2\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.2.cmml\">D</mi><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.3\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.3.cmml\">j</mi></msub><mo mathbackground=\"#F5FAFC\" stretchy=\"false\" id=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.5\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.3.cmml\">)</mo></mrow></mrow></mrow><mo mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.8.8.4.5\" xref=\"S3.SS3.p4.7.m7.8.8.4.5.cmml\">âˆ’</mo><mrow id=\"S3.SS3.p4.7.m7.8.8.4.4\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"S3.SS3.p4.7.m7.8.8.4.4.4\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.4.cmml\">Î”</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.SS3.p4.7.m7.8.8.4.4.3\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.3.cmml\">â€‹</mo><mrow id=\"S3.SS3.p4.7.m7.8.8.4.4.2.2\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.2.3.cmml\"><mo mathbackground=\"#F5FAFC\" stretchy=\"false\" id=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.3\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.2.3.cmml\">(</mo><msub id=\"S3.SS3.p4.7.m7.7.7.3.3.1.1.1\" xref=\"S3.SS3.p4.7.m7.7.7.3.3.1.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.7.7.3.3.1.1.1.2\" xref=\"S3.SS3.p4.7.m7.7.7.3.3.1.1.1.2.cmml\">Î¸</mi><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.7.7.3.3.1.1.1.3\" xref=\"S3.SS3.p4.7.m7.7.7.3.3.1.1.1.3.cmml\">t</mi></msub><mo mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.4\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.2.3.cmml\">,</mo><msub id=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.2\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.2.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.2.2\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.2.2.cmml\">D</mi><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.2.3\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.2.3.cmml\">j</mi></msub><mo mathbackground=\"#F5FAFC\" stretchy=\"false\" id=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.5\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.2.3.cmml\">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p4.7.m7.8b\"><apply id=\"S3.SS3.p4.7.m7.8.8.cmml\" xref=\"S3.SS3.p4.7.m7.8.8\"><eq id=\"S3.SS3.p4.7.m7.8.8.5.cmml\" xref=\"S3.SS3.p4.7.m7.8.8.5\"></eq><apply id=\"S3.SS3.p4.7.m7.8.8.6.cmml\" xref=\"S3.SS3.p4.7.m7.8.8.6\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p4.7.m7.8.8.6.1.cmml\" xref=\"S3.SS3.p4.7.m7.8.8.6\">subscript</csymbol><ci id=\"S3.SS3.p4.7.m7.8.8.6.2.cmml\" xref=\"S3.SS3.p4.7.m7.8.8.6.2\">ğ¼</ci><list id=\"S3.SS3.p4.7.m7.2.2.2.3.cmml\" xref=\"S3.SS3.p4.7.m7.2.2.2.4\"><ci id=\"S3.SS3.p4.7.m7.1.1.1.1.cmml\" xref=\"S3.SS3.p4.7.m7.1.1.1.1\">ğ‘–</ci><ci id=\"S3.SS3.p4.7.m7.2.2.2.2.cmml\" xref=\"S3.SS3.p4.7.m7.2.2.2.2\">ğ‘—</ci></list></apply><apply id=\"S3.SS3.p4.7.m7.8.8.4.cmml\" xref=\"S3.SS3.p4.7.m7.8.8.4\"><minus id=\"S3.SS3.p4.7.m7.8.8.4.5.cmml\" xref=\"S3.SS3.p4.7.m7.8.8.4.5\"></minus><apply id=\"S3.SS3.p4.7.m7.6.6.2.2.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2\"><apply id=\"S3.SS3.p4.7.m7.6.6.2.2.3.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p4.7.m7.6.6.2.2.3.1.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3\">superscript</csymbol><apply id=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.1.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3\">subscript</csymbol><sum id=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.2.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.2\"></sum><apply id=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3\"><eq id=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3.1.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3.1\"></eq><ci id=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3.2.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3.2\">ğ‘¡</ci><cn type=\"integer\" id=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3.3.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3.3\">1</cn></apply></apply><ci id=\"S3.SS3.p4.7.m7.6.6.2.2.3.3.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3.3\">ğ‘‡</ci></apply><apply id=\"S3.SS3.p4.7.m7.6.6.2.2.2.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2\"><times id=\"S3.SS3.p4.7.m7.6.6.2.2.2.3.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.3\"></times><ci id=\"S3.SS3.p4.7.m7.6.6.2.2.2.4.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.4\">Î”</ci><interval closure=\"open\" id=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.3.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2\"><apply id=\"S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1.cmml\" xref=\"S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1.1.cmml\" xref=\"S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1\">subscript</csymbol><ci id=\"S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1.2.cmml\" xref=\"S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1.2\">ğœƒ</ci><list id=\"S3.SS3.p4.7.m7.4.4.2.3.cmml\" xref=\"S3.SS3.p4.7.m7.4.4.2.2\"><ci id=\"S3.SS3.p4.7.m7.3.3.1.1.cmml\" xref=\"S3.SS3.p4.7.m7.3.3.1.1\">ğ‘¡</ci><apply id=\"S3.SS3.p4.7.m7.4.4.2.2.1.cmml\" xref=\"S3.SS3.p4.7.m7.4.4.2.2.1\"><minus id=\"S3.SS3.p4.7.m7.4.4.2.2.1.1.cmml\" xref=\"S3.SS3.p4.7.m7.4.4.2.2.1\"></minus><ci id=\"S3.SS3.p4.7.m7.4.4.2.2.1.2.cmml\" xref=\"S3.SS3.p4.7.m7.4.4.2.2.1.2\">ğ‘–</ci></apply></list></apply><apply id=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.1.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2\">subscript</csymbol><ci id=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.2.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.2\">ğ·</ci><ci id=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.3.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.3\">ğ‘—</ci></apply></interval></apply></apply><apply id=\"S3.SS3.p4.7.m7.8.8.4.4.cmml\" xref=\"S3.SS3.p4.7.m7.8.8.4.4\"><times id=\"S3.SS3.p4.7.m7.8.8.4.4.3.cmml\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.3\"></times><ci id=\"S3.SS3.p4.7.m7.8.8.4.4.4.cmml\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.4\">Î”</ci><interval closure=\"open\" id=\"S3.SS3.p4.7.m7.8.8.4.4.2.3.cmml\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.2.2\"><apply id=\"S3.SS3.p4.7.m7.7.7.3.3.1.1.1.cmml\" xref=\"S3.SS3.p4.7.m7.7.7.3.3.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p4.7.m7.7.7.3.3.1.1.1.1.cmml\" xref=\"S3.SS3.p4.7.m7.7.7.3.3.1.1.1\">subscript</csymbol><ci id=\"S3.SS3.p4.7.m7.7.7.3.3.1.1.1.2.cmml\" xref=\"S3.SS3.p4.7.m7.7.7.3.3.1.1.1.2\">ğœƒ</ci><ci id=\"S3.SS3.p4.7.m7.7.7.3.3.1.1.1.3.cmml\" xref=\"S3.SS3.p4.7.m7.7.7.3.3.1.1.1.3\">ğ‘¡</ci></apply><apply id=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.2.cmml\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.2.1.cmml\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.2\">subscript</csymbol><ci id=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.2.2.cmml\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.2.2\">ğ·</ci><ci id=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.2.3.cmml\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.2.3\">ğ‘—</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p4.7.m7.8c\">I_{i,j}=\\sum_{t=1}^{T}\\Delta(\\theta_{t,-i},D_{j})-\\Delta(\\theta_{t},D_{j})</annotation></semantics></math>, where <math id=\"S3.SS3.p4.8.m8.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\theta_{t}\" display=\"inline\"><semantics id=\"S3.SS3.p4.8.m8.1a\"><msub id=\"S3.SS3.p4.8.m8.1.1\" xref=\"S3.SS3.p4.8.m8.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.8.m8.1.1.2\" xref=\"S3.SS3.p4.8.m8.1.1.2.cmml\">Î¸</mi><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.8.m8.1.1.3\" xref=\"S3.SS3.p4.8.m8.1.1.3.cmml\">t</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p4.8.m8.1b\"><apply id=\"S3.SS3.p4.8.m8.1.1.cmml\" xref=\"S3.SS3.p4.8.m8.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p4.8.m8.1.1.1.cmml\" xref=\"S3.SS3.p4.8.m8.1.1\">subscript</csymbol><ci id=\"S3.SS3.p4.8.m8.1.1.2.cmml\" xref=\"S3.SS3.p4.8.m8.1.1.2\">ğœƒ</ci><ci id=\"S3.SS3.p4.8.m8.1.1.3.cmml\" xref=\"S3.SS3.p4.8.m8.1.1.3\">ğ‘¡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p4.8.m8.1c\">\\theta_{t}</annotation></semantics></math> is the global model (i.e., the aggregated model overall local updated models) and <math id=\"S3.SS3.p4.9.m9.2\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\theta_{t,-i}\" display=\"inline\"><semantics id=\"S3.SS3.p4.9.m9.2a\"><msub id=\"S3.SS3.p4.9.m9.2.3\" xref=\"S3.SS3.p4.9.m9.2.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.9.m9.2.3.2\" xref=\"S3.SS3.p4.9.m9.2.3.2.cmml\">Î¸</mi><mrow id=\"S3.SS3.p4.9.m9.2.2.2.2\" xref=\"S3.SS3.p4.9.m9.2.2.2.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.9.m9.1.1.1.1\" xref=\"S3.SS3.p4.9.m9.1.1.1.1.cmml\">t</mi><mo mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.9.m9.2.2.2.2.2\" xref=\"S3.SS3.p4.9.m9.2.2.2.3.cmml\">,</mo><mrow id=\"S3.SS3.p4.9.m9.2.2.2.2.1\" xref=\"S3.SS3.p4.9.m9.2.2.2.2.1.cmml\"><mo mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.9.m9.2.2.2.2.1a\" xref=\"S3.SS3.p4.9.m9.2.2.2.2.1.cmml\">âˆ’</mo><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.9.m9.2.2.2.2.1.2\" xref=\"S3.SS3.p4.9.m9.2.2.2.2.1.2.cmml\">i</mi></mrow></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p4.9.m9.2b\"><apply id=\"S3.SS3.p4.9.m9.2.3.cmml\" xref=\"S3.SS3.p4.9.m9.2.3\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p4.9.m9.2.3.1.cmml\" xref=\"S3.SS3.p4.9.m9.2.3\">subscript</csymbol><ci id=\"S3.SS3.p4.9.m9.2.3.2.cmml\" xref=\"S3.SS3.p4.9.m9.2.3.2\">ğœƒ</ci><list id=\"S3.SS3.p4.9.m9.2.2.2.3.cmml\" xref=\"S3.SS3.p4.9.m9.2.2.2.2\"><ci id=\"S3.SS3.p4.9.m9.1.1.1.1.cmml\" xref=\"S3.SS3.p4.9.m9.1.1.1.1\">ğ‘¡</ci><apply id=\"S3.SS3.p4.9.m9.2.2.2.2.1.cmml\" xref=\"S3.SS3.p4.9.m9.2.2.2.2.1\"><minus id=\"S3.SS3.p4.9.m9.2.2.2.2.1.1.cmml\" xref=\"S3.SS3.p4.9.m9.2.2.2.2.1\"></minus><ci id=\"S3.SS3.p4.9.m9.2.2.2.2.1.2.cmml\" xref=\"S3.SS3.p4.9.m9.2.2.2.2.1.2\">ğ‘–</ci></apply></list></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p4.9.m9.2c\">\\theta_{t,-i}</annotation></semantics></math> is the aggregated model over local updated models from parties excluding party <math id=\"S3.SS3.p4.10.m10.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"i\" display=\"inline\"><semantics id=\"S3.SS3.p4.10.m10.1a\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.10.m10.1.1\" xref=\"S3.SS3.p4.10.m10.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p4.10.m10.1b\"><ci id=\"S3.SS3.p4.10.m10.1.1.cmml\" xref=\"S3.SS3.p4.10.m10.1.1\">ğ‘–</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p4.10.m10.1c\">i</annotation></semantics></math>. If party <math id=\"S3.SS3.p4.11.m11.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"i\" display=\"inline\"><semantics id=\"S3.SS3.p4.11.m11.1a\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.11.m11.1.1\" xref=\"S3.SS3.p4.11.m11.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p4.11.m11.1b\"><ci id=\"S3.SS3.p4.11.m11.1.1.cmml\" xref=\"S3.SS3.p4.11.m11.1.1\">ğ‘–</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p4.11.m11.1c\">i</annotation></semantics></math> improves fairness for party <math id=\"S3.SS3.p4.12.m12.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"j\" display=\"inline\"><semantics id=\"S3.SS3.p4.12.m12.1a\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.12.m12.1.1\" xref=\"S3.SS3.p4.12.m12.1.1.cmml\">j</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p4.12.m12.1b\"><ci id=\"S3.SS3.p4.12.m12.1.1.cmml\" xref=\"S3.SS3.p4.12.m12.1.1\">ğ‘—</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p4.12.m12.1c\">j</annotation></semantics></math>, the influence is positive and vice versa. We compute the influence for all pairs of parties, and the most influential pairs are shown in FigureÂ <a href=\"#S3.F4\" title=\"Figure 4 â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. We observe that a less biased party has a positive influence on fairness for other parties, while a more biased party has a negative influence on other parties. This result shows that a biased party can negatively influence other partiesâ€™ fairness via aggregation throughout the training.</p>\n</div>\n<div id=\"S3.SS3.p5\" class=\"ltx_para ltx_noindent\">\n<p id=\"S3.SS3.p5.1\" class=\"ltx_p\">Furthermore, we investigate the relationship between a partyâ€™s bias (i.e., the bias of the partyâ€™s standalone model) and its average influence on all partiesâ€™ fairness (i.e., <math id=\"S3.SS3.p5.1.m1.2\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\sum_{k=1}^{K}I_{i,k}/K\" display=\"inline\"><semantics id=\"S3.SS3.p5.1.m1.2a\"><mrow id=\"S3.SS3.p5.1.m1.2.3\" xref=\"S3.SS3.p5.1.m1.2.3.cmml\"><msubsup id=\"S3.SS3.p5.1.m1.2.3.1\" xref=\"S3.SS3.p5.1.m1.2.3.1.cmml\"><mo mathbackground=\"#F5FAFC\" id=\"S3.SS3.p5.1.m1.2.3.1.2.2\" xref=\"S3.SS3.p5.1.m1.2.3.1.2.2.cmml\">âˆ‘</mo><mrow id=\"S3.SS3.p5.1.m1.2.3.1.2.3\" xref=\"S3.SS3.p5.1.m1.2.3.1.2.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p5.1.m1.2.3.1.2.3.2\" xref=\"S3.SS3.p5.1.m1.2.3.1.2.3.2.cmml\">k</mi><mo mathbackground=\"#F5FAFC\" id=\"S3.SS3.p5.1.m1.2.3.1.2.3.1\" xref=\"S3.SS3.p5.1.m1.2.3.1.2.3.1.cmml\">=</mo><mn mathbackground=\"#F5FAFC\" id=\"S3.SS3.p5.1.m1.2.3.1.2.3.3\" xref=\"S3.SS3.p5.1.m1.2.3.1.2.3.3.cmml\">1</mn></mrow><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p5.1.m1.2.3.1.3\" xref=\"S3.SS3.p5.1.m1.2.3.1.3.cmml\">K</mi></msubsup><mrow id=\"S3.SS3.p5.1.m1.2.3.2\" xref=\"S3.SS3.p5.1.m1.2.3.2.cmml\"><msub id=\"S3.SS3.p5.1.m1.2.3.2.2\" xref=\"S3.SS3.p5.1.m1.2.3.2.2.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p5.1.m1.2.3.2.2.2\" xref=\"S3.SS3.p5.1.m1.2.3.2.2.2.cmml\">I</mi><mrow id=\"S3.SS3.p5.1.m1.2.2.2.4\" xref=\"S3.SS3.p5.1.m1.2.2.2.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p5.1.m1.1.1.1.1\" xref=\"S3.SS3.p5.1.m1.1.1.1.1.cmml\">i</mi><mo mathbackground=\"#F5FAFC\" id=\"S3.SS3.p5.1.m1.2.2.2.4.1\" xref=\"S3.SS3.p5.1.m1.2.2.2.3.cmml\">,</mo><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p5.1.m1.2.2.2.2\" xref=\"S3.SS3.p5.1.m1.2.2.2.2.cmml\">k</mi></mrow></msub><mo mathbackground=\"#F5FAFC\" id=\"S3.SS3.p5.1.m1.2.3.2.1\" xref=\"S3.SS3.p5.1.m1.2.3.2.1.cmml\">/</mo><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p5.1.m1.2.3.2.3\" xref=\"S3.SS3.p5.1.m1.2.3.2.3.cmml\">K</mi></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p5.1.m1.2b\"><apply id=\"S3.SS3.p5.1.m1.2.3.cmml\" xref=\"S3.SS3.p5.1.m1.2.3\"><apply id=\"S3.SS3.p5.1.m1.2.3.1.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p5.1.m1.2.3.1.1.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.1\">superscript</csymbol><apply id=\"S3.SS3.p5.1.m1.2.3.1.2.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p5.1.m1.2.3.1.2.1.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.1\">subscript</csymbol><sum id=\"S3.SS3.p5.1.m1.2.3.1.2.2.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.1.2.2\"></sum><apply id=\"S3.SS3.p5.1.m1.2.3.1.2.3.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.1.2.3\"><eq id=\"S3.SS3.p5.1.m1.2.3.1.2.3.1.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.1.2.3.1\"></eq><ci id=\"S3.SS3.p5.1.m1.2.3.1.2.3.2.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.1.2.3.2\">ğ‘˜</ci><cn type=\"integer\" id=\"S3.SS3.p5.1.m1.2.3.1.2.3.3.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.1.2.3.3\">1</cn></apply></apply><ci id=\"S3.SS3.p5.1.m1.2.3.1.3.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.1.3\">ğ¾</ci></apply><apply id=\"S3.SS3.p5.1.m1.2.3.2.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.2\"><divide id=\"S3.SS3.p5.1.m1.2.3.2.1.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.2.1\"></divide><apply id=\"S3.SS3.p5.1.m1.2.3.2.2.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p5.1.m1.2.3.2.2.1.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.2.2\">subscript</csymbol><ci id=\"S3.SS3.p5.1.m1.2.3.2.2.2.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.2.2.2\">ğ¼</ci><list id=\"S3.SS3.p5.1.m1.2.2.2.3.cmml\" xref=\"S3.SS3.p5.1.m1.2.2.2.4\"><ci id=\"S3.SS3.p5.1.m1.1.1.1.1.cmml\" xref=\"S3.SS3.p5.1.m1.1.1.1.1\">ğ‘–</ci><ci id=\"S3.SS3.p5.1.m1.2.2.2.2.cmml\" xref=\"S3.SS3.p5.1.m1.2.2.2.2\">ğ‘˜</ci></list></apply><ci id=\"S3.SS3.p5.1.m1.2.3.2.3.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.2.3\">ğ¾</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p5.1.m1.2c\">\\sum_{k=1}^{K}I_{i,k}/K</annotation></semantics></math>). The results are presented in FigureÂ <a href=\"#S3.F6\" title=\"Figure 6 â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, which shows a strong correlation between these two factors. This finding further supports our conclusion that the more biased parties have a stronger negative influence on other partiesâ€™ fairness, while the less biased parties have a stronger positive influence.</p>\n</div>\n<div id=\"S3.SS3.p6\" class=\"ltx_para ltx_noindent\">\n<p id=\"S3.SS3.p6.1\" class=\"ltx_p\">Finally, we analyze the dynamics of the influence of the top 5 most biased parties (i.e., parties with the largest fairness gap in the standalone setting) and the top 5 least biased parties, as shown in FigureÂ <a href=\"#S3.F6\" title=\"Figure 6 â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. We observe that the influence of the most biased parties is monotonically increasing throughout the training, while the influence of the least biased parties is decreasing. In other words, biased parties consistently have a negative influence on othersâ€™ fairness gaps, while less biased parties have a positive influence. These results suggest that FL can propagate bias among parties: the bias from biased parties negatively influence the fairness of other parties via aggregation throughout the training. Next, we will explore how the bias is propagated in FL.</p>\n</div>\n<figure id=\"S3.F7\" class=\"ltx_figure\"><img src=\"/html/2309.02160/assets/images/main_png/income_attribution_distribution.png\" id=\"S3.F7.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"170\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 7: </span><span id=\"S3.F7.2.1\" class=\"ltx_text ltx_font_bold\">Histogram of feature attribution value for the sensitive attribute - Income (Sex).</span> The average attribution value for the female group and male group is shown in the figure. The results are computed on all test data points over five different runs. We show the results for the most biased party with the highest fairness gap in the standalone setting.\n</figcaption>\n</figure>\n<section id=\"S3.SS4\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.4 </span>How is bias propagated in FL?</h3>\n\n<div id=\"S3.SS4.p1\" class=\"ltx_para\">\n<p id=\"S3.SS4.p1.1\" class=\"ltx_p\"><span id=\"S3.SS4.p1.1.1\" class=\"ltx_text ltx_font_bold\">Disparate treatment causes large fairness gaps.</span>\nOur first investigation explores what the bias represents, specifically whether the bias increase in FL is directly caused by the disparate treatment of the model among sensitive groups. To answer this question, we utilize Integrated GradientsÂ <cite class=\"ltx_cite ltx_citemacro_citep\">(Sundararajan etÂ al., <a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">2017</a>)</cite> to measure the attribution of each input feature to the modelsâ€™ predictions with respect to the positive class.\nFigureÂ <a href=\"#S3.F7\" title=\"Figure 7 â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the attribution value distribution for the sensitive attribute \"Sex\" over individual test points from the female and male groups. We notice that the sex attribute has a large attribution value for the standalone modelâ€™s predictions and the FL modelâ€™s predictions. This finding implies that the predictions of those models are heavily dependent on the sensitive attribute of the test data. Moreover, the sensitive attribute affects the model predictions differently for the male and female groups, with the average attribution value being positive for the male group and negative for the female group. Furthermore, we find that there is minimal difference in the attribution value for other attributes with respect to the female and male groups (see FigureÂ <a href=\"#A2.F19\" title=\"Figure 19 â€£ B.4 Attribution value for sensitive attribute â€£ Appendix B Additional Experimental Results â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> in AppendixÂ <a href=\"#A2\" title=\"Appendix B Additional Experimental Results â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>). This indicates that the large fairness gap in the models is not caused by the distinct distribution of other insensitive attributes over protected groups; rather, it is mainly caused by the modelsâ€™ disparate treatment of protected groups.</p>\n</div>\n<figure id=\"S3.F9\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure id=\"S3.F9.1\" class=\"ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle\" style=\"width:212.5pt;\"><img src=\"/html/2309.02160/assets/images/main_png/income_attribution_dynamic.png\" id=\"S3.F9.1.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"598\" height=\"245\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 8: </span><span id=\"S3.F9.1.2.1\" class=\"ltx_text ltx_font_bold\">Dynamic of absolute attribution value for the sensitive attribute - Income (Sex).</span> The results of different models are shown in different lines. We present the results for the most biased party and the least biased party.\n</figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure id=\"S3.F9.2\" class=\"ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle\" style=\"width:212.5pt;\"><img src=\"/html/2309.02160/assets/images/main_png/income_attribution_local_agg.png\" id=\"S3.F9.2.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"598\" height=\"245\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 9: </span><span id=\"S3.F9.2.2.1\" class=\"ltx_text ltx_font_bold\">Effect of local update and aggregation on the attribution value - Income (Sex).</span> We show the attribution value for sex attribute before and after aggregation for the most biased party and the least biased party during the training. </figcaption>\n</figure>\n</div>\n</div>\n</figure>\n<div id=\"S3.SS4.p2\" class=\"ltx_para ltx_noindent\">\n<p id=\"S3.SS4.p2.1\" class=\"ltx_p\"><span id=\"S3.SS4.p2.1.1\" class=\"ltx_text ltx_font_bold\">FL model learns more biased patterns. </span>\nIn FigureÂ <a href=\"#S3.F7\" title=\"Figure 7 â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, we compare the attribution value of â€œSexâ€ for different models and find that, while the predictions of the FL model depend less heavily on the sensitive attribute compared to those of the standalone model, the dependence is still stronger than that of the centralized model. This suggests that the FL model learns a more biased pattern compared to what could be learned in the centralized setting.\nFigureÂ <a href=\"#S3.F9\" title=\"Figure 9 â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows the dynamic of the average absolute feature attribution for â€œSexâ€ during the training. We find that standalone training increases the modelâ€™s dependence on the sensitive attribute throughout training for the most biased party, but centralized training decreases it. This suggests that collaboration through centralized training improves local fairness by guiding the model to learn less biased features. In FL, however, the absolute attribution value barely changes after 100 rounds and remains significantly greater than that in the centralized setting. This implies that the FL algorithm may introduce bias to the final model by inhibiting the model from learning less biased features.</p>\n</div>\n<div id=\"S3.SS4.p3\" class=\"ltx_para ltx_noindent\">\n<p id=\"S3.SS4.p3.1\" class=\"ltx_p\"><span id=\"S3.SS4.p3.1.1\" class=\"ltx_text ltx_font_bold\">Biased parties increase the model dependence on the sensitive attribute. </span>\nFigureÂ <a href=\"#S3.F9\" title=\"Figure 9 â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows the attribution value of the aggregated model and locally updated model from the most biased party and least biased party. We observe that the biased party increases the global model dependence on sensitive attributes during the local update, and this increase persists throughout the training. In contrast, the least biased party reduces the dependence on the sensitive attribute, which is aligned with the trend of the fairness gap in FigureÂ <a href=\"#S3.F3\" title=\"Figure 3 â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. This suggests that the biased parties have a negative impact on the fairness of other parties by increasing the modelâ€™s dependence on the sensitive attribute.</p>\n</div>\n<div id=\"S3.SS4.p4\" class=\"ltx_para\">\n<p id=\"S3.SS4.p4.1\" class=\"ltx_p\"><span id=\"S3.SS4.p4.1.1\" class=\"ltx_text ltx_font_bold\">Bias is encoded in a few parameters. </span></p>\n</div>\n<figure id=\"S3.F10\" class=\"ltx_figure\"><img src=\"/html/2309.02160/assets/images/main_png/income_effect_weights.png\" id=\"S3.F10.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"141\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 10: </span><span id=\"S3.F10.5.1\" class=\"ltx_text ltx_font_bold\">Effect of a few parameters on fairness - Income (Sex).</span> <span id=\"S3.F10.6.2\" class=\"ltx_text ltx_font_italic\">(a) Effect of local update and aggregation on the parameter values:</span> The y-axis shows the norm of parameters that are directly computed on the sensitive attribute, normalized by the parameter norm of the first layer. We show the dynamic of this parameter norm during FL for the aggregated model and locally updated model for the parties who benefit most from FL and suffer the most from FL (represented by the line with â€œBenefitâ€ and â€œSufferâ€ respectively). <span id=\"S3.F10.7.3\" class=\"ltx_text ltx_font_italic\">(b) Correlation between standalone bias and parameter values:</span> The x-axis shows the bias a party receives in the standalone setting, and the y-axis shows the normalized norm of parameters (associated with sex attribute) for the locally updated model in the last round. <span id=\"S3.F10.8.4\" class=\"ltx_text ltx_font_italic\">(c) Effect of scaling the parameter values:</span> The figure shows the model performance in terms of accuracy and fairness gap when the parameter values (associated with sex attribute) are multiplied by a scaling factor.</figcaption>\n</figure>\n<div id=\"S3.SS4.p5\" class=\"ltx_para ltx_noindent\">\n<p id=\"S3.SS4.p5.1\" class=\"ltx_p\">Biased parties increase the model dependence on the sensitive attribute. But how does this increase propagate to other parties in the network? Since parties share the model parameters of the local model with the server, the bias is likely encoded in the model parameters. Therefore, we investigate which parameters are related to the modelâ€™s bias. Intuitively, the parameters used to extract sensitive attribute information impact the attribution value of the sensitive attribute to the model prediction. If the absolute value (signal) of those parameters is substantial, the value of the sensitive attribute will significantly affect the modelâ€™s prediction. In our evaluation, the sensitive attribute is part of the input feature, so the parameters directly applied to the â€œSexâ€ attribute in the first layer of the neural network should contribute to the modelâ€™s bias.</p>\n</div>\n<div id=\"S3.SS4.p6\" class=\"ltx_para ltx_noindent\">\n<p id=\"S3.SS4.p6.1\" class=\"ltx_p\">FigureÂ <a href=\"#S3.F10\" title=\"Figure 10 â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>(a) shows the normalized norm of the parameters associated with the sensitive attribute for the most and least biased parties in the aggregated and locally updated models. The normalized norm is defined as the norm of parameters associated with the sensitive attribute divided by the parameter norm of the first layer. We observe that the least biased party reduces the parameter norm, hence decreasing the modelâ€™s sensitivity to the sensitive attribute. On the other hand, the biased party increases the norm for those parameters, amplifying the impact of the sensitive attribute on the modelâ€™s prediction. Through aggregation, this amplification will be propagated to the global model.\nFigureÂ <a href=\"#S3.F10\" title=\"Figure 10 â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>(b) reveals a moderate correlation between the fairness gap party experiences in the standalone setting and the normalized parameter norm for the parameters used to extract sensitive attribute information. This implies that biased parties increase the parameter value associated with sensitive attributes during the local update, thereby boosting the modelâ€™s susceptibility to the sensitive attribute.</p>\n</div>\n<div id=\"S3.SS4.p7\" class=\"ltx_para ltx_noindent\">\n<p id=\"S3.SS4.p7.18\" class=\"ltx_p\"><span id=\"S3.SS4.p7.18.1\" class=\"ltx_text ltx_font_bold\">Controlling fairness gap by scaling a few parameters. </span>\nTo further investigate the impact of the parameters associated with the sensitive attribute (i.e., <math id=\"S3.SS4.p7.1.m1.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"104\" display=\"inline\"><semantics id=\"S3.SS4.p7.1.m1.1a\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.1.m1.1.1\" xref=\"S3.SS4.p7.1.m1.1.1.cmml\">104</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.1.m1.1b\"><cn type=\"integer\" id=\"S3.SS4.p7.1.m1.1.1.cmml\" xref=\"S3.SS4.p7.1.m1.1.1\">104</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.1.m1.1c\">104</annotation></semantics></math> parameters out of <math id=\"S3.SS4.p7.2.m2.2\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"1,792\" display=\"inline\"><semantics id=\"S3.SS4.p7.2.m2.2a\"><mrow id=\"S3.SS4.p7.2.m2.2.3.2\" xref=\"S3.SS4.p7.2.m2.2.3.1.cmml\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.2.m2.1.1\" xref=\"S3.SS4.p7.2.m2.1.1.cmml\">1</mn><mo mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.2.m2.2.3.2.1\" xref=\"S3.SS4.p7.2.m2.2.3.1.cmml\">,</mo><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.2.m2.2.2\" xref=\"S3.SS4.p7.2.m2.2.2.cmml\">792</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.2.m2.2b\"><list id=\"S3.SS4.p7.2.m2.2.3.1.cmml\" xref=\"S3.SS4.p7.2.m2.2.3.2\"><cn type=\"integer\" id=\"S3.SS4.p7.2.m2.1.1.cmml\" xref=\"S3.SS4.p7.2.m2.1.1\">1</cn><cn type=\"integer\" id=\"S3.SS4.p7.2.m2.2.2.cmml\" xref=\"S3.SS4.p7.2.m2.2.2\">792</cn></list></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.2.m2.2c\">1,792</annotation></semantics></math>) on model fairness, we examine the effect of scaling these parameters on the fairness gap in FigureÂ <a href=\"#S3.F10\" title=\"Figure 10 â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>(c). We find that the fairness gap can be greatly widened or narrowed at the expense of a moderate degree of accuracy. Specifically, by scaling the parameter value by <math id=\"S3.SS4.p7.3.m3.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"0.1\" display=\"inline\"><semantics id=\"S3.SS4.p7.3.m3.1a\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.3.m3.1.1\" xref=\"S3.SS4.p7.3.m3.1.1.cmml\">0.1</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.3.m3.1b\"><cn type=\"float\" id=\"S3.SS4.p7.3.m3.1.1.cmml\" xref=\"S3.SS4.p7.3.m3.1.1\">0.1</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.3.m3.1c\">0.1</annotation></semantics></math> for the trained FL model, we significantly reduce the EO gap by <math id=\"S3.SS4.p7.4.m4.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"74.6\\%\" display=\"inline\"><semantics id=\"S3.SS4.p7.4.m4.1a\"><mrow id=\"S3.SS4.p7.4.m4.1.1\" xref=\"S3.SS4.p7.4.m4.1.1.cmml\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.4.m4.1.1.2\" xref=\"S3.SS4.p7.4.m4.1.1.2.cmml\">74.6</mn><mo mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.4.m4.1.1.1\" xref=\"S3.SS4.p7.4.m4.1.1.1.cmml\">%</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.4.m4.1b\"><apply id=\"S3.SS4.p7.4.m4.1.1.cmml\" xref=\"S3.SS4.p7.4.m4.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS4.p7.4.m4.1.1.1.cmml\" xref=\"S3.SS4.p7.4.m4.1.1.1\">percent</csymbol><cn type=\"float\" id=\"S3.SS4.p7.4.m4.1.1.2.cmml\" xref=\"S3.SS4.p7.4.m4.1.1.2\">74.6</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.4.m4.1c\">74.6\\%</annotation></semantics></math> (from <math id=\"S3.SS4.p7.5.m5.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"0.198\" display=\"inline\"><semantics id=\"S3.SS4.p7.5.m5.1a\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.5.m5.1.1\" xref=\"S3.SS4.p7.5.m5.1.1.cmml\">0.198</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.5.m5.1b\"><cn type=\"float\" id=\"S3.SS4.p7.5.m5.1.1.cmml\" xref=\"S3.SS4.p7.5.m5.1.1\">0.198</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.5.m5.1c\">0.198</annotation></semantics></math> to <math id=\"S3.SS4.p7.6.m6.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"0.05\" display=\"inline\"><semantics id=\"S3.SS4.p7.6.m6.1a\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.6.m6.1.1\" xref=\"S3.SS4.p7.6.m6.1.1.cmml\">0.05</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.6.m6.1b\"><cn type=\"float\" id=\"S3.SS4.p7.6.m6.1.1.cmml\" xref=\"S3.SS4.p7.6.m6.1.1\">0.05</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.6.m6.1c\">0.05</annotation></semantics></math>) and the DP gap by <math id=\"S3.SS4.p7.7.m7.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"69.1\\%\" display=\"inline\"><semantics id=\"S3.SS4.p7.7.m7.1a\"><mrow id=\"S3.SS4.p7.7.m7.1.1\" xref=\"S3.SS4.p7.7.m7.1.1.cmml\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.7.m7.1.1.2\" xref=\"S3.SS4.p7.7.m7.1.1.2.cmml\">69.1</mn><mo mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.7.m7.1.1.1\" xref=\"S3.SS4.p7.7.m7.1.1.1.cmml\">%</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.7.m7.1b\"><apply id=\"S3.SS4.p7.7.m7.1.1.cmml\" xref=\"S3.SS4.p7.7.m7.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS4.p7.7.m7.1.1.1.cmml\" xref=\"S3.SS4.p7.7.m7.1.1.1\">percent</csymbol><cn type=\"float\" id=\"S3.SS4.p7.7.m7.1.1.2.cmml\" xref=\"S3.SS4.p7.7.m7.1.1.2\">69.1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.7.m7.1c\">69.1\\%</annotation></semantics></math> (from <math id=\"S3.SS4.p7.8.m8.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"0.217\" display=\"inline\"><semantics id=\"S3.SS4.p7.8.m8.1a\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.8.m8.1.1\" xref=\"S3.SS4.p7.8.m8.1.1.cmml\">0.217</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.8.m8.1b\"><cn type=\"float\" id=\"S3.SS4.p7.8.m8.1.1.cmml\" xref=\"S3.SS4.p7.8.m8.1.1\">0.217</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.8.m8.1c\">0.217</annotation></semantics></math> to <math id=\"S3.SS4.p7.9.m9.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"0.067\" display=\"inline\"><semantics id=\"S3.SS4.p7.9.m9.1a\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.9.m9.1.1\" xref=\"S3.SS4.p7.9.m9.1.1.cmml\">0.067</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.9.m9.1b\"><cn type=\"float\" id=\"S3.SS4.p7.9.m9.1.1.cmml\" xref=\"S3.SS4.p7.9.m9.1.1\">0.067</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.9.m9.1c\">0.067</annotation></semantics></math>) with just a moderate accuracy loss of <math id=\"S3.SS4.p7.10.m10.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"0.8\\%\" display=\"inline\"><semantics id=\"S3.SS4.p7.10.m10.1a\"><mrow id=\"S3.SS4.p7.10.m10.1.1\" xref=\"S3.SS4.p7.10.m10.1.1.cmml\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.10.m10.1.1.2\" xref=\"S3.SS4.p7.10.m10.1.1.2.cmml\">0.8</mn><mo mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.10.m10.1.1.1\" xref=\"S3.SS4.p7.10.m10.1.1.1.cmml\">%</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.10.m10.1b\"><apply id=\"S3.SS4.p7.10.m10.1.1.cmml\" xref=\"S3.SS4.p7.10.m10.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS4.p7.10.m10.1.1.1.cmml\" xref=\"S3.SS4.p7.10.m10.1.1.1\">percent</csymbol><cn type=\"float\" id=\"S3.SS4.p7.10.m10.1.1.2.cmml\" xref=\"S3.SS4.p7.10.m10.1.1.2\">0.8</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.10.m10.1c\">0.8\\%</annotation></semantics></math> (from <math id=\"S3.SS4.p7.11.m11.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"0.785\" display=\"inline\"><semantics id=\"S3.SS4.p7.11.m11.1a\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.11.m11.1.1\" xref=\"S3.SS4.p7.11.m11.1.1.cmml\">0.785</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.11.m11.1b\"><cn type=\"float\" id=\"S3.SS4.p7.11.m11.1.1.cmml\" xref=\"S3.SS4.p7.11.m11.1.1\">0.785</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.11.m11.1c\">0.785</annotation></semantics></math> to <math id=\"S3.SS4.p7.12.m12.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"0.779\" display=\"inline\"><semantics id=\"S3.SS4.p7.12.m12.1a\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.12.m12.1.1\" xref=\"S3.SS4.p7.12.m12.1.1.cmml\">0.779</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.12.m12.1b\"><cn type=\"float\" id=\"S3.SS4.p7.12.m12.1.1.cmml\" xref=\"S3.SS4.p7.12.m12.1.1\">0.779</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.12.m12.1c\">0.779</annotation></semantics></math>). In contrast, scaling the same set of parameters by a factor of 10 increases the EO gap to <math id=\"S3.SS4.p7.13.m13.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"0.96\" display=\"inline\"><semantics id=\"S3.SS4.p7.13.m13.1a\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.13.m13.1.1\" xref=\"S3.SS4.p7.13.m13.1.1.cmml\">0.96</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.13.m13.1b\"><cn type=\"float\" id=\"S3.SS4.p7.13.m13.1.1.cmml\" xref=\"S3.SS4.p7.13.m13.1.1\">0.96</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.13.m13.1c\">0.96</annotation></semantics></math> (the maximal fairness gap is <math id=\"S3.SS4.p7.14.m14.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"1\" display=\"inline\"><semantics id=\"S3.SS4.p7.14.m14.1a\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.14.m14.1.1\" xref=\"S3.SS4.p7.14.m14.1.1.cmml\">1</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.14.m14.1b\"><cn type=\"integer\" id=\"S3.SS4.p7.14.m14.1.1.cmml\" xref=\"S3.SS4.p7.14.m14.1.1\">1</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.14.m14.1c\">1</annotation></semantics></math>), which is almost five times larger, and increases the DP gap by <math id=\"S3.SS4.p7.15.m15.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"259\\%\" display=\"inline\"><semantics id=\"S3.SS4.p7.15.m15.1a\"><mrow id=\"S3.SS4.p7.15.m15.1.1\" xref=\"S3.SS4.p7.15.m15.1.1.cmml\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.15.m15.1.1.2\" xref=\"S3.SS4.p7.15.m15.1.1.2.cmml\">259</mn><mo mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.15.m15.1.1.1\" xref=\"S3.SS4.p7.15.m15.1.1.1.cmml\">%</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.15.m15.1b\"><apply id=\"S3.SS4.p7.15.m15.1.1.cmml\" xref=\"S3.SS4.p7.15.m15.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS4.p7.15.m15.1.1.1.cmml\" xref=\"S3.SS4.p7.15.m15.1.1.1\">percent</csymbol><cn type=\"integer\" id=\"S3.SS4.p7.15.m15.1.1.2.cmml\" xref=\"S3.SS4.p7.15.m15.1.1.2\">259</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.15.m15.1c\">259\\%</annotation></semantics></math> to <math id=\"S3.SS4.p7.16.m16.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"0.78\" display=\"inline\"><semantics id=\"S3.SS4.p7.16.m16.1a\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.16.m16.1.1\" xref=\"S3.SS4.p7.16.m16.1.1.cmml\">0.78</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.16.m16.1b\"><cn type=\"float\" id=\"S3.SS4.p7.16.m16.1.1.cmml\" xref=\"S3.SS4.p7.16.m16.1.1\">0.78</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.16.m16.1c\">0.78</annotation></semantics></math>, while reducing the accuracy from <math id=\"S3.SS4.p7.17.m17.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"0.785\" display=\"inline\"><semantics id=\"S3.SS4.p7.17.m17.1a\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.17.m17.1.1\" xref=\"S3.SS4.p7.17.m17.1.1.cmml\">0.785</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.17.m17.1b\"><cn type=\"float\" id=\"S3.SS4.p7.17.m17.1.1.cmml\" xref=\"S3.SS4.p7.17.m17.1.1\">0.785</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.17.m17.1c\">0.785</annotation></semantics></math> to <math id=\"S3.SS4.p7.18.m18.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"0.67\" display=\"inline\"><semantics id=\"S3.SS4.p7.18.m18.1a\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.18.m18.1.1\" xref=\"S3.SS4.p7.18.m18.1.1.cmml\">0.67</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.18.m18.1b\"><cn type=\"float\" id=\"S3.SS4.p7.18.m18.1.1.cmml\" xref=\"S3.SS4.p7.18.m18.1.1\">0.67</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.18.m18.1c\">0.67</annotation></semantics></math>. These findings explain how bias is propagated in FL: biased parties magnify the impact of sensitive attributes on model predictions by increasing the model parameter used to extract sensitive attributes. This rise in parameters is subsequently propagated to the global model through aggregation, further aggravating the issue of fairness for other parties. Our results explain how bias is propagated in FL: <span id=\"S3.SS4.p7.18.2\" class=\"ltx_text ltx_font_bold\">Biased parties encode bias in a few parameters through a local update, and this bias is consequently propagated to the entire network through parameter aggregation.</span></p>\n</div>\n<section id=\"S4\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">4 </span>Related work</h2>\n\n<div id=\"S4.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"S4.p1.1\" class=\"ltx_p\">Fairness has received considerable attention due to the growing deployment of machine learning in decision-making processes. Various definitions of fairness have been presentedÂ <cite class=\"ltx_cite ltx_citemacro_citep\">(Hardt etÂ al., <a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">2016</a>; Dwork etÂ al., <a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">2012</a>; Calders etÂ al., <a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\">2009</a>)</cite>. Specifically, group fairness requires that the model behave similarly for groups defined by a sensitive attribute (e.g., race). While how machine learning algorithms propagate data bias to the final model has been extensively investigated in a centralized setting Â <cite class=\"ltx_cite ltx_citemacro_citep\">(Blum and Stangl, <a href=\"#bib.bib3\" title=\"\" class=\"ltx_ref\">2020</a>; Lakkaraju etÂ al., <a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">2017</a>; Rambachan and Roth, <a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">2020</a>; Friedler etÂ al., <a href=\"#bib.bib15\" title=\"\" class=\"ltx_ref\">2019</a>; Dullerud etÂ al., <a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>, the effect of FL on model fairness is not yet fully understood.</p>\n</div>\n<div id=\"S4.p2\" class=\"ltx_para ltx_noindent\">\n<p id=\"S4.p2.1\" class=\"ltx_p\">The existing literature on fairness in FL mainly focuses on the performance disparity of FL models across parties, rather than demographic groupsÂ <cite class=\"ltx_cite ltx_citemacro_citep\">(Li etÂ al., <a href=\"#bib.bib29\" title=\"\" class=\"ltx_ref\">2021</a>; Zhao and Joshi, <a href=\"#bib.bib55\" title=\"\" class=\"ltx_ref\">2022</a>; Li etÂ al., <a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">2019</a>; Mohri etÂ al., <a href=\"#bib.bib34\" title=\"\" class=\"ltx_ref\">2019</a>; Deng etÂ al., <a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">2020</a>; Donahue and Kleinberg, <a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">2021</a>; Hao etÂ al., <a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">2021</a>; Zhou etÂ al., <a href=\"#bib.bib56\" title=\"\" class=\"ltx_ref\">2021</a>; Yu etÂ al., <a href=\"#bib.bib48\" title=\"\" class=\"ltx_ref\">2020</a>; Lyu etÂ al., <a href=\"#bib.bib32\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>. However, we focus on group fairness, which concerns performance disparity among groups. In terms of group fairness, <cite class=\"ltx_cite ltx_citemacro_cite\">Abay etÂ al. (<a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2020</a>)</cite> listed a few potential sources of bias in FL. Recently, considerable progress has been made in training group fair models in FLÂ <cite class=\"ltx_cite ltx_citemacro_citep\">(Abay etÂ al., <a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2020</a>; Chu etÂ al., <a href=\"#bib.bib5\" title=\"\" class=\"ltx_ref\">2021</a>; Zeng etÂ al., <a href=\"#bib.bib51\" title=\"\" class=\"ltx_ref\">2021a</a>; Hu etÂ al., <a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">2022</a>; Du etÂ al., <a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">2021</a>; Ezzeldin etÂ al., <a href=\"#bib.bib14\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>. Nonetheless, the majority of these works suggest techniques for achieving fairness on a single test distribution. Instead, we focus on fairness issues for parties. Some studiesÂ <cite class=\"ltx_cite ltx_citemacro_citep\">(Cui etÂ al., <a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">2021</a>; Papadaki etÂ al., <a href=\"#bib.bib37\" title=\"\" class=\"ltx_ref\">2022</a>)</cite> proposed algorithms to improve local fairness for parties. Our purpose, instead, is to gain a comprehensive understanding of how FL influences local fairness on its own, which we believe is equally crucial as designing fair algorithms.</p>\n</div>\n<section id=\"S5\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">5 </span>Future Work &amp; Conclusion</h2>\n\n<div id=\"S5.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"S5.p1.1\" class=\"ltx_p\"><span id=\"S5.p1.1.1\" class=\"ltx_text ltx_font_bold\">Future Work. </span>\nIn this work, we have investigated how bias is propagated in FL when the sensitive attribute is included as an input feature. In practice, however, sensitive attributes may be prohibited from being included in input features. In such situations, the model may still be heavily biased due to variables that are correlated with the (unobserved) sensitive attribute. For instance, a personâ€™s zip code may be highly correlated with their race, a phenomenon known as \"redlining\". A promising direction for future work is to identify which model parameters contribute to the bias and to audit the bias propagation in this setting. Another important direction is to design FL algorithms that are robust to bias propagation. In AppendixÂ <a href=\"#A5\" title=\"Appendix E Potential Mitigation â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>, we briefly discuss a few potential ways to achieve this goal.</p>\n</div>\n<div id=\"S5.p2\" class=\"ltx_para ltx_noindent\">\n<p id=\"S5.p2.1\" class=\"ltx_p\"><span id=\"S5.p2.1.1\" class=\"ltx_text ltx_font_bold\">Conclusion. </span> Federated learning has become increasingly popular in various applications with significant individual-level consequences, making it essential to anticipate the possible bias introduced by FL. Our paper takes the first step in this direction by providing a comprehensive analysis of the impact of FL on local fairness for parties. We demonstrated that the FL algorithm could introduce bias on its own which may exacerbate the issue of fairness for the involved parties. Moreover, we showed that this exacerbation is not evenly distributed among parties, as FL can propagate bias among them. Finally, we explained how bias is propagated in FL: biased parties encode their bias into the local updates by increasing the signal of a few parameters steadily throughout the training process, which is then propagated to the global model via aggregation and, ultimately, to other parties.</p>\n</div>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<section id=\"S6\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">6 </span>Acknowledgement</h2>\n\n<div id=\"S6.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"S6.p1.1\" class=\"ltx_p\">The authors would like to thank Ergute Bao, Ta Duy Nguyen, and Martin Strobel for their valuable feedback on earlier versions of this paper, as well as the anonymous reviewers for their insightful comments. This research is supported by Google PDPO faculty research award, Intel within the www.private-ai.org center, Meta faculty research award, the NUS Early Career Research Award (NUS ECRA award num- ber NUS ECRA FY19 P16), and the National Research Foundation, Singapore under its Strategic Capability Research Centres Funding Initiative. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the author(s) and do not reflect the views of the National Research Foundation, Singapore.</p>\n</div>\n<section id=\"S7\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">7 </span>Ethics Statement</h2>\n\n<div id=\"S7.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"S7.p1.1\" class=\"ltx_p\">Our analysis focuses on group fairness, which is typically used to audit the model or system for any bias or discrimination. Auditing the bias with respect to other definitions of fairness, such as individual fairness, may result in different conclusions. Moreover, our study focuses primarily on the binary notion of sex attributes and the multi-valued race attribute. We recognize that there are numerous protected groups outside those considered in the analysis, such as those defined by multiple sensitive attributes. The propagation of bias against fine-grained subgroups may be even more substantial than we found in the paper.</p>\n</div>\n<section id=\"S8\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">8 </span>Reproducibility Statement</h2>\n\n<div id=\"S8.p1\" class=\"ltx_para\">\n<p id=\"S8.p1.1\" class=\"ltx_p\">We provide details about the model, datasets, and implementations in AppendixÂ <a href=\"#A1\" title=\"Appendix A Details about Experiment Setup â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>, and the code for the paper is available at <a target=\"_blank\" href=\"https://github.com/privacytrustlab/bias_in_FL\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://github.com/privacytrustlab/bias_in_FL</a>.</p>\n</div>\n<section id=\"bib\" class=\"ltx_bibliography\">\n<h2 class=\"ltx_title ltx_title_bibliography\">References</h2>\n\n<ul class=\"ltx_biblist\">\n<li id=\"bib.bib1\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Abay etÂ al. [2020]</span>\n<span class=\"ltx_bibblock\">\nAnnie Abay, YiÂ Zhou, Nathalie Baracaldo, Shashank Rajamoni, Ebube Chuba, and\nHeiko Ludwig.\n\n</span>\n<span class=\"ltx_bibblock\">Mitigating bias in federated learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib1.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2012.02447</em>, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib2\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Asad etÂ al. [2020]</span>\n<span class=\"ltx_bibblock\">\nMuhammad Asad, Ahmed Moustafa, and Takayuki Ito.\n\n</span>\n<span class=\"ltx_bibblock\">Fedopt: Towards communication efficiency and privacy preservation in\nfederated learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib2.1.1\" class=\"ltx_emph ltx_font_italic\">Applied Sciences</em>, 10(8):2864, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib3\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Blum and Stangl [2020]</span>\n<span class=\"ltx_bibblock\">\nAvrim Blum and Kevin Stangl.\n\n</span>\n<span class=\"ltx_bibblock\">Recovering from biased data: Can fairness constraints improve\naccuracy?\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib3.1.1\" class=\"ltx_emph ltx_font_italic\">1st Symposium on Foundations of Responsible Computing</em>,\n2020.\n\n</span>\n</li>\n<li id=\"bib.bib4\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Calders etÂ al. [2009]</span>\n<span class=\"ltx_bibblock\">\nToon Calders, Faisal Kamiran, and Mykola Pechenizkiy.\n\n</span>\n<span class=\"ltx_bibblock\">Building classifiers with independency constraints.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib4.1.1\" class=\"ltx_emph ltx_font_italic\">2009 IEEE International Conference on Data Mining\nWorkshops</em>, pages 13â€“18. IEEE, 2009.\n\n</span>\n</li>\n<li id=\"bib.bib5\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chu etÂ al. [2021]</span>\n<span class=\"ltx_bibblock\">\nLingyang Chu, Lanjun Wang, Yanjie Dong, Jian Pei, Zirui Zhou, and Yong Zhang.\n\n</span>\n<span class=\"ltx_bibblock\">Fedfair: Training fair models in cross-silo federated learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib5.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2109.05662</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib6\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Combalia etÂ al. [2019]</span>\n<span class=\"ltx_bibblock\">\nMarc Combalia, NoelÂ CF Codella, Veronica Rotemberg, Brian Helba, Veronica\nVilaplana, Ofer Reiter, Cristina Carrera, Alicia Barreiro, AllanÂ C Halpern,\nSusana Puig, etÂ al.\n\n</span>\n<span class=\"ltx_bibblock\">Bcn20000: Dermoscopic lesions in the wild.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib6.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:1908.02288</em>, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib7\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Cui etÂ al. [2021]</span>\n<span class=\"ltx_bibblock\">\nSen Cui, Weishen Pan, Jian Liang, Changshui Zhang, and Fei Wang.\n\n</span>\n<span class=\"ltx_bibblock\">Fair and consistent federated learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib7.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv e-prints</em>, pages arXivâ€“2108, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib8\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Deng etÂ al. [2020]</span>\n<span class=\"ltx_bibblock\">\nYuyang Deng, MohammadÂ Mahdi Kamani, and Mehrdad Mahdavi.\n\n</span>\n<span class=\"ltx_bibblock\">Distributionally robust federated averaging.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib8.1.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>,\n33:15111â€“15122, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib9\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ding etÂ al. [2021]</span>\n<span class=\"ltx_bibblock\">\nFrances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt.\n\n</span>\n<span class=\"ltx_bibblock\">Retiring adult: New datasets for fair machine learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib9.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2108.04884</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib10\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Donahue and Kleinberg [2021]</span>\n<span class=\"ltx_bibblock\">\nKate Donahue and JonÂ M. Kleinberg.\n\n</span>\n<span class=\"ltx_bibblock\">Models of fairness in federated learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib10.1.1\" class=\"ltx_emph ltx_font_italic\">CoRR</em>, abs/2112.00818, 2021.\n\n</span>\n<span class=\"ltx_bibblock\">URL <a target=\"_blank\" href=\"https://arxiv.org/abs/2112.00818\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://arxiv.org/abs/2112.00818</a>.\n\n</span>\n</li>\n<li id=\"bib.bib11\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Du etÂ al. [2021]</span>\n<span class=\"ltx_bibblock\">\nWei Du, Depeng Xu, Xintao Wu, and Hanghang Tong.\n\n</span>\n<span class=\"ltx_bibblock\">Fairness-aware agnostic federated learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib11.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 2021 SIAM International Conference on\nData Mining (SDM)</em>, pages 181â€“189. SIAM, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib12\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Dullerud etÂ al. [2022]</span>\n<span class=\"ltx_bibblock\">\nNatalie Dullerud, Karsten Roth, Kimia Hamidieh, Nicolas Papernot, and Marzyeh\nGhassemi.\n\n</span>\n<span class=\"ltx_bibblock\">Is fairness only metric deep? evaluating and addressing subgroup gaps\nin deep metric learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib12.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2203.12748</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib13\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Dwork etÂ al. [2012]</span>\n<span class=\"ltx_bibblock\">\nCynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel.\n\n</span>\n<span class=\"ltx_bibblock\">Fairness through awareness.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib13.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 3rd Innovations in Theoretical Computer\nScience Conference</em>, ITCS â€™12, page 214â€“226, New York, NY, USA, 2012.\nAssociation for Computing Machinery.\n\n</span>\n<span class=\"ltx_bibblock\">ISBN 9781450311151.\n\n</span>\n<span class=\"ltx_bibblock\">doi: <span class=\"ltx_ref ltx_nolink ltx_Url ltx_ref_self\">10.1145/2090236.2090255</span>.\n\n</span>\n<span class=\"ltx_bibblock\">URL <a target=\"_blank\" href=\"https://doi.org/10.1145/2090236.2090255\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://doi.org/10.1145/2090236.2090255</a>.\n\n</span>\n</li>\n<li id=\"bib.bib14\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ezzeldin etÂ al. [2021]</span>\n<span class=\"ltx_bibblock\">\nYahyaÂ H Ezzeldin, Shen Yan, Chaoyang He, Emilio Ferrara, and Salman Avestimehr.\n\n</span>\n<span class=\"ltx_bibblock\">Fairfed: Enabling group fairness in federated learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib14.1.1\" class=\"ltx_emph ltx_font_italic\">NeurIPS Workshop on New Frontiers in Federated Learning (NFFL\n2021)</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib15\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Friedler etÂ al. [2019]</span>\n<span class=\"ltx_bibblock\">\nSorelleÂ A Friedler, Carlos Scheidegger, Suresh Venkatasubramanian, Sonam\nChoudhary, EvanÂ P Hamilton, and Derek Roth.\n\n</span>\n<span class=\"ltx_bibblock\">A comparative study of fairness-enhancing interventions in machine\nlearning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib15.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the conference on fairness, accountability,\nand transparency</em>, pages 329â€“338, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib16\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Grabowicz etÂ al. [2022]</span>\n<span class=\"ltx_bibblock\">\nPrzemyslawÂ A Grabowicz, Nicholas Perello, and Aarshee Mishra.\n\n</span>\n<span class=\"ltx_bibblock\">Marrying fairness and explainability in supervised learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib16.1.1\" class=\"ltx_emph ltx_font_italic\">2022 ACM Conference on Fairness, Accountability, and\nTransparency</em>, pages 1905â€“1916, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib17\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hao etÂ al. [2021]</span>\n<span class=\"ltx_bibblock\">\nWeituo Hao, Mostafa El-Khamy, Jungwon Lee, Jianyi Zhang, KevinÂ J Liang,\nChangyou Chen, and LawrenceÂ Carin Duke.\n\n</span>\n<span class=\"ltx_bibblock\">Towards fair federated learning with zero-shot data augmentation.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib17.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition</em>, pages 3310â€“3319, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib18\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hardt etÂ al. [2016]</span>\n<span class=\"ltx_bibblock\">\nMoritz Hardt, Eric Price, Eric Price, and Nati Srebro.\n\n</span>\n<span class=\"ltx_bibblock\">Equality of opportunity in supervised learning.\n\n</span>\n<span class=\"ltx_bibblock\">In D.Â Lee, M.Â Sugiyama, U.Â Luxburg, I.Â Guyon, and R.Â Garnett,\neditors, <em id=\"bib.bib18.1.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, volumeÂ 29.\nCurran Associates, Inc., 2016.\n\n</span>\n<span class=\"ltx_bibblock\">URL\n<a target=\"_blank\" href=\"https://proceedings.neurips.cc/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://proceedings.neurips.cc/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf</a>.\n\n</span>\n</li>\n<li id=\"bib.bib19\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hashimoto etÂ al. [2018]</span>\n<span class=\"ltx_bibblock\">\nTatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang.\n\n</span>\n<span class=\"ltx_bibblock\">Fairness without demographics in repeated loss minimization.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib19.1.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Machine Learning</em>, pages\n1929â€“1938. PMLR, 2018.\n\n</span>\n</li>\n<li id=\"bib.bib20\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">He etÂ al. [2020]</span>\n<span class=\"ltx_bibblock\">\nChaoyang He, Songze Li, Jinhyun So, Xiao Zeng, MiÂ Zhang, Hongyi Wang, Xiaoyang\nWang, Praneeth Vepakomma, Abhishek Singh, Hang Qiu, etÂ al.\n\n</span>\n<span class=\"ltx_bibblock\">Fedml: A research library and benchmark for federated machine\nlearning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib20.1.1\" class=\"ltx_emph ltx_font_italic\">NeurIPS 2020 FL Workshop Best Paper Award</em>, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib21\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hu etÂ al. [2022]</span>\n<span class=\"ltx_bibblock\">\nShengyuan Hu, ZhiweiÂ Steven Wu, and Virginia Smith.\n\n</span>\n<span class=\"ltx_bibblock\">Provably fair federated learning via bounded group loss.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib21.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2203.10190</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib22\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Kamiran and Calders [2012]</span>\n<span class=\"ltx_bibblock\">\nFaisal Kamiran and Toon Calders.\n\n</span>\n<span class=\"ltx_bibblock\">Data preprocessing techniques for classification without\ndiscrimination.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib22.1.1\" class=\"ltx_emph ltx_font_italic\">Knowledge and information systems</em>, 33(1):1â€“33, 2012.\n\n</span>\n</li>\n<li id=\"bib.bib23\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Karimireddy etÂ al. [2020a]</span>\n<span class=\"ltx_bibblock\">\nSaiÂ Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, SashankÂ J\nReddi, SebastianÂ U Stich, and AnandaÂ Theertha Suresh.\n\n</span>\n<span class=\"ltx_bibblock\">Mime: Mimicking centralized stochastic algorithms in federated\nlearning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib23.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2008.03606</em>, 2020a.\n\n</span>\n</li>\n<li id=\"bib.bib24\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Karimireddy etÂ al. [2020b]</span>\n<span class=\"ltx_bibblock\">\nSaiÂ Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian\nStich, and AnandaÂ Theertha Suresh.\n\n</span>\n<span class=\"ltx_bibblock\">Scaffold: Stochastic controlled averaging for federated learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib24.1.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Machine Learning</em>, pages\n5132â€“5143. PMLR, 2020b.\n\n</span>\n</li>\n<li id=\"bib.bib25\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Kokhlikyan etÂ al. [2020]</span>\n<span class=\"ltx_bibblock\">\nNarine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh,\nJonathan Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi\nYan, and Orion Reblitz-Richardson.\n\n</span>\n<span class=\"ltx_bibblock\">Captum: A unified and generic model interpretability library for\npytorch, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib26\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Lakkaraju etÂ al. [2017]</span>\n<span class=\"ltx_bibblock\">\nHimabindu Lakkaraju, Jon Kleinberg, Jure Leskovec, Jens Ludwig, and Sendhil\nMullainathan.\n\n</span>\n<span class=\"ltx_bibblock\">The selective labels problem: Evaluating algorithmic predictions in\nthe presence of unobservables.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib26.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 23rd ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining</em>, pages 275â€“284, 2017.\n\n</span>\n</li>\n<li id=\"bib.bib27\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Li etÂ al. [2019]</span>\n<span class=\"ltx_bibblock\">\nTian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith.\n\n</span>\n<span class=\"ltx_bibblock\">Fair resource allocation in federated learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib27.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:1905.10497</em>, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib28\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Li etÂ al. [2020]</span>\n<span class=\"ltx_bibblock\">\nTian Li, AnitÂ Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and\nVirginia Smith.\n\n</span>\n<span class=\"ltx_bibblock\">Federated optimization in heterogeneous networks.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib28.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of Machine Learning and Systems</em>, 2:429â€“450, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib29\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Li etÂ al. [2021]</span>\n<span class=\"ltx_bibblock\">\nTian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith.\n\n</span>\n<span class=\"ltx_bibblock\">Ditto: Fair and robust federated learning through personalization.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib29.1.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Machine Learning</em>, pages\n6357â€“6368. PMLR, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib30\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Liu etÂ al. [2022]</span>\n<span class=\"ltx_bibblock\">\nJiÂ Liu, Zenan Li, Yuan Yao, Feng Xu, Xiaoxing Ma, Miao Xu, and Hanghang Tong.\n\n</span>\n<span class=\"ltx_bibblock\">Fair representation learning: An alternative to mutual information.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib30.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 28th ACM SIGKDD Conference on Knowledge\nDiscovery and Data Mining</em>, pages 1088â€“1097, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib31\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Liu etÂ al. [2015]</span>\n<span class=\"ltx_bibblock\">\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.\n\n</span>\n<span class=\"ltx_bibblock\">Deep learning face attributes in the wild.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib31.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of International Conference on Computer Vision\n(ICCV)</em>, December 2015.\n\n</span>\n</li>\n<li id=\"bib.bib32\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Lyu etÂ al. [2020]</span>\n<span class=\"ltx_bibblock\">\nLingjuan Lyu, Xinyi Xu, Qian Wang, and Han Yu.\n\n</span>\n<span class=\"ltx_bibblock\">Collaborative fairness in federated learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib32.1.1\" class=\"ltx_emph ltx_font_italic\">Federated Learning</em>, pages 189â€“204. Springer, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib33\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">McMahan etÂ al. [2017]</span>\n<span class=\"ltx_bibblock\">\nH.Â Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and\nBlaiseÂ AgÃ¼era yÂ Arcas.\n\n</span>\n<span class=\"ltx_bibblock\">Communication-efficient learning of deep networks from decentralized\ndata, 2017.\n\n</span>\n</li>\n<li id=\"bib.bib34\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Mohri etÂ al. [2019]</span>\n<span class=\"ltx_bibblock\">\nMehryar Mohri, Gary Sivek, and AnandaÂ Theertha Suresh.\n\n</span>\n<span class=\"ltx_bibblock\">Agnostic federated learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib34.1.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Machine Learning</em>, pages\n4615â€“4625. PMLR, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib35\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">OgierÂ du Terrail etÂ al. [2022]</span>\n<span class=\"ltx_bibblock\">\nJean OgierÂ du Terrail, Samy-Safwan Ayed, Edwige Cyffers, Felix Grimberg,\nChaoyang He, Regis Loeb, Paul Mangold, Tanguy Marchand, Othmane Marfoq, Erum\nMushtaq, Boris Muzellec, Constantin Philippenko, Santiago Silva, Maria\nTeleÅ„czuk, Shadi Albarqouni, Salman Avestimehr, AurÃ©lien Bellet, Aymeric\nDieuleveut, Martin Jaggi, SaiÂ Praneeth Karimireddy, Marco Lorenzi, Giovanni\nNeglia, Marc Tommasi, and Mathieu Andreux.\n\n</span>\n<span class=\"ltx_bibblock\">Flamby: Datasets and benchmarks for cross-silo federated learning in\nrealistic healthcare settings, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib36\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Papadaki etÂ al. [2021]</span>\n<span class=\"ltx_bibblock\">\nAfroditi Papadaki, Natalia Martinez, Martin Bertran, Guillermo Sapiro, and\nMiguel Rodrigues.\n\n</span>\n<span class=\"ltx_bibblock\">Federating for learning group fair models.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib36.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2110.01999</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib37\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Papadaki etÂ al. [2022]</span>\n<span class=\"ltx_bibblock\">\nAfroditi Papadaki, Natalia Martinez, Martin Bertran, Guillermo Sapiro, and\nMiguel Rodrigues.\n\n</span>\n<span class=\"ltx_bibblock\">Minimax demographic group fairness in federated learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib37.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2201.08304</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib38\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Paszke etÂ al. [2019]</span>\n<span class=\"ltx_bibblock\">\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban\nDesmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan\nTejani, Sasank Chilamkurthy, Benoit Steiner, LuÂ Fang, Junjie Bai, and Soumith\nChintala.\n\n</span>\n<span class=\"ltx_bibblock\">Pytorch: An imperative style, high-performance deep learning library.\n\n</span>\n<span class=\"ltx_bibblock\">In H.Â Wallach, H.Â Larochelle, A.Â Beygelzimer, F.Â d'AlchÃ©-Buc, E.Â Fox, and R.Â Garnett, editors, <em id=\"bib.bib38.1.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural\nInformation Processing Systems 32</em>, pages 8024â€“8035. Curran Associates,\nInc., 2019.\n\n</span>\n<span class=\"ltx_bibblock\">URL\n<a target=\"_blank\" href=\"http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</a>.\n\n</span>\n</li>\n<li id=\"bib.bib39\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Rambachan and Roth [2020]</span>\n<span class=\"ltx_bibblock\">\nAshesh Rambachan and Jonathan Roth.\n\n</span>\n<span class=\"ltx_bibblock\">Bias in, bias out? evaluating the folk wisdom.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib39.1.1\" class=\"ltx_emph ltx_font_italic\">1st Symposium on Foundations of Responsible Computing</em>,\n2020.\n\n</span>\n</li>\n<li id=\"bib.bib40\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Reddi etÂ al. [2020]</span>\n<span class=\"ltx_bibblock\">\nSashankÂ J Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,\nJakub KoneÄná»³, Sanjiv Kumar, and HughÂ Brendan McMahan.\n\n</span>\n<span class=\"ltx_bibblock\">Adaptive federated optimization.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib40.1.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Learning Representations</em>, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib41\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Rieke etÂ al. [2020]</span>\n<span class=\"ltx_bibblock\">\nNicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletari, HolgerÂ R Roth, Shadi\nAlbarqouni, Spyridon Bakas, MathieuÂ N Galtier, BennettÂ A Landman, Klaus\nMaier-Hein, etÂ al.\n\n</span>\n<span class=\"ltx_bibblock\">The future of digital health with federated learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib41.1.1\" class=\"ltx_emph ltx_font_italic\">NPJ digital medicine</em>, 3(1):1â€“7, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib42\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Sarah etÂ al. [2020]</span>\n<span class=\"ltx_bibblock\">\nFlood Sarah, King Miriam, Rodger Renae, Ruggles Steven, and Warren J.Â Robert.\n\n</span>\n<span class=\"ltx_bibblock\">Integrated public use microdata series, current population survey:\nVersion 8.0 [dataset], 2020.\n\n</span>\n<span class=\"ltx_bibblock\">URL <a target=\"_blank\" href=\"https://www.ipums.org/projects/ipums-cps/d030.v8.0\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://www.ipums.org/projects/ipums-cps/d030.v8.0</a>.\n\n</span>\n</li>\n<li id=\"bib.bib43\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Sundararajan etÂ al. [2017]</span>\n<span class=\"ltx_bibblock\">\nMukund Sundararajan, Ankur Taly, and Qiqi Yan.\n\n</span>\n<span class=\"ltx_bibblock\">Axiomatic attribution for deep networks.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib43.1.1\" class=\"ltx_emph ltx_font_italic\">International conference on machine learning</em>, pages\n3319â€“3328. PMLR, 2017.\n\n</span>\n</li>\n<li id=\"bib.bib44\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Tan and Le [2019]</span>\n<span class=\"ltx_bibblock\">\nMingxing Tan and Quoc Le.\n\n</span>\n<span class=\"ltx_bibblock\">Efficientnet: Rethinking model scaling for convolutional neural\nnetworks.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib44.1.1\" class=\"ltx_emph ltx_font_italic\">International conference on machine learning</em>, pages\n6105â€“6114. PMLR, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib45\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Tschandl etÂ al. [2018]</span>\n<span class=\"ltx_bibblock\">\nPhilipp Tschandl, Cliff Rosendahl, and Harald Kittler.\n\n</span>\n<span class=\"ltx_bibblock\">The ham10000 dataset, a large collection of multi-source\ndermatoscopic images of common pigmented skin lesions.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib45.1.1\" class=\"ltx_emph ltx_font_italic\">Scientific data</em>, 5(1):1â€“9, 2018.\n\n</span>\n</li>\n<li id=\"bib.bib46\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wang etÂ al. [2020]</span>\n<span class=\"ltx_bibblock\">\nJianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and HÂ Vincent Poor.\n\n</span>\n<span class=\"ltx_bibblock\">Tackling the objective inconsistency problem in heterogeneous\nfederated optimization.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib46.1.1\" class=\"ltx_emph ltx_font_italic\">Advances in neural information processing systems</em>,\n33:7611â€“7623, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib47\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yang etÂ al. [2019]</span>\n<span class=\"ltx_bibblock\">\nWensi Yang, Yuhang Zhang, Kejiang Ye, LiÂ Li, and Cheng-Zhong Xu.\n\n</span>\n<span class=\"ltx_bibblock\">Ffd: A federated learning based method for credit card fraud\ndetection.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib47.1.1\" class=\"ltx_emph ltx_font_italic\">International conference on big data</em>, pages 18â€“32.\nSpringer, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib48\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yu etÂ al. [2020]</span>\n<span class=\"ltx_bibblock\">\nHan Yu, Zelei Liu, Yang Liu, Tianjian Chen, Mingshu Cong, XiÂ Weng, Dusit\nNiyato, and Qiang Yang.\n\n</span>\n<span class=\"ltx_bibblock\">A fairness-aware incentive scheme for federated learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib48.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the AAAI/ACM Conference on AI, Ethics, and\nSociety</em>, pages 393â€“399, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib49\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zafar etÂ al. [2017]</span>\n<span class=\"ltx_bibblock\">\nMuhammadÂ Bilal Zafar, Isabel Valera, Manuel GomezÂ Rodriguez, and KrishnaÂ P\nGummadi.\n\n</span>\n<span class=\"ltx_bibblock\">Fairness beyond disparate treatment &amp; disparate impact: Learning\nclassification without disparate mistreatment.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib49.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 26th international conference on world\nwide web</em>, pages 1171â€“1180, 2017.\n\n</span>\n</li>\n<li id=\"bib.bib50\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zemel etÂ al. [2013]</span>\n<span class=\"ltx_bibblock\">\nRich Zemel, YuÂ Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork.\n\n</span>\n<span class=\"ltx_bibblock\">Learning fair representations.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib50.1.1\" class=\"ltx_emph ltx_font_italic\">International conference on machine learning</em>, pages\n325â€“333. PMLR, 2013.\n\n</span>\n</li>\n<li id=\"bib.bib51\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zeng etÂ al. [2021a]</span>\n<span class=\"ltx_bibblock\">\nYuchen Zeng, Hongxu Chen, and Kangwook Lee.\n\n</span>\n<span class=\"ltx_bibblock\">Improving fairness via federated learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib51.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2110.15545</em>, 2021a.\n\n</span>\n</li>\n<li id=\"bib.bib52\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zeng etÂ al. [2021b]</span>\n<span class=\"ltx_bibblock\">\nZiqian Zeng, Rashidul Islam, KamrunÂ Naher Keya, James Foulds, Yangqiu Song, and\nShimei Pan.\n\n</span>\n<span class=\"ltx_bibblock\">Fair representation learning for heterogeneous information networks.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib52.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the International AAAI Conference on Weblogs\nand Social Media</em>, volumeÂ 15, 2021b.\n\n</span>\n</li>\n<li id=\"bib.bib53\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang etÂ al. [2020]</span>\n<span class=\"ltx_bibblock\">\nXueru Zhang, Ruibo Tu, Yang Liu, Mingyan Liu, Hedvig Kjellstrom, Kun Zhang, and\nCheng Zhang.\n\n</span>\n<span class=\"ltx_bibblock\">How do fair decisions fare in long-term qualification?\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib53.1.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>,\n33:18457â€“18469, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib54\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhao etÂ al. [2019]</span>\n<span class=\"ltx_bibblock\">\nHan Zhao, Amanda Coston, Tameem Adel, and GeoffreyÂ J Gordon.\n\n</span>\n<span class=\"ltx_bibblock\">Conditional learning of fair representations.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib54.1.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Learning Representations</em>, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib55\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhao and Joshi [2022]</span>\n<span class=\"ltx_bibblock\">\nZhiyuan Zhao and Gauri Joshi.\n\n</span>\n<span class=\"ltx_bibblock\">A dynamic reweighting strategy for fair federated learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib55.1.1\" class=\"ltx_emph ltx_font_italic\">ICASSP 2022-2022 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP)</em>, pages 8772â€“8776. IEEE, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib56\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhou etÂ al. [2021]</span>\n<span class=\"ltx_bibblock\">\nZirui Zhou, Lingyang Chu, Changxin Liu, Lanjun Wang, Jian Pei, and Yong Zhang.\n\n</span>\n<span class=\"ltx_bibblock\">Towards fair federated learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib56.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 27th ACM SIGKDD Conference on Knowledge\nDiscovery &amp; Data Mining</em>, pages 4100â€“4101, 2021.\n\n</span>\n</li>\n</ul>\n</section>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<section id=\"Ax1\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">Appendix</h2>\n\n<div id=\"Ax1.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"Ax1.p1.1\" class=\"ltx_p\">This appendix is divided into five sections. AppendixÂ <a href=\"#A1\" title=\"Appendix A Details about Experiment Setup â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">A</span></a> provides additional details about the experimental setups, including information about the models, datasets, and hyperparameters used. AppendixÂ <a href=\"#A2\" title=\"Appendix B Additional Experimental Results â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> presents further experimental results that support the claims made in the paper. In AppendixÂ <a href=\"#A3\" title=\"Appendix C Evaluation on existing Fair FL â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>, we discuss the results of an existing fair FL algorithm. AppendixÂ <a href=\"#A4\" title=\"Appendix D Extending to a real-world medical dataset â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">D</span></a> extends our analysis to real-world medical datasets. Finally, in AppendixÂ <a href=\"#A5\" title=\"Appendix E Potential Mitigation â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>, we discuss potential methods for mitigating bias in FL.</p>\n</div>\n</section>\n<section id=\"A1\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix A </span>Details about Experiment Setup</h2>\n\n<section id=\"A1.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.1 </span>Datasets and Models</h3>\n\n<div id=\"A1.SS1.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"A1.SS1.p1.1\" class=\"ltx_p\">We explain the datasets and models used in the paper.</p>\n</div>\n<section id=\"A1.SS1.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Census Dataset</h4>\n\n<div id=\"A1.SS1.SSS0.Px1.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"A1.SS1.SSS0.Px1.p1.1\" class=\"ltx_p\">We use the datasets provided by folktables. In particular, we consider the ACSIncome, ACSPublicCoverage, and ACSEmployment tasks defined in the forlktables. In the ACSIncome, the goal is to predict whether an individualâ€™s Income is above $50,000. In the ACSEmployment task, the goal is to predict whether an individual is employed. Similarly, in the Health (i.e., ACSPublicCoverage) task, the objective is to predict whether an individual is covered by public health insurance. We use the same pre-processing as in the folktables and train a fully connected neural network model with one hidden layer of 32 neurons for Income and 64 neurons for Employment and Health tasks. For all the tasks, we use the RELU activation function. We use an SGD optimizer with a learning rate of 0.001 for centralized training on Health and Employment datasets and 0.1 for other settings, and the batch size is 32. We train the NN models for 200 epochs. In FL, each client updates the global mdoel for 1 epoch and shares it with the server. We encode the categorical features based on the encoding template provided in folktables. After the encoding, the input feature size for Income is 54, 154 for Health, and 109 for Employment. We consider sex and race as sensitive attributes. Accordingly, there are two gender groups (male and female) and nine racial groups (\"White alone,\" \"Black or African American alone,\" \"American Indian alone,\" \"Alaska Native alone,\" and \"American Indian and Alaska Native tribes specified; or American Indian or Alaska Native, not specified and no other,\" \"Asian alone,\" \"Native Hawaiian and Other Pacific Islander alone,\" \"Some Other Race alone,\" \"Two or More Races\").</p>\n</div>\n</section>\n<section id=\"A1.SS1.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">CelebA Dataset</h4>\n\n<div id=\"A1.SS1.SSS0.Px2.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"A1.SS1.SSS0.Px2.p1.1\" class=\"ltx_p\">We train CNN models on the dataset with one CNN layer whose output channel is 32, kernel size is 3, and stride is 1. We use â€™sameâ€™ padding for the CNN layer. Following this CNN layer, we have the Batch normalization layer and Max Pooling layer. After which, we have the connected layer. We train the model for 500 communication rounds or epochs with SGD optimizer. The learning rate is 0.1, and the batch size is 128. The train, test, and validation datasets ratio for each party is 6:2:2.</p>\n</div>\n</section>\n</section>\n<section id=\"A1.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.2 </span>Hyper-parameter for FL algorithms and Implementation</h3>\n\n<div id=\"A1.SS2.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"A1.SS2.p1.1\" class=\"ltx_p\">In our paper, we also evaluate various popular FL algorithms, including FedNovaÂ <cite class=\"ltx_cite ltx_citemacro_citep\">[Wang etÂ al., <a href=\"#bib.bib46\" title=\"\" class=\"ltx_ref\">2020</a>]</cite>, ScaffoldÂ <cite class=\"ltx_cite ltx_citemacro_cite\">Karimireddy etÂ al. [<a href=\"#bib.bib24\" title=\"\" class=\"ltx_ref\">2020b</a>]</cite>, FedOptÂ <cite class=\"ltx_cite ltx_citemacro_citep\">[Reddi etÂ al., <a href=\"#bib.bib40\" title=\"\" class=\"ltx_ref\">2020</a>]</cite>, FedProxÂ <cite class=\"ltx_cite ltx_citemacro_cite\">Li etÂ al. [<a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">2020</a>]</cite>, and MimeÂ <cite class=\"ltx_cite ltx_citemacro_citep\">[Karimireddy etÂ al., <a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">2020a</a>]</cite>. We use the same local learning rate and the number of the local epoch as in FedAvg. We provided the detailed hyper-parameters for each of the algorithms in our code (See supplementary). We run all experiments on Ubuntu with two NVIDIA TITAN RTX GPUs.</p>\n</div>\n</section>\n<section id=\"A1.SS3\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.3 </span>Data Heterogeneity</h3>\n\n<div id=\"A1.SS3.p1\" class=\"ltx_para\">\n<p id=\"A1.SS3.p1.1\" class=\"ltx_p\">FigureÂ <a href=\"#A1.F11\" title=\"Figure 11 â€£ A.3 Data Heterogeneity â€£ Appendix A Details about Experiment Setup â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> shows the fraction of samples for each subgroup across all parties, while FigureÂ <a href=\"#A1.F12\" title=\"Figure 12 â€£ A.3 Data Heterogeneity â€£ Appendix A Details about Experiment Setup â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> shows the histogram of subgroup proportions for each party. We observe that the parties have different fractions of samples from each group in the Income dataset, indicating that their local data distributions are dissimilar. In contrast, for the Health and Employment datasets, parties have similar fractions of samples from each subgroup, suggesting that their data distributions are more alike than those of the Income dataset.</p>\n</div>\n<figure id=\"A1.F11\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_3\"><img src=\"/html/2309.02160/assets/images/income/non-iid/subgroup_distribution_attr_1.png\" id=\"A1.F11.g1\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait\" width=\"168\" height=\"955\" alt=\"Refer to caption\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_3\"><img src=\"/html/2309.02160/assets/images/employment/non-iid/subgroup_distribution_attr_1.png\" id=\"A1.F11.g2\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait\" width=\"174\" height=\"950\" alt=\"Refer to caption\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_3\"><img src=\"/html/2309.02160/assets/images/health/non-iid/subgroup_distribution_attr_1.png\" id=\"A1.F11.g3\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait\" width=\"168\" height=\"955\" alt=\"Refer to caption\"></div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 11: </span>Fraction of samples for each subgroup - Sex</figcaption>\n</figure>\n<figure id=\"A1.F12\" class=\"ltx_figure\"><img src=\"/html/2309.02160/assets/images/hist_base_rate_attr_combined.png\" id=\"A1.F12.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"159\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 12: </span>Histogram of the fraction of samples for each group with positive label - Sex</figcaption>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>\n</section>\n<section id=\"A2\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix B </span>Additional Experimental Results</h2>\n\n<section id=\"A2.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.1 </span>Bias propagation effect</h3>\n\n<section id=\"A2.SS1.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Census dataset</h4>\n\n<div id=\"A2.SS1.SSS0.Px1.p1\" class=\"ltx_para\">\n<p id=\"A2.SS1.SSS0.Px1.p1.1\" class=\"ltx_p\">We show the bias propagation effect of FL for the Health and Employment task in FigureÂ <a href=\"#A2.F13\" title=\"Figure 13 â€£ Census dataset â€£ B.1 Bias propagation effect â€£ Appendix B Additional Experimental Results â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> and <a href=\"#A2.F14\" title=\"Figure 14 â€£ Census dataset â€£ B.1 Bias propagation effect â€£ Appendix B Additional Experimental Results â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">14</span></a> respectively.</p>\n</div>\n<figure id=\"A2.F13\" class=\"ltx_figure\"><img src=\"/html/2309.02160/assets/images/health/bias_propagation/combined_attrs.png\" id=\"A2.F13.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"112\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 13: </span><span id=\"A2.F13.2.1\" class=\"ltx_text ltx_font_bold\">Correlation between the fairness gap of the standalone model and the benefit obtained from FL - Health</span></figcaption>\n</figure>\n<figure id=\"A2.F14\" class=\"ltx_figure\"><img src=\"/html/2309.02160/assets/images/employment/bias_propagation/combined_attrs.png\" id=\"A2.F14.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"111\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 14: </span><span id=\"A2.F14.2.1\" class=\"ltx_text ltx_font_bold\">Correlation between the fairness gap of the standalone model and the benefit obtained from FL - Employment</span></figcaption>\n</figure>\n</section>\n<section id=\"A2.SS1.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">CelebA</h4>\n\n<div id=\"A2.SS1.SSS0.Px2.p1\" class=\"ltx_para\">\n<p id=\"A2.SS1.SSS0.Px2.p1.1\" class=\"ltx_p\">We show the bias propagation effect of FL on CelebA dataset. Besides partitioning the data in an IID manner (we refer to as setting (i)), we also evaluate two different settings, where we change the number of samples from the minority subgroup (the female group with the â€œNot Youngâ€ age label). In this way, we aim to change the data bias in the local training datasets for clients. In particular, in setting (ii), half of the parties have more samples from the minority subgroup than another half of the parties. The ratio is 8:2. In setting (iii), a single party has half of the data from the minority subgroup, and the data is then iid partitioned among the other parties. In the figure, we find that the in the non-iid setting, the more biased parties have a larger and more positive benefit, while FL hurts the less biased parties. FigureÂ <a href=\"#A2.F15\" title=\"Figure 15 â€£ CelebA â€£ B.1 Bias propagation effect â€£ Appendix B Additional Experimental Results â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">15</span></a> shows the correlation between the fairness gap of the standalone model and the benefit a party gets in the FL. We find that the in the non-iid setting, the more biased parties have a larger and more positive benefit, while FL hurts the less biased parties.</p>\n</div>\n<figure id=\"A2.F15\" class=\"ltx_figure\"><img src=\"/html/2309.02160/assets/images/celeba/bias_propagation_celeba_age.png\" id=\"A2.F15.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"292\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 15: </span><span id=\"A2.F15.2.1\" class=\"ltx_text ltx_font_bold\">Correlation between the fairness gap of the standalone model and the benefit obtained from FL - CelebA (Age)</span></figcaption>\n</figure>\n</section>\n</section>\n<section id=\"A2.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.2 </span>Aggregation contradicts with local update</h3>\n\n<div id=\"A2.SS2.p1\" class=\"ltx_para\">\n<p id=\"A2.SS2.p1.1\" class=\"ltx_p\">FigureÂ <a href=\"#A2.F16\" title=\"Figure 16 â€£ B.2 Aggregation contradicts with local update â€£ Appendix B Additional Experimental Results â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> shows the dynamic of the fairness gap for the aggregated model and locally updated model from the most biased party and least biased party during the training.</p>\n</div>\n<figure id=\"A2.F16\" class=\"ltx_figure\"><img src=\"/html/2309.02160/assets/images/income/dynamic/attr_1_metric_dp_most_biased_200.png\" id=\"A2.F16.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"321\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 16: </span><span id=\"A2.F16.2.1\" class=\"ltx_text ltx_font_bold\">Dynamic of <math id=\"A2.F16.2.1.m1.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{DP}\" display=\"inline\"><semantics id=\"A2.F16.2.1.m1.1b\"><msup id=\"A2.F16.2.1.m1.1.1\" xref=\"A2.F16.2.1.m1.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A2.F16.2.1.m1.1.1.2\" xref=\"A2.F16.2.1.m1.1.1.2.cmml\">Î”</mi><mrow id=\"A2.F16.2.1.m1.1.1.3\" xref=\"A2.F16.2.1.m1.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A2.F16.2.1.m1.1.1.3.2\" xref=\"A2.F16.2.1.m1.1.1.3.2.cmml\">D</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A2.F16.2.1.m1.1.1.3.1\" xref=\"A2.F16.2.1.m1.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#F5FAFC\" id=\"A2.F16.2.1.m1.1.1.3.3\" xref=\"A2.F16.2.1.m1.1.1.3.3.cmml\">P</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A2.F16.2.1.m1.1c\"><apply id=\"A2.F16.2.1.m1.1.1.cmml\" xref=\"A2.F16.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.F16.2.1.m1.1.1.1.cmml\" xref=\"A2.F16.2.1.m1.1.1\">superscript</csymbol><ci id=\"A2.F16.2.1.m1.1.1.2.cmml\" xref=\"A2.F16.2.1.m1.1.1.2\">Î”</ci><apply id=\"A2.F16.2.1.m1.1.1.3.cmml\" xref=\"A2.F16.2.1.m1.1.1.3\"><times id=\"A2.F16.2.1.m1.1.1.3.1.cmml\" xref=\"A2.F16.2.1.m1.1.1.3.1\"></times><ci id=\"A2.F16.2.1.m1.1.1.3.2.cmml\" xref=\"A2.F16.2.1.m1.1.1.3.2\">ğ·</ci><ci id=\"A2.F16.2.1.m1.1.1.3.3.cmml\" xref=\"A2.F16.2.1.m1.1.1.3.3\">ğ‘ƒ</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.F16.2.1.m1.1d\">\\Delta^{DP}</annotation></semantics></math> during the training - Income (sex). </span> Figure shows the fairness gap for the aggregated model and local updated model from the most biased party and least biased party during the training. The most (least) biased party is the party with the highest (lowest) fairness gap in the standalone setting. The fairness gap in the standalone setting for those parties is shown in the title.\n</figcaption>\n</figure>\n</section>\n<section id=\"A2.SS3\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.3 </span>Influence Sub-graphs</h3>\n\n<div id=\"A2.SS3.p1\" class=\"ltx_para\">\n<p id=\"A2.SS3.p1.1\" class=\"ltx_p\">We show the top 10 maximal positive and top 10 maximal native influence client pairs in FigureÂ <a href=\"#A2.F17\" title=\"Figure 17 â€£ B.3 Influence Sub-graphs â€£ Appendix B Additional Experimental Results â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> and the top 15 in FigureÂ <a href=\"#A2.F18\" title=\"Figure 18 â€£ B.3 Influence Sub-graphs â€£ Appendix B Additional Experimental Results â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">18</span></a>. We can see that the less biased client has a strong positive influence on other clients while the more biased client has a strong negative influence on other clients.</p>\n</div>\n<figure id=\"A2.F17\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><img src=\"/html/2309.02160/assets/images/income/propagation_path/overall_attr_1_metric_d_dp_appendix_top_10.png\" id=\"A2.F17.g1\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" width=\"598\" height=\"129\" alt=\"Refer to caption\"></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><img src=\"/html/2309.02160/assets/images/income/propagation_path/overall_attr_0_metric_d_dp_appendix_top_10.png\" id=\"A2.F17.g2\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" width=\"598\" height=\"130\" alt=\"Refer to caption\"></div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 17: </span><span id=\"A2.F17.2.1\" class=\"ltx_text ltx_font_bold\">Influence subgraph - Income (<math id=\"A2.F17.2.1.m1.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{DP}\" display=\"inline\"><semantics id=\"A2.F17.2.1.m1.1b\"><msup id=\"A2.F17.2.1.m1.1.1\" xref=\"A2.F17.2.1.m1.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A2.F17.2.1.m1.1.1.2\" xref=\"A2.F17.2.1.m1.1.1.2.cmml\">Î”</mi><mrow id=\"A2.F17.2.1.m1.1.1.3\" xref=\"A2.F17.2.1.m1.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A2.F17.2.1.m1.1.1.3.2\" xref=\"A2.F17.2.1.m1.1.1.3.2.cmml\">D</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A2.F17.2.1.m1.1.1.3.1\" xref=\"A2.F17.2.1.m1.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#F5FAFC\" id=\"A2.F17.2.1.m1.1.1.3.3\" xref=\"A2.F17.2.1.m1.1.1.3.3.cmml\">P</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A2.F17.2.1.m1.1c\"><apply id=\"A2.F17.2.1.m1.1.1.cmml\" xref=\"A2.F17.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.F17.2.1.m1.1.1.1.cmml\" xref=\"A2.F17.2.1.m1.1.1\">superscript</csymbol><ci id=\"A2.F17.2.1.m1.1.1.2.cmml\" xref=\"A2.F17.2.1.m1.1.1.2\">Î”</ci><apply id=\"A2.F17.2.1.m1.1.1.3.cmml\" xref=\"A2.F17.2.1.m1.1.1.3\"><times id=\"A2.F17.2.1.m1.1.1.3.1.cmml\" xref=\"A2.F17.2.1.m1.1.1.3.1\"></times><ci id=\"A2.F17.2.1.m1.1.1.3.2.cmml\" xref=\"A2.F17.2.1.m1.1.1.3.2\">ğ·</ci><ci id=\"A2.F17.2.1.m1.1.1.3.3.cmml\" xref=\"A2.F17.2.1.m1.1.1.3.3\">ğ‘ƒ</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.F17.2.1.m1.1d\">\\Delta^{DP}</annotation></semantics></math>)</span> Top 10 maximal positive influential pair and top 10 maximal negative influential pair. The <span id=\"A2.F17.9.2\" class=\"ltx_text\" style=\"color:#7FB77E;\">green</span> edge and <span id=\"A2.F17.10.3\" class=\"ltx_text\" style=\"color:#EC7272;\">red</span> edge represent the <span id=\"A2.F17.11.4\" class=\"ltx_text\" style=\"color:#7FB77E;\">positive</span> and <span id=\"A2.F17.12.5\" class=\"ltx_text\" style=\"color:#EC7272;\">negative</span> influence, respectively. The <span id=\"A2.F17.13.6\" class=\"ltx_text\" style=\"color:#5C8AA8;\">color</span> of the node represents the <span id=\"A2.F17.14.7\" class=\"ltx_text\" style=\"color:#5C8AA8;\">fairness gap in the standalone setting</span>.</figcaption>\n</figure>\n<figure id=\"A2.F18\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><img src=\"/html/2309.02160/assets/images/income/propagation_path/overall_attr_1_metric_d_dp_appendix_top_15.png\" id=\"A2.F18.g1\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" width=\"598\" height=\"129\" alt=\"Refer to caption\"></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><img src=\"/html/2309.02160/assets/images/income/propagation_path/overall_attr_0_metric_d_dp_appendix_top_15.png\" id=\"A2.F18.g2\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" width=\"598\" height=\"130\" alt=\"Refer to caption\"></div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 18: </span><span id=\"A2.F18.2.1\" class=\"ltx_text ltx_font_bold\">Influence subgraph - Income (<math id=\"A2.F18.2.1.m1.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{DP}\" display=\"inline\"><semantics id=\"A2.F18.2.1.m1.1b\"><msup id=\"A2.F18.2.1.m1.1.1\" xref=\"A2.F18.2.1.m1.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A2.F18.2.1.m1.1.1.2\" xref=\"A2.F18.2.1.m1.1.1.2.cmml\">Î”</mi><mrow id=\"A2.F18.2.1.m1.1.1.3\" xref=\"A2.F18.2.1.m1.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A2.F18.2.1.m1.1.1.3.2\" xref=\"A2.F18.2.1.m1.1.1.3.2.cmml\">D</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A2.F18.2.1.m1.1.1.3.1\" xref=\"A2.F18.2.1.m1.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#F5FAFC\" id=\"A2.F18.2.1.m1.1.1.3.3\" xref=\"A2.F18.2.1.m1.1.1.3.3.cmml\">P</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A2.F18.2.1.m1.1c\"><apply id=\"A2.F18.2.1.m1.1.1.cmml\" xref=\"A2.F18.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.F18.2.1.m1.1.1.1.cmml\" xref=\"A2.F18.2.1.m1.1.1\">superscript</csymbol><ci id=\"A2.F18.2.1.m1.1.1.2.cmml\" xref=\"A2.F18.2.1.m1.1.1.2\">Î”</ci><apply id=\"A2.F18.2.1.m1.1.1.3.cmml\" xref=\"A2.F18.2.1.m1.1.1.3\"><times id=\"A2.F18.2.1.m1.1.1.3.1.cmml\" xref=\"A2.F18.2.1.m1.1.1.3.1\"></times><ci id=\"A2.F18.2.1.m1.1.1.3.2.cmml\" xref=\"A2.F18.2.1.m1.1.1.3.2\">ğ·</ci><ci id=\"A2.F18.2.1.m1.1.1.3.3.cmml\" xref=\"A2.F18.2.1.m1.1.1.3.3\">ğ‘ƒ</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.F18.2.1.m1.1d\">\\Delta^{DP}</annotation></semantics></math>)</span> Top 15 maximal positive influential pair and top 15 maximal negative influential pair. The <span id=\"A2.F18.9.2\" class=\"ltx_text\" style=\"color:#7FB77E;\">green</span> edge and <span id=\"A2.F18.10.3\" class=\"ltx_text\" style=\"color:#EC7272;\">red</span> edge represent the <span id=\"A2.F18.11.4\" class=\"ltx_text\" style=\"color:#7FB77E;\">positive</span> and <span id=\"A2.F18.12.5\" class=\"ltx_text\" style=\"color:#EC7272;\">negative</span> influence, respectively. The <span id=\"A2.F18.13.6\" class=\"ltx_text\" style=\"color:#5C8AA8;\">color</span> of the node represents the <span id=\"A2.F18.14.7\" class=\"ltx_text\" style=\"color:#5C8AA8;\">fairness gap in the standalone setting</span>.</figcaption>\n</figure>\n</section>\n<section id=\"A2.SS4\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.4 </span>Attribution value for sensitive attribute</h3>\n\n<div id=\"A2.SS4.p1\" class=\"ltx_para\">\n<p id=\"A2.SS4.p1.1\" class=\"ltx_p\">We show the attribution value for all input features for the most biased party and least biased party in FigureÂ <a href=\"#A2.F19\" title=\"Figure 19 â€£ B.4 Attribution value for sensitive attribute â€£ Appendix B Additional Experimental Results â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> and FigureÂ <a href=\"#A2.F20\" title=\"Figure 20 â€£ B.4 Attribution value for sensitive attribute â€£ Appendix B Additional Experimental Results â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, respectively. We observe that the attribution value for other features is similar for female and male groups. However, the sex attribute has the largest attribution value and affects the groups differently. It implies that the models are highly dependent on the sensitive attribute (i.e., \"Sex\") for making predictions.</p>\n</div>\n<figure id=\"A2.F19\" class=\"ltx_figure\"><img src=\"/html/2309.02160/assets/images/income/attribution_value/client_43_gender_199.png\" id=\"A2.F19.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"171\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 19: </span><span id=\"A2.F19.2.1\" class=\"ltx_text ltx_font_bold\">Feature attribution value - Income (Most biased party)</span> We shows the feature attribution value for all the input features. The model trained in FL and standalone setting has a large dependency on the sensitive attribute \"Sex\". </figcaption>\n</figure>\n<figure id=\"A2.F20\" class=\"ltx_figure\"><img src=\"/html/2309.02160/assets/images/income/attribution_value/client_50_gender_199.png\" id=\"A2.F20.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"171\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 20: </span><span id=\"A2.F20.2.1\" class=\"ltx_text ltx_font_bold\">Feature attribution value - Income (Least biased party)</span> We shows the feature attribution value for all the input features. The model trained in FL and standalone setting has a large dependency on the sensitive attribute \"Sex\". </figcaption>\n</figure>\n</section>\n<section id=\"A2.SS5\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.5 </span>Effect of FL algorithm</h3>\n\n<div id=\"A2.SS5.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"A2.SS5.p1.1\" class=\"ltx_p\">TableÂ <a href=\"#A2.T2\" title=\"Table 2 â€£ B.5 Effect of FL algorithm â€£ Appendix B Additional Experimental Results â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows results on other FL algorithms, including FedNovaÂ <cite class=\"ltx_cite ltx_citemacro_citep\">[Wang etÂ al., <a href=\"#bib.bib46\" title=\"\" class=\"ltx_ref\">2020</a>]</cite>, ScaffoldÂ <cite class=\"ltx_cite ltx_citemacro_cite\">Karimireddy etÂ al. [<a href=\"#bib.bib24\" title=\"\" class=\"ltx_ref\">2020b</a>]</cite>, FedOptÂ <cite class=\"ltx_cite ltx_citemacro_citep\">[Asad etÂ al., <a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">2020</a>]</cite>, FedProxÂ <cite class=\"ltx_cite ltx_citemacro_cite\">Li etÂ al. [<a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">2020</a>]</cite>, and MimeÂ <cite class=\"ltx_cite ltx_citemacro_citep\">[Karimireddy etÂ al., <a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">2020a</a>]</cite>.\nWe find that when the FL model achieves higher accuracy than standalone models, the fairness gap of the FL model can be higher than that of standalone models. This observation is consistent across multiple FL algorithms. This strong evidence implies that the improvement of accuracy in FL can come at the cost of fairness. In addition, this is not unique to only the FedAvg algorithm.</p>\n</div>\n<figure id=\"A2.T2\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span><span id=\"A2.T2.6.1\" class=\"ltx_text ltx_font_bold\">Benefit of collaboration on local datasets - Various FL algorithms (Income).</span> The standard deviation across five runs is indicated between parentheses.</figcaption>\n<div id=\"A2.T2.4\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:433.6pt;height:42.2pt;vertical-align:-32.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(42.2pt,-0.9pt) scale(1.2415726193884,1.2415726193884) ;\"><span id=\"A2.T2.4.5\" class=\"ltx_ERROR undefined\">\\csvreader</span>\n<p id=\"A2.T2.4.4\" class=\"ltx_p\">[tabular=ca*2e*2d,table head=    <span id=\"A2.T2.4.4.4\" class=\"ltx_text\" style=\"background-color:#F6FCF2;\"> Race     <span id=\"A2.T2.4.4.4.4\" class=\"ltx_text\" style=\"background-color:#F5FAFC;\">Sex  \n<br class=\"ltx_break\">Algorithm  Accuracy  <math id=\"A2.T2.1.1.1.1.m1.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{EO}\" display=\"inline\"><semantics id=\"A2.T2.1.1.1.1.m1.1a\"><msup id=\"A2.T2.1.1.1.1.m1.1.1\" xref=\"A2.T2.1.1.1.1.m1.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A2.T2.1.1.1.1.m1.1.1.2\" xref=\"A2.T2.1.1.1.1.m1.1.1.2.cmml\">Î”</mi><mrow id=\"A2.T2.1.1.1.1.m1.1.1.3\" xref=\"A2.T2.1.1.1.1.m1.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.1.1.1.1.m1.1.1.3.2\" xref=\"A2.T2.1.1.1.1.m1.1.1.3.2.cmml\">E</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A2.T2.1.1.1.1.m1.1.1.3.1\" xref=\"A2.T2.1.1.1.1.m1.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.1.1.1.1.m1.1.1.3.3\" xref=\"A2.T2.1.1.1.1.m1.1.1.3.3.cmml\">O</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A2.T2.1.1.1.1.m1.1b\"><apply id=\"A2.T2.1.1.1.1.m1.1.1.cmml\" xref=\"A2.T2.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.T2.1.1.1.1.m1.1.1.1.cmml\" xref=\"A2.T2.1.1.1.1.m1.1.1\">superscript</csymbol><ci id=\"A2.T2.1.1.1.1.m1.1.1.2.cmml\" xref=\"A2.T2.1.1.1.1.m1.1.1.2\">Î”</ci><apply id=\"A2.T2.1.1.1.1.m1.1.1.3.cmml\" xref=\"A2.T2.1.1.1.1.m1.1.1.3\"><times id=\"A2.T2.1.1.1.1.m1.1.1.3.1.cmml\" xref=\"A2.T2.1.1.1.1.m1.1.1.3.1\"></times><ci id=\"A2.T2.1.1.1.1.m1.1.1.3.2.cmml\" xref=\"A2.T2.1.1.1.1.m1.1.1.3.2\">ğ¸</ci><ci id=\"A2.T2.1.1.1.1.m1.1.1.3.3.cmml\" xref=\"A2.T2.1.1.1.1.m1.1.1.3.3\">ğ‘‚</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T2.1.1.1.1.m1.1c\">\\Delta^{EO}</annotation></semantics></math>  <math id=\"A2.T2.2.2.2.2.m2.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{DP}\" display=\"inline\"><semantics id=\"A2.T2.2.2.2.2.m2.1a\"><msup id=\"A2.T2.2.2.2.2.m2.1.1\" xref=\"A2.T2.2.2.2.2.m2.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A2.T2.2.2.2.2.m2.1.1.2\" xref=\"A2.T2.2.2.2.2.m2.1.1.2.cmml\">Î”</mi><mrow id=\"A2.T2.2.2.2.2.m2.1.1.3\" xref=\"A2.T2.2.2.2.2.m2.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.2.2.2.2.m2.1.1.3.2\" xref=\"A2.T2.2.2.2.2.m2.1.1.3.2.cmml\">D</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A2.T2.2.2.2.2.m2.1.1.3.1\" xref=\"A2.T2.2.2.2.2.m2.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.2.2.2.2.m2.1.1.3.3\" xref=\"A2.T2.2.2.2.2.m2.1.1.3.3.cmml\">P</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A2.T2.2.2.2.2.m2.1b\"><apply id=\"A2.T2.2.2.2.2.m2.1.1.cmml\" xref=\"A2.T2.2.2.2.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.T2.2.2.2.2.m2.1.1.1.cmml\" xref=\"A2.T2.2.2.2.2.m2.1.1\">superscript</csymbol><ci id=\"A2.T2.2.2.2.2.m2.1.1.2.cmml\" xref=\"A2.T2.2.2.2.2.m2.1.1.2\">Î”</ci><apply id=\"A2.T2.2.2.2.2.m2.1.1.3.cmml\" xref=\"A2.T2.2.2.2.2.m2.1.1.3\"><times id=\"A2.T2.2.2.2.2.m2.1.1.3.1.cmml\" xref=\"A2.T2.2.2.2.2.m2.1.1.3.1\"></times><ci id=\"A2.T2.2.2.2.2.m2.1.1.3.2.cmml\" xref=\"A2.T2.2.2.2.2.m2.1.1.3.2\">ğ·</ci><ci id=\"A2.T2.2.2.2.2.m2.1.1.3.3.cmml\" xref=\"A2.T2.2.2.2.2.m2.1.1.3.3\">ğ‘ƒ</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T2.2.2.2.2.m2.1c\">\\Delta^{DP}</annotation></semantics></math> <math id=\"A2.T2.3.3.3.3.m3.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{EO}\" display=\"inline\"><semantics id=\"A2.T2.3.3.3.3.m3.1a\"><msup id=\"A2.T2.3.3.3.3.m3.1.1\" xref=\"A2.T2.3.3.3.3.m3.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A2.T2.3.3.3.3.m3.1.1.2\" xref=\"A2.T2.3.3.3.3.m3.1.1.2.cmml\">Î”</mi><mrow id=\"A2.T2.3.3.3.3.m3.1.1.3\" xref=\"A2.T2.3.3.3.3.m3.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.3.3.3.3.m3.1.1.3.2\" xref=\"A2.T2.3.3.3.3.m3.1.1.3.2.cmml\">E</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A2.T2.3.3.3.3.m3.1.1.3.1\" xref=\"A2.T2.3.3.3.3.m3.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.3.3.3.3.m3.1.1.3.3\" xref=\"A2.T2.3.3.3.3.m3.1.1.3.3.cmml\">O</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A2.T2.3.3.3.3.m3.1b\"><apply id=\"A2.T2.3.3.3.3.m3.1.1.cmml\" xref=\"A2.T2.3.3.3.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.T2.3.3.3.3.m3.1.1.1.cmml\" xref=\"A2.T2.3.3.3.3.m3.1.1\">superscript</csymbol><ci id=\"A2.T2.3.3.3.3.m3.1.1.2.cmml\" xref=\"A2.T2.3.3.3.3.m3.1.1.2\">Î”</ci><apply id=\"A2.T2.3.3.3.3.m3.1.1.3.cmml\" xref=\"A2.T2.3.3.3.3.m3.1.1.3\"><times id=\"A2.T2.3.3.3.3.m3.1.1.3.1.cmml\" xref=\"A2.T2.3.3.3.3.m3.1.1.3.1\"></times><ci id=\"A2.T2.3.3.3.3.m3.1.1.3.2.cmml\" xref=\"A2.T2.3.3.3.3.m3.1.1.3.2\">ğ¸</ci><ci id=\"A2.T2.3.3.3.3.m3.1.1.3.3.cmml\" xref=\"A2.T2.3.3.3.3.m3.1.1.3.3\">ğ‘‚</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T2.3.3.3.3.m3.1c\">\\Delta^{EO}</annotation></semantics></math>  <math id=\"A2.T2.4.4.4.4.m4.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{DP}\" display=\"inline\"><semantics id=\"A2.T2.4.4.4.4.m4.1a\"><msup id=\"A2.T2.4.4.4.4.m4.1.1\" xref=\"A2.T2.4.4.4.4.m4.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A2.T2.4.4.4.4.m4.1.1.2\" xref=\"A2.T2.4.4.4.4.m4.1.1.2.cmml\">Î”</mi><mrow id=\"A2.T2.4.4.4.4.m4.1.1.3\" xref=\"A2.T2.4.4.4.4.m4.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.4.4.4.4.m4.1.1.3.2\" xref=\"A2.T2.4.4.4.4.m4.1.1.3.2.cmml\">D</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A2.T2.4.4.4.4.m4.1.1.3.1\" xref=\"A2.T2.4.4.4.4.m4.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.4.4.4.4.m4.1.1.3.3\" xref=\"A2.T2.4.4.4.4.m4.1.1.3.3.cmml\">P</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A2.T2.4.4.4.4.m4.1b\"><apply id=\"A2.T2.4.4.4.4.m4.1.1.cmml\" xref=\"A2.T2.4.4.4.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.T2.4.4.4.4.m4.1.1.1.cmml\" xref=\"A2.T2.4.4.4.4.m4.1.1\">superscript</csymbol><ci id=\"A2.T2.4.4.4.4.m4.1.1.2.cmml\" xref=\"A2.T2.4.4.4.4.m4.1.1.2\">Î”</ci><apply id=\"A2.T2.4.4.4.4.m4.1.1.3.cmml\" xref=\"A2.T2.4.4.4.4.m4.1.1.3\"><times id=\"A2.T2.4.4.4.4.m4.1.1.3.1.cmml\" xref=\"A2.T2.4.4.4.4.m4.1.1.3.1\"></times><ci id=\"A2.T2.4.4.4.4.m4.1.1.3.2.cmml\" xref=\"A2.T2.4.4.4.4.m4.1.1.3.2\">ğ·</ci><ci id=\"A2.T2.4.4.4.4.m4.1.1.3.3.cmml\" xref=\"A2.T2.4.4.4.4.m4.1.1.3.3\">ğ‘ƒ</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T2.4.4.4.4.m4.1c\">\\Delta^{DP}</annotation></semantics></math> \n<br class=\"ltx_break\">, table foot=]data/data_other_fl.csv\n <span id=\"A2.T2.4.4.4.4.1\" class=\"ltx_ERROR undefined\">\\csvcoli</span> <span id=\"A2.T2.4.4.4.4.2\" class=\"ltx_ERROR undefined\">\\csvcolii</span>Â (<span id=\"A2.T2.4.4.4.4.3\" class=\"ltx_ERROR undefined\">\\csvcoliii</span>)  <span id=\"A2.T2.4.4.4.4.4\" class=\"ltx_ERROR undefined\">\\csvcoliv</span>Â (<span id=\"A2.T2.4.4.4.4.5\" class=\"ltx_ERROR undefined\">\\csvcolv</span>)  <span id=\"A2.T2.4.4.4.4.6\" class=\"ltx_ERROR undefined\">\\csvcolvi</span>Â (<span id=\"A2.T2.4.4.4.4.7\" class=\"ltx_ERROR undefined\">\\csvcolvii</span>)  <span id=\"A2.T2.4.4.4.4.8\" class=\"ltx_ERROR undefined\">\\csvcolviii</span>Â (<span id=\"A2.T2.4.4.4.4.9\" class=\"ltx_ERROR undefined\">\\csvcolix</span>)  <span id=\"A2.T2.4.4.4.4.10\" class=\"ltx_ERROR undefined\">\\csvcolx</span>Â (<span id=\"A2.T2.4.4.4.4.11\" class=\"ltx_ERROR undefined\">\\csvcolxi</span>)</span></span></p>\n</span></div>\n</figure>\n</section>\n<section id=\"A2.SS6\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.6 </span>Group Performance</h3>\n\n<div id=\"A2.SS6.p1\" class=\"ltx_para\">\n<p id=\"A2.SS6.p1.1\" class=\"ltx_p\">In FigureÂ <a href=\"#A2.F21\" title=\"Figure 21 â€£ B.6 Group Performance â€£ Appendix B Additional Experimental Results â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">21</span></a>, we present the model ROC on groups to illustrate the performance disparity across groups. Surprisingly, there is no huge difference between the ROC between groups. In contrast, FigureÂ <a href=\"#A2.F22\" title=\"Figure 22 â€£ B.6 Group Performance â€£ Appendix B Additional Experimental Results â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">22</span></a> shows the noticeable difference between groups with respect to the precision-recall curve, especially for the most biased party. The main reason is that the dataset is imbalanced. Thus, ROC is usually misleading. On the precision-recall curve, we find that the model achieves a higher precision on the majority (i.e., Male group) compared to the minority group (i.e., Female group).</p>\n</div>\n<figure id=\"A2.F21\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><img src=\"/html/2309.02160/assets/images/income/group_roc/roc_attr_1_43.png\" id=\"A2.F21.g1\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" width=\"598\" height=\"190\" alt=\"Refer to caption\"></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><img src=\"/html/2309.02160/assets/images/income/group_roc/roc_attr_1_50.png\" id=\"A2.F21.g2\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" width=\"598\" height=\"190\" alt=\"Refer to caption\"></div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 21: </span>Receiver operating characteristic (ROC) of different models on groups - (Income, Sex)</figcaption>\n</figure>\n<figure id=\"A2.F22\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><img src=\"/html/2309.02160/assets/images/income/group_roc/ppc_attr_1_43.png\" id=\"A2.F22.g1\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" width=\"598\" height=\"189\" alt=\"Refer to caption\"></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><img src=\"/html/2309.02160/assets/images/income/group_roc/ppc_attr_1_50.png\" id=\"A2.F22.g2\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" width=\"598\" height=\"189\" alt=\"Refer to caption\"></div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 22: </span>Precision-Recall curve (PRC) of different models on groups - (Income, Sex)</figcaption>\n</figure>\n</section>\n<section id=\"A2.SS7\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.7 </span>Effect of scaling other parameters</h3>\n\n<div id=\"A2.SS7.p1\" class=\"ltx_para\">\n<p id=\"A2.SS7.p1.1\" class=\"ltx_p\">FigureÂ <a href=\"#A2.F23\" title=\"Figure 23 â€£ B.7 Effect of scaling other parameters â€£ Appendix B Additional Experimental Results â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">23</span></a> shows a comparison of the modelâ€™s performance when scaling different model parameters. Specifically, we examine the impact of scaling the parameters in the first layer of the neural network that does not have any computation on the Sex attribute (referred to as \"Other parameters\"), as well as the parameter related to the Sex attribute (referred to as \"Related parameters\"). The results indicate that up-scaling or down-scaling other parameters do not significantly affect the fairness gap while scaling the related parameters has a considerable impact on model fairness. This finding supports our hypothesis that the modelâ€™s bias is primarily encoded in a few model parameters. In FL, biased parties introduce bias during local training by increasing the weights for those related parameters.</p>\n</div>\n<figure id=\"A2.F23\" class=\"ltx_figure\"><img src=\"/html/2309.02160/assets/images/income/effect_parameter/effect_scaling_comparision_199.png\" id=\"A2.F23.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"169\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 23: </span>Effect of scaling model parameters - (Income, Sex)</figcaption>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>\n</section>\n<section id=\"A3\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix C </span>Evaluation on existing Fair FL</h2>\n\n<div id=\"A3.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"A3.p1.1\" class=\"ltx_p\">TableÂ <a href=\"#A3.T3\" title=\"Table 3 â€£ Appendix C Evaluation on existing Fair FL â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the results for a fair FL algorithm proposed by <cite class=\"ltx_cite ltx_citemacro_cite\">Abay etÂ al. [<a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2020</a>]</cite>, which uses a reweighting algorithm proposed in the centralized settingÂ <cite class=\"ltx_cite ltx_citemacro_citep\">[Kamiran and Calders, <a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">2012</a>]</cite>. The algorithm involves assigning weights to each subgroup (defined by the label and a sensitive attribute) before FL, based on the local or global training dataset. This is called \"local reweighting\" or \"global reweighting,\" respectively. During FL, parties update the FL model to minimize the weighted loss. The table presents the accuracy and fairness gap of the FL algorithm using local and global reweighting for the Income, Health, and Employment datasets.</p>\n</div>\n<figure id=\"A3.T3\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span><span id=\"A3.T3.6.1\" class=\"ltx_text ltx_font_bold\">Effect of Fair FLÂ <cite class=\"ltx_cite ltx_citemacro_citep\">[Abay etÂ al., <a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2020</a>]</cite> - (Income).</span> The standard deviation across five runs is indicated between parentheses. The \"Sensitive Attribute\" column indicates the sensitive attribute used in the reweighting algorithm.</figcaption>\n<div id=\"A3.T3.4\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:433.6pt;height:40.8pt;vertical-align:-31.8pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(36.1pt,-0.7pt) scale(1.19959219035754,1.19959219035754) ;\"><span id=\"A3.T3.4.5\" class=\"ltx_ERROR undefined\">\\csvreader</span>\n<p id=\"A3.T3.4.4\" class=\"ltx_p\">[tabular=cca*2e*2d,table head=     <span id=\"A3.T3.4.4.4\" class=\"ltx_text\" style=\"background-color:#F6FCF2;\"> Race     <span id=\"A3.T3.4.4.4.4\" class=\"ltx_text\" style=\"background-color:#F5FAFC;\">Sex  \n<br class=\"ltx_break\">Algorithm  Sensitive Attribute  Accuracy  <math id=\"A3.T3.1.1.1.1.m1.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{EO}\" display=\"inline\"><semantics id=\"A3.T3.1.1.1.1.m1.1a\"><msup id=\"A3.T3.1.1.1.1.m1.1.1\" xref=\"A3.T3.1.1.1.1.m1.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A3.T3.1.1.1.1.m1.1.1.2\" xref=\"A3.T3.1.1.1.1.m1.1.1.2.cmml\">Î”</mi><mrow id=\"A3.T3.1.1.1.1.m1.1.1.3\" xref=\"A3.T3.1.1.1.1.m1.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.1.1.1.1.m1.1.1.3.2\" xref=\"A3.T3.1.1.1.1.m1.1.1.3.2.cmml\">E</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A3.T3.1.1.1.1.m1.1.1.3.1\" xref=\"A3.T3.1.1.1.1.m1.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.1.1.1.1.m1.1.1.3.3\" xref=\"A3.T3.1.1.1.1.m1.1.1.3.3.cmml\">O</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A3.T3.1.1.1.1.m1.1b\"><apply id=\"A3.T3.1.1.1.1.m1.1.1.cmml\" xref=\"A3.T3.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T3.1.1.1.1.m1.1.1.1.cmml\" xref=\"A3.T3.1.1.1.1.m1.1.1\">superscript</csymbol><ci id=\"A3.T3.1.1.1.1.m1.1.1.2.cmml\" xref=\"A3.T3.1.1.1.1.m1.1.1.2\">Î”</ci><apply id=\"A3.T3.1.1.1.1.m1.1.1.3.cmml\" xref=\"A3.T3.1.1.1.1.m1.1.1.3\"><times id=\"A3.T3.1.1.1.1.m1.1.1.3.1.cmml\" xref=\"A3.T3.1.1.1.1.m1.1.1.3.1\"></times><ci id=\"A3.T3.1.1.1.1.m1.1.1.3.2.cmml\" xref=\"A3.T3.1.1.1.1.m1.1.1.3.2\">ğ¸</ci><ci id=\"A3.T3.1.1.1.1.m1.1.1.3.3.cmml\" xref=\"A3.T3.1.1.1.1.m1.1.1.3.3\">ğ‘‚</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T3.1.1.1.1.m1.1c\">\\Delta^{EO}</annotation></semantics></math>  <math id=\"A3.T3.2.2.2.2.m2.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{DP}\" display=\"inline\"><semantics id=\"A3.T3.2.2.2.2.m2.1a\"><msup id=\"A3.T3.2.2.2.2.m2.1.1\" xref=\"A3.T3.2.2.2.2.m2.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A3.T3.2.2.2.2.m2.1.1.2\" xref=\"A3.T3.2.2.2.2.m2.1.1.2.cmml\">Î”</mi><mrow id=\"A3.T3.2.2.2.2.m2.1.1.3\" xref=\"A3.T3.2.2.2.2.m2.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.2.2.2.2.m2.1.1.3.2\" xref=\"A3.T3.2.2.2.2.m2.1.1.3.2.cmml\">D</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A3.T3.2.2.2.2.m2.1.1.3.1\" xref=\"A3.T3.2.2.2.2.m2.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.2.2.2.2.m2.1.1.3.3\" xref=\"A3.T3.2.2.2.2.m2.1.1.3.3.cmml\">P</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A3.T3.2.2.2.2.m2.1b\"><apply id=\"A3.T3.2.2.2.2.m2.1.1.cmml\" xref=\"A3.T3.2.2.2.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T3.2.2.2.2.m2.1.1.1.cmml\" xref=\"A3.T3.2.2.2.2.m2.1.1\">superscript</csymbol><ci id=\"A3.T3.2.2.2.2.m2.1.1.2.cmml\" xref=\"A3.T3.2.2.2.2.m2.1.1.2\">Î”</ci><apply id=\"A3.T3.2.2.2.2.m2.1.1.3.cmml\" xref=\"A3.T3.2.2.2.2.m2.1.1.3\"><times id=\"A3.T3.2.2.2.2.m2.1.1.3.1.cmml\" xref=\"A3.T3.2.2.2.2.m2.1.1.3.1\"></times><ci id=\"A3.T3.2.2.2.2.m2.1.1.3.2.cmml\" xref=\"A3.T3.2.2.2.2.m2.1.1.3.2\">ğ·</ci><ci id=\"A3.T3.2.2.2.2.m2.1.1.3.3.cmml\" xref=\"A3.T3.2.2.2.2.m2.1.1.3.3\">ğ‘ƒ</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T3.2.2.2.2.m2.1c\">\\Delta^{DP}</annotation></semantics></math> <math id=\"A3.T3.3.3.3.3.m3.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{EO}\" display=\"inline\"><semantics id=\"A3.T3.3.3.3.3.m3.1a\"><msup id=\"A3.T3.3.3.3.3.m3.1.1\" xref=\"A3.T3.3.3.3.3.m3.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A3.T3.3.3.3.3.m3.1.1.2\" xref=\"A3.T3.3.3.3.3.m3.1.1.2.cmml\">Î”</mi><mrow id=\"A3.T3.3.3.3.3.m3.1.1.3\" xref=\"A3.T3.3.3.3.3.m3.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.3.3.3.3.m3.1.1.3.2\" xref=\"A3.T3.3.3.3.3.m3.1.1.3.2.cmml\">E</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A3.T3.3.3.3.3.m3.1.1.3.1\" xref=\"A3.T3.3.3.3.3.m3.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.3.3.3.3.m3.1.1.3.3\" xref=\"A3.T3.3.3.3.3.m3.1.1.3.3.cmml\">O</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A3.T3.3.3.3.3.m3.1b\"><apply id=\"A3.T3.3.3.3.3.m3.1.1.cmml\" xref=\"A3.T3.3.3.3.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T3.3.3.3.3.m3.1.1.1.cmml\" xref=\"A3.T3.3.3.3.3.m3.1.1\">superscript</csymbol><ci id=\"A3.T3.3.3.3.3.m3.1.1.2.cmml\" xref=\"A3.T3.3.3.3.3.m3.1.1.2\">Î”</ci><apply id=\"A3.T3.3.3.3.3.m3.1.1.3.cmml\" xref=\"A3.T3.3.3.3.3.m3.1.1.3\"><times id=\"A3.T3.3.3.3.3.m3.1.1.3.1.cmml\" xref=\"A3.T3.3.3.3.3.m3.1.1.3.1\"></times><ci id=\"A3.T3.3.3.3.3.m3.1.1.3.2.cmml\" xref=\"A3.T3.3.3.3.3.m3.1.1.3.2\">ğ¸</ci><ci id=\"A3.T3.3.3.3.3.m3.1.1.3.3.cmml\" xref=\"A3.T3.3.3.3.3.m3.1.1.3.3\">ğ‘‚</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T3.3.3.3.3.m3.1c\">\\Delta^{EO}</annotation></semantics></math>  <math id=\"A3.T3.4.4.4.4.m4.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{DP}\" display=\"inline\"><semantics id=\"A3.T3.4.4.4.4.m4.1a\"><msup id=\"A3.T3.4.4.4.4.m4.1.1\" xref=\"A3.T3.4.4.4.4.m4.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A3.T3.4.4.4.4.m4.1.1.2\" xref=\"A3.T3.4.4.4.4.m4.1.1.2.cmml\">Î”</mi><mrow id=\"A3.T3.4.4.4.4.m4.1.1.3\" xref=\"A3.T3.4.4.4.4.m4.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.4.4.4.4.m4.1.1.3.2\" xref=\"A3.T3.4.4.4.4.m4.1.1.3.2.cmml\">D</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A3.T3.4.4.4.4.m4.1.1.3.1\" xref=\"A3.T3.4.4.4.4.m4.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.4.4.4.4.m4.1.1.3.3\" xref=\"A3.T3.4.4.4.4.m4.1.1.3.3.cmml\">P</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A3.T3.4.4.4.4.m4.1b\"><apply id=\"A3.T3.4.4.4.4.m4.1.1.cmml\" xref=\"A3.T3.4.4.4.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T3.4.4.4.4.m4.1.1.1.cmml\" xref=\"A3.T3.4.4.4.4.m4.1.1\">superscript</csymbol><ci id=\"A3.T3.4.4.4.4.m4.1.1.2.cmml\" xref=\"A3.T3.4.4.4.4.m4.1.1.2\">Î”</ci><apply id=\"A3.T3.4.4.4.4.m4.1.1.3.cmml\" xref=\"A3.T3.4.4.4.4.m4.1.1.3\"><times id=\"A3.T3.4.4.4.4.m4.1.1.3.1.cmml\" xref=\"A3.T3.4.4.4.4.m4.1.1.3.1\"></times><ci id=\"A3.T3.4.4.4.4.m4.1.1.3.2.cmml\" xref=\"A3.T3.4.4.4.4.m4.1.1.3.2\">ğ·</ci><ci id=\"A3.T3.4.4.4.4.m4.1.1.3.3.cmml\" xref=\"A3.T3.4.4.4.4.m4.1.1.3.3\">ğ‘ƒ</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T3.4.4.4.4.m4.1c\">\\Delta^{DP}</annotation></semantics></math> \n<br class=\"ltx_break\">, table foot=]data/data_reweiging.csv\n <span id=\"A3.T3.4.4.4.4.1\" class=\"ltx_ERROR undefined\">\\csvcoli</span> <span id=\"A3.T3.4.4.4.4.2\" class=\"ltx_ERROR undefined\">\\csvcolxii</span><span id=\"A3.T3.4.4.4.4.3\" class=\"ltx_ERROR undefined\">\\csvcolii</span>Â (<span id=\"A3.T3.4.4.4.4.4\" class=\"ltx_ERROR undefined\">\\csvcoliii</span>)  <span id=\"A3.T3.4.4.4.4.5\" class=\"ltx_ERROR undefined\">\\csvcoliv</span>Â (<span id=\"A3.T3.4.4.4.4.6\" class=\"ltx_ERROR undefined\">\\csvcolv</span>)  <span id=\"A3.T3.4.4.4.4.7\" class=\"ltx_ERROR undefined\">\\csvcolvi</span>Â (<span id=\"A3.T3.4.4.4.4.8\" class=\"ltx_ERROR undefined\">\\csvcolvii</span>)  <span id=\"A3.T3.4.4.4.4.9\" class=\"ltx_ERROR undefined\">\\csvcolviii</span>Â (<span id=\"A3.T3.4.4.4.4.10\" class=\"ltx_ERROR undefined\">\\csvcolix</span>)  <span id=\"A3.T3.4.4.4.4.11\" class=\"ltx_ERROR undefined\">\\csvcolx</span>Â (<span id=\"A3.T3.4.4.4.4.12\" class=\"ltx_ERROR undefined\">\\csvcolxi</span>)</span></span></p>\n</span></div>\n</figure>\n<div id=\"A3.p2\" class=\"ltx_para ltx_noindent\">\n<p id=\"A3.p2.1\" class=\"ltx_p\"><span id=\"A3.p2.1.1\" class=\"ltx_text\">We found that applying reweighing algorithms in FL reduces the average fairness gap across parties. This is due to the fact that, after reweighing, the parties that were initially biased do not introduce significant bias to the model during local training, resulting in a reduction of the fairness gap in the FL model.</span></p>\n</div>\n<div id=\"A3.p3\" class=\"ltx_para\">\n<p id=\"A3.p3.1\" class=\"ltx_p\">However, we also noted that the fairness gap in the model remains high for the Race sensitive attribute, indicating that the global and local reweighing algorithms do not entirely eliminate bias in FL. Furthermore, we would like to highlight some concerns regarding the fair FL algorithm:</p>\n<ol id=\"A3.I1\" class=\"ltx_enumerate\">\n<li id=\"A3.I1.i1\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">1.</span> \n<div id=\"A3.I1.i1.p1\" class=\"ltx_para\">\n<p id=\"A3.I1.i1.p1.1\" class=\"ltx_p\">The fair FL algorithm enhances the performance of the minority group at the expense of the majority group. FigureÂ <a href=\"#A3.F24\" title=\"Figure 24 â€£ Appendix C Evaluation on existing Fair FL â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">24</span></a> illustrates that the minority group (i.e., the Female group) has a better performance after the reweighing, but at the cost of reduced performance for the majority.</p>\n</div>\n</li>\n<li id=\"A3.I1.i2\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">2.</span> \n<div id=\"A3.I1.i2.p1\" class=\"ltx_para\">\n<p id=\"A3.I1.i2.p1.1\" class=\"ltx_p\">When improving fairness with respect to one sensitive attribute, the fairness issue with respect to another sensitive attribute may worsen, as shown inÂ TableÂ <a href=\"#A3.T3\" title=\"Table 3 â€£ Appendix C Evaluation on existing Fair FL â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. For instance, when applying local reweighing to reduce bias with respect to the \"Sex\" attribute, the fairness gap with respect to \"Race\" increased from 0.514 (results on FedAvg) to 0.521, indicating that reweighing to improve fairness with respect to Sex may amplify the bias with respect to Race. This implies that the bias with respect to Race can still propagate in FL, and our analysis remains valid.</p>\n</div>\n</li>\n<li id=\"A3.I1.i3\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">3.</span> \n<div id=\"A3.I1.i3.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"A3.I1.i3.p1.1\" class=\"ltx_p\">The reweighing algorithms assume that parties are interested in mitigating the bias with respect to the same sensitive attribute, which may not be the case in practice. FigureÂ <a href=\"#A3.F25\" title=\"Figure 25 â€£ Appendix C Evaluation on existing Fair FL â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">25</span></a> shows the fairness gap of standalone models with respect to different sensitive attributes. We observe that many parties have a low fairness gap with respect to one sensitive attribute and a high fairness gap with respect to another sensitive attribute. Thus, parties may aim to enhance fairness with respect to different sensitive attributes, which renders this reweighing algorithm unsuitable. This problem is more prevalent in FL, given the different data distributions of parties.</p>\n</div>\n</li>\n</ol>\n</div>\n<figure id=\"A3.F24\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\"><img src=\"/html/2309.02160/assets/images/income/reweiging/income_group_PPV_global_reweighing_fedavg.png\" id=\"A3.F24.g1\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" width=\"287\" height=\"216\" alt=\"Refer to caption\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\"><img src=\"/html/2309.02160/assets/images/income/reweiging/income_group_TPR_global_reweighing_fedavg.png\" id=\"A3.F24.g2\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" width=\"287\" height=\"216\" alt=\"Refer to caption\"></div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 24: </span>Effect of Fair FL algorithmÂ <cite class=\"ltx_cite ltx_citemacro_citep\">[Abay etÂ al., <a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2020</a>]</cite> on group performance - (Income, Sex)</figcaption>\n</figure>\n<figure id=\"A3.F25\" class=\"ltx_figure\"><img src=\"/html/2309.02160/assets/images/income/reweiging/income_standalone_race_sex_gap.png\" id=\"A3.F25.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"479\" height=\"180\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 25: </span>Fairness gap of standalone models with respect to different sensitive attributes - (Income). Each point represents the result for a single party in one run. We show the fairness gap with respect to different sensitive attributes differs a lot for each party.</figcaption>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>\n<section id=\"A4\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix D </span>Extending to a real-world medical dataset</h2>\n\n<figure id=\"A4.T4\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span><span id=\"A4.T4.6.1\" class=\"ltx_text ltx_font_bold\">Effect of FL on the real-world medical dataset - (ISIC2019).</span> The standard deviation across four runs is indicated between parentheses. </figcaption>\n<div id=\"A4.T4.4\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:433.6pt;height:62.9pt;vertical-align:-54.8pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(16.9pt,-0.3pt) scale(1.0845000976968,1.0845000976968) ;\"><span id=\"A4.T4.4.5\" class=\"ltx_ERROR undefined\">\\csvreader</span>\n<p id=\"A4.T4.4.4\" class=\"ltx_p\">[tabular=c*2a*2e*2d*2f,table head= Setting   <span id=\"A4.T4.4.4.4\" class=\"ltx_text\" style=\"background-color:#FFF3F3;\"> Party 1    <span id=\"A4.T4.4.4.4.4\" class=\"ltx_text\" style=\"background-color:#F6FCF2;\"> Party 2     <span id=\"A4.T4.4.4.4.4.4\" class=\"ltx_text\" style=\"background-color:#F5FAFC;\"> Party 3     <span id=\"A4.T4.4.4.4.4.4.4\" class=\"ltx_text\" style=\"background-color:#FFF9F8;\"> Party 4 \n<br class=\"ltx_break\">Algorithm  Accuracy  <math id=\"A4.T4.1.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\Delta^{Acc}\" display=\"inline\"><semantics id=\"A4.T4.1.1.1.1.1.1.m1.1a\"><msup id=\"A4.T4.1.1.1.1.1.1.m1.1.1\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.cmml\"><mi mathbackground=\"#FFF9F8\" mathvariant=\"normal\" id=\"A4.T4.1.1.1.1.1.1.m1.1.1.2\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.2.cmml\">Î”</mi><mrow id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.cmml\"><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.2\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.1\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.3\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.3.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.1a\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.4\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.4.cmml\">c</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A4.T4.1.1.1.1.1.1.m1.1b\"><apply id=\"A4.T4.1.1.1.1.1.1.m1.1.1.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A4.T4.1.1.1.1.1.1.m1.1.1.1.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1\">superscript</csymbol><ci id=\"A4.T4.1.1.1.1.1.1.m1.1.1.2.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.2\">Î”</ci><apply id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3\"><times id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.1.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.1\"></times><ci id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.2.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.2\">ğ´</ci><ci id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.3.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.3\">ğ‘</ci><ci id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.4.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.4\">ğ‘</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T4.1.1.1.1.1.1.m1.1c\">\\Delta^{Acc}</annotation></semantics></math>  Accuracy  <math id=\"A4.T4.2.2.2.2.2.2.m2.1\" class=\"ltx_Math\" alttext=\"\\Delta^{Acc}\" display=\"inline\"><semantics id=\"A4.T4.2.2.2.2.2.2.m2.1a\"><msup id=\"A4.T4.2.2.2.2.2.2.m2.1.1\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.cmml\"><mi mathbackground=\"#FFF9F8\" mathvariant=\"normal\" id=\"A4.T4.2.2.2.2.2.2.m2.1.1.2\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.2.cmml\">Î”</mi><mrow id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.cmml\"><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.2\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.1\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.3\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.3.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.1a\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.4\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.4.cmml\">c</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A4.T4.2.2.2.2.2.2.m2.1b\"><apply id=\"A4.T4.2.2.2.2.2.2.m2.1.1.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"A4.T4.2.2.2.2.2.2.m2.1.1.1.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1\">superscript</csymbol><ci id=\"A4.T4.2.2.2.2.2.2.m2.1.1.2.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.2\">Î”</ci><apply id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3\"><times id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.1.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.1\"></times><ci id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.2.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.2\">ğ´</ci><ci id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.3.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.3\">ğ‘</ci><ci id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.4.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.4\">ğ‘</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T4.2.2.2.2.2.2.m2.1c\">\\Delta^{Acc}</annotation></semantics></math>  Accuracy  <math id=\"A4.T4.3.3.3.3.3.3.m3.1\" class=\"ltx_Math\" alttext=\"\\Delta^{Acc}\" display=\"inline\"><semantics id=\"A4.T4.3.3.3.3.3.3.m3.1a\"><msup id=\"A4.T4.3.3.3.3.3.3.m3.1.1\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.cmml\"><mi mathbackground=\"#FFF9F8\" mathvariant=\"normal\" id=\"A4.T4.3.3.3.3.3.3.m3.1.1.2\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.2.cmml\">Î”</mi><mrow id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.cmml\"><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.2\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.1\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.3\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.3.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.1a\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.4\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.4.cmml\">c</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A4.T4.3.3.3.3.3.3.m3.1b\"><apply id=\"A4.T4.3.3.3.3.3.3.m3.1.1.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"A4.T4.3.3.3.3.3.3.m3.1.1.1.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1\">superscript</csymbol><ci id=\"A4.T4.3.3.3.3.3.3.m3.1.1.2.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.2\">Î”</ci><apply id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3\"><times id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.1.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.1\"></times><ci id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.2.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.2\">ğ´</ci><ci id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.3.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.3\">ğ‘</ci><ci id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.4.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.4\">ğ‘</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T4.3.3.3.3.3.3.m3.1c\">\\Delta^{Acc}</annotation></semantics></math>  Accuracy  <math id=\"A4.T4.4.4.4.4.4.4.m4.1\" class=\"ltx_Math\" alttext=\"\\Delta^{Acc}\" display=\"inline\"><semantics id=\"A4.T4.4.4.4.4.4.4.m4.1a\"><msup id=\"A4.T4.4.4.4.4.4.4.m4.1.1\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.cmml\"><mi mathbackground=\"#FFF9F8\" mathvariant=\"normal\" id=\"A4.T4.4.4.4.4.4.4.m4.1.1.2\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.2.cmml\">Î”</mi><mrow id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.cmml\"><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.2\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.1\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.3\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.3.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.1a\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.4\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.4.cmml\">c</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A4.T4.4.4.4.4.4.4.m4.1b\"><apply id=\"A4.T4.4.4.4.4.4.4.m4.1.1.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"A4.T4.4.4.4.4.4.4.m4.1.1.1.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1\">superscript</csymbol><ci id=\"A4.T4.4.4.4.4.4.4.m4.1.1.2.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.2\">Î”</ci><apply id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3\"><times id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.1.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.1\"></times><ci id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.2.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.2\">ğ´</ci><ci id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.3.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.3\">ğ‘</ci><ci id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.4.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.4\">ğ‘</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T4.4.4.4.4.4.4.m4.1c\">\\Delta^{Acc}</annotation></semantics></math> \n<br class=\"ltx_break\">, table foot=]data/isic2019.csv\n <span id=\"A4.T4.4.4.4.4.4.4.1\" class=\"ltx_ERROR undefined\">\\csvcolii</span> <span id=\"A4.T4.4.4.4.4.4.4.2\" class=\"ltx_ERROR undefined\">\\csvcoliii</span>Â (<span id=\"A4.T4.4.4.4.4.4.4.3\" class=\"ltx_ERROR undefined\">\\csvcoliv</span>)  <span id=\"A4.T4.4.4.4.4.4.4.4\" class=\"ltx_ERROR undefined\">\\csvcolv</span>Â (<span id=\"A4.T4.4.4.4.4.4.4.5\" class=\"ltx_ERROR undefined\">\\csvcolvi</span>)  <span id=\"A4.T4.4.4.4.4.4.4.6\" class=\"ltx_ERROR undefined\">\\csvcolvii</span>Â (<span id=\"A4.T4.4.4.4.4.4.4.7\" class=\"ltx_ERROR undefined\">\\csvcolviii</span>)  <span id=\"A4.T4.4.4.4.4.4.4.8\" class=\"ltx_ERROR undefined\">\\csvcolix</span>Â (<span id=\"A4.T4.4.4.4.4.4.4.9\" class=\"ltx_ERROR undefined\">\\csvcolx</span>)  <span id=\"A4.T4.4.4.4.4.4.4.10\" class=\"ltx_ERROR undefined\">\\csvcolxi</span>Â (<span id=\"A4.T4.4.4.4.4.4.4.11\" class=\"ltx_ERROR undefined\">\\csvcolxii</span>)  <span id=\"A4.T4.4.4.4.4.4.4.12\" class=\"ltx_ERROR undefined\">\\csvcolxiii</span>Â (<span id=\"A4.T4.4.4.4.4.4.4.13\" class=\"ltx_ERROR undefined\">\\csvcolxiv</span>)  <span id=\"A4.T4.4.4.4.4.4.4.14\" class=\"ltx_ERROR undefined\">\\csvcolxv</span>Â (<span id=\"A4.T4.4.4.4.4.4.4.15\" class=\"ltx_ERROR undefined\">\\csvcolxvi</span>)  <span id=\"A4.T4.4.4.4.4.4.4.16\" class=\"ltx_ERROR undefined\">\\csvcolxvii</span>Â (<span id=\"A4.T4.4.4.4.4.4.4.17\" class=\"ltx_ERROR undefined\">\\csvcolxviii</span>)</span></span></span></span></p>\n</span></div>\n</figure>\n<figure id=\"A4.F26\" class=\"ltx_figure\"><img src=\"/html/2309.02160/assets/images/isic/hist.png\" id=\"A4.F26.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"479\" height=\"357\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 26: </span>Number of samples from each class for each party - ISIC2019</figcaption>\n</figure>\n<div id=\"A4.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"A4.p1.1\" class=\"ltx_p\">We evaluate the effect of FL on local fairness on a real-world medical dataset, ISIC2019Â <cite class=\"ltx_cite ltx_citemacro_citep\">[Abay etÂ al., <a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2020</a>, Tschandl etÂ al., <a href=\"#bib.bib45\" title=\"\" class=\"ltx_ref\">2018</a>, Combalia etÂ al., <a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">2019</a>]</cite>, which contains dermoscopic images of skin lesions collected from six medical centers. The task is to classify dermoscopic images among nine different diagnostic categories. We filter out medical centers with less than 1900 images and regard each medical center with over 1900 images as a party (4 centers remain after filtering). For every party, we randomly select 1500 and 400 images for training and validation, respectively. We regard the binary notion of Sex as our sensitive attribute. In this multi-class and medical dataset setting, we measure the bias (fairness gap) based on the accuracy gap across groups. Following Â <cite class=\"ltx_cite ltx_citemacro_citep\">[OgierÂ du Terrail etÂ al., <a href=\"#bib.bib35\" title=\"\" class=\"ltx_ref\">2022</a>]</cite>Â <span id=\"footnote2\" class=\"ltx_note ltx_role_footnote\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://github.com/owkin/FLamby</span></span></span>, we train EfficientNetsÂ <cite class=\"ltx_cite ltx_citemacro_citep\">[Tan and Le, <a href=\"#bib.bib44\" title=\"\" class=\"ltx_ref\">2019</a>]</cite>, the state-of-the-art model structure for medical data, under the centralized, FL, and standalone setting. The accuracy and fairness gap for each client in different settings are shown in TableÂ <a href=\"#A4.T4\" title=\"Table 4 â€£ Appendix D Extending to a real-world medical dataset â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n</div>\n<div id=\"A4.p2\" class=\"ltx_para ltx_noindent\">\n<p id=\"A4.p2.1\" class=\"ltx_p\">FedAvg does not improve accuracy, possibly due to data heterogeneity among parties (as shown in FigureÂ <a href=\"#A4.F26\" title=\"Figure 26 â€£ Appendix D Extending to a real-world medical dataset â€£ 8 Reproducibility Statement â€£ 7 Ethics Statement â€£ 6 Acknowledgement â€£ 5 Future Work &amp; Conclusion â€£ 4 Related work â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">26</span></a>), resulting in the modelâ€™s instability and slow convergence, which is known as â€˜client-driftâ€™ issueÂ <cite class=\"ltx_cite ltx_citemacro_citep\">[Karimireddy etÂ al., <a href=\"#bib.bib24\" title=\"\" class=\"ltx_ref\">2020b</a>]</cite>.</p>\n</div>\n<div id=\"A4.p3\" class=\"ltx_para ltx_noindent\">\n<p id=\"A4.p3.1\" class=\"ltx_p\">Moreover, the FedAvg model results in a higher accuracy gap between groups, worsening the fairness issue compared to the standalone setting or the centralized setting (see the fairness gaps for party 1, party 3, and party 4 as examples). Our results suggest that, in real-world settings, FedAvg may not improve accuracy compared to the standalone setting for the local data distribution and may even exacerbate the fairness issue, leading to a more biased model.</p>\n</div>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>\n<section id=\"A5\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix E </span>Potential Mitigation</h2>\n\n<div id=\"A5.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"A5.p1.1\" class=\"ltx_p\">In this section, we suggest some potential solutions to reduce the effect of bias propagation in FL. Our hope is that this will inspire further research on developing fair FL in the future.</p>\n</div>\n<section id=\"A5.SS0.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Personalized FL</h4>\n\n<div id=\"A5.SS0.SSS0.Px1.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"A5.SS0.SSS0.Px1.p1.1\" class=\"ltx_p\">Our study has revealed that aggregate updates, which involve the average local update from all parties, may result in a higher bias than local updates for less biased parties (see FigureÂ <a href=\"#S3.F3\" title=\"Figure 3 â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). This can cause bias from other parties to propagate to the local model, thus increasing the local fairness gap. To mitigate the bias propagation effect, one possible approach is to avoid completely overwriting the local model with the global model. Personalized Federated Learning (PFL) techniquesÂ <cite class=\"ltx_cite ltx_citemacro_citep\">[Li etÂ al., <a href=\"#bib.bib29\" title=\"\" class=\"ltx_ref\">2021</a>]</cite> have recently been proposed to address this issue by focusing on improving the modelâ€™s accuracy. PFL can be a promising direction for developing fairness-aware personalized federated learning approaches that can help reduce the bias propagation effect in FL.</p>\n</div>\n</section>\n<section id=\"A5.SS0.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Fair Representation Learning</h4>\n\n<div id=\"A5.SS0.SSS0.Px2.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"A5.SS0.SSS0.Px2.p1.1\" class=\"ltx_p\">Our analysis demonstrated that bias is encoded in a small number of parameters in the first layer of the neural network, which is typically considered the feature extractor (as illustrated in FigureÂ <a href=\"#S3.F10\" title=\"Figure 10 â€£ 3.4 How is bias propagated in FL? â€£ 3.3 FL propagates bias among parties â€£ 3.2 Collaboration via FL can worsen fairness issue â€£ 3 Empirical Analysis â€£ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>). This suggests that the FL model is biased because the learned representation is biased, heavily influenced by the sensitive attribute. To address this issue, one possible solution is to implement fair representation learning techniques <cite class=\"ltx_cite ltx_citemacro_citep\">[Zemel etÂ al., <a href=\"#bib.bib50\" title=\"\" class=\"ltx_ref\">2013</a>, Zhao etÂ al., <a href=\"#bib.bib54\" title=\"\" class=\"ltx_ref\">2019</a>, Liu etÂ al., <a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">2022</a>, Zeng etÂ al., <a href=\"#bib.bib52\" title=\"\" class=\"ltx_ref\">2021b</a>]</cite> that aim to learn a feature extractor which is fair, meaning that it minimizes the dependence on the sensitive attribute while still retaining enough information about the task (or input features). Therefore, using fair representation learning in FL can potentially mitigate the propagation of bias effects.</p>\n</div>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>\n</section>\n</section>\n</section>\n</section>\n</section>\n</section>\n</section>\n</section>\n</div>\n</div>\n\n",
        "footnotes": "[tabular=c*3a*3e*3d,table head=    Accuracy     Î”Eâ€‹OsuperscriptÎ”ğ¸ğ‘‚\\Delta^{EO}    Î”Dâ€‹PsuperscriptÎ”ğ·ğ‘ƒ\\Delta^{DP}  \nDataset  Standalone  Centralized  FedAvg  Standalone  Centralized  FedAvg  Standalone  Centralized  FedAvg \n, table foot=]data/data_benefit.csv\n\\csvcoli- \\csvcoliiTableÂ 3.2 presents the average accuracy and fairness gaps of the centralized model, FL model, and standalone models on local datasets. Centralized training is observed to improve accuracy and fairness at the same time for parties. For instance, on the Income dataset, centralized training improves the accuracy by 6%percent66\\% and reduces the fairness gap across racial groups by 5.5%percent5.55.5\\% with respect to equalized odds and by 5.5%percent5.55.5\\% with respect to demographic parity. However, FL may not achieve the same benefit as centralized training in terms of fairness and can even exacerbate the fairness issue for parties. For example, on the Income dataset, the EO fairness gap of the FL model for the sex groups increases by 35%percent3535\\%, and the DP fairness gap increases by 29.9%percent29.929.9\\%, compared to standalone models. We include results for other popular FL algorithms, including FedNovaÂ (Wang etÂ al., 2020), ScaffoldÂ Karimireddy etÂ al. (2020b), FedOptÂ (Reddi etÂ al., 2020), FedProxÂ Li etÂ al. (2020), and MimeÂ (Karimireddy etÂ al., 2020a), in TableÂ 2 in AppendixÂ B, which show a similar pattern to that of the FedAvg algorithm. AppendixÂ B provides more detailed results on other FL algorithms.\nThe centralized and FL models are trained on the same dataset. The difference between the FL model and the centralized model in terms of fairness suggests FL algorithm can introduce more bias compared to standard training. Therefore, the explanation of how bias is introduced during standard training in the centralized setting may not fully explain how FL introduces bias. In the following, we further explore how FL impacts fairness from the party level.Disparate impact of FL on group fairness across parties. \nFigureÂ 1 illustrates the benefit of FL on fairness and accuracy for parties. We observe that FL improves accuracy for almost all parties, and the variance in accuracy improvement across parties is small. On the other hand, the fairness benefit of FL is negative for most parties. It implies that most parties obtain a more biased model in FL compared to standalone training. Furthermore, we notice that the variance in the fairness benefits across parties is large, suggesting that although all parties have the same global model in FL, they do not benefit from the FL model equally (each party evaluates the model performance on their local test dataset).To explore the impact of FL on fairness at the party level, FigureÂ 2 shows strong correlations between the fairness benefit a party obtains in FL and the fairness gap of the standalone model for the party, i.e., the bias level of the party. This finding highlights the disparate impact of FL on fairness: FL can improve fairness for more biased parties but at the cost of worsening the issue for less biased parties.Contradiction between aggregation and local update.\nDuring the training process of FL, we observe that aggregation and local update contradict each other, shown in FigureÂ 3. Local update from the least biased party, whose standalone model has the lowest fairness gap, reduces the fairness gap of the model. However, this reduction is eliminated by the aggregation step. Conversely, the local update from the most biased party increases the fairness gap of the model, which is then reduced by aggregation. This finding implies that the aggregation contributes to the disparate impact of FL on fairness, improving fairness for more biased parties but worsening the fairness for less biased parties compared to the standalone setting.Biased parties negatively influence other parties via aggregation throughout the training. \nWhy does aggregation have disparate impacts on local fairness for parties? Specifically, we ask which partyâ€™s local update causes the increase (or decrease) of the fairness gap for other parties via aggregation. To answer this question, we look into the influence of a partyâ€™s local update on other partiesâ€™ fairness via aggregation. More precisely, we compute the influence of party iğ‘–i on party jğ‘—j as the fairness gap increase when party iğ‘–iâ€™s local update is removed from the aggregation in each round tğ‘¡t and sum over all the training rounds. Formally, we define the influence of party iğ‘–i on party jğ‘—j as Ii,j=âˆ‘t=1TÎ”â€‹(Î¸t,âˆ’i,Dj)âˆ’Î”â€‹(Î¸t,Dj)subscriptğ¼ğ‘–ğ‘—superscriptsubscriptğ‘¡1ğ‘‡Î”subscriptğœƒğ‘¡ğ‘–subscriptğ·ğ‘—Î”subscriptğœƒğ‘¡subscriptğ·ğ‘—I_{i,j}=\\sum_{t=1}^{T}\\Delta(\\theta_{t,-i},D_{j})-\\Delta(\\theta_{t},D_{j}), where Î¸tsubscriptğœƒğ‘¡\\theta_{t} is the global model (i.e., the aggregated model overall local updated models) and Î¸t,âˆ’isubscriptğœƒğ‘¡ğ‘–\\theta_{t,-i} is the aggregated model over local updated models from parties excluding party iğ‘–i. If party iğ‘–i improves fairness for party jğ‘—j, the influence is positive and vice versa. We compute the influence for all pairs of parties, and the most influential pairs are shown in FigureÂ 4. We observe that a less biased party has a positive influence on fairness for other parties, while a more biased party has a negative influence on other parties. This result shows that a biased party can negatively influence other partiesâ€™ fairness via aggregation throughout the training.Furthermore, we investigate the relationship between a partyâ€™s bias (i.e., the bias of the partyâ€™s standalone model) and its average influence on all partiesâ€™ fairness (i.e., âˆ‘k=1KIi,k/Ksuperscriptsubscriptğ‘˜1ğ¾subscriptğ¼ğ‘–ğ‘˜ğ¾\\sum_{k=1}^{K}I_{i,k}/K). The results are presented in FigureÂ 6, which shows a strong correlation between these two factors. This finding further supports our conclusion that the more biased parties have a stronger negative influence on other partiesâ€™ fairness, while the less biased parties have a stronger positive influence.Finally, we analyze the dynamics of the influence of the top 5 most biased parties (i.e., parties with the largest fairness gap in the standalone setting) and the top 5 least biased parties, as shown in FigureÂ 6. We observe that the influence of the most biased parties is monotonically increasing throughout the training, while the influence of the least biased parties is decreasing. In other words, biased parties consistently have a negative influence on othersâ€™ fairness gaps, while less biased parties have a positive influence. These results suggest that FL can propagate bias among parties: the bias from biased parties negatively influence the fairness of other parties via aggregation throughout the training. Next, we will explore how the bias is propagated in FL.Disparate treatment causes large fairness gaps.\nOur first investigation explores what the bias represents, specifically whether the bias increase in FL is directly caused by the disparate treatment of the model among sensitive groups. To answer this question, we utilize Integrated GradientsÂ (Sundararajan etÂ al., 2017) to measure the attribution of each input feature to the modelsâ€™ predictions with respect to the positive class.\nFigureÂ 7 shows the attribution value distribution for the sensitive attribute \"Sex\" over individual test points from the female and male groups. We notice that the sex attribute has a large attribution value for the standalone modelâ€™s predictions and the FL modelâ€™s predictions. This finding implies that the predictions of those models are heavily dependent on the sensitive attribute of the test data. Moreover, the sensitive attribute affects the model predictions differently for the male and female groups, with the average attribution value being positive for the male group and negative for the female group. Furthermore, we find that there is minimal difference in the attribution value for other attributes with respect to the female and male groups (see FigureÂ 19 in AppendixÂ B). This indicates that the large fairness gap in the models is not caused by the distinct distribution of other insensitive attributes over protected groups; rather, it is mainly caused by the modelsâ€™ disparate treatment of protected groups.FL model learns more biased patterns. \nIn FigureÂ 7, we compare the attribution value of â€œSexâ€ for different models and find that, while the predictions of the FL model depend less heavily on the sensitive attribute compared to those of the standalone model, the dependence is still stronger than that of the centralized model. This suggests that the FL model learns a more biased pattern compared to what could be learned in the centralized setting.\nFigureÂ 9 shows the dynamic of the average absolute feature attribution for â€œSexâ€ during the training. We find that standalone training increases the modelâ€™s dependence on the sensitive attribute throughout training for the most biased party, but centralized training decreases it. This suggests that collaboration through centralized training improves local fairness by guiding the model to learn less biased features. In FL, however, the absolute attribution value barely changes after 100 rounds and remains significantly greater than that in the centralized setting. This implies that the FL algorithm may introduce bias to the final model by inhibiting the model from learning less biased features.Biased parties increase the model dependence on the sensitive attribute. \nFigureÂ 9 shows the attribution value of the aggregated model and locally updated model from the most biased party and least biased party. We observe that the biased party increases the global model dependence on sensitive attributes during the local update, and this increase persists throughout the training. In contrast, the least biased party reduces the dependence on the sensitive attribute, which is aligned with the trend of the fairness gap in FigureÂ 3. This suggests that the biased parties have a negative impact on the fairness of other parties by increasing the modelâ€™s dependence on the sensitive attribute.Bias is encoded in a few parameters. Biased parties increase the model dependence on the sensitive attribute. But how does this increase propagate to other parties in the network? Since parties share the model parameters of the local model with the server, the bias is likely encoded in the model parameters. Therefore, we investigate which parameters are related to the modelâ€™s bias. Intuitively, the parameters used to extract sensitive attribute information impact the attribution value of the sensitive attribute to the model prediction. If the absolute value (signal) of those parameters is substantial, the value of the sensitive attribute will significantly affect the modelâ€™s prediction. In our evaluation, the sensitive attribute is part of the input feature, so the parameters directly applied to the â€œSexâ€ attribute in the first layer of the neural network should contribute to the modelâ€™s bias.FigureÂ 10(a) shows the normalized norm of the parameters associated with the sensitive attribute for the most and least biased parties in the aggregated and locally updated models. The normalized norm is defined as the norm of parameters associated with the sensitive attribute divided by the parameter norm of the first layer. We observe that the least biased party reduces the parameter norm, hence decreasing the modelâ€™s sensitivity to the sensitive attribute. On the other hand, the biased party increases the norm for those parameters, amplifying the impact of the sensitive attribute on the modelâ€™s prediction. Through aggregation, this amplification will be propagated to the global model.\nFigureÂ 10(b) reveals a moderate correlation between the fairness gap party experiences in the standalone setting and the normalized parameter norm for the parameters used to extract sensitive attribute information. This implies that biased parties increase the parameter value associated with sensitive attributes during the local update, thereby boosting the modelâ€™s susceptibility to the sensitive attribute.Controlling fairness gap by scaling a few parameters. \nTo further investigate the impact of the parameters associated with the sensitive attribute (i.e., 104104104 parameters out of 1,79217921,792) on model fairness, we examine the effect of scaling these parameters on the fairness gap in FigureÂ 10(c). We find that the fairness gap can be greatly widened or narrowed at the expense of a moderate degree of accuracy. Specifically, by scaling the parameter value by 0.10.10.1 for the trained FL model, we significantly reduce the EO gap by 74.6%percent74.674.6\\% (from 0.1980.1980.198 to 0.050.050.05) and the DP gap by 69.1%percent69.169.1\\% (from 0.2170.2170.217 to 0.0670.0670.067) with just a moderate accuracy loss of 0.8%percent0.80.8\\% (from 0.7850.7850.785 to 0.7790.7790.779). In contrast, scaling the same set of parameters by a factor of 10 increases the EO gap to 0.960.960.96 (the maximal fairness gap is 111), which is almost five times larger, and increases the DP gap by 259%percent259259\\% to 0.780.780.78, while reducing the accuracy from 0.7850.7850.785 to 0.670.670.67. These findings explain how bias is propagated in FL: biased parties magnify the impact of sensitive attributes on model predictions by increasing the model parameter used to extract sensitive attributes. This rise in parameters is subsequently propagated to the global model through aggregation, further aggravating the issue of fairness for other parties. Our results explain how bias is propagated in FL: Biased parties encode bias in a few parameters through a local update, and this bias is consequently propagated to the entire network through parameter aggregation.Fairness has received considerable attention due to the growing deployment of machine learning in decision-making processes. Various definitions of fairness have been presentedÂ (Hardt etÂ al., 2016; Dwork etÂ al., 2012; Calders etÂ al., 2009). Specifically, group fairness requires that the model behave similarly for groups defined by a sensitive attribute (e.g., race). While how machine learning algorithms propagate data bias to the final model has been extensively investigated in a centralized setting Â (Blum and Stangl, 2020; Lakkaraju etÂ al., 2017; Rambachan and Roth, 2020; Friedler etÂ al., 2019; Dullerud etÂ al., 2022), the effect of FL on model fairness is not yet fully understood.The existing literature on fairness in FL mainly focuses on the performance disparity of FL models across parties, rather than demographic groupsÂ (Li etÂ al., 2021; Zhao and Joshi, 2022; Li etÂ al., 2019; Mohri etÂ al., 2019; Deng etÂ al., 2020; Donahue and Kleinberg, 2021; Hao etÂ al., 2021; Zhou etÂ al., 2021; Yu etÂ al., 2020; Lyu etÂ al., 2020). However, we focus on group fairness, which concerns performance disparity among groups. In terms of group fairness, Abay etÂ al. (2020) listed a few potential sources of bias in FL. Recently, considerable progress has been made in training group fair models in FLÂ (Abay etÂ al., 2020; Chu etÂ al., 2021; Zeng etÂ al., 2021a; Hu etÂ al., 2022; Du etÂ al., 2021; Ezzeldin etÂ al., 2021). Nonetheless, the majority of these works suggest techniques for achieving fairness on a single test distribution. Instead, we focus on fairness issues for parties. Some studiesÂ (Cui etÂ al., 2021; Papadaki etÂ al., 2022) proposed algorithms to improve local fairness for parties. Our purpose, instead, is to gain a comprehensive understanding of how FL influences local fairness on its own, which we believe is equally crucial as designing fair algorithms.Future Work. \nIn this work, we have investigated how bias is propagated in FL when the sensitive attribute is included as an input feature. In practice, however, sensitive attributes may be prohibited from being included in input features. In such situations, the model may still be heavily biased due to variables that are correlated with the (unobserved) sensitive attribute. For instance, a personâ€™s zip code may be highly correlated with their race, a phenomenon known as \"redlining\". A promising direction for future work is to identify which model parameters contribute to the bias and to audit the bias propagation in this setting. Another important direction is to design FL algorithms that are robust to bias propagation. In AppendixÂ E, we briefly discuss a few potential ways to achieve this goal.Conclusion.  Federated learning has become increasingly popular in various applications with significant individual-level consequences, making it essential to anticipate the possible bias introduced by FL. Our paper takes the first step in this direction by providing a comprehensive analysis of the impact of FL on local fairness for parties. We demonstrated that the FL algorithm could introduce bias on its own which may exacerbate the issue of fairness for the involved parties. Moreover, we showed that this exacerbation is not evenly distributed among parties, as FL can propagate bias among them. Finally, we explained how bias is propagated in FL: biased parties encode their bias into the local updates by increasing the signal of a few parameters steadily throughout the training process, which is then propagated to the global model via aggregation and, ultimately, to other parties.The authors would like to thank Ergute Bao, Ta Duy Nguyen, and Martin Strobel for their valuable feedback on earlier versions of this paper, as well as the anonymous reviewers for their insightful comments. This research is supported by Google PDPO faculty research award, Intel within the www.private-ai.org center, Meta faculty research award, the NUS Early Career Research Award (NUS ECRA award num- ber NUS ECRA FY19 P16), and the National Research Foundation, Singapore under its Strategic Capability Research Centres Funding Initiative. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the author(s) and do not reflect the views of the National Research Foundation, Singapore.Our analysis focuses on group fairness, which is typically used to audit the model or system for any bias or discrimination. Auditing the bias with respect to other definitions of fairness, such as individual fairness, may result in different conclusions. Moreover, our study focuses primarily on the binary notion of sex attributes and the multi-valued race attribute. We recognize that there are numerous protected groups outside those considered in the analysis, such as those defined by multiple sensitive attributes. The propagation of bias against fine-grained subgroups may be even more substantial than we found in the paper.We provide details about the model, datasets, and implementations in AppendixÂ A, and the code for the paper is available at https://github.com/privacytrustlab/bias_in_FL.This appendix is divided into five sections. AppendixÂ A provides additional details about the experimental setups, including information about the models, datasets, and hyperparameters used. AppendixÂ B presents further experimental results that support the claims made in the paper. In AppendixÂ C, we discuss the results of an existing fair FL algorithm. AppendixÂ D extends our analysis to real-world medical datasets. Finally, in AppendixÂ E, we discuss potential methods for mitigating bias in FL.We explain the datasets and models used in the paper.We use the datasets provided by folktables. In particular, we consider the ACSIncome, ACSPublicCoverage, and ACSEmployment tasks defined in the forlktables. In the ACSIncome, the goal is to predict whether an individualâ€™s Income is above $50,000. In the ACSEmployment task, the goal is to predict whether an individual is employed. Similarly, in the Health (i.e., ACSPublicCoverage) task, the objective is to predict whether an individual is covered by public health insurance. We use the same pre-processing as in the folktables and train a fully connected neural network model with one hidden layer of 32 neurons for Income and 64 neurons for Employment and Health tasks. For all the tasks, we use the RELU activation function. We use an SGD optimizer with a learning rate of 0.001 for centralized training on Health and Employment datasets and 0.1 for other settings, and the batch size is 32. We train the NN models for 200 epochs. In FL, each client updates the global mdoel for 1 epoch and shares it with the server. We encode the categorical features based on the encoding template provided in folktables. After the encoding, the input feature size for Income is 54, 154 for Health, and 109 for Employment. We consider sex and race as sensitive attributes. Accordingly, there are two gender groups (male and female) and nine racial groups (\"White alone,\" \"Black or African American alone,\" \"American Indian alone,\" \"Alaska Native alone,\" and \"American Indian and Alaska Native tribes specified; or American Indian or Alaska Native, not specified and no other,\" \"Asian alone,\" \"Native Hawaiian and Other Pacific Islander alone,\" \"Some Other Race alone,\" \"Two or More Races\").We train CNN models on the dataset with one CNN layer whose output channel is 32, kernel size is 3, and stride is 1. We use â€™sameâ€™ padding for the CNN layer. Following this CNN layer, we have the Batch normalization layer and Max Pooling layer. After which, we have the connected layer. We train the model for 500 communication rounds or epochs with SGD optimizer. The learning rate is 0.1, and the batch size is 128. The train, test, and validation datasets ratio for each party is 6:2:2.In our paper, we also evaluate various popular FL algorithms, including FedNovaÂ [Wang etÂ al., 2020], ScaffoldÂ Karimireddy etÂ al. [2020b], FedOptÂ [Reddi etÂ al., 2020], FedProxÂ Li etÂ al. [2020], and MimeÂ [Karimireddy etÂ al., 2020a]. We use the same local learning rate and the number of the local epoch as in FedAvg. We provided the detailed hyper-parameters for each of the algorithms in our code (See supplementary). We run all experiments on Ubuntu with two NVIDIA TITAN RTX GPUs.FigureÂ 11 shows the fraction of samples for each subgroup across all parties, while FigureÂ 12 shows the histogram of subgroup proportions for each party. We observe that the parties have different fractions of samples from each group in the Income dataset, indicating that their local data distributions are dissimilar. In contrast, for the Health and Employment datasets, parties have similar fractions of samples from each subgroup, suggesting that their data distributions are more alike than those of the Income dataset.We show the bias propagation effect of FL for the Health and Employment task in FigureÂ 13 and 14 respectively.We show the bias propagation effect of FL on CelebA dataset. Besides partitioning the data in an IID manner (we refer to as setting (i)), we also evaluate two different settings, where we change the number of samples from the minority subgroup (the female group with the â€œNot Youngâ€ age label). In this way, we aim to change the data bias in the local training datasets for clients. In particular, in setting (ii), half of the parties have more samples from the minority subgroup than another half of the parties. The ratio is 8:2. In setting (iii), a single party has half of the data from the minority subgroup, and the data is then iid partitioned among the other parties. In the figure, we find that the in the non-iid setting, the more biased parties have a larger and more positive benefit, while FL hurts the less biased parties. FigureÂ 15 shows the correlation between the fairness gap of the standalone model and the benefit a party gets in the FL. We find that the in the non-iid setting, the more biased parties have a larger and more positive benefit, while FL hurts the less biased parties.FigureÂ 16 shows the dynamic of the fairness gap for the aggregated model and locally updated model from the most biased party and least biased party during the training.We show the top 10 maximal positive and top 10 maximal native influence client pairs in FigureÂ 17 and the top 15 in FigureÂ 18. We can see that the less biased client has a strong positive influence on other clients while the more biased client has a strong negative influence on other clients.We show the attribution value for all input features for the most biased party and least biased party in FigureÂ 19 and FigureÂ 20, respectively. We observe that the attribution value for other features is similar for female and male groups. However, the sex attribute has the largest attribution value and affects the groups differently. It implies that the models are highly dependent on the sensitive attribute (i.e., \"Sex\") for making predictions.TableÂ 2 shows results on other FL algorithms, including FedNovaÂ [Wang etÂ al., 2020], ScaffoldÂ Karimireddy etÂ al. [2020b], FedOptÂ [Asad etÂ al., 2020], FedProxÂ Li etÂ al. [2020], and MimeÂ [Karimireddy etÂ al., 2020a].\nWe find that when the FL model achieves higher accuracy than standalone models, the fairness gap of the FL model can be higher than that of standalone models. This observation is consistent across multiple FL algorithms. This strong evidence implies that the improvement of accuracy in FL can come at the cost of fairness. In addition, this is not unique to only the FedAvg algorithm.[tabular=ca*2e*2d,table head=     Race     Sex  \nAlgorithm  Accuracy  Î”Eâ€‹OsuperscriptÎ”ğ¸ğ‘‚\\Delta^{EO}  Î”Dâ€‹PsuperscriptÎ”ğ·ğ‘ƒ\\Delta^{DP} Î”Eâ€‹OsuperscriptÎ”ğ¸ğ‘‚\\Delta^{EO}  Î”Dâ€‹PsuperscriptÎ”ğ·ğ‘ƒ\\Delta^{DP} \n, table foot=]data/data_other_fl.csv\n \\csvcoli \\csvcoliiÂ (\\csvcoliii)  \\csvcolivÂ (\\csvcolv)  \\csvcolviÂ (\\csvcolvii)  \\csvcolviiiÂ (\\csvcolix)  \\csvcolxÂ (\\csvcolxi)In FigureÂ 21, we present the model ROC on groups to illustrate the performance disparity across groups. Surprisingly, there is no huge difference between the ROC between groups. In contrast, FigureÂ 22 shows the noticeable difference between groups with respect to the precision-recall curve, especially for the most biased party. The main reason is that the dataset is imbalanced. Thus, ROC is usually misleading. On the precision-recall curve, we find that the model achieves a higher precision on the majority (i.e., Male group) compared to the minority group (i.e., Female group).FigureÂ 23 shows a comparison of the modelâ€™s performance when scaling different model parameters. Specifically, we examine the impact of scaling the parameters in the first layer of the neural network that does not have any computation on the Sex attribute (referred to as \"Other parameters\"), as well as the parameter related to the Sex attribute (referred to as \"Related parameters\"). The results indicate that up-scaling or down-scaling other parameters do not significantly affect the fairness gap while scaling the related parameters has a considerable impact on model fairness. This finding supports our hypothesis that the modelâ€™s bias is primarily encoded in a few model parameters. In FL, biased parties introduce bias during local training by increasing the weights for those related parameters.TableÂ 3 presents the results for a fair FL algorithm proposed by Abay etÂ al. [2020], which uses a reweighting algorithm proposed in the centralized settingÂ [Kamiran and Calders, 2012]. The algorithm involves assigning weights to each subgroup (defined by the label and a sensitive attribute) before FL, based on the local or global training dataset. This is called \"local reweighting\" or \"global reweighting,\" respectively. During FL, parties update the FL model to minimize the weighted loss. The table presents the accuracy and fairness gap of the FL algorithm using local and global reweighting for the Income, Health, and Employment datasets.[tabular=cca*2e*2d,table head=      Race     Sex  \nAlgorithm  Sensitive Attribute  Accuracy  Î”Eâ€‹OsuperscriptÎ”ğ¸ğ‘‚\\Delta^{EO}  Î”Dâ€‹PsuperscriptÎ”ğ·ğ‘ƒ\\Delta^{DP} Î”Eâ€‹OsuperscriptÎ”ğ¸ğ‘‚\\Delta^{EO}  Î”Dâ€‹PsuperscriptÎ”ğ·ğ‘ƒ\\Delta^{DP} \n, table foot=]data/data_reweiging.csv\n \\csvcoli \\csvcolxii\\csvcoliiÂ (\\csvcoliii)  \\csvcolivÂ (\\csvcolv)  \\csvcolviÂ (\\csvcolvii)  \\csvcolviiiÂ (\\csvcolix)  \\csvcolxÂ (\\csvcolxi)We found that applying reweighing algorithms in FL reduces the average fairness gap across parties. This is due to the fact that, after reweighing, the parties that were initially biased do not introduce significant bias to the model during local training, resulting in a reduction of the fairness gap in the FL model.However, we also noted that the fairness gap in the model remains high for the Race sensitive attribute, indicating that the global and local reweighing algorithms do not entirely eliminate bias in FL. Furthermore, we would like to highlight some concerns regarding the fair FL algorithm:The fair FL algorithm enhances the performance of the minority group at the expense of the majority group. FigureÂ 24 illustrates that the minority group (i.e., the Female group) has a better performance after the reweighing, but at the cost of reduced performance for the majority.When improving fairness with respect to one sensitive attribute, the fairness issue with respect to another sensitive attribute may worsen, as shown inÂ TableÂ 3. For instance, when applying local reweighing to reduce bias with respect to the \"Sex\" attribute, the fairness gap with respect to \"Race\" increased from 0.514 (results on FedAvg) to 0.521, indicating that reweighing to improve fairness with respect to Sex may amplify the bias with respect to Race. This implies that the bias with respect to Race can still propagate in FL, and our analysis remains valid.The reweighing algorithms assume that parties are interested in mitigating the bias with respect to the same sensitive attribute, which may not be the case in practice. FigureÂ 25 shows the fairness gap of standalone models with respect to different sensitive attributes. We observe that many parties have a low fairness gap with respect to one sensitive attribute and a high fairness gap with respect to another sensitive attribute. Thus, parties may aim to enhance fairness with respect to different sensitive attributes, which renders this reweighing algorithm unsuitable. This problem is more prevalent in FL, given the different data distributions of parties.[tabular=c*2a*2e*2d*2f,table head= Setting    Party 1     Party 2      Party 3      Party 4 \nAlgorithm  Accuracy  Î”Aâ€‹câ€‹csuperscriptÎ”ğ´ğ‘ğ‘\\Delta^{Acc}  Accuracy  Î”Aâ€‹câ€‹csuperscriptÎ”ğ´ğ‘ğ‘\\Delta^{Acc}  Accuracy  Î”Aâ€‹câ€‹csuperscriptÎ”ğ´ğ‘ğ‘\\Delta^{Acc}  Accuracy  Î”Aâ€‹câ€‹csuperscriptÎ”ğ´ğ‘ğ‘\\Delta^{Acc} \n, table foot=]data/isic2019.csv\n \\csvcolii \\csvcoliiiÂ (\\csvcoliv)  \\csvcolvÂ (\\csvcolvi)  \\csvcolviiÂ (\\csvcolviii)  \\csvcolixÂ (\\csvcolx)  \\csvcolxiÂ (\\csvcolxii)  \\csvcolxiiiÂ (\\csvcolxiv)  \\csvcolxvÂ (\\csvcolxvi)  \\csvcolxviiÂ (\\csvcolxviii)We evaluate the effect of FL on local fairness on a real-world medical dataset, ISIC2019Â [Abay etÂ al., 2020, Tschandl etÂ al., 2018, Combalia etÂ al., 2019], which contains dermoscopic images of skin lesions collected from six medical centers. The task is to classify dermoscopic images among nine different diagnostic categories. We filter out medical centers with less than 1900 images and regard each medical center with over 1900 images as a party (4 centers remain after filtering). For every party, we randomly select 1500 and 400 images for training and validation, respectively. We regard the binary notion of Sex as our sensitive attribute. In this multi-class and medical dataset setting, we measure the bias (fairness gap) based on the accuracy gap across groups. Following Â [OgierÂ du Terrail etÂ al., 2022]Â 222https://github.com/owkin/FLamby, we train EfficientNetsÂ [Tan and Le, 2019], the state-of-the-art model structure for medical data, under the centralized, FL, and standalone setting. The accuracy and fairness gap for each client in different settings are shown in TableÂ 4.FedAvg does not improve accuracy, possibly due to data heterogeneity among parties (as shown in FigureÂ 26), resulting in the modelâ€™s instability and slow convergence, which is known as â€˜client-driftâ€™ issueÂ [Karimireddy etÂ al., 2020b].Moreover, the FedAvg model results in a higher accuracy gap between groups, worsening the fairness issue compared to the standalone setting or the centralized setting (see the fairness gaps for party 1, party 3, and party 4 as examples). Our results suggest that, in real-world settings, FedAvg may not improve accuracy compared to the standalone setting for the local data distribution and may even exacerbate the fairness issue, leading to a more biased model.In this section, we suggest some potential solutions to reduce the effect of bias propagation in FL. Our hope is that this will inspire further research on developing fair FL in the future.Our study has revealed that aggregate updates, which involve the average local update from all parties, may result in a higher bias than local updates for less biased parties (see FigureÂ 3). This can cause bias from other parties to propagate to the local model, thus increasing the local fairness gap. To mitigate the bias propagation effect, one possible approach is to avoid completely overwriting the local model with the global model. Personalized Federated Learning (PFL) techniquesÂ [Li etÂ al., 2021] have recently been proposed to address this issue by focusing on improving the modelâ€™s accuracy. PFL can be a promising direction for developing fairness-aware personalized federated learning approaches that can help reduce the bias propagation effect in FL.Our analysis demonstrated that bias is encoded in a small number of parameters in the first layer of the neural network, which is typically considered the feature extractor (as illustrated in FigureÂ 10). This suggests that the FL model is biased because the learned representation is biased, heavily influenced by the sensitive attribute. To address this issue, one possible solution is to implement fair representation learning techniques [Zemel etÂ al., 2013, Zhao etÂ al., 2019, Liu etÂ al., 2022, Zeng etÂ al., 2021b] that aim to learn a feature extractor which is fair, meaning that it minimizes the dependence on the sensitive attribute while still retaining enough information about the task (or input features). Therefore, using fair representation learning in FL can potentially mitigate the propagation of bias effects.",
        "references": [
            []
        ]
    },
    "A2.T2": {
        "caption": "Table 2: Benefit of collaboration on local datasets - Various FL algorithms (Income). The standard deviation across five runs is indicated between parentheses.",
        "table": "<div id=\"A2.T2.4\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:433.6pt;height:42.2pt;vertical-align:-32.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(42.2pt,-0.9pt) scale(1.2415726193884,1.2415726193884) ;\"><span id=\"A2.T2.4.5\" class=\"ltx_ERROR undefined\">\\csvreader</span>\n<p id=\"A2.T2.4.4\" class=\"ltx_p\">[tabular=ca*2e*2d,table head=    <span id=\"A2.T2.4.4.4\" class=\"ltx_text\" style=\"background-color:#F6FCF2;\"> Race     <span id=\"A2.T2.4.4.4.4\" class=\"ltx_text\" style=\"background-color:#F5FAFC;\">Sex  \n<br class=\"ltx_break\">Algorithm  Accuracy  <math id=\"A2.T2.1.1.1.1.m1.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{EO}\" display=\"inline\"><semantics id=\"A2.T2.1.1.1.1.m1.1a\"><msup id=\"A2.T2.1.1.1.1.m1.1.1\" xref=\"A2.T2.1.1.1.1.m1.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A2.T2.1.1.1.1.m1.1.1.2\" xref=\"A2.T2.1.1.1.1.m1.1.1.2.cmml\">Î”</mi><mrow id=\"A2.T2.1.1.1.1.m1.1.1.3\" xref=\"A2.T2.1.1.1.1.m1.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.1.1.1.1.m1.1.1.3.2\" xref=\"A2.T2.1.1.1.1.m1.1.1.3.2.cmml\">E</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A2.T2.1.1.1.1.m1.1.1.3.1\" xref=\"A2.T2.1.1.1.1.m1.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.1.1.1.1.m1.1.1.3.3\" xref=\"A2.T2.1.1.1.1.m1.1.1.3.3.cmml\">O</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A2.T2.1.1.1.1.m1.1b\"><apply id=\"A2.T2.1.1.1.1.m1.1.1.cmml\" xref=\"A2.T2.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.T2.1.1.1.1.m1.1.1.1.cmml\" xref=\"A2.T2.1.1.1.1.m1.1.1\">superscript</csymbol><ci id=\"A2.T2.1.1.1.1.m1.1.1.2.cmml\" xref=\"A2.T2.1.1.1.1.m1.1.1.2\">Î”</ci><apply id=\"A2.T2.1.1.1.1.m1.1.1.3.cmml\" xref=\"A2.T2.1.1.1.1.m1.1.1.3\"><times id=\"A2.T2.1.1.1.1.m1.1.1.3.1.cmml\" xref=\"A2.T2.1.1.1.1.m1.1.1.3.1\"></times><ci id=\"A2.T2.1.1.1.1.m1.1.1.3.2.cmml\" xref=\"A2.T2.1.1.1.1.m1.1.1.3.2\">ğ¸</ci><ci id=\"A2.T2.1.1.1.1.m1.1.1.3.3.cmml\" xref=\"A2.T2.1.1.1.1.m1.1.1.3.3\">ğ‘‚</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T2.1.1.1.1.m1.1c\">\\Delta^{EO}</annotation></semantics></math>  <math id=\"A2.T2.2.2.2.2.m2.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{DP}\" display=\"inline\"><semantics id=\"A2.T2.2.2.2.2.m2.1a\"><msup id=\"A2.T2.2.2.2.2.m2.1.1\" xref=\"A2.T2.2.2.2.2.m2.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A2.T2.2.2.2.2.m2.1.1.2\" xref=\"A2.T2.2.2.2.2.m2.1.1.2.cmml\">Î”</mi><mrow id=\"A2.T2.2.2.2.2.m2.1.1.3\" xref=\"A2.T2.2.2.2.2.m2.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.2.2.2.2.m2.1.1.3.2\" xref=\"A2.T2.2.2.2.2.m2.1.1.3.2.cmml\">D</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A2.T2.2.2.2.2.m2.1.1.3.1\" xref=\"A2.T2.2.2.2.2.m2.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.2.2.2.2.m2.1.1.3.3\" xref=\"A2.T2.2.2.2.2.m2.1.1.3.3.cmml\">P</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A2.T2.2.2.2.2.m2.1b\"><apply id=\"A2.T2.2.2.2.2.m2.1.1.cmml\" xref=\"A2.T2.2.2.2.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.T2.2.2.2.2.m2.1.1.1.cmml\" xref=\"A2.T2.2.2.2.2.m2.1.1\">superscript</csymbol><ci id=\"A2.T2.2.2.2.2.m2.1.1.2.cmml\" xref=\"A2.T2.2.2.2.2.m2.1.1.2\">Î”</ci><apply id=\"A2.T2.2.2.2.2.m2.1.1.3.cmml\" xref=\"A2.T2.2.2.2.2.m2.1.1.3\"><times id=\"A2.T2.2.2.2.2.m2.1.1.3.1.cmml\" xref=\"A2.T2.2.2.2.2.m2.1.1.3.1\"></times><ci id=\"A2.T2.2.2.2.2.m2.1.1.3.2.cmml\" xref=\"A2.T2.2.2.2.2.m2.1.1.3.2\">ğ·</ci><ci id=\"A2.T2.2.2.2.2.m2.1.1.3.3.cmml\" xref=\"A2.T2.2.2.2.2.m2.1.1.3.3\">ğ‘ƒ</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T2.2.2.2.2.m2.1c\">\\Delta^{DP}</annotation></semantics></math> <math id=\"A2.T2.3.3.3.3.m3.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{EO}\" display=\"inline\"><semantics id=\"A2.T2.3.3.3.3.m3.1a\"><msup id=\"A2.T2.3.3.3.3.m3.1.1\" xref=\"A2.T2.3.3.3.3.m3.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A2.T2.3.3.3.3.m3.1.1.2\" xref=\"A2.T2.3.3.3.3.m3.1.1.2.cmml\">Î”</mi><mrow id=\"A2.T2.3.3.3.3.m3.1.1.3\" xref=\"A2.T2.3.3.3.3.m3.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.3.3.3.3.m3.1.1.3.2\" xref=\"A2.T2.3.3.3.3.m3.1.1.3.2.cmml\">E</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A2.T2.3.3.3.3.m3.1.1.3.1\" xref=\"A2.T2.3.3.3.3.m3.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.3.3.3.3.m3.1.1.3.3\" xref=\"A2.T2.3.3.3.3.m3.1.1.3.3.cmml\">O</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A2.T2.3.3.3.3.m3.1b\"><apply id=\"A2.T2.3.3.3.3.m3.1.1.cmml\" xref=\"A2.T2.3.3.3.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.T2.3.3.3.3.m3.1.1.1.cmml\" xref=\"A2.T2.3.3.3.3.m3.1.1\">superscript</csymbol><ci id=\"A2.T2.3.3.3.3.m3.1.1.2.cmml\" xref=\"A2.T2.3.3.3.3.m3.1.1.2\">Î”</ci><apply id=\"A2.T2.3.3.3.3.m3.1.1.3.cmml\" xref=\"A2.T2.3.3.3.3.m3.1.1.3\"><times id=\"A2.T2.3.3.3.3.m3.1.1.3.1.cmml\" xref=\"A2.T2.3.3.3.3.m3.1.1.3.1\"></times><ci id=\"A2.T2.3.3.3.3.m3.1.1.3.2.cmml\" xref=\"A2.T2.3.3.3.3.m3.1.1.3.2\">ğ¸</ci><ci id=\"A2.T2.3.3.3.3.m3.1.1.3.3.cmml\" xref=\"A2.T2.3.3.3.3.m3.1.1.3.3\">ğ‘‚</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T2.3.3.3.3.m3.1c\">\\Delta^{EO}</annotation></semantics></math>  <math id=\"A2.T2.4.4.4.4.m4.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{DP}\" display=\"inline\"><semantics id=\"A2.T2.4.4.4.4.m4.1a\"><msup id=\"A2.T2.4.4.4.4.m4.1.1\" xref=\"A2.T2.4.4.4.4.m4.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A2.T2.4.4.4.4.m4.1.1.2\" xref=\"A2.T2.4.4.4.4.m4.1.1.2.cmml\">Î”</mi><mrow id=\"A2.T2.4.4.4.4.m4.1.1.3\" xref=\"A2.T2.4.4.4.4.m4.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.4.4.4.4.m4.1.1.3.2\" xref=\"A2.T2.4.4.4.4.m4.1.1.3.2.cmml\">D</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A2.T2.4.4.4.4.m4.1.1.3.1\" xref=\"A2.T2.4.4.4.4.m4.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.4.4.4.4.m4.1.1.3.3\" xref=\"A2.T2.4.4.4.4.m4.1.1.3.3.cmml\">P</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A2.T2.4.4.4.4.m4.1b\"><apply id=\"A2.T2.4.4.4.4.m4.1.1.cmml\" xref=\"A2.T2.4.4.4.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.T2.4.4.4.4.m4.1.1.1.cmml\" xref=\"A2.T2.4.4.4.4.m4.1.1\">superscript</csymbol><ci id=\"A2.T2.4.4.4.4.m4.1.1.2.cmml\" xref=\"A2.T2.4.4.4.4.m4.1.1.2\">Î”</ci><apply id=\"A2.T2.4.4.4.4.m4.1.1.3.cmml\" xref=\"A2.T2.4.4.4.4.m4.1.1.3\"><times id=\"A2.T2.4.4.4.4.m4.1.1.3.1.cmml\" xref=\"A2.T2.4.4.4.4.m4.1.1.3.1\"></times><ci id=\"A2.T2.4.4.4.4.m4.1.1.3.2.cmml\" xref=\"A2.T2.4.4.4.4.m4.1.1.3.2\">ğ·</ci><ci id=\"A2.T2.4.4.4.4.m4.1.1.3.3.cmml\" xref=\"A2.T2.4.4.4.4.m4.1.1.3.3\">ğ‘ƒ</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T2.4.4.4.4.m4.1c\">\\Delta^{DP}</annotation></semantics></math> \n<br class=\"ltx_break\">, table foot=]data/data_other_fl.csv\n <span id=\"A2.T2.4.4.4.4.1\" class=\"ltx_ERROR undefined\">\\csvcoli</span> <span id=\"A2.T2.4.4.4.4.2\" class=\"ltx_ERROR undefined\">\\csvcolii</span>Â (<span id=\"A2.T2.4.4.4.4.3\" class=\"ltx_ERROR undefined\">\\csvcoliii</span>)  <span id=\"A2.T2.4.4.4.4.4\" class=\"ltx_ERROR undefined\">\\csvcoliv</span>Â (<span id=\"A2.T2.4.4.4.4.5\" class=\"ltx_ERROR undefined\">\\csvcolv</span>)  <span id=\"A2.T2.4.4.4.4.6\" class=\"ltx_ERROR undefined\">\\csvcolvi</span>Â (<span id=\"A2.T2.4.4.4.4.7\" class=\"ltx_ERROR undefined\">\\csvcolvii</span>)  <span id=\"A2.T2.4.4.4.4.8\" class=\"ltx_ERROR undefined\">\\csvcolviii</span>Â (<span id=\"A2.T2.4.4.4.4.9\" class=\"ltx_ERROR undefined\">\\csvcolix</span>)  <span id=\"A2.T2.4.4.4.4.10\" class=\"ltx_ERROR undefined\">\\csvcolx</span>Â (<span id=\"A2.T2.4.4.4.4.11\" class=\"ltx_ERROR undefined\">\\csvcolxi</span>)</span></span></p>\n</span></div>\n\n",
        "footnotes": "[tabular=ca*2e*2d,table head=     Race     Sex  \nAlgorithm  Accuracy  Î”Eâ€‹OsuperscriptÎ”ğ¸ğ‘‚\\Delta^{EO}  Î”Dâ€‹PsuperscriptÎ”ğ·ğ‘ƒ\\Delta^{DP} Î”Eâ€‹OsuperscriptÎ”ğ¸ğ‘‚\\Delta^{EO}  Î”Dâ€‹PsuperscriptÎ”ğ·ğ‘ƒ\\Delta^{DP} \n, table foot=]data/data_other_fl.csv\n \\csvcoli \\csvcoliiÂ (\\csvcoliii)  \\csvcolivÂ (\\csvcolv)  \\csvcolviÂ (\\csvcolvii)  \\csvcolviiiÂ (\\csvcolix)  \\csvcolxÂ (\\csvcolxi)",
        "references": [
            "TableÂ 2 shows results on other FL algorithms, including FedNovaÂ [Wang etÂ al., 2020], ScaffoldÂ Karimireddy etÂ al. [2020b], FedOptÂ [Asad etÂ al., 2020], FedProxÂ Li etÂ al. [2020], and MimeÂ [Karimireddy etÂ al., 2020a].\nWe find that when the FL model achieves higher accuracy than standalone models, the fairness gap of the FL model can be higher than that of standalone models. This observation is consistent across multiple FL algorithms. This strong evidence implies that the improvement of accuracy in FL can come at the cost of fairness. In addition, this is not unique to only the FedAvg algorithm."
        ]
    },
    "A3.T3": {
        "caption": "Table 3: Effect of Fair FLÂ [Abay etÂ al., 2020] - (Income). The standard deviation across five runs is indicated between parentheses. The \"Sensitive Attribute\" column indicates the sensitive attribute used in the reweighting algorithm.",
        "table": "<div id=\"A3.T3.4\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:433.6pt;height:40.8pt;vertical-align:-31.8pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(36.1pt,-0.7pt) scale(1.19959219035754,1.19959219035754) ;\"><span id=\"A3.T3.4.5\" class=\"ltx_ERROR undefined\">\\csvreader</span>\n<p id=\"A3.T3.4.4\" class=\"ltx_p\">[tabular=cca*2e*2d,table head=     <span id=\"A3.T3.4.4.4\" class=\"ltx_text\" style=\"background-color:#F6FCF2;\"> Race     <span id=\"A3.T3.4.4.4.4\" class=\"ltx_text\" style=\"background-color:#F5FAFC;\">Sex  \n<br class=\"ltx_break\">Algorithm  Sensitive Attribute  Accuracy  <math id=\"A3.T3.1.1.1.1.m1.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{EO}\" display=\"inline\"><semantics id=\"A3.T3.1.1.1.1.m1.1a\"><msup id=\"A3.T3.1.1.1.1.m1.1.1\" xref=\"A3.T3.1.1.1.1.m1.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A3.T3.1.1.1.1.m1.1.1.2\" xref=\"A3.T3.1.1.1.1.m1.1.1.2.cmml\">Î”</mi><mrow id=\"A3.T3.1.1.1.1.m1.1.1.3\" xref=\"A3.T3.1.1.1.1.m1.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.1.1.1.1.m1.1.1.3.2\" xref=\"A3.T3.1.1.1.1.m1.1.1.3.2.cmml\">E</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A3.T3.1.1.1.1.m1.1.1.3.1\" xref=\"A3.T3.1.1.1.1.m1.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.1.1.1.1.m1.1.1.3.3\" xref=\"A3.T3.1.1.1.1.m1.1.1.3.3.cmml\">O</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A3.T3.1.1.1.1.m1.1b\"><apply id=\"A3.T3.1.1.1.1.m1.1.1.cmml\" xref=\"A3.T3.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T3.1.1.1.1.m1.1.1.1.cmml\" xref=\"A3.T3.1.1.1.1.m1.1.1\">superscript</csymbol><ci id=\"A3.T3.1.1.1.1.m1.1.1.2.cmml\" xref=\"A3.T3.1.1.1.1.m1.1.1.2\">Î”</ci><apply id=\"A3.T3.1.1.1.1.m1.1.1.3.cmml\" xref=\"A3.T3.1.1.1.1.m1.1.1.3\"><times id=\"A3.T3.1.1.1.1.m1.1.1.3.1.cmml\" xref=\"A3.T3.1.1.1.1.m1.1.1.3.1\"></times><ci id=\"A3.T3.1.1.1.1.m1.1.1.3.2.cmml\" xref=\"A3.T3.1.1.1.1.m1.1.1.3.2\">ğ¸</ci><ci id=\"A3.T3.1.1.1.1.m1.1.1.3.3.cmml\" xref=\"A3.T3.1.1.1.1.m1.1.1.3.3\">ğ‘‚</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T3.1.1.1.1.m1.1c\">\\Delta^{EO}</annotation></semantics></math>  <math id=\"A3.T3.2.2.2.2.m2.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{DP}\" display=\"inline\"><semantics id=\"A3.T3.2.2.2.2.m2.1a\"><msup id=\"A3.T3.2.2.2.2.m2.1.1\" xref=\"A3.T3.2.2.2.2.m2.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A3.T3.2.2.2.2.m2.1.1.2\" xref=\"A3.T3.2.2.2.2.m2.1.1.2.cmml\">Î”</mi><mrow id=\"A3.T3.2.2.2.2.m2.1.1.3\" xref=\"A3.T3.2.2.2.2.m2.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.2.2.2.2.m2.1.1.3.2\" xref=\"A3.T3.2.2.2.2.m2.1.1.3.2.cmml\">D</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A3.T3.2.2.2.2.m2.1.1.3.1\" xref=\"A3.T3.2.2.2.2.m2.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.2.2.2.2.m2.1.1.3.3\" xref=\"A3.T3.2.2.2.2.m2.1.1.3.3.cmml\">P</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A3.T3.2.2.2.2.m2.1b\"><apply id=\"A3.T3.2.2.2.2.m2.1.1.cmml\" xref=\"A3.T3.2.2.2.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T3.2.2.2.2.m2.1.1.1.cmml\" xref=\"A3.T3.2.2.2.2.m2.1.1\">superscript</csymbol><ci id=\"A3.T3.2.2.2.2.m2.1.1.2.cmml\" xref=\"A3.T3.2.2.2.2.m2.1.1.2\">Î”</ci><apply id=\"A3.T3.2.2.2.2.m2.1.1.3.cmml\" xref=\"A3.T3.2.2.2.2.m2.1.1.3\"><times id=\"A3.T3.2.2.2.2.m2.1.1.3.1.cmml\" xref=\"A3.T3.2.2.2.2.m2.1.1.3.1\"></times><ci id=\"A3.T3.2.2.2.2.m2.1.1.3.2.cmml\" xref=\"A3.T3.2.2.2.2.m2.1.1.3.2\">ğ·</ci><ci id=\"A3.T3.2.2.2.2.m2.1.1.3.3.cmml\" xref=\"A3.T3.2.2.2.2.m2.1.1.3.3\">ğ‘ƒ</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T3.2.2.2.2.m2.1c\">\\Delta^{DP}</annotation></semantics></math> <math id=\"A3.T3.3.3.3.3.m3.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{EO}\" display=\"inline\"><semantics id=\"A3.T3.3.3.3.3.m3.1a\"><msup id=\"A3.T3.3.3.3.3.m3.1.1\" xref=\"A3.T3.3.3.3.3.m3.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A3.T3.3.3.3.3.m3.1.1.2\" xref=\"A3.T3.3.3.3.3.m3.1.1.2.cmml\">Î”</mi><mrow id=\"A3.T3.3.3.3.3.m3.1.1.3\" xref=\"A3.T3.3.3.3.3.m3.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.3.3.3.3.m3.1.1.3.2\" xref=\"A3.T3.3.3.3.3.m3.1.1.3.2.cmml\">E</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A3.T3.3.3.3.3.m3.1.1.3.1\" xref=\"A3.T3.3.3.3.3.m3.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.3.3.3.3.m3.1.1.3.3\" xref=\"A3.T3.3.3.3.3.m3.1.1.3.3.cmml\">O</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A3.T3.3.3.3.3.m3.1b\"><apply id=\"A3.T3.3.3.3.3.m3.1.1.cmml\" xref=\"A3.T3.3.3.3.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T3.3.3.3.3.m3.1.1.1.cmml\" xref=\"A3.T3.3.3.3.3.m3.1.1\">superscript</csymbol><ci id=\"A3.T3.3.3.3.3.m3.1.1.2.cmml\" xref=\"A3.T3.3.3.3.3.m3.1.1.2\">Î”</ci><apply id=\"A3.T3.3.3.3.3.m3.1.1.3.cmml\" xref=\"A3.T3.3.3.3.3.m3.1.1.3\"><times id=\"A3.T3.3.3.3.3.m3.1.1.3.1.cmml\" xref=\"A3.T3.3.3.3.3.m3.1.1.3.1\"></times><ci id=\"A3.T3.3.3.3.3.m3.1.1.3.2.cmml\" xref=\"A3.T3.3.3.3.3.m3.1.1.3.2\">ğ¸</ci><ci id=\"A3.T3.3.3.3.3.m3.1.1.3.3.cmml\" xref=\"A3.T3.3.3.3.3.m3.1.1.3.3\">ğ‘‚</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T3.3.3.3.3.m3.1c\">\\Delta^{EO}</annotation></semantics></math>  <math id=\"A3.T3.4.4.4.4.m4.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{DP}\" display=\"inline\"><semantics id=\"A3.T3.4.4.4.4.m4.1a\"><msup id=\"A3.T3.4.4.4.4.m4.1.1\" xref=\"A3.T3.4.4.4.4.m4.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A3.T3.4.4.4.4.m4.1.1.2\" xref=\"A3.T3.4.4.4.4.m4.1.1.2.cmml\">Î”</mi><mrow id=\"A3.T3.4.4.4.4.m4.1.1.3\" xref=\"A3.T3.4.4.4.4.m4.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.4.4.4.4.m4.1.1.3.2\" xref=\"A3.T3.4.4.4.4.m4.1.1.3.2.cmml\">D</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A3.T3.4.4.4.4.m4.1.1.3.1\" xref=\"A3.T3.4.4.4.4.m4.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.4.4.4.4.m4.1.1.3.3\" xref=\"A3.T3.4.4.4.4.m4.1.1.3.3.cmml\">P</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A3.T3.4.4.4.4.m4.1b\"><apply id=\"A3.T3.4.4.4.4.m4.1.1.cmml\" xref=\"A3.T3.4.4.4.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T3.4.4.4.4.m4.1.1.1.cmml\" xref=\"A3.T3.4.4.4.4.m4.1.1\">superscript</csymbol><ci id=\"A3.T3.4.4.4.4.m4.1.1.2.cmml\" xref=\"A3.T3.4.4.4.4.m4.1.1.2\">Î”</ci><apply id=\"A3.T3.4.4.4.4.m4.1.1.3.cmml\" xref=\"A3.T3.4.4.4.4.m4.1.1.3\"><times id=\"A3.T3.4.4.4.4.m4.1.1.3.1.cmml\" xref=\"A3.T3.4.4.4.4.m4.1.1.3.1\"></times><ci id=\"A3.T3.4.4.4.4.m4.1.1.3.2.cmml\" xref=\"A3.T3.4.4.4.4.m4.1.1.3.2\">ğ·</ci><ci id=\"A3.T3.4.4.4.4.m4.1.1.3.3.cmml\" xref=\"A3.T3.4.4.4.4.m4.1.1.3.3\">ğ‘ƒ</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T3.4.4.4.4.m4.1c\">\\Delta^{DP}</annotation></semantics></math> \n<br class=\"ltx_break\">, table foot=]data/data_reweiging.csv\n <span id=\"A3.T3.4.4.4.4.1\" class=\"ltx_ERROR undefined\">\\csvcoli</span> <span id=\"A3.T3.4.4.4.4.2\" class=\"ltx_ERROR undefined\">\\csvcolxii</span><span id=\"A3.T3.4.4.4.4.3\" class=\"ltx_ERROR undefined\">\\csvcolii</span>Â (<span id=\"A3.T3.4.4.4.4.4\" class=\"ltx_ERROR undefined\">\\csvcoliii</span>)  <span id=\"A3.T3.4.4.4.4.5\" class=\"ltx_ERROR undefined\">\\csvcoliv</span>Â (<span id=\"A3.T3.4.4.4.4.6\" class=\"ltx_ERROR undefined\">\\csvcolv</span>)  <span id=\"A3.T3.4.4.4.4.7\" class=\"ltx_ERROR undefined\">\\csvcolvi</span>Â (<span id=\"A3.T3.4.4.4.4.8\" class=\"ltx_ERROR undefined\">\\csvcolvii</span>)  <span id=\"A3.T3.4.4.4.4.9\" class=\"ltx_ERROR undefined\">\\csvcolviii</span>Â (<span id=\"A3.T3.4.4.4.4.10\" class=\"ltx_ERROR undefined\">\\csvcolix</span>)  <span id=\"A3.T3.4.4.4.4.11\" class=\"ltx_ERROR undefined\">\\csvcolx</span>Â (<span id=\"A3.T3.4.4.4.4.12\" class=\"ltx_ERROR undefined\">\\csvcolxi</span>)</span></span></p>\n</span></div>\n\n",
        "footnotes": "[tabular=cca*2e*2d,table head=      Race     Sex  \nAlgorithm  Sensitive Attribute  Accuracy  Î”Eâ€‹OsuperscriptÎ”ğ¸ğ‘‚\\Delta^{EO}  Î”Dâ€‹PsuperscriptÎ”ğ·ğ‘ƒ\\Delta^{DP} Î”Eâ€‹OsuperscriptÎ”ğ¸ğ‘‚\\Delta^{EO}  Î”Dâ€‹PsuperscriptÎ”ğ·ğ‘ƒ\\Delta^{DP} \n, table foot=]data/data_reweiging.csv\n \\csvcoli \\csvcolxii\\csvcoliiÂ (\\csvcoliii)  \\csvcolivÂ (\\csvcolv)  \\csvcolviÂ (\\csvcolvii)  \\csvcolviiiÂ (\\csvcolix)  \\csvcolxÂ (\\csvcolxi)",
        "references": [
            "TableÂ 3 presents the results for a fair FL algorithm proposed by Abay etÂ al. [2020], which uses a reweighting algorithm proposed in the centralized settingÂ [Kamiran and Calders, 2012]. The algorithm involves assigning weights to each subgroup (defined by the label and a sensitive attribute) before FL, based on the local or global training dataset. This is called \"local reweighting\" or \"global reweighting,\" respectively. During FL, parties update the FL model to minimize the weighted loss. The table presents the accuracy and fairness gap of the FL algorithm using local and global reweighting for the Income, Health, and Employment datasets.",
            "When improving fairness with respect to one sensitive attribute, the fairness issue with respect to another sensitive attribute may worsen, as shown inÂ TableÂ 3. For instance, when applying local reweighing to reduce bias with respect to the \"Sex\" attribute, the fairness gap with respect to \"Race\" increased from 0.514 (results on FedAvg) to 0.521, indicating that reweighing to improve fairness with respect to Sex may amplify the bias with respect to Race. This implies that the bias with respect to Race can still propagate in FL, and our analysis remains valid."
        ]
    },
    "A4.T4": {
        "caption": "Table 4: Effect of FL on the real-world medical dataset - (ISIC2019). The standard deviation across four runs is indicated between parentheses. ",
        "table": "<div id=\"A4.T4.4\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:433.6pt;height:62.9pt;vertical-align:-54.8pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(16.9pt,-0.3pt) scale(1.0845000976968,1.0845000976968) ;\"><span id=\"A4.T4.4.5\" class=\"ltx_ERROR undefined\">\\csvreader</span>\n<p id=\"A4.T4.4.4\" class=\"ltx_p\">[tabular=c*2a*2e*2d*2f,table head= Setting   <span id=\"A4.T4.4.4.4\" class=\"ltx_text\" style=\"background-color:#FFF3F3;\"> Party 1    <span id=\"A4.T4.4.4.4.4\" class=\"ltx_text\" style=\"background-color:#F6FCF2;\"> Party 2     <span id=\"A4.T4.4.4.4.4.4\" class=\"ltx_text\" style=\"background-color:#F5FAFC;\"> Party 3     <span id=\"A4.T4.4.4.4.4.4.4\" class=\"ltx_text\" style=\"background-color:#FFF9F8;\"> Party 4 \n<br class=\"ltx_break\">Algorithm  Accuracy  <math id=\"A4.T4.1.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\Delta^{Acc}\" display=\"inline\"><semantics id=\"A4.T4.1.1.1.1.1.1.m1.1a\"><msup id=\"A4.T4.1.1.1.1.1.1.m1.1.1\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.cmml\"><mi mathbackground=\"#FFF9F8\" mathvariant=\"normal\" id=\"A4.T4.1.1.1.1.1.1.m1.1.1.2\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.2.cmml\">Î”</mi><mrow id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.cmml\"><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.2\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.1\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.3\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.3.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.1a\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.4\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.4.cmml\">c</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A4.T4.1.1.1.1.1.1.m1.1b\"><apply id=\"A4.T4.1.1.1.1.1.1.m1.1.1.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A4.T4.1.1.1.1.1.1.m1.1.1.1.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1\">superscript</csymbol><ci id=\"A4.T4.1.1.1.1.1.1.m1.1.1.2.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.2\">Î”</ci><apply id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3\"><times id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.1.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.1\"></times><ci id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.2.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.2\">ğ´</ci><ci id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.3.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.3\">ğ‘</ci><ci id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.4.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.4\">ğ‘</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T4.1.1.1.1.1.1.m1.1c\">\\Delta^{Acc}</annotation></semantics></math>  Accuracy  <math id=\"A4.T4.2.2.2.2.2.2.m2.1\" class=\"ltx_Math\" alttext=\"\\Delta^{Acc}\" display=\"inline\"><semantics id=\"A4.T4.2.2.2.2.2.2.m2.1a\"><msup id=\"A4.T4.2.2.2.2.2.2.m2.1.1\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.cmml\"><mi mathbackground=\"#FFF9F8\" mathvariant=\"normal\" id=\"A4.T4.2.2.2.2.2.2.m2.1.1.2\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.2.cmml\">Î”</mi><mrow id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.cmml\"><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.2\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.1\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.3\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.3.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.1a\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.4\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.4.cmml\">c</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A4.T4.2.2.2.2.2.2.m2.1b\"><apply id=\"A4.T4.2.2.2.2.2.2.m2.1.1.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"A4.T4.2.2.2.2.2.2.m2.1.1.1.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1\">superscript</csymbol><ci id=\"A4.T4.2.2.2.2.2.2.m2.1.1.2.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.2\">Î”</ci><apply id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3\"><times id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.1.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.1\"></times><ci id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.2.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.2\">ğ´</ci><ci id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.3.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.3\">ğ‘</ci><ci id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.4.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.4\">ğ‘</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T4.2.2.2.2.2.2.m2.1c\">\\Delta^{Acc}</annotation></semantics></math>  Accuracy  <math id=\"A4.T4.3.3.3.3.3.3.m3.1\" class=\"ltx_Math\" alttext=\"\\Delta^{Acc}\" display=\"inline\"><semantics id=\"A4.T4.3.3.3.3.3.3.m3.1a\"><msup id=\"A4.T4.3.3.3.3.3.3.m3.1.1\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.cmml\"><mi mathbackground=\"#FFF9F8\" mathvariant=\"normal\" id=\"A4.T4.3.3.3.3.3.3.m3.1.1.2\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.2.cmml\">Î”</mi><mrow id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.cmml\"><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.2\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.1\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.3\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.3.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.1a\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.4\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.4.cmml\">c</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A4.T4.3.3.3.3.3.3.m3.1b\"><apply id=\"A4.T4.3.3.3.3.3.3.m3.1.1.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"A4.T4.3.3.3.3.3.3.m3.1.1.1.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1\">superscript</csymbol><ci id=\"A4.T4.3.3.3.3.3.3.m3.1.1.2.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.2\">Î”</ci><apply id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3\"><times id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.1.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.1\"></times><ci id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.2.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.2\">ğ´</ci><ci id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.3.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.3\">ğ‘</ci><ci id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.4.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.4\">ğ‘</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T4.3.3.3.3.3.3.m3.1c\">\\Delta^{Acc}</annotation></semantics></math>  Accuracy  <math id=\"A4.T4.4.4.4.4.4.4.m4.1\" class=\"ltx_Math\" alttext=\"\\Delta^{Acc}\" display=\"inline\"><semantics id=\"A4.T4.4.4.4.4.4.4.m4.1a\"><msup id=\"A4.T4.4.4.4.4.4.4.m4.1.1\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.cmml\"><mi mathbackground=\"#FFF9F8\" mathvariant=\"normal\" id=\"A4.T4.4.4.4.4.4.4.m4.1.1.2\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.2.cmml\">Î”</mi><mrow id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.cmml\"><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.2\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.1\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.3\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.3.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.1a\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.1.cmml\">â€‹</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.4\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.4.cmml\">c</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A4.T4.4.4.4.4.4.4.m4.1b\"><apply id=\"A4.T4.4.4.4.4.4.4.m4.1.1.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"A4.T4.4.4.4.4.4.4.m4.1.1.1.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1\">superscript</csymbol><ci id=\"A4.T4.4.4.4.4.4.4.m4.1.1.2.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.2\">Î”</ci><apply id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3\"><times id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.1.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.1\"></times><ci id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.2.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.2\">ğ´</ci><ci id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.3.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.3\">ğ‘</ci><ci id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.4.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.4\">ğ‘</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T4.4.4.4.4.4.4.m4.1c\">\\Delta^{Acc}</annotation></semantics></math> \n<br class=\"ltx_break\">, table foot=]data/isic2019.csv\n <span id=\"A4.T4.4.4.4.4.4.4.1\" class=\"ltx_ERROR undefined\">\\csvcolii</span> <span id=\"A4.T4.4.4.4.4.4.4.2\" class=\"ltx_ERROR undefined\">\\csvcoliii</span>Â (<span id=\"A4.T4.4.4.4.4.4.4.3\" class=\"ltx_ERROR undefined\">\\csvcoliv</span>)  <span id=\"A4.T4.4.4.4.4.4.4.4\" class=\"ltx_ERROR undefined\">\\csvcolv</span>Â (<span id=\"A4.T4.4.4.4.4.4.4.5\" class=\"ltx_ERROR undefined\">\\csvcolvi</span>)  <span id=\"A4.T4.4.4.4.4.4.4.6\" class=\"ltx_ERROR undefined\">\\csvcolvii</span>Â (<span id=\"A4.T4.4.4.4.4.4.4.7\" class=\"ltx_ERROR undefined\">\\csvcolviii</span>)  <span id=\"A4.T4.4.4.4.4.4.4.8\" class=\"ltx_ERROR undefined\">\\csvcolix</span>Â (<span id=\"A4.T4.4.4.4.4.4.4.9\" class=\"ltx_ERROR undefined\">\\csvcolx</span>)  <span id=\"A4.T4.4.4.4.4.4.4.10\" class=\"ltx_ERROR undefined\">\\csvcolxi</span>Â (<span id=\"A4.T4.4.4.4.4.4.4.11\" class=\"ltx_ERROR undefined\">\\csvcolxii</span>)  <span id=\"A4.T4.4.4.4.4.4.4.12\" class=\"ltx_ERROR undefined\">\\csvcolxiii</span>Â (<span id=\"A4.T4.4.4.4.4.4.4.13\" class=\"ltx_ERROR undefined\">\\csvcolxiv</span>)  <span id=\"A4.T4.4.4.4.4.4.4.14\" class=\"ltx_ERROR undefined\">\\csvcolxv</span>Â (<span id=\"A4.T4.4.4.4.4.4.4.15\" class=\"ltx_ERROR undefined\">\\csvcolxvi</span>)  <span id=\"A4.T4.4.4.4.4.4.4.16\" class=\"ltx_ERROR undefined\">\\csvcolxvii</span>Â (<span id=\"A4.T4.4.4.4.4.4.4.17\" class=\"ltx_ERROR undefined\">\\csvcolxviii</span>)</span></span></span></span></p>\n</span></div>\n\n",
        "footnotes": "[tabular=c*2a*2e*2d*2f,table head= Setting    Party 1     Party 2      Party 3      Party 4 \nAlgorithm  Accuracy  Î”Aâ€‹câ€‹csuperscriptÎ”ğ´ğ‘ğ‘\\Delta^{Acc}  Accuracy  Î”Aâ€‹câ€‹csuperscriptÎ”ğ´ğ‘ğ‘\\Delta^{Acc}  Accuracy  Î”Aâ€‹câ€‹csuperscriptÎ”ğ´ğ‘ğ‘\\Delta^{Acc}  Accuracy  Î”Aâ€‹câ€‹csuperscriptÎ”ğ´ğ‘ğ‘\\Delta^{Acc} \n, table foot=]data/isic2019.csv\n \\csvcolii \\csvcoliiiÂ (\\csvcoliv)  \\csvcolvÂ (\\csvcolvi)  \\csvcolviiÂ (\\csvcolviii)  \\csvcolixÂ (\\csvcolx)  \\csvcolxiÂ (\\csvcolxii)  \\csvcolxiiiÂ (\\csvcolxiv)  \\csvcolxvÂ (\\csvcolxvi)  \\csvcolxviiÂ (\\csvcolxviii)",
        "references": [
            "We evaluate the effect of FL on local fairness on a real-world medical dataset, ISIC2019Â [Abay etÂ al., 2020, Tschandl etÂ al., 2018, Combalia etÂ al., 2019], which contains dermoscopic images of skin lesions collected from six medical centers. The task is to classify dermoscopic images among nine different diagnostic categories. We filter out medical centers with less than 1900 images and regard each medical center with over 1900 images as a party (4 centers remain after filtering). For every party, we randomly select 1500 and 400 images for training and validation, respectively. We regard the binary notion of Sex as our sensitive attribute. In this multi-class and medical dataset setting, we measure the bias (fairness gap) based on the accuracy gap across groups. Following Â [OgierÂ du Terrail etÂ al., 2022]Â 222https://github.com/owkin/FLamby, we train EfficientNetsÂ [Tan and Le, 2019], the state-of-the-art model structure for medical data, under the centralized, FL, and standalone setting. The accuracy and fairness gap for each client in different settings are shown in TableÂ 4."
        ]
    }
}