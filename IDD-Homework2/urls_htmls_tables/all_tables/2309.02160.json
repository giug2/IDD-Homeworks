{
    "PAPER'S NUMBER OF TABLES": 4,
    "S3.SS2.7": {
        "caption": "Table 1: Average benefit of collaboration and FL. The lowest accuracy and highest fairness gap are bold. The standard deviation across five runs is indicated between parentheses. The green arrows (red arrows) represent the positive (negative) impacts of FL or centralized learning compared to standalone training (i.e., FL or centralized training increases the accuracy or decreases the fairness gap compared to standalone training). ",
        "table": "<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<div id=\"S3.SS2.2.2\" class=\"ltx_inline-block ltx_figure_panel ltx_transformed_outer\" style=\"width:433.6pt;height:50pt;vertical-align:-40.5pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(17.5pt,-0.4pt) scale(1.08790103394876,1.08790103394876) ;\"><span id=\"S3.SS2.2.2.3\" class=\"ltx_ERROR undefined\">\\csvreader</span>\n<p id=\"S3.SS2.2.2.2\" class=\"ltx_p\">[tabular=c*3a*3e*3d,table head=   <span id=\"S3.SS2.2.2.2.2\" class=\"ltx_text\" style=\"background-color:#FFF6F6;\"> Accuracy    <span id=\"S3.SS2.2.2.2.2.2\" class=\"ltx_text\" style=\"background-color:#F6FCF2;\"> <math id=\"S3.SS2.1.1.1.1.1.m1.1\" class=\"ltx_Math\" style=\"background-color:#F6FCF2;\" alttext=\"\\Delta^{EO}\" display=\"inline\"><semantics id=\"S3.SS2.1.1.1.1.1.m1.1a\"><msup id=\"S3.SS2.1.1.1.1.1.m1.1.1\" xref=\"S3.SS2.1.1.1.1.1.m1.1.1.cmml\"><mi mathbackground=\"#F6FCF2\" mathvariant=\"normal\" id=\"S3.SS2.1.1.1.1.1.m1.1.1.2\" xref=\"S3.SS2.1.1.1.1.1.m1.1.1.2.cmml\">Δ</mi><mrow id=\"S3.SS2.1.1.1.1.1.m1.1.1.3\" xref=\"S3.SS2.1.1.1.1.1.m1.1.1.3.cmml\"><mi mathbackground=\"#F6FCF2\" id=\"S3.SS2.1.1.1.1.1.m1.1.1.3.2\" xref=\"S3.SS2.1.1.1.1.1.m1.1.1.3.2.cmml\">E</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.SS2.1.1.1.1.1.m1.1.1.3.1\" xref=\"S3.SS2.1.1.1.1.1.m1.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#F6FCF2\" id=\"S3.SS2.1.1.1.1.1.m1.1.1.3.3\" xref=\"S3.SS2.1.1.1.1.1.m1.1.1.3.3.cmml\">O</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS2.1.1.1.1.1.m1.1b\"><apply id=\"S3.SS2.1.1.1.1.1.m1.1.1.cmml\" xref=\"S3.SS2.1.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS2.1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S3.SS2.1.1.1.1.1.m1.1.1\">superscript</csymbol><ci id=\"S3.SS2.1.1.1.1.1.m1.1.1.2.cmml\" xref=\"S3.SS2.1.1.1.1.1.m1.1.1.2\">Δ</ci><apply id=\"S3.SS2.1.1.1.1.1.m1.1.1.3.cmml\" xref=\"S3.SS2.1.1.1.1.1.m1.1.1.3\"><times id=\"S3.SS2.1.1.1.1.1.m1.1.1.3.1.cmml\" xref=\"S3.SS2.1.1.1.1.1.m1.1.1.3.1\"></times><ci id=\"S3.SS2.1.1.1.1.1.m1.1.1.3.2.cmml\" xref=\"S3.SS2.1.1.1.1.1.m1.1.1.3.2\">𝐸</ci><ci id=\"S3.SS2.1.1.1.1.1.m1.1.1.3.3.cmml\" xref=\"S3.SS2.1.1.1.1.1.m1.1.1.3.3\">𝑂</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS2.1.1.1.1.1.m1.1c\">\\Delta^{EO}</annotation></semantics></math>    <math id=\"S3.SS2.2.2.2.2.2.m2.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{DP}\" display=\"inline\"><semantics id=\"S3.SS2.2.2.2.2.2.m2.1a\"><msup id=\"S3.SS2.2.2.2.2.2.m2.1.1\" xref=\"S3.SS2.2.2.2.2.2.m2.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"S3.SS2.2.2.2.2.2.m2.1.1.2\" xref=\"S3.SS2.2.2.2.2.2.m2.1.1.2.cmml\">Δ</mi><mrow id=\"S3.SS2.2.2.2.2.2.m2.1.1.3\" xref=\"S3.SS2.2.2.2.2.2.m2.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS2.2.2.2.2.2.m2.1.1.3.2\" xref=\"S3.SS2.2.2.2.2.2.m2.1.1.3.2.cmml\">D</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.SS2.2.2.2.2.2.m2.1.1.3.1\" xref=\"S3.SS2.2.2.2.2.2.m2.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#F5FAFC\" id=\"S3.SS2.2.2.2.2.2.m2.1.1.3.3\" xref=\"S3.SS2.2.2.2.2.2.m2.1.1.3.3.cmml\">P</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS2.2.2.2.2.2.m2.1b\"><apply id=\"S3.SS2.2.2.2.2.2.m2.1.1.cmml\" xref=\"S3.SS2.2.2.2.2.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS2.2.2.2.2.2.m2.1.1.1.cmml\" xref=\"S3.SS2.2.2.2.2.2.m2.1.1\">superscript</csymbol><ci id=\"S3.SS2.2.2.2.2.2.m2.1.1.2.cmml\" xref=\"S3.SS2.2.2.2.2.2.m2.1.1.2\">Δ</ci><apply id=\"S3.SS2.2.2.2.2.2.m2.1.1.3.cmml\" xref=\"S3.SS2.2.2.2.2.2.m2.1.1.3\"><times id=\"S3.SS2.2.2.2.2.2.m2.1.1.3.1.cmml\" xref=\"S3.SS2.2.2.2.2.2.m2.1.1.3.1\"></times><ci id=\"S3.SS2.2.2.2.2.2.m2.1.1.3.2.cmml\" xref=\"S3.SS2.2.2.2.2.2.m2.1.1.3.2\">𝐷</ci><ci id=\"S3.SS2.2.2.2.2.2.m2.1.1.3.3.cmml\" xref=\"S3.SS2.2.2.2.2.2.m2.1.1.3.3\">𝑃</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS2.2.2.2.2.2.m2.1c\">\\Delta^{DP}</annotation></semantics></math><span id=\"S3.SS2.2.2.2.2.2.1\" class=\"ltx_text\" style=\"background-color:#F5FAFC;\">  \n<br class=\"ltx_break\">Dataset  Standalone  Centralized  FedAvg  Standalone  Centralized  FedAvg  Standalone  Centralized  FedAvg \n<br class=\"ltx_break\">, table foot=]data/data_benefit.csv\n<span id=\"S3.SS2.2.2.2.2.2.1.1\" class=\"ltx_ERROR undefined\">\\csvcoli</span>- <span id=\"S3.SS2.2.2.2.2.2.1.2\" class=\"ltx_ERROR undefined\">\\csvcolii</span></span></span></span></p>\n</span></div>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p id=\"S3.SS2.7.7\" class=\"ltx_p ltx_figure_panel\"><span id=\"S3.SS2.7.7.5\" class=\"ltx_text\" style=\"background-color:#F5FAFC;\">Table <a href=\"#S3.SS2\" title=\"3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a> presents the average accuracy and fairness gaps of the centralized model, FL model, and standalone models on local datasets. Centralized training is observed to improve accuracy and fairness at the same time for parties. For instance, on the Income dataset, centralized training improves the accuracy by <math id=\"S3.SS2.3.3.1.m1.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"6\\%\" display=\"inline\"><semantics id=\"S3.SS2.3.3.1.m1.1a\"><mrow id=\"S3.SS2.3.3.1.m1.1.1\" xref=\"S3.SS2.3.3.1.m1.1.1.cmml\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS2.3.3.1.m1.1.1.2\" xref=\"S3.SS2.3.3.1.m1.1.1.2.cmml\">6</mn><mo mathbackground=\"#F5FAFC\" id=\"S3.SS2.3.3.1.m1.1.1.1\" xref=\"S3.SS2.3.3.1.m1.1.1.1.cmml\">%</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS2.3.3.1.m1.1b\"><apply id=\"S3.SS2.3.3.1.m1.1.1.cmml\" xref=\"S3.SS2.3.3.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS2.3.3.1.m1.1.1.1.cmml\" xref=\"S3.SS2.3.3.1.m1.1.1.1\">percent</csymbol><cn type=\"integer\" id=\"S3.SS2.3.3.1.m1.1.1.2.cmml\" xref=\"S3.SS2.3.3.1.m1.1.1.2\">6</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS2.3.3.1.m1.1c\">6\\%</annotation></semantics></math> and reduces the fairness gap across racial groups by <math id=\"S3.SS2.4.4.2.m2.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"5.5\\%\" display=\"inline\"><semantics id=\"S3.SS2.4.4.2.m2.1a\"><mrow id=\"S3.SS2.4.4.2.m2.1.1\" xref=\"S3.SS2.4.4.2.m2.1.1.cmml\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS2.4.4.2.m2.1.1.2\" xref=\"S3.SS2.4.4.2.m2.1.1.2.cmml\">5.5</mn><mo mathbackground=\"#F5FAFC\" id=\"S3.SS2.4.4.2.m2.1.1.1\" xref=\"S3.SS2.4.4.2.m2.1.1.1.cmml\">%</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS2.4.4.2.m2.1b\"><apply id=\"S3.SS2.4.4.2.m2.1.1.cmml\" xref=\"S3.SS2.4.4.2.m2.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS2.4.4.2.m2.1.1.1.cmml\" xref=\"S3.SS2.4.4.2.m2.1.1.1\">percent</csymbol><cn type=\"float\" id=\"S3.SS2.4.4.2.m2.1.1.2.cmml\" xref=\"S3.SS2.4.4.2.m2.1.1.2\">5.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS2.4.4.2.m2.1c\">5.5\\%</annotation></semantics></math> with respect to equalized odds and by <math id=\"S3.SS2.5.5.3.m3.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"5.5\\%\" display=\"inline\"><semantics id=\"S3.SS2.5.5.3.m3.1a\"><mrow id=\"S3.SS2.5.5.3.m3.1.1\" xref=\"S3.SS2.5.5.3.m3.1.1.cmml\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS2.5.5.3.m3.1.1.2\" xref=\"S3.SS2.5.5.3.m3.1.1.2.cmml\">5.5</mn><mo mathbackground=\"#F5FAFC\" id=\"S3.SS2.5.5.3.m3.1.1.1\" xref=\"S3.SS2.5.5.3.m3.1.1.1.cmml\">%</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS2.5.5.3.m3.1b\"><apply id=\"S3.SS2.5.5.3.m3.1.1.cmml\" xref=\"S3.SS2.5.5.3.m3.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS2.5.5.3.m3.1.1.1.cmml\" xref=\"S3.SS2.5.5.3.m3.1.1.1\">percent</csymbol><cn type=\"float\" id=\"S3.SS2.5.5.3.m3.1.1.2.cmml\" xref=\"S3.SS2.5.5.3.m3.1.1.2\">5.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS2.5.5.3.m3.1c\">5.5\\%</annotation></semantics></math> with respect to demographic parity. However, FL may not achieve the same benefit as centralized training in terms of fairness and can even exacerbate the fairness issue for parties. For example, on the Income dataset, the EO fairness gap of the FL model for the sex groups increases by <math id=\"S3.SS2.6.6.4.m4.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"35\\%\" display=\"inline\"><semantics id=\"S3.SS2.6.6.4.m4.1a\"><mrow id=\"S3.SS2.6.6.4.m4.1.1\" xref=\"S3.SS2.6.6.4.m4.1.1.cmml\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS2.6.6.4.m4.1.1.2\" xref=\"S3.SS2.6.6.4.m4.1.1.2.cmml\">35</mn><mo mathbackground=\"#F5FAFC\" id=\"S3.SS2.6.6.4.m4.1.1.1\" xref=\"S3.SS2.6.6.4.m4.1.1.1.cmml\">%</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS2.6.6.4.m4.1b\"><apply id=\"S3.SS2.6.6.4.m4.1.1.cmml\" xref=\"S3.SS2.6.6.4.m4.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS2.6.6.4.m4.1.1.1.cmml\" xref=\"S3.SS2.6.6.4.m4.1.1.1\">percent</csymbol><cn type=\"integer\" id=\"S3.SS2.6.6.4.m4.1.1.2.cmml\" xref=\"S3.SS2.6.6.4.m4.1.1.2\">35</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS2.6.6.4.m4.1c\">35\\%</annotation></semantics></math>, and the DP fairness gap increases by <math id=\"S3.SS2.7.7.5.m5.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"29.9\\%\" display=\"inline\"><semantics id=\"S3.SS2.7.7.5.m5.1a\"><mrow id=\"S3.SS2.7.7.5.m5.1.1\" xref=\"S3.SS2.7.7.5.m5.1.1.cmml\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS2.7.7.5.m5.1.1.2\" xref=\"S3.SS2.7.7.5.m5.1.1.2.cmml\">29.9</mn><mo mathbackground=\"#F5FAFC\" id=\"S3.SS2.7.7.5.m5.1.1.1\" xref=\"S3.SS2.7.7.5.m5.1.1.1.cmml\">%</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS2.7.7.5.m5.1b\"><apply id=\"S3.SS2.7.7.5.m5.1.1.cmml\" xref=\"S3.SS2.7.7.5.m5.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS2.7.7.5.m5.1.1.1.cmml\" xref=\"S3.SS2.7.7.5.m5.1.1.1\">percent</csymbol><cn type=\"float\" id=\"S3.SS2.7.7.5.m5.1.1.2.cmml\" xref=\"S3.SS2.7.7.5.m5.1.1.2\">29.9</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS2.7.7.5.m5.1c\">29.9\\%</annotation></semantics></math>, compared to standalone models. We include results for other popular FL algorithms, including FedNova <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et al., <a href=\"#bib.bib46\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>, Scaffold <cite class=\"ltx_cite ltx_citemacro_cite\">Karimireddy et al. (<a href=\"#bib.bib24\" title=\"\" class=\"ltx_ref\">2020b</a>)</cite>, FedOpt <cite class=\"ltx_cite ltx_citemacro_citep\">(Reddi et al., <a href=\"#bib.bib40\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>, FedProx <cite class=\"ltx_cite ltx_citemacro_cite\">Li et al. (<a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>, and Mime <cite class=\"ltx_cite ltx_citemacro_citep\">(Karimireddy et al., <a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">2020a</a>)</cite>, in Table <a href=\"#A2.T2\" title=\"Table 2 ‣ B.5 Effect of FL algorithm ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> in Appendix <a href=\"#A2\" title=\"Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>, which show a similar pattern to that of the FedAvg algorithm. Appendix <a href=\"#A2\" title=\"Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> provides more detailed results on other FL algorithms.\nThe centralized and FL models are trained on the same dataset. The difference between the FL model and the centralized model in terms of fairness suggests FL algorithm can introduce more bias compared to standard training. Therefore, the explanation of how bias is introduced during standard training in the centralized setting may not fully explain how FL introduces bias. In the following, we further explore how FL impacts fairness from the party level.</span></p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<section id=\"S3.SS3\" class=\"ltx_subsection ltx_figure_panel\" style=\"background-color:#F5FAFC;\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.3 </span>FL propagates bias among parties</h3>\n\n<div id=\"S3.SS3.p1\" class=\"ltx_para\">\n<p id=\"S3.SS3.p1.1\" class=\"ltx_p\"><span id=\"S3.SS3.p1.1.1\" class=\"ltx_text ltx_font_bold\">Disparate impact of FL on group fairness across parties. </span>\nFigure <a href=\"#S3.F1\" title=\"Figure 1 ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> illustrates the benefit of FL on fairness and accuracy for parties. We observe that FL improves accuracy for almost all parties, and the variance in accuracy improvement across parties is small. On the other hand, the fairness benefit of FL is negative for most parties. It implies that most parties obtain a more biased model in FL compared to standalone training. Furthermore, we notice that the variance in the fairness benefits across parties is large, suggesting that although all parties have the same global model in FL, they do not benefit from the FL model equally (each party evaluates the model performance on their local test dataset).</p>\n</div>\n<figure id=\"S3.F1\" class=\"ltx_figure ltx_align_floatright\"><img src=\"/html/2309.02160/assets/images/main_png/income_benefit_box_all.png\" id=\"S3.F1.1.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"209\" height=\"130\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">Figure 1: </span><span id=\"S3.F1.3.1\" class=\"ltx_text ltx_font_bold\">Accuracy and Fairness Benefit of FL - Income (Sex).</span> The benefit of FL is the increase in accuracy or reduction in the fairness gap of the FL model compared to standalone training.\n</figcaption>\n</figure>\n<div id=\"S3.SS3.p2\" class=\"ltx_para ltx_noindent\">\n<p id=\"S3.SS3.p2.1\" class=\"ltx_p\">To explore the impact of FL on fairness at the party level, Figure <a href=\"#S3.F2\" title=\"Figure 2 ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows strong correlations between the fairness benefit a party obtains in FL and the fairness gap of the standalone model for the party, i.e., the bias level of the party. This finding highlights the disparate impact of FL on fairness: FL can improve fairness for more biased parties but at the cost of worsening the issue for less biased parties.</p>\n</div>\n<figure id=\"S3.F2\" class=\"ltx_figure\"><img src=\"/html/2309.02160/assets/images/main_png/income_bias_propagation.png\" id=\"S3.F2.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"129\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 2: </span><span id=\"S3.F2.4.1\" class=\"ltx_text ltx_font_bold\">Correlation between the fairness gap of the standalone model and the benefit of FL - Income.</span> The x-axis shows the fairness gap of the standalone model, and the y-axis shows the fairness benefit of FL, which is the fairness gap of the standalone model subtracted by that of the FL model (defined in Section <a href=\"#S2\" title=\"2 Background ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>). The Pearson correlation coefficients between the fairness gap of the parties’ standalone models and the fairness benefit they obtain from FL are presented. The p-value for all settings is smaller than <math id=\"S3.F2.2.m1.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"0.0001\" display=\"inline\"><semantics id=\"S3.F2.2.m1.1b\"><mn mathbackground=\"#F5FAFC\" id=\"S3.F2.2.m1.1.1\" xref=\"S3.F2.2.m1.1.1.cmml\">0.0001</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.F2.2.m1.1c\"><cn type=\"float\" id=\"S3.F2.2.m1.1.1.cmml\" xref=\"S3.F2.2.m1.1.1\">0.0001</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.F2.2.m1.1d\">0.0001</annotation></semantics></math>.\n</figcaption>\n</figure>\n<figure id=\"S3.F3\" class=\"ltx_figure\"><img src=\"/html/2309.02160/assets/images/main_png/income_effect_local_agg.png\" id=\"S3.F3.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"479\" height=\"198\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 3: </span><span id=\"S3.F3.2.1\" class=\"ltx_text ltx_font_bold\">Dynamic of fairness gap during the training - Income (Sex, DP). </span> Figure shows the fairness gap of the global model and locally updated model for the most biased party and least biased party in the first 30 rounds (Figure <a href=\"#A2.F16\" title=\"Figure 16 ‣ B.2 Aggregation contradicts with local update ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> in Appendix <a href=\"#A2\" title=\"Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> shows the results for all training rounds.). The most (least) biased party has the highest (lowest) fairness gap in the standalone setting. The fairness gap in the standalone setting for those parties is shown in the title.\n</figcaption>\n</figure>\n<div id=\"S3.SS3.p3\" class=\"ltx_para ltx_noindent\">\n<p id=\"S3.SS3.p3.1\" class=\"ltx_p\"><span id=\"S3.SS3.p3.1.1\" class=\"ltx_text ltx_font_bold\">Contradiction between aggregation and local update.</span>\nDuring the training process of FL, we observe that aggregation and local update contradict each other, shown in Figure <a href=\"#S3.F3\" title=\"Figure 3 ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. Local update from the least biased party, whose standalone model has the lowest fairness gap, reduces the fairness gap of the model. However, this reduction is eliminated by the aggregation step. Conversely, the local update from the most biased party increases the fairness gap of the model, which is then reduced by aggregation. This finding implies that the aggregation contributes to the disparate impact of FL on fairness, improving fairness for more biased parties but worsening the fairness for less biased parties compared to the standalone setting.</p>\n</div>\n<figure id=\"S3.F4\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\"><img src=\"/html/2309.02160/assets/images/income/propagation_path/overall_attr_1_metric_d_dp.png\" id=\"S3.F4.g1\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" width=\"240\" height=\"120\" alt=\"Refer to caption\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\"><img src=\"/html/2309.02160/assets/images/income/propagation_path/overall_attr_0_metric_d_dp.png\" id=\"S3.F4.g2\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" width=\"240\" height=\"122\" alt=\"Refer to caption\"></div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 4: </span><span id=\"S3.F4.2.1\" class=\"ltx_text ltx_font_bold\">Influence graphs - Income (DP). </span> The figure shows the most influential pairs of parties (with respect to demographic parity), with nodes on the top representing parties that influence other parties and the nodes on the bottom representing parties that are influenced by others. The number inside the node represents the party index, and the color of the node represents the fairness gap in the standalone setting. Green edges (resp. red edges) connect pairs of parties where the top party positively (negatively) influences the bottom party. We show the top 5 pairs with maximal positive influence and the top 5 pairs with maximal negative influence. </figcaption>\n</figure>\n<figure id=\"S3.F6\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure id=\"S3.F6.1\" class=\"ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle\" style=\"width:208.1pt;\"><img src=\"/html/2309.02160/assets/images/main_png/income_influence_standalone_dp.png\" id=\"S3.F6.1.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"598\" height=\"247\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">Figure 5: </span><span id=\"S3.F6.1.2.1\" class=\"ltx_text ltx_font_bold\">Correlation between the influence and standalone bias - Income (DP).</span> The y-axis represents the average influence of each party. The Pearson correlation coefficient between the fairness gap a party obtains in the standalone setting and the impact she has on local fairness for parties in FL are shown in the figure.\n</figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure id=\"S3.F6.2\" class=\"ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle\" style=\"width:208.1pt;\"><img src=\"/html/2309.02160/assets/images/main_png/income_cum_influence.png\" id=\"S3.F6.2.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"598\" height=\"241\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 6: </span><span id=\"S3.F6.2.2.1\" class=\"ltx_text ltx_font_bold\">Cumulative influence - Income (DP). </span> The y-axis shows the cumulative influence of each party on all parties up until the current round. The results for the top five most biased parties (parties with the highest fairness gap) and the top five least biased parties (parties with the lowest fairness gap) are presented. </figcaption>\n</figure>\n</div>\n</div>\n</figure>\n<div id=\"S3.SS3.p4\" class=\"ltx_para ltx_noindent\">\n<p id=\"S3.SS3.p4.12\" class=\"ltx_p\"><span id=\"S3.SS3.p4.12.1\" class=\"ltx_text ltx_font_bold\">Biased parties negatively influence other parties via aggregation throughout the training. </span>\nWhy does aggregation have disparate impacts on local fairness for parties? Specifically, we ask which party’s local update causes the increase (or decrease) of the fairness gap for other parties via aggregation. To answer this question, we look into the influence of a party’s local update on other parties’ fairness via aggregation. More precisely, we compute the influence of party <math id=\"S3.SS3.p4.1.m1.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"i\" display=\"inline\"><semantics id=\"S3.SS3.p4.1.m1.1a\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.1.m1.1.1\" xref=\"S3.SS3.p4.1.m1.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p4.1.m1.1b\"><ci id=\"S3.SS3.p4.1.m1.1.1.cmml\" xref=\"S3.SS3.p4.1.m1.1.1\">𝑖</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p4.1.m1.1c\">i</annotation></semantics></math> on party <math id=\"S3.SS3.p4.2.m2.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"j\" display=\"inline\"><semantics id=\"S3.SS3.p4.2.m2.1a\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.2.m2.1.1\" xref=\"S3.SS3.p4.2.m2.1.1.cmml\">j</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p4.2.m2.1b\"><ci id=\"S3.SS3.p4.2.m2.1.1.cmml\" xref=\"S3.SS3.p4.2.m2.1.1\">𝑗</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p4.2.m2.1c\">j</annotation></semantics></math> as the fairness gap increase when party <math id=\"S3.SS3.p4.3.m3.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"i\" display=\"inline\"><semantics id=\"S3.SS3.p4.3.m3.1a\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.3.m3.1.1\" xref=\"S3.SS3.p4.3.m3.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p4.3.m3.1b\"><ci id=\"S3.SS3.p4.3.m3.1.1.cmml\" xref=\"S3.SS3.p4.3.m3.1.1\">𝑖</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p4.3.m3.1c\">i</annotation></semantics></math>’s local update is removed from the aggregation in each round <math id=\"S3.SS3.p4.4.m4.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"t\" display=\"inline\"><semantics id=\"S3.SS3.p4.4.m4.1a\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.4.m4.1.1\" xref=\"S3.SS3.p4.4.m4.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p4.4.m4.1b\"><ci id=\"S3.SS3.p4.4.m4.1.1.cmml\" xref=\"S3.SS3.p4.4.m4.1.1\">𝑡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p4.4.m4.1c\">t</annotation></semantics></math> and sum over all the training rounds. Formally, we define the influence of party <math id=\"S3.SS3.p4.5.m5.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"i\" display=\"inline\"><semantics id=\"S3.SS3.p4.5.m5.1a\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.5.m5.1.1\" xref=\"S3.SS3.p4.5.m5.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p4.5.m5.1b\"><ci id=\"S3.SS3.p4.5.m5.1.1.cmml\" xref=\"S3.SS3.p4.5.m5.1.1\">𝑖</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p4.5.m5.1c\">i</annotation></semantics></math> on party <math id=\"S3.SS3.p4.6.m6.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"j\" display=\"inline\"><semantics id=\"S3.SS3.p4.6.m6.1a\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.6.m6.1.1\" xref=\"S3.SS3.p4.6.m6.1.1.cmml\">j</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p4.6.m6.1b\"><ci id=\"S3.SS3.p4.6.m6.1.1.cmml\" xref=\"S3.SS3.p4.6.m6.1.1\">𝑗</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p4.6.m6.1c\">j</annotation></semantics></math> as <math id=\"S3.SS3.p4.7.m7.8\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"I_{i,j}=\\sum_{t=1}^{T}\\Delta(\\theta_{t,-i},D_{j})-\\Delta(\\theta_{t},D_{j})\" display=\"inline\"><semantics id=\"S3.SS3.p4.7.m7.8a\"><mrow id=\"S3.SS3.p4.7.m7.8.8\" xref=\"S3.SS3.p4.7.m7.8.8.cmml\"><msub id=\"S3.SS3.p4.7.m7.8.8.6\" xref=\"S3.SS3.p4.7.m7.8.8.6.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.8.8.6.2\" xref=\"S3.SS3.p4.7.m7.8.8.6.2.cmml\">I</mi><mrow id=\"S3.SS3.p4.7.m7.2.2.2.4\" xref=\"S3.SS3.p4.7.m7.2.2.2.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.1.1.1.1\" xref=\"S3.SS3.p4.7.m7.1.1.1.1.cmml\">i</mi><mo mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.2.2.2.4.1\" xref=\"S3.SS3.p4.7.m7.2.2.2.3.cmml\">,</mo><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.2.2.2.2\" xref=\"S3.SS3.p4.7.m7.2.2.2.2.cmml\">j</mi></mrow></msub><mo mathbackground=\"#F5FAFC\" rspace=\"0.111em\" id=\"S3.SS3.p4.7.m7.8.8.5\" xref=\"S3.SS3.p4.7.m7.8.8.5.cmml\">=</mo><mrow id=\"S3.SS3.p4.7.m7.8.8.4\" xref=\"S3.SS3.p4.7.m7.8.8.4.cmml\"><mrow id=\"S3.SS3.p4.7.m7.6.6.2.2\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.cmml\"><msubsup id=\"S3.SS3.p4.7.m7.6.6.2.2.3\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3.cmml\"><mo mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.2\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.2.cmml\">∑</mo><mrow id=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3.2\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3.2.cmml\">t</mi><mo mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3.1\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3.1.cmml\">=</mo><mn mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3.3\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3.3.cmml\">1</mn></mrow><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.6.6.2.2.3.3\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3.3.cmml\">T</mi></msubsup><mrow id=\"S3.SS3.p4.7.m7.6.6.2.2.2\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"S3.SS3.p4.7.m7.6.6.2.2.2.4\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.4.cmml\">Δ</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.SS3.p4.7.m7.6.6.2.2.2.3\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.3.cmml\">​</mo><mrow id=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.3.cmml\"><mo mathbackground=\"#F5FAFC\" stretchy=\"false\" id=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.3\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.3.cmml\">(</mo><msub id=\"S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1\" xref=\"S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1.2\" xref=\"S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1.2.cmml\">θ</mi><mrow id=\"S3.SS3.p4.7.m7.4.4.2.2\" xref=\"S3.SS3.p4.7.m7.4.4.2.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.3.3.1.1\" xref=\"S3.SS3.p4.7.m7.3.3.1.1.cmml\">t</mi><mo mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.4.4.2.2.2\" xref=\"S3.SS3.p4.7.m7.4.4.2.3.cmml\">,</mo><mrow id=\"S3.SS3.p4.7.m7.4.4.2.2.1\" xref=\"S3.SS3.p4.7.m7.4.4.2.2.1.cmml\"><mo mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.4.4.2.2.1a\" xref=\"S3.SS3.p4.7.m7.4.4.2.2.1.cmml\">−</mo><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.4.4.2.2.1.2\" xref=\"S3.SS3.p4.7.m7.4.4.2.2.1.2.cmml\">i</mi></mrow></mrow></msub><mo mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.4\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.3.cmml\">,</mo><msub id=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.2\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.2.cmml\">D</mi><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.3\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.3.cmml\">j</mi></msub><mo mathbackground=\"#F5FAFC\" stretchy=\"false\" id=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.5\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.3.cmml\">)</mo></mrow></mrow></mrow><mo mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.8.8.4.5\" xref=\"S3.SS3.p4.7.m7.8.8.4.5.cmml\">−</mo><mrow id=\"S3.SS3.p4.7.m7.8.8.4.4\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"S3.SS3.p4.7.m7.8.8.4.4.4\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.4.cmml\">Δ</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S3.SS3.p4.7.m7.8.8.4.4.3\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.3.cmml\">​</mo><mrow id=\"S3.SS3.p4.7.m7.8.8.4.4.2.2\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.2.3.cmml\"><mo mathbackground=\"#F5FAFC\" stretchy=\"false\" id=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.3\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.2.3.cmml\">(</mo><msub id=\"S3.SS3.p4.7.m7.7.7.3.3.1.1.1\" xref=\"S3.SS3.p4.7.m7.7.7.3.3.1.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.7.7.3.3.1.1.1.2\" xref=\"S3.SS3.p4.7.m7.7.7.3.3.1.1.1.2.cmml\">θ</mi><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.7.7.3.3.1.1.1.3\" xref=\"S3.SS3.p4.7.m7.7.7.3.3.1.1.1.3.cmml\">t</mi></msub><mo mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.4\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.2.3.cmml\">,</mo><msub id=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.2\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.2.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.2.2\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.2.2.cmml\">D</mi><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.2.3\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.2.3.cmml\">j</mi></msub><mo mathbackground=\"#F5FAFC\" stretchy=\"false\" id=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.5\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.2.3.cmml\">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p4.7.m7.8b\"><apply id=\"S3.SS3.p4.7.m7.8.8.cmml\" xref=\"S3.SS3.p4.7.m7.8.8\"><eq id=\"S3.SS3.p4.7.m7.8.8.5.cmml\" xref=\"S3.SS3.p4.7.m7.8.8.5\"></eq><apply id=\"S3.SS3.p4.7.m7.8.8.6.cmml\" xref=\"S3.SS3.p4.7.m7.8.8.6\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p4.7.m7.8.8.6.1.cmml\" xref=\"S3.SS3.p4.7.m7.8.8.6\">subscript</csymbol><ci id=\"S3.SS3.p4.7.m7.8.8.6.2.cmml\" xref=\"S3.SS3.p4.7.m7.8.8.6.2\">𝐼</ci><list id=\"S3.SS3.p4.7.m7.2.2.2.3.cmml\" xref=\"S3.SS3.p4.7.m7.2.2.2.4\"><ci id=\"S3.SS3.p4.7.m7.1.1.1.1.cmml\" xref=\"S3.SS3.p4.7.m7.1.1.1.1\">𝑖</ci><ci id=\"S3.SS3.p4.7.m7.2.2.2.2.cmml\" xref=\"S3.SS3.p4.7.m7.2.2.2.2\">𝑗</ci></list></apply><apply id=\"S3.SS3.p4.7.m7.8.8.4.cmml\" xref=\"S3.SS3.p4.7.m7.8.8.4\"><minus id=\"S3.SS3.p4.7.m7.8.8.4.5.cmml\" xref=\"S3.SS3.p4.7.m7.8.8.4.5\"></minus><apply id=\"S3.SS3.p4.7.m7.6.6.2.2.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2\"><apply id=\"S3.SS3.p4.7.m7.6.6.2.2.3.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p4.7.m7.6.6.2.2.3.1.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3\">superscript</csymbol><apply id=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.1.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3\">subscript</csymbol><sum id=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.2.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.2\"></sum><apply id=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3\"><eq id=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3.1.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3.1\"></eq><ci id=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3.2.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3.2\">𝑡</ci><cn type=\"integer\" id=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3.3.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3.2.3.3\">1</cn></apply></apply><ci id=\"S3.SS3.p4.7.m7.6.6.2.2.3.3.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.3.3\">𝑇</ci></apply><apply id=\"S3.SS3.p4.7.m7.6.6.2.2.2.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2\"><times id=\"S3.SS3.p4.7.m7.6.6.2.2.2.3.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.3\"></times><ci id=\"S3.SS3.p4.7.m7.6.6.2.2.2.4.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.4\">Δ</ci><interval closure=\"open\" id=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.3.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2\"><apply id=\"S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1.cmml\" xref=\"S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1.1.cmml\" xref=\"S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1\">subscript</csymbol><ci id=\"S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1.2.cmml\" xref=\"S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1.2\">𝜃</ci><list id=\"S3.SS3.p4.7.m7.4.4.2.3.cmml\" xref=\"S3.SS3.p4.7.m7.4.4.2.2\"><ci id=\"S3.SS3.p4.7.m7.3.3.1.1.cmml\" xref=\"S3.SS3.p4.7.m7.3.3.1.1\">𝑡</ci><apply id=\"S3.SS3.p4.7.m7.4.4.2.2.1.cmml\" xref=\"S3.SS3.p4.7.m7.4.4.2.2.1\"><minus id=\"S3.SS3.p4.7.m7.4.4.2.2.1.1.cmml\" xref=\"S3.SS3.p4.7.m7.4.4.2.2.1\"></minus><ci id=\"S3.SS3.p4.7.m7.4.4.2.2.1.2.cmml\" xref=\"S3.SS3.p4.7.m7.4.4.2.2.1.2\">𝑖</ci></apply></list></apply><apply id=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.1.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2\">subscript</csymbol><ci id=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.2.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.2\">𝐷</ci><ci id=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.3.cmml\" xref=\"S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.3\">𝑗</ci></apply></interval></apply></apply><apply id=\"S3.SS3.p4.7.m7.8.8.4.4.cmml\" xref=\"S3.SS3.p4.7.m7.8.8.4.4\"><times id=\"S3.SS3.p4.7.m7.8.8.4.4.3.cmml\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.3\"></times><ci id=\"S3.SS3.p4.7.m7.8.8.4.4.4.cmml\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.4\">Δ</ci><interval closure=\"open\" id=\"S3.SS3.p4.7.m7.8.8.4.4.2.3.cmml\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.2.2\"><apply id=\"S3.SS3.p4.7.m7.7.7.3.3.1.1.1.cmml\" xref=\"S3.SS3.p4.7.m7.7.7.3.3.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p4.7.m7.7.7.3.3.1.1.1.1.cmml\" xref=\"S3.SS3.p4.7.m7.7.7.3.3.1.1.1\">subscript</csymbol><ci id=\"S3.SS3.p4.7.m7.7.7.3.3.1.1.1.2.cmml\" xref=\"S3.SS3.p4.7.m7.7.7.3.3.1.1.1.2\">𝜃</ci><ci id=\"S3.SS3.p4.7.m7.7.7.3.3.1.1.1.3.cmml\" xref=\"S3.SS3.p4.7.m7.7.7.3.3.1.1.1.3\">𝑡</ci></apply><apply id=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.2.cmml\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.2.1.cmml\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.2\">subscript</csymbol><ci id=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.2.2.cmml\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.2.2\">𝐷</ci><ci id=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.2.3.cmml\" xref=\"S3.SS3.p4.7.m7.8.8.4.4.2.2.2.3\">𝑗</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p4.7.m7.8c\">I_{i,j}=\\sum_{t=1}^{T}\\Delta(\\theta_{t,-i},D_{j})-\\Delta(\\theta_{t},D_{j})</annotation></semantics></math>, where <math id=\"S3.SS3.p4.8.m8.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\theta_{t}\" display=\"inline\"><semantics id=\"S3.SS3.p4.8.m8.1a\"><msub id=\"S3.SS3.p4.8.m8.1.1\" xref=\"S3.SS3.p4.8.m8.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.8.m8.1.1.2\" xref=\"S3.SS3.p4.8.m8.1.1.2.cmml\">θ</mi><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.8.m8.1.1.3\" xref=\"S3.SS3.p4.8.m8.1.1.3.cmml\">t</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p4.8.m8.1b\"><apply id=\"S3.SS3.p4.8.m8.1.1.cmml\" xref=\"S3.SS3.p4.8.m8.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p4.8.m8.1.1.1.cmml\" xref=\"S3.SS3.p4.8.m8.1.1\">subscript</csymbol><ci id=\"S3.SS3.p4.8.m8.1.1.2.cmml\" xref=\"S3.SS3.p4.8.m8.1.1.2\">𝜃</ci><ci id=\"S3.SS3.p4.8.m8.1.1.3.cmml\" xref=\"S3.SS3.p4.8.m8.1.1.3\">𝑡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p4.8.m8.1c\">\\theta_{t}</annotation></semantics></math> is the global model (i.e., the aggregated model overall local updated models) and <math id=\"S3.SS3.p4.9.m9.2\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\theta_{t,-i}\" display=\"inline\"><semantics id=\"S3.SS3.p4.9.m9.2a\"><msub id=\"S3.SS3.p4.9.m9.2.3\" xref=\"S3.SS3.p4.9.m9.2.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.9.m9.2.3.2\" xref=\"S3.SS3.p4.9.m9.2.3.2.cmml\">θ</mi><mrow id=\"S3.SS3.p4.9.m9.2.2.2.2\" xref=\"S3.SS3.p4.9.m9.2.2.2.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.9.m9.1.1.1.1\" xref=\"S3.SS3.p4.9.m9.1.1.1.1.cmml\">t</mi><mo mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.9.m9.2.2.2.2.2\" xref=\"S3.SS3.p4.9.m9.2.2.2.3.cmml\">,</mo><mrow id=\"S3.SS3.p4.9.m9.2.2.2.2.1\" xref=\"S3.SS3.p4.9.m9.2.2.2.2.1.cmml\"><mo mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.9.m9.2.2.2.2.1a\" xref=\"S3.SS3.p4.9.m9.2.2.2.2.1.cmml\">−</mo><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.9.m9.2.2.2.2.1.2\" xref=\"S3.SS3.p4.9.m9.2.2.2.2.1.2.cmml\">i</mi></mrow></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p4.9.m9.2b\"><apply id=\"S3.SS3.p4.9.m9.2.3.cmml\" xref=\"S3.SS3.p4.9.m9.2.3\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p4.9.m9.2.3.1.cmml\" xref=\"S3.SS3.p4.9.m9.2.3\">subscript</csymbol><ci id=\"S3.SS3.p4.9.m9.2.3.2.cmml\" xref=\"S3.SS3.p4.9.m9.2.3.2\">𝜃</ci><list id=\"S3.SS3.p4.9.m9.2.2.2.3.cmml\" xref=\"S3.SS3.p4.9.m9.2.2.2.2\"><ci id=\"S3.SS3.p4.9.m9.1.1.1.1.cmml\" xref=\"S3.SS3.p4.9.m9.1.1.1.1\">𝑡</ci><apply id=\"S3.SS3.p4.9.m9.2.2.2.2.1.cmml\" xref=\"S3.SS3.p4.9.m9.2.2.2.2.1\"><minus id=\"S3.SS3.p4.9.m9.2.2.2.2.1.1.cmml\" xref=\"S3.SS3.p4.9.m9.2.2.2.2.1\"></minus><ci id=\"S3.SS3.p4.9.m9.2.2.2.2.1.2.cmml\" xref=\"S3.SS3.p4.9.m9.2.2.2.2.1.2\">𝑖</ci></apply></list></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p4.9.m9.2c\">\\theta_{t,-i}</annotation></semantics></math> is the aggregated model over local updated models from parties excluding party <math id=\"S3.SS3.p4.10.m10.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"i\" display=\"inline\"><semantics id=\"S3.SS3.p4.10.m10.1a\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.10.m10.1.1\" xref=\"S3.SS3.p4.10.m10.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p4.10.m10.1b\"><ci id=\"S3.SS3.p4.10.m10.1.1.cmml\" xref=\"S3.SS3.p4.10.m10.1.1\">𝑖</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p4.10.m10.1c\">i</annotation></semantics></math>. If party <math id=\"S3.SS3.p4.11.m11.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"i\" display=\"inline\"><semantics id=\"S3.SS3.p4.11.m11.1a\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.11.m11.1.1\" xref=\"S3.SS3.p4.11.m11.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p4.11.m11.1b\"><ci id=\"S3.SS3.p4.11.m11.1.1.cmml\" xref=\"S3.SS3.p4.11.m11.1.1\">𝑖</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p4.11.m11.1c\">i</annotation></semantics></math> improves fairness for party <math id=\"S3.SS3.p4.12.m12.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"j\" display=\"inline\"><semantics id=\"S3.SS3.p4.12.m12.1a\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p4.12.m12.1.1\" xref=\"S3.SS3.p4.12.m12.1.1.cmml\">j</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p4.12.m12.1b\"><ci id=\"S3.SS3.p4.12.m12.1.1.cmml\" xref=\"S3.SS3.p4.12.m12.1.1\">𝑗</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p4.12.m12.1c\">j</annotation></semantics></math>, the influence is positive and vice versa. We compute the influence for all pairs of parties, and the most influential pairs are shown in Figure <a href=\"#S3.F4\" title=\"Figure 4 ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. We observe that a less biased party has a positive influence on fairness for other parties, while a more biased party has a negative influence on other parties. This result shows that a biased party can negatively influence other parties’ fairness via aggregation throughout the training.</p>\n</div>\n<div id=\"S3.SS3.p5\" class=\"ltx_para ltx_noindent\">\n<p id=\"S3.SS3.p5.1\" class=\"ltx_p\">Furthermore, we investigate the relationship between a party’s bias (i.e., the bias of the party’s standalone model) and its average influence on all parties’ fairness (i.e., <math id=\"S3.SS3.p5.1.m1.2\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\sum_{k=1}^{K}I_{i,k}/K\" display=\"inline\"><semantics id=\"S3.SS3.p5.1.m1.2a\"><mrow id=\"S3.SS3.p5.1.m1.2.3\" xref=\"S3.SS3.p5.1.m1.2.3.cmml\"><msubsup id=\"S3.SS3.p5.1.m1.2.3.1\" xref=\"S3.SS3.p5.1.m1.2.3.1.cmml\"><mo mathbackground=\"#F5FAFC\" id=\"S3.SS3.p5.1.m1.2.3.1.2.2\" xref=\"S3.SS3.p5.1.m1.2.3.1.2.2.cmml\">∑</mo><mrow id=\"S3.SS3.p5.1.m1.2.3.1.2.3\" xref=\"S3.SS3.p5.1.m1.2.3.1.2.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p5.1.m1.2.3.1.2.3.2\" xref=\"S3.SS3.p5.1.m1.2.3.1.2.3.2.cmml\">k</mi><mo mathbackground=\"#F5FAFC\" id=\"S3.SS3.p5.1.m1.2.3.1.2.3.1\" xref=\"S3.SS3.p5.1.m1.2.3.1.2.3.1.cmml\">=</mo><mn mathbackground=\"#F5FAFC\" id=\"S3.SS3.p5.1.m1.2.3.1.2.3.3\" xref=\"S3.SS3.p5.1.m1.2.3.1.2.3.3.cmml\">1</mn></mrow><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p5.1.m1.2.3.1.3\" xref=\"S3.SS3.p5.1.m1.2.3.1.3.cmml\">K</mi></msubsup><mrow id=\"S3.SS3.p5.1.m1.2.3.2\" xref=\"S3.SS3.p5.1.m1.2.3.2.cmml\"><msub id=\"S3.SS3.p5.1.m1.2.3.2.2\" xref=\"S3.SS3.p5.1.m1.2.3.2.2.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p5.1.m1.2.3.2.2.2\" xref=\"S3.SS3.p5.1.m1.2.3.2.2.2.cmml\">I</mi><mrow id=\"S3.SS3.p5.1.m1.2.2.2.4\" xref=\"S3.SS3.p5.1.m1.2.2.2.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p5.1.m1.1.1.1.1\" xref=\"S3.SS3.p5.1.m1.1.1.1.1.cmml\">i</mi><mo mathbackground=\"#F5FAFC\" id=\"S3.SS3.p5.1.m1.2.2.2.4.1\" xref=\"S3.SS3.p5.1.m1.2.2.2.3.cmml\">,</mo><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p5.1.m1.2.2.2.2\" xref=\"S3.SS3.p5.1.m1.2.2.2.2.cmml\">k</mi></mrow></msub><mo mathbackground=\"#F5FAFC\" id=\"S3.SS3.p5.1.m1.2.3.2.1\" xref=\"S3.SS3.p5.1.m1.2.3.2.1.cmml\">/</mo><mi mathbackground=\"#F5FAFC\" id=\"S3.SS3.p5.1.m1.2.3.2.3\" xref=\"S3.SS3.p5.1.m1.2.3.2.3.cmml\">K</mi></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS3.p5.1.m1.2b\"><apply id=\"S3.SS3.p5.1.m1.2.3.cmml\" xref=\"S3.SS3.p5.1.m1.2.3\"><apply id=\"S3.SS3.p5.1.m1.2.3.1.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p5.1.m1.2.3.1.1.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.1\">superscript</csymbol><apply id=\"S3.SS3.p5.1.m1.2.3.1.2.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.1\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p5.1.m1.2.3.1.2.1.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.1\">subscript</csymbol><sum id=\"S3.SS3.p5.1.m1.2.3.1.2.2.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.1.2.2\"></sum><apply id=\"S3.SS3.p5.1.m1.2.3.1.2.3.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.1.2.3\"><eq id=\"S3.SS3.p5.1.m1.2.3.1.2.3.1.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.1.2.3.1\"></eq><ci id=\"S3.SS3.p5.1.m1.2.3.1.2.3.2.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.1.2.3.2\">𝑘</ci><cn type=\"integer\" id=\"S3.SS3.p5.1.m1.2.3.1.2.3.3.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.1.2.3.3\">1</cn></apply></apply><ci id=\"S3.SS3.p5.1.m1.2.3.1.3.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.1.3\">𝐾</ci></apply><apply id=\"S3.SS3.p5.1.m1.2.3.2.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.2\"><divide id=\"S3.SS3.p5.1.m1.2.3.2.1.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.2.1\"></divide><apply id=\"S3.SS3.p5.1.m1.2.3.2.2.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.2.2\"><csymbol cd=\"ambiguous\" id=\"S3.SS3.p5.1.m1.2.3.2.2.1.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.2.2\">subscript</csymbol><ci id=\"S3.SS3.p5.1.m1.2.3.2.2.2.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.2.2.2\">𝐼</ci><list id=\"S3.SS3.p5.1.m1.2.2.2.3.cmml\" xref=\"S3.SS3.p5.1.m1.2.2.2.4\"><ci id=\"S3.SS3.p5.1.m1.1.1.1.1.cmml\" xref=\"S3.SS3.p5.1.m1.1.1.1.1\">𝑖</ci><ci id=\"S3.SS3.p5.1.m1.2.2.2.2.cmml\" xref=\"S3.SS3.p5.1.m1.2.2.2.2\">𝑘</ci></list></apply><ci id=\"S3.SS3.p5.1.m1.2.3.2.3.cmml\" xref=\"S3.SS3.p5.1.m1.2.3.2.3\">𝐾</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS3.p5.1.m1.2c\">\\sum_{k=1}^{K}I_{i,k}/K</annotation></semantics></math>). The results are presented in Figure <a href=\"#S3.F6\" title=\"Figure 6 ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, which shows a strong correlation between these two factors. This finding further supports our conclusion that the more biased parties have a stronger negative influence on other parties’ fairness, while the less biased parties have a stronger positive influence.</p>\n</div>\n<div id=\"S3.SS3.p6\" class=\"ltx_para ltx_noindent\">\n<p id=\"S3.SS3.p6.1\" class=\"ltx_p\">Finally, we analyze the dynamics of the influence of the top 5 most biased parties (i.e., parties with the largest fairness gap in the standalone setting) and the top 5 least biased parties, as shown in Figure <a href=\"#S3.F6\" title=\"Figure 6 ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. We observe that the influence of the most biased parties is monotonically increasing throughout the training, while the influence of the least biased parties is decreasing. In other words, biased parties consistently have a negative influence on others’ fairness gaps, while less biased parties have a positive influence. These results suggest that FL can propagate bias among parties: the bias from biased parties negatively influence the fairness of other parties via aggregation throughout the training. Next, we will explore how the bias is propagated in FL.</p>\n</div>\n<figure id=\"S3.F7\" class=\"ltx_figure\"><img src=\"/html/2309.02160/assets/images/main_png/income_attribution_distribution.png\" id=\"S3.F7.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"170\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 7: </span><span id=\"S3.F7.2.1\" class=\"ltx_text ltx_font_bold\">Histogram of feature attribution value for the sensitive attribute - Income (Sex).</span> The average attribution value for the female group and male group is shown in the figure. The results are computed on all test data points over five different runs. We show the results for the most biased party with the highest fairness gap in the standalone setting.\n</figcaption>\n</figure>\n<section id=\"S3.SS4\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.4 </span>How is bias propagated in FL?</h3>\n\n<div id=\"S3.SS4.p1\" class=\"ltx_para\">\n<p id=\"S3.SS4.p1.1\" class=\"ltx_p\"><span id=\"S3.SS4.p1.1.1\" class=\"ltx_text ltx_font_bold\">Disparate treatment causes large fairness gaps.</span>\nOur first investigation explores what the bias represents, specifically whether the bias increase in FL is directly caused by the disparate treatment of the model among sensitive groups. To answer this question, we utilize Integrated Gradients <cite class=\"ltx_cite ltx_citemacro_citep\">(Sundararajan et al., <a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">2017</a>)</cite> to measure the attribution of each input feature to the models’ predictions with respect to the positive class.\nFigure <a href=\"#S3.F7\" title=\"Figure 7 ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> shows the attribution value distribution for the sensitive attribute \"Sex\" over individual test points from the female and male groups. We notice that the sex attribute has a large attribution value for the standalone model’s predictions and the FL model’s predictions. This finding implies that the predictions of those models are heavily dependent on the sensitive attribute of the test data. Moreover, the sensitive attribute affects the model predictions differently for the male and female groups, with the average attribution value being positive for the male group and negative for the female group. Furthermore, we find that there is minimal difference in the attribution value for other attributes with respect to the female and male groups (see Figure <a href=\"#A2.F19\" title=\"Figure 19 ‣ B.4 Attribution value for sensitive attribute ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> in Appendix <a href=\"#A2\" title=\"Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>). This indicates that the large fairness gap in the models is not caused by the distinct distribution of other insensitive attributes over protected groups; rather, it is mainly caused by the models’ disparate treatment of protected groups.</p>\n</div>\n<figure id=\"S3.F9\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure id=\"S3.F9.1\" class=\"ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle\" style=\"width:212.5pt;\"><img src=\"/html/2309.02160/assets/images/main_png/income_attribution_dynamic.png\" id=\"S3.F9.1.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"598\" height=\"245\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 8: </span><span id=\"S3.F9.1.2.1\" class=\"ltx_text ltx_font_bold\">Dynamic of absolute attribution value for the sensitive attribute - Income (Sex).</span> The results of different models are shown in different lines. We present the results for the most biased party and the least biased party.\n</figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure id=\"S3.F9.2\" class=\"ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle\" style=\"width:212.5pt;\"><img src=\"/html/2309.02160/assets/images/main_png/income_attribution_local_agg.png\" id=\"S3.F9.2.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"598\" height=\"245\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 9: </span><span id=\"S3.F9.2.2.1\" class=\"ltx_text ltx_font_bold\">Effect of local update and aggregation on the attribution value - Income (Sex).</span> We show the attribution value for sex attribute before and after aggregation for the most biased party and the least biased party during the training. </figcaption>\n</figure>\n</div>\n</div>\n</figure>\n<div id=\"S3.SS4.p2\" class=\"ltx_para ltx_noindent\">\n<p id=\"S3.SS4.p2.1\" class=\"ltx_p\"><span id=\"S3.SS4.p2.1.1\" class=\"ltx_text ltx_font_bold\">FL model learns more biased patterns. </span>\nIn Figure <a href=\"#S3.F7\" title=\"Figure 7 ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, we compare the attribution value of “Sex” for different models and find that, while the predictions of the FL model depend less heavily on the sensitive attribute compared to those of the standalone model, the dependence is still stronger than that of the centralized model. This suggests that the FL model learns a more biased pattern compared to what could be learned in the centralized setting.\nFigure <a href=\"#S3.F9\" title=\"Figure 9 ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows the dynamic of the average absolute feature attribution for “Sex” during the training. We find that standalone training increases the model’s dependence on the sensitive attribute throughout training for the most biased party, but centralized training decreases it. This suggests that collaboration through centralized training improves local fairness by guiding the model to learn less biased features. In FL, however, the absolute attribution value barely changes after 100 rounds and remains significantly greater than that in the centralized setting. This implies that the FL algorithm may introduce bias to the final model by inhibiting the model from learning less biased features.</p>\n</div>\n<div id=\"S3.SS4.p3\" class=\"ltx_para ltx_noindent\">\n<p id=\"S3.SS4.p3.1\" class=\"ltx_p\"><span id=\"S3.SS4.p3.1.1\" class=\"ltx_text ltx_font_bold\">Biased parties increase the model dependence on the sensitive attribute. </span>\nFigure <a href=\"#S3.F9\" title=\"Figure 9 ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> shows the attribution value of the aggregated model and locally updated model from the most biased party and least biased party. We observe that the biased party increases the global model dependence on sensitive attributes during the local update, and this increase persists throughout the training. In contrast, the least biased party reduces the dependence on the sensitive attribute, which is aligned with the trend of the fairness gap in Figure <a href=\"#S3.F3\" title=\"Figure 3 ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. This suggests that the biased parties have a negative impact on the fairness of other parties by increasing the model’s dependence on the sensitive attribute.</p>\n</div>\n<div id=\"S3.SS4.p4\" class=\"ltx_para\">\n<p id=\"S3.SS4.p4.1\" class=\"ltx_p\"><span id=\"S3.SS4.p4.1.1\" class=\"ltx_text ltx_font_bold\">Bias is encoded in a few parameters. </span></p>\n</div>\n<figure id=\"S3.F10\" class=\"ltx_figure\"><img src=\"/html/2309.02160/assets/images/main_png/income_effect_weights.png\" id=\"S3.F10.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"141\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 10: </span><span id=\"S3.F10.5.1\" class=\"ltx_text ltx_font_bold\">Effect of a few parameters on fairness - Income (Sex).</span> <span id=\"S3.F10.6.2\" class=\"ltx_text ltx_font_italic\">(a) Effect of local update and aggregation on the parameter values:</span> The y-axis shows the norm of parameters that are directly computed on the sensitive attribute, normalized by the parameter norm of the first layer. We show the dynamic of this parameter norm during FL for the aggregated model and locally updated model for the parties who benefit most from FL and suffer the most from FL (represented by the line with “Benefit” and “Suffer” respectively). <span id=\"S3.F10.7.3\" class=\"ltx_text ltx_font_italic\">(b) Correlation between standalone bias and parameter values:</span> The x-axis shows the bias a party receives in the standalone setting, and the y-axis shows the normalized norm of parameters (associated with sex attribute) for the locally updated model in the last round. <span id=\"S3.F10.8.4\" class=\"ltx_text ltx_font_italic\">(c) Effect of scaling the parameter values:</span> The figure shows the model performance in terms of accuracy and fairness gap when the parameter values (associated with sex attribute) are multiplied by a scaling factor.</figcaption>\n</figure>\n<div id=\"S3.SS4.p5\" class=\"ltx_para ltx_noindent\">\n<p id=\"S3.SS4.p5.1\" class=\"ltx_p\">Biased parties increase the model dependence on the sensitive attribute. But how does this increase propagate to other parties in the network? Since parties share the model parameters of the local model with the server, the bias is likely encoded in the model parameters. Therefore, we investigate which parameters are related to the model’s bias. Intuitively, the parameters used to extract sensitive attribute information impact the attribution value of the sensitive attribute to the model prediction. If the absolute value (signal) of those parameters is substantial, the value of the sensitive attribute will significantly affect the model’s prediction. In our evaluation, the sensitive attribute is part of the input feature, so the parameters directly applied to the “Sex” attribute in the first layer of the neural network should contribute to the model’s bias.</p>\n</div>\n<div id=\"S3.SS4.p6\" class=\"ltx_para ltx_noindent\">\n<p id=\"S3.SS4.p6.1\" class=\"ltx_p\">Figure <a href=\"#S3.F10\" title=\"Figure 10 ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>(a) shows the normalized norm of the parameters associated with the sensitive attribute for the most and least biased parties in the aggregated and locally updated models. The normalized norm is defined as the norm of parameters associated with the sensitive attribute divided by the parameter norm of the first layer. We observe that the least biased party reduces the parameter norm, hence decreasing the model’s sensitivity to the sensitive attribute. On the other hand, the biased party increases the norm for those parameters, amplifying the impact of the sensitive attribute on the model’s prediction. Through aggregation, this amplification will be propagated to the global model.\nFigure <a href=\"#S3.F10\" title=\"Figure 10 ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>(b) reveals a moderate correlation between the fairness gap party experiences in the standalone setting and the normalized parameter norm for the parameters used to extract sensitive attribute information. This implies that biased parties increase the parameter value associated with sensitive attributes during the local update, thereby boosting the model’s susceptibility to the sensitive attribute.</p>\n</div>\n<div id=\"S3.SS4.p7\" class=\"ltx_para ltx_noindent\">\n<p id=\"S3.SS4.p7.18\" class=\"ltx_p\"><span id=\"S3.SS4.p7.18.1\" class=\"ltx_text ltx_font_bold\">Controlling fairness gap by scaling a few parameters. </span>\nTo further investigate the impact of the parameters associated with the sensitive attribute (i.e., <math id=\"S3.SS4.p7.1.m1.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"104\" display=\"inline\"><semantics id=\"S3.SS4.p7.1.m1.1a\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.1.m1.1.1\" xref=\"S3.SS4.p7.1.m1.1.1.cmml\">104</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.1.m1.1b\"><cn type=\"integer\" id=\"S3.SS4.p7.1.m1.1.1.cmml\" xref=\"S3.SS4.p7.1.m1.1.1\">104</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.1.m1.1c\">104</annotation></semantics></math> parameters out of <math id=\"S3.SS4.p7.2.m2.2\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"1,792\" display=\"inline\"><semantics id=\"S3.SS4.p7.2.m2.2a\"><mrow id=\"S3.SS4.p7.2.m2.2.3.2\" xref=\"S3.SS4.p7.2.m2.2.3.1.cmml\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.2.m2.1.1\" xref=\"S3.SS4.p7.2.m2.1.1.cmml\">1</mn><mo mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.2.m2.2.3.2.1\" xref=\"S3.SS4.p7.2.m2.2.3.1.cmml\">,</mo><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.2.m2.2.2\" xref=\"S3.SS4.p7.2.m2.2.2.cmml\">792</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.2.m2.2b\"><list id=\"S3.SS4.p7.2.m2.2.3.1.cmml\" xref=\"S3.SS4.p7.2.m2.2.3.2\"><cn type=\"integer\" id=\"S3.SS4.p7.2.m2.1.1.cmml\" xref=\"S3.SS4.p7.2.m2.1.1\">1</cn><cn type=\"integer\" id=\"S3.SS4.p7.2.m2.2.2.cmml\" xref=\"S3.SS4.p7.2.m2.2.2\">792</cn></list></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.2.m2.2c\">1,792</annotation></semantics></math>) on model fairness, we examine the effect of scaling these parameters on the fairness gap in Figure <a href=\"#S3.F10\" title=\"Figure 10 ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>(c). We find that the fairness gap can be greatly widened or narrowed at the expense of a moderate degree of accuracy. Specifically, by scaling the parameter value by <math id=\"S3.SS4.p7.3.m3.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"0.1\" display=\"inline\"><semantics id=\"S3.SS4.p7.3.m3.1a\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.3.m3.1.1\" xref=\"S3.SS4.p7.3.m3.1.1.cmml\">0.1</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.3.m3.1b\"><cn type=\"float\" id=\"S3.SS4.p7.3.m3.1.1.cmml\" xref=\"S3.SS4.p7.3.m3.1.1\">0.1</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.3.m3.1c\">0.1</annotation></semantics></math> for the trained FL model, we significantly reduce the EO gap by <math id=\"S3.SS4.p7.4.m4.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"74.6\\%\" display=\"inline\"><semantics id=\"S3.SS4.p7.4.m4.1a\"><mrow id=\"S3.SS4.p7.4.m4.1.1\" xref=\"S3.SS4.p7.4.m4.1.1.cmml\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.4.m4.1.1.2\" xref=\"S3.SS4.p7.4.m4.1.1.2.cmml\">74.6</mn><mo mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.4.m4.1.1.1\" xref=\"S3.SS4.p7.4.m4.1.1.1.cmml\">%</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.4.m4.1b\"><apply id=\"S3.SS4.p7.4.m4.1.1.cmml\" xref=\"S3.SS4.p7.4.m4.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS4.p7.4.m4.1.1.1.cmml\" xref=\"S3.SS4.p7.4.m4.1.1.1\">percent</csymbol><cn type=\"float\" id=\"S3.SS4.p7.4.m4.1.1.2.cmml\" xref=\"S3.SS4.p7.4.m4.1.1.2\">74.6</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.4.m4.1c\">74.6\\%</annotation></semantics></math> (from <math id=\"S3.SS4.p7.5.m5.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"0.198\" display=\"inline\"><semantics id=\"S3.SS4.p7.5.m5.1a\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.5.m5.1.1\" xref=\"S3.SS4.p7.5.m5.1.1.cmml\">0.198</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.5.m5.1b\"><cn type=\"float\" id=\"S3.SS4.p7.5.m5.1.1.cmml\" xref=\"S3.SS4.p7.5.m5.1.1\">0.198</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.5.m5.1c\">0.198</annotation></semantics></math> to <math id=\"S3.SS4.p7.6.m6.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"0.05\" display=\"inline\"><semantics id=\"S3.SS4.p7.6.m6.1a\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.6.m6.1.1\" xref=\"S3.SS4.p7.6.m6.1.1.cmml\">0.05</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.6.m6.1b\"><cn type=\"float\" id=\"S3.SS4.p7.6.m6.1.1.cmml\" xref=\"S3.SS4.p7.6.m6.1.1\">0.05</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.6.m6.1c\">0.05</annotation></semantics></math>) and the DP gap by <math id=\"S3.SS4.p7.7.m7.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"69.1\\%\" display=\"inline\"><semantics id=\"S3.SS4.p7.7.m7.1a\"><mrow id=\"S3.SS4.p7.7.m7.1.1\" xref=\"S3.SS4.p7.7.m7.1.1.cmml\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.7.m7.1.1.2\" xref=\"S3.SS4.p7.7.m7.1.1.2.cmml\">69.1</mn><mo mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.7.m7.1.1.1\" xref=\"S3.SS4.p7.7.m7.1.1.1.cmml\">%</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.7.m7.1b\"><apply id=\"S3.SS4.p7.7.m7.1.1.cmml\" xref=\"S3.SS4.p7.7.m7.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS4.p7.7.m7.1.1.1.cmml\" xref=\"S3.SS4.p7.7.m7.1.1.1\">percent</csymbol><cn type=\"float\" id=\"S3.SS4.p7.7.m7.1.1.2.cmml\" xref=\"S3.SS4.p7.7.m7.1.1.2\">69.1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.7.m7.1c\">69.1\\%</annotation></semantics></math> (from <math id=\"S3.SS4.p7.8.m8.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"0.217\" display=\"inline\"><semantics id=\"S3.SS4.p7.8.m8.1a\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.8.m8.1.1\" xref=\"S3.SS4.p7.8.m8.1.1.cmml\">0.217</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.8.m8.1b\"><cn type=\"float\" id=\"S3.SS4.p7.8.m8.1.1.cmml\" xref=\"S3.SS4.p7.8.m8.1.1\">0.217</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.8.m8.1c\">0.217</annotation></semantics></math> to <math id=\"S3.SS4.p7.9.m9.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"0.067\" display=\"inline\"><semantics id=\"S3.SS4.p7.9.m9.1a\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.9.m9.1.1\" xref=\"S3.SS4.p7.9.m9.1.1.cmml\">0.067</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.9.m9.1b\"><cn type=\"float\" id=\"S3.SS4.p7.9.m9.1.1.cmml\" xref=\"S3.SS4.p7.9.m9.1.1\">0.067</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.9.m9.1c\">0.067</annotation></semantics></math>) with just a moderate accuracy loss of <math id=\"S3.SS4.p7.10.m10.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"0.8\\%\" display=\"inline\"><semantics id=\"S3.SS4.p7.10.m10.1a\"><mrow id=\"S3.SS4.p7.10.m10.1.1\" xref=\"S3.SS4.p7.10.m10.1.1.cmml\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.10.m10.1.1.2\" xref=\"S3.SS4.p7.10.m10.1.1.2.cmml\">0.8</mn><mo mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.10.m10.1.1.1\" xref=\"S3.SS4.p7.10.m10.1.1.1.cmml\">%</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.10.m10.1b\"><apply id=\"S3.SS4.p7.10.m10.1.1.cmml\" xref=\"S3.SS4.p7.10.m10.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS4.p7.10.m10.1.1.1.cmml\" xref=\"S3.SS4.p7.10.m10.1.1.1\">percent</csymbol><cn type=\"float\" id=\"S3.SS4.p7.10.m10.1.1.2.cmml\" xref=\"S3.SS4.p7.10.m10.1.1.2\">0.8</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.10.m10.1c\">0.8\\%</annotation></semantics></math> (from <math id=\"S3.SS4.p7.11.m11.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"0.785\" display=\"inline\"><semantics id=\"S3.SS4.p7.11.m11.1a\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.11.m11.1.1\" xref=\"S3.SS4.p7.11.m11.1.1.cmml\">0.785</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.11.m11.1b\"><cn type=\"float\" id=\"S3.SS4.p7.11.m11.1.1.cmml\" xref=\"S3.SS4.p7.11.m11.1.1\">0.785</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.11.m11.1c\">0.785</annotation></semantics></math> to <math id=\"S3.SS4.p7.12.m12.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"0.779\" display=\"inline\"><semantics id=\"S3.SS4.p7.12.m12.1a\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.12.m12.1.1\" xref=\"S3.SS4.p7.12.m12.1.1.cmml\">0.779</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.12.m12.1b\"><cn type=\"float\" id=\"S3.SS4.p7.12.m12.1.1.cmml\" xref=\"S3.SS4.p7.12.m12.1.1\">0.779</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.12.m12.1c\">0.779</annotation></semantics></math>). In contrast, scaling the same set of parameters by a factor of 10 increases the EO gap to <math id=\"S3.SS4.p7.13.m13.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"0.96\" display=\"inline\"><semantics id=\"S3.SS4.p7.13.m13.1a\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.13.m13.1.1\" xref=\"S3.SS4.p7.13.m13.1.1.cmml\">0.96</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.13.m13.1b\"><cn type=\"float\" id=\"S3.SS4.p7.13.m13.1.1.cmml\" xref=\"S3.SS4.p7.13.m13.1.1\">0.96</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.13.m13.1c\">0.96</annotation></semantics></math> (the maximal fairness gap is <math id=\"S3.SS4.p7.14.m14.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"1\" display=\"inline\"><semantics id=\"S3.SS4.p7.14.m14.1a\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.14.m14.1.1\" xref=\"S3.SS4.p7.14.m14.1.1.cmml\">1</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.14.m14.1b\"><cn type=\"integer\" id=\"S3.SS4.p7.14.m14.1.1.cmml\" xref=\"S3.SS4.p7.14.m14.1.1\">1</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.14.m14.1c\">1</annotation></semantics></math>), which is almost five times larger, and increases the DP gap by <math id=\"S3.SS4.p7.15.m15.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"259\\%\" display=\"inline\"><semantics id=\"S3.SS4.p7.15.m15.1a\"><mrow id=\"S3.SS4.p7.15.m15.1.1\" xref=\"S3.SS4.p7.15.m15.1.1.cmml\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.15.m15.1.1.2\" xref=\"S3.SS4.p7.15.m15.1.1.2.cmml\">259</mn><mo mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.15.m15.1.1.1\" xref=\"S3.SS4.p7.15.m15.1.1.1.cmml\">%</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.15.m15.1b\"><apply id=\"S3.SS4.p7.15.m15.1.1.cmml\" xref=\"S3.SS4.p7.15.m15.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS4.p7.15.m15.1.1.1.cmml\" xref=\"S3.SS4.p7.15.m15.1.1.1\">percent</csymbol><cn type=\"integer\" id=\"S3.SS4.p7.15.m15.1.1.2.cmml\" xref=\"S3.SS4.p7.15.m15.1.1.2\">259</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.15.m15.1c\">259\\%</annotation></semantics></math> to <math id=\"S3.SS4.p7.16.m16.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"0.78\" display=\"inline\"><semantics id=\"S3.SS4.p7.16.m16.1a\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.16.m16.1.1\" xref=\"S3.SS4.p7.16.m16.1.1.cmml\">0.78</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.16.m16.1b\"><cn type=\"float\" id=\"S3.SS4.p7.16.m16.1.1.cmml\" xref=\"S3.SS4.p7.16.m16.1.1\">0.78</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.16.m16.1c\">0.78</annotation></semantics></math>, while reducing the accuracy from <math id=\"S3.SS4.p7.17.m17.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"0.785\" display=\"inline\"><semantics id=\"S3.SS4.p7.17.m17.1a\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.17.m17.1.1\" xref=\"S3.SS4.p7.17.m17.1.1.cmml\">0.785</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.17.m17.1b\"><cn type=\"float\" id=\"S3.SS4.p7.17.m17.1.1.cmml\" xref=\"S3.SS4.p7.17.m17.1.1\">0.785</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.17.m17.1c\">0.785</annotation></semantics></math> to <math id=\"S3.SS4.p7.18.m18.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"0.67\" display=\"inline\"><semantics id=\"S3.SS4.p7.18.m18.1a\"><mn mathbackground=\"#F5FAFC\" id=\"S3.SS4.p7.18.m18.1.1\" xref=\"S3.SS4.p7.18.m18.1.1.cmml\">0.67</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p7.18.m18.1b\"><cn type=\"float\" id=\"S3.SS4.p7.18.m18.1.1.cmml\" xref=\"S3.SS4.p7.18.m18.1.1\">0.67</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p7.18.m18.1c\">0.67</annotation></semantics></math>. These findings explain how bias is propagated in FL: biased parties magnify the impact of sensitive attributes on model predictions by increasing the model parameter used to extract sensitive attributes. This rise in parameters is subsequently propagated to the global model through aggregation, further aggravating the issue of fairness for other parties. Our results explain how bias is propagated in FL: <span id=\"S3.SS4.p7.18.2\" class=\"ltx_text ltx_font_bold\">Biased parties encode bias in a few parameters through a local update, and this bias is consequently propagated to the entire network through parameter aggregation.</span></p>\n</div>\n<section id=\"S4\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">4 </span>Related work</h2>\n\n<div id=\"S4.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"S4.p1.1\" class=\"ltx_p\">Fairness has received considerable attention due to the growing deployment of machine learning in decision-making processes. Various definitions of fairness have been presented <cite class=\"ltx_cite ltx_citemacro_citep\">(Hardt et al., <a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">2016</a>; Dwork et al., <a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">2012</a>; Calders et al., <a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\">2009</a>)</cite>. Specifically, group fairness requires that the model behave similarly for groups defined by a sensitive attribute (e.g., race). While how machine learning algorithms propagate data bias to the final model has been extensively investigated in a centralized setting  <cite class=\"ltx_cite ltx_citemacro_citep\">(Blum and Stangl, <a href=\"#bib.bib3\" title=\"\" class=\"ltx_ref\">2020</a>; Lakkaraju et al., <a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">2017</a>; Rambachan and Roth, <a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">2020</a>; Friedler et al., <a href=\"#bib.bib15\" title=\"\" class=\"ltx_ref\">2019</a>; Dullerud et al., <a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>, the effect of FL on model fairness is not yet fully understood.</p>\n</div>\n<div id=\"S4.p2\" class=\"ltx_para ltx_noindent\">\n<p id=\"S4.p2.1\" class=\"ltx_p\">The existing literature on fairness in FL mainly focuses on the performance disparity of FL models across parties, rather than demographic groups <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et al., <a href=\"#bib.bib29\" title=\"\" class=\"ltx_ref\">2021</a>; Zhao and Joshi, <a href=\"#bib.bib55\" title=\"\" class=\"ltx_ref\">2022</a>; Li et al., <a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">2019</a>; Mohri et al., <a href=\"#bib.bib34\" title=\"\" class=\"ltx_ref\">2019</a>; Deng et al., <a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">2020</a>; Donahue and Kleinberg, <a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">2021</a>; Hao et al., <a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">2021</a>; Zhou et al., <a href=\"#bib.bib56\" title=\"\" class=\"ltx_ref\">2021</a>; Yu et al., <a href=\"#bib.bib48\" title=\"\" class=\"ltx_ref\">2020</a>; Lyu et al., <a href=\"#bib.bib32\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>. However, we focus on group fairness, which concerns performance disparity among groups. In terms of group fairness, <cite class=\"ltx_cite ltx_citemacro_cite\">Abay et al. (<a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2020</a>)</cite> listed a few potential sources of bias in FL. Recently, considerable progress has been made in training group fair models in FL <cite class=\"ltx_cite ltx_citemacro_citep\">(Abay et al., <a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2020</a>; Chu et al., <a href=\"#bib.bib5\" title=\"\" class=\"ltx_ref\">2021</a>; Zeng et al., <a href=\"#bib.bib51\" title=\"\" class=\"ltx_ref\">2021a</a>; Hu et al., <a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">2022</a>; Du et al., <a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">2021</a>; Ezzeldin et al., <a href=\"#bib.bib14\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>. Nonetheless, the majority of these works suggest techniques for achieving fairness on a single test distribution. Instead, we focus on fairness issues for parties. Some studies <cite class=\"ltx_cite ltx_citemacro_citep\">(Cui et al., <a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">2021</a>; Papadaki et al., <a href=\"#bib.bib37\" title=\"\" class=\"ltx_ref\">2022</a>)</cite> proposed algorithms to improve local fairness for parties. Our purpose, instead, is to gain a comprehensive understanding of how FL influences local fairness on its own, which we believe is equally crucial as designing fair algorithms.</p>\n</div>\n<section id=\"S5\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">5 </span>Future Work &amp; Conclusion</h2>\n\n<div id=\"S5.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"S5.p1.1\" class=\"ltx_p\"><span id=\"S5.p1.1.1\" class=\"ltx_text ltx_font_bold\">Future Work. </span>\nIn this work, we have investigated how bias is propagated in FL when the sensitive attribute is included as an input feature. In practice, however, sensitive attributes may be prohibited from being included in input features. In such situations, the model may still be heavily biased due to variables that are correlated with the (unobserved) sensitive attribute. For instance, a person’s zip code may be highly correlated with their race, a phenomenon known as \"redlining\". A promising direction for future work is to identify which model parameters contribute to the bias and to audit the bias propagation in this setting. Another important direction is to design FL algorithms that are robust to bias propagation. In Appendix <a href=\"#A5\" title=\"Appendix E Potential Mitigation ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>, we briefly discuss a few potential ways to achieve this goal.</p>\n</div>\n<div id=\"S5.p2\" class=\"ltx_para ltx_noindent\">\n<p id=\"S5.p2.1\" class=\"ltx_p\"><span id=\"S5.p2.1.1\" class=\"ltx_text ltx_font_bold\">Conclusion. </span> Federated learning has become increasingly popular in various applications with significant individual-level consequences, making it essential to anticipate the possible bias introduced by FL. Our paper takes the first step in this direction by providing a comprehensive analysis of the impact of FL on local fairness for parties. We demonstrated that the FL algorithm could introduce bias on its own which may exacerbate the issue of fairness for the involved parties. Moreover, we showed that this exacerbation is not evenly distributed among parties, as FL can propagate bias among them. Finally, we explained how bias is propagated in FL: biased parties encode their bias into the local updates by increasing the signal of a few parameters steadily throughout the training process, which is then propagated to the global model via aggregation and, ultimately, to other parties.</p>\n</div>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<section id=\"S6\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">6 </span>Acknowledgement</h2>\n\n<div id=\"S6.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"S6.p1.1\" class=\"ltx_p\">The authors would like to thank Ergute Bao, Ta Duy Nguyen, and Martin Strobel for their valuable feedback on earlier versions of this paper, as well as the anonymous reviewers for their insightful comments. This research is supported by Google PDPO faculty research award, Intel within the www.private-ai.org center, Meta faculty research award, the NUS Early Career Research Award (NUS ECRA award num- ber NUS ECRA FY19 P16), and the National Research Foundation, Singapore under its Strategic Capability Research Centres Funding Initiative. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the author(s) and do not reflect the views of the National Research Foundation, Singapore.</p>\n</div>\n<section id=\"S7\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">7 </span>Ethics Statement</h2>\n\n<div id=\"S7.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"S7.p1.1\" class=\"ltx_p\">Our analysis focuses on group fairness, which is typically used to audit the model or system for any bias or discrimination. Auditing the bias with respect to other definitions of fairness, such as individual fairness, may result in different conclusions. Moreover, our study focuses primarily on the binary notion of sex attributes and the multi-valued race attribute. We recognize that there are numerous protected groups outside those considered in the analysis, such as those defined by multiple sensitive attributes. The propagation of bias against fine-grained subgroups may be even more substantial than we found in the paper.</p>\n</div>\n<section id=\"S8\" class=\"ltx_section\">\n<h2 class=\"ltx_title ltx_title_section\">\n<span class=\"ltx_tag ltx_tag_section\">8 </span>Reproducibility Statement</h2>\n\n<div id=\"S8.p1\" class=\"ltx_para\">\n<p id=\"S8.p1.1\" class=\"ltx_p\">We provide details about the model, datasets, and implementations in Appendix <a href=\"#A1\" title=\"Appendix A Details about Experiment Setup ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>, and the code for the paper is available at <a target=\"_blank\" href=\"https://github.com/privacytrustlab/bias_in_FL\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://github.com/privacytrustlab/bias_in_FL</a>.</p>\n</div>\n<section id=\"bib\" class=\"ltx_bibliography\">\n<h2 class=\"ltx_title ltx_title_bibliography\">References</h2>\n\n<ul class=\"ltx_biblist\">\n<li id=\"bib.bib1\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Abay et al. [2020]</span>\n<span class=\"ltx_bibblock\">\nAnnie Abay, Yi Zhou, Nathalie Baracaldo, Shashank Rajamoni, Ebube Chuba, and\nHeiko Ludwig.\n\n</span>\n<span class=\"ltx_bibblock\">Mitigating bias in federated learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib1.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2012.02447</em>, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib2\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Asad et al. [2020]</span>\n<span class=\"ltx_bibblock\">\nMuhammad Asad, Ahmed Moustafa, and Takayuki Ito.\n\n</span>\n<span class=\"ltx_bibblock\">Fedopt: Towards communication efficiency and privacy preservation in\nfederated learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib2.1.1\" class=\"ltx_emph ltx_font_italic\">Applied Sciences</em>, 10(8):2864, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib3\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Blum and Stangl [2020]</span>\n<span class=\"ltx_bibblock\">\nAvrim Blum and Kevin Stangl.\n\n</span>\n<span class=\"ltx_bibblock\">Recovering from biased data: Can fairness constraints improve\naccuracy?\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib3.1.1\" class=\"ltx_emph ltx_font_italic\">1st Symposium on Foundations of Responsible Computing</em>,\n2020.\n\n</span>\n</li>\n<li id=\"bib.bib4\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Calders et al. [2009]</span>\n<span class=\"ltx_bibblock\">\nToon Calders, Faisal Kamiran, and Mykola Pechenizkiy.\n\n</span>\n<span class=\"ltx_bibblock\">Building classifiers with independency constraints.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib4.1.1\" class=\"ltx_emph ltx_font_italic\">2009 IEEE International Conference on Data Mining\nWorkshops</em>, pages 13–18. IEEE, 2009.\n\n</span>\n</li>\n<li id=\"bib.bib5\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chu et al. [2021]</span>\n<span class=\"ltx_bibblock\">\nLingyang Chu, Lanjun Wang, Yanjie Dong, Jian Pei, Zirui Zhou, and Yong Zhang.\n\n</span>\n<span class=\"ltx_bibblock\">Fedfair: Training fair models in cross-silo federated learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib5.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2109.05662</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib6\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Combalia et al. [2019]</span>\n<span class=\"ltx_bibblock\">\nMarc Combalia, Noel CF Codella, Veronica Rotemberg, Brian Helba, Veronica\nVilaplana, Ofer Reiter, Cristina Carrera, Alicia Barreiro, Allan C Halpern,\nSusana Puig, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Bcn20000: Dermoscopic lesions in the wild.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib6.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:1908.02288</em>, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib7\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Cui et al. [2021]</span>\n<span class=\"ltx_bibblock\">\nSen Cui, Weishen Pan, Jian Liang, Changshui Zhang, and Fei Wang.\n\n</span>\n<span class=\"ltx_bibblock\">Fair and consistent federated learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib7.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv e-prints</em>, pages arXiv–2108, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib8\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Deng et al. [2020]</span>\n<span class=\"ltx_bibblock\">\nYuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi.\n\n</span>\n<span class=\"ltx_bibblock\">Distributionally robust federated averaging.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib8.1.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>,\n33:15111–15122, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib9\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ding et al. [2021]</span>\n<span class=\"ltx_bibblock\">\nFrances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt.\n\n</span>\n<span class=\"ltx_bibblock\">Retiring adult: New datasets for fair machine learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib9.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2108.04884</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib10\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Donahue and Kleinberg [2021]</span>\n<span class=\"ltx_bibblock\">\nKate Donahue and Jon M. Kleinberg.\n\n</span>\n<span class=\"ltx_bibblock\">Models of fairness in federated learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib10.1.1\" class=\"ltx_emph ltx_font_italic\">CoRR</em>, abs/2112.00818, 2021.\n\n</span>\n<span class=\"ltx_bibblock\">URL <a target=\"_blank\" href=\"https://arxiv.org/abs/2112.00818\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://arxiv.org/abs/2112.00818</a>.\n\n</span>\n</li>\n<li id=\"bib.bib11\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Du et al. [2021]</span>\n<span class=\"ltx_bibblock\">\nWei Du, Depeng Xu, Xintao Wu, and Hanghang Tong.\n\n</span>\n<span class=\"ltx_bibblock\">Fairness-aware agnostic federated learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib11.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 2021 SIAM International Conference on\nData Mining (SDM)</em>, pages 181–189. SIAM, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib12\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Dullerud et al. [2022]</span>\n<span class=\"ltx_bibblock\">\nNatalie Dullerud, Karsten Roth, Kimia Hamidieh, Nicolas Papernot, and Marzyeh\nGhassemi.\n\n</span>\n<span class=\"ltx_bibblock\">Is fairness only metric deep? evaluating and addressing subgroup gaps\nin deep metric learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib12.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2203.12748</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib13\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Dwork et al. [2012]</span>\n<span class=\"ltx_bibblock\">\nCynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel.\n\n</span>\n<span class=\"ltx_bibblock\">Fairness through awareness.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib13.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 3rd Innovations in Theoretical Computer\nScience Conference</em>, ITCS ’12, page 214–226, New York, NY, USA, 2012.\nAssociation for Computing Machinery.\n\n</span>\n<span class=\"ltx_bibblock\">ISBN 9781450311151.\n\n</span>\n<span class=\"ltx_bibblock\">doi: <span class=\"ltx_ref ltx_nolink ltx_Url ltx_ref_self\">10.1145/2090236.2090255</span>.\n\n</span>\n<span class=\"ltx_bibblock\">URL <a target=\"_blank\" href=\"https://doi.org/10.1145/2090236.2090255\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://doi.org/10.1145/2090236.2090255</a>.\n\n</span>\n</li>\n<li id=\"bib.bib14\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ezzeldin et al. [2021]</span>\n<span class=\"ltx_bibblock\">\nYahya H Ezzeldin, Shen Yan, Chaoyang He, Emilio Ferrara, and Salman Avestimehr.\n\n</span>\n<span class=\"ltx_bibblock\">Fairfed: Enabling group fairness in federated learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib14.1.1\" class=\"ltx_emph ltx_font_italic\">NeurIPS Workshop on New Frontiers in Federated Learning (NFFL\n2021)</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib15\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Friedler et al. [2019]</span>\n<span class=\"ltx_bibblock\">\nSorelle A Friedler, Carlos Scheidegger, Suresh Venkatasubramanian, Sonam\nChoudhary, Evan P Hamilton, and Derek Roth.\n\n</span>\n<span class=\"ltx_bibblock\">A comparative study of fairness-enhancing interventions in machine\nlearning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib15.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the conference on fairness, accountability,\nand transparency</em>, pages 329–338, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib16\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Grabowicz et al. [2022]</span>\n<span class=\"ltx_bibblock\">\nPrzemyslaw A Grabowicz, Nicholas Perello, and Aarshee Mishra.\n\n</span>\n<span class=\"ltx_bibblock\">Marrying fairness and explainability in supervised learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib16.1.1\" class=\"ltx_emph ltx_font_italic\">2022 ACM Conference on Fairness, Accountability, and\nTransparency</em>, pages 1905–1916, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib17\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hao et al. [2021]</span>\n<span class=\"ltx_bibblock\">\nWeituo Hao, Mostafa El-Khamy, Jungwon Lee, Jianyi Zhang, Kevin J Liang,\nChangyou Chen, and Lawrence Carin Duke.\n\n</span>\n<span class=\"ltx_bibblock\">Towards fair federated learning with zero-shot data augmentation.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib17.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition</em>, pages 3310–3319, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib18\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hardt et al. [2016]</span>\n<span class=\"ltx_bibblock\">\nMoritz Hardt, Eric Price, Eric Price, and Nati Srebro.\n\n</span>\n<span class=\"ltx_bibblock\">Equality of opportunity in supervised learning.\n\n</span>\n<span class=\"ltx_bibblock\">In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett,\neditors, <em id=\"bib.bib18.1.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>, volume 29.\nCurran Associates, Inc., 2016.\n\n</span>\n<span class=\"ltx_bibblock\">URL\n<a target=\"_blank\" href=\"https://proceedings.neurips.cc/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://proceedings.neurips.cc/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf</a>.\n\n</span>\n</li>\n<li id=\"bib.bib19\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hashimoto et al. [2018]</span>\n<span class=\"ltx_bibblock\">\nTatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang.\n\n</span>\n<span class=\"ltx_bibblock\">Fairness without demographics in repeated loss minimization.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib19.1.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Machine Learning</em>, pages\n1929–1938. PMLR, 2018.\n\n</span>\n</li>\n<li id=\"bib.bib20\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">He et al. [2020]</span>\n<span class=\"ltx_bibblock\">\nChaoyang He, Songze Li, Jinhyun So, Xiao Zeng, Mi Zhang, Hongyi Wang, Xiaoyang\nWang, Praneeth Vepakomma, Abhishek Singh, Hang Qiu, et al.\n\n</span>\n<span class=\"ltx_bibblock\">Fedml: A research library and benchmark for federated machine\nlearning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib20.1.1\" class=\"ltx_emph ltx_font_italic\">NeurIPS 2020 FL Workshop Best Paper Award</em>, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib21\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Hu et al. [2022]</span>\n<span class=\"ltx_bibblock\">\nShengyuan Hu, Zhiwei Steven Wu, and Virginia Smith.\n\n</span>\n<span class=\"ltx_bibblock\">Provably fair federated learning via bounded group loss.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib21.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2203.10190</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib22\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Kamiran and Calders [2012]</span>\n<span class=\"ltx_bibblock\">\nFaisal Kamiran and Toon Calders.\n\n</span>\n<span class=\"ltx_bibblock\">Data preprocessing techniques for classification without\ndiscrimination.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib22.1.1\" class=\"ltx_emph ltx_font_italic\">Knowledge and information systems</em>, 33(1):1–33, 2012.\n\n</span>\n</li>\n<li id=\"bib.bib23\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Karimireddy et al. [2020a]</span>\n<span class=\"ltx_bibblock\">\nSai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank J\nReddi, Sebastian U Stich, and Ananda Theertha Suresh.\n\n</span>\n<span class=\"ltx_bibblock\">Mime: Mimicking centralized stochastic algorithms in federated\nlearning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib23.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2008.03606</em>, 2020a.\n\n</span>\n</li>\n<li id=\"bib.bib24\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Karimireddy et al. [2020b]</span>\n<span class=\"ltx_bibblock\">\nSai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian\nStich, and Ananda Theertha Suresh.\n\n</span>\n<span class=\"ltx_bibblock\">Scaffold: Stochastic controlled averaging for federated learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib24.1.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Machine Learning</em>, pages\n5132–5143. PMLR, 2020b.\n\n</span>\n</li>\n<li id=\"bib.bib25\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Kokhlikyan et al. [2020]</span>\n<span class=\"ltx_bibblock\">\nNarine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh,\nJonathan Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi\nYan, and Orion Reblitz-Richardson.\n\n</span>\n<span class=\"ltx_bibblock\">Captum: A unified and generic model interpretability library for\npytorch, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib26\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Lakkaraju et al. [2017]</span>\n<span class=\"ltx_bibblock\">\nHimabindu Lakkaraju, Jon Kleinberg, Jure Leskovec, Jens Ludwig, and Sendhil\nMullainathan.\n\n</span>\n<span class=\"ltx_bibblock\">The selective labels problem: Evaluating algorithmic predictions in\nthe presence of unobservables.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib26.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 23rd ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining</em>, pages 275–284, 2017.\n\n</span>\n</li>\n<li id=\"bib.bib27\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Li et al. [2019]</span>\n<span class=\"ltx_bibblock\">\nTian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith.\n\n</span>\n<span class=\"ltx_bibblock\">Fair resource allocation in federated learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib27.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:1905.10497</em>, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib28\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Li et al. [2020]</span>\n<span class=\"ltx_bibblock\">\nTian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and\nVirginia Smith.\n\n</span>\n<span class=\"ltx_bibblock\">Federated optimization in heterogeneous networks.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib28.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of Machine Learning and Systems</em>, 2:429–450, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib29\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Li et al. [2021]</span>\n<span class=\"ltx_bibblock\">\nTian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith.\n\n</span>\n<span class=\"ltx_bibblock\">Ditto: Fair and robust federated learning through personalization.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib29.1.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Machine Learning</em>, pages\n6357–6368. PMLR, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib30\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Liu et al. [2022]</span>\n<span class=\"ltx_bibblock\">\nJi Liu, Zenan Li, Yuan Yao, Feng Xu, Xiaoxing Ma, Miao Xu, and Hanghang Tong.\n\n</span>\n<span class=\"ltx_bibblock\">Fair representation learning: An alternative to mutual information.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib30.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 28th ACM SIGKDD Conference on Knowledge\nDiscovery and Data Mining</em>, pages 1088–1097, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib31\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Liu et al. [2015]</span>\n<span class=\"ltx_bibblock\">\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.\n\n</span>\n<span class=\"ltx_bibblock\">Deep learning face attributes in the wild.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib31.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of International Conference on Computer Vision\n(ICCV)</em>, December 2015.\n\n</span>\n</li>\n<li id=\"bib.bib32\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Lyu et al. [2020]</span>\n<span class=\"ltx_bibblock\">\nLingjuan Lyu, Xinyi Xu, Qian Wang, and Han Yu.\n\n</span>\n<span class=\"ltx_bibblock\">Collaborative fairness in federated learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib32.1.1\" class=\"ltx_emph ltx_font_italic\">Federated Learning</em>, pages 189–204. Springer, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib33\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">McMahan et al. [2017]</span>\n<span class=\"ltx_bibblock\">\nH. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and\nBlaise Agüera y Arcas.\n\n</span>\n<span class=\"ltx_bibblock\">Communication-efficient learning of deep networks from decentralized\ndata, 2017.\n\n</span>\n</li>\n<li id=\"bib.bib34\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Mohri et al. [2019]</span>\n<span class=\"ltx_bibblock\">\nMehryar Mohri, Gary Sivek, and Ananda Theertha Suresh.\n\n</span>\n<span class=\"ltx_bibblock\">Agnostic federated learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib34.1.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Machine Learning</em>, pages\n4615–4625. PMLR, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib35\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ogier du Terrail et al. [2022]</span>\n<span class=\"ltx_bibblock\">\nJean Ogier du Terrail, Samy-Safwan Ayed, Edwige Cyffers, Felix Grimberg,\nChaoyang He, Regis Loeb, Paul Mangold, Tanguy Marchand, Othmane Marfoq, Erum\nMushtaq, Boris Muzellec, Constantin Philippenko, Santiago Silva, Maria\nTeleńczuk, Shadi Albarqouni, Salman Avestimehr, Aurélien Bellet, Aymeric\nDieuleveut, Martin Jaggi, Sai Praneeth Karimireddy, Marco Lorenzi, Giovanni\nNeglia, Marc Tommasi, and Mathieu Andreux.\n\n</span>\n<span class=\"ltx_bibblock\">Flamby: Datasets and benchmarks for cross-silo federated learning in\nrealistic healthcare settings, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib36\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Papadaki et al. [2021]</span>\n<span class=\"ltx_bibblock\">\nAfroditi Papadaki, Natalia Martinez, Martin Bertran, Guillermo Sapiro, and\nMiguel Rodrigues.\n\n</span>\n<span class=\"ltx_bibblock\">Federating for learning group fair models.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib36.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2110.01999</em>, 2021.\n\n</span>\n</li>\n<li id=\"bib.bib37\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Papadaki et al. [2022]</span>\n<span class=\"ltx_bibblock\">\nAfroditi Papadaki, Natalia Martinez, Martin Bertran, Guillermo Sapiro, and\nMiguel Rodrigues.\n\n</span>\n<span class=\"ltx_bibblock\">Minimax demographic group fairness in federated learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib37.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2201.08304</em>, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib38\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Paszke et al. [2019]</span>\n<span class=\"ltx_bibblock\">\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban\nDesmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan\nTejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith\nChintala.\n\n</span>\n<span class=\"ltx_bibblock\">Pytorch: An imperative style, high-performance deep learning library.\n\n</span>\n<span class=\"ltx_bibblock\">In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, <em id=\"bib.bib38.1.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural\nInformation Processing Systems 32</em>, pages 8024–8035. Curran Associates,\nInc., 2019.\n\n</span>\n<span class=\"ltx_bibblock\">URL\n<a target=\"_blank\" href=\"http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</a>.\n\n</span>\n</li>\n<li id=\"bib.bib39\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Rambachan and Roth [2020]</span>\n<span class=\"ltx_bibblock\">\nAshesh Rambachan and Jonathan Roth.\n\n</span>\n<span class=\"ltx_bibblock\">Bias in, bias out? evaluating the folk wisdom.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib39.1.1\" class=\"ltx_emph ltx_font_italic\">1st Symposium on Foundations of Responsible Computing</em>,\n2020.\n\n</span>\n</li>\n<li id=\"bib.bib40\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Reddi et al. [2020]</span>\n<span class=\"ltx_bibblock\">\nSashank J Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,\nJakub Konečnỳ, Sanjiv Kumar, and Hugh Brendan McMahan.\n\n</span>\n<span class=\"ltx_bibblock\">Adaptive federated optimization.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib40.1.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Learning Representations</em>, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib41\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Rieke et al. [2020]</span>\n<span class=\"ltx_bibblock\">\nNicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletari, Holger R Roth, Shadi\nAlbarqouni, Spyridon Bakas, Mathieu N Galtier, Bennett A Landman, Klaus\nMaier-Hein, et al.\n\n</span>\n<span class=\"ltx_bibblock\">The future of digital health with federated learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib41.1.1\" class=\"ltx_emph ltx_font_italic\">NPJ digital medicine</em>, 3(1):1–7, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib42\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Sarah et al. [2020]</span>\n<span class=\"ltx_bibblock\">\nFlood Sarah, King Miriam, Rodger Renae, Ruggles Steven, and Warren J. Robert.\n\n</span>\n<span class=\"ltx_bibblock\">Integrated public use microdata series, current population survey:\nVersion 8.0 [dataset], 2020.\n\n</span>\n<span class=\"ltx_bibblock\">URL <a target=\"_blank\" href=\"https://www.ipums.org/projects/ipums-cps/d030.v8.0\" title=\"\" class=\"ltx_ref ltx_url ltx_font_typewriter\">https://www.ipums.org/projects/ipums-cps/d030.v8.0</a>.\n\n</span>\n</li>\n<li id=\"bib.bib43\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Sundararajan et al. [2017]</span>\n<span class=\"ltx_bibblock\">\nMukund Sundararajan, Ankur Taly, and Qiqi Yan.\n\n</span>\n<span class=\"ltx_bibblock\">Axiomatic attribution for deep networks.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib43.1.1\" class=\"ltx_emph ltx_font_italic\">International conference on machine learning</em>, pages\n3319–3328. PMLR, 2017.\n\n</span>\n</li>\n<li id=\"bib.bib44\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Tan and Le [2019]</span>\n<span class=\"ltx_bibblock\">\nMingxing Tan and Quoc Le.\n\n</span>\n<span class=\"ltx_bibblock\">Efficientnet: Rethinking model scaling for convolutional neural\nnetworks.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib44.1.1\" class=\"ltx_emph ltx_font_italic\">International conference on machine learning</em>, pages\n6105–6114. PMLR, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib45\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Tschandl et al. [2018]</span>\n<span class=\"ltx_bibblock\">\nPhilipp Tschandl, Cliff Rosendahl, and Harald Kittler.\n\n</span>\n<span class=\"ltx_bibblock\">The ham10000 dataset, a large collection of multi-source\ndermatoscopic images of common pigmented skin lesions.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib45.1.1\" class=\"ltx_emph ltx_font_italic\">Scientific data</em>, 5(1):1–9, 2018.\n\n</span>\n</li>\n<li id=\"bib.bib46\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wang et al. [2020]</span>\n<span class=\"ltx_bibblock\">\nJianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor.\n\n</span>\n<span class=\"ltx_bibblock\">Tackling the objective inconsistency problem in heterogeneous\nfederated optimization.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib46.1.1\" class=\"ltx_emph ltx_font_italic\">Advances in neural information processing systems</em>,\n33:7611–7623, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib47\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yang et al. [2019]</span>\n<span class=\"ltx_bibblock\">\nWensi Yang, Yuhang Zhang, Kejiang Ye, Li Li, and Cheng-Zhong Xu.\n\n</span>\n<span class=\"ltx_bibblock\">Ffd: A federated learning based method for credit card fraud\ndetection.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib47.1.1\" class=\"ltx_emph ltx_font_italic\">International conference on big data</em>, pages 18–32.\nSpringer, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib48\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yu et al. [2020]</span>\n<span class=\"ltx_bibblock\">\nHan Yu, Zelei Liu, Yang Liu, Tianjian Chen, Mingshu Cong, Xi Weng, Dusit\nNiyato, and Qiang Yang.\n\n</span>\n<span class=\"ltx_bibblock\">A fairness-aware incentive scheme for federated learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib48.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the AAAI/ACM Conference on AI, Ethics, and\nSociety</em>, pages 393–399, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib49\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zafar et al. [2017]</span>\n<span class=\"ltx_bibblock\">\nMuhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P\nGummadi.\n\n</span>\n<span class=\"ltx_bibblock\">Fairness beyond disparate treatment &amp; disparate impact: Learning\nclassification without disparate mistreatment.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib49.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 26th international conference on world\nwide web</em>, pages 1171–1180, 2017.\n\n</span>\n</li>\n<li id=\"bib.bib50\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zemel et al. [2013]</span>\n<span class=\"ltx_bibblock\">\nRich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork.\n\n</span>\n<span class=\"ltx_bibblock\">Learning fair representations.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib50.1.1\" class=\"ltx_emph ltx_font_italic\">International conference on machine learning</em>, pages\n325–333. PMLR, 2013.\n\n</span>\n</li>\n<li id=\"bib.bib51\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zeng et al. [2021a]</span>\n<span class=\"ltx_bibblock\">\nYuchen Zeng, Hongxu Chen, and Kangwook Lee.\n\n</span>\n<span class=\"ltx_bibblock\">Improving fairness via federated learning.\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib51.1.1\" class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2110.15545</em>, 2021a.\n\n</span>\n</li>\n<li id=\"bib.bib52\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zeng et al. [2021b]</span>\n<span class=\"ltx_bibblock\">\nZiqian Zeng, Rashidul Islam, Kamrun Naher Keya, James Foulds, Yangqiu Song, and\nShimei Pan.\n\n</span>\n<span class=\"ltx_bibblock\">Fair representation learning for heterogeneous information networks.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib52.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the International AAAI Conference on Weblogs\nand Social Media</em>, volume 15, 2021b.\n\n</span>\n</li>\n<li id=\"bib.bib53\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et al. [2020]</span>\n<span class=\"ltx_bibblock\">\nXueru Zhang, Ruibo Tu, Yang Liu, Mingyan Liu, Hedvig Kjellstrom, Kun Zhang, and\nCheng Zhang.\n\n</span>\n<span class=\"ltx_bibblock\">How do fair decisions fare in long-term qualification?\n\n</span>\n<span class=\"ltx_bibblock\"><em id=\"bib.bib53.1.1\" class=\"ltx_emph ltx_font_italic\">Advances in Neural Information Processing Systems</em>,\n33:18457–18469, 2020.\n\n</span>\n</li>\n<li id=\"bib.bib54\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhao et al. [2019]</span>\n<span class=\"ltx_bibblock\">\nHan Zhao, Amanda Coston, Tameem Adel, and Geoffrey J Gordon.\n\n</span>\n<span class=\"ltx_bibblock\">Conditional learning of fair representations.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib54.1.1\" class=\"ltx_emph ltx_font_italic\">International Conference on Learning Representations</em>, 2019.\n\n</span>\n</li>\n<li id=\"bib.bib55\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhao and Joshi [2022]</span>\n<span class=\"ltx_bibblock\">\nZhiyuan Zhao and Gauri Joshi.\n\n</span>\n<span class=\"ltx_bibblock\">A dynamic reweighting strategy for fair federated learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib55.1.1\" class=\"ltx_emph ltx_font_italic\">ICASSP 2022-2022 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP)</em>, pages 8772–8776. IEEE, 2022.\n\n</span>\n</li>\n<li id=\"bib.bib56\" class=\"ltx_bibitem\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhou et al. [2021]</span>\n<span class=\"ltx_bibblock\">\nZirui Zhou, Lingyang Chu, Changxin Liu, Lanjun Wang, Jian Pei, and Yong Zhang.\n\n</span>\n<span class=\"ltx_bibblock\">Towards fair federated learning.\n\n</span>\n<span class=\"ltx_bibblock\">In <em id=\"bib.bib56.1.1\" class=\"ltx_emph ltx_font_italic\">Proceedings of the 27th ACM SIGKDD Conference on Knowledge\nDiscovery &amp; Data Mining</em>, pages 4100–4101, 2021.\n\n</span>\n</li>\n</ul>\n</section>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<section id=\"Ax1\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">Appendix</h2>\n\n<div id=\"Ax1.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"Ax1.p1.1\" class=\"ltx_p\">This appendix is divided into five sections. Appendix <a href=\"#A1\" title=\"Appendix A Details about Experiment Setup ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">A</span></a> provides additional details about the experimental setups, including information about the models, datasets, and hyperparameters used. Appendix <a href=\"#A2\" title=\"Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> presents further experimental results that support the claims made in the paper. In Appendix <a href=\"#A3\" title=\"Appendix C Evaluation on existing Fair FL ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>, we discuss the results of an existing fair FL algorithm. Appendix <a href=\"#A4\" title=\"Appendix D Extending to a real-world medical dataset ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">D</span></a> extends our analysis to real-world medical datasets. Finally, in Appendix <a href=\"#A5\" title=\"Appendix E Potential Mitigation ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>, we discuss potential methods for mitigating bias in FL.</p>\n</div>\n</section>\n<section id=\"A1\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix A </span>Details about Experiment Setup</h2>\n\n<section id=\"A1.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.1 </span>Datasets and Models</h3>\n\n<div id=\"A1.SS1.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"A1.SS1.p1.1\" class=\"ltx_p\">We explain the datasets and models used in the paper.</p>\n</div>\n<section id=\"A1.SS1.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Census Dataset</h4>\n\n<div id=\"A1.SS1.SSS0.Px1.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"A1.SS1.SSS0.Px1.p1.1\" class=\"ltx_p\">We use the datasets provided by folktables. In particular, we consider the ACSIncome, ACSPublicCoverage, and ACSEmployment tasks defined in the forlktables. In the ACSIncome, the goal is to predict whether an individual’s Income is above $50,000. In the ACSEmployment task, the goal is to predict whether an individual is employed. Similarly, in the Health (i.e., ACSPublicCoverage) task, the objective is to predict whether an individual is covered by public health insurance. We use the same pre-processing as in the folktables and train a fully connected neural network model with one hidden layer of 32 neurons for Income and 64 neurons for Employment and Health tasks. For all the tasks, we use the RELU activation function. We use an SGD optimizer with a learning rate of 0.001 for centralized training on Health and Employment datasets and 0.1 for other settings, and the batch size is 32. We train the NN models for 200 epochs. In FL, each client updates the global mdoel for 1 epoch and shares it with the server. We encode the categorical features based on the encoding template provided in folktables. After the encoding, the input feature size for Income is 54, 154 for Health, and 109 for Employment. We consider sex and race as sensitive attributes. Accordingly, there are two gender groups (male and female) and nine racial groups (\"White alone,\" \"Black or African American alone,\" \"American Indian alone,\" \"Alaska Native alone,\" and \"American Indian and Alaska Native tribes specified; or American Indian or Alaska Native, not specified and no other,\" \"Asian alone,\" \"Native Hawaiian and Other Pacific Islander alone,\" \"Some Other Race alone,\" \"Two or More Races\").</p>\n</div>\n</section>\n<section id=\"A1.SS1.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">CelebA Dataset</h4>\n\n<div id=\"A1.SS1.SSS0.Px2.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"A1.SS1.SSS0.Px2.p1.1\" class=\"ltx_p\">We train CNN models on the dataset with one CNN layer whose output channel is 32, kernel size is 3, and stride is 1. We use ’same’ padding for the CNN layer. Following this CNN layer, we have the Batch normalization layer and Max Pooling layer. After which, we have the connected layer. We train the model for 500 communication rounds or epochs with SGD optimizer. The learning rate is 0.1, and the batch size is 128. The train, test, and validation datasets ratio for each party is 6:2:2.</p>\n</div>\n</section>\n</section>\n<section id=\"A1.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.2 </span>Hyper-parameter for FL algorithms and Implementation</h3>\n\n<div id=\"A1.SS2.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"A1.SS2.p1.1\" class=\"ltx_p\">In our paper, we also evaluate various popular FL algorithms, including FedNova <cite class=\"ltx_cite ltx_citemacro_citep\">[Wang et al., <a href=\"#bib.bib46\" title=\"\" class=\"ltx_ref\">2020</a>]</cite>, Scaffold <cite class=\"ltx_cite ltx_citemacro_cite\">Karimireddy et al. [<a href=\"#bib.bib24\" title=\"\" class=\"ltx_ref\">2020b</a>]</cite>, FedOpt <cite class=\"ltx_cite ltx_citemacro_citep\">[Reddi et al., <a href=\"#bib.bib40\" title=\"\" class=\"ltx_ref\">2020</a>]</cite>, FedProx <cite class=\"ltx_cite ltx_citemacro_cite\">Li et al. [<a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">2020</a>]</cite>, and Mime <cite class=\"ltx_cite ltx_citemacro_citep\">[Karimireddy et al., <a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">2020a</a>]</cite>. We use the same local learning rate and the number of the local epoch as in FedAvg. We provided the detailed hyper-parameters for each of the algorithms in our code (See supplementary). We run all experiments on Ubuntu with two NVIDIA TITAN RTX GPUs.</p>\n</div>\n</section>\n<section id=\"A1.SS3\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">A.3 </span>Data Heterogeneity</h3>\n\n<div id=\"A1.SS3.p1\" class=\"ltx_para\">\n<p id=\"A1.SS3.p1.1\" class=\"ltx_p\">Figure <a href=\"#A1.F11\" title=\"Figure 11 ‣ A.3 Data Heterogeneity ‣ Appendix A Details about Experiment Setup ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> shows the fraction of samples for each subgroup across all parties, while Figure <a href=\"#A1.F12\" title=\"Figure 12 ‣ A.3 Data Heterogeneity ‣ Appendix A Details about Experiment Setup ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> shows the histogram of subgroup proportions for each party. We observe that the parties have different fractions of samples from each group in the Income dataset, indicating that their local data distributions are dissimilar. In contrast, for the Health and Employment datasets, parties have similar fractions of samples from each subgroup, suggesting that their data distributions are more alike than those of the Income dataset.</p>\n</div>\n<figure id=\"A1.F11\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_3\"><img src=\"/html/2309.02160/assets/images/income/non-iid/subgroup_distribution_attr_1.png\" id=\"A1.F11.g1\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait\" width=\"168\" height=\"955\" alt=\"Refer to caption\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_3\"><img src=\"/html/2309.02160/assets/images/employment/non-iid/subgroup_distribution_attr_1.png\" id=\"A1.F11.g2\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait\" width=\"174\" height=\"950\" alt=\"Refer to caption\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_3\"><img src=\"/html/2309.02160/assets/images/health/non-iid/subgroup_distribution_attr_1.png\" id=\"A1.F11.g3\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait\" width=\"168\" height=\"955\" alt=\"Refer to caption\"></div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 11: </span>Fraction of samples for each subgroup - Sex</figcaption>\n</figure>\n<figure id=\"A1.F12\" class=\"ltx_figure\"><img src=\"/html/2309.02160/assets/images/hist_base_rate_attr_combined.png\" id=\"A1.F12.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"159\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 12: </span>Histogram of the fraction of samples for each group with positive label - Sex</figcaption>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>\n</section>\n<section id=\"A2\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix B </span>Additional Experimental Results</h2>\n\n<section id=\"A2.SS1\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.1 </span>Bias propagation effect</h3>\n\n<section id=\"A2.SS1.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Census dataset</h4>\n\n<div id=\"A2.SS1.SSS0.Px1.p1\" class=\"ltx_para\">\n<p id=\"A2.SS1.SSS0.Px1.p1.1\" class=\"ltx_p\">We show the bias propagation effect of FL for the Health and Employment task in Figure <a href=\"#A2.F13\" title=\"Figure 13 ‣ Census dataset ‣ B.1 Bias propagation effect ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> and <a href=\"#A2.F14\" title=\"Figure 14 ‣ Census dataset ‣ B.1 Bias propagation effect ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">14</span></a> respectively.</p>\n</div>\n<figure id=\"A2.F13\" class=\"ltx_figure\"><img src=\"/html/2309.02160/assets/images/health/bias_propagation/combined_attrs.png\" id=\"A2.F13.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"112\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 13: </span><span id=\"A2.F13.2.1\" class=\"ltx_text ltx_font_bold\">Correlation between the fairness gap of the standalone model and the benefit obtained from FL - Health</span></figcaption>\n</figure>\n<figure id=\"A2.F14\" class=\"ltx_figure\"><img src=\"/html/2309.02160/assets/images/employment/bias_propagation/combined_attrs.png\" id=\"A2.F14.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"111\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 14: </span><span id=\"A2.F14.2.1\" class=\"ltx_text ltx_font_bold\">Correlation between the fairness gap of the standalone model and the benefit obtained from FL - Employment</span></figcaption>\n</figure>\n</section>\n<section id=\"A2.SS1.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">CelebA</h4>\n\n<div id=\"A2.SS1.SSS0.Px2.p1\" class=\"ltx_para\">\n<p id=\"A2.SS1.SSS0.Px2.p1.1\" class=\"ltx_p\">We show the bias propagation effect of FL on CelebA dataset. Besides partitioning the data in an IID manner (we refer to as setting (i)), we also evaluate two different settings, where we change the number of samples from the minority subgroup (the female group with the “Not Young” age label). In this way, we aim to change the data bias in the local training datasets for clients. In particular, in setting (ii), half of the parties have more samples from the minority subgroup than another half of the parties. The ratio is 8:2. In setting (iii), a single party has half of the data from the minority subgroup, and the data is then iid partitioned among the other parties. In the figure, we find that the in the non-iid setting, the more biased parties have a larger and more positive benefit, while FL hurts the less biased parties. Figure <a href=\"#A2.F15\" title=\"Figure 15 ‣ CelebA ‣ B.1 Bias propagation effect ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">15</span></a> shows the correlation between the fairness gap of the standalone model and the benefit a party gets in the FL. We find that the in the non-iid setting, the more biased parties have a larger and more positive benefit, while FL hurts the less biased parties.</p>\n</div>\n<figure id=\"A2.F15\" class=\"ltx_figure\"><img src=\"/html/2309.02160/assets/images/celeba/bias_propagation_celeba_age.png\" id=\"A2.F15.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"292\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 15: </span><span id=\"A2.F15.2.1\" class=\"ltx_text ltx_font_bold\">Correlation between the fairness gap of the standalone model and the benefit obtained from FL - CelebA (Age)</span></figcaption>\n</figure>\n</section>\n</section>\n<section id=\"A2.SS2\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.2 </span>Aggregation contradicts with local update</h3>\n\n<div id=\"A2.SS2.p1\" class=\"ltx_para\">\n<p id=\"A2.SS2.p1.1\" class=\"ltx_p\">Figure <a href=\"#A2.F16\" title=\"Figure 16 ‣ B.2 Aggregation contradicts with local update ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">16</span></a> shows the dynamic of the fairness gap for the aggregated model and locally updated model from the most biased party and least biased party during the training.</p>\n</div>\n<figure id=\"A2.F16\" class=\"ltx_figure\"><img src=\"/html/2309.02160/assets/images/income/dynamic/attr_1_metric_dp_most_biased_200.png\" id=\"A2.F16.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"321\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 16: </span><span id=\"A2.F16.2.1\" class=\"ltx_text ltx_font_bold\">Dynamic of <math id=\"A2.F16.2.1.m1.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{DP}\" display=\"inline\"><semantics id=\"A2.F16.2.1.m1.1b\"><msup id=\"A2.F16.2.1.m1.1.1\" xref=\"A2.F16.2.1.m1.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A2.F16.2.1.m1.1.1.2\" xref=\"A2.F16.2.1.m1.1.1.2.cmml\">Δ</mi><mrow id=\"A2.F16.2.1.m1.1.1.3\" xref=\"A2.F16.2.1.m1.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A2.F16.2.1.m1.1.1.3.2\" xref=\"A2.F16.2.1.m1.1.1.3.2.cmml\">D</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A2.F16.2.1.m1.1.1.3.1\" xref=\"A2.F16.2.1.m1.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#F5FAFC\" id=\"A2.F16.2.1.m1.1.1.3.3\" xref=\"A2.F16.2.1.m1.1.1.3.3.cmml\">P</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A2.F16.2.1.m1.1c\"><apply id=\"A2.F16.2.1.m1.1.1.cmml\" xref=\"A2.F16.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.F16.2.1.m1.1.1.1.cmml\" xref=\"A2.F16.2.1.m1.1.1\">superscript</csymbol><ci id=\"A2.F16.2.1.m1.1.1.2.cmml\" xref=\"A2.F16.2.1.m1.1.1.2\">Δ</ci><apply id=\"A2.F16.2.1.m1.1.1.3.cmml\" xref=\"A2.F16.2.1.m1.1.1.3\"><times id=\"A2.F16.2.1.m1.1.1.3.1.cmml\" xref=\"A2.F16.2.1.m1.1.1.3.1\"></times><ci id=\"A2.F16.2.1.m1.1.1.3.2.cmml\" xref=\"A2.F16.2.1.m1.1.1.3.2\">𝐷</ci><ci id=\"A2.F16.2.1.m1.1.1.3.3.cmml\" xref=\"A2.F16.2.1.m1.1.1.3.3\">𝑃</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.F16.2.1.m1.1d\">\\Delta^{DP}</annotation></semantics></math> during the training - Income (sex). </span> Figure shows the fairness gap for the aggregated model and local updated model from the most biased party and least biased party during the training. The most (least) biased party is the party with the highest (lowest) fairness gap in the standalone setting. The fairness gap in the standalone setting for those parties is shown in the title.\n</figcaption>\n</figure>\n</section>\n<section id=\"A2.SS3\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.3 </span>Influence Sub-graphs</h3>\n\n<div id=\"A2.SS3.p1\" class=\"ltx_para\">\n<p id=\"A2.SS3.p1.1\" class=\"ltx_p\">We show the top 10 maximal positive and top 10 maximal native influence client pairs in Figure <a href=\"#A2.F17\" title=\"Figure 17 ‣ B.3 Influence Sub-graphs ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">17</span></a> and the top 15 in Figure <a href=\"#A2.F18\" title=\"Figure 18 ‣ B.3 Influence Sub-graphs ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">18</span></a>. We can see that the less biased client has a strong positive influence on other clients while the more biased client has a strong negative influence on other clients.</p>\n</div>\n<figure id=\"A2.F17\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><img src=\"/html/2309.02160/assets/images/income/propagation_path/overall_attr_1_metric_d_dp_appendix_top_10.png\" id=\"A2.F17.g1\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" width=\"598\" height=\"129\" alt=\"Refer to caption\"></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><img src=\"/html/2309.02160/assets/images/income/propagation_path/overall_attr_0_metric_d_dp_appendix_top_10.png\" id=\"A2.F17.g2\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" width=\"598\" height=\"130\" alt=\"Refer to caption\"></div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 17: </span><span id=\"A2.F17.2.1\" class=\"ltx_text ltx_font_bold\">Influence subgraph - Income (<math id=\"A2.F17.2.1.m1.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{DP}\" display=\"inline\"><semantics id=\"A2.F17.2.1.m1.1b\"><msup id=\"A2.F17.2.1.m1.1.1\" xref=\"A2.F17.2.1.m1.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A2.F17.2.1.m1.1.1.2\" xref=\"A2.F17.2.1.m1.1.1.2.cmml\">Δ</mi><mrow id=\"A2.F17.2.1.m1.1.1.3\" xref=\"A2.F17.2.1.m1.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A2.F17.2.1.m1.1.1.3.2\" xref=\"A2.F17.2.1.m1.1.1.3.2.cmml\">D</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A2.F17.2.1.m1.1.1.3.1\" xref=\"A2.F17.2.1.m1.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#F5FAFC\" id=\"A2.F17.2.1.m1.1.1.3.3\" xref=\"A2.F17.2.1.m1.1.1.3.3.cmml\">P</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A2.F17.2.1.m1.1c\"><apply id=\"A2.F17.2.1.m1.1.1.cmml\" xref=\"A2.F17.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.F17.2.1.m1.1.1.1.cmml\" xref=\"A2.F17.2.1.m1.1.1\">superscript</csymbol><ci id=\"A2.F17.2.1.m1.1.1.2.cmml\" xref=\"A2.F17.2.1.m1.1.1.2\">Δ</ci><apply id=\"A2.F17.2.1.m1.1.1.3.cmml\" xref=\"A2.F17.2.1.m1.1.1.3\"><times id=\"A2.F17.2.1.m1.1.1.3.1.cmml\" xref=\"A2.F17.2.1.m1.1.1.3.1\"></times><ci id=\"A2.F17.2.1.m1.1.1.3.2.cmml\" xref=\"A2.F17.2.1.m1.1.1.3.2\">𝐷</ci><ci id=\"A2.F17.2.1.m1.1.1.3.3.cmml\" xref=\"A2.F17.2.1.m1.1.1.3.3\">𝑃</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.F17.2.1.m1.1d\">\\Delta^{DP}</annotation></semantics></math>)</span> Top 10 maximal positive influential pair and top 10 maximal negative influential pair. The <span id=\"A2.F17.9.2\" class=\"ltx_text\" style=\"color:#7FB77E;\">green</span> edge and <span id=\"A2.F17.10.3\" class=\"ltx_text\" style=\"color:#EC7272;\">red</span> edge represent the <span id=\"A2.F17.11.4\" class=\"ltx_text\" style=\"color:#7FB77E;\">positive</span> and <span id=\"A2.F17.12.5\" class=\"ltx_text\" style=\"color:#EC7272;\">negative</span> influence, respectively. The <span id=\"A2.F17.13.6\" class=\"ltx_text\" style=\"color:#5C8AA8;\">color</span> of the node represents the <span id=\"A2.F17.14.7\" class=\"ltx_text\" style=\"color:#5C8AA8;\">fairness gap in the standalone setting</span>.</figcaption>\n</figure>\n<figure id=\"A2.F18\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><img src=\"/html/2309.02160/assets/images/income/propagation_path/overall_attr_1_metric_d_dp_appendix_top_15.png\" id=\"A2.F18.g1\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" width=\"598\" height=\"129\" alt=\"Refer to caption\"></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><img src=\"/html/2309.02160/assets/images/income/propagation_path/overall_attr_0_metric_d_dp_appendix_top_15.png\" id=\"A2.F18.g2\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" width=\"598\" height=\"130\" alt=\"Refer to caption\"></div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 18: </span><span id=\"A2.F18.2.1\" class=\"ltx_text ltx_font_bold\">Influence subgraph - Income (<math id=\"A2.F18.2.1.m1.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{DP}\" display=\"inline\"><semantics id=\"A2.F18.2.1.m1.1b\"><msup id=\"A2.F18.2.1.m1.1.1\" xref=\"A2.F18.2.1.m1.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A2.F18.2.1.m1.1.1.2\" xref=\"A2.F18.2.1.m1.1.1.2.cmml\">Δ</mi><mrow id=\"A2.F18.2.1.m1.1.1.3\" xref=\"A2.F18.2.1.m1.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A2.F18.2.1.m1.1.1.3.2\" xref=\"A2.F18.2.1.m1.1.1.3.2.cmml\">D</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A2.F18.2.1.m1.1.1.3.1\" xref=\"A2.F18.2.1.m1.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#F5FAFC\" id=\"A2.F18.2.1.m1.1.1.3.3\" xref=\"A2.F18.2.1.m1.1.1.3.3.cmml\">P</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A2.F18.2.1.m1.1c\"><apply id=\"A2.F18.2.1.m1.1.1.cmml\" xref=\"A2.F18.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.F18.2.1.m1.1.1.1.cmml\" xref=\"A2.F18.2.1.m1.1.1\">superscript</csymbol><ci id=\"A2.F18.2.1.m1.1.1.2.cmml\" xref=\"A2.F18.2.1.m1.1.1.2\">Δ</ci><apply id=\"A2.F18.2.1.m1.1.1.3.cmml\" xref=\"A2.F18.2.1.m1.1.1.3\"><times id=\"A2.F18.2.1.m1.1.1.3.1.cmml\" xref=\"A2.F18.2.1.m1.1.1.3.1\"></times><ci id=\"A2.F18.2.1.m1.1.1.3.2.cmml\" xref=\"A2.F18.2.1.m1.1.1.3.2\">𝐷</ci><ci id=\"A2.F18.2.1.m1.1.1.3.3.cmml\" xref=\"A2.F18.2.1.m1.1.1.3.3\">𝑃</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.F18.2.1.m1.1d\">\\Delta^{DP}</annotation></semantics></math>)</span> Top 15 maximal positive influential pair and top 15 maximal negative influential pair. The <span id=\"A2.F18.9.2\" class=\"ltx_text\" style=\"color:#7FB77E;\">green</span> edge and <span id=\"A2.F18.10.3\" class=\"ltx_text\" style=\"color:#EC7272;\">red</span> edge represent the <span id=\"A2.F18.11.4\" class=\"ltx_text\" style=\"color:#7FB77E;\">positive</span> and <span id=\"A2.F18.12.5\" class=\"ltx_text\" style=\"color:#EC7272;\">negative</span> influence, respectively. The <span id=\"A2.F18.13.6\" class=\"ltx_text\" style=\"color:#5C8AA8;\">color</span> of the node represents the <span id=\"A2.F18.14.7\" class=\"ltx_text\" style=\"color:#5C8AA8;\">fairness gap in the standalone setting</span>.</figcaption>\n</figure>\n</section>\n<section id=\"A2.SS4\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.4 </span>Attribution value for sensitive attribute</h3>\n\n<div id=\"A2.SS4.p1\" class=\"ltx_para\">\n<p id=\"A2.SS4.p1.1\" class=\"ltx_p\">We show the attribution value for all input features for the most biased party and least biased party in Figure <a href=\"#A2.F19\" title=\"Figure 19 ‣ B.4 Attribution value for sensitive attribute ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">19</span></a> and Figure <a href=\"#A2.F20\" title=\"Figure 20 ‣ B.4 Attribution value for sensitive attribute ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">20</span></a>, respectively. We observe that the attribution value for other features is similar for female and male groups. However, the sex attribute has the largest attribution value and affects the groups differently. It implies that the models are highly dependent on the sensitive attribute (i.e., \"Sex\") for making predictions.</p>\n</div>\n<figure id=\"A2.F19\" class=\"ltx_figure\"><img src=\"/html/2309.02160/assets/images/income/attribution_value/client_43_gender_199.png\" id=\"A2.F19.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"171\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 19: </span><span id=\"A2.F19.2.1\" class=\"ltx_text ltx_font_bold\">Feature attribution value - Income (Most biased party)</span> We shows the feature attribution value for all the input features. The model trained in FL and standalone setting has a large dependency on the sensitive attribute \"Sex\". </figcaption>\n</figure>\n<figure id=\"A2.F20\" class=\"ltx_figure\"><img src=\"/html/2309.02160/assets/images/income/attribution_value/client_50_gender_199.png\" id=\"A2.F20.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"171\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 20: </span><span id=\"A2.F20.2.1\" class=\"ltx_text ltx_font_bold\">Feature attribution value - Income (Least biased party)</span> We shows the feature attribution value for all the input features. The model trained in FL and standalone setting has a large dependency on the sensitive attribute \"Sex\". </figcaption>\n</figure>\n</section>\n<section id=\"A2.SS5\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.5 </span>Effect of FL algorithm</h3>\n\n<div id=\"A2.SS5.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"A2.SS5.p1.1\" class=\"ltx_p\">Table <a href=\"#A2.T2\" title=\"Table 2 ‣ B.5 Effect of FL algorithm ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows results on other FL algorithms, including FedNova <cite class=\"ltx_cite ltx_citemacro_citep\">[Wang et al., <a href=\"#bib.bib46\" title=\"\" class=\"ltx_ref\">2020</a>]</cite>, Scaffold <cite class=\"ltx_cite ltx_citemacro_cite\">Karimireddy et al. [<a href=\"#bib.bib24\" title=\"\" class=\"ltx_ref\">2020b</a>]</cite>, FedOpt <cite class=\"ltx_cite ltx_citemacro_citep\">[Asad et al., <a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">2020</a>]</cite>, FedProx <cite class=\"ltx_cite ltx_citemacro_cite\">Li et al. [<a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">2020</a>]</cite>, and Mime <cite class=\"ltx_cite ltx_citemacro_citep\">[Karimireddy et al., <a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">2020a</a>]</cite>.\nWe find that when the FL model achieves higher accuracy than standalone models, the fairness gap of the FL model can be higher than that of standalone models. This observation is consistent across multiple FL algorithms. This strong evidence implies that the improvement of accuracy in FL can come at the cost of fairness. In addition, this is not unique to only the FedAvg algorithm.</p>\n</div>\n<figure id=\"A2.T2\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span><span id=\"A2.T2.6.1\" class=\"ltx_text ltx_font_bold\">Benefit of collaboration on local datasets - Various FL algorithms (Income).</span> The standard deviation across five runs is indicated between parentheses.</figcaption>\n<div id=\"A2.T2.4\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:433.6pt;height:42.2pt;vertical-align:-32.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(42.2pt,-0.9pt) scale(1.2415726193884,1.2415726193884) ;\"><span id=\"A2.T2.4.5\" class=\"ltx_ERROR undefined\">\\csvreader</span>\n<p id=\"A2.T2.4.4\" class=\"ltx_p\">[tabular=ca*2e*2d,table head=    <span id=\"A2.T2.4.4.4\" class=\"ltx_text\" style=\"background-color:#F6FCF2;\"> Race     <span id=\"A2.T2.4.4.4.4\" class=\"ltx_text\" style=\"background-color:#F5FAFC;\">Sex  \n<br class=\"ltx_break\">Algorithm  Accuracy  <math id=\"A2.T2.1.1.1.1.m1.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{EO}\" display=\"inline\"><semantics id=\"A2.T2.1.1.1.1.m1.1a\"><msup id=\"A2.T2.1.1.1.1.m1.1.1\" xref=\"A2.T2.1.1.1.1.m1.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A2.T2.1.1.1.1.m1.1.1.2\" xref=\"A2.T2.1.1.1.1.m1.1.1.2.cmml\">Δ</mi><mrow id=\"A2.T2.1.1.1.1.m1.1.1.3\" xref=\"A2.T2.1.1.1.1.m1.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.1.1.1.1.m1.1.1.3.2\" xref=\"A2.T2.1.1.1.1.m1.1.1.3.2.cmml\">E</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A2.T2.1.1.1.1.m1.1.1.3.1\" xref=\"A2.T2.1.1.1.1.m1.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.1.1.1.1.m1.1.1.3.3\" xref=\"A2.T2.1.1.1.1.m1.1.1.3.3.cmml\">O</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A2.T2.1.1.1.1.m1.1b\"><apply id=\"A2.T2.1.1.1.1.m1.1.1.cmml\" xref=\"A2.T2.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.T2.1.1.1.1.m1.1.1.1.cmml\" xref=\"A2.T2.1.1.1.1.m1.1.1\">superscript</csymbol><ci id=\"A2.T2.1.1.1.1.m1.1.1.2.cmml\" xref=\"A2.T2.1.1.1.1.m1.1.1.2\">Δ</ci><apply id=\"A2.T2.1.1.1.1.m1.1.1.3.cmml\" xref=\"A2.T2.1.1.1.1.m1.1.1.3\"><times id=\"A2.T2.1.1.1.1.m1.1.1.3.1.cmml\" xref=\"A2.T2.1.1.1.1.m1.1.1.3.1\"></times><ci id=\"A2.T2.1.1.1.1.m1.1.1.3.2.cmml\" xref=\"A2.T2.1.1.1.1.m1.1.1.3.2\">𝐸</ci><ci id=\"A2.T2.1.1.1.1.m1.1.1.3.3.cmml\" xref=\"A2.T2.1.1.1.1.m1.1.1.3.3\">𝑂</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T2.1.1.1.1.m1.1c\">\\Delta^{EO}</annotation></semantics></math>  <math id=\"A2.T2.2.2.2.2.m2.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{DP}\" display=\"inline\"><semantics id=\"A2.T2.2.2.2.2.m2.1a\"><msup id=\"A2.T2.2.2.2.2.m2.1.1\" xref=\"A2.T2.2.2.2.2.m2.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A2.T2.2.2.2.2.m2.1.1.2\" xref=\"A2.T2.2.2.2.2.m2.1.1.2.cmml\">Δ</mi><mrow id=\"A2.T2.2.2.2.2.m2.1.1.3\" xref=\"A2.T2.2.2.2.2.m2.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.2.2.2.2.m2.1.1.3.2\" xref=\"A2.T2.2.2.2.2.m2.1.1.3.2.cmml\">D</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A2.T2.2.2.2.2.m2.1.1.3.1\" xref=\"A2.T2.2.2.2.2.m2.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.2.2.2.2.m2.1.1.3.3\" xref=\"A2.T2.2.2.2.2.m2.1.1.3.3.cmml\">P</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A2.T2.2.2.2.2.m2.1b\"><apply id=\"A2.T2.2.2.2.2.m2.1.1.cmml\" xref=\"A2.T2.2.2.2.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.T2.2.2.2.2.m2.1.1.1.cmml\" xref=\"A2.T2.2.2.2.2.m2.1.1\">superscript</csymbol><ci id=\"A2.T2.2.2.2.2.m2.1.1.2.cmml\" xref=\"A2.T2.2.2.2.2.m2.1.1.2\">Δ</ci><apply id=\"A2.T2.2.2.2.2.m2.1.1.3.cmml\" xref=\"A2.T2.2.2.2.2.m2.1.1.3\"><times id=\"A2.T2.2.2.2.2.m2.1.1.3.1.cmml\" xref=\"A2.T2.2.2.2.2.m2.1.1.3.1\"></times><ci id=\"A2.T2.2.2.2.2.m2.1.1.3.2.cmml\" xref=\"A2.T2.2.2.2.2.m2.1.1.3.2\">𝐷</ci><ci id=\"A2.T2.2.2.2.2.m2.1.1.3.3.cmml\" xref=\"A2.T2.2.2.2.2.m2.1.1.3.3\">𝑃</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T2.2.2.2.2.m2.1c\">\\Delta^{DP}</annotation></semantics></math> <math id=\"A2.T2.3.3.3.3.m3.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{EO}\" display=\"inline\"><semantics id=\"A2.T2.3.3.3.3.m3.1a\"><msup id=\"A2.T2.3.3.3.3.m3.1.1\" xref=\"A2.T2.3.3.3.3.m3.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A2.T2.3.3.3.3.m3.1.1.2\" xref=\"A2.T2.3.3.3.3.m3.1.1.2.cmml\">Δ</mi><mrow id=\"A2.T2.3.3.3.3.m3.1.1.3\" xref=\"A2.T2.3.3.3.3.m3.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.3.3.3.3.m3.1.1.3.2\" xref=\"A2.T2.3.3.3.3.m3.1.1.3.2.cmml\">E</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A2.T2.3.3.3.3.m3.1.1.3.1\" xref=\"A2.T2.3.3.3.3.m3.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.3.3.3.3.m3.1.1.3.3\" xref=\"A2.T2.3.3.3.3.m3.1.1.3.3.cmml\">O</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A2.T2.3.3.3.3.m3.1b\"><apply id=\"A2.T2.3.3.3.3.m3.1.1.cmml\" xref=\"A2.T2.3.3.3.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.T2.3.3.3.3.m3.1.1.1.cmml\" xref=\"A2.T2.3.3.3.3.m3.1.1\">superscript</csymbol><ci id=\"A2.T2.3.3.3.3.m3.1.1.2.cmml\" xref=\"A2.T2.3.3.3.3.m3.1.1.2\">Δ</ci><apply id=\"A2.T2.3.3.3.3.m3.1.1.3.cmml\" xref=\"A2.T2.3.3.3.3.m3.1.1.3\"><times id=\"A2.T2.3.3.3.3.m3.1.1.3.1.cmml\" xref=\"A2.T2.3.3.3.3.m3.1.1.3.1\"></times><ci id=\"A2.T2.3.3.3.3.m3.1.1.3.2.cmml\" xref=\"A2.T2.3.3.3.3.m3.1.1.3.2\">𝐸</ci><ci id=\"A2.T2.3.3.3.3.m3.1.1.3.3.cmml\" xref=\"A2.T2.3.3.3.3.m3.1.1.3.3\">𝑂</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T2.3.3.3.3.m3.1c\">\\Delta^{EO}</annotation></semantics></math>  <math id=\"A2.T2.4.4.4.4.m4.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{DP}\" display=\"inline\"><semantics id=\"A2.T2.4.4.4.4.m4.1a\"><msup id=\"A2.T2.4.4.4.4.m4.1.1\" xref=\"A2.T2.4.4.4.4.m4.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A2.T2.4.4.4.4.m4.1.1.2\" xref=\"A2.T2.4.4.4.4.m4.1.1.2.cmml\">Δ</mi><mrow id=\"A2.T2.4.4.4.4.m4.1.1.3\" xref=\"A2.T2.4.4.4.4.m4.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.4.4.4.4.m4.1.1.3.2\" xref=\"A2.T2.4.4.4.4.m4.1.1.3.2.cmml\">D</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A2.T2.4.4.4.4.m4.1.1.3.1\" xref=\"A2.T2.4.4.4.4.m4.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.4.4.4.4.m4.1.1.3.3\" xref=\"A2.T2.4.4.4.4.m4.1.1.3.3.cmml\">P</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A2.T2.4.4.4.4.m4.1b\"><apply id=\"A2.T2.4.4.4.4.m4.1.1.cmml\" xref=\"A2.T2.4.4.4.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.T2.4.4.4.4.m4.1.1.1.cmml\" xref=\"A2.T2.4.4.4.4.m4.1.1\">superscript</csymbol><ci id=\"A2.T2.4.4.4.4.m4.1.1.2.cmml\" xref=\"A2.T2.4.4.4.4.m4.1.1.2\">Δ</ci><apply id=\"A2.T2.4.4.4.4.m4.1.1.3.cmml\" xref=\"A2.T2.4.4.4.4.m4.1.1.3\"><times id=\"A2.T2.4.4.4.4.m4.1.1.3.1.cmml\" xref=\"A2.T2.4.4.4.4.m4.1.1.3.1\"></times><ci id=\"A2.T2.4.4.4.4.m4.1.1.3.2.cmml\" xref=\"A2.T2.4.4.4.4.m4.1.1.3.2\">𝐷</ci><ci id=\"A2.T2.4.4.4.4.m4.1.1.3.3.cmml\" xref=\"A2.T2.4.4.4.4.m4.1.1.3.3\">𝑃</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T2.4.4.4.4.m4.1c\">\\Delta^{DP}</annotation></semantics></math> \n<br class=\"ltx_break\">, table foot=]data/data_other_fl.csv\n <span id=\"A2.T2.4.4.4.4.1\" class=\"ltx_ERROR undefined\">\\csvcoli</span> <span id=\"A2.T2.4.4.4.4.2\" class=\"ltx_ERROR undefined\">\\csvcolii</span> (<span id=\"A2.T2.4.4.4.4.3\" class=\"ltx_ERROR undefined\">\\csvcoliii</span>)  <span id=\"A2.T2.4.4.4.4.4\" class=\"ltx_ERROR undefined\">\\csvcoliv</span> (<span id=\"A2.T2.4.4.4.4.5\" class=\"ltx_ERROR undefined\">\\csvcolv</span>)  <span id=\"A2.T2.4.4.4.4.6\" class=\"ltx_ERROR undefined\">\\csvcolvi</span> (<span id=\"A2.T2.4.4.4.4.7\" class=\"ltx_ERROR undefined\">\\csvcolvii</span>)  <span id=\"A2.T2.4.4.4.4.8\" class=\"ltx_ERROR undefined\">\\csvcolviii</span> (<span id=\"A2.T2.4.4.4.4.9\" class=\"ltx_ERROR undefined\">\\csvcolix</span>)  <span id=\"A2.T2.4.4.4.4.10\" class=\"ltx_ERROR undefined\">\\csvcolx</span> (<span id=\"A2.T2.4.4.4.4.11\" class=\"ltx_ERROR undefined\">\\csvcolxi</span>)</span></span></p>\n</span></div>\n</figure>\n</section>\n<section id=\"A2.SS6\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.6 </span>Group Performance</h3>\n\n<div id=\"A2.SS6.p1\" class=\"ltx_para\">\n<p id=\"A2.SS6.p1.1\" class=\"ltx_p\">In Figure <a href=\"#A2.F21\" title=\"Figure 21 ‣ B.6 Group Performance ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">21</span></a>, we present the model ROC on groups to illustrate the performance disparity across groups. Surprisingly, there is no huge difference between the ROC between groups. In contrast, Figure <a href=\"#A2.F22\" title=\"Figure 22 ‣ B.6 Group Performance ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">22</span></a> shows the noticeable difference between groups with respect to the precision-recall curve, especially for the most biased party. The main reason is that the dataset is imbalanced. Thus, ROC is usually misleading. On the precision-recall curve, we find that the model achieves a higher precision on the majority (i.e., Male group) compared to the minority group (i.e., Female group).</p>\n</div>\n<figure id=\"A2.F21\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><img src=\"/html/2309.02160/assets/images/income/group_roc/roc_attr_1_43.png\" id=\"A2.F21.g1\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" width=\"598\" height=\"190\" alt=\"Refer to caption\"></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><img src=\"/html/2309.02160/assets/images/income/group_roc/roc_attr_1_50.png\" id=\"A2.F21.g2\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" width=\"598\" height=\"190\" alt=\"Refer to caption\"></div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 21: </span>Receiver operating characteristic (ROC) of different models on groups - (Income, Sex)</figcaption>\n</figure>\n<figure id=\"A2.F22\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><img src=\"/html/2309.02160/assets/images/income/group_roc/ppc_attr_1_43.png\" id=\"A2.F22.g1\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" width=\"598\" height=\"189\" alt=\"Refer to caption\"></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><img src=\"/html/2309.02160/assets/images/income/group_roc/ppc_attr_1_50.png\" id=\"A2.F22.g2\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" width=\"598\" height=\"189\" alt=\"Refer to caption\"></div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 22: </span>Precision-Recall curve (PRC) of different models on groups - (Income, Sex)</figcaption>\n</figure>\n</section>\n<section id=\"A2.SS7\" class=\"ltx_subsection\">\n<h3 class=\"ltx_title ltx_title_subsection\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.7 </span>Effect of scaling other parameters</h3>\n\n<div id=\"A2.SS7.p1\" class=\"ltx_para\">\n<p id=\"A2.SS7.p1.1\" class=\"ltx_p\">Figure <a href=\"#A2.F23\" title=\"Figure 23 ‣ B.7 Effect of scaling other parameters ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">23</span></a> shows a comparison of the model’s performance when scaling different model parameters. Specifically, we examine the impact of scaling the parameters in the first layer of the neural network that does not have any computation on the Sex attribute (referred to as \"Other parameters\"), as well as the parameter related to the Sex attribute (referred to as \"Related parameters\"). The results indicate that up-scaling or down-scaling other parameters do not significantly affect the fairness gap while scaling the related parameters has a considerable impact on model fairness. This finding supports our hypothesis that the model’s bias is primarily encoded in a few model parameters. In FL, biased parties introduce bias during local training by increasing the weights for those related parameters.</p>\n</div>\n<figure id=\"A2.F23\" class=\"ltx_figure\"><img src=\"/html/2309.02160/assets/images/income/effect_parameter/effect_scaling_comparision_199.png\" id=\"A2.F23.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"598\" height=\"169\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 23: </span>Effect of scaling model parameters - (Income, Sex)</figcaption>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>\n</section>\n<section id=\"A3\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix C </span>Evaluation on existing Fair FL</h2>\n\n<div id=\"A3.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"A3.p1.1\" class=\"ltx_p\">Table <a href=\"#A3.T3\" title=\"Table 3 ‣ Appendix C Evaluation on existing Fair FL ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents the results for a fair FL algorithm proposed by <cite class=\"ltx_cite ltx_citemacro_cite\">Abay et al. [<a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2020</a>]</cite>, which uses a reweighting algorithm proposed in the centralized setting <cite class=\"ltx_cite ltx_citemacro_citep\">[Kamiran and Calders, <a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">2012</a>]</cite>. The algorithm involves assigning weights to each subgroup (defined by the label and a sensitive attribute) before FL, based on the local or global training dataset. This is called \"local reweighting\" or \"global reweighting,\" respectively. During FL, parties update the FL model to minimize the weighted loss. The table presents the accuracy and fairness gap of the FL algorithm using local and global reweighting for the Income, Health, and Employment datasets.</p>\n</div>\n<figure id=\"A3.T3\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span><span id=\"A3.T3.6.1\" class=\"ltx_text ltx_font_bold\">Effect of Fair FL <cite class=\"ltx_cite ltx_citemacro_citep\">[Abay et al., <a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2020</a>]</cite> - (Income).</span> The standard deviation across five runs is indicated between parentheses. The \"Sensitive Attribute\" column indicates the sensitive attribute used in the reweighting algorithm.</figcaption>\n<div id=\"A3.T3.4\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:433.6pt;height:40.8pt;vertical-align:-31.8pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(36.1pt,-0.7pt) scale(1.19959219035754,1.19959219035754) ;\"><span id=\"A3.T3.4.5\" class=\"ltx_ERROR undefined\">\\csvreader</span>\n<p id=\"A3.T3.4.4\" class=\"ltx_p\">[tabular=cca*2e*2d,table head=     <span id=\"A3.T3.4.4.4\" class=\"ltx_text\" style=\"background-color:#F6FCF2;\"> Race     <span id=\"A3.T3.4.4.4.4\" class=\"ltx_text\" style=\"background-color:#F5FAFC;\">Sex  \n<br class=\"ltx_break\">Algorithm  Sensitive Attribute  Accuracy  <math id=\"A3.T3.1.1.1.1.m1.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{EO}\" display=\"inline\"><semantics id=\"A3.T3.1.1.1.1.m1.1a\"><msup id=\"A3.T3.1.1.1.1.m1.1.1\" xref=\"A3.T3.1.1.1.1.m1.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A3.T3.1.1.1.1.m1.1.1.2\" xref=\"A3.T3.1.1.1.1.m1.1.1.2.cmml\">Δ</mi><mrow id=\"A3.T3.1.1.1.1.m1.1.1.3\" xref=\"A3.T3.1.1.1.1.m1.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.1.1.1.1.m1.1.1.3.2\" xref=\"A3.T3.1.1.1.1.m1.1.1.3.2.cmml\">E</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A3.T3.1.1.1.1.m1.1.1.3.1\" xref=\"A3.T3.1.1.1.1.m1.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.1.1.1.1.m1.1.1.3.3\" xref=\"A3.T3.1.1.1.1.m1.1.1.3.3.cmml\">O</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A3.T3.1.1.1.1.m1.1b\"><apply id=\"A3.T3.1.1.1.1.m1.1.1.cmml\" xref=\"A3.T3.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T3.1.1.1.1.m1.1.1.1.cmml\" xref=\"A3.T3.1.1.1.1.m1.1.1\">superscript</csymbol><ci id=\"A3.T3.1.1.1.1.m1.1.1.2.cmml\" xref=\"A3.T3.1.1.1.1.m1.1.1.2\">Δ</ci><apply id=\"A3.T3.1.1.1.1.m1.1.1.3.cmml\" xref=\"A3.T3.1.1.1.1.m1.1.1.3\"><times id=\"A3.T3.1.1.1.1.m1.1.1.3.1.cmml\" xref=\"A3.T3.1.1.1.1.m1.1.1.3.1\"></times><ci id=\"A3.T3.1.1.1.1.m1.1.1.3.2.cmml\" xref=\"A3.T3.1.1.1.1.m1.1.1.3.2\">𝐸</ci><ci id=\"A3.T3.1.1.1.1.m1.1.1.3.3.cmml\" xref=\"A3.T3.1.1.1.1.m1.1.1.3.3\">𝑂</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T3.1.1.1.1.m1.1c\">\\Delta^{EO}</annotation></semantics></math>  <math id=\"A3.T3.2.2.2.2.m2.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{DP}\" display=\"inline\"><semantics id=\"A3.T3.2.2.2.2.m2.1a\"><msup id=\"A3.T3.2.2.2.2.m2.1.1\" xref=\"A3.T3.2.2.2.2.m2.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A3.T3.2.2.2.2.m2.1.1.2\" xref=\"A3.T3.2.2.2.2.m2.1.1.2.cmml\">Δ</mi><mrow id=\"A3.T3.2.2.2.2.m2.1.1.3\" xref=\"A3.T3.2.2.2.2.m2.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.2.2.2.2.m2.1.1.3.2\" xref=\"A3.T3.2.2.2.2.m2.1.1.3.2.cmml\">D</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A3.T3.2.2.2.2.m2.1.1.3.1\" xref=\"A3.T3.2.2.2.2.m2.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.2.2.2.2.m2.1.1.3.3\" xref=\"A3.T3.2.2.2.2.m2.1.1.3.3.cmml\">P</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A3.T3.2.2.2.2.m2.1b\"><apply id=\"A3.T3.2.2.2.2.m2.1.1.cmml\" xref=\"A3.T3.2.2.2.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T3.2.2.2.2.m2.1.1.1.cmml\" xref=\"A3.T3.2.2.2.2.m2.1.1\">superscript</csymbol><ci id=\"A3.T3.2.2.2.2.m2.1.1.2.cmml\" xref=\"A3.T3.2.2.2.2.m2.1.1.2\">Δ</ci><apply id=\"A3.T3.2.2.2.2.m2.1.1.3.cmml\" xref=\"A3.T3.2.2.2.2.m2.1.1.3\"><times id=\"A3.T3.2.2.2.2.m2.1.1.3.1.cmml\" xref=\"A3.T3.2.2.2.2.m2.1.1.3.1\"></times><ci id=\"A3.T3.2.2.2.2.m2.1.1.3.2.cmml\" xref=\"A3.T3.2.2.2.2.m2.1.1.3.2\">𝐷</ci><ci id=\"A3.T3.2.2.2.2.m2.1.1.3.3.cmml\" xref=\"A3.T3.2.2.2.2.m2.1.1.3.3\">𝑃</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T3.2.2.2.2.m2.1c\">\\Delta^{DP}</annotation></semantics></math> <math id=\"A3.T3.3.3.3.3.m3.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{EO}\" display=\"inline\"><semantics id=\"A3.T3.3.3.3.3.m3.1a\"><msup id=\"A3.T3.3.3.3.3.m3.1.1\" xref=\"A3.T3.3.3.3.3.m3.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A3.T3.3.3.3.3.m3.1.1.2\" xref=\"A3.T3.3.3.3.3.m3.1.1.2.cmml\">Δ</mi><mrow id=\"A3.T3.3.3.3.3.m3.1.1.3\" xref=\"A3.T3.3.3.3.3.m3.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.3.3.3.3.m3.1.1.3.2\" xref=\"A3.T3.3.3.3.3.m3.1.1.3.2.cmml\">E</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A3.T3.3.3.3.3.m3.1.1.3.1\" xref=\"A3.T3.3.3.3.3.m3.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.3.3.3.3.m3.1.1.3.3\" xref=\"A3.T3.3.3.3.3.m3.1.1.3.3.cmml\">O</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A3.T3.3.3.3.3.m3.1b\"><apply id=\"A3.T3.3.3.3.3.m3.1.1.cmml\" xref=\"A3.T3.3.3.3.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T3.3.3.3.3.m3.1.1.1.cmml\" xref=\"A3.T3.3.3.3.3.m3.1.1\">superscript</csymbol><ci id=\"A3.T3.3.3.3.3.m3.1.1.2.cmml\" xref=\"A3.T3.3.3.3.3.m3.1.1.2\">Δ</ci><apply id=\"A3.T3.3.3.3.3.m3.1.1.3.cmml\" xref=\"A3.T3.3.3.3.3.m3.1.1.3\"><times id=\"A3.T3.3.3.3.3.m3.1.1.3.1.cmml\" xref=\"A3.T3.3.3.3.3.m3.1.1.3.1\"></times><ci id=\"A3.T3.3.3.3.3.m3.1.1.3.2.cmml\" xref=\"A3.T3.3.3.3.3.m3.1.1.3.2\">𝐸</ci><ci id=\"A3.T3.3.3.3.3.m3.1.1.3.3.cmml\" xref=\"A3.T3.3.3.3.3.m3.1.1.3.3\">𝑂</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T3.3.3.3.3.m3.1c\">\\Delta^{EO}</annotation></semantics></math>  <math id=\"A3.T3.4.4.4.4.m4.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{DP}\" display=\"inline\"><semantics id=\"A3.T3.4.4.4.4.m4.1a\"><msup id=\"A3.T3.4.4.4.4.m4.1.1\" xref=\"A3.T3.4.4.4.4.m4.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A3.T3.4.4.4.4.m4.1.1.2\" xref=\"A3.T3.4.4.4.4.m4.1.1.2.cmml\">Δ</mi><mrow id=\"A3.T3.4.4.4.4.m4.1.1.3\" xref=\"A3.T3.4.4.4.4.m4.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.4.4.4.4.m4.1.1.3.2\" xref=\"A3.T3.4.4.4.4.m4.1.1.3.2.cmml\">D</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A3.T3.4.4.4.4.m4.1.1.3.1\" xref=\"A3.T3.4.4.4.4.m4.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.4.4.4.4.m4.1.1.3.3\" xref=\"A3.T3.4.4.4.4.m4.1.1.3.3.cmml\">P</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A3.T3.4.4.4.4.m4.1b\"><apply id=\"A3.T3.4.4.4.4.m4.1.1.cmml\" xref=\"A3.T3.4.4.4.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T3.4.4.4.4.m4.1.1.1.cmml\" xref=\"A3.T3.4.4.4.4.m4.1.1\">superscript</csymbol><ci id=\"A3.T3.4.4.4.4.m4.1.1.2.cmml\" xref=\"A3.T3.4.4.4.4.m4.1.1.2\">Δ</ci><apply id=\"A3.T3.4.4.4.4.m4.1.1.3.cmml\" xref=\"A3.T3.4.4.4.4.m4.1.1.3\"><times id=\"A3.T3.4.4.4.4.m4.1.1.3.1.cmml\" xref=\"A3.T3.4.4.4.4.m4.1.1.3.1\"></times><ci id=\"A3.T3.4.4.4.4.m4.1.1.3.2.cmml\" xref=\"A3.T3.4.4.4.4.m4.1.1.3.2\">𝐷</ci><ci id=\"A3.T3.4.4.4.4.m4.1.1.3.3.cmml\" xref=\"A3.T3.4.4.4.4.m4.1.1.3.3\">𝑃</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T3.4.4.4.4.m4.1c\">\\Delta^{DP}</annotation></semantics></math> \n<br class=\"ltx_break\">, table foot=]data/data_reweiging.csv\n <span id=\"A3.T3.4.4.4.4.1\" class=\"ltx_ERROR undefined\">\\csvcoli</span> <span id=\"A3.T3.4.4.4.4.2\" class=\"ltx_ERROR undefined\">\\csvcolxii</span><span id=\"A3.T3.4.4.4.4.3\" class=\"ltx_ERROR undefined\">\\csvcolii</span> (<span id=\"A3.T3.4.4.4.4.4\" class=\"ltx_ERROR undefined\">\\csvcoliii</span>)  <span id=\"A3.T3.4.4.4.4.5\" class=\"ltx_ERROR undefined\">\\csvcoliv</span> (<span id=\"A3.T3.4.4.4.4.6\" class=\"ltx_ERROR undefined\">\\csvcolv</span>)  <span id=\"A3.T3.4.4.4.4.7\" class=\"ltx_ERROR undefined\">\\csvcolvi</span> (<span id=\"A3.T3.4.4.4.4.8\" class=\"ltx_ERROR undefined\">\\csvcolvii</span>)  <span id=\"A3.T3.4.4.4.4.9\" class=\"ltx_ERROR undefined\">\\csvcolviii</span> (<span id=\"A3.T3.4.4.4.4.10\" class=\"ltx_ERROR undefined\">\\csvcolix</span>)  <span id=\"A3.T3.4.4.4.4.11\" class=\"ltx_ERROR undefined\">\\csvcolx</span> (<span id=\"A3.T3.4.4.4.4.12\" class=\"ltx_ERROR undefined\">\\csvcolxi</span>)</span></span></p>\n</span></div>\n</figure>\n<div id=\"A3.p2\" class=\"ltx_para ltx_noindent\">\n<p id=\"A3.p2.1\" class=\"ltx_p\"><span id=\"A3.p2.1.1\" class=\"ltx_text\">We found that applying reweighing algorithms in FL reduces the average fairness gap across parties. This is due to the fact that, after reweighing, the parties that were initially biased do not introduce significant bias to the model during local training, resulting in a reduction of the fairness gap in the FL model.</span></p>\n</div>\n<div id=\"A3.p3\" class=\"ltx_para\">\n<p id=\"A3.p3.1\" class=\"ltx_p\">However, we also noted that the fairness gap in the model remains high for the Race sensitive attribute, indicating that the global and local reweighing algorithms do not entirely eliminate bias in FL. Furthermore, we would like to highlight some concerns regarding the fair FL algorithm:</p>\n<ol id=\"A3.I1\" class=\"ltx_enumerate\">\n<li id=\"A3.I1.i1\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">1.</span> \n<div id=\"A3.I1.i1.p1\" class=\"ltx_para\">\n<p id=\"A3.I1.i1.p1.1\" class=\"ltx_p\">The fair FL algorithm enhances the performance of the minority group at the expense of the majority group. Figure <a href=\"#A3.F24\" title=\"Figure 24 ‣ Appendix C Evaluation on existing Fair FL ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">24</span></a> illustrates that the minority group (i.e., the Female group) has a better performance after the reweighing, but at the cost of reduced performance for the majority.</p>\n</div>\n</li>\n<li id=\"A3.I1.i2\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">2.</span> \n<div id=\"A3.I1.i2.p1\" class=\"ltx_para\">\n<p id=\"A3.I1.i2.p1.1\" class=\"ltx_p\">When improving fairness with respect to one sensitive attribute, the fairness issue with respect to another sensitive attribute may worsen, as shown in Table <a href=\"#A3.T3\" title=\"Table 3 ‣ Appendix C Evaluation on existing Fair FL ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. For instance, when applying local reweighing to reduce bias with respect to the \"Sex\" attribute, the fairness gap with respect to \"Race\" increased from 0.514 (results on FedAvg) to 0.521, indicating that reweighing to improve fairness with respect to Sex may amplify the bias with respect to Race. This implies that the bias with respect to Race can still propagate in FL, and our analysis remains valid.</p>\n</div>\n</li>\n<li id=\"A3.I1.i3\" class=\"ltx_item\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">3.</span> \n<div id=\"A3.I1.i3.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"A3.I1.i3.p1.1\" class=\"ltx_p\">The reweighing algorithms assume that parties are interested in mitigating the bias with respect to the same sensitive attribute, which may not be the case in practice. Figure <a href=\"#A3.F25\" title=\"Figure 25 ‣ Appendix C Evaluation on existing Fair FL ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">25</span></a> shows the fairness gap of standalone models with respect to different sensitive attributes. We observe that many parties have a low fairness gap with respect to one sensitive attribute and a high fairness gap with respect to another sensitive attribute. Thus, parties may aim to enhance fairness with respect to different sensitive attributes, which renders this reweighing algorithm unsuitable. This problem is more prevalent in FL, given the different data distributions of parties.</p>\n</div>\n</li>\n</ol>\n</div>\n<figure id=\"A3.F24\" class=\"ltx_figure\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\"><img src=\"/html/2309.02160/assets/images/income/reweiging/income_group_PPV_global_reweighing_fedavg.png\" id=\"A3.F24.g1\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" width=\"287\" height=\"216\" alt=\"Refer to caption\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\"><img src=\"/html/2309.02160/assets/images/income/reweiging/income_group_TPR_global_reweighing_fedavg.png\" id=\"A3.F24.g2\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" width=\"287\" height=\"216\" alt=\"Refer to caption\"></div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 24: </span>Effect of Fair FL algorithm <cite class=\"ltx_cite ltx_citemacro_citep\">[Abay et al., <a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2020</a>]</cite> on group performance - (Income, Sex)</figcaption>\n</figure>\n<figure id=\"A3.F25\" class=\"ltx_figure\"><img src=\"/html/2309.02160/assets/images/income/reweiging/income_standalone_race_sex_gap.png\" id=\"A3.F25.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"479\" height=\"180\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 25: </span>Fairness gap of standalone models with respect to different sensitive attributes - (Income). Each point represents the result for a single party in one run. We show the fairness gap with respect to different sensitive attributes differs a lot for each party.</figcaption>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>\n<section id=\"A4\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix D </span>Extending to a real-world medical dataset</h2>\n\n<figure id=\"A4.T4\" class=\"ltx_table\">\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span><span id=\"A4.T4.6.1\" class=\"ltx_text ltx_font_bold\">Effect of FL on the real-world medical dataset - (ISIC2019).</span> The standard deviation across four runs is indicated between parentheses. </figcaption>\n<div id=\"A4.T4.4\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:433.6pt;height:62.9pt;vertical-align:-54.8pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(16.9pt,-0.3pt) scale(1.0845000976968,1.0845000976968) ;\"><span id=\"A4.T4.4.5\" class=\"ltx_ERROR undefined\">\\csvreader</span>\n<p id=\"A4.T4.4.4\" class=\"ltx_p\">[tabular=c*2a*2e*2d*2f,table head= Setting   <span id=\"A4.T4.4.4.4\" class=\"ltx_text\" style=\"background-color:#FFF3F3;\"> Party 1    <span id=\"A4.T4.4.4.4.4\" class=\"ltx_text\" style=\"background-color:#F6FCF2;\"> Party 2     <span id=\"A4.T4.4.4.4.4.4\" class=\"ltx_text\" style=\"background-color:#F5FAFC;\"> Party 3     <span id=\"A4.T4.4.4.4.4.4.4\" class=\"ltx_text\" style=\"background-color:#FFF9F8;\"> Party 4 \n<br class=\"ltx_break\">Algorithm  Accuracy  <math id=\"A4.T4.1.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\Delta^{Acc}\" display=\"inline\"><semantics id=\"A4.T4.1.1.1.1.1.1.m1.1a\"><msup id=\"A4.T4.1.1.1.1.1.1.m1.1.1\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.cmml\"><mi mathbackground=\"#FFF9F8\" mathvariant=\"normal\" id=\"A4.T4.1.1.1.1.1.1.m1.1.1.2\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.2.cmml\">Δ</mi><mrow id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.cmml\"><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.2\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.1\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.3\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.3.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.1a\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.4\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.4.cmml\">c</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A4.T4.1.1.1.1.1.1.m1.1b\"><apply id=\"A4.T4.1.1.1.1.1.1.m1.1.1.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A4.T4.1.1.1.1.1.1.m1.1.1.1.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1\">superscript</csymbol><ci id=\"A4.T4.1.1.1.1.1.1.m1.1.1.2.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.2\">Δ</ci><apply id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3\"><times id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.1.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.1\"></times><ci id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.2.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.2\">𝐴</ci><ci id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.3.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.3\">𝑐</ci><ci id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.4.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.4\">𝑐</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T4.1.1.1.1.1.1.m1.1c\">\\Delta^{Acc}</annotation></semantics></math>  Accuracy  <math id=\"A4.T4.2.2.2.2.2.2.m2.1\" class=\"ltx_Math\" alttext=\"\\Delta^{Acc}\" display=\"inline\"><semantics id=\"A4.T4.2.2.2.2.2.2.m2.1a\"><msup id=\"A4.T4.2.2.2.2.2.2.m2.1.1\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.cmml\"><mi mathbackground=\"#FFF9F8\" mathvariant=\"normal\" id=\"A4.T4.2.2.2.2.2.2.m2.1.1.2\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.2.cmml\">Δ</mi><mrow id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.cmml\"><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.2\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.1\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.3\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.3.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.1a\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.4\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.4.cmml\">c</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A4.T4.2.2.2.2.2.2.m2.1b\"><apply id=\"A4.T4.2.2.2.2.2.2.m2.1.1.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"A4.T4.2.2.2.2.2.2.m2.1.1.1.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1\">superscript</csymbol><ci id=\"A4.T4.2.2.2.2.2.2.m2.1.1.2.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.2\">Δ</ci><apply id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3\"><times id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.1.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.1\"></times><ci id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.2.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.2\">𝐴</ci><ci id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.3.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.3\">𝑐</ci><ci id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.4.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.4\">𝑐</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T4.2.2.2.2.2.2.m2.1c\">\\Delta^{Acc}</annotation></semantics></math>  Accuracy  <math id=\"A4.T4.3.3.3.3.3.3.m3.1\" class=\"ltx_Math\" alttext=\"\\Delta^{Acc}\" display=\"inline\"><semantics id=\"A4.T4.3.3.3.3.3.3.m3.1a\"><msup id=\"A4.T4.3.3.3.3.3.3.m3.1.1\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.cmml\"><mi mathbackground=\"#FFF9F8\" mathvariant=\"normal\" id=\"A4.T4.3.3.3.3.3.3.m3.1.1.2\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.2.cmml\">Δ</mi><mrow id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.cmml\"><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.2\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.1\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.3\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.3.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.1a\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.4\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.4.cmml\">c</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A4.T4.3.3.3.3.3.3.m3.1b\"><apply id=\"A4.T4.3.3.3.3.3.3.m3.1.1.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"A4.T4.3.3.3.3.3.3.m3.1.1.1.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1\">superscript</csymbol><ci id=\"A4.T4.3.3.3.3.3.3.m3.1.1.2.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.2\">Δ</ci><apply id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3\"><times id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.1.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.1\"></times><ci id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.2.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.2\">𝐴</ci><ci id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.3.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.3\">𝑐</ci><ci id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.4.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.4\">𝑐</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T4.3.3.3.3.3.3.m3.1c\">\\Delta^{Acc}</annotation></semantics></math>  Accuracy  <math id=\"A4.T4.4.4.4.4.4.4.m4.1\" class=\"ltx_Math\" alttext=\"\\Delta^{Acc}\" display=\"inline\"><semantics id=\"A4.T4.4.4.4.4.4.4.m4.1a\"><msup id=\"A4.T4.4.4.4.4.4.4.m4.1.1\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.cmml\"><mi mathbackground=\"#FFF9F8\" mathvariant=\"normal\" id=\"A4.T4.4.4.4.4.4.4.m4.1.1.2\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.2.cmml\">Δ</mi><mrow id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.cmml\"><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.2\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.1\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.3\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.3.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.1a\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.4\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.4.cmml\">c</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A4.T4.4.4.4.4.4.4.m4.1b\"><apply id=\"A4.T4.4.4.4.4.4.4.m4.1.1.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"A4.T4.4.4.4.4.4.4.m4.1.1.1.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1\">superscript</csymbol><ci id=\"A4.T4.4.4.4.4.4.4.m4.1.1.2.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.2\">Δ</ci><apply id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3\"><times id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.1.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.1\"></times><ci id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.2.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.2\">𝐴</ci><ci id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.3.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.3\">𝑐</ci><ci id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.4.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.4\">𝑐</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T4.4.4.4.4.4.4.m4.1c\">\\Delta^{Acc}</annotation></semantics></math> \n<br class=\"ltx_break\">, table foot=]data/isic2019.csv\n <span id=\"A4.T4.4.4.4.4.4.4.1\" class=\"ltx_ERROR undefined\">\\csvcolii</span> <span id=\"A4.T4.4.4.4.4.4.4.2\" class=\"ltx_ERROR undefined\">\\csvcoliii</span> (<span id=\"A4.T4.4.4.4.4.4.4.3\" class=\"ltx_ERROR undefined\">\\csvcoliv</span>)  <span id=\"A4.T4.4.4.4.4.4.4.4\" class=\"ltx_ERROR undefined\">\\csvcolv</span> (<span id=\"A4.T4.4.4.4.4.4.4.5\" class=\"ltx_ERROR undefined\">\\csvcolvi</span>)  <span id=\"A4.T4.4.4.4.4.4.4.6\" class=\"ltx_ERROR undefined\">\\csvcolvii</span> (<span id=\"A4.T4.4.4.4.4.4.4.7\" class=\"ltx_ERROR undefined\">\\csvcolviii</span>)  <span id=\"A4.T4.4.4.4.4.4.4.8\" class=\"ltx_ERROR undefined\">\\csvcolix</span> (<span id=\"A4.T4.4.4.4.4.4.4.9\" class=\"ltx_ERROR undefined\">\\csvcolx</span>)  <span id=\"A4.T4.4.4.4.4.4.4.10\" class=\"ltx_ERROR undefined\">\\csvcolxi</span> (<span id=\"A4.T4.4.4.4.4.4.4.11\" class=\"ltx_ERROR undefined\">\\csvcolxii</span>)  <span id=\"A4.T4.4.4.4.4.4.4.12\" class=\"ltx_ERROR undefined\">\\csvcolxiii</span> (<span id=\"A4.T4.4.4.4.4.4.4.13\" class=\"ltx_ERROR undefined\">\\csvcolxiv</span>)  <span id=\"A4.T4.4.4.4.4.4.4.14\" class=\"ltx_ERROR undefined\">\\csvcolxv</span> (<span id=\"A4.T4.4.4.4.4.4.4.15\" class=\"ltx_ERROR undefined\">\\csvcolxvi</span>)  <span id=\"A4.T4.4.4.4.4.4.4.16\" class=\"ltx_ERROR undefined\">\\csvcolxvii</span> (<span id=\"A4.T4.4.4.4.4.4.4.17\" class=\"ltx_ERROR undefined\">\\csvcolxviii</span>)</span></span></span></span></p>\n</span></div>\n</figure>\n<figure id=\"A4.F26\" class=\"ltx_figure\"><img src=\"/html/2309.02160/assets/images/isic/hist.png\" id=\"A4.F26.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"479\" height=\"357\" alt=\"Refer to caption\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 26: </span>Number of samples from each class for each party - ISIC2019</figcaption>\n</figure>\n<div id=\"A4.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"A4.p1.1\" class=\"ltx_p\">We evaluate the effect of FL on local fairness on a real-world medical dataset, ISIC2019 <cite class=\"ltx_cite ltx_citemacro_citep\">[Abay et al., <a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2020</a>, Tschandl et al., <a href=\"#bib.bib45\" title=\"\" class=\"ltx_ref\">2018</a>, Combalia et al., <a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">2019</a>]</cite>, which contains dermoscopic images of skin lesions collected from six medical centers. The task is to classify dermoscopic images among nine different diagnostic categories. We filter out medical centers with less than 1900 images and regard each medical center with over 1900 images as a party (4 centers remain after filtering). For every party, we randomly select 1500 and 400 images for training and validation, respectively. We regard the binary notion of Sex as our sensitive attribute. In this multi-class and medical dataset setting, we measure the bias (fairness gap) based on the accuracy gap across groups. Following  <cite class=\"ltx_cite ltx_citemacro_citep\">[Ogier du Terrail et al., <a href=\"#bib.bib35\" title=\"\" class=\"ltx_ref\">2022</a>]</cite> <span id=\"footnote2\" class=\"ltx_note ltx_role_footnote\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>https://github.com/owkin/FLamby</span></span></span>, we train EfficientNets <cite class=\"ltx_cite ltx_citemacro_citep\">[Tan and Le, <a href=\"#bib.bib44\" title=\"\" class=\"ltx_ref\">2019</a>]</cite>, the state-of-the-art model structure for medical data, under the centralized, FL, and standalone setting. The accuracy and fairness gap for each client in different settings are shown in Table <a href=\"#A4.T4\" title=\"Table 4 ‣ Appendix D Extending to a real-world medical dataset ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n</div>\n<div id=\"A4.p2\" class=\"ltx_para ltx_noindent\">\n<p id=\"A4.p2.1\" class=\"ltx_p\">FedAvg does not improve accuracy, possibly due to data heterogeneity among parties (as shown in Figure <a href=\"#A4.F26\" title=\"Figure 26 ‣ Appendix D Extending to a real-world medical dataset ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">26</span></a>), resulting in the model’s instability and slow convergence, which is known as ‘client-drift’ issue <cite class=\"ltx_cite ltx_citemacro_citep\">[Karimireddy et al., <a href=\"#bib.bib24\" title=\"\" class=\"ltx_ref\">2020b</a>]</cite>.</p>\n</div>\n<div id=\"A4.p3\" class=\"ltx_para ltx_noindent\">\n<p id=\"A4.p3.1\" class=\"ltx_p\">Moreover, the FedAvg model results in a higher accuracy gap between groups, worsening the fairness issue compared to the standalone setting or the centralized setting (see the fairness gaps for party 1, party 3, and party 4 as examples). Our results suggest that, in real-world settings, FedAvg may not improve accuracy compared to the standalone setting for the local data distribution and may even exacerbate the fairness issue, leading to a more biased model.</p>\n</div>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>\n<section id=\"A5\" class=\"ltx_appendix\">\n<h2 class=\"ltx_title ltx_title_appendix\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix E </span>Potential Mitigation</h2>\n\n<div id=\"A5.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"A5.p1.1\" class=\"ltx_p\">In this section, we suggest some potential solutions to reduce the effect of bias propagation in FL. Our hope is that this will inspire further research on developing fair FL in the future.</p>\n</div>\n<section id=\"A5.SS0.SSS0.Px1\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Personalized FL</h4>\n\n<div id=\"A5.SS0.SSS0.Px1.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"A5.SS0.SSS0.Px1.p1.1\" class=\"ltx_p\">Our study has revealed that aggregate updates, which involve the average local update from all parties, may result in a higher bias than local updates for less biased parties (see Figure <a href=\"#S3.F3\" title=\"Figure 3 ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). This can cause bias from other parties to propagate to the local model, thus increasing the local fairness gap. To mitigate the bias propagation effect, one possible approach is to avoid completely overwriting the local model with the global model. Personalized Federated Learning (PFL) techniques <cite class=\"ltx_cite ltx_citemacro_citep\">[Li et al., <a href=\"#bib.bib29\" title=\"\" class=\"ltx_ref\">2021</a>]</cite> have recently been proposed to address this issue by focusing on improving the model’s accuracy. PFL can be a promising direction for developing fairness-aware personalized federated learning approaches that can help reduce the bias propagation effect in FL.</p>\n</div>\n</section>\n<section id=\"A5.SS0.SSS0.Px2\" class=\"ltx_paragraph\">\n<h4 class=\"ltx_title ltx_title_paragraph\">Fair Representation Learning</h4>\n\n<div id=\"A5.SS0.SSS0.Px2.p1\" class=\"ltx_para ltx_noindent\">\n<p id=\"A5.SS0.SSS0.Px2.p1.1\" class=\"ltx_p\">Our analysis demonstrated that bias is encoded in a small number of parameters in the first layer of the neural network, which is typically considered the feature extractor (as illustrated in Figure <a href=\"#S3.F10\" title=\"Figure 10 ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>). This suggests that the FL model is biased because the learned representation is biased, heavily influenced by the sensitive attribute. To address this issue, one possible solution is to implement fair representation learning techniques <cite class=\"ltx_cite ltx_citemacro_citep\">[Zemel et al., <a href=\"#bib.bib50\" title=\"\" class=\"ltx_ref\">2013</a>, Zhao et al., <a href=\"#bib.bib54\" title=\"\" class=\"ltx_ref\">2019</a>, Liu et al., <a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">2022</a>, Zeng et al., <a href=\"#bib.bib52\" title=\"\" class=\"ltx_ref\">2021b</a>]</cite> that aim to learn a feature extractor which is fair, meaning that it minimizes the dependence on the sensitive attribute while still retaining enough information about the task (or input features). Therefore, using fair representation learning in FL can potentially mitigate the propagation of bias effects.</p>\n</div>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>\n</section>\n</section>\n</section>\n</section>\n</section>\n</section>\n</section>\n</section>\n</div>\n</div>\n\n",
        "footnotes": "[tabular=c*3a*3e*3d,table head=    Accuracy     ΔE​OsuperscriptΔ𝐸𝑂\\Delta^{EO}    ΔD​PsuperscriptΔ𝐷𝑃\\Delta^{DP}  \nDataset  Standalone  Centralized  FedAvg  Standalone  Centralized  FedAvg  Standalone  Centralized  FedAvg \n, table foot=]data/data_benefit.csv\n\\csvcoli- \\csvcoliiTable 3.2 presents the average accuracy and fairness gaps of the centralized model, FL model, and standalone models on local datasets. Centralized training is observed to improve accuracy and fairness at the same time for parties. For instance, on the Income dataset, centralized training improves the accuracy by 6%percent66\\% and reduces the fairness gap across racial groups by 5.5%percent5.55.5\\% with respect to equalized odds and by 5.5%percent5.55.5\\% with respect to demographic parity. However, FL may not achieve the same benefit as centralized training in terms of fairness and can even exacerbate the fairness issue for parties. For example, on the Income dataset, the EO fairness gap of the FL model for the sex groups increases by 35%percent3535\\%, and the DP fairness gap increases by 29.9%percent29.929.9\\%, compared to standalone models. We include results for other popular FL algorithms, including FedNova (Wang et al., 2020), Scaffold Karimireddy et al. (2020b), FedOpt (Reddi et al., 2020), FedProx Li et al. (2020), and Mime (Karimireddy et al., 2020a), in Table 2 in Appendix B, which show a similar pattern to that of the FedAvg algorithm. Appendix B provides more detailed results on other FL algorithms.\nThe centralized and FL models are trained on the same dataset. The difference between the FL model and the centralized model in terms of fairness suggests FL algorithm can introduce more bias compared to standard training. Therefore, the explanation of how bias is introduced during standard training in the centralized setting may not fully explain how FL introduces bias. In the following, we further explore how FL impacts fairness from the party level.Disparate impact of FL on group fairness across parties. \nFigure 1 illustrates the benefit of FL on fairness and accuracy for parties. We observe that FL improves accuracy for almost all parties, and the variance in accuracy improvement across parties is small. On the other hand, the fairness benefit of FL is negative for most parties. It implies that most parties obtain a more biased model in FL compared to standalone training. Furthermore, we notice that the variance in the fairness benefits across parties is large, suggesting that although all parties have the same global model in FL, they do not benefit from the FL model equally (each party evaluates the model performance on their local test dataset).To explore the impact of FL on fairness at the party level, Figure 2 shows strong correlations between the fairness benefit a party obtains in FL and the fairness gap of the standalone model for the party, i.e., the bias level of the party. This finding highlights the disparate impact of FL on fairness: FL can improve fairness for more biased parties but at the cost of worsening the issue for less biased parties.Contradiction between aggregation and local update.\nDuring the training process of FL, we observe that aggregation and local update contradict each other, shown in Figure 3. Local update from the least biased party, whose standalone model has the lowest fairness gap, reduces the fairness gap of the model. However, this reduction is eliminated by the aggregation step. Conversely, the local update from the most biased party increases the fairness gap of the model, which is then reduced by aggregation. This finding implies that the aggregation contributes to the disparate impact of FL on fairness, improving fairness for more biased parties but worsening the fairness for less biased parties compared to the standalone setting.Biased parties negatively influence other parties via aggregation throughout the training. \nWhy does aggregation have disparate impacts on local fairness for parties? Specifically, we ask which party’s local update causes the increase (or decrease) of the fairness gap for other parties via aggregation. To answer this question, we look into the influence of a party’s local update on other parties’ fairness via aggregation. More precisely, we compute the influence of party i𝑖i on party j𝑗j as the fairness gap increase when party i𝑖i’s local update is removed from the aggregation in each round t𝑡t and sum over all the training rounds. Formally, we define the influence of party i𝑖i on party j𝑗j as Ii,j=∑t=1TΔ​(θt,−i,Dj)−Δ​(θt,Dj)subscript𝐼𝑖𝑗superscriptsubscript𝑡1𝑇Δsubscript𝜃𝑡𝑖subscript𝐷𝑗Δsubscript𝜃𝑡subscript𝐷𝑗I_{i,j}=\\sum_{t=1}^{T}\\Delta(\\theta_{t,-i},D_{j})-\\Delta(\\theta_{t},D_{j}), where θtsubscript𝜃𝑡\\theta_{t} is the global model (i.e., the aggregated model overall local updated models) and θt,−isubscript𝜃𝑡𝑖\\theta_{t,-i} is the aggregated model over local updated models from parties excluding party i𝑖i. If party i𝑖i improves fairness for party j𝑗j, the influence is positive and vice versa. We compute the influence for all pairs of parties, and the most influential pairs are shown in Figure 4. We observe that a less biased party has a positive influence on fairness for other parties, while a more biased party has a negative influence on other parties. This result shows that a biased party can negatively influence other parties’ fairness via aggregation throughout the training.Furthermore, we investigate the relationship between a party’s bias (i.e., the bias of the party’s standalone model) and its average influence on all parties’ fairness (i.e., ∑k=1KIi,k/Ksuperscriptsubscript𝑘1𝐾subscript𝐼𝑖𝑘𝐾\\sum_{k=1}^{K}I_{i,k}/K). The results are presented in Figure 6, which shows a strong correlation between these two factors. This finding further supports our conclusion that the more biased parties have a stronger negative influence on other parties’ fairness, while the less biased parties have a stronger positive influence.Finally, we analyze the dynamics of the influence of the top 5 most biased parties (i.e., parties with the largest fairness gap in the standalone setting) and the top 5 least biased parties, as shown in Figure 6. We observe that the influence of the most biased parties is monotonically increasing throughout the training, while the influence of the least biased parties is decreasing. In other words, biased parties consistently have a negative influence on others’ fairness gaps, while less biased parties have a positive influence. These results suggest that FL can propagate bias among parties: the bias from biased parties negatively influence the fairness of other parties via aggregation throughout the training. Next, we will explore how the bias is propagated in FL.Disparate treatment causes large fairness gaps.\nOur first investigation explores what the bias represents, specifically whether the bias increase in FL is directly caused by the disparate treatment of the model among sensitive groups. To answer this question, we utilize Integrated Gradients (Sundararajan et al., 2017) to measure the attribution of each input feature to the models’ predictions with respect to the positive class.\nFigure 7 shows the attribution value distribution for the sensitive attribute \"Sex\" over individual test points from the female and male groups. We notice that the sex attribute has a large attribution value for the standalone model’s predictions and the FL model’s predictions. This finding implies that the predictions of those models are heavily dependent on the sensitive attribute of the test data. Moreover, the sensitive attribute affects the model predictions differently for the male and female groups, with the average attribution value being positive for the male group and negative for the female group. Furthermore, we find that there is minimal difference in the attribution value for other attributes with respect to the female and male groups (see Figure 19 in Appendix B). This indicates that the large fairness gap in the models is not caused by the distinct distribution of other insensitive attributes over protected groups; rather, it is mainly caused by the models’ disparate treatment of protected groups.FL model learns more biased patterns. \nIn Figure 7, we compare the attribution value of “Sex” for different models and find that, while the predictions of the FL model depend less heavily on the sensitive attribute compared to those of the standalone model, the dependence is still stronger than that of the centralized model. This suggests that the FL model learns a more biased pattern compared to what could be learned in the centralized setting.\nFigure 9 shows the dynamic of the average absolute feature attribution for “Sex” during the training. We find that standalone training increases the model’s dependence on the sensitive attribute throughout training for the most biased party, but centralized training decreases it. This suggests that collaboration through centralized training improves local fairness by guiding the model to learn less biased features. In FL, however, the absolute attribution value barely changes after 100 rounds and remains significantly greater than that in the centralized setting. This implies that the FL algorithm may introduce bias to the final model by inhibiting the model from learning less biased features.Biased parties increase the model dependence on the sensitive attribute. \nFigure 9 shows the attribution value of the aggregated model and locally updated model from the most biased party and least biased party. We observe that the biased party increases the global model dependence on sensitive attributes during the local update, and this increase persists throughout the training. In contrast, the least biased party reduces the dependence on the sensitive attribute, which is aligned with the trend of the fairness gap in Figure 3. This suggests that the biased parties have a negative impact on the fairness of other parties by increasing the model’s dependence on the sensitive attribute.Bias is encoded in a few parameters. Biased parties increase the model dependence on the sensitive attribute. But how does this increase propagate to other parties in the network? Since parties share the model parameters of the local model with the server, the bias is likely encoded in the model parameters. Therefore, we investigate which parameters are related to the model’s bias. Intuitively, the parameters used to extract sensitive attribute information impact the attribution value of the sensitive attribute to the model prediction. If the absolute value (signal) of those parameters is substantial, the value of the sensitive attribute will significantly affect the model’s prediction. In our evaluation, the sensitive attribute is part of the input feature, so the parameters directly applied to the “Sex” attribute in the first layer of the neural network should contribute to the model’s bias.Figure 10(a) shows the normalized norm of the parameters associated with the sensitive attribute for the most and least biased parties in the aggregated and locally updated models. The normalized norm is defined as the norm of parameters associated with the sensitive attribute divided by the parameter norm of the first layer. We observe that the least biased party reduces the parameter norm, hence decreasing the model’s sensitivity to the sensitive attribute. On the other hand, the biased party increases the norm for those parameters, amplifying the impact of the sensitive attribute on the model’s prediction. Through aggregation, this amplification will be propagated to the global model.\nFigure 10(b) reveals a moderate correlation between the fairness gap party experiences in the standalone setting and the normalized parameter norm for the parameters used to extract sensitive attribute information. This implies that biased parties increase the parameter value associated with sensitive attributes during the local update, thereby boosting the model’s susceptibility to the sensitive attribute.Controlling fairness gap by scaling a few parameters. \nTo further investigate the impact of the parameters associated with the sensitive attribute (i.e., 104104104 parameters out of 1,79217921,792) on model fairness, we examine the effect of scaling these parameters on the fairness gap in Figure 10(c). We find that the fairness gap can be greatly widened or narrowed at the expense of a moderate degree of accuracy. Specifically, by scaling the parameter value by 0.10.10.1 for the trained FL model, we significantly reduce the EO gap by 74.6%percent74.674.6\\% (from 0.1980.1980.198 to 0.050.050.05) and the DP gap by 69.1%percent69.169.1\\% (from 0.2170.2170.217 to 0.0670.0670.067) with just a moderate accuracy loss of 0.8%percent0.80.8\\% (from 0.7850.7850.785 to 0.7790.7790.779). In contrast, scaling the same set of parameters by a factor of 10 increases the EO gap to 0.960.960.96 (the maximal fairness gap is 111), which is almost five times larger, and increases the DP gap by 259%percent259259\\% to 0.780.780.78, while reducing the accuracy from 0.7850.7850.785 to 0.670.670.67. These findings explain how bias is propagated in FL: biased parties magnify the impact of sensitive attributes on model predictions by increasing the model parameter used to extract sensitive attributes. This rise in parameters is subsequently propagated to the global model through aggregation, further aggravating the issue of fairness for other parties. Our results explain how bias is propagated in FL: Biased parties encode bias in a few parameters through a local update, and this bias is consequently propagated to the entire network through parameter aggregation.Fairness has received considerable attention due to the growing deployment of machine learning in decision-making processes. Various definitions of fairness have been presented (Hardt et al., 2016; Dwork et al., 2012; Calders et al., 2009). Specifically, group fairness requires that the model behave similarly for groups defined by a sensitive attribute (e.g., race). While how machine learning algorithms propagate data bias to the final model has been extensively investigated in a centralized setting  (Blum and Stangl, 2020; Lakkaraju et al., 2017; Rambachan and Roth, 2020; Friedler et al., 2019; Dullerud et al., 2022), the effect of FL on model fairness is not yet fully understood.The existing literature on fairness in FL mainly focuses on the performance disparity of FL models across parties, rather than demographic groups (Li et al., 2021; Zhao and Joshi, 2022; Li et al., 2019; Mohri et al., 2019; Deng et al., 2020; Donahue and Kleinberg, 2021; Hao et al., 2021; Zhou et al., 2021; Yu et al., 2020; Lyu et al., 2020). However, we focus on group fairness, which concerns performance disparity among groups. In terms of group fairness, Abay et al. (2020) listed a few potential sources of bias in FL. Recently, considerable progress has been made in training group fair models in FL (Abay et al., 2020; Chu et al., 2021; Zeng et al., 2021a; Hu et al., 2022; Du et al., 2021; Ezzeldin et al., 2021). Nonetheless, the majority of these works suggest techniques for achieving fairness on a single test distribution. Instead, we focus on fairness issues for parties. Some studies (Cui et al., 2021; Papadaki et al., 2022) proposed algorithms to improve local fairness for parties. Our purpose, instead, is to gain a comprehensive understanding of how FL influences local fairness on its own, which we believe is equally crucial as designing fair algorithms.Future Work. \nIn this work, we have investigated how bias is propagated in FL when the sensitive attribute is included as an input feature. In practice, however, sensitive attributes may be prohibited from being included in input features. In such situations, the model may still be heavily biased due to variables that are correlated with the (unobserved) sensitive attribute. For instance, a person’s zip code may be highly correlated with their race, a phenomenon known as \"redlining\". A promising direction for future work is to identify which model parameters contribute to the bias and to audit the bias propagation in this setting. Another important direction is to design FL algorithms that are robust to bias propagation. In Appendix E, we briefly discuss a few potential ways to achieve this goal.Conclusion.  Federated learning has become increasingly popular in various applications with significant individual-level consequences, making it essential to anticipate the possible bias introduced by FL. Our paper takes the first step in this direction by providing a comprehensive analysis of the impact of FL on local fairness for parties. We demonstrated that the FL algorithm could introduce bias on its own which may exacerbate the issue of fairness for the involved parties. Moreover, we showed that this exacerbation is not evenly distributed among parties, as FL can propagate bias among them. Finally, we explained how bias is propagated in FL: biased parties encode their bias into the local updates by increasing the signal of a few parameters steadily throughout the training process, which is then propagated to the global model via aggregation and, ultimately, to other parties.The authors would like to thank Ergute Bao, Ta Duy Nguyen, and Martin Strobel for their valuable feedback on earlier versions of this paper, as well as the anonymous reviewers for their insightful comments. This research is supported by Google PDPO faculty research award, Intel within the www.private-ai.org center, Meta faculty research award, the NUS Early Career Research Award (NUS ECRA award num- ber NUS ECRA FY19 P16), and the National Research Foundation, Singapore under its Strategic Capability Research Centres Funding Initiative. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the author(s) and do not reflect the views of the National Research Foundation, Singapore.Our analysis focuses on group fairness, which is typically used to audit the model or system for any bias or discrimination. Auditing the bias with respect to other definitions of fairness, such as individual fairness, may result in different conclusions. Moreover, our study focuses primarily on the binary notion of sex attributes and the multi-valued race attribute. We recognize that there are numerous protected groups outside those considered in the analysis, such as those defined by multiple sensitive attributes. The propagation of bias against fine-grained subgroups may be even more substantial than we found in the paper.We provide details about the model, datasets, and implementations in Appendix A, and the code for the paper is available at https://github.com/privacytrustlab/bias_in_FL.This appendix is divided into five sections. Appendix A provides additional details about the experimental setups, including information about the models, datasets, and hyperparameters used. Appendix B presents further experimental results that support the claims made in the paper. In Appendix C, we discuss the results of an existing fair FL algorithm. Appendix D extends our analysis to real-world medical datasets. Finally, in Appendix E, we discuss potential methods for mitigating bias in FL.We explain the datasets and models used in the paper.We use the datasets provided by folktables. In particular, we consider the ACSIncome, ACSPublicCoverage, and ACSEmployment tasks defined in the forlktables. In the ACSIncome, the goal is to predict whether an individual’s Income is above $50,000. In the ACSEmployment task, the goal is to predict whether an individual is employed. Similarly, in the Health (i.e., ACSPublicCoverage) task, the objective is to predict whether an individual is covered by public health insurance. We use the same pre-processing as in the folktables and train a fully connected neural network model with one hidden layer of 32 neurons for Income and 64 neurons for Employment and Health tasks. For all the tasks, we use the RELU activation function. We use an SGD optimizer with a learning rate of 0.001 for centralized training on Health and Employment datasets and 0.1 for other settings, and the batch size is 32. We train the NN models for 200 epochs. In FL, each client updates the global mdoel for 1 epoch and shares it with the server. We encode the categorical features based on the encoding template provided in folktables. After the encoding, the input feature size for Income is 54, 154 for Health, and 109 for Employment. We consider sex and race as sensitive attributes. Accordingly, there are two gender groups (male and female) and nine racial groups (\"White alone,\" \"Black or African American alone,\" \"American Indian alone,\" \"Alaska Native alone,\" and \"American Indian and Alaska Native tribes specified; or American Indian or Alaska Native, not specified and no other,\" \"Asian alone,\" \"Native Hawaiian and Other Pacific Islander alone,\" \"Some Other Race alone,\" \"Two or More Races\").We train CNN models on the dataset with one CNN layer whose output channel is 32, kernel size is 3, and stride is 1. We use ’same’ padding for the CNN layer. Following this CNN layer, we have the Batch normalization layer and Max Pooling layer. After which, we have the connected layer. We train the model for 500 communication rounds or epochs with SGD optimizer. The learning rate is 0.1, and the batch size is 128. The train, test, and validation datasets ratio for each party is 6:2:2.In our paper, we also evaluate various popular FL algorithms, including FedNova [Wang et al., 2020], Scaffold Karimireddy et al. [2020b], FedOpt [Reddi et al., 2020], FedProx Li et al. [2020], and Mime [Karimireddy et al., 2020a]. We use the same local learning rate and the number of the local epoch as in FedAvg. We provided the detailed hyper-parameters for each of the algorithms in our code (See supplementary). We run all experiments on Ubuntu with two NVIDIA TITAN RTX GPUs.Figure 11 shows the fraction of samples for each subgroup across all parties, while Figure 12 shows the histogram of subgroup proportions for each party. We observe that the parties have different fractions of samples from each group in the Income dataset, indicating that their local data distributions are dissimilar. In contrast, for the Health and Employment datasets, parties have similar fractions of samples from each subgroup, suggesting that their data distributions are more alike than those of the Income dataset.We show the bias propagation effect of FL for the Health and Employment task in Figure 13 and 14 respectively.We show the bias propagation effect of FL on CelebA dataset. Besides partitioning the data in an IID manner (we refer to as setting (i)), we also evaluate two different settings, where we change the number of samples from the minority subgroup (the female group with the “Not Young” age label). In this way, we aim to change the data bias in the local training datasets for clients. In particular, in setting (ii), half of the parties have more samples from the minority subgroup than another half of the parties. The ratio is 8:2. In setting (iii), a single party has half of the data from the minority subgroup, and the data is then iid partitioned among the other parties. In the figure, we find that the in the non-iid setting, the more biased parties have a larger and more positive benefit, while FL hurts the less biased parties. Figure 15 shows the correlation between the fairness gap of the standalone model and the benefit a party gets in the FL. We find that the in the non-iid setting, the more biased parties have a larger and more positive benefit, while FL hurts the less biased parties.Figure 16 shows the dynamic of the fairness gap for the aggregated model and locally updated model from the most biased party and least biased party during the training.We show the top 10 maximal positive and top 10 maximal native influence client pairs in Figure 17 and the top 15 in Figure 18. We can see that the less biased client has a strong positive influence on other clients while the more biased client has a strong negative influence on other clients.We show the attribution value for all input features for the most biased party and least biased party in Figure 19 and Figure 20, respectively. We observe that the attribution value for other features is similar for female and male groups. However, the sex attribute has the largest attribution value and affects the groups differently. It implies that the models are highly dependent on the sensitive attribute (i.e., \"Sex\") for making predictions.Table 2 shows results on other FL algorithms, including FedNova [Wang et al., 2020], Scaffold Karimireddy et al. [2020b], FedOpt [Asad et al., 2020], FedProx Li et al. [2020], and Mime [Karimireddy et al., 2020a].\nWe find that when the FL model achieves higher accuracy than standalone models, the fairness gap of the FL model can be higher than that of standalone models. This observation is consistent across multiple FL algorithms. This strong evidence implies that the improvement of accuracy in FL can come at the cost of fairness. In addition, this is not unique to only the FedAvg algorithm.[tabular=ca*2e*2d,table head=     Race     Sex  \nAlgorithm  Accuracy  ΔE​OsuperscriptΔ𝐸𝑂\\Delta^{EO}  ΔD​PsuperscriptΔ𝐷𝑃\\Delta^{DP} ΔE​OsuperscriptΔ𝐸𝑂\\Delta^{EO}  ΔD​PsuperscriptΔ𝐷𝑃\\Delta^{DP} \n, table foot=]data/data_other_fl.csv\n \\csvcoli \\csvcolii (\\csvcoliii)  \\csvcoliv (\\csvcolv)  \\csvcolvi (\\csvcolvii)  \\csvcolviii (\\csvcolix)  \\csvcolx (\\csvcolxi)In Figure 21, we present the model ROC on groups to illustrate the performance disparity across groups. Surprisingly, there is no huge difference between the ROC between groups. In contrast, Figure 22 shows the noticeable difference between groups with respect to the precision-recall curve, especially for the most biased party. The main reason is that the dataset is imbalanced. Thus, ROC is usually misleading. On the precision-recall curve, we find that the model achieves a higher precision on the majority (i.e., Male group) compared to the minority group (i.e., Female group).Figure 23 shows a comparison of the model’s performance when scaling different model parameters. Specifically, we examine the impact of scaling the parameters in the first layer of the neural network that does not have any computation on the Sex attribute (referred to as \"Other parameters\"), as well as the parameter related to the Sex attribute (referred to as \"Related parameters\"). The results indicate that up-scaling or down-scaling other parameters do not significantly affect the fairness gap while scaling the related parameters has a considerable impact on model fairness. This finding supports our hypothesis that the model’s bias is primarily encoded in a few model parameters. In FL, biased parties introduce bias during local training by increasing the weights for those related parameters.Table 3 presents the results for a fair FL algorithm proposed by Abay et al. [2020], which uses a reweighting algorithm proposed in the centralized setting [Kamiran and Calders, 2012]. The algorithm involves assigning weights to each subgroup (defined by the label and a sensitive attribute) before FL, based on the local or global training dataset. This is called \"local reweighting\" or \"global reweighting,\" respectively. During FL, parties update the FL model to minimize the weighted loss. The table presents the accuracy and fairness gap of the FL algorithm using local and global reweighting for the Income, Health, and Employment datasets.[tabular=cca*2e*2d,table head=      Race     Sex  \nAlgorithm  Sensitive Attribute  Accuracy  ΔE​OsuperscriptΔ𝐸𝑂\\Delta^{EO}  ΔD​PsuperscriptΔ𝐷𝑃\\Delta^{DP} ΔE​OsuperscriptΔ𝐸𝑂\\Delta^{EO}  ΔD​PsuperscriptΔ𝐷𝑃\\Delta^{DP} \n, table foot=]data/data_reweiging.csv\n \\csvcoli \\csvcolxii\\csvcolii (\\csvcoliii)  \\csvcoliv (\\csvcolv)  \\csvcolvi (\\csvcolvii)  \\csvcolviii (\\csvcolix)  \\csvcolx (\\csvcolxi)We found that applying reweighing algorithms in FL reduces the average fairness gap across parties. This is due to the fact that, after reweighing, the parties that were initially biased do not introduce significant bias to the model during local training, resulting in a reduction of the fairness gap in the FL model.However, we also noted that the fairness gap in the model remains high for the Race sensitive attribute, indicating that the global and local reweighing algorithms do not entirely eliminate bias in FL. Furthermore, we would like to highlight some concerns regarding the fair FL algorithm:The fair FL algorithm enhances the performance of the minority group at the expense of the majority group. Figure 24 illustrates that the minority group (i.e., the Female group) has a better performance after the reweighing, but at the cost of reduced performance for the majority.When improving fairness with respect to one sensitive attribute, the fairness issue with respect to another sensitive attribute may worsen, as shown in Table 3. For instance, when applying local reweighing to reduce bias with respect to the \"Sex\" attribute, the fairness gap with respect to \"Race\" increased from 0.514 (results on FedAvg) to 0.521, indicating that reweighing to improve fairness with respect to Sex may amplify the bias with respect to Race. This implies that the bias with respect to Race can still propagate in FL, and our analysis remains valid.The reweighing algorithms assume that parties are interested in mitigating the bias with respect to the same sensitive attribute, which may not be the case in practice. Figure 25 shows the fairness gap of standalone models with respect to different sensitive attributes. We observe that many parties have a low fairness gap with respect to one sensitive attribute and a high fairness gap with respect to another sensitive attribute. Thus, parties may aim to enhance fairness with respect to different sensitive attributes, which renders this reweighing algorithm unsuitable. This problem is more prevalent in FL, given the different data distributions of parties.[tabular=c*2a*2e*2d*2f,table head= Setting    Party 1     Party 2      Party 3      Party 4 \nAlgorithm  Accuracy  ΔA​c​csuperscriptΔ𝐴𝑐𝑐\\Delta^{Acc}  Accuracy  ΔA​c​csuperscriptΔ𝐴𝑐𝑐\\Delta^{Acc}  Accuracy  ΔA​c​csuperscriptΔ𝐴𝑐𝑐\\Delta^{Acc}  Accuracy  ΔA​c​csuperscriptΔ𝐴𝑐𝑐\\Delta^{Acc} \n, table foot=]data/isic2019.csv\n \\csvcolii \\csvcoliii (\\csvcoliv)  \\csvcolv (\\csvcolvi)  \\csvcolvii (\\csvcolviii)  \\csvcolix (\\csvcolx)  \\csvcolxi (\\csvcolxii)  \\csvcolxiii (\\csvcolxiv)  \\csvcolxv (\\csvcolxvi)  \\csvcolxvii (\\csvcolxviii)We evaluate the effect of FL on local fairness on a real-world medical dataset, ISIC2019 [Abay et al., 2020, Tschandl et al., 2018, Combalia et al., 2019], which contains dermoscopic images of skin lesions collected from six medical centers. The task is to classify dermoscopic images among nine different diagnostic categories. We filter out medical centers with less than 1900 images and regard each medical center with over 1900 images as a party (4 centers remain after filtering). For every party, we randomly select 1500 and 400 images for training and validation, respectively. We regard the binary notion of Sex as our sensitive attribute. In this multi-class and medical dataset setting, we measure the bias (fairness gap) based on the accuracy gap across groups. Following  [Ogier du Terrail et al., 2022] 222https://github.com/owkin/FLamby, we train EfficientNets [Tan and Le, 2019], the state-of-the-art model structure for medical data, under the centralized, FL, and standalone setting. The accuracy and fairness gap for each client in different settings are shown in Table 4.FedAvg does not improve accuracy, possibly due to data heterogeneity among parties (as shown in Figure 26), resulting in the model’s instability and slow convergence, which is known as ‘client-drift’ issue [Karimireddy et al., 2020b].Moreover, the FedAvg model results in a higher accuracy gap between groups, worsening the fairness issue compared to the standalone setting or the centralized setting (see the fairness gaps for party 1, party 3, and party 4 as examples). Our results suggest that, in real-world settings, FedAvg may not improve accuracy compared to the standalone setting for the local data distribution and may even exacerbate the fairness issue, leading to a more biased model.In this section, we suggest some potential solutions to reduce the effect of bias propagation in FL. Our hope is that this will inspire further research on developing fair FL in the future.Our study has revealed that aggregate updates, which involve the average local update from all parties, may result in a higher bias than local updates for less biased parties (see Figure 3). This can cause bias from other parties to propagate to the local model, thus increasing the local fairness gap. To mitigate the bias propagation effect, one possible approach is to avoid completely overwriting the local model with the global model. Personalized Federated Learning (PFL) techniques [Li et al., 2021] have recently been proposed to address this issue by focusing on improving the model’s accuracy. PFL can be a promising direction for developing fairness-aware personalized federated learning approaches that can help reduce the bias propagation effect in FL.Our analysis demonstrated that bias is encoded in a small number of parameters in the first layer of the neural network, which is typically considered the feature extractor (as illustrated in Figure 10). This suggests that the FL model is biased because the learned representation is biased, heavily influenced by the sensitive attribute. To address this issue, one possible solution is to implement fair representation learning techniques [Zemel et al., 2013, Zhao et al., 2019, Liu et al., 2022, Zeng et al., 2021b] that aim to learn a feature extractor which is fair, meaning that it minimizes the dependence on the sensitive attribute while still retaining enough information about the task (or input features). Therefore, using fair representation learning in FL can potentially mitigate the propagation of bias effects.",
        "references": [
            []
        ]
    },
    "A2.T2": {
        "caption": "Table 2: Benefit of collaboration on local datasets - Various FL algorithms (Income). The standard deviation across five runs is indicated between parentheses.",
        "table": "<div id=\"A2.T2.4\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:433.6pt;height:42.2pt;vertical-align:-32.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(42.2pt,-0.9pt) scale(1.2415726193884,1.2415726193884) ;\"><span id=\"A2.T2.4.5\" class=\"ltx_ERROR undefined\">\\csvreader</span>\n<p id=\"A2.T2.4.4\" class=\"ltx_p\">[tabular=ca*2e*2d,table head=    <span id=\"A2.T2.4.4.4\" class=\"ltx_text\" style=\"background-color:#F6FCF2;\"> Race     <span id=\"A2.T2.4.4.4.4\" class=\"ltx_text\" style=\"background-color:#F5FAFC;\">Sex  \n<br class=\"ltx_break\">Algorithm  Accuracy  <math id=\"A2.T2.1.1.1.1.m1.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{EO}\" display=\"inline\"><semantics id=\"A2.T2.1.1.1.1.m1.1a\"><msup id=\"A2.T2.1.1.1.1.m1.1.1\" xref=\"A2.T2.1.1.1.1.m1.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A2.T2.1.1.1.1.m1.1.1.2\" xref=\"A2.T2.1.1.1.1.m1.1.1.2.cmml\">Δ</mi><mrow id=\"A2.T2.1.1.1.1.m1.1.1.3\" xref=\"A2.T2.1.1.1.1.m1.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.1.1.1.1.m1.1.1.3.2\" xref=\"A2.T2.1.1.1.1.m1.1.1.3.2.cmml\">E</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A2.T2.1.1.1.1.m1.1.1.3.1\" xref=\"A2.T2.1.1.1.1.m1.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.1.1.1.1.m1.1.1.3.3\" xref=\"A2.T2.1.1.1.1.m1.1.1.3.3.cmml\">O</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A2.T2.1.1.1.1.m1.1b\"><apply id=\"A2.T2.1.1.1.1.m1.1.1.cmml\" xref=\"A2.T2.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.T2.1.1.1.1.m1.1.1.1.cmml\" xref=\"A2.T2.1.1.1.1.m1.1.1\">superscript</csymbol><ci id=\"A2.T2.1.1.1.1.m1.1.1.2.cmml\" xref=\"A2.T2.1.1.1.1.m1.1.1.2\">Δ</ci><apply id=\"A2.T2.1.1.1.1.m1.1.1.3.cmml\" xref=\"A2.T2.1.1.1.1.m1.1.1.3\"><times id=\"A2.T2.1.1.1.1.m1.1.1.3.1.cmml\" xref=\"A2.T2.1.1.1.1.m1.1.1.3.1\"></times><ci id=\"A2.T2.1.1.1.1.m1.1.1.3.2.cmml\" xref=\"A2.T2.1.1.1.1.m1.1.1.3.2\">𝐸</ci><ci id=\"A2.T2.1.1.1.1.m1.1.1.3.3.cmml\" xref=\"A2.T2.1.1.1.1.m1.1.1.3.3\">𝑂</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T2.1.1.1.1.m1.1c\">\\Delta^{EO}</annotation></semantics></math>  <math id=\"A2.T2.2.2.2.2.m2.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{DP}\" display=\"inline\"><semantics id=\"A2.T2.2.2.2.2.m2.1a\"><msup id=\"A2.T2.2.2.2.2.m2.1.1\" xref=\"A2.T2.2.2.2.2.m2.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A2.T2.2.2.2.2.m2.1.1.2\" xref=\"A2.T2.2.2.2.2.m2.1.1.2.cmml\">Δ</mi><mrow id=\"A2.T2.2.2.2.2.m2.1.1.3\" xref=\"A2.T2.2.2.2.2.m2.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.2.2.2.2.m2.1.1.3.2\" xref=\"A2.T2.2.2.2.2.m2.1.1.3.2.cmml\">D</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A2.T2.2.2.2.2.m2.1.1.3.1\" xref=\"A2.T2.2.2.2.2.m2.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.2.2.2.2.m2.1.1.3.3\" xref=\"A2.T2.2.2.2.2.m2.1.1.3.3.cmml\">P</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A2.T2.2.2.2.2.m2.1b\"><apply id=\"A2.T2.2.2.2.2.m2.1.1.cmml\" xref=\"A2.T2.2.2.2.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.T2.2.2.2.2.m2.1.1.1.cmml\" xref=\"A2.T2.2.2.2.2.m2.1.1\">superscript</csymbol><ci id=\"A2.T2.2.2.2.2.m2.1.1.2.cmml\" xref=\"A2.T2.2.2.2.2.m2.1.1.2\">Δ</ci><apply id=\"A2.T2.2.2.2.2.m2.1.1.3.cmml\" xref=\"A2.T2.2.2.2.2.m2.1.1.3\"><times id=\"A2.T2.2.2.2.2.m2.1.1.3.1.cmml\" xref=\"A2.T2.2.2.2.2.m2.1.1.3.1\"></times><ci id=\"A2.T2.2.2.2.2.m2.1.1.3.2.cmml\" xref=\"A2.T2.2.2.2.2.m2.1.1.3.2\">𝐷</ci><ci id=\"A2.T2.2.2.2.2.m2.1.1.3.3.cmml\" xref=\"A2.T2.2.2.2.2.m2.1.1.3.3\">𝑃</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T2.2.2.2.2.m2.1c\">\\Delta^{DP}</annotation></semantics></math> <math id=\"A2.T2.3.3.3.3.m3.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{EO}\" display=\"inline\"><semantics id=\"A2.T2.3.3.3.3.m3.1a\"><msup id=\"A2.T2.3.3.3.3.m3.1.1\" xref=\"A2.T2.3.3.3.3.m3.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A2.T2.3.3.3.3.m3.1.1.2\" xref=\"A2.T2.3.3.3.3.m3.1.1.2.cmml\">Δ</mi><mrow id=\"A2.T2.3.3.3.3.m3.1.1.3\" xref=\"A2.T2.3.3.3.3.m3.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.3.3.3.3.m3.1.1.3.2\" xref=\"A2.T2.3.3.3.3.m3.1.1.3.2.cmml\">E</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A2.T2.3.3.3.3.m3.1.1.3.1\" xref=\"A2.T2.3.3.3.3.m3.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.3.3.3.3.m3.1.1.3.3\" xref=\"A2.T2.3.3.3.3.m3.1.1.3.3.cmml\">O</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A2.T2.3.3.3.3.m3.1b\"><apply id=\"A2.T2.3.3.3.3.m3.1.1.cmml\" xref=\"A2.T2.3.3.3.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.T2.3.3.3.3.m3.1.1.1.cmml\" xref=\"A2.T2.3.3.3.3.m3.1.1\">superscript</csymbol><ci id=\"A2.T2.3.3.3.3.m3.1.1.2.cmml\" xref=\"A2.T2.3.3.3.3.m3.1.1.2\">Δ</ci><apply id=\"A2.T2.3.3.3.3.m3.1.1.3.cmml\" xref=\"A2.T2.3.3.3.3.m3.1.1.3\"><times id=\"A2.T2.3.3.3.3.m3.1.1.3.1.cmml\" xref=\"A2.T2.3.3.3.3.m3.1.1.3.1\"></times><ci id=\"A2.T2.3.3.3.3.m3.1.1.3.2.cmml\" xref=\"A2.T2.3.3.3.3.m3.1.1.3.2\">𝐸</ci><ci id=\"A2.T2.3.3.3.3.m3.1.1.3.3.cmml\" xref=\"A2.T2.3.3.3.3.m3.1.1.3.3\">𝑂</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T2.3.3.3.3.m3.1c\">\\Delta^{EO}</annotation></semantics></math>  <math id=\"A2.T2.4.4.4.4.m4.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{DP}\" display=\"inline\"><semantics id=\"A2.T2.4.4.4.4.m4.1a\"><msup id=\"A2.T2.4.4.4.4.m4.1.1\" xref=\"A2.T2.4.4.4.4.m4.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A2.T2.4.4.4.4.m4.1.1.2\" xref=\"A2.T2.4.4.4.4.m4.1.1.2.cmml\">Δ</mi><mrow id=\"A2.T2.4.4.4.4.m4.1.1.3\" xref=\"A2.T2.4.4.4.4.m4.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.4.4.4.4.m4.1.1.3.2\" xref=\"A2.T2.4.4.4.4.m4.1.1.3.2.cmml\">D</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A2.T2.4.4.4.4.m4.1.1.3.1\" xref=\"A2.T2.4.4.4.4.m4.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#F5FAFC\" id=\"A2.T2.4.4.4.4.m4.1.1.3.3\" xref=\"A2.T2.4.4.4.4.m4.1.1.3.3.cmml\">P</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A2.T2.4.4.4.4.m4.1b\"><apply id=\"A2.T2.4.4.4.4.m4.1.1.cmml\" xref=\"A2.T2.4.4.4.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"A2.T2.4.4.4.4.m4.1.1.1.cmml\" xref=\"A2.T2.4.4.4.4.m4.1.1\">superscript</csymbol><ci id=\"A2.T2.4.4.4.4.m4.1.1.2.cmml\" xref=\"A2.T2.4.4.4.4.m4.1.1.2\">Δ</ci><apply id=\"A2.T2.4.4.4.4.m4.1.1.3.cmml\" xref=\"A2.T2.4.4.4.4.m4.1.1.3\"><times id=\"A2.T2.4.4.4.4.m4.1.1.3.1.cmml\" xref=\"A2.T2.4.4.4.4.m4.1.1.3.1\"></times><ci id=\"A2.T2.4.4.4.4.m4.1.1.3.2.cmml\" xref=\"A2.T2.4.4.4.4.m4.1.1.3.2\">𝐷</ci><ci id=\"A2.T2.4.4.4.4.m4.1.1.3.3.cmml\" xref=\"A2.T2.4.4.4.4.m4.1.1.3.3\">𝑃</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T2.4.4.4.4.m4.1c\">\\Delta^{DP}</annotation></semantics></math> \n<br class=\"ltx_break\">, table foot=]data/data_other_fl.csv\n <span id=\"A2.T2.4.4.4.4.1\" class=\"ltx_ERROR undefined\">\\csvcoli</span> <span id=\"A2.T2.4.4.4.4.2\" class=\"ltx_ERROR undefined\">\\csvcolii</span> (<span id=\"A2.T2.4.4.4.4.3\" class=\"ltx_ERROR undefined\">\\csvcoliii</span>)  <span id=\"A2.T2.4.4.4.4.4\" class=\"ltx_ERROR undefined\">\\csvcoliv</span> (<span id=\"A2.T2.4.4.4.4.5\" class=\"ltx_ERROR undefined\">\\csvcolv</span>)  <span id=\"A2.T2.4.4.4.4.6\" class=\"ltx_ERROR undefined\">\\csvcolvi</span> (<span id=\"A2.T2.4.4.4.4.7\" class=\"ltx_ERROR undefined\">\\csvcolvii</span>)  <span id=\"A2.T2.4.4.4.4.8\" class=\"ltx_ERROR undefined\">\\csvcolviii</span> (<span id=\"A2.T2.4.4.4.4.9\" class=\"ltx_ERROR undefined\">\\csvcolix</span>)  <span id=\"A2.T2.4.4.4.4.10\" class=\"ltx_ERROR undefined\">\\csvcolx</span> (<span id=\"A2.T2.4.4.4.4.11\" class=\"ltx_ERROR undefined\">\\csvcolxi</span>)</span></span></p>\n</span></div>\n\n",
        "footnotes": "[tabular=ca*2e*2d,table head=     Race     Sex  \nAlgorithm  Accuracy  ΔE​OsuperscriptΔ𝐸𝑂\\Delta^{EO}  ΔD​PsuperscriptΔ𝐷𝑃\\Delta^{DP} ΔE​OsuperscriptΔ𝐸𝑂\\Delta^{EO}  ΔD​PsuperscriptΔ𝐷𝑃\\Delta^{DP} \n, table foot=]data/data_other_fl.csv\n \\csvcoli \\csvcolii (\\csvcoliii)  \\csvcoliv (\\csvcolv)  \\csvcolvi (\\csvcolvii)  \\csvcolviii (\\csvcolix)  \\csvcolx (\\csvcolxi)",
        "references": [
            "Table 2 shows results on other FL algorithms, including FedNova [Wang et al., 2020], Scaffold Karimireddy et al. [2020b], FedOpt [Asad et al., 2020], FedProx Li et al. [2020], and Mime [Karimireddy et al., 2020a].\nWe find that when the FL model achieves higher accuracy than standalone models, the fairness gap of the FL model can be higher than that of standalone models. This observation is consistent across multiple FL algorithms. This strong evidence implies that the improvement of accuracy in FL can come at the cost of fairness. In addition, this is not unique to only the FedAvg algorithm."
        ]
    },
    "A3.T3": {
        "caption": "Table 3: Effect of Fair FL [Abay et al., 2020] - (Income). The standard deviation across five runs is indicated between parentheses. The \"Sensitive Attribute\" column indicates the sensitive attribute used in the reweighting algorithm.",
        "table": "<div id=\"A3.T3.4\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:433.6pt;height:40.8pt;vertical-align:-31.8pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(36.1pt,-0.7pt) scale(1.19959219035754,1.19959219035754) ;\"><span id=\"A3.T3.4.5\" class=\"ltx_ERROR undefined\">\\csvreader</span>\n<p id=\"A3.T3.4.4\" class=\"ltx_p\">[tabular=cca*2e*2d,table head=     <span id=\"A3.T3.4.4.4\" class=\"ltx_text\" style=\"background-color:#F6FCF2;\"> Race     <span id=\"A3.T3.4.4.4.4\" class=\"ltx_text\" style=\"background-color:#F5FAFC;\">Sex  \n<br class=\"ltx_break\">Algorithm  Sensitive Attribute  Accuracy  <math id=\"A3.T3.1.1.1.1.m1.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{EO}\" display=\"inline\"><semantics id=\"A3.T3.1.1.1.1.m1.1a\"><msup id=\"A3.T3.1.1.1.1.m1.1.1\" xref=\"A3.T3.1.1.1.1.m1.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A3.T3.1.1.1.1.m1.1.1.2\" xref=\"A3.T3.1.1.1.1.m1.1.1.2.cmml\">Δ</mi><mrow id=\"A3.T3.1.1.1.1.m1.1.1.3\" xref=\"A3.T3.1.1.1.1.m1.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.1.1.1.1.m1.1.1.3.2\" xref=\"A3.T3.1.1.1.1.m1.1.1.3.2.cmml\">E</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A3.T3.1.1.1.1.m1.1.1.3.1\" xref=\"A3.T3.1.1.1.1.m1.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.1.1.1.1.m1.1.1.3.3\" xref=\"A3.T3.1.1.1.1.m1.1.1.3.3.cmml\">O</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A3.T3.1.1.1.1.m1.1b\"><apply id=\"A3.T3.1.1.1.1.m1.1.1.cmml\" xref=\"A3.T3.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T3.1.1.1.1.m1.1.1.1.cmml\" xref=\"A3.T3.1.1.1.1.m1.1.1\">superscript</csymbol><ci id=\"A3.T3.1.1.1.1.m1.1.1.2.cmml\" xref=\"A3.T3.1.1.1.1.m1.1.1.2\">Δ</ci><apply id=\"A3.T3.1.1.1.1.m1.1.1.3.cmml\" xref=\"A3.T3.1.1.1.1.m1.1.1.3\"><times id=\"A3.T3.1.1.1.1.m1.1.1.3.1.cmml\" xref=\"A3.T3.1.1.1.1.m1.1.1.3.1\"></times><ci id=\"A3.T3.1.1.1.1.m1.1.1.3.2.cmml\" xref=\"A3.T3.1.1.1.1.m1.1.1.3.2\">𝐸</ci><ci id=\"A3.T3.1.1.1.1.m1.1.1.3.3.cmml\" xref=\"A3.T3.1.1.1.1.m1.1.1.3.3\">𝑂</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T3.1.1.1.1.m1.1c\">\\Delta^{EO}</annotation></semantics></math>  <math id=\"A3.T3.2.2.2.2.m2.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{DP}\" display=\"inline\"><semantics id=\"A3.T3.2.2.2.2.m2.1a\"><msup id=\"A3.T3.2.2.2.2.m2.1.1\" xref=\"A3.T3.2.2.2.2.m2.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A3.T3.2.2.2.2.m2.1.1.2\" xref=\"A3.T3.2.2.2.2.m2.1.1.2.cmml\">Δ</mi><mrow id=\"A3.T3.2.2.2.2.m2.1.1.3\" xref=\"A3.T3.2.2.2.2.m2.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.2.2.2.2.m2.1.1.3.2\" xref=\"A3.T3.2.2.2.2.m2.1.1.3.2.cmml\">D</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A3.T3.2.2.2.2.m2.1.1.3.1\" xref=\"A3.T3.2.2.2.2.m2.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.2.2.2.2.m2.1.1.3.3\" xref=\"A3.T3.2.2.2.2.m2.1.1.3.3.cmml\">P</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A3.T3.2.2.2.2.m2.1b\"><apply id=\"A3.T3.2.2.2.2.m2.1.1.cmml\" xref=\"A3.T3.2.2.2.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T3.2.2.2.2.m2.1.1.1.cmml\" xref=\"A3.T3.2.2.2.2.m2.1.1\">superscript</csymbol><ci id=\"A3.T3.2.2.2.2.m2.1.1.2.cmml\" xref=\"A3.T3.2.2.2.2.m2.1.1.2\">Δ</ci><apply id=\"A3.T3.2.2.2.2.m2.1.1.3.cmml\" xref=\"A3.T3.2.2.2.2.m2.1.1.3\"><times id=\"A3.T3.2.2.2.2.m2.1.1.3.1.cmml\" xref=\"A3.T3.2.2.2.2.m2.1.1.3.1\"></times><ci id=\"A3.T3.2.2.2.2.m2.1.1.3.2.cmml\" xref=\"A3.T3.2.2.2.2.m2.1.1.3.2\">𝐷</ci><ci id=\"A3.T3.2.2.2.2.m2.1.1.3.3.cmml\" xref=\"A3.T3.2.2.2.2.m2.1.1.3.3\">𝑃</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T3.2.2.2.2.m2.1c\">\\Delta^{DP}</annotation></semantics></math> <math id=\"A3.T3.3.3.3.3.m3.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{EO}\" display=\"inline\"><semantics id=\"A3.T3.3.3.3.3.m3.1a\"><msup id=\"A3.T3.3.3.3.3.m3.1.1\" xref=\"A3.T3.3.3.3.3.m3.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A3.T3.3.3.3.3.m3.1.1.2\" xref=\"A3.T3.3.3.3.3.m3.1.1.2.cmml\">Δ</mi><mrow id=\"A3.T3.3.3.3.3.m3.1.1.3\" xref=\"A3.T3.3.3.3.3.m3.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.3.3.3.3.m3.1.1.3.2\" xref=\"A3.T3.3.3.3.3.m3.1.1.3.2.cmml\">E</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A3.T3.3.3.3.3.m3.1.1.3.1\" xref=\"A3.T3.3.3.3.3.m3.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.3.3.3.3.m3.1.1.3.3\" xref=\"A3.T3.3.3.3.3.m3.1.1.3.3.cmml\">O</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A3.T3.3.3.3.3.m3.1b\"><apply id=\"A3.T3.3.3.3.3.m3.1.1.cmml\" xref=\"A3.T3.3.3.3.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T3.3.3.3.3.m3.1.1.1.cmml\" xref=\"A3.T3.3.3.3.3.m3.1.1\">superscript</csymbol><ci id=\"A3.T3.3.3.3.3.m3.1.1.2.cmml\" xref=\"A3.T3.3.3.3.3.m3.1.1.2\">Δ</ci><apply id=\"A3.T3.3.3.3.3.m3.1.1.3.cmml\" xref=\"A3.T3.3.3.3.3.m3.1.1.3\"><times id=\"A3.T3.3.3.3.3.m3.1.1.3.1.cmml\" xref=\"A3.T3.3.3.3.3.m3.1.1.3.1\"></times><ci id=\"A3.T3.3.3.3.3.m3.1.1.3.2.cmml\" xref=\"A3.T3.3.3.3.3.m3.1.1.3.2\">𝐸</ci><ci id=\"A3.T3.3.3.3.3.m3.1.1.3.3.cmml\" xref=\"A3.T3.3.3.3.3.m3.1.1.3.3\">𝑂</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T3.3.3.3.3.m3.1c\">\\Delta^{EO}</annotation></semantics></math>  <math id=\"A3.T3.4.4.4.4.m4.1\" class=\"ltx_Math\" style=\"background-color:#F5FAFC;\" alttext=\"\\Delta^{DP}\" display=\"inline\"><semantics id=\"A3.T3.4.4.4.4.m4.1a\"><msup id=\"A3.T3.4.4.4.4.m4.1.1\" xref=\"A3.T3.4.4.4.4.m4.1.1.cmml\"><mi mathbackground=\"#F5FAFC\" mathvariant=\"normal\" id=\"A3.T3.4.4.4.4.m4.1.1.2\" xref=\"A3.T3.4.4.4.4.m4.1.1.2.cmml\">Δ</mi><mrow id=\"A3.T3.4.4.4.4.m4.1.1.3\" xref=\"A3.T3.4.4.4.4.m4.1.1.3.cmml\"><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.4.4.4.4.m4.1.1.3.2\" xref=\"A3.T3.4.4.4.4.m4.1.1.3.2.cmml\">D</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A3.T3.4.4.4.4.m4.1.1.3.1\" xref=\"A3.T3.4.4.4.4.m4.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#F5FAFC\" id=\"A3.T3.4.4.4.4.m4.1.1.3.3\" xref=\"A3.T3.4.4.4.4.m4.1.1.3.3.cmml\">P</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A3.T3.4.4.4.4.m4.1b\"><apply id=\"A3.T3.4.4.4.4.m4.1.1.cmml\" xref=\"A3.T3.4.4.4.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T3.4.4.4.4.m4.1.1.1.cmml\" xref=\"A3.T3.4.4.4.4.m4.1.1\">superscript</csymbol><ci id=\"A3.T3.4.4.4.4.m4.1.1.2.cmml\" xref=\"A3.T3.4.4.4.4.m4.1.1.2\">Δ</ci><apply id=\"A3.T3.4.4.4.4.m4.1.1.3.cmml\" xref=\"A3.T3.4.4.4.4.m4.1.1.3\"><times id=\"A3.T3.4.4.4.4.m4.1.1.3.1.cmml\" xref=\"A3.T3.4.4.4.4.m4.1.1.3.1\"></times><ci id=\"A3.T3.4.4.4.4.m4.1.1.3.2.cmml\" xref=\"A3.T3.4.4.4.4.m4.1.1.3.2\">𝐷</ci><ci id=\"A3.T3.4.4.4.4.m4.1.1.3.3.cmml\" xref=\"A3.T3.4.4.4.4.m4.1.1.3.3\">𝑃</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T3.4.4.4.4.m4.1c\">\\Delta^{DP}</annotation></semantics></math> \n<br class=\"ltx_break\">, table foot=]data/data_reweiging.csv\n <span id=\"A3.T3.4.4.4.4.1\" class=\"ltx_ERROR undefined\">\\csvcoli</span> <span id=\"A3.T3.4.4.4.4.2\" class=\"ltx_ERROR undefined\">\\csvcolxii</span><span id=\"A3.T3.4.4.4.4.3\" class=\"ltx_ERROR undefined\">\\csvcolii</span> (<span id=\"A3.T3.4.4.4.4.4\" class=\"ltx_ERROR undefined\">\\csvcoliii</span>)  <span id=\"A3.T3.4.4.4.4.5\" class=\"ltx_ERROR undefined\">\\csvcoliv</span> (<span id=\"A3.T3.4.4.4.4.6\" class=\"ltx_ERROR undefined\">\\csvcolv</span>)  <span id=\"A3.T3.4.4.4.4.7\" class=\"ltx_ERROR undefined\">\\csvcolvi</span> (<span id=\"A3.T3.4.4.4.4.8\" class=\"ltx_ERROR undefined\">\\csvcolvii</span>)  <span id=\"A3.T3.4.4.4.4.9\" class=\"ltx_ERROR undefined\">\\csvcolviii</span> (<span id=\"A3.T3.4.4.4.4.10\" class=\"ltx_ERROR undefined\">\\csvcolix</span>)  <span id=\"A3.T3.4.4.4.4.11\" class=\"ltx_ERROR undefined\">\\csvcolx</span> (<span id=\"A3.T3.4.4.4.4.12\" class=\"ltx_ERROR undefined\">\\csvcolxi</span>)</span></span></p>\n</span></div>\n\n",
        "footnotes": "[tabular=cca*2e*2d,table head=      Race     Sex  \nAlgorithm  Sensitive Attribute  Accuracy  ΔE​OsuperscriptΔ𝐸𝑂\\Delta^{EO}  ΔD​PsuperscriptΔ𝐷𝑃\\Delta^{DP} ΔE​OsuperscriptΔ𝐸𝑂\\Delta^{EO}  ΔD​PsuperscriptΔ𝐷𝑃\\Delta^{DP} \n, table foot=]data/data_reweiging.csv\n \\csvcoli \\csvcolxii\\csvcolii (\\csvcoliii)  \\csvcoliv (\\csvcolv)  \\csvcolvi (\\csvcolvii)  \\csvcolviii (\\csvcolix)  \\csvcolx (\\csvcolxi)",
        "references": [
            "Table 3 presents the results for a fair FL algorithm proposed by Abay et al. [2020], which uses a reweighting algorithm proposed in the centralized setting [Kamiran and Calders, 2012]. The algorithm involves assigning weights to each subgroup (defined by the label and a sensitive attribute) before FL, based on the local or global training dataset. This is called \"local reweighting\" or \"global reweighting,\" respectively. During FL, parties update the FL model to minimize the weighted loss. The table presents the accuracy and fairness gap of the FL algorithm using local and global reweighting for the Income, Health, and Employment datasets.",
            "When improving fairness with respect to one sensitive attribute, the fairness issue with respect to another sensitive attribute may worsen, as shown in Table 3. For instance, when applying local reweighing to reduce bias with respect to the \"Sex\" attribute, the fairness gap with respect to \"Race\" increased from 0.514 (results on FedAvg) to 0.521, indicating that reweighing to improve fairness with respect to Sex may amplify the bias with respect to Race. This implies that the bias with respect to Race can still propagate in FL, and our analysis remains valid."
        ]
    },
    "A4.T4": {
        "caption": "Table 4: Effect of FL on the real-world medical dataset - (ISIC2019). The standard deviation across four runs is indicated between parentheses. ",
        "table": "<div id=\"A4.T4.4\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:433.6pt;height:62.9pt;vertical-align:-54.8pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(16.9pt,-0.3pt) scale(1.0845000976968,1.0845000976968) ;\"><span id=\"A4.T4.4.5\" class=\"ltx_ERROR undefined\">\\csvreader</span>\n<p id=\"A4.T4.4.4\" class=\"ltx_p\">[tabular=c*2a*2e*2d*2f,table head= Setting   <span id=\"A4.T4.4.4.4\" class=\"ltx_text\" style=\"background-color:#FFF3F3;\"> Party 1    <span id=\"A4.T4.4.4.4.4\" class=\"ltx_text\" style=\"background-color:#F6FCF2;\"> Party 2     <span id=\"A4.T4.4.4.4.4.4\" class=\"ltx_text\" style=\"background-color:#F5FAFC;\"> Party 3     <span id=\"A4.T4.4.4.4.4.4.4\" class=\"ltx_text\" style=\"background-color:#FFF9F8;\"> Party 4 \n<br class=\"ltx_break\">Algorithm  Accuracy  <math id=\"A4.T4.1.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\Delta^{Acc}\" display=\"inline\"><semantics id=\"A4.T4.1.1.1.1.1.1.m1.1a\"><msup id=\"A4.T4.1.1.1.1.1.1.m1.1.1\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.cmml\"><mi mathbackground=\"#FFF9F8\" mathvariant=\"normal\" id=\"A4.T4.1.1.1.1.1.1.m1.1.1.2\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.2.cmml\">Δ</mi><mrow id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.cmml\"><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.2\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.1\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.3\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.3.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.1a\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.4\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.4.cmml\">c</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A4.T4.1.1.1.1.1.1.m1.1b\"><apply id=\"A4.T4.1.1.1.1.1.1.m1.1.1.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A4.T4.1.1.1.1.1.1.m1.1.1.1.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1\">superscript</csymbol><ci id=\"A4.T4.1.1.1.1.1.1.m1.1.1.2.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.2\">Δ</ci><apply id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3\"><times id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.1.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.1\"></times><ci id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.2.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.2\">𝐴</ci><ci id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.3.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.3\">𝑐</ci><ci id=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.4.cmml\" xref=\"A4.T4.1.1.1.1.1.1.m1.1.1.3.4\">𝑐</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T4.1.1.1.1.1.1.m1.1c\">\\Delta^{Acc}</annotation></semantics></math>  Accuracy  <math id=\"A4.T4.2.2.2.2.2.2.m2.1\" class=\"ltx_Math\" alttext=\"\\Delta^{Acc}\" display=\"inline\"><semantics id=\"A4.T4.2.2.2.2.2.2.m2.1a\"><msup id=\"A4.T4.2.2.2.2.2.2.m2.1.1\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.cmml\"><mi mathbackground=\"#FFF9F8\" mathvariant=\"normal\" id=\"A4.T4.2.2.2.2.2.2.m2.1.1.2\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.2.cmml\">Δ</mi><mrow id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.cmml\"><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.2\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.1\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.3\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.3.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.1a\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.4\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.4.cmml\">c</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A4.T4.2.2.2.2.2.2.m2.1b\"><apply id=\"A4.T4.2.2.2.2.2.2.m2.1.1.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"A4.T4.2.2.2.2.2.2.m2.1.1.1.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1\">superscript</csymbol><ci id=\"A4.T4.2.2.2.2.2.2.m2.1.1.2.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.2\">Δ</ci><apply id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3\"><times id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.1.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.1\"></times><ci id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.2.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.2\">𝐴</ci><ci id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.3.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.3\">𝑐</ci><ci id=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.4.cmml\" xref=\"A4.T4.2.2.2.2.2.2.m2.1.1.3.4\">𝑐</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T4.2.2.2.2.2.2.m2.1c\">\\Delta^{Acc}</annotation></semantics></math>  Accuracy  <math id=\"A4.T4.3.3.3.3.3.3.m3.1\" class=\"ltx_Math\" alttext=\"\\Delta^{Acc}\" display=\"inline\"><semantics id=\"A4.T4.3.3.3.3.3.3.m3.1a\"><msup id=\"A4.T4.3.3.3.3.3.3.m3.1.1\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.cmml\"><mi mathbackground=\"#FFF9F8\" mathvariant=\"normal\" id=\"A4.T4.3.3.3.3.3.3.m3.1.1.2\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.2.cmml\">Δ</mi><mrow id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.cmml\"><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.2\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.1\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.3\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.3.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.1a\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.4\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.4.cmml\">c</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A4.T4.3.3.3.3.3.3.m3.1b\"><apply id=\"A4.T4.3.3.3.3.3.3.m3.1.1.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"A4.T4.3.3.3.3.3.3.m3.1.1.1.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1\">superscript</csymbol><ci id=\"A4.T4.3.3.3.3.3.3.m3.1.1.2.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.2\">Δ</ci><apply id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3\"><times id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.1.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.1\"></times><ci id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.2.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.2\">𝐴</ci><ci id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.3.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.3\">𝑐</ci><ci id=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.4.cmml\" xref=\"A4.T4.3.3.3.3.3.3.m3.1.1.3.4\">𝑐</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T4.3.3.3.3.3.3.m3.1c\">\\Delta^{Acc}</annotation></semantics></math>  Accuracy  <math id=\"A4.T4.4.4.4.4.4.4.m4.1\" class=\"ltx_Math\" alttext=\"\\Delta^{Acc}\" display=\"inline\"><semantics id=\"A4.T4.4.4.4.4.4.4.m4.1a\"><msup id=\"A4.T4.4.4.4.4.4.4.m4.1.1\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.cmml\"><mi mathbackground=\"#FFF9F8\" mathvariant=\"normal\" id=\"A4.T4.4.4.4.4.4.4.m4.1.1.2\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.2.cmml\">Δ</mi><mrow id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.cmml\"><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.2\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.2.cmml\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.1\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.3\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.3.cmml\">c</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.1a\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.1.cmml\">​</mo><mi mathbackground=\"#FFF9F8\" id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.4\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.4.cmml\">c</mi></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A4.T4.4.4.4.4.4.4.m4.1b\"><apply id=\"A4.T4.4.4.4.4.4.4.m4.1.1.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"A4.T4.4.4.4.4.4.4.m4.1.1.1.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1\">superscript</csymbol><ci id=\"A4.T4.4.4.4.4.4.4.m4.1.1.2.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.2\">Δ</ci><apply id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3\"><times id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.1.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.1\"></times><ci id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.2.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.2\">𝐴</ci><ci id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.3.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.3\">𝑐</ci><ci id=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.4.cmml\" xref=\"A4.T4.4.4.4.4.4.4.m4.1.1.3.4\">𝑐</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T4.4.4.4.4.4.4.m4.1c\">\\Delta^{Acc}</annotation></semantics></math> \n<br class=\"ltx_break\">, table foot=]data/isic2019.csv\n <span id=\"A4.T4.4.4.4.4.4.4.1\" class=\"ltx_ERROR undefined\">\\csvcolii</span> <span id=\"A4.T4.4.4.4.4.4.4.2\" class=\"ltx_ERROR undefined\">\\csvcoliii</span> (<span id=\"A4.T4.4.4.4.4.4.4.3\" class=\"ltx_ERROR undefined\">\\csvcoliv</span>)  <span id=\"A4.T4.4.4.4.4.4.4.4\" class=\"ltx_ERROR undefined\">\\csvcolv</span> (<span id=\"A4.T4.4.4.4.4.4.4.5\" class=\"ltx_ERROR undefined\">\\csvcolvi</span>)  <span id=\"A4.T4.4.4.4.4.4.4.6\" class=\"ltx_ERROR undefined\">\\csvcolvii</span> (<span id=\"A4.T4.4.4.4.4.4.4.7\" class=\"ltx_ERROR undefined\">\\csvcolviii</span>)  <span id=\"A4.T4.4.4.4.4.4.4.8\" class=\"ltx_ERROR undefined\">\\csvcolix</span> (<span id=\"A4.T4.4.4.4.4.4.4.9\" class=\"ltx_ERROR undefined\">\\csvcolx</span>)  <span id=\"A4.T4.4.4.4.4.4.4.10\" class=\"ltx_ERROR undefined\">\\csvcolxi</span> (<span id=\"A4.T4.4.4.4.4.4.4.11\" class=\"ltx_ERROR undefined\">\\csvcolxii</span>)  <span id=\"A4.T4.4.4.4.4.4.4.12\" class=\"ltx_ERROR undefined\">\\csvcolxiii</span> (<span id=\"A4.T4.4.4.4.4.4.4.13\" class=\"ltx_ERROR undefined\">\\csvcolxiv</span>)  <span id=\"A4.T4.4.4.4.4.4.4.14\" class=\"ltx_ERROR undefined\">\\csvcolxv</span> (<span id=\"A4.T4.4.4.4.4.4.4.15\" class=\"ltx_ERROR undefined\">\\csvcolxvi</span>)  <span id=\"A4.T4.4.4.4.4.4.4.16\" class=\"ltx_ERROR undefined\">\\csvcolxvii</span> (<span id=\"A4.T4.4.4.4.4.4.4.17\" class=\"ltx_ERROR undefined\">\\csvcolxviii</span>)</span></span></span></span></p>\n</span></div>\n\n",
        "footnotes": "[tabular=c*2a*2e*2d*2f,table head= Setting    Party 1     Party 2      Party 3      Party 4 \nAlgorithm  Accuracy  ΔA​c​csuperscriptΔ𝐴𝑐𝑐\\Delta^{Acc}  Accuracy  ΔA​c​csuperscriptΔ𝐴𝑐𝑐\\Delta^{Acc}  Accuracy  ΔA​c​csuperscriptΔ𝐴𝑐𝑐\\Delta^{Acc}  Accuracy  ΔA​c​csuperscriptΔ𝐴𝑐𝑐\\Delta^{Acc} \n, table foot=]data/isic2019.csv\n \\csvcolii \\csvcoliii (\\csvcoliv)  \\csvcolv (\\csvcolvi)  \\csvcolvii (\\csvcolviii)  \\csvcolix (\\csvcolx)  \\csvcolxi (\\csvcolxii)  \\csvcolxiii (\\csvcolxiv)  \\csvcolxv (\\csvcolxvi)  \\csvcolxvii (\\csvcolxviii)",
        "references": [
            "We evaluate the effect of FL on local fairness on a real-world medical dataset, ISIC2019 [Abay et al., 2020, Tschandl et al., 2018, Combalia et al., 2019], which contains dermoscopic images of skin lesions collected from six medical centers. The task is to classify dermoscopic images among nine different diagnostic categories. We filter out medical centers with less than 1900 images and regard each medical center with over 1900 images as a party (4 centers remain after filtering). For every party, we randomly select 1500 and 400 images for training and validation, respectively. We regard the binary notion of Sex as our sensitive attribute. In this multi-class and medical dataset setting, we measure the bias (fairness gap) based on the accuracy gap across groups. Following  [Ogier du Terrail et al., 2022] 222https://github.com/owkin/FLamby, we train EfficientNets [Tan and Le, 2019], the state-of-the-art model structure for medical data, under the centralized, FL, and standalone setting. The accuracy and fairness gap for each client in different settings are shown in Table 4."
        ]
    }
}