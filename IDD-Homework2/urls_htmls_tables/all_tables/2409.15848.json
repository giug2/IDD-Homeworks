{
    "id_table_1": {
        "caption": "TABLE I:  As an example, a real-world dataset was collected from a computerized ticketing system. 39,100 labeled data objects are divided into 15 classes, and the column Size indicates the number of data objects. 80% of the data was used for training, and 20% for testing. The column Recall shows the results of testing the best CatBoost model developed with the traditional ML workflow in Fig.  1 (a).",
        "table": "S2.T1.1",
        "footnotes": [
            ""
        ],
        "references": [
            "In conventional ML workflows, as shown in Fig.  1 (a), ML developers perform numerous iterations from data preparation to learning preparation, training, and testing  [ 69 ] . As demonstrated by many existing VIS4ML techniques (Section  1 ), VIS techniques can help reduce the number of iterations and speed up the human-centric processes in these iterations. When the first author started his 2-month placement in Inetum, we quickly identified the first requirement based on our observation of the existing ML workflows and our knowledge and experience of VIS4ML, i.e.,  R1: Using more VIS techniques to help identify possible causes of errors .",
            "The three main requirements for this work were identified in an agile manner, which is consistent with the nested model approach  [ 70 ] . During the 2-month placement, we formulated a VIS4ML workflow as shown in Fig.  1 (b). We will detail these VIS techniques in Section  3 , and VIS-guided data synthesis in Section  4 . Following the placement, we designed and prototyped a VIS4ML tool, called iGAiVA ( integrated Generative AI and Visual Analytics ), for enabling VIS-guided data synthesis in iterative ML workflows as illustrated in Fig.  1 (c). We will detail the design of iGAiVA and its prototype in Section  5 . There were two further placements, where the system was evaluated and improved in an agile manner. We will report this process in Section  7 .",
            "Before advanced VIS techniques were deployed, the team of ML developers worked with the traditional ML workflow as shown in Fig.  1 (a) for many months in conjunction with a dataset consisting of 39,100 labeled data objects (messages). The team used two ML methods, namely gradient-boosted decision trees (using CatBoost) and convolutional neural network (CNN, using TensorFlow), to train the models in conjunction with 80% randomly data objects, and tested with the remaining 20% data objects. A large number of iterations were focused on hyper-parameters of the training process. The recall column in Table  I  shows the results of testing the best CatBoost model obtained using the traditional ML workflow. Note that each class  T i subscript T i T_{i} italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  has  p i subscript p i p_{i} italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  labelled positive messages and  n i = Total  p i subscript n i Total subscript p i n_{i}=\\text{Total}-p_{i} italic_n start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = Total - italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  labelled negative messages. Hence,  n i  p i much-greater-than subscript n i subscript p i n_{i}\\gg p_{i} italic_n start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , especially for some small classes. In this work, we focus on recall that is not affected by  n i  p i much-greater-than subscript n i subscript p i n_{i}\\gg p_{i} italic_n start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , though we also used other performance metrics (e.g., accuracy, F1-score, and so on).",
            "As illustrated in the second workflow in Fig.  1 , the VIS process helped ML developers formulate hypotheses about data-related causes of erroneous results, allowing ML developers to shift their focus from fine-tuning hyper-parameters during the previous months (in the first workflow) to improving training data. As it was not feasible to collect more training data, we decided to experiment with synthetic data generated using large language models (LLMs), i.e., requirement  R2  (Section  2 ).",
            "As illustrated in Fig.  1 , we divided the workflow into four major stages, (A) data synthesis, (B) data selection and integration, (C) model training and testing, and (D) results evaluation. Although an ML developer is most likely to commence at stage (B) for selecting the given training and testing data without any synthetic data, after the first iteration involving (B    \\rightarrow   C    \\rightarrow   D), the ML developer will follow the sequence of (A    \\rightarrow   B    \\rightarrow   C    \\rightarrow   D) in most of the subsequent iterations, which typically take more than a few days.",
            "Using iGAiVA to Conduct Further Experiments.  The four views corresponding to the four major steps in Fig.  1 (c). With iGAiVA, the processes described in Sections  3  and  4  can be carried out iteratively and systematically. We provide a video in the supplementary material to demonstrate how multiple iterations were used to improve a model, together with an appendix, where we provide further technical details about processes shown in the video.",
            "As described in Section  2 , the three requirements were identified in multiple steps during the 2-month placement of the first author in the FabLab of Inetum, Spain. The experimental workflow in Fig.  1 , including the VIS and LLM solutions described in Sections  3  and  4 , was developed and evaluated during the placement as there were daily contacts between the first author (who is specialized in both VIS and ML) and the ML researchers and developers in the FabLab. There were two formal meetings and six informal meetings during the 2-month period. The first author wrote two reports and gave two presentations on the VIS and LLM solutions in the experimental workflow. In addition, there were weekly online technical meetings, involving the third author, who visited the Inetum, Spain before the placement."
        ]
    },
    "id_table_2": {
        "caption": "TABLE II:  Example testing results with synthetic data. The  test  column shows the number of data objects in the testing dataset, which is used to obtain all testing results in the table. The  train  columns show the numbers of data objects for training. In the five cases of synthetic data, a +num indicates the number of synthetic text messages that were added to the specific class in the training data, while the blank cell indicates that no synthetic data was added to a class. The five    \\Delta roman_ - recall  columns show the difference between the original recall (column 4) and recall values obtained from testing the five models trained with the corresponding training data (i.e., original training data + synthetic data). See also Appendix  A .",
        "table": "S3.F4.10",
        "footnotes": [
            ""
        ],
        "references": [
            "As shown in Fig.  2 , one of our initial visualizations revealed some noticeable correlation between the number of data objects in a class and the recall of the class. We suspected that there were some data sampling issues, and therefore focused our attention on data distribution. While visualizing summary statistics (e.g., Fig.  2 ) can suggest aspects to be investigated, it is necessary to conduct a more detailed investigation into the potential causes of the high error rates in some classes.",
            "For the requirement  R1  discussed in Section  2 , we use four types of VA techniques for organizing data objects using analytical algorithms and observing different patterns visually. From the visual patterns, we hypothesize potential causes of errors and use statistics to evaluate such hypotheses. In the following four subsections, we describe how each type of VA enables pattern discovery and hypothesis generation.",
            "As illustrated in the second workflow in Fig.  1 , the VIS process helped ML developers formulate hypotheses about data-related causes of erroneous results, allowing ML developers to shift their focus from fine-tuning hyper-parameters during the previous months (in the first workflow) to improving training data. As it was not feasible to collect more training data, we decided to experiment with synthetic data generated using large language models (LLMs), i.e., requirement  R2  (Section  2 ).",
            "In the previous two sections, we reported the successful use of VIS and LLM techniques for improving the performance of ML models for text classification. As mentioned in Section  2 , this successful experience led to the requirement for designing and prototyping a VIS4ML tool where VIS and LLM techniques would be integrated ( R3 ). When we were working on the VIS and LLM techniques in the experimental workflow, we used VIS techniques, including PCA scatter plots, RBF heatmaps, and tag-treemaps frequently. On average, we would view at least 10 plots (e.g., Fig.  4 ) before a run of LLMs to generate some synthetic data. After the synthetic data was generated, one would visualize the synthetic data (e.g., Fig.  5 (c)) before retraining the model. After the retrained model is tested, we would visualize the testing results (e.g., Fig.  5 (d,e)). We concluded that the VIS techniques should ideally be available in almost every stage of the workflow.",
            "As described in Section  2 , the three requirements were identified in multiple steps during the 2-month placement of the first author in the FabLab of Inetum, Spain. The experimental workflow in Fig.  1 , including the VIS and LLM solutions described in Sections  3  and  4 , was developed and evaluated during the placement as there were daily contacts between the first author (who is specialized in both VIS and ML) and the ML researchers and developers in the FabLab. There were two formal meetings and six informal meetings during the 2-month period. The first author wrote two reports and gave two presentations on the VIS and LLM solutions in the experimental workflow. In addition, there were weekly online technical meetings, involving the third author, who visited the Inetum, Spain before the placement."
        ]
    },
    "id_table_3": {
        "caption": "TABLE III:  Further results from the first iteration: retraining with different class-based synthetic data. The    \\Delta roman_ -recall results were compared with the original model M0 (Major Column 2).",
        "table": "S3.F5.8",
        "footnotes": [],
        "references": [
            "The three main requirements for this work were identified in an agile manner, which is consistent with the nested model approach  [ 70 ] . During the 2-month placement, we formulated a VIS4ML workflow as shown in Fig.  1 (b). We will detail these VIS techniques in Section  3 , and VIS-guided data synthesis in Section  4 . Following the placement, we designed and prototyped a VIS4ML tool, called iGAiVA ( integrated Generative AI and Visual Analytics ), for enabling VIS-guided data synthesis in iterative ML workflows as illustrated in Fig.  1 (c). We will detail the design of iGAiVA and its prototype in Section  5 . There were two further placements, where the system was evaluated and improved in an agile manner. We will report this process in Section  7 .",
            "t-SNE is an unsupervised non-linear dimensionality reduction that constructs a  k k k italic_k -D data model based on a  n n n italic_n -D dataset (typically  n  k much-greater-than n k n\\gg k italic_n  italic_k ). When  k = 2 k 2 k=2 italic_k = 2 , the data model is commonly visualized using a scatter plot. Fig.  3  shows the application of t-SNE to the aforementioned dataset captured from an IT ticketing system. From the figure, we can observe two groups (or kinds) of patterns and each group has three subgroups:",
            "In a t-SNE scatter plot, when data objects in a class are clumped together or isolated from other classes, it indicates that the two feature dimensions formulated by the t-SNE dimensionality reduction algorithm can be used to distinguish this class from others relatively easily. Likely a reasonably good ML model can learn similar features and hence classify this class accurately. When data objects in a class are mingled with others or scattered around, it indicates that the two feature dimensions used for plotting the data objects are not ideal for distinguishing this class from others. It suggests that an ML model may have some difficulties in classifying this class. However, since the ML model will likely learn many other features, some of which could still help achieve good performance. For example, in Fig.  3 , the visual patterns for classes T2 and T3 are not ideal, but their classification results are fairly good (Table  I ). The ML model must have learned some useful features that are not characterized by the t-SNE plot.",
            "Without VIS techniques described in Section  3 , we would naturally target all classes with relatively poor recall, e.g., T6, T8-T10, T12-T15. We would have to select example text messages (i.e., data objects) randomly from all data objects in a class. With the VIS techniques, we can target data synthesis to more specific subareas in many ways, e.g.,",
            "In order to maintain the testing consistently, we define a testing dataset with 20% of data objects from each class. The other 80% data is used for training. Because we have some very small classes, e.g., T11-T15, such a training-testing division is unavoidable. As described in Section  3 , we use tag-treemap to compare the training and testing datasets for each class to ensure that they both represent the class in a similar way. When we generate synthetic data using the LLM, we always select example messages from the training data as we are aware that selecting examples from testing data could introduce biases in favor of testing.",
            "Synthesis View for Data Synthesis.  In iGAiVA, this view plays a distinct role in supporting an ML workflow involving data synthesis, while the ML tasks supported by the other three views are comparatively common in conventional ML workflows. At this stage, the primary task of an ML developer is to identify a set of example messages that can be used as inputs to LLMs for generating synthetic data. As demonstrated in Sections  3  and  4 , this task can benefit from VIS techniques extensively, with which the ML developer may use a t-SNE scatter plot to select a class to work on, use PCA scatter plots to observe patterns in different combinations of PCA dimensions; use RBF heatmaps to visualize subareas in a more quantitative and predictive manner; use tag-treemaps to compare the keywords statistics of different subareas; and use PCA scatter plots or RBF heatmaps to determine a subarea for selecting examples (from the training data) as inputs to LLMs.",
            "Using iGAiVA to Conduct Further Experiments.  The four views corresponding to the four major steps in Fig.  1 (c). With iGAiVA, the processes described in Sections  3  and  4  can be carried out iteratively and systematically. We provide a video in the supplementary material to demonstrate how multiple iterations were used to improve a model, together with an appendix, where we provide further technical details about processes shown in the video.",
            "As described in Section  2 , the three requirements were identified in multiple steps during the 2-month placement of the first author in the FabLab of Inetum, Spain. The experimental workflow in Fig.  1 , including the VIS and LLM solutions described in Sections  3  and  4 , was developed and evaluated during the placement as there were daily contacts between the first author (who is specialized in both VIS and ML) and the ML researchers and developers in the FabLab. There were two formal meetings and six informal meetings during the 2-month period. The first author wrote two reports and gave two presentations on the VIS and LLM solutions in the experimental workflow. In addition, there were weekly online technical meetings, involving the third author, who visited the Inetum, Spain before the placement.",
            "Through these frequent engagements and the development of VIS and LLM solutions, the three requirements emerged gradually. These requirements were evaluated continually when iGAiVA was developed. Below we summarized the major feedback on the technical work presented in Sections  3 ,  4 , and  5 , which correspond to the three requirements. All texts in  italic  are from the minutes of meetings and transcripts of conversations.",
            "In Section  3 , we explained the VIS-assisted processes for testing different synthetic data in order to improve an existing text classification model M0. Table  II  provides some of the testing results. In this appendix, we provide more testing results, including those produced by further iterations in the workflow.",
            "T11-s2 : Results in Table   III , Major Column 3.    \\rightarrow   T11 recall: +0.021, overall recall:   similar-to \\sim  0.000.",
            "T12-s2 : Results in Table   III , Major Column 4.    \\rightarrow   T12 recall: +0.125, overall recall: +0.008.",
            "T13-s2 : Results in Table   III , Major Column 5.    \\rightarrow   T13 recall: +0.022, overall recall:   similar-to \\sim  0.000.",
            "T14-s2 : Results in Table   III , Major Column 6.    \\rightarrow   T14 recall: +0.111, overall recall:   0.002 0.002 -0.002 - 0.002 .",
            "T15-s2 : Results in Table   III , Major Column 7.    \\rightarrow   T15 recall: +0.086, overall recall: +0.001."
        ]
    },
    "id_table_4": {
        "caption": "TABLE IV:  Results of the second iteration: retraining with different class-based synthetic data in addition to  T11-s1  synthetic data. The    \\Delta roman_ -recall results were compared with the original model M0 (Major Column 2), while the M1a results are given in Major Column 3.",
        "table": "S3.T2.82",
        "footnotes": [],
        "references": [
            "The three main requirements for this work were identified in an agile manner, which is consistent with the nested model approach  [ 70 ] . During the 2-month placement, we formulated a VIS4ML workflow as shown in Fig.  1 (b). We will detail these VIS techniques in Section  3 , and VIS-guided data synthesis in Section  4 . Following the placement, we designed and prototyped a VIS4ML tool, called iGAiVA ( integrated Generative AI and Visual Analytics ), for enabling VIS-guided data synthesis in iterative ML workflows as illustrated in Fig.  1 (c). We will detail the design of iGAiVA and its prototype in Section  5 . There were two further placements, where the system was evaluated and improved in an agile manner. We will report this process in Section  7 .",
            "In order to visualize the data distribution of each class in relation to a good number of features, we use PCA to extract  K K K italic_K  features (we set  K = 20 K 20 K=20 italic_K = 20  for the data in this paper). As shown in Fig.  4 , we use a scatter plot to visualize the data points in each class with the  x x x italic_x  and  y y y italic_y  axes corresponding to two dimensions  d i , d j subscript d i subscript d j d_{i},d_{j} italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  such that  d i , d j  [ 0 , K  1 ] subscript d i subscript d j 0 K 1 d_{i},d_{j}\\in[0,K-1] italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  [ 0 , italic_K - 1 ] , and  d i = d j subscript d i subscript d j d_{i}\\neq d_{j} italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT . In particular, we color the data objects that we correctly classified in blue and otherwise in red. As the visual context, data objects in other classes are shown in grey.",
            "In Fig.  4 , there are two examples of PCA scatter plots, which show patterns of  P3  and  P5 . It focuses on class T12, within PCA dimensions 0 and 2 in (a) and dimensions 1 and 13 in (b). The data objects used for training are shown in yellow-green, testing data objects are shown in blue (if correct) or red (if incorrect), and the data objects in other classes are shown in grey to provide a holistic context. In (a), the colored dots on the left are largely red (incorrect), scattered among grey dots. In (b), the colored dots in the upper part exhibit a similar pattern.",
            "From the PCA scatter plots in Fig.  4 , one cannot always judge easily whether there are more red or blue dots if the numbers of correct and incorrect results are not significantly different, because counting demands costly cognitive effort. Furthermore, ML developers are often curious about areas between sampled data objects in a class, since PCA assumes that each feature dimension is continuous, and potentially there are other messages that could have feature values between the known values of two sampled messages. We therefore use a radial basis function (RBF) to estimate the recall error rate across the 2D feature space depicted in a 2D PCA scatter plot. In Fig,  4 , the second column shows two heatmaps after applying an RBF to the two PCA scatter plots on the left. We superimpose the testing data objects, as dots, on the top of the RBF heatmap. This allows viewers to judge if a pixel in the heatmap is further away or close to the tested data objects, providing intuitive and implicit uncertainty information about the colors in the RBF heatmap. From an RBF plot, one can observe the following patterns:",
            "For example, we can define all testing data objects in a class as a parent set. We can consider two subsets, for the data objects classified correctly and those incorrectly. We can also divide testing data objects based on their features. In Fig.  4 , the two RBF heatmaps can be divided into two regions, and we can enrich the relatively abstract patterns in the PCA scatter plots and RBF heatmaps by juxtaposing the tag clouds for these regions as shown on the right of Fig.  4 . Each of these two tag-treemaps reveals the characteristic difference between the two regions. In (a), the tag clouds on the left and right convey the keyword statistics of 38 and 26 text messages respectively. We can observe the right (i.e., the more accurate part) has a much evener distribution of keywords than the left. Likely the ML model has learned some features similar to PCA Dimension 0 and may treat these two parts differently. In (b), the tag clouds above (33 messages) and below (31 messages) show similar divergence between the two parts, though the divergence is not as substantial as (a). We also use similar tag-treemaps to check whether the testing data has the same keyword statistics as the training data and all data objects in the class. In this case, the checking confirms the similarity. If there were noticeable divergences, the ML developers could resample the testing and training data to ensure that the two datasets could represent the whole class.",
            "which is one of the red dots in the PCA scatter plot in Fig.  4 (a) was misclassified by an existing ML model. We can use this as a piece of input text to the Inetum LLM. With appropriate parameters such as temperature = 0.7, max tokens = 550, top p = 0.5, frequency penalty = 0.3, presence penalty = 0.0, and so on, the LLM model generates  k k k italic_k  similar messages as the output:",
            "Fig.  4  suggests that T12 can potentially be divided into two parts based on PCA Dimension 0 or Dimension 13.",
            "In the previous two sections, we reported the successful use of VIS and LLM techniques for improving the performance of ML models for text classification. As mentioned in Section  2 , this successful experience led to the requirement for designing and prototyping a VIS4ML tool where VIS and LLM techniques would be integrated ( R3 ). When we were working on the VIS and LLM techniques in the experimental workflow, we used VIS techniques, including PCA scatter plots, RBF heatmaps, and tag-treemaps frequently. On average, we would view at least 10 plots (e.g., Fig.  4 ) before a run of LLMs to generate some synthetic data. After the synthetic data was generated, one would visualize the synthetic data (e.g., Fig.  5 (c)) before retraining the model. After the retrained model is tested, we would visualize the testing results (e.g., Fig.  5 (d,e)). We concluded that the VIS techniques should ideally be available in almost every stage of the workflow.",
            "Synthesis View for Data Synthesis.  In iGAiVA, this view plays a distinct role in supporting an ML workflow involving data synthesis, while the ML tasks supported by the other three views are comparatively common in conventional ML workflows. At this stage, the primary task of an ML developer is to identify a set of example messages that can be used as inputs to LLMs for generating synthetic data. As demonstrated in Sections  3  and  4 , this task can benefit from VIS techniques extensively, with which the ML developer may use a t-SNE scatter plot to select a class to work on, use PCA scatter plots to observe patterns in different combinations of PCA dimensions; use RBF heatmaps to visualize subareas in a more quantitative and predictive manner; use tag-treemaps to compare the keywords statistics of different subareas; and use PCA scatter plots or RBF heatmaps to determine a subarea for selecting examples (from the training data) as inputs to LLMs.",
            "Using iGAiVA to Conduct Further Experiments.  The four views corresponding to the four major steps in Fig.  1 (c). With iGAiVA, the processes described in Sections  3  and  4  can be carried out iteratively and systematically. We provide a video in the supplementary material to demonstrate how multiple iterations were used to improve a model, together with an appendix, where we provide further technical details about processes shown in the video.",
            "As described in Section  2 , the three requirements were identified in multiple steps during the 2-month placement of the first author in the FabLab of Inetum, Spain. The experimental workflow in Fig.  1 , including the VIS and LLM solutions described in Sections  3  and  4 , was developed and evaluated during the placement as there were daily contacts between the first author (who is specialized in both VIS and ML) and the ML researchers and developers in the FabLab. There were two formal meetings and six informal meetings during the 2-month period. The first author wrote two reports and gave two presentations on the VIS and LLM solutions in the experimental workflow. In addition, there were weekly online technical meetings, involving the third author, who visited the Inetum, Spain before the placement.",
            "Through these frequent engagements and the development of VIS and LLM solutions, the three requirements emerged gradually. These requirements were evaluated continually when iGAiVA was developed. Below we summarized the major feedback on the technical work presented in Sections  3 ,  4 , and  5 , which correspond to the three requirements. All texts in  italic  are from the minutes of meetings and transcripts of conversations.",
            "T11-s1 + T12-s1 : Results in Table   IV , Major Column 4.    \\rightarrow   T12 recall: +0.047, overall recall:  + 0.004 0.004 +0.004 + 0.004 .    \\rightarrow   Comparing with M1a (T11-s1) overall recall:  + 0.001 0.001 +0.001 + 0.001 .",
            "T11-s1 + T12-s2 : Results in Table   IV , Major Column 5.    \\rightarrow   T12 recall:   0.016 0.016 -0.016 - 0.016 , overall recall:   0.003 0.003 -0.003 - 0.003 .    \\rightarrow   Comparing with M1a (T11-s1) overall recall:   0.006 0.006 -0.006 - 0.006 .",
            "T11-s1 + T13-s1 : Results in Table   IV , Major Column 6.    \\rightarrow   T13 recall: +0.0.089, overall recall:   0.004 0.004 -0.004 - 0.004 .    \\rightarrow   Comparing with M1a (T11-s1) overall recall:   0.007 0.007 -0.007 - 0.007 .",
            "T11-s1 + T13-s2 : Results in Table   IV , Major Column 7.    \\rightarrow   T13 recall: +0.111, overall recall:   0.003 0.003 -0.003 - 0.003 .    \\rightarrow   Comparing with M1a (T11-s1) overall recall:   0.006 0.006 -0.006 - 0.006 .",
            "In this appendix, we provide high-resolution versions of the images in Figs.  4 ,  5 , and  6 .",
            "Original Fig.  4  Caption     Two examples of detailed visual analysis for investigating class T12. The two PCA scatter plots on the left show that Dimension 0 in (a) and Dimension 13 in (b) can separate the data objects into two regions, and data objects in one region have higher recall, while the overall class recall is only 37.5%. Each RBF plot in the second column makes the boundary between the high-recall and low-recall regions clearer, enabling the selection of a division line to study the summary statistics of the messages in the two regions using a tag-treemap on the right."
        ]
    },
    "id_table_5": {
        "caption": "TABLE V:  Results of the second iteration: retraining with different class-based synthetic data in addition to  T13-s1  synthetic data. The    \\Delta roman_ -recall results were compared with the original model M0 (Major Column 2), while the M1b results are given in Major Column 3.",
        "table": "S5.F6.4",
        "footnotes": [],
        "references": [
            "The three main requirements for this work were identified in an agile manner, which is consistent with the nested model approach  [ 70 ] . During the 2-month placement, we formulated a VIS4ML workflow as shown in Fig.  1 (b). We will detail these VIS techniques in Section  3 , and VIS-guided data synthesis in Section  4 . Following the placement, we designed and prototyped a VIS4ML tool, called iGAiVA ( integrated Generative AI and Visual Analytics ), for enabling VIS-guided data synthesis in iterative ML workflows as illustrated in Fig.  1 (c). We will detail the design of iGAiVA and its prototype in Section  5 . There were two further placements, where the system was evaluated and improved in an agile manner. We will report this process in Section  7 .",
            "As shown in Table  I , Class T13 has the lowest recall. One of the PCA scatter plots, as shown in Fig.  5 (a), reveals that the data objects in the class also exhibit two parts, the left part is less accurate than the right. The RBF heatmap in Fig.  5 (b) helps us determine a separation line. We then select example messages from the training data on the left part and generate 525 synthetic messages as additional training data. Fig.  5 (c) shows these synthetic data objects in purple. After retraining the model, the testing results show that the recall of the class improved from 18% to 31%. Fig.  5 (d) indicates noticeable changes in the blue region and the reduction of the shade of red. Fig.  5 (e) juxtaposes two zoomed-in PCA scatter plots and we can observe the changes of some red dots to blue dots.",
            "In the previous two sections, we reported the successful use of VIS and LLM techniques for improving the performance of ML models for text classification. As mentioned in Section  2 , this successful experience led to the requirement for designing and prototyping a VIS4ML tool where VIS and LLM techniques would be integrated ( R3 ). When we were working on the VIS and LLM techniques in the experimental workflow, we used VIS techniques, including PCA scatter plots, RBF heatmaps, and tag-treemaps frequently. On average, we would view at least 10 plots (e.g., Fig.  4 ) before a run of LLMs to generate some synthetic data. After the synthetic data was generated, one would visualize the synthetic data (e.g., Fig.  5 (c)) before retraining the model. After the retrained model is tested, we would visualize the testing results (e.g., Fig.  5 (d,e)). We concluded that the VIS techniques should ideally be available in almost every stage of the workflow.",
            "We also notice that the high-level visualization tasks in these four stages are quite different, though some VIS techniques may be required by multiple stages. For example, RBF heatmaps are useful at stage (A) for enabling a user to determine a dividing line as in Fig.  5 (b), and they are also useful at Stage (D) for comparing the results before and after retraining, e.g., comparing Fig.  5 (b) and (e). We therefore identify a set of VIS techniques for each major stage and make these VIS techniques available through a user interface that is designed to suit the tasks at each stage. This results in a design of the VIS4ML tool with four views corresponding to the four major stages of the workflow. We name this VIS4ML tool as  iGAiVA , which stands for  Integrated Generative AI and Visual Analytics . In the following subsections, we describe the functions of each view of iGAiVA.",
            "Through these frequent engagements and the development of VIS and LLM solutions, the three requirements emerged gradually. These requirements were evaluated continually when iGAiVA was developed. Below we summarized the major feedback on the technical work presented in Sections  3 ,  4 , and  5 , which correspond to the three requirements. All texts in  italic  are from the minutes of meetings and transcripts of conversations.",
            "T13-s1 + T11-s1 : Results in Table   V , Major Column 4.    \\rightarrow   T11 recall: +0.021, overall recall:   0.000 similar-to absent 0.000 \\sim 0.000  0.000 .    \\rightarrow   Comparing with M1b (T13-s1) overall recall:   0.009 0.009 -0.009 - 0.009 .",
            "T13-s1 + T11-s2 : Results in Table   V , Major Column 5.    \\rightarrow   T11 recall: +0.021, overall recall:  + 0.004 0.004 +0.004 + 0.004 .    \\rightarrow   Comparing with M1b (T13-s1) overall recall:   0.005 0.005 -0.005 - 0.005 .",
            "T13-s1 + T12-s1 : Results in Table   V , Major Column 6.    \\rightarrow   T12 recall: +0.031, overall recall:  + 0.004 0.004 +0.004 + 0.004 .    \\rightarrow   Comparing with M1b (T13-s1) overall recall:   0.005 0.005 -0.005 - 0.005 .",
            "T13-s1 + T12-s2 : Results in Table   V , Major Column 7.    \\rightarrow   T12 recall:   0.000 similar-to absent 0.000 \\sim 0.000  0.000 , overall recall:   0.000 similar-to absent 0.000 \\sim 0.000  0.000 .    \\rightarrow   Comparing with M1b (T13-s1) overall recall:   0.009 0.009 -0.009 - 0.009 .",
            "In this appendix, we provide high-resolution versions of the images in Figs.  4 ,  5 , and  6 .",
            "Original Fig.  5  Caption      The class T13 has the lowest recall among all classes. The scatter plot in (a) indicates more classification errors (red dots) when the data objects are associated with lower values in PCA feature dimension 0. The RBF heatmap in (b) confirms this pattern and enables data synthesis to be targeted at an erroneous cluster on the left as shown in (c). The model retrained with additional LLM-synthesized data is improved in (d). The RBF heatmap for the new testing results in (e) and the zoomed-in scatter plots confirm the improvement."
        ]
    },
    "id_table_6": {
        "caption": "TABLE VI:  Results of the second iteration: retraining with different class-based synthetic data in addition to  T11-s1  and  T12-s1  synthetic data. The    \\Delta roman_ -recall results were compared with the original model M0 (Major Column 2).",
        "table": "A1.T3.82",
        "footnotes": [],
        "references": [
            "As there are usually a few dozen of the PCA dimensions to be considered, the ML developer is expected to spend a fair amount of time performing interactive visualization. We, therefore, designed this view as a  2  3 2 3 2\\times 3 2  3  matrix as shown in Fig.  6 (a). The interaction panel above the  2  3 2 3 2\\times 3 2  3  matrix is used to control what data to be visualized, such as the whole dataset or a class; which class; how a class is subdivided, previous testing results, training data, or synthesized data; and so on.",
            "An ML developer can select one or more synthetic datasets from the cache area, and place them on the small tiles on the right of the screen as shown in Fig.  6 (b). These selected datasets are combined with the main dataset (usually the original training data) and are visualized in the large canvas in the middle. For example, in Fig.  6 (b), the smaller tag-treemap on the top-right shows the keyword statistics of the original training data in a class (i.e., the main dataset). The two smaller tag-treemaps below show the keyword statistics of two synthetic datasets. The larger tag-treemap in the middle shows the keyword statistics when three datasets are combined. In this way, the ML developer can assess the impact of the synthetic data, e.g., to observe whether the level of skewness increases or decreases.",
            "Usually, training and testing may take some time, and the Model View is designed to focus on unhurried monitoring rather than intensive analysis. As shown in Fig.  6 (c), the VIS techniques used include a line chart for progress monitoring, tree visualization for decision trees or random forest models, and log data display. The Model View supports the monitoring of intermediate testing results, and there are numerical and visual displays showing performance metrics, e.g., accuracy and confusion matrix. It was intentional for not providing more complicated VIS techniques to examine the testing results in detail. As the main tasks in Model View are machine-centric, while the tasks of results analysis and evaluation are human-centric, we purposely designed the Results View for the human-centric tasks.",
            "Results View for Results Analysis and Evaluation.  The primary task at this stage is to analyze the testing results from the Model View in order to make some high-level decisions about the next iteration, e.g., go to the Synthesis View to generate another synthetic dataset or go to the Data View to configure a different integration. A major part of the analysis is to compare the latest results with those of the previous models. This is one important reason why the Model View cannot support results analysis easily. As shown in Fig.  6 (d) it has a similar  2  3 2 3 2\\times 3 2  3  matrix layout as the Synthesis View. The main difference is that the interaction panel above the  2  3 2 3 2\\times 3 2  3  matrix is for selecting results data, which is combined with the data objects in the testing data. For example, each data object in the testing data is automatically annotated with correct and incorrect labels, and the testing dataset can be visualized using a PCA scatter plot or an RBF heatmap. There are also commonly used plots and various statistics about the results.",
            "T11-s1 + T12-s1 + T13-s1 : Results in Table   VI , Major Column 3.    \\rightarrow   T13 recall:  + 0.200 0.200 +0.200 + 0.200 , overall recall:   0.000 similar-to absent 0.000 \\sim 0.000  0.000     \\rightarrow   Comparing with M2 ( T11-s1 + T12-s1 ) overall recall:   0.004 0.004 -0.004 - 0.004 .",
            "T11-s1 + T12-s1 + T13-s2 : Results in Table   VI , Major Column 4.    \\rightarrow   T13 recall:  + 0.155 0.155 +0.155 + 0.155 , overall recall:   0.007 0.007 -0.007 - 0.007     \\rightarrow   Comparing with M2 ( T11-s1 + T12-s1 ) overall recall:   0.011 0.011 -0.011 - 0.011 .",
            "T11-s1 + T12-s1 + T14-s1 : Results in Table   VI , Major Column 5.    \\rightarrow   T14 recall:  + 0.030 0.030 +0.030 + 0.030 , overall recall:   0.004 0.004 -0.004 - 0.004     \\rightarrow   Comparing with M2 ( T11-s1 + T12-s1 ) overall recall:   0.008 0.008 -0.008 - 0.008 .",
            "T11-s1 + T12-s1 + T14-s2 : Results in Table   VI , Major Column 6.    \\rightarrow   T14 recall:  + 0.009 0.009 +0.009 + 0.009 , overall recall:   0.008 0.008 -0.008 - 0.008     \\rightarrow   Comparing with M2 ( T11-s1 + T12-s1 ) overall recall:   0.012 0.012 -0.012 - 0.012 .",
            "In this appendix, we provide high-resolution versions of the images in Figs.  4 ,  5 , and  6 .",
            "Original Fig.  6  Caption      Four views of the iGAiVA tool. The user can switch between views using the top menu bar. (a) The Synthesis View is for supporting mainly the tasks for identifying suitable example data objects as inputs to LLMs for generating synthetic data. (b) The Data View is for selecting a subset of synthetic datasets and combining them with the original training data. (c) The Model View is for monitoring the process of retraining a model and running the retrained model against one or more predefined testing datasets."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "A1.T4.85",
        "footnotes": [],
        "references": [
            "The three main requirements for this work were identified in an agile manner, which is consistent with the nested model approach  [ 70 ] . During the 2-month placement, we formulated a VIS4ML workflow as shown in Fig.  1 (b). We will detail these VIS techniques in Section  3 , and VIS-guided data synthesis in Section  4 . Following the placement, we designed and prototyped a VIS4ML tool, called iGAiVA ( integrated Generative AI and Visual Analytics ), for enabling VIS-guided data synthesis in iterative ML workflows as illustrated in Fig.  1 (c). We will detail the design of iGAiVA and its prototype in Section  5 . There were two further placements, where the system was evaluated and improved in an agile manner. We will report this process in Section  7 ."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "A1.T5.80",
        "footnotes": [],
        "references": []
    },
    "id_table_9": {
        "caption": "",
        "table": "A1.T6.65",
        "footnotes": [],
        "references": []
    }
}