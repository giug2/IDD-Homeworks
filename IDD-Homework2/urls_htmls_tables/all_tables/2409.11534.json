{
    "id_table_1": {
        "caption": "TABLE I:  Dataset description - RSNA Screening Mammography Breast Cancer Detection AI Challenge dataset for training and internal testing; Mayo Clinic Private dataset for External testing",
        "table": "S4.T1.1.1",
        "footnotes": [],
        "references": [
            "OOD  data, also called  anomaly ,  outlier , usually refers to data that shows dissimilarity from the training or in distribution (ID). Given an image  x x x italic_x , the goal of  OOD detection  is to identify whether  x x x italic_x  is from ID dataset  D i  n subscript D i n D_{in} italic_D start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT  or OOD dataset  D o  u  t subscript D o u t D_{out} italic_D start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT . OOD data can be categorized as two broad classes (Fig.  1 ) -  (i) intra-class:  OOD data belonging this type, which is also called  novelty data , often shares severe similarity with the ID classes and is extremely challenging to distinguish,  e.g. , the mammogram with biopsy clip presents close appearance with the normal images;  (ii) inter-class:  this data is significantly different from ID samples,  e.g.,  a natural image from ImageNet is much different in shape and color from the mammogram images. In theory, there exists an infinite number of potential categories for intra- and inter-class OOD scenarios within each domain. This complexity makes training a supervised model for OOD identification impractical, especially when considering unseen external data.",
            "Network training:  Given the open ended anomaly detection problem, the hybrid anomaly detection model is trained in an self-supervised learning (SSL) manner since no real ODD data is being used in the training. All the input images are ID data, and around half of input have random transformation to get synthetic OOD data. For each batch, we generated an even mixed of ID and OOD data during the network training. Two backpropagation steps are used in each batch to update the network. The first backpropagation uses loss formulated in Eqn.  1 . Eqn.  1  is composed ID reconstruction loss shown in Eqn  2  to learn reconstruction of ID images, and BCE loss from Eqn.  3  to train discriminator to distinguish ID or OOD images. The two parts of losses are been weighted by   1 subscript  1 \\alpha_{1} italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and   2 subscript  2 \\alpha_{2} italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  to balanced the the 1st backpropagation loss  L 1 . The second backpropagation uses gradient reversal with only OOD reconstruction loss shown in Eqn.  4 . The gradient reversal is done by multiplying gradient of  L 2  with (    -\\lambda - italic_ ), issuing       L 2     x   subscript L 2   x -\\lambda\\frac{\\delta L_{2}}{\\delta\\theta x} - italic_ divide start_ARG italic_ italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG italic_ italic_ italic_x end_ARG  where  x x x italic_x  is the input image."
        ]
    },
    "id_table_2": {
        "caption": "TABLE II:  Comparative overall and sub-types performance for anomaly detection on both internal and external datasets",
        "table": "S4.T2.1.1",
        "footnotes": [],
        "references": [
            "Figure  2  shows design of proposed hybrid  HAND 3 3 3 GitHub implantation can be found:  https://github.com/zheminzhang96/HAND_mammo.git    architecture for unsupervised anomaly detection which consists of a CNN-transformer backbone to learn reconstructions of input, and a parallel discriminator branch out from latent space to learn differentiation between the ID and OOD samples. HAND uses self-supervised learning (SSL) to train the hybrid architecture and the discriminator branch. Gradient reversal is applied to synthetic OOD reconstruction for model unlearning. During inference, a weighted combination of reconstruction loss and probability from discriminator branch yields anomaly score for the input.",
            "Given the unsupervised nature of the framework, synthetic OOD samples are generated as a part of SSL to enhance the models ability to learn ID data. Figure  2  illustrates six augmentations from an input that we adopted to replicate unseen OOD appearances for the mammogram images based on limited prior knowledge. We selected color jittering, invert function and median filter to simulate the artifacts, motion and image normalization errors. Random masked images using round and natural image masks are used to replicate the possible appearances of foreign body, e.g. implant, biopsy clip. Our hypothesis is that although synthetic out-of-distribution (OOD) samples do not provide supervised training data, their close resemblance to input images (ID) will enable the model to learn subtle differences that indicate potential OODs from IDs. During model training, around half of the ID input images are selected for synthetic ODD generation by a random binary seed value. The type of augmentation applied to original input is also decided randomly from six different transformation functions.",
            "Discriminator  plays a crucial role in anomaly detection, since the reconstruction of input image alone is not sufficient for anomaly score  [ 42 ,  43 ] . This is due to the fact that the reconstruction loss for low resolution, area masked, pure black, or blurred images are intrinsically low since the model is trained to learn to reconstruct ID images, which has more complex tissue structures. The OOD data mentioned before are easier for the model to reconstruct, resulting lower reconstruction errors, which indicates lower anomaly score for these anomalous samples. The discriminator shown in Figure  2  uses MLP framework which branches out from the latent space after transformer layers. One output from MLP with sigmoid activation function to get OOD probability. Higher probability score indicates higher chance the input image is anomaly. Discriminator loss calculated by BCE loss between actual label and predicted probability is used to update CNN encoder and transformer.",
            "Network training:  Given the open ended anomaly detection problem, the hybrid anomaly detection model is trained in an self-supervised learning (SSL) manner since no real ODD data is being used in the training. All the input images are ID data, and around half of input have random transformation to get synthetic OOD data. For each batch, we generated an even mixed of ID and OOD data during the network training. Two backpropagation steps are used in each batch to update the network. The first backpropagation uses loss formulated in Eqn.  1 . Eqn.  1  is composed ID reconstruction loss shown in Eqn  2  to learn reconstruction of ID images, and BCE loss from Eqn.  3  to train discriminator to distinguish ID or OOD images. The two parts of losses are been weighted by   1 subscript  1 \\alpha_{1} italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and   2 subscript  2 \\alpha_{2} italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  to balanced the the 1st backpropagation loss  L 1 . The second backpropagation uses gradient reversal with only OOD reconstruction loss shown in Eqn.  4 . The gradient reversal is done by multiplying gradient of  L 2  with (    -\\lambda - italic_ ), issuing       L 2     x   subscript L 2   x -\\lambda\\frac{\\delta L_{2}}{\\delta\\theta x} - italic_ divide start_ARG italic_ italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG italic_ italic_ italic_x end_ARG  where  x x x italic_x  is the input image."
        ]
    },
    "id_table_3": {
        "caption": "TABLE III:  Internal reconstruction example across different models",
        "table": "S4.T3.35.35",
        "footnotes": [],
        "references": [
            "Network training:  Given the open ended anomaly detection problem, the hybrid anomaly detection model is trained in an self-supervised learning (SSL) manner since no real ODD data is being used in the training. All the input images are ID data, and around half of input have random transformation to get synthetic OOD data. For each batch, we generated an even mixed of ID and OOD data during the network training. Two backpropagation steps are used in each batch to update the network. The first backpropagation uses loss formulated in Eqn.  1 . Eqn.  1  is composed ID reconstruction loss shown in Eqn  2  to learn reconstruction of ID images, and BCE loss from Eqn.  3  to train discriminator to distinguish ID or OOD images. The two parts of losses are been weighted by   1 subscript  1 \\alpha_{1} italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and   2 subscript  2 \\alpha_{2} italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  to balanced the the 1st backpropagation loss  L 1 . The second backpropagation uses gradient reversal with only OOD reconstruction loss shown in Eqn.  4 . The gradient reversal is done by multiplying gradient of  L 2  with (    -\\lambda - italic_ ), issuing       L 2     x   subscript L 2   x -\\lambda\\frac{\\delta L_{2}}{\\delta\\theta x} - italic_ divide start_ARG italic_ italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG italic_ italic_ italic_x end_ARG  where  x x x italic_x  is the input image.",
            "Reconstruction Performance  We measure the SSIM (Structure Similarity Index Measure) score to evaluate the quality of reconstructed image from the original input image. The higher SSIM score ( > 0.8 absent 0.8 >0.8 > 0.8 ) indicates the reconstructed image is more similar with the original input image. Fig. 3 . presents the SSIM score of test dataset across different models. For each model, the test dataset is divided into three categories for SSIM evaluation - ID is the in-distribution mammograms, OOD includes out-of-distribution mammograms cases, such as implant, implant rapture, poor quality, and noises added to mammograms. Natural category include the images from ImageNet which are not mammograms.",
            "For anomaly detection using generative models, a high SSIM score for the ODD mammograms and natural images suggests that the model cannot differentiate between ID and ODD when reconstructing the images. A good anomaly detection model should have a much higher SSIM score for the ID category than the other two categories, which shows a clear separation between ID data and others. Following that notion, VQVAE perform worsen ( > 0.7 absent 0.7 >0.7 > 0.7  average SSIM score for ODD) than f-AnoGAN ( > 0.55 absent 0.55 >0.55 > 0.55  average SSIM score for ODD) and hybrid models ( < 0.01 absent 0.01 <0.01 < 0.01  average SSIM score for ODD) as shown in Fig. 3 . f-AnoGAN and hybrid model follow the pattern of obtaining ID SSIM score much higher than OOD and natural categories. Having a more distinctive SSIM score between ID and ODD leads to higher reconstruction loss for OOD and natural samples. Although, as described in Section  III , the models reconstruction ability alone is not sufficient to use as anomaly score since SSIM score for ID outliers significantly overlaps with ODDs - even for the hybrid model.",
            "Anomaly Detection Performance  Figure  3 .b. and Table  II  shows accuracy and AUC for anomaly detection of comparative baselines and the proposed hybrid model with discriminator branch using gradient reversal. Even though encoder-based models obtained high average accuracy (CVAE - 0.81, VQ-VAE - 0.86 AUROC) for the internal data, there is a significant drop on the external data (CVAE - 0.4, VQ-VAE - 0.46 AUROC). While CVAD - encoder with discriminator branch obtained 0.83 AUROC on internal and 0.73 AUROC on the external data, f-AnoGAN obtained 0.93 and 0.67 AUROC on internal and external data respectively. Proposed hybrid model  HAND outperformed all the baselines and obtained 0.93 and 0.85 AUROC on internal and external data. Although there is a minor drop in the performance on the external data, HAND is able to achieve higher performance for all the anomaly sub-types of the external data - implant (0.9), biopsy clip (0.89) and poor quality (0.77). All the baselines along with the proposed model struggle to differentiate the poor quality images from external ID data which could be due to the variation between internal and external data, and heterogeneous and unknown nature of the imaging artifacts.",
            "The proposed model outperformed encoder-based and GAN-based baselines which have been trained on the same ID data. Interestingly, it also outperformed the hybrid baselines and hybrid model with discriminator branch with significant margins, particularly for the external data which highlights the fact that addition of discriminator loss and penalizing the model with negative gradient help to learn fine-grain normality features. Such training supports both intra- and inter-class anomaly detection for mammogram, even for small anomalies, such as presence of biopsy clip. With addition of each proposed component, we observed a constant improvement for ID reconstruction and worsening for OOD reconstruction quality (Fig   3 )."
        ]
    },
    "id_table_4": {
        "caption": "TABLE IV:  External reconstruction samples across different models",
        "table": "S4.T4.25.25",
        "footnotes": [],
        "references": [
            "Network training:  Given the open ended anomaly detection problem, the hybrid anomaly detection model is trained in an self-supervised learning (SSL) manner since no real ODD data is being used in the training. All the input images are ID data, and around half of input have random transformation to get synthetic OOD data. For each batch, we generated an even mixed of ID and OOD data during the network training. Two backpropagation steps are used in each batch to update the network. The first backpropagation uses loss formulated in Eqn.  1 . Eqn.  1  is composed ID reconstruction loss shown in Eqn  2  to learn reconstruction of ID images, and BCE loss from Eqn.  3  to train discriminator to distinguish ID or OOD images. The two parts of losses are been weighted by   1 subscript  1 \\alpha_{1} italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and   2 subscript  2 \\alpha_{2} italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  to balanced the the 1st backpropagation loss  L 1 . The second backpropagation uses gradient reversal with only OOD reconstruction loss shown in Eqn.  4 . The gradient reversal is done by multiplying gradient of  L 2  with (    -\\lambda - italic_ ), issuing       L 2     x   subscript L 2   x -\\lambda\\frac{\\delta L_{2}}{\\delta\\theta x} - italic_ divide start_ARG italic_ italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG italic_ italic_ italic_x end_ARG  where  x x x italic_x  is the input image.",
            "Mammogram images used in this study have been processed in the following steps. First, from original DICOM files, an open source ImageExtractor - Niffler 5 5 5 https://github.com/Emory-HITI/Niffler , is applied to do proper windowing and also extract comprehensive DICOM metadata as a single CSV file. The metadata is merged with clinical labels (presence of implant, biopsy, cancer etc). We have drop duplicate images of same laterality (L/R) and view (CC/MLO) under same study ID according to latest acquisition time which in results 47,688 records, with MLO and CC view each contains 23,826 records. 684 implant records, distributed equally between MLO and CC. Inversion of a image been performed if MONOCHROME dicom metadata tag is 1 (white background with black tissue). After the intensity based pre-processing, we cropped the breast tissue from background using pre-defined thresholds and we have decided to stitch right and left MLO mammogram images together to represent the study. Fig.  4 .c. shows an example of stitched right (Fig.  4 .a.) and left (Fig.  4 .b.) MLO views under the same study ID. If there is one side (right or left) missing for the same study ID and same view, we have dropped these cases. Resulting in 22,778 records of stitched mammograms, with MLO and CC views distributed equally."
        ]
    },
    "id_table_5": {
        "caption": "TABLE V:  Evaluation of anomaly detection performance for ablation study",
        "table": "S4.T5.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_6": {
        "caption": "TABLE VI:  Ablation: internal reconstruction samples across different hybrid models",
        "table": "S4.T6.28.28",
        "footnotes": [],
        "references": []
    },
    "id_table_7": {
        "caption": "TABLE VII:  Ablation: external reconstruction samples across different hybrid models",
        "table": "S4.T7.20.20",
        "footnotes": [],
        "references": []
    },
    "id_table_8": {
        "caption": "TABLE VIII:  Network details of HAND with layers and output dimension.",
        "table": "A1.T8.41.41",
        "footnotes": [],
        "references": [
            "Complete network details is provided in Table  VIII ."
        ]
    }
}