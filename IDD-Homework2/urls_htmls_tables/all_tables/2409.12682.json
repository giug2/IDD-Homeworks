{
    "id_table_1": {
        "caption": "Table 1 .  Distribution of collected data.",
        "table": "S3.T1.4.1",
        "footnotes": [],
        "references": [
            "This section details the methodology of our study, i.e., the research questions, the data collection, the retrieval augmented test generation framework, baseline LLMs, and the evaluation metrics.  Figure  1  depicts the overall flow of our study.",
            "After mapping all documents to an API, we rank them by the number of documents. To ensure a balanced score, the ranking scores are calculated using the harmonic mean of the number of issue documents and Q&A documents. The distribution of the final number of APIs after filtering and the number of documents is presented in Table  1 . To ensure that APIs have sufficient documentation and make the experiment cost-feasible, we select the top 10% APIs from the ranking as the target APIs, involving a total of 188 APIs."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 .  RQ1: Result of LLMs for each project using zero-shot prompting. The first row denotes the five projects. The second row denotes the LLMs. PR% denotes parse rate, EX% denotes execution rate, PS% denotes pass rate, and CV denotes code coverage.",
        "table": "S4.T2.4.1",
        "footnotes": [],
        "references": [
            "We organized the rest of this paper as follows.  Section  2  introduces the background of our study.  Section  3  details how we collected the dataset and implemented the RAG-based unit test generation.  Section  4  shows the results of our experiments.  Section  5  discusses the possible threats in the study.  Section  6  concludes this paper.",
            "We task an LLM with generating unit test cases for each API to ensure maximum coverage of its functionality. Additionally, we require generating new tests only if they cover new lines, allowing the model to determine the number of test cases needed autonomously. The generated test suite must utilize the  unittest  library and include all necessary imports and main functions to ensure the code is directly executable. Figure  2  illustrates the prompt used for zero-shot prompting.",
            "Result.   Table  2  presents the results of LLMs in generating unit test cases across five projects. Most state-of-the-art (SOTA) LLMs are capable of generating syntactically correct unit test cases, with parse rates ranging from 63% to 100%. Additionally, most LLMs were able to generate a substantial number of executable unit test cases, with some exceptions."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 .  RQ2: Result of LLMs for using basic RAG prompting. FM denotes the LLMs. BL denotes the baselines: zero-shot (ZS), combined (CB), GitHub issues (GH), and StackOverflow Q&As (SO). In the second row, PR% denotes parse rate, EX% denotes execution rate, PS% denotes pass rate and CV denotes coverage.",
        "table": "S4.T3.4.1",
        "footnotes": [],
        "references": [
            "We organized the rest of this paper as follows.  Section  2  introduces the background of our study.  Section  3  details how we collected the dataset and implemented the RAG-based unit test generation.  Section  4  shows the results of our experiments.  Section  5  discusses the possible threats in the study.  Section  6  concludes this paper.",
            "We apply a simple string-matching heuristic to filter out documents irrelevant to API usage. Specifically, we only include GitHub issues and StackOverflow Q&A documents that mention any of the APIs for the RAG document. We link the mentioned APIs to each document and use this mapping information for the API-level RAG approach. More details on the API-level RAG are discussed in Section  3.3 . Any APIs not mentioned in either source (issues and Q&As) are excluded from data collection. This results in a many-to-many relationship between issues and Q&A documents with APIs, while API documents maintain a one-to-one relationship.",
            "When augmenting the prompt with documents retrieved from the RAG database, we instruct the model to use the provided documents to make the test case more compilable, and passable and cover more lines. The additional prompt we use for augmented generation is listed in Fig.  3 . For Basic RAG prompting, we set the number of retrieved documents to three, as previous research  (Chen et al . ,  2024a )  indicated that metric improvements plateaued beyond this number. Specifically, we retrieve the top three API documents for Basic API-documentation RAG, the top three issue documents for Basic GitHub-issue RAG, the top three Q&A documents for basic StackOverflow-Q&A RAG, and three documents for the Basic combined RAG.",
            "For API-level RAG prompting, we set different numbers of documents for retrieval: 1) one for the API document, as there is only one document per API, 2) three for GitHub issues and StackOverflow Q&A, similar to Basic RAG prompting, and 3) one document each (total of three) for the combined API-level RAG approach. We add the query listed in  3  for each augmented document. We use the top response generated by the LLMs, with the temperature set to zero to ensure a more deterministic and reliable response  (Ouyang et al . ,  2023 ) .",
            "Result.   Table  3  presents the results of LLMs in generating unit test cases using the Basic RAG approach. For comparison, we also report the results of zero-shot prompting. Overall, we observe that employing Basic RAG enhances code coverage compared to zero-shot prompting. However, the augmentation of documents appears to negatively impact the parse rate for certain models, such as GPT-3.5-turbo and Mistral. This is attributed to the fact that these relatively smaller models are disrupted by the lengthy augmented documents, causing them to deviate from the initial prompt to generate directly runnable test suites and produce unparsable code."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 .  RQ3: Result of LLMs library using API RAG prompting.",
        "table": "S4.T4.4.1",
        "footnotes": [],
        "references": [
            "We organized the rest of this paper as follows.  Section  2  introduces the background of our study.  Section  3  details how we collected the dataset and implemented the RAG-based unit test generation.  Section  4  shows the results of our experiments.  Section  5  discusses the possible threats in the study.  Section  6  concludes this paper.",
            "We report the win count for code coverage for each approach, as depicted in Fig.  4 -a and c. In a total of 20 cases (5 projects   \\times   4 LLMs), the combined Basic RAG outperformed zero-shot prompting 14 times, the API document Basic RAG 13 times, the GitHub issues Basic RAG 15 times, and the StackOverflow Q&As Basic RAG 15 times. Contrary to our assumption that combined RAGs would outperform individual RAGs due to the inclusion of all documents, the results did not consistently support this hypothesis. When comparing the combined Basic RAG to the individual RAGs, the combined Basic RAG outperformed the API documents 14 times, the GitHub issues 9 times, and the StackOverflow Q&As 12 times. Fig.  4 -c  illustrates the win counts within all the Basic RAG approaches.  From the Friedman ranking test, we obtained the following average rankings: (1) Basic RAG combined: 2.55, (2) Basic RAG GitHub issues: 2.55, (3) Basic RAG StackOverflow Q&As: 2.8, (4) Basic RAG API documents: 3.3, and (5) zero-shot: 3.85. The p-value from the Friedman test was 0.0348, indicating the significance of the rankings.",
            "Result.   Table  4  presents the results of LLMs in generating unit test cases using the API-level RAG approach. For comparative purposes, zero-shot prompting results are also included. Consistent with the findings in RQ2, it is evident that the API-level approach enhances performance in terms of code coverage compared to zero-shot prompting. A similar decline in parse rate was observed for GPT-3.5-turbo and Mistral, attributable to the same underlying factors.",
            "Figures  4 -b and  4 -d illustrate the win counts of API-level RAG versus zero-shot prompting and within each API-level RAG. Out of 20 cases, the combined API-level RAG outperformed zero-shot prompting in 16 instances, the API document API-level RAG in 14 instances, the GitHub issues API-level RAG in 18 instances, and the StackOverflow Q&As API-level RAG in 15 instances. The win count for API-level RAG versus zero-shot was higher compared to Basic RAG versus zero-shot, with two additional wins for the combined approach, one additional win for the API document approach, three additional wins for the GitHub issues approach, and an equal number of wins for the StackOverflow Q&As approach."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 .  RQ4: Result of GPT-4o for tf and torch with limited test cases. The number in the first column denotes the limited number of test cases. ZS denotes zero-shot, CB denotes basic RAG combined, AD denotes basic RAG on API documentation, GH denotes basic RAG on GitHub issues, and SO denotes basic RAG on StackOverflow Q&As. The column denotes baselines, parse rate, execution rate, pass rate, coverage, input, and output token cost respectively.",
        "table": "S4.T5.4.1",
        "footnotes": [],
        "references": [
            "We organized the rest of this paper as follows.  Section  2  introduces the background of our study.  Section  3  details how we collected the dataset and implemented the RAG-based unit test generation.  Section  4  shows the results of our experiments.  Section  5  discusses the possible threats in the study.  Section  6  concludes this paper.",
            "Result.   Table  5  presents the results of RQ4, examining the performance of GPT-4o on the TensorFlow and PyTorch libraries using zero-shot prompting and Basic RAG. The results indicate the ranking of input costs (in descending order) as follows: 1) Basic RAG with GitHub issues, 2) Basic RAG combined, 3) Basic RAG with API documentation, 4) Basic RAG with StackOverflow Q&As, and 5) zero-shot prompting. This ranking is directly influenced by the average length of each document, which may vary across different projects. Nonetheless, TensorFlow and PyTorch exhibited similar trends.",
            "However, we identified instances where documents from different sources helped generate unit test cases that covered unique lines. For example, a GitHub issue provided an example case that covered unique lines. In Listing  5 , a GitHub issue queried how to initialize multiple  CheckpointManager  instances. The issue was addressed with a comment containing a code example, which helped the LLM generate a similar test, covering unique lines in the source. This coverage was achieved because  manager2  was initialized on a  checkpoint  with a saved state from  manager1 ."
        ]
    },
    "global_footnotes": []
}