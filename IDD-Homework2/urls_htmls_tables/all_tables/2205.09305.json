{
    "PAPER'S NUMBER OF TABLES": 9,
    "S5.T1": {
        "caption": "Table 1: Experiment setup of Rotated-CIFAR10 involves three clients with distinct rotation degrees. Each client also contains three sub-environments to simulate intra-silo feature heterogeneity.",
        "table": "<table id=\"S5.T1.4\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.4.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.1.1.1\" class=\"ltx_td ltx_border_tt\"></td>\n<th id=\"S5.T1.4.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Sub Env 1</th>\n<th id=\"S5.T1.4.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Sub Env 2</th>\n<th id=\"S5.T1.4.1.1.4\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Sub Env 3</th>\n</tr>\n<tr id=\"S5.T1.4.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Client 1</td>\n<td id=\"S5.T1.4.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_t\">10°</td>\n<td id=\"S5.T1.4.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_t\">25°</td>\n<td id=\"S5.T1.4.2.2.4\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\">40°</td>\n</tr>\n<tr id=\"S5.T1.4.3.3\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.3.3.1\" class=\"ltx_td ltx_align_left\">Client 2</td>\n<td id=\"S5.T1.4.3.3.2\" class=\"ltx_td ltx_align_left\">60°</td>\n<td id=\"S5.T1.4.3.3.3\" class=\"ltx_td ltx_align_left\">75°</td>\n<td id=\"S5.T1.4.3.3.4\" class=\"ltx_td ltx_nopad_r ltx_align_left\">90°</td>\n</tr>\n<tr id=\"S5.T1.4.4.4\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.4.4.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">Client 3</td>\n<td id=\"S5.T1.4.4.4.2\" class=\"ltx_td ltx_align_left ltx_border_bb\">-10°</td>\n<td id=\"S5.T1.4.4.4.3\" class=\"ltx_td ltx_align_left ltx_border_bb\">-40°</td>\n<td id=\"S5.T1.4.4.4.4\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_bb\">-90°</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Based on the conventional CIFAR10 dataset (45), we also develop our own synthetic dataset - Rotated-CIFAR10 as our benchmark dataset. We rotate CIFAR10 images with different angles to simulate the non-i.i.d. data in the federated networks. In our federated learning settings, there are three clients with distinct data distributions. Further, we also establish three sub-environments in each client to simulate intra-silo feature heterogeneity. The experiment setup is demonstrated in Table 1. Moreover, the OOD test set is configured with a rotation degree from -90° to 90° to evaluate the performance of the global model. Our goal is to correctly classify the 32x32 RGB colored images with 10 classes. Therefore, we adopt a 3-layer CNN architecture as our classification model. We also use the Adam optimizer with a fixed learning rate of 0.0001 and a weight decay of 0.001. Particularly, we select λ=1.0𝜆1.0\\lambda=1.0 as the Fishr hyperparameter."
        ]
    },
    "S6.T2": {
        "caption": "Table 2: The OOD test loss comparison among different algorithms on the Color-MNIST, Rotated-CIFAR10, and eICU dataset. ",
        "table": "<table id=\"S6.T2.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T2.4.1.1\" class=\"ltx_tr\">\n<td id=\"S6.T2.4.1.1.1\" class=\"ltx_td ltx_border_tt\"></td>\n<th id=\"S6.T2.4.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Color-MNIST</th>\n<th id=\"S6.T2.4.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Rotated-CIFAR10</th>\n<th id=\"S6.T2.4.1.1.4\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt\">eICU</th>\n</tr>\n<tr id=\"S6.T2.4.2.2\" class=\"ltx_tr\">\n<td id=\"S6.T2.4.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_tt\">FedSGD</td>\n<td id=\"S6.T2.4.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_tt\">0.566±0.009</td>\n<td id=\"S6.T2.4.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_tt\">1.681±0.010</td>\n<td id=\"S6.T2.4.2.2.4\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_tt\">0.302±0.017</td>\n</tr>\n<tr id=\"S6.T2.4.3.3\" class=\"ltx_tr\">\n<td id=\"S6.T2.4.3.3.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Geometric</td>\n<td id=\"S6.T2.4.3.3.2\" class=\"ltx_td ltx_align_left ltx_border_t\">0.562±0.006</td>\n<td id=\"S6.T2.4.3.3.3\" class=\"ltx_td ltx_align_left ltx_border_t\">1.681±0.006</td>\n<td id=\"S6.T2.4.3.3.4\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\"><span id=\"S6.T2.4.3.3.4.1\" class=\"ltx_text ltx_font_bold\">0.289±0.009</span></td>\n</tr>\n<tr id=\"S6.T2.4.4.4\" class=\"ltx_tr\">\n<td id=\"S6.T2.4.4.4.1\" class=\"ltx_td ltx_align_left ltx_border_t\">FedCurv</td>\n<td id=\"S6.T2.4.4.4.2\" class=\"ltx_td ltx_align_left ltx_border_t\">0.559±0.003</td>\n<td id=\"S6.T2.4.4.4.3\" class=\"ltx_td ltx_align_left ltx_border_t\">1.666±0.024</td>\n<td id=\"S6.T2.4.4.4.4\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\">0.298±0.009</td>\n</tr>\n<tr id=\"S6.T2.4.5.5\" class=\"ltx_tr\">\n<td id=\"S6.T2.4.5.5.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S6.T2.4.5.5.1.1\" class=\"ltx_text ltx_font_bold\">Fishr+Inter-Geo</span></td>\n<td id=\"S6.T2.4.5.5.2\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S6.T2.4.5.5.2.1\" class=\"ltx_text ltx_font_bold\">0.542±0.006</span></td>\n<td id=\"S6.T2.4.5.5.3\" class=\"ltx_td ltx_align_left ltx_border_t\">1.658±0.004</td>\n<td id=\"S6.T2.4.5.5.4\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\">0.293±0.009</td>\n</tr>\n<tr id=\"S6.T2.4.6.6\" class=\"ltx_tr\">\n<td id=\"S6.T2.4.6.6.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Fishr+Intra-Arith</td>\n<td id=\"S6.T2.4.6.6.2\" class=\"ltx_td ltx_align_left ltx_border_t\">0.555±0.004</td>\n<td id=\"S6.T2.4.6.6.3\" class=\"ltx_td ltx_align_left ltx_border_t\">1.655±0.007</td>\n<td id=\"S6.T2.4.6.6.4\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\">0.302±0.009</td>\n</tr>\n<tr id=\"S6.T2.4.7.7\" class=\"ltx_tr\">\n<td id=\"S6.T2.4.7.7.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span id=\"S6.T2.4.7.7.1.1\" class=\"ltx_text ltx_font_bold\">Fishr+Intra-Geo</span></td>\n<td id=\"S6.T2.4.7.7.2\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">0.552±0.005</td>\n<td id=\"S6.T2.4.7.7.3\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span id=\"S6.T2.4.7.7.3.1\" class=\"ltx_text ltx_font_bold\">1.648±0.006</span></td>\n<td id=\"S6.T2.4.7.7.4\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_t\"><span id=\"S6.T2.4.7.7.4.1\" class=\"ltx_text ltx_font_bold\">0.289±0.008</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We run each algorithm for five different seeds, then average their loss, the AUCROC, AUCPR, as well as the accuracy on the OOD test set when the OOD test loss is lowest, which are demonstrated in Table 2, Table 4 Table 6, and Figure 3. Further, we also conduct the inter-silo and intra-silo accuracy comparison among each sub environment on the Rotated-CIFAR10 dataset when the OOD test loss is lowest 5. Specifically, we also experiment with the model’s performance regarding different Fishr regularizer λ𝜆\\lambda on the eICU dataset 2. Moreover, we also plot the training loss, validation loss, and OOD test loss among different algorithms, which are demonstrated in Figure 5, Figure 6, and Figure 7 in the Appendix section. From the figures, our proposed methods, Fishr+Inter-Geo and Fishr+Intra-Geo, significantly outperform other methods in domain adaptation, including the baseline algorithm FedSGD and the state-of-the-art algorithm FedCurv."
        ]
    },
    "S6.T3": {
        "caption": "Table 3: The variance of the accuracy distribution and KL-divergence between the normalized accuracy vector and the uniform distribution (which can be directly translated to the entropy of accuracy) on the Rotated-CIFAR10 dataset when the OOD test loss is lowest.",
        "table": "<table id=\"S6.T3.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T3.4.1.1\" class=\"ltx_tr\">\n<td id=\"S6.T3.4.1.1.1\" class=\"ltx_td ltx_border_tt\"></td>\n<th id=\"S6.T3.4.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Variance (x1000)</th>\n<th id=\"S6.T3.4.1.1.3\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Entropy (x10)</th>\n</tr>\n<tr id=\"S6.T3.4.2.2\" class=\"ltx_tr\">\n<td id=\"S6.T3.4.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_tt\">FedSGD</td>\n<td id=\"S6.T3.4.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_tt\">0.809±0.251</td>\n<td id=\"S6.T3.4.2.2.3\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_tt\">21.948±0.007</td>\n</tr>\n<tr id=\"S6.T3.4.3.3\" class=\"ltx_tr\">\n<td id=\"S6.T3.4.3.3.1\" class=\"ltx_td ltx_align_left ltx_border_t\">FedCurv</td>\n<td id=\"S6.T3.4.3.3.2\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S6.T3.4.3.3.2.1\" class=\"ltx_text ltx_font_bold\">0.606±0.093</span></td>\n<td id=\"S6.T3.4.3.3.3\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\">21.955±0.003</td>\n</tr>\n<tr id=\"S6.T3.4.4.4\" class=\"ltx_tr\">\n<td id=\"S6.T3.4.4.4.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S6.T3.4.4.4.1.1\" class=\"ltx_text ltx_font_bold\">Fishr+Inter-Geo</span></td>\n<td id=\"S6.T3.4.4.4.2\" class=\"ltx_td ltx_align_left ltx_border_t\">0.687±0.234</td>\n<td id=\"S6.T3.4.4.4.3\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\">21.953±0.007</td>\n</tr>\n<tr id=\"S6.T3.4.5.5\" class=\"ltx_tr\">\n<td id=\"S6.T3.4.5.5.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span id=\"S6.T3.4.5.5.1.1\" class=\"ltx_text ltx_font_bold\">Fishr+Intra-Geo</span></td>\n<td id=\"S6.T3.4.5.5.2\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">0.611±0.120</td>\n<td id=\"S6.T3.4.5.5.3\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_t\"><span id=\"S6.T3.4.5.5.3.1\" class=\"ltx_text ltx_font_bold\">21.956±0.003</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Last but not least, we also measure the fairness of our proposed algorithm by evaluating (1) the variance of the accuracy distribution and (2) the KL-divergence between the normalized accuracy and the uniform distribution, which can be directly translated to the entropy of accuracy defined in the paper (47). The result of our experiments is demonstrated in Table 3."
        ]
    },
    "S6.T4": {
        "caption": "Table 4: The AUCROC and AUCPR comparison among different algorithms on the Color-MNIST, Rotated-CIFAR10, and eICU dataset when the OOD test loss is lowest.",
        "table": "<table id=\"S6.T4.st1.4\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T4.st1.4.1.1\" class=\"ltx_tr\">\n<td id=\"S6.T4.st1.4.1.1.1\" class=\"ltx_td ltx_border_t\"></td>\n<th id=\"S6.T4.st1.4.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">Color-MNIST</th>\n<th id=\"S6.T4.st1.4.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">eICU</th>\n</tr>\n<tr id=\"S6.T4.st1.4.2.2\" class=\"ltx_tr\">\n<td id=\"S6.T4.st1.4.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_tt\">FedSGD</td>\n<td id=\"S6.T4.st1.4.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_tt\">0.780±0.001</td>\n<td id=\"S6.T4.st1.4.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_tt\">0.556±0.004</td>\n</tr>\n<tr id=\"S6.T4.st1.4.3.3\" class=\"ltx_tr\">\n<td id=\"S6.T4.st1.4.3.3.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Geometric</td>\n<td id=\"S6.T4.st1.4.3.3.2\" class=\"ltx_td ltx_align_left ltx_border_t\">0.781±0.003</td>\n<td id=\"S6.T4.st1.4.3.3.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.575±0.004</td>\n</tr>\n<tr id=\"S6.T4.st1.4.4.4\" class=\"ltx_tr\">\n<td id=\"S6.T4.st1.4.4.4.1\" class=\"ltx_td ltx_align_left ltx_border_t\">FedCurv</td>\n<td id=\"S6.T4.st1.4.4.4.2\" class=\"ltx_td ltx_align_left ltx_border_t\">0.780±0.003</td>\n<td id=\"S6.T4.st1.4.4.4.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.556±0.005</td>\n</tr>\n<tr id=\"S6.T4.st1.4.5.5\" class=\"ltx_tr\">\n<td id=\"S6.T4.st1.4.5.5.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S6.T4.st1.4.5.5.1.1\" class=\"ltx_text ltx_font_bold\">Fishr+Inter-Geo</span></td>\n<td id=\"S6.T4.st1.4.5.5.2\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S6.T4.st1.4.5.5.2.1\" class=\"ltx_text ltx_font_bold\">0.789±0.002</span></td>\n<td id=\"S6.T4.st1.4.5.5.3\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S6.T4.st1.4.5.5.3.1\" class=\"ltx_text ltx_font_bold\">0.577±0.005</span></td>\n</tr>\n<tr id=\"S6.T4.st1.4.6.6\" class=\"ltx_tr\">\n<td id=\"S6.T4.st1.4.6.6.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Fishr+Intra-Arith</td>\n<td id=\"S6.T4.st1.4.6.6.2\" class=\"ltx_td ltx_align_left ltx_border_t\">0.782±0.005</td>\n<td id=\"S6.T4.st1.4.6.6.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.553±0.002</td>\n</tr>\n<tr id=\"S6.T4.st1.4.7.7\" class=\"ltx_tr\">\n<td id=\"S6.T4.st1.4.7.7.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span id=\"S6.T4.st1.4.7.7.1.1\" class=\"ltx_text ltx_font_bold\">Fishr+Intra-Geo</span></td>\n<td id=\"S6.T4.st1.4.7.7.2\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">0.778±0.002</td>\n<td id=\"S6.T4.st1.4.7.7.3\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">0.567±0.004</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We run each algorithm for five different seeds, then average their loss, the AUCROC, AUCPR, as well as the accuracy on the OOD test set when the OOD test loss is lowest, which are demonstrated in Table 2, Table 4 Table 6, and Figure 3. Further, we also conduct the inter-silo and intra-silo accuracy comparison among each sub environment on the Rotated-CIFAR10 dataset when the OOD test loss is lowest 5. Specifically, we also experiment with the model’s performance regarding different Fishr regularizer λ𝜆\\lambda on the eICU dataset 2. Moreover, we also plot the training loss, validation loss, and OOD test loss among different algorithms, which are demonstrated in Figure 5, Figure 6, and Figure 7 in the Appendix section. From the figures, our proposed methods, Fishr+Inter-Geo and Fishr+Intra-Geo, significantly outperform other methods in domain adaptation, including the baseline algorithm FedSGD and the state-of-the-art algorithm FedCurv."
        ]
    },
    "S6.T4.st1": {
        "caption": "(a) AUCROC",
        "table": "<table id=\"S6.T4.st1.4\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T4.st1.4.1.1\" class=\"ltx_tr\">\n<td id=\"S6.T4.st1.4.1.1.1\" class=\"ltx_td ltx_border_t\"></td>\n<th id=\"S6.T4.st1.4.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">Color-MNIST</th>\n<th id=\"S6.T4.st1.4.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">eICU</th>\n</tr>\n<tr id=\"S6.T4.st1.4.2.2\" class=\"ltx_tr\">\n<td id=\"S6.T4.st1.4.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_tt\">FedSGD</td>\n<td id=\"S6.T4.st1.4.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_tt\">0.780±0.001</td>\n<td id=\"S6.T4.st1.4.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_tt\">0.556±0.004</td>\n</tr>\n<tr id=\"S6.T4.st1.4.3.3\" class=\"ltx_tr\">\n<td id=\"S6.T4.st1.4.3.3.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Geometric</td>\n<td id=\"S6.T4.st1.4.3.3.2\" class=\"ltx_td ltx_align_left ltx_border_t\">0.781±0.003</td>\n<td id=\"S6.T4.st1.4.3.3.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.575±0.004</td>\n</tr>\n<tr id=\"S6.T4.st1.4.4.4\" class=\"ltx_tr\">\n<td id=\"S6.T4.st1.4.4.4.1\" class=\"ltx_td ltx_align_left ltx_border_t\">FedCurv</td>\n<td id=\"S6.T4.st1.4.4.4.2\" class=\"ltx_td ltx_align_left ltx_border_t\">0.780±0.003</td>\n<td id=\"S6.T4.st1.4.4.4.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.556±0.005</td>\n</tr>\n<tr id=\"S6.T4.st1.4.5.5\" class=\"ltx_tr\">\n<td id=\"S6.T4.st1.4.5.5.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S6.T4.st1.4.5.5.1.1\" class=\"ltx_text ltx_font_bold\">Fishr+Inter-Geo</span></td>\n<td id=\"S6.T4.st1.4.5.5.2\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S6.T4.st1.4.5.5.2.1\" class=\"ltx_text ltx_font_bold\">0.789±0.002</span></td>\n<td id=\"S6.T4.st1.4.5.5.3\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S6.T4.st1.4.5.5.3.1\" class=\"ltx_text ltx_font_bold\">0.577±0.005</span></td>\n</tr>\n<tr id=\"S6.T4.st1.4.6.6\" class=\"ltx_tr\">\n<td id=\"S6.T4.st1.4.6.6.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Fishr+Intra-Arith</td>\n<td id=\"S6.T4.st1.4.6.6.2\" class=\"ltx_td ltx_align_left ltx_border_t\">0.782±0.005</td>\n<td id=\"S6.T4.st1.4.6.6.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.553±0.002</td>\n</tr>\n<tr id=\"S6.T4.st1.4.7.7\" class=\"ltx_tr\">\n<td id=\"S6.T4.st1.4.7.7.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span id=\"S6.T4.st1.4.7.7.1.1\" class=\"ltx_text ltx_font_bold\">Fishr+Intra-Geo</span></td>\n<td id=\"S6.T4.st1.4.7.7.2\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">0.778±0.002</td>\n<td id=\"S6.T4.st1.4.7.7.3\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">0.567±0.004</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "(2) Gradient-level agreement: Another approach, on the contrary, works at gradient level, which intends to find local or global minimums commonly shared across all the environments in the loss space (17; 19; 18). In other words, the goal minimums are agreements among the gradients backpropagated in the network, and allow it to share similar Hessians for different domains. Specifically, Parascandolo el al. proposed geometric averaging of Hessians to capture the invariance among different domains (17). Compared with arithmetic mean, which operates \"logical OR\" on the Hessians, geometric mean performs a \"logical AND\" operation, and achieves better outcomes in finding consistencies among environments. Nonetheless, the geometric mean is intrinsically blemished when the signs are inconsistent, which can be prevalent in non-convex settings. Furthermore, he also applied an AND-masking strategy to the arithmetic mean, which defines a peculiar agreement threshold as a hyper-parameter, and allows the parameters of network to be updated only if all the gradients flowing in that parameter agree on a certain direction based on the threshold. Nevertheless, the masking strategy requires the direction to which the gradients from different environments point strictly match with each other in order to allow the gradients to update that particular parameter. To address this issue, Soroosh proposed a smoothed AND-masking algorithm (SAND-mask), which not only validates the invariance in the direction of gradients, but also promotes the agreement among the gradient magnitudes (19). Yet, regarding their experiment results, the proposed masking strategy barely competes the performance of other algorithms, including AND-masking strategy. Moreover, Rame proposed Fishr in his paper to leverage the domain-level gradient covariance to promote invariances across domains (20). However, it is paramount to know the number of environments beforehand to calculate gradient covariance across domains, which is intractable in the centralized way of training.",
            "Federated learning aims to train a global model by aggregating the updates from a slew of client models spatially distributed on different devices while preserving privacy. In 2017, McMahan et al., for the first time, presented the notion of federated learning based on data parallelism and proposed both FedSGD and FedAvg algorithms. (1). FedSGD algorithm updates the global model by averaging the gradients uploaded by the clients, whilst the FedAvg methodology employs the averages of clients’ model parameters to update the global model’s weights. While federated learning provides a nascent methodology in privacy preservation, many challenges arise in terms of the domain shift problem engendered by the non-i.i.d data. Although the authors in (1) contend that FedAvg can mitigate the domain adaptation issue to a certain degree, myriads of research have evinced that a deterioration in the federated model’s accuracy is almost inevitable on the heterogeneous data (24). The state-of-the-art approaches to alleviate the non-i.i.d. problems in federated learning can be categorized into the data-based (25; 26; 27; 28), algorithm-based (29; 30; 31; 32; 33; 21), and system-based approaches (34; 35; 36; 37). Precisely, among the algorithm-based methodology, Shoham et al. proposed a federated curvature (FedCurv) algorithm, which is similar to the Fishr paper, while in a decentralized way to disentangle the non-i.i.d issue in federated learning (21). During each round, participants transmit updated models together with the diagonal of the Fisher information matrix, which represents the most informative features for the current task. A penalty term is added to the loss function for each participant to promote the convergence towards a globally shared optimum.",
            "However, since it is of paramount importance to know the number of environments beforehand for gradient covariance calculation, Rame hypothesized that the number of domains is accessible during training, which is barely tractable in a centralized way of training. However, we could utilize Fishr in the federated learning settings, where the number of federated clients ℰℰ\\mathcal{E} are antecedently accessible.",
            "Based on the conventional CIFAR10 dataset (45), we also develop our own synthetic dataset - Rotated-CIFAR10 as our benchmark dataset. We rotate CIFAR10 images with different angles to simulate the non-i.i.d. data in the federated networks. In our federated learning settings, there are three clients with distinct data distributions. Further, we also establish three sub-environments in each client to simulate intra-silo feature heterogeneity. The experiment setup is demonstrated in Table 1. Moreover, the OOD test set is configured with a rotation degree from -90° to 90° to evaluate the performance of the global model. Our goal is to correctly classify the 32x32 RGB colored images with 10 classes. Therefore, we adopt a 3-layer CNN architecture as our classification model. We also use the Adam optimizer with a fixed learning rate of 0.0001 and a weight decay of 0.001. Particularly, we select λ=1.0𝜆1.0\\lambda=1.0 as the Fishr hyperparameter.",
            "We run each algorithm for five different seeds, then average their loss, the AUCROC, AUCPR, as well as the accuracy on the OOD test set when the OOD test loss is lowest, which are demonstrated in Table 2, Table 4 Table 6, and Figure 3. Further, we also conduct the inter-silo and intra-silo accuracy comparison among each sub environment on the Rotated-CIFAR10 dataset when the OOD test loss is lowest 5. Specifically, we also experiment with the model’s performance regarding different Fishr regularizer λ𝜆\\lambda on the eICU dataset 2. Moreover, we also plot the training loss, validation loss, and OOD test loss among different algorithms, which are demonstrated in Figure 5, Figure 6, and Figure 7 in the Appendix section. From the figures, our proposed methods, Fishr+Inter-Geo and Fishr+Intra-Geo, significantly outperform other methods in domain adaptation, including the baseline algorithm FedSGD and the state-of-the-art algorithm FedCurv.",
            "Last but not least, we also measure the fairness of our proposed algorithm by evaluating (1) the variance of the accuracy distribution and (2) the KL-divergence between the normalized accuracy and the uniform distribution, which can be directly translated to the entropy of accuracy defined in the paper (47). The result of our experiments is demonstrated in Table 3."
        ]
    },
    "S6.T4.st2": {
        "caption": "(b) AUCPR",
        "table": "<table id=\"S6.T4.st2.4\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T4.st2.4.1.1\" class=\"ltx_tr\">\n<td id=\"S6.T4.st2.4.1.1.1\" class=\"ltx_td ltx_border_t\"></td>\n<th id=\"S6.T4.st2.4.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">Color-MNIST</th>\n<th id=\"S6.T4.st2.4.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">eICU</th>\n</tr>\n<tr id=\"S6.T4.st2.4.2.2\" class=\"ltx_tr\">\n<td id=\"S6.T4.st2.4.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_tt\">FedSGD</td>\n<td id=\"S6.T4.st2.4.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_tt\">0.837±0.002</td>\n<td id=\"S6.T4.st2.4.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_tt\">0.209±0.010</td>\n</tr>\n<tr id=\"S6.T4.st2.4.3.3\" class=\"ltx_tr\">\n<td id=\"S6.T4.st2.4.3.3.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Geometric</td>\n<td id=\"S6.T4.st2.4.3.3.2\" class=\"ltx_td ltx_align_left ltx_border_t\">0.838±0.004</td>\n<td id=\"S6.T4.st2.4.3.3.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.214±0.008</td>\n</tr>\n<tr id=\"S6.T4.st2.4.4.4\" class=\"ltx_tr\">\n<td id=\"S6.T4.st2.4.4.4.1\" class=\"ltx_td ltx_align_left ltx_border_t\">FedCurv</td>\n<td id=\"S6.T4.st2.4.4.4.2\" class=\"ltx_td ltx_align_left ltx_border_t\">0.838±0.004</td>\n<td id=\"S6.T4.st2.4.4.4.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.217±0.018</td>\n</tr>\n<tr id=\"S6.T4.st2.4.5.5\" class=\"ltx_tr\">\n<td id=\"S6.T4.st2.4.5.5.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S6.T4.st2.4.5.5.1.1\" class=\"ltx_text ltx_font_bold\">Fishr+Inter-Geo</span></td>\n<td id=\"S6.T4.st2.4.5.5.2\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S6.T4.st2.4.5.5.2.1\" class=\"ltx_text ltx_font_bold\">0.845±0.002</span></td>\n<td id=\"S6.T4.st2.4.5.5.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.218±0.009</td>\n</tr>\n<tr id=\"S6.T4.st2.4.6.6\" class=\"ltx_tr\">\n<td id=\"S6.T4.st2.4.6.6.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Fishr+Intra-Arith</td>\n<td id=\"S6.T4.st2.4.6.6.2\" class=\"ltx_td ltx_align_left ltx_border_t\">0.839±0.004</td>\n<td id=\"S6.T4.st2.4.6.6.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.216±0.016</td>\n</tr>\n<tr id=\"S6.T4.st2.4.7.7\" class=\"ltx_tr\">\n<td id=\"S6.T4.st2.4.7.7.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span id=\"S6.T4.st2.4.7.7.1.1\" class=\"ltx_text ltx_font_bold\">Fishr+Intra-Geo</span></td>\n<td id=\"S6.T4.st2.4.7.7.2\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">0.836±0.001</td>\n<td id=\"S6.T4.st2.4.7.7.3\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span id=\"S6.T4.st2.4.7.7.3.1\" class=\"ltx_text ltx_font_bold\">0.221±0.013</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "(2) Gradient-level agreement: Another approach, on the contrary, works at gradient level, which intends to find local or global minimums commonly shared across all the environments in the loss space (17; 19; 18). In other words, the goal minimums are agreements among the gradients backpropagated in the network, and allow it to share similar Hessians for different domains. Specifically, Parascandolo el al. proposed geometric averaging of Hessians to capture the invariance among different domains (17). Compared with arithmetic mean, which operates \"logical OR\" on the Hessians, geometric mean performs a \"logical AND\" operation, and achieves better outcomes in finding consistencies among environments. Nonetheless, the geometric mean is intrinsically blemished when the signs are inconsistent, which can be prevalent in non-convex settings. Furthermore, he also applied an AND-masking strategy to the arithmetic mean, which defines a peculiar agreement threshold as a hyper-parameter, and allows the parameters of network to be updated only if all the gradients flowing in that parameter agree on a certain direction based on the threshold. Nevertheless, the masking strategy requires the direction to which the gradients from different environments point strictly match with each other in order to allow the gradients to update that particular parameter. To address this issue, Soroosh proposed a smoothed AND-masking algorithm (SAND-mask), which not only validates the invariance in the direction of gradients, but also promotes the agreement among the gradient magnitudes (19). Yet, regarding their experiment results, the proposed masking strategy barely competes the performance of other algorithms, including AND-masking strategy. Moreover, Rame proposed Fishr in his paper to leverage the domain-level gradient covariance to promote invariances across domains (20). However, it is paramount to know the number of environments beforehand to calculate gradient covariance across domains, which is intractable in the centralized way of training.",
            "Federated learning aims to train a global model by aggregating the updates from a slew of client models spatially distributed on different devices while preserving privacy. In 2017, McMahan et al., for the first time, presented the notion of federated learning based on data parallelism and proposed both FedSGD and FedAvg algorithms. (1). FedSGD algorithm updates the global model by averaging the gradients uploaded by the clients, whilst the FedAvg methodology employs the averages of clients’ model parameters to update the global model’s weights. While federated learning provides a nascent methodology in privacy preservation, many challenges arise in terms of the domain shift problem engendered by the non-i.i.d data. Although the authors in (1) contend that FedAvg can mitigate the domain adaptation issue to a certain degree, myriads of research have evinced that a deterioration in the federated model’s accuracy is almost inevitable on the heterogeneous data (24). The state-of-the-art approaches to alleviate the non-i.i.d. problems in federated learning can be categorized into the data-based (25; 26; 27; 28), algorithm-based (29; 30; 31; 32; 33; 21), and system-based approaches (34; 35; 36; 37). Precisely, among the algorithm-based methodology, Shoham et al. proposed a federated curvature (FedCurv) algorithm, which is similar to the Fishr paper, while in a decentralized way to disentangle the non-i.i.d issue in federated learning (21). During each round, participants transmit updated models together with the diagonal of the Fisher information matrix, which represents the most informative features for the current task. A penalty term is added to the loss function for each participant to promote the convergence towards a globally shared optimum.",
            "However, since it is of paramount importance to know the number of environments beforehand for gradient covariance calculation, Rame hypothesized that the number of domains is accessible during training, which is barely tractable in a centralized way of training. However, we could utilize Fishr in the federated learning settings, where the number of federated clients ℰℰ\\mathcal{E} are antecedently accessible.",
            "Based on the conventional CIFAR10 dataset (45), we also develop our own synthetic dataset - Rotated-CIFAR10 as our benchmark dataset. We rotate CIFAR10 images with different angles to simulate the non-i.i.d. data in the federated networks. In our federated learning settings, there are three clients with distinct data distributions. Further, we also establish three sub-environments in each client to simulate intra-silo feature heterogeneity. The experiment setup is demonstrated in Table 1. Moreover, the OOD test set is configured with a rotation degree from -90° to 90° to evaluate the performance of the global model. Our goal is to correctly classify the 32x32 RGB colored images with 10 classes. Therefore, we adopt a 3-layer CNN architecture as our classification model. We also use the Adam optimizer with a fixed learning rate of 0.0001 and a weight decay of 0.001. Particularly, we select λ=1.0𝜆1.0\\lambda=1.0 as the Fishr hyperparameter.",
            "We run each algorithm for five different seeds, then average their loss, the AUCROC, AUCPR, as well as the accuracy on the OOD test set when the OOD test loss is lowest, which are demonstrated in Table 2, Table 4 Table 6, and Figure 3. Further, we also conduct the inter-silo and intra-silo accuracy comparison among each sub environment on the Rotated-CIFAR10 dataset when the OOD test loss is lowest 5. Specifically, we also experiment with the model’s performance regarding different Fishr regularizer λ𝜆\\lambda on the eICU dataset 2. Moreover, we also plot the training loss, validation loss, and OOD test loss among different algorithms, which are demonstrated in Figure 5, Figure 6, and Figure 7 in the Appendix section. From the figures, our proposed methods, Fishr+Inter-Geo and Fishr+Intra-Geo, significantly outperform other methods in domain adaptation, including the baseline algorithm FedSGD and the state-of-the-art algorithm FedCurv.",
            "Last but not least, we also measure the fairness of our proposed algorithm by evaluating (1) the variance of the accuracy distribution and (2) the KL-divergence between the normalized accuracy and the uniform distribution, which can be directly translated to the entropy of accuracy defined in the paper (47). The result of our experiments is demonstrated in Table 3."
        ]
    },
    "S7.12": {
        "caption": "",
        "table": "<table id=\"S7.12.12\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S7.12.12.13.1\" class=\"ltx_tr\">\n<th id=\"S7.12.12.13.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Notation</th>\n<th id=\"S7.12.12.13.1.2\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Description</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S7.1.1.1\" class=\"ltx_tr\">\n<th id=\"S7.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><math id=\"S7.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\mathcal{E}\" display=\"inline\"><semantics id=\"S7.1.1.1.1.m1.1a\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S7.1.1.1.1.m1.1.1\" xref=\"S7.1.1.1.1.m1.1.1.cmml\">ℰ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S7.1.1.1.1.m1.1b\"><ci id=\"S7.1.1.1.1.m1.1.1.cmml\" xref=\"S7.1.1.1.1.m1.1.1\">ℰ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.1.1.1.1.m1.1c\">\\mathcal{E}</annotation></semantics></math></th>\n<td id=\"S7.1.1.1.2\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\">Total number of clients in the federated networks</td>\n</tr>\n<tr id=\"S7.2.2.2\" class=\"ltx_tr\">\n<th id=\"S7.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math id=\"S7.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"e\" display=\"inline\"><semantics id=\"S7.2.2.2.1.m1.1a\"><mi id=\"S7.2.2.2.1.m1.1.1\" xref=\"S7.2.2.2.1.m1.1.1.cmml\">e</mi><annotation-xml encoding=\"MathML-Content\" id=\"S7.2.2.2.1.m1.1b\"><ci id=\"S7.2.2.2.1.m1.1.1.cmml\" xref=\"S7.2.2.2.1.m1.1.1\">𝑒</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.2.2.2.1.m1.1c\">e</annotation></semantics></math></th>\n<td id=\"S7.2.2.2.2\" class=\"ltx_td ltx_nopad_r ltx_align_left\">An individual federated client in the federated networks</td>\n</tr>\n<tr id=\"S7.3.3.3\" class=\"ltx_tr\">\n<th id=\"S7.3.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math id=\"S7.3.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\mathcal{D}\" display=\"inline\"><semantics id=\"S7.3.3.3.1.m1.1a\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S7.3.3.3.1.m1.1.1\" xref=\"S7.3.3.3.1.m1.1.1.cmml\">𝒟</mi><annotation-xml encoding=\"MathML-Content\" id=\"S7.3.3.3.1.m1.1b\"><ci id=\"S7.3.3.3.1.m1.1.1.cmml\" xref=\"S7.3.3.3.1.m1.1.1\">𝒟</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.3.3.3.1.m1.1c\">\\mathcal{D}</annotation></semantics></math></th>\n<td id=\"S7.3.3.3.2\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Local dataset of client</td>\n</tr>\n<tr id=\"S7.4.4.4\" class=\"ltx_tr\">\n<th id=\"S7.4.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math id=\"S7.4.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"w\" display=\"inline\"><semantics id=\"S7.4.4.4.1.m1.1a\"><mi id=\"S7.4.4.4.1.m1.1.1\" xref=\"S7.4.4.4.1.m1.1.1.cmml\">w</mi><annotation-xml encoding=\"MathML-Content\" id=\"S7.4.4.4.1.m1.1b\"><ci id=\"S7.4.4.4.1.m1.1.1.cmml\" xref=\"S7.4.4.4.1.m1.1.1\">𝑤</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.4.4.4.1.m1.1c\">w</annotation></semantics></math></th>\n<td id=\"S7.4.4.4.2\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Global weights of server</td>\n</tr>\n<tr id=\"S7.7.7.7\" class=\"ltx_tr\">\n<th id=\"S7.6.6.6.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<math id=\"S7.5.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"\\left[\\mathcal{E}^{+}\\right]_{i}\" display=\"inline\"><semantics id=\"S7.5.5.5.1.m1.1a\"><msub id=\"S7.5.5.5.1.m1.1.1\" xref=\"S7.5.5.5.1.m1.1.1.cmml\"><mrow id=\"S7.5.5.5.1.m1.1.1.1.1\" xref=\"S7.5.5.5.1.m1.1.1.1.2.cmml\"><mo id=\"S7.5.5.5.1.m1.1.1.1.1.2\" xref=\"S7.5.5.5.1.m1.1.1.1.2.1.cmml\">[</mo><msup id=\"S7.5.5.5.1.m1.1.1.1.1.1\" xref=\"S7.5.5.5.1.m1.1.1.1.1.1.cmml\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S7.5.5.5.1.m1.1.1.1.1.1.2\" xref=\"S7.5.5.5.1.m1.1.1.1.1.1.2.cmml\">ℰ</mi><mo id=\"S7.5.5.5.1.m1.1.1.1.1.1.3\" xref=\"S7.5.5.5.1.m1.1.1.1.1.1.3.cmml\">+</mo></msup><mo id=\"S7.5.5.5.1.m1.1.1.1.1.3\" xref=\"S7.5.5.5.1.m1.1.1.1.2.1.cmml\">]</mo></mrow><mi id=\"S7.5.5.5.1.m1.1.1.3\" xref=\"S7.5.5.5.1.m1.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S7.5.5.5.1.m1.1b\"><apply id=\"S7.5.5.5.1.m1.1.1.cmml\" xref=\"S7.5.5.5.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S7.5.5.5.1.m1.1.1.2.cmml\" xref=\"S7.5.5.5.1.m1.1.1\">subscript</csymbol><apply id=\"S7.5.5.5.1.m1.1.1.1.2.cmml\" xref=\"S7.5.5.5.1.m1.1.1.1.1\"><csymbol cd=\"latexml\" id=\"S7.5.5.5.1.m1.1.1.1.2.1.cmml\" xref=\"S7.5.5.5.1.m1.1.1.1.1.2\">delimited-[]</csymbol><apply id=\"S7.5.5.5.1.m1.1.1.1.1.1.cmml\" xref=\"S7.5.5.5.1.m1.1.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S7.5.5.5.1.m1.1.1.1.1.1.1.cmml\" xref=\"S7.5.5.5.1.m1.1.1.1.1.1\">superscript</csymbol><ci id=\"S7.5.5.5.1.m1.1.1.1.1.1.2.cmml\" xref=\"S7.5.5.5.1.m1.1.1.1.1.1.2\">ℰ</ci><plus id=\"S7.5.5.5.1.m1.1.1.1.1.1.3.cmml\" xref=\"S7.5.5.5.1.m1.1.1.1.1.1.3\"></plus></apply></apply><ci id=\"S7.5.5.5.1.m1.1.1.3.cmml\" xref=\"S7.5.5.5.1.m1.1.1.3\">𝑖</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.5.5.5.1.m1.1c\">\\left[\\mathcal{E}^{+}\\right]_{i}</annotation></semantics></math> / <math id=\"S7.6.6.6.2.m2.1\" class=\"ltx_Math\" alttext=\"\\left[\\mathcal{E}^{-}\\right]_{i}\" display=\"inline\"><semantics id=\"S7.6.6.6.2.m2.1a\"><msub id=\"S7.6.6.6.2.m2.1.1\" xref=\"S7.6.6.6.2.m2.1.1.cmml\"><mrow id=\"S7.6.6.6.2.m2.1.1.1.1\" xref=\"S7.6.6.6.2.m2.1.1.1.2.cmml\"><mo id=\"S7.6.6.6.2.m2.1.1.1.1.2\" xref=\"S7.6.6.6.2.m2.1.1.1.2.1.cmml\">[</mo><msup id=\"S7.6.6.6.2.m2.1.1.1.1.1\" xref=\"S7.6.6.6.2.m2.1.1.1.1.1.cmml\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S7.6.6.6.2.m2.1.1.1.1.1.2\" xref=\"S7.6.6.6.2.m2.1.1.1.1.1.2.cmml\">ℰ</mi><mo id=\"S7.6.6.6.2.m2.1.1.1.1.1.3\" xref=\"S7.6.6.6.2.m2.1.1.1.1.1.3.cmml\">−</mo></msup><mo id=\"S7.6.6.6.2.m2.1.1.1.1.3\" xref=\"S7.6.6.6.2.m2.1.1.1.2.1.cmml\">]</mo></mrow><mi id=\"S7.6.6.6.2.m2.1.1.3\" xref=\"S7.6.6.6.2.m2.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S7.6.6.6.2.m2.1b\"><apply id=\"S7.6.6.6.2.m2.1.1.cmml\" xref=\"S7.6.6.6.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S7.6.6.6.2.m2.1.1.2.cmml\" xref=\"S7.6.6.6.2.m2.1.1\">subscript</csymbol><apply id=\"S7.6.6.6.2.m2.1.1.1.2.cmml\" xref=\"S7.6.6.6.2.m2.1.1.1.1\"><csymbol cd=\"latexml\" id=\"S7.6.6.6.2.m2.1.1.1.2.1.cmml\" xref=\"S7.6.6.6.2.m2.1.1.1.1.2\">delimited-[]</csymbol><apply id=\"S7.6.6.6.2.m2.1.1.1.1.1.cmml\" xref=\"S7.6.6.6.2.m2.1.1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S7.6.6.6.2.m2.1.1.1.1.1.1.cmml\" xref=\"S7.6.6.6.2.m2.1.1.1.1.1\">superscript</csymbol><ci id=\"S7.6.6.6.2.m2.1.1.1.1.1.2.cmml\" xref=\"S7.6.6.6.2.m2.1.1.1.1.1.2\">ℰ</ci><minus id=\"S7.6.6.6.2.m2.1.1.1.1.1.3.cmml\" xref=\"S7.6.6.6.2.m2.1.1.1.1.1.3\"></minus></apply></apply><ci id=\"S7.6.6.6.2.m2.1.1.3.cmml\" xref=\"S7.6.6.6.2.m2.1.1.3\">𝑖</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.6.6.6.2.m2.1c\">\\left[\\mathcal{E}^{-}\\right]_{i}</annotation></semantics></math>\n</th>\n<td id=\"S7.7.7.7.3\" class=\"ltx_td ltx_nopad_r ltx_align_left\">The number of clients with positive / negative gradients for <math id=\"S7.7.7.7.3.m1.1\" class=\"ltx_Math\" alttext=\"i\" display=\"inline\"><semantics id=\"S7.7.7.7.3.m1.1a\"><mi id=\"S7.7.7.7.3.m1.1.1\" xref=\"S7.7.7.7.3.m1.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S7.7.7.7.3.m1.1b\"><ci id=\"S7.7.7.7.3.m1.1.1.cmml\" xref=\"S7.7.7.7.3.m1.1.1\">𝑖</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.7.7.7.3.m1.1c\">i</annotation></semantics></math>-th data example</td>\n</tr>\n<tr id=\"S7.8.8.8\" class=\"ltx_tr\">\n<th id=\"S7.8.8.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math id=\"S7.8.8.8.1.m1.1\" class=\"ltx_Math\" alttext=\"\\nabla^{\\wedge}F\" display=\"inline\"><semantics id=\"S7.8.8.8.1.m1.1a\"><mrow id=\"S7.8.8.8.1.m1.1.1\" xref=\"S7.8.8.8.1.m1.1.1.cmml\"><msup id=\"S7.8.8.8.1.m1.1.1.1\" xref=\"S7.8.8.8.1.m1.1.1.1.cmml\"><mo id=\"S7.8.8.8.1.m1.1.1.1.2\" xref=\"S7.8.8.8.1.m1.1.1.1.2.cmml\">∇</mo><mo id=\"S7.8.8.8.1.m1.1.1.1.3\" xref=\"S7.8.8.8.1.m1.1.1.1.3.cmml\">∧</mo></msup><mi id=\"S7.8.8.8.1.m1.1.1.2\" xref=\"S7.8.8.8.1.m1.1.1.2.cmml\">F</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S7.8.8.8.1.m1.1b\"><apply id=\"S7.8.8.8.1.m1.1.1.cmml\" xref=\"S7.8.8.8.1.m1.1.1\"><apply id=\"S7.8.8.8.1.m1.1.1.1.cmml\" xref=\"S7.8.8.8.1.m1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S7.8.8.8.1.m1.1.1.1.1.cmml\" xref=\"S7.8.8.8.1.m1.1.1.1\">superscript</csymbol><ci id=\"S7.8.8.8.1.m1.1.1.1.2.cmml\" xref=\"S7.8.8.8.1.m1.1.1.1.2\">∇</ci><and id=\"S7.8.8.8.1.m1.1.1.1.3.cmml\" xref=\"S7.8.8.8.1.m1.1.1.1.3\"></and></apply><ci id=\"S7.8.8.8.1.m1.1.1.2.cmml\" xref=\"S7.8.8.8.1.m1.1.1.2\">𝐹</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.8.8.8.1.m1.1c\">\\nabla^{\\wedge}F</annotation></semantics></math></th>\n<td id=\"S7.8.8.8.2\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Weighted geometric mean of gradients</td>\n</tr>\n<tr id=\"S7.9.9.9\" class=\"ltx_tr\">\n<th id=\"S7.9.9.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math id=\"S7.9.9.9.1.m1.1\" class=\"ltx_Math\" alttext=\"\\nabla^{+}F\" display=\"inline\"><semantics id=\"S7.9.9.9.1.m1.1a\"><mrow id=\"S7.9.9.9.1.m1.1.1\" xref=\"S7.9.9.9.1.m1.1.1.cmml\"><msup id=\"S7.9.9.9.1.m1.1.1.1\" xref=\"S7.9.9.9.1.m1.1.1.1.cmml\"><mo id=\"S7.9.9.9.1.m1.1.1.1.2\" xref=\"S7.9.9.9.1.m1.1.1.1.2.cmml\">∇</mo><mo id=\"S7.9.9.9.1.m1.1.1.1.3\" xref=\"S7.9.9.9.1.m1.1.1.1.3.cmml\">+</mo></msup><mi id=\"S7.9.9.9.1.m1.1.1.2\" xref=\"S7.9.9.9.1.m1.1.1.2.cmml\">F</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S7.9.9.9.1.m1.1b\"><apply id=\"S7.9.9.9.1.m1.1.1.cmml\" xref=\"S7.9.9.9.1.m1.1.1\"><apply id=\"S7.9.9.9.1.m1.1.1.1.cmml\" xref=\"S7.9.9.9.1.m1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S7.9.9.9.1.m1.1.1.1.1.cmml\" xref=\"S7.9.9.9.1.m1.1.1.1\">superscript</csymbol><ci id=\"S7.9.9.9.1.m1.1.1.1.2.cmml\" xref=\"S7.9.9.9.1.m1.1.1.1.2\">∇</ci><plus id=\"S7.9.9.9.1.m1.1.1.1.3.cmml\" xref=\"S7.9.9.9.1.m1.1.1.1.3\"></plus></apply><ci id=\"S7.9.9.9.1.m1.1.1.2.cmml\" xref=\"S7.9.9.9.1.m1.1.1.2\">𝐹</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.9.9.9.1.m1.1c\">\\nabla^{+}F</annotation></semantics></math></th>\n<td id=\"S7.9.9.9.2\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Arithmetic mean of gradients</td>\n</tr>\n<tr id=\"S7.10.10.10\" class=\"ltx_tr\">\n<th id=\"S7.10.10.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math id=\"S7.10.10.10.1.m1.1\" class=\"ltx_Math\" alttext=\"\\eta\" display=\"inline\"><semantics id=\"S7.10.10.10.1.m1.1a\"><mi id=\"S7.10.10.10.1.m1.1.1\" xref=\"S7.10.10.10.1.m1.1.1.cmml\">η</mi><annotation-xml encoding=\"MathML-Content\" id=\"S7.10.10.10.1.m1.1b\"><ci id=\"S7.10.10.10.1.m1.1.1.cmml\" xref=\"S7.10.10.10.1.m1.1.1\">𝜂</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.10.10.10.1.m1.1c\">\\eta</annotation></semantics></math></th>\n<td id=\"S7.10.10.10.2\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Learning rate</td>\n</tr>\n<tr id=\"S7.11.11.11\" class=\"ltx_tr\">\n<th id=\"S7.11.11.11.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math id=\"S7.11.11.11.1.m1.1\" class=\"ltx_Math\" alttext=\"\\lambda\" display=\"inline\"><semantics id=\"S7.11.11.11.1.m1.1a\"><mi id=\"S7.11.11.11.1.m1.1.1\" xref=\"S7.11.11.11.1.m1.1.1.cmml\">λ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S7.11.11.11.1.m1.1b\"><ci id=\"S7.11.11.11.1.m1.1.1.cmml\" xref=\"S7.11.11.11.1.m1.1.1\">𝜆</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.11.11.11.1.m1.1c\">\\lambda</annotation></semantics></math></th>\n<td id=\"S7.11.11.11.2\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Hyper-parameter to control the Fishr regularization strength</td>\n</tr>\n<tr id=\"S7.12.12.12\" class=\"ltx_tr\">\n<th id=\"S7.12.12.12.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><math id=\"S7.12.12.12.1.m1.1\" class=\"ltx_Math\" alttext=\"C\" display=\"inline\"><semantics id=\"S7.12.12.12.1.m1.1a\"><mi id=\"S7.12.12.12.1.m1.1.1\" xref=\"S7.12.12.12.1.m1.1.1.cmml\">C</mi><annotation-xml encoding=\"MathML-Content\" id=\"S7.12.12.12.1.m1.1b\"><ci id=\"S7.12.12.12.1.m1.1.1.cmml\" xref=\"S7.12.12.12.1.m1.1.1\">𝐶</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.12.12.12.1.m1.1c\">C</annotation></semantics></math></th>\n<td id=\"S7.12.12.12.2\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_bb\">Gradient covariance matrix</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "(2) Gradient-level agreement: Another approach, on the contrary, works at gradient level, which intends to find local or global minimums commonly shared across all the environments in the loss space (17; 19; 18). In other words, the goal minimums are agreements among the gradients backpropagated in the network, and allow it to share similar Hessians for different domains. Specifically, Parascandolo el al. proposed geometric averaging of Hessians to capture the invariance among different domains (17). Compared with arithmetic mean, which operates \"logical OR\" on the Hessians, geometric mean performs a \"logical AND\" operation, and achieves better outcomes in finding consistencies among environments. Nonetheless, the geometric mean is intrinsically blemished when the signs are inconsistent, which can be prevalent in non-convex settings. Furthermore, he also applied an AND-masking strategy to the arithmetic mean, which defines a peculiar agreement threshold as a hyper-parameter, and allows the parameters of network to be updated only if all the gradients flowing in that parameter agree on a certain direction based on the threshold. Nevertheless, the masking strategy requires the direction to which the gradients from different environments point strictly match with each other in order to allow the gradients to update that particular parameter. To address this issue, Soroosh proposed a smoothed AND-masking algorithm (SAND-mask), which not only validates the invariance in the direction of gradients, but also promotes the agreement among the gradient magnitudes (19). Yet, regarding their experiment results, the proposed masking strategy barely competes the performance of other algorithms, including AND-masking strategy. Moreover, Rame proposed Fishr in his paper to leverage the domain-level gradient covariance to promote invariances across domains (20). However, it is paramount to know the number of environments beforehand to calculate gradient covariance across domains, which is intractable in the centralized way of training.",
            "Federated learning aims to train a global model by aggregating the updates from a slew of client models spatially distributed on different devices while preserving privacy. In 2017, McMahan et al., for the first time, presented the notion of federated learning based on data parallelism and proposed both FedSGD and FedAvg algorithms. (1). FedSGD algorithm updates the global model by averaging the gradients uploaded by the clients, whilst the FedAvg methodology employs the averages of clients’ model parameters to update the global model’s weights. While federated learning provides a nascent methodology in privacy preservation, many challenges arise in terms of the domain shift problem engendered by the non-i.i.d data. Although the authors in (1) contend that FedAvg can mitigate the domain adaptation issue to a certain degree, myriads of research have evinced that a deterioration in the federated model’s accuracy is almost inevitable on the heterogeneous data (24). The state-of-the-art approaches to alleviate the non-i.i.d. problems in federated learning can be categorized into the data-based (25; 26; 27; 28), algorithm-based (29; 30; 31; 32; 33; 21), and system-based approaches (34; 35; 36; 37). Precisely, among the algorithm-based methodology, Shoham et al. proposed a federated curvature (FedCurv) algorithm, which is similar to the Fishr paper, while in a decentralized way to disentangle the non-i.i.d issue in federated learning (21). During each round, participants transmit updated models together with the diagonal of the Fisher information matrix, which represents the most informative features for the current task. A penalty term is added to the loss function for each participant to promote the convergence towards a globally shared optimum.",
            "However, since it is of paramount importance to know the number of environments beforehand for gradient covariance calculation, Rame hypothesized that the number of domains is accessible during training, which is barely tractable in a centralized way of training. However, we could utilize Fishr in the federated learning settings, where the number of federated clients ℰℰ\\mathcal{E} are antecedently accessible.",
            "Based on the conventional CIFAR10 dataset (45), we also develop our own synthetic dataset - Rotated-CIFAR10 as our benchmark dataset. We rotate CIFAR10 images with different angles to simulate the non-i.i.d. data in the federated networks. In our federated learning settings, there are three clients with distinct data distributions. Further, we also establish three sub-environments in each client to simulate intra-silo feature heterogeneity. The experiment setup is demonstrated in Table 1. Moreover, the OOD test set is configured with a rotation degree from -90° to 90° to evaluate the performance of the global model. Our goal is to correctly classify the 32x32 RGB colored images with 10 classes. Therefore, we adopt a 3-layer CNN architecture as our classification model. We also use the Adam optimizer with a fixed learning rate of 0.0001 and a weight decay of 0.001. Particularly, we select λ=1.0𝜆1.0\\lambda=1.0 as the Fishr hyperparameter.",
            "We run each algorithm for five different seeds, then average their loss, the AUCROC, AUCPR, as well as the accuracy on the OOD test set when the OOD test loss is lowest, which are demonstrated in Table 2, Table 4 Table 6, and Figure 3. Further, we also conduct the inter-silo and intra-silo accuracy comparison among each sub environment on the Rotated-CIFAR10 dataset when the OOD test loss is lowest 5. Specifically, we also experiment with the model’s performance regarding different Fishr regularizer λ𝜆\\lambda on the eICU dataset 2. Moreover, we also plot the training loss, validation loss, and OOD test loss among different algorithms, which are demonstrated in Figure 5, Figure 6, and Figure 7 in the Appendix section. From the figures, our proposed methods, Fishr+Inter-Geo and Fishr+Intra-Geo, significantly outperform other methods in domain adaptation, including the baseline algorithm FedSGD and the state-of-the-art algorithm FedCurv.",
            "Last but not least, we also measure the fairness of our proposed algorithm by evaluating (1) the variance of the accuracy distribution and (2) the KL-divergence between the normalized accuracy and the uniform distribution, which can be directly translated to the entropy of accuracy defined in the paper (47). The result of our experiments is demonstrated in Table 3."
        ]
    },
    "S7.T5": {
        "caption": "Table 5: The inter-silo and intra-silo accuracy comparison on the Rotated-CIFAR10 dataset when the OOD test loss is lowest. ",
        "table": "<table id=\"S7.T5.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S7.T5.4.1.1\" class=\"ltx_tr\">\n<td id=\"S7.T5.4.1.1.1\" class=\"ltx_td ltx_border_tt\"></td>\n<th id=\"S7.T5.4.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">FedSGD</th>\n<th id=\"S7.T5.4.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">FedCurv</th>\n<th id=\"S7.T5.4.1.1.4\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span id=\"S7.T5.4.1.1.4.1\" class=\"ltx_text ltx_font_bold\">Fishr+Inter-Geo</span></th>\n<th id=\"S7.T5.4.1.1.5\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span id=\"S7.T5.4.1.1.5.1\" class=\"ltx_text ltx_font_bold\">Fishr+Intra-Geo</span></th>\n</tr>\n<tr id=\"S7.T5.4.2.2\" class=\"ltx_tr\">\n<td id=\"S7.T5.4.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_tt\">Client 1 Sub Env 1</td>\n<td id=\"S7.T5.4.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_tt\">0.443±0.010</td>\n<td id=\"S7.T5.4.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_tt\">0.435±0.013</td>\n<td id=\"S7.T5.4.2.2.4\" class=\"ltx_td ltx_align_left ltx_border_tt\">0.446±0.020</td>\n<td id=\"S7.T5.4.2.2.5\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_tt\"><span id=\"S7.T5.4.2.2.5.1\" class=\"ltx_text ltx_font_bold\">0.456±0.017</span></td>\n</tr>\n<tr id=\"S7.T5.4.3.3\" class=\"ltx_tr\">\n<td id=\"S7.T5.4.3.3.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Client 1 Sub Env 2</td>\n<td id=\"S7.T5.4.3.3.2\" class=\"ltx_td ltx_align_left ltx_border_t\">0.434±0.017</td>\n<td id=\"S7.T5.4.3.3.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.448±0.008</td>\n<td id=\"S7.T5.4.3.3.4\" class=\"ltx_td ltx_align_left ltx_border_t\">0.438±0.009</td>\n<td id=\"S7.T5.4.3.3.5\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\"><span id=\"S7.T5.4.3.3.5.1\" class=\"ltx_text ltx_font_bold\">0.451±0.011</span></td>\n</tr>\n<tr id=\"S7.T5.4.4.4\" class=\"ltx_tr\">\n<td id=\"S7.T5.4.4.4.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Client 1 Sub Env 3</td>\n<td id=\"S7.T5.4.4.4.2\" class=\"ltx_td ltx_align_left ltx_border_t\">0.428±0.008</td>\n<td id=\"S7.T5.4.4.4.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.424±0.006</td>\n<td id=\"S7.T5.4.4.4.4\" class=\"ltx_td ltx_align_left ltx_border_t\">0.442±0.013</td>\n<td id=\"S7.T5.4.4.4.5\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\"><span id=\"S7.T5.4.4.4.5.1\" class=\"ltx_text ltx_font_bold\">0.444±0.009</span></td>\n</tr>\n<tr id=\"S7.T5.4.5.5\" class=\"ltx_tr\">\n<td id=\"S7.T5.4.5.5.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Client 2 Sub Env 1</td>\n<td id=\"S7.T5.4.5.5.2\" class=\"ltx_td ltx_align_left ltx_border_t\">0.420±0.020</td>\n<td id=\"S7.T5.4.5.5.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.425±0.004</td>\n<td id=\"S7.T5.4.5.5.4\" class=\"ltx_td ltx_align_left ltx_border_t\">0.435±0.004</td>\n<td id=\"S7.T5.4.5.5.5\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\"><span id=\"S7.T5.4.5.5.5.1\" class=\"ltx_text ltx_font_bold\">0.441±0.008</span></td>\n</tr>\n<tr id=\"S7.T5.4.6.6\" class=\"ltx_tr\">\n<td id=\"S7.T5.4.6.6.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Client 2 Sub Env 2</td>\n<td id=\"S7.T5.4.6.6.2\" class=\"ltx_td ltx_align_left ltx_border_t\">0.440±0.009</td>\n<td id=\"S7.T5.4.6.6.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.437±0.011</td>\n<td id=\"S7.T5.4.6.6.4\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S7.T5.4.6.6.4.1\" class=\"ltx_text ltx_font_bold\">0.443±0.009</span></td>\n<td id=\"S7.T5.4.6.6.5\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\">0.438±0.008</td>\n</tr>\n<tr id=\"S7.T5.4.7.7\" class=\"ltx_tr\">\n<td id=\"S7.T5.4.7.7.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Client 2 Sub Env 3</td>\n<td id=\"S7.T5.4.7.7.2\" class=\"ltx_td ltx_align_left ltx_border_t\">0.411±0.007</td>\n<td id=\"S7.T5.4.7.7.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.409±0.005</td>\n<td id=\"S7.T5.4.7.7.4\" class=\"ltx_td ltx_align_left ltx_border_t\">0.404±0.008</td>\n<td id=\"S7.T5.4.7.7.5\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\"><span id=\"S7.T5.4.7.7.5.1\" class=\"ltx_text ltx_font_bold\">0.419±0.011</span></td>\n</tr>\n<tr id=\"S7.T5.4.8.8\" class=\"ltx_tr\">\n<td id=\"S7.T5.4.8.8.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Client 3 Sub Env 1</td>\n<td id=\"S7.T5.4.8.8.2\" class=\"ltx_td ltx_align_left ltx_border_t\">0.413±0.014</td>\n<td id=\"S7.T5.4.8.8.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.431±0.011</td>\n<td id=\"S7.T5.4.8.8.4\" class=\"ltx_td ltx_align_left ltx_border_t\">0.427±0.017</td>\n<td id=\"S7.T5.4.8.8.5\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\"><span id=\"S7.T5.4.8.8.5.1\" class=\"ltx_text ltx_font_bold\">0.443±0.004</span></td>\n</tr>\n<tr id=\"S7.T5.4.9.9\" class=\"ltx_tr\">\n<td id=\"S7.T5.4.9.9.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Client 3 Sub Env 2</td>\n<td id=\"S7.T5.4.9.9.2\" class=\"ltx_td ltx_align_left ltx_border_t\">0.367±0.010</td>\n<td id=\"S7.T5.4.9.9.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.379±0.014</td>\n<td id=\"S7.T5.4.9.9.4\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S7.T5.4.9.9.4.1\" class=\"ltx_text ltx_font_bold\">0.392±0.012</span></td>\n<td id=\"S7.T5.4.9.9.5\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\">0.388±0.012</td>\n</tr>\n<tr id=\"S7.T5.4.10.10\" class=\"ltx_tr\">\n<td id=\"S7.T5.4.10.10.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">Client 3 Sub Env 3</td>\n<td id=\"S7.T5.4.10.10.2\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">0.371±0.010</td>\n<td id=\"S7.T5.4.10.10.3\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">0.380±0.010</td>\n<td id=\"S7.T5.4.10.10.4\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">0.381±0.016</td>\n<td id=\"S7.T5.4.10.10.5\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_t\"><span id=\"S7.T5.4.10.10.5.1\" class=\"ltx_text ltx_font_bold\">0.396±0.010</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            []
        ]
    },
    "S7.T6": {
        "caption": "Table 6: The accuracy comparison among different algorithms on the Color-MNIST, Rotated-CIFAR10, and eICU dataset when the OOD test loss is lowest. Since the eICU dataset is highly imbalanced, the accuracy on the eICU dataset is not fully informative.",
        "table": "<table id=\"S7.T6.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S7.T6.4.1.1\" class=\"ltx_tr\">\n<td id=\"S7.T6.4.1.1.1\" class=\"ltx_td ltx_border_tt\"></td>\n<th id=\"S7.T6.4.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Color-MNIST</th>\n<th id=\"S7.T6.4.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Rotated-CIFAR10</th>\n<th id=\"S7.T6.4.1.1.4\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt\">eICU</th>\n</tr>\n<tr id=\"S7.T6.4.2.2\" class=\"ltx_tr\">\n<td id=\"S7.T6.4.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_tt\">FedSGD</td>\n<td id=\"S7.T6.4.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_tt\">0.781±0.001</td>\n<td id=\"S7.T6.4.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_tt\">0.404±0.004</td>\n<td id=\"S7.T6.4.2.2.4\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_tt\">0.919±0.008</td>\n</tr>\n<tr id=\"S7.T6.4.3.3\" class=\"ltx_tr\">\n<td id=\"S7.T6.4.3.3.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Geometric</td>\n<td id=\"S7.T6.4.3.3.2\" class=\"ltx_td ltx_align_left ltx_border_t\">0.781±0.003</td>\n<td id=\"S7.T6.4.3.3.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.405±0.003</td>\n<td id=\"S7.T6.4.3.3.4\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\">0.900±0.005</td>\n</tr>\n<tr id=\"S7.T6.4.4.4\" class=\"ltx_tr\">\n<td id=\"S7.T6.4.4.4.1\" class=\"ltx_td ltx_align_left ltx_border_t\">FedCurv</td>\n<td id=\"S7.T6.4.4.4.2\" class=\"ltx_td ltx_align_left ltx_border_t\">0.780±0.004</td>\n<td id=\"S7.T6.4.4.4.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.408±0.010</td>\n<td id=\"S7.T6.4.4.4.4\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\"><span id=\"S7.T6.4.4.4.4.1\" class=\"ltx_text ltx_font_bold\">0.923±0.005</span></td>\n</tr>\n<tr id=\"S7.T6.4.5.5\" class=\"ltx_tr\">\n<td id=\"S7.T6.4.5.5.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S7.T6.4.5.5.1.1\" class=\"ltx_text ltx_font_bold\">Fishr+Inter-Geo</span></td>\n<td id=\"S7.T6.4.5.5.2\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S7.T6.4.5.5.2.1\" class=\"ltx_text ltx_font_bold\">0.790±0.002</span></td>\n<td id=\"S7.T6.4.5.5.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.414±0.003</td>\n<td id=\"S7.T6.4.5.5.4\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\">0.907±0.003</td>\n</tr>\n<tr id=\"S7.T6.4.6.6\" class=\"ltx_tr\">\n<td id=\"S7.T6.4.6.6.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Fishr+Intra-Arith</td>\n<td id=\"S7.T6.4.6.6.2\" class=\"ltx_td ltx_align_left ltx_border_t\">0.783±0.005</td>\n<td id=\"S7.T6.4.6.6.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.414±0.003</td>\n<td id=\"S7.T6.4.6.6.4\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\"><span id=\"S7.T6.4.6.6.4.1\" class=\"ltx_text ltx_font_bold\">0.923±0.006</span></td>\n</tr>\n<tr id=\"S7.T6.4.7.7\" class=\"ltx_tr\">\n<td id=\"S7.T6.4.7.7.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span id=\"S7.T6.4.7.7.1.1\" class=\"ltx_text ltx_font_bold\">Fishr+Intra-Geo</span></td>\n<td id=\"S7.T6.4.7.7.2\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">0.779±0.002</td>\n<td id=\"S7.T6.4.7.7.3\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span id=\"S7.T6.4.7.7.3.1\" class=\"ltx_text ltx_font_bold\">0.419±0.004</span></td>\n<td id=\"S7.T6.4.7.7.4\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_t\">0.917±0.006</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We run each algorithm for five different seeds, then average their loss, the AUCROC, AUCPR, as well as the accuracy on the OOD test set when the OOD test loss is lowest, which are demonstrated in Table 2, Table 4 Table 6, and Figure 3. Further, we also conduct the inter-silo and intra-silo accuracy comparison among each sub environment on the Rotated-CIFAR10 dataset when the OOD test loss is lowest 5. Specifically, we also experiment with the model’s performance regarding different Fishr regularizer λ𝜆\\lambda on the eICU dataset 2. Moreover, we also plot the training loss, validation loss, and OOD test loss among different algorithms, which are demonstrated in Figure 5, Figure 6, and Figure 7 in the Appendix section. From the figures, our proposed methods, Fishr+Inter-Geo and Fishr+Intra-Geo, significantly outperform other methods in domain adaptation, including the baseline algorithm FedSGD and the state-of-the-art algorithm FedCurv."
        ]
    }
}