{
    "PAPER'S NUMBER OF TABLES": 3,
    "S3.T1": {
        "caption": "Table 1: Compared results before and after party 2 quits on CIFAR10.",
        "table": "<table id=\"S3.T1.3\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S3.T1.3.1\" class=\"ltx_tr\">\n<td id=\"S3.T1.3.1.1\" class=\"ltx_td ltx_border_r ltx_border_tt\"></td>\n<td id=\"S3.T1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.3.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Accuracy(%)</span></td>\n</tr>\n<tr id=\"S3.T1.3.2\" class=\"ltx_tr\">\n<td id=\"S3.T1.3.2.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S3.T1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Before party 2 quits</span></td>\n<td id=\"S3.T1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.3.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">74.53</span></td>\n</tr>\n<tr id=\"S3.T1.3.3\" class=\"ltx_tr\">\n<td id=\"S3.T1.3.3.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S3.T1.3.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">After party 2 quits</span></td>\n<td id=\"S3.T1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.3.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">51.24</span></td>\n</tr>\n<tr id=\"S3.T1.3.4\" class=\"ltx_tr\">\n<td id=\"S3.T1.3.4.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S3.T1.3.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Party 1 standalone</span></td>\n<td id=\"S3.T1.3.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S3.T1.3.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">62.84</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "During the deployment phase, some passive parties (say the ",
                "k",
                "𝑘",
                "k",
                "-th party) could quit unexpectedly due to a network crash or the termination of collaboration. Without the representations uploaded by party ",
                "k",
                "𝑘",
                "k",
                ", the active party can still conduct inference by setting ",
                "H",
                "i",
                "k",
                "subscript",
                "superscript",
                "𝐻",
                "𝑘",
                "𝑖",
                "H^{k}_{i}",
                " as a zero vector. However, there will be a substantial performance drop. We conduct two-party experiments on CIFAR10 to investigate this performance drop. We follow previous works ",
                "[",
                "24",
                ", ",
                "21",
                "]",
                " to split CIFAR10 images into two parts and assign them to the two parties using ResNet18 as backbone models. The active party (party 1) and passive party (party 2) collaborate to train the models. We evaluate and compare the inference accuracy before and after party 2 quits in the deployment phase. When party 2 quits, party 1 sets ",
                "H",
                "i",
                "2",
                "subscript",
                "superscript",
                "𝐻",
                "2",
                "𝑖",
                "H^{2}_{i}",
                " as a zero vector and conducts inference. Zero vectors are used because the passive party typically does not allow the active party to utilize its representations in any way (e.g., an average vector) after the termination of collaboration. We set the standalone results as a baseline, where the active party trains a model independently without ever collaborating with the passive party. The results are shown in ",
                "Tab.",
                " ",
                "1",
                ".",
                "The results show that the accuracy drops more than 20% after party 2 quits. Furthermore, the VFL model after party 2 quits achieves even lower accuracy than the model party 1 trained without any collaboration, undermining the motivation of VFL."
            ]
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Compared accuracy of the model on party 2 by conducting MC attack and collecting labels to train from scratch.",
        "table": "<table id=\"S3.T2.3\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S3.T2.3.1\" class=\"ltx_tr\">\n<td id=\"S3.T2.3.1.1\" class=\"ltx_td ltx_border_r ltx_border_tt\"></td>\n<td id=\"S3.T2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T2.3.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Accuracy(%)</span></td>\n</tr>\n<tr id=\"S3.T2.3.2\" class=\"ltx_tr\">\n<td id=\"S3.T2.3.2.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S3.T2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">MC attack (400 labels)</span></td>\n<td id=\"S3.T2.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.3.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">58.02</span></td>\n</tr>\n<tr id=\"S3.T2.3.3\" class=\"ltx_tr\">\n<td id=\"S3.T2.3.3.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S3.T2.3.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Train from scratch w. all the labels</span></td>\n<td id=\"S3.T2.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S3.T2.3.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">59.73</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "The collaborative training process enables passive parties to extract representations useful for the task of VFL, which is defined by and learned from the labels of the active party. Even after the collaboration ends, the passive parties will retain access to the representation extractors since they conduct the training and deployment of the extractors. These extractors allow the passive parties to fine-tune classifier heads with very few labeled data and conduct inference with decent accuracy after quitting the collaboration. Given the active party’s significant investment of effort and money in labeling the data, these extractors retained by the passive parties constitute costly IP leakage of these labels. To demonstrate the extent of IP leakage by the feature extractors of the passive parties, we follow the experimental setup in ",
                "Sec.",
                " ",
                "3.2",
                " and let party 2 conduct model completion (MC) attack ",
                "[",
                "10",
                "]",
                " to train a classifier using a small number of labeled samples. We report the test accuracy of the complete model of party 2 created by the MC attack. For comparison, we also assume party 2 annotates all the training data to train a model from scratch and we report the accuracy in ",
                "Tab.",
                " ",
                "2",
                ".",
                "By fine-tuning a classifier with the extractor, the passive party can achieve comparable accuracy using less than 1% of the labeled data compared to training a model from scratch with all the labels. This demonstrates that the label information from the active party is leaked and embedded in the passive party’s extractor."
            ]
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Results of Party-wise Dropout on CIFAR100.",
        "table": "<table id=\"S5.T3.5\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S5.T3.5.6\" class=\"ltx_tr\">\n<td id=\"S5.T3.5.6.1\" class=\"ltx_td ltx_border_r ltx_border_tt\"></td>\n<td id=\"S5.T3.5.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">\n<span id=\"S5.T3.5.6.2.1\" class=\"ltx_text\"></span><span id=\"S5.T3.5.6.2.2\" class=\"ltx_text\" style=\"font-size:90%;\"> </span><span id=\"S5.T3.5.6.2.3\" class=\"ltx_text\" style=\"font-size:90%;\">\n<span id=\"S5.T3.5.6.2.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T3.5.6.2.3.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T3.5.6.2.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Accuracy before</span></span>\n<span id=\"S5.T3.5.6.2.3.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T3.5.6.2.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">party 2 quits(%)</span></span>\n</span></span><span id=\"S5.T3.5.6.2.4\" class=\"ltx_text\"></span><span id=\"S5.T3.5.6.2.5\" class=\"ltx_text\" style=\"font-size:90%;\"></span>\n</td>\n<td id=\"S5.T3.5.6.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S5.T3.5.6.3.1\" class=\"ltx_text\"></span><span id=\"S5.T3.5.6.3.2\" class=\"ltx_text\" style=\"font-size:90%;\"> </span><span id=\"S5.T3.5.6.3.3\" class=\"ltx_text\" style=\"font-size:90%;\">\n<span id=\"S5.T3.5.6.3.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T3.5.6.3.3.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T3.5.6.3.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Accuracy after</span></span>\n<span id=\"S5.T3.5.6.3.3.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T3.5.6.3.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">party 2 quits(%)</span></span>\n</span></span><span id=\"S5.T3.5.6.3.4\" class=\"ltx_text\"></span><span id=\"S5.T3.5.6.3.5\" class=\"ltx_text\" style=\"font-size:90%;\"></span>\n</td>\n</tr>\n<tr id=\"S5.T3.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><math id=\"S5.T3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"p=0\" display=\"inline\"><semantics id=\"S5.T3.1.1.1.m1.1a\"><mrow id=\"S5.T3.1.1.1.m1.1.1\" xref=\"S5.T3.1.1.1.m1.1.1.cmml\"><mi mathsize=\"90%\" id=\"S5.T3.1.1.1.m1.1.1.2\" xref=\"S5.T3.1.1.1.m1.1.1.2.cmml\">p</mi><mo mathsize=\"90%\" id=\"S5.T3.1.1.1.m1.1.1.1\" xref=\"S5.T3.1.1.1.m1.1.1.1.cmml\">=</mo><mn mathsize=\"90%\" id=\"S5.T3.1.1.1.m1.1.1.3\" xref=\"S5.T3.1.1.1.m1.1.1.3.cmml\">0</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.1.1.1.m1.1b\"><apply id=\"S5.T3.1.1.1.m1.1.1.cmml\" xref=\"S5.T3.1.1.1.m1.1.1\"><eq id=\"S5.T3.1.1.1.m1.1.1.1.cmml\" xref=\"S5.T3.1.1.1.m1.1.1.1\"></eq><ci id=\"S5.T3.1.1.1.m1.1.1.2.cmml\" xref=\"S5.T3.1.1.1.m1.1.1.2\">𝑝</ci><cn type=\"integer\" id=\"S5.T3.1.1.1.m1.1.1.3.cmml\" xref=\"S5.T3.1.1.1.m1.1.1.3\">0</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.1.1.1.m1.1c\">p=0</annotation></semantics></math></td>\n<td id=\"S5.T3.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">44.95</span></td>\n<td id=\"S5.T3.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">26.65</span></td>\n</tr>\n<tr id=\"S5.T3.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<math id=\"S5.T3.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"p=0\" display=\"inline\"><semantics id=\"S5.T3.2.2.1.m1.1a\"><mrow id=\"S5.T3.2.2.1.m1.1.1\" xref=\"S5.T3.2.2.1.m1.1.1.cmml\"><mi mathsize=\"90%\" id=\"S5.T3.2.2.1.m1.1.1.2\" xref=\"S5.T3.2.2.1.m1.1.1.2.cmml\">p</mi><mo mathsize=\"90%\" id=\"S5.T3.2.2.1.m1.1.1.1\" xref=\"S5.T3.2.2.1.m1.1.1.1.cmml\">=</mo><mn mathsize=\"90%\" id=\"S5.T3.2.2.1.m1.1.1.3\" xref=\"S5.T3.2.2.1.m1.1.1.3.cmml\">0</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.2.2.1.m1.1b\"><apply id=\"S5.T3.2.2.1.m1.1.1.cmml\" xref=\"S5.T3.2.2.1.m1.1.1\"><eq id=\"S5.T3.2.2.1.m1.1.1.1.cmml\" xref=\"S5.T3.2.2.1.m1.1.1.1\"></eq><ci id=\"S5.T3.2.2.1.m1.1.1.2.cmml\" xref=\"S5.T3.2.2.1.m1.1.1.2\">𝑝</ci><cn type=\"integer\" id=\"S5.T3.2.2.1.m1.1.1.3.cmml\" xref=\"S5.T3.2.2.1.m1.1.1.3\">0</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.2.2.1.m1.1c\">p=0</annotation></semantics></math><span id=\"S5.T3.2.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">.05</span>\n</td>\n<td id=\"S5.T3.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.2.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">44.58</span></td>\n<td id=\"S5.T3.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.2.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">32.01</span></td>\n</tr>\n<tr id=\"S5.T3.3.3\" class=\"ltx_tr\">\n<td id=\"S5.T3.3.3.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<math id=\"S5.T3.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"p=0\" display=\"inline\"><semantics id=\"S5.T3.3.3.1.m1.1a\"><mrow id=\"S5.T3.3.3.1.m1.1.1\" xref=\"S5.T3.3.3.1.m1.1.1.cmml\"><mi mathsize=\"90%\" id=\"S5.T3.3.3.1.m1.1.1.2\" xref=\"S5.T3.3.3.1.m1.1.1.2.cmml\">p</mi><mo mathsize=\"90%\" id=\"S5.T3.3.3.1.m1.1.1.1\" xref=\"S5.T3.3.3.1.m1.1.1.1.cmml\">=</mo><mn mathsize=\"90%\" id=\"S5.T3.3.3.1.m1.1.1.3\" xref=\"S5.T3.3.3.1.m1.1.1.3.cmml\">0</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.3.3.1.m1.1b\"><apply id=\"S5.T3.3.3.1.m1.1.1.cmml\" xref=\"S5.T3.3.3.1.m1.1.1\"><eq id=\"S5.T3.3.3.1.m1.1.1.1.cmml\" xref=\"S5.T3.3.3.1.m1.1.1.1\"></eq><ci id=\"S5.T3.3.3.1.m1.1.1.2.cmml\" xref=\"S5.T3.3.3.1.m1.1.1.2\">𝑝</ci><cn type=\"integer\" id=\"S5.T3.3.3.1.m1.1.1.3.cmml\" xref=\"S5.T3.3.3.1.m1.1.1.3\">0</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.3.3.1.m1.1c\">p=0</annotation></semantics></math><span id=\"S5.T3.3.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">.1</span>\n</td>\n<td id=\"S5.T3.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">44.29</span></td>\n<td id=\"S5.T3.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.3.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">32.03</span></td>\n</tr>\n<tr id=\"S5.T3.4.4\" class=\"ltx_tr\">\n<td id=\"S5.T3.4.4.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<math id=\"S5.T3.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"p=0\" display=\"inline\"><semantics id=\"S5.T3.4.4.1.m1.1a\"><mrow id=\"S5.T3.4.4.1.m1.1.1\" xref=\"S5.T3.4.4.1.m1.1.1.cmml\"><mi mathsize=\"90%\" id=\"S5.T3.4.4.1.m1.1.1.2\" xref=\"S5.T3.4.4.1.m1.1.1.2.cmml\">p</mi><mo mathsize=\"90%\" id=\"S5.T3.4.4.1.m1.1.1.1\" xref=\"S5.T3.4.4.1.m1.1.1.1.cmml\">=</mo><mn mathsize=\"90%\" id=\"S5.T3.4.4.1.m1.1.1.3\" xref=\"S5.T3.4.4.1.m1.1.1.3.cmml\">0</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.4.4.1.m1.1b\"><apply id=\"S5.T3.4.4.1.m1.1.1.cmml\" xref=\"S5.T3.4.4.1.m1.1.1\"><eq id=\"S5.T3.4.4.1.m1.1.1.1.cmml\" xref=\"S5.T3.4.4.1.m1.1.1.1\"></eq><ci id=\"S5.T3.4.4.1.m1.1.1.2.cmml\" xref=\"S5.T3.4.4.1.m1.1.1.2\">𝑝</ci><cn type=\"integer\" id=\"S5.T3.4.4.1.m1.1.1.3.cmml\" xref=\"S5.T3.4.4.1.m1.1.1.3\">0</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.4.4.1.m1.1c\">p=0</annotation></semantics></math><span id=\"S5.T3.4.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">.3</span>\n</td>\n<td id=\"S5.T3.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.4.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">42.11</span></td>\n<td id=\"S5.T3.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.4.4.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">33.05</span></td>\n</tr>\n<tr id=\"S5.T3.5.5\" class=\"ltx_tr\">\n<td id=\"S5.T3.5.5.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<math id=\"S5.T3.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"p=0\" display=\"inline\"><semantics id=\"S5.T3.5.5.1.m1.1a\"><mrow id=\"S5.T3.5.5.1.m1.1.1\" xref=\"S5.T3.5.5.1.m1.1.1.cmml\"><mi mathsize=\"90%\" id=\"S5.T3.5.5.1.m1.1.1.2\" xref=\"S5.T3.5.5.1.m1.1.1.2.cmml\">p</mi><mo mathsize=\"90%\" id=\"S5.T3.5.5.1.m1.1.1.1\" xref=\"S5.T3.5.5.1.m1.1.1.1.cmml\">=</mo><mn mathsize=\"90%\" id=\"S5.T3.5.5.1.m1.1.1.3\" xref=\"S5.T3.5.5.1.m1.1.1.3.cmml\">0</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.5.5.1.m1.1b\"><apply id=\"S5.T3.5.5.1.m1.1.1.cmml\" xref=\"S5.T3.5.5.1.m1.1.1\"><eq id=\"S5.T3.5.5.1.m1.1.1.1.cmml\" xref=\"S5.T3.5.5.1.m1.1.1.1\"></eq><ci id=\"S5.T3.5.5.1.m1.1.1.2.cmml\" xref=\"S5.T3.5.5.1.m1.1.1.2\">𝑝</ci><cn type=\"integer\" id=\"S5.T3.5.5.1.m1.1.1.3.cmml\" xref=\"S5.T3.5.5.1.m1.1.1.3\">0</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.5.5.1.m1.1c\">p=0</annotation></semantics></math><span id=\"S5.T3.5.5.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">.5</span>\n</td>\n<td id=\"S5.T3.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.5.5.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">40.29</span></td>\n<td id=\"S5.T3.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.5.5.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">33.85</span></td>\n</tr>\n<tr id=\"S5.T3.5.7\" class=\"ltx_tr\">\n<td id=\"S5.T3.5.7.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<span id=\"S5.T3.5.7.1.1\" class=\"ltx_text\"></span><span id=\"S5.T3.5.7.1.2\" class=\"ltx_text\" style=\"font-size:90%;\">\n<span id=\"S5.T3.5.7.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T3.5.7.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T3.5.7.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Standalone</span></span>\n</span></span><span id=\"S5.T3.5.7.1.3\" class=\"ltx_text\"></span><span id=\"S5.T3.5.7.1.4\" class=\"ltx_text\" style=\"font-size:90%;\"></span>\n</td>\n<td id=\"S5.T3.5.7.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.5.7.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">N/A</span></td>\n<td id=\"S5.T3.5.7.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.5.7.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">34.02</span></td>\n</tr>\n<tr id=\"S5.T3.5.8\" class=\"ltx_tr\">\n<td id=\"S5.T3.5.8.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S5.T3.5.8.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Multi-head training</span></td>\n<td id=\"S5.T3.5.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S5.T3.5.8.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">39.17</span></td>\n<td id=\"S5.T3.5.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S5.T3.5.8.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">32.72</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "For simplicity, we use ",
                "p",
                "𝑝",
                "p",
                " to denote ",
                "p",
                "2",
                "superscript",
                "𝑝",
                "2",
                "p^{2}",
                " in ",
                "Eq.",
                " ",
                "2",
                " in our two-party experiments. We set ",
                "p",
                "𝑝",
                "p",
                " from 0 to 0.5 to simulate the settings that the passive party has different levels of reliability. We evaluate the trade-off between the accuracy before and after the passive party quits in the deployment phase. The results of CIFAR10 are shown in ",
                "Fig.",
                " ",
                "3",
                ", and the results of CIFAR100 are shown in ",
                "Tab.",
                " ",
                "3",
                ". The upper bound of the test accuracy after party 2 quits is the accuracy of the model that party 1 trains independently (standalone). For CIFAR10, it is shown that applying Party-wise Dropout can improve the accuracy after party 2 quits by more than 7% with nearly no accuracy drop before party 2 quits. By applying Party-wise Dropout, the active party can achieve nearly the same accuracy as retraining a model locally after the passive party quits by sacrificing less than 1.5% accuracy before the passive party quits. Multi-head training can also mitigate the accuracy drop after party 2 quits. However, it cannot achieve a better trade-off than ours since our method is equivalent to multi-objective training. Notably, Multi-head training requires the active party to train ",
                "2",
                "K",
                "−",
                "1",
                "superscript",
                "2",
                "𝐾",
                "1",
                "2^{K-1}",
                " head models according to ",
                "Eq.",
                " ",
                "2",
                ", which introduces significant computational overhead increasing rapidly with a larger ",
                "K",
                "𝐾",
                "K",
                ".",
                "For CIFAR100, Party-wise Dropout improves the accuracy after party 2 quits by more than 5.5% with less than 0.5% accuracy drop before party 2 quits. Multi-head training mitigates the accuracy drop after party 2 quits, but it does not achieve a better trade-off than our method. It is shown that applying Party-wise Dropout by just setting a relatively small ",
                "p",
                "𝑝",
                "p",
                " value can significantly improve the robustness of VFL against unexpected quitting, which shows the effectiveness of Party-wise Dropout in solving the problem of party-wise neuron co-adaptation.",
                "A naïve solution for mitigating the accuracy drop is to fine-tune the head model after a passive party quits. However, this process is time-consuming, and the service provider cannot afford to shut down the service while fine-tuning. Therefore, achieving a decent accuracy before fine-tuning is crucial. In addition, the Party-wise Dropout complements fine-tuning and can reduce computation and communication cost by starting fine-tuning from a better point."
            ]
        ]
    }
}