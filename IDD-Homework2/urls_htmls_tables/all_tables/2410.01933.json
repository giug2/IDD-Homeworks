{
    "id_table_1": {
        "caption": "Table 2:  Augmentation performances of different synthetic tabular data generation models. All tasks are binary classification tasks, and weighted F1, accuracy (Acc), and ROC AUC (AUC) scores are calculated. Best performances are in bold.",
        "table": "S4.T1.1.1",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "In this paper, we propose a novel GAN architecture for tabular model generation: Tabular Auto-Encoder Generative Adversarial Network (TAEGAN). TAEGAN outperforms all existing deep learning models 1 1 1 In this paper, we refer to non-LLM neural networks as deep learning methods. , including all GANs, on classical synthetic data metrics. It also achieves the best data augmentation effect among all models, including not only deep learning ones, but also LLM-based and tree-based ones. TAEGAN essentially changes the generator of GAN into a masked auto-encoder (see Figure  1 ). By incorporating flexible masks and masked data as conditions, we eliminate the need for a classifier network, which is typically required in GANs for tabular data with a target column  (Park et al.,  2018 ; Zhao et al.,  2021 ;  2024 ) . This approach allows the generator to learn better inter-feature relationships. TAEGAN generates data with the highest quality among all deep-learning methods in 9 out of 10 test datasets, and best augments the downstream classification tasks in 7 out of 8 smaller datasets.",
            "To maximize the capability of the auto-encoder to learn data relations, we can vary the masks not only in terms of the dimensions it masks out, but also the number of dimensions it masks out. The auto-encoder auxiliary network can thus be understood as a network with partial data known and full data to be recovered. This coincides somehow with the generator when the GAN is designed to be conditional. Using a conditional generator for tabular data generation in GAN is first introduced in CTGAN  (Xu et al.,  2019 )  and has been empirically shown to be highly effective. This conditional generator takes in partially known data, which is one randomly selected categorical column, and generates the full raw data. The only difference between this conditional generator and the proposed auto-encoder is the number of dimensions masked. Therefore, it makes sense to combine the auto-encoder and conditional generator into the same network. The mask indicator and the masked data put together can be considered equivalent to the conditional vector of the generator of previous works. Noise is still necessary for the generator, but to ensure the encoder learns the tabular data without interference, the noise is concatenated to the encoders output and used as input for the decoder. The discriminator can keep as it is. The resulting network structure can be found in Figure  1 .",
            "We base our work on the state-of-the-art GAN model on tabular data, CTAB-GAN+  (Zhao et al.,  2024 ) . We swapped the main architecture of CTAB-GAN+, which is CNN-based GAN, DCGAN  (Radford et al.,  2016 ) , to MLPs because we focus on small datasets on this table, for which MLPs are considered sufficient. There are actually some other arguments against DCGAN-based structure for synthetic tabular data generation, which is elaborated in Appendix  A.1 . CombU  (Li et al.,  2024 )  has been shown to be an effective activation function for tabular data generation. Therefore, we replace all activation layers, except for the output layer, with CombU.",
            "There are three major tasks during the training of TAEGAN: generation, discrimination, as in any GAN, and reconstruction, which is defined for the auto-encoder. Discrimination task requires gradients only on the discriminator network,  D D D italic_D , while the reconstruction task requires gradients on the generator, i.e., the auto-encoder network,  G G G italic_G . Hence, the two networks may be pre-trained jointly before the adversarial generation starts. Then, the adversarial generation step can be added for the main training stage. The pre-training allows both the generator and discriminator networks to get some knowledge of the data, so that the adversarial training of GAN can be more stable. Pseudo-code for the process can be found in Algorithm  1 .",
            "Data augmentation performance is evaluated through machine learning tasks, where the training set consists of both the original training data and synthetic data generated by the trained generator, with the synthetic data matching the size of the original training set. The test set is always composed of real data and remains untouched by the generator to prevent data leakage. The machine learning model used is XGBoost  (Chen & Guestrin,  2016 ) . Experiments are run on 8 commonly used benchmark tabular datasets on OpenML  (Vanschoren et al.,  2013 )  that have less than 2k rows, which we interpret as datasets with a need for augmentation. Dataset is summarized in Table  1 . More details can be found in Appendix  B.1 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 3:  Data quality by machine learning performance of train-on-synthetic-test-on-real strategy. All tasks are binary classification tasks except for CV, which is multi-class classification, and weighted F1, accuracy (Acc), and ROC AUC (AUC) scores (OVR for multi-class) are calculated. Best performances among all models and among deep learning methods are both in bold.",
        "table": "S4.T2.144.144",
        "footnotes": [],
        "references": [
            "To understand TAEGAN, it is important to understand the conditional vector and training-by-sampling mechanism proposed by  Xu et al. ( 2019 )  for CTGAN, which is inherited by subsequent works  (Zhao et al.,  2021 ;  2024 ) . The generator is conditional, that takes in not only noise, but also a conditional vector    \\bm{\\gamma} bold_italic_  with  D  =  n = 1 N D d  n subscript D  superscript subscript n 1 N subscript D d n D_{\\gamma}=\\sum_{n=1}^{N}D_{dn} italic_D start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT =  start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_D start_POSTSUBSCRIPT italic_d italic_n end_POSTSUBSCRIPT  dimensions. The conditional vector is dependent on a mask indicator, represented as the binary vector  m = m 1  m 2    m N m direct-sum subscript m 1 subscript m 2  subscript m N \\mathbf{m}=\\mathbf{m}_{1}\\oplus\\mathbf{m}_{2}\\oplus\\dots\\oplus\\mathbf{m}_{N} bold_m = bold_m start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  bold_m start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT    bold_m start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT , where  | m n | subscript m n |\\mathbf{m}_{n}| | bold_m start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT |  is 1 if  n n n italic_n -th column is categorical, and 2 if it is numeric. We call each individual dimension of  m m \\mathbf{m} bold_m  a component, so each column has either 1 or 2 components. Let  D m = | m | subscript D m m D_{m}=|\\mathbf{m}| italic_D start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT = | bold_m |  be the total number of components. The mask indicator can be expanded by the dimensions to create the data masks to be another binary vector   =  d  1   c  1   d  2   c  2     d  N   c  N  direct-sum subscript  d 1 subscript  c 1 subscript  d 2 subscript  c 2  subscript  d N subscript  c N \\bm{\\mu}=\\bm{\\mu}_{d1}\\oplus\\bm{\\mu}_{c1}\\oplus\\bm{\\mu}_{d2}\\oplus\\bm{\\mu}_{c2% }\\oplus\\dots\\oplus\\bm{\\mu}_{dN}\\oplus\\bm{\\mu}_{cN} bold_italic_ = bold_italic_ start_POSTSUBSCRIPT italic_d 1 end_POSTSUBSCRIPT  bold_italic_ start_POSTSUBSCRIPT italic_c 1 end_POSTSUBSCRIPT  bold_italic_ start_POSTSUBSCRIPT italic_d 2 end_POSTSUBSCRIPT  bold_italic_ start_POSTSUBSCRIPT italic_c 2 end_POSTSUBSCRIPT    bold_italic_ start_POSTSUBSCRIPT italic_d italic_N end_POSTSUBSCRIPT  bold_italic_ start_POSTSUBSCRIPT italic_c italic_N end_POSTSUBSCRIPT , where  |  d  n | = D d  n , |  c  n | = D c  n formulae-sequence subscript  d n subscript D d n subscript  c n subscript D c n |\\bm{\\mu}_{dn}|=D_{dn},|\\bm{\\mu}_{cn}|=D_{cn} | bold_italic_ start_POSTSUBSCRIPT italic_d italic_n end_POSTSUBSCRIPT | = italic_D start_POSTSUBSCRIPT italic_d italic_n end_POSTSUBSCRIPT , | bold_italic_ start_POSTSUBSCRIPT italic_c italic_n end_POSTSUBSCRIPT | = italic_D start_POSTSUBSCRIPT italic_c italic_n end_POSTSUBSCRIPT .   d  n ,  c  n subscript  d n subscript  c n \\bm{\\mu}_{dn},\\bm{\\mu}_{cn} bold_italic_ start_POSTSUBSCRIPT italic_d italic_n end_POSTSUBSCRIPT , bold_italic_ start_POSTSUBSCRIPT italic_c italic_n end_POSTSUBSCRIPT  are always vectors with only one unique value for any  n n n italic_n , and this value corresponds to the value in  m n subscript m n \\mathbf{m}_{n} bold_m start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT . For CTAB-GAN+  (Zhao et al.,  2024 ) , the conditional vector must satisfy the constraint that in  m m \\mathbf{m} bold_m , only one dimension can be 1 (the rest are 0, so   m  1 = 1 subscript norm m 1 1 \\|\\mathbf{m}\\|_{1}=1  bold_m  start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 1 ), and it must correspond to a discrete component. Then, the mask  m m \\mathbf{m} bold_m , and hence    \\bm{\\mu} bold_italic_ , generates the conditional vector   =  T  x   superscript  T x \\bm{\\gamma}=\\bm{\\mu}^{T}\\cdot\\mathbf{x} bold_italic_ = bold_italic_ start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT  bold_x . Figure  2  shows an example with the notions mentioned previously summarized.",
            "The data and mask sampling for generation is not based on logarithm-transformed frequency. It is essentially  W W \\mathcal{W} caligraphic_W  with all values being  1 / M 1 M 1/M 1 / italic_M . Now that the mask is arbitrary as long as non-empty, a single row can be sampled in multiple steps, or in the extreme case, one component after one. Although each time all dimensions are predicted, we select only one additional and proceed to a next iteration. The full process is described in Algorithm  2 .",
            "We also evaluate the performance of synthetic data quality by classical machine learning efficacy based on train-on-synthetic-test-on-real framework. This metric is run on the 8 small datasets for data augmentation and 2 additional larger datasets. We compare the performance of TAEGAN with the state-of-the-art public 2 2 2 Public by open-source code available. So, for example, although TabMT  (Gulati & Roysdon,  2023 )  claims to be better than REaLTabFormer  (Solatorio & Dupriez,  2023 ) , it is not used.  version for various types of models, ARF  (Watson et al.,  2023 )  for non-neural-network method, CTAB-GAN+  (Zhao et al.,  2024 )  for GAN  (Goodfellow et al.,  2014 ) , TVAE  (Xu et al.,  2019 )  for VAE  (Kingma & Welling,  2013 ) , TabDDPM  (Kotelnikov et al.,  2023 )  for Diffusion model  (Ho et al.,  2020 ) , and REaLTabFormer  (Solatorio & Dupriez,  2023 )  for large language models (LLMs). Implementation details of all models, including TAEGAN, are found in Appendix  B.2 .",
            "Table  2  summarizes the augmentation effects for all synthetic data models. Not all models always guarantee the existence of data augmentation effect. However, TAEGAN indeed augmented data to improve the machine learning performance in all experimented datasets, and achieves the best data augmentation effect on 7 out of 8 of them compared to other synthetic data generation models."
        ]
    },
    "id_table_3": {
        "caption": "Table 4:  ML efficacy performances across different datasets for ablation study. Each ablation experiments description has the described change only, and the content in bracket means the actual setting in TAEGAN for easier reference. The reported scores are weighted F1 for XGBoost classification.",
        "table": "S4.T3.180.180",
        "footnotes": [],
        "references": [
            "Proposed a novel approach for score normalization and the aggregation of multiple metrics to provide a more comprehensive and concise interpretation of data quality (see Appendix  B.3 ).",
            "Then, in order to counter the imbalance of data, the training-by-sampling method is proposed such that during training, whenever a new sample is needed, the sample is selected by firstly randomly select  n  { 1 , 2 , ... , N } n 1 2 ... N n\\in\\{1,2,\\dots,N\\} italic_n  { 1 , 2 , ... , italic_N } , so that  m n subscript m n \\mathbf{m}_{n} bold_m start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT  is selected to have its categorical component as 1. Based on the probability mass function constructed by normalized logarithm-transformed smoothed-by-1 frequency of each value in this categorical component, randomly sample a value for this component. A row in  T T \\mathcal{T} caligraphic_T  with this value in the selected component is thus sampled. During sampling, this same process is done but without logarithm-transformation and smoothing. Figure  3(a)  shows an example.",
            "By making   m  1 = 1 subscript norm m 1 1 \\|\\mathbf{m}\\|_{1}=1  bold_m  start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 1  no longer an invariant, the column-then-value sampling method of selecting    \\bm{\\gamma} bold_italic_  becomes inapplicable. However, the same idea can be maintained if we describe the method in another way. For each discrete component, a weight vector  w d  n subscript w d n \\mathbf{w}_{dn} bold_w start_POSTSUBSCRIPT italic_d italic_n end_POSTSUBSCRIPT  can be constructed by the following process, also seen in Figure  3(b) :",
            "Training-by-sampling is essentially randomly picking a discrete component and picking a row based on its corresponding weight vector. In TAEGAN, we propose a novel strategy  multivariate training-by-sampling. We relax the constraint that picked component must be discrete, by binning the continuous component so that they become discrete too. By putting the weight vectors of all components together, we can thus create a weight matrix  W  [ 0 , 1 ] M  D m W superscript 0 1 M subscript D m \\mathcal{W}\\in[0,1]^{M\\times D_{m}} caligraphic_W  [ 0 , 1 ] start_POSTSUPERSCRIPT italic_M  italic_D start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT end_POSTSUPERSCRIPT . Instead of randomly selecting one component to be the weight for row selection, TAEGAN calculates the mean over selected components in the mask to be the weight for row selection. Thus we extend training-by-sampling to multiple components. Nevertheless, we will ignore the interaction between components for the sampling process, as it is aimed at countering the column imbalance during model training. The overall multivariate training-by-sampling process is summarized in Figure  3(c) .",
            "TAEGAN also achieves the best performance over a variety of datasets and metrics among all deep learning methods, but is not always the best outside this group. Table  3  gives the result of machine learning efficacy performances.",
            "In Section  3.4 , we also mentioned that language models sampling process is slow, so that repeated sampling by gradually removing the masks takes a long time. We show the sampling time on the two larger datasets in Table  5 . TAEGAN samples values for each column one at a time and gradually removes the mask, and REaLTabFormer samples all values at once. The sampling time of it multiplied by the total number of columns are also shown in the table, and its inefficiency is clearly demonstrated compared against TAEGAN. This inefficiency problem would be particularly a problem with REaLTabFormer with iterative masked generation when the number of columns are large. The number of columns not only result in an O(#C) additional sampling time, but also explodes the sequence lengths, which usually requires the model size to increase in O(#C 2 ) due to the existence of attention layers in transformers  (Vaswani et al.,  2017 ) .",
            "Table  9  gives the discrimination scores for the 10 datasets. LLM-based REaLTabFormer shows a clear advantage in this metric, especially on large datasets. ARF also performs quite good, which is also expected because ARF is tuned based on a random-forest discriminator, which is expected to perform better than MLP discriminators in GANs. Among all deep learning models, GANs perform better in this metric, and TAEGAN performs generally better than CTAB-GAN+. In other words, the conclusion made in Section  4.3  still holds for discrimination metric."
        ]
    },
    "id_table_4": {
        "caption": "Table 5:  Sampling time (unit in second) of TAEGAN versus REaLTabFormer on larger datasets. REaLTabFormer   \\times   #C means the the time for REaLTabFormer multiplied by the total number of columns in the dataset, showing the estimated time when it is also generated iteratively with masks removed gradually.",
        "table": "S4.T4.18.18",
        "footnotes": [],
        "references": [
            "The results show TAEGANs superiority among deep learning methods. However, among all models, LLM-based REaLTabFormer  (Solatorio & Dupriez,  2023 )  tends to be better, especially on larger datasets. This is unsurprising as LLMs naturally learn better with abundance of data, and LLMs has a few tens of the number of parameters than deep learning methods. REaLTabFormer for these datasets typically constructs a neural network with about 40M parameters, while TAEGANs network has only 1-2M parameters. Also, both ARF and REaLTabFormer suffer from privacy concerns due to a potential risk of reproducing data records that are identical to real data (details see Appendix  A ). The results of some other metrics can be found in Appendix  B.4 .",
            "For ablation study, we focus on data quality by machine learning efficacy, and base our experiments on three more commonly used benchmark datasets: AD, CR, and DI. The results are shown in Table  4 . The experiments verify the effect of all optimizations introduced in TAEGAN.",
            "In Section  3.4 , we also mentioned that language models sampling process is slow, so that repeated sampling by gradually removing the masks takes a long time. We show the sampling time on the two larger datasets in Table  5 . TAEGAN samples values for each column one at a time and gradually removes the mask, and REaLTabFormer samples all values at once. The sampling time of it multiplied by the total number of columns are also shown in the table, and its inefficiency is clearly demonstrated compared against TAEGAN. This inefficiency problem would be particularly a problem with REaLTabFormer with iterative masked generation when the number of columns are large. The number of columns not only result in an O(#C) additional sampling time, but also explodes the sequence lengths, which usually requires the model size to increase in O(#C 2 ) due to the existence of attention layers in transformers  (Vaswani et al.,  2017 ) .",
            "Table  9  gives the discrimination scores for the 10 datasets. LLM-based REaLTabFormer shows a clear advantage in this metric, especially on large datasets. ARF also performs quite good, which is also expected because ARF is tuned based on a random-forest discriminator, which is expected to perform better than MLP discriminators in GANs. Among all deep learning models, GANs perform better in this metric, and TAEGAN performs generally better than CTAB-GAN+. In other words, the conclusion made in Section  4.3  still holds for discrimination metric."
        ]
    },
    "id_table_5": {
        "caption": "Table 6:  Details of datasets used for evaluation. Size means the set of each split. #C and #N refers to the number of categorical and numeric columns, inclusive of target, respectively. Aug? means whether the dataset is used for augmentation experiments, Ab? means whether the dataset is used for ablation experiments, and an X means that the dataset is.",
        "table": "A1.T5.5",
        "footnotes": [],
        "references": [
            "This approach makes the generation of each component in a row sequential, which indeed slows down the overall sampling speed. Fortunately, due to the small model size, the inference speed of TAEGAN is still much faster than transformer-based methods such as REaLTabFormer  (Solatorio & Dupriez,  2023 ) . A detailed discussion is provided in Appendix  5  due to space constraints.",
            "In Section  3.4 , we also mentioned that language models sampling process is slow, so that repeated sampling by gradually removing the masks takes a long time. We show the sampling time on the two larger datasets in Table  5 . TAEGAN samples values for each column one at a time and gradually removes the mask, and REaLTabFormer samples all values at once. The sampling time of it multiplied by the total number of columns are also shown in the table, and its inefficiency is clearly demonstrated compared against TAEGAN. This inefficiency problem would be particularly a problem with REaLTabFormer with iterative masked generation when the number of columns are large. The number of columns not only result in an O(#C) additional sampling time, but also explodes the sequence lengths, which usually requires the model size to increase in O(#C 2 ) due to the existence of attention layers in transformers  (Vaswani et al.,  2017 ) ."
        ]
    },
    "id_table_6": {
        "caption": "Table 7:  Normalized marginal distribution of each model. D stands for discrete, and C stands for continuous.",
        "table": "A2.T6.1",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "All datasets are obtained from OpenML  (Vanschoren et al.,  2013 )  by  sklearn.datasets.from_openml(HANDLE) . All datasets are randomly split into  1 : 1 : 1 : 1 1 : 1 1:1:1 1 : 1 : 1  for training, validation, and testing sets. Table  6  summarizes the information of all datasets used. All datasets are binary classification sets except for CV, which is multi-class with 7 classes."
        ]
    },
    "id_table_7": {
        "caption": "Table 8:  Normalized correlation scores of each model. D stands for discrete, and C stands for continuous, so DD stands for the correlation between discrete columns, DC stands for the correlation between discrete and continuous columns, and CC stands for the correlation between continuous columns.",
        "table": "A2.T7.120.120",
        "footnotes": [],
        "references": [
            "Table  7 - 8  gives the marginal distribution scores and correlation scores for the 10 datasets. Almost all models are able to capture marginal distribution and correlation relatively good for all datasets. ARF and REaLTabFormer are particularly good in marginal distribution, which is unsurprising as ARF keeps the exact distribution in all tree nodes, and REaLTabFormer uses the sampling method from transformers. GANs are of the next tier, still having almost all scores over 0.95, and TVAE and TabDDPM are slightly worse, yet still having all scores over 0.9."
        ]
    },
    "id_table_8": {
        "caption": "Table 9:  Normalized discrimination scores of each model. The best scores of all models, and of deep learning models are both in bold.",
        "table": "A2.T8.144.144",
        "footnotes": [],
        "references": [
            "Table  7 - 8  gives the marginal distribution scores and correlation scores for the 10 datasets. Almost all models are able to capture marginal distribution and correlation relatively good for all datasets. ARF and REaLTabFormer are particularly good in marginal distribution, which is unsurprising as ARF keeps the exact distribution in all tree nodes, and REaLTabFormer uses the sampling method from transformers. GANs are of the next tier, still having almost all scores over 0.95, and TVAE and TabDDPM are slightly worse, yet still having all scores over 0.9."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "A2.T9.180.180",
        "footnotes": [],
        "references": [
            "Table  9  gives the discrimination scores for the 10 datasets. LLM-based REaLTabFormer shows a clear advantage in this metric, especially on large datasets. ARF also performs quite good, which is also expected because ARF is tuned based on a random-forest discriminator, which is expected to perform better than MLP discriminators in GANs. Among all deep learning models, GANs perform better in this metric, and TAEGAN performs generally better than CTAB-GAN+. In other words, the conclusion made in Section  4.3  still holds for discrimination metric."
        ]
    }
}