{
    "id_table_1": {
        "caption": "TABLE I:  Resuls on aRTF dataset.",
        "table": "S4.T1.2",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "Garments, as one of the most ubiquitous items in home environments, have long been a focal point in assistive robotics research  [ 1 ,  2 ] . Tasks such as washing, folding, and ironing garments exemplify how robots can assist with everyday household activities. However, despite advancements in robotic vision and manipulation technologies, accurately recognizing and manipulating garments remains a challenge due to their diverse shapes and deformability  [ 3 ,  4 ] . Robots must not only recognize keypoints on garments but also adapt to the constantly changing states of these flexible objects to perform precise manipulations. This adaptability is essential for ensuring reliable performance in complex real-world scenarios.",
            "To address these limitations, we propose state-aware paired keypoint formation, which generates  S tate-aware  K eypoint  T rajectories for vision-language models (SKT). By utilizing the unified paired keypoints trajectory formulation, our proposed method generalizes well to various garment environments, such as flat, folded, and deformed garments (Fig.  1 ). Furthermore, by harnessing the combined power of vision and language, our approach enables robots to interpret visual cues alongside textual descriptions of garment parts and manipulation tasks. the system to go beyond traditional 3D data, providing a more holistic understanding of the garments current state and its corresponding keypoints. This integration enables the system to transcend traditional 3D data, offering a more holistic understanding of the garments current state and its corresponding keypoints. Finally, the use of VLMs enables the robot to dynamically adapt to various garment states, as the model can process both visual features and language-based queries related to the manipulation tasks at hand.",
            "Robotic manipulation of garments is a critical challenge in assistive robotics due to the deformable and highly variable nature of clothing items  [ 8 ,  9 ,  1 ,  10 ,  6 ] . Much of the existing research has focused on key tasks such as unfolding (flattening) and folding garments. While unfolding systems have made notable progress  [ 2 ,  11 ,  6 ] , they often leave garments imperfectly flattened, and their ability to handle a wide variety of clothing types and environmental conditions remains limited.",
            "Traditionally, folding tasks for flattened garments rely on predefined state representations and scripted policies  [ 2 ,  12 ] . Researchers have explored various approaches for generating these state representations, including template fitting  [ 1 ,  2 ]  and semantic keypoint detection  [ 13 ,  6 ,  14 ,  15 ] . Many of these methods leverage depth images  [ 14 ,  16 ,  15 ] , which are advantageous for capturing geometric data unaffected by lighting or background clutter. However, depth images can omit critical visual information such as garment patterns and seams, which can be important for accurate manipulation  [ 16 ] . To address these limitations, our work utilizes RGB images, which offer richer visual information that can be crucial for precise garment manipulation.",
            "The use of synthetic data has become increasingly prevalent in robotic cloth manipulation, particularly for training models that need to generalize across diverse garment configurations and manipulation tasks  [ 17 ,  14 ,  18 ,  2 ,  5 ] . However, creating high-quality 3D assets that represent a wide variety of garments and states remains a significant challenge. Some approaches rely on limited sets of manually annotated pre-made cloth meshes  [ 14 ,  2 ,  19 ] , while others use procedural generation techniques to produce single-layer meshes  [ 17 ,  5 ] . Although large-scale datasets like Cloth3D  [ 4 ]  have been developed, they often lack detailed semantic annotations required for precise manipulation tasks.",
            "Dense object descriptors, which capture point- or pixel-level object representations, have been widely applied to various robotic manipulation tasks  [ 20 ] . These descriptors have been extended in numerous works to propose grasping poses  [ 21 ,  22 ] , manipulate deformable objects like ropes  [ 23 ] , and smooth fabrics  [ 24 ] . Additionally, point-level affordance learning has been explored for articulated objects  [ 25 ,  26 ] , deformable objects  [ 27 ] , and even in tasks involving language-guided  [ 28 ]  and bimanual manipulation  [ 6 ,  29 ] . These approaches enable more effective interaction and contact point selection, facilitating a variety of downstream tasks  [ 6 ,  30 ] . In this paper, our work extends the use of dense correspondence by applying these dense point representations specifically to deformable garment manipulation. By leveraging point-level affordance learning, we aim to enhance the robots ability to detect manipulation-relevant keypoints and improve performance across diverse garment states.",
            "The initial step involves the creation of garment meshes in multiple folded states. We began by defining a set of 2D boundary vertices based on templates specific to each garment type, inspired by  [ 13 ] . These vertices were connected using Bezier curves to form single-layer meshes, enhancing realism by representing features such as the neckline of a T-shirt with smooth edges. Parameters for the mesh skeleton, Bezier curves, and corner rounding radii were sampled from carefully calibrated ranges to ensure variability. The meshes were then triangulated with edge lengths constrained to 1 cm, and UV maps were generated to allow for subsequent texturing. During the folding process, we tracked vertices corresponding to key semantic regions, enabling automatic keypoint labeling at different folding stages.",
            "To generate high-quality synthetic images, we applied textures sourced from PolyHaven  [ 31 ]  to both the environment and the folding surface. Cloth meshes were solidified and textured, and distractor objects from the Google Scanned Objects dataset  [ 32 ]  were strategically placed in the scene to improve keypoint detection in varied environments. Cameras were positioned randomly around the garment, and the Cycles rendering engine was employed to produce photorealistic images with a range of lighting and viewpoint variations. A few examples of the synthetic image can be found in Fig.   3 ."
        ]
    },
    "id_table_2": {
        "caption": "TABLE II:  Ablation study",
        "table": "S4.T2.2.2",
        "footnotes": [],
        "references": [
            "Garments, as one of the most ubiquitous items in home environments, have long been a focal point in assistive robotics research  [ 1 ,  2 ] . Tasks such as washing, folding, and ironing garments exemplify how robots can assist with everyday household activities. However, despite advancements in robotic vision and manipulation technologies, accurately recognizing and manipulating garments remains a challenge due to their diverse shapes and deformability  [ 3 ,  4 ] . Robots must not only recognize keypoints on garments but also adapt to the constantly changing states of these flexible objects to perform precise manipulations. This adaptability is essential for ensuring reliable performance in complex real-world scenarios.",
            "Robotic manipulation of garments is a critical challenge in assistive robotics due to the deformable and highly variable nature of clothing items  [ 8 ,  9 ,  1 ,  10 ,  6 ] . Much of the existing research has focused on key tasks such as unfolding (flattening) and folding garments. While unfolding systems have made notable progress  [ 2 ,  11 ,  6 ] , they often leave garments imperfectly flattened, and their ability to handle a wide variety of clothing types and environmental conditions remains limited.",
            "Traditionally, folding tasks for flattened garments rely on predefined state representations and scripted policies  [ 2 ,  12 ] . Researchers have explored various approaches for generating these state representations, including template fitting  [ 1 ,  2 ]  and semantic keypoint detection  [ 13 ,  6 ,  14 ,  15 ] . Many of these methods leverage depth images  [ 14 ,  16 ,  15 ] , which are advantageous for capturing geometric data unaffected by lighting or background clutter. However, depth images can omit critical visual information such as garment patterns and seams, which can be important for accurate manipulation  [ 16 ] . To address these limitations, our work utilizes RGB images, which offer richer visual information that can be crucial for precise garment manipulation.",
            "The use of synthetic data has become increasingly prevalent in robotic cloth manipulation, particularly for training models that need to generalize across diverse garment configurations and manipulation tasks  [ 17 ,  14 ,  18 ,  2 ,  5 ] . However, creating high-quality 3D assets that represent a wide variety of garments and states remains a significant challenge. Some approaches rely on limited sets of manually annotated pre-made cloth meshes  [ 14 ,  2 ,  19 ] , while others use procedural generation techniques to produce single-layer meshes  [ 17 ,  5 ] . Although large-scale datasets like Cloth3D  [ 4 ]  have been developed, they often lack detailed semantic annotations required for precise manipulation tasks.",
            "Dense object descriptors, which capture point- or pixel-level object representations, have been widely applied to various robotic manipulation tasks  [ 20 ] . These descriptors have been extended in numerous works to propose grasping poses  [ 21 ,  22 ] , manipulate deformable objects like ropes  [ 23 ] , and smooth fabrics  [ 24 ] . Additionally, point-level affordance learning has been explored for articulated objects  [ 25 ,  26 ] , deformable objects  [ 27 ] , and even in tasks involving language-guided  [ 28 ]  and bimanual manipulation  [ 6 ,  29 ] . These approaches enable more effective interaction and contact point selection, facilitating a variety of downstream tasks  [ 6 ,  30 ] . In this paper, our work extends the use of dense correspondence by applying these dense point representations specifically to deformable garment manipulation. By leveraging point-level affordance learning, we aim to enhance the robots ability to detect manipulation-relevant keypoints and improve performance across diverse garment states.",
            "To generate high-quality synthetic images, we applied textures sourced from PolyHaven  [ 31 ]  to both the environment and the folding surface. Cloth meshes were solidified and textured, and distractor objects from the Google Scanned Objects dataset  [ 32 ]  were strategically placed in the scene to improve keypoint detection in varied environments. Cameras were positioned randomly around the garment, and the Cycles rendering engine was employed to produce photorealistic images with a range of lighting and viewpoint variations. A few examples of the synthetic image can be found in Fig.   3 .",
            "As shown in   2 , our vision language model is built upon the SPHINX-X framework  [ 33 ] , with LLaMA2 serving as its core language backbone. We chose this model due to its unique capability to concentrate on fine-grained, region-specific details of objects, making it well-suited for tasks requiring detailed visual analysis. Our model employs the any resolution strategy introduced by SPHINX  [ 33 ] . The input images are divided into smaller sub-images, after which the visual encoders process them to extract essential features. Given the dual need for both global and local visual comprehension in manipulation tasks, we integrate several image encoders: CLIP  [ 34 ]  and DINOv2  [ 35 ]  to capture localized semantic information, and QFormer   [ 36 ]  to summarize global features. These local and global features are then concatenated at the channel level. The spatial alignment between visual tokens and their corresponding language tokens is managed through projection layers."
        ]
    }
}