{
    "id_table_1": {
        "caption": "Table 1:  Comparison of  LLaVA-Video-178K  and other video-language datasets . Average FPS represents the average number of frames per second that are used to prompt GPT-4o/GPT-4V for annotation.  VIDAL, WebVid, ActivityNet.  Panda-70M, Pexels, Pixabay, Mixkit, BDD100K, Ego4d.  HD-VILA-100M, Kinetics-700M, Ego4D, VidOR, InternVid, YouCook2, ActivityNet, Sth-sthv2, VIDAL, Charades.",
        "table": "S3.F5.31.30",
        "footnotes": [],
        "references": [
            "In this work, our goal is to create a high-quality video-language dataset that goes beyond simple video captions. We aim to improve the ability to follow instructions, which includes detailed video descriptions, open-ended video question-answering, and multiple-choice video question-answering data. We discuss related datasets in Table  1 . Previous video-language datasets  (Miech et al.,  2019 )  include manually annotated data for various tasks, such as video captions  (Chen & Dolan,  2011 ; Xu et al.,  2016 ; Rohrbach et al.,  2015 ; Anne Hendricks et al.,  2017a ; Caba Heilbron et al.,  2015 ; Zhou & Corso,  2017 ) , and video question-answering  (Yu et al.,  2019 ; Zadeh et al.,  2019 ; Xiao et al.,  2021 ) . However, manual annotation is expensive and limits the size of such datasets. To address the shortage of data, studies like  (Miech et al.,  2019 ; Lee et al.,  2021 ; Zellers et al.,  2021 ; Xue et al.,  2022 )  suggest automatically annotating data using subtitles created by ASR. While this method greatly expands the dataset size to 100 million samples, the subtitles often fail to accurately describe the main video content. Additionally, other studies  (Xu et al.,  2017 ; Grunde-McLaughlin et al.,  2021 ; Wu et al.,  2024a )  use language models  (Xu et al.,  2017 )  or question templates  (Grunde-McLaughlin et al.,  2021 ; Wu et al.,  2024a )  to generate question-answer pairs. Although this approach can generate a large number of questions and answers, it often produces poor-quality questions that do not reflect real-world user inquiries. More recent research  (Chen et al.,  2024b )  has prompted video-language models such as BLIP-2  (Li et al.,  2023 ) , VideoChat  (Li et al.,  2024e ) , Video-LLaMA  (Zhang et al.,  2023 ) , and MiniGPT-4  (Zhu et al.,  2023b )  to generate video captions. However, these models are limited in their ability to provide detailed descriptions.",
            "One important starting point in building a high-quality video instruction-following dataset is to find a sufficiently diverse pool of video data. From this pool, we can select the qualified videos. In our study of public video-language datasetsincluding video captioning, video question answering, video summarization, and moment-wise captioningwe noticed that although different datasets focus on various video understanding tasks ( e.g. , AGQA  (Grunde-McLaughlin et al.,  2021 )  for spatial-temporal relations and STAR  (Wu et al.,  2024a )  for situational reasoning), most are sourced from ten main video sources. For instance, both AGQA and STAR use data from Charades  (Sigurdsson et al.,  2016 ) . Specifically, these ten sources are HD-VILA-100M  (Xue et al.,  2022 ) , InternVid-10M  (Wang et al.,  2023 ) , VidOR  (Shang et al.,  2019 ) , VIDAL (YouTube Shorts) (Zhu et al.,  2023a ) , YouCook2 (Zhou & Corso,  2017 ) , Charades  (Sigurdsson et al.,  2016 ) , ActivityNet  (Caba Heilbron et al.,  2015 ) , Kinetics-700  (Kay et al.,  2017 ) , Something-Something v2  (Goyal et al.,  2017 ) , and Ego4d  (Grauman et al.,  2022 ) . These sources offer a wide range of video data from different websites, viewpoints, and domains. The relationship between these ten selected video datasets and others is shown in Fig.  1 . The videos from this ten datsets build the video pool for the further video selection. Notably, we use untrimmed videos from each source except for YouCook2 and Kinetics-700. We believe that cutting videos into clips can break the plot continuity, which is essential for understanding the videos.",
            "Based on the video pool, we aim to select dynamic videos. In Figure  1 , we outline our criteria for selecting high-quality data. Our main method for identifying dynamic content involves using PySceneDetect, which calculates the number of scenes in a video We found that the number of scenes is a good indicator of video dynamism. Additionally, we have designed a specific approach 4 to exclude videos that mainly contain slides.\"",
            "For each video in  LLaVA-Video-178K , referencing InsTag  (Lu et al.,  2023 ) , we employ an in-house tagging model to categorize the video content. Figure  7  displays the distribution of ten uniformly sampled video categories, showcasing examples from four of these categories. Among all videos, comedy predominates, primarily because YouTube Shorts is one of the most common sources in our dataset. Comedy is a typical genre that tends to attract high view countsvideos with large viewerships are more likely to be collected, as indicated in Table  1 . Additionally, our dataset includes some domains less represented in current video-language datasets, such as computer games.",
            "We provide a comparison of high-quality instruction following video-language datasets, with a focus on synthetic data created with strong AI models, as shown in Table  1 .  ( i ) i (i) ( italic_i )   A broad collection of dynamic videos.  In terms of video sources, although LLaVA-Hound  (Zhang et al.,  2024d )  contains the largest number of videos, 44% of its video data are sourced from WebVid  (Bain et al.,  2021 ) , where most videos are static. ShareGPT4Video  (Chen et al.,  2024a )  includes 30% of its videos from Pexels, Pixabay, and Mixkit, which are aesthetically good but also mostly static. Additionally, the majority of its videos come from Panda-70M, which are short clips from longer videossuggesting simpler plots. In contrast, we carefully select video sources that offer dynamic, untrimmed videos with complex plots, which are crucial for developing a powerful video understanding model. 1 1 1 Example videos:  WebVid , Pixabay , Pexels , Mixkit .   ( i  i ) i i (ii) ( italic_i italic_i )   High frames per second . Regarding frame sampling in language annotations, the proposed datasest considers 1 FPS, while other datasets consider much lower FPS. LLaVA-Hound uniformly samples 10 frames from videos of any length. The average FPS is 0.008, which may miss some fine details. ShareGPT4Video picks key frames using CLIP  (Radford et al.,  2021 )  based on frame uniqueness. This method might also miss subtle changes in the video because CLIP embeddings do not capture fine-grained dynamics well. Our method samples FPS=1 without using key frame selection algorithms, ensuring the detailed temproal information can be expressed in annotations and high coverage.  ( i  i  i ) i i i (iii) ( italic_i italic_i italic_i )   Diverse tasks.  The proposed dataset considers three common task types, including caption, free-form and closed-form QA, while existing datasets only consider a subset. Meanwhile, the quality and numbers of samples in our dataset is higher.",
            "Two group of experiments are considered to assess the data quality of  LLaVA-Video-178K  compare to LLaVA-Hound and ShareGPT4Video. In the first group, to compare  LLaVA-Video-178K  with LLaVA-Hound, we randomly selected 900K open-ended questions to match the number in LLaVA-Hound. We included all captions and did not sample the multiple-choice questions. In the second group, comparing  LLaVA-Video-178K  to ShareGPT4Video, we randomly sampled 40K video captions to align with those in ShareGPT4Video. Since ShareGPT4Video lacks open-ended and multiple-choice questions, we supplemented with annotations from NExT-QA, PerceptionTest, and ActivityNet-QA. In the first group of Table  4 , we compare  LLaVA-Video-178K  with LLaVA-Hound. Although LLaVA-Hound has more captions than  LLaVA-Video-178K , our results are still better. As shown in Table  1 , despite LLaVA-Hound annotates more videos, its quality is limited due to two main issues: (1) Static video: Its primary video source is WebVid  (Bain et al.,  2021 ) , which tends to have relatively static content. (2) Sparse sampling: Although it includes data sources with dynamic videos, its sampling rate of 10 frames per video leads to annotations that do not fully capture the complete plot of the video. This underscores that the quality of video instruction-following data is more important than its quantity. Additionally, the second experiment group in Table  4  shows that the model trained with  LLaVA-Video-178K  outperforms that of ShareGPT4Video, highlighting the superiority of our datas quality.",
            "Optical Illusion : As shown in Table  11 ,  LLaVA-Video  recognizes that the green dragon in the video is not a real 3D object. It appears three-dimensional due to an optical illusion that affects human perception.",
            "Special Domain : As indicated in Table  11 ,  LLaVA-Video  understands the content within special domains in the video, such as sketches and fights in video games.",
            "Unusual Action : As detailed in Table  12 ,  LLaVA-Video  identifies atypical actions in the video, such as \"physical therapy\" for pets, beyond ordinary activities.",
            "Physical Laws : As shown in Table  13 ,  LLaVA-Video  comprehends basic physical laws demonstrated in the video, like zero gravity in space stations, which allows objects to float without falling."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  LLaVA-Video  performance on video benchmarks. We report the score out of 5 for VideoDC, VideoChatGPT while other results are reported in accuracy. All results are reported as 0-shot accuracy. *indicates that the training set has been observed in our data mixture.",
        "table": "S3.T1.3",
        "footnotes": [],
        "references": [
            "For selected videos, we use GPT-4o  (OpenAI,  2024 )  to systematically describe their content. We start by sampling video frames at one frame per second (fps). However, due to the input size constraints of GPT-4o, we cannot use all sampled frames. Instead, we describe the videos sequentially, as shown in Fig  2 . We create descriptions at three distinct levels, detailed below.",
            "For ablation studies in .  4.2  and Sec.  4.3 , we conduct evaluation across 4 datasets. NExT-QA  (Xiao et al.,  2021 )  and PerceptionTest  (Patraucean et al.,  2023 ) , which use training data from the  LLaVA-Video-178K , are treated as in-domain datasets. Conversely, VideoMME  (Fu et al.,  2024 )  and EgoSchema  (Mangalam et al.,  2024 )  are considegreen zero-shot datasets.",
            "In Table  2 , we compare the performance of different models on various video benchmarks. The 72B model performs as well as the commercial, closed-source model Gemini-1.5-Flash  (Team et al.,  2023 ) , highlighting the effectiveness of open-source efforts in achieving comparable results. The  LLaVA-Video -7B model outperforms the previous top model, LLaVA-OV-7B, in seven out of ten datasets. Analysis of individual datasets shows some noteworthy trends. For instance, on benchmarks like MLVU, LongVideoBench, and VideoMME, which primarily use video data from YouTube, this improvement may be due to the inclusion of extensive YouTube data in  LLaVA-Video-178K , as illustrated in Fig.  5 . Additionally, the improvement on ActivityNet-QA is small; this could be because many questions in ActivityNet-QA, such as Whats the color of the ball? can be answered by viewing a single frame. The visibility of the ball from the beginning to the end of the video means understanding the video sequence is unnecessary, so  LLaVA-Video-178K  offers little advantage in this context. We find that  LLaVA-Video -7B is notably weaker in the specialized task of EgoSchema, an ego-centric dataset. This weakness may be due to a significant reduction in the proportion of ego-centric data in the training dataset of  LLaVA-Video . However, this impact is less pronounced in larger models, as demonstrated by the  LLaVA-Video -72B models superior performance over LLaVA-OV-72B in EgoSchema.",
            "As discussed in Section  3.2 , we show that generating  level-1 description  should consider historical context. Figure  9  illustrates the impact of excluding historical context on the quality of video descriptions. Specifically, including historical context helps accurately identify characters across different times as the same individual.",
            "Unusual Action : As detailed in Table  12 ,  LLaVA-Video  identifies atypical actions in the video, such as \"physical therapy\" for pets, beyond ordinary activities."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Ablation study on the  LLaVA-Video  model with various configurations of training data. Three Q&A datasets indicate: NExT-QA, ActivityNet-QA and PerceptionTest.",
        "table": "S4.T2.3",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "In addition to detailed video descriptions, our dataset includes a variety of question-answer pairs designed for complex interactions. This setup improves the video understanding models ability to handle real-life queries. We refer to public video question-answering benchmarks  (Xiao et al.,  2021 ; Yu et al.,  2019 ; khattak et al.,  2024 ; Liu et al.,  2024b )  to organize these questions into 16 specific categories, as shown in Fig.  3 .",
            "For ablation studies in .  4.2  and Sec.  4.3 , we conduct evaluation across 4 datasets. NExT-QA  (Xiao et al.,  2021 )  and PerceptionTest  (Patraucean et al.,  2023 ) , which use training data from the  LLaVA-Video-178K , are treated as in-domain datasets. Conversely, VideoMME  (Fu et al.,  2024 )  and EgoSchema  (Mangalam et al.,  2024 )  are considegreen zero-shot datasets.",
            "The results are presented in Table  3 . Initially, we used a basic model trained solely on the LLaVA-Hound dataset as our baseline. Compared to this baseline, adding the  LLaVA-Video-178K  dataset significantly improved performance, enhancing scores in both in-domain and out-of-domain tasks. Specifically, we observed a 31.9-point increase in NExT-QA scores and a 9.1-point rise in VideoMME scores. Furthermore, including the PerceptionTest dataset significantly enhanced its associated task. Additionally, integrating high-quality image data provided modest benefits on EgoSchema.",
            "As discussed in Section  3.2 , we show that generating  level-1 description  should consider historical context. Figure  9  illustrates the impact of excluding historical context on the quality of video descriptions. Specifically, including historical context helps accurately identify characters across different times as the same individual.",
            "Physical Laws : As shown in Table  13 ,  LLaVA-Video  comprehends basic physical laws demonstrated in the video, like zero gravity in space stations, which allows objects to float without falling."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Comparison of  LLaVA-Video-178K  and other video instruction-following datasets.",
        "table": "S4.T3.3",
        "footnotes": [],
        "references": [
            "For ablation studies in .  4.2  and Sec.  4.3 , we conduct evaluation across 4 datasets. NExT-QA  (Xiao et al.,  2021 )  and PerceptionTest  (Patraucean et al.,  2023 ) , which use training data from the  LLaVA-Video-178K , are treated as in-domain datasets. Conversely, VideoMME  (Fu et al.,  2024 )  and EgoSchema  (Mangalam et al.,  2024 )  are considegreen zero-shot datasets.",
            "We conduct two ablation studies to further analyze our dataset and training strategy. As shown in Table  4 , we compared three datasets where the language annotations are from GPT-4V/GPT-4o. For each experiment, we fine-tune the LLaVA-OneVision (SI) model separately on each specific dataset setting, utilizing a video representation defined by  V = ( 64 , 679 , 1 , 2 ) V 64 679 1 2 \\mathcal{V}=(64,679,1,2) caligraphic_V = ( 64 , 679 , 1 , 2 ) .",
            "Two group of experiments are considered to assess the data quality of  LLaVA-Video-178K  compare to LLaVA-Hound and ShareGPT4Video. In the first group, to compare  LLaVA-Video-178K  with LLaVA-Hound, we randomly selected 900K open-ended questions to match the number in LLaVA-Hound. We included all captions and did not sample the multiple-choice questions. In the second group, comparing  LLaVA-Video-178K  to ShareGPT4Video, we randomly sampled 40K video captions to align with those in ShareGPT4Video. Since ShareGPT4Video lacks open-ended and multiple-choice questions, we supplemented with annotations from NExT-QA, PerceptionTest, and ActivityNet-QA. In the first group of Table  4 , we compare  LLaVA-Video-178K  with LLaVA-Hound. Although LLaVA-Hound has more captions than  LLaVA-Video-178K , our results are still better. As shown in Table  1 , despite LLaVA-Hound annotates more videos, its quality is limited due to two main issues: (1) Static video: Its primary video source is WebVid  (Bain et al.,  2021 ) , which tends to have relatively static content. (2) Sparse sampling: Although it includes data sources with dynamic videos, its sampling rate of 10 frames per video leads to annotations that do not fully capture the complete plot of the video. This underscores that the quality of video instruction-following data is more important than its quantity. Additionally, the second experiment group in Table  4  shows that the model trained with  LLaVA-Video-178K  outperforms that of ShareGPT4Video, highlighting the superiority of our datas quality.",
            "In Table  5 , we list the names and descriptions of different question types and their corresponding proportions in the  LLaVA-Video-178K  dataset. The prompt used to generate video question-answer pairs from GPT-4O is shown in Table.  6 . In Fig.  4 , we show an example of a video along with its detailed description, an open-ended question, and a multiple-choice question."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Question types for video question answering in data creation. For each type, we provide its name, description, and the proportion it represents in the  LLaVA-Video-178K .",
        "table": "S4.T4.3",
        "footnotes": [],
        "references": [
            "In Table  2 , we compare the performance of different models on various video benchmarks. The 72B model performs as well as the commercial, closed-source model Gemini-1.5-Flash  (Team et al.,  2023 ) , highlighting the effectiveness of open-source efforts in achieving comparable results. The  LLaVA-Video -7B model outperforms the previous top model, LLaVA-OV-7B, in seven out of ten datasets. Analysis of individual datasets shows some noteworthy trends. For instance, on benchmarks like MLVU, LongVideoBench, and VideoMME, which primarily use video data from YouTube, this improvement may be due to the inclusion of extensive YouTube data in  LLaVA-Video-178K , as illustrated in Fig.  5 . Additionally, the improvement on ActivityNet-QA is small; this could be because many questions in ActivityNet-QA, such as Whats the color of the ball? can be answered by viewing a single frame. The visibility of the ball from the beginning to the end of the video means understanding the video sequence is unnecessary, so  LLaVA-Video-178K  offers little advantage in this context. We find that  LLaVA-Video -7B is notably weaker in the specialized task of EgoSchema, an ego-centric dataset. This weakness may be due to a significant reduction in the proportion of ego-centric data in the training dataset of  LLaVA-Video . However, this impact is less pronounced in larger models, as demonstrated by the  LLaVA-Video -72B models superior performance over LLaVA-OV-72B in EgoSchema.",
            "In Table  5 , we list the names and descriptions of different question types and their corresponding proportions in the  LLaVA-Video-178K  dataset. The prompt used to generate video question-answer pairs from GPT-4O is shown in Table.  6 . In Fig.  4 , we show an example of a video along with its detailed description, an open-ended question, and a multiple-choice question."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  We explain the process of creating prompts for GPT-4O to gather question-answer pairs from each video description.  tasks  includes the definition of all question types along with examples of question-answer pairs. We instruct GPT-4O to generate questions that cover as many question types as possible.",
        "table": "A3.T5.1.p1.pic1.1.1.1.1.1",
        "footnotes": [],
        "references": [
            "We present the distribution in Figure  6 . Our dataset shows a balanced mix across different video sources, providing a varied content selection. For each task type (caption, open-ended question, multiple-choice question), VIDAL (YouTube Shorts) has the highest share at 24.8%, 31.1%, and 30.1% respectively. It is followed by HD-VILA-100M (21.7%, 27.5%, 26.4%) and InternVid-10M (20.3%, 25.6%, 24.6%).",
            "Figure  6  (Left) illustrates the distrubtion of the video duration. Video lengths range from 0s to 180s, with each length category containing at least 600 videos. Videos shorter than 50 seconds are numerous, mainly because all videos from VIDAL (24.8% of the dataset), which contains YouTube Shorts with lengths under 45 seconds. Figure  6  (Middle) illustrates the distribution on the number of words for the synthetic captions. Figure  6  (Right) shows how video length correlates with the length of captions. Generally, longer videos feature longer captions.",
            "In Table  5 , we list the names and descriptions of different question types and their corresponding proportions in the  LLaVA-Video-178K  dataset. The prompt used to generate video question-answer pairs from GPT-4O is shown in Table.  6 . In Fig.  4 , we show an example of a video along with its detailed description, an open-ended question, and a multiple-choice question."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Comparison of  LLaVA-Video-178K  and other video-language datasets . Average FPS represents the average number of frames per second that are used to prompt GPT-4o/GPT-4V for annotation.",
        "table": "A3.T6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14",
        "footnotes": [],
        "references": [
            "For each video in  LLaVA-Video-178K , referencing InsTag  (Lu et al.,  2023 ) , we employ an in-house tagging model to categorize the video content. Figure  7  displays the distribution of ten uniformly sampled video categories, showcasing examples from four of these categories. Among all videos, comedy predominates, primarily because YouTube Shorts is one of the most common sources in our dataset. Comedy is a typical genre that tends to attract high view countsvideos with large viewerships are more likely to be collected, as indicated in Table  1 . Additionally, our dataset includes some domains less represented in current video-language datasets, such as computer games.",
            "We provide a more comprehensive comparison of  LLaVA-Video-178K  with other video-language datasets for the video caption task and video question answer task. Specifically, we organize the table into four groups, each characterized by its method of text annotation. As shown in Table  7 , unlike other datasets,  LLaVA-Video-178K  uniquely includes all three types of annotations: captions, open-ended questions, and multiple-choice questions."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Visual Representation Configurations and Performance Correlation.  T train superscript T train T^{\\text{train}} italic_T start_POSTSUPERSCRIPT train end_POSTSUPERSCRIPT  and  T test superscript T test T^{\\text{test}} italic_T start_POSTSUPERSCRIPT test end_POSTSUPERSCRIPT  are the number of frames in the training and inference stage, respectively.  M / p 2 M superscript p 2 M/{p^{2}} italic_M / italic_p start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT : number of visual tokens per frame.",
        "table": "A3.T7.3",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "In Table  8 , the first group shows an increase in the number of frames from 32 to 110. We set 110 frames as the upper limit to avoid overloading the GPU. With more frames, we see significant improvements in all datasets. While its generally expected that using more frames boosts performance, previous studies  (Luo et al.,  2021 ; Lei et al.,  2021 ;  2022 )  have noted that performance tends to plateau when training with more than 16 frames. We propose that the saturation observed in earlier studies arises due to the selection of training datasets such as MSVD  (Chen & Dolan,  2011 )  and WebVid  (Bain et al.,  2021 ) , where the video content is highly static, allowing a small number of frames to represent the entire video effectively. In contrast, the dynamic nature of the videos and the detailed nature of the annotations in  LLaVA-Video-178K  allow for continuous benefits from extensive sampling",
            "The second group in Table  8  demonstrates the effects of varying the number of inference frames while keeping the number of training frames constant. A modest increase in the inference frames slightly enhances performance; however, excessively increasing the number of inference frames can degrade it.",
            "In Table  8 s third group, we illustrates the trade-off between the number of frames and the number of tokens per frame. Configurations with fewer tokens per frame but more frames yield superior results, even with a lower total count of visual tokens (18,590 versus 21,632). This finding emphasizes that increasing the number of frames, rather than the tokens per frame or the total number of tokens, enhances performance. However, a balance is necessary; as the number of frames increases to 440 and the tokens per frame decreases to 64, performance drops. This observation led us to use  LLaVA-Video SlowFast SlowFast {}_{\\leavevmode\\nobreak\\ \\mathtt{SlowFast}} start_FLOATSUBSCRIPT typewriter_SlowFast end_FLOATSUBSCRIPT  for video representation."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Comparison of different video representations. The video representation  V V \\mathcal{V} caligraphic_V  is consistent in training and inference for all methods, except that SlowFast-LLaVA considers simple representation  V V \\mathcal{V} caligraphic_V  in training and its specified  V V \\mathcal{V} caligraphic_V  in inference.",
        "table": "A3.T8.9",
        "footnotes": [],
        "references": [
            "As discussed in Section  3.2 , we show that generating  level-1 description  should consider historical context. Figure  9  illustrates the impact of excluding historical context on the quality of video descriptions. Specifically, including historical context helps accurately identify characters across different times as the same individual."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:  LLaVA-Video  learns to understand the  optical illusion  in the video.",
        "table": "A3.T9.8",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "Table 11:  LLaVA-Video  learns to understand the the video in  special domain .",
        "table": "A5.1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1",
        "footnotes": [],
        "references": [
            "Optical Illusion : As shown in Table  11 ,  LLaVA-Video  recognizes that the green dragon in the video is not a real 3D object. It appears three-dimensional due to an optical illusion that affects human perception.",
            "Special Domain : As indicated in Table  11 ,  LLaVA-Video  understands the content within special domains in the video, such as sketches and fights in video games."
        ]
    },
    "id_table_12": {
        "caption": "Table 12:  LLaVA-Video  learns to understand the  unusual action  in the video.",
        "table": "A5.2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2",
        "footnotes": [],
        "references": [
            "Unusual Action : As detailed in Table  12 ,  LLaVA-Video  identifies atypical actions in the video, such as \"physical therapy\" for pets, beyond ordinary activities."
        ]
    },
    "id_table_13": {
        "caption": "Table 13:  LLaVA-Video  learns to understand the  physical laws  in the video.",
        "table": "A5.3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1",
        "footnotes": [],
        "references": [
            "Physical Laws : As shown in Table  13 ,  LLaVA-Video  comprehends basic physical laws demonstrated in the video, like zero gravity in space stations, which allows objects to float without falling."
        ]
    }
}