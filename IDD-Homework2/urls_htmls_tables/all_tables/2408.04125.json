{
    "id_table_1": {
        "caption": "TABLE I:  The Studied Datasets",
        "table": "S4.T1.3",
        "footnotes": [],
        "references": [
            "Figure  1  presents the workflow for the three proposed strategies, and all of the three proposed augmentation strategies can be abstracted into at most four components if applicable as follows:",
            "As presented in Figure  1 , the overall workflow for  Mutation  is straightforward. To augment  N N N italic_N  vulnerable samples,  N N N italic_N  vulnerable samples are randomly sampled from the input dataset, and then are directly fed into the Formulator to instantiate the template, as a result,  N N N italic_N  prompts are instantiated by filling the templates. The prompts are then fed to the Generator to generate  N N N italic_N  vulnerable samples. Its worth noting that on average 3% of the generated samples will be filtered out in the Verifier component and so if one desires to end up with at least  N N N italic_N  samples, a higher target should be selected in the generator phase, or else they should redo the generation after the verification to create more items to reach the target.",
            "Algorithm  1  outlines our method for constructing a dataset of  clean-vul  pairs through a retrieval process. Given a dataset containing both vulnerable samples ( V V V italic_V ) and clean samples ( C C C italic_C ), our goal is to retrieve  N N N italic_N   clean-vul  pairs. A straightforward approach would be to retrieve the most similar vulnerable samples for each clean sample, sort them in descending order of similarity, and select the top  N N N italic_N  pairs. However, focusing solely on similarity reduces the diversity of the retrieved samples, which is counterproductive for data augmentation. Previous studies have shown that higher diversity in the training dataset improves the generalizability of deep learning models  [ 37 ,  38 ] . Therefore, to enhance diversity, we incorporate a clustering phase to group the vulnerable samples into  G G G italic_G  clusters, ensuring that samples from all clusters are considered for selection."
        ]
    },
    "id_table_2": {
        "caption": "TABLE II:  Comparison of VulScribeRs strategies with baselines using ChatGPT and CodeQwen when augmenting 5K Samples. The cells with larger values (better performance) compared to NoAug are highlighted darker.",
        "table": "S5.T2.1.1",
        "footnotes": [],
        "references": [
            "The majority of previous works on program augmentation  [ 24 ,  25 ,  26 ,  27 ,  28 ,  29 ] , rely on code transforms that do not change the flow, semantics, and syntactical correctness of the program using program analysis. Variable name changing, replacing for with while loops and vice versa, and adding dead code are examples of this. However, program analysis is very time-consuming, and selecting the locations for transformation is challenging. Previous studies usually select the types and locations of transformation randomly without any program comprehension  [ 24 ,  25 ,  26 ,  27 ,  28 ,  29 ] . In  Mutation  strategy, we aim to augment vulnerable code samples by utilizing the program comprehension capability of LLM to mutate existing vulnerable code samples and let the LLM choose both the type of transformation and the potential statements to transform. In this way, we get around the program analysis and rely on LLMs creativity to generate more suitable and diverse items. We design the prompt template as shown in Figure  2 ."
        ]
    },
    "id_table_3": {
        "caption": "TABLE III:  Results of Ablation Studies on the Retriever and Clustering for Injection.",
        "table": "S5.T3.1.1",
        "footnotes": [],
        "references": [
            "Similar to recent works  [ 12 ,  11 ,  13 ] , we also focus on injecting vulnerable code segments into a clean sample, but we aim to cover all types of vulnerabilities and not just single statement vulnerabilities. More specifically, we instruct the LLM to inject the logic of the vulnerable sample into a clean sample by prioritizing the injection of vulnerable segments. Using an LLM for injecting the vulnerable segments into a clean item gives the freedom to LLM to identify the best location. We present our  Injection  template prompt as shown in Figure  3 ."
        ]
    },
    "id_table_4": {
        "caption": "TABLE IV:  Results of Ablation Studies on the Retriever and Clustering for Extension.",
        "table": "S5.T4.1.1",
        "footnotes": [],
        "references": [
            "While injecting vulnerable code segments into clean code is the defacto yet effective way of augmenting vulnerabilities, extending an already vulnerable code snippet by adding additional logic can also generate a new vulnerable sample, which enriches the context where vulnerable code happens.  Adding certain parts from a clean sample to an already vulnerable sample has a higher chance of keeping the context of the original vulnerable code intact compared to injecting vulnerable segments into a clean item. By extending an already vulnerable sample, only a small section will be irrelevant to the vulnerable part of the code which is the important part for the models, while injecting vulnerable segments into a clean sample results in a code snippet where the vulnerable section has little connection to the original context and gives itself away. Therefore, we aim to explore it as an alternative to vulnerability injection. We refer to this strategy as  Extension . We present the prompt template for  Extension  in Figure  4 ."
        ]
    },
    "id_table_5": {
        "caption": "TABLE V:  Impact of Generated Samples on Improving DVLD models Performance at 5K, 10K, and 15K",
        "table": "A0.T5.1.1",
        "footnotes": [],
        "references": [
            "All our proposed LLM-based augmentation strategies outperform the baselines. Typically, Injection and Extension always outperform baselines in all instances. Injection outperforms NoAug, Vulgen, VGX, and ROS by 30.80%, 27.48%, 27.93%, and 15.41% on average F1-score, respectively.  As shown in Table  II ,  Injection  and  Extension  strategies outperform baselines (i.e., NoAug, VulGen, VGX, ROS) in all of the experimental instances in terms of F1-score.  More specifically,  Extension  beats the baselines: NoAug, Vulgen, VGX, and ROS by 29.68%, 26.27%, 26.90%, and 14.35% on average F1-score, while the  Injection  strategy beats the baselines: NoAug, Vulgen, VGX, and ROS by 30.80%, 27.48%, 27.93%, and 15.41% on average F1-score respectively.      As observed,  Mutation  beats most baselines by a large margin. However,  Mutation  does not have a dominating advantage over ROS, and ROS beats  Mutation  in some (4 out of 12) instances. It is worth noting that ROS beats the baseline, Vulgen, and VGX by 13.96%, 10.72%, and 11.21% in terms of F1-score on average.     As discussed in Section  II , SOTA approaches like VGX and VulGen only focus on single-statement that limits the diversity of generated vulnerabilities. However, our approaches do not have this limitation and are able to generate more diverse vulnerable samples. To examine this, we measure the diversity of the augmented vulnerable samples by our approaches and VGX and VulGen. By following previous studies, we first apply principle component analysis (PCA) on CodeBERTs embeddings of the generated vulnerable samples and reduce the dimension to three. Next, we calculate the histogram using 10 bins, from which we calculate the entropy of the vectors. Higher entropy values indicate greater diversity among the vulnerable samples, while lower values suggest a more concentrated distribution with similar samples. As depicted in Figure  5  the entropy for  Mutation ,  Injection , and  Extension  are 4.79, 4.5, and 4.62 for ChatGPT, and 4.66, 4.45, 4.42 for CodeQwen, while for Vulgen and VGX, it is 4.42 and 4.31 respectively. The results demonstrate that our approaches have a better potential for generating more diverse samples as we beat them while using only one of the datasets they used for vulnerability mining.  VulScribeR produces more diverse vulnerable samples compared to SOTA approaches VXG and VulGen.      Lastly, it is worth noting that the results on both of the LLMs perform very close to each other. CodeQwen1.5-7B-Chat slightly outperforms ChatGPT3.5 Turbo by a tiny margin (i.e. 1.49%) averaged across all three strategies, yet one cannot conclude that CodeQwen is a better LLM for vulnerability generation as we did not explore the hyper-parameters for the optimum setting for each of the LLMs. However, we can conclude that our strategies work with similar LLM to ChatGPT and CodeQwen (e.g. GPT4  [ 51 ]  and DeepSeek-Coder  [ 52 ] ) and do not require an LLM that is trained specifically on code."
        ]
    },
    "global_footnotes": []
}