{
    "id_table_1": {
        "caption": "TABLE I:  Comparative analysis of discriminative score for leading time series generation techniques (Lower scores are better)",
        "table": "S6.EGx1",
        "footnotes": [],
        "references": [
            "The ChronoGAN framework is developed to enhance the efficacy and robustness of time series generation by accomplishing several critical objectives. First, it optimizes performance across both short and long sequences. Second, it enhances data reconstruction by the decoder and data generation by the generator through providing more accurate adversarial feedback to both the autoencoder and generator. Third, it facilitates the convergence of both the generator and autoencoder networks through the implementation of novel loss functions. Finally, it incorporates an early generation algorithm to achieve consistent optimal results under the same hyperparameters. Fig.  1  illustrates the implementation of ChronoGAN.",
            "Based on Fig.  1 , the framework includes five networks: an autoencoder (encoder and decoder), a generator, a supervisor, and a discriminator. The autoencoders role is to facilitate training by generating compressed representations in the latent space, thereby reducing the likelihood of non-convergence within the GAN framework. The generator produces data in this lower-dimensional latent space, as opposed to the feature space. The supervisor network, integrated with a supervised loss function, is specifically designed to learn the temporal dynamics of the time series data. This is crucial, as sole reliance on the discriminators adversarial feedback may not sufficiently prompt the generator to capture the datas stepwise conditional distributions. The discriminator network differentiates between fake and real data in the feature space, providing more accurate feedback to both the generator and autoencoder.",
            "In Fig.  1 ,  H A  E = e  ( X ) superscript H A E e X H^{AE}=e(X) italic_H start_POSTSUPERSCRIPT italic_A italic_E end_POSTSUPERSCRIPT = italic_e ( italic_X )  represents the encoding of the input data  X X X italic_X  into a latent space  H A  E superscript H A E H^{AE} italic_H start_POSTSUPERSCRIPT italic_A italic_E end_POSTSUPERSCRIPT  using the encoder function  e e e italic_e . The reconstructed data  X A  E = r  ( H A  E ) superscript X A E r superscript H A E X^{AE}=r(H^{AE}) italic_X start_POSTSUPERSCRIPT italic_A italic_E end_POSTSUPERSCRIPT = italic_r ( italic_H start_POSTSUPERSCRIPT italic_A italic_E end_POSTSUPERSCRIPT )  is obtained by decoding  H A  E superscript H A E H^{AE} italic_H start_POSTSUPERSCRIPT italic_A italic_E end_POSTSUPERSCRIPT  using the recovery function  r r r italic_r , aiming to replicate the original input data as closely as possible. The generator function  g g g italic_g  transforms a random noise vector  Z Z Z italic_Z  into synthetic latent data  H G = g  ( Z ) superscript H G g Z H^{G}=g(Z) italic_H start_POSTSUPERSCRIPT italic_G end_POSTSUPERSCRIPT = italic_g ( italic_Z ) , which is then reconstructed into synthetic data  X G = r  ( H G ) superscript X G r superscript H G X^{G}=r(H^{G}) italic_X start_POSTSUPERSCRIPT italic_G end_POSTSUPERSCRIPT = italic_r ( italic_H start_POSTSUPERSCRIPT italic_G end_POSTSUPERSCRIPT ) . The supervisor network  s s s italic_s  processes  H G superscript H G H^{G} italic_H start_POSTSUPERSCRIPT italic_G end_POSTSUPERSCRIPT  to produce a supervised latent representation  H S = s  ( H G ) superscript H S s superscript H G H^{S}=s(H^{G}) italic_H start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT = italic_s ( italic_H start_POSTSUPERSCRIPT italic_G end_POSTSUPERSCRIPT ) , from which the final synthetic data  X ~ = r  ( H S ) ~ X r superscript H S \\tilde{X}=r(H^{S}) over~ start_ARG italic_X end_ARG = italic_r ( italic_H start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT )  is reconstructed. The discriminator  d d d italic_d  evaluates the authenticity of the synthetic and real data by outputting  y ~ ~ y \\tilde{y} over~ start_ARG italic_y end_ARG  for synthetic data and  y y y italic_y  for real data.",
            "In a joint training scheme involving a GAN network and an autoencoder, relying solely on reconstruction loss for the autoencoder results in noisy outputs, where the autoencoders output fails to fully retain the inputs characteristics  [ 21 ] . Additionally, adversarial training within an embedding space leads to the generation of noisy data after decoding the generators output. The issue arises when the encoders output ( H A  E superscript H A E H^{AE} italic_H start_POSTSUPERSCRIPT italic_A italic_E end_POSTSUPERSCRIPT ) is regarded as real data and the generators output ( H G superscript H G H^{G} italic_H start_POSTSUPERSCRIPT italic_G end_POSTSUPERSCRIPT ) as synthetic during the adversarial training process. This practice reduces the discriminators ability to accurately differentiate between the attributes of real and synthetic data. A significant limitation is that the discriminator does not account for the error rate and data loss inherent in the autoencoders performance. This oversight may compromise the efficacy of the discriminator, resulting in suboptimal performance in distinguishing between real and generated data attributes. Consequently, this leads to less precise feedback being provided to the generator network, potentially affecting the overall quality of the synthetic data. To address this, as shown in Fig.  1 , discriminating in the feature space allows for defining real data as the dataset ( X X X italic_X ) and fake data as the decoding of the generators output ( X G superscript X G X^{G} italic_X start_POSTSUPERSCRIPT italic_G end_POSTSUPERSCRIPT ). This facilitates more accurate training for the discriminator, thus yielding improved feedback for the generator. Additionally, discrimination in the feature space provides valuable adversarial feedback to the autoencoder, enhancing its reconstruction capabilities in conjunction with conventional reconstruction loss. In the context of time series data, the feature space denotes the original dimensions, such as individual time points and their observed values. The latent or embedding space, achieved through an encoding process, represents the data in a lower-dimensional form, capturing its essential patterns and structures in a more compact and informative manner  [ 22 ] .",
            "Other components of  L T  S subscript L T S \\mathcal{L}_{TS} caligraphic_L start_POSTSUBSCRIPT italic_T italic_S end_POSTSUBSCRIPT , such as skewness, weighted average, and median, are calculated similarly to ( 10 ), ( 12 ), and ( III-B ). The only difference is that instead of using the formula for slope, the formulas for skewness (skew), weighted average (wAvg), and median are applied.",
            "Another prevalent issue with GANs is stability. To enhance the stability of the network, we employ an early generation algorithm since the optimal results may be achieved after a random, rather than a specific, number of iterations. Accordingly, as per Algorithm  1 , after half the number of epochs, we generate synthetic data and calculate the discriminative score and predictive score between real and synthetic data at intervals of every 500 epochs. Additionally, we compute the MSE of the mean and MSE of the std of real and synthetic data to verify whether the synthetic data matches the distribution of the real data. By integrating the results of the discriminative score, predictive score, and MSE of the mean and std, we determine whether to save the current model and generated data. Upon the completion of training, we ensure that the framework has produced the optimal results, consistently delivering reliable and precise outcomes after each training session. It is crucial to determine the appropriate weights for these metrics in order to integrate them and compare them with the previously saved model. The proportion of the discriminative score, predictive score, and MSE of the mean and std can vary depending on the characteristics of the dataset. Therefore, it is inappropriate to establish fixed hyperparameters to combine these three metrics. To address this issue, we initially calculate the hyperparameters  p  1 p 1 p1 italic_p 1  and  p  2 p 2 p2 italic_p 2  during the first assessment of these metrics. Once established, these hyperparameters are consistently applied in all subsequent epochs."
        ]
    },
    "id_table_2": {
        "caption": "TABLE II:  Comparative analysis of predictive score for leading time series generation techniques (Lower scores are better)",
        "table": "S6.EGx2",
        "footnotes": [],
        "references": [
            "Other components of  L T  S subscript L T S \\mathcal{L}_{TS} caligraphic_L start_POSTSUBSCRIPT italic_T italic_S end_POSTSUBSCRIPT , such as skewness, weighted average, and median, are calculated similarly to ( 10 ), ( 12 ), and ( III-B ). The only difference is that instead of using the formula for slope, the formulas for skewness (skew), weighted average (wAvg), and median are applied.",
            "A time series generation framework should be capable of handling both short and long sequences and, more importantly, be accurate on both. The exclusive use of either LSTM or GRU as the network architecture can lead to weaknesses in handling either long or short sequences. As shown in Fig.  2 , by implementing both network architectures and merging the results via a multilayer perceptron, the network becomes more generalized, making it more powerful in learning both long and short sequences. We employ multiple layers of GRU and LSTM separately to produce output, and then merge them using a multilayer perceptron network to obtain the final output. We utilize the same architecture and number of layers for all five networks within the ChronoGAN framework."
        ]
    },
    "id_table_3": {
        "caption": "",
        "table": "S4.T1.40",
        "footnotes": [],
        "references": [
            "In terms of predictive score, ChronoGAN reduces the error by approximately 10.82% across the four datasets compared to TimeGAN. This underscores the effectiveness of our novel time series-based ( L T  S subscript L T S \\mathcal{L}_{TS} caligraphic_L start_POSTSUBSCRIPT italic_T italic_S end_POSTSUBSCRIPT ) and supervised ( L S subscript L S \\mathcal{L}_{S} caligraphic_L start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ) loss functions, which significantly improve the generators ability to capture the temporal dynamics of the data more accurately. As demonstrated in Figs.  3  and  4 , we present several examples of synthetic samples generated by ChronoGAN for both the Sines and ECG datasets. These examples highlight ChronoGANs ability to effectively learn the temporal distributions of the real data and generate high-quality synthetic data that accurately reflect those patterns."
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "S4.T2.40",
        "footnotes": [],
        "references": [
            "In terms of predictive score, ChronoGAN reduces the error by approximately 10.82% across the four datasets compared to TimeGAN. This underscores the effectiveness of our novel time series-based ( L T  S subscript L T S \\mathcal{L}_{TS} caligraphic_L start_POSTSUBSCRIPT italic_T italic_S end_POSTSUBSCRIPT ) and supervised ( L S subscript L S \\mathcal{L}_{S} caligraphic_L start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ) loss functions, which significantly improve the generators ability to capture the temporal dynamics of the data more accurately. As demonstrated in Figs.  3  and  4 , we present several examples of synthetic samples generated by ChronoGAN for both the Sines and ECG datasets. These examples highlight ChronoGANs ability to effectively learn the temporal distributions of the real data and generate high-quality synthetic data that accurately reflect those patterns."
        ]
    }
}