{
    "S2.T1": {
        "caption": "Table 1: A comparison of our method with human performance based on manually curated scene graphs.",
        "table": "<table id=\"S2.T1.4.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S2.T1.4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S2.T1.4.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Method</th>\n<th id=\"S2.T1.4.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Binary</th>\n<th id=\"S2.T1.4.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Open</th>\n<th id=\"S2.T1.4.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Consistency</th>\n<th id=\"S2.T1.4.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Validity</th>\n<th id=\"S2.T1.4.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Plausibility</th>\n<th id=\"S2.T1.4.1.1.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Accuracy</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S2.T1.4.1.2.1\" class=\"ltx_tr\">\n<td id=\"S2.T1.4.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">Human <cite class=\"ltx_cite ltx_citemacro_citep\">(Hudson &amp; Manning, <a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">2019b</a>)</cite>\n</td>\n<td id=\"S2.T1.4.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">91.2</td>\n<td id=\"S2.T1.4.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">87.4</td>\n<td id=\"S2.T1.4.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">98.4</td>\n<td id=\"S2.T1.4.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">98.9</td>\n<td id=\"S2.T1.4.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">97.2</td>\n<td id=\"S2.T1.4.1.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">89.3</td>\n</tr>\n<tr id=\"S2.T1.4.1.3.2\" class=\"ltx_tr\">\n<td id=\"S2.T1.4.1.3.2.1\" class=\"ltx_td ltx_align_center\">TRRNet</td>\n<td id=\"S2.T1.4.1.3.2.2\" class=\"ltx_td ltx_align_center\">77.91</td>\n<td id=\"S2.T1.4.1.3.2.3\" class=\"ltx_td ltx_align_center\">50.22</td>\n<td id=\"S2.T1.4.1.3.2.4\" class=\"ltx_td ltx_align_center\">89.84</td>\n<td id=\"S2.T1.4.1.3.2.5\" class=\"ltx_td ltx_align_center\">85.15</td>\n<td id=\"S2.T1.4.1.3.2.6\" class=\"ltx_td ltx_align_center\">96.47</td>\n<td id=\"S2.T1.4.1.3.2.7\" class=\"ltx_td ltx_align_center\">63.20</td>\n</tr>\n<tr id=\"S2.T1.4.1.4.3\" class=\"ltx_tr\">\n<td id=\"S2.T1.4.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_b\">Our Method</td>\n<td id=\"S2.T1.4.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_b\">90.41</td>\n<td id=\"S2.T1.4.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b\">90.86</td>\n<td id=\"S2.T1.4.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_b\">91.92</td>\n<td id=\"S2.T1.4.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_b\">93.68</td>\n<td id=\"S2.T1.4.1.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_b\">93.13</td>\n<td id=\"S2.T1.4.1.4.3.7\" class=\"ltx_td ltx_align_center ltx_border_b\">90.63</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "In this work, we report first results on an experimental study on manually curated scene graphs that are provided in the GQA dataset. In this setting, the true reasoning and language understanding capabilities of our model can be analyzed.\nTable 1 shows the performance of our method and compares it with the human performance reported in (Hudson & Manning, 2019b) and with TRRNet111https://evalai.cloudcv.org/web/challenges/challenge-page/225/leaderboard/733#leaderboardrank-5, the best performing single method submission to the GQA Real-World Visual Reasoning Challenge 2019.\nAlong with the accuracy on open questions (“Open”), binary questions (yes/no) (“Binary”), and the overall accuracy (“Accuracy”), we also report the additional metric “Consistency” (answers should not contradict themselves), “Validity” (answers are in the range of a question; e.g., red is a valid answer when asked for the color of an object), “Plausibility” (answers should be reasonable; e.g., red is a reasonable color of an apple reasonable, blue is not), as proposed in (Hudson & Manning, 2019b).",
            "The results in Table 1 show that our method achieves human level performance with respect to most metrics. Figure 3 shows three examples of scene graph traversals, which produced the correct answer. An advantage of our approach is that the sequential reasoning process makes the model output transparent and easier to debug in cases of failures.",
            "Although the results in Table 1 are very encouraging, the performance numbers are not directly comparable, since the underlying data sets are different and since we operated on manually curated scene graphs.\nAs part of ongoing work, we are exploring different methods for extracting the scene graph automatically from the images. This step is not really the focus of this work but turns out to be the weak part of our overall VQA approach. By using the scene graph generation procedure proposed in (Yang et al., 2018), we found that in the cases where the answer to an open question was contained in the scene graph, our method was able to achieve 53.24%percent53.2453.24\\% accuracy on this subset of the data (which could be compared to the 50.22% accuracy of TRRNet). We are currently working on improving the scene graph generation framework by integrating recent advancements in object detection such as (Tan et al., 2019) or in scene graph generation (Zellers et al., 2018; Zhang et al., 2019; Koner et al., 2020). We hope that these methods lead to more accurate scene graphs so that our method is able to retain close to human performance as presented in this paper."
        ]
    }
}