{
    "S3.T1.4": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T1.4.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.4.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.4.1.1.1.1\">Irrelevant</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.4.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.4.1.1.2.1\">Road</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.4.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.4.1.1.3.1\">Walkway</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.4.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.4.1.1.4.1\">Vegetation</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.4.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.4.2.2.1\">58.7%</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.4.2.2.2\">14.8%</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.4.2.2.3\">5.22%</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.4.2.2.4\">19.9%</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.4.3.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.4.3.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.4.3.1.1.1\">Parking</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.4.3.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.4.3.1.2.1\">Traffic Island</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.4.3.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.4.3.1.3.1\">Symbol</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.4.3.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.4.3.1.4.1\">Lane Marking</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.4.4.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S3.T1.4.4.2.1\">0.210%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S3.T1.4.4.2.2\">0.214%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S3.T1.4.4.2.3\">0.109%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S3.T1.4.4.2.4\">0.826%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "TABLE I :  Dataset Class Distribution",
        "footnotes": [],
        "references": [
            "Figure 1 shows an annotation overlay on the RGB image. The class distribution (see Table I) reveals a notable imbalance among the classes. Particularly relevant classes, such as lane markings and symbols, are noticeably underrepresented. Although more than 50% of the total annotation points are used for annotating these polygons, they only cover less than 1.5% of the total area. Consequently, the weight of these classes was significantly increased during training. Another challenge arises from the fact that objects like lane markings or symbols are very small despite the high ground-sampling distance, yet they need to be accurately segmented, especially in the case of road arrows. Therefore, we chose network architectures capable of providing non-upscaled semantic segmentation at full or half input resolution."
        ]
    },
    "S6.T2.4": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S6.T2.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S6.T2.4.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"S6.T2.4.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.4.1.1.1.1\">Architecture</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S6.T2.4.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.4.1.1.2.1\">Resolution</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"S6.T2.4.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.4.1.1.3.1\">mIoU</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S6.T2.4.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.4.1.1.4.1\">IoU[Symbol]</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S6.T2.4.2.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S6.T2.4.2.1.1\">UNet</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T2.4.2.1.2\">full</td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S6.T2.4.2.1.3\">0.64</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.4.2.1.4\">0.74</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.4.3.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S6.T2.4.3.2.1\">UNet</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T2.4.3.2.2\">half</td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S6.T2.4.3.2.3\">0.65</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.4.3.2.4\">0.76</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.4.4.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S6.T2.4.4.3.1\">UPerNet</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T2.4.4.3.2\">half</td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S6.T2.4.4.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.4.4.3.3.1\">0.70</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.4.4.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.4.4.3.4.1\">0.80</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.4.5.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\" id=\"S6.T2.4.5.4.1\">UPerNet</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S6.T2.4.5.4.2\">quarter</td>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\" id=\"S6.T2.4.5.4.3\">0.69</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T2.4.5.4.4\">0.76</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "TABLE II :  Semantic Segmentation Results",
        "footnotes": [],
        "references": [
            "To train the semantic segmentation networks, we randomly divide the entire dataset into training, validation, and testing sets in a 70:15:15 ratio. We train all networks using SGD with a learning rate of 0.01. Since the aerial images are large, we train on random crops and apply additional augmentation techniques such as rotations, scaling, and mirroring. Given the under-representation of lane markings and symbols in the cross-entropy loss, we empirically set the class weights for these two classes to 40 and 20, respectively. Table II presents a comparison of networks that use mIoU and IoU for the symbol class, which is crucial for the symbol classifier. The results indicate that the segmentation resolution used does not significantly affect the mIoU. However, upon qualitative examination of the segmentation masks, it is evident that only at half or full resolution is the segmentation of symbols sufficiently detailed for subsequent classification.\nA more substantial quantitative difference is observed when using different architectures, with the UPerNet architecture yielding significantly better results. Furthermore, the results exhibit less noise, likely due to the larger receptive field. One limitation of all networks is the poor segmentation of parking areas and traffic islands. However, even for human annotators, distinguishing these areas during the annotation process was challenging, and the data only sparsely represents them (see Section III). Fortunately, these classes are less critical for HD map creation than the classes used to derive contours (lane markings, symbols, and roads)."
        ]
    },
    "S6.T3.4": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S6.T3.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S6.T3.4.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S6.T3.4.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T3.4.1.1.1.1\">Input</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S6.T3.4.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T3.4.1.1.2.1\">Precision</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S6.T3.4.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T3.4.1.1.3.1\">Recall</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S6.T3.4.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T3.4.2.1.1\">reference (human annotations)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T3.4.2.1.2\">0.97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T3.4.2.1.3\">0.97</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T3.4.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T3.4.3.2.1\">prediction (trained model outputs)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T3.4.3.2.2\">0.96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S6.T3.4.3.2.3\">0.96</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "TABLE III :  Map Creation Error",
        "footnotes": [],
        "references": [
            "To determine the impact of semantic segmentation errors on the generated map, we created maps for both the manually annotated semantic segmentation and the semantic segmentation created by the trained network. The results for the test set (see Table III) show that the polylines are positioned with a high recall of 97% on the ground truth data. A high recall is essential, as it largely eliminates the need for time-consuming manual annotation of the contours. At the same time, there are only a few false positives, resulting in a precision of 97%. Visually analyzing the results reveals that errors typically occur near the image border and in more complex situations. The HD map created based on the semantic segmentation predicted by the neural network is only slightly worse, with a recall of 96% and a precision of 96%.\nThis indicates that the quality of semantic segmentation is high enough for the intended purpose.\nIn summary, the mapping results suggest that more than 95% of the manual contour annotations can be done automatically. This drastically reduces the effort required to create maps from aerial imagery."
        ]
    }
}