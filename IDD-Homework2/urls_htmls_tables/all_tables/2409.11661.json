{
    "id_table_1": {
        "caption": "Table 1 :  List of data augmentations and equivalent commands in Albumentations v1.3.1  [ 104 ] .",
        "table": "Sx1.EGx1",
        "footnotes": [
            ""
        ],
        "references": [
            "The inaccessibility of space poses a particularly challenging logistics problem to not only overcoming the domain gap but also verifying it during the stringent pre-flight verification processes typically required of space missions. The most prominent approach taken by the aerospace community is to utilize a robotic testbed to simulate the space environment on-ground. The idea is to physically stimulate the vision-based sensors using a mockup model of the target placed in a facility simulating the high-fidelity space-like illumination conditions. A well-calibrated testbed can then be used to re-create various RPOD scenarios, generating pose-annotated images at scale with minimal human intervention for the purpose of validating a NNs robustness across the sim2real gap. One such example is the Testbed for Rendezvous and Optical Navigation (TRON) facility at the Stanfords Space Rendezvous Laboratory (SLAB) 3 3 3 https://slab.stanford.edu/   [ 14 ]  which was used to create the so-called Hardware-In-the-Loop (HIL) images that constitute datasets such as SPEED+  [ 15 ]  and SHIRT  [ 16 ] .  Figure   1  shows the TRON facility and a few example images from the two HIL domains of the SPEED+ dataset lightbox  and  sunlamp . In contrast to SPEED+ which contains static images at random poses, the SHIRT dataset includes sequential  lightbox  images obtained during dynamic RPOD scenarios. This allows the testing of navigation filters with ML algorithms in the loop. Overall, these HIL images can then be used as on-ground surrogates of otherwise unavailable spaceborne images for the evaluation of NN and the navigation algorithm robustness across domain gaps. SPEED+ was used as the core dataset of the second Satellite Pose Estimation Competition (SPEC2021) 4 4 4 https://kelvins.esa.int/pose-estimation-2021/  co-organized by SLAB and the Advanced Concepts Team (ACT) of the European Space Agency (ESA), in which the participants were tasked to predict the poses on SPEED+ HIL images while only having access to the labeled  synthetic  and unlabeled HIL domain images  [ 17 ] .",
            "As explained in  section   1 , it may be desirable to perform online training of NNs in space to fully close the domain gap by directly incorporating the flight images that only become available during RPOD  [ 19 ] . In this case, the NN must also be computationally efficient to run not only forward but also backward gradient propagation. Unlike inference, the training most certainly requires a GPU for the operation. However, the bright side is that the training need not run for every acquired image but only when there has been a substantial change in the imagery compared to the previous training round. The motivation is to avoid overfitting the NN to the specific scenery (e.g., Earth background) and view of the target since they do not change so abruptly within the short time frame in space especially at higher altitudes. Therefore, as long as there is a GPU onboard, the training latency is likely to be less of a computational bottleneck than inference which should be running in real time.",
            "The goal of this section is two-fold. One is to introduce the overall pose estimation architecture given an image of the target at a visible distance in an RPOD scenario ( section   4.1 ). Then, different NN architectures ( section   4.2 ) and data augmentation techniques ( section   4.3 ) for training a pose estimation NN are explained which are combined in various configurations to study and explore which aspects of NN architecture and training algorithm contribute the most to the OOD robustness and reduced latency of a pose estimation NN (Section  5 ). While SPN is the name of the first spacecraft pose estimation CNN introduced by  Sharma and DAmico [ 6 ] , it is also used throughout this work as a legacy name referring to all NN architectures designed or adopted for spacecraft pose estimation by SLAB. When referring to the first original SPN, it is always accompanied by the reference to the relevant work  [ 6 ] .",
            "Data augmentation is vital to training NNs that generalize well beyond their training data distribution and to preventing overfitting. As basic sets of augmentation, SPN heavily leverages the Albumentations library  [ 104 ]  for various image processing operations. For each training sample,  N N N italic_N  augmentation operations are randomly chosen from the set of designated operations in a manner similar to RandAugment  [ 105 ] . Specifically, while RandAugment utilizes a single hyperparameter to control the constant level of magnitude of these operations (e.g., standard deviation of white Gaussian noise), this work simply picks each operation with a random magnitude within some pre-defined window. Then, these  N N N italic_N  operations are applied to the original image in sequence. The complete list of employed augmentations is provided in  Table   1  along with their equivalent Albumentations commands. All commands are run with the default range of magnitudes.",
            "This section performs the trade-off analyses between robustness across domain gap and training/inference latency for various configurations of pose estimation architectures ( section   4.2 ), data augmentation techniques ( section   4.3 ), and other aspects of NN training such as transfer learning and input resolution. The ultimate goal is to find the configuration that is both robust on SPEED+ HIL domain images and computationally efficient on a representative hardware, capable of operating above nominal GN&C update frequency ( section   3.1 ) while also being batch-agnostic for potential online training ( section   3.3 ).",
            "Following SPEC2021  [ 17 ] , this section performs experiments using the SPEED+ dataset  [ 15 ,  113 ]  which consists of 59,960  synthetic  images split in 80:20 ratio into the training and validation sets. It further includes two HIL image domains acquired from the TRON facility and a mockup model of the Tango spacecraft: 6,740  lightbox  and 2,791  sunlamp  images which are reserved solely for testing. In this section, all NN models are trained on the  synthetic  training set and evaluated on the  synthetic  validation set,  lightbox  and  sunlamp  test sets, and 25 labeled flight images from acquired in-space during RPO of the PRISMA mission ( prisma25 ). Recall  Fig.   1  for visualization of a few HIL domain images.",
            "Given the ongoing interest in Artificial Intelligence (AI) for space applications, this work assumes that hardware support for edge computing to run NN onboard satellites will increase in the upcoming years (see  section   3.1.1 ). Therefore, this work adopts an NVIDIA Jetson Nano 4GB which contains a Quad-Core Arm   Cortex   A57 CPU and a 128-core NVIDIA MaxwellTM architecture GPU  [ 23 ] . Nano is the most restrictive version of the commercially available Jetson family, which suits the limited processing power of onboard GPUs as evidenced in the NASA small spacecraft technology report  [ 83 ] . Given its deprecated GPU architecture and CUDA language support, all NN training and inference on Jetson Nano are performed with full 32-bit Floating Point (FP32) precision using the C++ API of PyTorch 1.10.0.",
            "Note that in the overall pose estimation scenario ( section   4.1 ), the information on the relative position of the target is mostly included in the RoI detected from either the navigation filter or a separate object detection NN. Since the ground-truth RoIs are used for image pre-processing in this section, mean translation errors are expected to be very small throughout the experiments. Therefore, only the mean orientation errors ( E q subscript E q E_{\\text{q}} italic_E start_POSTSUBSCRIPT q end_POSTSUBSCRIPT  /  E q  superscript subscript E q E_{\\text{q}}^{*} italic_E start_POSTSUBSCRIPT q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) are reported for the majority of this section for brevity.",
            "Recall from the earlier discussion that the training is expected to run only occasionally whenever there has been a substantial change in the scenery and the targets pose ( section   3.1 ). Therefore, even the EfficientNet-B6 backbone could perform onboard online training asynchronously on a GPU given that there is enough power generated to support such a computationally heavy operation. In terms of the inference speed, all models considered in  Table   3  can effectively run in real-time for 0.5 Hz GN&C updates of the CPOD reference mission  [ 82 ]  assuming that the pre- and post-processing of the NN inputs and outputs do not become the computational bottleneck.",
            "The previous section investigates different aspects of designing and training a robust pose estimation NN. Under the same training conditions with identical input image resolutions, EfficientDet ( EfficientNet ) consistently outperforms all other models in terms of the OOD robustness as measured on the SPEED+ HIL domains. On the other hand, ViTPose ( ViT ) dominates the training and inference latency for different model sizes but with inferior OOD robustness. However, various ablation studies reveal that the ViTPose models can be made as robust as EfficientDet with improved pre-training of the ViT backbone, extensive data augmentation and increased input image resolution. Specifically, even with twice higher image resolution, the inference of ViTPose ( ViT-S/16 ) is still faster than that of EfficientDet ( EfficientNet-B0 ) that is 1/5 in size. Doubling the input resolution does lead to the increase of training latency by a factor of 4, but as stated in Requirement #1 ( section   3.1 ), the increased training time is less likely to become as mission-critical as the inference time on a satellite avionic with an onboard GPU. Moreover, ViTPose satisfies Requirement #3 by design ( section   3.3 ), completely lacking BN layers that could interfere with accurate online training on single images."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Ablation study on different pose estimation architectures and backbones. Latency and peak memory during training are measured on an NVIDIA RTX 4090 24GB GPU with batch size 1 and FP32 precision. Mean (std. dev.)  across all samples for each domain are reported. The boldface denotes the best performance within each group.",
        "table": "S4.T1.4",
        "footnotes": [],
        "references": [
            "This paper is organized as follows.  Section   2  provides a brief overview of literature on ML-based monocular pose estimation and existing methods to bridge domain gap.  Section   3  then goes over three key requirements that must be met by spaceborne ML models in addition to OOD robustness, two of which drive various design and training options for SPNv3 introduced in  section   4 .  Section   5  shows extensive experiments on the SPEED+ dataset to identify key elements of the design and training of SPNv3 that contribute the most to its robustness on HIL domain images and computational efficiency. The paper ends with conclusions and future works in  Section   6 .",
            "Many top-performing entries of SPEC2019 have adopted diverse CNN architectures and pose estimation strategies, such as probabilistic orientation estimation via soft classification  [ 7 ]  or estimation of 2D pixel coordinates of the designated keypoints on the spacecraft surface  [ 32 ,  33 ] . Specifically,  Chen et al. [ 33 ]  and  Park et al. [ 32 ] , who respectively ranked first and fourth places in the challenge, independently proposed a three-stage architecture: 1) an object detection CNN which is used to identify the Region-of-Interest (RoI) around the target, 2) a pose estimation CNN which takes in an image cropped around the detected RoI and outputs the 2D keypoint locations, and 3) a P n n n italic_n P module which solves for the full 6D pose based on the known correspondence of detected keypoint 2D locations and 3D model coordinates. Notably,  Park et al. [ 32 ]  directly regresses  ( x , y ) x y (x,y) ( italic_x , italic_y )  coordinates of the keypoints, whereas  Chen et al. [ 33 ]  outputs a set of 2D heatmaps whose peaks correspond to the locations of the keypoints. They also showed that cropping the input image around the target using a detected bounding box is crucial to performing pose estimation of the far-away target. For example, the original resolution of the SPEED images is 1920   \\times   1200, while most CNNs for ImageNet  [ 34 ]  classification expect 224   \\times   224 inputs. Cropping around the target prior to such dramatic down-scaling helps preserve much of the detailed target features that would otherwise be lost due to large inter-spacecraft separation as visualized in  Fig.   2 . Many CNN models that followed SPEC2019 also adopt similar strategies as well  [ 35 ,  36 ,  37 ,  8 ,  38 ] . The readers are referred to  Pasqualetto Cassinis et al. [ 39 ]  for a more comprehensive review of monocular spacecraft pose estimation using both conventional and deep learning-based methods.",
            "The goal of this section is two-fold. One is to introduce the overall pose estimation architecture given an image of the target at a visible distance in an RPOD scenario ( section   4.1 ). Then, different NN architectures ( section   4.2 ) and data augmentation techniques ( section   4.3 ) for training a pose estimation NN are explained which are combined in various configurations to study and explore which aspects of NN architecture and training algorithm contribute the most to the OOD robustness and reduced latency of a pose estimation NN (Section  5 ). While SPN is the name of the first spacecraft pose estimation CNN introduced by  Sharma and DAmico [ 6 ] , it is also used throughout this work as a legacy name referring to all NN architectures designed or adopted for spacecraft pose estimation by SLAB. When referring to the first original SPN, it is always accompanied by the reference to the relevant work  [ 6 ] .",
            "This section performs the trade-off analyses between robustness across domain gap and training/inference latency for various configurations of pose estimation architectures ( section   4.2 ), data augmentation techniques ( section   4.3 ), and other aspects of NN training such as transfer learning and input resolution. The ultimate goal is to find the configuration that is both robust on SPEED+ HIL domain images and computationally efficient on a representative hardware, capable of operating above nominal GN&C update frequency ( section   3.1 ) while also being batch-agnostic for potential online training ( section   3.3 ).",
            "This section begins with the study on OOD robustness and latency of various pose estimation architectures and NN backbones. First,  Table   2  tabulates the latency and pose accuracy of different pose estimation architectures with various backbone NNs of different sizes. These backbones cover a wide variety of NN architectures, including conventional CNNs with BN layers (e.g., EfficientNet  [ 20 ] , EfficientNetV2  [ 123 ] , MobileNetV3  [ 119 ] , HRNet  [ 101 ] ); NFNet  [ 125 ]  which has no separate  NORM  layers but performs weight standardization of the convolution weights  [ 127 ] ; Vision Transformers (ViT)  [ 22 ]  with self-attention modules  [ 128 ] ; architectures purely based on Multi-Layer Perceptrons (MLP) (e.g., ResMLP  [ 126 ] ); and hybrid architectures which both include convolution operations and self-attention layers for image patches as hinted by the use of both BN/LN layers (e.g., HRFormer  [ 120 ] , EfficientFormerV2  [ 121 ] , MobileViTv2  [ 122 ] .",
            "In order to facilitate readability,  Table   2  sorts different architectures according to their total number of parameters in ascending order and grouped into similar sizes. Overall, larger NNs tend to result in better OOD robustness as measured by the performance on the SPEED+  lightbox  and  sunlamp  domains. Then, by investigating the models within each size group, it becomes immediately clear that the EfficientDet ( EfficientNet ) and EfficientDet ( EfficientNetV2 ) consistently outperform all other models across different size groups in terms of robustness. They are closely followed by EfficientDet ( NFNet ) and ViTPose ( ViT ).",
            "Figure   6  condenses the information in  Table   2  by plotting the mean orientation error on the  sunlamp  domain and the inference latency against the number of parameters. Different families of backbones are plotted with different colors and marker shapes. First, the  E q  superscript subscript E q E_{\\text{q}}^{*} italic_E start_POSTSUBSCRIPT q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  plot in  Fig.   6  ( left ) shows that EfficientDet ( EfficientNet ) and EfficientDet ( EfficientNetV2 ) consistently outperform all other models in terms of OOD robustness. While EfficientDet ( NFNet ) models are closely behind them, there is no smaller variant of NFNet pre-trained on IN-1K that is publicly available. On the other hand, the latency plot in  Fig.   6  ( right ) shows that ViTPose ( ViT ) and ViTPose ( ResMLP ) are clear winners. Both architectures consist solely of MLP layers, enabling a faster throughput compared to those with convolution operations. Unfortunately, ResMLP cannot perform well across domain gaps, which is no surprise since it is a pure MLP-based NN with no inductive bias of convolution operations or global spatial knowledge via self-attention layers. However, ViTPose ( ViT ) models closely follow EfficientDet ( EfficientNet ) in terms of OOD robustness.",
            "The latency measured on the Jetson Nano reported in  Table   3  reaffirms the observation made in  Table   2  and  Fig.   6 . For the same number of parameters, EfficientDet models dominate on the OOD robustness front compared to the ViTPose models. However, ViTPose ( ViT ) models are consistently faster for inference regardless of the size. In fact, the largest ViT backbone considered in this studyViT-M/16 with nearly 40M parametersruns nearly 2.5 times faster than the smallest variant of the EfficientNet backbone that is 1/10 in size, and its training latency is on par with that of EfficientNet-B1 which only has 6.6M parameters. It also requires at most about 650 MB of GPU virtual memory for each training session with a single image input. Considering the 4GB of GPU memory available on the Jetson Nano and assuming that most of the GN&C-related operations run on CPUs, 650 MB or less is completely within the range of operation excluding the possibility of running other image-processing algorithms on the GPU.",
            "Judging from the comparison on the NVIDIA Jetson Nano reported in  Table   3 , it appears that EfficientDet models are more OOD robust for the same number of parameters, but the larger ViTPose models are still more computationally efficient than small EfficientDet models, overcoming its comparative weakness in OOD robustness with increased size. There is one more reason that ViTPose is favored over EfficientDet, and that is due to the requirement that the onboard NNs must be batch-agnostic for online training ( section   3.3 ). As evidenced in  Table   2 , most CNN backbones designed for ImageNet classification adopt BN layers, and even hybrid architectures always pair BN layers to each convolution operation. Then, is it possible to replace those BN layers with other batch-agnostic layers (e.g., LN, GN) when the overwhelming majority of publicly available CNN backbones pre-trained on ImageNet come with BN layers?"
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Peak training memory and latency of select models from  Table   2  are measured on an onboard GPU of NVIDIA Jetson Nano 4GB with PyTorch C++ API for batch size 1 and FP32 precision. The boldface denotes the best performance within each group.",
        "table": "Sx1.EGx2",
        "footnotes": [
            ""
        ],
        "references": [
            "This paper is organized as follows.  Section   2  provides a brief overview of literature on ML-based monocular pose estimation and existing methods to bridge domain gap.  Section   3  then goes over three key requirements that must be met by spaceborne ML models in addition to OOD robustness, two of which drive various design and training options for SPNv3 introduced in  section   4 .  Section   5  shows extensive experiments on the SPEED+ dataset to identify key elements of the design and training of SPNv3 that contribute the most to its robustness on HIL domain images and computational efficiency. The paper ends with conclusions and future works in  Section   6 .",
            "The goal of this section is two-fold. One is to introduce the overall pose estimation architecture given an image of the target at a visible distance in an RPOD scenario ( section   4.1 ). Then, different NN architectures ( section   4.2 ) and data augmentation techniques ( section   4.3 ) for training a pose estimation NN are explained which are combined in various configurations to study and explore which aspects of NN architecture and training algorithm contribute the most to the OOD robustness and reduced latency of a pose estimation NN (Section  5 ). While SPN is the name of the first spacecraft pose estimation CNN introduced by  Sharma and DAmico [ 6 ] , it is also used throughout this work as a legacy name referring to all NN architectures designed or adopted for spacecraft pose estimation by SLAB. When referring to the first original SPN, it is always accompanied by the reference to the relevant work  [ 6 ] .",
            "In all vision-based RPOD scenarios, the servicer spacecraft begins tracking the non-cooperative target at kilometers of separations as shown in  Fig.   3 . For example, Angles-Only Navigation (AON)  [ 96 ,  97 ,  95 ,  98 ]  obtains bearing angle measurements of the target from a Narrow Field-Of-View (NFOV) camera such as a star tracker which allows tracking of the targets relative orbital state via nonlinear filtering. AON continues until the inter-spacecraft distance becomes small enough such that the target appears resolved in the camera view, at which point the pose estimation algorithm kicks in.",
            "Given the scenario, this work adopts the two-stage approach visualized in  Fig.   3  as the common mechanism of the ML-based pose estimation algorithm which was first independently proposed by  Park et al. [ 32 ]  and  Chen et al. [ 33 ]  for SPEC2019  [ 31 ] :",
            "This section performs the trade-off analyses between robustness across domain gap and training/inference latency for various configurations of pose estimation architectures ( section   4.2 ), data augmentation techniques ( section   4.3 ), and other aspects of NN training such as transfer learning and input resolution. The ultimate goal is to find the configuration that is both robust on SPEED+ HIL domain images and computationally efficient on a representative hardware, capable of operating above nominal GN&C update frequency ( section   3.1 ) while also being batch-agnostic for potential online training ( section   3.3 ).",
            "Given the ongoing interest in Artificial Intelligence (AI) for space applications, this work assumes that hardware support for edge computing to run NN onboard satellites will increase in the upcoming years (see  section   3.1.1 ). Therefore, this work adopts an NVIDIA Jetson Nano 4GB which contains a Quad-Core Arm   Cortex   A57 CPU and a 128-core NVIDIA MaxwellTM architecture GPU  [ 23 ] . Nano is the most restrictive version of the commercially available Jetson family, which suits the limited processing power of onboard GPUs as evidenced in the NASA small spacecraft technology report  [ 83 ] . Given its deprecated GPU architecture and CUDA language support, all NN training and inference on Jetson Nano are performed with full 32-bit Floating Point (FP32) precision using the C++ API of PyTorch 1.10.0.",
            "The latency measured on the Jetson Nano reported in  Table   3  reaffirms the observation made in  Table   2  and  Fig.   6 . For the same number of parameters, EfficientDet models dominate on the OOD robustness front compared to the ViTPose models. However, ViTPose ( ViT ) models are consistently faster for inference regardless of the size. In fact, the largest ViT backbone considered in this studyViT-M/16 with nearly 40M parametersruns nearly 2.5 times faster than the smallest variant of the EfficientNet backbone that is 1/10 in size, and its training latency is on par with that of EfficientNet-B1 which only has 6.6M parameters. It also requires at most about 650 MB of GPU virtual memory for each training session with a single image input. Considering the 4GB of GPU memory available on the Jetson Nano and assuming that most of the GN&C-related operations run on CPUs, 650 MB or less is completely within the range of operation excluding the possibility of running other image-processing algorithms on the GPU.",
            "Recall from the earlier discussion that the training is expected to run only occasionally whenever there has been a substantial change in the scenery and the targets pose ( section   3.1 ). Therefore, even the EfficientNet-B6 backbone could perform onboard online training asynchronously on a GPU given that there is enough power generated to support such a computationally heavy operation. In terms of the inference speed, all models considered in  Table   3  can effectively run in real-time for 0.5 Hz GN&C updates of the CPOD reference mission  [ 82 ]  assuming that the pre- and post-processing of the NN inputs and outputs do not become the computational bottleneck.",
            "Judging from the comparison on the NVIDIA Jetson Nano reported in  Table   3 , it appears that EfficientDet models are more OOD robust for the same number of parameters, but the larger ViTPose models are still more computationally efficient than small EfficientDet models, overcoming its comparative weakness in OOD robustness with increased size. There is one more reason that ViTPose is favored over EfficientDet, and that is due to the requirement that the onboard NNs must be batch-agnostic for online training ( section   3.3 ). As evidenced in  Table   2 , most CNN backbones designed for ImageNet classification adopt BN layers, and even hybrid architectures always pair BN layers to each convolution operation. Then, is it possible to replace those BN layers with other batch-agnostic layers (e.g., LN, GN) when the overwhelming majority of publicly available CNN backbones pre-trained on ImageNet come with BN layers?",
            "The results so far indicate that ViTPose models are superior in terms of computational efficiency and batch-agnostic by nature, satisfying both Requirements #1 and #3 of  section   3 . However, they still fall short in OOD robustness when compared to EfficientDet models of comparable sizes. Therefore, it would be favorable to improve the OOD robustness of ViTPose models without substantial sacrifice of computational advantages. One potential remedy is to improve the transfer learning from the ImageNet pre-training. The ViT backbones of the ViTPose architecture so far are all pre-trained on IN-1K according to the DeiT III recipe  [ 124 ] . This is an improved training method compared to DeiT  [ 130 ]  which is, in turn, an improved training method compared to the vanilla ViT  [ 22 ] . In this section, the effect of ImageNet pre-training for ViT backbones is analyzed in detail by comparing the two different pre-training methods (DeiT vs. DeiT III) and datasets in  Table   5 . First, using random initialization without any pre-training on a large-scale dataset does not train well on SPEED+. For ViT-T/16 and ViT-S/16, using DeiT III leads to improved performance over DeiT when using only IN-1K for supervised pre-training. When the backbone is trained instead on a much larger ImageNet-22K (IN-22K)  [ 34 ]  which contains nearly 14M images and 22K class labels then fine-tuned on IN-1K, there is a marginal improvement in OOD robustness for both ViT-S/16 and ViT-M/16 models. The results overall indicate that better generalization of the backbone NN on a large-scale image classification dataset leads to a noticeable improvement in the downstream pose estimation task across the sim2real gap, though it seems the size of the pre-training dataset has little effect on the OOD robustness beyond the scale of IN-1K.",
            "The experiments so far used all three data augmentation techniquesStyle Augmentation, DeepAugment and RandConv. In order to verify the individual contribution of these augmentations,  Table   7  reports the mean orientation errors for different data augmentation configurations. The default baseline only consists of 5 random augmentations from the Albumentations library implemented in the style of RandAugment  [ 105 ]  ( section   4.3 ).",
            "The previous section investigates different aspects of designing and training a robust pose estimation NN. Under the same training conditions with identical input image resolutions, EfficientDet ( EfficientNet ) consistently outperforms all other models in terms of the OOD robustness as measured on the SPEED+ HIL domains. On the other hand, ViTPose ( ViT ) dominates the training and inference latency for different model sizes but with inferior OOD robustness. However, various ablation studies reveal that the ViTPose models can be made as robust as EfficientDet with improved pre-training of the ViT backbone, extensive data augmentation and increased input image resolution. Specifically, even with twice higher image resolution, the inference of ViTPose ( ViT-S/16 ) is still faster than that of EfficientDet ( EfficientNet-B0 ) that is 1/5 in size. Doubling the input resolution does lead to the increase of training latency by a factor of 4, but as stated in Requirement #1 ( section   3.1 ), the increased training time is less likely to become as mission-critical as the inference time on a satellite avionic with an onboard GPU. Moreover, ViTPose satisfies Requirement #3 by design ( section   3.3 ), completely lacking BN layers that could interfere with accurate online training on single images.",
            "Moving forward, the ViTPose models trained with the data augmentation proposed in  section   4.3  are referred to as SPNv3 for convenience. The default SPNv3 configuration is the ViT backbone pre-trained via DeiT III  [ 124 ]  on IN-22K then fine-tuned on IN-1K, patch size  P = 16 P 16 P=16 italic_P = 16 , and input resolution 448   \\times   448."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  EfficientDet architecture with the EfficientNet-B0 backbone evaluated with different  NORM  layers. The architectures and the backbones  NORM  layers during IN-1K pretraining are both noted. Mean (std. dev.)  of average performances over 3 training sessions with different random seeds are reported. Latency is measured on an NVIDIA Jetson Nano 4G. The boldface denotes the best performance within each group.",
        "table": "S5.E3",
        "footnotes": [],
        "references": [
            "This paper is organized as follows.  Section   2  provides a brief overview of literature on ML-based monocular pose estimation and existing methods to bridge domain gap.  Section   3  then goes over three key requirements that must be met by spaceborne ML models in addition to OOD robustness, two of which drive various design and training options for SPNv3 introduced in  section   4 .  Section   5  shows extensive experiments on the SPEED+ dataset to identify key elements of the design and training of SPNv3 that contribute the most to its robustness on HIL domain images and computational efficiency. The paper ends with conclusions and future works in  Section   6 .",
            "The goal of this section is two-fold. One is to introduce the overall pose estimation architecture given an image of the target at a visible distance in an RPOD scenario ( section   4.1 ). Then, different NN architectures ( section   4.2 ) and data augmentation techniques ( section   4.3 ) for training a pose estimation NN are explained which are combined in various configurations to study and explore which aspects of NN architecture and training algorithm contribute the most to the OOD robustness and reduced latency of a pose estimation NN (Section  5 ). While SPN is the name of the first spacecraft pose estimation CNN introduced by  Sharma and DAmico [ 6 ] , it is also used throughout this work as a legacy name referring to all NN architectures designed or adopted for spacecraft pose estimation by SLAB. When referring to the first original SPN, it is always accompanied by the reference to the relevant work  [ 6 ] .",
            "This section explores three different heatmap detection architecturesEfficientDet  [ 100 ] , HRNet  [ 101 ]  and ViTPose  [ 102 ] visualized in  Fig.   4 . Note that this is not a comprehensive list of state-of-the-art pose estimation NN architectures. Nonetheless, they are chosen due to their simplicity and in order to keep the number of training sessions tractable while exploring vastly different architectural options.",
            "EfficientDet was originally proposed by  Tan et al. [ 100 ]  to enable computationally efficient yet high-performance object detection and semantic segmentation. EfficientDet builds upon the EfficientNet  [ 20 ]  backbone designed for the image classification task. At the heart of EfficientDet is the Bidirectional Feature Pyramid Network (BiFPN), a novel FPN architecture  [ 103 ]  which fuses and processes feature outputs from the backbone at multiple scales. By fusing features from low- to high-resolution and vice versa, BiFPN outperforms previous state-of-the-art FPN architectures on various benchmark object detection and semantic segmentation datasets. In the end, the feature outputs from BiFPN are processed by the task-specific prediction heads composed of convolution layers. For tasks that require multi-scale predictions (e.g., object detection), features from all resolutions are processed. In this work, the heatmap prediction head is attached only to the P2 level feature as shown in  Fig.   4 , where P n n n italic_n  indicates that the feature size is  2  n superscript 2 n 2^{-n} 2 start_POSTSUPERSCRIPT - italic_n end_POSTSUPERSCRIPT  of the input image resolution. Only the P2 features (i.e., heatmap has 1/4 of the input resolution) are used since the accuracy of heatmap prediction heavily depends on the heatmap pixel resolution.",
            "High Resolution Network (HRNet)  [ 101 ]  is also designed for performing tasks that require multi-scale predictions. Recall that EfficientDet builds upon a CNN backbone that is typically designed for the image classification task, resulting in an architecture which continuously reduces the feature resolution all the way down to 1/32 of the original input. On the other hand, HRNet is designed to maintain high-resolution features throughout the entire forward propagation, outputting the multi-resolution features by the nature of its design. To facilitate learning at different scales, HRNet first processes the input image to 1/4 resolution, then gradually adds lower-resolution features in parallel, continuously fusing information across different scales as visualized in  Fig.   4 .",
            "This section performs the trade-off analyses between robustness across domain gap and training/inference latency for various configurations of pose estimation architectures ( section   4.2 ), data augmentation techniques ( section   4.3 ), and other aspects of NN training such as transfer learning and input resolution. The ultimate goal is to find the configuration that is both robust on SPEED+ HIL domain images and computationally efficient on a representative hardware, capable of operating above nominal GN&C update frequency ( section   3.1 ) while also being batch-agnostic for potential online training ( section   3.3 ).",
            "Note that in the overall pose estimation scenario ( section   4.1 ), the information on the relative position of the target is mostly included in the RoI detected from either the navigation filter or a separate object detection NN. Since the ground-truth RoIs are used for image pre-processing in this section, mean translation errors are expected to be very small throughout the experiments. Therefore, only the mean orientation errors ( E q subscript E q E_{\\text{q}} italic_E start_POSTSUBSCRIPT q end_POSTSUBSCRIPT  /  E q  superscript subscript E q E_{\\text{q}}^{*} italic_E start_POSTSUBSCRIPT q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) are reported for the majority of this section for brevity.",
            "Table   4  reports the orientation error of the EfficientDet ( EfficientNet-B0 ) model (4.1M parameters) with different  NORM  layers during the IN-1K pre-training and the main training on SPEED+  synthetic  images. The baseline is using BN layers for both sessions, which reports  E q = 6.63  subscript E q superscript 6.63 E_{\\text{q}}=6.63^{\\circ} italic_E start_POSTSUBSCRIPT q end_POSTSUBSCRIPT = 6.63 start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  on  lightbox  and  E q = 10.77  subscript E q superscript 10.77 E_{\\text{q}}=10.77^{\\circ} italic_E start_POSTSUBSCRIPT q end_POSTSUBSCRIPT = 10.77 start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  on  sunlamp  across 3 training sessions with different random seeds. When the BN layers of a pre-trained backbone is replaced with GN layers with group size 8, fully inheriting the trained affine parameters of the BN layers, the mean orientation error degrades by more than a factor of two. Training the network longer for 60 epochs instead of 30 does improve the overall performance, but it still does not reach the baseline level for all image domains. This was the conclusion of SPNv2 which also reported degradation in performance after replacing the BN with GN layers  [ 18 ] .",
            "The experiments so far used all three data augmentation techniquesStyle Augmentation, DeepAugment and RandConv. In order to verify the individual contribution of these augmentations,  Table   7  reports the mean orientation errors for different data augmentation configurations. The default baseline only consists of 5 random augmentations from the Albumentations library implemented in the style of RandAugment  [ 105 ]  ( section   4.3 ).",
            "Moving forward, the ViTPose models trained with the data augmentation proposed in  section   4.3  are referred to as SPNv3 for convenience. The default SPNv3 configuration is the ViT backbone pre-trained via DeiT III  [ 124 ]  on IN-22K then fine-tuned on IN-1K, patch size  P = 16 P 16 P=16 italic_P = 16 , and input resolution 448   \\times   448."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 :  Performance of the ViTPose pose estimation architecture with three backbones (ViT-T/16 & ViT-S/16 & ViT-M/16) with different pretraining datasets (None & ImageNet-1K (IN-1K) & ImageNet-22K (IN-22K)) and methods (DeiT  [ 130 ]  & DeiT III  [ 124 ] ). Mean (std. dev.)  of average performances over 3 training sessions with different random seeds are reported. The boldface denotes the best performance within each group.",
        "table": "Sx1.EGx4",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "This paper is organized as follows.  Section   2  provides a brief overview of literature on ML-based monocular pose estimation and existing methods to bridge domain gap.  Section   3  then goes over three key requirements that must be met by spaceborne ML models in addition to OOD robustness, two of which drive various design and training options for SPNv3 introduced in  section   4 .  Section   5  shows extensive experiments on the SPEED+ dataset to identify key elements of the design and training of SPNv3 that contribute the most to its robustness on HIL domain images and computational efficiency. The paper ends with conclusions and future works in  Section   6 .",
            "The goal of this section is two-fold. One is to introduce the overall pose estimation architecture given an image of the target at a visible distance in an RPOD scenario ( section   4.1 ). Then, different NN architectures ( section   4.2 ) and data augmentation techniques ( section   4.3 ) for training a pose estimation NN are explained which are combined in various configurations to study and explore which aspects of NN architecture and training algorithm contribute the most to the OOD robustness and reduced latency of a pose estimation NN (Section  5 ). While SPN is the name of the first spacecraft pose estimation CNN introduced by  Sharma and DAmico [ 6 ] , it is also used throughout this work as a legacy name referring to all NN architectures designed or adopted for spacecraft pose estimation by SLAB. When referring to the first original SPN, it is always accompanied by the reference to the relevant work  [ 6 ] .",
            "The results so far indicate that ViTPose models are superior in terms of computational efficiency and batch-agnostic by nature, satisfying both Requirements #1 and #3 of  section   3 . However, they still fall short in OOD robustness when compared to EfficientDet models of comparable sizes. Therefore, it would be favorable to improve the OOD robustness of ViTPose models without substantial sacrifice of computational advantages. One potential remedy is to improve the transfer learning from the ImageNet pre-training. The ViT backbones of the ViTPose architecture so far are all pre-trained on IN-1K according to the DeiT III recipe  [ 124 ] . This is an improved training method compared to DeiT  [ 130 ]  which is, in turn, an improved training method compared to the vanilla ViT  [ 22 ] . In this section, the effect of ImageNet pre-training for ViT backbones is analyzed in detail by comparing the two different pre-training methods (DeiT vs. DeiT III) and datasets in  Table   5 . First, using random initialization without any pre-training on a large-scale dataset does not train well on SPEED+. For ViT-T/16 and ViT-S/16, using DeiT III leads to improved performance over DeiT when using only IN-1K for supervised pre-training. When the backbone is trained instead on a much larger ImageNet-22K (IN-22K)  [ 34 ]  which contains nearly 14M images and 22K class labels then fine-tuned on IN-1K, there is a marginal improvement in OOD robustness for both ViT-S/16 and ViT-M/16 models. The results overall indicate that better generalization of the backbone NN on a large-scale image classification dataset leads to a noticeable improvement in the downstream pose estimation task across the sim2real gap, though it seems the size of the pre-training dataset has little effect on the OOD robustness beyond the scale of IN-1K.",
            "The proposed SPNv3 based on ViTPose is not without limitations. One major limitation is its dependence on a pre-trained ViT backbone. More specifically,  Table   5  reveals that the OOD robustness of the downstream pose estimation task depends not just on what dataset the ViT backbone is pre-trained on but also on how the pre-training was conducted. This observation implies that those who cannot afford the computational power nor the expertise of ImageNet pre-training are restricted in exploring and fine-tuning different architectural options. In fact, the dependence on ImageNet pre-training prevents this work from exploring other aspects of ViT backbones, e.g., varying the patch sizes ( P P P italic_P ). Furthermore, the optimally pre-trained backbone may not exist, in which case the sub-optimally pre-trained alternative may have to be used for training on SPEED+. In response to such cases, the authors previous work  [ 19 ]  shows how one can further train the sub-optimal NN onboard the satellite avionics in real-time during RPOD. However, if a NN can be trained according to the recipe analyzed in this section to be as OOD robust as possible, the online training on satellite avionics may be minimized or skipped altogether."
        ]
    },
    "id_table_6": {
        "caption": "Table 6 :  Performance of ViTPose ( ViT-S/16 ) pre-trained via DeiT III on IN-22K. The input image resolution and the standard deviation (   \\sigma italic_ ) of the ground-truth heatmaps are varied. Mean (std. dev.)  of average performances over 3 training sessions with different random seeds are reported. Latency is measured on an NVIDIA Jetson Nano 4GB. The boldface denotes the best performance.",
        "table": "Sx1.EGx5",
        "footnotes": [],
        "references": [
            "This paper is organized as follows.  Section   2  provides a brief overview of literature on ML-based monocular pose estimation and existing methods to bridge domain gap.  Section   3  then goes over three key requirements that must be met by spaceborne ML models in addition to OOD robustness, two of which drive various design and training options for SPNv3 introduced in  section   4 .  Section   5  shows extensive experiments on the SPEED+ dataset to identify key elements of the design and training of SPNv3 that contribute the most to its robustness on HIL domain images and computational efficiency. The paper ends with conclusions and future works in  Section   6 .",
            "Figure   6  condenses the information in  Table   2  by plotting the mean orientation error on the  sunlamp  domain and the inference latency against the number of parameters. Different families of backbones are plotted with different colors and marker shapes. First, the  E q  superscript subscript E q E_{\\text{q}}^{*} italic_E start_POSTSUBSCRIPT q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  plot in  Fig.   6  ( left ) shows that EfficientDet ( EfficientNet ) and EfficientDet ( EfficientNetV2 ) consistently outperform all other models in terms of OOD robustness. While EfficientDet ( NFNet ) models are closely behind them, there is no smaller variant of NFNet pre-trained on IN-1K that is publicly available. On the other hand, the latency plot in  Fig.   6  ( right ) shows that ViTPose ( ViT ) and ViTPose ( ResMLP ) are clear winners. Both architectures consist solely of MLP layers, enabling a faster throughput compared to those with convolution operations. Unfortunately, ResMLP cannot perform well across domain gaps, which is no surprise since it is a pure MLP-based NN with no inductive bias of convolution operations or global spatial knowledge via self-attention layers. However, ViTPose ( ViT ) models closely follow EfficientDet ( EfficientNet ) in terms of OOD robustness.",
            "Based on the OOD robustness and latency trade-off in  Fig.   6 , only EfficientDet ( EfficientNet ), EfficientDet ( EfficientNetV2 ) and ViTPose ( ViT ) are considered for further analyses.",
            "The latency measured on the Jetson Nano reported in  Table   3  reaffirms the observation made in  Table   2  and  Fig.   6 . For the same number of parameters, EfficientDet models dominate on the OOD robustness front compared to the ViTPose models. However, ViTPose ( ViT ) models are consistently faster for inference regardless of the size. In fact, the largest ViT backbone considered in this studyViT-M/16 with nearly 40M parametersruns nearly 2.5 times faster than the smallest variant of the EfficientNet backbone that is 1/10 in size, and its training latency is on par with that of EfficientNet-B1 which only has 6.6M parameters. It also requires at most about 650 MB of GPU virtual memory for each training session with a single image input. Considering the 4GB of GPU memory available on the Jetson Nano and assuming that most of the GN&C-related operations run on CPUs, 650 MB or less is completely within the range of operation excluding the possibility of running other image-processing algorithms on the GPU.",
            "Another potential remedy to improve the OOD robustness of ViTPose models is to increase the input image resolution. The effect of input image resolution of ViTPose on the OOD robustness and latency on the Jetson Nano is reported in  Table   6 . Unsurprisingly, increasing the resolution of the input image cropped around the target leads to improved performance across the board but at the cost of increased peak training memory and latency. Specifically, doubling the input resolution from 224   \\times   224 to 448   \\times   448 nearly quadruples the training time for each input. This is expected, since doubling the input resolution but maintaining the same patch size (16   \\times   16) leads to double the length of patch tokens fed into the transformer blocks, and the memory footprint of the scaled dot-product operation of self-attention modules grows quadratically to the token length. Even though the training latency is tantamount to that of EfficientDet ( EfficientNet-B6 ) operating on 224   \\times   224 inputs with nearly 40M parameters, the inference latency is still about a third and the training memory nearly half of EfficientDet ( EfficientNet-B6 ) while performing better on all image domains. The result suggests that it could be beneficial to use ViT-based models with higher input resolutions as long as the training time does not become the bottleneck.",
            "In the end, the ensemble performance with 448   \\times   448 inputs would rank SPNv3-S at first place in the  lightbox  category and third place in the  sunlamp  category of SPEC2021  [ 17 ] . Indeed, running an ensemble of multiple models may be computationally expensive especially onboard the satellite avionics. However, considering that the inference of SPNv3-S with 448   \\times   448 inputs only takes about 34 ms on a GPU of an NVIDIA Jetson Nano (see  Table   6 ), it may be possible to run an ensemble of a few models in real-time during RPOD. The largest variant of equivalent size, SPNv3-B with 86.3M parameters, marginally improves the accuracy, nearly matching that of EagerNet on  lightbox  yet still remaining in third place on  sunlamp . However, such a difference in performance between SPNv3-S and SPNv3-B would likely not yield significant improvement that justifies a near quadruple increase in size when processed by an onboard navigation filter."
        ]
    },
    "id_table_7": {
        "caption": "Table 7 :  Performance of the ViTPose-S/16 different different data augmentation configurations. Mean (std. dev.)  of average performances over 3 training sessions with different random seeds are reported. The boldface denotes the best performance.",
        "table": "S5.T2.41",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "The experiments so far used all three data augmentation techniquesStyle Augmentation, DeepAugment and RandConv. In order to verify the individual contribution of these augmentations,  Table   7  reports the mean orientation errors for different data augmentation configurations. The default baseline only consists of 5 random augmentations from the Albumentations library implemented in the style of RandAugment  [ 105 ]  ( section   4.3 ).",
            "Table   8  compares the final performances of SPNv3-S, a mid-size variant that takes the ViT-S/16 backbone (22.7M parameters), against the top entries of SPEC2021  [ 17 ]  and other state-of-the-art models introduced during the post-mortem competition following SPEC2021 and others. It can be seen that, with a higher input resolution at 448   \\times   448, SPNv3-S already outperforms the top entries of the  lightbox  category. Moreover, ensembling the output heatmap predictions from three models trained with different random seeds further improves the overall performance, achieving  E q = 2.69  subscript E q superscript 2.69 E_{\\text{q}}=2.69^{\\circ} italic_E start_POSTSUBSCRIPT q end_POSTSUBSCRIPT = 2.69 start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  on  lightbox  and  E q = 3.89  subscript E q superscript 3.89 E_{\\text{q}}=3.89^{\\circ} italic_E start_POSTSUBSCRIPT q end_POSTSUBSCRIPT = 3.89 start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  on  sunlamp . Some of the pose predictions by the ensemble are visualized in  Fig.   7  via reprojection of the Tango wireframe model. Note that the winning teams for respective HIL categoriesTangoUnchained and lava1302 adopted adversarial training, directly utilizing the unlabeled images of the HIL domains. On the other hand, the proposed SPNv3 does  not  utilize any information about the HIL domain during the training phase. The performance of SPNv3 is comparable to that of EagerNet  [ 80 ]  which also does not involve the HIL domain images during training. Note that EagerNet uses a pre-trained ConvNeXt-Base network with 89M parameters and generates multiple pose hypotheses from dense predictions of model coordinates and predicted errors, which are further refined iteratively using a region-based approach. On the other hand, SPNv3-S is smaller and simpler, predicting keypoint locations from lower-resolution heatmaps after just a single forward propagation."
        ]
    },
    "id_table_8": {
        "caption": "Table 8 :  Performance of SPNv3 models and state-of-the-art. Mean (std. dev.)  denotes the average performance over 3 training sessions with different random seeds. The boldface denotes the best performance in each group.",
        "table": "S5.T3.21",
        "footnotes": [],
        "references": [
            "Table   8  compares the final performances of SPNv3-S, a mid-size variant that takes the ViT-S/16 backbone (22.7M parameters), against the top entries of SPEC2021  [ 17 ]  and other state-of-the-art models introduced during the post-mortem competition following SPEC2021 and others. It can be seen that, with a higher input resolution at 448   \\times   448, SPNv3-S already outperforms the top entries of the  lightbox  category. Moreover, ensembling the output heatmap predictions from three models trained with different random seeds further improves the overall performance, achieving  E q = 2.69  subscript E q superscript 2.69 E_{\\text{q}}=2.69^{\\circ} italic_E start_POSTSUBSCRIPT q end_POSTSUBSCRIPT = 2.69 start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  on  lightbox  and  E q = 3.89  subscript E q superscript 3.89 E_{\\text{q}}=3.89^{\\circ} italic_E start_POSTSUBSCRIPT q end_POSTSUBSCRIPT = 3.89 start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  on  sunlamp . Some of the pose predictions by the ensemble are visualized in  Fig.   7  via reprojection of the Tango wireframe model. Note that the winning teams for respective HIL categoriesTangoUnchained and lava1302 adopted adversarial training, directly utilizing the unlabeled images of the HIL domains. On the other hand, the proposed SPNv3 does  not  utilize any information about the HIL domain during the training phase. The performance of SPNv3 is comparable to that of EagerNet  [ 80 ]  which also does not involve the HIL domain images during training. Note that EagerNet uses a pre-trained ConvNeXt-Base network with 89M parameters and generates multiple pose hypotheses from dense predictions of model coordinates and predicted errors, which are further refined iteratively using a region-based approach. On the other hand, SPNv3-S is smaller and simpler, predicting keypoint locations from lower-resolution heatmaps after just a single forward propagation."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "S5.T4.9",
        "footnotes": [],
        "references": []
    },
    "id_table_10": {
        "caption": "",
        "table": "S5.T5.18",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "",
        "table": "S5.T6.14",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "",
        "table": "S5.T7.3",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "",
        "table": "S5.T8.10",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": []
    }
}