{
    "PAPER'S NUMBER OF TABLES": 3,
    "S3.tab1": {
        "caption": "Table 1: Dataset composition of all training and test sets. The proportion of male and female speakers is close to 50%.",
        "table": "<table id=\"S3.tab1.1\" class=\"ltx_tabular ltx_figure_panel ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.tab1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.tab1.1.1.1.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_bb ltx_border_l ltx_border_tt\">Dataset</th>\n<td id=\"S3.tab1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_bb ltx_border_tt\">Utterances</td>\n<td id=\"S3.tab1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_bb ltx_border_tt\">Duration [h]</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "& 37768  134.5\nTraining federated  110570  367.8\ncomplete  148338  502.3\ninitial  487  1.7\nTest federated  682  2.3\nunseen  476  1.1The results of the PyTorch-Kaldi experiments are summarized in Table¬†4.1. As expected, the performance improves along the training phases, from the GMM-HMM alignment model via the initial model to the FL-models. In all cases, the upper bound reference model performs best. FL-training was performed at the E-level, where the number in parentheses shows the number of epochs after which communication takes place. We find that more frequent update rounds of FedAvg slightly improve the overall results, with best performance observed for updates after every 1/2121/2 or 1/4141/4 of an epoch. The highest improvement through FL-training is seen on the federated test set, which contains utterances of the FL speakers. The recognition quality of the other test sets does not degrade after FL-training.& federated  initial  cost[GB]\nAligner  32.76  33.22  26.96  - \nInitial  19.06  18.18  14.05 - \nE(2)-M  19.08  17.86  14.05  326\nE(1)-M  19.01  17.79  14.05  651\nE(1/2)-M  19.02  17.67  13.90  1302\nE(1/4)-M  18.94  17.71  13.94  2604\nRef.¬†1  17.77  15.78  13.24  -\nRef.¬†2  17.70  16.13  13.29  -\n\n\n\n\nTable¬†4.2 shows the experimental results using the ESPnet toolkit. Analogously to the PyTorch-Kaldi experiments, the upper-bound reference model provides best performance. For C-level FL, both averaging methods are applied, but as there is no large difference between W- and M-averaging, we applied only W-averaging in the later experiments.The E-level experiments (E-100-W, E-350-W, and E-all-W) are trained on randomly selected N=[100,350,all]ùëÅ100350allN=[100,350,\\text{all}] speakers per round. As expected, we see that a larger number of clients NùëÅN tends towards a better performance.We also tested the B-level strategy (B), updating the model after every minibatch, which we hoped would give optimal performance, as it best approximates the situation in non-FL training. However, while the communication cost became higher as expected, there are slight performance improvements at best, which do not justify this extra effort.&federated  initial  cost [GB]\nInitial  8.98  6.94  4.23  - \nC-M 9.00 6.88 4.18 317 \nC-W 9.01 6.88 4.15 317\nE-100-W  8.91  6.89  4.17  925\nE-350-W  8.87  6.91  4.14  1618\nE-all-W  8.81  6.86  4.17  4758\nB-W 8.93 6.81 4.15  13319\nRef  6.08  3.37  3.07  -We also see that the C-level updates, most effective in terms of communication cost, can still improve the recognition accuracy compared to the pretrained model, but the improvement does not reach that of the E-level updates. E-level updates with NùëÅN randomly selected speakers may also be closest to a realistic scenario, as we would not wish to ask every client to participate in every one of the model updates.We have shown that federated learning can slightly improve the recognition rates in automatic speech recognition, both using hybrid and end-to-end models. We suspect that one reason for the disappointing performance is to be found at the heart of the federated averaging approach. During FL-training, federated averaging leads to smaller gradients and, hence, to slow improvements.\nWhile it is possible that better performance may be attained by engaging more clients and allowing for more model updates, this comes at an excruciating communication cost. Therefore, we believe that it is necessary to focus on more effective collaborative learning strategies, e.g., based on suitable client selection criteria or on re-training only parts of the networks.\nFurthermore, McMahan et al.¬†[6] point to an inherent problem of the federated learning approach, when it is applied on non-Independent, Identically Distributed (non-IID) client data. Since on-device training data is naturally grouped by speaker, there are different statistical properties in every client, which may degrade the model performance. To counter this problem, a cross-silo training approach can be beneficial and will be subject of our future investigations. Federated Variational Noise (FVN) will also be investigated, which has shown its value in recovering FL-ASR performance despite non-IID client data [16].In addition to this global view, it is also interesting to look at the model architectures in detail:For hybrid models, federated learning imposes several difficulties and impracticalities. Due to the comparatively smaller size of the initial training data, decision tree learning results in a smaller output layer size of the FL-trained models in contrast to the full model. This also hints at a similar issue in the calculation of the language model, which we did not consider here, but which would also likely have a smaller size and larger perplexity if it is not updated‚Äîwhile simultaneously, any updates of the language model by nature lead to a leakage of private speech content.We also found that fewer iteration steps between FedAvg improve the quality of the model, but impose a heavy computational burden. Finally, the alignment model is not updated at the moment, and no realignments of client training data were performed after model averaging, which may also be a factor in the observed performance issues. This additionally raises the interesting question, in how far‚Äîexplicit or implicit‚Äîalignment is an issue in federated learning for time-series modeling in general.In contrast, the end-to-end-model suffers neither from an impacted decision-tree nor from a missing explicit re-alignment. We conducted many experiments to assess the influence of different parameters and hyper-parameters of federated training, finding that allowing more participants in each round of model averaging yields better performance, and that in trying to balance the performance and cost, epoch-level averaging appears most promising.All in all, however, we see the outcomes of our experiment as a pointer to underlying issues of the idea of federated averaging, when it is applied to complex, highly variable time series data, as seen in large-vocabulary speech recognition. We are releasing our source code in the hope of setting a starting point for further investigations into the important, yet also challenging question of how to achieve a reasonable balance of performance and cost in privacy-preserving machine learning.This work was funded by the PhD School ‚ÄúSecHuman - Security for Humans in Cyberspace‚Äù by the federal state of NRW, and by the German Federal Ministry of Education and Research (BMBF) within the ‚ÄúInnovations for Tomorrow‚Äôs Production, Services, and Work‚Äù Program (02L19C200), a project that is implemented by the Project Management Agency Karlsruhe (PTKA). The authors are responsible for the content of this publication.",
        "references": [
            [
                "For this work, we reorganized the Librispeech",
                "[",
                "22",
                "]",
                " dataset, creating dedicated training, test and development sets to simulate a realistic federated learning environment.",
                "Therefore, we created a server-side training set for our initial and alignment models (",
                "initial",
                "). Additionally, we created user datasets, which contain training, development, and test sets for every client. All 1372 clients are disjoint from the initial training set and from each other.\nFurthermore, the union of the ",
                "federated",
                " and ",
                "initial",
                " training sets is termed ",
                "complete",
                " and used to obtain an upper performance bound, corresponding to what would be seen if all data were located on a central server, i.e.¬†with standard, non-privacy-preserving learning.",
                "For testing, we created three sets: ",
                "unseen",
                " is composed of previously unseen speakers, ",
                "initial",
                " contains speakers from the training set of the initial model, and ",
                "federated",
                " contains speakers from the federated training set, so it simulates clients that can profit from their data contribution.",
                "Statistics of this dataset partitioning are shown in Tab.¬†",
                "3",
                "."
            ]
        ]
    },
    "S4.SS1.tab1": {
        "caption": "Table 2: WER (%) of hybrid models trained conventionally and using E-level federated learning, with associated communication cost [GB]. The final two rows show the WERs of upper bound reference models trained on the complete set. Ref.¬†1 learns its decision tree on the complete set; Ref.¬†2 uses the decision tree of the initial model.",
        "table": "",
        "footnotes": "",
        "references": [
            [
                "The results of the PyTorch-Kaldi experiments are summarized in Table¬†",
                "4.1",
                ". As expected, the performance improves along the training phases, from the GMM-HMM alignment model via the initial model to the FL-models. In all cases, the upper bound reference model performs best. FL-training was performed at the E-level, where the number in parentheses shows the number of epochs after which communication takes place. We find that more frequent update rounds of ",
                "FedAvg",
                " slightly improve the overall results, with best performance observed for updates after every ",
                "1",
                "/",
                "2",
                "1",
                "2",
                "1/2",
                " or ",
                "1",
                "/",
                "4",
                "1",
                "4",
                "1/4",
                " of an epoch. The highest improvement through FL-training is seen on the ",
                "federated",
                " test set, which contains utterances of the FL speakers. The recognition quality of the other test sets does not degrade after FL-training.",
                "& ",
                "federated",
                "  ",
                "initial",
                "  cost[GB]\nAligner  32.76  33.22  26.96  - \nInitial  19.06  18.18  14.05 - \nE(2)-M  19.08  17.86  14.05  326\nE(1)-M  19.01  17.79  14.05  651\nE(1/2)-M  19.02  ",
                "17.67",
                "  ",
                "13.90",
                "  1302\nE(1/4)-M  ",
                "18.94",
                "  17.71  13.94  2604\nRef.¬†1  17.77  15.78  13.24  -\nRef.¬†2  17.70  16.13  13.29  -\n\n\n\n\n"
            ]
        ]
    },
    "S4.SS2.tab1": {
        "caption": "Table 3: WER (%) of end-to-end models trained on the initial set and subsequently applying FL. Last row: WER of a reference model trained on the complete set. C, E, B represent C-, E-, and B-level updates; M and W indicate M- and W-averaging.",
        "table": "",
        "footnotes": "",
        "references": [
            [
                "Table¬†",
                "4.2",
                " shows the experimental results using the ESPnet toolkit. Analogously to the PyTorch-Kaldi experiments, the upper-bound reference model provides best performance. For C-level FL, both averaging methods are applied, but as there is no large difference between W- and M-averaging, we applied only W-averaging in the later experiments.",
                "The E-level experiments (E-100-W, E-350-W, and E-all-W) are trained on randomly selected ",
                "N",
                "=",
                "[",
                "100",
                ",",
                "350",
                ",",
                "all",
                "]",
                "ùëÅ",
                "100",
                "350",
                "all",
                "N=[100,350,\\text{all}]",
                " speakers per round. As expected, we see that a larger number of clients ",
                "N",
                "ùëÅ",
                "N",
                " tends towards a better performance.",
                "We also tested the B-level strategy (",
                "B",
                "), updating the model after every minibatch, which we hoped would give optimal performance, as it best approximates the situation in non-FL training. However, while the communication cost became higher as expected, there are slight performance improvements at best, which do not justify this extra effort.",
                "&",
                "federated",
                "  ",
                "initial",
                "  cost [GB]\nInitial  8.98  6.94  4.23  - \nC-M 9.00 6.88 4.18 317 \nC-W 9.01 6.88 4.15 317\nE-100-W  8.91  6.89  4.17  925\nE-350-W  8.87  6.91  ",
                "4.14",
                "  1618\nE-all-W  ",
                "8.81",
                "  6.86  4.17  4758\nB-W 8.93 ",
                "6.81",
                " 4.15  13319\nRef  6.08  3.37  3.07  -",
                "We also see that the C-level updates, most effective in terms of communication cost, can still improve the recognition accuracy compared to the pretrained model, but the improvement does not reach that of the E-level updates. E-level updates with ",
                "N",
                "ùëÅ",
                "N",
                " randomly selected speakers may also be closest to a realistic scenario, as we would not wish to ask every client to participate in every one of the model updates."
            ]
        ]
    }
}