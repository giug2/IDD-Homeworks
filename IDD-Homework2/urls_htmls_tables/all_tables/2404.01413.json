{
    "A3.T2": {
        "caption": "Table 2: Evaluation cross-entropy loss for different models at model-fitting iterations 1, 4 and 10 for replacement and accumulation regimes. (*) indicates a replacement regime with growing dataset size to ablate for total train set size.",
        "table": "<table id=\"A3.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A3.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"A3.T2.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">Model</th>\n<th id=\"A3.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">t=1</th>\n<th id=\"A3.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">t=4 (acc)</th>\n<th id=\"A3.T2.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">t=4 (repl)</th>\n<th id=\"A3.T2.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">t=10 (repl)</th>\n<th id=\"A3.T2.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">t=4 (*)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A3.T2.1.2.1\" class=\"ltx_tr\">\n<td id=\"A3.T2.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">GPT-2 (9M)</td>\n<td id=\"A3.T2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.82</td>\n<td id=\"A3.T2.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.74 (-0.07)</td>\n<td id=\"A3.T2.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.39 (+0.58)</td>\n<td id=\"A3.T2.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.91 (+1.09)</td>\n<td id=\"A3.T2.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">2.18 (+0.36)</td>\n</tr>\n<tr id=\"A3.T2.1.3.2\" class=\"ltx_tr\">\n<td id=\"A3.T2.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_r\">GPT-2 (9M) (temp=0.3)</td>\n<td id=\"A3.T2.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">1.82</td>\n<td id=\"A3.T2.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">1.75 (-0.06)</td>\n<td id=\"A3.T2.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\">5.82 (+4.00)</td>\n<td id=\"A3.T2.1.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\">9.85 (+8.04)</td>\n<td id=\"A3.T2.1.3.2.6\" class=\"ltx_td ltx_align_center\">n/a</td>\n</tr>\n<tr id=\"A3.T2.1.4.3\" class=\"ltx_tr\">\n<td id=\"A3.T2.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_r\">GPT-2 (9M) (small dataset)</td>\n<td id=\"A3.T2.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">2.56</td>\n<td id=\"A3.T2.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">2.28 (-0.28)</td>\n<td id=\"A3.T2.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\">3.21 (+0.65)</td>\n<td id=\"A3.T2.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_r\">3.72 (+1.16)</td>\n<td id=\"A3.T2.1.4.3.6\" class=\"ltx_td ltx_align_center\">2.91 (+0.35)</td>\n</tr>\n<tr id=\"A3.T2.1.5.4\" class=\"ltx_tr\">\n<td id=\"A3.T2.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_r\">ibid (+ 3 epochs)</td>\n<td id=\"A3.T2.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">1.99</td>\n<td id=\"A3.T2.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">1.87 (-0.12)</td>\n<td id=\"A3.T2.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_r\">2.62 (+0.63)</td>\n<td id=\"A3.T2.1.5.4.5\" class=\"ltx_td ltx_align_center ltx_border_r\">n/a</td>\n<td id=\"A3.T2.1.5.4.6\" class=\"ltx_td ltx_align_center\">n/a</td>\n</tr>\n<tr id=\"A3.T2.1.6.5\" class=\"ltx_tr\">\n<td id=\"A3.T2.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Llama-2 (12M)</td>\n<td id=\"A3.T2.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\">2.06</td>\n<td id=\"A3.T2.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\">1.94 (-0.12)</td>\n<td id=\"A3.T2.1.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_r\">2.72 (+0.66)</td>\n<td id=\"A3.T2.1.6.5.5\" class=\"ltx_td ltx_align_center ltx_border_r\">n/a</td>\n<td id=\"A3.T2.1.6.5.6\" class=\"ltx_td ltx_align_center\">n/a</td>\n</tr>\n<tr id=\"A3.T2.1.7.6\" class=\"ltx_tr\">\n<td id=\"A3.T2.1.7.6.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Llama-2 (42M)</td>\n<td id=\"A3.T2.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\">1.90</td>\n<td id=\"A3.T2.1.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\">1.76 (-0.14)</td>\n<td id=\"A3.T2.1.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_r\">2.52 (+0.62)</td>\n<td id=\"A3.T2.1.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_r\">n/a</td>\n<td id=\"A3.T2.1.7.6.6\" class=\"ltx_td ltx_align_center\">n/a</td>\n</tr>\n<tr id=\"A3.T2.1.8.7\" class=\"ltx_tr\">\n<td id=\"A3.T2.1.8.7.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Llama-2 (126M)</td>\n<td id=\"A3.T2.1.8.7.2\" class=\"ltx_td ltx_align_center ltx_border_r\">1.71</td>\n<td id=\"A3.T2.1.8.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\">1.59 (-0.12)</td>\n<td id=\"A3.T2.1.8.7.4\" class=\"ltx_td ltx_align_center ltx_border_r\">2.23 (+0.53)</td>\n<td id=\"A3.T2.1.8.7.5\" class=\"ltx_td ltx_align_center ltx_border_r\">n/a</td>\n<td id=\"A3.T2.1.8.7.6\" class=\"ltx_td ltx_align_center\">n/a</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "We ablate for several additional potential confounds beyond generation temperature. First, when accumulating data, subsequent model iterations are trained on larger datasets than when replacing data. To control for this, we also perform experiments in which data is replaced, but the size of the (fully synthetic) dataset is grown to match the training set size in the accumulation regime. We find that model performance still degrades (albeit at a lower rate). This is shown in Appendix\u00a0C, Table\u00a02, right-most column.\nSecond, a possible concern could be that degrading performance when replacing data could be due to low model performance in iteration 1 (and thus the quality of the first synthetic dataset). We control for this by varying the amount of training performed in iteration 1 only and find that this has no significant impact.\nLastly, we find that our results are also consistent across varying dataset sizes and training epochs. These ablations are discussed in Appendix \u00a0F.",
            "One possible concern is that when accumulating data, the train dataset size will grow at each model-fitting iteration, meaning subsequent models will be trained on more aggregate data than their counterparts in the replacement regime. To control for this, we run experiments controlling for this. In this \u201creplace-multiple\u201d regime, we create a fully synthetic dataset at the end of each model-fitting iteration, but grow the size of this dataset to match that of the accumulated data in the accumulation regime. Table\u00a02 rightmost column shows that in this regime, evaluation loss still increases over model-fitting iterations.",
            "Most of our language model experiments were run with sampling temperature 1.01.01.0 during generation of new datasets. To ensure that this choice is not critical, we also run one experiment with temperature 0.30.30.3, and see that this shows similar results (with even larger increases in validation loss in the replacement regime than temperature 1.01.01.0), as shown in Table\u00a02, row 2, and Figure\u00a013.",
            "We similarly vary the size of the initial (and subsequent) training datasets and number of training epochs, and see that this has no qualitative effect on the results (Table\u00a02, rows 3 & 4 show training on 1/5th of the TinyStories dataset for 1 & 3 epochs, respectively)."
        ]
    },
    "S2.T1": {
        "caption": "Table 1: Data Accumulation Avoids Model Collapse in Language Modeling. Both 125M-parameter Llama2 as well as 9M GPT-2 models show decreasing quality when replacing data (R), but maintain high-quality text generations when accumulating data (A).",
        "table": "<table id=\"S2.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S2.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S2.T1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">Model</th>\n<th id=\"S2.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">Iteration</th>\n<th id=\"S2.T1.1.1.1.3\" class=\"ltx_td ltx_align_justify ltx_th ltx_th_column\">\n<span id=\"S2.T1.1.1.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.1.1.3.1.1\" class=\"ltx_p\">Sample Generation</span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S2.T1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Llama2 (125M)</td>\n<td id=\"S2.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3 (A)</td>\n<td id=\"S2.T1.1.2.1.3\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S2.T1.1.2.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.2.1.3.1.1\" class=\"ltx_p\">In the end, the crab found a smooth shell. He took it to a safe place under a tree. The crab put the shell where he found it. Tim and his mom were tired, but they were happy. They had a fun day at the beach. And they lived happily ever after. The end.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.3.2.1\" class=\"ltx_td ltx_border_r\"></td>\n<td id=\"S2.T1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">3 (R)</td>\n<td id=\"S2.T1.1.3.2.3\" class=\"ltx_td ltx_align_justify\">\n<span id=\"S2.T1.1.3.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.3.2.3.1.1\" class=\"ltx_p\">Henry asked his Mom why the golf sounded so special. His Mom explained that the line of lumber had something special that would help. She said that if you&#8217;re not sure, the lumber is special.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.4.3.1\" class=\"ltx_td ltx_border_r\"></td>\n<td id=\"S2.T1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">8 (R)</td>\n<td id=\"S2.T1.1.4.3.3\" class=\"ltx_td ltx_align_justify\">\n<span id=\"S2.T1.1.4.3.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.4.3.3.1.1\" class=\"ltx_p\">Friend Stan and Millie laughed together and prepared to spend the morning together. Mamaing Grandma&#8217;s possibilitant, twice would measure how much she lovedk. Everyone started to get ready when they started arguing until their mum upset.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.1.5.4\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">GPT2 (9M)</td>\n<td id=\"S2.T1.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">5 (A)</td>\n<td id=\"S2.T1.1.5.4.3\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S2.T1.1.5.4.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.5.4.3.1.1\" class=\"ltx_p\">Jack was so happy that he took care of the honey. He thought, &#8221;I care about the beautiful garden, because it is nice and clean.&#8221; He started to feed the flower every day. The flower grew bigger and taller, and Jack became very happy.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.1.6.5\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.6.5.1\" class=\"ltx_td ltx_border_r\"></td>\n<td id=\"S2.T1.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\">5 (R)</td>\n<td id=\"S2.T1.1.6.5.3\" class=\"ltx_td ltx_align_justify\">\n<span id=\"S2.T1.1.6.5.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.6.5.3.1.1\" class=\"ltx_p\">After playing, Lily got tired and quickly ran back to playing with her dolls. She opened her eyes and played with her dolls all day long. Her grandma was so happy that she screamed as she watched her look back at her original clothes and laughed.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.1.7.6\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.7.6.1\" class=\"ltx_td ltx_border_b ltx_border_r\"></td>\n<td id=\"S2.T1.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">10 (R)</td>\n<td id=\"S2.T1.1.7.6.3\" class=\"ltx_td ltx_align_justify ltx_border_b\">\n<span id=\"S2.T1.1.7.6.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.7.6.3.1.1\" class=\"ltx_p\">When she finished eating it, she tasted it all up. She said goodbye to her mom and said goodbye. Mommy smiled, feeling very proud of her. It was other. She knew that sharing is always easy to share her meal with her mom.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "We found that for all architectures, parameter counts, and sampling temperatures, as the number of model-fitting iterations increased, replacing data led to an increase in test cross entropy (Fig. 2 top). We also found that for all architectures, parameter counts, and sampling temperatures, as the number of model-fitting iterations increased, accumulating data led to equal-or-lower test cross entropy (Fig. 2 bottom). Lower temperature (0.3) led to a faster increase in test error than higher temperature (1.0) (Appendix Fig. 13), but the trend was consistent for both temperatures. Table\u00a01 shows samples of generated texts for GPT2 (9M) and Llama2 (125M) models at model-fitting iterations 3-5 when both accumulating and replacing data, as well as iterations 8-10 (replacing only)."
        ]
    }
}