{
    "S4.T1": {
        "caption": "Table 1: Few-shot classification accuracy (%) on Omniglot and miniImageNet benchmarks. We report the average accuracy over 2000 few-shot tasks for PsCo and self-supervised learning methods. Other reported numbers borrow from Khodadadeh etÂ al. (2021); Kong etÂ al. (2021). Bold entries indicate the best for each task configuration, among unsupervised and self-supervised methods.",
        "table": null,
        "footnotes": [],
        "references": [
            "We achieve state-of-the-art performance on few-shot classification benchmarks, Omniglot (Lake etÂ al., 2011) and miniImageNet (Ravi & Larochelle, 2017). For example, PsCo outperforms the prior art of UML, Meta-SVEBM (Kong etÂ al., 2021), by 5% accuracy gain (58.03â†’â†’\\rightarrow63.26), for 5-way 5-shot tasks of miniImageNet (see Table 1).",
            "Few-shot classification results.\nTable 1 shows the results of the few-shot classification with various (way, shot) tasks of Omniglot and miniImageNet. PsCo achieves state-of-the-art performance on both Omniglot and miniImageNet benchmarks under the unsupervised setting. For example, we obtain 5% accuracy gain (67.07 â†’â†’\\rightarrow 72.22) on miniImageNet 5-way 20-shot tasks. Moreover, the performance is even competitive with supervised meta-learning methods, ProtoNets (Snell etÂ al., 2017), and MAML (Finn etÂ al., 2017) as well.",
            "Supervised meta-learning models.\nWe use MAML (Finn etÂ al., 2017) and ProtoNets (Snell etÂ al., 2017) of Conv5 architectures of miniImageNet pretrained. Following the procedure of Snell etÂ al. (2017), we train the models via Adam (Kingma & Ba, 2015) with a learning rate of 0.001 and cut the learning rate in half for every training of 2000 episodes. We train them for 60K episodes and use the model of the best validation accuracy. We train them through a 5-way 5-shot, and the rest of the hyperparameters are referenced in their respective papers. We observe that their performances are similar to the performance described in Table 1.",
            "We here provide the experimental results of Table 1, 2, and 3 with 95% confidence intervals in Table 13, 14, and 15, respectively."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Few-shot classification accuracy (%) on cross-domain few-shot classification benchmarks. We transfer Conv5 trained on miniImageNet to each benchmark. We report the average accuracy over 2000 few-shot tasks for all methods, except Meta-SVEBM as it is evaluated over 200 tasks due to the long evaluation time. Bold entries indicate the best for each task configuration, among unsupervised and self-supervised methods.",
        "table": null,
        "footnotes": [],
        "references": [
            "We show that PsCo achieves comparable performance with supervised meta-learning methods in various few-shot classification benchmarks. For example, PsCo achieves 44.01% accuracy for 5-way 5-shot tasks of an unseen domain, Cars (Krause etÂ al., 2013), while supervised MAML (Finn etÂ al., 2017) does 41.17% (see Table 2).",
            "Small-scale cross-domain few-shot classification results. We here evaluate various Conv5 models meta-trained on miniImageNet as used in Section 4.1. Table 2 shows that PsCo outperforms all the baselines across all the benchmarks, except ChestX, which is too different from the distribution of miniImageNet (Oh etÂ al., 2022). Somewhat interestingly, PsCo competitive with supervised learning under these benchmarks, e.g., PsCo achieves 88% accuracy on CropDiseases 5-way 5-shot tasks, whereas MAML gets 77%. This implies that our unsupervised method, PsCo, generalizes on more diverse tasks than supervised learning, which is specialized to in-domain tasks.",
            "We here provide the experimental results of Table 1, 2, and 3 with 95% confidence intervals in Table 13, 14, and 15, respectively."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: 5-way 5-shot classification accuracy (%) on cross-domain few-shot benchmarks. We transfer ImageNet-trained ResNet-50 models to each benchmark. We report the average accuracy over 600 few-shot tasks.",
        "table": null,
        "footnotes": [],
        "references": [
            "We validate PsCo is also applicable to a large-scale dataset: e.g., we improve PsCo by 5.78% accuracy gain (47.67â†’â†’\\rightarrow53.45) for 5-way 5-shot tasks of Cars using large-scale unlabeled data, ImageNet (Deng etÂ al., 2009) (see Table 3).",
            "Table 3 shows that (i) PsCo consistently improves both MoCo and BYOL under this setup (e.g., 67%â†’82%â†’percent67percent8267\\%\\rightarrow 82\\% in CUB), and (ii) PsCo benefits from the large-scale dataset as we obtain a huge amount of performance gain on the benchmarks of large-similarity with ImageNet: CUB, Cars, Places, and Plantae. Consequently, we achieve comparable performance with the supervised learning baseline, except Cars, which shows that our PsCo is applicable to large-scale unlabeled datasets.",
            "We here provide the experimental results of Table 1, 2, and 3 with 95% confidence intervals in Table 13, 14, and 15, respectively."
        ]
    },
    "S4.T4": {
        "caption": "Table 4: Component ablation studies on Omniglot.",
        "table": null,
        "footnotes": [],
        "references": [
            "Component analysis.\nIn Table 4, we demonstrate the necessity of each component in PsCo by removing the components one by one: momentum encoder Ï•italic-Ï•\\phi, prediction head hâ„h, Sinkhorn-Knopp algorithm, top-Kğ¾K sampling for sampling support samples, and the MoCo objective, â„’ğ™¼ğš˜ğ™²ğš˜subscriptâ„’ğ™¼ğš˜ğ™²ğš˜\\mathcal{L}_{\\mathtt{MoCo}} (6). We found that the momentum network Ï•italic-Ï•\\phi and the prediction head hâ„h are critical architectural components in our framework like recent self-supervised learning frameworks (Grill etÂ al., 2020; Chen etÂ al., 2021). In addition, Table 4 shows that training with only our objective, â„’ğ™¿ğšœğ™²ğš˜subscriptâ„’ğ™¿ğšœğ™²ğš˜\\mathcal{L}_{\\mathtt{PsCo}} (5), achieves meaningful performance, but incorporating it into MoCo is more beneficial. To further validate that our task construction is progressively improved during meta-learning, we evaluate whether a query and a corresponding support sample have the same true label. Figure 2(a) shows that our task construction is progressively improved, i.e., the task requirement (i) described in Section 3.1 satisfies.",
            "Table 4 also verifies the contribution of the Sinkhorn-Knopp algorithm and Top-Kğ¾K sampling for the performance of PsCo. We further analyze the effect of the Sinkhorn-Knopp algorithm by measuring the overlap ratio of selected supports between different pseudo-labels. As shown in Figure 2(b), there are almost zero overlaps when using the Sinkhorn-Knopp algorithm, which means the constructed task is a valid few-shot task, satisfying the task requirement (ii) described in Section 3.1."
        ]
    },
    "S4.T6": {
        "caption": "Table 5: The ablation study with varying augmentation choices for ğ’œ1subscriptğ’œ1\\mathcal{A}_{1} and ğ’œ2subscriptğ’œ2\\mathcal{A}_{2} on miniImageNet.",
        "table": null,
        "footnotes": [],
        "references": [
            "In this section, we demonstrate the effectiveness of the proposed framework under standard few-shot learning benchmarks (Section 4.1) and cross-domain few-shot learning benchmarks (Section 4.2). We provide ablation studies regarding PsCo in Section 4.3. Following Lee etÂ al. (2021), we mainly use Conv4 and Conv5 architectures for Omniglot (Lake etÂ al., 2011) and miniImageNet (Ravi & Larochelle, 2017), respectively, for the backbone feature extractor fÎ¸subscriptğ‘“ğœƒf_{\\theta}. For the number of shots during meta-learning, we use K=1ğ¾1K=1 for Omniglot and K=4ğ¾4K=4 for miniImageNet (see Table 6 for the sensitivity of Kğ¾K). Other details are fully described in Appendix A. We omit the confidence intervals in this section for clarity, and the full results with them are provided in Appendix F.",
            "Augmentations.\nWe here confirm that weak augmentation for the momentum network (i.e., ğ’œ2subscriptğ’œ2\\mathcal{A}_{2}) is more effective than strong augmentation unlike other self-supervised learning literature (Chen etÂ al., 2020a; He etÂ al., 2020). We denote the standard augmentation consisting of both geometric and color transformations by Strong, and a weaker augmentation consisting of only geometric transformations as Weak (see details in Appendix A). As shown in Table 6, utilizing the weak augmentation for ğ’œ2subscriptğ’œ2\\mathcal{A}_{2} is much more beneficial since it helps to find an accurate pseudo-label assignment matrix ğ€ğ€{\\mathbf{A}}.",
            "Training Kğ¾K.\nWe also look at the effect of the training Kğ¾K, i.e. number of shots sampled online. We conduct the experiment with Kâˆˆ{1,4,16,64}ğ¾141664K\\in\\{1,4,16,64\\}. We observe that PsCo performs consistently well regardless of the choice of Kğ¾K as shown in Table 6. The proper Kğ¾K is suggested to obtain the best-performing models, e.g., K=4ğ¾4K=4 for miniImageNet and K=1ğ¾1K=1 for Omniglot are the best.",
            "We train our models via stochastic gradient descent (SGD) with a batch size of N=256ğ‘256N=256 for 400 epochs. Following Chen etÂ al. (2020b); Chen & He (2021), we use an initial learning rate of 0.03 with the cosine learning schedule, Ï„ğ™¼ğš˜ğ™²ğš˜=0.2subscriptğœğ™¼ğš˜ğ™²ğš˜0.2\\tau_{\\mathtt{MoCo}}=0.2, and a weight decay of 5Ã—10âˆ’45superscript1045\\times 10^{-4}. We use a queue size of M=16384ğ‘€16384M=16384 since Omniglot (Lake etÂ al., 2011) and miniImageNet (Ravi & Larochelle, 2017) has roughly 100k meta-training samples. Following Lee etÂ al. (2021), we use Conv4 and Conv5 for Omniglot and miniImageNet, respectively, for the backbone feature extractor fÎ¸subscriptğ‘“ğœƒf_{\\theta}. We describe the detailed architectures in Table 7. For projection and prediction MLPs, gÎ¸subscriptğ‘”ğœƒg_{\\theta} and hÎ¸subscriptâ„ğœƒh_{\\theta}, we use 2-layer MLPs with a hidden size of 2048 and an output dimension of 128. For the hyperparameters of PsCo, we use Ï„ğ™¿ğšœğ™²ğš˜=1subscriptğœğ™¿ğšœğ™²ğš˜1\\tau_{\\mathtt{PsCo}}=1 and a momentum parameter of m=0.99ğ‘š0.99m=0.99 (see Appendix B for the hyperparameter sensitivity). For the number of shots during meta-learning, we use K=1ğ¾1K=1 for Omniglot and K=4ğ¾4K=4 for miniImageNet (see Table 6 for the sensitivity of Kğ¾K). We use the last-epoch model for evaluation without any guidance from the meta-validation dataset."
        ]
    },
    "A1.T7": {
        "caption": "Table 7: Pytorch-like architecture descriptions for standard few-shot benchmarks",
        "table": null,
        "footnotes": [],
        "references": [
            "We train our models via stochastic gradient descent (SGD) with a batch size of N=256ğ‘256N=256 for 400 epochs. Following Chen etÂ al. (2020b); Chen & He (2021), we use an initial learning rate of 0.03 with the cosine learning schedule, Ï„ğ™¼ğš˜ğ™²ğš˜=0.2subscriptğœğ™¼ğš˜ğ™²ğš˜0.2\\tau_{\\mathtt{MoCo}}=0.2, and a weight decay of 5Ã—10âˆ’45superscript1045\\times 10^{-4}. We use a queue size of M=16384ğ‘€16384M=16384 since Omniglot (Lake etÂ al., 2011) and miniImageNet (Ravi & Larochelle, 2017) has roughly 100k meta-training samples. Following Lee etÂ al. (2021), we use Conv4 and Conv5 for Omniglot and miniImageNet, respectively, for the backbone feature extractor fÎ¸subscriptğ‘“ğœƒf_{\\theta}. We describe the detailed architectures in Table 7. For projection and prediction MLPs, gÎ¸subscriptğ‘”ğœƒg_{\\theta} and hÎ¸subscriptâ„ğœƒh_{\\theta}, we use 2-layer MLPs with a hidden size of 2048 and an output dimension of 128. For the hyperparameters of PsCo, we use Ï„ğ™¿ğšœğ™²ğš˜=1subscriptğœğ™¿ğšœğ™²ğš˜1\\tau_{\\mathtt{PsCo}}=1 and a momentum parameter of m=0.99ğ‘š0.99m=0.99 (see Appendix B for the hyperparameter sensitivity). For the number of shots during meta-learning, we use K=1ğ¾1K=1 for Omniglot and K=4ğ¾4K=4 for miniImageNet (see Table 6 for the sensitivity of Kğ¾K). We use the last-epoch model for evaluation without any guidance from the meta-validation dataset."
        ]
    },
    "A1.T8": {
        "caption": "Table 8: Pytorch-like augmentation descriptions for Omniglot and miniImageNet",
        "table": null,
        "footnotes": [],
        "references": [
            "Augmentations.\nWe describe the augmentations for Omniglot and miniImagenet in Table 8. For Omniglot, because it is difficult to apply many augmentations to gray-scale images, we use the same rule for weak and strong augmentations. For miniImageNet, we use only geometric transformations for the weak augmentation following Zheng etÂ al. (2021)."
        ]
    },
    "A2.T10": {
        "caption": "Table 9: Sensitivity of momentum mğ‘šm on miniImageNet (way, shot).",
        "table": null,
        "footnotes": [],
        "references": [
            "For the small-scale experiments, we use a momentum of m=0.99ğ‘š0.99m=0.99 and a temperature of Ï„ğ™¿ğšœğ™²ğš˜=1subscriptğœğ™¿ğšœğ™²ğš˜1\\tau_{\\mathtt{PsCo}}=1. We here provide more ablation experiments with varying the hyperparameters mğ‘šm and Ï„ğ™¿ğšœğ™²ğš˜subscriptğœğ™¿ğšœğ™²ğš˜\\tau_{\\mathtt{PsCo}}. Table 10 and 10 show the sensitivity of hyperparameters on the miniImageNet dataset. We observe that PsCo achieves good performance even for non-optimal hyperparameters."
        ]
    },
    "A3.T11": {
        "caption": "Table 11: Before and after adaptation of PsCo in few-shot classification.",
        "table": null,
        "footnotes": [],
        "references": [
            "We measure the performance with and without our adaptation scheme on various domains using miniImageNet-pretrained PsCo. Table 11 shows that our adaptation scheme enhances the way to adapt to each domain. In particular, the adaptation scheme is highly suggested for cross-domain few-shot classification scenarios."
        ]
    },
    "A5.T12": {
        "caption": "Table 12: Information of datasets for cross-domain few-shot benchmarks.",
        "table": null,
        "footnotes": [],
        "references": [
            "For the cross-domain few-shot benchmarks, we use eight different datasets. We describe the dataset information in Table 12.\nWe use the dataset split described in Tseng etÂ al. (2020) for the benchmark of high-similarity and we use the dataset split described in Guo etÂ al. (2020) for the benchmark of low-similarity.\nBecause we do not perform the meta-training procedure using the datasets of cross-domain benchmarks, we only utilize the meta-test splits on these datasets. We use the 84Ã—84848484\\times 84 resized samples for evaluation on small-scale experiments."
        ]
    },
    "A6.T13": {
        "caption": "Table 13: Few-shot classification accuracy (%) on Omniglot and miniImageNet with a 95% confidence interval over 2000 few-shot tasks.",
        "table": null,
        "footnotes": [],
        "references": [
            "We here provide the experimental results of Table 1, 2, and 3 with 95% confidence intervals in Table 13, 14, and 15, respectively."
        ]
    },
    "A6.T14": {
        "caption": "Table 14: Few-shot classification accuracy (%) on cross-domain few-shot classification benchmarks of Conv5 pretrained on miniImageNet with a 95% confidence interval over 2000 few-shot tasks.",
        "table": null,
        "footnotes": [],
        "references": [
            "We here provide the experimental results of Table 1, 2, and 3 with 95% confidence intervals in Table 13, 14, and 15, respectively."
        ]
    },
    "A6.T15": {
        "caption": "Table 15: Few-shot classification accuracy (%) on cross-domain few-shot classification benchmarks of pretrained ResNet-18/50 on ImageNet with a 95% confidence interval (5-way 5-shot).",
        "table": null,
        "footnotes": [],
        "references": [
            "We here provide the experimental results of Table 1, 2, and 3 with 95% confidence intervals in Table 13, 14, and 15, respectively."
        ]
    }
}