{
    "S4.T1": {
        "caption": "Table 1: Few-shot classification accuracy (%) on Omniglot and miniImageNet benchmarks. We report the average accuracy over 2000 few-shot tasks for PsCo and self-supervised learning methods. Other reported numbers borrow from Khodadadeh et al. (2021); Kong et al. (2021). Bold entries indicate the best for each task configuration, among unsupervised and self-supervised methods.",
        "table": null,
        "footnotes": [],
        "references": [
            "We achieve state-of-the-art performance on few-shot classification benchmarks, Omniglot (Lake et al., 2011) and miniImageNet (Ravi & Larochelle, 2017). For example, PsCo outperforms the prior art of UML, Meta-SVEBM (Kong et al., 2021), by 5% accuracy gain (58.03→→\\rightarrow63.26), for 5-way 5-shot tasks of miniImageNet (see Table 1).",
            "Few-shot classification results.\nTable 1 shows the results of the few-shot classification with various (way, shot) tasks of Omniglot and miniImageNet. PsCo achieves state-of-the-art performance on both Omniglot and miniImageNet benchmarks under the unsupervised setting. For example, we obtain 5% accuracy gain (67.07 →→\\rightarrow 72.22) on miniImageNet 5-way 20-shot tasks. Moreover, the performance is even competitive with supervised meta-learning methods, ProtoNets (Snell et al., 2017), and MAML (Finn et al., 2017) as well.",
            "Supervised meta-learning models.\nWe use MAML (Finn et al., 2017) and ProtoNets (Snell et al., 2017) of Conv5 architectures of miniImageNet pretrained. Following the procedure of Snell et al. (2017), we train the models via Adam (Kingma & Ba, 2015) with a learning rate of 0.001 and cut the learning rate in half for every training of 2000 episodes. We train them for 60K episodes and use the model of the best validation accuracy. We train them through a 5-way 5-shot, and the rest of the hyperparameters are referenced in their respective papers. We observe that their performances are similar to the performance described in Table 1.",
            "We here provide the experimental results of Table 1, 2, and 3 with 95% confidence intervals in Table 13, 14, and 15, respectively."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Few-shot classification accuracy (%) on cross-domain few-shot classification benchmarks. We transfer Conv5 trained on miniImageNet to each benchmark. We report the average accuracy over 2000 few-shot tasks for all methods, except Meta-SVEBM as it is evaluated over 200 tasks due to the long evaluation time. Bold entries indicate the best for each task configuration, among unsupervised and self-supervised methods.",
        "table": null,
        "footnotes": [],
        "references": [
            "We show that PsCo achieves comparable performance with supervised meta-learning methods in various few-shot classification benchmarks. For example, PsCo achieves 44.01% accuracy for 5-way 5-shot tasks of an unseen domain, Cars (Krause et al., 2013), while supervised MAML (Finn et al., 2017) does 41.17% (see Table 2).",
            "Small-scale cross-domain few-shot classification results. We here evaluate various Conv5 models meta-trained on miniImageNet as used in Section 4.1. Table 2 shows that PsCo outperforms all the baselines across all the benchmarks, except ChestX, which is too different from the distribution of miniImageNet (Oh et al., 2022). Somewhat interestingly, PsCo competitive with supervised learning under these benchmarks, e.g., PsCo achieves 88% accuracy on CropDiseases 5-way 5-shot tasks, whereas MAML gets 77%. This implies that our unsupervised method, PsCo, generalizes on more diverse tasks than supervised learning, which is specialized to in-domain tasks.",
            "We here provide the experimental results of Table 1, 2, and 3 with 95% confidence intervals in Table 13, 14, and 15, respectively."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: 5-way 5-shot classification accuracy (%) on cross-domain few-shot benchmarks. We transfer ImageNet-trained ResNet-50 models to each benchmark. We report the average accuracy over 600 few-shot tasks.",
        "table": null,
        "footnotes": [],
        "references": [
            "We validate PsCo is also applicable to a large-scale dataset: e.g., we improve PsCo by 5.78% accuracy gain (47.67→→\\rightarrow53.45) for 5-way 5-shot tasks of Cars using large-scale unlabeled data, ImageNet (Deng et al., 2009) (see Table 3).",
            "Table 3 shows that (i) PsCo consistently improves both MoCo and BYOL under this setup (e.g., 67%→82%→percent67percent8267\\%\\rightarrow 82\\% in CUB), and (ii) PsCo benefits from the large-scale dataset as we obtain a huge amount of performance gain on the benchmarks of large-similarity with ImageNet: CUB, Cars, Places, and Plantae. Consequently, we achieve comparable performance with the supervised learning baseline, except Cars, which shows that our PsCo is applicable to large-scale unlabeled datasets.",
            "We here provide the experimental results of Table 1, 2, and 3 with 95% confidence intervals in Table 13, 14, and 15, respectively."
        ]
    },
    "S4.T4": {
        "caption": "Table 4: Component ablation studies on Omniglot.",
        "table": null,
        "footnotes": [],
        "references": [
            "Component analysis.\nIn Table 4, we demonstrate the necessity of each component in PsCo by removing the components one by one: momentum encoder ϕitalic-ϕ\\phi, prediction head hℎh, Sinkhorn-Knopp algorithm, top-K𝐾K sampling for sampling support samples, and the MoCo objective, ℒ𝙼𝚘𝙲𝚘subscriptℒ𝙼𝚘𝙲𝚘\\mathcal{L}_{\\mathtt{MoCo}} (6). We found that the momentum network ϕitalic-ϕ\\phi and the prediction head hℎh are critical architectural components in our framework like recent self-supervised learning frameworks (Grill et al., 2020; Chen et al., 2021). In addition, Table 4 shows that training with only our objective, ℒ𝙿𝚜𝙲𝚘subscriptℒ𝙿𝚜𝙲𝚘\\mathcal{L}_{\\mathtt{PsCo}} (5), achieves meaningful performance, but incorporating it into MoCo is more beneficial. To further validate that our task construction is progressively improved during meta-learning, we evaluate whether a query and a corresponding support sample have the same true label. Figure 2(a) shows that our task construction is progressively improved, i.e., the task requirement (i) described in Section 3.1 satisfies.",
            "Table 4 also verifies the contribution of the Sinkhorn-Knopp algorithm and Top-K𝐾K sampling for the performance of PsCo. We further analyze the effect of the Sinkhorn-Knopp algorithm by measuring the overlap ratio of selected supports between different pseudo-labels. As shown in Figure 2(b), there are almost zero overlaps when using the Sinkhorn-Knopp algorithm, which means the constructed task is a valid few-shot task, satisfying the task requirement (ii) described in Section 3.1."
        ]
    },
    "S4.T6": {
        "caption": "Table 5: The ablation study with varying augmentation choices for 𝒜1subscript𝒜1\\mathcal{A}_{1} and 𝒜2subscript𝒜2\\mathcal{A}_{2} on miniImageNet.",
        "table": null,
        "footnotes": [],
        "references": [
            "In this section, we demonstrate the effectiveness of the proposed framework under standard few-shot learning benchmarks (Section 4.1) and cross-domain few-shot learning benchmarks (Section 4.2). We provide ablation studies regarding PsCo in Section 4.3. Following Lee et al. (2021), we mainly use Conv4 and Conv5 architectures for Omniglot (Lake et al., 2011) and miniImageNet (Ravi & Larochelle, 2017), respectively, for the backbone feature extractor fθsubscript𝑓𝜃f_{\\theta}. For the number of shots during meta-learning, we use K=1𝐾1K=1 for Omniglot and K=4𝐾4K=4 for miniImageNet (see Table 6 for the sensitivity of K𝐾K). Other details are fully described in Appendix A. We omit the confidence intervals in this section for clarity, and the full results with them are provided in Appendix F.",
            "Augmentations.\nWe here confirm that weak augmentation for the momentum network (i.e., 𝒜2subscript𝒜2\\mathcal{A}_{2}) is more effective than strong augmentation unlike other self-supervised learning literature (Chen et al., 2020a; He et al., 2020). We denote the standard augmentation consisting of both geometric and color transformations by Strong, and a weaker augmentation consisting of only geometric transformations as Weak (see details in Appendix A). As shown in Table 6, utilizing the weak augmentation for 𝒜2subscript𝒜2\\mathcal{A}_{2} is much more beneficial since it helps to find an accurate pseudo-label assignment matrix 𝐀𝐀{\\mathbf{A}}.",
            "Training K𝐾K.\nWe also look at the effect of the training K𝐾K, i.e. number of shots sampled online. We conduct the experiment with K∈{1,4,16,64}𝐾141664K\\in\\{1,4,16,64\\}. We observe that PsCo performs consistently well regardless of the choice of K𝐾K as shown in Table 6. The proper K𝐾K is suggested to obtain the best-performing models, e.g., K=4𝐾4K=4 for miniImageNet and K=1𝐾1K=1 for Omniglot are the best.",
            "We train our models via stochastic gradient descent (SGD) with a batch size of N=256𝑁256N=256 for 400 epochs. Following Chen et al. (2020b); Chen & He (2021), we use an initial learning rate of 0.03 with the cosine learning schedule, τ𝙼𝚘𝙲𝚘=0.2subscript𝜏𝙼𝚘𝙲𝚘0.2\\tau_{\\mathtt{MoCo}}=0.2, and a weight decay of 5×10−45superscript1045\\times 10^{-4}. We use a queue size of M=16384𝑀16384M=16384 since Omniglot (Lake et al., 2011) and miniImageNet (Ravi & Larochelle, 2017) has roughly 100k meta-training samples. Following Lee et al. (2021), we use Conv4 and Conv5 for Omniglot and miniImageNet, respectively, for the backbone feature extractor fθsubscript𝑓𝜃f_{\\theta}. We describe the detailed architectures in Table 7. For projection and prediction MLPs, gθsubscript𝑔𝜃g_{\\theta} and hθsubscriptℎ𝜃h_{\\theta}, we use 2-layer MLPs with a hidden size of 2048 and an output dimension of 128. For the hyperparameters of PsCo, we use τ𝙿𝚜𝙲𝚘=1subscript𝜏𝙿𝚜𝙲𝚘1\\tau_{\\mathtt{PsCo}}=1 and a momentum parameter of m=0.99𝑚0.99m=0.99 (see Appendix B for the hyperparameter sensitivity). For the number of shots during meta-learning, we use K=1𝐾1K=1 for Omniglot and K=4𝐾4K=4 for miniImageNet (see Table 6 for the sensitivity of K𝐾K). We use the last-epoch model for evaluation without any guidance from the meta-validation dataset."
        ]
    },
    "A1.T7": {
        "caption": "Table 7: Pytorch-like architecture descriptions for standard few-shot benchmarks",
        "table": null,
        "footnotes": [],
        "references": [
            "We train our models via stochastic gradient descent (SGD) with a batch size of N=256𝑁256N=256 for 400 epochs. Following Chen et al. (2020b); Chen & He (2021), we use an initial learning rate of 0.03 with the cosine learning schedule, τ𝙼𝚘𝙲𝚘=0.2subscript𝜏𝙼𝚘𝙲𝚘0.2\\tau_{\\mathtt{MoCo}}=0.2, and a weight decay of 5×10−45superscript1045\\times 10^{-4}. We use a queue size of M=16384𝑀16384M=16384 since Omniglot (Lake et al., 2011) and miniImageNet (Ravi & Larochelle, 2017) has roughly 100k meta-training samples. Following Lee et al. (2021), we use Conv4 and Conv5 for Omniglot and miniImageNet, respectively, for the backbone feature extractor fθsubscript𝑓𝜃f_{\\theta}. We describe the detailed architectures in Table 7. For projection and prediction MLPs, gθsubscript𝑔𝜃g_{\\theta} and hθsubscriptℎ𝜃h_{\\theta}, we use 2-layer MLPs with a hidden size of 2048 and an output dimension of 128. For the hyperparameters of PsCo, we use τ𝙿𝚜𝙲𝚘=1subscript𝜏𝙿𝚜𝙲𝚘1\\tau_{\\mathtt{PsCo}}=1 and a momentum parameter of m=0.99𝑚0.99m=0.99 (see Appendix B for the hyperparameter sensitivity). For the number of shots during meta-learning, we use K=1𝐾1K=1 for Omniglot and K=4𝐾4K=4 for miniImageNet (see Table 6 for the sensitivity of K𝐾K). We use the last-epoch model for evaluation without any guidance from the meta-validation dataset."
        ]
    },
    "A1.T8": {
        "caption": "Table 8: Pytorch-like augmentation descriptions for Omniglot and miniImageNet",
        "table": null,
        "footnotes": [],
        "references": [
            "Augmentations.\nWe describe the augmentations for Omniglot and miniImagenet in Table 8. For Omniglot, because it is difficult to apply many augmentations to gray-scale images, we use the same rule for weak and strong augmentations. For miniImageNet, we use only geometric transformations for the weak augmentation following Zheng et al. (2021)."
        ]
    },
    "A2.T10": {
        "caption": "Table 9: Sensitivity of momentum m𝑚m on miniImageNet (way, shot).",
        "table": null,
        "footnotes": [],
        "references": [
            "For the small-scale experiments, we use a momentum of m=0.99𝑚0.99m=0.99 and a temperature of τ𝙿𝚜𝙲𝚘=1subscript𝜏𝙿𝚜𝙲𝚘1\\tau_{\\mathtt{PsCo}}=1. We here provide more ablation experiments with varying the hyperparameters m𝑚m and τ𝙿𝚜𝙲𝚘subscript𝜏𝙿𝚜𝙲𝚘\\tau_{\\mathtt{PsCo}}. Table 10 and 10 show the sensitivity of hyperparameters on the miniImageNet dataset. We observe that PsCo achieves good performance even for non-optimal hyperparameters."
        ]
    },
    "A3.T11": {
        "caption": "Table 11: Before and after adaptation of PsCo in few-shot classification.",
        "table": null,
        "footnotes": [],
        "references": [
            "We measure the performance with and without our adaptation scheme on various domains using miniImageNet-pretrained PsCo. Table 11 shows that our adaptation scheme enhances the way to adapt to each domain. In particular, the adaptation scheme is highly suggested for cross-domain few-shot classification scenarios."
        ]
    },
    "A5.T12": {
        "caption": "Table 12: Information of datasets for cross-domain few-shot benchmarks.",
        "table": null,
        "footnotes": [],
        "references": [
            "For the cross-domain few-shot benchmarks, we use eight different datasets. We describe the dataset information in Table 12.\nWe use the dataset split described in Tseng et al. (2020) for the benchmark of high-similarity and we use the dataset split described in Guo et al. (2020) for the benchmark of low-similarity.\nBecause we do not perform the meta-training procedure using the datasets of cross-domain benchmarks, we only utilize the meta-test splits on these datasets. We use the 84×84848484\\times 84 resized samples for evaluation on small-scale experiments."
        ]
    },
    "A6.T13": {
        "caption": "Table 13: Few-shot classification accuracy (%) on Omniglot and miniImageNet with a 95% confidence interval over 2000 few-shot tasks.",
        "table": null,
        "footnotes": [],
        "references": [
            "We here provide the experimental results of Table 1, 2, and 3 with 95% confidence intervals in Table 13, 14, and 15, respectively."
        ]
    },
    "A6.T14": {
        "caption": "Table 14: Few-shot classification accuracy (%) on cross-domain few-shot classification benchmarks of Conv5 pretrained on miniImageNet with a 95% confidence interval over 2000 few-shot tasks.",
        "table": null,
        "footnotes": [],
        "references": [
            "We here provide the experimental results of Table 1, 2, and 3 with 95% confidence intervals in Table 13, 14, and 15, respectively."
        ]
    },
    "A6.T15": {
        "caption": "Table 15: Few-shot classification accuracy (%) on cross-domain few-shot classification benchmarks of pretrained ResNet-18/50 on ImageNet with a 95% confidence interval (5-way 5-shot).",
        "table": null,
        "footnotes": [],
        "references": [
            "We here provide the experimental results of Table 1, 2, and 3 with 95% confidence intervals in Table 13, 14, and 15, respectively."
        ]
    }
}