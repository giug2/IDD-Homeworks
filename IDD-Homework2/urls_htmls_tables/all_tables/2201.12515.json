{
    "PAPER'S NUMBER OF TABLES": 2,
    "S5.T1": {
        "caption": "TABLE I: Model structure for the three datasets.",
        "table": "<table id=\"S5.T1.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T1.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\">Benchmark</th>\n<th id=\"S5.T1.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Model structure configuration</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"4\"><span id=\"S5.T1.1.1.2.1.1.1\" class=\"ltx_text\">MNIST</span></th>\n<td id=\"S5.T1.1.1.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_t\">Conv2D(1, 20, 5, 1, 2), MaxPool, ReLU</td>\n</tr>\n<tr id=\"S5.T1.1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.1.3.2.1\" class=\"ltx_td ltx_align_left\">Conv2D(20, 50, 5, 1, 2), DropOut(0.5), MaxPool, ReLU</td>\n</tr>\n<tr id=\"S5.T1.1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.1.4.3.1\" class=\"ltx_td ltx_align_left\">FC(800, 50), ReLU</td>\n</tr>\n<tr id=\"S5.T1.1.1.5.4\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.1.5.4.1\" class=\"ltx_td ltx_align_left\">DropOut(0.5), FC(50, 10)</td>\n</tr>\n<tr id=\"S5.T1.1.1.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"4\"><span id=\"S5.T1.1.1.6.5.1.1\" class=\"ltx_text\">\n<span id=\"S5.T1.1.1.6.5.1.1.1\" class=\"ltx_inline-block\">\n<span id=\"S5.T1.1.1.6.5.1.1.1.1\" class=\"ltx_p\">Fashion</span>\n<span id=\"S5.T1.1.1.6.5.1.1.1.2\" class=\"ltx_p\">-MNIST</span>\n</span></span></th>\n<td id=\"S5.T1.1.1.6.5.2\" class=\"ltx_td ltx_align_left ltx_border_t\">Conv2D(1, 16, 5, 1, 2), MaxPool, ReLU</td>\n</tr>\n<tr id=\"S5.T1.1.1.7.6\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.1.7.6.1\" class=\"ltx_td ltx_align_left\">Conv2D(16, 32, 5, 1, 2), DropOut(0.5), MaxPool, ReLU</td>\n</tr>\n<tr id=\"S5.T1.1.1.8.7\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.1.8.7.1\" class=\"ltx_td ltx_align_left\">FC(512, 50), ReLU</td>\n</tr>\n<tr id=\"S5.T1.1.1.9.8\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.1.9.8.1\" class=\"ltx_td ltx_align_left\">DropOut(0.5), FC(50, 10)</td>\n</tr>\n<tr id=\"S5.T1.1.1.10.9\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.1.10.9.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" rowspan=\"5\"><span id=\"S5.T1.1.1.10.9.1.1\" class=\"ltx_text\">CIFAR-10</span></th>\n<td id=\"S5.T1.1.1.10.9.2\" class=\"ltx_td ltx_align_left ltx_border_t\">Conv2D(3, 6, 5, 1, 2), ReLU, MaxPool</td>\n</tr>\n<tr id=\"S5.T1.1.1.11.10\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.1.11.10.1\" class=\"ltx_td ltx_align_left\">Conv2D(6, 16, 5, 1, 2), ReLU, MaxPool</td>\n</tr>\n<tr id=\"S5.T1.1.1.12.11\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.1.12.11.1\" class=\"ltx_td ltx_align_left\">FC(400, 120), ReLU</td>\n</tr>\n<tr id=\"S5.T1.1.1.13.12\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.1.13.12.1\" class=\"ltx_td ltx_align_left\">FC(120, 84), ReLU</td>\n</tr>\n<tr id=\"S5.T1.1.1.14.13\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.1.14.13.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">FC(84, 10)</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "To evaluate the performance of FLDG and FLDG-L, we performed experiments on a workstation with Intel i7-9700k CPU, 16GB memory, NVIDIA GeForce GTX 1080Ti GPU, and Ubuntu operating system (version 18.04).\nWe conducted three experiments on benchmarks MNIST, Fashion-MNIST, and CIFAR-10¬†[38], respectively, and constructed corresponding FL models as shown in Table I using PyTorch (version 1.3.1).\nFor each convolution layer (i.e., Conv2D), we provide the information of input and output dimensions, kernel size, stride and padding.\nFor the fully connected layer (i.e., FC), we provide the information about input and output dimensions.\nFor the dropout layer (i.e., DropOut), we provide the probability of an element to be zeroed.\nTo enable device grouping, we used the pre-trained MobileNetV2 model from Keras (version 2.3.1) to extract feature maps (with a size of 1280√ó1128011280\\times 1) from device raw data.\nSince FLDG and FLDG-L involve random operations (e.g., device selection from each group), we ran each experiment ten times and used the mean value for fair comparison."
        ]
    },
    "S5.T2": {
        "caption": "TABLE II: Four cases of non-IID data distributions",
        "table": "<table id=\"S5.T2.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\">Dataset</th>\n<td id=\"S5.T2.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">MNIST</td>\n<td id=\"S5.T2.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">Fashion-MNIST</td>\n<td id=\"S5.T2.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">CIFAR-10</td>\n</tr>\n<tr id=\"S5.T2.1.1.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Sample # in total</th>\n<td id=\"S5.T2.1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">60000</td>\n<td id=\"S5.T2.1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">60000</td>\n<td id=\"S5.T2.1.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">50000</td>\n</tr>\n<tr id=\"S5.T2.1.1.3.3\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Sample # per IoT device</th>\n<td id=\"S5.T2.1.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">600</td>\n<td id=\"S5.T2.1.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">600</td>\n<td id=\"S5.T2.1.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">500</td>\n</tr>\n<tr id=\"S5.T2.1.1.4.4\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Case #</th>\n<td id=\"S5.T2.1.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\">Data Distributions</td>\n</tr>\n<tr id=\"S5.T2.1.1.5.5\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">1</th>\n<td id=\"S5.T2.1.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\">100% belong to one label</td>\n</tr>\n<tr id=\"S5.T2.1.1.6.6\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.1.6.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">2</th>\n<td id=\"S5.T2.1.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\">100% evenly belong to two labels</td>\n</tr>\n<tr id=\"S5.T2.1.1.7.7\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.1.7.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">3</th>\n<td id=\"S5.T2.1.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\">\n<table id=\"S5.T2.1.1.7.7.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T2.1.1.7.7.2.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.1.7.7.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">80% belong to one label,</td>\n</tr>\n<tr id=\"S5.T2.1.1.7.7.2.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.1.7.7.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">the remaining 20% belong to other labels</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr id=\"S5.T2.1.1.8.8\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.1.8.8.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\">4</th>\n<td id=\"S5.T2.1.1.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" colspan=\"3\">\n<table id=\"S5.T2.1.1.8.8.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T2.1.1.8.8.2.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.1.8.8.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">50% belong to one label,</td>\n</tr>\n<tr id=\"S5.T2.1.1.8.8.2.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.1.8.8.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">the remaining 50% belong to other labels</td>\n</tr>\n</table>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Although vanilla FL (i.e., FedAvg) has shown its excellent performance on collaborative learning with IID data, it cannot be directly applied to IoT scenarios.\nThis is because the data distributed on IoT devices is mostly non-IID due to various factors such as user preferences and device locations.\nWhen dealing with non-IID scenarios, the performance of vanilla FL degrades dramatically.\nFigure¬†1 shows a performance comparison among IID and four non-IID scenarios for FedAvg, which follow the distributions defined in Table¬†II.",
            "In the experiments, we considered an IoT application with 100 IoT devices. For FL training, we set the default number of groups and learning rate Œ∑ùúÇ\\eta to 10 and 0.01, respectively.\nFor each IoT device, we set the batch size and epoch of local training to 50 and 5,\nrespectively.\nFor the LSH functions, we choose the window size r=3.0ùëü3.0r=3.0.\nAccording to [18], we synthesized four non-IID data distributions shown in Table II based on Dirichlet distribution to generate FL training data.\nFor each experiment, we checked the test accuracy of obtained global models based on all the benchmark test data.\nThe following sub-sections firstly investigate the impacts of output dimension and the number of groups on FLDG and FLDG-L, and then compare the performance of FLDG and FLDG-L with state-of-the-art methods.",
            "To show the effectiveness of FLDG and FLDG-L for non-IID scenarios, we used the state-of-the-art approaches, i.e., vanilla FL (FedAvg)¬†[1] and K-Center¬†[20] as baselines for the comparison.\nFigure¬†6 presents the test accuracy results of the four approaches for the four non-IID cases defined in Table¬†II.\nFrom Figure¬†6 we can find that for all the four non-IID cases, FLDG and FLDG-L greatly outperform the two baselines in terms of convergence rate and test accuracy.\nFor example, in Figure¬†6(a) FLDG achieves better accuracy than FedAvg and K-Center by 13.2% and 46.3% in the 100t‚Äãhsuperscript100ùë°‚Ñé100^{th} round, respectively.\nSimilarly, in the same round FLDG-L outperforms the two baselines by 11.8% and 44.6%, respectively.\nFor all the cases, the performance of FLDG-L is similar to that of FLDG.\nIn other words, although FLDG-L is more secure than FLDG, the performance of FLDG-L can still be guaranteed with negligible LSH hashing overhead.\nThe reason why our approaches are superior is mainly because our device grouping method can reduce the disadvantages of weight divergence during the FL training process.\nTherefore, the model optimization direction of each aggregation round becomes more consistent with the optimization direction of the overall training.\nNote that in most cases the performance of K-Center fluctuates greatly.\nThis is because here K-Center conducts device clustering based on the model similarity in every 10 rounds, where the first round has the best performance while the last round gives the worst performance.\nMoreover, we can find that for each benchmark, the four methods in Case 1 have the largest difference, but in Case 4 the four methods have similar performance.\nThis is because that Case 1 has the most skewed non-IID device data while Case 4 has the\nleast skewed non-IID device data.\nIn other words, Case 4 tends to be more IID."
        ]
    }
}