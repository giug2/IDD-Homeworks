{
    "id_table_1": {
        "caption": "Table 1:  Comparison of RISSOLE with RDM [ Blattmann et al.(2022)Blattmann, Rombach, Oktay, Muller, and  Ommer ] , Patch Diffusion [ Wang et al.(2023)Wang, Jiang, Zheng, Wang, He, Wang, Chen, and  Zhou ]  and other variants of RISSOLE for unconditional image generation on ImageNet100  [ Deng et al.(2009)Deng, Dong, Socher, Li, Li, and  Fei-Fei ] .",
        "table": "S5.T1.1",
        "footnotes": [],
        "references": [
            "Our block-wise generation approach applies to DDPMs operating in the image space and those operating in a latent space. This work will illustrate our approach using the latent diffusion model (LDM)  [ Rombach et al.(2022)Rombach, Blattmann, Lorenz, Esser, and  Ommer ] , an example of the latter class of methods. Our approach (Fig.  1 ) assumes that the input to each forward/reverse diffusion step is partitioned into  b b b italic_b  disjoint blocks. Omitting the time-step subscript  t t t italic_t  and denoting the latent representation at any time-step as  z z z italic_z , we assume it to be partitioned into  b b b italic_b  equal-sized disjoint blocks as  z =  i = 0 b  1 z i z superscript subscript i 0 b 1 superscript z i z=\\bigcup_{i=0}^{b-1}z^{i} italic_z =  start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_b - 1 end_POSTSUPERSCRIPT italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT . Operating on smaller-sized blocks  z i superscript z i z^{i} italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT  naturally results in a model with fewer parameters, which is our primary goal. However, the challenge here is to ensure coherence among the blocks (e.g., avoiding artifacts at the block boundaries, semantic inconsistency between adjacent blocks, etc.). To achieve coherence, each block  z i superscript z i z^{i} italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT  is also conditioned on the  corresponding  block of a set of  k k k italic_k  nearest neighbors retrieved from the external database; Sec.  4.1.3  provides the details. We refer to our approach as RISSOLE (Paramete R -effic I ent Diffu S ion Models via Block-wi S e Generati O n and Retrieva L -Guidanc E ).",
            "Like RDM  [ Blattmann et al.(2022)Blattmann, Rombach, Oktay, Muller, and  Ommer ] , RISSOLE begins by picking a random  pseudo-query  from the database for generating new samples. However, unlike RDM, which uses a whole image as the query, we use its first block  z ^ 0 superscript ^ z 0 \\hat{z}^{0} over^ start_ARG italic_z end_ARG start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT  from the first block of our database  D 0 subscript D 0 \\mathcal{D}_{0} caligraphic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT . We then use the query  z ^ 0 superscript ^ z 0 \\hat{z}^{0} over^ start_ARG italic_z end_ARG start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT  to retrieve  k k k italic_k  nearest neighbors from the database (see Fig.  1  (d)).",
            "In training the VQ-GAN encoder-decoder, we have maintained  f = 8 f 8 f=8 italic_f = 8  (refer to Section  4.1.1 ) across both datasets. Specifically, for the CelebA dataset, the VQ-GANs latent dimension is configured to 10. Conversely, due to ImageNets higher resolution, the latent dimension is adjusted to 32. Consequently, the latent resolution for an image from CelebA will be  10  16  16 10 16 16 10\\times 16\\times 16 10  16  16 , while for ImageNet, it will be  32  28  28 32 28 28 32\\times 28\\times 28 32  28  28 . To retrieve the nearest neighbors from  D D \\mathcal{D} caligraphic_D , we use ScanNN  [ Guo et al.(2020)Guo, Sun, Lindgren, Geng, Simcha, Chern, and  Kumar ]  search algorithm in the latent representation generated by a VQ-GAN  [ Esser et al.(2021)Esser, Rombach, and Ommer ] .",
            "In alignment with the requirements of the chosen retrieval mechanism, each latent representation is flattened. For instance, considering the ImageNet100 dataset  X  R 224  224  3 X superscript R 224 224 3 \\mathcal{X}\\subset\\mathbb{R}^{224\\times 224\\times 3} caligraphic_X  blackboard_R start_POSTSUPERSCRIPT 224  224  3 end_POSTSUPERSCRIPT  with  | X | = n X n |\\mathcal{X}|=n | caligraphic_X | = italic_n , and  z  R 28  28  32 z superscript R 28 28 32 z\\in\\mathbb{R}^{28\\times 28\\times 32} italic_z  blackboard_R start_POSTSUPERSCRIPT 28  28  32 end_POSTSUPERSCRIPT  the dimensionality of  D D D italic_D  is denoted as  R b  n  6272 superscript R b n 6272 \\mathbb{R}^{b\\times n\\times 6272} blackboard_R start_POSTSUPERSCRIPT italic_b  italic_n  6272 end_POSTSUPERSCRIPT , with each  D i  R n  6272 subscript D i superscript R n 6272 \\mathcal{D}_{i}\\in\\mathbb{R}^{n\\times 6272} caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_n  6272 end_POSTSUPERSCRIPT  where  6272 6272 6272 6272  emerges from the flattening process applied to  z z z italic_z . Our models are trained for 200 epochs on an NVIDIA 1080Ti (CelebA64) and an NVIDIA A30 (ImageNet100). The number of nearest neighbors  k k k italic_k  can influence the performance of retrieval-based approaches. In our experiments,  k = 20 k 20 k=20 italic_k = 20  was found to work best for RDM  [ Blattmann et al.(2022)Blattmann, Rombach, Oktay, Muller, and  Ommer ]  and  k = 10 k 10 k=10 italic_k = 10  worked best for RISSOLE. In addition, we also conduct an ablation experiment in Sec.  5.4.1  to assess RISSOLEs performance when conditioned with positional information.",
            "Table  1  presents the FID scores obtained by RISSOLE and the other baseline models on both the CelebA and ImageNet datasets. RISSOLE achieves significantly lower FID scores, indicating that its generated samples are more likely to be from the original dataset distribution. These results demonstrate that RISSOLE captures the intricate patterns and structures in the data and generates more realistic and coherent images than the other baselines with similar model sizes. To further illustrate the qualitative differences, Fig.  3  compares samples generated by RISSOLE, original images from both datasets and samples from the RDM baseline. As depicted in Fig.  3 , RISSOLE produces noticeably better samples, closely resembling the characteristics of the original images. For fairness of comparisons, we aimed to ensure that the model sizes for RDM and RISSOLE are kept similar, as reported in Table  1 . To ensure roughly equal model sizes for RDM and RISSOLE, the U-net architecture used by RDM was suitably modified. However, note that because RDM still utilizes the entire image or latent space for training or sampling, it still requires a somewhat higher number of parameters under this constrained setting. Table  1  shows that RISSOLE outperforms RDM on both datasets and Patch Diffusion on CelebA  64  64 64 64 64\\times 64 64  64   3 3 3 FID values for Patch Diffusion are from the original paper. No FID results for ImageNet100 were reported. . When constrained to small model size, RDM struggles to model the full-sized images with high fidelity and faces difficulty producing coherent and interpretable sample outputs, as shown in the middle row of Fig.  3 . In contrast, RISSOLE operates only on small-sized blocks and can, therefore, model these blocks more effectively despite its small model size.",
            "In Section  4.1.3 , we described how RISSOLE learns a specific segment of the latent space,  z i superscript z i z^{i} italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , by conditioning it with  M D i ( k ) superscript subscript M subscript D i k \\mathcal{M}_{\\mathcal{D}_{i}}^{(k)} caligraphic_M start_POSTSUBSCRIPT caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , while also integrating positional information, represented by  i i i italic_i . This section delves into the significance of positional information within RISSOLE, suggesting that such information might already be inherent in  p   ( z i | M D i ( k ) ) subscript p  conditional superscript z i superscript subscript M subscript D i k p_{\\psi}(z^{i}|\\mathcal{M}_{\\mathcal{D}_{i}}^{(k)}) italic_p start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT | caligraphic_M start_POSTSUBSCRIPT caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) . Hence, the model can represented by a modified version of Equation  1 :",
            "The experiments are conducted using the CelebA dataset with a block count of  b = 4 b 4 b=4 italic_b = 4 . Figure  4  demonstrates two sets of samples generated by RISSOLE; the upper row displays samples with included positional information, while the lower row exhibits samples without positional information. Observation of the figure visually confirms that samples generated from RISSOLE without positional conditioning exhibit sharper and more realistic characteristics compared to those with positional encoding. This phenomenon arises from the additional complexity introduced by positional encoding, making it more challenging for the model to estimate the underlying data distribution accurately. While the CelebA samples generated with positional conditioning yield an FID score of 11.93, as reported in Table  1  RISSOLE without positional encoding surpasses this with an FID of  9.82 . This finding substantiates our conjecture that retrieval-augmented generation and block-wise conditioning inherently preserve positional information, consequently enabling RISSOLE to be trained without explicit positional conditioning, thereby enhancing the models time efficiency.",
            "We also explore conditioning on the previously generated block  in addition to  the nearest neighbors from the external database and compare the models performance with and without this extra conditioning to assess its impact on image synthesis quality.  Figure  5  indicates that  also  employing the previous block in conditioning results in a degradation in image quality compared to when it is not used. Specifically, samples generated without utilizing the previous block exhibit finer details, sharper features, and higher fidelity, indicating superior visual quality. FID scores of RISSOLE with the previous block serving as a context (RISSOLE-P) are presented in Table  1 . Incorporating the previous block as a condition may impose limitations restricting the models capacity to comprehend the complete diversity and intricacy of the underlying data distribution. Conversely, training the model without any context from the preceding or other blocks enables the parallelization of the training process, resulting in enhanced speed.",
            "We examine the role of the retrieval augmented generation (RAG) mechanism in our diffusion model by comparing the generated samples with and without RAG in Figure  6 . When the RAG mechanism is omitted, we observe a degradation in image quality, resulting in incoherent and less visually appealing samples. Table  1  shows the FID scores of a RISSOLE model trained without RAG conditioning, which quantitatively confirms the same."
        ]
    },
    "global_footnotes": [
        "This research work was partially supported by the Research-I Foundation of IIT Kanpur.",
        "An example provided in https://www.databricks.com/blog/stable-diffusion-2",
        "Another design choice can be to use the entire",
        "to do the retrieval since we have access to the full image and its latent representation. We show some results with this choice in the Appendix.",
        "FID values for Patch Diffusion are from the original paper. No FID results for ImageNet100 were reported."
    ]
}