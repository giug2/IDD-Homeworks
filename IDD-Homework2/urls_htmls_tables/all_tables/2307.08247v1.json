{
    "S4.T1": {
        "caption": "TABLE I: Experimental results on the ViVQA dataset. Note that (*) indicates results from [35].",
        "table": "<table id=\"S4.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span id=\"S4.T1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Methods</span></th>\n<td id=\"S4.T1.1.1.1.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S4.T1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">EM</span></td>\n</tr>\n<tr id=\"S4.T1.1.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">LSTM + W2V (*)</th>\n<td id=\"S4.T1.1.2.2.2\" class=\"ltx_td ltx_align_right ltx_border_t\">0.3228</td>\n</tr>\n<tr id=\"S4.T1.1.3.3\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">LSTM + FastText (*)</th>\n<td id=\"S4.T1.1.3.3.2\" class=\"ltx_td ltx_align_right\">0.3299</td>\n</tr>\n<tr id=\"S4.T1.1.4.4\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">LSTM + ELMO (*)</th>\n<td id=\"S4.T1.1.4.4.2\" class=\"ltx_td ltx_align_right\">0.3154</td>\n</tr>\n<tr id=\"S4.T1.1.5.5\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">LTSM + PhoW2Vec (*)</th>\n<td id=\"S4.T1.1.5.5.2\" class=\"ltx_td ltx_align_right\">0.3385</td>\n</tr>\n<tr id=\"S4.T1.1.6.6\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Bi-LSTM + W2V (*)</th>\n<td id=\"S4.T1.1.6.6.2\" class=\"ltx_td ltx_align_right\">0.3125</td>\n</tr>\n<tr id=\"S4.T1.1.7.7\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.7.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Bi-LSTM + FastText (*)</th>\n<td id=\"S4.T1.1.7.7.2\" class=\"ltx_td ltx_align_right\">0.3348</td>\n</tr>\n<tr id=\"S4.T1.1.8.8\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.8.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Bi-LSTM + ELMO (*)</th>\n<td id=\"S4.T1.1.8.8.2\" class=\"ltx_td ltx_align_right\">0.3203</td>\n</tr>\n<tr id=\"S4.T1.1.9.9\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.9.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Bi-LTSM + PhoW2Vec (*)</th>\n<td id=\"S4.T1.1.9.9.2\" class=\"ltx_td ltx_align_right\">0.3397</td>\n</tr>\n<tr id=\"S4.T1.1.10.10\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.10.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Hierarchical Co-Attention + LSTM (*)</th>\n<td id=\"S4.T1.1.10.10.2\" class=\"ltx_td ltx_align_right\">0.3496</td>\n</tr>\n<tr id=\"S4.T1.1.11.11\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.11.11.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">SAAA</th>\n<td id=\"S4.T1.1.11.11.2\" class=\"ltx_td ltx_align_right ltx_border_t\">0.5415</td>\n</tr>\n<tr id=\"S4.T1.1.12.12\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.12.12.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MCAN</th>\n<td id=\"S4.T1.1.12.12.2\" class=\"ltx_td ltx_align_left\">0.5711</td>\n</tr>\n<tr id=\"S4.T1.1.13.13\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.13.13.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\"><span id=\"S4.T1.1.13.13.1.1\" class=\"ltx_text ltx_font_bold\">PAT (ours)</span></th>\n<td id=\"S4.T1.1.13.13.2\" class=\"ltx_td ltx_align_left ltx_border_b\"><span id=\"S4.T1.1.13.13.2.1\" class=\"ltx_text ltx_font_bold\">0.6055</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "As indicated in Table I, SAAA and MCAN achieved significantly better results compared to all implementations of Tran et al. [35]. Straightforward structures such as the combination of pre-trained word embeddings and LSTM [10] do not tackle effectively such complicated tasks as VQA, while deeper and ingeniously designed methods such as SAAA and MCAN took over the ViVQA dataset better."
        ]
    },
    "S4.T2": {
        "caption": "TABLE II: Ablation study for PAT method.",
        "table": "<table id=\"S4.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span id=\"S4.T2.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Methods</span></th>\n<th id=\"S4.T2.1.1.1.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T2.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">EM</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.1.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">PAT w/o Hier.</th>\n<td id=\"S4.T2.1.2.1.2\" class=\"ltx_td ltx_align_right ltx_border_t\">0.5868</td>\n</tr>\n<tr id=\"S4.T2.1.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">PAT w LSTM</th>\n<td id=\"S4.T2.1.3.2.2\" class=\"ltx_td ltx_align_right\">0.5981</td>\n</tr>\n<tr id=\"S4.T2.1.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\"><span id=\"S4.T2.1.4.3.1.1\" class=\"ltx_text ltx_font_bold\">PAT</span></th>\n<td id=\"S4.T2.1.4.3.2\" class=\"ltx_td ltx_align_right ltx_border_b\"><span id=\"S4.T2.1.4.3.2.1\" class=\"ltx_text ltx_font_bold\">0.6055</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "we conduct an ablation study to comprehensively discover how our two proposed modules, Hierarchical Linguistic Feature Extractor, and Parallel Attention module, contribute to the overall result of the PAT. Results are shown in Table II.",
            "According to Table II, the PAT which does not use LSTM or Hierarchical Linguistic Features Extractor to extract features of questions obtained lower accuracy. When equipped with LSTM or Hierarchical Linguistic Extractor, PAT achieved better results. Especially it achieved the best results when using the Hierarchical Linguistic Extractor. This result proves that the Hierarchical Linguistic Feature Extractor leverages the grammar dependency as well as the context of Vietnamese better than a simple LSTM network."
        ]
    },
    "S4.T3": {
        "caption": "TABLE III: Ablation study for PAT that use 1-size kernel CNN to extract unigram features",
        "table": "<table id=\"S4.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span id=\"S4.T3.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Methods</span></th>\n<th id=\"S4.T3.1.1.1.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T3.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">EM</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.1.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">PAT w/o 1-size kernel CNN</th>\n<td id=\"S4.T3.1.2.1.2\" class=\"ltx_td ltx_align_right ltx_border_t\">0.5848</td>\n</tr>\n<tr id=\"S4.T3.1.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\">PAT w 1-size kernel CNN</th>\n<td id=\"S4.T3.1.3.2.2\" class=\"ltx_td ltx_align_right ltx_border_b\">0.6055</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Moreover, as stated in Section III-A, the Hierarchical Linguistic Feature Extractor uses CNN to extract up to 4-gram features, including the unigram features. This is necessary as we assume the 1-size kernel CNN used to extract unigram is used to project the pre-trained word embedding features into the latent space of PAT where it finds easier to fuse information with features from images. In Table III, we proved our assumption where PAT which uses an additional 1-size kernel CNN has a better result than one using unigram features extracted from the pre-trained word embedding."
        ]
    }
}