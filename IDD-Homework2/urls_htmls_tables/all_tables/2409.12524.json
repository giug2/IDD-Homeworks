{
    "id_table_1": {
        "caption": "Table 1:  Average turns and words per conversation of LUFY-Dataset compared to existing text-to-text dialog datasets. The average length of a conversation in ours is 4.5x that of MSC  (Xu et al.,  2022 ) , distributed over 4.8x more turns.",
        "table": "S1.T1.1",
        "footnotes": [],
        "references": [
            "To empirically validate our proposed model and its effectiveness in enhancing long-term conversational memory, we conducted an experiment involving human participants. In this experiment, participants engaged in text-based conversations with chatbots equipped with different forgetting modules, each for at least two hours. This duration is at least 4.5 times longer than that of the most extensive existing studies of conversational abilities to date, as shown in Table  1 . Our results show that prioritizing arousing memories while discarding most of the conversation content significantly enhances user experience. This enhancement is evidenced by human assessments, sentiment analysis and GPT-4 evaluations. Furthermore, our method improved the precision of information retrieval by over 17% compared to the Naive RAG system with no forgetting mechanism, highlighting its potential for more accurate and relevant conversational responses.",
            "Fourth, approximately 70% of conversation time is dedicated to the exchange of information about contemporary events  (Dunbar et al.,  1997 ) . Therefore, we take the recency of the memory into account at the retrieval stage, as detailed in Section  4.1.1 .",
            "As illustrated in Figure  1 , the importance assignment process involves three key steps.  Step 1: Metric Calculation For each chatbot-user utterance pair (defined as a memory), the psychological metrics outlined above are calculated.  Step 2: Strength Determination Next, the strength  S S S italic_S  is computed by summing the weighted metrics, which reflects the composite significance of the memory.",
            "We compared three different RAG chatbots:  Naive RAG ,  MemoryBank , and  LUFY . Naive RAG stores all memories, while MemoryBank and LUFY are equipped with a forgetting mechanism that retains only 10% of memories. MemoryBank assesses the importance of a memory based solely on retrieval counts, whereas LUFY evaluates memory importance using six memory-related metrics, as depicted in Figure  1 . Additionally, although both Naive RAG and MemoryBank use Cosine similarity only to assess the relevance of a memory to current conversations, LUFY also considers importance in this assessment. The differences between the three systems are summarized in Table  3 .",
            "We conducted four rounds of dialogue, significantly exceeding the interaction length of existing benchmarks  (Xu et al.,  2022 ) , as shown in Table  1 .",
            "We aim for the  LUFY-Dataset  to serve as a benchmark for long-term conversations, given its unique length, as shown in Table  1 .",
            "DistilRoBERTa is fine-tuned with polar sentiment dataset of sentences from financial news, consisting of 4840 sentences from English language financial news categorised by sentiment. Table  10  shows the hyperparameters.",
            "As shown in Table  11 , MemoryBank and LUFY remembered a very tiny portion of the conversations, mostly less then 10% of the conversations."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Fitted parameters for the Memory-related Psychological Metrics. Arousal has the highest weight relative to other metrics.",
        "table": "S2.T2.5",
        "footnotes": [],
        "references": [
            "To account for aspects of importance that might not be fully captured by the aforementioned metrics in Section  2 , we also add the LLM-estimated importance score, which relies on the language models interpretation of the conversational context. The prompt given to estimate the importance is detailed in the Appendix  A .  These five metrics are regularized to ensure consistent scaling with the same average, minimum, and maximum values. See the Appendix  B  for further details.",
            "The results of the fitted parameters are given in Table  2 . Notably, Arousal (A) exhibits the highest weight out of all metrics, suggesting that arousal plays a significant role in determining the memorability and importance of an utterance."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Comparison of systems in terms of forgetting and retrieval methods.  Naive RAG : a conventional RAG chatbot model that exhibits no forgetting,  MemoryBank : the model from previous work  (Zhong et al.,  2024 ) , and  LUFY : our proposed model.",
        "table": "S3.T3.1",
        "footnotes": [],
        "references": [
            "As depicted in Figure  3 , the response generation process consists of two main parts: the retrieval part and the concatenation of various information to provide to the LLM. We provide a detailed explanation of the retrieval method, as it is central to the RAG chatbots.",
            "As depicted in Figure  3 , the forgetting process is executed in two steps: ranking the memories according to importance, and retaining the top 10% important memories. We provide the actual retention rate in the Appendix  D .",
            "We compared three different RAG chatbots:  Naive RAG ,  MemoryBank , and  LUFY . Naive RAG stores all memories, while MemoryBank and LUFY are equipped with a forgetting mechanism that retains only 10% of memories. MemoryBank assesses the importance of a memory based solely on retrieval counts, whereas LUFY evaluates memory importance using six memory-related metrics, as depicted in Figure  1 . Additionally, although both Naive RAG and MemoryBank use Cosine similarity only to assess the relevance of a memory to current conversations, LUFY also considers importance in this assessment. The differences between the three systems are summarized in Table  3 .",
            "Similar to the annotation procedure described in Section  3 , at least three annotators reviewed and labeled the conversations from the user experiment using a binary scale: 1 for \"important\" and 0 for \"unimportant.\" In addition to the annotation of important memories, each conversation script includes three QA pairs. For the de-identification process, multiple reviewers examined the conversations to ensure that all personally identifiable information (PII) was removed or modified. Further details are provided in Appendix  F .",
            "Out of the five metricsA, P, L, R1, and R2only P, which stands for perplexity, can have a substantially large value. Therefore, we set a threshold of 160 for any case where perplexity exceeds this value. We chose this threshold because more than 95% of the memories had a value below 160. However, future work should focus on determining an optimal threshold for detecting unusually large perplexity values. Additionally, using the human-RAG chatbot conversations (a total of 200 utterance pairs) in Section  3 , we regularized the five metrics to have the same average, minimum, and maximum values.\""
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Subjective Evaluation of three criteria: Personalization, Flow of Conversation and Overall, with ratings on a scale from 1 (lowest) to 5 (highest).",
        "table": "S5.T4.1",
        "footnotes": [],
        "references": [
            "Fourth, approximately 70% of conversation time is dedicated to the exchange of information about contemporary events  (Dunbar et al.,  1997 ) . Therefore, we take the recency of the memory into account at the retrieval stage, as detailed in Section  4.1.1 .",
            "Cosine Similarity Threshold:  During the retrieval stage, we use cosine similarity to assess the relevance between the embeddings of recent conversations and the entries in the memory database. To ensure a high level of contextual relevance, we have set the threshold at 0.8, consistent with commonly used standards in popular RAG system frameworks like LlamaIndex. This choice is further validated by our analysis of Question and Answer pairs gathered from the user experiment. More details are available in Section  5.4 .",
            "For subjective methods, each conversation script was evaluated by three annotators based on three criteria: Personalization, Flow of the Conversation, and Overall experience. The results are shown in Table  4 . LUFY outperformed the other two models in almost all sessions across all criteria, with a notable difference in Session 4. This highlights an improved user experience with LUFY, especially in Session 4, where the difference becomes more pronounced in longer conversations. The definitions and specific scoring criteria are provided in Appendix  G ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Sentiment Analysis Results: Average rating of user utterances (+1 = positive, 0 = neutral, -1 = negative).",
        "table": "S5.T5.1",
        "footnotes": [],
        "references": [
            "Cosine Similarity Threshold:  During the retrieval stage, we use cosine similarity to assess the relevance between the embeddings of recent conversations and the entries in the memory database. To ensure a high level of contextual relevance, we have set the threshold at 0.8, consistent with commonly used standards in popular RAG system frameworks like LlamaIndex. This choice is further validated by our analysis of Question and Answer pairs gathered from the user experiment. More details are available in Section  5.4 .",
            "As part of the automatic method to score the user experience, we conducted sentiment analysis using versions of RoBERTa fine-tuned with sentiment datasets. As shown in Table  5 , participants had more positive conversations when interacting with LUFY. Kindly refer to Appendix  C  for more details.",
            "We used the LUFY-Dataset, which consists of 612 QA pairs. For each pair, we retrieved ten relevant memories with a cosine similarity threshold of at least 0.6 and annotated whether the retrieved memory was correct for answering the question. Figure  5  shows the percentage of correct memories for each cosine similarity threshold range. Our findings include:",
            "As shown in Figure  5 , we found that"
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Rating by GPT-4o, average of 30 ratings.",
        "table": "S5.T6.1",
        "footnotes": [],
        "references": [
            "As part of the automatic method to score the user experience, we assessed the users overall satisfaction using GPT-4o. We used GPT-4o as an evaluator because strong LLMs achieve performance comparable to human evaluators  (Zheng et al.,  2024 ) . As shown in Table  6 , there is a notable difference in Session 4, which aligns with the findings from the subjective evaluations."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Ratio of the correct memory being in various top- k  memories. The correct memory is included in the top-5 memories 84.8% of the time.",
        "table": "S5.T7.1",
        "footnotes": [],
        "references": [
            "Using the same 612 QA pairs from the LUFY-Dataset, we retrieved the top 10 memories for each question and analyzed the position of the correct memory among them. The results are shown in Table  7 . Notably, we found that 85% of the correct memories appear within the top 5, with only marginal improvement when increasing  k . This suggests that selecting the top 5 memories is effective for conversational RAG chatbots, aligning with previous studies on RAG systems in non-conversational settings  (Li et al.,  2024 ) . Aligning our method with the concept of RIF  (Hirst and Echterhoff,  2012 ) , we retrieved the top 2 memories despite the higher effectiveness observed with the top 5."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Comparison of Precision, Recall, and F1 Score for different systems and sessions. Recall in S3 represents its average accuracy on questions across sessions 1 to 3.",
        "table": "S5.T8.1",
        "footnotes": [],
        "references": [
            "We assessed the models ability to remember using the QA pairs collected in the User Experiment. After each session, we asked each model the questions and calculated three scores: Precision, Recall, and F1 Score. The results are shown in Table  8 ."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Agreement probability with other annotators on whether a memory is important.",
        "table": "S5.T9.1",
        "footnotes": [],
        "references": [
            "To numerically analyze how closely the remaining memories align with human annotators, we calculated the agreement probability with annotators on whether a memory is important. As shown in Table  9 , LUFY outperforms MemoryBank in all sessions except for S3. However, it is not yet on par with human performance."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:  Hyperparameters Used in the Experiment",
        "table": "A3.T10.1",
        "footnotes": [],
        "references": [
            "DistilRoBERTa is fine-tuned with polar sentiment dataset of sentences from financial news, consisting of 4840 sentences from English language financial news categorised by sentiment. Table  10  shows the hyperparameters."
        ]
    },
    "id_table_11": {
        "caption": "Table 11:  Comparison of Retention Rate for different systems across sessions.",
        "table": "A4.T11.1",
        "footnotes": [],
        "references": [
            "As shown in Table  11 , MemoryBank and LUFY remembered a very tiny portion of the conversations, mostly less then 10% of the conversations."
        ]
    },
    "id_table_12": {
        "caption": "Table 12:  Recall of responses to questions from various sessions. Q1 denotes Questions about session1, Q2 denotes Questions about session2 and so on. S2 denotes that the questions are answered right before session2 and so on.",
        "table": "A8.T12.1",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "Table 13:  Precision Table of responses to questions from various sessions. Q1 denotes Questions about session1, Q2 denotes Questions about session2 and so on. S2 denotes that the questions are answered right before session2 and so on.",
        "table": "A8.T13.1",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "Table 14:  Ablation Study for Precision, Recall, and F1 Score.   A A -A - italic_A  indicates that  w A subscript w A w_{A} italic_w start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT  is set to 0. The largest drop in performance is highlighted in bold.",
        "table": "A8.T14.5",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "Table 15:  Ablation Study for the Agreement probability with other annotators on whether memories are important.",
        "table": "A8.T15.5",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": []
}