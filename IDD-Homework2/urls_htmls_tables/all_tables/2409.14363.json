{
    "id_table_1": {
        "caption": "Table 1:  Automated Evaluation MANTA Win Percentage",
        "table": "S4.T1.1",
        "footnotes": [],
        "references": [
            "Some core contributions from their paper include (1) introducing an early framework for  concept mapping , and then (2) associating each adapter with the concept-mapped keywords that can reframe the problem into a retrieval augmented generation situation  1 .",
            "However, this often leads to extremely vague images if the model isnt properly fine tuned for that specific idea. To address this, we enable the user to specify extremely vague prompts, and use LLMs to generate reasonable additional details based on the concept. The sample prompt is referenced here  A.3.1  . Some examples of detail enhancement output for the prompt previously mentioned is shown below, for the techno samurai warrior concept.",
            "The vision language model was then asked to rate each of the images for that category, and based on the category, come up with a preference for either images contained within the MANTA group, or the basis of comparison. In the table  1 , the total percentage of the times Stylus won has been recorded.",
            "Shown is a direct comparison of token counts from run using the previous Stylus iteration, versus the current run  11 .",
            "Ultimately, the choice of how much variance should one relagate to the model is left to the user. We do want to close this discussion by pointing out that prompt-based variance appears to be more than model induced variance, as compared between CFG 7 and CFG 4 images  13 :",
            "Within this area of prompting, including closely associated concepts, even vague prompts resulted in high quality images. For example, a simple prompt such as \"a man teaching a boy how to surf\" came up with the variety shown  14 .",
            "Synthetic Data Generation.  We do see a strong use case of MANTA as a synthetic \"data factory\", which leverages checkpoints and LoRAs to create highly diverse images. We present some realistic example that may pass off with some minimal tolerance as a potential training data sample  15 .",
            "In this situation, MANTA would over-focus on one concept, ignoring or omitting other concepts that would have been relevant. Oftentimes, we found this would happen when it would be challenging to pick a leading  main  concept. This led to a concept perpetually being subdued to the background or a corner of the image. In the case of latest iteration of MANTA, it would often ignore the secondary concept entirely, resulting in higher fidelity images at a heavy alignment penalty for ignoring a concept  16 . The other algorithms attempted to superimpose the two concepts, leading to low quality images where the second object wasnt often created with the high quality of the first.",
            "Similar to the previous concept, this issue occurs when two concepts intersect. If the model doesnt seem to have experience understanding how to relate the two objects, it results in naive merger or low diversity output. An example of a failing prompt for this case is  A bear carries a pink ball by the river side   17 . While the output can be considered to be novel and artistically diverse, the diversity isnt sensibly generated - the interactions of the concepts arent in the realm of possibility one would expect."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  No-Concept Evaluation MANTA Win Percentage vs. Stylus",
        "table": "S4.T2.1",
        "footnotes": [],
        "references": [
            "Stylus fosters image diversity through pure randomness, randomly drawing permutations of LoRAs that seem be relevant within reason. While this method did foster diversity, there are visible limitations to the extent of which this diversity reaches, which stems from the lack of vetting adapters with previously tested examples of output  2 .",
            "The table  2  demonstrates the significant but non-encompassing impact of prompt enhancement via the loose concept framework. While there was a notable decrease in the diversity (  17 % percent 17 -17\\% - 17 % ) and minor increase quality ( + 4 % percent 4 +4\\% + 4 %  ) win rate against Stylus Rank, we still see an remarkable win rate against Stylus, suggesting concept enhancement provided valuable improvement in diversity, but the system could still provide reliable image diversity in not used. We postulate that the gap comes primarily from checkpoint inductive bias - which we further explore in the next ablation study. By repeatedly using the same one model, we believe that the original Stylus system suffers from a significantly higher model-based bias, in comparison to a system that has the flexibility to alternatively consider from a sampled set of closely relevant checkpoints."
        ]
    }
}