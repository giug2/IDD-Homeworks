{
    "S3.T2": {
        "caption": "Table 2: Inception score calculated for each synthetic dataset used in the calibration process (higher is better).",
        "table": "<table id=\"S3.T2.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T2.2.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Model</th>\n<th id=\"S3.T2.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Inception Score</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T2.2.2.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.2.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">StyleGAN2-ADA</th>\n<td id=\"S3.T2.2.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">10.34</td>\n</tr>\n<tr id=\"S3.T2.2.3.2\" class=\"ltx_tr\">\n<th id=\"S3.T2.2.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">DiStyleGAN</th>\n<td id=\"S3.T2.2.3.2.2\" class=\"ltx_td ltx_align_center\">6.78</td>\n</tr>\n<tr id=\"S3.T2.2.4.3\" class=\"ltx_tr\">\n<th id=\"S3.T2.2.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Fractal</th>\n<td id=\"S3.T2.2.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">3.31</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Samples from each of the four data distributions for the classes \u201dAirplane\u201c and \u201dHorse\u201c are illustrated in Figure\u00a02, while samples for the rest of the classes can be found in Appendix\u00a0A. Additionally, Table\u00a02 presents the Inception Score (IS) [32] calculated for each data distribution. The inception score is a metric that evaluates the quality of synthetic generated images. The calculated score is based on the ability of a pre-trained InceptionV3 [37] model to classify the images of a synthetic dataset produced by a generative model. In our case, we calculated the inception scores using datasets of 50,0005000050,000 synthetic samples from each distribution.",
            "LABEL:tab:def-quant-acc-drop and\u00a0LABEL:tab:acc-quant-acc-drop showcase the accuracy degradation percentage of the quantized models with respect to the performance of the original PyTorch models on the classification task on the CIFAR-10 test set. These results were obtained using the two different quantization algorithms, default and accuracy-aware, and the four calibration datasets described in Section\u00a03.3. In addition, Table\u00a02 presents the inception scores of the synthetic calibration datasets, a quantitative metric that indicates the difference in the quality of the generated images between the four aforementioned datasets.",
            "Finally, it is essential to notice that the two synthetic datasets, StyleGAN2-ADA and DiStyleGAN, lead to comparable results with the official CIFAR-10 dataset in the case of quantization. Surprisingly enough, although the DiStyleGAN model was trained through knowledge distillation, with the StyleGAN2-ADA as the teacher network, there are cases where the quantization process using the dataset generated by the former leads to better results compared to when the latter is used. This finding is also in contrast with the quantitative results for the synthetic generated images (Tab.\u00a02) which clearly showcase the superiority of the StyleGAN2-ADA model compared to our DiStyleGAN, in terms of the quality of their synthetic generated images. However, the inception score metric does not take into consideration how synthetic images compare to real images. Other metrics (e.g. Fr\u00e9chet inception distance (FID) [19]) that evaluate the synthetic images while taking into account the distribution of the real data, should also be included in further experimentation."
        ]
    }
}