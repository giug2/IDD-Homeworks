{
    "id_table_1": {
        "caption": "Table 1:  Parameter estimates and their standard errors. The standard errors are shown in parentheses and are obtained by bootstrapping. We show the estimates from  Besiroglu et al. ( 2024 )  (re-estimated from  Hoffmann et al. ( 2022 ) ) for comparison and added the constants  c 1 subscript c 1 c_{1} italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and  c 2 subscript c 2 c_{2} italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  for text diversity and syntheticity respectively.",
        "table": "A8.EGx1",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "In this work, we model the zero-shot accuracy on common sense reasoning as we postulate that the score provides an indication on how much reasoning ability a given data  D D D italic_D  could possibly instill. To incorporate data quality into this framework, we propose to use a quality term  Q Q Q italic_Q  to provide a quality-adjusted number of training tokens ( D q subscript D q D_{q} italic_D start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ), combining Eq.  1  and Eq.  2 :",
            "By over 200 training runs, we re-estimate all the constants which we show in Table  1 . Here we first discuss the estimation of constants that relate to accuracy and the rest of the scaling parameters in Eq.  4 . In particular, we discuss the scaling factor  Q Q Q italic_Q  and how it can be applied to pretraining scenarios.",
            "In Table  1 , we show the estimated constants for the scaling law Eq. 4  and the proposed scaling factor term Eq. 3 . The constants were estimated with the nonlinear least-squares method with the Scipy optimizer 5 5 5 https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html , where the initial guesses were the original Chinchilla scaling law constants in  Hoffmann et al. ( 2022 ) , and the maximum number of function calls was set as  2000 2000 2000 2000 . To validate our estimated constants, we provide a predicted vs. true accuracy plot and the Pearson correlation in Figure  5 . This gives us ideas on how strongly these constants are correlated to the training set used to estimate our revised scaling formulation. Strikingly, this amounts to the correlation strength of + 0.83 0.83 0.83 0.83  across all model sizes and data samples. We attribute the robustness of the formulation to the use of data-agnostic compression ratio and a reasonably-capable language model as teacher."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Equations and their corresponding  R 2 superscript R 2 R^{2} italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  values",
        "table": "A8.EGx2",
        "footnotes": [],
        "references": [
            "In this work, we model the zero-shot accuracy on common sense reasoning as we postulate that the score provides an indication on how much reasoning ability a given data  D D D italic_D  could possibly instill. To incorporate data quality into this framework, we propose to use a quality term  Q Q Q italic_Q  to provide a quality-adjusted number of training tokens ( D q subscript D q D_{q} italic_D start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ), combining Eq.  1  and Eq.  2 :",
            "To put them in context, we present a comparative analysis in Figure  2 , which displays the relationship between effective token counts  D q subscript D q D_{q} italic_D start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT  and the total number of tokens  D D D italic_D . It clearly demonstrates that data synthesis has a more substantial impact on increasing the effective token count compared to data selection and the use of original datasets. This underscores the value of synthesis in optimizing data quality for model training."
        ]
    },
    "id_table_3": {
        "caption": "",
        "table": "S6.T1.7.7",
        "footnotes": [
            ""
        ],
        "references": [
            "To ensure a diverse range of training data, we constructed several datasets from multiple sources, including  random  data (8B tokens),  selected  data (7B tokens), and  synthetic  data (2B tokens). The  selected  data was curated based on the evaluation set of the eight tasks using importance sampling  Xie et al. ( 2023 ) , while the  synthetic  data was generated through instructional prompts aimed at paraphrasing each pretraining document. In contrast, the  random  data was noted for its high diversity but low syntheticity, as discussed in  Section   3 . Conversely, the  synthetic  data exhibited the lowest diversity but the highest syntheticity score.",
            "In Table  1 , we show the estimated constants for the scaling law Eq. 4  and the proposed scaling factor term Eq. 3 . The constants were estimated with the nonlinear least-squares method with the Scipy optimizer 5 5 5 https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html , where the initial guesses were the original Chinchilla scaling law constants in  Hoffmann et al. ( 2022 ) , and the maximum number of function calls was set as  2000 2000 2000 2000 . To validate our estimated constants, we provide a predicted vs. true accuracy plot and the Pearson correlation in Figure  5 . This gives us ideas on how strongly these constants are correlated to the training set used to estimate our revised scaling formulation. Strikingly, this amounts to the correlation strength of + 0.83 0.83 0.83 0.83  across all model sizes and data samples. We attribute the robustness of the formulation to the use of data-agnostic compression ratio and a reasonably-capable language model as teacher.",
            "In the left plot of Figure  3 , we first explore the impact of effective tokens on model accuracy. It is evident that an increase in effective tokens correlates with higher accuracy. However, the influence of the scaling factor  Q Q Q italic_Q  varies across different models. Notably, the impact of data quality is more pronounced in smaller model sizes ranging from 25M to 500M, and it gradually levels off as the value of scaling factor  Q Q Q italic_Q  increases, eventually reaching a point where effective tokens  D q subscript D q D_{q} italic_D start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT  are predominantly determined by the sheer number of tokens. Additionally, we examine the interplay between the scaling factor  Q Q Q italic_Q , diversity, and syntheticity in the right plot of Figure  3 . Several key observations emerge:"
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "A4.T2.5",
        "footnotes": [],
        "references": [
            "Diversity:  Following  Shaib et al. ( 2024 ) , we utilize the compression ratio, which has been demonstrated to be effective for large-scale pretraining datasets and correlates well with other diversity metrics (Figure  4 ). Past metrics generally quantify the number of repeated substrings across outputs. Among these, the token-type ratio is calculated by dividing the count of unique tokens by the total number of tokens in a text. To capture the lexical dynamics across varying text lengths, the moving average token type ratios (MATTRs) were introduced, providing a robust measure that is insensitive to text length  Covington and McFall ( 2010 ) . This metric focuses on the frequency of individual word repetition within text segments and does not account for longer repeated sequences.",
            "By over 200 training runs, we re-estimate all the constants which we show in Table  1 . Here we first discuss the estimation of constants that relate to accuracy and the rest of the scaling parameters in Eq.  4 . In particular, we discuss the scaling factor  Q Q Q italic_Q  and how it can be applied to pretraining scenarios.",
            "In Table  1 , we show the estimated constants for the scaling law Eq. 4  and the proposed scaling factor term Eq. 3 . The constants were estimated with the nonlinear least-squares method with the Scipy optimizer 5 5 5 https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html , where the initial guesses were the original Chinchilla scaling law constants in  Hoffmann et al. ( 2022 ) , and the maximum number of function calls was set as  2000 2000 2000 2000 . To validate our estimated constants, we provide a predicted vs. true accuracy plot and the Pearson correlation in Figure  5 . This gives us ideas on how strongly these constants are correlated to the training set used to estimate our revised scaling formulation. Strikingly, this amounts to the correlation strength of + 0.83 0.83 0.83 0.83  across all model sizes and data samples. We attribute the robustness of the formulation to the use of data-agnostic compression ratio and a reasonably-capable language model as teacher.",
            "These insights establish some basic guidelines: To enhance data quality in smaller models, introducing synthetic data can be beneficial, as it typically yields less diverse but more synthetic data with a higher scaling factor  Q Q Q italic_Q . However, it is crucial for training practitioners to recognize that while increasing text syntheticity can scale up data quality, the total token count ultimately plays a more dominant role in improving model accuracy in larger models that are more data-hungry (e.g. greater than 1.5B in our experiments), as illustrated in Figure  4 ."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "A6.p1.1.1",
        "footnotes": [],
        "references": [
            "In Table  1 , we show the estimated constants for the scaling law Eq. 4  and the proposed scaling factor term Eq. 3 . The constants were estimated with the nonlinear least-squares method with the Scipy optimizer 5 5 5 https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html , where the initial guesses were the original Chinchilla scaling law constants in  Hoffmann et al. ( 2022 ) , and the maximum number of function calls was set as  2000 2000 2000 2000 . To validate our estimated constants, we provide a predicted vs. true accuracy plot and the Pearson correlation in Figure  5 . This gives us ideas on how strongly these constants are correlated to the training set used to estimate our revised scaling formulation. Strikingly, this amounts to the correlation strength of + 0.83 0.83 0.83 0.83  across all model sizes and data samples. We attribute the robustness of the formulation to the use of data-agnostic compression ratio and a reasonably-capable language model as teacher."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "A7.p1.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_7": {
        "caption": "",
        "table": "A7.p2.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_8": {
        "caption": "",
        "table": "A7.p3.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_9": {
        "caption": "",
        "table": "A7.p4.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_10": {
        "caption": "",
        "table": "A7.p5.1.1",
        "footnotes": [],
        "references": []
    }
}