{
    "PAPER'S NUMBER OF TABLES": 8,
    "S4.T1": {
        "caption": "TABLE I: Datasets and models.",
        "table": "<table id=\"S4.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Dataset</th>\n<th id=\"S4.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Task</th>\n<th id=\"S4.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Clients</th>\n<th id=\"S4.T1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Total Samples</th>\n<th id=\"S4.T1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Model</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">CIFAR-10</td>\n<td id=\"S4.T1.1.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Image Classification</td>\n<td id=\"S4.T1.1.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">50/100</td>\n<td id=\"S4.T1.1.2.1.4\" class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">60,000</td>\n<td id=\"S4.T1.1.2.1.5\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">ConvNet, ViT</td>\n</tr>\n<tr id=\"S4.T1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.3.2.1\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">CIFAR-100</td>\n<td id=\"S4.T1.1.3.2.2\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Image Classification</td>\n<td id=\"S4.T1.1.3.2.3\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">50/100</td>\n<td id=\"S4.T1.1.3.2.4\" class=\"ltx_td ltx_align_right\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">60,000</td>\n<td id=\"S4.T1.1.3.2.5\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">ConvNet, ViT</td>\n</tr>\n<tr id=\"S4.T1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Shakespeare</td>\n<td id=\"S4.T1.1.4.3.2\" class=\"ltx_td ltx_align_left ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Next-character Prediction</td>\n<td id=\"S4.T1.1.4.3.3\" class=\"ltx_td ltx_align_left ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">683</td>\n<td id=\"S4.T1.1.4.3.4\" class=\"ltx_td ltx_align_right ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">2,578,349</td>\n<td id=\"S4.T1.1.4.3.5\" class=\"ltx_td ltx_align_left ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">LSTM, Transformer</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "2) Non-IID Settings of Datasets:\nWe use three popular benchmark datasets: CIFAR-10, CIFAR-100 [13], and Shakespeare[46, 1]. The first two are image datasets and the last one is a language dataset.\nFor CIFAR-10 and CIFAR-100, we applied two split strategies to simulate non-IID scenarios. The first is a “Pathological” setting, where each client is randomly assigned two/ten classes for CIFAR-10/CIFAR-100 as in [24]. The sample rate on client i𝑖i of selected class c𝑐c is obtained by ai,c/∑jaj,csubscript𝑎𝑖𝑐subscript𝑗subscript𝑎𝑗𝑐{a_{i,c}}/{\\sum_{j}a_{j,c}}, where ai,c∼U​(.4,.6)similar-tosubscript𝑎𝑖𝑐𝑈.4.6a_{i,c}\\sim U(.4,.6).\nThe second is a federated version created by randomly partitioning the datasets among clients using a symmetric Dirichlet distribution with parameter α=0.3𝛼0.3\\alpha=0.3, as in [47, 29]. We create federated versions of CIFAR-10 by randomly partitioning samples with the same label among clients according to a Dirichlet distribution with parameter α=0.3𝛼0.3\\alpha=0.3. As for CIFAR-100, in order to create more realistic local datasets, we use a two-stage Pachinko allocation method to partition samples over “coarse” and “fine” labels. This method firstly generates a Dirichlet distribution with parameter α=0.3𝛼0.3\\alpha=0.3 over the coarse labels for each client, and then generates a Dirichlet distribution with parameter β=10𝛽10\\beta=10 over the coarse corresponding fine labels. For both partitions, the classes and distribution of classes in training and test set of each client are the same.\nFor Shakespeare, similar to the partitions in [29], we maintained the original split between the training and testing data specifically, 80%percent\\% for training and 20%percent\\% for testing.\nTable I summarizes the datasets, corresponding tasks, and the number of clients and models."
        ]
    },
    "S4.T2": {
        "caption": "TABLE II: Average test accuracy for FedTP and several Transformer-based methods with different non-iid settings.\n",
        "table": "<table id=\"S4.T2.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"></th>\n<td id=\"S4.T2.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\" colspan=\"4\">CIFAR-10</td>\n<td id=\"S4.T2.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\" colspan=\"4\">CIFAR-100</td>\n</tr>\n<tr id=\"S4.T2.1.1.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">#setting</th>\n<td id=\"S4.T2.1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\" colspan=\"2\">Pathological</td>\n<td id=\"S4.T2.1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\" colspan=\"2\">Dirichlet</td>\n<td id=\"S4.T2.1.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\" colspan=\"2\">Pathological</td>\n<td id=\"S4.T2.1.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\" colspan=\"2\">Dirichlet</td>\n</tr>\n<tr id=\"S4.T2.1.1.3.3\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">#client</th>\n<td id=\"S4.T2.1.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">50</td>\n<td id=\"S4.T2.1.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">100</td>\n<td id=\"S4.T2.1.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">50</td>\n<td id=\"S4.T2.1.1.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">100</td>\n<td id=\"S4.T2.1.1.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">50</td>\n<td id=\"S4.T2.1.1.3.3.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">100</td>\n<td id=\"S4.T2.1.1.3.3.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">50</td>\n<td id=\"S4.T2.1.1.3.3.9\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">100</td>\n</tr>\n<tr id=\"S4.T2.1.1.4.4\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Local-T</th>\n<td id=\"S4.T2.1.1.4.4.2\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">84.55±0.15</td>\n<td id=\"S4.T2.1.1.4.4.3\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">82.21±0.08</td>\n<td id=\"S4.T2.1.1.4.4.4\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">69.94±0.13</td>\n<td id=\"S4.T2.1.1.4.4.5\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">66.68±0.13</td>\n<td id=\"S4.T2.1.1.4.4.6\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">55.91±0.17</td>\n<td id=\"S4.T2.1.1.4.4.7\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">49.25±0.11</td>\n<td id=\"S4.T2.1.1.4.4.8\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">27.87±0.12</td>\n<td id=\"S4.T2.1.1.4.4.9\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">23.34±0.10</td>\n</tr>\n<tr id=\"S4.T2.1.1.5.5\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">FedAvg-T <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">1</a>]</cite>\n</th>\n<td id=\"S4.T2.1.1.5.5.2\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">50.42±4.22</td>\n<td id=\"S4.T2.1.1.5.5.3\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">46.28±4.23</td>\n<td id=\"S4.T2.1.1.5.5.4\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">61.85±1.5</td>\n<td id=\"S4.T2.1.1.5.5.5\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">59.23±1.93</td>\n<td id=\"S4.T2.1.1.5.5.6\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">34.02±0.88</td>\n<td id=\"S4.T2.1.1.5.5.7\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">30.20±0.95</td>\n<td id=\"S4.T2.1.1.5.5.8\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">38.64±0.22</td>\n<td id=\"S4.T2.1.1.5.5.9\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">34.89±0.45</td>\n</tr>\n<tr id=\"S4.T2.1.1.6.6\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">FedProx-T <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">23</a>]</cite>\n</th>\n<td id=\"S4.T2.1.1.6.6.2\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">47.28±5.09</td>\n<td id=\"S4.T2.1.1.6.6.3\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">44.22±3.81</td>\n<td id=\"S4.T2.1.1.6.6.4\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">61.00±1.74</td>\n<td id=\"S4.T2.1.1.6.6.5\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">60.13±1.71</td>\n<td id=\"S4.T2.1.1.6.6.6\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">29.77±1.67</td>\n<td id=\"S4.T2.1.1.6.6.7\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">28.92±0.83</td>\n<td id=\"S4.T2.1.1.6.6.8\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">37.63±0.35</td>\n<td id=\"S4.T2.1.1.6.6.9\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">32.98±0.43</td>\n</tr>\n<tr id=\"S4.T2.1.1.7.7\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.7.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">FedPer-T <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib34\" title=\"\" class=\"ltx_ref\">34</a>]</cite>\n</th>\n<td id=\"S4.T2.1.1.7.7.2\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">89.86±0.89</td>\n<td id=\"S4.T2.1.1.7.7.3\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T2.1.1.7.7.3.1\" class=\"ltx_text ltx_font_bold\">89.01±0.12</span></td>\n<td id=\"S4.T2.1.1.7.7.4\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">79.41±0.16</td>\n<td id=\"S4.T2.1.1.7.7.5\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">77.70±0.14</td>\n<td id=\"S4.T2.1.1.7.7.6\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">67.23±0.32</td>\n<td id=\"S4.T2.1.1.7.7.7\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">61.72±0.16</td>\n<td id=\"S4.T2.1.1.7.7.8\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">37.19±0.18</td>\n<td id=\"S4.T2.1.1.7.7.9\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">29.58±0.14</td>\n</tr>\n<tr id=\"S4.T2.1.1.8.8\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.8.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">pFedMe-T <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib24\" title=\"\" class=\"ltx_ref\">24</a>]</cite>\n</th>\n<td id=\"S4.T2.1.1.8.8.2\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">82.26±0.61</td>\n<td id=\"S4.T2.1.1.8.8.3\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">77.57±0.52</td>\n<td id=\"S4.T2.1.1.8.8.4\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">71.45±0.87</td>\n<td id=\"S4.T2.1.1.8.8.5\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">68.13±0.67</td>\n<td id=\"S4.T2.1.1.8.8.6\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">53.08±0.72</td>\n<td id=\"S4.T2.1.1.8.8.7\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">39.94±0.91</td>\n<td id=\"S4.T2.1.1.8.8.8\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">33.59±0.52</td>\n<td id=\"S4.T2.1.1.8.8.9\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">25.95±0.64</td>\n</tr>\n<tr id=\"S4.T2.1.1.9.9\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.9.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Vanilla Personalized-T</th>\n<td id=\"S4.T2.1.1.9.9.2\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">86.97±0.07</td>\n<td id=\"S4.T2.1.1.9.9.3\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">84.90±0.11</td>\n<td id=\"S4.T2.1.1.9.9.4\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">75.14±0.12</td>\n<td id=\"S4.T2.1.1.9.9.5\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">72.33±0.16</td>\n<td id=\"S4.T2.1.1.9.9.6\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">61.98±0.13</td>\n<td id=\"S4.T2.1.1.9.9.7\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">57.60±0.12</td>\n<td id=\"S4.T2.1.1.9.9.8\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">37.21±0.11</td>\n<td id=\"S4.T2.1.1.9.9.9\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">32.81±0.11</td>\n</tr>\n<tr id=\"S4.T2.1.1.10.10\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.10.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">FedTP</th>\n<td id=\"S4.T2.1.1.10.10.2\" class=\"ltx_td ltx_align_left ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T2.1.1.10.10.2.1\" class=\"ltx_text ltx_font_bold\">90.31±0.26</span></td>\n<td id=\"S4.T2.1.1.10.10.3\" class=\"ltx_td ltx_align_left ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">88.39±0.14</td>\n<td id=\"S4.T2.1.1.10.10.4\" class=\"ltx_td ltx_align_left ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T2.1.1.10.10.4.1\" class=\"ltx_text ltx_font_bold\">81.24±0.17</span></td>\n<td id=\"S4.T2.1.1.10.10.5\" class=\"ltx_td ltx_align_left ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T2.1.1.10.10.5.1\" class=\"ltx_text ltx_font_bold\">80.27±0.28</span></td>\n<td id=\"S4.T2.1.1.10.10.6\" class=\"ltx_td ltx_align_left ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T2.1.1.10.10.6.1\" class=\"ltx_text ltx_font_bold\">68.05±0.24</span></td>\n<td id=\"S4.T2.1.1.10.10.7\" class=\"ltx_td ltx_align_left ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T2.1.1.10.10.7.1\" class=\"ltx_text ltx_font_bold\">63.76±0.39</span></td>\n<td id=\"S4.T2.1.1.10.10.8\" class=\"ltx_td ltx_align_left ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T2.1.1.10.10.8.1\" class=\"ltx_text ltx_font_bold\">46.35±0.29</span></td>\n<td id=\"S4.T2.1.1.10.10.9\" class=\"ltx_td ltx_align_left ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T2.1.1.10.10.9.1\" class=\"ltx_text ltx_font_bold\">43.74±0.39</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "2) Performance Analysis:\nWe compared the performance of FedTP to some well-known federated learning methods. Note that these methods were originally implemented based on CNN backbones. Here we used the same Transformer architecture of FedTP to replace their CNN backbones for a fair comparison. To mark the modification, we added a “-T” after the algorithm name to denote these methods. The average test accuracy of these algorithms is reported in Table II, which shows that FedTP outperforms all of them by a clear margin. This also validates our prior claims in the Introduction: 1) the FedAvg algorithm may impair the client-specific representations in Transformers as Local-T works even better than FedAvg-T; 2) the personalized self-attention learned by FedTP can handle the data heterogeneity effectively.\nAs shown in Table II, FedTP significantly outperforms Vanilla Personalized-T in all settings, which verified that learn-to-personalize leverages the strengths of self-attention in Transformer models to improve their performance in federated learning over heterogeneous data.\nAdditionally, FedTP is also superior to FedPer-T. This indicates that personalizing the projection matrices in self-attention layers is much more effective than personalizing the last classification head.",
            "3) Analysis of Network Backbone:\nWe also implemented experiments to compare the performance of FedTP to many state-of-the-art federated learning methods that are based on CNN/LSTM.\nThe average test accuracy of these algorithms for the image datasets and language dataset are shown in Table III and Table IV, respectively. Clearly, FedTP outperforms the baseline methods in terms of the average test accuracy for most cases. It is worth noting that, the hypernetworks in FedTP only produce the parameters of the self-attention layers, which take up around 9.8% parameters of the whole network. Although FedTP uses a much smaller ratio of personalized parameters, it still significantly outperforms pFedHN with an 11.31% improvement on average for CIFAR-100. As shown in Table III and Table IV, changing the network backbone from a CNN or an LSTM to Transformers will bring a significant improvement in the models’ ability to overcome data heterogeneity.\nThrough comparing Table II and Table III, the decrease in the accuracy of pFedMe-T and FedProx-T in some settings indicates that the performance of Transformer-based models will be affected when there is a regularization term in these federated learning methods. This is possible because the regularization term may impair the self-attention mechanism, while our FedTP maintains the personalization of the self-attention layers through the learn-to-personalize mechanism. More details about the effects of learn-to-personalize will be shown in 5) Visualization of Attention Maps.",
            "8) Analysis of Hypernetworks:\nTo analyze the effects of hypernetworks, we compared FedTP with Vanilla Personalized-T, which restores each client’s projection parameters Wisubscript𝑊𝑖W_{i} locally without using hypernetworks. Table II shows that FedTP has a prominent lead over Vanilla Personalized-T, indicating that hypernetworks indeed play an important role in FedTP. We further notice that even if hypernetworks only produce the parameters of the self-attention layer, it is still good at encoding client-specific information into client embeddings zisubscript𝑧𝑖z_{i}. The hypernetworks can simultaneously map the client embeddings zisubscript𝑧𝑖z_{i} into a manifold parameterized by hypernetwork parameters φ𝜑\\varphi."
        ]
    },
    "S4.T3": {
        "caption": "TABLE III: The results of FedTP and the benchmark methods on the image datasets with different non-IID settings.",
        "table": "<table id=\"S4.T3.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"></th>\n<th id=\"S4.T3.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\" colspan=\"4\">CIFAR-10</th>\n<th id=\"S4.T3.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\" colspan=\"4\">CIFAR-100</th>\n</tr>\n<tr id=\"S4.T3.1.1.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">#setting</th>\n<th id=\"S4.T3.1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\" colspan=\"2\">Pathological</th>\n<th id=\"S4.T3.1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\" colspan=\"2\">Dirichlet</th>\n<th id=\"S4.T3.1.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\" colspan=\"2\">Pathological</th>\n<th id=\"S4.T3.1.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\" colspan=\"2\">Dirichlet</th>\n</tr>\n<tr id=\"S4.T3.1.1.3.3\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">#client</th>\n<th id=\"S4.T3.1.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">50</th>\n<th id=\"S4.T3.1.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">100</th>\n<th id=\"S4.T3.1.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">50</th>\n<th id=\"S4.T3.1.1.3.3.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">100</th>\n<th id=\"S4.T3.1.1.3.3.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">50</th>\n<th id=\"S4.T3.1.1.3.3.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">100</th>\n<th id=\"S4.T3.1.1.3.3.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">50</th>\n<th id=\"S4.T3.1.1.3.3.9\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">100</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.1.1.4.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.1.4.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">FedAvg <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">1</a>]</cite>\n</th>\n<td id=\"S4.T3.1.1.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">47.79±4.48*</td>\n<td id=\"S4.T3.1.1.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">44.12±3.10*</td>\n<td id=\"S4.T3.1.1.4.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">56.59±0.91</td>\n<td id=\"S4.T3.1.1.4.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">57.52±1.01</td>\n<td id=\"S4.T3.1.1.4.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">15.71±0.35*</td>\n<td id=\"S4.T3.1.1.4.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">14.59±0.40*</td>\n<td id=\"S4.T3.1.1.4.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">18.16±0.58</td>\n<td id=\"S4.T3.1.1.4.1.9\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">20.34±1.34</td>\n</tr>\n<tr id=\"S4.T3.1.1.5.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.1.5.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">FedProx <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">23</a>]</cite>\n</th>\n<td id=\"S4.T3.1.1.5.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">50.81±2.94*</td>\n<td id=\"S4.T3.1.1.5.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">57.38±1.08*</td>\n<td id=\"S4.T3.1.1.5.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">58.51±0.65</td>\n<td id=\"S4.T3.1.1.5.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">56.46±0.66</td>\n<td id=\"S4.T3.1.1.5.2.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">19.39±0.63*</td>\n<td id=\"S4.T3.1.1.5.2.7\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">21.32±0.71*</td>\n<td id=\"S4.T3.1.1.5.2.8\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">19.18±0.30</td>\n<td id=\"S4.T3.1.1.5.2.9\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">19.40±1.76</td>\n</tr>\n<tr id=\"S4.T3.1.1.6.3\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.1.6.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">FedPer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib34\" title=\"\" class=\"ltx_ref\">34</a>]</cite>\n</th>\n<td id=\"S4.T3.1.1.6.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">83.39±0.47*</td>\n<td id=\"S4.T3.1.1.6.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">80.99±0.71*</td>\n<td id=\"S4.T3.1.1.6.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">77.99±0.02</td>\n<td id=\"S4.T3.1.1.6.3.5\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">74.21±0.07</td>\n<td id=\"S4.T3.1.1.6.3.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">48.32±1.46*</td>\n<td id=\"S4.T3.1.1.6.3.7\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">42.08±0.18*</td>\n<td id=\"S4.T3.1.1.6.3.8\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">22.60±0.59</td>\n<td id=\"S4.T3.1.1.6.3.9\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">20.06±0.26</td>\n</tr>\n<tr id=\"S4.T3.1.1.7.4\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.1.7.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">pFedMe <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib24\" title=\"\" class=\"ltx_ref\">24</a>]</cite>\n</th>\n<td id=\"S4.T3.1.1.7.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">86.09±0.32*</td>\n<td id=\"S4.T3.1.1.7.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">85.23±0.58*</td>\n<td id=\"S4.T3.1.1.7.4.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">76.29±0.44</td>\n<td id=\"S4.T3.1.1.7.4.5\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">74.83±0.28</td>\n<td id=\"S4.T3.1.1.7.4.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">49.09±1.10*</td>\n<td id=\"S4.T3.1.1.7.4.7\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">45.57±1.02*</td>\n<td id=\"S4.T3.1.1.7.4.8\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">31.60±0.46</td>\n<td id=\"S4.T3.1.1.7.4.9\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">25.43±0.52</td>\n</tr>\n<tr id=\"S4.T3.1.1.8.5\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.1.8.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">FedBN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib36\" title=\"\" class=\"ltx_ref\">36</a>]</cite>\n</th>\n<td id=\"S4.T3.1.1.8.5.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">87.45±0.95</td>\n<td id=\"S4.T3.1.1.8.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">86.71±0.56</td>\n<td id=\"S4.T3.1.1.8.5.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">74.63±0.60</td>\n<td id=\"S4.T3.1.1.8.5.5\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">75.41±0.37</td>\n<td id=\"S4.T3.1.1.8.5.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">50.01±0.59</td>\n<td id=\"S4.T3.1.1.8.5.7\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">48.37±0.56</td>\n<td id=\"S4.T3.1.1.8.5.8\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">28.81±0.50</td>\n<td id=\"S4.T3.1.1.8.5.9\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">28.70±0.46</td>\n</tr>\n<tr id=\"S4.T3.1.1.9.6\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.1.9.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">pFedHN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">43</a>]</cite>\n</th>\n<td id=\"S4.T3.1.1.9.6.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">88.38±0.29*</td>\n<td id=\"S4.T3.1.1.9.6.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">87.97±0.70*</td>\n<td id=\"S4.T3.1.1.9.6.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">71.79±0.57</td>\n<td id=\"S4.T3.1.1.9.6.5\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">68.36±0.86</td>\n<td id=\"S4.T3.1.1.9.6.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">59.48±0.67*</td>\n<td id=\"S4.T3.1.1.9.6.7\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">53.24±0.31*</td>\n<td id=\"S4.T3.1.1.9.6.8\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">34.05±0.41</td>\n<td id=\"S4.T3.1.1.9.6.9\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">29.87±0.69</td>\n</tr>\n<tr id=\"S4.T3.1.1.10.7\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.1.10.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">pFedGP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib37\" title=\"\" class=\"ltx_ref\">37</a>]</cite>\n</th>\n<td id=\"S4.T3.1.1.10.7.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">89.20±0.30*</td>\n<td id=\"S4.T3.1.1.10.7.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">88.80±0.20*</td>\n<td id=\"S4.T3.1.1.10.7.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">—</td>\n<td id=\"S4.T3.1.1.10.7.5\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">—</td>\n<td id=\"S4.T3.1.1.10.7.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">63.30±0.10*</td>\n<td id=\"S4.T3.1.1.10.7.7\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">61.30±0.20*</td>\n<td id=\"S4.T3.1.1.10.7.8\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">—</td>\n<td id=\"S4.T3.1.1.10.7.9\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">—</td>\n</tr>\n<tr id=\"S4.T3.1.1.11.8\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.1.11.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">FedRoD <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib45\" title=\"\" class=\"ltx_ref\">45</a>]</cite>\n</th>\n<td id=\"S4.T3.1.1.11.8.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">89.87±0.03</td>\n<td id=\"S4.T3.1.1.11.8.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T3.1.1.11.8.3.1\" class=\"ltx_text ltx_font_bold\">89.05±0.04</span></td>\n<td id=\"S4.T3.1.1.11.8.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">75.01±0.09</td>\n<td id=\"S4.T3.1.1.11.8.5\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">73.99±0.09</td>\n<td id=\"S4.T3.1.1.11.8.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">56.28±0.14</td>\n<td id=\"S4.T3.1.1.11.8.7\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">54.96±1.30</td>\n<td id=\"S4.T3.1.1.11.8.8\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">27.45±0.73</td>\n<td id=\"S4.T3.1.1.11.8.9\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">28.29±1.53</td>\n</tr>\n<tr id=\"S4.T3.1.1.12.9\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.1.12.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">FedTP</th>\n<td id=\"S4.T3.1.1.12.9.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T3.1.1.12.9.2.1\" class=\"ltx_text ltx_font_bold\">90.31±0.26</span></td>\n<td id=\"S4.T3.1.1.12.9.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">88.39±0.14</td>\n<td id=\"S4.T3.1.1.12.9.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T3.1.1.12.9.4.1\" class=\"ltx_text ltx_font_bold\">81.24±0.17</span></td>\n<td id=\"S4.T3.1.1.12.9.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T3.1.1.12.9.5.1\" class=\"ltx_text ltx_font_bold\">80.27±0.28</span></td>\n<td id=\"S4.T3.1.1.12.9.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T3.1.1.12.9.6.1\" class=\"ltx_text ltx_font_bold\">68.05±0.24</span></td>\n<td id=\"S4.T3.1.1.12.9.7\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T3.1.1.12.9.7.1\" class=\"ltx_text ltx_font_bold\">63.76±0.39</span></td>\n<td id=\"S4.T3.1.1.12.9.8\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T3.1.1.12.9.8.1\" class=\"ltx_text ltx_font_bold\">46.35±0.29</span></td>\n<td id=\"S4.T3.1.1.12.9.9\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T3.1.1.12.9.9.1\" class=\"ltx_text ltx_font_bold\">43.74±0.39</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "We use * to represent the results reported in previous works [43, 37] under the same experimental settings.",
        "references": [
            "3) Analysis of Network Backbone:\nWe also implemented experiments to compare the performance of FedTP to many state-of-the-art federated learning methods that are based on CNN/LSTM.\nThe average test accuracy of these algorithms for the image datasets and language dataset are shown in Table III and Table IV, respectively. Clearly, FedTP outperforms the baseline methods in terms of the average test accuracy for most cases. It is worth noting that, the hypernetworks in FedTP only produce the parameters of the self-attention layers, which take up around 9.8% parameters of the whole network. Although FedTP uses a much smaller ratio of personalized parameters, it still significantly outperforms pFedHN with an 11.31% improvement on average for CIFAR-100. As shown in Table III and Table IV, changing the network backbone from a CNN or an LSTM to Transformers will bring a significant improvement in the models’ ability to overcome data heterogeneity.\nThrough comparing Table II and Table III, the decrease in the accuracy of pFedMe-T and FedProx-T in some settings indicates that the performance of Transformer-based models will be affected when there is a regularization term in these federated learning methods. This is possible because the regularization term may impair the self-attention mechanism, while our FedTP maintains the personalization of the self-attention layers through the learn-to-personalize mechanism. More details about the effects of learn-to-personalize will be shown in 5) Visualization of Attention Maps."
        ]
    },
    "S4.T4": {
        "caption": "TABLE IV: \nAverage test accuracy on the language dataset Shakespeare of different methods.\n",
        "table": "<table id=\"S4.T4.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T4.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\">Method</th>\n<th id=\"S4.T4.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">FedAvg</th>\n<th id=\"S4.T4.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">FedProx</th>\n<th id=\"S4.T4.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">FedPer</th>\n<th id=\"S4.T4.1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">pFedMe</th>\n<th id=\"S4.T4.1.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">FedTP</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T4.1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t\">Test accuracy</th>\n<td id=\"S4.T4.1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">21.34±1.04</td>\n<td id=\"S4.T4.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">20.48±1.09</td>\n<td id=\"S4.T4.1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">27.56±0.65</td>\n<td id=\"S4.T4.1.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">21.14±1.12</td>\n<td id=\"S4.T4.1.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S4.T4.1.1.2.1.6.1\" class=\"ltx_text ltx_font_bold\">84.40±0.10</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "3) Analysis of Network Backbone:\nWe also implemented experiments to compare the performance of FedTP to many state-of-the-art federated learning methods that are based on CNN/LSTM.\nThe average test accuracy of these algorithms for the image datasets and language dataset are shown in Table III and Table IV, respectively. Clearly, FedTP outperforms the baseline methods in terms of the average test accuracy for most cases. It is worth noting that, the hypernetworks in FedTP only produce the parameters of the self-attention layers, which take up around 9.8% parameters of the whole network. Although FedTP uses a much smaller ratio of personalized parameters, it still significantly outperforms pFedHN with an 11.31% improvement on average for CIFAR-100. As shown in Table III and Table IV, changing the network backbone from a CNN or an LSTM to Transformers will bring a significant improvement in the models’ ability to overcome data heterogeneity.\nThrough comparing Table II and Table III, the decrease in the accuracy of pFedMe-T and FedProx-T in some settings indicates that the performance of Transformer-based models will be affected when there is a regularization term in these federated learning methods. This is possible because the regularization term may impair the self-attention mechanism, while our FedTP maintains the personalization of the self-attention layers through the learn-to-personalize mechanism. More details about the effects of learn-to-personalize will be shown in 5) Visualization of Attention Maps."
        ]
    },
    "S4.T5": {
        "caption": "TABLE V: \nTest accuracy on average for Transformer-based models with different personalized part over 50 clients.\n",
        "table": "<table id=\"S4.T5.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T5.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T5.1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\" rowspan=\"2\"><span id=\"S4.T5.1.1.1.1.1.1\" class=\"ltx_text\">\n<span id=\"S4.T5.1.1.1.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S4.T5.1.1.1.1.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S4.T5.1.1.1.1.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Personalized part</span></span>\n</span></span></th>\n<th id=\"S4.T5.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\" colspan=\"2\">CIFAR-10</th>\n<th id=\"S4.T5.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\" colspan=\"2\">CIFAR-100</th>\n</tr>\n<tr id=\"S4.T5.1.1.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T5.1.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Pathological</th>\n<th id=\"S4.T5.1.1.2.2.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Dirichlet</th>\n<th id=\"S4.T5.1.1.2.2.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Pathological</th>\n<th id=\"S4.T5.1.1.2.2.4\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Dirichlet</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T5.1.1.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T5.1.1.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Self-attention (FedTP)</th>\n<td id=\"S4.T5.1.1.3.1.2\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T5.1.1.3.1.2.1\" class=\"ltx_text ltx_font_bold\">90.31±0.26</span></td>\n<td id=\"S4.T5.1.1.3.1.3\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T5.1.1.3.1.3.1\" class=\"ltx_text ltx_font_bold\">81.24±0.17</span></td>\n<td id=\"S4.T5.1.1.3.1.4\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T5.1.1.3.1.4.1\" class=\"ltx_text ltx_font_bold\">68.05±0.24</span></td>\n<td id=\"S4.T5.1.1.3.1.5\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T5.1.1.3.1.5.1\" class=\"ltx_text ltx_font_bold\">46.35±0.29</span></td>\n</tr>\n<tr id=\"S4.T5.1.1.4.2\" class=\"ltx_tr\">\n<th id=\"S4.T5.1.1.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">MLP Layers</th>\n<td id=\"S4.T5.1.1.4.2.2\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">88.45±0.14</td>\n<td id=\"S4.T5.1.1.4.2.3\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">77.36±0.17</td>\n<td id=\"S4.T5.1.1.4.2.4\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">59.76±0.14</td>\n<td id=\"S4.T5.1.1.4.2.5\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">35.65±0.16</td>\n</tr>\n<tr id=\"S4.T5.1.1.5.3\" class=\"ltx_tr\">\n<th id=\"S4.T5.1.1.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Normalization Layers</th>\n<td id=\"S4.T5.1.1.5.3.2\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">89.56±0.45</td>\n<td id=\"S4.T5.1.1.5.3.3\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">78.15±0.27</td>\n<td id=\"S4.T5.1.1.5.3.4\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">64.23±0.37</td>\n<td id=\"S4.T5.1.1.5.3.5\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">41.22±0.39</td>\n</tr>\n<tr id=\"S4.T5.1.1.6.4\" class=\"ltx_tr\">\n<th id=\"S4.T5.1.1.6.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">The Whole Encoder</th>\n<td id=\"S4.T5.1.1.6.4.2\" class=\"ltx_td ltx_align_left ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">82.34±0.43</td>\n<td id=\"S4.T5.1.1.6.4.3\" class=\"ltx_td ltx_align_left ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">73.65±0.52</td>\n<td id=\"S4.T5.1.1.6.4.4\" class=\"ltx_td ltx_align_left ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">59.79±0.24</td>\n<td id=\"S4.T5.1.1.6.4.5\" class=\"ltx_td ltx_align_left ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">33.95±0.37</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "4) Analysis of Different Personalized Parts:\nHere we investigate the impacts of personalizing different components in the Transformer: (1) the self-attention layers (our method), (2) the MLP layers, (3) the normalization layers, (4) the whole encoder. In this experiment, we use the same hypernetwork to generate the parameters of these different components, and keep the same structures of ViT as we described in the Subsection of Implementation Details. The results are shown in Table V, from which we can see that personalizing self-attention layers achieves the best results compared to personalizing other components. Besides, Table V also shows that personalizing the normalization layers works better than personalizing the MLP layers and the whole encoder."
        ]
    },
    "S4.T6": {
        "caption": "TABLE VI: \nTest accuracy on average for FedTP and its variants over 100 clients.\n",
        "table": "<table id=\"S4.T6.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T6.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T6.1.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"></th>\n<th id=\"S4.T6.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\" colspan=\"2\">CIFAR-10</th>\n<th id=\"S4.T6.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\" colspan=\"2\">CIFAR-100</th>\n</tr>\n<tr id=\"S4.T6.1.1.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T6.1.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">#setting</th>\n<th id=\"S4.T6.1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Pathological</th>\n<th id=\"S4.T6.1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Dirichlet</th>\n<th id=\"S4.T6.1.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Pathological</th>\n<th id=\"S4.T6.1.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Dirichlet</th>\n</tr>\n<tr id=\"S4.T6.1.1.3.3\" class=\"ltx_tr\">\n<th id=\"S4.T6.1.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">FedTP (ours)</th>\n<th id=\"S4.T6.1.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">88.30±0.35</th>\n<th id=\"S4.T6.1.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">79.22±0.29</th>\n<th id=\"S4.T6.1.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">60.90±0.26</th>\n<th id=\"S4.T6.1.1.3.3.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">39.74±0.32</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T6.1.1.4.1\" class=\"ltx_tr\">\n<th id=\"S4.T6.1.1.4.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">FedTP+FedPer</th>\n<td id=\"S4.T6.1.1.4.1.2\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">89.42±0.10</td>\n<td id=\"S4.T6.1.1.4.1.3\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">78.18±0.14</td>\n<td id=\"S4.T6.1.1.4.1.4\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">61.78±0.14</td>\n<td id=\"S4.T6.1.1.4.1.5\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">32.86±0.19</td>\n</tr>\n<tr id=\"S4.T6.1.1.5.2\" class=\"ltx_tr\">\n<th id=\"S4.T6.1.1.5.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">FedTP+FedRoD</th>\n<td id=\"S4.T6.1.1.5.2.2\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">87.66±0.31</td>\n<td id=\"S4.T6.1.1.5.2.3\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">78.67±0.37</td>\n<td id=\"S4.T6.1.1.5.2.4\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">64.62±0.22</td>\n<td id=\"S4.T6.1.1.5.2.5\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">42.30±0.26</td>\n</tr>\n<tr id=\"S4.T6.1.1.6.3\" class=\"ltx_tr\">\n<th id=\"S4.T6.1.1.6.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">FedTP+KNN</th>\n<td id=\"S4.T6.1.1.6.3.2\" class=\"ltx_td ltx_align_left ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">88.84±0.18</td>\n<td id=\"S4.T6.1.1.6.3.3\" class=\"ltx_td ltx_align_left ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">80.18±0.12</td>\n<td id=\"S4.T6.1.1.6.3.4\" class=\"ltx_td ltx_align_left ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">64.30±0.27</td>\n<td id=\"S4.T6.1.1.6.3.5\" class=\"ltx_td ltx_align_left ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">40.70±0.26</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "6) Extension of FedTP:\nIn this section, we mainly explore the compatibility of FedTP, and our FedTP is compatible with methods with personalized classifier head including FedPer [34], FedRod [45], and methods making use of local memory like KNN-Per [29]. We inserted those modules into FedTP and evaluated their performance on the CIFAR-10/CIFAR-100 datasets. In our implementation, FedTP+FedPer retains each client’s own classification head locally based on FedTP.\nFedTP+FedRoD computes the sum of the output of the personalized classification head and the global classification head as the prediction logits.\nFedTP+KNN establishes and maintains a local repository in a similar way to Knn-Per which relies on FAISS library [51].\nThe results are shown in Table VI."
        ]
    },
    "S4.T7": {
        "caption": "TABLE VII: The results of FedTP and the benchmark methods on the image datasets over 100 clients with different α𝛼\\alpha of Dirichlet setting.",
        "table": "<table id=\"S4.T7.3.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T7.3.1.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T7.3.1.2.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"></th>\n<th id=\"S4.T7.3.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\" colspan=\"5\">CIFAR-10</th>\n<th id=\"S4.T7.3.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\" colspan=\"5\">CIFAR-100</th>\n</tr>\n<tr id=\"S4.T7.3.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T7.3.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><math id=\"S4.T7.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\#\\alpha\" display=\"inline\"><semantics id=\"S4.T7.3.1.1.1.m1.1a\"><mrow id=\"S4.T7.3.1.1.1.m1.1.1\" xref=\"S4.T7.3.1.1.1.m1.1.1.cmml\"><mi mathvariant=\"normal\" id=\"S4.T7.3.1.1.1.m1.1.1.2\" xref=\"S4.T7.3.1.1.1.m1.1.1.2.cmml\">#</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.T7.3.1.1.1.m1.1.1.1\" xref=\"S4.T7.3.1.1.1.m1.1.1.1.cmml\">​</mo><mi id=\"S4.T7.3.1.1.1.m1.1.1.3\" xref=\"S4.T7.3.1.1.1.m1.1.1.3.cmml\">α</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T7.3.1.1.1.m1.1b\"><apply id=\"S4.T7.3.1.1.1.m1.1.1.cmml\" xref=\"S4.T7.3.1.1.1.m1.1.1\"><times id=\"S4.T7.3.1.1.1.m1.1.1.1.cmml\" xref=\"S4.T7.3.1.1.1.m1.1.1.1\"></times><ci id=\"S4.T7.3.1.1.1.m1.1.1.2.cmml\" xref=\"S4.T7.3.1.1.1.m1.1.1.2\">#</ci><ci id=\"S4.T7.3.1.1.1.m1.1.1.3.cmml\" xref=\"S4.T7.3.1.1.1.m1.1.1.3\">𝛼</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T7.3.1.1.1.m1.1c\">\\#\\alpha</annotation></semantics></math></th>\n<th id=\"S4.T7.3.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">0.1</th>\n<th id=\"S4.T7.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">0.3</th>\n<th id=\"S4.T7.3.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">0.5</th>\n<th id=\"S4.T7.3.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">0.7</th>\n<th id=\"S4.T7.3.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">0.9</th>\n<th id=\"S4.T7.3.1.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">0.1</th>\n<th id=\"S4.T7.3.1.1.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">0.3</th>\n<th id=\"S4.T7.3.1.1.9\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">0.5</th>\n<th id=\"S4.T7.3.1.1.10\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">0.7</th>\n<th id=\"S4.T7.3.1.1.11\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">0.9</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T7.3.1.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T7.3.1.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">FedAvg-T</th>\n<td id=\"S4.T7.3.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">40.99±6.20</td>\n<td id=\"S4.T7.3.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">59.23±1.93</td>\n<td id=\"S4.T7.3.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">63.69±1.33</td>\n<td id=\"S4.T7.3.1.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">65.29±1.12</td>\n<td id=\"S4.T7.3.1.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">65.82±0.82</td>\n<td id=\"S4.T7.3.1.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">32.72±0.81</td>\n<td id=\"S4.T7.3.1.3.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">34.89±0.45</td>\n<td id=\"S4.T7.3.1.3.1.9\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">36.25±1.79</td>\n<td id=\"S4.T7.3.1.3.1.10\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">36.99±0.44</td>\n<td id=\"S4.T7.3.1.3.1.11\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">37.51±0.33</td>\n</tr>\n<tr id=\"S4.T7.3.1.4.2\" class=\"ltx_tr\">\n<th id=\"S4.T7.3.1.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">FedPer-T</th>\n<td id=\"S4.T7.3.1.4.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">87.45±0.14</td>\n<td id=\"S4.T7.3.1.4.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">77.7±0.14</td>\n<td id=\"S4.T7.3.1.4.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">72.44±0.22</td>\n<td id=\"S4.T7.3.1.4.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">70.11±0.21</td>\n<td id=\"S4.T7.3.1.4.2.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">71.13±0.14</td>\n<td id=\"S4.T7.3.1.4.2.7\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">40.92±0.23</td>\n<td id=\"S4.T7.3.1.4.2.8\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">29.58±0.14</td>\n<td id=\"S4.T7.3.1.4.2.9\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">27.02±0.11</td>\n<td id=\"S4.T7.3.1.4.2.10\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">27.12±0.09</td>\n<td id=\"S4.T7.3.1.4.2.11\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">25.29±0.13</td>\n</tr>\n<tr id=\"S4.T7.3.1.5.3\" class=\"ltx_tr\">\n<th id=\"S4.T7.3.1.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">pFedHN</th>\n<td id=\"S4.T7.3.1.5.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">83.07±1.07</td>\n<td id=\"S4.T7.3.1.5.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">68.36±0.86</td>\n<td id=\"S4.T7.3.1.5.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">71.42±0.62</td>\n<td id=\"S4.T7.3.1.5.3.5\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">68.19±0.81</td>\n<td id=\"S4.T7.3.1.5.3.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">67.62±0.75</td>\n<td id=\"S4.T7.3.1.5.3.7\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">41.37±0.50</td>\n<td id=\"S4.T7.3.1.5.3.8\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">29.87±0.69</td>\n<td id=\"S4.T7.3.1.5.3.9\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">34.55±0.72</td>\n<td id=\"S4.T7.3.1.5.3.10\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">34.17±0.65</td>\n<td id=\"S4.T7.3.1.5.3.11\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">33.75±0.58</td>\n</tr>\n<tr id=\"S4.T7.3.1.6.4\" class=\"ltx_tr\">\n<th id=\"S4.T7.3.1.6.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">FedBN</th>\n<td id=\"S4.T7.3.1.6.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">84.93±0.53</td>\n<td id=\"S4.T7.3.1.6.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">75.41±0.37</td>\n<td id=\"S4.T7.3.1.6.4.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">71.79±0.63</td>\n<td id=\"S4.T7.3.1.6.4.5\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">69.57±0.56</td>\n<td id=\"S4.T7.3.1.6.4.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">68.70±0.47</td>\n<td id=\"S4.T7.3.1.6.4.7\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">33.+1±0.04</td>\n<td id=\"S4.T7.3.1.6.4.8\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">28.70±0.46</td>\n<td id=\"S4.T7.3.1.6.4.9\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">26.07±0.45</td>\n<td id=\"S4.T7.3.1.6.4.10\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">26.13±0.35</td>\n<td id=\"S4.T7.3.1.6.4.11\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">25.08±0.39</td>\n</tr>\n<tr id=\"S4.T7.3.1.7.5\" class=\"ltx_tr\">\n<th id=\"S4.T7.3.1.7.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">FedTP</th>\n<td id=\"S4.T7.3.1.7.5.2\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T7.3.1.7.5.2.1\" class=\"ltx_text ltx_font_bold\">87.67±0.15</span></td>\n<td id=\"S4.T7.3.1.7.5.3\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T7.3.1.7.5.3.1\" class=\"ltx_text ltx_font_bold\">80.27±0.28</span></td>\n<td id=\"S4.T7.3.1.7.5.4\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T7.3.1.7.5.4.1\" class=\"ltx_text ltx_font_bold\">75.75±0.26</span></td>\n<td id=\"S4.T7.3.1.7.5.5\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T7.3.1.7.5.5.1\" class=\"ltx_text ltx_font_bold\">73.16±0.25</span></td>\n<td id=\"S4.T7.3.1.7.5.6\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T7.3.1.7.5.6.1\" class=\"ltx_text ltx_font_bold\">72.12±0.27</span></td>\n<td id=\"S4.T7.3.1.7.5.7\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T7.3.1.7.5.7.1\" class=\"ltx_text ltx_font_bold\">48.27±0.26</span></td>\n<td id=\"S4.T7.3.1.7.5.8\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T7.3.1.7.5.8.1\" class=\"ltx_text ltx_font_bold\">41.98±0.22</span></td>\n<td id=\"S4.T7.3.1.7.5.9\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T7.3.1.7.5.9.1\" class=\"ltx_text ltx_font_bold\">38.83±0.23</span></td>\n<td id=\"S4.T7.3.1.7.5.10\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T7.3.1.7.5.10.1\" class=\"ltx_text ltx_font_bold\">38.06±0.31</span></td>\n<td id=\"S4.T7.3.1.7.5.11\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T7.3.1.7.5.11.1\" class=\"ltx_text ltx_font_bold\">38.06±0.22</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "From the results in Table VII, it can be easily found that with a higher degree of data heterogeneity, performance for FedAvg-T decreases while performance for methods with personalized modules increases. Among these methods, FedTP outperforms them in every case and is much more robust. Hence, FedTP could well overcome label distribution heterogeneity in a wide range.\nAs α𝛼\\alpha increases, some personalized federated learning algorithms may fail to make full use of heterogeneity in each client and even falls behind FedAvg-T in accuracy. FedTP, however, is still able to work well. This indicates that FedTP could better mine and exploit heterogeneity among clients even if heterogeneity is not so prominent."
        ]
    },
    "S4.T8": {
        "caption": "TABLE VIII: The test accuracy of FedTP and FedAvg-T on image datasets over 50 clients with different numbers of self-attention blocks in ViT.",
        "table": "<table id=\"S4.T8.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T8.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T8.1.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"></th>\n<th id=\"S4.T8.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\" colspan=\"4\">FedAvg-T</th>\n<th id=\"S4.T8.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\" colspan=\"4\">FedTP</th>\n</tr>\n<tr id=\"S4.T8.1.1.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T8.1.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Block number</th>\n<th id=\"S4.T8.1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\" colspan=\"2\">CIFAR-10</th>\n<th id=\"S4.T8.1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\" colspan=\"2\">CIFAR-100</th>\n<th id=\"S4.T8.1.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\" colspan=\"2\">CIFAR-10</th>\n<th id=\"S4.T8.1.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\" colspan=\"2\">CIFAR-100</th>\n</tr>\n<tr id=\"S4.T8.1.1.3.3\" class=\"ltx_tr\">\n<th id=\"S4.T8.1.1.3.3.1\" class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"></th>\n<th id=\"S4.T8.1.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Pathological</th>\n<th id=\"S4.T8.1.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Dirichlet</th>\n<th id=\"S4.T8.1.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Pathological</th>\n<th id=\"S4.T8.1.1.3.3.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Dirichlet</th>\n<th id=\"S4.T8.1.1.3.3.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Pathological</th>\n<th id=\"S4.T8.1.1.3.3.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Dirichlet</th>\n<th id=\"S4.T8.1.1.3.3.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Pathological</th>\n<th id=\"S4.T8.1.1.3.3.9\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Dirichlet</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T8.1.1.4.1\" class=\"ltx_tr\">\n<th id=\"S4.T8.1.1.4.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">1</th>\n<td id=\"S4.T8.1.1.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">36.40±5.48</td>\n<td id=\"S4.T8.1.1.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">49.38±2.60</td>\n<td id=\"S4.T8.1.1.4.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">23.05±1.03</td>\n<td id=\"S4.T8.1.1.4.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">27.02±0.44</td>\n<td id=\"S4.T8.1.1.4.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">85.15±0.17</td>\n<td id=\"S4.T8.1.1.4.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">72.56±0.24</td>\n<td id=\"S4.T8.1.1.4.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">59.02±0.30</td>\n<td id=\"S4.T8.1.1.4.1.9\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">34.97±0.23</td>\n</tr>\n<tr id=\"S4.T8.1.1.5.2\" class=\"ltx_tr\">\n<th id=\"S4.T8.1.1.5.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">2</th>\n<td id=\"S4.T8.1.1.5.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">46.58±4.11</td>\n<td id=\"S4.T8.1.1.5.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">55.82±2.31</td>\n<td id=\"S4.T8.1.1.5.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">27.58±1.04</td>\n<td id=\"S4.T8.1.1.5.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">32.92±0.51</td>\n<td id=\"S4.T8.1.1.5.2.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">88.42±0.17</td>\n<td id=\"S4.T8.1.1.5.2.7\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">77.04±0.23</td>\n<td id=\"S4.T8.1.1.5.2.8\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">64.40±0.22</td>\n<td id=\"S4.T8.1.1.5.2.9\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">41.20±0.27</td>\n</tr>\n<tr id=\"S4.T8.1.1.6.3\" class=\"ltx_tr\">\n<th id=\"S4.T8.1.1.6.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">4</th>\n<td id=\"S4.T8.1.1.6.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">50.21±4.18</td>\n<td id=\"S4.T8.1.1.6.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">60.19±1.75</td>\n<td id=\"S4.T8.1.1.6.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">31.65±0.85</td>\n<td id=\"S4.T8.1.1.6.3.5\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">36.23±0.37</td>\n<td id=\"S4.T8.1.1.6.3.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">89.58±0.12</td>\n<td id=\"S4.T8.1.1.6.3.7\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">78.86±0.26</td>\n<td id=\"S4.T8.1.1.6.3.8\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">66.35±0.21</td>\n<td id=\"S4.T8.1.1.6.3.9\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">43.90±0.26</td>\n</tr>\n<tr id=\"S4.T8.1.1.7.4\" class=\"ltx_tr\">\n<th id=\"S4.T8.1.1.7.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">6</th>\n<td id=\"S4.T8.1.1.7.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">49.75±4.12</td>\n<td id=\"S4.T8.1.1.7.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">61.34±1.48</td>\n<td id=\"S4.T8.1.1.7.4.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">31.74±0.73</td>\n<td id=\"S4.T8.1.1.7.4.5\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">36.15±0.32</td>\n<td id=\"S4.T8.1.1.7.4.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">89.95±0.15</td>\n<td id=\"S4.T8.1.1.7.4.7\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">80.00±0.34</td>\n<td id=\"S4.T8.1.1.7.4.8\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">66.51±0.35</td>\n<td id=\"S4.T8.1.1.7.4.9\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">43.69±0.17</td>\n</tr>\n<tr id=\"S4.T8.1.1.8.5\" class=\"ltx_tr\">\n<th id=\"S4.T8.1.1.8.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">8</th>\n<td id=\"S4.T8.1.1.8.5.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T8.1.1.8.5.2.1\" class=\"ltx_text ltx_font_bold\">50.42±4.22</span></td>\n<td id=\"S4.T8.1.1.8.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">61.85±1.52</td>\n<td id=\"S4.T8.1.1.8.5.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T8.1.1.8.5.4.1\" class=\"ltx_text ltx_font_bold\">34.02±0.88</span></td>\n<td id=\"S4.T8.1.1.8.5.5\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T8.1.1.8.5.5.1\" class=\"ltx_text ltx_font_bold\">38.64±0.22</span></td>\n<td id=\"S4.T8.1.1.8.5.6\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T8.1.1.8.5.6.1\" class=\"ltx_text ltx_font_bold\">90.31±0.26</span></td>\n<td id=\"S4.T8.1.1.8.5.7\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">81.24±0.17</td>\n<td id=\"S4.T8.1.1.8.5.8\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">68.05±0.24</td>\n<td id=\"S4.T8.1.1.8.5.9\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T8.1.1.8.5.9.1\" class=\"ltx_text ltx_font_bold\">46.35±0.29</span></td>\n</tr>\n<tr id=\"S4.T8.1.1.9.6\" class=\"ltx_tr\">\n<th id=\"S4.T8.1.1.9.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">10</th>\n<td id=\"S4.T8.1.1.9.6.2\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">46.73±4.43</td>\n<td id=\"S4.T8.1.1.9.6.3\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T8.1.1.9.6.3.1\" class=\"ltx_text ltx_font_bold\">62.17±1.57</span></td>\n<td id=\"S4.T8.1.1.9.6.4\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">33.46±0.82</td>\n<td id=\"S4.T8.1.1.9.6.5\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">37.52±0.21</td>\n<td id=\"S4.T8.1.1.9.6.6\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">89.98±0.26</td>\n<td id=\"S4.T8.1.1.9.6.7\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T8.1.1.9.6.7.1\" class=\"ltx_text ltx_font_bold\">82.02±0.21</span></td>\n<td id=\"S4.T8.1.1.9.6.8\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T8.1.1.9.6.8.1\" class=\"ltx_text ltx_font_bold\">69.14±0.44</span></td>\n<td id=\"S4.T8.1.1.9.6.9\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">45.42±0.26</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "3) Impact of the Self-Attention Block Number:\nHere we examined the impact of self-attention block number with its value L∈{1,2,4,6,8,10}𝐿1246810L\\in\\{1,2,4,6,8,10\\}. Table VIII shows results for FedTP with different block numbers. As can be seen, accumulating attention blocks will help the model develop a better ability to catch its data heterogeneity and improve model behavior to a certain extent. According to this table’s results, we choose 8 as our FedTP’s default attention block number for CIFAR-10 and CIFAR-100."
        ]
    }
}