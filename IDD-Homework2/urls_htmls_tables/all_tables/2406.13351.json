{
    "PAPER'S NUMBER OF TABLES": 5,
    "S1.T1": {
        "caption": "Table 1: Comparison of existing works",
        "table": "<table id=\"S1.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S1.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S1.T1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Related works</th>\n<th id=\"S1.T1.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Computing Resource</th>\n<th id=\"S1.T1.1.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Communication Resource</th>\n<th id=\"S1.T1.1.1.1.4\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Aggregation mode</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S1.T1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S1.T1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\n<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">12</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">13</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib15\" title=\"\" class=\"ltx_ref\">15</a>]</cite>\n</th>\n<td id=\"S1.T1.1.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_t\">Limited</td>\n<td id=\"S1.T1.1.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_t\">Sufficient</td>\n<td id=\"S1.T1.1.2.1.4\" class=\"ltx_td ltx_align_left ltx_border_t\">Synchronous</td>\n</tr>\n<tr id=\"S1.T1.1.3.2\" class=\"ltx_tr\">\n<th id=\"S1.T1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib14\" title=\"\" class=\"ltx_ref\">14</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib16\" title=\"\" class=\"ltx_ref\">16</a>]</cite>\n</th>\n<td id=\"S1.T1.1.3.2.2\" class=\"ltx_td ltx_align_left\">Sufficient</td>\n<td id=\"S1.T1.1.3.2.3\" class=\"ltx_td ltx_align_left\">Limited</td>\n<td id=\"S1.T1.1.3.2.4\" class=\"ltx_td ltx_align_left\">Synchronous</td>\n</tr>\n<tr id=\"S1.T1.1.4.3\" class=\"ltx_tr\">\n<th id=\"S1.T1.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">9</a>]</cite>, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">17</a>]</cite>\n</th>\n<td id=\"S1.T1.1.4.3.2\" class=\"ltx_td ltx_align_left\">Limited</td>\n<td id=\"S1.T1.1.4.3.3\" class=\"ltx_td ltx_align_left\">Limited</td>\n<td id=\"S1.T1.1.4.3.4\" class=\"ltx_td ltx_align_left\">Synchronous</td>\n</tr>\n<tr id=\"S1.T1.1.5.4\" class=\"ltx_tr\">\n<th id=\"S1.T1.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Our work</th>\n<td id=\"S1.T1.1.5.4.2\" class=\"ltx_td ltx_align_left ltx_border_bb\">Limited</td>\n<td id=\"S1.T1.1.5.4.3\" class=\"ltx_td ltx_align_left ltx_border_bb\">Limited</td>\n<td id=\"S1.T1.1.5.4.4\" class=\"ltx_td ltx_align_left ltx_border_bb\">Asynchronous</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "In the past decades, a series of relevant works have been proposed to train a large-scale ML model among resource-constrained clients under FL framework [9, 10, 11, 12, 13, 5, 14, 15], which generally fall into two categories. The works in the first branch [12, 13, 15] focus on clients with limited computational resources. By splitting the whole ML model into multiple submodels, each client only trains one of the submodels locally according to its computation ability. In the model aggregation step, those submodels will be aggregated into a global one and disseminated to the clients for the next round of training.\nIn the second branch, the works mainly address the communication efficiency problem in FL since clients may have limited bandwidth. The techniques such as model compression, fine-tuning, and model scaling have been adopted in [10], [5, 14], and [11] separately, which mitigate the communication overhead for clients in local model updating and global model disseminating steps. Additionally, in  [9], the computing and network resources constraints are simultaneously considered in their model splitting process. Whereas, all of the works mentioned above are executed under the synchronous aggregation mode, i.e. the model aggregation in each training round starts only when all the clients have their local models updated. Thus, the efficiency of the FL process will be delayed by the slowest client, which is often the client with the smallest computing and communication resources. The summary of the previous works is listed in Table 1. To the best of our knowledge, few of the works consider the FL problem under the constrained computing and communication resources simultaneously under the asynchronous setting."
        ]
    },
    "S6.T2": {
        "caption": "Table 2: Comparison of the time and epochs required to achieve specified accuracy.\n<MNIST,0.93>, <CIFAR-10,0.45>, and <CIFAR-100,0.19> are the tuple of <Dataset, Accuracy>.",
        "table": "<table id=\"S6.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S6.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S6.T2.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Time (s)</th>\n<th id=\"S6.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">&lt;MNIST,0.93&gt;</th>\n<th id=\"S6.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">&lt;CIFAR-10,0.45&gt;</th>\n<th id=\"S6.T2.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">&lt;CIFAR-100,0.19&gt;</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T2.1.2.1\" class=\"ltx_tr\">\n<th id=\"S6.T2.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Fed-Avg</th>\n<td id=\"S6.T2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">406.35</td>\n<td id=\"S6.T2.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">390.83</td>\n<td id=\"S6.T2.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">435.51</td>\n</tr>\n<tr id=\"S6.T2.1.3.2\" class=\"ltx_tr\">\n<th id=\"S6.T2.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Fed-Sync</th>\n<td id=\"S6.T2.1.3.2.2\" class=\"ltx_td ltx_align_center\">242.46</td>\n<td id=\"S6.T2.1.3.2.3\" class=\"ltx_td ltx_align_center\">253.11</td>\n<td id=\"S6.T2.1.3.2.4\" class=\"ltx_td ltx_align_center\">1123.25</td>\n</tr>\n<tr id=\"S6.T2.1.4.3\" class=\"ltx_tr\">\n<th id=\"S6.T2.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Fed-Prox</th>\n<td id=\"S6.T2.1.4.3.2\" class=\"ltx_td ltx_align_center\">866.40</td>\n<td id=\"S6.T2.1.4.3.3\" class=\"ltx_td ltx_align_center\">870.44</td>\n<td id=\"S6.T2.1.4.3.4\" class=\"ltx_td ltx_align_center\">910.98</td>\n</tr>\n<tr id=\"S6.T2.1.5.4\" class=\"ltx_tr\">\n<th id=\"S6.T2.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Fed-Prun</th>\n<td id=\"S6.T2.1.5.4.2\" class=\"ltx_td ltx_align_center\">776.82</td>\n<td id=\"S6.T2.1.5.4.3\" class=\"ltx_td ltx_align_center\">975.48</td>\n<td id=\"S6.T2.1.5.4.4\" class=\"ltx_td ltx_align_center\">3158.55</td>\n</tr>\n<tr id=\"S6.T2.1.6.5\" class=\"ltx_tr\">\n<th id=\"S6.T2.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Split-FL</th>\n<td id=\"S6.T2.1.6.5.2\" class=\"ltx_td ltx_align_center\">840.15</td>\n<td id=\"S6.T2.1.6.5.3\" class=\"ltx_td ltx_align_center\">769.88</td>\n<td id=\"S6.T2.1.6.5.4\" class=\"ltx_td ltx_align_center\">2181.59</td>\n</tr>\n<tr id=\"S6.T2.1.7.6\" class=\"ltx_tr\">\n<th id=\"S6.T2.1.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Ram-Fed</th>\n<td id=\"S6.T2.1.7.6.2\" class=\"ltx_td ltx_align_center\">808.96</td>\n<td id=\"S6.T2.1.7.6.3\" class=\"ltx_td ltx_align_center\">783.06</td>\n<td id=\"S6.T2.1.7.6.4\" class=\"ltx_td ltx_align_center\">918.11</td>\n</tr>\n<tr id=\"S6.T2.1.8.7\" class=\"ltx_tr\">\n<th id=\"S6.T2.1.8.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Fed-RAA</th>\n<td id=\"S6.T2.1.8.7.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S6.T2.1.8.7.2.1\" class=\"ltx_text ltx_font_bold\">91.06</span></td>\n<td id=\"S6.T2.1.8.7.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S6.T2.1.8.7.3.1\" class=\"ltx_text ltx_font_bold\">89.85</span></td>\n<td id=\"S6.T2.1.8.7.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S6.T2.1.8.7.4.1\" class=\"ltx_text ltx_font_bold\">101.86</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We conducted experiments across various datasets with our algorithm and baseline algorithms and found that our approach converges more rapidly. As demonstrated in Table 2 and Fig. 3, on the MNIST dataset, our algorithm achieved a 0.93% accuracy in just 91.06 seconds. This faster convergence can be attributed to more frequent updates within shorter intervals due to its asynchronous nature. Similar effects were observed on other datasets as well. This highlights our algorithm’s superiority in rapidly updating and converging compared to traditional synchronous methods and other asynchronous or submodel training approaches."
        ]
    },
    "S6.T3": {
        "caption": "Table 3: Comparison of algorithms on MNIST, CIFAR-10, and CIFAR-100 datasets",
        "table": "<table id=\"S6.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S6.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S6.T3.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"></th>\n<th id=\"S6.T3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\">MNIST</th>\n<th id=\"S6.T3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\">CIFAR-10</th>\n<th id=\"S6.T3.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\">CIFAR-100</th>\n</tr>\n<tr id=\"S6.T3.1.2.2\" class=\"ltx_tr\">\n<th id=\"S6.T3.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\">Baseline</th>\n<th id=\"S6.T3.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Accuracy (%)</th>\n<th id=\"S6.T3.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Time (s)</th>\n<th id=\"S6.T3.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Accuracy (%)</th>\n<th id=\"S6.T3.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Time (s)</th>\n<th id=\"S6.T3.1.2.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Accuracy (%)</th>\n<th id=\"S6.T3.1.2.2.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Time (s)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T3.1.3.1\" class=\"ltx_tr\">\n<th id=\"S6.T3.1.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Gre-RAA</th>\n<td id=\"S6.T3.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">93.95</td>\n<td id=\"S6.T3.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">69.46</td>\n<td id=\"S6.T3.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">47.43</td>\n<td id=\"S6.T3.1.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">68.58</td>\n<td id=\"S6.T3.1.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">20.74</td>\n<td id=\"S6.T3.1.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">75.96</td>\n</tr>\n<tr id=\"S6.T3.1.4.2\" class=\"ltx_tr\">\n<th id=\"S6.T3.1.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Random</th>\n<td id=\"S6.T3.1.4.2.2\" class=\"ltx_td ltx_align_center\">93.76</td>\n<td id=\"S6.T3.1.4.2.3\" class=\"ltx_td ltx_align_center\">78.54</td>\n<td id=\"S6.T3.1.4.2.4\" class=\"ltx_td ltx_align_center\">47.43</td>\n<td id=\"S6.T3.1.4.2.5\" class=\"ltx_td ltx_align_center\">84.63</td>\n<td id=\"S6.T3.1.4.2.6\" class=\"ltx_td ltx_align_center\">18.77</td>\n<td id=\"S6.T3.1.4.2.7\" class=\"ltx_td ltx_align_center\">60.10</td>\n</tr>\n<tr id=\"S6.T3.1.5.3\" class=\"ltx_tr\">\n<th id=\"S6.T3.1.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MP-RAA</th>\n<td id=\"S6.T3.1.5.3.2\" class=\"ltx_td ltx_align_center\">93.88</td>\n<td id=\"S6.T3.1.5.3.3\" class=\"ltx_td ltx_align_center\">80.73</td>\n<td id=\"S6.T3.1.5.3.4\" class=\"ltx_td ltx_align_center\">47.39</td>\n<td id=\"S6.T3.1.5.3.5\" class=\"ltx_td ltx_align_center\">84.77</td>\n<td id=\"S6.T3.1.5.3.6\" class=\"ltx_td ltx_align_center\">20.75</td>\n<td id=\"S6.T3.1.5.3.7\" class=\"ltx_td ltx_align_center\">78.28</td>\n</tr>\n<tr id=\"S6.T3.1.6.4\" class=\"ltx_tr\">\n<th id=\"S6.T3.1.6.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Sync</th>\n<td id=\"S6.T3.1.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">93.94</td>\n<td id=\"S6.T3.1.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">162.71</td>\n<td id=\"S6.T3.1.6.4.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">47.84</td>\n<td id=\"S6.T3.1.6.4.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">162.82</td>\n<td id=\"S6.T3.1.6.4.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">21.46</td>\n<td id=\"S6.T3.1.6.4.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">179.44</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "In addressing the model assignment problem within Fed-RAA, we designed a scheduler-based online algorithm, named Gre-RAA, as outlined in Algorithm 2.\nAlthough it has been theoretically affirmed that Gre-RAA achieves the same upper bound of training delay as that achieved by the optimal offline model assignment strategy, it remains imperative to conduct ablation studies to validate the contributions of Gre-RAA toward enhancing both the convergence speed and the maximum accuracy within Fed-RAA. To this end, we instituted two variance algorithms, encompassing: (1) Random Fed-RAA: random assignment, and (2) Minimum Priority Fed-RAA(MP-RAA): consistently allocating the model segment that has undergone the least amount of training.\nFurthermore, to validate the effectiveness of the asynchronous aggregation approach utilized by Fed-RAA within this framework, we designed a variance algorithm for an ablation study: Sync Fed-RAA. This variant employs a synchronous aggregation method for FL, with all other aspects remaining identical to Fed-RAA.\nWe evaluate the impact of these variance algorithms on Fed-RAA’s performance under the conditions of three distinct dataset scenarios.\nThe accuracy results from this comparative analysis are depicted in Figure 4 and Table 3."
        ]
    },
    "A3.T4": {
        "caption": "Table 4: model partition strategy",
        "table": "<table id=\"A3.T4.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A3.T4.1.1.1\" class=\"ltx_tr\">\n<th id=\"A3.T4.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Number of partitions</th>\n<th id=\"A3.T4.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Partition ratio</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A3.T4.1.2.1\" class=\"ltx_tr\">\n<th id=\"A3.T4.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">2</th>\n<td id=\"A3.T4.1.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_t\">40%, 60%</td>\n</tr>\n<tr id=\"A3.T4.1.3.2\" class=\"ltx_tr\">\n<th id=\"A3.T4.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">3</th>\n<td id=\"A3.T4.1.3.2.2\" class=\"ltx_td ltx_align_left\">20%, 30%, 50%</td>\n</tr>\n<tr id=\"A3.T4.1.4.3\" class=\"ltx_tr\">\n<th id=\"A3.T4.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">4</th>\n<td id=\"A3.T4.1.4.3.2\" class=\"ltx_td ltx_align_left\">10%, 20%, 30%, 40%</td>\n</tr>\n<tr id=\"A3.T4.1.5.4\" class=\"ltx_tr\">\n<th id=\"A3.T4.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\">5</th>\n<td id=\"A3.T4.1.5.4.2\" class=\"ltx_td ltx_align_left ltx_border_bb\">5% , 10%, 20%, 30%, 35%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Model Splitting Strategy. \nIn the Fed-RAA framework, sub-models are formed by a subset of neurons taken from the global model, enabling clients to train a reduced set of parameters to enhance the convergence speed. This is realized by implementing a partitioning strategy on the global model, which is governed by predefined hyperparameters and results in the model being divided into distinct segments. Each client, guided by its computational performance, selects an appropriate sub-model for its training endeavors. We have devised four distinct partition strategies, where the global model’s designated layers are segmented into 2, 3, 4, or 5 parts respectively, with the detailed allocation of these partitions outlined in Table 4. To ensure compatibility across a wide range of client capabilities, the partitioning strategy employs a non-uniform approach: weaker devices are assigned smaller sub-models to minimize total training time, whereas more capable clients receive larger sub-models to optimally leverage their superior processing power."
        ]
    },
    "A3.T5": {
        "caption": "Table 5: Comparison of accuracy and time cost across different baselines",
        "table": "<table id=\"A3.T5.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A3.T5.1.1.1\" class=\"ltx_tr\">\n<th id=\"A3.T5.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Baseline</th>\n<th id=\"A3.T5.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\">MNIST</th>\n<th id=\"A3.T5.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\">CIFAR-10</th>\n<th id=\"A3.T5.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\">CIFAR-100</th>\n</tr>\n<tr id=\"A3.T5.1.2.2\" class=\"ltx_tr\">\n<th id=\"A3.T5.1.2.2.1\" class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th id=\"A3.T5.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Accuracy (%)</th>\n<th id=\"A3.T5.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Time (s)</th>\n<th id=\"A3.T5.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Accuracy (%)</th>\n<th id=\"A3.T5.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Time (s)</th>\n<th id=\"A3.T5.1.2.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Accuracy (%)</th>\n<th id=\"A3.T5.1.2.2.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Time (s)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A3.T5.1.3.1\" class=\"ltx_tr\">\n<th id=\"A3.T5.1.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">FedAvg</th>\n<td id=\"A3.T5.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">96.35</td>\n<td id=\"A3.T5.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">1219.05</td>\n<td id=\"A3.T5.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">52.88</td>\n<td id=\"A3.T5.1.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">1172.49</td>\n<td id=\"A3.T5.1.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">25.22</td>\n<td id=\"A3.T5.1.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">1306.53</td>\n</tr>\n<tr id=\"A3.T5.1.4.2\" class=\"ltx_tr\">\n<th id=\"A3.T5.1.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">FedSync</th>\n<td id=\"A3.T5.1.4.2.2\" class=\"ltx_td ltx_align_center\">96.50</td>\n<td id=\"A3.T5.1.4.2.3\" class=\"ltx_td ltx_align_center\">727.38</td>\n<td id=\"A3.T5.1.4.2.4\" class=\"ltx_td ltx_align_center\">53.01</td>\n<td id=\"A3.T5.1.4.2.5\" class=\"ltx_td ltx_align_center\">759.33</td>\n<td id=\"A3.T5.1.4.2.6\" class=\"ltx_td ltx_align_center\">25.41</td>\n<td id=\"A3.T5.1.4.2.7\" class=\"ltx_td ltx_align_center\">3369.75</td>\n</tr>\n<tr id=\"A3.T5.1.5.3\" class=\"ltx_tr\">\n<th id=\"A3.T5.1.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">FedProx</th>\n<td id=\"A3.T5.1.5.3.2\" class=\"ltx_td ltx_align_center\">96.40</td>\n<td id=\"A3.T5.1.5.3.3\" class=\"ltx_td ltx_align_center\">2599.20</td>\n<td id=\"A3.T5.1.5.3.4\" class=\"ltx_td ltx_align_center\">52.89</td>\n<td id=\"A3.T5.1.5.3.5\" class=\"ltx_td ltx_align_center\">2611.32</td>\n<td id=\"A3.T5.1.5.3.6\" class=\"ltx_td ltx_align_center\">25.87</td>\n<td id=\"A3.T5.1.5.3.7\" class=\"ltx_td ltx_align_center\">2732.94</td>\n</tr>\n<tr id=\"A3.T5.1.6.4\" class=\"ltx_tr\">\n<th id=\"A3.T5.1.6.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">FedPrun</th>\n<td id=\"A3.T5.1.6.4.2\" class=\"ltx_td ltx_align_center\">94.13</td>\n<td id=\"A3.T5.1.6.4.3\" class=\"ltx_td ltx_align_center\">2330.46</td>\n<td id=\"A3.T5.1.6.4.4\" class=\"ltx_td ltx_align_center\">53.60</td>\n<td id=\"A3.T5.1.6.4.5\" class=\"ltx_td ltx_align_center\">2926.44</td>\n<td id=\"A3.T5.1.6.4.6\" class=\"ltx_td ltx_align_center\">24.97</td>\n<td id=\"A3.T5.1.6.4.7\" class=\"ltx_td ltx_align_center\">9475.65</td>\n</tr>\n<tr id=\"A3.T5.1.7.5\" class=\"ltx_tr\">\n<th id=\"A3.T5.1.7.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">SplitFL</th>\n<td id=\"A3.T5.1.7.5.2\" class=\"ltx_td ltx_align_center\">95.30</td>\n<td id=\"A3.T5.1.7.5.3\" class=\"ltx_td ltx_align_center\">2520.45</td>\n<td id=\"A3.T5.1.7.5.4\" class=\"ltx_td ltx_align_center\">51.30</td>\n<td id=\"A3.T5.1.7.5.5\" class=\"ltx_td ltx_align_center\">2309.64</td>\n<td id=\"A3.T5.1.7.5.6\" class=\"ltx_td ltx_align_center\">23.68</td>\n<td id=\"A3.T5.1.7.5.7\" class=\"ltx_td ltx_align_center\">6544.77</td>\n</tr>\n<tr id=\"A3.T5.1.8.6\" class=\"ltx_tr\">\n<th id=\"A3.T5.1.8.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Ram-Fed</th>\n<td id=\"A3.T5.1.8.6.2\" class=\"ltx_td ltx_align_center\">96.55</td>\n<td id=\"A3.T5.1.8.6.3\" class=\"ltx_td ltx_align_center\">2426.88</td>\n<td id=\"A3.T5.1.8.6.4\" class=\"ltx_td ltx_align_center\">54.22</td>\n<td id=\"A3.T5.1.8.6.5\" class=\"ltx_td ltx_align_center\">2349.18</td>\n<td id=\"A3.T5.1.8.6.6\" class=\"ltx_td ltx_align_center\">27.04</td>\n<td id=\"A3.T5.1.8.6.7\" class=\"ltx_td ltx_align_center\">2754.33</td>\n</tr>\n<tr id=\"A3.T5.1.9.7\" class=\"ltx_tr\">\n<th id=\"A3.T5.1.9.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Fed-RAA</th>\n<td id=\"A3.T5.1.9.7.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">93.86</td>\n<td id=\"A3.T5.1.9.7.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">273.18</td>\n<td id=\"A3.T5.1.9.7.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">47.38</td>\n<td id=\"A3.T5.1.9.7.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">269.54</td>\n<td id=\"A3.T5.1.9.7.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">20.55</td>\n<td id=\"A3.T5.1.9.7.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">305.58</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Upon meticulous scrutiny of the experimental data encapsulated in Tables ",
                "5",
                " and ",
                "2",
                ", our Fed-RAA algorithm emerges as conspicuously advantageous in terms of time efficiency relative to conventional federated learning benchmarks. The empirical evidence illustrates that Fed-RAA considerably diminishes the wall-clock time to reach benchmark accuracies on the MNIST, CIFAR-10, and CIFAR-100 datasets. It notably completes training in 91.06 seconds for MNIST, 89.85 seconds for CIFAR-10, and 101.86 seconds for CIFAR-100, thereby outperforming the closest competitor, FedSync, by over a twofold improvement in speed.",
                "Although Fed-RAA displays slightly reduced accuracy in comparison to top-performing algorithms like RamFed, the marginal accuracy trade-off is counterbalanced by the dramatic reductions in training time, making Fed-RAA an appealing option in time-critical circumstances. This delicate balance of time and accuracy underscores Fed-RAA’s practical relevance for rapid model updates and accelerates deployment, a significant contribution to the domain of federated learning that warrants emphasis in our experimental analysis.",
                "It is noteworthy, however, that Fed-RAA’s accelerated processing did necessitate a higher number of epochs, with 98399 for MNIST, 98499 for CIFAR-10, and 98599 for CIFAR-100, surpassing the epoch requirements of other baselines. This observation, notwithstanding the reduced time per epoch, implies that Fed-RAA capitalizes on computational efficiency, permitting swift iterative processes even when individual epochs are of minimal computational cost. This attribute makes Fed-RAA particularly valuable in contexts where communication latency poses significant challenges, highlighting the algorithm’s capacity to enable rapid convergence and frequent model updates within distributed ML frameworks. Consequently, this dual analysis of time and epoch dynamics bolsters the case for Fed-RAA’s adoption in environments where both expedited learning and resource optimization are paramount.",
                "The experiment details shown in Figure ",
                "3",
                " demonstrate the efficacy of the Fed-RAA algorithm in federated learning environments, particularly under asynchronous operations. In the MNIST and CIFAR-10 datasets, Fed-RAA shows significant advantages in balancing accuracy and computational time, outperforming other algorithms such as Fed-Avg, Fed-Sync, and Fed-Prox, which either sacrifice accuracy for lower time costs or vice versa. For the more complex CIFAR-100 dataset, Fed-RAA remarkably excels by achieving higher accuracies more rapidly compared to its counterparts, which face sharp increases in computational time with minimal accuracy gains. This consistent performance underscores the benefits of asynchronous learning and training submodels, suggesting that Fed-RAA’s approach to handling partial model training can substantially enhance the efficiency and effectiveness of federated learning systems."
            ]
        ]
    }
}