{
    "id_table_1": {
        "caption": "Table 1.  The main results of our designed system evaluated in the public test dataset.",
        "table": "S4.T1.3.3",
        "footnotes": [],
        "references": [
            "In this paper, we introduce a novel Retrieval-Augmented Generation (RAG) system designed for real-world applications. Our system was evaluated on the CRAG benchmark  (Yang et al . ,  2024a ) , achieving the third position in Task 1 and securing first place for  5 5 5 5  out of  7 7 7 7  question types in Task 2. The rest of the paper is structured as follows. In Section  2 , we provide an overview of related work in the context of RAG system design. We introduce the CRAG benchmark in Section  3 . Our RAG system design is detailed in Section  4  and the complete system architecture is illustrated in Figure  1 . The performance of our system is reported and discussed in Section  5 . Finally, we conclude the paper in Section  6 , including a  Discussion Section  of potential improvements for future iterations of our system.",
            "The complete design of our system is shown in Figure  1 . There are  6 6 6 6  critical modules in our system, including (1) web page processing, (2) attribute predictor, (3) numerical calculator, (4) LLM knowledge extractor, (5) KG Module, and (6) reasoning module. We have enhanced the systems capabilities in information extraction, reducing hallucinations, numerical calculation accuracy, and higher-order reasoning through these modules. Additionally, we have implemented special handling for corner cases. We will introduce these modules as follows.",
            "In-Context Learning.  Large language models demonstrate robust natural language understanding and strong multi-task generalization abilities.  We prompt the model with classification instructions and  5 5 5 5  demonstrations for categories, instructing it to classify subsequent questions.  The demonstrations are randomly selected from the publicly available CRAG dataset.  All prompts utilized in the classification process can be found in Appendix  D.1 .",
            "To enhance classification reliability, we adopted a self-consistency strategy involving multiple samplings from the large language model. The category that appeared most frequently among the sampled results was designated as the classification for the question.   SVM.  We also tried to train an SVM classifier using the CRAG public dataset to reduce computational overhead. We used the  all-MiniLM-L6-v2  model to get the sentence embeddings, which are used to train the SVM.  We observed that the SVM model can get higher accuracy in predicting the attributes with less time and computation consumption.  However, we didnt have time to merge the code into our final version for evaluation.  So, consequently, the submitted version relied on the few-shot learning approach with the large language model for classification, and we leave the improvement for this module as future work. We will show the detailed results analysis and comparison in Section  5.1 .",
            "We conducted ample experiments to verify the effectiveness of each module. The main results of our local evaluation are shown in Table  1 , where we got a lot of improvement in the public test set compared with the baseline of Task 1. We got a  15.8 % percent 15.8 15.8\\% 15.8 %  score where we greatly reduced the hallucination ratio and changed these hallucinations into answering I dont know. We show detailed analysis and ablation studies of our evaluation results. In the final private test, we got a  21.8 % percent 21.8 21.8\\% 21.8 %  score in Task 1. We will also show an analysis of the private evaluation. We will use Official RAG Baseline as the baseline model of Task 1 in the following tables."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.  Ablation results of our system. The modules are gradually added to the system. The results are tested in Task 1 and Task 2, which are only different in using KG.",
        "table": "S5.T2.3.3",
        "footnotes": [],
        "references": [
            "In this paper, we introduce a novel Retrieval-Augmented Generation (RAG) system designed for real-world applications. Our system was evaluated on the CRAG benchmark  (Yang et al . ,  2024a ) , achieving the third position in Task 1 and securing first place for  5 5 5 5  out of  7 7 7 7  question types in Task 2. The rest of the paper is structured as follows. In Section  2 , we provide an overview of related work in the context of RAG system design. We introduce the CRAG benchmark in Section  3 . Our RAG system design is detailed in Section  4  and the complete system architecture is illustrated in Figure  1 . The performance of our system is reported and discussed in Section  5 . Finally, we conclude the paper in Section  6 , including a  Discussion Section  of potential improvements for future iterations of our system.",
            "To address this issue, we employed an approach that leverages external tools, drawing inspiration from previous research  (Schick et al . ,  2023 ; Paranjape et al . ,  2023 ; Yuan et al . ,  2024 ) .  We encourage the large language model to articulate the reasoning steps necessary to solve the problem as mathematical expressions while delegating the actual numerical calculations to an external Python interpreter.  We specifically integrate retrieved text chunks and tables that may contain numerical information into the models prompts and employ prompt techniques that encourage the model to generate valid Python expressions directly.  Detailed specifications of the prompts are provided in the Appendix  D.2   In the submitted version, we utilized multiple sampling and processed the generated Python expressions using the  eval  function.",
            "We performed extensive experiments to validate the enhancements contributed by individual components of our system. We developed the system incrementally, systematically verifying and integrating beneficial modules. Consequently, comprehensive ablation studies, which would involve removing each component from the final system, were not conducted. Instead, we documented the rationale behind the inclusion of each module and its resulting improvements. Table  2  outlines the primary construction pathway of our system.",
            "We started from the baseline model of Task 1 1 1 1 The model can be found  here .  and did a lot of refinement and added modules to it. As shown in Table  2 , each module we added would increase the final score. The main optimization directions are reducing hallucinations and increasing correctness."
        ]
    },
    "id_table_3": {
        "caption": "Table 3.  The evaluation results in the private test set.",
        "table": "S5.T2.3.3.6.3.1.1",
        "footnotes": [],
        "references": [
            "In this paper, we introduce a novel Retrieval-Augmented Generation (RAG) system designed for real-world applications. Our system was evaluated on the CRAG benchmark  (Yang et al . ,  2024a ) , achieving the third position in Task 1 and securing first place for  5 5 5 5  out of  7 7 7 7  question types in Task 2. The rest of the paper is structured as follows. In Section  2 , we provide an overview of related work in the context of RAG system design. We introduce the CRAG benchmark in Section  3 . Our RAG system design is detailed in Section  4  and the complete system architecture is illustrated in Figure  1 . The performance of our system is reported and discussed in Section  5 . Finally, we conclude the paper in Section  6 , including a  Discussion Section  of potential improvements for future iterations of our system.",
            "The process of extracting knowledge from the model closely resembles the normal model generation process.  It similarly utilizes zero-shot indications, which include prompts requiring the model to assess whether a given query pertains to a false-premise issue and to generate more concise responses.  However, a notable distinction exists in the lack of reference documents sourced from external knowledge bases within the prompts, as well as the exclusion of multiple sampling, which is intended to reduce computational overhead.  In this way, the LLM could respond solely based on the knowledge internalized within its parameters during the training.  Our findings suggest that this approach results in favorable performance on questions classified as slow-changing and stable.  We anticipate that this approach will effectively align the knowledge embedded in the LLMs parameters with external reference documents, thereby mitigating the issue of the models excessive dependence on externally retrieved information. Moreover, we also use the zero-shot CoT to let the model reasoning by itself for more accurate knowledge. The prompt template is shown in Appendix  D.3",
            "To gain a deeper understanding of the strengths and weaknesses of our system across various aspects, we conducted a meticulous analysis of the evaluation results on the public test set. Figure  3  shows the detailed scores in Task 1 setting.",
            "Although the competition organizers have not provided a comprehensive and detailed analysis of the results on the private leaderboard, we can still present the currently published results and analyze our system. Table  3  shows the score for Task 1 in private evaluation. Our system achieved scores close to the champion in Task 1 but fell significantly behind in Task 2 and Task 3. We believe this is due to our underutilization of the information from the knowledge graph. Table  4  shows the prizes we won from 5 out of 7 question types in Task 2. We find that our system performs well on question types that require complex reasoning, such as aggregation and multi-hop, which we attribute to our reasoning module."
        ]
    },
    "id_table_4": {
        "caption": "Table 4.  The scores of the prizes we won in Task 2.",
        "table": "S5.T2.3.3.13.10.1.1",
        "footnotes": [],
        "references": [
            "In this paper, we introduce a novel Retrieval-Augmented Generation (RAG) system designed for real-world applications. Our system was evaluated on the CRAG benchmark  (Yang et al . ,  2024a ) , achieving the third position in Task 1 and securing first place for  5 5 5 5  out of  7 7 7 7  question types in Task 2. The rest of the paper is structured as follows. In Section  2 , we provide an overview of related work in the context of RAG system design. We introduce the CRAG benchmark in Section  3 . Our RAG system design is detailed in Section  4  and the complete system architecture is illustrated in Figure  1 . The performance of our system is reported and discussed in Section  5 . Finally, we conclude the paper in Section  6 , including a  Discussion Section  of potential improvements for future iterations of our system.",
            "However, as described previously, letting the model directly answer the questions will introduce hallucinations in its knowledge, although with zero-shot CoT reasoning. To balance the hallucination and knowledge from the LLM itself, we only treat the output of this module as one of the references. We have carefully designed prompts to ensure that the model neither overly relies on document references nor excessively trusts the LLMs knowledge. Section  4.6  will provide a more detailed introduction.",
            "In addition to web references, Task 2 and Task 3 also provide a mock API for querying the provided knowledge graph (KG). As a structured knowledge base, a KG provides accurate information. However, the generation of a KG query is crucial to determining whether the system can retrieve the correct answer. We started from the KG baseline, which extracted the entities in the query with an LLM, and generated the query by manual rules. The quality of rule-based queries is limited by the complexity of the rules, and hard to scale. So we tried a function-calling method, which makes all the mock APIs as the input of the LLM, and lets it generate a proper function calling. However, due to limitations in time and resources, we were unable to optimize the models and prompts for the function-calling method, resulting in suboptimal performance. Therefore, we reverted to the KG baseline method and did not make further improvements in the submitted version. We show the prompts for the function-calling methods in Appendix  D.4 .",
            "Although the competition organizers have not provided a comprehensive and detailed analysis of the results on the private leaderboard, we can still present the currently published results and analyze our system. Table  3  shows the score for Task 1 in private evaluation. Our system achieved scores close to the champion in Task 1 but fell significantly behind in Task 2 and Task 3. We believe this is due to our underutilization of the information from the knowledge graph. Table  4  shows the prizes we won from 5 out of 7 question types in Task 2. We find that our system performs well on question types that require complex reasoning, such as aggregation and multi-hop, which we attribute to our reasoning module.",
            "Furthermore, Figure  4  illustrates the scores across various attributes, revealing that our focus has been on static and slow-changing questions, neglecting the challenging time-varying ones. This deficiency in handling dynamic questions has also led to suboptimal performance in financial question types. The score results align with those from our internal assessment; however, we encountered a higher number of incorrect responses in the movie domain and on simple question types. We hypothesize that this discrepancy may stem from variations in question distribution between our local evaluation and online test datasets. Additionally, our performance in popularity aligns well with the expectations set forth in the CRAG Benchmark  (Yang et al . ,  2024a ) ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5.  Hyper-parameters of our final system.",
        "table": "S5.T3.1.1",
        "footnotes": [],
        "references": [
            "In this paper, we introduce a novel Retrieval-Augmented Generation (RAG) system designed for real-world applications. Our system was evaluated on the CRAG benchmark  (Yang et al . ,  2024a ) , achieving the third position in Task 1 and securing first place for  5 5 5 5  out of  7 7 7 7  question types in Task 2. The rest of the paper is structured as follows. In Section  2 , we provide an overview of related work in the context of RAG system design. We introduce the CRAG benchmark in Section  3 . Our RAG system design is detailed in Section  4  and the complete system architecture is illustrated in Figure  1 . The performance of our system is reported and discussed in Section  5 . Finally, we conclude the paper in Section  6 , including a  Discussion Section  of potential improvements for future iterations of our system.",
            "To enhance classification reliability, we adopted a self-consistency strategy involving multiple samplings from the large language model. The category that appeared most frequently among the sampled results was designated as the classification for the question.   SVM.  We also tried to train an SVM classifier using the CRAG public dataset to reduce computational overhead. We used the  all-MiniLM-L6-v2  model to get the sentence embeddings, which are used to train the SVM.  We observed that the SVM model can get higher accuracy in predicting the attributes with less time and computation consumption.  However, we didnt have time to merge the code into our final version for evaluation.  So, consequently, the submitted version relied on the few-shot learning approach with the large language model for classification, and we leave the improvement for this module as future work. We will show the detailed results analysis and comparison in Section  5.1 .",
            "After all the previously introduced processing methods, we get text chunks, tables, triplets from KG, and knowledge from LLM weights as the references. We carefully designed a prompt template to let the LLM do reasoning from all these references and get the final answer. We control the reasoning process by output format demonstration and zero-shot CoT, which is useful for multi-hop questions. Leveraging the strong instruction-following capabilities of Llama3-70B-Instruct, weve successfully maintained steady progress in controlling reasoning tasks. We designed several rules to constrain the reasoning path and output format, including that the output should be precise, and guide the model reasoning by asking intermediate questions in the prompt. The complete prompt is shown in Appendix  D.5 .",
            "Invalid Questions.   There are some questions that have false premises, which means the query is contradictory to the fact. For these questions, the model should output invalid questions. To identify this type of question, the model needs to carefully analyze the references provided. We add special rules in the reasoning prompt shown in Appendix  D.5",
            "We list the hyper-parameters of our final system here in Table  5 , more details can be found in our  source code . Moreover, we enable all modules for the static questions."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "S5.T4.1.1",
        "footnotes": [],
        "references": [
            "In this paper, we introduce a novel Retrieval-Augmented Generation (RAG) system designed for real-world applications. Our system was evaluated on the CRAG benchmark  (Yang et al . ,  2024a ) , achieving the third position in Task 1 and securing first place for  5 5 5 5  out of  7 7 7 7  question types in Task 2. The rest of the paper is structured as follows. In Section  2 , we provide an overview of related work in the context of RAG system design. We introduce the CRAG benchmark in Section  3 . Our RAG system design is detailed in Section  4  and the complete system architecture is illustrated in Figure  1 . The performance of our system is reported and discussed in Section  5 . Finally, we conclude the paper in Section  6 , including a  Discussion Section  of potential improvements for future iterations of our system.",
            "However, as described previously, letting the model directly answer the questions will introduce hallucinations in its knowledge, although with zero-shot CoT reasoning. To balance the hallucination and knowledge from the LLM itself, we only treat the output of this module as one of the references. We have carefully designed prompts to ensure that the model neither overly relies on document references nor excessively trusts the LLMs knowledge. Section  4.6  will provide a more detailed introduction.",
            "Incorrect Format.   Cause we didnt conduct constrained sampling for the reasoning output, there is the possibility that the model will output answers that can not be parsed. To handle this situation, we design a backup summarization agent to summarize the final answer precisely and concisely based on the reasoning modules output when the parse fails.  The prompt for this module is shown in Appendix  D.6"
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "A1.T5.3",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": [
        "The model can be found",
        "."
    ]
}