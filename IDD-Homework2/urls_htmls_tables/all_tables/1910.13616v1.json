{
    "S5.T1": {
        "caption": "Table 1: Mean square error (MSE) on the multimodal 5-shot regression with 2, 3, and 5 modes. A Gaussian noise with Œº=0ùúá0\\mu=0 and œÉ=0.3ùúé0.3\\sigma=0.3 is applied. Multi-MAML uses ground-truth task modes to select the corresponding MAML model. Our method (with FiLM modulation) outperforms other methods by a margin.",
        "table": null,
        "footnotes": [],
        "references": [
            "Results. The quantitative results are shown in Table 1. We observe that MAML has the highest error in all settings and that incorporating task identity (Multi-MAML) can improve over MAML significantly. This suggests that MAML degenerates under multimodal task distributions. The LSTM learner outperforms both MAML and Multi-MAML, showing that the sequence model can effectively tackle this regression task. MMAML improves over the LSTM learner significantly, which indicates that with a better initialization (produced by the modulation network), gradient-based optimization can lead to superior performance. Finally, since FiLM outperforms Softmax consistently in the regression experiments, we use it for as the modulation method in the rest of experiments."
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Classification testing accuracies on the multimodal few-shot image classification with 2, 3, and 5 modes.\nMulti-MAML uses ground-truth dataset labels to select corresponding MAML models. Our method outperforms MAML and achieve comparable results with Multi-MAML in all the scenarios.",
        "table": null,
        "footnotes": [],
        "references": [
            "Results. The results are shown in Table 2, demonstrating that our method achieves better results against MAML and performs comparably to Multi-MAML.\nThe performance gap between ours and MAML becomes larger when the number of modes increases, suggesting our method can handle multimodal task distributions better than MAML.\nAlso, compared to Multi-MAML, our method achieves slightly better results partially because our method learns from all the datasets yet each Multi-MAML is likely to overfit to a single dataset with a smaller number of classes (e.g.¬†Mini-ImageNet and FC100).\nThis finding aligns with the current trend of meta-learning from multiple datasets¬†[53].\nThe detailed performance on each dataset can be found in the supplementary material."
        ]
    },
    "S5.T3": {
        "caption": "Table 3: The mean and standard deviation of cumulative reward per episode for multimodal reinforcement learning problems with 2, 4 and 6 modes reported across 3 random seeds. Multi-ProMP is ProMP trained on an easier task distribution which consists of a single mode of the multimodal distribution to provide an approximate upper limit on the performance on each task.",
        "table": null,
        "footnotes": [],
        "references": [
            "Results. The results for the meta-RL experiments presented in Table 3 show that MMAML consistently outperforms the unmodulated ProMP. The good performance of Multi-ProMP, which only considers a single mode suggests that the difficulty of adaptation in our environments results mainly from the multiple modes. We find that the difficulty of the RL tasks does not scale directly with the number of modes, i.e.¬†the performance gap between MMAML and ProMP for Point Mass with 6 modes is smaller than the gap between them for 4 modes. We hypothesize the more distinct the different modes of the task distribution are, the more difficult it is for a single policy initialization to master. Therefore, adding intermediate modes (going from 4 to 6 modes) can in some cases make the task distribution easier to learn."
        ]
    },
    "A4.T5": {
        "caption": "Table 5: Dataset details.",
        "table": null,
        "footnotes": [],
        "references": [
            "To create a meta-dataset by merging multiple datasets, we utilize five popular datasets: Omniglot, Mini-ImageNet, FC100, CUB, and Aircraft.\nThe detailed information of all the datasets are summarized in Table 5.\nTo fit the images from all the datasets to a model, we resize all the images to 84√ó84848484\\times 84.\nThe images randomly sampled from all the datasets are shown in Figure 9, demonstrating a diverse set of modes."
        ]
    },
    "A4.T6": {
        "caption": "Table 6: Hyperparameters for multimodal few-shot image classification experiments.\nWe experiment different hyperparameters for each dataset for Multi-MAML. The dataset group Grayscale includes Omniglot and RGB includes Mini-ImageNet and FC100, CUB, and Aircraft.",
        "table": null,
        "footnotes": [],
        "references": [
            "We present the hyperparameters for all the experiments in Table 6.\nWe use the same set of hyperparameters to train our model and MAML for all experiments,\nexcept that we use a smaller meta batch-size for 20-way tasks and train the jobs for more iterations due to the limited memory of GPUs that we have access to."
        ]
    },
    "A4.T7": {
        "caption": "Table 7: The performance (classification accuracy) on the multimodal few-shot image classification with 2 modes on each dataset.",
        "table": null,
        "footnotes": [],
        "references": [
            "We provide the detailed performance of our method and the baselines on each individual dataset for all 2, 3, and 5 mode experiments, shown in Table 7, Table 8, and Table 9, respectively.\nNote that the main paper presents the overall performance (the last columns of each table) on each of 2, 3, and 5 mode experiments."
        ]
    },
    "A4.T8": {
        "caption": "Table 8: The performance (classification accuracy) on the multimodal few-shot image classification with 3 modes on each dataset.",
        "table": null,
        "footnotes": [],
        "references": [
            "We provide the detailed performance of our method and the baselines on each individual dataset for all 2, 3, and 5 mode experiments, shown in Table 7, Table 8, and Table 9, respectively.\nNote that the main paper presents the overall performance (the last columns of each table) on each of 2, 3, and 5 mode experiments."
        ]
    },
    "A4.T9": {
        "caption": "Table 9: The performance (classification accuracy) on the multimodal few-shot image classification with 5 modes on each dataset.",
        "table": null,
        "footnotes": [],
        "references": [
            "We provide the detailed performance of our method and the baselines on each individual dataset for all 2, 3, and 5 mode experiments, shown in Table 7, Table 8, and Table 9, respectively.\nNote that the main paper presents the overall performance (the last columns of each table) on each of 2, 3, and 5 mode experiments."
        ]
    },
    "A4.T10": {
        "caption": "Table 10: Hyperparameter settings for reinforcement learning.",
        "table": null,
        "footnotes": [],
        "references": [
            "We sample 40 tasks for each update step. For each gradient step for each task we sample 20 trajectories. The hyperparameters, which differ from setting to setting are presented in Table 10."
        ]
    }
}