{
    "id_table_1": {
        "caption": "Table 1:  Results for Coinrun 500k dataset. Methods trained for 3 days on a single A100. Shading indicates method requires access to the model parameters. Brackets indicate trainable parameters.",
        "table": "A2.EGx1",
        "footnotes": [],
        "references": [
            "We evaluate AVID in two different domains with different pretrained base models. Details about the datasets are provided in Appendix  B.1  and details about the pretrained models are in Appendix  B.2 . As the focus of our work is on training lightweight adapters with limited compute, we compare adapter models with limited parameters under a computational budget. Code to reproduce our experiments is available at  https://github.com/microsoft/causica/tree/main/research_experiments/avid .",
            "The values for every evaluation metric are reported in Tables  1  and  2 . For Coinrun500k, AVID performs the best for every evaluation metric at the smaller 22M model size. For the larger model size of 71M, AVID performs the second best for most metrics to ControlNet, but obtains the best performance for Action Error Ratio. In RT1, AVID is the strongest in the metrics that make framewise comparisons (SSIM/LPIPS/PSNR) across all model sizes. In our setting, where the goal is to generate accurate videos according to the input action sequence, these metrics are more suitable than comparing the overall distribution of images and videos (i.e. FID and FVD)  (Zhu et al.,  2024a ) . ControlNet Small generally obtains the best performance for FVD and FID, while Action-Conditioned Diffusion is consistently the best for Action Error Ratio. Poor performance was obtained for Classifier Guidance and Action Classifier-Free Guidance across both domains. For standard implementations of classifier(-free) guidance, the unconditional model is trained on the trained on data from the target domain. In contrast, in our setting the pretrained models are not trained on data from Coinrun or RT1 which may explain the lackluster performance of these methods. Product of Experts also performed poorly (PoE). However, for PSNR and SSIM, PoE did slightly outperform both of the models from which it is composed.",
            "The pretrained model is DynamiCrafter  (Xing et al.,  2023 ) , a latent video diffusion model. DynamiCrafter is trained to generate videos at a resolution of  320  512 320 512 320\\times 512 320  512 . To accommodate this, we resize and pad the images from the RT1 dataset to this resolution. DynamiCrafter is trained to optionally accept language conditoning. We use an empty language prompt, except in the case where the model is finetuned with language conditioning (see Section  4.1 ). DynamiCrafter uses the 1.4B 3D UNet architecture from  (Chen et al.,  2023a )  and the autoencoder from Stable Diffusion  (Rombach et al.,  2022 ) .",
            "The minimum and maximum values used for normalization are summarised in Tables  10  and  11 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Quantitative results for RT1 dataset. Methods trained for 7 days on  4  4\\times 4   A100. Shading indicates method requires access to the model parameters. Brackets indicate trainable parameters.",
        "table": "A2.EGx2",
        "footnotes": [],
        "references": [
            "We evaluate AVID in two different domains with different pretrained base models. Details about the datasets are provided in Appendix  B.1  and details about the pretrained models are in Appendix  B.2 . As the focus of our work is on training lightweight adapters with limited compute, we compare adapter models with limited parameters under a computational budget. Code to reproduce our experiments is available at  https://github.com/microsoft/causica/tree/main/research_experiments/avid .",
            "Qualitative Results    Examples of generated videos are in Figures  2  and  3 , with further examples in Appendix  A.5 . We observe for both RT1 and Coinrun that the action-conditioned diffusion model trained from scratch fails to maintain consistency with the original conditioning image: objects in the initial image disappear in later frames in the video. In contrast, the videos generated by AVID are consistent throughout. In both AVID and action-conditioned diffusion, we observe that the motion in the generated videos is accurate compared to the ground truth motion. The pretrained base models do not generate accurate videos for either domain (Appendix  A.5 ). The masks generated by AVID are visualised in Figures  2  and  3 . The lighter parts of the mask show that the pretrained models predictions are predominantly used by AVID for maintaining background textures. The mask is reduced nearer to 0 around the robot arm in RT1 and the character in Coinrun, showing that the adapter outputs are predominantly used for action-relevant parts of the video. In Appendix  A.6  we show that AVID can be used to generate predictions for different actions given the same initial frame.",
            "The values for every evaluation metric are reported in Tables  1  and  2 . For Coinrun500k, AVID performs the best for every evaluation metric at the smaller 22M model size. For the larger model size of 71M, AVID performs the second best for most metrics to ControlNet, but obtains the best performance for Action Error Ratio. In RT1, AVID is the strongest in the metrics that make framewise comparisons (SSIM/LPIPS/PSNR) across all model sizes. In our setting, where the goal is to generate accurate videos according to the input action sequence, these metrics are more suitable than comparing the overall distribution of images and videos (i.e. FID and FVD)  (Zhu et al.,  2024a ) . ControlNet Small generally obtains the best performance for FVD and FID, while Action-Conditioned Diffusion is consistently the best for Action Error Ratio. Poor performance was obtained for Classifier Guidance and Action Classifier-Free Guidance across both domains. For standard implementations of classifier(-free) guidance, the unconditional model is trained on the trained on data from the target domain. In contrast, in our setting the pretrained models are not trained on data from Coinrun or RT1 which may explain the lackluster performance of these methods. Product of Experts also performed poorly (PoE). However, for PSNR and SSIM, PoE did slightly outperform both of the models from which it is composed."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Results for AVID ablations (extended in Table  6 , Appendix  A.2 ).",
        "table": "A2.EGx3",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "In this benchmark, the pretrained model is DynamiCrafter  (Xing et al.,  2023 )  which is currently one of the best performing image-to-video models in the VBench image-to-video leaderboard  (Huang et al.,  2024 ) . DynamiCrafter is a latent image-to-video diffusion model that uses the autoencoder from Stable Diffusion  (Rombach et al.,  2022 )  and a 1.4B parameter 3D UNet trained on web-scale video data. As DynamiCrafter is a latent diffusion model, we assume that we can run inference on the encoder and decoder as described in Section  3.3 . For the action-labelled dataset we use the RT1 dataset  (Brohan et al.,  2022 )  which consists of real-world robotics videos. The action at each step is a 7 dimensional continuous vector corresponding to the movement and rotation of the end effector and opening or closing of the gripper. The models are trained on trajectories of 16 frames. Each adaptation approach is limited to 7 days of training on  4  4\\times 4   A100 GPUs, and is evaluated using a held-out test set of ground truth trajectories.",
            "Action-Conditioned Finetuning   tunes all of the parameters of the pretrained model on the action-conditioned dataset. To add action-conditioning, we first compute an action embedding according to Section  3.3 . For Procgen and RT1, we concatenate and add the action embeddings with the time step embeddings respectively.",
            "Action Error Ratio   To assess the consistency between the videos and the action sequences, we train a model to predict actions from real videos. The Action Error Ratio is the ratio of errors obtained by using this model on generated videos, divided by the error obtained on real videos. More details are in Appendix  B.3 .",
            "Qualitative Results    Examples of generated videos are in Figures  2  and  3 , with further examples in Appendix  A.5 . We observe for both RT1 and Coinrun that the action-conditioned diffusion model trained from scratch fails to maintain consistency with the original conditioning image: objects in the initial image disappear in later frames in the video. In contrast, the videos generated by AVID are consistent throughout. In both AVID and action-conditioned diffusion, we observe that the motion in the generated videos is accurate compared to the ground truth motion. The pretrained base models do not generate accurate videos for either domain (Appendix  A.5 ). The masks generated by AVID are visualised in Figures  2  and  3 . The lighter parts of the mask show that the pretrained models predictions are predominantly used by AVID for maintaining background textures. The mask is reduced nearer to 0 around the robot arm in RT1 and the character in Coinrun, showing that the adapter outputs are predominantly used for action-relevant parts of the video. In Appendix  A.6  we show that AVID can be used to generate predictions for different actions given the same initial frame.",
            "AVID Ablations    We evaluate the following two ablations of AVID:  No Mask  (NM): The output adapter does not output a mask. Instead, the outputs of the pretrained model and adapter are directly added:   final =  pre +  adapt subscript italic- final subscript italic- pre subscript italic- adapt \\epsilon_{\\textnormal{final}}=\\epsilon_{\\textnormal{pre}}+\\epsilon_{% \\textnormal{adapt}} italic_ start_POSTSUBSCRIPT final end_POSTSUBSCRIPT = italic_ start_POSTSUBSCRIPT pre end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT adapt end_POSTSUBSCRIPT .  No Conditioning  (NC): The adapter is no longer conditioned on the output of the pretrained model,   pre subscript italic- pre \\epsilon_{\\textnormal{pre}} italic_ start_POSTSUBSCRIPT pre end_POSTSUBSCRIPT . Results for the ablations are in Table  3 . For NM, performance across most metrics is worse for RT1, but similar for Coinrun500k. For NC, the performance on most metrics gets significantly worse for both Coinrun500k and RT1. Thus, conditioning on the outputs of the pretrained model is the most crucial component of our approach.",
            "As discussed in Section  3.3 , we use the same autoencoder for our AVID adapter, as well as all baselines. For all methods on RT1, we train a 3D UNet with the same architecture as Dynamicrafter from  (Chen et al.,  2023a ) , but with a reduced number of parameters. The hyperparameters for each of the models can be found in Table  9 ."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Quantitative results for Coinrun 100k dataset. Shaded rows indicate that the method requires access to the model parameters.",
        "table": "A2.EGx4",
        "footnotes": [],
        "references": [
            "As a result, the true noise prediction of the target distribution also cannot be expressed as a sum:   PoE  ( x t , t , c ) =  pre  ( x t , t , c ) +  adapt  ( x t , t , c ) subscript italic- PoE subscript x t t c subscript italic- pre subscript x t t c subscript italic- adapt subscript x t t c \\mathbf{\\epsilon}_{\\text{PoE}}(\\mathbf{x}_{t},t,c)\\neq\\mathbf{\\epsilon}_{\\text% {pre}}(\\mathbf{x}_{t},t,c)+\\mathbf{\\epsilon}_{\\text{adapt}}(\\mathbf{x}_{t},t,c) italic_ start_POSTSUBSCRIPT PoE end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t , italic_c ) = italic_ start_POSTSUBSCRIPT pre end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t , italic_c ) + italic_ start_POSTSUBSCRIPT adapt end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t , italic_c ) . Therefore, the composition in  Equation   4  does not hold and will result in biased samples. In the following section, we propose AVID to overcome this limitation. Rather than attempting to compose two independently trained models, AVID uses the outputs of the pretrained model to train an adapter that directly optimizes the denoising loss.",
            "We bold results within 2% of the best performance in each model size category. We also compute normalized metrics by normalizing each value between 0 and 1. Details are in Appendix  B.4 .",
            "Quantitative Results    To evaluate overall performance, in Figures  4(a)  and  4(b)  we plot the normalized performance averaged across all evaluation metrics. AVID obtains similar or slightly better overall performance compared to ControlNet/ControlNet Small on both Coinrun500k and RT1. Note that unlike ControlNet variants, AVID does not require access to the weights of the original pretrained model. For RT1, we observe that training an action-conditioned diffusion model performs slightly worse than AVID at the largest model size. For Coinrun500k, AVID significantly outperforms action-conditioned diffusion at the larger model size. As the number of trainable parameters is reduced, the performance of action-conditioned diffusion declines much more quickly than AVID in both domains, and therefore AVID is considerably stronger at smaller model sizes. In Figure  4(c)  we plot the overall performance of these three approaches against the Coinrun dataset size. A similar trend is observed for all approaches as dataset size is modified.",
            "In Figure  4(d)  we plot the average mask value at each step of the diffusion process. On RT1 AVID has a higher mask value, and therefore uses the pretrained model more heavily than on Coinrun. This is likely because the backgrounds in RT1 are mostly static and DynamiCrafter is a strong model. We see that the mask values are lower at diffusion steps where the noise level is high. This indicates that the adapter model is more responsible for generating low-frequency information, such as the positions of objects, which is defined early in the reverse process. Towards the end of the reverse process, where fine details are generated  (Ho et al.,  2020 ) , the pretrained model is relied on more.",
            "Tables  4  and  5  contain results for Coinrun datasets of different sizes (100k and 2.5M). The results in the main paper use a dataset size of 500k.",
            "The pretrained model is DynamiCrafter  (Xing et al.,  2023 ) , a latent video diffusion model. DynamiCrafter is trained to generate videos at a resolution of  320  512 320 512 320\\times 512 320  512 . To accommodate this, we resize and pad the images from the RT1 dataset to this resolution. DynamiCrafter is trained to optionally accept language conditoning. We use an empty language prompt, except in the case where the model is finetuned with language conditioning (see Section  4.1 ). DynamiCrafter uses the 1.4B 3D UNet architecture from  (Chen et al.,  2023a )  and the autoencoder from Stable Diffusion  (Rombach et al.,  2022 ) .",
            "To compute normalized evaluation metrics plotted in Figures  4(a) ,  4(b)  and  4(c) , we first normalize each evaluation metric to between 0 and 1. In RT1, 0 represents the worst performance for each metric across the Small, Medium or Large model sizes for Action-Conditioned Diffusion, ControlNet-Small, or AVID. 1 represents the best performance for each metric across these models. In Coinrun, 0 and 1 represent the worst and best performance for each metric across the Medium or Large model sizes for Action-Conditioned Diffusion, ControlNet/ControlNet-Small, or AVID, across all of the three datasets: Coinrun100k, Coinrun500k, and Coinrun2.5M.",
            "Once the metrics have been normalized, we compute the mean across all 6 normalized metrics. This final value is plotted in Figures  4(a) ,  4(b)  and  4(c) ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Quantitative results for Coinrun 2.5M dataset. Shaded rows indicate that the method requires access to the model parameters.",
        "table": "A2.EGx5",
        "footnotes": [],
        "references": [
            "We compare AVID with several baselines, both with and without access to the parameters of the pretrained model. Further details, including hyperparameter tuning, are in Appendix  B.5 .",
            "Qualitative Results    Examples of generated videos are in Figures  2  and  3 , with further examples in Appendix  A.5 . We observe for both RT1 and Coinrun that the action-conditioned diffusion model trained from scratch fails to maintain consistency with the original conditioning image: objects in the initial image disappear in later frames in the video. In contrast, the videos generated by AVID are consistent throughout. In both AVID and action-conditioned diffusion, we observe that the motion in the generated videos is accurate compared to the ground truth motion. The pretrained base models do not generate accurate videos for either domain (Appendix  A.5 ). The masks generated by AVID are visualised in Figures  2  and  3 . The lighter parts of the mask show that the pretrained models predictions are predominantly used by AVID for maintaining background textures. The mask is reduced nearer to 0 around the robot arm in RT1 and the character in Coinrun, showing that the adapter outputs are predominantly used for action-relevant parts of the video. In Appendix  A.6  we show that AVID can be used to generate predictions for different actions given the same initial frame.",
            "Tables  4  and  5  contain results for Coinrun datasets of different sizes (100k and 2.5M). The results in the main paper use a dataset size of 500k.",
            "Figures  5  and  6  illustrate examples of the mask generated by AVID which is used to mix the pretrained model and adapter outputs."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Full results for AVID ablations.",
        "table": "S4.T1.6",
        "footnotes": [],
        "references": [
            "In the case where   pre subscript italic- pre \\epsilon_{\\textnormal{pre}} italic_ start_POSTSUBSCRIPT pre end_POSTSUBSCRIPT  is a latent diffusion model, we assume that we can also run inference on the corresponding encoder and decoder. For each training example, we first encode the video:  z = enc  ( x ) z enc x \\mathbf{z}=\\mathtt{enc}(\\mathbf{x}) bold_z = typewriter_enc ( bold_x )  where  z  R T  h   w   c  z superscript R T superscript h  superscript w  superscript c  \\mathbf{z}\\in\\mathbb{R}^{T\\times h^{\\prime}\\times w^{\\prime}\\times c^{\\prime}} bold_z  blackboard_R start_POSTSUPERSCRIPT italic_T  italic_h start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  italic_w start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  italic_c start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT , and add noise to this latent representation of the video to produce noisy latent  z i subscript z i \\mathbf{z}_{i} bold_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . The rest of the pipeline proceeds in the same manner, except that the initial image  x 0 superscript x 0 x^{0} italic_x start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT  is replaced by the latent corresponding to the first frame,  z 0 superscript z 0 z^{0} italic_z start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT , and the noisy video  x i subscript x i \\mathbf{x}_{i} bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is replaced by the noisy latent  z i subscript z i \\mathbf{z}_{i} bold_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . The adapter loss in Equation  6  predicts the noise added to the latent sample, and we decode the sampled latent to generate the final video.",
            "Qualitative Results    Examples of generated videos are in Figures  2  and  3 , with further examples in Appendix  A.5 . We observe for both RT1 and Coinrun that the action-conditioned diffusion model trained from scratch fails to maintain consistency with the original conditioning image: objects in the initial image disappear in later frames in the video. In contrast, the videos generated by AVID are consistent throughout. In both AVID and action-conditioned diffusion, we observe that the motion in the generated videos is accurate compared to the ground truth motion. The pretrained base models do not generate accurate videos for either domain (Appendix  A.5 ). The masks generated by AVID are visualised in Figures  2  and  3 . The lighter parts of the mask show that the pretrained models predictions are predominantly used by AVID for maintaining background textures. The mask is reduced nearer to 0 around the robot arm in RT1 and the character in Coinrun, showing that the adapter outputs are predominantly used for action-relevant parts of the video. In Appendix  A.6  we show that AVID can be used to generate predictions for different actions given the same initial frame.",
            "Table  6  contains ablation results for AVID across the full range of model sizes.",
            "Figures  5  and  6  illustrate examples of the mask generated by AVID which is used to mix the pretrained model and adapter outputs."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Quantitative results for RT1 dataset with greater computational budget.",
        "table": "S4.T2.6",
        "footnotes": [],
        "references": [
            "The results in the main paper train models for RT1 using a compute limit of 7 days of 4  \\times   A100 GPUs. To provide reference values for performance we also evaluated IRASim  (Zhu et al.,  2024a )  using our evaluation setup. IRASim is a 679M parameter model trained using 100 GPU-days on A800 GPUs. We also include results for action-conditioned finetuning of DynamiCrafter using a similar amount of compute (104 GPU-days on A100 GPUs). The results for these two models are in Table  7 . By finetuning DynamiCrafter, we obtain slightly stronger performance than IRASim."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Hyperparameters for models trained on Procgen and Coinrun.",
        "table": "S4.T3.6",
        "footnotes": [],
        "references": [
            "For both the pretrained model trained on 15 of the 16 Procgen games, and the adapters trained on the Coinrun datasets, we use the 3D UNet architecture from Video Diffusion Models  (Ho et al.,  2022c )  trained on videos of resolution  64  64 64 64 64\\times 64 64  64 . We condition each model on two initial images to allow the initial direction the agent is moving in to be inferred and allow for more accurate video generation. Detailed hyperparameters for each of the models are in Table  8 ."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Hyperparameters for models trained on RT1.",
        "table": "A1.T4.6",
        "footnotes": [],
        "references": [
            "In Figure  9  we generate videos by providing the same initial conditioning frame but different actions. The provided action is fixed for all 16 steps of the video.",
            "As discussed in Section  3.3 , we use the same autoencoder for our AVID adapter, as well as all baselines. For all methods on RT1, we train a 3D UNet with the same architecture as Dynamicrafter from  (Chen et al.,  2023a ) , but with a reduced number of parameters. The hyperparameters for each of the models can be found in Table  9 ."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:  Minimum and maximum values for metric normalization in RT1.",
        "table": "A1.T5.6",
        "footnotes": [],
        "references": [
            "The minimum and maximum values used for normalization are summarised in Tables  10  and  11 ."
        ]
    },
    "id_table_11": {
        "caption": "Table 11:  Minimum and maximum values for metric normalization in Coinrun.",
        "table": "A1.T6.6",
        "footnotes": [],
        "references": [
            "The minimum and maximum values used for normalization are summarised in Tables  10  and  11 ."
        ]
    },
    "id_table_12": {
        "caption": "",
        "table": "A1.T7.6",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "",
        "table": "A2.T8.2",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "A2.T9.2",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "",
        "table": "A2.T10.1",
        "footnotes": [],
        "references": []
    },
    "id_table_16": {
        "caption": "",
        "table": "A2.T11.1",
        "footnotes": [],
        "references": []
    }
}