{
    "id_table_1": {
        "caption": "Table 1 :  A brief comparison of the data type and results from  [ 12 ,  30 ]  and our technique for watermarking generative tabular data. Although our method achieves high fidelity and robustness, it does not handle categorical data and suffer from high detection cost. Among these three methods, there is not a clear \"winner\" as each paper focused on improving a different property of the watermark.",
        "table": "S2.T1.3",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "In this paper, we focus on establishing a theoretical framework to watermark tabular datasets. Our approach leverages the overall structure of the feature space to form pairs of  ( k  e  y , v  a  l  u  e ) k e y v a l u e (key,value) ( italic_k italic_e italic_y , italic_v italic_a italic_l italic_u italic_e )  columns for a more fine-grained watermark embedding. The elements in the  k  e  y k e y key italic_k italic_e italic_y  columns are divided into consecutive intervals, whose centers determine the seed to generate random \"red\" and \"green\" intervals for the corresponding  v  a  l  u  e v a l u e value italic_v italic_a italic_l italic_u italic_e  columns. We embed the watermark into the data by promoting all elements in the  v  a  l  u  e v a l u e value italic_v italic_a italic_l italic_u italic_e  columns to fall into these green intervals. For an illustrative example of this watermarking process on a stylized  3  4 3 4 3\\times 4 3  4  dataset, see  Figure   1 . During the detection phase, we employ a statistical hypothesis test to examine characteristics of the empirical data distribution. Finally, we provide theoretical and empirical analysis of the watermarks robustness to various data manipulation techniques on both synthetic and real-world datasets. We briefly summarize our main contributions as follows:",
            "Inspired by the recent watermarking techniques in large language models  [ 15 ,  1 ,  18 ] , and in particular the last one,  He et al. [ 12 ]  and  Zheng et al. [ 30 ]  have made advances in watermarking generative tabular data by splitting the value range into red and green intervals.  He et al. [ 12 ]  proposed a watermarking scheme through data binning, where they ensure that all elements in the original data are close to a green interval. This embedding method, in conjunction with a statistical hypothesis-test for detection, allow the authors to protect the watermark from additive noise attack. However, the authors assumed that both the elements of the tabular dataset and the additive noise are sampled from a continuous distribution, which do not account for attacks such as feature selection or truncation.  Zheng et al. [ 30 ]  instead only embedded the watermark in the prediction target feature. While the authors showed that this watermarking technique can handle several attacks as well as categorical features, their result mostly focused on watermarking one feature using a random seed, which is often insufficient in practice. In contrast, our technique guarantees that half of the dataset are watermarked with the seed chosen based on the data in  k  e  y k e y key italic_k italic_e italic_y  columns. For a comparison of our method with these two works, see  Table   1 .",
            "In this section, we provide the details of the watermarking algorithm. At a high level, we embed the watermark into a tabular dataset  X X \\mathbf{X} bold_X  by leveraging the pairwise structure of its feature space. Particularly, we first partition the feature space into pairs of  ( k  e  y , v  a  l  u  e ) k e y v a l u e (key,value) ( italic_k italic_e italic_y , italic_v italic_a italic_l italic_u italic_e )  columns using a subroutine  PAIR PAIR \\mathrm{PAIR} roman_PAIR . Detail of this subroutine will be discussed in the later section. Then, we finely divide the range of elements in each  k  e  y k e y key italic_k italic_e italic_y  column into bins of size  1 / b 1 b \\nicefrac{{1}}{{b}} / start_ARG 1 end_ARG start_ARG italic_b end_ARG  to form  b b b italic_b  consecutive intervals. The center of the bins for each  k  e  y k e y key italic_k italic_e italic_y  column is then used to compute a hash, which becomes the seed for a random number generator. This random number generator then randomly generates red and green intervals for the corresponding  v  a  l  u  e v a l u e value italic_v italic_a italic_l italic_u italic_e  column, where each interval is of size  1 / b 1 b \\nicefrac{{1}}{{b}} / start_ARG 1 end_ARG start_ARG italic_b end_ARG . Finally, we embed the watermark in elements of this  v  a  l  u  e v a l u e value italic_v italic_a italic_l italic_u italic_e  column by ensuring that they are always in a nearby green interval with minimal distortion. This process is repeated until all  v  a  l  u  e v a l u e value italic_v italic_a italic_l italic_u italic_e  columns are watermarked. We formally describe our proposed watermarking method in  Algorithm   1  below.",
            "Equation   1  gives rise to a natural corollary that upper bounds the Wasserstein distance, i.e., the distance between the empirical distributions of  X X \\mathbf{X} bold_X  and  X w subscript X w \\mathbf{X}_{w} bold_X start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT . Together,  Equation   1  and  Corollary   4.2  show that in expectation, the watermarked dataset  X w subscript X w \\mathbf{X}_{w} bold_X start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT  is close to the original dataset  X X \\mathbf{X} bold_X . Thus, downstream tasks operated on  X w subscript X w \\mathbf{X}_{w} bold_X start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT  instead of  X X \\mathbf{X} bold_X  would only induce additional error in the order of  1 / b 1 b \\nicefrac{{1}}{{b}} / start_ARG 1 end_ARG start_ARG italic_b end_ARG  with high probability. We empirically show the impact of this additional error for several synthetic and real-world datasets in  Section   7 .",
            "In this section, we provide the detail of the watermark detection protocol. Informally, the detection protocol employs standard statistical measures to determine whether a dataset is watermarked with minimal knowledge assumptions. Particularly, we introduce the following lemma that shows, with increasing number of bins, the probability of any element in a  v  a  l  u  e v a l u e value italic_v italic_a italic_l italic_u italic_e  column belonging to a green interval approaches  1 / 2 1 2 \\nicefrac{{1}}{{2}} / start_ARG 1 end_ARG start_ARG 2 end_ARG . That is, without running the watermarking algorithm  1 , we have a baseline for the expected number of elements in green intervals for each  v  a  l  u  e v a l u e value italic_v italic_a italic_l italic_u italic_e  column.",
            "We formalize the process of detecting watermark through a hypothesis test. Intuitively, the result of  Lemma   5.1  implies that, for any  v  a  l  u  e v a l u e value italic_v italic_a italic_l italic_u italic_e  column, the probability of an element being in a green list interval is approximately  1 / 2 1 2 \\nicefrac{{1}}{{2}} / start_ARG 1 end_ARG start_ARG 2 end_ARG . While this convergence is agnostic to how the green intervals are chosen, it is non-trivial for a data-provider to detect the watermark due to the pairwise structure of  Algorithm   1 . Particularly, if the data-provider has knowledge of the  v  a  l  u  e v a l u e value italic_v italic_a italic_l italic_u italic_e  columns and the hash function, they still need to individually check which  k  e  y k e y key italic_k italic_e italic_y  column corresponds to the selected  v  a  l  u  e v a l u e value italic_v italic_a italic_l italic_u italic_e  column. In the worst case, all  n n n italic_n   k  e  y k e y key italic_k italic_e italic_y  columns must be checked for each  v  a  l  u  e v a l u e value italic_v italic_a italic_l italic_u italic_e  column before the data-provider can confidently claim that the dataset is not watermarked. With this knowledge, we formulate the hypothesis test as follows:",
            "Algorithm   1  takes a black-box pairing subroutine  PAIR PAIR \\mathrm{PAIR} roman_PAIR  as an input to determine the set of  ( k  e  y , v  a  l  u  e ) k e y v a l u e (key,value) ( italic_k italic_e italic_y , italic_v italic_a italic_l italic_u italic_e )  columns. In the following analysis, we consider two feature pairing schemes: (i)  uniform : features are paired uniformly at random, or (ii)  feature importance : features are paired according to the feature importance ordering, where features with similar importance are paired. Without loss of generality, we assume that the columns of the original dataset are ordered in descending order of feature importance. Note that this reordering of features does not affect the uniform pairing scheme and only serves to simplify notations in our analysis. Formally, given two columns  X i subscript X i \\mathbf{X}_{i} bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and  X j subscript X j \\mathbf{X}_{j} bold_X start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , we define the probability of  ( X i , X j ) subscript X i subscript X j (\\mathbf{X}_{i},\\mathbf{X}_{j}) ( bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_X start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT )  being a  ( k  e  y , v  a  l  u  e ) k e y v a l u e (key,value) ( italic_k italic_e italic_y , italic_v italic_a italic_l italic_u italic_e )  pair as proportional to the inverse of the distance between their indices.",
            "Given a watermarked dataset  X w subscript X w \\mathbf{X}_{w} bold_X start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT  and a data scientist attacking  X w subscript X w \\mathbf{X}_{w} bold_X start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT  with feature selection as in  6.1 . Then, the number of preserved column pairs under feature importance pairing is at least twice as many as that under uniformly random pairing.",
            "First, we determine how this truncation operation influence the distribution of watermarked elements in green intervals. When a watermarked element  x x x italic_x  in a  v  a  l  u  e v a l u e value italic_v italic_a italic_l italic_u italic_e  column is truncated to  x tr subscript x tr x_{\\mathrm{tr}} italic_x start_POSTSUBSCRIPT roman_tr end_POSTSUBSCRIPT , it can fall out of the original green interval if the bins  [ 0 , 1 / b ] ,  , [ b  1 / b , 1 ] 0 1 b  b 1 b 1 [0,\\nicefrac{{1}}{{b}}],\\cdots,[\\nicefrac{{b-1}}{{b}},1] [ 0 , / start_ARG 1 end_ARG start_ARG italic_b end_ARG ] ,  , [ / start_ARG italic_b - 1 end_ARG start_ARG italic_b end_ARG , 1 ]  and the hundredth grid points  { 0 , 0.01 , 0.02 ,  , 0.99 , 1 } 0 0.01 0.02  0.99 1 \\{0,0.01,0.02,\\cdots,0.99,1\\} { 0 , 0.01 , 0.02 ,  , 0.99 , 1 }  are not perfectly aligned. To illustrate this phenomenon, we presented stylized example where  Algorithm   1  uses  b = 150 b 150 b=150 italic_b = 150  bins for its watermarking procedure. Then, in the second bin  I 2 = [ 1 / 150 , 2 / 150 ] subscript I 2 1 150 2 150 I_{2}=[\\nicefrac{{1}}{{150}},\\nicefrac{{2}}{{150}}] italic_I start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = [ / start_ARG 1 end_ARG start_ARG 150 end_ARG , / start_ARG 2 end_ARG start_ARG 150 end_ARG ] , any element  x  [ 1 / 150 , 0.01 ) x 1 150 0.01 x\\in[\\nicefrac{{1}}{{150}},0.01) italic_x  [ / start_ARG 1 end_ARG start_ARG 150 end_ARG , 0.01 )  will be truncated to  x tr = 0.0  I 1 subscript x tr 0.0 subscript I 1 x_{\\mathrm{tr}}=0.0\\in I_{1} italic_x start_POSTSUBSCRIPT roman_tr end_POSTSUBSCRIPT = 0.0  italic_I start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . If  I 1 subscript I 1 I_{1} italic_I start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  is chosen to be a red interval by the random number generator in  Algorithm   1 , then the truncation operation has successfully moved elements out of the green intervals. In the following theorem, we show the probability of successful truncation attack as a function of the bin width.",
            "Intuitively, with larger bin size  1 / b 1 b \\nicefrac{{1}}{{b}} / start_ARG 1 end_ARG start_ARG italic_b end_ARG , the probability that our proposed watermarking scheme can withstand truncation attack increases as the truncated elements are more likely to fall into the same bins as the original elements. On the other hand, when the bins are more fine-grained, truncation would almost surely move the watermarked data outside of the original intervals. This result presents an interesting trade-off between choosing smaller bin width for higher fidelity (see  Equation   1 ) and bigger bin width for better robustness, which has not been studied in prior work on watermarking generative tabular data. With this insight, we can choose the bin width to be the same as the truncation grid size, i.e.,  1 / b = 1 / 10 p 1 b 1 superscript 10 p \\nicefrac{{1}}{{b}}=\\nicefrac{{1}}{{10^{p}}} / start_ARG 1 end_ARG start_ARG italic_b end_ARG = / start_ARG 1 end_ARG start_ARG 10 start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT end_ARG  or  b = 10 p b superscript 10 p b=10^{p} italic_b = 10 start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT  to ensure high fidelity and robustness.",
            "Now, we can look at the number of preserved pairs of columns under feature importance pairing. Note that we assume the feature selection is performed according to  6.1 , and the columns are in descending order of feature importance. Since we are only retaining the top  k k k italic_k  most important features after feature selection, this is equivalent to counting the number of preserved pairs among the first  k k k italic_k  features after reordering."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Accuracy of the downstream models under various attacks to the watermarked datasets. In particular, we provide accuracy for the original dataset and its watermarked counterpart. Additionally, we consider truncation as well as the column dropping separately that are typical preprocessing steps in a machine learning pipeline. We note that the effect of our watermarking technique is negligible in terms of accuracy in all cases while maintaining high detectability.",
        "table": "S2.T1.3.6.3.2.1",
        "footnotes": [],
        "references": [
            "Equation   1  gives rise to a natural corollary that upper bounds the Wasserstein distance, i.e., the distance between the empirical distributions of  X X \\mathbf{X} bold_X  and  X w subscript X w \\mathbf{X}_{w} bold_X start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT . Together,  Equation   1  and  Corollary   4.2  show that in expectation, the watermarked dataset  X w subscript X w \\mathbf{X}_{w} bold_X start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT  is close to the original dataset  X X \\mathbf{X} bold_X . Thus, downstream tasks operated on  X w subscript X w \\mathbf{X}_{w} bold_X start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT  instead of  X X \\mathbf{X} bold_X  would only induce additional error in the order of  1 / b 1 b \\nicefrac{{1}}{{b}} / start_ARG 1 end_ARG start_ARG italic_b end_ARG  with high probability. We empirically show the impact of this additional error for several synthetic and real-world datasets in  Section   7 .",
            "Theorem   6.2  implies that, under the feature importance pairing scheme, the truncated dataset would retain more valuable information for the downstream task. In  Section   7 , we empirically show how this theoretical guarantee translates to improved utility in the downstream task for various datasets.",
            "We generate a dataset of size  2000  2 2000 2 2000\\times 2 2000  2  using the standard Gaussian distribution. One column is designated as the seed column and the other is watermarked. Figures  2(a)  and  2(b)  contain KDE plots showing minimal difference between the distribution before and after watermarking.",
            "In the second experiment, we generate multiple datasets each containing 50 columns and varying numbers of rows from 20 to 100 in order to validate the effect that the row count has on the maximum achievable  z z z italic_z -score. We repeat this process 5 times and take the average  z z z italic_z -score. As shown in Figure  2(c) , we find that as the number of rows in the dataset increases, so does the maximum possible  z z z italic_z -score. This means that given a choice of  z z z italic_z -score threshold, i) there is an increasing minimum number of total rows that the dataset must contain to achieve that score and ii) as the number of rows increases, so does the number of rows that an attacker must sufficiently alter to break the watermark.",
            "Finally, we consider the  fidelity vs. robustness  trade off that the  bin size  parameter poses. We again generate a dataset of size  2000  2 2000 2 2000\\times 2 2000  2  using the standard Gaussian distribution. We watermark one column, using the other for seeding, and vary the  bin size  between  10  4 superscript 10 4 10^{-4} 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT  and  10  1 superscript 10 1 10^{-1} 10 start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT . The average mean squared error across 5 runs between the original data and the watermarked data for each  bin size  is shown in Figure  2(d) . As expected, greater fidelity is maintained with smaller bins since adjusting the data to fall into the nearest green bin requires smaller perturbations. Figure  2(e)  displays the accompanying susceptibility to noise that comes with smaller bins. Averaging across 5 runs, we add zero-mean Gaussian noise with standard deviation varying from  10  3 superscript 10 3 10^{-3} 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT  to  10  1 superscript 10 1 10^{-1} 10 start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT  to the watermarked data , and measure the effect on the  z z z italic_z -score. In each case, we find that smaller bins results in lower scores.",
            "For each of the watermarked datasets, we train a Random Forest classifier and use the resulting feature importances to drop subsets of columns varying in size from  20 % percent 20 20\\% 20 %  to  80 % percent 80 80\\% 80 % . The metric of interest in this experiment is the percentage of pairs retained after column dropping. Both columns in a pair must still remain in the dataset to be counted as a preserved pair. This entire process is repeated 5 times, and the averaged results are shown in Figure  2(f) . The results show there is a significant increase in preserved pairs when the pairs are created using the feature importance scheme.",
            "We find that the effect of our watermarking technique is negligible in terms of accuracy in all cases while maintaining high detectability as seen in Table  2 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Accuracy of the downstream models under various attacks to the watermarked datasets. In particular, we provide accuracy for the original dataset and its watermarked counterpart. Classifier used for utility evaluation is  XGBoost.  In addition to this, we add the standard deviation of each record.",
        "table": "A5.EGx1",
        "footnotes": [],
        "references": []
    },
    "id_table_4": {
        "caption": "Table 4 :  Accuracy of the downstream models under various attacks to the watermarked datasets. In particular, we provide accuracy for the original dataset and its watermarked counterpart. Classifier used for utility evaluation is  Random Forest.  In addition to this, we add the standard deviation of each record.",
        "table": "S7.T2.2",
        "footnotes": [],
        "references": [
            "Equation   1  gives rise to a natural corollary that upper bounds the Wasserstein distance, i.e., the distance between the empirical distributions of  X X \\mathbf{X} bold_X  and  X w subscript X w \\mathbf{X}_{w} bold_X start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT . Together,  Equation   1  and  Corollary   4.2  show that in expectation, the watermarked dataset  X w subscript X w \\mathbf{X}_{w} bold_X start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT  is close to the original dataset  X X \\mathbf{X} bold_X . Thus, downstream tasks operated on  X w subscript X w \\mathbf{X}_{w} bold_X start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT  instead of  X X \\mathbf{X} bold_X  would only induce additional error in the order of  1 / b 1 b \\nicefrac{{1}}{{b}} / start_ARG 1 end_ARG start_ARG italic_b end_ARG  with high probability. We empirically show the impact of this additional error for several synthetic and real-world datasets in  Section   7 .",
            "When the columns are paired according to the feature importance ordering, we match them according to  Equation   4 . Define the event  Y i , j = 1  [ columns   ( i , j )   is a pair ] subscript Y i j 1 delimited-[] columns  i j  is a pair Y_{i,j}=\\mathbf{1}[\\text{columns }(i,j)\\text{ is a pair}] italic_Y start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT = bold_1 [ columns ( italic_i , italic_j ) is a pair ] . Then, for fixed columns  i i i italic_i  and  j j j italic_j , we have:"
        ]
    },
    "id_table_5": {
        "caption": "Table 5 :  Number of column pair tests executed during detection process. The process is stopped early when the watermark is detected. Thus, when a watermark cannot be found, the result becomes  N 2  N superscript N 2 N N^{2}-N italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - italic_N , where  N N N italic_N  is the number of columns in the dataset.",
        "table": "S7.T2.2.2.2.3.1",
        "footnotes": [],
        "references": [
            "We formalize the process of detecting watermark through a hypothesis test. Intuitively, the result of  Lemma   5.1  implies that, for any  v  a  l  u  e v a l u e value italic_v italic_a italic_l italic_u italic_e  column, the probability of an element being in a green list interval is approximately  1 / 2 1 2 \\nicefrac{{1}}{{2}} / start_ARG 1 end_ARG start_ARG 2 end_ARG . While this convergence is agnostic to how the green intervals are chosen, it is non-trivial for a data-provider to detect the watermark due to the pairwise structure of  Algorithm   1 . Particularly, if the data-provider has knowledge of the  v  a  l  u  e v a l u e value italic_v italic_a italic_l italic_u italic_e  columns and the hash function, they still need to individually check which  k  e  y k e y key italic_k italic_e italic_y  column corresponds to the selected  v  a  l  u  e v a l u e value italic_v italic_a italic_l italic_u italic_e  column. In the worst case, all  n n n italic_n   k  e  y k e y key italic_k italic_e italic_y  columns must be checked for each  v  a  l  u  e v a l u e value italic_v italic_a italic_l italic_u italic_e  column before the data-provider can confidently claim that the dataset is not watermarked. With this knowledge, we formulate the hypothesis test as follows:",
            "Given a watermarked element  x  I j = [ j  1 / b , j / b ] x subscript I j j 1 b j b x\\in I_{j}=[\\nicefrac{{j-1}}{{b}},\\nicefrac{{j}}{{b}}] italic_x  italic_I start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = [ / start_ARG italic_j - 1 end_ARG start_ARG italic_b end_ARG , / start_ARG italic_j end_ARG start_ARG italic_b end_ARG ]  and the truncation function defined in  Equation   5 . Then, the probability that the truncated element  x tr subscript x tr x_{\\mathrm{tr}} italic_x start_POSTSUBSCRIPT roman_tr end_POSTSUBSCRIPT  falls out of its original green interval is",
            "Since the truncation function defined in  Equation   5  always truncate an element  x x x italic_x  to the nearest left grid point, which lies outside of  I j subscript I j I_{j} italic_I start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , we have"
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "S7.T2.2.2.2.4.1",
        "footnotes": [],
        "references": [
            "That is, when the null hypothesis  H 0 subscript H 0 H_{0} italic_H start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  holds, then it means all of the individual null hypotheses for the  i i i italic_i -th value column must hold simultaneously. Thus, the data-owner who want to detect the watermark for a dataset  X X \\mathbf{X} bold_X  would need to perform the hypothesis test for each  v  a  l  u  e v a l u e value italic_v italic_a italic_l italic_u italic_e  column individually. If the goal is to reject the null hypothesis  H 0 subscript H 0 H_{0} italic_H start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  when the  p p p italic_p -value is less than a predetermined significant threshold    \\alpha italic_  (typically chosen to be  0.05 0.05 0.05 0.05  to represent  5 % percent 5 5\\% 5 %  risk of incorrectly rejecting the null hypothesis), then the data-provider would check if the  p p p italic_p -value for each individual null hypothesis  H 0 , i subscript H 0 i H_{0,i} italic_H start_POSTSUBSCRIPT 0 , italic_i end_POSTSUBSCRIPT  is lower than   / n 2  superscript n 2 \\nicefrac{{\\alpha}}{{n^{2}}} / start_ARG italic_ end_ARG start_ARG italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG  (after accounting for the family-wise error rate using Bonferroni correction  [ 6 ] ).  2 2 2 With knowledge of the feature importance (detailed explanation in  Section   6 ), the data-owner can instead choose adaptive significant level    \\alpha italic_  for each individual hypothesis test. Informally, we put more weight on pairs of  ( k  e  y , v  a  l  u  e ) k e y v a l u e (key,value) ( italic_k italic_e italic_y , italic_v italic_a italic_l italic_u italic_e )  columns that are closer together in their feature importance while ensuring that all significant levels still sum up to the desired    \\alpha italic_  threshold.",
            "Given a watermarked dataset  X w subscript X w \\mathbf{X}_{w} bold_X start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT  and a data scientist attacking  X w subscript X w \\mathbf{X}_{w} bold_X start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT  with feature selection as in  6.1 . Then, the number of preserved column pairs under feature importance pairing is at least twice as many as that under uniformly random pairing.",
            "Theorem   6.2  implies that, under the feature importance pairing scheme, the truncated dataset would retain more valuable information for the downstream task. In  Section   7 , we empirically show how this theoretical guarantee translates to improved utility in the downstream task for various datasets.",
            "Now, we can look at the number of preserved pairs of columns under feature importance pairing. Note that we assume the feature selection is performed according to  6.1 , and the columns are in descending order of feature importance. Since we are only retaining the top  k k k italic_k  most important features after feature selection, this is equivalent to counting the number of preserved pairs among the first  k k k italic_k  features after reordering."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "A5.EGx2",
        "footnotes": [],
        "references": [
            "Equation   1  gives rise to a natural corollary that upper bounds the Wasserstein distance, i.e., the distance between the empirical distributions of  X X \\mathbf{X} bold_X  and  X w subscript X w \\mathbf{X}_{w} bold_X start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT . Together,  Equation   1  and  Corollary   4.2  show that in expectation, the watermarked dataset  X w subscript X w \\mathbf{X}_{w} bold_X start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT  is close to the original dataset  X X \\mathbf{X} bold_X . Thus, downstream tasks operated on  X w subscript X w \\mathbf{X}_{w} bold_X start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT  instead of  X X \\mathbf{X} bold_X  would only induce additional error in the order of  1 / b 1 b \\nicefrac{{1}}{{b}} / start_ARG 1 end_ARG start_ARG italic_b end_ARG  with high probability. We empirically show the impact of this additional error for several synthetic and real-world datasets in  Section   7 .",
            "Theorem   6.2  implies that, under the feature importance pairing scheme, the truncated dataset would retain more valuable information for the downstream task. In  Section   7 , we empirically show how this theoretical guarantee translates to improved utility in the downstream task for various datasets."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "A5.EGx3",
        "footnotes": [],
        "references": []
    },
    "id_table_9": {
        "caption": "",
        "table": "A5.EGx4",
        "footnotes": [],
        "references": []
    },
    "id_table_10": {
        "caption": "",
        "table": "A5.EGx5",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "",
        "table": "A5.EGx6",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "",
        "table": "A5.EGx7",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "",
        "table": "A5.EGx8",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "A5.EGx9",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "",
        "table": "A5.EGx10",
        "footnotes": [],
        "references": []
    },
    "id_table_16": {
        "caption": "",
        "table": "A5.EGx11",
        "footnotes": [],
        "references": []
    },
    "id_table_17": {
        "caption": "",
        "table": "A5.EGx12",
        "footnotes": [],
        "references": []
    },
    "id_table_18": {
        "caption": "",
        "table": "A5.EGx13",
        "footnotes": [],
        "references": []
    },
    "id_table_19": {
        "caption": "",
        "table": "A4.T3.30",
        "footnotes": [],
        "references": []
    },
    "id_table_20": {
        "caption": "",
        "table": "A4.T3.30.32.2.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_21": {
        "caption": "",
        "table": "A4.T4.30",
        "footnotes": [],
        "references": []
    },
    "id_table_22": {
        "caption": "",
        "table": "A4.T4.30.32.2.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_23": {
        "caption": "",
        "table": "A4.T5.18",
        "footnotes": [],
        "references": []
    }
}