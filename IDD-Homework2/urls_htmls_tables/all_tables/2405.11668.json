{
    "id2.2": {
        "caption": null,
        "table": null,
        "footnotes": [],
        "references": []
    },
    "S5.T1.1": {
        "caption": "Average score for all error types for different metrics.",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T1.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" id=\"S5.T1.1.1.1.1\">Metric</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T1.1.1.1.2\">Average Score for all error types</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T1.1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T1.1.2.1.1\">SacreBLEU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.1.2.1.2\">0.30</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.1.3.2.1\">Meteor</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.3.2.2\">0.56</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.1.4.3.1\">RougeL</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.4.3.2\">0.59</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.1.5.4.1\">BERTScore</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.5.4.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.1.5.4.2.1\">0.91</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.1.6.5.1\">BERTScore_sc</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.6.5.2\">0.51</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.7.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.1.7.6.1\">TER</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.7.6.2\">0.55</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.8.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S5.T1.1.8.7.1\">Google_BLEU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T1.1.8.7.2\">0.34</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "Table 1 shows the average score for each metric on the translated tweets with critical errors. The highest score is the BERTScore which gives an average of 0.91 to the translation, falsely indicating a very good performance of the MT system. The lowest score is the SacreBLEU which gives an average score of 0.30. However, given the fact that all the translations in the corpus have critical errors, the SacreBLEU score is still not indicative of the problems of the MT in this dataset. Overall, the average scores of all the studied metrics give a false confidence in an MT output with critical errors and hence these metrics may not be the optimal solution in assessing translation quality in high-risk uses of MT tools.\n"
        ]
    }
}