{
    "id_table_1": {
        "caption": "Table 1:  Composition of our Darija-SFT-Mixture instruction-tuning dataset.",
        "table": "S2.T1.1",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "In developing Atlas-Chat, we chose to use instruction-tuning on a base model rather than training from scratch. This decision was primarily driven by the fact that training an LLM from the ground up requires extensive data, which is not readily available for Darija, a low-resource dialect. For the same reason, our training process does not include the additional continual pre-training phase typically seen in many language adaptation efforts. However, to mitigate this limitation, we designed a synthetic instruction dataset (see Section  5.3 ) that, to some extent, mimics the next-word prediction task over a relatively longer context, typically performed during (continual) pre-training. Moreover, recent studies show that multilingual LLMs often exhibit a bias toward internally solving tasks in English, even when trained on multiple languages  (Zhao et al.,  2024 ) , and perform best with English prompts, followed by mixed prompts, while non-English prompts significantly underperform  (Kmainasi et al.,  2024 ) . This observation led us to limit the scope of our work to a monolingual LLM, making Atlas-Chat  Darija-centric . We focus on developing a model that accurately understands prompts written in Darija, generates Darija content, respects its cultural context, and remains accessible and adaptable for native speakers. Therefore, we directed our efforts towards creating an extensive and diverse Darija dataset for instruction-tuning. Table  1  summarizes the composition of our Darija-SFT-Mixture dataset. We employed a multifaceted approach to data preparation.  First , we reviewed previous research in Darija NLP and collected the majority of available native Darija datasets that met our quality standards. The data selection rule established by native speakers was as follows: if the data is a mix of Darija with some MSA, it is acceptable; if it is mixed with other dialects, it is not. In total, ten datasets covering tasks such as translation, summarization, and sentiment analysis were selected.  Second , we synthesized high-quality instruction data using advanced proprietary models, drawing on sources such as Wikipedia pages, social media posts, and stories written in Darija. The native and synthetic datasets were then converted into training instructions using templates, with 80% formatted as zero-shot, 10% as few-shot, and 10% as multi-turn samples.  Third , we translated high-quality English instruction datasets into Darija with stringent quality control to expand the range of scenarios, domains, and tasks covered by our dataset. By combining these different sources, we aimed to enhance the models ability to understand and generate Darija across various contexts.",
            "We collected four datasets containing sentence translations between Darija, MSA, English, and French. These datasets were then converted into training instructions using the templates provided in Appendix  A.1 . Since our model is Darija-centric, we consider six translation directions: Darija to English, French, MSA, and vice versa. All instructions are written in Darija for each case.  DODa-10K 6 6 6 https://hf.co/datasets/MBZUAI-Paris/DoDa-10K . The Darija Open Dataset (DODa)  (Outchakoucht and Es-Samaali,  2021 ,  2024 ) 7 7 7 https://github.com/darija-open-dataset  is an open-source collaborative project for collecting Darija language resource, including lexicons in semantic and syntactic categories, Darija-English parallel corpus, and etc. Darija is represented in Latin script, as well as in automatically converted Arabic script. We augmented the first 10K examples of the parallel corpus, with MSA and French translated from the English text, by leveraging GPT-4. The final DODa-10K dataset includes translation quintuples between  Darija  (in both Arabic and Latin scripts),  MSA ,  English , and  French . The dataset was then extensively reviewed by groups of native Darija-speaking annotators to ensure the quality of the entire dataset. In addition to translation, to enhance the models ability to convert between Darija in Arabic and Latin scripts (also known as the  transliteration  task), we transformed 10K parallel forms into instructions using templates found in Appendix  A.2 .  MADAR   (Bouamor et al.,  2018 ) 8 8 8 https://sites.google.com/nyu.edu/madar . The Multi-Arabic Dialect Applications and Resources (MADAR) corpus is a collection of parallel sentences covering the dialects of 25 Arab cities, built upon the Basic Traveling Expression Corpus  (Takezawa et al.,  2007 ) . We select the dialect of Rabat city as  Darija  translation, along with  MSA , resulting in 12K sentence pairs. The split corpus-6-test-corpus-26-test is reserved for the evaluation.  NLLB-Seed   (Maillard et al.,  2023 ) 9 9 9 https://github.com/openlanguagedata/seed . The Seed machine translation dataset contains 6K sentences sampled from English Wikipedia and translated into 39 low-resource languages. We extract the  Darija  and  English  pairs.  FLORES+ 10 10 10 https://github.com/openlanguagedata/flores . Built upon FLORES-200  (Costa-jussa et al.,  2022 ) , this corpus is specifically designed to support multilingual research and evaluation. The English sentences were sampled in equal amounts from Wikinews (an international news source), Wikijunior (a collection of age-appropriate non-fiction books), and Wikivoyage (a travel guide). These were then translated into other languages. For each language, the dataset has 997 sentences for the dev split and 1012 sentences for the devtest split. we selected those in  Darija ,  MSA ,  English , and  French . dev is severed as training, while devtest for the evaluation.",
            "Finally, we expanded our instruction-tuning data by translating appropriate English datasets into Darija. Since English instruction-tuning datasets incorporate diverse sources, such as human-written examples, expert-curated tasks, and synthetic data generated by advanced models. These datasets cover a wide range of scenarios and improve the models generalization capabilities. We began by reviewing the most widely used datasets for fine-tuning state-of-the-art models to ensure that our translation efforts would lead to meaningful improvements. After careful consideration, we decided to focus on the  TULU-V2-mix   (Ivison et al.,  2023 ) 26 26 26 https://hf.co/datasets/allenai/tulu-v2-sft-mixture  dataset for several reasons. It offers a comprehensive dataset composition, including samples from some of the most widely used datasets, such as FLAN and ShareGPT, for fine-tuning state-of-the-art models. Appendix  B.1  presents descriptions of each of these datasets and describes how the subset was sampled. The dataset mixture was meticulously designed based on ablation studies of both human-annotated and AI-generated data, with a focus on complexity and diversity. Models fine-tuned on it showed significant improvements in overall performance on key benchmarks compared to those trained on individual datasets. We adopted the user-assistant message format from TULU-V2-mix (see Appendix  B.2 ) to structure our entire Darija-SFT-Mixture dataset. To ensure translation quality, we first filtered out instructions from TULU-V2-mix that are either inappropriate for typical Darija speakers or could lose meaning or coherence when translated, such as scientific content, translation tasks, and non-English samples. We then experimented with several open-source and closed-source models for English-to-Darija translation, including NLLB  (Costa-jussa et al.,  2022 ) , GPT, and others. Our results showed that closed-source models consistently outperformed open-source alternatives, with Claude 3.5 Sonnet emerging as our final choice. Finally, we implemented several post-processing measures to correct errors introduced by the automatic translation. All details are provided in Appendix  B.3 .",
            "LLM-as-a-judge for summarization . We evaluated the model-generated summaries by comparing them with human-written ground truths. The pre-defined criteria, adapted from  Fabbri et al. ( 2021 ) , consider wordness, conciseness, and relevancy aspects. The prompt leveraged for the evaluation is provided in Appendix  C.4 . This approach is particularly suitable for tasks requiring subjective evaluation, such as open-ended questions, dialogue generation, and summarization. To mitigate biases such as verbosity and position bias identified by  Zheng et al. ( 2023 ) , all models were additionally instructed to limit their output. Results in Figure  1  and Table  2  show that, within the LLM-as-a-judge framework, the judge model selected Atlas-Chats responses 59.76% of the time over ground truth answers. This surpasses its closest competitor in the same model-size category, Gemma-2-9B-It, by approximately 46% in win rate on the same metric."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Zero-shot performance comparison of Atlas-Chat and state-of-the-art models on the evaluation suite with prompts written in  Darija . The highest scores are indicated in  bold  and the second-highest are  underlined .",
        "table": "S8.T2.1.1",
        "footnotes": [],
        "references": [
            "We collected four datasets containing sentence translations between Darija, MSA, English, and French. These datasets were then converted into training instructions using the templates provided in Appendix  A.1 . Since our model is Darija-centric, we consider six translation directions: Darija to English, French, MSA, and vice versa. All instructions are written in Darija for each case.  DODa-10K 6 6 6 https://hf.co/datasets/MBZUAI-Paris/DoDa-10K . The Darija Open Dataset (DODa)  (Outchakoucht and Es-Samaali,  2021 ,  2024 ) 7 7 7 https://github.com/darija-open-dataset  is an open-source collaborative project for collecting Darija language resource, including lexicons in semantic and syntactic categories, Darija-English parallel corpus, and etc. Darija is represented in Latin script, as well as in automatically converted Arabic script. We augmented the first 10K examples of the parallel corpus, with MSA and French translated from the English text, by leveraging GPT-4. The final DODa-10K dataset includes translation quintuples between  Darija  (in both Arabic and Latin scripts),  MSA ,  English , and  French . The dataset was then extensively reviewed by groups of native Darija-speaking annotators to ensure the quality of the entire dataset. In addition to translation, to enhance the models ability to convert between Darija in Arabic and Latin scripts (also known as the  transliteration  task), we transformed 10K parallel forms into instructions using templates found in Appendix  A.2 .  MADAR   (Bouamor et al.,  2018 ) 8 8 8 https://sites.google.com/nyu.edu/madar . The Multi-Arabic Dialect Applications and Resources (MADAR) corpus is a collection of parallel sentences covering the dialects of 25 Arab cities, built upon the Basic Traveling Expression Corpus  (Takezawa et al.,  2007 ) . We select the dialect of Rabat city as  Darija  translation, along with  MSA , resulting in 12K sentence pairs. The split corpus-6-test-corpus-26-test is reserved for the evaluation.  NLLB-Seed   (Maillard et al.,  2023 ) 9 9 9 https://github.com/openlanguagedata/seed . The Seed machine translation dataset contains 6K sentences sampled from English Wikipedia and translated into 39 low-resource languages. We extract the  Darija  and  English  pairs.  FLORES+ 10 10 10 https://github.com/openlanguagedata/flores . Built upon FLORES-200  (Costa-jussa et al.,  2022 ) , this corpus is specifically designed to support multilingual research and evaluation. The English sentences were sampled in equal amounts from Wikinews (an international news source), Wikijunior (a collection of age-appropriate non-fiction books), and Wikivoyage (a travel guide). These were then translated into other languages. For each language, the dataset has 997 sentences for the dev split and 1012 sentences for the devtest split. we selected those in  Darija ,  MSA ,  English , and  French . dev is severed as training, while devtest for the evaluation.",
            "MSM-MG 22 22 22 https://hf.co/datasets/MBZUAI-Paris/MoroccanSocialMedia-MultiGen , a dataset introduced as part of this work, comprises 12,973 pairs of native Darija social media posts (tweets and YouTube comments) and their synthetic counterparts, covering various NLP tasks. The pairs were converted into instructions using the template provided in Appendix  A.6 . The synthetic generations are created based on six specific tasks:  Continuation ,  Reply ,  Summarization ,  Rephrasing ,  Explanation , and  Safe Response , by prompting Claude 3.5 Sonnet to respectively consider the original post as incomplete and continue it, reply to it, summarize its content, rephrase it, explain its topic, and respond safely to potentially offensive content. 9,754 Tweets were employed for the first five tasks, while 3,219 YouTube comments were utilized for the last task. The posts were collected from three sources:  QADI   (Abdelali et al.,  2021 ) 23 23 23 https://github.com/qcri/QADI : From this Arabic dialect identification dataset, 12,813 Moroccan tweets were initially sampled. After a thorough review by native Darija speakers, tweets that were no longer accessible or contained non-Darija Arabic dialects were filtered out, resulting in 6,362 valid tweets.  Twitter API : 4,226 tweets were gathered directly from the Twitter API by searching for Darija-specific keywords. The DarijaBERT paper  (Gaanoun et al.,  2024 )  identified 31 keywords exclusive to Darija, but upon review, five were found to also exist in other Arabic dialects and were excluded. The remaining 26 keywords can be found in Appendix  C.2 .  OMCD   (Essefar et al.,  2023 ) 24 24 24 https://github.com/kabilessefar/OMCD-Offensive-Moroccan-Comments-Dataset : This is a dataset for offensive content identification collected from Moroccan YouTube comments. For the purposes of this study, only comments labeled as offensive from the training split were selected. These comments were specifically utilized for the final generation task, which involved generating safe responses to potentially offensive content.",
            "Finally, we expanded our instruction-tuning data by translating appropriate English datasets into Darija. Since English instruction-tuning datasets incorporate diverse sources, such as human-written examples, expert-curated tasks, and synthetic data generated by advanced models. These datasets cover a wide range of scenarios and improve the models generalization capabilities. We began by reviewing the most widely used datasets for fine-tuning state-of-the-art models to ensure that our translation efforts would lead to meaningful improvements. After careful consideration, we decided to focus on the  TULU-V2-mix   (Ivison et al.,  2023 ) 26 26 26 https://hf.co/datasets/allenai/tulu-v2-sft-mixture  dataset for several reasons. It offers a comprehensive dataset composition, including samples from some of the most widely used datasets, such as FLAN and ShareGPT, for fine-tuning state-of-the-art models. Appendix  B.1  presents descriptions of each of these datasets and describes how the subset was sampled. The dataset mixture was meticulously designed based on ablation studies of both human-annotated and AI-generated data, with a focus on complexity and diversity. Models fine-tuned on it showed significant improvements in overall performance on key benchmarks compared to those trained on individual datasets. We adopted the user-assistant message format from TULU-V2-mix (see Appendix  B.2 ) to structure our entire Darija-SFT-Mixture dataset. To ensure translation quality, we first filtered out instructions from TULU-V2-mix that are either inappropriate for typical Darija speakers or could lose meaning or coherence when translated, such as scientific content, translation tasks, and non-English samples. We then experimented with several open-source and closed-source models for English-to-Darija translation, including NLLB  (Costa-jussa et al.,  2022 ) , GPT, and others. Our results showed that closed-source models consistently outperformed open-source alternatives, with Claude 3.5 Sonnet emerging as our final choice. Finally, we implemented several post-processing measures to correct errors introduced by the automatic translation. All details are provided in Appendix  B.3 .",
            "Evaluation metrics . We employed the Accuracy metric to evaluate models on multiple-choice benchmarks, including DarijaMMLU, DarijaHellaSwag, Belebele_Ary, and the discriminative Sentiment Analysis task within DarijaBench. For translation and summarization tasks, we adopted the conventional BLEU  (Papineni et al.,  2002 )  and ROUGE-1/L  (Lin,  2004 ) , respectively. However, since these metrics are based on  n n n italic_n -grams, they are not well-suited for assessing Darija. For example, the same word in Darija can be written in multiple ways (\"How are you?\" = \" \\< >\" = \" \\< >\" = \" \\<  >\") due to the lack of standardized spelling (e.g., code-switching, diacritics, agglutinations, borrowings), making these two metrics overly rigid in cases where slight variations still convey the same meaning. To gain a more fine-grained insight, we also included chrF  (Popovic,  2015 ) , which operates at the level of character  n n n italic_n -grams. Finally, to capture higher-level semantic similarity, we also used BERTScore  (Zhang et al.,  2019 ) , with DarijaBERT as the reference model for summarization, and multilingual BERT 37 37 37 https://hf.co/google-bert/bert-base-multilingual-cased  for translation. All our evaluations were conducted in a zero-shot setting with greedy decoding.  Result analysis . We compared Atlas-Chat with instruction-tuned models from the Jais series (including the  -family  models trained from scratch and the  -adapted  ones based on LLaMA 2), along with AceGPT, LLaMA 3.1, and Gemma 2 (our base model). Given that Atlas-Chat features 2B and 9B sizes, we extended our comparison to the closest larger-sized model above 9B when available (e.g., AceGPT-13B-chat), while included all versions with smaller sizes. The evaluation results are shown in Table  2 , demonstrating the exceptional performance of Atlas-Chat models across multiple Darija benchmarks and metrics. When compared to other models with 7B parameters or fewer, Atlas-Chat-2B has a significantly superior performance. On the DarijaMMLU, DarijaHellaSwag, Belebele Ary, and Sentiment Analysis benchmarks, Atlas-Chat-2B achieves accuracy scores of 44.97%, 41.48%, 53.89%, and 73.99% respectively, surpassing its closest competitor in the same category, namely Jais-family-6.7B, by performance gaps of 5.01% on DarijaMMLU, 2.67% on Belebele_Ary, and 17.21% on Sentiment Analysis, while being the second in performance on DarijaHellaSwag, trailing by only 0.09% from Jais-family-6.7B, despite having approximately one-third the number of parameters. In translation tasks, Atlas-Chat-2B outperformed the other models in the same category, with chrF, BLEU, and BERTScore metrics of 44.86%, 22.76%, and 73.72%, respectively. Similarly, in summarization tasks, Atlas-Chat-2B consistently ranks as the top performer in its category, with chrF, ROUGE-1/L, and BERTScore metrics of 28.8%, 9%/8.88%, and 44.71%, respectively. Atlas-Chat-2Bs strong performance is further complemented by its larger counterpart, Atlas-Chat-9B, which consistently outperforms other state-of-the-art models, achieving the highest scores in 9 out of 11 metrics. Its strength is especially evident in translation tasks, where it leads all three metrics (chrF: 50.48%, BLEU: 28.08%, BERTScore: 76.31%) by a significant margin. The model also excels in tasks such as DarijaMMLU (58.23%), DarijaHellaSwag (57.75%), Belebele_Ary (74.56%), and Sentiment Analysis (81.89%), surpassing larger models like AceGPT-13B-chat and Jais-family-13B-Chat. However, in summarization tasks evaluated using metrics based on lexical overlapping, particularly ROUGE, Atlas-Chat did not demonstrate the same level of superiority. This could be because these metrics may not fully capture the nuances of Darija. Additionally, summarization is a less constrained generation task, often resulting in equally valid summaries with varying formulations  (Guo et al.,  2024 ) . To further evaluate summarization performance, we employ the LLM-as-a-judge approach  (Zheng et al.,  2023 ) , using Claude 3.5 Sonnet as the reference model in the following section.",
            "LLM-as-a-judge for summarization . We evaluated the model-generated summaries by comparing them with human-written ground truths. The pre-defined criteria, adapted from  Fabbri et al. ( 2021 ) , consider wordness, conciseness, and relevancy aspects. The prompt leveraged for the evaluation is provided in Appendix  C.4 . This approach is particularly suitable for tasks requiring subjective evaluation, such as open-ended questions, dialogue generation, and summarization. To mitigate biases such as verbosity and position bias identified by  Zheng et al. ( 2023 ) , all models were additionally instructed to limit their output. Results in Figure  1  and Table  2  show that, within the LLM-as-a-judge framework, the judge model selected Atlas-Chats responses 59.76% of the time over ground truth answers. This surpasses its closest competitor in the same model-size category, Gemma-2-9B-It, by approximately 46% in win rate on the same metric.",
            "Figure  2  shows how samples from TULU-V2-mix are formatted.",
            "We provide the 26 Darija-specific keywords used for tweet collection through the Twitter API, as referenced in Section  5.2 .  {arabtext} , , , , , , , , , , , , , , , , , , , , , , , , , ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Subsets of TULU-V2-mix.",
        "table": "A1.SS1.2.2",
        "footnotes": [],
        "references": [
            "In developing Atlas-Chat, we chose to use instruction-tuning on a base model rather than training from scratch. This decision was primarily driven by the fact that training an LLM from the ground up requires extensive data, which is not readily available for Darija, a low-resource dialect. For the same reason, our training process does not include the additional continual pre-training phase typically seen in many language adaptation efforts. However, to mitigate this limitation, we designed a synthetic instruction dataset (see Section  5.3 ) that, to some extent, mimics the next-word prediction task over a relatively longer context, typically performed during (continual) pre-training. Moreover, recent studies show that multilingual LLMs often exhibit a bias toward internally solving tasks in English, even when trained on multiple languages  (Zhao et al.,  2024 ) , and perform best with English prompts, followed by mixed prompts, while non-English prompts significantly underperform  (Kmainasi et al.,  2024 ) . This observation led us to limit the scope of our work to a monolingual LLM, making Atlas-Chat  Darija-centric . We focus on developing a model that accurately understands prompts written in Darija, generates Darija content, respects its cultural context, and remains accessible and adaptable for native speakers. Therefore, we directed our efforts towards creating an extensive and diverse Darija dataset for instruction-tuning. Table  1  summarizes the composition of our Darija-SFT-Mixture dataset. We employed a multifaceted approach to data preparation.  First , we reviewed previous research in Darija NLP and collected the majority of available native Darija datasets that met our quality standards. The data selection rule established by native speakers was as follows: if the data is a mix of Darija with some MSA, it is acceptable; if it is mixed with other dialects, it is not. In total, ten datasets covering tasks such as translation, summarization, and sentiment analysis were selected.  Second , we synthesized high-quality instruction data using advanced proprietary models, drawing on sources such as Wikipedia pages, social media posts, and stories written in Darija. The native and synthetic datasets were then converted into training instructions using templates, with 80% formatted as zero-shot, 10% as few-shot, and 10% as multi-turn samples.  Third , we translated high-quality English instruction datasets into Darija with stringent quality control to expand the range of scenarios, domains, and tasks covered by our dataset. By combining these different sources, we aimed to enhance the models ability to understand and generate Darija across various contexts.",
            "We collected five datasets for sentiment analysis, primarily sourced from social networks. Two datasets are annotated with three labels (positive, negative, and neutral), while the other three have two labels (positive and negative). These datasets were then transformed into training instructions using templates from Appendix  A.3 .  MSDA   (Boujou et al.,  2021 ) 11 11 11 https://cc.um6p.ma/cc_datasets . It is an open dataset for sentiment analysis, designed to support research in NLP for Arabic dialects and social media. The dataset includes 52K tweets in Darija, categorized into three labels:  positive ,  neural , or  negative . The tweets are preprocessed, and emojis are retained because they play a significant role in expressing sentiment. Labels are annotated semi-automatically and bootstrapped with human intervention.  MSAC   (Oussous et al.,  2018 ,  2020 ) 12 12 12 https://github.com/ososs/Arabic-Sentiment-Analysis-corpus . The Moroccan Sentiment Analysis Corpus (MSAC) is a manually prepared dataset consisting of reviewers opinions from Hespress 13 13 13 https://www.hespress.com  on various published articles, as well as a collection of Arabic reviews and comments from Facebook, Twitter and YouTube. It includes content in both MSA and Darija, consisting of 2K sentences labeled as  positive  or  negative  in equal proportions.  ElecMorocco2016   (Elouardighi et al.,  2017 ) 14 14 14 https://github.com/sentiprojects/ElecMorocco2016 . The 2016 Moroccan elections (ElecMorocco2016) is a sentiment analysis dataset comprising 10K Facebook comments about Moroccans legislative elections held on October 7, 2016. Each comment is labeled as either  positive  or  negative . The comments are written in Darija and MSA.  MYC   (Jbel et al.,  2024 ) 15 15 15 https://github.com/MouadJb/MYC . The Moroccan Youtube Corpus (MYC) is a dataset of Moroccan YouTube comments designed for sentiment analysis. The dataset prioritizes variety over size. Comments are collected from Moroccan YouTube channels covering various topics. It contains 20K manually labeled comments, evenly divided between  positive  and  negative . Notably, the 20K comments are equally balanced between those written in Arabic script and those in Latin script.  MAC   (Garouani and Kharroubi,  2021 ) 16 16 16 https://github.com/LeMGarouani/MAC : The Moroccan Arabic Corpus (MAC) is a free, large-scale Darija corpus for sentiment analysis, consisting of 18K manually labeled tweets categorized as  positive ,  neutral ,  negative , or mixed. Only 643 tweets are labeled as mixed, so we filtered them out.",
            "Finally, we expanded our instruction-tuning data by translating appropriate English datasets into Darija. Since English instruction-tuning datasets incorporate diverse sources, such as human-written examples, expert-curated tasks, and synthetic data generated by advanced models. These datasets cover a wide range of scenarios and improve the models generalization capabilities. We began by reviewing the most widely used datasets for fine-tuning state-of-the-art models to ensure that our translation efforts would lead to meaningful improvements. After careful consideration, we decided to focus on the  TULU-V2-mix   (Ivison et al.,  2023 ) 26 26 26 https://hf.co/datasets/allenai/tulu-v2-sft-mixture  dataset for several reasons. It offers a comprehensive dataset composition, including samples from some of the most widely used datasets, such as FLAN and ShareGPT, for fine-tuning state-of-the-art models. Appendix  B.1  presents descriptions of each of these datasets and describes how the subset was sampled. The dataset mixture was meticulously designed based on ablation studies of both human-annotated and AI-generated data, with a focus on complexity and diversity. Models fine-tuned on it showed significant improvements in overall performance on key benchmarks compared to those trained on individual datasets. We adopted the user-assistant message format from TULU-V2-mix (see Appendix  B.2 ) to structure our entire Darija-SFT-Mixture dataset. To ensure translation quality, we first filtered out instructions from TULU-V2-mix that are either inappropriate for typical Darija speakers or could lose meaning or coherence when translated, such as scientific content, translation tasks, and non-English samples. We then experimented with several open-source and closed-source models for English-to-Darija translation, including NLLB  (Costa-jussa et al.,  2022 ) , GPT, and others. Our results showed that closed-source models consistently outperformed open-source alternatives, with Claude 3.5 Sonnet emerging as our final choice. Finally, we implemented several post-processing measures to correct errors introduced by the automatic translation. All details are provided in Appendix  B.3 .",
            "To evaluate LLM performance in Darija, we developed a comprehensive evaluation suite that includes benchmarks such as DarijaMMLU, DarijaHellaSwag, and DarijaBench. Additionally, we evaluated using an existing benchmark, Belebele. All our custom benchmarks are integrated into a fork 29 29 29 https://github.com/MBZUAI-Paris/lm-evaluation-harness-atlas-chat  of the LM-Evaluation-Harness repository  (Gao et al.,  2024 )  to ensure reproducibility and foster future model comparison.  DarijaMMLU 30 30 30 https://hf.co/datasets/MBZUAI-Paris/DarijaMMLU . It is constructed by translating selected subsets from two major benchmarks into Darija from English and MSA: Massive Multitask Language Understanding (MMLU)  (Hendrycks et al.,  2020 ) 31 31 31 https://hf.co/datasets/cais/mmlu  and ArabicMMLU  (Koto et al.,  2024 ) 32 32 32 https://hf.co/datasets/MBZUAI/ArabicMMLU . While constructing DarijaMMLU, subsets from MMLU and ArabicMMLU that were either too technical (beyond typical user needs) or culturally inappropriate for the Moroccan context were excluded. The remaining samples were translated into Darija using Claude 3.5 Sonnet. The benchmark consists of 22,027 multiple-choice questions, with the number of choices ranging from 2 to 5. The selected subjects are listed in Appendix  C.3 .  DarijaHellaSwag 33 33 33 https://hf.co/datasets/MBZUAI-Paris/DarijaHellaSwag . HellaSwag 34 34 34 https://hf.co/datasets/Rowan/hellaswag   (Zellers et al.,  2019 )  is a challenging multiple-choice dataset designed to evaluate machine reading comprehension and commonsense reasoning. It presents complex scenarios where models must select the most plausible continuation of a passage from four options, challenging nuanced language understanding and contextual inference. Using Claude 3.5 Sonnet, We translated the HellaSwag validation set into Darija.  Belebele_Ary . Belebele  (Bandarkar et al.,  2024 ) 35 35 35 https://hf.co/datasets/facebook/belebele  is a comprehensive multiple-choice machine reading comprehension dataset designed to evaluate both monolingual and multilingual models across 122 language variants. Each question is paired with a brief passage and offers four multiple-choice answers. For our work, we specifically used the Ary_Arab subset of Belebele, focusing on Darija to evaluate our models.  DarijaBench 36 36 36 https://hf.co/datasets/MBZUAI-Paris/DarijaBench . In addition to the above benchmarks, we evaluated with the test sets from the native Darija datasets (see Section  4 ). Typically, 10% of each subset is reserved for testing, unless the original source provides a pre-defined separate test set. The combined test sets, referred to as DarijaBench, encompass three tasks: Translation, Sentiment Analysis, and Summarization.",
            "TULU-V2-mix incorporates subsets from the following datasets: FLAN  (Wei et al.,  2021 ) 38 38 38 https://github.com/google-research/FLAN/tree/main , Open Assistant 1  (Kopf et al.,  2024 ) 39 39 39 https://hf.co/datasets/OpenAssistant/oasst1 , ShareGPT  (Chen et al.,  2023 ) 40 40 40 https://hf.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered , GPT4-Alpaca  (Peng et al.,  2023 ) 41 41 41 https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM#data-release , Code-Alpaca 42 42 42 https://github.com/sahil280114/codealpaca , LIMA  (Zhou et al.,  2024 ) 43 43 43 https://hf.co/datasets/GAIR/lima , WizardLM Evol Instruct  (Xu et al.,  2023 ) 44 44 44 https://hf.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k , and Open-Orca  (Mukherjee et al.,  2023 ) 45 45 45 https://hf.co/datasets/Open-Orca/OpenOrca . The mixture also incorporates hard-coded instructions and a set of science-related questions derived from scientific documents. Table  3  presents descriptions of each of these datasets and describes how the subset in TULU-V2-mix was sampled.",
            "We used Amazon Bedrock 54 54 54 https://aws.amazon.com/bedrock , a cloud-based machine learning service from AWS, to translate the dataset into Darija. We provided specific instructions to Claude 3.5 Sonnet for handling the translations, refining the prompt after several rounds of experimentation. The final version of the prompt that produced the best results is shown in Figure  3 . We altered this prompt slightly as needed for each subset of the dataset, ensuring that the translation remained consistent with the context and structure of each specific subset."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Translation example for model comparison.",
        "table": "A1.SS2.2.2",
        "footnotes": [],
        "references": [
            "We found only one Darija dataset for summarization. The documents and corresponding summaries were converted into instructions using the template in Appendix  A.4 .  MArSum   (Gaanoun et al.,  2022 ) 17 17 17 https://github.com/KamelGaanoun/MoroccanSummarization . The Moroccan Articles Summarization dataset (MArSum) contains 19K news articles written in Darija, along with their titles. The articles were crawled from Goud.ma 18 18 18 http://www.goud.ma/ . While some content includes MSA, all titles are written in Darija. Since the articles are relatively concise and the titles are sufficiently informative, the titles are considered as summaries of the articles. The average length of the titles is 14.6 words.",
            "To evaluate LLM performance in Darija, we developed a comprehensive evaluation suite that includes benchmarks such as DarijaMMLU, DarijaHellaSwag, and DarijaBench. Additionally, we evaluated using an existing benchmark, Belebele. All our custom benchmarks are integrated into a fork 29 29 29 https://github.com/MBZUAI-Paris/lm-evaluation-harness-atlas-chat  of the LM-Evaluation-Harness repository  (Gao et al.,  2024 )  to ensure reproducibility and foster future model comparison.  DarijaMMLU 30 30 30 https://hf.co/datasets/MBZUAI-Paris/DarijaMMLU . It is constructed by translating selected subsets from two major benchmarks into Darija from English and MSA: Massive Multitask Language Understanding (MMLU)  (Hendrycks et al.,  2020 ) 31 31 31 https://hf.co/datasets/cais/mmlu  and ArabicMMLU  (Koto et al.,  2024 ) 32 32 32 https://hf.co/datasets/MBZUAI/ArabicMMLU . While constructing DarijaMMLU, subsets from MMLU and ArabicMMLU that were either too technical (beyond typical user needs) or culturally inappropriate for the Moroccan context were excluded. The remaining samples were translated into Darija using Claude 3.5 Sonnet. The benchmark consists of 22,027 multiple-choice questions, with the number of choices ranging from 2 to 5. The selected subjects are listed in Appendix  C.3 .  DarijaHellaSwag 33 33 33 https://hf.co/datasets/MBZUAI-Paris/DarijaHellaSwag . HellaSwag 34 34 34 https://hf.co/datasets/Rowan/hellaswag   (Zellers et al.,  2019 )  is a challenging multiple-choice dataset designed to evaluate machine reading comprehension and commonsense reasoning. It presents complex scenarios where models must select the most plausible continuation of a passage from four options, challenging nuanced language understanding and contextual inference. Using Claude 3.5 Sonnet, We translated the HellaSwag validation set into Darija.  Belebele_Ary . Belebele  (Bandarkar et al.,  2024 ) 35 35 35 https://hf.co/datasets/facebook/belebele  is a comprehensive multiple-choice machine reading comprehension dataset designed to evaluate both monolingual and multilingual models across 122 language variants. Each question is paired with a brief passage and offers four multiple-choice answers. For our work, we specifically used the Ary_Arab subset of Belebele, focusing on Darija to evaluate our models.  DarijaBench 36 36 36 https://hf.co/datasets/MBZUAI-Paris/DarijaBench . In addition to the above benchmarks, we evaluated with the test sets from the native Darija datasets (see Section  4 ). Typically, 10% of each subset is reserved for testing, unless the original source provides a pre-defined separate test set. The combined test sets, referred to as DarijaBench, encompass three tasks: Translation, Sentiment Analysis, and Summarization.",
            "LLM-as-a-judge for summarization . We evaluated the model-generated summaries by comparing them with human-written ground truths. The pre-defined criteria, adapted from  Fabbri et al. ( 2021 ) , consider wordness, conciseness, and relevancy aspects. The prompt leveraged for the evaluation is provided in Appendix  C.4 . This approach is particularly suitable for tasks requiring subjective evaluation, such as open-ended questions, dialogue generation, and summarization. To mitigate biases such as verbosity and position bias identified by  Zheng et al. ( 2023 ) , all models were additionally instructed to limit their output. Results in Figure  1  and Table  2  show that, within the LLM-as-a-judge framework, the judge model selected Atlas-Chats responses 59.76% of the time over ground truth answers. This surpasses its closest competitor in the same model-size category, Gemma-2-9B-It, by approximately 46% in win rate on the same metric.",
            "We experimented with several open-source and closed-source Darija translation models, including NLLB-200-3.3B 48 48 48 https://hf.co/facebook/nllb-200-3.3B  (No Language Left Behind 49 49 49 https://ai.meta.com/research/no-language-left-behind ), Terjman-Ultra 50 50 50 https://hf.co/atlasia/Terjman-Ultra , GPT-4o 51 51 51 https://openai.com/index/hello-gpt-4o , Claude 3 Opus 52 52 52 https://www.anthropic.com/news/claude-3-family , and Claude 3.5 Sonnet 53 53 53 https://www.anthropic.com/news/claude-3-5-sonnet . Our results showed that closed-source models consistently outperformed open-source alternatives, with GPT-4o and Claude 3.5 Sonnet taking the lead. We ultimately chose  Claude 3.5 Sonnet , as it slightly outperformed GPT-4o and offered compatibility with Amazon Bedrock. Table  4  shows a comparison of an instruction translated to Darija using each of the models we tested. We observed that open-source models, namely NLLB-200-3.3B and Terjman-Ultra, tend to use more MSA, while closed-source models produce translations closer to Moroccan Darija. They also retain key formatting elements like line breaks (\\n) and tags (###), which are crucial for preserving the structure of the instructions.",
            "We manually created 13 instruction samples to ensure that the model responds correctly to identity and creator-related questions, such as Who created you? and What is your name?. Each instruction is repeated 10 times to reinforce the memorization of the answers. Figure  4  presents the full list of hard-coded instruction-answer pairs."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "A1.SS3.5.5",
        "footnotes": [],
        "references": [
            "In developing Atlas-Chat, we chose to use instruction-tuning on a base model rather than training from scratch. This decision was primarily driven by the fact that training an LLM from the ground up requires extensive data, which is not readily available for Darija, a low-resource dialect. For the same reason, our training process does not include the additional continual pre-training phase typically seen in many language adaptation efforts. However, to mitigate this limitation, we designed a synthetic instruction dataset (see Section  5.3 ) that, to some extent, mimics the next-word prediction task over a relatively longer context, typically performed during (continual) pre-training. Moreover, recent studies show that multilingual LLMs often exhibit a bias toward internally solving tasks in English, even when trained on multiple languages  (Zhao et al.,  2024 ) , and perform best with English prompts, followed by mixed prompts, while non-English prompts significantly underperform  (Kmainasi et al.,  2024 ) . This observation led us to limit the scope of our work to a monolingual LLM, making Atlas-Chat  Darija-centric . We focus on developing a model that accurately understands prompts written in Darija, generates Darija content, respects its cultural context, and remains accessible and adaptable for native speakers. Therefore, we directed our efforts towards creating an extensive and diverse Darija dataset for instruction-tuning. Table  1  summarizes the composition of our Darija-SFT-Mixture dataset. We employed a multifaceted approach to data preparation.  First , we reviewed previous research in Darija NLP and collected the majority of available native Darija datasets that met our quality standards. The data selection rule established by native speakers was as follows: if the data is a mix of Darija with some MSA, it is acceptable; if it is mixed with other dialects, it is not. In total, ten datasets covering tasks such as translation, summarization, and sentiment analysis were selected.  Second , we synthesized high-quality instruction data using advanced proprietary models, drawing on sources such as Wikipedia pages, social media posts, and stories written in Darija. The native and synthetic datasets were then converted into training instructions using templates, with 80% formatted as zero-shot, 10% as few-shot, and 10% as multi-turn samples.  Third , we translated high-quality English instruction datasets into Darija with stringent quality control to expand the range of scenarios, domains, and tasks covered by our dataset. By combining these different sources, we aimed to enhance the models ability to understand and generate Darija across various contexts.",
            "MW-QA 19 19 19 https://hf.co/datasets/MBZUAI-Paris/MoroccanWikipedia-QA  is a dataset derived from Moroccan Wikipedia dump 20 20 20 https://dumps.wikimedia.org/arywiki/latest/ , developed in our work to enhance the models question-answering (QA) capability. The dataset is divided into four tasks: Open QA (8%), Multiple-Choice QA (40%) (MMLU-alike), Extractive QA (10%), and Multiple-Choice Extractive QA (42%) (Belebele-alike), with each percentage reflecting the proportion of Wikipedia pages used for the respective task. The latter two tasks provide context along with the questions, whereas the former two do not. In Open QA and Extractive QA, answers are provided in sentence form. In the multiple-choice tasks, four answer options are presented, with the index of the correct option serving as the answer. The distribution of correct answers (e.g., A, B, C, D) are balanced. The QAs were converted into instructions with the template in Appendix  A.5 . The dataset generation involved providing each Wikipedia page to Claude 3.5 Sonnet 21 21 21 https://www.anthropic.com/news/claude-3-5-sonnet  and prompting it to generate QA pairs tailored to the four task categories. The prompts followed a one-shot or two-shot format to ensure that output adhered to the desired structure. For the extractive tasks, rather than splitting the page into paragraphsan approach that risked losing contextual meaningwe opted to present the entire page to Claude. The model was instructed to first extract a meaningful passage from the page and then generate a QA pair based on the content of that passage. Additionally, the model was directed to ensure that the extracted passages were long, self-contained, and did not lose meaning when removed from their original context. A total of 8,730 pages were collected and pre-processed by removing scraping errors. Among these pages, some followed a uniform structure, typically consisting of a brief description of a village or community followed by statistical data (e.g., literacy rates and unemployment figures). Given that these statistical sections could become meaningless when extracted from their context, they were allocated to non-extractive tasks, which could still utilize the statistical information to enrich the fine-tuned models knowledge base. The final distribution of QA pairs is as follows: 15.7% Open QA, 43.1% Multiple-Choice QA, 6.9% Extractive QA, and 34.3% Multiple-Choice Extractive QA. These percentages differ from the initial page distribution because Claude generated varying numbers of samples for each task. For example, the average number of samples generated for Open QA is 7.73, while for Extractive QA, it is 2.72.",
            "We provide the 26 Darija-specific keywords used for tweet collection through the Twitter API, as referenced in Section  5.2 .  {arabtext} , , , , , , , , , , , , , , , , , , , , , , , , , .",
            "Following the work of  Zheng et al. ( 2023 )  and  Fabbri et al. ( 2021 ) , which used advanced LLMs to evaluate responses from other LLMs, we employed Claude 3.5 Sonnet to assess the models summarization capabilities. Summarization is subjective, and traditional text overlap-based methods often struggle to provide accurate evaluations. As shown in Figure  5 , we instructed Claude to evaluate model-generated summaries based on three main criteria: wordness, conciseness, and relevance. The objective of the Darija summarization task is to produce a concise summary in native Darija using the fewest words possible, without introducing external information. At each evaluation step, two summaries were presented to Claude: one generated by an LLM and the corresponding ground truth summary. To mitigate biases such as verbosity and position bias, identified by  Zheng et al. ( 2023 ) , all models were instructed to generate summaries of no more than 30 words (the average length of title summaries). Additionally, each pair of generated and ground truth summaries was presented to Claude twice, with their positions swapped. Pairs in which position swapping influenced Claudes decision were discarded. The win rate of a models summary was calculated based on how often Claude preferred the models summary over the ground truth."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "A1.SS4.2.2",
        "footnotes": [],
        "references": [
            "MSM-MG 22 22 22 https://hf.co/datasets/MBZUAI-Paris/MoroccanSocialMedia-MultiGen , a dataset introduced as part of this work, comprises 12,973 pairs of native Darija social media posts (tweets and YouTube comments) and their synthetic counterparts, covering various NLP tasks. The pairs were converted into instructions using the template provided in Appendix  A.6 . The synthetic generations are created based on six specific tasks:  Continuation ,  Reply ,  Summarization ,  Rephrasing ,  Explanation , and  Safe Response , by prompting Claude 3.5 Sonnet to respectively consider the original post as incomplete and continue it, reply to it, summarize its content, rephrase it, explain its topic, and respond safely to potentially offensive content. 9,754 Tweets were employed for the first five tasks, while 3,219 YouTube comments were utilized for the last task. The posts were collected from three sources:  QADI   (Abdelali et al.,  2021 ) 23 23 23 https://github.com/qcri/QADI : From this Arabic dialect identification dataset, 12,813 Moroccan tweets were initially sampled. After a thorough review by native Darija speakers, tweets that were no longer accessible or contained non-Darija Arabic dialects were filtered out, resulting in 6,362 valid tweets.  Twitter API : 4,226 tweets were gathered directly from the Twitter API by searching for Darija-specific keywords. The DarijaBERT paper  (Gaanoun et al.,  2024 )  identified 31 keywords exclusive to Darija, but upon review, five were found to also exist in other Arabic dialects and were excluded. The remaining 26 keywords can be found in Appendix  C.2 .  OMCD   (Essefar et al.,  2023 ) 24 24 24 https://github.com/kabilessefar/OMCD-Offensive-Moroccan-Comments-Dataset : This is a dataset for offensive content identification collected from Moroccan YouTube comments. For the purposes of this study, only comments labeled as offensive from the training split were selected. These comments were specifically utilized for the final generation task, which involved generating safe responses to potentially offensive content.",
            "Figure  6  and  7  present some samples of Atlas-Chat responses on a variety of questions."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "A1.SS5.p1.3.3",
        "footnotes": [],
        "references": [
            "To mitigate the limitation of performing only instruction-tuning for language adaptation without the typical continual pre-training phasedue to the lack of sufficient amount of Darija pre-training datawe designed a synthetic story completion dataset, aiming to enhance the next-word prediction capability in Darija for our models over a relatively longer context. First, we collected 4,392 long stories from 9esa 25 25 25 https://www.9esa.com , a website featuring a rich collection of various stories entirely written in Darija. We denote this dataset as DarijaStory. The scraped stories were then divided into segments of approximately 2,048 tokens, adhering to the base model tokenizers vocabulary. The segments were further divided into two parts of varying lengths: the beginning part and the ending part to be completed. For the two segmentation steps above, the split point is preferably placed at line breaks. Finally, the pairs were converted into story completion instructions using the template provided in Appendix  A.7 .",
            "Figure  6  and  7  present some samples of Atlas-Chat responses on a variety of questions."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "A1.SS5.p1.7.4",
        "footnotes": [],
        "references": []
    },
    "id_table_9": {
        "caption": "",
        "table": "A1.SS5.p1.10.3",
        "footnotes": [],
        "references": []
    },
    "id_table_10": {
        "caption": "",
        "table": "A1.SS6.p1.2.2",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "",
        "table": "A1.SS6.p1.4.2",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "",
        "table": "A1.SS6.p1.6.2",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "",
        "table": "A1.SS6.p1.8.2",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "A1.SS6.p1.10.2",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "",
        "table": "A1.SS6.p1.12.2",
        "footnotes": [],
        "references": []
    },
    "id_table_16": {
        "caption": "",
        "table": "A1.SS7.2.2",
        "footnotes": [],
        "references": []
    },
    "id_table_17": {
        "caption": "",
        "table": "A2.T3.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_18": {
        "caption": "",
        "table": "A2.T4.15",
        "footnotes": [],
        "references": []
    }
}