{
    "id_table_1": {
        "caption": "Table 1:  The components and data statistics of MathCode-Pile.",
        "table": "S2.T1.1",
        "footnotes": [],
        "references": [
            "Second, we propose a novel method for generating large amounts of paired mathematical reasoning steps and their corresponding Python code. Given a piece of text from the pretraining corpus collected above, we wrap it in a carefully designed prompt that instructs a Llama-3.1-70B-Instruct model to extract LaTeX expressions along with their relevant context, including the conditions for each expression and the result of its computation. This results in a list of comprehensive mathematical reasoning steps, complete with the necessary conditions, the computations taken, and the results. Then, we prompt the model to translate each reasoning step into a Python code snippet that captures the underlying reasoning process. The generated Python snippets are executed, and only those that run successfully and produce outputs matching the expected results are retained. By pairing the code with the corresponding reasoning step, we create the final data. The process is demonstrated in the lower half of Fig.  1 . This process yields a 2.7B-token corpus of mathematical code snippets accompanied with their corresponding reasoning steps, which we combine with the data generated in the first step to create a 19.2B-token pretraining dataset, named  MathCode-Pile .",
            "We curate our mathematical pretraining dataset, MathCode-Pile, in two steps: first, we collect the basic data in Sec.  2.1 , and then we use it to generate mathematical code snippets with their corresponding natural language reasoning steps in Sec.  2.2 .",
            "Leveraging the Llama-3.1-70B-Instruct model, we initially generated 3.1B tokens of the data. After applying the filtering process, we obtain a total of 2.7B tokens of high-quality data of mathematical code snippets accompanied with their corresponding reasoning steps. This newly generated data significantly enriches our original pretraining corpus. By combining this data with the basic pretraining data, we create a comprehensive pretraining dataset totaling 19.2B tokens, which we refer to as  MathCode-Pile . Detailed statistics of MathCode-Pile are presented in Tab.  1 . This dataset is tailored specifically for enhancing the mathematical and coding abilities of LLMs. To avoid benchmark contamination, we follow  Yang et al. ( 2024b )  to filter out samples that have significant overlaps with any of the questions from benchmark datasets used in evaluation. We use exact match to remove the identical samples and further apply 13-gram deduplication (with a condition that the Jaccard similarity should be larger than 0.6) to remove more samples that might cause contamination.",
            "This session shows several examples of the translation from math-related texts to mathematical code acompanied with corresponding reasoning steps. As shown in Tab.  8 , Tab.  9 , Tab.  10  and Tab.  11 , the model first extract the LaTex expression alone with its conditions and result from the original text, then generates an Python code snippet based on this information.",
            "In this section, we present several examples in the original OpenWebMath dataset that are irrelevant to mathematical reasoning and removed in the filtering process. As shown in Tab.  12 , Tab.  13 , and Tab.  14 , the content of these documents are not related to math, but instead are about subjects such as politics, testing software, or web development. Removing these irrelevant texts have no obvious impact on the mathematical continued pretraining performance."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Performance of various pretrained models on five representative mathematical datasets. All results reported are based on greedy decoding. Code-open shows whether the code for data-processing and model-training is open-sourced. The  red  numbers show the improvements compared to the base model from which each MathCoder2 model is trained.",
        "table": "S3.T2.3",
        "footnotes": [],
        "references": [
            "We curate our mathematical pretraining dataset, MathCode-Pile, in two steps: first, we collect the basic data in Sec.  2.1 , and then we use it to generate mathematical code snippets with their corresponding natural language reasoning steps in Sec.  2.2 .",
            "Our method begins with taking a piece of text from the basic pretraining data and wrapping it in a carefully designed prompt, as shown in Fig.  2 . This prompt instructs the model to identify  LaTeX expressions  denoting complex computations, along with the necessary context, including the  conditions required for the computation  and the  expected result . By explicitly extracting the conditions of the LaTeX expression, we enhance the models ability to comprehend the underlying mathematical reasoning behind the usage of the expression. The expected result of the computation can later serve as a basis for verifying the correctness of the generated code. A mathematical reasoning step is constructed by combining the conditions, expression and result. The prompt then directs the model to produce a  Python code snippet  that accurately reflects the underlying reasoning process behind the extracted reasoning step. The model is asked to present the conditions, LaTeX expression, result, and Python code snippet in a structured format, ensuring that each part can be easily extracted from the generated text. Examples of generated texts are shown in Appendix  C .",
            "Results : As demonstrated in Tab.  2 , continued pretraining on MathCode-Pile consistently improves performance across all five benchmark datasets. MathCoder2 models rival the performance of top models like InternLM2-Math-Base, InternLM2.5, and DeepSeekMath. In particular, MathCoder2-DeepSeekMath demonstrates that our method continues to enhance the performance of DeepSeekMath, a model that has already been extensively trained on large amounts of math-related data. However, there remains a performance gap between MathCoder2 and the Qwen2-Math and Qwen2.5-Math models. This gap might be attributed to their superior computational, manual, and financial resources, which enable the scaling of data size and the further improvement of data quality, reporting a mathemtaical dataset of 700B tokens  (Yang et al.,  2024b ) .",
            "In this section, we present several examples in the original OpenWebMath dataset that are irrelevant to mathematical reasoning and removed in the filtering process. As shown in Tab.  12 , Tab.  13 , and Tab.  14 , the content of these documents are not related to math, but instead are about subjects such as politics, testing software, or web development. Removing these irrelevant texts have no obvious impact on the mathematical continued pretraining performance."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Performance of various finetuned models on five representative mathematical datasets. All results reported are based on greedy decoding.",
        "table": "S3.T3.1",
        "footnotes": [],
        "references": [
            "The results are shown in Tab.  3 . MathCoder2-Instruct-TIR achieves high performance on all five datasets, reaching 69.7% on MATH and 86.5% on GSM8K, outperforming many of the best open-source models of similar size and demonstrating our methods potential to improve performance on downstream mathematical reasoning tasks. As this paper focuses on continued mathematical pretraining, the post-training serves only as a validation of the potential of our models. We conducted only simple supervised fine-tuning, without performing reinforcement learning or direct preference optimization, which could further improve performance on downstream tasks.",
            "In this section, we present the prompt we used for annotation of documents in OpenWebMath and the initially filtered CC-En. The prompt, as shown in Fig.  3 , asks the model to classify the document into one of seven types, which are types of documents that frequently appear in the datasets. We observe that this method helps the model to better identify and filter out irrelevant text than using a binary classification of whether the text is related to math.",
            "In this section, we present several examples in the original OpenWebMath dataset that are irrelevant to mathematical reasoning and removed in the filtering process. As shown in Tab.  12 , Tab.  13 , and Tab.  14 , the content of these documents are not related to math, but instead are about subjects such as politics, testing software, or web development. Removing these irrelevant texts have no obvious impact on the mathematical continued pretraining performance."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Analysis of the impact of the mathematical code. The upper half of the table presents the results of both using and not using the mathematical code data. The lower half displays an ablation study on the design of concatenating the reasoning steps and code snippets for pretraining. Basic + Reasoning-step-only represents only adding the conditions, expressions, and results, while Basic + Trans-code-only represents only adding the translated code. Reasoning-Step&Code represents the concatenated data combining both. Basic + No-code-prompt represents using a prompt that simply instruct Llama-3.1-70B-Instruct to rewrite texts to improve their quality.",
        "table": "S3.T4.1",
        "footnotes": [],
        "references": [
            "Analysis of the impact of the mathematical code . We analyze the impact of the mathematical code on continued pretraining by comparing the results of adding and not adding the mathematical code. As shown in Tab.  4 , the addition of the mathematical code in the pretraining corpus significantly improves performance across all five datasets. Note that the mathematical code only constitutes 14.1% of the 19.2B tokens in the MathCode-Pile dataset, yet the improvement in accuracy it brings about compared to the total improvement in accuracy ( a  c  c MathCode-Pile  a  c  c basic a  c  c MathCodePile  a  c  c orig a c subscript c MathCode-Pile a c subscript c basic a c subscript c MathCodePile a c subscript c orig \\frac{acc_{\\text{MathCode-Pile}}-acc_{\\text{basic}}}{acc_{\\text{MathCodePile}}% -acc_{\\text{orig}}} divide start_ARG italic_a italic_c italic_c start_POSTSUBSCRIPT MathCode-Pile end_POSTSUBSCRIPT - italic_a italic_c italic_c start_POSTSUBSCRIPT basic end_POSTSUBSCRIPT end_ARG start_ARG italic_a italic_c italic_c start_POSTSUBSCRIPT MathCodePile end_POSTSUBSCRIPT - italic_a italic_c italic_c start_POSTSUBSCRIPT orig end_POSTSUBSCRIPT end_ARG ) is 21.8%, 27.1%, 44.5%, 66.2%, and 35.1% on the five benchmark datasets, respectively, demonstrating the effectiveness of the mathematical code. Comparison across different training steps is shown in Appendix  F .",
            "We also analyze the design choice of concatenating the natural language reasoning step with the mathematical code for pretraining. This analysis is conducted by studying the results of adding only the natural language reasoning steps, and separately adding only the translated code. As shown in Tab.  4 , Basic + Reasoning-step-only represents adding only the natural language reasoning steps; Basic + Trans-code-only represents adding only the translated code; and Basic + Reasoning-Step&Code represents adding the concatenated data that combines both. The Basic + Reasoning-Step&Code configuration results in the best performance, demonstrating the importance of including both the natural language reasoning step and the translated mathematical code.",
            "To rule out the possibility that the improvement comes from the higher quality of texts generated by Llama-3.1-70B-Instruct, we use a prompt that asks Llama-3.1-70B-Instruct to rewrite the given text. The details of this prompt are provided in Appendix  E . We present the results of replacing the mathematical code with texts generated using this prompt in Tab.  4 , labeled as Basic + No-code-prompt. Our method of generating mathematical code accompanied with corresponding reasoning steps outperforms this baseline, demonstrating the effectiveness of our approach.",
            "In this section, we present several examples in the original OpenWebMath dataset that are irrelevant to mathematical reasoning and removed in the filtering process. As shown in Tab.  12 , Tab.  13 , and Tab.  14 , the content of these documents are not related to math, but instead are about subjects such as politics, testing software, or web development. Removing these irrelevant texts have no obvious impact on the mathematical continued pretraining performance.",
            "To rule out the possibility that the improvement results solely from the enhanced quality of the texts generated by Llama-3.1-70B-Instruct, we designed a prompt asking Llama-3.1-70B-Instruct to rewrite the text, checking for mistakes in content and format to enhance accuracy and clarity, as shown in Fig.  4 ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Analysis of the effect of different components in MathCoder2-Pile. The base model is DeepSeekCoder-1.3B.",
        "table": "S3.T5.1",
        "footnotes": [],
        "references": [
            "Analysis of the impact of various parts of the basic data . We perform experiments on a smaller model, DeepSeekCoder-1.3B, using different parts of the basic data. As demonstrated in Tab.  5 , filtered-OpenWebMath and filtered-CC-En-math significantly improve the performance of the model. In comparison, textbooks, synthetic data, and code are smaller in data size and play a less important role. As each of these parts of data is too small for individual pretraining, we combine them with OpenWebMath-filtered to show that they each bring a small yet noticeable improvement compared to using only OpenWebMath-filtered. Since we performed filtering on OpenWebMath and the initially filtered CC-En to remove irrelevant data, we also compare the performance before and after filtering. We observe that there is no obvious degradation in performance after removing irrelevant content, showing the effectiveness of the filtering.",
            "In this section, we present the comparison between adding and not adding mathematical code across different training steps. The experiments are conducted on Llama-3 8B. As shown in Fig.  5  and Fig.  6 , adding the model-translated mathematical code improves accuracy across different training steps."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Comparison between MathCode-Pile and other Mathematical Pretrain datasets.",
        "table": "S3.T6.1",
        "footnotes": [],
        "references": [
            "Comparison with other open-source mathematical pretraining corpora . We compare MathCode-Pile with various other open-source mathematical pretraining corpora using DeepSeekCoder-1.3B. We train each corpus for 3 epochs with a global batch size of 2 million tokens and a 4096 token context length, since we observe that the models performance usually saturates around 3 epochs. As shown in Tab.  6 , MathCode-Pile significantly outperforms OpenWebMath, Proof-Pile-2, and MathPile when trained on DeepSeekCoder-1.3B. The DeepSeekMath Corpus is not open-source, and its performance on DeepSeekLLM-1.3B is taken from  Shao et al. ( 2024 ) , which is trained for 150B tokens, more than our MathCode-Piles training of approximately 60B tokens. The 1.3B model trained with MathCode-Pile outperforms the 1.3B model trained with DeepSeekMath Corpus.",
            "In this section, we present the comparison between adding and not adding mathematical code across different training steps. The experiments are conducted on Llama-3 8B. As shown in Fig.  5  and Fig.  6 , adding the model-translated mathematical code improves accuracy across different training steps."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Comparison between finetuning the original Llama-3-8B, MathCoder2-Basic-Llama-3-8B, and MathCoder2-Llama-3-8B on NuminaMath-TIR. MathCoder2-Basic-Llama-3-8B is the model resulting from continued pretraining on the basic data without adding the model-translated mathematical code.",
        "table": "S3.T7.1",
        "footnotes": [],
        "references": [
            "Analysis of the improvement on the potential of being finetuned for TIR reasoning . To analyze the effect of the model-translated mathematical code on LLMs potential to be finetuned for TIR reasoning, we finetune the original Llama-3-8B, MathCoder2-Basic-Llama-3-8B, and MathCoder2-Llama-3-8B on NuminaMath-TIR 5 5 5 https://huggingface.co/datasets/AI-MO/NuminaMath-TIR  for three epochs, respectively. As shown in Tab.  7 , the results of finetuning on MathCoder2-Basic-Llama-3-8B are higher than the results of finetuning on Llama-3-8B. Finetuning on MathCoder2-Llama-3-8B results in even higher performance than finetuning on MathCoder2-Basic-Llama-3-8B, showing that the addition of mathematical code effectively enhances the models potential of being finetuned for TIR reasoning."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  An example of translating a piece of text to mathematical code and a corresponding reasoning step.",
        "table": "A3.T8.3",
        "footnotes": [],
        "references": [
            "This session shows several examples of the translation from math-related texts to mathematical code acompanied with corresponding reasoning steps. As shown in Tab.  8 , Tab.  9 , Tab.  10  and Tab.  11 , the model first extract the LaTex expression alone with its conditions and result from the original text, then generates an Python code snippet based on this information."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  An example of translating a piece of text to mathematical code and a corresponding reasoning step.",
        "table": "A3.T9.32",
        "footnotes": [],
        "references": [
            "This session shows several examples of the translation from math-related texts to mathematical code acompanied with corresponding reasoning steps. As shown in Tab.  8 , Tab.  9 , Tab.  10  and Tab.  11 , the model first extract the LaTex expression alone with its conditions and result from the original text, then generates an Python code snippet based on this information."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:  An example of translating a piece of text to mathematical code and a corresponding reasoning step.",
        "table": "A3.T10.9",
        "footnotes": [],
        "references": [
            "This session shows several examples of the translation from math-related texts to mathematical code acompanied with corresponding reasoning steps. As shown in Tab.  8 , Tab.  9 , Tab.  10  and Tab.  11 , the model first extract the LaTex expression alone with its conditions and result from the original text, then generates an Python code snippet based on this information."
        ]
    },
    "id_table_11": {
        "caption": "Table 11:  An example of translating a piece of text to mathematical code and a corresponding reasoning step.",
        "table": "A3.T11.9",
        "footnotes": [],
        "references": [
            "This session shows several examples of the translation from math-related texts to mathematical code acompanied with corresponding reasoning steps. As shown in Tab.  8 , Tab.  9 , Tab.  10  and Tab.  11 , the model first extract the LaTex expression alone with its conditions and result from the original text, then generates an Python code snippet based on this information."
        ]
    },
    "id_table_12": {
        "caption": "Table 12:  An example of removed text irrelevant to mathematical reasoning in OpenWebMath.",
        "table": "A4.T12.1",
        "footnotes": [],
        "references": [
            "In this section, we present several examples in the original OpenWebMath dataset that are irrelevant to mathematical reasoning and removed in the filtering process. As shown in Tab.  12 , Tab.  13 , and Tab.  14 , the content of these documents are not related to math, but instead are about subjects such as politics, testing software, or web development. Removing these irrelevant texts have no obvious impact on the mathematical continued pretraining performance."
        ]
    },
    "id_table_13": {
        "caption": "Table 13:  An example of removed text irrelevant to mathematical reasoning in OpenWebMath.",
        "table": "A4.T13.1",
        "footnotes": [],
        "references": [
            "In this section, we present several examples in the original OpenWebMath dataset that are irrelevant to mathematical reasoning and removed in the filtering process. As shown in Tab.  12 , Tab.  13 , and Tab.  14 , the content of these documents are not related to math, but instead are about subjects such as politics, testing software, or web development. Removing these irrelevant texts have no obvious impact on the mathematical continued pretraining performance."
        ]
    },
    "id_table_14": {
        "caption": "Table 14:  An example of removed text irrelevant to mathematical reasoning in OpenWebMath.",
        "table": "A4.T14.1",
        "footnotes": [],
        "references": [
            "In this section, we present several examples in the original OpenWebMath dataset that are irrelevant to mathematical reasoning and removed in the filtering process. As shown in Tab.  12 , Tab.  13 , and Tab.  14 , the content of these documents are not related to math, but instead are about subjects such as politics, testing software, or web development. Removing these irrelevant texts have no obvious impact on the mathematical continued pretraining performance."
        ]
    }
}