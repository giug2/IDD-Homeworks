{
    "id_table_1": {
        "caption": "Table 1 :  Experimental results of dataset condensation methods on CIFAR-10. HMDC is the best or second-best performer in most cases.",
        "table": "Sx1.EGx1",
        "footnotes": [],
        "references": [
            "Traditionally, dataset condensation methods have employed a compact 3-layered model named ConvNet  [ 38 ]  for the condensation process  [ 2 ,  34 ,  40 ,  28 ] . The standard evaluation method for assessing the performance of condensed images has been the  ConvNet-to-ConvNet  test, where condensation and evaluation are conducted on ConvNet. Some studies have delved into assessing the general performance of condensed images on other shallow models, such as 3-layered MLP or AlexNet  [ 24 ] . However, as illustrated in Figure  1 , the effectiveness of generated images diminishes when applied to widely used models like ResNet  [ 13 ]  and Vision Transformer (ViT)  [ 8 ] . This indicates that synthetic images are over-condensed on ConvNet, showing a high model dependence. This dependency significantly constrains the versatility of condensed images. Introducing a completely new model necessitates training a new model and generating a new condensed image, implying that training data must be stored in some way. Consequently, there is a need for a model-agnostic benchmark, not limited to  ConvNet-to-ConvNet , that operates independently of the specific model.",
            "To measure how the condensed image effectively trains the large model, We conducted experiments on CIFAR10  [ 23 ]  on Images Per Class (IPC) 1, 10, 50. Our assessment followed the experimental procedures detailed in each referenced paper. For evaluation, we utilized ConvNet, ViT-tiny  [ 8 ] , ResNet18  [ 13 ] , ViT-small, ResNet50, ResNet101, and ViT-base models, each with specific learning rates ( 0.01 0.01 0.01 0.01 ,  0.001 0.001 0.001 0.001 ,  0.001 0.001 0.001 0.001 ,  0.001 0.001 0.001 0.001 ,  0.001 0.001 0.001 0.001 ,  0.0001 0.0001 0.0001 0.0001 , and  0.0001 0.0001 0.0001 0.0001 , respectively). The best scores were measured as performance metrics during the training of the models for a consistent duration of 2,000 epochs. Because training a large model on a small dataset makes it difficult to compare performance, every model in Table  1  except ConvNet was pre-trained on the ImagiNet-1K  [ 6 ]  dataset.",
            "Table  1  presents a comprehensive performance comparison of the condensed images generated by each technique on CIFAR10. The proposed HMDC demonstrates commendable performance across most models, except ConvNet. Notably, other techniques generally exhibit inferior performance compared to Random across all models, except for ConvNet. The described methods collaboratively yield a condensed image that is impactful without inducing over-condensation, particularly evident in the case of ConvNet. This trend becomes more pronounced as IPC decreases. Simultaneously, our method competes favorably with other methods on ConvNet. Noteworthy is that HMDC performs the best on IPC 1 for all models. This suggests that the proposed HMDC effectively captures general features and incorporates them into a limited synthetic image. Unlike previous methods, HMDC shows promise for training large models from images compressed from relatively small models, aligning with the goal of dataset condensation. Despite utilizing two models, HMDC requires only 100 iterations and consumes less time than other models, typically using 1,200 to 20,000 iterations."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Table illustrating the outcomes of the ablation study. GBM means Gradient Balance Module and MD means Mutual Distillation by Spatial-Semantic Decomposition. The results demonstrate the individual contributions of the presented factors to performance enhancements, revealing a synergistic effect when employing them simultaneously.",
        "table": "Sx1.EGx2",
        "footnotes": [],
        "references": [
            "Here,  f  subscript f  f_{\\theta} italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  represents a model parameterized by    \\theta italic_  and  L CE subscript L CE \\mathcal{L}_{\\mathrm{CE}} caligraphic_L start_POSTSUBSCRIPT roman_CE end_POSTSUBSCRIPT  is the cross-entropy loss. However, these approaches exhibit a model-dependent nature since they focus on training the models path on the image. In contrast, our HMDC seeks to extract the common features by concurrently considering the training paths of two models,  f  1 subscript f subscript  1 f_{\\theta_{1}} italic_f start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT  and  f  2 subscript f subscript  2 f_{\\theta_{2}} italic_f start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , where two models complement their features each other as illustrated in Figure  2 . This is expressed mathematically as:",
            "To address this, we propose the gradient balance module as illustrated in Figure  2 , which sets up an accumulator to store the size of the gradient from each optimization target. The accumulator  A = { a 1 , a 2 , ... , a k }  R k A subscript a 1 subscript a 2 ... subscript a k superscript R k \\mathcal{A}=\\left\\{a_{1},a_{2},...,a_{k}\\right\\}\\in\\mathbb{R}^{k} caligraphic_A = { italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT }  blackboard_R start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT  is expressed as:",
            "We conducted an ablation study to evaluate the impact of the proposed features on performance. Specifically, we examined performance by removing Mutual Distillation by Spatial-Sementic Decomposition and the Gradient Balance Module. Table  2  presents the experimental result of ablation. The experimental results demonstrate that each element contributes to performance, and when both methods are employed, they exhibit synergy."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :   The performance variations across model combinations, depicting results for pairs of identical models (CNN + CNN) and pairs involving larger models than those employed in the experiment.",
        "table": "Sx1.EGx3",
        "footnotes": [],
        "references": [
            "The optimization of Eq.  3  faces new challenges due to a significant disparity in the gradient magnitude among   L 1 ,  L 2 , ... ,  L k  superscript L 1  superscript L 2 ...  superscript L k \\nabla\\mathcal{L}^{1},\\nabla\\mathcal{L}^{2},...,\\nabla\\mathcal{L}^{k}  caligraphic_L start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ,  caligraphic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , ... ,  caligraphic_L start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT . This discrepancy can stem from various factors, such as differences in model structure, depth, and feature dimensionality. Furthermore, if there are additional optimization targets, a hyperparameter search becomes necessary. The varying magnitudes in these gradients could potentially lead to the neglect of one side or hinder convergence.",
            "Eq.  3  designed to track the learning path of each model through the gradient matching. However, if the meanings of the two features differ significantly, it impedes the effective learning of synthetic images and hinders their convergence. To mitigate this, we propose a new mutual distillation loss that restricts the semantic divergence between the two models in training processes. On top of that, since two different models vary in depth, dimensionality, and the number of features, effective distillation between them requires careful consideration.",
            "Figure  3(a)  presents a condensed image generated using the DREAM  [ 28 ]  method. In contrast, Figure  3(b)  depicts a condensed image created using the Heterogeneous Model Dataset Condensation approach. Overall, we observe that objects exhibit the same characteristics, such as increased contrast and sharpened edges. While the visual disparities are minimal, it is noteworthy that the image generated by our method exhibits fewer artifacts and distortions compared to DREAM. These distortions appear to be caused by over-condensation, improving performance on certain models but degrading performance on others.",
            "Table  3  illustrates the performance outcomes of Heterogeneous Model Dataset Condensation across various model combinations. Significantly, there is an evident decrease in overall performance when the CNN is used uniformly, contrary to the intended objective of the method. In the third row, we adjusted the learning rate of the affine layer and layer-matching matrix to  0.001 0.001 0.001 0.001 . This suggests that with minimal modification, larger models can be accommodated using HMDC. We can also see that the use of larger models shows an overall benefit."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  Comparison table between simply using ViT-Tiny and using the presented method.",
        "table": "Sx1.EGx4",
        "footnotes": [],
        "references": [
            "Table  4  presents a comparative analysis of our experimental results with the state-of-the-art method, DREAM, changing the model into ViT-Tiny. The results reveal an overall enhancement in performance for ViT-Tiny attributed to an increased understanding of the model. However, overall performance drops, especially in the ConvNet and it is notably worse compared to using random images. This trend is further emphasized by the large gap between HMDC in combination with ConvNet. Conversely, in ConvNet, overall performance is diminished due to the models limited capacity, with a pronounced decline in the ViT series. This observation underscores the model dependency of the condensed image and the condensed image is over-condensed.",
            "Figure  4  depicts the maximum gradient magnitude of the synthetic image after the learning step. On the left, before integrating the Gradient Balance Module, there is a substantial disparity in the gradient magnitudes, leading to the neglect of other losses. Following the inclusion of the Gradient Balance Module, the gradient magnitudes from each loss function become uniform. This ensures balanced dataset condensation irrespective of the model, underscoring the essential role of the Gradient Balance Module in Heterogeneous Model Dataset Condensation."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "Sx1.EGx5",
        "footnotes": [],
        "references": []
    },
    "id_table_6": {
        "caption": "",
        "table": "Sx1.EGx6",
        "footnotes": [],
        "references": []
    },
    "id_table_7": {
        "caption": "",
        "table": "Sx1.EGx7",
        "footnotes": [],
        "references": []
    },
    "id_table_8": {
        "caption": "",
        "table": "Sx1.EGx8",
        "footnotes": [],
        "references": []
    },
    "id_table_9": {
        "caption": "",
        "table": "Sx1.EGx9",
        "footnotes": [],
        "references": []
    },
    "id_table_10": {
        "caption": "",
        "table": "Sx1.EGx10",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "",
        "table": "Sx1.EGx11",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "",
        "table": "Sx1.EGx12",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "",
        "table": "S4.T1.4.1",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "S4.T2.4.1",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "",
        "table": "S4.T3.4.1",
        "footnotes": [],
        "references": []
    },
    "id_table_16": {
        "caption": "",
        "table": "S4.T4.4.1",
        "footnotes": [],
        "references": []
    }
}