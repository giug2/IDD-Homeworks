{
    "id_table_1": {
        "caption": "Table 1.  Statistics of the L-Eval  (An et al . ,  2023 )  Dataset.",
        "table": "S2.T1.1",
        "footnotes": [],
        "references": [
            "We observe that both token recomputation and KV offload adhere to the standard LLM inference process, maintaining the LLM forward pass or  KV cache   as is  to manage and restore historical contextual states. Consequently, these approaches occupy the  extreme ends of the design spectrum, solely utilizing GPU computational or I/O  capabilities, thus leaving system resources underutilized.  In this paper, we introduce  HCache , a novel LLM state restoration method  to break this conventional wisdom by using GPU computational and IO resources simultaneously in a low overhead manner compared with existing methods (see Figure  1 ).",
            "In this section, we first provide the background on the transformer architecture  ( 2.1 ) and LLM serving systems ( 2.2 ).  Then, we show the examples, characteristics, and overhead of stateful LLM  applications in  2.3  and  2.4 .",
            "L-Eval  (An et al . ,  2023 )  is an open-sourced dataset tailored for long-context LLM evaluation.  It contains 20 sub-tasks, including long context Q&A, few-shot examples helped reasoning, and deducing the output of a piece of code. In Table  1 ,  we present the statistics of three representative sub-tasks in L-Eval.  These traces exhibit bimodal characteristics  the average length of context contents  can extend up to 16K. In contrast, the lengths of instructions and outputs typically remain below 100.",
            "To accelerate the state restoration speed,  we aim to seek a new approach that  simultaneously utilizes the computation capability and transmission bandwidth,  while reducing unnecessary data transfers and recomputations .  Our initial attempt is a  simple combination of recomputation and KV offload. Specifically, the contextual tokens  are divided into two parts, and we use recomputation and KV offload to restore them concurrently;  hence, the computation and transmission resources can be utilized in parallel.  However, this naive hybrid approach still keeps the LLM forward pass or KV cache as is,  which does not fundamentally reduce the computation and IO overhead, resulting in suboptimal performance (see  6.3.1 ).",
            "To address the above challenges, we further enhance  HCache  with bubble-free restoration and efficient storage management.  Figure  7  depicts the overall architecture of  HCache  and  how it interacts with an LLM inference engine.  When a user request arrives, the inference engine first decides whether the requests history states should be restored.  If so, the inference engine utilizes the bubble-free restoration scheduler ( 4.1 )  to generate an optimal restoration scheme  to restore its  KV cache  (to solve  C1 ). With the restored  KV cache ,  the user request is further processed with prompt prefilling and token generation  to generate an answer. During the prefilling and token generation phase, the storage manager ( 4.2 ) efficiently  saves the newly computed  hidden states  to host storage (to solve  C2 ). Since  HCache  focuses on state restoration speed, we do not cache and reuse  KV cache  in GPU.",
            "The bubble-free restoration scheduler combines different restoration methods dynamically to eliminate  pipeline bubbles. Specifically, the scheduler partitions a models state across layers, with most states  be managed via  hidden states , while using a resource-complementary method (token recomputation or KV offload)  for other states to fill in the bubble.  For example, bubbles exist in transmission when the computation speed is slow, so we offload part  of the models state as  KV cache  to prevent the need for computation.   HCache  determines an optimal partition scheme via offline profiling of the  hardware characteristics. We further illustrate how a models state should be partitioned ( 4.1.1 )  and the detailed partition algorithm ( 4.1.2 ).",
            "The storage manager is tasked with managing contextual states in host storage.  We have adopted a storage format optimized for state restoration, as reducing TTFT  is our primary design goal. Additionally, we introduce a two-stage state-saving mechanism  to mitigate the overhead associated with saving newly generated states.  Note that with our state partition algorithm in  4.1.2 ,  the contextual states at each model layer are stored either in  hidden states ,  KV cache , or original tokens.  Here, we focus on the management of  hidden states  since the  KV cache  can be managed similarly.",
            "We evaluate the overall performance of  HCache  against baseline methods in the  long-context applications setting with the L-Eval trace. Since the GPU HBM can  only serve 1-3 long-context requests, we evaluate different models with a batch  size equal to 1. Figure  10  shows the TTFT of three representative  subtasks and the sampled 200 requests from the trace (i.e., mixed).  HCache  can  achieve 1.62-1.93  \\times   speed up for TTFT compared over KV offload and 2.66-5.73  \\times   over token recomputation.  It is worth noticing that these tasks history length spans within a large range from 4K to 16K, demonstrating  HCache s  excellent scalability.",
            "The restoration phase of  HCache  includes the projection of  hidden states  to  KV cache , so its performance  is sensitive to the computation power of GPU. We evaluate the restoration speed of  HCache  and baselines on various GPU platforms. We use host DRAM as the storage backend so that the transmission speed is not the bottleneck. Figures  11 a-c shows that  HCache  outperforms KV offload by 1.331.81 and Recomputation by 5.049.05 in restoration speed across different platforms.  Platforms with low computation capability are unfriendly to  HCache , such as running a 7B model on an A30 GPU.  In this case,  HCache  computation is longer than transmission and makes the speed up less than 2  \\times  .  Even so,  HCache  still restores 1.33  \\times   faster than KV offload.  On platforms with faster computation speeds,  HCache  shows improved performance, achieving a 1.66  \\times   increase on A100 GPU  and a 1.77  \\times   increase on the H800 GPU for 13B models compared to KV offload.  Compared to token recomputation,  HCache  achieves 5.04-9.05  \\times   speed up on different GPUs. This is attributed to the 6  \\times   theoretical speedup against token recomputation by skipping the attention and FFN modules.",
            "Figures  11 d-f present the sensitivity evaluation results related to transmission speed.  In terms of the restoration speed across various disk configurations,   HCache  significantly outperforms KV offload, delivering a 1.7 to 2.6  \\times   improvement  and surpasses recomputation with a 2.3-6.1  \\times   enhancement.  For platforms with fewer disks, IO transmission can be slower than computation tasks;  based on our theoretical analysis,  HCache  can perform at least  2  2\\times 2   better, and  the bubble-free scheduler can bring extra speed up. For example, on the platform with one SSD per GPU,  the overall improvement of  HCache  over KV offload is 2.09-2.66  \\times  .  For platforms with more disks, instead, the recomputation operation in  HCache  can be longer than the transmission in this setting, so the speed up can be less than 2  \\times  . However, the recomputation overhead is much lower than  token recomputation. The overall speedup is 1.33-1.81  \\times   on 7B-30B models compared with KV offload and 5.46-6.08  \\times   compared with recomputation from the token.",
            "We further investigate the impact of sequence length on  HCache s restoration speed.  Following the same configuration as the overall experiment, we test different models restoration speeds on A100 GPUs  with 4 SSDs by varying the history length.  As shown in Figures  11 g-i, token recomputation can not scale well with the increasing history length; the restoration speed of a 7B model drops by 28% as the history length increases from 1K to 16K.  Token recomputations quadratic complexity attention mechanism incurs a high overhead in contexts with a long history.  The  KV cache  can scale well with different history lengths because its transmission size is proportional to the number of tokens.  HCache  also scales well with different history lengths for two reasons. First, the transmission size of  hidden states  and recomputation cost from it to  KV cache  are all proportional to the number of tokens (see   3.2 ). Second,  HCache  has the bubble-free restoration scheduler and may opportunistically combine the usage of the token recomputation to fill in the bubble in the short context. With long historical contexts, the scheduler detects that token computation is very expensive and falls back to the  HCache -only approach.",
            "The results are shown in Figure  12 .  Without  hidden states , Naive hybrid is the best method that uses both compute and transmission resources.   HCache  outperforms this approach by 1.28-1.42  \\times  . Both methods have no bubbles,  so the performance gain of  HCache  comes from the  hidden states  as it has theoretically lower computational and IO overhead.",
            "We compare different methods to partition the state of the model. We measure the restoration speed  of the 13B model using history contexts with 1024 tokens. We run the model on one A100 with one SSD.  With the layer-wise partition, our algorithm produces a scheme that uses  hidden states  for 31 layers  and token recomputation for the remaining 9 layers. Instead, a naive token-wise partition uses  hidden states   for 794 tokens and token recomputation for the remaining 230 tokens. A round-up optimization over the token-wise  partition uses the nearest optimal size (i.e., 768) to manage tokens with  HCache .  As Figure  13  shows, the restoration speed with the naive token-wise partition is  12% slower. While the round-up optimization can improve performance by issuing a more performant cuBLAS kernel,  it is still 7% slower than layer-wise partition because of the unbalanced workload in computation and transmission.  As auxiliary information, we also report the restoration time of GEMM operation in one layer with varying numbers of  tokens (see Figure  13 b).",
            "To evaluate the effectiveness of the two-stage  hidden states  saving strategy. We implement a variant of  HCache , which directly saves  hidden states  to SSDs (i.e., DirectIO in Figure  14 ).  We test the TBT with different numbers of sequences in the decoding batch and set the history length of each to 512.",
            "The results are shown in Figure  15 . With a uniform arrival pattern, the cache hit ratio is 15%, and the TTFT is similar to the overall experiment, which does not incorporate caching. In the uniform setting,  HCache  is 1.67  \\times   faster than KV offloading. As the skewness increases from uniform to   = 2.0  2.0 \\alpha=2.0 italic_ = 2.0 , the cache hit ratio rises to 94%, allowing the GPU cache to significantly reduce TTFT by 3.76  \\times   to 10.03  \\times  . A high cache hit ratio diminishes the performance gains of  HCache  due to fewer state restoration events. Nevertheless,  HCache  remains 1.15  \\times   faster than KV offloading and 1.98  \\times   faster than recomputation in high-skewness workloads."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.  Hardware Characteristics of Different Platforms.    Note that    indicates the FLOPS of FP16 operations.",
        "table": "S6.T2.1",
        "footnotes": [],
        "references": [
            "In this section, we first provide the background on the transformer architecture  ( 2.1 ) and LLM serving systems ( 2.2 ).  Then, we show the examples, characteristics, and overhead of stateful LLM  applications in  2.3  and  2.4 .",
            "Most LLMs today adopt the transformer architecture as their building block. As shown in Figure  2 ,  LLM has N repetitive transformer layers,  each comprising two major components: the attention module and the FFN (feed-forward network).",
            "Recall from Figure  2  that the  hidden states  represent the input to each transformer layer.  Within each transformer layer, the KV tensor pairs of each token are derived directly from the   hidden states  by performing the projection operation in the attention module. Specifically,  we can use the following equations to restore the  KV cache  for token  i i i italic_i :",
            "Conclusion.  As a whole, the transmission size of  HCache  is always 2  \\times   less than  that of transmitting the  KV cache ,  while the recomputation of  KV cache  from  hidden states  is at least 6  \\times   faster than token recomputation.  This theoretical support makes  HCache  beneficial on mainstream platforms (see  6.2 ).",
            "To address the above challenges, we further enhance  HCache  with bubble-free restoration and efficient storage management.  Figure  7  depicts the overall architecture of  HCache  and  how it interacts with an LLM inference engine.  When a user request arrives, the inference engine first decides whether the requests history states should be restored.  If so, the inference engine utilizes the bubble-free restoration scheduler ( 4.1 )  to generate an optimal restoration scheme  to restore its  KV cache  (to solve  C1 ). With the restored  KV cache ,  the user request is further processed with prompt prefilling and token generation  to generate an answer. During the prefilling and token generation phase, the storage manager ( 4.2 ) efficiently  saves the newly computed  hidden states  to host storage (to solve  C2 ). Since  HCache  focuses on state restoration speed, we do not cache and reuse  KV cache  in GPU.",
            "The bubble-free restoration scheduler combines different restoration methods dynamically to eliminate  pipeline bubbles. Specifically, the scheduler partitions a models state across layers, with most states  be managed via  hidden states , while using a resource-complementary method (token recomputation or KV offload)  for other states to fill in the bubble.  For example, bubbles exist in transmission when the computation speed is slow, so we offload part  of the models state as  KV cache  to prevent the need for computation.   HCache  determines an optimal partition scheme via offline profiling of the  hardware characteristics. We further illustrate how a models state should be partitioned ( 4.1.1 )  and the detailed partition algorithm ( 4.1.2 ).",
            "The storage manager is tasked with managing contextual states in host storage.  We have adopted a storage format optimized for state restoration, as reducing TTFT  is our primary design goal. Additionally, we introduce a two-stage state-saving mechanism  to mitigate the overhead associated with saving newly generated states.  Note that with our state partition algorithm in  4.1.2 ,  the contextual states at each model layer are stored either in  hidden states ,  KV cache , or original tokens.  Here, we focus on the management of  hidden states  since the  KV cache  can be managed similarly.",
            "Testbed.   Unless otherwise stated, all our experiments are conducted on a server equipped with  4  \\times   A100-40G SXM4 connected via NVLink. The host has 2  \\times   AMD EPYC 7642  CPUs, 256G DDR4 memory, and  4  4\\times 4   Samsung PM9A3 4TB enterprise SSDs. In our sensitivity experiments,  we change the hardware configuration by using cloud servers equipped with different GPUs.  For these cloud servers, we directly use the host DRAM as the storage backend.  The hardware characteristics of these GPUs are shown in Table  2 .",
            "Models : We use Llama2-7B, Llama2-13B, OPT-30B as our test models. We  expand the maximum context length of these models to 16K to accommodate L-Eval benchmarks and long conversation history.  For Llama2-7B/13B, we use a single A100 GPU to serve them. For OPT-30B, we run it  on 4 A100 GPUs with tensor parallelism unless otherwise stated.  We use the ShareGPT4 and L-Eval described in  2.3  as our test traces to imitate the real-world LLM use cases like multi-round conversation chatbot or RAG application.",
            "We further investigate the impact of sequence length on  HCache s restoration speed.  Following the same configuration as the overall experiment, we test different models restoration speeds on A100 GPUs  with 4 SSDs by varying the history length.  As shown in Figures  11 g-i, token recomputation can not scale well with the increasing history length; the restoration speed of a 7B model drops by 28% as the history length increases from 1K to 16K.  Token recomputations quadratic complexity attention mechanism incurs a high overhead in contexts with a long history.  The  KV cache  can scale well with different history lengths because its transmission size is proportional to the number of tokens.  HCache  also scales well with different history lengths for two reasons. First, the transmission size of  hidden states  and recomputation cost from it to  KV cache  are all proportional to the number of tokens (see   3.2 ). Second,  HCache  has the bubble-free restoration scheduler and may opportunistically combine the usage of the token recomputation to fill in the bubble in the short context. With long historical contexts, the scheduler detects that token computation is very expensive and falls back to the  HCache -only approach.",
            "The results are shown in Figure  12 .  Without  hidden states , Naive hybrid is the best method that uses both compute and transmission resources.   HCache  outperforms this approach by 1.28-1.42  \\times  . Both methods have no bubbles,  so the performance gain of  HCache  comes from the  hidden states  as it has theoretically lower computational and IO overhead."
        ]
    },
    "id_table_3": {
        "caption": "Table 3.  Scheduling Results and Storage Cost.   H, KV, and RE indicate  hidden states ,  KV cache , and recomputation, respectively.",
        "table": "S6.T3.1",
        "footnotes": [],
        "references": [
            "In this section, we first provide the background on the transformer architecture  ( 2.1 ) and LLM serving systems ( 2.2 ).  Then, we show the examples, characteristics, and overhead of stateful LLM  applications in  2.3  and  2.4 .",
            "A representative dataset of multi-round conversation is ShareGPT4  (sha,  2024 ) , a trace of human conversations with GPT-4.  Figure  3 a shows the average length of new prompt tokens and  output tokens in one round. The average input length of each round is 66.8 tokens, and the output is 358.8 tokens.  Though one rounds input and output length is relatively short, they gradually accumulate in the conversations history as  the dialogue progresses. Figure  3 b depicts the CDF of the length of history tokens (truncated at 16K), and we can see that  the length of half of the conversations is over 2.5K.",
            "To accelerate the state restoration speed,  we aim to seek a new approach that  simultaneously utilizes the computation capability and transmission bandwidth,  while reducing unnecessary data transfers and recomputations .  Our initial attempt is a  simple combination of recomputation and KV offload. Specifically, the contextual tokens  are divided into two parts, and we use recomputation and KV offload to restore them concurrently;  hence, the computation and transmission resources can be utilized in parallel.  However, this naive hybrid approach still keeps the LLM forward pass or KV cache as is,  which does not fundamentally reduce the computation and IO overhead, resulting in suboptimal performance (see  6.3.1 ).",
            "Models : We use Llama2-7B, Llama2-13B, OPT-30B as our test models. We  expand the maximum context length of these models to 16K to accommodate L-Eval benchmarks and long conversation history.  For Llama2-7B/13B, we use a single A100 GPU to serve them. For OPT-30B, we run it  on 4 A100 GPUs with tensor parallelism unless otherwise stated.  We use the ShareGPT4 and L-Eval described in  2.3  as our test traces to imitate the real-world LLM use cases like multi-round conversation chatbot or RAG application.",
            "In Table  3 , we report the schedule results of  HCache , in terms of how  model layers are managed and the per-token storage space consumption. We also  report the storage cost of KV offload as a comparison.  Specifically, the 7B model on one A100 has balanced speed for recomputation from   hidden states  into  KV cache  and the transmission of  hidden states , so we use  hidden states   for 31 layers and only transmit the  KV cache  for one layer to fill in the bubble.  For the 13B and 30B model,  HCache  use  hidden states  for more than 80% layers,  while the rest of the layers are managed via different resource-complementary methods.  Regarding space consumption, the size of  hidden states  for one token is half that of  KV cache .  Combined with the zero-bubble scheduler, some layers may not even need to be stored because they can be  recomputed from tokens. As a result,  HCache s per token storage space is 1.92-2.40  \\times   lower than KV offload. To achieve a balanced speed between computation and transmission using only hidden states, approximately 24GB/s, 21GB/s, and 37GB/s of storage bandwidth are needed for the 7B, 13B, and 30B models, respectively.",
            "We further investigate the impact of sequence length on  HCache s restoration speed.  Following the same configuration as the overall experiment, we test different models restoration speeds on A100 GPUs  with 4 SSDs by varying the history length.  As shown in Figures  11 g-i, token recomputation can not scale well with the increasing history length; the restoration speed of a 7B model drops by 28% as the history length increases from 1K to 16K.  Token recomputations quadratic complexity attention mechanism incurs a high overhead in contexts with a long history.  The  KV cache  can scale well with different history lengths because its transmission size is proportional to the number of tokens.  HCache  also scales well with different history lengths for two reasons. First, the transmission size of  hidden states  and recomputation cost from it to  KV cache  are all proportional to the number of tokens (see   3.2 ). Second,  HCache  has the bubble-free restoration scheduler and may opportunistically combine the usage of the token recomputation to fill in the bubble in the short context. With long historical contexts, the scheduler detects that token computation is very expensive and falls back to the  HCache -only approach.",
            "We compare different methods to partition the state of the model. We measure the restoration speed  of the 13B model using history contexts with 1024 tokens. We run the model on one A100 with one SSD.  With the layer-wise partition, our algorithm produces a scheme that uses  hidden states  for 31 layers  and token recomputation for the remaining 9 layers. Instead, a naive token-wise partition uses  hidden states   for 794 tokens and token recomputation for the remaining 230 tokens. A round-up optimization over the token-wise  partition uses the nearest optimal size (i.e., 768) to manage tokens with  HCache .  As Figure  13  shows, the restoration speed with the naive token-wise partition is  12% slower. While the round-up optimization can improve performance by issuing a more performant cuBLAS kernel,  it is still 7% slower than layer-wise partition because of the unbalanced workload in computation and transmission.  As auxiliary information, we also report the restoration time of GEMM operation in one layer with varying numbers of  tokens (see Figure  13 b)."
        ]
    },
    "global_footnotes": [
        "A multiply-add operator is regarded as 2 float point operations."
    ]
}