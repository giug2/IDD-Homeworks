{
    "id_table_1": {
        "caption": "Table 1:  Overall performance of FactMM-RAG and baselines under the multimodal retrieval-augmentation setting. Models are evaluated by textual similarity and factual similarity between generated and reference reports. FactMM-RAG outperforms the best baseline with p-value < 0.05.",
        "table": "S4.T1.1.1",
        "footnotes": [],
        "references": [
            "In this section, we present the overall methodology of FactMM-RAG.  We first detail the training procedure of our fact-aware medical multimodal retriever in Section  3.1 .  We then provide the pipeline for retrieval-augmented radiology report generation with our multimodal retriever in section  3.2 . The overview is illustrated in Figure  1 .",
            "In this section, we present our experimental results.  We first evaluate the overall performance between different retrievers under two settings in section  5.1 .  Next, we discuss the ablation studies in section  5.2 .  We then explore the fact-aware capability of our retriever in section  3  and section  5.4 .  Lastly, we show the superiority of our retriever through a case study in section  5.5 .",
            "The results of our fact-aware RAG system are shown in Table  1 .  In MIMIC-CXR, FactMM-RAG outperforms state-of-the-art retrievers by a significant margin, up to 6.5% in F1CheXbert and 2% in F1RadGraph.  In the CheXpert zero-shot evaluation, FactMM-RAG outperforms state-of-the-art retrievers by 2% and 1.2% in these two metrics, indicating our retrievers generalization capability compared to other models.      Besides, we can observe that adopting the baseline retrievers on top of multimodal foundation models only yields marginal gains compared to the finetuning of foundation model generation without retrieval-augmentation.  This shows that reports retrieved by baseline retrievers are factually-inferior to those from our retriever, potentially passing misleading information that prevents the foundation model from generating factual reports.       Specifically, compared to the retriever Med-MARVEL, we also observe factual-correctness performance gain based on two clinical metrics.  Both use the same universal encoder backbone, but FactMM-RAG benefits from the injected factual medical knowledge, allowing it to search for the most similar and factually correct reports, thereby assisting the multimodal foundation model in generating more accurate reports.",
            "The factual similarity threshold in Equation  1  plays a critical role in controlling the fact-awareness of our multimodal retriever.  We examine the performance of FactMM-RAG under different thresholds, as shown in Figure  2 .  Not only utilizing F1RadGraph thresholds, we also employ F1CheXbert to curate additional thresholds from the reports diagnostic labels to mine report pairs.      Under the same F1CheXbert threshold for mining report pairs, we observe that an increase in the F1RadGraph threshold correlates with an improvement in factual performance.  However, adopting stricter thresholds for identifying report pairs does not yield further improvements and reaches saturation.  After calculating the average number of report pairs per query, we find that high thresholds can exclude many relevant report pairs, as shown in Figure  3 . This exclusion results in the potential loss of factually useful pairs, thereby hindering the training of our multimodal retriever driven by additional factual medical knowledge.      Rather than relying on diagnostic labels from CheXbert to identify high-quality report pairs, Figure  2(a)  demonstrates that the F1RadGraph threshold alone can also effectively mine factual report pairs for training our multimodal retriever.  As the F1RadGraph threshold increases, FactMM-RAG even matches the performance under high threshold settings in Figure  2(d) .  This signifies that employing our training strategy with curated factual query-report pairs still imposes useful supervision signals without relying on explicit diagnostic label guidance."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Ablation study of FactMM-RAG including multimodal retrieval and backbone variation.",
        "table": "S5.T2.4.4",
        "footnotes": [],
        "references": [
            "The rest of this paper is organized as follows.  We review related work in in Section  2 . We discuss the pipeline of FactMM-RAG in Sections  3 . Section  4  and  5  discuss our experimental setup and results.",
            "In this section, we present the overall methodology of FactMM-RAG.  We first detail the training procedure of our fact-aware medical multimodal retriever in Section  3.1 .  We then provide the pipeline for retrieval-augmented radiology report generation with our multimodal retriever in section  3.2 . The overview is illustrated in Figure  1 .",
            "In this section, we present our experimental results.  We first evaluate the overall performance between different retrievers under two settings in section  5.1 .  Next, we discuss the ablation studies in section  5.2 .  We then explore the fact-aware capability of our retriever in section  3  and section  5.4 .  Lastly, we show the superiority of our retriever through a case study in section  5.5 .",
            "Multimodal Retrieval.  Instead of relying on the multimodal foundation model to generate reports, we also evaluate the performance of the multimodal retrievers by directly encoding radiology images from the testing corpus and searching for the closest report from the training corpus for comparison with ground-truth reports.  Table  2  shows that our retriever also achieves the best factual retrieval performance compared to other baselines under this setting across two datasets.  This demonstrates that training the multimodal retriever with mined factually-informed report pairs can enhance its radiology image understanding capabilities and directly align it with precise reports.        Backbone Variation.  We also investigate the impact of different retriever and foundation model backbones on radiology report generation in Table  2 .  We initialize our retriever model from two checkpoints: WebQA and ClueWeb in  (Zhou et al.,  2024 ) .  We observe that the ClueWeb checkpoint provides a marginal gain compared to the WebQA checkpoint.  This can be attributed to the larger scale of the ClueWeb dataset used for pretraining.  We also utilize Med-MARVEL as our retriever backbone, which exhibits similar performance to other backbones after training.  This implies that even if our retriever is initialized with a backbone from a general domain, our factually-informed training strategy enables it to fully leverage medical knowledge and quickly adapt to the radiology-specific domain without degrading performance.",
            "The factual similarity threshold in Equation  1  plays a critical role in controlling the fact-awareness of our multimodal retriever.  We examine the performance of FactMM-RAG under different thresholds, as shown in Figure  2 .  Not only utilizing F1RadGraph thresholds, we also employ F1CheXbert to curate additional thresholds from the reports diagnostic labels to mine report pairs.      Under the same F1CheXbert threshold for mining report pairs, we observe that an increase in the F1RadGraph threshold correlates with an improvement in factual performance.  However, adopting stricter thresholds for identifying report pairs does not yield further improvements and reaches saturation.  After calculating the average number of report pairs per query, we find that high thresholds can exclude many relevant report pairs, as shown in Figure  3 . This exclusion results in the potential loss of factually useful pairs, thereby hindering the training of our multimodal retriever driven by additional factual medical knowledge.      Rather than relying on diagnostic labels from CheXbert to identify high-quality report pairs, Figure  2(a)  demonstrates that the F1RadGraph threshold alone can also effectively mine factual report pairs for training our multimodal retriever.  As the F1RadGraph threshold increases, FactMM-RAG even matches the performance under high threshold settings in Figure  2(d) .  This signifies that employing our training strategy with curated factual query-report pairs still imposes useful supervision signals without relying on explicit diagnostic label guidance."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  One case study from MIMIC-CXR.  Cyan  text indicates radiological consistency with the ground-truth report.  Orange  text highlights extra accurate details provided by FactMM-RAG compared to Med-MARVEL.  Red  text denotes observations missing in Med-MARVEL.",
        "table": "S5.T3.2.2",
        "footnotes": [],
        "references": [
            "The rest of this paper is organized as follows.  We review related work in in Section  2 . We discuss the pipeline of FactMM-RAG in Sections  3 . Section  4  and  5  discuss our experimental setup and results.",
            "In this section, we present the overall methodology of FactMM-RAG.  We first detail the training procedure of our fact-aware medical multimodal retriever in Section  3.1 .  We then provide the pipeline for retrieval-augmented radiology report generation with our multimodal retriever in section  3.2 . The overview is illustrated in Figure  1 .",
            "In this section, we present our experimental results.  We first evaluate the overall performance between different retrievers under two settings in section  5.1 .  Next, we discuss the ablation studies in section  5.2 .  We then explore the fact-aware capability of our retriever in section  3  and section  5.4 .  Lastly, we show the superiority of our retriever through a case study in section  5.5 .",
            "The factual similarity threshold in Equation  1  plays a critical role in controlling the fact-awareness of our multimodal retriever.  We examine the performance of FactMM-RAG under different thresholds, as shown in Figure  2 .  Not only utilizing F1RadGraph thresholds, we also employ F1CheXbert to curate additional thresholds from the reports diagnostic labels to mine report pairs.      Under the same F1CheXbert threshold for mining report pairs, we observe that an increase in the F1RadGraph threshold correlates with an improvement in factual performance.  However, adopting stricter thresholds for identifying report pairs does not yield further improvements and reaches saturation.  After calculating the average number of report pairs per query, we find that high thresholds can exclude many relevant report pairs, as shown in Figure  3 . This exclusion results in the potential loss of factually useful pairs, thereby hindering the training of our multimodal retriever driven by additional factual medical knowledge.      Rather than relying on diagnostic labels from CheXbert to identify high-quality report pairs, Figure  2(a)  demonstrates that the F1RadGraph threshold alone can also effectively mine factual report pairs for training our multimodal retriever.  As the F1RadGraph threshold increases, FactMM-RAG even matches the performance under high threshold settings in Figure  2(d) .  This signifies that employing our training strategy with curated factual query-report pairs still imposes useful supervision signals without relying on explicit diagnostic label guidance.",
            "In this section, we present two examples from MIMIC-CXR to qualitatively analyze our retrievers fact-aware capability, as illustrated in Table  3 .  In the first example, we observe that FactMM-RAG provides symptom observations consistent with the ground-truth report and generates more accurate factual details compared to Med-MARVEL, e.g., post median sternotomy, atelectasis, not pneumothorax;  In the second example, we further observe that although both retrievers generate reports with diagnostic labels matching the ground-truth report, FactMM-RAG provides additional details compared to Med-MARVEL, such as pulmonary vasculature is normal, no acute osseous abnormalities.  These characteristics confirm that adopting our fact-aware retriever can assist multimodal foundation models in generating more accurate radiology reports."
        ]
    },
    "global_footnotes": [
        "Code will be public upon acceptance.",
        "These authors contributed equally to this work."
    ]
}