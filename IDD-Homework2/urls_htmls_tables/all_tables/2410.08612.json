{
    "id_table_1": {
        "caption": "Table 1 :  Image-to-Image Quantitative Metrics",
        "table": "S5.T1.2",
        "footnotes": [],
        "references": [
            "In this section, we provide a detailed description of the proposed Synth-SONAR framework for generating sonar images. The overall architecture of the model is depicted in Fig.  1 , which consists of three key phases. Phase 1 is the  Data Acquisition Phase , as explained in Section  3.1 . It entails the collection of real-world, CAD-simulated, and Gen-AI-generated images. Phases 2 and 3 utilize  text-conditioned dual diffusion models and GPT-based prompting  to synthesize both coarse\" and fine\" grained sonar images. In particular, Phase-2 customizes pre-trained diffusion models for generating sonar images as described in Section  3.2 . Whereas, Phase-3 fine-tunes and generalizes the diffusion models for generating fine\"-grained sonar images as described in Section  3.3 .",
            "Underwater sonar imagery is a critical domain, wherein the data collection and processing of such a large training dataset is both expensive and challenging. Some of the common ways are to leverage the publicly available datasets e.g. Seabed Objects KLSG dataset, SCTD (see Section  4.1.1 ) or the CAD-based simulated dataset such as S3 simulator data (see Section  4.1.2 ). Further, we advance the sonar image synthesis via Generative AI techniques such as style injection (see Section  3.1.1 ), as explained in the forthcoming section.",
            "The generated images from Phase-1: Style Injection (Section 5.1.2) are quantitatively evaluated using FID, SSIM, PSNR, and IS scores, as shown in Table  1 . Lower FID scores and higher SSIM and PSNR values indicate that the style injection method successfully maintains image quality, while the IS scores reflect the diversity of the generated images. From the results, the FID score is lowest for the Seafloor class (1.12), indicating the highest fidelity to the real data, while the Plane class has the lowest PSNR (15.022), suggesting relatively lower noise in image reconstruction. The SSIM values show that structural similarity is strongest for the Seafloor class (0.452), and the average across all classes indicates that the style injection improves both image quality and consistency. The IS scores across all classes are fairly similar, but they still indicate a moderate level of diversity in the generated images. These results suggest that improvements in both image quality and diversity are due to effective style injection, which introduces stylistic variations while preserving key structural elements.",
            "We conducted an in-depth analysis to regulate the results generated by the trained model. The style-injection ablation study is extensively discussed in Section  5.3.1 , while the fine-tuning ablation study is detailed in Section  5.3.2 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Descriptions generated by the GPT in Phase 3. In this phase, images produced from Phase 2 are described using a domain-specific prompt. The table includes four columns: (1)  Image : the generated image from Phase 2, (2)  Prompt : the domain-specific prompt used to generate the descriptions, (3)  Low-Level Description : a detailed description focusing on simple, observable details and objects in the image, and (4)  High-Level Description : an interpretive description providing a broader understanding of the scene depicted in the image. The abbreviations used in the prompt (e.g., PL__*, SH__*, CYM*, ASF*, TCM*, AS__*, AP__*, SEF*) represent various objects and background elements in the image, with numbers indicating different instances.",
        "table": "S5.T2.1",
        "footnotes": [],
        "references": [
            "In this section, we provide a detailed description of the proposed Synth-SONAR framework for generating sonar images. The overall architecture of the model is depicted in Fig.  1 , which consists of three key phases. Phase 1 is the  Data Acquisition Phase , as explained in Section  3.1 . It entails the collection of real-world, CAD-simulated, and Gen-AI-generated images. Phases 2 and 3 utilize  text-conditioned dual diffusion models and GPT-based prompting  to synthesize both coarse\" and fine\" grained sonar images. In particular, Phase-2 customizes pre-trained diffusion models for generating sonar images as described in Section  3.2 . Whereas, Phase-3 fine-tunes and generalizes the diffusion models for generating fine\"-grained sonar images as described in Section  3.3 .",
            "Underwater sonar imagery is a critical domain, wherein the data collection and processing of such a large training dataset is both expensive and challenging. Some of the common ways are to leverage the publicly available datasets e.g. Seabed Objects KLSG dataset, SCTD (see Section  4.1.1 ) or the CAD-based simulated dataset such as S3 simulator data (see Section  4.1.2 ). Further, we advance the sonar image synthesis via Generative AI techniques such as style injection (see Section  3.1.1 ), as explained in the forthcoming section.",
            "Style injection is a technique in computer visions image-to-image tasks, that combines content and style features to generate a new image, where the objective is to transform the content image (in our case, it is generated by the prompts from GPT) by infusing it with the stylistic elements of another image while preserving the original structure  [ 7 ] . In this work, style injection is utilized to add more diversity to the real sonar data and to increase the availability of sonar data. Refering to Fig.  2 , we leverage the generative capability of a pre-trained large-scale model to generate content images and transfer sonar style information to the generated content images using a pre-trained stable diffusion model, thereby resolving the issue of the traditional data-collection process.",
            "where    \\gamma italic_  is a blending ratio that controls the amount of style injected. For sonar-specific style injection, referring to Fig.  2 , the latent noise representations of the content and style images are denoted by  z c superscript z c z^{c} italic_z start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT  and  z s superscript z s z^{s} italic_z start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT , respectively. These representations are obtained through the DDIM inversion process, which reconstructs both the content and style images into Gaussian noise at time step  t = T t T t=T italic_t = italic_T . During the DDIM inversion process, we collect the query features  Q t c subscript superscript Q c t Q^{c}_{t} italic_Q start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  from the content and the key-value pairs  K t s subscript superscript K s t K^{s}_{t} italic_K start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ,  V t s subscript superscript V s t V^{s}_{t} italic_V start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  from the style at each time step  t t t italic_t . Once the inversion is completed, we initialize the stylized latent noise  z T c  s subscript superscript z c s T z^{cs}_{T} italic_z start_POSTSUPERSCRIPT italic_c italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  by copying the content latent noise  z T c subscript superscript z c T z^{c}_{T} italic_z start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT . To inject the style into the content, we blend the content and stylized queries using a blending ratio    \\gamma italic_  from equation  6 . The blended query  Q ~ t c  s subscript superscript ~ Q c s t \\tilde{Q}^{cs}_{t} over~ start_ARG italic_Q end_ARG start_POSTSUPERSCRIPT italic_c italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  is then passed through the attention mechanism along with the styles key and value features. By adjusting    \\gamma italic_ , we control the degree of style transfer, where a higher    \\gamma italic_  retains more of the original content features, and a lower    \\gamma italic_  strengthens the influence of the style features. This approach allows for precise control over the stylistic outcome, ensuring a smooth transition between content preservation and style injection.",
            "Our objective is to generate a sequence of sonar images that correspond to the given text conditioning. In the case of text-to-image generation, referring to the equation  2 , the conditioning variable    \\psi italic_  corresponds to the text features. The cross-attention mechanism aligns the textual description with the visual features to generate an image that matches the text prompt.",
            "The proposed hierarchical framework incorporates a Visual Language Model (VLM) to generate descriptions that enhance the image generation process. As detailed in Table  2 , the VLM supports GPT to provide both low-level descriptions (e.g., shape, size, texture of objects) and high-level descriptions (e.g., object relationships and interactions with the environment) to refine the stable diffusion model.",
            "We conducted an in-depth analysis to regulate the results generated by the trained model. The style-injection ablation study is extensively discussed in Section  5.3.1 , while the fine-tuning ablation study is detailed in Section  5.3.2 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Test Accuracy on Real Dataset",
        "table": "S5.T3.2",
        "footnotes": [],
        "references": [
            "In this section, we provide a detailed description of the proposed Synth-SONAR framework for generating sonar images. The overall architecture of the model is depicted in Fig.  1 , which consists of three key phases. Phase 1 is the  Data Acquisition Phase , as explained in Section  3.1 . It entails the collection of real-world, CAD-simulated, and Gen-AI-generated images. Phases 2 and 3 utilize  text-conditioned dual diffusion models and GPT-based prompting  to synthesize both coarse\" and fine\" grained sonar images. In particular, Phase-2 customizes pre-trained diffusion models for generating sonar images as described in Section  3.2 . Whereas, Phase-3 fine-tunes and generalizes the diffusion models for generating fine\"-grained sonar images as described in Section  3.3 .",
            "Underwater sonar imagery is a critical domain, wherein the data collection and processing of such a large training dataset is both expensive and challenging. Some of the common ways are to leverage the publicly available datasets e.g. Seabed Objects KLSG dataset, SCTD (see Section  4.1.1 ) or the CAD-based simulated dataset such as S3 simulator data (see Section  4.1.2 ). Further, we advance the sonar image synthesis via Generative AI techniques such as style injection (see Section  3.1.1 ), as explained in the forthcoming section.",
            "In this work, publicly available datasets, specifically the Seabed Objects KLSG  [ 16 ]  and the sonar Common Target Detection Dataset (SCTD)  [ 53 ] , are utilized. The Seabed Objects KLSG dataset   [ 16 ]  includes 1,190 side-scan sonar images, featuring 385 shipwrecks, 36 drowning victims, 62 planes, 129 mines, and 578 seafloor images. Collected over ten years with the help of commercial sonar suppliers like Lcocean, Klein Martin, and EdgeTech, a subset of 1,171 images (excluding mines) is publicly available for academic research. The sonar Common Target Detection Dataset (SCTD) 1.0   [ 53 ] , developed by multiple universities, contains 596 images across 3 classes. Some sample images of the publicly available data are shown in Fig.  3 .",
            "To support the quantitative measure of sonar image synthesis, a classification task is carried out using the generated synthetic data. In particular, we developed classification models leveraging transfer learning on backbone models such as VGG16, ResNet50, DenseNet121, MobileNetV2, Xception, and InceptionResNetV2 With the real, synthetic, and real + synthetic dataset and the performance of the developed image classification model is assessed with real sonar dataset. Refer to Table  3  for the test accuracies of various models on real dataset. The model developed with fully synthetic dataset gave a maximum accuracy of 79%, using the MobileNetV2 as base model. When using only real data, the DenseNet121 model reached the highest accuracy of 96%, and when using the real + synthetic dataset, DenseNet121 again achieved the best performance with 97%. This demonstrates that our approach can generate high-quality synthetic sonar data, and the inclusion of synthetic data slightly improves model performance on real datasets.",
            "We conducted an in-depth analysis to regulate the results generated by the trained model. The style-injection ablation study is extensively discussed in Section  5.3.1 , while the fine-tuning ablation study is detailed in Section  5.3.2 ."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  SSIM & PSNR w.r.t. fidelity (   \\gamma italic_ ) for the images in Fig.  8",
        "table": "S5.T4.1",
        "footnotes": [
            ""
        ],
        "references": [
            "Underwater sonar imagery is a critical domain, wherein the data collection and processing of such a large training dataset is both expensive and challenging. Some of the common ways are to leverage the publicly available datasets e.g. Seabed Objects KLSG dataset, SCTD (see Section  4.1.1 ) or the CAD-based simulated dataset such as S3 simulator data (see Section  4.1.2 ). Further, we advance the sonar image synthesis via Generative AI techniques such as style injection (see Section  3.1.1 ), as explained in the forthcoming section.",
            "The S3Simulator  [ 37 ]  dataset is a novel benchmark of simulated side-scan sonar images designed to overcome challenges in acquiring high-quality sonar data. Using advanced simulation techniques, the dataset accurately replicates underwater conditions and produces diverse synthetic sonar images, as illustrated in Fig.  4 . Tools like the Segment Anything Model (SAM) and Gazebo are utilized for optimal object segmentation and visualization, enhancing the quality of data for AI model training in underwater object classification.",
            "The generated images appear to be similar to each other. However, qualitative metrics such as SSIM and PSNR (refer to Table  4 ) were computed for images generated with   = { 0.9 , 0.75 , 0.5 , 0.3 }  0.9 0.75 0.5 0.3 \\gamma=\\{0.9,0.75,0.5,0.3\\} italic_ = { 0.9 , 0.75 , 0.5 , 0.3 }  and found that as    \\gamma italic_  decreases, there is a noticeable improvement in both SSIM and PSNR, indicating a gradual enhancement in visual quality and fidelity. For our main model, we set   = 0.5  0.5 \\gamma=0.5 italic_ = 0.5  as the default value, striking a balance between preserving content and injecting stylistic information while avoiding excessive distortion or loss of essential content features."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 :  State-of-the-art Comparison",
        "table": "S5.T5.2",
        "footnotes": [
            "",
            "",
            ""
        ],
        "references": [
            "The images generated using the style injection are depicted in Fig.  5 . The content image is generated from the prompt developed using GPT, the actual image is from a real sonar dataset, and the output is the stylized image from the proposed framework with the value of    \\gamma italic_  set to 0.5. The stylized output images retain the core structural elements necessary for accurate sonar image interpretation, such as the clear depiction of object boundaries and the representation of noise patterns that are characteristic of underwater acoustic imaging. This ensures that the model does not over-stylize the content, but instead enhances the aesthetic appeal while preserving the scientific accuracy of the generated outputs. Moreover, the generated images show clearer details and better feature separation, indicating that style injection with an attention mechanism helps the diffusion model distinguish foreground objects from background noise. This makes the images more similar to real sonar data while adding subtle artistic variations that enhance diversity.",
            "We conducted an in-depth analysis to regulate the results generated by the trained model. The style-injection ablation study is extensively discussed in Section  5.3.1 , while the fine-tuning ablation study is detailed in Section  5.3.2 .",
            "To the best of our knowledge, our work is the first work in text-conditioned image generation for underwatersonarimagery, and it lacks benchmark datasets, thereby hindering our ability to fully assess the models performance. However, to showcase the efficacy of the proposed architecture, we compared our work with existing similar image-to-image synthesis work and tabulated the results in the Table.  5 . Our model outperforms the state-of-the-art models  [ 50 ] ,  [ 45 ] , [ 48 ]  in terms of image quality, with high SSIM & PSNR values and low FID value."
        ]
    }
}