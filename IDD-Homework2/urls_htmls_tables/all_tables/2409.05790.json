{
    "id_table_1": {
        "caption": "Table 1 :  Statistical metrics for the absolute relative errors.",
        "table": "S3.E4",
        "footnotes": [],
        "references": [
            "CHF, also known as departure from nucleate boiling, is a critical phenomenon in heat transfer. It occurs when a heated surface reaches a point where it can no longer efficiently transfer heat to the surrounding fluid. In nuclear reactors, exceeding the CHF limit could potentially lead to fuel rod failure, which is a significant safety concern especially for pressurized water reactors. Therefore, it is essential to limit the heat flux of the fuel rods to a value below the CHF threshold. The collection of CHF measurement data can be challenging due to the nature of the experiments. Recently, the US NRC has published the largest known CHF dataset publicly available  [ 24 ] . It consists of nearly 25,000 CHF measurements in vertical uniformly-heated water-cooled cylindrical tubes, gathered over a span of 60 years from 59 different sources. In these experiments, CHF values were measured at various TH initial/boundary conditions: pressure ( P P P italic_P ), mass flux ( G G G italic_G ), inlet temperature ( T in subscript T in T_{\\text{in}} italic_T start_POSTSUBSCRIPT in end_POSTSUBSCRIPT ), as well as geometrical parameters like test section diameter ( D D D italic_D ), and heated length ( L L L italic_L ). Additionally, the dataset contains calculated parameters derived from measurements and water properties, including outlet equilibrium quality ( X X X italic_X ), and inlet enthalpy (   h in  subscript h in \\Delta h_{\\text{in}} roman_ italic_h start_POSTSUBSCRIPT in end_POSTSUBSCRIPT ). The distributions of the values of these parameters, along with their pair-wise correlations, are illustrated in Figure  1 .",
            "The DNN predictive model was also constructed in TensorFlow using eight hidden layers to transform the  7 7 7 7  input parameters listed in Section  4.1 . The network depth, number of neurons per layer, activation functions, and learning rate were optimized using RayTune  [ 32 ]  using  1000 1000 1000 1000  hyperparameter configurations in a random search. This tuning process was performed using the validation partition of the dataset. The training, validation, and testing datasets used between the CVAE and DNN were identical to ensure an accurate and fair comparison between the two methods. Once the hyperparameters were set, the model was trained for a total of  500 500 500 500  epochs with an exponential learning decay rate of  0.96 0.96 0.96 0.96 . The fully trained network was then evaluated against the testing set using the set of metrics provided in Section  5 . The DNN model is much more straightforward to train since it only builds a black-box surrogate model of the training dataset without considering the data distribution. Our goal is to use such a fine tuned DNN model to benchmark the performance of the CVAE generative model using a comprehensive collection of metrics presented in the following sections.",
            "Table  1  includes statistical metrics calculated based on the absolute relative errors and the  R 2 superscript R 2 R^{2} italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  metric for both models. It provides the mean, maximum, and standard deviations of the absolute relative error values, along with the fraction of testing points resulting in an absolute relative error value greater than 10%. The statistical metrics show that both models were able to predict CHF values accurately, with mean absolute relative error values of 1.4907% for the CVAE model and 1.8473% for the DNN model. The maximum error values were significantly higher than the mean values, however, it is reassuring to note that only 1.34% of the points in the testing data resulted in an absolute relative error greater than 10% for the DNN model compared to 0.56% for the CVAE model.",
            "The last row in Figure  1  presents the pair-wise correlations between the measured CHF values and each of the seven parameters in the vector of TH conditions. It is desirable to maintain such correlations in the generated and predicted CHF data. The correlations between the predicted and generated CHF values and the TH parameters in the testing dataset were compared with the real data to determine whether the models can learn and preserve these correlations. Figure  4  compares the correlations of the TH parameters with the true CHF values and the generated CHF values using the CVAE model. Similarly, Figure  5  compares the correlations of the TH parameters with the true CHF values and the predicted CHF values using the DNN model. These figures also illustrate the TH parameter ranges in the testing dataset.",
            "This process resulted in a relative standard deviation value for each of the output means, providing quantification of the models uncertainty. By comparing the relative standard deviations from the CVAE to those of the ensemble of DNNs, we can assess the effectiveness of each approach in capturing the uncertainty associated with the model predictions. This is done using the 6 metrics reported in Table  2 , which also include the standard deviation of the error across all prediction means, as was done in Section  5.1 . In terms of error values, the DNN achieves a significantly smaller mean relative error of  0.8868 % percent 0.8868 0.8868\\% 0.8868 %  when compared to the  1.4797 % percent 1.4797 1.4797\\% 1.4797 %  of the CVAE. The standard deviation of this error distribution is also tighter in the DNNs case,  1.5364 % percent 1.5364 1.5364\\% 1.5364 %  versus  1.8496 % percent 1.8496 1.8496\\% 1.8496 % , but with generally comparable values when considering their small magnitudes. The DNNs maximum value of the error distribution,  37.307 % percent 37.307 37.307\\% 37.307 % , is observed to be significantly larger than the CVAEs of  22.334 % percent 22.334 22.334\\% 22.334 % . Both of these maximum values are located at relatively small CHF values near the origin.",
            "The UQ analysis of the CVAE model showed that its generative predictions have small variations at a specific input. This is demonstrated by the very small mean relative standard deviations of its predictions, which is 0.2579%. Additionally, changes in all metric values were very small when comparing the UQ and non-UQ resuts, as shown in Tables  1  and  2 . These indicate that the CVAE model has low sensitivity to the random samples from the latent vector during the generation process. The CVAE model is thus robust, consistently maintaining similar error behavior across multiple predictions. This consistency indicates that, although the model produces slightly different outputs each time, the predictions will remain within the quantified error ranges, ensuring reliable performance even with variations in the latent vectors.",
            "On the other hand, quantifying uncertainties for the DNN model requires training multiple DNNs and measuring the variations between their predictions, which is more laborious than the CVAE model. The ensembled results showed significant improvement in all error metrics, except for the maximum error value, as shown in Tables  1  and  2 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Statistical metrics for the absolute relative errors with UQ.",
        "table": "S5.T1.5",
        "footnotes": [],
        "references": [
            "The remainder of this paper is structured as follows: in Section  2 , we present the training data details. Section  3  introduces the methodologies for the CVAE generative model and DNN regression model. The models specifications along with UQ and domain generalization methods are presented in Section  4 . The results of this work are presented in Section  5 , including uncertainty estimates and model domain generalization. Further discussions on this work are provided in Section  6 . Finally, our findings and conclusions are summarized in Section  7 .",
            "The use of DNNs has become widespread in multiple fields of nuclear engineering as a fast-and-accurate approach to create surrogate models, including in the prediction of CHF as discussed in Section  2 . DNNs are based on the transformation of a set of input values to produce a set of outputs (or single output) using a network of multiplying weights and additive biases. These are located on neurons, with an activation function applied to each of their outputs to introduce non-linearity. An objective function, typically the mean-squared error ( MSE ), then evaluates how different the predicted outputs are from the true values. This value is then used to adjust the weights via backpropagation, which will minimize the objective function over a series of iterations through a section of the original dataset known as the training set. Once the surrogate model is fully trained, it is evaluated with a testing dataset that the model has never seen during training, providing a true estimate of the models ability to generalize to new data.",
            "VAEs  [ 4 ]  are a family of DGMs that was introduced to learn the data distribution uniquely through variational inference (VI)  [ 7 ] . A VAE model consists of three main components: the encoder, latent space, and decoder, as shown in Figure  2 . By passing the input to the encoder, it encodes it as a distribution in the latent space. Hence, the latent space consists of two vectors that represent the mean value and standard deviation of the encoded distribution. The decoder utilizes the encoded information to reconstruct the input by reversing the encoding process. This approach creates a structured latent space that can be effectively utilized for data generation. Once the model is trained, the decoder can be used to generate new samples by taking a vector from the latent space.",
            "This process does not allow for generating targeted samples, since the only input passed to the decoder is a random vector from the latent space. To enable targeted data generation, a CVAE is employed. CVAEs are a variant of VAEs that use labels or conditions for targeted data generation  [ 18 ] . Generating targeted data is achieved by passing specified conditions (vector  c c \\mathbf{c} bold_c ) to the decoder as shown in Figure  2 .",
            "The CVAE generative model will be used to approximate the underlying distribution of the NRC CHF dataset to generate new CHF values under specific TH conditions. The vector of TH conditions consists of seven parameters:  P P P italic_P ,  G G G italic_G ,  T in subscript T in T_{\\text{in}} italic_T start_POSTSUBSCRIPT in end_POSTSUBSCRIPT ,  D D D italic_D ,  L L L italic_L ,  X X X italic_X , and    h in  subscript h in \\Delta h_{\\text{in}} roman_ italic_h start_POSTSUBSCRIPT in end_POSTSUBSCRIPT , as discussed in Section  2 . During the training process, these seven parameters were provided to the model as conditions, that is, the vector  c c \\mathbf{c} bold_c  in Equation ( 5 ) and Figure  2 , allowing it to learn how to generate CHF values under user desired conditions. If a conventional VAE generative model were to be trained, one would not condition the model on the vector of TH parameters. In this case, the trained VAE model can only generate synthetic samples randomly, making it difficult to evaluate its performance by comparing with the real experimental data, since it is almost impossible for the random samples to have TH conditions that exactly match the real experimental data.",
            "This process resulted in a relative standard deviation value for each of the output means, providing quantification of the models uncertainty. By comparing the relative standard deviations from the CVAE to those of the ensemble of DNNs, we can assess the effectiveness of each approach in capturing the uncertainty associated with the model predictions. This is done using the 6 metrics reported in Table  2 , which also include the standard deviation of the error across all prediction means, as was done in Section  5.1 . In terms of error values, the DNN achieves a significantly smaller mean relative error of  0.8868 % percent 0.8868 0.8868\\% 0.8868 %  when compared to the  1.4797 % percent 1.4797 1.4797\\% 1.4797 %  of the CVAE. The standard deviation of this error distribution is also tighter in the DNNs case,  1.5364 % percent 1.5364 1.5364\\% 1.5364 %  versus  1.8496 % percent 1.8496 1.8496\\% 1.8496 % , but with generally comparable values when considering their small magnitudes. The DNNs maximum value of the error distribution,  37.307 % percent 37.307 37.307\\% 37.307 % , is observed to be significantly larger than the CVAEs of  22.334 % percent 22.334 22.334\\% 22.334 % . Both of these maximum values are located at relatively small CHF values near the origin.",
            "Now considering the distribution of relative standard deviations, the CVAEs results report significantly smaller mean and maximum relative Std values compared to the DNNs. The mean relative standard deviation of the DNN ( 1.8023 % percent 1.8023 1.8023\\% 1.8023 % ) is over seven times greater than that of the CVAE ( 0.2579 % percent 0.2579 0.2579\\% 0.2579 % ), indicating that the CVAE is more confident in its predictions with less variability in its outputs. This is similarly mirrored in their maximum values, where the DNN has a value of nearly  30 % percent 30 30\\% 30 %  with the CVAE reporting  5.5714 % percent 5.5714 5.5714\\% 5.5714 % . The final metric, the fraction of predictions with relative errors above  10 % percent 10 10\\% 10 % , has comparable values between the two models. Considering the combination of the metrics in Table  2 , the CVAE outputs are shown to have similar performance compared to a DNN ensemble while achieving model uncertainties significantly smaller.",
            "The UQ analysis of the CVAE model showed that its generative predictions have small variations at a specific input. This is demonstrated by the very small mean relative standard deviations of its predictions, which is 0.2579%. Additionally, changes in all metric values were very small when comparing the UQ and non-UQ resuts, as shown in Tables  1  and  2 . These indicate that the CVAE model has low sensitivity to the random samples from the latent vector during the generation process. The CVAE model is thus robust, consistently maintaining similar error behavior across multiple predictions. This consistency indicates that, although the model produces slightly different outputs each time, the predictions will remain within the quantified error ranges, ensuring reliable performance even with variations in the latent vectors.",
            "On the other hand, quantifying uncertainties for the DNN model requires training multiple DNNs and measuring the variations between their predictions, which is more laborious than the CVAE model. The ensembled results showed significant improvement in all error metrics, except for the maximum error value, as shown in Tables  1  and  2 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Statistics of the absolute relative errors for testing samples inside and outside the convex hull for the DNN and CVAE models.",
        "table": "S5.T2.4",
        "footnotes": [],
        "references": [
            "The remainder of this paper is structured as follows: in Section  2 , we present the training data details. Section  3  introduces the methodologies for the CVAE generative model and DNN regression model. The models specifications along with UQ and domain generalization methods are presented in Section  4 . The results of this work are presented in Section  5 , including uncertainty estimates and model domain generalization. Further discussions on this work are provided in Section  6 . Finally, our findings and conclusions are summarized in Section  7 .",
            "Figure  3(a)  shows a comparison of the distributions of the relative errors from the CVAE and DNN models. Both models have most of the relative error values concentrated around zero, indicating that the models were successful in generating and predicting CHF values that closely align with the true values. The CVAEs error distribution is observed to have a slightly smaller mean value and standard deviation. Figure  3(b)  shows a direct comparison of the true CHF values with those generated by the CVAE model and those predicted by the DNN model, with a   10 % plus-or-minus percent 10 \\pm 10\\%  10 %  error bounds. The results show a strong agreement with the true CHF values, with a slight increase in deviation observed after CHF value above 6,000  kW   m  2 times kilowatt meter 2 \\mathrm{kW}\\text{\\,}{\\mathrm{m}}^{-2} start_ARG roman_kW end_ARG start_ARG times end_ARG start_ARG power start_ARG roman_m end_ARG start_ARG - 2 end_ARG end_ARG . It is also noticeable that there are very few points having relative errors greater than 10%.",
            "To quantify the model uncertainty, 200 unique samples were computed for each of the testing sets input entries using the trained CVAE model. From these, the means and standard deviations of these samples were taken for each of these input entries. The ensemble approach described in Section  4.3  was then implemented using  n = 20 n 20 n=20 italic_n = 20  models each initialized with a different random seed. Each of these models were identical with the exception of the initialization. As with the CVAE model, the means and standard deviations were computed along the 200 samples for each input vector of the testing set. The standard deviations were then taken to compute the relative standard deviation using Equation  6 . This metric standardizes the uncertainty measure by expressing the standard deviation as a percentage of the mean, making it easier to interpret and to compare across different scales of the outputs.",
            "As shown in Figure  6 , the error distributions for the two subsets (inside and outside the convex hull) behave similarly, with small deviations observed in the mean and standard deviation values. The maximum error values occur when predictions were made under conditions outside the training data domain. This applies to both the CVAE and DNN models. The mean, maximum and standard deviation values were calculated for the absolute relative errors for both models and subsets. These values, along with the fraction of points with error values greater than 10% in each subset are listed in Table  3 ."
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "S5.T3.8",
        "footnotes": [],
        "references": [
            "The remainder of this paper is structured as follows: in Section  2 , we present the training data details. Section  3  introduces the methodologies for the CVAE generative model and DNN regression model. The models specifications along with UQ and domain generalization methods are presented in Section  4 . The results of this work are presented in Section  5 , including uncertainty estimates and model domain generalization. Further discussions on this work are provided in Section  6 . Finally, our findings and conclusions are summarized in Section  7 .",
            "The loss function in Equation ( 4 ) does not incorporate conditions, hence it is used to train the VAEs model. The CVAEs loss function can be constructed by conditioning the VAEs loss function on the conditions  c c \\mathbf{c} bold_c  as follows:",
            "The DNN predictive model was also constructed in TensorFlow using eight hidden layers to transform the  7 7 7 7  input parameters listed in Section  4.1 . The network depth, number of neurons per layer, activation functions, and learning rate were optimized using RayTune  [ 32 ]  using  1000 1000 1000 1000  hyperparameter configurations in a random search. This tuning process was performed using the validation partition of the dataset. The training, validation, and testing datasets used between the CVAE and DNN were identical to ensure an accurate and fair comparison between the two methods. Once the hyperparameters were set, the model was trained for a total of  500 500 500 500  epochs with an exponential learning decay rate of  0.96 0.96 0.96 0.96 . The fully trained network was then evaluated against the testing set using the set of metrics provided in Section  5 . The DNN model is much more straightforward to train since it only builds a black-box surrogate model of the training dataset without considering the data distribution. Our goal is to use such a fine tuned DNN model to benchmark the performance of the CVAE generative model using a comprehensive collection of metrics presented in the following sections.",
            "The last row in Figure  1  presents the pair-wise correlations between the measured CHF values and each of the seven parameters in the vector of TH conditions. It is desirable to maintain such correlations in the generated and predicted CHF data. The correlations between the predicted and generated CHF values and the TH parameters in the testing dataset were compared with the real data to determine whether the models can learn and preserve these correlations. Figure  4  compares the correlations of the TH parameters with the true CHF values and the generated CHF values using the CVAE model. Similarly, Figure  5  compares the correlations of the TH parameters with the true CHF values and the predicted CHF values using the DNN model. These figures also illustrate the TH parameter ranges in the testing dataset.",
            "Figures  4  and  5  show that the correlations between the TH parameters and the generated/predicted CHF values align closely with those found in the real data. Both models have consistent behavior in terms of the correlations. While largest relative absolute error is observed at smaller CHF values, the largest absolute deviations from the true values are observed at higher CHF values, which are associated with smaller values of  D D D italic_D ,  L L L italic_L ,  P P P italic_P  and  T in subscript T in T_{\\text{in}} italic_T start_POSTSUBSCRIPT in end_POSTSUBSCRIPT , subcooled coolant conditions, and high  G G G italic_G  values. It is important to note that these regions contain fewer data points in both training and testing datasets, resulting in the models deviations from the true values under these conditions. However, these deviations are not significant and do not heavily influence the error distributions, as these CHF conditions are less common in practical applications and thus in the CHF dataset.",
            "To quantify the model uncertainty, 200 unique samples were computed for each of the testing sets input entries using the trained CVAE model. From these, the means and standard deviations of these samples were taken for each of these input entries. The ensemble approach described in Section  4.3  was then implemented using  n = 20 n 20 n=20 italic_n = 20  models each initialized with a different random seed. Each of these models were identical with the exception of the initialization. As with the CVAE model, the means and standard deviations were computed along the 200 samples for each input vector of the testing set. The standard deviations were then taken to compute the relative standard deviation using Equation  6 . This metric standardizes the uncertainty measure by expressing the standard deviation as a percentage of the mean, making it easier to interpret and to compare across different scales of the outputs."
        ]
    }
}