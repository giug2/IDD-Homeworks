{
    "PAPER'S NUMBER OF TABLES": 2,
    "S3.T0.st1": {
        "caption": "(a) Mendeley clients.",
        "table": "<table id=\"S3.T0.st1.4\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S3.T0.st1.4.1\" class=\"ltx_tr\">\n<td id=\"S3.T0.st1.4.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S3.T0.st1.4.1.1.1\" class=\"ltx_text\"></span> <span id=\"S3.T0.st1.4.1.1.2\" class=\"ltx_text\">\n<span id=\"S3.T0.st1.4.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S3.T0.st1.4.1.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S3.T0.st1.4.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S3.T0.st1.4.1.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">No.</span></span></span>\n<span id=\"S3.T0.st1.4.1.1.2.1.2\" class=\"ltx_tr\">\n<span id=\"S3.T0.st1.4.1.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S3.T0.st1.4.1.1.2.1.2.1.1\" class=\"ltx_text ltx_font_bold\">Clients</span></span></span>\n</span></span><span id=\"S3.T0.st1.4.1.1.3\" class=\"ltx_text\"></span>\n</td>\n<td id=\"S3.T0.st1.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T0.st1.4.1.2.1\" class=\"ltx_text ltx_font_bold\">Train</span></td>\n<td id=\"S3.T0.st1.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T0.st1.4.1.3.1\" class=\"ltx_text ltx_font_bold\">Val.</span></td>\n<td id=\"S3.T0.st1.4.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S3.T0.st1.4.1.4.1\" class=\"ltx_text ltx_font_bold\">Test</span></td>\n<td id=\"S3.T0.st1.4.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T0.st1.4.1.5.1\" class=\"ltx_text ltx_font_bold\">Total</span></td>\n</tr>\n<tr id=\"S3.T0.st1.4.2\" class=\"ltx_tr\">\n<td id=\"S3.T0.st1.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\">2</td>\n<td id=\"S3.T0.st1.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">350</td>\n<td id=\"S3.T0.st1.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">75</td>\n<td id=\"S3.T0.st1.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">75</td>\n<td id=\"S3.T0.st1.4.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">500</td>\n</tr>\n<tr id=\"S3.T0.st1.4.3\" class=\"ltx_tr\">\n<td id=\"S3.T0.st1.4.3.1\" class=\"ltx_td ltx_align_center\">2</td>\n<td id=\"S3.T0.st1.4.3.2\" class=\"ltx_td ltx_align_center\">140</td>\n<td id=\"S3.T0.st1.4.3.3\" class=\"ltx_td ltx_align_center\">30</td>\n<td id=\"S3.T0.st1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\">30</td>\n<td id=\"S3.T0.st1.4.3.5\" class=\"ltx_td ltx_align_center\">200</td>\n</tr>\n<tr id=\"S3.T0.st1.4.4\" class=\"ltx_tr\">\n<td id=\"S3.T0.st1.4.4.1\" class=\"ltx_td ltx_align_center\">2</td>\n<td id=\"S3.T0.st1.4.4.2\" class=\"ltx_td ltx_align_center\">70</td>\n<td id=\"S3.T0.st1.4.4.3\" class=\"ltx_td ltx_align_center\">15</td>\n<td id=\"S3.T0.st1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_r\">15</td>\n<td id=\"S3.T0.st1.4.4.5\" class=\"ltx_td ltx_align_center\">100</td>\n</tr>\n<tr id=\"S3.T0.st1.4.5\" class=\"ltx_tr\">\n<td id=\"S3.T0.st1.4.5.1\" class=\"ltx_td ltx_align_center\">2</td>\n<td id=\"S3.T0.st1.4.5.2\" class=\"ltx_td ltx_align_center\">10</td>\n<td id=\"S3.T0.st1.4.5.3\" class=\"ltx_td ltx_align_center\">10</td>\n<td id=\"S3.T0.st1.4.5.4\" class=\"ltx_td ltx_align_center ltx_border_r\">10</td>\n<td id=\"S3.T0.st1.4.5.5\" class=\"ltx_td ltx_align_center\">30</td>\n</tr>\n<tr id=\"S3.T0.st1.4.6\" class=\"ltx_tr\">\n<td id=\"S3.T0.st1.4.6.1\" class=\"ltx_td ltx_align_center\">2</td>\n<td id=\"S3.T0.st1.4.6.2\" class=\"ltx_td ltx_align_center\">4</td>\n<td id=\"S3.T0.st1.4.6.3\" class=\"ltx_td ltx_align_center\">3</td>\n<td id=\"S3.T0.st1.4.6.4\" class=\"ltx_td ltx_align_center ltx_border_r\">3</td>\n<td id=\"S3.T0.st1.4.6.5\" class=\"ltx_td ltx_align_center\">10</td>\n</tr>\n<tr id=\"S3.T0.st1.4.7\" class=\"ltx_tr\">\n<td id=\"S3.T0.st1.4.7.1\" class=\"ltx_td ltx_align_center\">2</td>\n<td id=\"S3.T0.st1.4.7.2\" class=\"ltx_td ltx_align_center\">2</td>\n<td id=\"S3.T0.st1.4.7.3\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"S3.T0.st1.4.7.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0</td>\n<td id=\"S3.T0.st1.4.7.5\" class=\"ltx_td ltx_align_center\">2</td>\n</tr>\n<tr id=\"S3.T0.st1.4.8\" class=\"ltx_tr\">\n<td id=\"S3.T0.st1.4.8.1\" class=\"ltx_td ltx_align_center\">2</td>\n<td id=\"S3.T0.st1.4.8.2\" class=\"ltx_td ltx_align_center\">1</td>\n<td id=\"S3.T0.st1.4.8.3\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"S3.T0.st1.4.8.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0</td>\n<td id=\"S3.T0.st1.4.8.5\" class=\"ltx_td ltx_align_center\">1</td>\n</tr>\n<tr id=\"S3.T0.st1.4.9\" class=\"ltx_tr\">\n<td id=\"S3.T0.st1.4.9.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">14</td>\n<td id=\"S3.T0.st1.4.9.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">1,686</td>\n<td id=\"S3.T0.st1.4.9.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">1,154</td>\n<td id=\"S3.T0.st1.4.9.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">266</td>\n<td id=\"S3.T0.st1.4.9.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">266</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We simulated five clients with large subsets of the original CheXpert training set, and 31 clients with small subsets of either the original CheXpert validation set or the original Mendeley training set. We randomly split the patients whose images are part of the original CheXpert training set into five equal parts and assign each part randomly to one of five clients. Table 3.2.1 shows the distribution of the CheXpert validation data and Mendeley data among the remaining 31 clients, split in training, validation and test set sizes. These clients are used as targets for the reconstruction attacks in SectionsÂ 4.2 and 4.3.3. Each clientâ€™s dataset was further split into a dedicated training, validation, and test set, consisting of 70%, 15%, and 15% of the clientâ€™s data, respectively. Clientsâ€™ datasets that are smaller than 50 images were split equally among the subsets. Datasets comprising less than ten images were used solely for training, omitting local validation or testing. All splits were performed randomly. No specific label distribution was enforced. We ensured that there was no patient overlap between clients and between training, validation, and test splits within each clientâ€™s dataset.",
            "In private training, the privacy loss is difficult to track for some layer types. This includes active batch normalization layers, which are part of both DenseNet and ResNet architectures, as they create arbitrary dependencies between samples within a single batch Ioffe and Szegedy (2015). We experimented with different model layer freezing techniques to avoid training batch normalization layers resulting in intractable privacy loss. We refer to rendering model layers untrainable as layer freezing. We considered full model training (no layer freezing), freezing batch normalization layers, and freezing all layers but the final classification layer.",
            "If Î´ğ›¿\\delta is equal to or greater than the inverse of the size of the dataset, it would allow for leakage of a whole record or data sample without violation of the privacy constraintÂ Dwork and Roth (2014). As this is unacceptable, Î´ğ›¿\\delta should be smaller than the inverse of the dataset size Kaissis etÂ al. (2021). Because the size of individual clientâ€™s dataset varies, we determined Î´ğ›¿\\delta as follows:",
            "Given the implications of different layer freezing techniques for privacy, we compared the outcomes of full model training\n(no layer freezing), freezing batch normalization layers, and freezing all layers but the final classification layer (TableÂ 4.1).",
            "We applied the reconstruction attack to a single clientâ€™s local model with a batch size of one during the first communication round. TableÂ 4.2.1 reports the mean PSNR and sample standard deviation over three trials per experiment. Fig. 2 shows the reconstructed images of the best attack trials. The attack was only successful in the case of batch normalization layer freezing, indicated by larger mean PSNR values of 12.2912.2912.29 (ResNet50) and 10.9810.9810.98 (DenseNet121). Training the full model as well as fine-tuning only the output layer prevented the recovery of any useful image features in this setting. We further observed that the DenseNet121 seems to be more robust to leakage from gradients in this example, although the ResNet50 is the larger architecture in terms of parameter count, containing more than three times as many trainable parameters as the DenseNet121.",
            "We investigated the impact of the training batch size in the setting where the attack was most successful, i.e., on models trained with frozen batch normalization layers attacked during late training. We attacked clients for which the considered batch size was equal to the available number of training images. The setting is equivalent to clients with larger datasets sharing model updates after every processed batch. A batch size of ten reduced attack success as the mean PSNR values over the batch decreased to 9.019.019.01 (ResNet50) and 8.158.158.15 (DenseNet121) (Table 4.2.3). While the quality of the reconstructions varied for individual images within a batch, at least one image out of each batch became recognizable. We note that the order of images in a batch may not be preserved in the reconstruction of larger batches, preventing a direct comparison between original and reconstructed data points. To assign a reconstructed image to its original for evaluation, we first obtained the PSNR of each original image with each reconstructed image. We then determined the first original-reconstruction pair as the one with the largest PSNR value. The next best pair was determined considering the PSNR values between the remaining original and reconstructed images. We iterated the procedure until all images have been assigned.",
            "Table 4.2.4 summarizes the auxiliary model predictions. The low baseline performance of the auxiliary models on original images compared to the classifier validation estimate is probably due to the small sample size of 353535 images. Superior results on images reconstructed from the ResNet50 in the case of sex prediction suggest an increased susceptibility to privacy violation of this architecture compared to the DenseNet121.",
            "Table 4.3.1 reports the modelsâ€™ performance with privacy budgets Îµâˆˆ{1,3,6,10}ğœ€13610\\varepsilon\\in\\{1,3,6,10\\}. We include the non-private baseline performance for comparison. Batch normalization layer parameters were not updated during model training. We report the exact privacy budget spent by each local model as optimal (Î±,Îµ)ğ›¼ğœ€(\\alpha,\\varepsilon)-pairs in Appendix Section C.",
            "We attempted to reconstruct the training image from the local model shared by a Mendeley client during private training. We performed the attack on models with frozen batch normalization layers during late training. Table 4.3.3 compares the mean PSNR over three trials between non-private and private training. The PSNR on all images from private models was significantly smaller than in the non-private setting. Fig. 8 confirms that the reconstructed images from both model architectures did not leak any visual parts of the training images. Differentially private training under all considered privacy budgets therefore successfully prevented the attack.",
            "To validate that no sensitive information was leaked, we applied the auxiliary models (first introduced in SectionÂ 4.2.4) to predict patient age and sex from images reconstructed from private models. Table 4.3.3 compares their performance on original and recovered images in private and non-private settings. We attacked the model with the weakest privacy guarantee of Îµ=10ğœ€10\\varepsilon=10. The AUC values of 0.490.490.49 and 0.470.470.47 on sex prediction indicate that the classifierâ€™s performance was equivalent to random label assignment in the private setting. The age predictions deviated around 191919 years on average from the true patientsâ€™ age. Differentially private model training prevented both auxiliary models to predict usable information about the patientsâ€™ demographic properties.",
            "As a step towards privacy-preserving distributed learning, we integrated RÃ©nyi differential privacy with a Gaussian noise mechanism into the federated learning process. The DenseNet121 achieved the best utility-privacy trade-off with a mean AUC of 0.9370.9370.937 for Îµ=6ğœ€6\\varepsilon=6, where we identified an expected cost in accuracy of 0.030.030.03 in terms of the AUC on CheXpert clientsâ€™ data compared to the non-private baseline. The results suggest that Îµâˆˆ[3,6]ğœ€36\\varepsilon\\in[3,6] are suitable candidates for private model training depending on the specific demands on model privacy and performance for the respective application. Overall, we found the DenseNet121 model superior to ResNet50 with regard to private model training for all considered Îµğœ€\\varepsilon values.",
            "The adverse impact of differential privacy on model performance must be carefully considered, particularly for medical use cases. Our results endorse that differentially private federated learning is feasible at a small cost in model accuracy for the classification of heterogeneous chest X-ray data. As real-world medical use cases become more complex in practice, future work may elaborate on the potential of differentially private federated learning for multi-label X-ray classification where heterogeneous data from a broader range of sources is effectively integrated under consideration of an improved bound on the privacy budget. We identified the DenseNet121 as a robust model architecture suitable for differentially private training. Further comparison with other neural network architectures may reveal key indicators for the suitability of different model types and provide guidance in the choice of models for privacy-preserving machine learning. We further suggest to extend our evaluation framework in future work to consider the vulnerability to other types of privacy breaches, enabling a comprehensive qualitative assessment of model privacy. Finally, other variants of differential privacy, e.g. Gaussian differential privacy Dong etÂ al. , may offer suitable alternatives to the application of RÃ©nyi differential privacy providing yet tighter bounds on the privacy loss.",
            "Table C reports the exact privacy budget spent by each local model trained with differential privacy as optimal (Î±,Îµ)ğ›¼ğœ€(\\alpha,\\varepsilon)-pairs."
        ]
    },
    "S3.T0.st2": {
        "caption": "(b) CheXpert clients.",
        "table": "<table id=\"S3.T0.st2.4\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S3.T0.st2.4.1\" class=\"ltx_tr\">\n<td id=\"S3.T0.st2.4.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S3.T0.st2.4.1.1.1\" class=\"ltx_text\"></span> <span id=\"S3.T0.st2.4.1.1.2\" class=\"ltx_text\">\n<span id=\"S3.T0.st2.4.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S3.T0.st2.4.1.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S3.T0.st2.4.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S3.T0.st2.4.1.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">No.</span></span></span>\n<span id=\"S3.T0.st2.4.1.1.2.1.2\" class=\"ltx_tr\">\n<span id=\"S3.T0.st2.4.1.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S3.T0.st2.4.1.1.2.1.2.1.1\" class=\"ltx_text ltx_font_bold\">Clients</span></span></span>\n</span></span><span id=\"S3.T0.st2.4.1.1.3\" class=\"ltx_text\"></span>\n</td>\n<td id=\"S3.T0.st2.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T0.st2.4.1.2.1\" class=\"ltx_text ltx_font_bold\">Train</span></td>\n<td id=\"S3.T0.st2.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T0.st2.4.1.3.1\" class=\"ltx_text ltx_font_bold\">Val.</span></td>\n<td id=\"S3.T0.st2.4.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S3.T0.st2.4.1.4.1\" class=\"ltx_text ltx_font_bold\">Test</span></td>\n<td id=\"S3.T0.st2.4.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T0.st2.4.1.5.1\" class=\"ltx_text ltx_font_bold\">Total</span></td>\n</tr>\n<tr id=\"S3.T0.st2.4.2\" class=\"ltx_tr\">\n<td id=\"S3.T0.st2.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\">2</td>\n<td id=\"S3.T0.st2.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">10</td>\n<td id=\"S3.T0.st2.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">10</td>\n<td id=\"S3.T0.st2.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">10</td>\n<td id=\"S3.T0.st2.4.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">30</td>\n</tr>\n<tr id=\"S3.T0.st2.4.3\" class=\"ltx_tr\">\n<td id=\"S3.T0.st2.4.3.1\" class=\"ltx_td ltx_align_center\">5</td>\n<td id=\"S3.T0.st2.4.3.2\" class=\"ltx_td ltx_align_center\">4</td>\n<td id=\"S3.T0.st2.4.3.3\" class=\"ltx_td ltx_align_center\">3</td>\n<td id=\"S3.T0.st2.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\">3</td>\n<td id=\"S3.T0.st2.4.3.5\" class=\"ltx_td ltx_align_center\">10</td>\n</tr>\n<tr id=\"S3.T0.st2.4.4\" class=\"ltx_tr\">\n<td id=\"S3.T0.st2.4.4.1\" class=\"ltx_td ltx_align_center\">5</td>\n<td id=\"S3.T0.st2.4.4.2\" class=\"ltx_td ltx_align_center\">2</td>\n<td id=\"S3.T0.st2.4.4.3\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"S3.T0.st2.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0</td>\n<td id=\"S3.T0.st2.4.4.5\" class=\"ltx_td ltx_align_center\">2</td>\n</tr>\n<tr id=\"S3.T0.st2.4.5\" class=\"ltx_tr\">\n<td id=\"S3.T0.st2.4.5.1\" class=\"ltx_td ltx_align_center\">5</td>\n<td id=\"S3.T0.st2.4.5.2\" class=\"ltx_td ltx_align_center\">1</td>\n<td id=\"S3.T0.st2.4.5.3\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"S3.T0.st2.4.5.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0</td>\n<td id=\"S3.T0.st2.4.5.5\" class=\"ltx_td ltx_align_center\">1</td>\n</tr>\n<tr id=\"S3.T0.st2.4.6\" class=\"ltx_tr\">\n<td id=\"S3.T0.st2.4.6.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">17</td>\n<td id=\"S3.T0.st2.4.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">125</td>\n<td id=\"S3.T0.st2.4.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">55</td>\n<td id=\"S3.T0.st2.4.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">35</td>\n<td id=\"S3.T0.st2.4.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">35</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We simulated five clients with large subsets of the original CheXpert training set, and 31 clients with small subsets of either the original CheXpert validation set or the original Mendeley training set. We randomly split the patients whose images are part of the original CheXpert training set into five equal parts and assign each part randomly to one of five clients. Table 3.2.1 shows the distribution of the CheXpert validation data and Mendeley data among the remaining 31 clients, split in training, validation and test set sizes. These clients are used as targets for the reconstruction attacks in SectionsÂ 4.2 and 4.3.3. Each clientâ€™s dataset was further split into a dedicated training, validation, and test set, consisting of 70%, 15%, and 15% of the clientâ€™s data, respectively. Clientsâ€™ datasets that are smaller than 50 images were split equally among the subsets. Datasets comprising less than ten images were used solely for training, omitting local validation or testing. All splits were performed randomly. No specific label distribution was enforced. We ensured that there was no patient overlap between clients and between training, validation, and test splits within each clientâ€™s dataset.",
            "In private training, the privacy loss is difficult to track for some layer types. This includes active batch normalization layers, which are part of both DenseNet and ResNet architectures, as they create arbitrary dependencies between samples within a single batch Ioffe and Szegedy (2015). We experimented with different model layer freezing techniques to avoid training batch normalization layers resulting in intractable privacy loss. We refer to rendering model layers untrainable as layer freezing. We considered full model training (no layer freezing), freezing batch normalization layers, and freezing all layers but the final classification layer.",
            "If Î´ğ›¿\\delta is equal to or greater than the inverse of the size of the dataset, it would allow for leakage of a whole record or data sample without violation of the privacy constraintÂ Dwork and Roth (2014). As this is unacceptable, Î´ğ›¿\\delta should be smaller than the inverse of the dataset size Kaissis etÂ al. (2021). Because the size of individual clientâ€™s dataset varies, we determined Î´ğ›¿\\delta as follows:",
            "Given the implications of different layer freezing techniques for privacy, we compared the outcomes of full model training\n(no layer freezing), freezing batch normalization layers, and freezing all layers but the final classification layer (TableÂ 4.1).",
            "We applied the reconstruction attack to a single clientâ€™s local model with a batch size of one during the first communication round. TableÂ 4.2.1 reports the mean PSNR and sample standard deviation over three trials per experiment. Fig. 2 shows the reconstructed images of the best attack trials. The attack was only successful in the case of batch normalization layer freezing, indicated by larger mean PSNR values of 12.2912.2912.29 (ResNet50) and 10.9810.9810.98 (DenseNet121). Training the full model as well as fine-tuning only the output layer prevented the recovery of any useful image features in this setting. We further observed that the DenseNet121 seems to be more robust to leakage from gradients in this example, although the ResNet50 is the larger architecture in terms of parameter count, containing more than three times as many trainable parameters as the DenseNet121.",
            "We investigated the impact of the training batch size in the setting where the attack was most successful, i.e., on models trained with frozen batch normalization layers attacked during late training. We attacked clients for which the considered batch size was equal to the available number of training images. The setting is equivalent to clients with larger datasets sharing model updates after every processed batch. A batch size of ten reduced attack success as the mean PSNR values over the batch decreased to 9.019.019.01 (ResNet50) and 8.158.158.15 (DenseNet121) (Table 4.2.3). While the quality of the reconstructions varied for individual images within a batch, at least one image out of each batch became recognizable. We note that the order of images in a batch may not be preserved in the reconstruction of larger batches, preventing a direct comparison between original and reconstructed data points. To assign a reconstructed image to its original for evaluation, we first obtained the PSNR of each original image with each reconstructed image. We then determined the first original-reconstruction pair as the one with the largest PSNR value. The next best pair was determined considering the PSNR values between the remaining original and reconstructed images. We iterated the procedure until all images have been assigned.",
            "Table 4.2.4 summarizes the auxiliary model predictions. The low baseline performance of the auxiliary models on original images compared to the classifier validation estimate is probably due to the small sample size of 353535 images. Superior results on images reconstructed from the ResNet50 in the case of sex prediction suggest an increased susceptibility to privacy violation of this architecture compared to the DenseNet121.",
            "Table 4.3.1 reports the modelsâ€™ performance with privacy budgets Îµâˆˆ{1,3,6,10}ğœ€13610\\varepsilon\\in\\{1,3,6,10\\}. We include the non-private baseline performance for comparison. Batch normalization layer parameters were not updated during model training. We report the exact privacy budget spent by each local model as optimal (Î±,Îµ)ğ›¼ğœ€(\\alpha,\\varepsilon)-pairs in Appendix Section C.",
            "We attempted to reconstruct the training image from the local model shared by a Mendeley client during private training. We performed the attack on models with frozen batch normalization layers during late training. Table 4.3.3 compares the mean PSNR over three trials between non-private and private training. The PSNR on all images from private models was significantly smaller than in the non-private setting. Fig. 8 confirms that the reconstructed images from both model architectures did not leak any visual parts of the training images. Differentially private training under all considered privacy budgets therefore successfully prevented the attack.",
            "To validate that no sensitive information was leaked, we applied the auxiliary models (first introduced in SectionÂ 4.2.4) to predict patient age and sex from images reconstructed from private models. Table 4.3.3 compares their performance on original and recovered images in private and non-private settings. We attacked the model with the weakest privacy guarantee of Îµ=10ğœ€10\\varepsilon=10. The AUC values of 0.490.490.49 and 0.470.470.47 on sex prediction indicate that the classifierâ€™s performance was equivalent to random label assignment in the private setting. The age predictions deviated around 191919 years on average from the true patientsâ€™ age. Differentially private model training prevented both auxiliary models to predict usable information about the patientsâ€™ demographic properties.",
            "As a step towards privacy-preserving distributed learning, we integrated RÃ©nyi differential privacy with a Gaussian noise mechanism into the federated learning process. The DenseNet121 achieved the best utility-privacy trade-off with a mean AUC of 0.9370.9370.937 for Îµ=6ğœ€6\\varepsilon=6, where we identified an expected cost in accuracy of 0.030.030.03 in terms of the AUC on CheXpert clientsâ€™ data compared to the non-private baseline. The results suggest that Îµâˆˆ[3,6]ğœ€36\\varepsilon\\in[3,6] are suitable candidates for private model training depending on the specific demands on model privacy and performance for the respective application. Overall, we found the DenseNet121 model superior to ResNet50 with regard to private model training for all considered Îµğœ€\\varepsilon values.",
            "The adverse impact of differential privacy on model performance must be carefully considered, particularly for medical use cases. Our results endorse that differentially private federated learning is feasible at a small cost in model accuracy for the classification of heterogeneous chest X-ray data. As real-world medical use cases become more complex in practice, future work may elaborate on the potential of differentially private federated learning for multi-label X-ray classification where heterogeneous data from a broader range of sources is effectively integrated under consideration of an improved bound on the privacy budget. We identified the DenseNet121 as a robust model architecture suitable for differentially private training. Further comparison with other neural network architectures may reveal key indicators for the suitability of different model types and provide guidance in the choice of models for privacy-preserving machine learning. We further suggest to extend our evaluation framework in future work to consider the vulnerability to other types of privacy breaches, enabling a comprehensive qualitative assessment of model privacy. Finally, other variants of differential privacy, e.g. Gaussian differential privacy Dong etÂ al. , may offer suitable alternatives to the application of RÃ©nyi differential privacy providing yet tighter bounds on the privacy loss.",
            "Table C reports the exact privacy budget spent by each local model trained with differential privacy as optimal (Î±,Îµ)ğ›¼ğœ€(\\alpha,\\varepsilon)-pairs."
        ]
    }
}