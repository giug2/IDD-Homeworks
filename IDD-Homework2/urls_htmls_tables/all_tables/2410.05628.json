{
    "id_table_1": {
        "caption": "Table 1:  Evaluation on Motion Reasoning task with  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  test set. Coh., Align., and Nat. denote logical coherence, content alignment, and naturalness, respectively.  Bold  indicates best performance and  underline  denotes the second best performance.",
        "table": "A1.EGx1",
        "footnotes": [],
        "references": [
            "Building upon our synthesized dataset, we present  VIM , a  V ersatile  I nteractive  M otion-language model designed for multi-turn conversations involving interactive motions. We pursue the versatility of  VIM  through a unified architecture that can simultaneously input and output both motion and text modalities. Based on the pre-trained LLMs, our training process can be divided into three stages: (1) training of the interaction motion tokenizer, (2) pre-training for motion and text representation alignment, and (3) instruction tuning with our synthesized dataset,  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , to handle more complex and multi-turn instructions. This enables  VIM  to effectively comprehend, generate, and control interactive motions, as illustrated in Figure  1 . To assess  VIM s capabilities, we introduce new evaluation protocols that evaluate its performance across various motion-related tasks. This include editing motions and reasoning about motion sequences based on contextual cues, highlighting its versatility and effectiveness in complex motion interaction scenarios.",
            "In the motion reasoning task, conversations about two interactive motions are examined to assess the models ability to deduce past or future events and comprehend the motivations driving the motions. The experimental results in Table  1  demonstrate that our unified model,  VIM , significantly outperforms two-stage approaches across all LLM-assisted and linguistic metrics. Specifically,  VIM  achieves improvements with performance increases exceeding 1.9 points in logical coherence, 1.1 points in content alignment, and nearly 0.2 points in naturalness compared to the best two-stage model. The baseline models trained solely on text-motion pair datasets, such as  VIM  w/o  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  and  MotionGPT  superscript MotionGPT \\text{MotionGPT}^{*} MotionGPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , show limited reasoning capabilities. Although  MotionGPT I  subscript superscript MotionGPT I \\text{MotionGPT}^{*}_{I} MotionGPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT , which incorporates  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  datasets, exhibits improved performance compared to baselines trained without  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , it still does not match the effectiveness of the two-stage approaches.",
            "This appendix provides a comprehensive set of supplementary materials that reinforce the main findings of the research. It covers key areas such as motion representation (Sec.  A.1 ) and  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  dataset sample visualization (Sec.  A.2 ), with accompanying dataset statistics (Sec.  A.3 ). In-depth task explanations are included (Sec.  A.4 ), alongside ablation studies that examine various pretraining methods (Sec.  A.5 ). The appendix also contains qualitative results (Sec.  A.6 ) and thorough explanations of two-stage baselines (Sec.  A.8 ). Additionally, it provides template forms for pre-training and instruction tuning (Sec.  A.9 ). We also report implementation details for MotionGPT   (Sec.  A.7 ), with implementation details for the proposed method (Sec.  A.10 ), detailed metrics explanation (Sec.  A.11 ), protocols for user subject studies (Sec.  A.12 ) focused on motion editing, prompts for data collection within the dataset (Sec.  A.13 ), and guidelines for LLM-assisted evaluation processes (Sec.  A.14 ).",
            "We visualize our result gallery on motion editing in Figure  8  and on motion reasoning in Figure  9 . Furthermore, the results for motion-to-text (Figure  10 ), text-to-motion (Figure  11 ), and reaction generation (Figure  12 ) are demonstrated.",
            "We will detail the template forms utilized during the pre-training and instruction-tuning stages of our model development. Tables  10  and  11  illustrate the specific formats employed in each stage, providing a structured approach to aligning motion data with textual descriptions and enhancing the models interactive capabilities. All the templates are from MotionGPT  (Jiang et al.,  2023 ) .",
            "During the pre-training stage, our objective is to align motion and language representations by leveraging large language models (LLMs). We design tasks such as Text-to-Motion, Motion-to-Text, Reaction Generation, and Motion Prediction using paired datasets like InterX  Xu et al. ( 2024a )  and Interhuman  Liang et al. ( 2024 ) . The pre-training templates involve generating captions from motion sequences, creating motions based on textual descriptions, producing reaction motions in response to initial motions, and predicting subsequent motions from partial sequences, as summarized in Table  10 . For single-person motion, we utilized text-to-motion, motion-to-text and motion prediction task during training.",
            "In the instruction-tuning stage, we enhance the models ability to follow diverse instructions presented in a conversational format. Utilizing the INTER2-MT dataset alongside single-turn data from previous interactive motion datasets, we format user instructions and assistant responses to facilitate multi-turn interactions. Table  11  outlines the templates used for tasks such as generating motions from user prompts, describing motions based on user queries, and predicting motion continuations. By structuring the interactions in this manner, the model becomes adept at understanding and responding to various motion-related commands, thereby improving its performance in interactive scenarios.",
            "The examples of ratings given to the user are shown in Figure  13 .",
            "Before participating in the main user studies, all participants must pass a qualifying test to ensure they understand the evaluation criteria. In this test, participants are asked to assess four samples based on three metrics: Content Alignment, Fidelity of Motion, and Quality of Motion. Among the four samples, two are high-quality and derived from the ground-truth dataset, while the other two are low-qualityone is a mismatched motion with a single instruction, and the other is generated by the least effective model, MotionGPT  . Participants must rate the low-quality samples lower than the high-quality ones in each of the three metrics. If any of the low-quality samples receive ratings that are equal to or higher than the high-quality samples in Content Alignment, Fidelity, or Quality of Motion, the participant will receive an error message and will need to adjust their ratings accordingly. This ensures that only participants who can accurately distinguish between high and low-quality motions based on the defined metrics proceed to the main study. The example of the qualifying test is demonstrated in Figure  14"
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Data-driven evaluation in motion editing in  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  test set.",
        "table": "S5.T1.11.7",
        "footnotes": [],
        "references": [
            "We utilize the Inter-X  (Xu et al.,  2024a )  and InterHuman  (Liang et al.,  2024 )  datasets as the foundation for building our datasets. We further employ GPT-4o  (OpenAI,  2024 )  to generate motion captions and conversational instructions for a variety of tasks, such as motion editing, reasoning, and story generation, enhancing the models versatility. Motion captions are sourced from these base datasets or generated by large language models (LLMs). We utilize the state-of-the-art text-to-motion diffusion model, InterGEN  (Liang et al.,  2024 ) , to generate corresponding motions that align with the generated caption from LLMs. Our data collection pipeline, shown in Figure  2 , comprises 82K samples of multi-turn conversational data involving interactive motions, including 96K of synthesized interactive motions and 56K motions from the source dataset.",
            "Motion reasoning involves predicting past or future events or interpreting current motions using prior conversational context. We utilize powerful LLMs, i.e., GPT-4o  (OpenAI,  2024 )  to assess the content alignment, naturalness, and logical coherence of the generated textual responses. Content alignment evaluates how accurately the text reflects the given motion data, logical coherence checks the consistency and reasoning accuracy of inferences made about past or future events, and naturalness evaluates the fluency of generated texts, with rating each metric on a 10-point scale. In addition, we utilized linguistic metrics like Rouge-L ( Lin ( 2004 ) ), METEOR ( Banerjee & Lavie ( 2005 ) ), and MAUVE ( Pillutla et al. ( 2021 ) ), to quantitatively assess the relevance, accuracy, and naturalness of the generated responses compared with labeled texts in  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  test dataset. We present the results on motion reasoning in  5.2 .",
            "In post hoc pairwise comparisons with  MotionGPT I  subscript superscript MotionGPT I \\text{MotionGPT}^{*}_{I} MotionGPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT , we observed significant differences, with our proposed method performing better in content similarity ( p = 0.005 p 0.005 p=0.005 italic_p = 0.005 ), instruction alignment ( p < 0.0005 p 0.0005 p<0.0005 italic_p < 0.0005 ), and motion quality ( p = 0.009 p 0.009 p=0.009 italic_p = 0.009 ). This suggests that the VQ-VAE-based tokenizer and conditional generation model negatively impacted performance. Additionally, compared to  VIM  w/o  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , there were significant differences in content similarity ( p = 0.010 p 0.010 p=0.010 italic_p = 0.010 ) and instruction alignment ( p = 0.001 p 0.001 p=0.001 italic_p = 0.001 ), indicating that without  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  data, the model struggles to control motion based on context and instructions. We also evaluated the proposed method using data-driven metrics, including FID and MPJPE, as shown in Table  2 . The proposed method outperforms the baselines on both measures, which is consistent with the results from user studies. Figure  6  illustrates the generated edited motions based on the source motion and instruction.",
            "This appendix provides a comprehensive set of supplementary materials that reinforce the main findings of the research. It covers key areas such as motion representation (Sec.  A.1 ) and  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  dataset sample visualization (Sec.  A.2 ), with accompanying dataset statistics (Sec.  A.3 ). In-depth task explanations are included (Sec.  A.4 ), alongside ablation studies that examine various pretraining methods (Sec.  A.5 ). The appendix also contains qualitative results (Sec.  A.6 ) and thorough explanations of two-stage baselines (Sec.  A.8 ). Additionally, it provides template forms for pre-training and instruction tuning (Sec.  A.9 ). We also report implementation details for MotionGPT   (Sec.  A.7 ), with implementation details for the proposed method (Sec.  A.10 ), detailed metrics explanation (Sec.  A.11 ), protocols for user subject studies (Sec.  A.12 ) focused on motion editing, prompts for data collection within the dataset (Sec.  A.13 ), and guidelines for LLM-assisted evaluation processes (Sec.  A.14 ).",
            "We visualize our result gallery on motion editing in Figure  8  and on motion reasoning in Figure  9 . Furthermore, the results for motion-to-text (Figure  10 ), text-to-motion (Figure  11 ), and reaction generation (Figure  12 ) are demonstrated."
        ]
    },
    "id_table_3": {
        "caption": "Figure 5:  User subject study results for motion editing.",
        "table": "S5.T2.10.10.6",
        "footnotes": [],
        "references": [
            "The goal of motion editing is to modify a reference motion based on user instructions. We conducted within-subject user studies to compare edited motion samples, with participants rating them on three metrics: content similarity, instruction alignment, and motion quality, using a 5-point Likert scale, following  Goel et al. ( 2024 ) . Content similarity evaluates whether the edited motion preserves the original meaning of the source motion, while instruction alignment assesses how accurately the edited motion follows the given command. The study involved 30 participants, each evaluating five samples from a set of 30 randomly selected test samples. Participants evaluated four baselines and our method, viewing randomly shuffled motion outputs side by side and providing feedback on all metrics. We also employed data-driven metrics, such as Frechet Inception Distance (FID) and mean per joint position error (MPJPE) in meters, to evaluate the quality of the generated edited motion against the labeled motions in the  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  test set, following  Goel et al. ( 2024 ) . The results on motion editing is shown in  5.3 .",
            "This appendix provides a comprehensive set of supplementary materials that reinforce the main findings of the research. It covers key areas such as motion representation (Sec.  A.1 ) and  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  dataset sample visualization (Sec.  A.2 ), with accompanying dataset statistics (Sec.  A.3 ). In-depth task explanations are included (Sec.  A.4 ), alongside ablation studies that examine various pretraining methods (Sec.  A.5 ). The appendix also contains qualitative results (Sec.  A.6 ) and thorough explanations of two-stage baselines (Sec.  A.8 ). Additionally, it provides template forms for pre-training and instruction tuning (Sec.  A.9 ). We also report implementation details for MotionGPT   (Sec.  A.7 ), with implementation details for the proposed method (Sec.  A.10 ), detailed metrics explanation (Sec.  A.11 ), protocols for user subject studies (Sec.  A.12 ) focused on motion editing, prompts for data collection within the dataset (Sec.  A.13 ), and guidelines for LLM-assisted evaluation processes (Sec.  A.14 ).",
            "The examples of ratings given to the user are shown in Figure  13 ."
        ]
    },
    "id_table_4": {
        "caption": "Table 3:  Comparisons for three motion-related tasks on Inter-X and InterHuman datasets. M2T denotes motion-to-text, T2M for text-to-motion, and Reaction Gen. for reaction generation.",
        "table": "S5.T3.9.9",
        "footnotes": [],
        "references": [
            "For standard motion-related tasks, we evaluated the proposed method in three traditional motion-relevant tasks in interactive motions: motion-to-text, text-to-motion, and reaction generation, in the union of the test set in InterHuman  (Liang et al.,  2024 )  and Inter-X  (Xu et al.,  2024a )  datasets. To evaluate the text-motion matching score, we report the retrieval precision based on the feature space of retrieval models ( Petrovich et al. ( 2023 ) ). This evaluates the accuracy of matching between texts and motions using Top 3 retrieval accuracy with a fixed batch of 32. The quality of motion was measured by Frechet Inception Distance (FID), which measures the distance of feature distribution between motion data and generated motion. In addition, we measure the mean per joint position error (MPJPE) in meters to evaluate the accuracy of the reaction motion. The results on motion-related tasks are shown in  5.4 .",
            "The improved performance of our unified model,  VIM , over two-stage approaches, appears to result from two key factors: error accumulation and interpretation ambiguity. First, in two-stage models, errors can accumulate; if the motion captioning model generates incorrect motion captions, those mistakes carry over to the second stage, reducing content alignment and coherence. In contrast, VIMs unified architecture integrates motion encoding and reasoning in a single framework, minimizing error propagation. Second, interpreting motions is not always straightforward, with multiple ways to understand and describe the same motion. In two-stage models, mapping the motion to a single caption for the second stage can lead to more contextually accurate reasoning of the given scenarios or contexts. Our unified model, however, is built to recognize these varied interpretations and generate reasoning that is more contextually accurate. Figure  4  showcases its ability to dynamically adjust interpretations and responses by incorporating context from previous conversations.",
            "We conducted ablation studies comparing the VQ-VAE-based model with our RQ-VAE-based approach, as shown in Table  4 . The RQ-VAE-based motion tokenizer outperformed the VQ-VAE model in motion reasoning tasks, achieving higher scores in coherence, alignment, and naturalness. This improvement is attributed to reduced information loss, allowing our model to capture finer motion details while also enhancing its motion-to-text retrieval precision. For generation and editing tasks, the VQ-VAE model achieved slightly better text-to-motion retrieval accuracy but performed worse in FID and MPJPE across editing, reaction generation, and T2M tasks, indicating degraded motion quality and less precise motion details. In contrast, our approach reduced MPJPE by 0.055 for reaction generation, preserving joint dynamics and producing more realistic and natural motions. VQ-VAEs limitations are especially problematic for modeling interactive motions, where precise relative positioning is crucial, making its information loss and reconstruction quality more evident.",
            "This appendix provides a comprehensive set of supplementary materials that reinforce the main findings of the research. It covers key areas such as motion representation (Sec.  A.1 ) and  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  dataset sample visualization (Sec.  A.2 ), with accompanying dataset statistics (Sec.  A.3 ). In-depth task explanations are included (Sec.  A.4 ), alongside ablation studies that examine various pretraining methods (Sec.  A.5 ). The appendix also contains qualitative results (Sec.  A.6 ) and thorough explanations of two-stage baselines (Sec.  A.8 ). Additionally, it provides template forms for pre-training and instruction tuning (Sec.  A.9 ). We also report implementation details for MotionGPT   (Sec.  A.7 ), with implementation details for the proposed method (Sec.  A.10 ), detailed metrics explanation (Sec.  A.11 ), protocols for user subject studies (Sec.  A.12 ) focused on motion editing, prompts for data collection within the dataset (Sec.  A.13 ), and guidelines for LLM-assisted evaluation processes (Sec.  A.14 ).",
            "Before participating in the main user studies, all participants must pass a qualifying test to ensure they understand the evaluation criteria. In this test, participants are asked to assess four samples based on three metrics: Content Alignment, Fidelity of Motion, and Quality of Motion. Among the four samples, two are high-quality and derived from the ground-truth dataset, while the other two are low-qualityone is a mismatched motion with a single instruction, and the other is generated by the least effective model, MotionGPT  . Participants must rate the low-quality samples lower than the high-quality ones in each of the three metrics. If any of the low-quality samples receive ratings that are equal to or higher than the high-quality samples in Content Alignment, Fidelity, or Quality of Motion, the participant will receive an error message and will need to adjust their ratings accordingly. This ensures that only participants who can accurately distinguish between high and low-quality motions based on the defined metrics proceed to the main study. The example of the qualifying test is demonstrated in Figure  14"
        ]
    },
    "id_table_5": {
        "caption": "Table 4:  Ablation Studies on motion tokenizer.",
        "table": "S5.T4.10.10",
        "footnotes": [],
        "references": [
            "Motion reasoning involves predicting past or future events or interpreting current motions using prior conversational context. We utilize powerful LLMs, i.e., GPT-4o  (OpenAI,  2024 )  to assess the content alignment, naturalness, and logical coherence of the generated textual responses. Content alignment evaluates how accurately the text reflects the given motion data, logical coherence checks the consistency and reasoning accuracy of inferences made about past or future events, and naturalness evaluates the fluency of generated texts, with rating each metric on a 10-point scale. In addition, we utilized linguistic metrics like Rouge-L ( Lin ( 2004 ) ), METEOR ( Banerjee & Lavie ( 2005 ) ), and MAUVE ( Pillutla et al. ( 2021 ) ), to quantitatively assess the relevance, accuracy, and naturalness of the generated responses compared with labeled texts in  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  test dataset. We present the results on motion reasoning in  5.2 .",
            "The goal of motion editing is to modify a reference motion based on user instructions. We conducted within-subject user studies to compare edited motion samples, with participants rating them on three metrics: content similarity, instruction alignment, and motion quality, using a 5-point Likert scale, following  Goel et al. ( 2024 ) . Content similarity evaluates whether the edited motion preserves the original meaning of the source motion, while instruction alignment assesses how accurately the edited motion follows the given command. The study involved 30 participants, each evaluating five samples from a set of 30 randomly selected test samples. Participants evaluated four baselines and our method, viewing randomly shuffled motion outputs side by side and providing feedback on all metrics. We also employed data-driven metrics, such as Frechet Inception Distance (FID) and mean per joint position error (MPJPE) in meters, to evaluate the quality of the generated edited motion against the labeled motions in the  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  test set, following  Goel et al. ( 2024 ) . The results on motion editing is shown in  5.3 .",
            "For standard motion-related tasks, we evaluated the proposed method in three traditional motion-relevant tasks in interactive motions: motion-to-text, text-to-motion, and reaction generation, in the union of the test set in InterHuman  (Liang et al.,  2024 )  and Inter-X  (Xu et al.,  2024a )  datasets. To evaluate the text-motion matching score, we report the retrieval precision based on the feature space of retrieval models ( Petrovich et al. ( 2023 ) ). This evaluates the accuracy of matching between texts and motions using Top 3 retrieval accuracy with a fixed batch of 32. The quality of motion was measured by Frechet Inception Distance (FID), which measures the distance of feature distribution between motion data and generated motion. In addition, we measure the mean per joint position error (MPJPE) in meters to evaluate the accuracy of the reaction motion. The results on motion-related tasks are shown in  5.4 .",
            "We aim to validate the hypothesis that people will perceive the generated edited interactive motion from the proposed method to be more content-consistent, instruction-aligned, and better quality, through user subject studies. To analyze the results, we conducted a repeated-measures multivariate analysis of variance on the rated measures. We observed that methods significantly affect the users perception of all dimensions;  F  ( 4 ) = 4.591 , p = 0.002 ,  2 = 0.137 formulae-sequence F 4 4.591 formulae-sequence p 0.002 superscript  2 0.137 F(4)=4.591,p=0.002,\\eta^{2}=0.137 italic_F ( 4 ) = 4.591 , italic_p = 0.002 , italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 0.137  for content similarity,  F  ( 4 ) = 7.134 , p = 0.000 ,  2 = 0.197 formulae-sequence F 4 7.134 formulae-sequence p 0.000 superscript  2 0.197 F(4)=7.134,p=0.000,\\eta^{2}=0.197 italic_F ( 4 ) = 7.134 , italic_p = 0.000 , italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 0.197  for instruction alignment, and  F  ( 4 ) = 4.781 , p = 0.001 ,  2 = 0.142 formulae-sequence F 4 4.781 formulae-sequence p 0.001 superscript  2 0.142 F(4)=4.781,p=0.001,\\eta^{2}=0.142 italic_F ( 4 ) = 4.781 , italic_p = 0.001 , italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 0.142  for motion quality, with all   = 0.05  0.05 \\alpha=0.05 italic_ = 0.05 . The estimated marginal mean of the rated score is reported in Figure  5 1 1 1 We plotted the difference in a post hoc pairwise comparison of the proposed method only. We denote * as  0.01 < p < 0.05 0.01 p 0.05 0.01<p<0.05 0.01 < italic_p < 0.05 , ** as  p < 0.01 p 0.01 p<0.01 italic_p < 0.01 , and *** as  p < 0.001 p 0.001 p<0.001 italic_p < 0.001 . The error bars represent 95% confidence intervals.  . The results show that the proposed method had better instruction alignment, quality, and content consistency across other baselines with significant differences.",
            "This appendix provides a comprehensive set of supplementary materials that reinforce the main findings of the research. It covers key areas such as motion representation (Sec.  A.1 ) and  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  dataset sample visualization (Sec.  A.2 ), with accompanying dataset statistics (Sec.  A.3 ). In-depth task explanations are included (Sec.  A.4 ), alongside ablation studies that examine various pretraining methods (Sec.  A.5 ). The appendix also contains qualitative results (Sec.  A.6 ) and thorough explanations of two-stage baselines (Sec.  A.8 ). Additionally, it provides template forms for pre-training and instruction tuning (Sec.  A.9 ). We also report implementation details for MotionGPT   (Sec.  A.7 ), with implementation details for the proposed method (Sec.  A.10 ), detailed metrics explanation (Sec.  A.11 ), protocols for user subject studies (Sec.  A.12 ) focused on motion editing, prompts for data collection within the dataset (Sec.  A.13 ), and guidelines for LLM-assisted evaluation processes (Sec.  A.14 )."
        ]
    },
    "id_table_6": {
        "caption": "Table 5:  Statistics on  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .",
        "table": "A1.EGx2",
        "footnotes": [],
        "references": [
            "In post hoc pairwise comparisons with  MotionGPT I  subscript superscript MotionGPT I \\text{MotionGPT}^{*}_{I} MotionGPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT , we observed significant differences, with our proposed method performing better in content similarity ( p = 0.005 p 0.005 p=0.005 italic_p = 0.005 ), instruction alignment ( p < 0.0005 p 0.0005 p<0.0005 italic_p < 0.0005 ), and motion quality ( p = 0.009 p 0.009 p=0.009 italic_p = 0.009 ). This suggests that the VQ-VAE-based tokenizer and conditional generation model negatively impacted performance. Additionally, compared to  VIM  w/o  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , there were significant differences in content similarity ( p = 0.010 p 0.010 p=0.010 italic_p = 0.010 ) and instruction alignment ( p = 0.001 p 0.001 p=0.001 italic_p = 0.001 ), indicating that without  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  data, the model struggles to control motion based on context and instructions. We also evaluated the proposed method using data-driven metrics, including FID and MPJPE, as shown in Table  2 . The proposed method outperforms the baselines on both measures, which is consistent with the results from user studies. Figure  6  illustrates the generated edited motions based on the source motion and instruction.",
            "This appendix provides a comprehensive set of supplementary materials that reinforce the main findings of the research. It covers key areas such as motion representation (Sec.  A.1 ) and  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  dataset sample visualization (Sec.  A.2 ), with accompanying dataset statistics (Sec.  A.3 ). In-depth task explanations are included (Sec.  A.4 ), alongside ablation studies that examine various pretraining methods (Sec.  A.5 ). The appendix also contains qualitative results (Sec.  A.6 ) and thorough explanations of two-stage baselines (Sec.  A.8 ). Additionally, it provides template forms for pre-training and instruction tuning (Sec.  A.9 ). We also report implementation details for MotionGPT   (Sec.  A.7 ), with implementation details for the proposed method (Sec.  A.10 ), detailed metrics explanation (Sec.  A.11 ), protocols for user subject studies (Sec.  A.12 ) focused on motion editing, prompts for data collection within the dataset (Sec.  A.13 ), and guidelines for LLM-assisted evaluation processes (Sec.  A.14 ).",
            "The quality of these motions is detailed in Table  6 . From the generated caption from a large language model, we evaluate the text-motion matching score based on retrieval precision based on the feature space of retrieval models from  Petrovich et al. ( 2023 ) . This evaluates the accuracy of matching between texts and motions using Top 3 retrieval accuracy with a fixed batch of 32. The tables first row shows the retrieval models performance, with a Top 3 retrieval precision of  0.870 0.870 0.870 0.870 . We found that the synthesized motions achieve a Top 3 retrieval precision of  0.668 0.668 0.668 0.668 , closely aligning with the reported precision of  0.645 0.645 0.645 0.645  from the text-to-motion diffusion model ( Liang et al. ( 2024 ) ). This demonstrates that the synthesized motions maintain a high level of quality, making the dataset valuable and suitable for further training and development."
        ]
    },
    "id_table_7": {
        "caption": "Table 6:  Comparison of retrieval precision, motion diversity (Div.), and motion quality metrics (MMDist. and FID) across synthesized and source motions. The synthesized motion dataset (96K pairs) shows a Top 3 retrieval precision of 0.668, comparable to the InterGEN models precision (0.645) on the InterX+H dataset, indicating competitive text-to-motion matching quality.",
        "table": "A1.T5.7",
        "footnotes": [],
        "references": [
            "The results in Table  7  support our hypothesis that utilizing the  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  dataset enhances the models performance in traditional motion tasks like motion-to-text (M2T), text-to-motion (T2M), and reaction generation. The first row (Real) shows retrieval accuracy, and FID scores from the dataset labels. Note that both  VIM  w/o  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  and MotionGPT   were trained on all of these tasks for fair comparison. Comparing the  VIM  w/o  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  to the version trained with  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  (Ours), we see improvements across all tasks. In M2T, Top-3 retrieval accuracy rose from 0.894 to 0.901. For T2M, Top-3 retrieval accuracy increased from 0.561 to 0.568, with FID dropping from 0.082 to 0.059, indicating better motion generation. In reaction generation, MPJPE dropped from 0.984 to 0.691, and FID from 0.031 to 0.019, confirming that multi-turn datasets improve motion comprehension and generation. Using the  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  dataset provides diverse, context-rich examples, helping the model learn more nuanced relationships between text and motion. Additionally, incorporating  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  in  MotionGPT  superscript MotionGPT \\text{MotionGPT}^{*} MotionGPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , denoted as  MotionGPT I  subscript superscript MotionGPT I \\text{MotionGPT}^{*}_{I} MotionGPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT , improved retrieval precision accuracy for M2T and T2M tasks, and joint position error in reaction generation.",
            "This appendix provides a comprehensive set of supplementary materials that reinforce the main findings of the research. It covers key areas such as motion representation (Sec.  A.1 ) and  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  dataset sample visualization (Sec.  A.2 ), with accompanying dataset statistics (Sec.  A.3 ). In-depth task explanations are included (Sec.  A.4 ), alongside ablation studies that examine various pretraining methods (Sec.  A.5 ). The appendix also contains qualitative results (Sec.  A.6 ) and thorough explanations of two-stage baselines (Sec.  A.8 ). Additionally, it provides template forms for pre-training and instruction tuning (Sec.  A.9 ). We also report implementation details for MotionGPT   (Sec.  A.7 ), with implementation details for the proposed method (Sec.  A.10 ), detailed metrics explanation (Sec.  A.11 ), protocols for user subject studies (Sec.  A.12 ) focused on motion editing, prompts for data collection within the dataset (Sec.  A.13 ), and guidelines for LLM-assisted evaluation processes (Sec.  A.14 ).",
            "The samples from the synthesized dataset,  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , are illustrated in Figure  7 .",
            "We conducted ablation studies on the pertaining method. All the baselines are pre-trained models, not including the fine-tuning stage. To evaluate the effectiveness of our pretraining approach, we conducted ablation studies comparing different methods on three motion-related tasks: Motion-to-Text (M2T), Text-to-Motion (T2M), and Reaction Generation. As shown in Table  7 , we compared our proposed method, VIM, against MotionGPT   and VIM-VQ, using the InterX  (Xu et al.,  2024a )  and Interhuman (H) datasets  (Liang et al.,  2024 ) . MotionGPT   serves as a baseline with 248M trainable parameters, achieving a retrieval Top3 score of 0.518 in M2T and 0.280 in T2M, with corresponding FID scores of 0.178 and 1.338 for T2M and Reaction Generation, respectively. VIM-VQ, with 726M parameters, improves the M2T retrieval Top3 to 0.709 and T2M retrieval Top3 to 0.511, while maintaining competitive FID scores."
        ]
    },
    "id_table_8": {
        "caption": "Table 7:  Ablation studies in pertaining stage for three motion-related tasks on InterX and Interhuman dataset.",
        "table": "A1.T6.1",
        "footnotes": [
            ""
        ],
        "references": [
            "This appendix provides a comprehensive set of supplementary materials that reinforce the main findings of the research. It covers key areas such as motion representation (Sec.  A.1 ) and  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  dataset sample visualization (Sec.  A.2 ), with accompanying dataset statistics (Sec.  A.3 ). In-depth task explanations are included (Sec.  A.4 ), alongside ablation studies that examine various pretraining methods (Sec.  A.5 ). The appendix also contains qualitative results (Sec.  A.6 ) and thorough explanations of two-stage baselines (Sec.  A.8 ). Additionally, it provides template forms for pre-training and instruction tuning (Sec.  A.9 ). We also report implementation details for MotionGPT   (Sec.  A.7 ), with implementation details for the proposed method (Sec.  A.10 ), detailed metrics explanation (Sec.  A.11 ), protocols for user subject studies (Sec.  A.12 ) focused on motion editing, prompts for data collection within the dataset (Sec.  A.13 ), and guidelines for LLM-assisted evaluation processes (Sec.  A.14 ).",
            "We visualize our result gallery on motion editing in Figure  8  and on motion reasoning in Figure  9 . Furthermore, the results for motion-to-text (Figure  10 ), text-to-motion (Figure  11 ), and reaction generation (Figure  12 ) are demonstrated.",
            "We first trained TM2T model with the InterHuman dataset  (Liang et al.,  2024 ) and the InterX  Xu et al. ( 2024a )  dataset, which we denote as TM2T   {}^{*}* start_FLOATSUPERSCRIPT  end_FLOATSUPERSCRIPT  . The performance is shown in Table  8 . The TM2T   model shows substantial improvements over the baseline MotionGPT   models across all evaluation metrics. Specifically, TM2T   achieves Retriveal Precision scores of 0.413 (Top1), 0.589 (Top2), and 0.696 (Top3), along with BLEU, METEOR, and Rouge-L scores of 0.192, 0.386, and 0.395, respectively. These results indicate that the task-specific TM2T   model effectively generates accurate and relevant motion captions, making it a reliable choice for motion editing tasks. Although there remains a performance gap compared to the proposed method, the TM2T   model provides a robust foundation for generating moderate-quality motion captions."
        ]
    },
    "id_table_9": {
        "caption": "Table 8:  Motion-to-Text performance for TM2T",
        "table": "A1.T7.6.6",
        "footnotes": [],
        "references": [
            "This appendix provides a comprehensive set of supplementary materials that reinforce the main findings of the research. It covers key areas such as motion representation (Sec.  A.1 ) and  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  dataset sample visualization (Sec.  A.2 ), with accompanying dataset statistics (Sec.  A.3 ). In-depth task explanations are included (Sec.  A.4 ), alongside ablation studies that examine various pretraining methods (Sec.  A.5 ). The appendix also contains qualitative results (Sec.  A.6 ) and thorough explanations of two-stage baselines (Sec.  A.8 ). Additionally, it provides template forms for pre-training and instruction tuning (Sec.  A.9 ). We also report implementation details for MotionGPT   (Sec.  A.7 ), with implementation details for the proposed method (Sec.  A.10 ), detailed metrics explanation (Sec.  A.11 ), protocols for user subject studies (Sec.  A.12 ) focused on motion editing, prompts for data collection within the dataset (Sec.  A.13 ), and guidelines for LLM-assisted evaluation processes (Sec.  A.14 ).",
            "We visualize our result gallery on motion editing in Figure  8  and on motion reasoning in Figure  9 . Furthermore, the results for motion-to-text (Figure  10 ), text-to-motion (Figure  11 ), and reaction generation (Figure  12 ) are demonstrated.",
            "Next, we trained the text-to-motion diffusion model, InterGEN for the second stage. The performance of this model is reported in Table  9 . InterGEN exhibits strong performance across all evaluation metrics, validating its effectiveness as the second stage in our two-stage approach. Specifically, InterGEN achieves Retrieval Precision scores of 0.403 (Top1), 0.557 (Top2), and 0.645 (Top3), which are substantially higher than those of the baseline MotionGPT   (0.180, 0.262, 0.328) and our unified VIM model (0.318, 0.469, 0.568). Additionally, InterGEN excels in Diversity with a score of 0.957 and maintains a low Maximum Mean Discrepancy (MMDist) of 1.115, indicating high-quality and varied motion generation. Its FID score of 0.078 is notably competitive, reflecting the realism and coherence of the generated motions. These results validate the use of InterGEN as the second stage in our framework."
        ]
    },
    "id_table_10": {
        "caption": "Table 9:  Text-to-Motion performance for InterGEN",
        "table": "A1.T8.9.9",
        "footnotes": [],
        "references": [
            "This appendix provides a comprehensive set of supplementary materials that reinforce the main findings of the research. It covers key areas such as motion representation (Sec.  A.1 ) and  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  dataset sample visualization (Sec.  A.2 ), with accompanying dataset statistics (Sec.  A.3 ). In-depth task explanations are included (Sec.  A.4 ), alongside ablation studies that examine various pretraining methods (Sec.  A.5 ). The appendix also contains qualitative results (Sec.  A.6 ) and thorough explanations of two-stage baselines (Sec.  A.8 ). Additionally, it provides template forms for pre-training and instruction tuning (Sec.  A.9 ). We also report implementation details for MotionGPT   (Sec.  A.7 ), with implementation details for the proposed method (Sec.  A.10 ), detailed metrics explanation (Sec.  A.11 ), protocols for user subject studies (Sec.  A.12 ) focused on motion editing, prompts for data collection within the dataset (Sec.  A.13 ), and guidelines for LLM-assisted evaluation processes (Sec.  A.14 ).",
            "We visualize our result gallery on motion editing in Figure  8  and on motion reasoning in Figure  9 . Furthermore, the results for motion-to-text (Figure  10 ), text-to-motion (Figure  11 ), and reaction generation (Figure  12 ) are demonstrated.",
            "We will detail the template forms utilized during the pre-training and instruction-tuning stages of our model development. Tables  10  and  11  illustrate the specific formats employed in each stage, providing a structured approach to aligning motion data with textual descriptions and enhancing the models interactive capabilities. All the templates are from MotionGPT  (Jiang et al.,  2023 ) .",
            "During the pre-training stage, our objective is to align motion and language representations by leveraging large language models (LLMs). We design tasks such as Text-to-Motion, Motion-to-Text, Reaction Generation, and Motion Prediction using paired datasets like InterX  Xu et al. ( 2024a )  and Interhuman  Liang et al. ( 2024 ) . The pre-training templates involve generating captions from motion sequences, creating motions based on textual descriptions, producing reaction motions in response to initial motions, and predicting subsequent motions from partial sequences, as summarized in Table  10 . For single-person motion, we utilized text-to-motion, motion-to-text and motion prediction task during training."
        ]
    },
    "id_table_11": {
        "caption": "Table 10:  Template for Pretraining",
        "table": "A1.T9.9.9",
        "footnotes": [],
        "references": [
            "This appendix provides a comprehensive set of supplementary materials that reinforce the main findings of the research. It covers key areas such as motion representation (Sec.  A.1 ) and  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  dataset sample visualization (Sec.  A.2 ), with accompanying dataset statistics (Sec.  A.3 ). In-depth task explanations are included (Sec.  A.4 ), alongside ablation studies that examine various pretraining methods (Sec.  A.5 ). The appendix also contains qualitative results (Sec.  A.6 ) and thorough explanations of two-stage baselines (Sec.  A.8 ). Additionally, it provides template forms for pre-training and instruction tuning (Sec.  A.9 ). We also report implementation details for MotionGPT   (Sec.  A.7 ), with implementation details for the proposed method (Sec.  A.10 ), detailed metrics explanation (Sec.  A.11 ), protocols for user subject studies (Sec.  A.12 ) focused on motion editing, prompts for data collection within the dataset (Sec.  A.13 ), and guidelines for LLM-assisted evaluation processes (Sec.  A.14 ).",
            "We visualize our result gallery on motion editing in Figure  8  and on motion reasoning in Figure  9 . Furthermore, the results for motion-to-text (Figure  10 ), text-to-motion (Figure  11 ), and reaction generation (Figure  12 ) are demonstrated.",
            "We will detail the template forms utilized during the pre-training and instruction-tuning stages of our model development. Tables  10  and  11  illustrate the specific formats employed in each stage, providing a structured approach to aligning motion data with textual descriptions and enhancing the models interactive capabilities. All the templates are from MotionGPT  (Jiang et al.,  2023 ) .",
            "In the instruction-tuning stage, we enhance the models ability to follow diverse instructions presented in a conversational format. Utilizing the INTER2-MT dataset alongside single-turn data from previous interactive motion datasets, we format user instructions and assistant responses to facilitate multi-turn interactions. Table  11  outlines the templates used for tasks such as generating motions from user prompts, describing motions based on user queries, and predicting motion continuations. By structuring the interactions in this manner, the model becomes adept at understanding and responding to various motion-related commands, thereby improving its performance in interactive scenarios."
        ]
    },
    "id_table_12": {
        "caption": "Table 11:  Template for Instruction Tunning",
        "table": "A1.T10.1",
        "footnotes": [],
        "references": [
            "This appendix provides a comprehensive set of supplementary materials that reinforce the main findings of the research. It covers key areas such as motion representation (Sec.  A.1 ) and  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  dataset sample visualization (Sec.  A.2 ), with accompanying dataset statistics (Sec.  A.3 ). In-depth task explanations are included (Sec.  A.4 ), alongside ablation studies that examine various pretraining methods (Sec.  A.5 ). The appendix also contains qualitative results (Sec.  A.6 ) and thorough explanations of two-stage baselines (Sec.  A.8 ). Additionally, it provides template forms for pre-training and instruction tuning (Sec.  A.9 ). We also report implementation details for MotionGPT   (Sec.  A.7 ), with implementation details for the proposed method (Sec.  A.10 ), detailed metrics explanation (Sec.  A.11 ), protocols for user subject studies (Sec.  A.12 ) focused on motion editing, prompts for data collection within the dataset (Sec.  A.13 ), and guidelines for LLM-assisted evaluation processes (Sec.  A.14 ).",
            "We visualize our result gallery on motion editing in Figure  8  and on motion reasoning in Figure  9 . Furthermore, the results for motion-to-text (Figure  10 ), text-to-motion (Figure  11 ), and reaction generation (Figure  12 ) are demonstrated."
        ]
    },
    "id_table_13": {
        "caption": "",
        "table": "A1.T11.2.2",
        "footnotes": [],
        "references": [
            "This appendix provides a comprehensive set of supplementary materials that reinforce the main findings of the research. It covers key areas such as motion representation (Sec.  A.1 ) and  Inter - MT 2 superscript MT 2 \\text{MT}^{2} MT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  dataset sample visualization (Sec.  A.2 ), with accompanying dataset statistics (Sec.  A.3 ). In-depth task explanations are included (Sec.  A.4 ), alongside ablation studies that examine various pretraining methods (Sec.  A.5 ). The appendix also contains qualitative results (Sec.  A.6 ) and thorough explanations of two-stage baselines (Sec.  A.8 ). Additionally, it provides template forms for pre-training and instruction tuning (Sec.  A.9 ). We also report implementation details for MotionGPT   (Sec.  A.7 ), with implementation details for the proposed method (Sec.  A.10 ), detailed metrics explanation (Sec.  A.11 ), protocols for user subject studies (Sec.  A.12 ) focused on motion editing, prompts for data collection within the dataset (Sec.  A.13 ), and guidelines for LLM-assisted evaluation processes (Sec.  A.14 ).",
            "The examples of ratings given to the user are shown in Figure  13 ."
        ]
    }
}