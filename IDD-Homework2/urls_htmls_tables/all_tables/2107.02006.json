{
    "PAPER'S NUMBER OF TABLES": 1,
    "S4.T1": {
        "caption": "TABLE I: \nOverhead associated with NL-FL\n\n",
        "table": "<table id=\"S4.T1.2\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.2.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.3.1.1\" class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S4.T1.2.3.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T1.2.3.1.1.1.1\" class=\"ltx_p\"><span id=\"S4.T1.2.3.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Metric</span></span>\n</span>\n</th>\n<th id=\"S4.T1.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T1.2.3.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">FL</span></th>\n<th id=\"S4.T1.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T1.2.3.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">NL-FL</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.2.4.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.4.1.1\" class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt\">\n<span id=\"S4.T1.2.4.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T1.2.4.1.1.1.1\" class=\"ltx_p\"><span id=\"S4.T1.2.4.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Training time</span></span>\n</span>\n</th>\n<td id=\"S4.T1.2.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\"><span id=\"S4.T1.2.4.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">27 ms/sample</span></td>\n</tr>\n<tr id=\"S4.T1.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.2.3\" class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S4.T1.2.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T1.2.2.3.1.1\" class=\"ltx_p\"><span id=\"S4.T1.2.2.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Inference time</span></span>\n</span>\n</th>\n<td id=\"S4.T1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S4.T1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">130 </span><math id=\"S4.T1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\mu\" display=\"inline\"><semantics id=\"S4.T1.1.1.1.m1.1a\"><mi mathsize=\"80%\" id=\"S4.T1.1.1.1.m1.1.1\" xref=\"S4.T1.1.1.1.m1.1.1.cmml\">μ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.1.1.1.m1.1b\"><ci id=\"S4.T1.1.1.1.m1.1.1.cmml\" xref=\"S4.T1.1.1.1.m1.1.1\">𝜇</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.1.1.1.m1.1c\">\\mu</annotation></semantics></math><span id=\"S4.T1.1.1.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">s/sample</span>\n</td>\n<td id=\"S4.T1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S4.T1.2.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">153 </span><math id=\"S4.T1.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\mu\" display=\"inline\"><semantics id=\"S4.T1.2.2.2.m1.1a\"><mi mathsize=\"80%\" id=\"S4.T1.2.2.2.m1.1.1\" xref=\"S4.T1.2.2.2.m1.1.1.cmml\">μ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.2.2.2.m1.1b\"><ci id=\"S4.T1.2.2.2.m1.1.1.cmml\" xref=\"S4.T1.2.2.2.m1.1.1\">𝜇</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.2.2.2.m1.1c\">\\mu</annotation></semantics></math><span id=\"S4.T1.2.2.2.2\" class=\"ltx_text\" style=\"font-size:80%;\">s/sample</span>\n</td>\n</tr>\n<tr id=\"S4.T1.2.5.2\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.5.2.1\" class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S4.T1.2.5.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T1.2.5.2.1.1.1\" class=\"ltx_p\"><span id=\"S4.T1.2.5.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Model size</span></span>\n</span>\n</th>\n<td id=\"S4.T1.2.5.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\"><span id=\"S4.T1.2.5.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">3.41 MByte</span></td>\n</tr>\n<tr id=\"S4.T1.2.6.3\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.6.3.1\" class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S4.T1.2.6.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T1.2.6.3.1.1.1\" class=\"ltx_p\"><span id=\"S4.T1.2.6.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Data transfer rate</span></span>\n</span>\n</th>\n<td id=\"S4.T1.2.6.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\"><span id=\"S4.T1.2.6.3.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">52.5 kbit/s</span></td>\n</tr>\n<tr id=\"S4.T1.2.7.4\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.7.4.1\" class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S4.T1.2.7.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T1.2.7.4.1.1.1\" class=\"ltx_p\"><span id=\"S4.T1.2.7.4.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Similarity info. size</span></span>\n</span>\n</th>\n<td id=\"S4.T1.2.7.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T1.2.7.4.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">–</span></td>\n<td id=\"S4.T1.2.7.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T1.2.7.4.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">7.2 kByte/epoch/node</span></td>\n</tr>\n<tr id=\"S4.T1.2.8.5\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.8.5.1\" class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S4.T1.2.8.5.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T1.2.8.5.1.1.1\" class=\"ltx_p\"><span id=\"S4.T1.2.8.5.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Relevance info. size</span></span>\n</span>\n</th>\n<td id=\"S4.T1.2.8.5.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T1.2.8.5.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">–</span></td>\n<td id=\"S4.T1.2.8.5.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T1.2.8.5.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">4 kByte/sample</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "A first question we seek to answer is to which extent a single misbehaving node mislabeling a single character can affect\nthe overall learning process. To this end, in Fig. ",
                "4",
                " we plot the classification accuracy for the whole dataset (yellow bars) and for the symbol ",
                "x",
                "𝑥",
                "x",
                " (blue bars) in three scenarios, namely: (i) when all nodes behave correctly; (ii) when one node misbehaves as detailed above, and (iii) when the misbehaving node is removed, and only the remaining nine take part in the learning.",
                "The average accuracy is not significantly affected by the presence of the misbehaving node; after all, only a small number of images is misclassified.\nFocusing instead on the occurrences of letter ",
                "x",
                "𝑥",
                "x",
                " (blue bars) and comparing the first and second groups of bars,\nit is possible to see a visible drop in the accuracy, which is even more serious because letter ",
                "x",
                "𝑥",
                "x",
                " is not very well\nclassified in the first place. Notice how such a significant effect is obtained by a single misbehaving node,\nin spite of the nine correct ones. Finally, the rightmost group of bars shows that identifying and removing\nthe misbehaving node is sufficient to essentially restore the original accuracy.",
                "We now seek to understand the usefulness of the ",
                "RAdist",
                " metric in identifying such a node.\nTo this end, Fig. ",
                "5",
                " shows the normalized ",
                "score",
                " for correct (gray) and misbehaving (red) nodes,\nwhere “score” is defined as:\n(i) cosine distance ",
                "[",
                "11",
                "]",
                " (dotted lines);\n(ii) a reputation system similar to ",
                "[",
                "12",
                "]",
                " (dashed lines), where the reputation of a node reflects its ability to properly classify its local dataset;\n(iii) ",
                "RAdist",
                " (solid lines).\nThe difference is very clear: dotted and dashed lines tend to lie close to each other, as both cosine distance and reputation tend to be swayed by the fact that even the misbehaving node behaves correctly ",
                "most of the times",
                ", and that even correct nodes may occasionally provide wrong results.\nOn the other hand, when moving to the ",
                "RAdist",
                " metric (solid lines), the misbehaving node emerges as having a significantly score distance than the others, for all epochs. In other words, relevance values act like a magnifying glass over the differences between parameters coming from correct and misbehaving nodes, allowing such differences to clearly emerge and drive decisions about client selection in FL.\nThis, in turn, allows us to neutralize the effect of the misbehaving nodes on the resulting accuracy, as shown in Fig. ",
                "4",
                ": specifically, leveraging the ",
                "RAdist",
                " metric we can move from the second to the third group of bars in Fig. ",
                "4",
                ".",
                "Last, Tab. ",
                "I",
                " summarizes the overhead of NL-FL in terms of computational time, additional storage, and network latency.\nAs discussed in Sec. ",
                "III",
                ", we can observe that NL-FL comes at a modest cost in terms of inference time, and the learning\nserver has to store additional information concerning how close updates from different nodes are, and relevance values for\neach sample.\nThe space taken by similarity information grows linearly with the number of learning nodes, e.g.,\na scenario including 1,000 learning nodes executing 100 epochs would result in\n720 MByte\nof similarity information – an acceptable overhead for a server with sufficient capabilities\nto coordinate 1,000 nodes. The size of relevance information\ncan become significant for very large-scale datasets. As discussed later, its impact can be reduced by activating NL-FL only when needed.\nImportantly, relevance and similarity information are created and stored locally at the coordinator, hence, neither contributes to the network overhead.\nIn Tab. ",
                "I",
                ", it is also important to observe the quantities that do ",
                "not",
                " change across columns.\nSpecifically, NL-FL changes neither the training performance (which is usually the most time-consuming part of learning),\nnor the network overhead. Furthermore, it places no additional burden (i.e., no extra computation or storage requirements)\non the learning nodes."
            ]
        ]
    }
}