{
    "S1.T1": {
        "caption": "Table S1: Table of quantizations procedures applied on the used LLMs. See https://github.com/ggerganov/llama.cpp.",
        "table": "<table id=\"S1.T1.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S1.T1.1.1\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\"><span id=\"S1.T1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">model</span></td>\n<td id=\"S1.T1.1.1.2\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S1.T1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Quantization type (size in GB)</span></td>\n</tr>\n<tr id=\"S1.T1.1.2\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S1.T1.1.2.1.1\" class=\"ltx_text ltx_font_bold\">Llama-7B</span></td>\n<td id=\"S1.T1.1.2.2\" class=\"ltx_td ltx_align_right ltx_border_t\">q8 (6.8 GB)</td>\n</tr>\n<tr id=\"S1.T1.1.3\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.3.1\" class=\"ltx_td ltx_align_left\"><span id=\"S1.T1.1.3.1.1\" class=\"ltx_text ltx_font_bold\">Llama-13B</span></td>\n<td id=\"S1.T1.1.3.2\" class=\"ltx_td ltx_align_right\">q8 (13.2 GB)</td>\n</tr>\n<tr id=\"S1.T1.1.4\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.4.1\" class=\"ltx_td ltx_align_left\"><span id=\"S1.T1.1.4.1.1\" class=\"ltx_text ltx_font_bold\">Llama-30B</span></td>\n<td id=\"S1.T1.1.4.2\" class=\"ltx_td ltx_align_right\">q5_K_M (21.9GB)</td>\n</tr>\n<tr id=\"S1.T1.1.5\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.5.1\" class=\"ltx_td ltx_align_left\"><span id=\"S1.T1.1.5.1.1\" class=\"ltx_text ltx_font_bold\">Llama-65B</span></td>\n<td id=\"S1.T1.1.5.2\" class=\"ltx_td ltx_align_right\">q5_K_M (44.1GB)</td>\n</tr>\n<tr id=\"S1.T1.1.6\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.6.1\" class=\"ltx_td ltx_align_left\"><span id=\"S1.T1.1.6.1.1\" class=\"ltx_text ltx_font_bold\">Alpaca-7B</span></td>\n<td id=\"S1.T1.1.6.2\" class=\"ltx_td ltx_align_right\">q8 (6.8 GB)</td>\n</tr>\n<tr id=\"S1.T1.1.7\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.7.1\" class=\"ltx_td ltx_align_left ltx_border_bb\"><span id=\"S1.T1.1.7.1.1\" class=\"ltx_text ltx_font_bold\">Vicuna-13B</span></td>\n<td id=\"S1.T1.1.7.2\" class=\"ltx_td ltx_align_right ltx_border_bb\">q8 (13.2 GB)</td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "Considering their particular fine-tuning, small adjustments were provided to the prompt presented in Figure S1 for Alpaca-7B and Vicuna-13B. All models were also quantized for memory efficient inferences (Table S1). Considering our available resources, we were not able to use the q8 (8 bits) quantization for LLaMA models >13\u200bBabsent13\ud835\udc35>13B and improvements in performance could then be expected. In parallel, we noticed significant performance degradations when using q\u200b4\ud835\udc5e4q4 (4 bits). We used llama.cpp111111https://github.com/ggerganov/llama.cpp for quantization and inferences."
        ]
    },
    "S1.T2": {
        "caption": "Table S2: Statistics of the number of trainable parameters per evaluated models.",
        "table": "<table id=\"S1.T2.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S1.T2.1.1\" class=\"ltx_tr\">\n<td id=\"S1.T2.1.1.1\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"S1.T2.1.1.2\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S1.T2.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Total parameters</span></td>\n<td id=\"S1.T2.1.1.3\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S1.T2.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Trainable parameters</span></td>\n</tr>\n<tr id=\"S1.T2.1.2\" class=\"ltx_tr\">\n<td id=\"S1.T2.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S1.T2.1.2.1.1\" class=\"ltx_text ltx_font_bold\">Seq2rel</span></td>\n<td id=\"S1.T2.1.2.2\" class=\"ltx_td ltx_align_right ltx_border_t\">118546185</td>\n<td id=\"S1.T2.1.2.3\" class=\"ltx_td ltx_align_right ltx_border_t\">118546185 (100%)</td>\n</tr>\n<tr id=\"S1.T2.1.3\" class=\"ltx_tr\">\n<td id=\"S1.T2.1.3.1\" class=\"ltx_td ltx_align_left\"><span id=\"S1.T2.1.3.1.1\" class=\"ltx_text ltx_font_bold\">BioGPT</span></td>\n<td id=\"S1.T2.1.3.2\" class=\"ltx_td ltx_align_right\">350649472</td>\n<td id=\"S1.T2.1.3.3\" class=\"ltx_td ltx_align_right\">3886208 (1.11%)</td>\n</tr>\n<tr id=\"S1.T2.1.4\" class=\"ltx_tr\">\n<td id=\"S1.T2.1.4.1\" class=\"ltx_td ltx_align_left\"><span id=\"S1.T2.1.4.1.1\" class=\"ltx_text ltx_font_bold\">BioGPT-Large</span></td>\n<td id=\"S1.T2.1.4.2\" class=\"ltx_td ltx_align_right\">1582722536</td>\n<td id=\"S1.T2.1.4.3\" class=\"ltx_td ltx_align_right\">11533736 (0.73%)</td>\n</tr>\n<tr id=\"S1.T2.1.5\" class=\"ltx_tr\">\n<td id=\"S1.T2.1.5.1\" class=\"ltx_td ltx_align_left ltx_border_bb\"><span id=\"S1.T2.1.5.1.1\" class=\"ltx_text ltx_font_bold\">GPT-2 <span id=\"S1.T2.1.5.1.1.1\" class=\"ltx_text ltx_font_italic\">Medium</span></span></td>\n<td id=\"S1.T2.1.5.2\" class=\"ltx_td ltx_align_right ltx_border_bb\">358381208</td>\n<td id=\"S1.T2.1.5.3\" class=\"ltx_td ltx_align_right ltx_border_bb\">3555992 (0.99%)</td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "Dettmers et\u00a0al. [73] demonstrated the efficacy of the QLoRA approach by showing that the loss in performance due to quantization can be fully recovered through subsequent fine-tuning of the adapters and that increasing the number of adapters is crucial to match full fine-tuning performance. By exploiting the memory benefits of the NF4 data type, we applied LoRA adapters to all linear blocks (except the initial embeddings layer) of the BioGPT and GPT-2 models. Details on the number of trained parameters are presented in Table S2. During training, the special tokens <BOS> and <EOS> are used to delimitate the input X\ud835\udc4bX and the expected linearised output Y\ud835\udc4cY, such as [X\ud835\udc4bX, <EOS> <BOS>, Y\ud835\udc4cY, <EOS>]. The <BOS> token triggers the RE task at inference time.\nFor BioGPT, we evaluated different hyperparameter settings using Optuna [112]. We applied the TPE (Tree-structured Parzen Estimator) algorithm with a pruner on median to speed up the process. The hyperparameters tuning was done on the Diversity-synt dataset, using the f1-score on the validation set as evaluation criteria. As in Giorgi et\u00a0al. [5], we used greedy decoding during the hyperparameter tuning step and subsequently tune the decoding strategy on the best obtained configuration. The list of hyperparameters is presented in Table S3. No significant impact of the batch-size on the performances was noticed, although larger values tends to improve the stability of the training. As already observed in [97, 74], increasing the rank of the LoRA adapters from r=8\ud835\udc5f8r=8 to r=16\ud835\udc5f16r=16 only led to marginal improvements. The best configuration obtained for BioGPT was reused for BioGPT-Large and GPT-2. For all evaluated datasets, models were then trained during 15 epochs (10 for BioGPT-Large) with 100 warm-up steps and the best epoch was selected using the validation set. We used the available implementation of QLoRA with PEFT [113]. We used the recommended 8\u2212b\u200bi\u200bt\u200bs8\ud835\udc4f\ud835\udc56\ud835\udc61\ud835\udc608-bits paged AdamW optimizer121212https://github.com/TimDettmers/bitsandbytes [98].\nSimilarly, we tuned the hyperparameters of Seq2rel on the Diversity-synt dataset (See Table S4). All the fine-tuning experiments were conducted on an NVIDIA GeForce RTX 3090."
        ]
    },
    "S1.T3": {
        "caption": "Table S3: Hyperparameters values used for BioGPT. Hyperparameters were fine-tuned using Optuna on the Diversity-synt dataset. Values between parentheses correspond to adaptation for BioGPT-Large. The following settings were evaluated: batch-size \u2208{4,8,16}absent4816\\in\\{4,8,16\\}; learning-rate \u2208[1\u200be\u22126;1\u200be\u22123]absent1\ud835\udc5261\ud835\udc523\\in[1e-6;1e-3]; LoRA configurations \u2208{(r=4,\u03b1=4),(r=4,\u03b1=8),(r=4,\u03b1=16),(r=8,\u03b1=8),(r=8,\u03b1=16),(r=8,\u03b1=32),(r=16,\u03b1=16),(r=16,\u03b1=32),(r=,16\u03b1=64)}\\in\\{(r=4,\\alpha=4),(r=4,\\alpha=8),(r=4,\\alpha=16),(r=8,\\alpha=8),(r=8,\\alpha=16),(r=8,\\alpha=32),(r=16,\\alpha=16),(r=16,\\alpha=32),(r=,16\\alpha=64)\\}. Gradient accumulation steps values were directly scaled in inverse proportion to the batch-size: {20,10,5}20105\\{20,10,5\\}. For the decoding strategies, the following settings were also evaluated: beam-size \u2208{3,5}absent35\\in\\{3,5\\}; stopping criteria \u2208{True,False,never}absentTrueFalsenever\\in\\{\\text{True},\\text{False},\\text{never}\\}; length penality \u2208[0,3]absent03\\in[0,3].",
        "table": "<table id=\"S1.T3.15\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S1.T3.15.1\" class=\"ltx_tr\">\n<td id=\"S1.T3.15.1.1\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"S1.T3.15.1.2\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S1.T3.15.1.2.1\" class=\"ltx_text ltx_font_bold\">Tuned ?</span></td>\n<td id=\"S1.T3.15.1.3\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S1.T3.15.1.3.1\" class=\"ltx_text ltx_font_bold\">Value</span></td>\n</tr>\n<tr id=\"S1.T3.15.2\" class=\"ltx_tr\">\n<td id=\"S1.T3.15.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S1.T3.15.2.1.1\" class=\"ltx_text ltx_font_bold\">Training</span></td>\n<td id=\"S1.T3.15.2.2\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S1.T3.15.2.3\" class=\"ltx_td ltx_border_t\"></td>\n</tr>\n<tr id=\"S1.T3.15.3\" class=\"ltx_tr\">\n<td id=\"S1.T3.15.3.1\" class=\"ltx_td ltx_align_left\"><span id=\"S1.T3.15.3.1.1\" class=\"ltx_text ltx_font_typewriter\">Batch size</span></td>\n<td id=\"S1.T3.15.3.2\" class=\"ltx_td ltx_align_right\">yes</td>\n<td id=\"S1.T3.15.3.3\" class=\"ltx_td ltx_align_right\">16 (12)</td>\n</tr>\n<tr id=\"S1.T3.15.4\" class=\"ltx_tr\">\n<td id=\"S1.T3.15.4.1\" class=\"ltx_td ltx_align_left\"><span id=\"S1.T3.15.4.1.1\" class=\"ltx_text ltx_font_typewriter\">Number of epochs</span></td>\n<td id=\"S1.T3.15.4.2\" class=\"ltx_td ltx_align_right\">no</td>\n<td id=\"S1.T3.15.4.3\" class=\"ltx_td ltx_align_right\">15</td>\n</tr>\n<tr id=\"S1.T3.15.5\" class=\"ltx_tr\">\n<td id=\"S1.T3.15.5.1\" class=\"ltx_td ltx_align_left\"><span id=\"S1.T3.15.5.1.1\" class=\"ltx_text ltx_font_typewriter\">LoRa r</span></td>\n<td id=\"S1.T3.15.5.2\" class=\"ltx_td ltx_align_right\">yes</td>\n<td id=\"S1.T3.15.5.3\" class=\"ltx_td ltx_align_right\">8</td>\n</tr>\n<tr id=\"S1.T3.15.6\" class=\"ltx_tr\">\n<td id=\"S1.T3.15.6.1\" class=\"ltx_td ltx_align_left\"><span id=\"S1.T3.15.6.1.1\" class=\"ltx_text ltx_font_typewriter\">LoRa alpha</span></td>\n<td id=\"S1.T3.15.6.2\" class=\"ltx_td ltx_align_right\">yes</td>\n<td id=\"S1.T3.15.6.3\" class=\"ltx_td ltx_align_right\">16</td>\n</tr>\n<tr id=\"S1.T3.15.7\" class=\"ltx_tr\">\n<td id=\"S1.T3.15.7.1\" class=\"ltx_td ltx_align_left\"><span id=\"S1.T3.15.7.1.1\" class=\"ltx_text ltx_font_typewriter\">Learning rate</span></td>\n<td id=\"S1.T3.15.7.2\" class=\"ltx_td ltx_align_right\">yes</td>\n<td id=\"S1.T3.15.7.3\" class=\"ltx_td ltx_align_right\">1.00e-4</td>\n</tr>\n<tr id=\"S1.T3.15.8\" class=\"ltx_tr\">\n<td id=\"S1.T3.15.8.1\" class=\"ltx_td ltx_align_left\"><span id=\"S1.T3.15.8.1.1\" class=\"ltx_text ltx_font_typewriter\">Weight decay</span></td>\n<td id=\"S1.T3.15.8.2\" class=\"ltx_td ltx_align_right\">no</td>\n<td id=\"S1.T3.15.8.3\" class=\"ltx_td ltx_align_right\">0.01</td>\n</tr>\n<tr id=\"S1.T3.15.9\" class=\"ltx_tr\">\n<td id=\"S1.T3.15.9.1\" class=\"ltx_td ltx_align_left\"><span id=\"S1.T3.15.9.1.1\" class=\"ltx_text ltx_font_typewriter\">Gradient accumulation steps</span></td>\n<td id=\"S1.T3.15.9.2\" class=\"ltx_td ltx_align_right\">no*</td>\n<td id=\"S1.T3.15.9.3\" class=\"ltx_td ltx_align_right\">5</td>\n</tr>\n<tr id=\"S1.T3.15.10\" class=\"ltx_tr\">\n<td id=\"S1.T3.15.10.1\" class=\"ltx_td ltx_align_left\"><span id=\"S1.T3.15.10.1.1\" class=\"ltx_text ltx_font_typewriter\">LoRA dropout</span></td>\n<td id=\"S1.T3.15.10.2\" class=\"ltx_td ltx_align_right\">no</td>\n<td id=\"S1.T3.15.10.3\" class=\"ltx_td ltx_align_right\">0.05</td>\n</tr>\n<tr id=\"S1.T3.15.11\" class=\"ltx_tr\">\n<td id=\"S1.T3.15.11.1\" class=\"ltx_td ltx_align_left\"><span id=\"S1.T3.15.11.1.1\" class=\"ltx_text ltx_font_typewriter\">LoRa target modules</span></td>\n<td id=\"S1.T3.15.11.2\" class=\"ltx_td ltx_align_right\">no</td>\n<td id=\"S1.T3.15.11.3\" class=\"ltx_td ltx_align_right\">q_proj, k_proj, v_proj, out_proj, fc1, fc2, output_projection</td>\n</tr>\n<tr id=\"S1.T3.15.12\" class=\"ltx_tr\">\n<td id=\"S1.T3.15.12.1\" class=\"ltx_td ltx_align_left\"><span id=\"S1.T3.15.12.1.1\" class=\"ltx_text ltx_font_bold\">Decoding</span></td>\n<td id=\"S1.T3.15.12.2\" class=\"ltx_td\"></td>\n<td id=\"S1.T3.15.12.3\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S1.T3.15.13\" class=\"ltx_tr\">\n<td id=\"S1.T3.15.13.1\" class=\"ltx_td ltx_align_left\"><span id=\"S1.T3.15.13.1.1\" class=\"ltx_text ltx_font_typewriter\">strategy</span></td>\n<td id=\"S1.T3.15.13.2\" class=\"ltx_td ltx_align_right\">yes</td>\n<td id=\"S1.T3.15.13.3\" class=\"ltx_td ltx_align_right\">beam search</td>\n</tr>\n<tr id=\"S1.T3.15.14\" class=\"ltx_tr\">\n<td id=\"S1.T3.15.14.1\" class=\"ltx_td ltx_align_left\"><span id=\"S1.T3.15.14.1.1\" class=\"ltx_text ltx_font_typewriter\">beam size</span></td>\n<td id=\"S1.T3.15.14.2\" class=\"ltx_td ltx_align_right\">yes</td>\n<td id=\"S1.T3.15.14.3\" class=\"ltx_td ltx_align_right\">3</td>\n</tr>\n<tr id=\"S1.T3.15.15\" class=\"ltx_tr\">\n<td id=\"S1.T3.15.15.1\" class=\"ltx_td ltx_align_left\"><span id=\"S1.T3.15.15.1.1\" class=\"ltx_text ltx_font_typewriter\">stopping criteria</span></td>\n<td id=\"S1.T3.15.15.2\" class=\"ltx_td ltx_align_right\">yes</td>\n<td id=\"S1.T3.15.15.3\" class=\"ltx_td ltx_align_right\">never</td>\n</tr>\n<tr id=\"S1.T3.15.16\" class=\"ltx_tr\">\n<td id=\"S1.T3.15.16.1\" class=\"ltx_td ltx_align_left\"><span id=\"S1.T3.15.16.1.1\" class=\"ltx_text ltx_font_typewriter\">length penality</span></td>\n<td id=\"S1.T3.15.16.2\" class=\"ltx_td ltx_align_right\">yes</td>\n<td id=\"S1.T3.15.16.3\" class=\"ltx_td ltx_align_right\">1.5</td>\n</tr>\n<tr id=\"S1.T3.15.17\" class=\"ltx_tr\">\n<td id=\"S1.T3.15.17.1\" class=\"ltx_td ltx_align_left ltx_border_bb\"><span id=\"S1.T3.15.17.1.1\" class=\"ltx_text ltx_font_typewriter\">temperature</span></td>\n<td id=\"S1.T3.15.17.2\" class=\"ltx_td ltx_align_right ltx_border_bb\">no</td>\n<td id=\"S1.T3.15.17.3\" class=\"ltx_td ltx_align_right ltx_border_bb\">0</td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "Dettmers et\u00a0al. [73] demonstrated the efficacy of the QLoRA approach by showing that the loss in performance due to quantization can be fully recovered through subsequent fine-tuning of the adapters and that increasing the number of adapters is crucial to match full fine-tuning performance. By exploiting the memory benefits of the NF4 data type, we applied LoRA adapters to all linear blocks (except the initial embeddings layer) of the BioGPT and GPT-2 models. Details on the number of trained parameters are presented in Table S2. During training, the special tokens <BOS> and <EOS> are used to delimitate the input X\ud835\udc4bX and the expected linearised output Y\ud835\udc4cY, such as [X\ud835\udc4bX, <EOS> <BOS>, Y\ud835\udc4cY, <EOS>]. The <BOS> token triggers the RE task at inference time.\nFor BioGPT, we evaluated different hyperparameter settings using Optuna [112]. We applied the TPE (Tree-structured Parzen Estimator) algorithm with a pruner on median to speed up the process. The hyperparameters tuning was done on the Diversity-synt dataset, using the f1-score on the validation set as evaluation criteria. As in Giorgi et\u00a0al. [5], we used greedy decoding during the hyperparameter tuning step and subsequently tune the decoding strategy on the best obtained configuration. The list of hyperparameters is presented in Table S3. No significant impact of the batch-size on the performances was noticed, although larger values tends to improve the stability of the training. As already observed in [97, 74], increasing the rank of the LoRA adapters from r=8\ud835\udc5f8r=8 to r=16\ud835\udc5f16r=16 only led to marginal improvements. The best configuration obtained for BioGPT was reused for BioGPT-Large and GPT-2. For all evaluated datasets, models were then trained during 15 epochs (10 for BioGPT-Large) with 100 warm-up steps and the best epoch was selected using the validation set. We used the available implementation of QLoRA with PEFT [113]. We used the recommended 8\u2212b\u200bi\u200bt\u200bs8\ud835\udc4f\ud835\udc56\ud835\udc61\ud835\udc608-bits paged AdamW optimizer121212https://github.com/TimDettmers/bitsandbytes [98].\nSimilarly, we tuned the hyperparameters of Seq2rel on the Diversity-synt dataset (See Table S4). All the fine-tuning experiments were conducted on an NVIDIA GeForce RTX 3090."
        ]
    },
    "S1.T4": {
        "caption": "Table S4: Hyperparameters values used for Seq2rel. Hyperparameters were fine-tuned using Optuna on the Diversity-synt dataset. All non-mentioned parameters were set according to the CDR configuration in the original Seq2rel\u2019s paper. The following configurations were evaluated: decoder\u2019s learning-rate \u2208[1\u200be\u22126;1\u200be\u22123]absent1\ud835\udc5261\ud835\udc523\\in[1e-6;1e-3]; beam-size \u2208[3,5]absent35\\in[3,5]; length penality \u2208[1,3]absent13\\in[1,3].",
        "table": "<table id=\"S1.T4.7\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S1.T4.7.1\" class=\"ltx_tr\">\n<td id=\"S1.T4.7.1.1\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"S1.T4.7.1.2\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S1.T4.7.1.2.1\" class=\"ltx_text ltx_font_bold\">Tuned ?</span></td>\n<td id=\"S1.T4.7.1.3\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S1.T4.7.1.3.1\" class=\"ltx_text ltx_font_bold\">Value</span></td>\n</tr>\n<tr id=\"S1.T4.7.2\" class=\"ltx_tr\">\n<td id=\"S1.T4.7.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S1.T4.7.2.1.1\" class=\"ltx_text ltx_font_bold\">Training</span></td>\n<td id=\"S1.T4.7.2.2\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S1.T4.7.2.3\" class=\"ltx_td ltx_align_right ltx_border_t\">//</td>\n</tr>\n<tr id=\"S1.T4.7.3\" class=\"ltx_tr\">\n<td id=\"S1.T4.7.3.1\" class=\"ltx_td ltx_align_left\">decodr learning rate</td>\n<td id=\"S1.T4.7.3.2\" class=\"ltx_td ltx_align_right\">yes</td>\n<td id=\"S1.T4.7.3.3\" class=\"ltx_td ltx_align_right\">9.00e-4</td>\n</tr>\n<tr id=\"S1.T4.7.4\" class=\"ltx_tr\">\n<td id=\"S1.T4.7.4.1\" class=\"ltx_td ltx_align_left\">batch size</td>\n<td id=\"S1.T4.7.4.2\" class=\"ltx_td ltx_align_right\">no</td>\n<td id=\"S1.T4.7.4.3\" class=\"ltx_td ltx_align_right\">4</td>\n</tr>\n<tr id=\"S1.T4.7.5\" class=\"ltx_tr\">\n<td id=\"S1.T4.7.5.1\" class=\"ltx_td ltx_align_left\">number of epochs</td>\n<td id=\"S1.T4.7.5.2\" class=\"ltx_td ltx_align_right\">yes</td>\n<td id=\"S1.T4.7.5.3\" class=\"ltx_td ltx_align_right\">20</td>\n</tr>\n<tr id=\"S1.T4.7.6\" class=\"ltx_tr\">\n<td id=\"S1.T4.7.6.1\" class=\"ltx_td ltx_align_left\">gradient accumulation steps</td>\n<td id=\"S1.T4.7.6.2\" class=\"ltx_td ltx_align_right\">no</td>\n<td id=\"S1.T4.7.6.3\" class=\"ltx_td ltx_align_right\">10</td>\n</tr>\n<tr id=\"S1.T4.7.7\" class=\"ltx_tr\">\n<td id=\"S1.T4.7.7.1\" class=\"ltx_td ltx_align_left\">others</td>\n<td id=\"S1.T4.7.7.2\" class=\"ltx_td ltx_align_right\">no</td>\n<td id=\"S1.T4.7.7.3\" class=\"ltx_td ltx_align_right\">identifical to seq2rel&#8217;s CDR config</td>\n</tr>\n<tr id=\"S1.T4.7.8\" class=\"ltx_tr\">\n<td id=\"S1.T4.7.8.1\" class=\"ltx_td ltx_align_left\"><span id=\"S1.T4.7.8.1.1\" class=\"ltx_text ltx_font_bold\">Decoding</span></td>\n<td id=\"S1.T4.7.8.2\" class=\"ltx_td\"></td>\n<td id=\"S1.T4.7.8.3\" class=\"ltx_td ltx_align_right\">//</td>\n</tr>\n<tr id=\"S1.T4.7.9\" class=\"ltx_tr\">\n<td id=\"S1.T4.7.9.1\" class=\"ltx_td ltx_align_left\">beam size</td>\n<td id=\"S1.T4.7.9.2\" class=\"ltx_td ltx_align_right\">yes</td>\n<td id=\"S1.T4.7.9.3\" class=\"ltx_td ltx_align_right\">5</td>\n</tr>\n<tr id=\"S1.T4.7.10\" class=\"ltx_tr\">\n<td id=\"S1.T4.7.10.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">length penality</td>\n<td id=\"S1.T4.7.10.2\" class=\"ltx_td ltx_align_right ltx_border_bb\">yes</td>\n<td id=\"S1.T4.7.10.3\" class=\"ltx_td ltx_align_right ltx_border_bb\">1</td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "Dettmers et\u00a0al. [73] demonstrated the efficacy of the QLoRA approach by showing that the loss in performance due to quantization can be fully recovered through subsequent fine-tuning of the adapters and that increasing the number of adapters is crucial to match full fine-tuning performance. By exploiting the memory benefits of the NF4 data type, we applied LoRA adapters to all linear blocks (except the initial embeddings layer) of the BioGPT and GPT-2 models. Details on the number of trained parameters are presented in Table S2. During training, the special tokens <BOS> and <EOS> are used to delimitate the input X\ud835\udc4bX and the expected linearised output Y\ud835\udc4cY, such as [X\ud835\udc4bX, <EOS> <BOS>, Y\ud835\udc4cY, <EOS>]. The <BOS> token triggers the RE task at inference time.\nFor BioGPT, we evaluated different hyperparameter settings using Optuna [112]. We applied the TPE (Tree-structured Parzen Estimator) algorithm with a pruner on median to speed up the process. The hyperparameters tuning was done on the Diversity-synt dataset, using the f1-score on the validation set as evaluation criteria. As in Giorgi et\u00a0al. [5], we used greedy decoding during the hyperparameter tuning step and subsequently tune the decoding strategy on the best obtained configuration. The list of hyperparameters is presented in Table S3. No significant impact of the batch-size on the performances was noticed, although larger values tends to improve the stability of the training. As already observed in [97, 74], increasing the rank of the LoRA adapters from r=8\ud835\udc5f8r=8 to r=16\ud835\udc5f16r=16 only led to marginal improvements. The best configuration obtained for BioGPT was reused for BioGPT-Large and GPT-2. For all evaluated datasets, models were then trained during 15 epochs (10 for BioGPT-Large) with 100 warm-up steps and the best epoch was selected using the validation set. We used the available implementation of QLoRA with PEFT [113]. We used the recommended 8\u2212b\u200bi\u200bt\u200bs8\ud835\udc4f\ud835\udc56\ud835\udc61\ud835\udc608-bits paged AdamW optimizer121212https://github.com/TimDettmers/bitsandbytes [98].\nSimilarly, we tuned the hyperparameters of Seq2rel on the Diversity-synt dataset (See Table S4). All the fine-tuning experiments were conducted on an NVIDIA GeForce RTX 3090."
        ]
    },
    "S2.T10": {
        "caption": "Table S10: Comparison of the performance of the prompted Vicuna-13B LLM and KeyBERT for keywords/keyphrases extraction on the SemEVAL2017 test set. The evaluation was only done on the top-10 extracted keywords for the both methods, to use the same configuration as in the experiments.",
        "table": "<table id=\"S2.T10.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S2.T10.1.1\" class=\"ltx_tr\">\n<td id=\"S2.T10.1.1.1\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"S2.T10.1.1.2\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S2.T10.1.1.2.1\" class=\"ltx_text ltx_font_bold\">TP in Top10</span></td>\n<td id=\"S2.T10.1.1.3\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S2.T10.1.1.3.1\" class=\"ltx_text ltx_font_bold\">FP in Top10</span></td>\n<td id=\"S2.T10.1.1.4\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S2.T10.1.1.4.1\" class=\"ltx_text ltx_font_bold\">Precision</span></td>\n</tr>\n<tr id=\"S2.T10.1.2\" class=\"ltx_tr\">\n<td id=\"S2.T10.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S2.T10.1.2.1.1\" class=\"ltx_text ltx_font_bold\">KeyBERT (all-MiniLM-L6-v2)</span></td>\n<td id=\"S2.T10.1.2.2\" class=\"ltx_td ltx_align_right ltx_border_t\">96</td>\n<td id=\"S2.T10.1.2.3\" class=\"ltx_td ltx_align_right ltx_border_t\">904</td>\n<td id=\"S2.T10.1.2.4\" class=\"ltx_td ltx_align_right ltx_border_t\">9.6</td>\n</tr>\n<tr id=\"S2.T10.1.3\" class=\"ltx_tr\">\n<td id=\"S2.T10.1.3.1\" class=\"ltx_td ltx_align_left ltx_border_bb\"><span id=\"S2.T10.1.3.1.1\" class=\"ltx_text ltx_font_bold\">Vicuna-13B</span></td>\n<td id=\"S2.T10.1.3.2\" class=\"ltx_td ltx_align_right ltx_border_bb\">321</td>\n<td id=\"S2.T10.1.3.3\" class=\"ltx_td ltx_align_right ltx_border_bb\">669</td>\n<td id=\"S2.T10.1.3.4\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S2.T10.1.3.4.1\" class=\"ltx_text ltx_font_bold\">32.424</span></td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "The SemEVAL2017 [93] evaluation dataset consists of 100 paragraphs, extracted from scientific publications in various domains, with on average 17.23 annotated keyphrases. While 3 subtasks are proposed in this challenge (classification and semantic relation), we only focused on the mention-level keyphrases identification. To consider similar settings as used for synthetic abstract generation, we evaluated the precision in the top-10 extracted keywords. The comparison is done by exact-match and results are presented in Table S10. Vicuna-13B largely outperforms the KeyBERT [115] baseline and show more than acceptable performance in zero-shot settings. KeyBERT was used with standard parameters: keyphrase_ngram_range: (1,2), stop_words: None, use_mmr: True, diversity: 0.7 and BERT model all-MiniLM-L6-v2 for base embeddings."
        ]
    },
    "S2.T5": {
        "caption": "Table S5: Impact of the pre-processing of the number of organisms, chemicals and relations.",
        "table": "<table id=\"S2.T5.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S2.T5.1.1\" class=\"ltx_tr\">\n<td id=\"S2.T5.1.1.1\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"S2.T5.1.1.2\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S2.T5.1.1.2.1\" class=\"ltx_text ltx_font_bold\"># Organisms</span></td>\n<td id=\"S2.T5.1.1.3\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S2.T5.1.1.3.1\" class=\"ltx_text ltx_font_bold\"># Chemicals</span></td>\n<td id=\"S2.T5.1.1.4\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S2.T5.1.1.4.1\" class=\"ltx_text ltx_font_bold\"># Relations</span></td>\n<td id=\"S2.T5.1.1.5\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S2.T5.1.1.5.1\" class=\"ltx_text ltx_font_bold\"># References</span></td>\n</tr>\n<tr id=\"S2.T5.1.2\" class=\"ltx_tr\">\n<td id=\"S2.T5.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S2.T5.1.2.1.1\" class=\"ltx_text ltx_font_bold\">Original dataset</span></td>\n<td id=\"S2.T5.1.2.2\" class=\"ltx_td ltx_align_right ltx_border_t\">36803</td>\n<td id=\"S2.T5.1.2.3\" class=\"ltx_td ltx_align_right ltx_border_t\">220783</td>\n<td id=\"S2.T5.1.2.4\" class=\"ltx_td ltx_align_right ltx_border_t\">533347</td>\n<td id=\"S2.T5.1.2.5\" class=\"ltx_td ltx_align_right ltx_border_t\">88810</td>\n</tr>\n<tr id=\"S2.T5.1.3\" class=\"ltx_tr\">\n<td id=\"S2.T5.1.3.1\" class=\"ltx_td ltx_align_left ltx_border_bb\"><span id=\"S2.T5.1.3.1.1\" class=\"ltx_text ltx_font_bold\">Pre-processed dataset</span></td>\n<td id=\"S2.T5.1.3.2\" class=\"ltx_td ltx_align_right ltx_border_bb\">14890</td>\n<td id=\"S2.T5.1.3.3\" class=\"ltx_td ltx_align_right ltx_border_bb\">56310</td>\n<td id=\"S2.T5.1.3.4\" class=\"ltx_td ltx_align_right ltx_border_bb\">102528</td>\n<td id=\"S2.T5.1.3.5\" class=\"ltx_td ltx_align_right ltx_border_bb\">32616</td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "The original dataset was first preprocessed and filtered prior to sampling to eliminate various sources of perturbations and unusable data in subsequent steps. Specifically, only documents with publicly available abstracts on PubMed were selected, and these were further filtered based on the number of reported relations. Indeed, a manual inspection of a subset of articles revealed that documents reporting large numbers of relations ([79, 80, 81, 82]) often propose genome-scale metabolic reconstructions, large screening analyses, or database releases. Although these documents may report hundreds of relationships, they are typically not expressed in the abstracts, making them useless examples for building a RE model. Only articles reporting less than 20 relations (corresponding to the quantile 93%) were then selected. Compared to organism names, the length of chemical names can exhibit extreme variability and exceed hundreds of characters depending on the nomenclature. To mitigate the issues posed by these lengthy labels, which are inordinate to decode and could consume an excessive portion of the context window, only relations involving chemicals with a label length l\u226460\ud835\udc5960l\\leq 60 characters were retained. More details in supplementary S2.1. See the global pre-processing statistics in supplementary table S5 and kingdom coverage in Figure 4.B top-right.",
            "The preprocessed dataset was first stratified according to the taxonomic classification (kingdoms) of the organisms associated with the relations reported in each document. Subsequently, the GME-sampler was applied to each subset (Figure 5:Top-panel) to monitor the evolution of the diversity metrics (HS\u200b(O)subscript\ud835\udc3b\ud835\udc46\ud835\udc42H_{S}(O) and HS\u200b(C)subscript\ud835\udc3b\ud835\udc46\ud835\udc36H_{S}(C)) and determine an optimal sample size. Indeed, the GME-sampler operates as a ranking method, where the article selected at step n\ud835\udc5bn, is the one which contributes the most to the diversity of the set of the n\u22121\ud835\udc5b1n-1 articles selected upstream. For both organisms and chemicals, diversity increases rapidly in the first hundred ranked items, followed by a plateau. Specifically for organisms (regardless of the kingdom), diversity showed a decline in the second half of the sampled items (supplementary table S6). This is the signal that the addition of new articles provide relations for already well-covered organisms and disrupted the existing balance in the organism distribution. In contrast, the impact of newly added articles on chemicals is negligible, likely because they represent a larger set of distinct entities (supplementary table S5). To keep a reasonable balance between diversity and sample size, we decided to only retain the top n=500\ud835\udc5b500n=500 ranked articles per kingdoms, ensuring at least 80%percent8080\\% of the maximal observed entropy on both organisms and chemicals (Figure 5:Bottom-panel). The proportions of maximal observed entropy at alternative sample sizes are presented in supplementary table S7.\nThe impact of the diversity-sampling strategy is evaluated by comparing the composition of the sample against 5 random samples of equivalent sizes555Each random sample is composed of 500 random literature items sampled per kingdoms. The original diversity sample and the extracted random samples are respectively denoted as Diversity and Random samples. While showing similar kingdoms\u2019 coverage because of the common stratification procedure (Figure 4.B bottom), the diversity sample is, as expected, significantly richer in terms of distinct number of chemicals, organisms and relations (Figure 4.C. This improved diversity is also reflected in a reduced pareto effect for the distribution of the organisms (negligible for chemicals), and overlap between the entities reported in each article (Figures 4.D and E). A more comprehensive comparison against others possible sampling strategies for diversity are discussed in supplementary S2.2."
        ]
    },
    "S2.T6": {
        "caption": "Table S6: Maximal number N\ud835\udc41N of literature items per kingdoms along with the value and the rank of the maximal reached entropies on organisms HS\u200b(O)subscript\ud835\udc3b\ud835\udc46\ud835\udc42H_{S}(O) and chemicals HS\u200b(C)subscript\ud835\udc3b\ud835\udc46\ud835\udc36H_{S}(C).",
        "table": "<table id=\"S2.T6.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S2.T6.2.2\" class=\"ltx_tr\">\n<td id=\"S2.T6.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_tt\"><span id=\"S2.T6.2.2.3.1\" class=\"ltx_text ltx_font_bold\">Kingdom</span></td>\n<td id=\"S2.T6.2.2.4\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S2.T6.2.2.4.1\" class=\"ltx_text ltx_font_bold\">N</span></td>\n<td id=\"S2.T6.1.1.1\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S2.T6.1.1.1.1\" class=\"ltx_text ltx_font_bold\">max <math id=\"S2.T6.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"H_{S}(O)\" display=\"inline\"><semantics id=\"S2.T6.1.1.1.1.m1.1a\"><mrow id=\"S2.T6.1.1.1.1.m1.1.2\" xref=\"S2.T6.1.1.1.1.m1.1.2.cmml\"><msub id=\"S2.T6.1.1.1.1.m1.1.2.2\" xref=\"S2.T6.1.1.1.1.m1.1.2.2.cmml\"><mi id=\"S2.T6.1.1.1.1.m1.1.2.2.2\" xref=\"S2.T6.1.1.1.1.m1.1.2.2.2.cmml\">H</mi><mi id=\"S2.T6.1.1.1.1.m1.1.2.2.3\" xref=\"S2.T6.1.1.1.1.m1.1.2.2.3.cmml\">S</mi></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.T6.1.1.1.1.m1.1.2.1\" xref=\"S2.T6.1.1.1.1.m1.1.2.1.cmml\">&#8203;</mo><mrow id=\"S2.T6.1.1.1.1.m1.1.2.3.2\" xref=\"S2.T6.1.1.1.1.m1.1.2.cmml\"><mo stretchy=\"false\" id=\"S2.T6.1.1.1.1.m1.1.2.3.2.1\" xref=\"S2.T6.1.1.1.1.m1.1.2.cmml\">(</mo><mi id=\"S2.T6.1.1.1.1.m1.1.1\" xref=\"S2.T6.1.1.1.1.m1.1.1.cmml\">O</mi><mo stretchy=\"false\" id=\"S2.T6.1.1.1.1.m1.1.2.3.2.2\" xref=\"S2.T6.1.1.1.1.m1.1.2.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T6.1.1.1.1.m1.1b\"><apply id=\"S2.T6.1.1.1.1.m1.1.2.cmml\" xref=\"S2.T6.1.1.1.1.m1.1.2\"><times id=\"S2.T6.1.1.1.1.m1.1.2.1.cmml\" xref=\"S2.T6.1.1.1.1.m1.1.2.1\"></times><apply id=\"S2.T6.1.1.1.1.m1.1.2.2.cmml\" xref=\"S2.T6.1.1.1.1.m1.1.2.2\"><csymbol cd=\"ambiguous\" id=\"S2.T6.1.1.1.1.m1.1.2.2.1.cmml\" xref=\"S2.T6.1.1.1.1.m1.1.2.2\">subscript</csymbol><ci id=\"S2.T6.1.1.1.1.m1.1.2.2.2.cmml\" xref=\"S2.T6.1.1.1.1.m1.1.2.2.2\">&#119867;</ci><ci id=\"S2.T6.1.1.1.1.m1.1.2.2.3.cmml\" xref=\"S2.T6.1.1.1.1.m1.1.2.2.3\">&#119878;</ci></apply><ci id=\"S2.T6.1.1.1.1.m1.1.1.cmml\" xref=\"S2.T6.1.1.1.1.m1.1.1\">&#119874;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T6.1.1.1.1.m1.1c\">H_{S}(O)</annotation></semantics></math> (rank)</span></td>\n<td id=\"S2.T6.2.2.2\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S2.T6.2.2.2.1\" class=\"ltx_text ltx_font_bold\">max <math id=\"S2.T6.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"H_{S}(C)\" display=\"inline\"><semantics id=\"S2.T6.2.2.2.1.m1.1a\"><mrow id=\"S2.T6.2.2.2.1.m1.1.2\" xref=\"S2.T6.2.2.2.1.m1.1.2.cmml\"><msub id=\"S2.T6.2.2.2.1.m1.1.2.2\" xref=\"S2.T6.2.2.2.1.m1.1.2.2.cmml\"><mi id=\"S2.T6.2.2.2.1.m1.1.2.2.2\" xref=\"S2.T6.2.2.2.1.m1.1.2.2.2.cmml\">H</mi><mi id=\"S2.T6.2.2.2.1.m1.1.2.2.3\" xref=\"S2.T6.2.2.2.1.m1.1.2.2.3.cmml\">S</mi></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.T6.2.2.2.1.m1.1.2.1\" xref=\"S2.T6.2.2.2.1.m1.1.2.1.cmml\">&#8203;</mo><mrow id=\"S2.T6.2.2.2.1.m1.1.2.3.2\" xref=\"S2.T6.2.2.2.1.m1.1.2.cmml\"><mo stretchy=\"false\" id=\"S2.T6.2.2.2.1.m1.1.2.3.2.1\" xref=\"S2.T6.2.2.2.1.m1.1.2.cmml\">(</mo><mi id=\"S2.T6.2.2.2.1.m1.1.1\" xref=\"S2.T6.2.2.2.1.m1.1.1.cmml\">C</mi><mo stretchy=\"false\" id=\"S2.T6.2.2.2.1.m1.1.2.3.2.2\" xref=\"S2.T6.2.2.2.1.m1.1.2.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T6.2.2.2.1.m1.1b\"><apply id=\"S2.T6.2.2.2.1.m1.1.2.cmml\" xref=\"S2.T6.2.2.2.1.m1.1.2\"><times id=\"S2.T6.2.2.2.1.m1.1.2.1.cmml\" xref=\"S2.T6.2.2.2.1.m1.1.2.1\"></times><apply id=\"S2.T6.2.2.2.1.m1.1.2.2.cmml\" xref=\"S2.T6.2.2.2.1.m1.1.2.2\"><csymbol cd=\"ambiguous\" id=\"S2.T6.2.2.2.1.m1.1.2.2.1.cmml\" xref=\"S2.T6.2.2.2.1.m1.1.2.2\">subscript</csymbol><ci id=\"S2.T6.2.2.2.1.m1.1.2.2.2.cmml\" xref=\"S2.T6.2.2.2.1.m1.1.2.2.2\">&#119867;</ci><ci id=\"S2.T6.2.2.2.1.m1.1.2.2.3.cmml\" xref=\"S2.T6.2.2.2.1.m1.1.2.2.3\">&#119878;</ci></apply><ci id=\"S2.T6.2.2.2.1.m1.1.1.cmml\" xref=\"S2.T6.2.2.2.1.m1.1.1\">&#119862;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T6.2.2.2.1.m1.1c\">H_{S}(C)</annotation></semantics></math> (rank)</span></td>\n</tr>\n<tr id=\"S2.T6.2.3\" class=\"ltx_tr\">\n<td id=\"S2.T6.2.3.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Archaeplastida</td>\n<td id=\"S2.T6.2.3.2\" class=\"ltx_td ltx_align_right ltx_border_t\">19491</td>\n<td id=\"S2.T6.2.3.3\" class=\"ltx_td ltx_align_right ltx_border_t\">8.73 (10512)</td>\n<td id=\"S2.T6.2.3.4\" class=\"ltx_td ltx_align_right ltx_border_t\">9.81 (10713)</td>\n</tr>\n<tr id=\"S2.T6.2.4\" class=\"ltx_tr\">\n<td id=\"S2.T6.2.4.1\" class=\"ltx_td ltx_align_left\">Fungi</td>\n<td id=\"S2.T6.2.4.2\" class=\"ltx_td ltx_align_right\">5023</td>\n<td id=\"S2.T6.2.4.3\" class=\"ltx_td ltx_align_right\">7.18 (2519)</td>\n<td id=\"S2.T6.2.4.4\" class=\"ltx_td ltx_align_right\">9.33 (5023)</td>\n</tr>\n<tr id=\"S2.T6.2.5\" class=\"ltx_tr\">\n<td id=\"S2.T6.2.5.1\" class=\"ltx_td ltx_align_left\">Metazoa</td>\n<td id=\"S2.T6.2.5.2\" class=\"ltx_td ltx_align_right\">1920</td>\n<td id=\"S2.T6.2.5.3\" class=\"ltx_td ltx_align_right\">6.72 (1304)</td>\n<td id=\"S2.T6.2.5.4\" class=\"ltx_td ltx_align_right\">8.33 (1920)</td>\n</tr>\n<tr id=\"S2.T6.2.6\" class=\"ltx_tr\">\n<td id=\"S2.T6.2.6.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">Not Attributed (Bacteria or Algae)</td>\n<td id=\"S2.T6.2.6.2\" class=\"ltx_td ltx_align_right ltx_border_bb\">6666</td>\n<td id=\"S2.T6.2.6.3\" class=\"ltx_td ltx_align_right ltx_border_bb\">6.96 (2503)</td>\n<td id=\"S2.T6.2.6.4\" class=\"ltx_td ltx_align_right ltx_border_bb\">8.90 (6666)</td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "The preprocessed dataset was first stratified according to the taxonomic classification (kingdoms) of the organisms associated with the relations reported in each document. Subsequently, the GME-sampler was applied to each subset (Figure 5:Top-panel) to monitor the evolution of the diversity metrics (HS\u200b(O)subscript\ud835\udc3b\ud835\udc46\ud835\udc42H_{S}(O) and HS\u200b(C)subscript\ud835\udc3b\ud835\udc46\ud835\udc36H_{S}(C)) and determine an optimal sample size. Indeed, the GME-sampler operates as a ranking method, where the article selected at step n\ud835\udc5bn, is the one which contributes the most to the diversity of the set of the n\u22121\ud835\udc5b1n-1 articles selected upstream. For both organisms and chemicals, diversity increases rapidly in the first hundred ranked items, followed by a plateau. Specifically for organisms (regardless of the kingdom), diversity showed a decline in the second half of the sampled items (supplementary table S6). This is the signal that the addition of new articles provide relations for already well-covered organisms and disrupted the existing balance in the organism distribution. In contrast, the impact of newly added articles on chemicals is negligible, likely because they represent a larger set of distinct entities (supplementary table S5). To keep a reasonable balance between diversity and sample size, we decided to only retain the top n=500\ud835\udc5b500n=500 ranked articles per kingdoms, ensuring at least 80%percent8080\\% of the maximal observed entropy on both organisms and chemicals (Figure 5:Bottom-panel). The proportions of maximal observed entropy at alternative sample sizes are presented in supplementary table S7.\nThe impact of the diversity-sampling strategy is evaluated by comparing the composition of the sample against 5 random samples of equivalent sizes555Each random sample is composed of 500 random literature items sampled per kingdoms. The original diversity sample and the extracted random samples are respectively denoted as Diversity and Random samples. While showing similar kingdoms\u2019 coverage because of the common stratification procedure (Figure 4.B bottom), the diversity sample is, as expected, significantly richer in terms of distinct number of chemicals, organisms and relations (Figure 4.C. This improved diversity is also reflected in a reduced pareto effect for the distribution of the organisms (negligible for chemicals), and overlap between the entities reported in each article (Figures 4.D and E). A more comprehensive comparison against others possible sampling strategies for diversity are discussed in supplementary S2.2."
        ]
    },
    "S2.T7": {
        "caption": "Table S7: Percentage of the maximal (observed) entropies HS\u200b(O)subscript\ud835\udc3b\ud835\udc46\ud835\udc42H_{\\displaystyle S}(O) and HS\u200b(C)subscript\ud835\udc3b\ud835\udc46\ud835\udc36H_{\\displaystyle S}(C) at different steps: 250, 500, 1000 and 2000 top ranked articles. ",
        "table": "<table id=\"S2.T7.5\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S2.T7.5.1\" class=\"ltx_tr\">\n<td id=\"S2.T7.5.1.1\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"S2.T7.5.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span id=\"S2.T7.5.1.2.1\" class=\"ltx_text ltx_font_bold\">Organisms (% of Max Entropy)</span></td>\n<td id=\"S2.T7.5.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span id=\"S2.T7.5.1.3.1\" class=\"ltx_text ltx_font_bold\">Chemicals (% of Max Entropy)</span></td>\n</tr>\n<tr id=\"S2.T7.5.2\" class=\"ltx_tr\">\n<td id=\"S2.T7.5.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S2.T7.5.2.1.1\" class=\"ltx_text ltx_font_bold\">Kingdom</span></td>\n<td id=\"S2.T7.5.2.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S2.T7.5.2.2.1\" class=\"ltx_text ltx_font_bold\">n=250</span></td>\n<td id=\"S2.T7.5.2.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S2.T7.5.2.3.1\" class=\"ltx_text ltx_font_bold\">n=500</span></td>\n<td id=\"S2.T7.5.2.4\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S2.T7.5.2.4.1\" class=\"ltx_text ltx_font_bold\">n=1000</span></td>\n<td id=\"S2.T7.5.2.5\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S2.T7.5.2.5.1\" class=\"ltx_text ltx_font_bold\">n=2000</span></td>\n<td id=\"S2.T7.5.2.6\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S2.T7.5.2.6.1\" class=\"ltx_text ltx_font_bold\">n=250</span></td>\n<td id=\"S2.T7.5.2.7\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S2.T7.5.2.7.1\" class=\"ltx_text ltx_font_bold\">n=500</span></td>\n<td id=\"S2.T7.5.2.8\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S2.T7.5.2.8.1\" class=\"ltx_text ltx_font_bold\">n=1000</span></td>\n<td id=\"S2.T7.5.2.9\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S2.T7.5.2.9.1\" class=\"ltx_text ltx_font_bold\">n=2000</span></td>\n</tr>\n<tr id=\"S2.T7.5.3\" class=\"ltx_tr\">\n<td id=\"S2.T7.5.3.1\" class=\"ltx_td ltx_align_left\">Archaeplastida</td>\n<td id=\"S2.T7.5.3.2\" class=\"ltx_td ltx_align_right\">75.5</td>\n<td id=\"S2.T7.5.3.3\" class=\"ltx_td ltx_align_right\">80.5</td>\n<td id=\"S2.T7.5.3.4\" class=\"ltx_td ltx_align_right\">86</td>\n<td id=\"S2.T7.5.3.5\" class=\"ltx_td ltx_align_right\">91.7</td>\n<td id=\"S2.T7.5.3.6\" class=\"ltx_td ltx_align_right\">76.9</td>\n<td id=\"S2.T7.5.3.7\" class=\"ltx_td ltx_align_right\">83.7</td>\n<td id=\"S2.T7.5.3.8\" class=\"ltx_td ltx_align_right\">89.6</td>\n<td id=\"S2.T7.5.3.9\" class=\"ltx_td ltx_align_right\">94.3</td>\n</tr>\n<tr id=\"S2.T7.5.4\" class=\"ltx_tr\">\n<td id=\"S2.T7.5.4.1\" class=\"ltx_td ltx_align_left\">Fungi</td>\n<td id=\"S2.T7.5.4.2\" class=\"ltx_td ltx_align_right\">80.7</td>\n<td id=\"S2.T7.5.4.3\" class=\"ltx_td ltx_align_right\">89</td>\n<td id=\"S2.T7.5.4.4\" class=\"ltx_td ltx_align_right\">96.6</td>\n<td id=\"S2.T7.5.4.5\" class=\"ltx_td ltx_align_right\">99.8</td>\n<td id=\"S2.T7.5.4.6\" class=\"ltx_td ltx_align_right\">83.7</td>\n<td id=\"S2.T7.5.4.7\" class=\"ltx_td ltx_align_right\">88.1</td>\n<td id=\"S2.T7.5.4.8\" class=\"ltx_td ltx_align_right\">92.3</td>\n<td id=\"S2.T7.5.4.9\" class=\"ltx_td ltx_align_right\">95.1</td>\n</tr>\n<tr id=\"S2.T7.5.5\" class=\"ltx_tr\">\n<td id=\"S2.T7.5.5.1\" class=\"ltx_td ltx_align_left\">Metazoa</td>\n<td id=\"S2.T7.5.5.2\" class=\"ltx_td ltx_align_right\">84.4</td>\n<td id=\"S2.T7.5.5.3\" class=\"ltx_td ltx_align_right\">93</td>\n<td id=\"S2.T7.5.5.4\" class=\"ltx_td ltx_align_right\">99.5</td>\n<td id=\"S2.T7.5.5.5\" class=\"ltx_td ltx_align_right\">96.1</td>\n<td id=\"S2.T7.5.5.6\" class=\"ltx_td ltx_align_right\">90</td>\n<td id=\"S2.T7.5.5.7\" class=\"ltx_td ltx_align_right\">94.6</td>\n<td id=\"S2.T7.5.5.8\" class=\"ltx_td ltx_align_right\">97.4</td>\n<td id=\"S2.T7.5.5.9\" class=\"ltx_td ltx_align_right\">100</td>\n</tr>\n<tr id=\"S2.T7.5.6\" class=\"ltx_tr\">\n<td id=\"S2.T7.5.6.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">Not Attributed (Bacteria or Algae)</td>\n<td id=\"S2.T7.5.6.2\" class=\"ltx_td ltx_align_right ltx_border_bb\">82.3</td>\n<td id=\"S2.T7.5.6.3\" class=\"ltx_td ltx_align_right ltx_border_bb\">90.7</td>\n<td id=\"S2.T7.5.6.4\" class=\"ltx_td ltx_align_right ltx_border_bb\">96.7</td>\n<td id=\"S2.T7.5.6.5\" class=\"ltx_td ltx_align_right ltx_border_bb\">99.9</td>\n<td id=\"S2.T7.5.6.6\" class=\"ltx_td ltx_align_right ltx_border_bb\">85.1</td>\n<td id=\"S2.T7.5.6.7\" class=\"ltx_td ltx_align_right ltx_border_bb\">89.6</td>\n<td id=\"S2.T7.5.6.8\" class=\"ltx_td ltx_align_right ltx_border_bb\">93.7</td>\n<td id=\"S2.T7.5.6.9\" class=\"ltx_td ltx_align_right ltx_border_bb\">97.2</td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "Figure 5: Top-panel: Evolution of the Entropy over the distribution of reported organisms (HS\u200b(O)subscript\ud835\udc3b\ud835\udc46\ud835\udc42H_{\\displaystyle S}(O)) and chemicals (HS\u200b(C)subscript\ud835\udc3b\ud835\udc46\ud835\udc36H_{\\displaystyle S}(C)) by adding iteratively a new article (d\u2217d*) in the built dataset, stratified by biological kingdoms. The step (500) when 80% of the maximal entropy is reached in all the kingdom\u2019s subsets, for organisms and chemicals, is indicated with the black arrow (more details in supplementary table S7). Bottom-panel:  Zoom of the evolution of HS\u200b(O)subscript\ud835\udc3b\ud835\udc46\ud835\udc42H_{\\displaystyle S}(O) and HS\u200b(C)subscript\ud835\udc3b\ud835\udc46\ud835\udc36H_{\\displaystyle S}(C) in the first 500 added articles. For each curve, the knee-points (bending points) with the corresponding rank and associated entropies are indicated.",
            "The preprocessed dataset was first stratified according to the taxonomic classification (kingdoms) of the organisms associated with the relations reported in each document. Subsequently, the GME-sampler was applied to each subset (Figure 5:Top-panel) to monitor the evolution of the diversity metrics (HS\u200b(O)subscript\ud835\udc3b\ud835\udc46\ud835\udc42H_{S}(O) and HS\u200b(C)subscript\ud835\udc3b\ud835\udc46\ud835\udc36H_{S}(C)) and determine an optimal sample size. Indeed, the GME-sampler operates as a ranking method, where the article selected at step n\ud835\udc5bn, is the one which contributes the most to the diversity of the set of the n\u22121\ud835\udc5b1n-1 articles selected upstream. For both organisms and chemicals, diversity increases rapidly in the first hundred ranked items, followed by a plateau. Specifically for organisms (regardless of the kingdom), diversity showed a decline in the second half of the sampled items (supplementary table S6). This is the signal that the addition of new articles provide relations for already well-covered organisms and disrupted the existing balance in the organism distribution. In contrast, the impact of newly added articles on chemicals is negligible, likely because they represent a larger set of distinct entities (supplementary table S5). To keep a reasonable balance between diversity and sample size, we decided to only retain the top n=500\ud835\udc5b500n=500 ranked articles per kingdoms, ensuring at least 80%percent8080\\% of the maximal observed entropy on both organisms and chemicals (Figure 5:Bottom-panel). The proportions of maximal observed entropy at alternative sample sizes are presented in supplementary table S7.\nThe impact of the diversity-sampling strategy is evaluated by comparing the composition of the sample against 5 random samples of equivalent sizes555Each random sample is composed of 500 random literature items sampled per kingdoms. The original diversity sample and the extracted random samples are respectively denoted as Diversity and Random samples. While showing similar kingdoms\u2019 coverage because of the common stratification procedure (Figure 4.B bottom), the diversity sample is, as expected, significantly richer in terms of distinct number of chemicals, organisms and relations (Figure 4.C. This improved diversity is also reflected in a reduced pareto effect for the distribution of the organisms (negligible for chemicals), and overlap between the entities reported in each article (Figures 4.D and E). A more comprehensive comparison against others possible sampling strategies for diversity are discussed in supplementary S2.2."
        ]
    },
    "S2.T8": {
        "caption": "Table S8: Statistics about created datasets\u2019 content. For Diversty-raw, the top-50 articles per biological kingdoms (with an available abstract) were reserved for the evaluation set. The train/valid sets are composed of the remaining items split in 90:10. A similar split was performed on the initial 5 random samples to obtain the train/valid datasets of equivalent sizes, referred as the Random-raw datasets. Their count statistics are averaged over the 5 seeds. Extended-raw is the fusion of the Diversty-raw plus the 5 Random-raw datasets. Full is a dataset containing all available examples from the LOTUS snapshot, except the 200 used in the evaluation set. For synthetic datasets, the number of relations, as well as the number of distinct chemicals, is split between chemical entities and chemical classes.",
        "table": "<table id=\"S2.T8.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S2.T8.1.1.1\" class=\"ltx_tr\">\n<td id=\"S2.T8.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\"><span id=\"S2.T8.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td id=\"S2.T8.1.1.1.2\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S2.T8.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Part.</span></td>\n<th id=\"S2.T8.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\">\n<span id=\"S2.T8.1.1.1.3.1\" class=\"ltx_text\"></span> <span id=\"S2.T8.1.1.1.3.2\" class=\"ltx_text\">\n<span id=\"S2.T8.1.1.1.3.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T8.1.1.1.3.2.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T8.1.1.1.3.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S2.T8.1.1.1.3.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\"># Relations</span></span></span>\n<span id=\"S2.T8.1.1.1.3.2.1.2\" class=\"ltx_tr\">\n<span id=\"S2.T8.1.1.1.3.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">(w. chem / w. class)</span></span>\n</span></span><span id=\"S2.T8.1.1.1.3.3\" class=\"ltx_text\"></span>\n</th>\n<td id=\"S2.T8.1.1.1.4\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S2.T8.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\"># Organisms</span></td>\n<th id=\"S2.T8.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\">\n<span id=\"S2.T8.1.1.1.5.1\" class=\"ltx_text\"></span> <span id=\"S2.T8.1.1.1.5.2\" class=\"ltx_text\">\n<span id=\"S2.T8.1.1.1.5.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T8.1.1.1.5.2.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T8.1.1.1.5.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S2.T8.1.1.1.5.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\"># Chemical entities</span></span></span>\n<span id=\"S2.T8.1.1.1.5.2.1.2\" class=\"ltx_tr\">\n<span id=\"S2.T8.1.1.1.5.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">(chem. / class.)</span></span>\n</span></span><span id=\"S2.T8.1.1.1.5.3\" class=\"ltx_text\"></span>\n</th>\n<td id=\"S2.T8.1.1.1.6\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S2.T8.1.1.1.6.1\" class=\"ltx_text ltx_font_bold\"># References</span></td>\n</tr>\n<tr id=\"S2.T8.1.2.2\" class=\"ltx_tr\">\n<td id=\"S2.T8.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\"><span id=\"S2.T8.1.2.2.1.1\" class=\"ltx_text ltx_font_bold\">Diversty-raw</span></td>\n<td id=\"S2.T8.1.2.2.2\" class=\"ltx_td ltx_align_right ltx_border_t\">train</td>\n<td id=\"S2.T8.1.2.2.3\" class=\"ltx_td ltx_align_right ltx_border_t\">12666</td>\n<td id=\"S2.T8.1.2.2.4\" class=\"ltx_td ltx_align_right ltx_border_t\">2644</td>\n<td id=\"S2.T8.1.2.2.5\" class=\"ltx_td ltx_align_right ltx_border_t\">10311</td>\n<td id=\"S2.T8.1.2.2.6\" class=\"ltx_td ltx_align_right ltx_border_t\">1519*</td>\n</tr>\n<tr id=\"S2.T8.1.3.3\" class=\"ltx_tr\">\n<td id=\"S2.T8.1.3.3.1\" class=\"ltx_td ltx_align_right\">valid</td>\n<td id=\"S2.T8.1.3.3.2\" class=\"ltx_td ltx_align_right\">1425</td>\n<td id=\"S2.T8.1.3.3.3\" class=\"ltx_td ltx_align_right\">301</td>\n<td id=\"S2.T8.1.3.3.4\" class=\"ltx_td ltx_align_right\">1211</td>\n<td id=\"S2.T8.1.3.3.5\" class=\"ltx_td ltx_align_right\">168*</td>\n</tr>\n<tr id=\"S2.T8.1.4.4\" class=\"ltx_tr\">\n<td id=\"S2.T8.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\"><span id=\"S2.T8.1.4.4.1.1\" class=\"ltx_text ltx_font_bold\">Random-raw</span></td>\n<td id=\"S2.T8.1.4.4.2\" class=\"ltx_td ltx_align_right ltx_border_t\">train</td>\n<td id=\"S2.T8.1.4.4.3\" class=\"ltx_td ltx_align_right ltx_border_t\">5102</td>\n<td id=\"S2.T8.1.4.4.4\" class=\"ltx_td ltx_align_right ltx_border_t\">1434</td>\n<td id=\"S2.T8.1.4.4.5\" class=\"ltx_td ltx_align_right ltx_border_t\">4286</td>\n<td id=\"S2.T8.1.4.4.6\" class=\"ltx_td ltx_align_right ltx_border_t\">1531*</td>\n</tr>\n<tr id=\"S2.T8.1.5.5\" class=\"ltx_tr\">\n<td id=\"S2.T8.1.5.5.1\" class=\"ltx_td ltx_align_right\">valid</td>\n<td id=\"S2.T8.1.5.5.2\" class=\"ltx_td ltx_align_right\">657</td>\n<td id=\"S2.T8.1.5.5.3\" class=\"ltx_td ltx_align_right\">220</td>\n<td id=\"S2.T8.1.5.5.4\" class=\"ltx_td ltx_align_right\">584</td>\n<td id=\"S2.T8.1.5.5.5\" class=\"ltx_td ltx_align_right\">189*</td>\n</tr>\n<tr id=\"S2.T8.1.6.6\" class=\"ltx_tr\">\n<td id=\"S2.T8.1.6.6.1\" class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\"><span id=\"S2.T8.1.6.6.1.1\" class=\"ltx_text ltx_font_bold\">Extended-raw</span></td>\n<td id=\"S2.T8.1.6.6.2\" class=\"ltx_td ltx_align_right ltx_border_t\">train</td>\n<td id=\"S2.T8.1.6.6.3\" class=\"ltx_td ltx_align_right ltx_border_t\">27952</td>\n<td id=\"S2.T8.1.6.6.4\" class=\"ltx_td ltx_align_right ltx_border_t\">5642</td>\n<td id=\"S2.T8.1.6.6.5\" class=\"ltx_td ltx_align_right ltx_border_t\">21028</td>\n<td id=\"S2.T8.1.6.6.6\" class=\"ltx_td ltx_align_right ltx_border_t\">7111*</td>\n</tr>\n<tr id=\"S2.T8.1.7.7\" class=\"ltx_tr\">\n<td id=\"S2.T8.1.7.7.1\" class=\"ltx_td ltx_align_right\">valid</td>\n<td id=\"S2.T8.1.7.7.2\" class=\"ltx_td ltx_align_right\">3355</td>\n<td id=\"S2.T8.1.7.7.3\" class=\"ltx_td ltx_align_right\">932</td>\n<td id=\"S2.T8.1.7.7.4\" class=\"ltx_td ltx_align_right\">2741</td>\n<td id=\"S2.T8.1.7.7.5\" class=\"ltx_td ltx_align_right\">790*</td>\n</tr>\n<tr id=\"S2.T8.1.8.8\" class=\"ltx_tr\">\n<td id=\"S2.T8.1.8.8.1\" class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\"><span id=\"S2.T8.1.8.8.1.1\" class=\"ltx_text ltx_font_bold\">Full</span></td>\n<td id=\"S2.T8.1.8.8.2\" class=\"ltx_td ltx_align_right ltx_border_t\">train</td>\n<td id=\"S2.T8.1.8.8.3\" class=\"ltx_td ltx_align_right ltx_border_t\">90326</td>\n<td id=\"S2.T8.1.8.8.4\" class=\"ltx_td ltx_align_right ltx_border_t\">13208</td>\n<td id=\"S2.T8.1.8.8.5\" class=\"ltx_td ltx_align_right ltx_border_t\">51658</td>\n<td id=\"S2.T8.1.8.8.6\" class=\"ltx_td ltx_align_right ltx_border_t\">28286</td>\n</tr>\n<tr id=\"S2.T8.1.9.9\" class=\"ltx_tr\">\n<td id=\"S2.T8.1.9.9.1\" class=\"ltx_td ltx_align_right\">valid</td>\n<td id=\"S2.T8.1.9.9.2\" class=\"ltx_td ltx_align_right\">1533</td>\n<td id=\"S2.T8.1.9.9.3\" class=\"ltx_td ltx_align_right\">484</td>\n<td id=\"S2.T8.1.9.9.4\" class=\"ltx_td ltx_align_right\">1288</td>\n<td id=\"S2.T8.1.9.9.5\" class=\"ltx_td ltx_align_right\">430</td>\n</tr>\n<tr id=\"S2.T8.1.10.10\" class=\"ltx_tr\">\n<td id=\"S2.T8.1.10.10.1\" class=\"ltx_td ltx_align_left\" rowspan=\"2\"><span id=\"S2.T8.1.10.10.1.1\" class=\"ltx_text ltx_font_bold\">Diversity-synt</span></td>\n<td id=\"S2.T8.1.10.10.2\" class=\"ltx_td ltx_align_right\">train</td>\n<td id=\"S2.T8.1.10.10.3\" class=\"ltx_td ltx_align_right\">11547 (10764 / 783)</td>\n<td id=\"S2.T8.1.10.10.4\" class=\"ltx_td ltx_align_right\">2154</td>\n<td id=\"S2.T8.1.10.10.5\" class=\"ltx_td ltx_align_right\">(9108 / 61)</td>\n<td id=\"S2.T8.1.10.10.6\" class=\"ltx_td ltx_align_right\">3562</td>\n</tr>\n<tr id=\"S2.T8.1.11.11\" class=\"ltx_tr\">\n<td id=\"S2.T8.1.11.11.1\" class=\"ltx_td ltx_align_right\">valid</td>\n<td id=\"S2.T8.1.11.11.2\" class=\"ltx_td ltx_align_right\">1197 (1096 / 101)</td>\n<td id=\"S2.T8.1.11.11.3\" class=\"ltx_td ltx_align_right\">220</td>\n<td id=\"S2.T8.1.11.11.4\" class=\"ltx_td ltx_align_right\">(998 / 37)</td>\n<td id=\"S2.T8.1.11.11.5\" class=\"ltx_td ltx_align_right\">389</td>\n</tr>\n<tr id=\"S2.T8.1.12.12\" class=\"ltx_tr\">\n<td id=\"S2.T8.1.12.12.1\" class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\"><span id=\"S2.T8.1.12.12.1.1\" class=\"ltx_text ltx_font_bold\">Random-synt</span></td>\n<td id=\"S2.T8.1.12.12.2\" class=\"ltx_td ltx_align_right ltx_border_t\">train</td>\n<td id=\"S2.T8.1.12.12.3\" class=\"ltx_td ltx_align_right ltx_border_t\">4825 (4474 / 351)</td>\n<td id=\"S2.T8.1.12.12.4\" class=\"ltx_td ltx_align_right ltx_border_t\">1267</td>\n<td id=\"S2.T8.1.12.12.5\" class=\"ltx_td ltx_align_right ltx_border_t\">(3854 / 53 )</td>\n<td id=\"S2.T8.1.12.12.6\" class=\"ltx_td ltx_align_right ltx_border_t\">3798</td>\n</tr>\n<tr id=\"S2.T8.1.13.13\" class=\"ltx_tr\">\n<td id=\"S2.T8.1.13.13.1\" class=\"ltx_td ltx_align_right\">valid</td>\n<td id=\"S2.T8.1.13.13.2\" class=\"ltx_td ltx_align_right\">609 (561 / 47)</td>\n<td id=\"S2.T8.1.13.13.3\" class=\"ltx_td ltx_align_right\">190</td>\n<td id=\"S2.T8.1.13.13.4\" class=\"ltx_td ltx_align_right\">(507 / 22)</td>\n<td id=\"S2.T8.1.13.13.5\" class=\"ltx_td ltx_align_right\">460</td>\n</tr>\n<tr id=\"S2.T8.1.14.14\" class=\"ltx_tr\">\n<td id=\"S2.T8.1.14.14.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"2\"><span id=\"S2.T8.1.14.14.1.1\" class=\"ltx_text ltx_font_bold\">Extended-synt</span></td>\n<td id=\"S2.T8.1.14.14.2\" class=\"ltx_td ltx_align_right ltx_border_t\">train</td>\n<td id=\"S2.T8.1.14.14.3\" class=\"ltx_td ltx_align_right ltx_border_t\">28614 (26373 / 2242)</td>\n<td id=\"S2.T8.1.14.14.4\" class=\"ltx_td ltx_align_right ltx_border_t\">5258</td>\n<td id=\"S2.T8.1.14.14.5\" class=\"ltx_td ltx_align_right ltx_border_t\">(20404 / 69)</td>\n<td id=\"S2.T8.1.14.14.6\" class=\"ltx_td ltx_align_right ltx_border_t\">23985</td>\n</tr>\n<tr id=\"S2.T8.1.15.15\" class=\"ltx_tr\">\n<td id=\"S2.T8.1.15.15.1\" class=\"ltx_td ltx_align_right ltx_border_bb\">valid</td>\n<td id=\"S2.T8.1.15.15.2\" class=\"ltx_td ltx_align_right ltx_border_bb\">1444 (1332 / 112)</td>\n<td id=\"S2.T8.1.15.15.3\" class=\"ltx_td ltx_align_right ltx_border_bb\">432</td>\n<td id=\"S2.T8.1.15.15.4\" class=\"ltx_td ltx_align_right ltx_border_bb\">(1122 / 37)</td>\n<td id=\"S2.T8.1.15.15.5\" class=\"ltx_td ltx_align_right ltx_border_bb\">1254</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "The mismatches between the standardized labels and the original abstracts have therefore been corrected for the evaluation set. However, due to the considerable investment of time and resources required for this task, the same corrections were not applied on the remaining data available for training. In this particular context of noisy data for end-to-end RE, two strategies were evaluated: standard fine-tuning and few-shot learning, the latter being able to rely only on a few manually selected examples. The performance of the fine-tuning strategy were evaluated based on all available examples (Full) and 3 alternatives train/valid datasets derived from the initial Diversity, Random and Extended samples, here after referred as: Diversity-raw, Random-raw and their union Extended-raw. Their respective sizes and splits are detailed in supplementary table S8. All datasets were used to train 3 models for end-to-end RE: Seq2rel, BioGPT and GPT-2. Six open LLMs were also evaluated in few-shot learning settings: LLaMA 7B, 13B, 30B and 65B, along with two models, respectively fine-tuned on instructions and conversations and derived from LLaMA 7B and 13B: Alpaca-7B and Vicuna-13B.\nBest performance in fine-tuning settings was achieved by BioGPT (Table 1). Regardless of the training dataset777Except for Full, it consistently outperformed Seq2rel and GPT-2 and demonstrated an f1-score of 32.5%percent32.532.5\\% when trained on Extended-raw. We also evaluated the influence of the different training datasets on models performance. The results indicate that models trained on Diversity-raw outperformed those models trained on Random-raw, with a notable improvement in recall at the expense of precision. Merging the datasets into a larger (Extended-raw) also resulted in improved performance for all models. However, expanding the dataset to all available examples only barely improved the previous performance and surprisingly underperformed with BioGPT. In few-shot learning scenarios, the best performance was obtained with LLaMA-65B and decline with smaller models. Although the performance was inferior compared to fine-tuned alternatives, the models achieved reasonable scores considering the limited number of archetypal examples provided. These results also emphasize the potential of few-shot learning or prompt-tuning based approaches in practical context with low-resources.",
            "While LLMs cannot compete in terms of performance with fine-tuned approaches in the evaluated settings, their generative abilities could be used alternatively to address the main bottleneck: the discrepancies between the input text and the labels in the training data. It requires going beyond distant supervision or data augmentation [31, 91, 33]. The former involves mapping relationships from a Knowledge Base to a large corpus of text to generate pseudo-labels, whereas the latter entails applying a range of transformations, permutations, or morphings to a core set of high-quality examples. In contrast, the adaptive described approach propose to generate a set of synthetic input abstracts from a pre-defined context and a set of expected output labels (i.e organism - NP relationships).\nTo maintain consistency, each synthetic abstract is based on the context and results reported from an original seed abstract. The first step is to generate the instructions to prompt the selected LLM for generation. The instructions are composed of a title, a list of keywords and the verbalised main findings (Method 2.3). We decided to use the open source Vicuna-13B [92]888version v1.3 from 22/06/2023: https://huggingface.co/lmsys/vicuna-13b-v1.3, a LLaMA-13B model fine-tuned on user-shared conversations collected from ShareGPT999https://sharegpt.com/, which outperforms alternatives of equivalent sizes on several benchmarks [73]. For each input seed abstract, the top-10 extracted keywords were used in the built instruction. As this is a crucial step, the performance of Vicuna-13B to extract keywords have been evaluated on the SemEval2017-Task10 dataset [93] in supplementary S2.6. To diversify the generated abstracts, m=10\ud835\udc5a10m=10 instructions prompts with different verbalisation patterns were then sampled per initial seed article. Finally, only the top k=3\ud835\udc583k=3 most relevant synthesized abstracts per seed were selected with the simple, yet effective, selector module.\nTo evaluate the impact of diversity-sampling on the seed articles used for synthetic generation, we created two new datasets: Diversity-synth and Random-synth, derived from the original abstracts in the Diversity-raw and Random-raw datasets, respectively. Several illustrative examples of synthetic abstracts from Diversity-synth are discussed in supplementary S2.7, highlighting both the variability and the potential caveats (errors, hallucinations) of the process. As with the original data, Diversity-synt and Random-synt were merged in Extended-synt to measure the impact of the dataset size. Statistics of the generated datasets are presented in supplementary table S8. In total, more than 25,000 synthetic abstracts were generated from the 7901 originally contained in the raw datasets. From Diversity-raw, 200 initial items were excluded by the selector module and 162 on average for Random-raw. While the distinct numbers of entities/relations dropped in synthetic datasets, the selector guarantees that these labels are part of the generated abstracts. Furthermore, the generation process enables the integration of examples with chemical classes in the input text and expected labels, which were not available in the original data.\n"
        ]
    },
    "S2.T9": {
        "caption": "Table S9: Statistics of the number of organisms, chemicals, and relations in the top-200 abstracts selected and curated in the evaluation set, compared to 200 randomly selected items (statistics averaged over 5 random seeds). For the evaluation set, the number of annotated distinct chemical compounds and chemical classes are respectively indicated between parentheses. In the curated evaluation set, 13 references had no relation directly expressed in the abstract.",
        "table": "<table id=\"S2.T9.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S2.T9.1.1\" class=\"ltx_tr\">\n<td id=\"S2.T9.1.1.1\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"S2.T9.1.1.2\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S2.T9.1.1.2.1\" class=\"ltx_text ltx_font_bold\"># Organisms</span></td>\n<td id=\"S2.T9.1.1.3\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S2.T9.1.1.3.1\" class=\"ltx_text ltx_font_bold\"># Chemicals</span></td>\n<td id=\"S2.T9.1.1.4\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S2.T9.1.1.4.1\" class=\"ltx_text ltx_font_bold\"># Relations</span></td>\n<td id=\"S2.T9.1.1.5\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S2.T9.1.1.5.1\" class=\"ltx_text ltx_font_bold\"># References</span></td>\n</tr>\n<tr id=\"S2.T9.1.2\" class=\"ltx_tr\">\n<td id=\"S2.T9.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S2.T9.1.2.1.1\" class=\"ltx_text ltx_font_bold\">eval-set (top200 diversity)</span></td>\n<td id=\"S2.T9.1.2.2\" class=\"ltx_td ltx_align_right ltx_border_t\">275</td>\n<td id=\"S2.T9.1.2.3\" class=\"ltx_td ltx_align_right ltx_border_t\">1197 ( 1092 / 105)</td>\n<td id=\"S2.T9.1.2.4\" class=\"ltx_td ltx_align_right ltx_border_t\">1488 (1297 / 191)</td>\n<td id=\"S2.T9.1.2.5\" class=\"ltx_td ltx_align_right ltx_border_t\">200 (187*)</td>\n</tr>\n<tr id=\"S2.T9.1.3\" class=\"ltx_tr\">\n<td id=\"S2.T9.1.3.1\" class=\"ltx_td ltx_align_left ltx_border_bb\"><span id=\"S2.T9.1.3.1.1\" class=\"ltx_text ltx_font_bold\">Random (200 articles)</span></td>\n<td id=\"S2.T9.1.3.2\" class=\"ltx_td ltx_align_right ltx_border_bb\">238</td>\n<td id=\"S2.T9.1.3.3\" class=\"ltx_td ltx_align_right ltx_border_bb\">610</td>\n<td id=\"S2.T9.1.3.4\" class=\"ltx_td ltx_align_right ltx_border_bb\">699</td>\n<td id=\"S2.T9.1.3.5\" class=\"ltx_td ltx_align_right ltx_border_bb\">200</td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "The composition of the curated evaluation dataset, in terms of number of distinct entities and relationships, is compared to 5 random sets of equivalent sizes. Firstly, 13 abstracts did not mention any relationships between organisms and chemicals in the curated dataset. Secondly, for the random sets, statistics were directly estimated from the LOTUS annotations. Then, they may represent an overestimate of the actual number of distinct entities, given that a manual curation could potentially eliminate some irrelevant annotations that are actually not mention in the abstracts. They should therefore be regarded as an approximate upper bound. Considering the last points, the proposed strategy for selecting the evaluation set has significantly improved the diversity, as highlighted in Table S9."
        ]
    },
    "S3.T1": {
        "caption": "Table 1: Performance of 5-shot in-context learning using LLaMA and LLaMA-instructed models compared to seq2rel, GPT-2, BioGPT finetuned models. Three types of training dataset are evaluated: the diversity sample (Diversty-raw), 5 random samples (Random-raw) and the extended sample (Extended-raw), which is the union of the previous samples. Full is a dataset contained all available examples from the LOTUS snapshot, except the 200 used in the evaluation set. Best performance via finetuning are bold, while best performance in few-shot settings are underlined.",
        "table": "<table id=\"S3.T1.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S3.T1.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\"><span id=\"S3.T1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">model</span></td>\n<td id=\"S3.T1.1.1.2\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S3.T1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Training</span></td>\n<td id=\"S3.T1.1.1.3\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S3.T1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">precision</span></td>\n<td id=\"S3.T1.1.1.4\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S3.T1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">recall</span></td>\n<td id=\"S3.T1.1.1.5\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S3.T1.1.1.5.1\" class=\"ltx_text ltx_font_bold\">f1</span></td>\n<td id=\"S3.T1.1.1.6\" class=\"ltx_td ltx_border_tt\"></td>\n</tr>\n<tr id=\"S3.T1.1.2\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S3.T1.1.2.1.1\" class=\"ltx_text ltx_font_bold\">LLaMA-7B</span></td>\n<td id=\"S3.T1.1.2.2\" class=\"ltx_td ltx_align_right ltx_border_t\" rowspan=\"6\"><span id=\"S3.T1.1.2.2.1\" class=\"ltx_text\">Few-shot learning (5-shot)</span></td>\n<td id=\"S3.T1.1.2.3\" class=\"ltx_td ltx_align_right ltx_border_t\">27.0</td>\n<td id=\"S3.T1.1.2.4\" class=\"ltx_td ltx_align_right ltx_border_t\">9.04</td>\n<td id=\"S3.T1.1.2.5\" class=\"ltx_td ltx_align_right ltx_border_t\">13.55</td>\n<td id=\"S3.T1.1.2.6\" class=\"ltx_td ltx_border_t\"></td>\n</tr>\n<tr id=\"S3.T1.1.3\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.3.1\" class=\"ltx_td ltx_align_left\"><span id=\"S3.T1.1.3.1.1\" class=\"ltx_text ltx_font_bold\">LLaMA-13B</span></td>\n<td id=\"S3.T1.1.3.2\" class=\"ltx_td ltx_align_right\">35.64</td>\n<td id=\"S3.T1.1.3.3\" class=\"ltx_td ltx_align_right\">23.64</td>\n<td id=\"S3.T1.1.3.4\" class=\"ltx_td ltx_align_right\">28.49</td>\n<td id=\"S3.T1.1.3.5\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T1.1.4\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.4.1\" class=\"ltx_td ltx_align_left\"><span id=\"S3.T1.1.4.1.1\" class=\"ltx_text ltx_font_bold\">LLaMA-30B</span></td>\n<td id=\"S3.T1.1.4.2\" class=\"ltx_td ltx_align_right\">38.51</td>\n<td id=\"S3.T1.1.4.3\" class=\"ltx_td ltx_align_right\"><span id=\"S3.T1.1.4.3.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">23.24</span></td>\n<td id=\"S3.T1.1.4.4\" class=\"ltx_td ltx_align_right\">28.99</td>\n<td id=\"S3.T1.1.4.5\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T1.1.5\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.5.1\" class=\"ltx_td ltx_align_left\"><span id=\"S3.T1.1.5.1.1\" class=\"ltx_text ltx_font_bold\">LLaMA-65B</span></td>\n<td id=\"S3.T1.1.5.2\" class=\"ltx_td ltx_align_right\"><span id=\"S3.T1.1.5.2.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">40.16</span></td>\n<td id=\"S3.T1.1.5.3\" class=\"ltx_td ltx_align_right\">22.97</td>\n<td id=\"S3.T1.1.5.4\" class=\"ltx_td ltx_align_right\"><span id=\"S3.T1.1.5.4.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">29.23</span></td>\n<td id=\"S3.T1.1.5.5\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T1.1.6\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.6.1\" class=\"ltx_td ltx_align_left\"><span id=\"S3.T1.1.6.1.1\" class=\"ltx_text ltx_font_bold\">Alpaca-7B</span></td>\n<td id=\"S3.T1.1.6.2\" class=\"ltx_td ltx_align_right\">15.14</td>\n<td id=\"S3.T1.1.6.3\" class=\"ltx_td ltx_align_right\">2.21</td>\n<td id=\"S3.T1.1.6.4\" class=\"ltx_td ltx_align_right\">5.86</td>\n<td id=\"S3.T1.1.6.5\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T1.1.7\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.7.1\" class=\"ltx_td ltx_align_left\"><span id=\"S3.T1.1.7.1.1\" class=\"ltx_text ltx_font_bold\">Vicuna-13B</span></td>\n<td id=\"S3.T1.1.7.2\" class=\"ltx_td ltx_align_right\">38.4</td>\n<td id=\"S3.T1.1.7.3\" class=\"ltx_td ltx_align_right\">20.43</td>\n<td id=\"S3.T1.1.7.4\" class=\"ltx_td ltx_align_right\">26.48</td>\n<td id=\"S3.T1.1.7.5\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T1.1.8\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.8.1\" class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\"><span id=\"S3.T1.1.8.1.1\" class=\"ltx_text ltx_font_bold\">Seq2rel</span></td>\n<td id=\"S3.T1.1.8.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S3.T1.1.8.2.1\" class=\"ltx_text ltx_font_typewriter\">Random-raw</span></td>\n<td id=\"S3.T1.1.8.3\" class=\"ltx_td ltx_align_right ltx_border_t\">43.2 +/- (6.67)</td>\n<td id=\"S3.T1.1.8.4\" class=\"ltx_td ltx_align_right ltx_border_t\">4.8 +/- (1.16)</td>\n<td id=\"S3.T1.1.8.5\" class=\"ltx_td ltx_align_right ltx_border_t\">8.6 +/- (2.00)</td>\n<td id=\"S3.T1.1.8.6\" class=\"ltx_td ltx_border_t\"></td>\n</tr>\n<tr id=\"S3.T1.1.9\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.9.1\" class=\"ltx_td ltx_align_right\"><span id=\"S3.T1.1.9.1.1\" class=\"ltx_text ltx_font_typewriter\">Diversty-raw</span></td>\n<td id=\"S3.T1.1.9.2\" class=\"ltx_td ltx_align_right\">39.6</td>\n<td id=\"S3.T1.1.9.3\" class=\"ltx_td ltx_align_right\">5.4</td>\n<td id=\"S3.T1.1.9.4\" class=\"ltx_td ltx_align_right\">9.5</td>\n<td id=\"S3.T1.1.9.5\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T1.1.10\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.10.1\" class=\"ltx_td ltx_align_right\"><span id=\"S3.T1.1.10.1.1\" class=\"ltx_text ltx_font_typewriter\">Extended-raw</span></td>\n<td id=\"S3.T1.1.10.2\" class=\"ltx_td ltx_align_right\"><span id=\"S3.T1.1.10.2.1\" class=\"ltx_text ltx_font_bold\">47.3</span></td>\n<td id=\"S3.T1.1.10.3\" class=\"ltx_td ltx_align_right\">5.8</td>\n<td id=\"S3.T1.1.10.4\" class=\"ltx_td ltx_align_right\">10.4</td>\n<td id=\"S3.T1.1.10.5\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T1.1.11\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.11.1\" class=\"ltx_td\"></td>\n<td id=\"S3.T1.1.11.2\" class=\"ltx_td ltx_align_right\"><span id=\"S3.T1.1.11.2.1\" class=\"ltx_text ltx_font_typewriter\">Full</span></td>\n<td id=\"S3.T1.1.11.3\" class=\"ltx_td ltx_align_right\">45.6</td>\n<td id=\"S3.T1.1.11.4\" class=\"ltx_td ltx_align_right\">7.1</td>\n<td id=\"S3.T1.1.11.5\" class=\"ltx_td ltx_align_right\">12.2</td>\n<td id=\"S3.T1.1.11.6\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T1.1.12\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.12.1\" class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\"><span id=\"S3.T1.1.12.1.1\" class=\"ltx_text ltx_font_bold\">GPT-2</span></td>\n<td id=\"S3.T1.1.12.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S3.T1.1.12.2.1\" class=\"ltx_text ltx_font_typewriter\">Random-raw</span></td>\n<td id=\"S3.T1.1.12.3\" class=\"ltx_td ltx_align_right ltx_border_t\">32.5 +/- (4.83)</td>\n<td id=\"S3.T1.1.12.4\" class=\"ltx_td ltx_align_right ltx_border_t\">11.8 +/- (5.25)</td>\n<td id=\"S3.T1.1.12.5\" class=\"ltx_td ltx_align_right ltx_border_t\">15.0 +/- (2.54)</td>\n<td id=\"S3.T1.1.12.6\" class=\"ltx_td ltx_border_t\"></td>\n</tr>\n<tr id=\"S3.T1.1.13\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.13.1\" class=\"ltx_td ltx_align_right\"><span id=\"S3.T1.1.13.1.1\" class=\"ltx_text ltx_font_typewriter\">Diversty-raw</span></td>\n<td id=\"S3.T1.1.13.2\" class=\"ltx_td ltx_align_right\">22.3</td>\n<td id=\"S3.T1.1.13.3\" class=\"ltx_td ltx_align_right\">19.2</td>\n<td id=\"S3.T1.1.13.4\" class=\"ltx_td ltx_align_right\">20.6</td>\n<td id=\"S3.T1.1.13.5\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T1.1.14\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.14.1\" class=\"ltx_td ltx_align_right\"><span id=\"S3.T1.1.14.1.1\" class=\"ltx_text ltx_font_typewriter\">Extended-raw</span></td>\n<td id=\"S3.T1.1.14.2\" class=\"ltx_td ltx_align_right\">44.8</td>\n<td id=\"S3.T1.1.14.3\" class=\"ltx_td ltx_align_right\">21.7</td>\n<td id=\"S3.T1.1.14.4\" class=\"ltx_td ltx_align_right\">29.3</td>\n<td id=\"S3.T1.1.14.5\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T1.1.15\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.15.1\" class=\"ltx_td\"></td>\n<td id=\"S3.T1.1.15.2\" class=\"ltx_td ltx_align_right\"><span id=\"S3.T1.1.15.2.1\" class=\"ltx_text ltx_font_typewriter\">Full</span></td>\n<td id=\"S3.T1.1.15.3\" class=\"ltx_td ltx_align_right\">47.5</td>\n<td id=\"S3.T1.1.15.4\" class=\"ltx_td ltx_align_right\">22.5</td>\n<td id=\"S3.T1.1.15.5\" class=\"ltx_td ltx_align_right\">30.5</td>\n<td id=\"S3.T1.1.15.6\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T1.1.16\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.16.1\" class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\"><span id=\"S3.T1.1.16.1.1\" class=\"ltx_text ltx_font_bold\">BioGPT</span></td>\n<td id=\"S3.T1.1.16.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S3.T1.1.16.2.1\" class=\"ltx_text ltx_font_typewriter\">Random-raw</span></td>\n<td id=\"S3.T1.1.16.3\" class=\"ltx_td ltx_align_right ltx_border_t\">47.2 +/- (4.01)</td>\n<td id=\"S3.T1.1.16.4\" class=\"ltx_td ltx_align_right ltx_border_t\">19.8 +/- (2.71)</td>\n<td id=\"S3.T1.1.16.5\" class=\"ltx_td ltx_align_right ltx_border_t\">27.6 +/- (2.48)</td>\n<td id=\"S3.T1.1.16.6\" class=\"ltx_td ltx_border_t\"></td>\n</tr>\n<tr id=\"S3.T1.1.17\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.17.1\" class=\"ltx_td ltx_align_right\"><span id=\"S3.T1.1.17.1.1\" class=\"ltx_text ltx_font_typewriter\">Diversty-raw</span></td>\n<td id=\"S3.T1.1.17.2\" class=\"ltx_td ltx_align_right\">37.1</td>\n<td id=\"S3.T1.1.17.3\" class=\"ltx_td ltx_align_right\"><span id=\"S3.T1.1.17.3.1\" class=\"ltx_text ltx_font_bold\">28.4</span></td>\n<td id=\"S3.T1.1.17.4\" class=\"ltx_td ltx_align_right\">32.2</td>\n<td id=\"S3.T1.1.17.5\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T1.1.18\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.18.1\" class=\"ltx_td ltx_align_right\"><span id=\"S3.T1.1.18.1.1\" class=\"ltx_text ltx_font_typewriter\">Extended-raw</span></td>\n<td id=\"S3.T1.1.18.2\" class=\"ltx_td ltx_align_right\">42.2</td>\n<td id=\"S3.T1.1.18.3\" class=\"ltx_td ltx_align_right\">26.5</td>\n<td id=\"S3.T1.1.18.4\" class=\"ltx_td ltx_align_right\"><span id=\"S3.T1.1.18.4.1\" class=\"ltx_text ltx_font_bold\">32.5</span></td>\n<td id=\"S3.T1.1.18.5\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T1.1.19\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.19.1\" class=\"ltx_td ltx_border_bb\"></td>\n<td id=\"S3.T1.1.19.2\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S3.T1.1.19.2.1\" class=\"ltx_text ltx_font_typewriter\">Full</span></td>\n<td id=\"S3.T1.1.19.3\" class=\"ltx_td ltx_align_right ltx_border_bb\">46.7</td>\n<td id=\"S3.T1.1.19.4\" class=\"ltx_td ltx_align_right ltx_border_bb\">21.3</td>\n<td id=\"S3.T1.1.19.5\" class=\"ltx_td ltx_align_right ltx_border_bb\">29.3</td>\n<td id=\"S3.T1.1.19.6\" class=\"ltx_td ltx_border_bb\"></td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "The mismatches between the standardized labels and the original abstracts have therefore been corrected for the evaluation set. However, due to the considerable investment of time and resources required for this task, the same corrections were not applied on the remaining data available for training. In this particular context of noisy data for end-to-end RE, two strategies were evaluated: standard fine-tuning and few-shot learning, the latter being able to rely only on a few manually selected examples. The performance of the fine-tuning strategy were evaluated based on all available examples (Full) and 3 alternatives train/valid datasets derived from the initial Diversity, Random and Extended samples, here after referred as: Diversity-raw, Random-raw and their union Extended-raw. Their respective sizes and splits are detailed in supplementary table S8. All datasets were used to train 3 models for end-to-end RE: Seq2rel, BioGPT and GPT-2. Six open LLMs were also evaluated in few-shot learning settings: LLaMA 7B, 13B, 30B and 65B, along with two models, respectively fine-tuned on instructions and conversations and derived from LLaMA 7B and 13B: Alpaca-7B and Vicuna-13B.\nBest performance in fine-tuning settings was achieved by BioGPT (Table 1). Regardless of the training dataset777Except for Full, it consistently outperformed Seq2rel and GPT-2 and demonstrated an f1-score of 32.5%percent32.532.5\\% when trained on Extended-raw. We also evaluated the influence of the different training datasets on models performance. The results indicate that models trained on Diversity-raw outperformed those models trained on Random-raw, with a notable improvement in recall at the expense of precision. Merging the datasets into a larger (Extended-raw) also resulted in improved performance for all models. However, expanding the dataset to all available examples only barely improved the previous performance and surprisingly underperformed with BioGPT. In few-shot learning scenarios, the best performance was obtained with LLaMA-65B and decline with smaller models. Although the performance was inferior compared to fine-tuned alternatives, the models achieved reasonable scores considering the limited number of archetypal examples provided. These results also emphasize the potential of few-shot learning or prompt-tuning based approaches in practical context with low-resources."
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Performance of Seq2rel, GPT-2 and BioGPT models fine-tuned on synthetic data. Three types of training dataset are evaluated: the diversity sample (Diversty-synt), 5 random samples (Random-synt) and the extended sample (Extended-synt), which is the union of the previous samples, all synthetically generated from the corresponding seed original samples. For Random-synt samples, results are averaged and standard deviations are reported. Best performance are bold, and second best performance are underlined.",
        "table": "<table id=\"S3.T2.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S3.T2.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\"><span id=\"S3.T2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">model</span></td>\n<td id=\"S3.T2.1.1.2\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S3.T2.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td id=\"S3.T2.1.1.3\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S3.T2.1.1.3.1\" class=\"ltx_text ltx_font_bold\">precision</span></td>\n<td id=\"S3.T2.1.1.4\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S3.T2.1.1.4.1\" class=\"ltx_text ltx_font_bold\">recall</span></td>\n<td id=\"S3.T2.1.1.5\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S3.T2.1.1.5.1\" class=\"ltx_text ltx_font_bold\">f1</span></td>\n<td id=\"S3.T2.1.1.6\" class=\"ltx_td ltx_border_tt\"></td>\n</tr>\n<tr id=\"S3.T2.1.2\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\"><span id=\"S3.T2.1.2.1.1\" class=\"ltx_text ltx_font_bold\">Seq2rel</span></td>\n<td id=\"S3.T2.1.2.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S3.T2.1.2.2.1\" class=\"ltx_text ltx_font_typewriter\">Random-synt</span></td>\n<td id=\"S3.T2.1.2.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S3.T2.1.2.3.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">62.4 +/- (1.03)</span></td>\n<td id=\"S3.T2.1.2.4\" class=\"ltx_td ltx_align_right ltx_border_t\">26.8 +/- (1.96)</td>\n<td id=\"S3.T2.1.2.5\" class=\"ltx_td ltx_align_right ltx_border_t\">37.5 +/- (1.90)</td>\n<td id=\"S3.T2.1.2.6\" class=\"ltx_td ltx_border_t\"></td>\n</tr>\n<tr id=\"S3.T2.1.3\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.3.1\" class=\"ltx_td ltx_align_right\"><span id=\"S3.T2.1.3.1.1\" class=\"ltx_text ltx_font_typewriter\">Diversty-synt</span></td>\n<td id=\"S3.T2.1.3.2\" class=\"ltx_td ltx_align_right\">61.5</td>\n<td id=\"S3.T2.1.3.3\" class=\"ltx_td ltx_align_right\">30.7</td>\n<td id=\"S3.T2.1.3.4\" class=\"ltx_td ltx_align_right\">40.1</td>\n<td id=\"S3.T2.1.3.5\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T2.1.4\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.4.1\" class=\"ltx_td ltx_align_right\"><span id=\"S3.T2.1.4.1.1\" class=\"ltx_text ltx_font_typewriter\">Extended-synt</span></td>\n<td id=\"S3.T2.1.4.2\" class=\"ltx_td ltx_align_right\"><span id=\"S3.T2.1.4.2.1\" class=\"ltx_text ltx_font_bold\">65.1</span></td>\n<td id=\"S3.T2.1.4.3\" class=\"ltx_td ltx_align_right\">29.9</td>\n<td id=\"S3.T2.1.4.4\" class=\"ltx_td ltx_align_right\">41.0</td>\n<td id=\"S3.T2.1.4.5\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T2.1.5\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.5.1\" class=\"ltx_td ltx_align_left\" rowspan=\"3\"><span id=\"S3.T2.1.5.1.1\" class=\"ltx_text ltx_font_bold\">GPT-2</span></td>\n<td id=\"S3.T2.1.5.2\" class=\"ltx_td ltx_align_right\"><span id=\"S3.T2.1.5.2.1\" class=\"ltx_text ltx_font_typewriter\">Random-synt</span></td>\n<td id=\"S3.T2.1.5.3\" class=\"ltx_td ltx_align_right\">42.6 +/- (2.89)</td>\n<td id=\"S3.T2.1.5.4\" class=\"ltx_td ltx_align_right\">32.7 +/- (2.81)</td>\n<td id=\"S3.T2.1.5.5\" class=\"ltx_td ltx_align_right\">37.2 +/- (2.80)</td>\n<td id=\"S3.T2.1.5.6\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T2.1.6\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.6.1\" class=\"ltx_td ltx_align_right\"><span id=\"S3.T2.1.6.1.1\" class=\"ltx_text ltx_font_typewriter\">Diversty-synt</span></td>\n<td id=\"S3.T2.1.6.2\" class=\"ltx_td ltx_align_right\">28.5</td>\n<td id=\"S3.T2.1.6.3\" class=\"ltx_td ltx_align_right\">39.4</td>\n<td id=\"S3.T2.1.6.4\" class=\"ltx_td ltx_align_right\">33.0</td>\n<td id=\"S3.T2.1.6.5\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T2.1.7\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.7.1\" class=\"ltx_td ltx_align_right\"><span id=\"S3.T2.1.7.1.1\" class=\"ltx_text ltx_font_typewriter\">Extended-synt</span></td>\n<td id=\"S3.T2.1.7.2\" class=\"ltx_td ltx_align_right\">52.0</td>\n<td id=\"S3.T2.1.7.3\" class=\"ltx_td ltx_align_right\">44.6</td>\n<td id=\"S3.T2.1.7.4\" class=\"ltx_td ltx_align_right\">48.0</td>\n<td id=\"S3.T2.1.7.5\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T2.1.8\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.8.1\" class=\"ltx_td ltx_align_left ltx_border_bb\" rowspan=\"3\"><span id=\"S3.T2.1.8.1.1\" class=\"ltx_text ltx_font_bold\">BioGPT</span></td>\n<td id=\"S3.T2.1.8.2\" class=\"ltx_td ltx_align_right\"><span id=\"S3.T2.1.8.2.1\" class=\"ltx_text ltx_font_typewriter\">Random-synt</span></td>\n<td id=\"S3.T2.1.8.3\" class=\"ltx_td ltx_align_right\">56.4 +/- (2.26)</td>\n<td id=\"S3.T2.1.8.4\" class=\"ltx_td ltx_align_right\">38.8 +/- (1.92)</td>\n<td id=\"S3.T2.1.8.5\" class=\"ltx_td ltx_align_right\">46.0 +/- 1.08</td>\n<td id=\"S3.T2.1.8.6\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T2.1.9\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.9.1\" class=\"ltx_td ltx_align_right\"><span id=\"S3.T2.1.9.1.1\" class=\"ltx_text ltx_font_typewriter\">Diversty-synt</span></td>\n<td id=\"S3.T2.1.9.2\" class=\"ltx_td ltx_align_right\">52.5</td>\n<td id=\"S3.T2.1.9.3\" class=\"ltx_td ltx_align_right\"><span id=\"S3.T2.1.9.3.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">41.2</span></td>\n<td id=\"S3.T2.1.9.4\" class=\"ltx_td ltx_align_right\"><span id=\"S3.T2.1.9.4.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">46.2</span></td>\n<td id=\"S3.T2.1.9.5\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T2.1.10\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.10.1\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S3.T2.1.10.1.1\" class=\"ltx_text ltx_font_typewriter\">Extended-synt</span></td>\n<td id=\"S3.T2.1.10.2\" class=\"ltx_td ltx_align_right ltx_border_bb\">63.7</td>\n<td id=\"S3.T2.1.10.3\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S3.T2.1.10.3.1\" class=\"ltx_text ltx_font_bold\">46.5</span></td>\n<td id=\"S3.T2.1.10.4\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S3.T2.1.10.4.1\" class=\"ltx_text ltx_font_bold\">53.8</span></td>\n<td id=\"S3.T2.1.10.5\" class=\"ltx_td ltx_border_bb\"></td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "The synthetic datasets were used to train new instances of the previously evaluated models: Seq2rel, GPT-2 and BioGPT. Although the synthetic training sets (Diversity-synt and Random-synt) are almost half the size of Extended-raw (resp. 3562 and 3798 compared to 7111 examples), on which was established the previous baseline with BioGPT (f1-score=32.5f1-score32.5\\text{f1-score}=32.5), all the trained models demonstrated improved performance (see Table 2). The ranking of the models and the impact of the synthetic training sets on the final performance align with the previous observations on the original data. BioGPT models persistently outperformed Seq2rel and GPT-2, and the training on Diversity-synt resulted in an improved recall at the expense of precision compared to Random-synt. However, the GPT-2 models trained on Random-synt on average outperformed the one trained on Diversity-synt, a departure from the trend observed with Seq2rel and BioGPT. Again, the best performance is achieved by BioGPT trained on the merged set, with f1-score=53.8f1-score53.8\\text{f1-score}=53.8.\nFinally, two BioGPT-Large models were trained on the Diversity-synt and Extended-synt (see Table 3). The model trained on Diversity-synt achieved f1-score=57.2f1-score57.2\\text{f1-score}=57.2, comparable to the new best model trained on the much larger merged set (f1-score==59.0\\text{f1-score}==59.0) and also demonstrated a better recall (56.9056.9056.90 against 51.651.651.6)."
        ]
    },
    "S3.T3": {
        "caption": "Table 3: Performance of finetuned bioGPT-Large models on the synthetic diversity sample and the Extended synthetic sample.",
        "table": "<table id=\"S3.T3.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S3.T3.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T3.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\"><span id=\"S3.T3.1.1.1.1\" class=\"ltx_text ltx_font_bold\">model</span></td>\n<td id=\"S3.T3.1.1.2\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S3.T3.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td id=\"S3.T3.1.1.3\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S3.T3.1.1.3.1\" class=\"ltx_text ltx_font_bold\">precision</span></td>\n<td id=\"S3.T3.1.1.4\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S3.T3.1.1.4.1\" class=\"ltx_text ltx_font_bold\">recall</span></td>\n<td id=\"S3.T3.1.1.5\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S3.T3.1.1.5.1\" class=\"ltx_text ltx_font_bold\">f1</span></td>\n<td id=\"S3.T3.1.1.6\" class=\"ltx_td ltx_border_tt\"></td>\n</tr>\n<tr id=\"S3.T3.1.2\" class=\"ltx_tr\">\n<td id=\"S3.T3.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\">BioGPT-Large</td>\n<td id=\"S3.T3.1.2.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S3.T3.1.2.2.1\" class=\"ltx_text ltx_font_typewriter\">Diversty-synt</span></td>\n<td id=\"S3.T3.1.2.3\" class=\"ltx_td ltx_align_right ltx_border_t\">57.50</td>\n<td id=\"S3.T3.1.2.4\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S3.T3.1.2.4.1\" class=\"ltx_text ltx_font_bold\">56.90</span></td>\n<td id=\"S3.T3.1.2.5\" class=\"ltx_td ltx_align_right ltx_border_t\">57.20</td>\n<td id=\"S3.T3.1.2.6\" class=\"ltx_td ltx_border_t\"></td>\n</tr>\n<tr id=\"S3.T3.1.3\" class=\"ltx_tr\">\n<td id=\"S3.T3.1.3.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">BioGPT-Large</td>\n<td id=\"S3.T3.1.3.2\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S3.T3.1.3.2.1\" class=\"ltx_text ltx_font_typewriter\">Extended-synt</span></td>\n<td id=\"S3.T3.1.3.3\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S3.T3.1.3.3.1\" class=\"ltx_text ltx_font_bold\">69.0</span></td>\n<td id=\"S3.T3.1.3.4\" class=\"ltx_td ltx_align_right ltx_border_bb\">51.6</td>\n<td id=\"S3.T3.1.3.5\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S3.T3.1.3.5.1\" class=\"ltx_text ltx_font_bold\">59.0</span></td>\n<td id=\"S3.T3.1.3.6\" class=\"ltx_td ltx_border_bb\"></td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "The synthetic datasets were used to train new instances of the previously evaluated models: Seq2rel, GPT-2 and BioGPT. Although the synthetic training sets (Diversity-synt and Random-synt) are almost half the size of Extended-raw (resp. 3562 and 3798 compared to 7111 examples), on which was established the previous baseline with BioGPT (f1-score=32.5f1-score32.5\\text{f1-score}=32.5), all the trained models demonstrated improved performance (see Table 2). The ranking of the models and the impact of the synthetic training sets on the final performance align with the previous observations on the original data. BioGPT models persistently outperformed Seq2rel and GPT-2, and the training on Diversity-synt resulted in an improved recall at the expense of precision compared to Random-synt. However, the GPT-2 models trained on Random-synt on average outperformed the one trained on Diversity-synt, a departure from the trend observed with Seq2rel and BioGPT. Again, the best performance is achieved by BioGPT trained on the merged set, with f1-score=53.8f1-score53.8\\text{f1-score}=53.8.\nFinally, two BioGPT-Large models were trained on the Diversity-synt and Extended-synt (see Table 3). The model trained on Diversity-synt achieved f1-score=57.2f1-score57.2\\text{f1-score}=57.2, comparable to the new best model trained on the much larger merged set (f1-score==59.0\\text{f1-score}==59.0) and also demonstrated a better recall (56.9056.9056.90 against 51.651.651.6)."
        ]
    }
}