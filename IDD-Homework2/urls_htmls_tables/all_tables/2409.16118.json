{
    "id_table_1": {
        "caption": "Table 1 :  Classification accuracy  (%) aggregated over six downstream predictors, comparing data augmentation on eight real-world tabular datasets with varied real data availability. We report the mean   plus-or-minus \\pm   std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher accuracy. Note that N/A denotes that a specific generator was not applicable, and the rank is computed with the mean balanced accuracy of other methods. We  bold  the highest accuracy for each dataset of different sample size. Our method, TabEBM, consistently outperforms training on real data alone, and achieves the best overall performance against Baseline and benchmark generators.",
        "table": "A3.EGx1",
        "footnotes": [],
        "references": [
            "Empirical:  We present the first comprehensive analysis of tabular data augmentation across different dataset sizes and use cases beyond predictive performance. Our analysis compares TabEBM with eight leading tabular generative models across various datasets, demonstrating that TabEBM consistently improves data augmentation performance on small datasets, while our generated data demonstrates better statistical fidelity and privacy-preserving properties (Figure  1 ).",
            "We derive each class-specific EBM  E c  ( x ) subscript E c x E_{c}({\\mathbf{x}}) italic_E start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_x )  by training a classifier on a novel task and reinterpreting its logits. Specifically, for each class  c c c italic_c , we propose a  surrogate binary classification task  to determine if a sample belongs to class  c c c italic_c  by comparing  X c subscript X c \\mathcal{X}_{c} caligraphic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT  against a set of surrogate negative samples  X c neg superscript subscript X c neg \\mathcal{X}_{c}^{\\text{neg}} caligraphic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT , which we show in  Figure   3 . Specifically, we generate the negative samples at the corners of a hypercube in  R D superscript R D R^{D} italic_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT . For each dimension  d d d italic_d , the coordinates of a negative sample are either   dist neg   d subscript superscript  neg dist subscript  d \\alpha^{\\text{neg}}_{\\text{dist}}\\sigma_{d} italic_ start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT dist end_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT  or    dist neg   d subscript superscript  neg dist subscript  d -\\alpha^{\\text{neg}}_{\\text{dist}}\\sigma_{d} - italic_ start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT dist end_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , where   dist neg subscript superscript  neg dist \\alpha^{\\text{neg}}_{\\text{dist}} italic_ start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT dist end_POSTSUBSCRIPT  is a fixed constant and   d subscript  d \\sigma_{d} italic_ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT  is the standard deviation of dimension  d d d italic_d . For example, in  R 3 superscript R 3 R^{3} italic_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT , a negative sample might have coordinates  [  dist neg   1 ,  dist neg   2 ,   dist neg   3 ] subscript superscript  neg dist subscript  1 subscript superscript  neg dist subscript  2 subscript superscript  neg dist subscript  3 [\\alpha^{\\text{neg}}_{\\text{dist}}\\sigma_{1},\\alpha^{\\text{neg}}_{\\text{dist}}% \\sigma_{2},-\\alpha^{\\text{neg}}_{\\text{dist}}\\sigma_{3}] [ italic_ start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT dist end_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_ start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT dist end_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , - italic_ start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT dist end_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ] . Placing the negative samples at the corners of a hypercube ensures they are easily distinguishable from the real data, which is crucial for an accurate energy function (see  Section   C.1.1 ). This placement is also robust to variations in the number and distance of the negative samples (see  Sections   C.1.2  and  C.1.3 ).",
            "Generating data with TabEBM  involves two steps. First, we sample a class  c c c italic_c  from the empirical distribution  c  p  ( y ) similar-to c p y c\\sim p(y) italic_c  italic_p ( italic_y ) . Then, we sample a data point  x x {\\mathbf{x}} bold_x  from the conditional distribution  x  p  ( x | y = c ) similar-to x p conditional x y c {\\mathbf{x}}\\sim p({\\mathbf{x}}|y=c) bold_x  italic_p ( bold_x | italic_y = italic_c )  approximated by the class-specific energy-based model  E c  ( x ) subscript E c x E_{c}({\\mathbf{x}}) italic_E start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_x ) , as outlined in  Algorithm   1 . We employ Stochastic Gradient Langevin Dynamics (SGLD)  [ 85 ]  to perform this sampling. SGLD is an efficient method for high-dimensional data, combining stochastic gradient descent (SGLD) with Langevin dynamics. The update rule for SGLD at each iteration is:",
            "Data Augmentation Improvement (Q1,  Section   3.1 ):  Can TabEBM generate synthetic data that improves the accuracy of downstream predictors via data augmentation?",
            "Datasets.  We utilise 14 open-source tabular datasetseight from OpenML  [ 7 ]  and six from UCI  [ 22 ]   across five domains: Medicine, Chemistry, Engineering, Language and Economics. These diverse datasets contain 7 to 77 features and 698 to 5500 samples across 2 to 26 classes. Five datasets contain both numerical and categorical features, while the remaining are numerical only. To provide comprehensive and fine-grained conclusions on model performance, we further enlarge the evaluation scope by varying the degrees of data availability (i.e.,  N real subscript N real N_{\\text{real}} italic_N start_POSTSUBSCRIPT real end_POSTSUBSCRIPT ), leading up to 33 different test cases for the eight OpenML datasets.  Section   A.1  provide detailed dataset descriptions.",
            "TabEBM effectively improves downstream performance across sample sizes, especially for very low-sample-size regimes.   Table   1  and  Figure   4  (Left) show that TabEBM exhibits competitive performance in data augmentation, generally achieving the highest downstream accuracy and average rank across most datasets and sample sizes. Notably, TabEBM is the only generator that consistently improves performance across sample sizes. A key observation is that most modern benchmark generators even underperform the Baseline, indicating poor approximated distributions in the low-sample-size regime. Moreover, TabEBM achieves the largest overall performance improvement on six additional UCI datasets, further supporting its effectiveness (see  Section   C.5.2  for detailed results).",
            "In  Figure   7  (a1&a2), TabEBM consistently achieves the highest accuracy and distribution similarity between real train data and synthetic data, showing that TabEBM learns the distributions of real train data better than benchmark generators. In  Section   C.6 , we further show that TabEBM remains the most competitive method in similarity between real test data and synthetic data. This indicates that TabEBM can extrapolate beyond real train data and thus generate synthetic data from a more general distribution that aligns with both train and test data. This extrapolation ability also explains why TabEBM can outperform Baseline via data augmentation ( Section   3.1 ).",
            "TabEBM.  In all our experiments, the surrogate binary classifier in TabEBM is a pretrained in-context model, TabPFN  [ 33 ] , using the official model weights released by the authors ( https://github.com/automl/TabPFN/raw/main/tabpfn/models_diff/prior_diff_real_checkpoint_n_0_epoch_42.cpkt ). We use TabPFN with three ensembles. We use four surrogate negative samples,  X c neg superscript subscript X c neg \\mathcal{X}_{c}^{\\text{neg}} caligraphic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT , positioned at   dist neg = 5 subscript superscript  neg dist 5 \\alpha^{\\text{neg}}_{\\text{dist}}=5 italic_ start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT dist end_POSTSUBSCRIPT = 5  standard deviations from zero, in random corners of a hypercube in  R D superscript R D {\\mathbb{R}}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT  (as explained in  Section   2.2 ), distant from any real data. In  Section   C.1 , we show that TabEBM is robust to the distribution of the negative samples.",
            "We showcase three limitations of current generative models: (1)  Figure   10  shows that models approximating the joint distribution  p  ( x , y ) p x y p({\\mathbf{x}},y) italic_p ( bold_x , italic_y )  may fail to preserve the stratification of the real data and even fail to generate samples from specific classes. (2)  Figure   11(e)  evaluates the approximated class-conditional distributions  p  ( x  y ) p conditional x y p({\\mathbf{x}}\\mid y) italic_p ( bold_x  italic_y )  on data with increasing noise levels, and (3)  Figure   12(d)  evaluates the approximated class-conditional distributions  p  ( x  y ) p conditional x y p({\\mathbf{x}}\\mid y) italic_p ( bold_x  italic_y )  on data with increasing class imbalance.",
            "Figure   13  shows TabEBMs energy  E c  ( x ) subscript E c x E_{c}(x) italic_E start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_x )  when varying the selection of the negative samples. TabEBM infers an accurate energy surface with distant negative samples, and the energy surface becomes inaccurate when negative samples resemble real samples. This occurs because TabPFN is uncertain when points of different classes are close, affecting its logits magnitude and making them unsuitable for density estimation.",
            "Table   4  shows the results across six datasets with  N real = 100 subscript N real 100 N_{\\text{real}}=100 italic_N start_POSTSUBSCRIPT real end_POSTSUBSCRIPT = 100  real samples, demonstrating that TabEBM is robust to imbalances in the surrogate binary tasks. The column with  | X c neg | = 4 subscript superscript X neg c 4 |\\mathcal{X}^{\\text{neg}}_{c}|=4 | caligraphic_X start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT | = 4  represents the TabEBM results from the main paper, where four negative samples were placed in the corners (as described in  Section   2 ). There are negligible differences in performance, and TabEBM consistently outperforms both the baseline and other generators (as shown in  Table   1 ).",
            "We vary two key hyperparameters of SGLD on the biodeg binary dataset with  N real = 100 subscript N real 100 N_{\\text{real}}=100 italic_N start_POSTSUBSCRIPT real end_POSTSUBSCRIPT = 100 : the step size   step subscript  step \\alpha_{\\text{step}} italic_ start_POSTSUBSCRIPT step end_POSTSUBSCRIPT  and the noise scale   noise subscript  noise \\alpha_{\\text{noise}} italic_ start_POSTSUBSCRIPT noise end_POSTSUBSCRIPT .  Table   6  shows that TabEBM remains stable with respect to these hyperparameters. Note that smaller values of   noise subscript  noise \\alpha_{\\text{noise}} italic_ start_POSTSUBSCRIPT noise end_POSTSUBSCRIPT  are expected to perform better because SGLD sampling adds noise at each iteration (see Line 7 in  Algorithm   1 ), thus larger values of   noise subscript  noise \\alpha_{\\text{noise}} italic_ start_POSTSUBSCRIPT noise end_POSTSUBSCRIPT  will hinder convergence of the SGLD sampler."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Comparison of the properties between TabEBM and prior tabular generative methods.  TabEBM has novel design rationales of training-free class-specific models, and TabEBM is highly practicable with wide applicability and consistent accuracy improvement.",
        "table": "A3.EGx2",
        "footnotes": [],
        "references": [
            "To address the challenges of class-conditional tabular generation, we introduce TabEBM (Figure  2 ), a new method for tabular data augmentation utilising Energy-Based Models (EBMs). Our method introduces two innovations: (i)  Distinct class-specific models:  TabEBM constructs a collection of individual models  one for each class  which, by design, enables learning distinct marginal distributions for the inputs associated with each class. This, in turn, enables performing data augmentation while maintaining the original label distribution. (ii)  Generative models:  we build novel class-specific generators that produce high-quality synthetic data even from extremely few samples. Specifically, we create a surrogate binary classification task for each class and fit it with a pre-trained tabular in-context classifier. We then convert the binary classifier into an EBM, a generative model, without additional training. Using class-specific EBMs makes the energy landscape more robust to class overlaps, compared to using a single shared EBM to approximate the class-conditional distribution.",
            "We derive each class-specific EBM  E c  ( x ) subscript E c x E_{c}({\\mathbf{x}}) italic_E start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_x )  by training a classifier on a novel task and reinterpreting its logits. Specifically, for each class  c c c italic_c , we propose a  surrogate binary classification task  to determine if a sample belongs to class  c c c italic_c  by comparing  X c subscript X c \\mathcal{X}_{c} caligraphic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT  against a set of surrogate negative samples  X c neg superscript subscript X c neg \\mathcal{X}_{c}^{\\text{neg}} caligraphic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT , which we show in  Figure   3 . Specifically, we generate the negative samples at the corners of a hypercube in  R D superscript R D R^{D} italic_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT . For each dimension  d d d italic_d , the coordinates of a negative sample are either   dist neg   d subscript superscript  neg dist subscript  d \\alpha^{\\text{neg}}_{\\text{dist}}\\sigma_{d} italic_ start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT dist end_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT  or    dist neg   d subscript superscript  neg dist subscript  d -\\alpha^{\\text{neg}}_{\\text{dist}}\\sigma_{d} - italic_ start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT dist end_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , where   dist neg subscript superscript  neg dist \\alpha^{\\text{neg}}_{\\text{dist}} italic_ start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT dist end_POSTSUBSCRIPT  is a fixed constant and   d subscript  d \\sigma_{d} italic_ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT  is the standard deviation of dimension  d d d italic_d . For example, in  R 3 superscript R 3 R^{3} italic_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT , a negative sample might have coordinates  [  dist neg   1 ,  dist neg   2 ,   dist neg   3 ] subscript superscript  neg dist subscript  1 subscript superscript  neg dist subscript  2 subscript superscript  neg dist subscript  3 [\\alpha^{\\text{neg}}_{\\text{dist}}\\sigma_{1},\\alpha^{\\text{neg}}_{\\text{dist}}% \\sigma_{2},-\\alpha^{\\text{neg}}_{\\text{dist}}\\sigma_{3}] [ italic_ start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT dist end_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_ start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT dist end_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , - italic_ start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT dist end_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ] . Placing the negative samples at the corners of a hypercube ensures they are easily distinguishable from the real data, which is crucial for an accurate energy function (see  Section   C.1.1 ). This placement is also robust to variations in the number and distance of the negative samples (see  Sections   C.1.2  and  C.1.3 ).",
            "where a Gaussian noise term   t subscript italic- t \\epsilon_{t} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  introduces randomness into the sampling process, enhancing the exploration of the distribution. In practice, the step size and the noise standard deviations are often chosen separately, resulting in a biased sampler that allows for faster training.  Section   C.2  further shows that TabEBM is stable to hyperparameters for the sampling process.",
            "Statistical Fidelity (Q2,  Section   3.2 ):  Can TabEBM generate synthetic data with high statistical fidelity (i.e., with similar distributions to those of real data)?",
            "General experimental setup.  For each dataset of  N N N italic_N  samples, we first split it into stratified train and test sets. We create large test sets to reduce the likelihood that the models performance is accidentally inflated due to a small, unrepresentative set of samples  [ 70 ] , and thus the test size is computed via  N test = min  ( N 2 , 500 ) subscript N test N 2 500 N_{\\text{test}}=\\min\\left(\\frac{N}{2},500\\right) italic_N start_POSTSUBSCRIPT test end_POSTSUBSCRIPT = roman_min ( divide start_ARG italic_N end_ARG start_ARG 2 end_ARG , 500 ) . The full train set approximates the upper bound of the quality of synthetic data, and we call this set oracle. We subsample the full train set to simulate different levels of data availability, thus the subset size  N real subscript N real N_{\\text{real}} italic_N start_POSTSUBSCRIPT real end_POSTSUBSCRIPT  varies over  { 20 , 50 , 100 , 200 , 500 } 20 50 100 200 500 \\{20,50,100,200,500\\} { 20 , 50 , 100 , 200 , 500 } . We split each subset into stratified training and validation sets with a ratio of 4:1. We provide detailed descriptions of data splitting in  Section   A.2  and preprocessing in  Section   A.3 . We repeat the splitting ten times, summing up to 10 runs per subset size. The reported results are averaged by default over ten runs on the test sets. When aggregating results across datasets, we use the average distance to the minimum (ADTM) metric via affine renormalisation between the top-performing and worse-performing models  [ 30 ,  54 ] . We provide the evaluation results averaged over six downstream predictors for a general conclusion, and the fine-grained numerical results for each predictor are in  Appendix   C .",
            "Data augmentation setup.  Given  N real subscript N real N_{\\text{real}} italic_N start_POSTSUBSCRIPT real end_POSTSUBSCRIPT  real samples, we first train generators on the real training data and then generate  N syn subscript N syn N_{\\text{syn}} italic_N start_POSTSUBSCRIPT syn end_POSTSUBSCRIPT  synthetic samples. For training the downstream predictors, we expand the real training split by adding the synthetic samples. The real validation data is used for early stopping, and the real test set is used for evaluating the predictors performance. The optimal  N syn subscript N syn N_{\\text{syn}} italic_N start_POSTSUBSCRIPT syn end_POSTSUBSCRIPT  remains an open problem for tabular data  [ 51 ,  72 ,  31 ] . Prior work  [ 47 ,  48 ]  mainly use synthetic sets with equivalent sizes to the real sets (i.e.,  N real = N syn subscript N real subscript N syn N_{\\text{real}}=N_{\\text{syn}} italic_N start_POSTSUBSCRIPT real end_POSTSUBSCRIPT = italic_N start_POSTSUBSCRIPT syn end_POSTSUBSCRIPT ). However, we observe that  N real = N syn subscript N real subscript N syn N_{\\text{real}}=N_{\\text{syn}} italic_N start_POSTSUBSCRIPT real end_POSTSUBSCRIPT = italic_N start_POSTSUBSCRIPT syn end_POSTSUBSCRIPT  can lead to highly unstable results, especially on small datasets we investigate. Recent work has used different  N syn subscript N syn N_{\\text{syn}} italic_N start_POSTSUBSCRIPT syn end_POSTSUBSCRIPT  for various generators, such as by applying post-processing  [ 31 ,  72 ] . In this work, we want to provide a head-to-head comparison of the effect of data augmentation across subsampled datasets of varying size  N real  { 20 , 50 , 100 , 200 , 500 } subscript N real 20 50 100 200 500 N_{\\text{real}}\\in\\{20,50,100,200,500\\} italic_N start_POSTSUBSCRIPT real end_POSTSUBSCRIPT  { 20 , 50 , 100 , 200 , 500 } . Therefore, we perform data augmentation with a large synthetic set ( N syn = 500 subscript N syn 500 N_{\\text{syn}}=500 italic_N start_POSTSUBSCRIPT syn end_POSTSUBSCRIPT = 500 ) across all splits, and the synthetic data has the same class distribution as the real training data. We provide an illustrative figure of the data splitting setup in  Section   A.2 .",
            "TabEBM effectively improves downstream performance across sample sizes, especially for very low-sample-size regimes.   Table   1  and  Figure   4  (Left) show that TabEBM exhibits competitive performance in data augmentation, generally achieving the highest downstream accuracy and average rank across most datasets and sample sizes. Notably, TabEBM is the only generator that consistently improves performance across sample sizes. A key observation is that most modern benchmark generators even underperform the Baseline, indicating poor approximated distributions in the low-sample-size regime. Moreover, TabEBM achieves the largest overall performance improvement on six additional UCI datasets, further supporting its effectiveness (see  Section   C.5.2  for detailed results).",
            "Figure   7  (b1&b2) shows that TabEBM consistently finds a better trade-off between accuracy and privacy preservation. Notably, the train-on-synthetic, test-on-real scenario poses a greater challenge for generators in achieving high accuracy because real data is inaccessible for model training and data augmentation. Despite this difficulty, TabEBM is the only generator that surpasses the overall performance of training on real data (i.e., Baseline). The relatively high DCR for TabEBM indicates that it can extrapolate beyond real train data, aligning with the finding that TabEBMs synthetic data is statistically similar to real test data ( Section   3.2 ). These results further suggest that TabEBM learns the general distribution of real data, and TabEBM can generate high-quality synthetic data suitable for various purposes, including privacy preservation.",
            "Section   3  showed that TabEBM efficiently generates high-fidelity data that can effectively improve the downstream performance via data augmentation. In  Table   2 , we further provide a summary of tabular data generative models analysed from three important perspectives: (i)  Training:  the type of distribution that the generators learn (crucial for preserving the original training label distribution), and the training costs associated with learning; (ii)  Generation:  do the generators employ class-specific models (reflecting their capability to capture unique features essential for label-invariant generation), and do models support stratified generation (crucial for effective data augmentation); (iii)  Practicability:  the scalability of the generators with respect to the number of classes (a common requirement in real-world multi-class tasks), and consistent downstream performance improvement across different class sizes.",
            "Limitations and Future Work.  TabEBM is a general method that relies on an underlying binary classifier, and as such, its strengths and weaknesses are directly tied to this classifier. We used TabPFN because it is a well-established open-source pre-trained model for tabular data. Therefore, TabEBM inherits some of TabPFNs limitations, particularly in scaling to a larger number of features. TabEBM can handle datasets with over 1000 samples, overcoming TabPFNs limitation, as it processes one class at a time. In  Section   C.5.3 , we show that TabEBM outperforms other generators on larger datasets, though the performance gains decrease as the sample size increases. Although TabPFN has a 10-class limit, TabEBM can handle an unlimited number of classes by training TabPFN on  binary  surrogate tasks instead of the original multi-class problem. Our results also show that TabEBM can handle categorical data by encoding categorical features with leave-one-out target statistics. We stress that TabEBM is compatible with any classifier that can be adapted into EBMs, as described in  Section   2 . As foundational models for tabular data evolve  [ 82 ] , new models capable of handling more features and samples are expected. Integrating them into TabEBM will enhance its ability to manage high-dimensional datasets, increasing its versatility and utility.",
            "TabEBM.  In all our experiments, the surrogate binary classifier in TabEBM is a pretrained in-context model, TabPFN  [ 33 ] , using the official model weights released by the authors ( https://github.com/automl/TabPFN/raw/main/tabpfn/models_diff/prior_diff_real_checkpoint_n_0_epoch_42.cpkt ). We use TabPFN with three ensembles. We use four surrogate negative samples,  X c neg superscript subscript X c neg \\mathcal{X}_{c}^{\\text{neg}} caligraphic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT , positioned at   dist neg = 5 subscript superscript  neg dist 5 \\alpha^{\\text{neg}}_{\\text{dist}}=5 italic_ start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT dist end_POSTSUBSCRIPT = 5  standard deviations from zero, in random corners of a hypercube in  R D superscript R D {\\mathbb{R}}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT  (as explained in  Section   2.2 ), distant from any real data. In  Section   C.1 , we show that TabEBM is robust to the distribution of the negative samples.",
            "We use SGLD  [ 85 ]  for sampling from TabEBM, where the starting points  x 0 synth superscript subscript x 0 synth {\\mathbf{x}}_{0}^{\\text{synth}} bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT synth end_POSTSUPERSCRIPT  are initialised by adding Gaussian noise with zero mean and standard deviation   start = 0.01 subscript  start 0.01 \\sigma_{\\text{start}}=0.01 italic_ start_POSTSUBSCRIPT start end_POSTSUBSCRIPT = 0.01  to a randomly selected sample of the specific class, i.e.,  x 0 synth  N  ( X c ,  start 2  I ) similar-to superscript subscript x 0 synth N subscript X c superscript subscript  start 2 I {\\mathbf{x}}_{0}^{\\text{synth}}\\sim\\mathcal{N}(\\mathcal{X}_{c},\\sigma_{\\text{% start}}^{2}\\mathbf{I}) bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT synth end_POSTSUPERSCRIPT  caligraphic_N ( caligraphic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT start end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_I ) . For SGLD, we used the following parameters: step size   step = 0.1 subscript  step 0.1 \\alpha_{\\text{step}}=0.1 italic_ start_POSTSUBSCRIPT step end_POSTSUBSCRIPT = 0.1 , noise scale   noise = 0.01 subscript  noise 0.01 \\alpha_{\\text{noise}}=0.01 italic_ start_POSTSUBSCRIPT noise end_POSTSUBSCRIPT = 0.01  and number of steps  T = 200 T 200 T=200 italic_T = 200 . We found TabEBM to be robust to the SGLD settings (see  Section   C.2 ). We attach the implementation code for TabEBM and will release it as a public open-source library available after publication (see  Section   A.5  for details on our library).",
            "We showcase three limitations of current generative models: (1)  Figure   10  shows that models approximating the joint distribution  p  ( x , y ) p x y p({\\mathbf{x}},y) italic_p ( bold_x , italic_y )  may fail to preserve the stratification of the real data and even fail to generate samples from specific classes. (2)  Figure   11(e)  evaluates the approximated class-conditional distributions  p  ( x  y ) p conditional x y p({\\mathbf{x}}\\mid y) italic_p ( bold_x  italic_y )  on data with increasing noise levels, and (3)  Figure   12(d)  evaluates the approximated class-conditional distributions  p  ( x  y ) p conditional x y p({\\mathbf{x}}\\mid y) italic_p ( bold_x  italic_y )  on data with increasing class imbalance.",
            "We evaluate the impact of the ratio  | X c neg | : | X c | : subscript superscript X neg c subscript X c |\\mathcal{X}^{\\text{neg}}_{c}|:|\\mathcal{X}_{c}| | caligraphic_X start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT | : | caligraphic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT |  between the negative samples  X c neg subscript superscript X neg c \\mathcal{X}^{\\text{neg}}_{c} caligraphic_X start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT  and the real samples  | X c | subscript X c |\\mathcal{X}_{c}| | caligraphic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT | . We vary  | X c neg | subscript superscript X neg c |\\mathcal{X}^{\\text{neg}}_{c}| | caligraphic_X start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT |  while keeping  | X c | subscript X c |\\mathcal{X}_{c}| | caligraphic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT |  fixed, simulating both balanced and highly imbalanced scenarios. The negative samples are placed in random corners of the hypercube (as described in  Section   2 ), at five standard deviations in each direction (i.e.,   dist neg = 5 subscript superscript  neg dist 5 \\alpha^{\\text{neg}}_{\\text{dist}}=5 italic_ start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT dist end_POSTSUBSCRIPT = 5 ). To ensure reliable outcomes, we maintained a consistent ratio across all classes, keeping the same proportion of negative samples for each class.",
            "Table   4  shows the results across six datasets with  N real = 100 subscript N real 100 N_{\\text{real}}=100 italic_N start_POSTSUBSCRIPT real end_POSTSUBSCRIPT = 100  real samples, demonstrating that TabEBM is robust to imbalances in the surrogate binary tasks. The column with  | X c neg | = 4 subscript superscript X neg c 4 |\\mathcal{X}^{\\text{neg}}_{c}|=4 | caligraphic_X start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT | = 4  represents the TabEBM results from the main paper, where four negative samples were placed in the corners (as described in  Section   2 ). There are negligible differences in performance, and TabEBM consistently outperforms both the baseline and other generators (as shown in  Table   1 ).",
            "We assess the effect of varying the distance of negative samples. We use TabEBM with four negative samples positioned randomly at the corners of the hypercube, as outlined in  Section   2  (this corresponds to the experimental setup from the main paper). The distance of the negative samples, denoted as   dist neg subscript superscript  neg dist \\alpha^{\\text{neg}}_{\\text{dist}} italic_ start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT dist end_POSTSUBSCRIPT , is varied.  Table   5  demonstrates that TabEBM remains generally robust to changes in this distance, with only small performance variations across different datasets. Importantly, using TabEBM for data augmentation consistently improves performance by approximately 3% compared to the Baseline, regardless of the distance used."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Details of the eight real-world tabular datasets.",
        "table": "A3.EGx3",
        "footnotes": [],
        "references": [
            "We derive each class-specific EBM  E c  ( x ) subscript E c x E_{c}({\\mathbf{x}}) italic_E start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_x )  by training a classifier on a novel task and reinterpreting its logits. Specifically, for each class  c c c italic_c , we propose a  surrogate binary classification task  to determine if a sample belongs to class  c c c italic_c  by comparing  X c subscript X c \\mathcal{X}_{c} caligraphic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT  against a set of surrogate negative samples  X c neg superscript subscript X c neg \\mathcal{X}_{c}^{\\text{neg}} caligraphic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT , which we show in  Figure   3 . Specifically, we generate the negative samples at the corners of a hypercube in  R D superscript R D R^{D} italic_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT . For each dimension  d d d italic_d , the coordinates of a negative sample are either   dist neg   d subscript superscript  neg dist subscript  d \\alpha^{\\text{neg}}_{\\text{dist}}\\sigma_{d} italic_ start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT dist end_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT  or    dist neg   d subscript superscript  neg dist subscript  d -\\alpha^{\\text{neg}}_{\\text{dist}}\\sigma_{d} - italic_ start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT dist end_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , where   dist neg subscript superscript  neg dist \\alpha^{\\text{neg}}_{\\text{dist}} italic_ start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT dist end_POSTSUBSCRIPT  is a fixed constant and   d subscript  d \\sigma_{d} italic_ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT  is the standard deviation of dimension  d d d italic_d . For example, in  R 3 superscript R 3 R^{3} italic_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT , a negative sample might have coordinates  [  dist neg   1 ,  dist neg   2 ,   dist neg   3 ] subscript superscript  neg dist subscript  1 subscript superscript  neg dist subscript  2 subscript superscript  neg dist subscript  3 [\\alpha^{\\text{neg}}_{\\text{dist}}\\sigma_{1},\\alpha^{\\text{neg}}_{\\text{dist}}% \\sigma_{2},-\\alpha^{\\text{neg}}_{\\text{dist}}\\sigma_{3}] [ italic_ start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT dist end_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_ start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT dist end_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , - italic_ start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT dist end_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ] . Placing the negative samples at the corners of a hypercube ensures they are easily distinguishable from the real data, which is crucial for an accurate energy function (see  Section   C.1.1 ). This placement is also robust to variations in the number and distance of the negative samples (see  Sections   C.1.2  and  C.1.3 ).",
            "For the binary classifier  f  c  (  ) subscript superscript f c   f^{c}_{\\theta}(\\cdot) italic_f start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT (  )  in the surrogate binary classification, we use TabPFN  [ 33 ] , a pre-trained tabular in-context model. Note that TabPFN is intended for inference only, with no updates to its parameters (see  Section   4  for more details about TabPFN). In this context, training the TabPFN classifier is analogous to the  K Nearest Neighbour  algorithm, which simply performs inference based on a training dataset provided to the model. We apply TabPFN multiple times on separate datasets  { D 1 , D 2 , ... , D C } subscript D 1 subscript D 2 ... subscript D C \\{\\mathcal{D}_{1},\\mathcal{D}_{2},\\ldots,\\mathcal{D}_{C}\\} { caligraphic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , caligraphic_D start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT }  to obtain multiple classifiers  { f  1 , f  2 , ... , f  C } superscript subscript f  1 superscript subscript f  2 ... superscript subscript f  C \\{f_{\\theta}^{1},f_{\\theta}^{2},\\ldots,f_{\\theta}^{C}\\} { italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , ... , italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT } . In  Section   3.4 , we explore why reinterpreting TabPFNs logits, trained on our surrogate binary tasks, can be useful for estimating an energy function. We emphasise that TabEBM is a general method, capable of using any gradient-based classifier that computes logits (using  Equation   3 ), and is not limited to TabPFN.",
            "Data Augmentation Improvement (Q1,  Section   3.1 ):  Can TabEBM generate synthetic data that improves the accuracy of downstream predictors via data augmentation?",
            "Statistical Fidelity (Q2,  Section   3.2 ):  Can TabEBM generate synthetic data with high statistical fidelity (i.e., with similar distributions to those of real data)?",
            "Privacy Preservation (Q3,  Section   3.3 ):  Can TabEBM generate synthetic data that finds a competitive trade-off between downstream performance and privacy preservation?",
            "Understanding TabEBMs energy formulation (Q4,  Section   3.4 ):  Why is TabEBMs class-specific energy effective, and how do the proposed surrogate tasks influence this?",
            "General experimental setup.  For each dataset of  N N N italic_N  samples, we first split it into stratified train and test sets. We create large test sets to reduce the likelihood that the models performance is accidentally inflated due to a small, unrepresentative set of samples  [ 70 ] , and thus the test size is computed via  N test = min  ( N 2 , 500 ) subscript N test N 2 500 N_{\\text{test}}=\\min\\left(\\frac{N}{2},500\\right) italic_N start_POSTSUBSCRIPT test end_POSTSUBSCRIPT = roman_min ( divide start_ARG italic_N end_ARG start_ARG 2 end_ARG , 500 ) . The full train set approximates the upper bound of the quality of synthetic data, and we call this set oracle. We subsample the full train set to simulate different levels of data availability, thus the subset size  N real subscript N real N_{\\text{real}} italic_N start_POSTSUBSCRIPT real end_POSTSUBSCRIPT  varies over  { 20 , 50 , 100 , 200 , 500 } 20 50 100 200 500 \\{20,50,100,200,500\\} { 20 , 50 , 100 , 200 , 500 } . We split each subset into stratified training and validation sets with a ratio of 4:1. We provide detailed descriptions of data splitting in  Section   A.2  and preprocessing in  Section   A.3 . We repeat the splitting ten times, summing up to 10 runs per subset size. The reported results are averaged by default over ten runs on the test sets. When aggregating results across datasets, we use the average distance to the minimum (ADTM) metric via affine renormalisation between the top-performing and worse-performing models  [ 30 ,  54 ] . We provide the evaluation results averaged over six downstream predictors for a general conclusion, and the fine-grained numerical results for each predictor are in  Appendix   C .",
            "In  Figure   7  (a1&a2), TabEBM consistently achieves the highest accuracy and distribution similarity between real train data and synthetic data, showing that TabEBM learns the distributions of real train data better than benchmark generators. In  Section   C.6 , we further show that TabEBM remains the most competitive method in similarity between real test data and synthetic data. This indicates that TabEBM can extrapolate beyond real train data and thus generate synthetic data from a more general distribution that aligns with both train and test data. This extrapolation ability also explains why TabEBM can outperform Baseline via data augmentation ( Section   3.1 ).",
            "Figure   7  (b1&b2) shows that TabEBM consistently finds a better trade-off between accuracy and privacy preservation. Notably, the train-on-synthetic, test-on-real scenario poses a greater challenge for generators in achieving high accuracy because real data is inaccessible for model training and data augmentation. Despite this difficulty, TabEBM is the only generator that surpasses the overall performance of training on real data (i.e., Baseline). The relatively high DCR for TabEBM indicates that it can extrapolate beyond real train data, aligning with the finding that TabEBMs synthetic data is statistically similar to real test data ( Section   3.2 ). These results further suggest that TabEBM learns the general distribution of real data, and TabEBM can generate high-quality synthetic data suitable for various purposes, including privacy preservation.",
            "We found it essential to place the negative samples far from the real data, since TabPFN, which pre-trained to approximate Bayesian inference  [ 33 ] , has its confidence influenced by the distance from the training data  [ 53 ] .  Figure   8  (left) shows that TabPFN outputs high logit values near the real data. As the distance from the real data increases, the logit  f  ( x )  [ 1 ] f x delimited-[] 1 f({\\mathbf{x}})[1] italic_f ( bold_x ) [ 1 ]  decreases smoothly until the two logits become similar, making the classifier uncertain (because the class probabilities become equal). The right figure shows that TabEBMs inferred density drops significantly as the maximum logit decreases, because  p c  ( x )  ( exp  ( f  ( x )  [ 0 ] ) + exp  ( f  ( x )  [ 1 ] ) ) proportional-to subscript p c x f x delimited-[] 0 f x delimited-[] 1 p_{c}({\\mathbf{x}})\\propto(\\exp(f({\\mathbf{x}})[0])+\\exp(f({\\mathbf{x}})[1])) italic_p start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_x )  ( roman_exp ( italic_f ( bold_x ) [ 0 ] ) + roman_exp ( italic_f ( bold_x ) [ 1 ] ) )  from  Equation   3 . Since SGLD sampling performs gradient ascent on the density, the TabEBM-generated samples will be close to the real data. These findings are consistent across datasets (see  Section   C.3 ), where TabPFNs logits remain positive, with similar ranges and a relatively constant sum as distance increases, warranting further investigation. Overall, TabPFNs distance-based uncertainty is useful for inferring accurate energy functions within our TabEBM framework. Since TabEBM can be paired with any other gradient-based classifier that produces logits, we leave these extensions for future work.",
            "Section   3  showed that TabEBM efficiently generates high-fidelity data that can effectively improve the downstream performance via data augmentation. In  Table   2 , we further provide a summary of tabular data generative models analysed from three important perspectives: (i)  Training:  the type of distribution that the generators learn (crucial for preserving the original training label distribution), and the training costs associated with learning; (ii)  Generation:  do the generators employ class-specific models (reflecting their capability to capture unique features essential for label-invariant generation), and do models support stratified generation (crucial for effective data augmentation); (iii)  Practicability:  the scalability of the generators with respect to the number of classes (a common requirement in real-world multi-class tasks), and consistent downstream performance improvement across different class sizes.",
            "Limitations and Future Work.  TabEBM is a general method that relies on an underlying binary classifier, and as such, its strengths and weaknesses are directly tied to this classifier. We used TabPFN because it is a well-established open-source pre-trained model for tabular data. Therefore, TabEBM inherits some of TabPFNs limitations, particularly in scaling to a larger number of features. TabEBM can handle datasets with over 1000 samples, overcoming TabPFNs limitation, as it processes one class at a time. In  Section   C.5.3 , we show that TabEBM outperforms other generators on larger datasets, though the performance gains decrease as the sample size increases. Although TabPFN has a 10-class limit, TabEBM can handle an unlimited number of classes by training TabPFN on  binary  surrogate tasks instead of the original multi-class problem. Our results also show that TabEBM can handle categorical data by encoding categorical features with leave-one-out target statistics. We stress that TabEBM is compatible with any classifier that can be adapted into EBMs, as described in  Section   2 . As foundational models for tabular data evolve  [ 82 ] , new models capable of handling more features and samples are expected. Integrating them into TabEBM will enhance its ability to manage high-dimensional datasets, increasing its versatility and utility.",
            "All eight datasets are publicly available on OpenML  [ 7 ] , and their details are listed in Table  3 . To ensure consistent stratified data-splitting across all datasets, we remove classes with fewer than 10 samples. For example, the original energy dataset contains 14 classes with fewer than 10 samples, which could result in a validation set lacking samples from these classes, leading to unstratified data splitting.",
            "Figure   9  shows the data splitting setup used across all datasets. Note that data sharing ( Section   3.3 ) shares the same data splitting as data augmentation, except that the Training set and Validation set containing real data are no longer used for training the downstream predictors.",
            "Figure   13  shows TabEBMs energy  E c  ( x ) subscript E c x E_{c}(x) italic_E start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_x )  when varying the selection of the negative samples. TabEBM infers an accurate energy surface with distant negative samples, and the energy surface becomes inaccurate when negative samples resemble real samples. This occurs because TabPFN is uncertain when points of different classes are close, affecting its logits magnitude and making them unsuitable for density estimation."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  Evaluating the impact of varying the ratio  | X c neg | : | X c | : subscript superscript X neg c subscript X c |\\mathcal{X}^{\\text{neg}}_{c}|:|\\mathcal{X}_{c}| | caligraphic_X start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT | : | caligraphic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT | . We show the test classification accuracy performance (%) of TabEBM on data augmentation averaged over six datasets and ten repeats. TabEBM shows consistent performance and outperforms the baseline, regardless of the number of negative samples.",
        "table": "A3.EGx4",
        "footnotes": [],
        "references": [
            "For the binary classifier  f  c  (  ) subscript superscript f c   f^{c}_{\\theta}(\\cdot) italic_f start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT (  )  in the surrogate binary classification, we use TabPFN  [ 33 ] , a pre-trained tabular in-context model. Note that TabPFN is intended for inference only, with no updates to its parameters (see  Section   4  for more details about TabPFN). In this context, training the TabPFN classifier is analogous to the  K Nearest Neighbour  algorithm, which simply performs inference based on a training dataset provided to the model. We apply TabPFN multiple times on separate datasets  { D 1 , D 2 , ... , D C } subscript D 1 subscript D 2 ... subscript D C \\{\\mathcal{D}_{1},\\mathcal{D}_{2},\\ldots,\\mathcal{D}_{C}\\} { caligraphic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , caligraphic_D start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT }  to obtain multiple classifiers  { f  1 , f  2 , ... , f  C } superscript subscript f  1 superscript subscript f  2 ... superscript subscript f  C \\{f_{\\theta}^{1},f_{\\theta}^{2},\\ldots,f_{\\theta}^{C}\\} { italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , ... , italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT } . In  Section   3.4 , we explore why reinterpreting TabPFNs logits, trained on our surrogate binary tasks, can be useful for estimating an energy function. We emphasise that TabEBM is a general method, capable of using any gradient-based classifier that computes logits (using  Equation   3 ), and is not limited to TabPFN.",
            "Understanding TabEBMs energy formulation (Q4,  Section   3.4 ):  Why is TabEBMs class-specific energy effective, and how do the proposed surrogate tasks influence this?",
            "TabEBM effectively improves downstream performance across sample sizes, especially for very low-sample-size regimes.   Table   1  and  Figure   4  (Left) show that TabEBM exhibits competitive performance in data augmentation, generally achieving the highest downstream accuracy and average rank across most datasets and sample sizes. Notably, TabEBM is the only generator that consistently improves performance across sample sizes. A key observation is that most modern benchmark generators even underperform the Baseline, indicating poor approximated distributions in the low-sample-size regime. Moreover, TabEBM achieves the largest overall performance improvement on six additional UCI datasets, further supporting its effectiveness (see  Section   C.5.2  for detailed results).",
            "TabEBM effectively improves downstream performance across the number of classes, especially for more than ten classes.   Figure   4  (Right) shows that TabEBM consistently outperforms the Baseline with notable improvements, particularly in datasets with more than ten classes. In contrast, an increased number of classes tends to cause a performance degradation in the benchmark generators.",
            "Table   4  shows the results across six datasets with  N real = 100 subscript N real 100 N_{\\text{real}}=100 italic_N start_POSTSUBSCRIPT real end_POSTSUBSCRIPT = 100  real samples, demonstrating that TabEBM is robust to imbalances in the surrogate binary tasks. The column with  | X c neg | = 4 subscript superscript X neg c 4 |\\mathcal{X}^{\\text{neg}}_{c}|=4 | caligraphic_X start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT | = 4  represents the TabEBM results from the main paper, where four negative samples were placed in the corners (as described in  Section   2 ). There are negligible differences in performance, and TabEBM consistently outperforms both the baseline and other generators (as shown in  Table   1 )."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 :  Evaluating the impact of varying the distance of the negative samples   dist neg subscript superscript  neg dist \\alpha^{\\text{neg}}_{\\text{dist}} italic_ start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT dist end_POSTSUBSCRIPT  across various datasets. We show the test classification accuracy performance (%) of TabEBM on data augmentation averaged over six datasets and ten repeats. TabEBM is robust, and optional tuning of the negative samples could slightly improve performance.",
        "table": "S3.T1.328.326",
        "footnotes": [],
        "references": [
            "Library:  We release TabEBM as an open-source library (included as a zip file with this submission and to be made publicly available after publication, details in  Section   A.5 ). Our library enables off-the-shelf data generation and data augmentation on any tabular dataset without requiring training.",
            "TabEBM effectively improves downstream performance across sample sizes, especially for very low-sample-size regimes.   Table   1  and  Figure   4  (Left) show that TabEBM exhibits competitive performance in data augmentation, generally achieving the highest downstream accuracy and average rank across most datasets and sample sizes. Notably, TabEBM is the only generator that consistently improves performance across sample sizes. A key observation is that most modern benchmark generators even underperform the Baseline, indicating poor approximated distributions in the low-sample-size regime. Moreover, TabEBM achieves the largest overall performance improvement on six additional UCI datasets, further supporting its effectiveness (see  Section   C.5.2  for detailed results).",
            "Limitations and Future Work.  TabEBM is a general method that relies on an underlying binary classifier, and as such, its strengths and weaknesses are directly tied to this classifier. We used TabPFN because it is a well-established open-source pre-trained model for tabular data. Therefore, TabEBM inherits some of TabPFNs limitations, particularly in scaling to a larger number of features. TabEBM can handle datasets with over 1000 samples, overcoming TabPFNs limitation, as it processes one class at a time. In  Section   C.5.3 , we show that TabEBM outperforms other generators on larger datasets, though the performance gains decrease as the sample size increases. Although TabPFN has a 10-class limit, TabEBM can handle an unlimited number of classes by training TabPFN on  binary  surrogate tasks instead of the original multi-class problem. Our results also show that TabEBM can handle categorical data by encoding categorical features with leave-one-out target statistics. We stress that TabEBM is compatible with any classifier that can be adapted into EBMs, as described in  Section   2 . As foundational models for tabular data evolve  [ 82 ] , new models capable of handling more features and samples are expected. Integrating them into TabEBM will enhance its ability to manage high-dimensional datasets, increasing its versatility and utility.",
            "We use SGLD  [ 85 ]  for sampling from TabEBM, where the starting points  x 0 synth superscript subscript x 0 synth {\\mathbf{x}}_{0}^{\\text{synth}} bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT synth end_POSTSUPERSCRIPT  are initialised by adding Gaussian noise with zero mean and standard deviation   start = 0.01 subscript  start 0.01 \\sigma_{\\text{start}}=0.01 italic_ start_POSTSUBSCRIPT start end_POSTSUBSCRIPT = 0.01  to a randomly selected sample of the specific class, i.e.,  x 0 synth  N  ( X c ,  start 2  I ) similar-to superscript subscript x 0 synth N subscript X c superscript subscript  start 2 I {\\mathbf{x}}_{0}^{\\text{synth}}\\sim\\mathcal{N}(\\mathcal{X}_{c},\\sigma_{\\text{% start}}^{2}\\mathbf{I}) bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT synth end_POSTSUPERSCRIPT  caligraphic_N ( caligraphic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT start end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_I ) . For SGLD, we used the following parameters: step size   step = 0.1 subscript  step 0.1 \\alpha_{\\text{step}}=0.1 italic_ start_POSTSUBSCRIPT step end_POSTSUBSCRIPT = 0.1 , noise scale   noise = 0.01 subscript  noise 0.01 \\alpha_{\\text{noise}}=0.01 italic_ start_POSTSUBSCRIPT noise end_POSTSUBSCRIPT = 0.01  and number of steps  T = 200 T 200 T=200 italic_T = 200 . We found TabEBM to be robust to the SGLD settings (see  Section   C.2 ). We attach the implementation code for TabEBM and will release it as a public open-source library available after publication (see  Section   A.5  for details on our library).",
            "We assess the effect of varying the distance of negative samples. We use TabEBM with four negative samples positioned randomly at the corners of the hypercube, as outlined in  Section   2  (this corresponds to the experimental setup from the main paper). The distance of the negative samples, denoted as   dist neg subscript superscript  neg dist \\alpha^{\\text{neg}}_{\\text{dist}} italic_ start_POSTSUPERSCRIPT neg end_POSTSUPERSCRIPT start_POSTSUBSCRIPT dist end_POSTSUBSCRIPT , is varied.  Table   5  demonstrates that TabEBM remains generally robust to changes in this distance, with only small performance variations across different datasets. Importantly, using TabEBM for data augmentation consistently improves performance by approximately 3% compared to the Baseline, regardless of the distance used."
        ]
    },
    "id_table_6": {
        "caption": "Table 6 :  Test classification accuracy (%) of TabEBM (averaged over six downstream predictors) with different SGLD settings. Increasing   noise subscript  noise \\alpha_{\\text{noise}} italic_ start_POSTSUBSCRIPT noise end_POSTSUBSCRIPT  (added at each SGLD step) is expected to degrade performance, as it causes the sampling to diverge further from the real data.",
        "table": "S4.T2.10.10",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Benchmark generators.  We compare TabEBM against eight existing tabular data generation methods of eight different categories: (i) a standard interpolation method SMOTE  [ 13 ] ; (ii) a Variational Autoencoders (VAE) based method TVAE  [ 86 ] ; (iii) a Generative Adversarial Networks (GAN) method CTGAN  [ 86 ] ; (iv) a normalising flow model Neural Spine Flows (NFLOW)  [ 24 ] ; (v) a diffusion model TabDDPM  [ 42 ] ; (vi) a tree-based method Adversarial Random Forests (ARF)  [ 84 ] ; (vii) a Graph Neural Network (GNN) based method GOGGLE  [ 47 ] ; and (viii) a Prior-Data Fitted Networks (PFN) based method TabPFGen  [ 48 ] . Furthermore, we also include a Baseline model, where no data augmentation is applied (i.e., only real data is used to train downstream predictors). In  Section   A.6 , we detail the settings used for TabEBM and all other generators. We have attached the implementation code for TabEBM and will make it publicly available post-publication.",
            "TabEBM is robust on imbalanced datasets.  For the three binary OpenML datasets (i.e., biodeg, steel and stock), we adjust the class distribution in the training data to vary the class imbalance, while keeping the test data fixed.  Figure   6  shows that TabEBM consistently outperforms Baseline, while the other generators exhibit performance degradation as data imbalance increases.",
            "TabEBM is computationally efficient.   Figure   6  shows the trade-off between accuracy and the time needed for generating stratified synthetic data (for data augmentation). We measure the total duration of (i) training the model and (ii) generating 500 synthetic samples. The results show that TabEBM is practical, as it achieves higher downstream accuracy with lower time costs.",
            "We evaluate the fidelity of synthetic data by focusing on two aspects of synthetic data: similarity to real  train  data and real  test  data. We evaluate the similarity between real and synthetic data via (i)  average inverse of the KullbackLeibler Divergence  (inverse KL)  [ 17 ] , (ii) p-value of  KolmogorovSmirnov test  (KS test)  [ 39 ]  and (iii)  p-value  of  Chi-squared test  (  2 superscript  2 \\chi^{2} italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  test)  [ 55 ] . The full results, including   2 superscript  2 \\chi^{2} italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  test and the similarity between real test data and synthetic data, are in  Section   C.6 . For all three metrics, a bigger value denotes that synthetic data is more likely to have the same distribution as real data.",
            "In  Figure   7  (a1&a2), TabEBM consistently achieves the highest accuracy and distribution similarity between real train data and synthetic data, showing that TabEBM learns the distributions of real train data better than benchmark generators. In  Section   C.6 , we further show that TabEBM remains the most competitive method in similarity between real test data and synthetic data. This indicates that TabEBM can extrapolate beyond real train data and thus generate synthetic data from a more general distribution that aligns with both train and test data. This extrapolation ability also explains why TabEBM can outperform Baseline via data augmentation ( Section   3.1 ).",
            "We vary two key hyperparameters of SGLD on the biodeg binary dataset with  N real = 100 subscript N real 100 N_{\\text{real}}=100 italic_N start_POSTSUBSCRIPT real end_POSTSUBSCRIPT = 100 : the step size   step subscript  step \\alpha_{\\text{step}} italic_ start_POSTSUBSCRIPT step end_POSTSUBSCRIPT  and the noise scale   noise subscript  noise \\alpha_{\\text{noise}} italic_ start_POSTSUBSCRIPT noise end_POSTSUBSCRIPT .  Table   6  shows that TabEBM remains stable with respect to these hyperparameters. Note that smaller values of   noise subscript  noise \\alpha_{\\text{noise}} italic_ start_POSTSUBSCRIPT noise end_POSTSUBSCRIPT  are expected to perform better because SGLD sampling adds noise at each iteration (see Line 7 in  Algorithm   1 ), thus larger values of   noise subscript  noise \\alpha_{\\text{noise}} italic_ start_POSTSUBSCRIPT noise end_POSTSUBSCRIPT  will hinder convergence of the SGLD sampler."
        ]
    },
    "id_table_7": {
        "caption": "Table 7 :  Classification accuracy  (%) of LR, comparing data augmentation on eight real-world tabular datasets with varied real data availability. We report the mean   plus-or-minus \\pm   std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher accuracy. Note that N/A denotes that a specific generator was not applicable or the downstream predictor failed to converge, and the rank is computed with the mean balanced accuracy of other methods. We  bold  the highest accuracy for each dataset of different sample sizes. TabEBM achieves the best overall performance against Baseline and benchmark generators.",
        "table": "A1.T3.3.3",
        "footnotes": [
            ""
        ],
        "references": [
            "In  Figure   7  (a1&a2), TabEBM consistently achieves the highest accuracy and distribution similarity between real train data and synthetic data, showing that TabEBM learns the distributions of real train data better than benchmark generators. In  Section   C.6 , we further show that TabEBM remains the most competitive method in similarity between real test data and synthetic data. This indicates that TabEBM can extrapolate beyond real train data and thus generate synthetic data from a more general distribution that aligns with both train and test data. This extrapolation ability also explains why TabEBM can outperform Baseline via data augmentation ( Section   3.1 ).",
            "Specifically, we evaluate synthetic data via three metrics: (i)  balanced accuracy  of downstream predictors trained with only synthetic data (i.e., train-on-synthetic, test-on-real  [ 86 ,  42 ,  88 ] ); (ii) median Distance to Closest Record (DCR)  [ 89 ] , where a greater DCR denotes synthetic data is less likely to be copied from real data; and (iii)    {\\delta} italic_ -presense  [ 62 ] , where a smaller value denotes a lower re-identification risk for real data from synthetic data. Full numerical results are in  Section   C.7 .",
            "Figure   7  (b1&b2) shows that TabEBM consistently finds a better trade-off between accuracy and privacy preservation. Notably, the train-on-synthetic, test-on-real scenario poses a greater challenge for generators in achieving high accuracy because real data is inaccessible for model training and data augmentation. Despite this difficulty, TabEBM is the only generator that surpasses the overall performance of training on real data (i.e., Baseline). The relatively high DCR for TabEBM indicates that it can extrapolate beyond real train data, aligning with the finding that TabEBMs synthetic data is statistically similar to real test data ( Section   3.2 ). These results further suggest that TabEBM learns the general distribution of real data, and TabEBM can generate high-quality synthetic data suitable for various purposes, including privacy preservation."
        ]
    },
    "id_table_8": {
        "caption": "Table 8 :  Classification accuracy  (%) of KNN, comparing data augmentation on eight real-world tabular datasets with varied real data availability. We report the mean   plus-or-minus \\pm   std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher accuracy. Note that N/A denotes that a specific generator was not applicable or the downstream predictor failed to converge, and the rank is computed with the mean balanced accuracy of other methods. We  bold  the highest accuracy for each dataset of different sample sizes. TabEBM achieves the best overall performance against Baseline and benchmark generators.",
        "table": "A3.T4.40.38",
        "footnotes": [],
        "references": [
            "Having established that TabEBM excels in data augmentation, we explore why classifier logits can be useful when reinterpreted as a class-conditional energy function.  Figure   8  shows the logit distribution of TabPFN trained on surrogate binary tasks and the corresponding energy function of TabEBM (with TabPFN as the binary classifier) as the Euclidean distance from the real data increases.",
            "We found it essential to place the negative samples far from the real data, since TabPFN, which pre-trained to approximate Bayesian inference  [ 33 ] , has its confidence influenced by the distance from the training data  [ 53 ] .  Figure   8  (left) shows that TabPFN outputs high logit values near the real data. As the distance from the real data increases, the logit  f  ( x )  [ 1 ] f x delimited-[] 1 f({\\mathbf{x}})[1] italic_f ( bold_x ) [ 1 ]  decreases smoothly until the two logits become similar, making the classifier uncertain (because the class probabilities become equal). The right figure shows that TabEBMs inferred density drops significantly as the maximum logit decreases, because  p c  ( x )  ( exp  ( f  ( x )  [ 0 ] ) + exp  ( f  ( x )  [ 1 ] ) ) proportional-to subscript p c x f x delimited-[] 0 f x delimited-[] 1 p_{c}({\\mathbf{x}})\\propto(\\exp(f({\\mathbf{x}})[0])+\\exp(f({\\mathbf{x}})[1])) italic_p start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( bold_x )  ( roman_exp ( italic_f ( bold_x ) [ 0 ] ) + roman_exp ( italic_f ( bold_x ) [ 1 ] ) )  from  Equation   3 . Since SGLD sampling performs gradient ascent on the density, the TabEBM-generated samples will be close to the real data. These findings are consistent across datasets (see  Section   C.3 ), where TabPFNs logits remain positive, with similar ranges and a relatively constant sum as distance increases, warranting further investigation. Overall, TabPFNs distance-based uncertainty is useful for inferring accurate energy functions within our TabEBM framework. Since TabEBM can be paired with any other gradient-based classifier that produces logits, we leave these extensions for future work."
        ]
    },
    "id_table_9": {
        "caption": "Table 9 :  Classification accuracy  (%) of MLP, comparing data augmentation on eight real-world tabular datasets with varied real data availability. We report the mean   plus-or-minus \\pm   std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher accuracy. Note that N/A denotes that a specific generator was not applicable or the downstream predictor failed to converge, and the rank is computed with the mean balanced accuracy of other methods. We  bold  the highest accuracy for each dataset of different sample sizes. TabEBM achieves the best overall performance against Baseline and benchmark generators.",
        "table": "A3.T5.45.43",
        "footnotes": [],
        "references": [
            "Figure   9  shows the data splitting setup used across all datasets. Note that data sharing ( Section   3.3 ) shares the same data splitting as data augmentation, except that the Training set and Validation set containing real data are no longer used for training the downstream predictors."
        ]
    },
    "id_table_10": {
        "caption": "Table 10 :  Classification accuracy  (%) of RF, comparing data augmentation on eight real-world tabular datasets with varied real data availability. We report the mean   plus-or-minus \\pm   std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher accuracy. Note that N/A denotes that a specific generator was not applicable or the downstream predictor failed to converge, and the rank is computed with the mean balanced accuracy of other methods. We  bold  the highest accuracy for each dataset of different sample sizes. TabEBM achieves the best overall performance against Baseline and benchmark generators.",
        "table": "A3.T6.4",
        "footnotes": [],
        "references": [
            "We showcase three limitations of current generative models: (1)  Figure   10  shows that models approximating the joint distribution  p  ( x , y ) p x y p({\\mathbf{x}},y) italic_p ( bold_x , italic_y )  may fail to preserve the stratification of the real data and even fail to generate samples from specific classes. (2)  Figure   11(e)  evaluates the approximated class-conditional distributions  p  ( x  y ) p conditional x y p({\\mathbf{x}}\\mid y) italic_p ( bold_x  italic_y )  on data with increasing noise levels, and (3)  Figure   12(d)  evaluates the approximated class-conditional distributions  p  ( x  y ) p conditional x y p({\\mathbf{x}}\\mid y) italic_p ( bold_x  italic_y )  on data with increasing class imbalance."
        ]
    },
    "id_table_11": {
        "caption": "Table 11 :  Classification accuracy  (%) of XGBoost, comparing data augmentation on eight real-world tabular datasets with varied real data availability. We report the mean   plus-or-minus \\pm   std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher accuracy. Note that N/A denotes that a specific generator was not applicable or the downstream predictor failed to converge, and the rank is computed with the mean balanced accuracy of other methods. We  bold  the highest accuracy for each dataset of different sample sizes. TabEBM achieves the best overall performance against Baseline and benchmark generators.",
        "table": "A3.T7.328.326",
        "footnotes": [],
        "references": [
            "We showcase three limitations of current generative models: (1)  Figure   10  shows that models approximating the joint distribution  p  ( x , y ) p x y p({\\mathbf{x}},y) italic_p ( bold_x , italic_y )  may fail to preserve the stratification of the real data and even fail to generate samples from specific classes. (2)  Figure   11(e)  evaluates the approximated class-conditional distributions  p  ( x  y ) p conditional x y p({\\mathbf{x}}\\mid y) italic_p ( bold_x  italic_y )  on data with increasing noise levels, and (3)  Figure   12(d)  evaluates the approximated class-conditional distributions  p  ( x  y ) p conditional x y p({\\mathbf{x}}\\mid y) italic_p ( bold_x  italic_y )  on data with increasing class imbalance."
        ]
    },
    "id_table_12": {
        "caption": "Table 12 :  Classification accuracy  (%) of TabPFN, comparing data augmentation on eight real-world tabular datasets with varied real data availability. We report the mean   plus-or-minus \\pm   std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher accuracy. Note that N/A denotes that a specific generator was not applicable or the downstream predictor failed to converge, and the rank is computed with the mean balanced accuracy of other methods. We  bold  the highest accuracy for each dataset of different sample sizes. TabEBM achieves the best overall performance against Baseline and benchmark generators.",
        "table": "A3.T8.328.326",
        "footnotes": [],
        "references": [
            "We showcase three limitations of current generative models: (1)  Figure   10  shows that models approximating the joint distribution  p  ( x , y ) p x y p({\\mathbf{x}},y) italic_p ( bold_x , italic_y )  may fail to preserve the stratification of the real data and even fail to generate samples from specific classes. (2)  Figure   11(e)  evaluates the approximated class-conditional distributions  p  ( x  y ) p conditional x y p({\\mathbf{x}}\\mid y) italic_p ( bold_x  italic_y )  on data with increasing noise levels, and (3)  Figure   12(d)  evaluates the approximated class-conditional distributions  p  ( x  y ) p conditional x y p({\\mathbf{x}}\\mid y) italic_p ( bold_x  italic_y )  on data with increasing class imbalance."
        ]
    },
    "id_table_13": {
        "caption": "Table 13 :  Details of the six real-world tabular datasets from UCI.",
        "table": "A3.T9.328.326",
        "footnotes": [],
        "references": [
            "Figure   13  shows TabEBMs energy  E c  ( x ) subscript E c x E_{c}(x) italic_E start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_x )  when varying the selection of the negative samples. TabEBM infers an accurate energy surface with distant negative samples, and the energy surface becomes inaccurate when negative samples resemble real samples. This occurs because TabPFN is uncertain when points of different classes are close, affecting its logits magnitude and making them unsuitable for density estimation."
        ]
    },
    "id_table_14": {
        "caption": "Table 14 :  Test classification accuracy (%) aggregated over six downstream predictors, comparing data augmentation on six leakage-free UCI datasets. Note that N/A denotes that a specific generator was not applicable. TabEBM still achieves the best overall performance against benchmark methods.",
        "table": "A3.T10.328.326",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "Table 15 :  Test classification accuracy (%) aggregated over six downstream predictors, comparing data augmentation with increased real data availability of the texture dataset. Note that N/A denotes that a specific generator was not applicable. On larger datasets, TabEBM still outperforms other generators, but training on real data alone appears sufficient. This highlights TabEBMs usefulness in fields with limited training samples.",
        "table": "A3.T11.327.325",
        "footnotes": [],
        "references": []
    },
    "id_table_16": {
        "caption": "Table 16 :  Inverse KL between real train data and synthetic data  on eight real-world tabular datasets with varied real data availability. We report the mean   plus-or-minus \\pm   std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher fidelity. Note that N/A denotes that a specific generator was not applicable, and the rank is computed with the mean result of other methods. We  bold  the highest result for each dataset of different sample sizes. TabEBM achieves the best overall performance against benchmark generators.",
        "table": "A3.T12.250.248",
        "footnotes": [],
        "references": []
    },
    "id_table_17": {
        "caption": "Table 17 :  KS test between real train data and synthetic data  on eight real-world tabular datasets with varied real data availability. We report the mean   plus-or-minus \\pm   std result and average rank across datasets. A higher rank implies higher fidelity. Note that N/A denotes that a specific generator was not applicable, and the rank is computed with the mean result of other methods. We  bold  the highest result for each dataset of different sample sizes. TabEBM achieves the best overall performance against benchmark generators.",
        "table": "A3.T13.2.2",
        "footnotes": [
            ""
        ],
        "references": []
    },
    "id_table_18": {
        "caption": "Table 18 :   2 superscript  2 \\chi^{2} italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  test between real train data and synthetic data  on eight real-world tabular datasets with varied real data availability. We report the mean   plus-or-minus \\pm   std result and average rank across datasets. A higher rank implies higher fidelity. Note that N/A denotes that a specific generator was not applicable, and the rank is computed with the mean result of other methods. We  bold  the highest result for each dataset of different sample sizes. TabEBM achieves the best overall performance against benchmark generators.",
        "table": "A3.T14.41.41",
        "footnotes": [],
        "references": []
    },
    "id_table_19": {
        "caption": "Table 19 :  Inverse KL between real test data and synthetic data  on eight real-world tabular datasets with varied real data availability. We report the mean   plus-or-minus \\pm   std result and average rank across datasets. A higher rank implies higher fidelity. Note that N/A denotes that a specific generator was not applicable, and the rank is computed with the mean result of other methods. We  bold  the highest result for each dataset of different sample sizes. TabEBM achieves the best overall performance against benchmark generators.",
        "table": "A3.T15.49.49",
        "footnotes": [],
        "references": []
    },
    "id_table_20": {
        "caption": "Table 20 :  KS test between real test data and synthetic data  on eight real-world tabular datasets with varied real data availability. We report the mean   plus-or-minus \\pm   std result and average rank across datasets. A higher rank implies higher fidelity. Note that N/A denotes that a specific generator was not applicable, and the rank is computed with the mean result of other methods. We  bold  the highest result for each dataset of different sample sizes. TabEBM achieves the best overall performance against benchmark generators.",
        "table": "A3.T16.294.292",
        "footnotes": [],
        "references": []
    },
    "id_table_21": {
        "caption": "Table 21 :   2 superscript  2 \\chi^{2} italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  test between real test data and synthetic data  on eight real-world tabular datasets with varied real data availability. We report the mean   plus-or-minus \\pm   std result and average rank across datasets. A higher rank implies higher fidelity. Note that N/A denotes that a specific generator was not applicable, and the rank is computed with the mean result of other methods. We  bold  the highest result for each dataset of different sample sizes. TabEBM achieves the best overall performance against benchmark generators.",
        "table": "A3.T17.294.292",
        "footnotes": [],
        "references": []
    },
    "id_table_22": {
        "caption": "Table 22 :  Classification accuracy  (%) aggregated over six downstream predictors, comparing data sharing on eight real-world tabular datasets with varied real data availability. We report the mean   plus-or-minus \\pm   std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher accuracy. Note that N/A denotes the inapplicability of a specific generator. Different from  Table   1 ,   - -  denotes a generator cannot satisfy the requirement of generating 500 stratified samples even after generating 10,000 synthetic samples. The results of these inapplicable or failed generators are computed with the mean results of other methods. We  bold  the highest result for each dataset of different sample sizes. TVAE learns the joint distribution  p  ( x , y ) p x y p({\\mathbf{x}},y) italic_p ( bold_x , italic_y )  and fails to maintain the original training label distribution. TabEBM achieves the best overall performance against Baseline and benchmark generators.",
        "table": "A3.T18.296.292",
        "footnotes": [
            ""
        ],
        "references": []
    },
    "id_table_23": {
        "caption": "Table 23 :  Classification accuracy  (%) of LR, comparing data sharing on eight real-world tabular datasets with varied real data availability. We report the mean   plus-or-minus \\pm   std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher accuracy. Note that N/A denotes the inapplicability of a specific generator. Different from  Table   1 ,   - -  denotes a generator cannot satisfy the requirement of generating 500 stratified samples even after generating 10,000 synthetic samples. The results of these inapplicable or failed generators are computed with the mean results of other methods. We  bold  the highest result for each dataset of different sample sizes. TVAE learns the joint distribution  p  ( x , y ) p x y p({\\mathbf{x}},y) italic_p ( bold_x , italic_y )  and fails to maintain the original training label distribution. TabEBM achieves the best overall performance against Baseline and benchmark generators.",
        "table": "A3.T19.294.292",
        "footnotes": [
            ""
        ],
        "references": []
    },
    "id_table_24": {
        "caption": "Table 24 :  Classification accuracy  (%) of KNN, comparing data sharing on eight real-world tabular datasets with varied real data availability. We report the mean   plus-or-minus \\pm   std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher accuracy. Note that N/A denotes the inapplicability of a specific generator. Different from  Table   1 ,   - -  denotes a generator cannot satisfy the requirement of generating 500 stratified samples even after generating 10,000 synthetic samples. The results of these inapplicable or failed generators are computed with the mean results of other methods. We  bold  the highest result for each dataset of different sample sizes. TVAE learns the joint distribution  p  ( x , y ) p x y p({\\mathbf{x}},y) italic_p ( bold_x , italic_y )  and fails to maintain the original training label distribution. TabEBM achieves the best overall performance against Baseline and benchmark generators.",
        "table": "A3.T20.294.292",
        "footnotes": [
            ""
        ],
        "references": []
    },
    "id_table_25": {
        "caption": "Table 25 :  Classification accuracy  (%) of MLP, comparing data sharing on eight real-world tabular datasets with varied real data availability. We report the mean   plus-or-minus \\pm   std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher accuracy. Note that N/A denotes the inapplicability of a specific generator. Different from  Table   1 ,   - -  denotes a generator cannot satisfy the requirement of generating 500 stratified samples even after generating 10,000 synthetic samples. The results of these inapplicable or failed generators are computed with the mean results of other methods. We  bold  the highest result for each dataset of different sample sizes. TVAE learns the joint distribution  p  ( x , y ) p x y p({\\mathbf{x}},y) italic_p ( bold_x , italic_y )  and fails to maintain the original training label distribution. TabEBM achieves the best overall performance against Baseline and benchmark generators.",
        "table": "A3.T21.296.292",
        "footnotes": [
            ""
        ],
        "references": []
    },
    "id_table_26": {
        "caption": "Table 26 :  Classification accuracy  (%) of RF, comparing data sharing on eight real-world tabular datasets with varied real data availability. We report the mean   plus-or-minus \\pm   std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher accuracy. Note that N/A denotes the inapplicability of a specific generator. Different from  Table   1 ,   - -  denotes a generator cannot satisfy the requirement of generating 500 stratified samples even after generating 10,000 synthetic samples. The results of these inapplicable or failed generators are computed with the mean results of other methods. We  bold  the highest result for each dataset of different sample sizes. TVAE learns the joint distribution  p  ( x , y ) p x y p({\\mathbf{x}},y) italic_p ( bold_x , italic_y )  and fails to maintain the original training label distribution. TabEBM achieves the best overall performance against Baseline and benchmark generators.",
        "table": "A3.T22.297.291",
        "footnotes": [
            ""
        ],
        "references": []
    },
    "id_table_27": {
        "caption": "Table 27 :  Classification accuracy  (%) of XGBoost, comparing data sharing on eight real-world tabular datasets with varied real data availability. We report the mean   plus-or-minus \\pm   std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher accuracy. Note that N/A denotes the inapplicability of a specific generator. Different from  Table   1 ,   - -  denotes a generator cannot satisfy the requirement of generating 500 stratified samples even after generating 10,000 synthetic samples. The results of these inapplicable or failed generators are computed with the mean results of other methods. We  bold  the highest result for each dataset of different sample sizes. TVAE learns the joint distribution  p  ( x , y ) p x y p({\\mathbf{x}},y) italic_p ( bold_x , italic_y )  and fails to maintain the original training label distribution. TabEBM achieves the best overall performance against Baseline and benchmark generators.",
        "table": "A3.T23.297.291",
        "footnotes": [
            ""
        ],
        "references": []
    },
    "id_table_28": {
        "caption": "Table 28 :  Classification accuracy  (%) of TabPFN, comparing data sharing on eight real-world tabular datasets with varied real data availability. We report the mean   plus-or-minus \\pm   std balanced accuracy and average accuracy rank across datasets. A higher rank implies higher accuracy. Note that N/A denotes the inapplicability of a specific generator. Different from  Table   1 ,   - -  denotes a generator cannot satisfy the requirement of generating 500 stratified samples even after generating 10,000 synthetic samples. The results of these inapplicable or failed generators are computed with the mean results of other methods. We  bold  the highest result for each dataset of different sample sizes. TVAE learns the joint distribution  p  ( x , y ) p x y p({\\mathbf{x}},y) italic_p ( bold_x , italic_y )  and fails to maintain the original training label distribution. TabEBM achieves the best overall performance against Baseline and benchmark generators.",
        "table": "A3.T24.297.291",
        "footnotes": [
            ""
        ],
        "references": []
    },
    "id_table_29": {
        "caption": "Table 29 :  DCR between real train data and synthetic data  on eight real-world tabular datasets with varied real data availability. We report the mean   plus-or-minus \\pm   std result and average rank across datasets. A higher rank implies better privacy preservation. Note that N/A denotes that a specific generator was not applicable, and the rank is computed with the mean result of other methods. We  bold  the highest result for each dataset of different sample sizes. Even though ARF and NFLOW show high DCR, our experiments demonstrate that they do not learn the data distribution well, leading to poor downstream accuracy. TabEBM achieves competitive overall DCR against benchmark generators.",
        "table": "A3.T25.297.291",
        "footnotes": [],
        "references": []
    },
    "id_table_30": {
        "caption": "Table 30 :    \\delta italic_ -presence between real train data and synthetic data  on eight real-world tabular datasets with varied real data availability. We report the mean   plus-or-minus \\pm   std result and average rank across datasets. A higher rank implies better privacy preservation. Note that N/A denotes that a specific generator was not applicable, and the rank is computed with the mean result of other methods. We  bold  the best result for each dataset of different sample sizes. TabEBM achieves the best overall performance against benchmark generators.",
        "table": "A3.T26.297.291",
        "footnotes": [],
        "references": []
    },
    "id_table_31": {
        "caption": "",
        "table": "A3.T27.288.282",
        "footnotes": [],
        "references": []
    },
    "id_table_32": {
        "caption": "",
        "table": "A3.T28.229.223",
        "footnotes": [],
        "references": []
    },
    "id_table_33": {
        "caption": "",
        "table": "A3.T29.260.258",
        "footnotes": [],
        "references": []
    },
    "id_table_34": {
        "caption": "",
        "table": "A3.T30.262.258",
        "footnotes": [],
        "references": []
    }
}