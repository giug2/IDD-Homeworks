{
    "id_table_1": {
        "caption": "Table 1:    Explanation of each verification, its definition, inputs, outputs and corresponding actions.",
        "table": "S3.T1.19.19",
        "footnotes": [],
        "references": [
            "However, challenges remain, including the generation of false information (hallucination)  (Li et al.,  2022 )  and difficulties in knowledge updating  (Zhang et al.,  2023 ) , which hinder the applicability of a wider variety. To address these challenges, Retrieval-Augmented Generation (RAG)  (Lewis et al.,  2020 )  has been proposed, positing that documents semantically similar to a query likely contain the information needed to answer that query. By leveraging the in-context learning capabilities of LLMs, RAG enhances the likelihood of accurate responses from semantically related document retrievals while effectively reducing hallucinations and facilitating knowledge acquisition  (Li et al.,  2024 ) . As RAG technology evolves, it gradually expands into the multimodal domain, achieving progress in tasks involving texts  (Wu et al.,  2024 ) , images  (Pan et al.,  2022 ) , etc., an example of multimodal RAG task is shown in Figure  1 .",
            "We optimize the performance of  R R \\mathcal{R} caligraphic_R  using the InfoNCE loss function  (He et al.,  2020 ) , as shown in Equation  1 , where  D p  o  s subscript D p o s \\boldsymbol{D}_{pos} bold_italic_D start_POSTSUBSCRIPT italic_p italic_o italic_s end_POSTSUBSCRIPT  represents the supporting documents for the answers, and    \\tau italic_  is the temperature hyperparameter. The goal is to minimize  l l \\ell roman_l  between positive and negative samples.",
            "The components of the SAM-RAG framework, as illustrated in Figure  2 , are described. The overall process of executing multimodal RAG tasks is divided into five steps:  modality alignment ,  document retrieval ,  relevance verification ,  answer generation , and  answer verification . Image information from the entire document corpus is first aligned with the text modality. Upon receiving a query, document retrieval is performed. The retrieved results are then ranked by similarity, and each is assessed for relevance to the query, with only the relevant results being retained. Based on the relevant documents, the framework generates  A A \\boldsymbol{A} bold_italic_A . The validity of the generated answer is subsequently verified, and if confirmed, the answer is returned, concluding the retrieval process. The core innovation of this method lies in the introduction of three key verification mechanisms, as shown in Table  1 . Prompts are listed in Appendix  B .",
            "To track the retrieval status, a flag  F F \\mathcal{F} caligraphic_F  is initialized to  False , and a storage space  C C \\boldsymbol{C} bold_italic_C  is created to hold relevant information. The retrieved documents are processed in batches, where the relevance of each retrieved text document  d t subscript d t \\boldsymbol{d}_{t} bold_italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  to the query is directly assessed using the relevance verification  isRel  (refer to Figure  2  and Table  1 , top row). If an image  d i subscript d i \\boldsymbol{d}_{i} bold_italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is retrieved instead, a question-specific caption  d s  c subscript d s c \\boldsymbol{d}_{sc} bold_italic_d start_POSTSUBSCRIPT italic_s italic_c end_POSTSUBSCRIPT  is generated based on the query, and its relevance is evaluated accordingly. When  isRel =True , the relevant information is stored in  C C \\boldsymbol{C} bold_italic_C , and  F F \\mathcal{F} caligraphic_F  is updated to  True . After processing a batch, if  isRel =False , the next batch is processed; otherwise, the retrieval process is paused, and the next stage is initiated.",
            "In this stage,  isUse  verification process (refer to Figure  2  and Table  1 , second row), is introduced to determine whether the generated answer  A A \\boldsymbol{A} bold_italic_A  effectively addresses the query. If  isUse =True , the next step will involve evaluating the support of  A A \\boldsymbol{A} bold_italic_A  using  isSup  (refer to Figure  2  and Table  1 , third row); otherwise,  A A \\boldsymbol{A} bold_italic_A  is regenerated based on  C C \\boldsymbol{C} bold_italic_C . The purpose of  isSup  is to confirm that  A A \\boldsymbol{A} bold_italic_A  is adequately supported by  C C \\boldsymbol{C} bold_italic_C , thereby preventing situations where  A A \\boldsymbol{A} bold_italic_A  lacks support from  D r  e  l subscript D r e l \\boldsymbol{D}_{rel} bold_italic_D start_POSTSUBSCRIPT italic_r italic_e italic_l end_POSTSUBSCRIPT . If  isSup =True , it is indicated that  A A \\boldsymbol{A} bold_italic_A  fulfills the task requirements, and  A A \\boldsymbol{A} bold_italic_A  is returned, concluding the retrieval process. If  isSup =False , it signifies that  A A \\boldsymbol{A} bold_italic_A  is unsupported by  C C \\boldsymbol{C} bold_italic_C , prompting a reset of both  C C \\boldsymbol{C} bold_italic_C  and  F F \\mathcal{F} caligraphic_F  to return to the first stage for further retrieval. If  isSup =Partial , it suggests incomplete support information, leading to the retention of  C C \\boldsymbol{C} bold_italic_C  while resetting  F F \\mathcal{F} caligraphic_F , and returning to the first stage to continue the retrieval. The objective of this stage is to ensure that the final output aligns with task requirements and minimizes the risk of potential hallucinations.",
            "Retrieved image: (Figure.  A1 )"
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Multimodal test-set synthesis evaluation results. \"RAG\" means conventional RAG with retrieval top k as 8.  R R \\mathcal{R} caligraphic_R  and  M M \\mathcal{M} caligraphic_M  indicate model is not finetuned;  R  superscript R \\mathcal{R}^{*} caligraphic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  and  M  superscript M \\mathcal{M}^{*} caligraphic_M start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  indicate model is finetuned;  G G \\mathcal{G} caligraphic_G  indicates model is  GPT .",
        "table": "S4.T2.18",
        "footnotes": [],
        "references": [
            "In this study, we propose a novel multimodal RAG framework, SAM-RAG, as illustrated in Figure  2 . SAM-RAG is designed to actively select multimodal data pertinent to the query for answer generation and to validate the generated responses from multiple perspectives, thereby ensuring the quality and reliability of the output.",
            "The components of the SAM-RAG framework, as illustrated in Figure  2 , are described. The overall process of executing multimodal RAG tasks is divided into five steps:  modality alignment ,  document retrieval ,  relevance verification ,  answer generation , and  answer verification . Image information from the entire document corpus is first aligned with the text modality. Upon receiving a query, document retrieval is performed. The retrieved results are then ranked by similarity, and each is assessed for relevance to the query, with only the relevant results being retained. Based on the relevant documents, the framework generates  A A \\boldsymbol{A} bold_italic_A . The validity of the generated answer is subsequently verified, and if confirmed, the answer is returned, concluding the retrieval process. The core innovation of this method lies in the introduction of three key verification mechanisms, as shown in Table  1 . Prompts are listed in Appendix  B .",
            "To track the retrieval status, a flag  F F \\mathcal{F} caligraphic_F  is initialized to  False , and a storage space  C C \\boldsymbol{C} bold_italic_C  is created to hold relevant information. The retrieved documents are processed in batches, where the relevance of each retrieved text document  d t subscript d t \\boldsymbol{d}_{t} bold_italic_d start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  to the query is directly assessed using the relevance verification  isRel  (refer to Figure  2  and Table  1 , top row). If an image  d i subscript d i \\boldsymbol{d}_{i} bold_italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is retrieved instead, a question-specific caption  d s  c subscript d s c \\boldsymbol{d}_{sc} bold_italic_d start_POSTSUBSCRIPT italic_s italic_c end_POSTSUBSCRIPT  is generated based on the query, and its relevance is evaluated accordingly. When  isRel =True , the relevant information is stored in  C C \\boldsymbol{C} bold_italic_C , and  F F \\mathcal{F} caligraphic_F  is updated to  True . After processing a batch, if  isRel =False , the next batch is processed; otherwise, the retrieval process is paused, and the next stage is initiated.",
            "In this stage,  isUse  verification process (refer to Figure  2  and Table  1 , second row), is introduced to determine whether the generated answer  A A \\boldsymbol{A} bold_italic_A  effectively addresses the query. If  isUse =True , the next step will involve evaluating the support of  A A \\boldsymbol{A} bold_italic_A  using  isSup  (refer to Figure  2  and Table  1 , third row); otherwise,  A A \\boldsymbol{A} bold_italic_A  is regenerated based on  C C \\boldsymbol{C} bold_italic_C . The purpose of  isSup  is to confirm that  A A \\boldsymbol{A} bold_italic_A  is adequately supported by  C C \\boldsymbol{C} bold_italic_C , thereby preventing situations where  A A \\boldsymbol{A} bold_italic_A  lacks support from  D r  e  l subscript D r e l \\boldsymbol{D}_{rel} bold_italic_D start_POSTSUBSCRIPT italic_r italic_e italic_l end_POSTSUBSCRIPT . If  isSup =True , it is indicated that  A A \\boldsymbol{A} bold_italic_A  fulfills the task requirements, and  A A \\boldsymbol{A} bold_italic_A  is returned, concluding the retrieval process. If  isSup =False , it signifies that  A A \\boldsymbol{A} bold_italic_A  is unsupported by  C C \\boldsymbol{C} bold_italic_C , prompting a reset of both  C C \\boldsymbol{C} bold_italic_C  and  F F \\mathcal{F} caligraphic_F  to return to the first stage for further retrieval. If  isSup =Partial , it suggests incomplete support information, leading to the retention of  C C \\boldsymbol{C} bold_italic_C  while resetting  F F \\mathcal{F} caligraphic_F , and returning to the first stage to continue the retrieval. The objective of this stage is to ensure that the final output aligns with task requirements and minimizes the risk of potential hallucinations.",
            "F1 score and exact match ( EM ) metrics  (Rajpurkar et al.,  2016 )  are employed to evaluate the quality of the generated answers. Additionally, two metrics are used to evaluate retrieval performance: Recall@N (as defined in Equation  2 ) and a newly introduced metric, the average retrieval number ( ARN ), which measures the average number of documents retrieved to generate the final answer, providing a more nuanced assessment of retrieval efficiency in SAM-RAG.",
            "The experimental results, summarized in Table  2 , demonstrate that SAM-RAG achieves superior performance across all evaluated tasks, significantly outperforming baseline models, including the state-of-the-art MuRAG. Below, we provide a detailed analysis of the performance improvements and insights gained from the comparisons.",
            "Performance Comparison with Baselines : Table  2  (top) demonstrates that fine-tuning the retrieval model  R R \\mathcal{R} caligraphic_R  or the VLM  M M \\mathcal{M} caligraphic_M  improves multimodal RAG performance. However, fine-tuning  M M \\mathcal{M} caligraphic_M  provides more substantial gains. This suggests that the baseline retrieval model  R R \\mathcal{R} caligraphic_R  is already sufficiently optimized, leading to smaller performance improvements from fine-tuning. In contrast, optimizing  M M \\mathcal{M} caligraphic_M  directly enhances the models ability to process multimodal data. Notably, the fine-tuned  RAG( R  superscript R \\mathcal{R}^{*} caligraphic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  +  M  superscript M \\mathcal{M}^{*} caligraphic_M start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT )  shows slightly lower performance than MuRAG, despite differences in the volume of training data.",
            "SAM-RAG vs. Conventional RAG : As shown in Table  2  (middle), SAM-RAG consistently outperforms conventional RAG methods. By dynamically retrieving documents until relevant information is found, SAM-RAG avoids the limitations of fixed retrieval strategies. Fine-tuning the retrieval model  R R \\mathcal{R} caligraphic_R  speeds up document retrieval but does not significantly impact overall performance, as the strength of SAM-RAG lies in leveraging the fine-tuned  M M \\mathcal{M} caligraphic_M . The results indicate that fine-tuning  M M \\mathcal{M} caligraphic_M  leads to the most notable performance improvements in SAM-RAG, especially for multimodal tasks.",
            "Effect of GPT Integration : Table  2  (bottom) highlights the substantial performance gains from integration of  G G \\mathcal{G} caligraphic_G  into the SAM-RAG and conventional RAG frameworks. Models incorporating  G G \\mathcal{G} caligraphic_G  outperform all other configurations, underscoring  G G \\mathcal{G} caligraphic_G s advanced reasoning and understanding abilities. SAM-RAG combined with  G G \\mathcal{G} caligraphic_G  shows the most significant improvements, particularly in visual tasks, where it exceeds textual performance. This suggests that the SAM-RAG framework, when paired with  G G \\mathcal{G} caligraphic_G , achieves a deeper understanding of visual content than other approaches.",
            "To assess retrieval effectiveness, we analyzed the top-performing SAM-RAG and conventional RAG models, as shown in Figure  3  and Table  4 . The results indicate that  R  superscript R \\mathcal{R}^{*} caligraphic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  consistently outperforms  R R \\mathcal{R} caligraphic_R  in Recall@N across all top  k k k italic_k  values, suggesting that fine-tuning the embeddings improves relevant context retrieval. Table  2  further shows that the EM scores for  R  superscript R \\mathcal{R}^{*} caligraphic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  are generally higher than for  R R \\mathcal{R} caligraphic_R . Notably, at a top  k k k italic_k  of 8, the EM score surpasses all scores of  R R \\mathcal{R} caligraphic_R  by nearly 20%, suggesting that fine-tuning  R R \\mathcal{R} caligraphic_R  benefits more from a moderate number of retrievals, while too many retrievals may introduce noise. Unlike conventional RAG, which often retrieves more documents than needed thus introduces irrelevant content, SAM-RAGs recall numbers closely match the \"Gold Reference\" values and yield higher EM scores for both text and image retrieval. This suggests that SAM-RAG effectively retrieves relevant information while reducing noise, resulting in more accurate and higher-quality outputs.",
            "Retrieved image: (Figure.  A2 )"
        ]
    },
    "id_table_3": {
        "caption": "Table 3:    Effect of different verification combinations.   \"RAG\" indicates conventional RAG pipeline.      \\Delta roman_ : The difference is between the best value and the corresponding value.",
        "table": "S4.T2.29",
        "footnotes": [],
        "references": [
            "To validate the effectiveness of each component of SAM-RAG, a series of ablation studies are performed that check the performance of the framework after removing each component, as shown in Table  3 . The results indicate that, compared to conventional RAG, the introduction of distinct verification, particularly  isRel  and  isSup , leads to notable improvements in both F1 and EM scores. Specifically, although introducing  isRel  only brings minimal improvement, the subsequent introduction of  isUse  and  isSup  makes a greater impact. When all verifications are combined (labeled \"with all\"), the EM score rises to the maximum value, reflecting an improvement of more than 20% compared to the performance of MuRAG. This finding suggests that simultaneous consideration of relevance ( isRel ), support ( isSup ), and usability ( isUse ) allows the model to effectively filter the most valuable information for the generation of answers, significantly improving the quality of the output.",
            "To assess retrieval effectiveness, we analyzed the top-performing SAM-RAG and conventional RAG models, as shown in Figure  3  and Table  4 . The results indicate that  R  superscript R \\mathcal{R}^{*} caligraphic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  consistently outperforms  R R \\mathcal{R} caligraphic_R  in Recall@N across all top  k k k italic_k  values, suggesting that fine-tuning the embeddings improves relevant context retrieval. Table  2  further shows that the EM scores for  R  superscript R \\mathcal{R}^{*} caligraphic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  are generally higher than for  R R \\mathcal{R} caligraphic_R . Notably, at a top  k k k italic_k  of 8, the EM score surpasses all scores of  R R \\mathcal{R} caligraphic_R  by nearly 20%, suggesting that fine-tuning  R R \\mathcal{R} caligraphic_R  benefits more from a moderate number of retrievals, while too many retrievals may introduce noise. Unlike conventional RAG, which often retrieves more documents than needed thus introduces irrelevant content, SAM-RAGs recall numbers closely match the \"Gold Reference\" values and yield higher EM scores for both text and image retrieval. This suggests that SAM-RAG effectively retrieves relevant information while reducing noise, resulting in more accurate and higher-quality outputs."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Multimodal test-set synthesis evaluation results and recall@N analysis.    \" Gold Reference \" means the texts and images which are labeled as \"supporting context\"; \" ARN \" means the average number of retrieved documents for the answers.",
        "table": "S4.T2.30",
        "footnotes": [],
        "references": [
            "Overall Performance : Across both text and image tasks, SAM-RAG delivers higher F1 and EM scores compared to baseline methods. These improvements underscore the effectiveness of the dynamic retrieval and generation mechanisms of SAM-RAG. The roles of key components,  isRel ,  isUse  and  isSup , are further analyzed in the ablation study ( 4.5 ) and illustrated through case studies ( 4.7 , Figure  4 ). Additional case examples are provided in Appendix  A .",
            "To assess retrieval effectiveness, we analyzed the top-performing SAM-RAG and conventional RAG models, as shown in Figure  3  and Table  4 . The results indicate that  R  superscript R \\mathcal{R}^{*} caligraphic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  consistently outperforms  R R \\mathcal{R} caligraphic_R  in Recall@N across all top  k k k italic_k  values, suggesting that fine-tuning the embeddings improves relevant context retrieval. Table  2  further shows that the EM scores for  R  superscript R \\mathcal{R}^{*} caligraphic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  are generally higher than for  R R \\mathcal{R} caligraphic_R . Notably, at a top  k k k italic_k  of 8, the EM score surpasses all scores of  R R \\mathcal{R} caligraphic_R  by nearly 20%, suggesting that fine-tuning  R R \\mathcal{R} caligraphic_R  benefits more from a moderate number of retrievals, while too many retrievals may introduce noise. Unlike conventional RAG, which often retrieves more documents than needed thus introduces irrelevant content, SAM-RAGs recall numbers closely match the \"Gold Reference\" values and yield higher EM scores for both text and image retrieval. This suggests that SAM-RAG effectively retrieves relevant information while reducing noise, resulting in more accurate and higher-quality outputs.",
            "To intuitively illustrate the effectiveness of SAM-RAG, we present selected examples in Figure  4 . The left panel contrasts the standard RAG pipeline with SAM-RAG. In (a), the conventional RAG retriever  R R \\mathcal{R} caligraphic_R  retrieves multiple documents, but despite including relevant content (e.g., the Sheffield United F.C. logo),  M M \\mathcal{M} caligraphic_M  is misled by irrelevant information, resulting in incorrect generation. In (b), SAM-RAG mitigates this problem by filtering irrelevant documents through  isRel , ensuring that  M M \\mathcal{M} caligraphic_M  focuses only on relevant information, leading to accurate generation based on essential content like  d s  c subscript d s c \\boldsymbol{d}_{sc} bold_italic_d start_POSTSUBSCRIPT italic_s italic_c end_POSTSUBSCRIPT ."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "S4.T2.31",
        "footnotes": [],
        "references": [
            "Overall Performance : Across both text and image tasks, SAM-RAG delivers higher F1 and EM scores compared to baseline methods. These improvements underscore the effectiveness of the dynamic retrieval and generation mechanisms of SAM-RAG. The roles of key components,  isRel ,  isUse  and  isSup , are further analyzed in the ablation study ( 4.5 ) and illustrated through case studies ( 4.7 , Figure  4 ). Additional case examples are provided in Appendix  A ."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "S4.T3.2.3",
        "footnotes": [],
        "references": []
    },
    "id_table_7": {
        "caption": "",
        "table": "S4.T3.2.4",
        "footnotes": [],
        "references": [
            "Overall Performance : Across both text and image tasks, SAM-RAG delivers higher F1 and EM scores compared to baseline methods. These improvements underscore the effectiveness of the dynamic retrieval and generation mechanisms of SAM-RAG. The roles of key components,  isRel ,  isUse  and  isSup , are further analyzed in the ablation study ( 4.5 ) and illustrated through case studies ( 4.7 , Figure  4 ). Additional case examples are provided in Appendix  A ."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "S4.T3.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_9": {
        "caption": "",
        "table": "S4.T3.2.5",
        "footnotes": [],
        "references": []
    },
    "id_table_10": {
        "caption": "",
        "table": "S4.T3.2.2",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "",
        "table": "S4.T4.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "",
        "table": "S4.T4.1.2",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "",
        "table": "S4.T4.1.3",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": [
        "https://github.com/FlagOpen/FlagEmbedding",
        "https://github.com/hiyouga/LLaMA-Factory",
        "version: GPT-4o-2024-05-13"
    ]
}