{
    "S5.T1": {
        "caption": "Table 1: Image captioning results. We compare our method which generates captions with GPT-3 with the General and the Question-based approaches. In the Question-based approach we concatenate all the outputs of GPT-3 after conditioning it with different questions related to the image. We compare the results against visual captions, contextual captions or both.",
        "table": "<table id=\"S5.T1.7\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T1.7.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.7.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r\">Description type</th>\n<th id=\"S5.T1.7.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r\">    Metric</th>\n<th id=\"S5.T1.7.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">  OFA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib38\" title=\"\" class=\"ltx_ref\">38</a>]</cite>\n</th>\n<th id=\"S5.T1.7.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">   Ours General</th>\n<th id=\"S5.T1.7.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Ours Question-based</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.7.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.7.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"4\"><span id=\"S5.T1.7.2.1.1.1\" class=\"ltx_text\">Visual</span></th>\n<th id=\"S5.T1.7.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">BLEU1</th>\n<td id=\"S5.T1.7.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.048</td>\n<td id=\"S5.T1.7.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T1.7.2.1.4.1\" class=\"ltx_text ltx_font_bold\">0.181</span></td>\n<td id=\"S5.T1.7.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.137</td>\n</tr>\n<tr id=\"S5.T1.7.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T1.7.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">ROUGE</th>\n<td id=\"S5.T1.7.3.2.2\" class=\"ltx_td ltx_align_center\">0.138</td>\n<td id=\"S5.T1.7.3.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T1.7.3.2.3.1\" class=\"ltx_text ltx_font_bold\">0.188</span></td>\n<td id=\"S5.T1.7.3.2.4\" class=\"ltx_td ltx_align_center\">0.16</td>\n</tr>\n<tr id=\"S5.T1.7.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T1.7.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">CIDEr</th>\n<td id=\"S5.T1.7.4.3.2\" class=\"ltx_td ltx_align_center\">0.091</td>\n<td id=\"S5.T1.7.4.3.3\" class=\"ltx_td ltx_align_center\">0.079</td>\n<td id=\"S5.T1.7.4.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T1.7.4.3.4.1\" class=\"ltx_text ltx_font_bold\">0.172</span></td>\n</tr>\n<tr id=\"S5.T1.7.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T1.7.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">COSINE</th>\n<td id=\"S5.T1.7.5.4.2\" class=\"ltx_td ltx_align_center\">0.113</td>\n<td id=\"S5.T1.7.5.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T1.7.5.4.3.1\" class=\"ltx_text ltx_font_bold\">0.157</span></td>\n<td id=\"S5.T1.7.5.4.4\" class=\"ltx_td ltx_align_center\">0.110</td>\n</tr>\n<tr id=\"S5.T1.7.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T1.7.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"4\"><span id=\"S5.T1.7.6.5.1.1\" class=\"ltx_text\">Contextual</span></th>\n<th id=\"S5.T1.7.6.5.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">BLEU1</th>\n<td id=\"S5.T1.7.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.002</td>\n<td id=\"S5.T1.7.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T1.7.6.5.4.1\" class=\"ltx_text ltx_font_bold\">0.168</span></td>\n<td id=\"S5.T1.7.6.5.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.160</td>\n</tr>\n<tr id=\"S5.T1.7.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T1.7.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">ROUGE</th>\n<td id=\"S5.T1.7.7.6.2\" class=\"ltx_td ltx_align_center\">0.062</td>\n<td id=\"S5.T1.7.7.6.3\" class=\"ltx_td ltx_align_center\">0.178</td>\n<td id=\"S5.T1.7.7.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T1.7.7.6.4.1\" class=\"ltx_text ltx_font_bold\">0.179</span></td>\n</tr>\n<tr id=\"S5.T1.7.8.7\" class=\"ltx_tr\">\n<th id=\"S5.T1.7.8.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">CIDEr</th>\n<td id=\"S5.T1.7.8.7.2\" class=\"ltx_td ltx_align_center\">0.000</td>\n<td id=\"S5.T1.7.8.7.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T1.7.8.7.3.1\" class=\"ltx_text ltx_font_bold\">0.248</span></td>\n<td id=\"S5.T1.7.8.7.4\" class=\"ltx_td ltx_align_center\">0.129</td>\n</tr>\n<tr id=\"S5.T1.7.9.8\" class=\"ltx_tr\">\n<th id=\"S5.T1.7.9.8.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">COSINE</th>\n<td id=\"S5.T1.7.9.8.2\" class=\"ltx_td ltx_align_center\">0.082</td>\n<td id=\"S5.T1.7.9.8.3\" class=\"ltx_td ltx_align_center\">0.218</td>\n<td id=\"S5.T1.7.9.8.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T1.7.9.8.4.1\" class=\"ltx_text ltx_font_bold\">0.324</span></td>\n</tr>\n<tr id=\"S5.T1.7.10.9\" class=\"ltx_tr\">\n<th id=\"S5.T1.7.10.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"4\"><span id=\"S5.T1.7.10.9.1.1\" class=\"ltx_text\">All</span></th>\n<th id=\"S5.T1.7.10.9.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">BLEU1</th>\n<td id=\"S5.T1.7.10.9.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.000</td>\n<td id=\"S5.T1.7.10.9.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.113</td>\n<td id=\"S5.T1.7.10.9.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T1.7.10.9.5.1\" class=\"ltx_text ltx_font_bold\">0.185</span></td>\n</tr>\n<tr id=\"S5.T1.7.11.10\" class=\"ltx_tr\">\n<th id=\"S5.T1.7.11.10.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">ROUGE</th>\n<td id=\"S5.T1.7.11.10.2\" class=\"ltx_td ltx_align_center\">0.053</td>\n<td id=\"S5.T1.7.11.10.3\" class=\"ltx_td ltx_align_center\">0.158</td>\n<td id=\"S5.T1.7.11.10.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T1.7.11.10.4.1\" class=\"ltx_text ltx_font_bold\">0.184</span></td>\n</tr>\n<tr id=\"S5.T1.7.12.11\" class=\"ltx_tr\">\n<th id=\"S5.T1.7.12.11.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">CIDEr</th>\n<td id=\"S5.T1.7.12.11.2\" class=\"ltx_td ltx_align_center\">0.000</td>\n<td id=\"S5.T1.7.12.11.3\" class=\"ltx_td ltx_align_center\">0.016</td>\n<td id=\"S5.T1.7.12.11.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T1.7.12.11.4.1\" class=\"ltx_text ltx_font_bold\">0.098</span></td>\n</tr>\n<tr id=\"S5.T1.7.13.12\" class=\"ltx_tr\">\n<th id=\"S5.T1.7.13.12.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">COSINE</th>\n<td id=\"S5.T1.7.13.12.2\" class=\"ltx_td ltx_align_center\">0.122</td>\n<td id=\"S5.T1.7.13.12.3\" class=\"ltx_td ltx_align_center\">0.253</td>\n<td id=\"S5.T1.7.13.12.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T1.7.13.12.4.1\" class=\"ltx_text ltx_font_bold\">0.341</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "We start by assessing the quality of the captions generated by GPT-3.\nFirst of all, we ask GPT-3 to generate captions with our General approach. In Tab. 1 we compare the captions using as reference visual captions, contextual captions and both. All reference captions are ground truth captions taken from the Artpedia dataset [31].",
            "We then evaluate the method by taking a concatenation of the outputs generated by GPT-3 after being conditioned by different questions related to the image.\nThis obviously introduces a strong bias, given also the fact that questions have been generated from information contained in the captions, but at the same time proves the usefulness of such captions for more advanced applications such as visual question answering. As can be seen in Tab. 1, conditioning GPT-3 with the captions leads to better captions according to most metrics.",
            "In Tab. 1 we also provide a baseline as reference, i.e. the output of the state of the art OFA captioning model [38]. We observe that captions generated by OFA do not align well with the ground truth sentences. We attribute this to a domain shift between the datasets commonly used to train captioning models and descriptions of artworks. In fact, the former are sentences written by non-experts while for applications in cultural heritage a domain knowledge is required. This further motivates the usage of GPT-3, which seems to have integrated sufficient knowledge to articulate complex sentences with a domain specific jargon."
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Experimental results for Visual Question Answering. We compare our approach against VQA-CH [6] to understand whether GPT-3 can replace information sheets for artworks either for visual or contextual questions. We compare two versions of our model, the General version, which produces generic descriptions of artworks and the Question-based version, where prompts are conditioned with the input question to generate more specific descriptions.",
        "table": "<table id=\"S5.T2.5\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T2.5.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.5.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r\"></th>\n<th id=\"S5.T2.5.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">Visual</th>\n<th id=\"S5.T2.5.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">Contextual</th>\n<th id=\"S5.T2.5.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">Accuracy</th>\n<th id=\"S5.T2.5.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">F1 score</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.5.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.5.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">VQA-CH <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">6</a>]</cite>\n</th>\n<td id=\"S5.T2.5.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">✗</td>\n<td id=\"S5.T2.5.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">✓</td>\n<td id=\"S5.T2.5.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.684</td>\n<td id=\"S5.T2.5.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.832</td>\n</tr>\n<tr id=\"S5.T2.5.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.5.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">VQA-CH <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">6</a>]</cite>\n</th>\n<td id=\"S5.T2.5.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">✓</td>\n<td id=\"S5.T2.5.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">✗</td>\n<td id=\"S5.T2.5.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0.176</td>\n<td id=\"S5.T2.5.3.2.5\" class=\"ltx_td ltx_align_center\">0.150</td>\n</tr>\n<tr id=\"S5.T2.5.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T2.5.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">VQA-CH <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">6</a>]</cite>\n</th>\n<td id=\"S5.T2.5.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">✓</td>\n<td id=\"S5.T2.5.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">✓</td>\n<td id=\"S5.T2.5.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0.504</td>\n<td id=\"S5.T2.5.4.3.5\" class=\"ltx_td ltx_align_center\">0.417</td>\n</tr>\n<tr id=\"S5.T2.5.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T2.5.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Ours - General</th>\n<td id=\"S5.T2.5.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">✗</td>\n<td id=\"S5.T2.5.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">✓</td>\n<td id=\"S5.T2.5.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.557</td>\n<td id=\"S5.T2.5.5.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.719</td>\n</tr>\n<tr id=\"S5.T2.5.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T2.5.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Ours - General</th>\n<td id=\"S5.T2.5.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\">✓</td>\n<td id=\"S5.T2.5.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\">✗</td>\n<td id=\"S5.T2.5.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0.070</td>\n<td id=\"S5.T2.5.6.5.5\" class=\"ltx_td ltx_align_center\">0.055</td>\n</tr>\n<tr id=\"S5.T2.5.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T2.5.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Ours - General</th>\n<td id=\"S5.T2.5.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\">✓</td>\n<td id=\"S5.T2.5.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\">✓</td>\n<td id=\"S5.T2.5.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0.239</td>\n<td id=\"S5.T2.5.7.6.5\" class=\"ltx_td ltx_align_center\">0.360</td>\n</tr>\n<tr id=\"S5.T2.5.8.7\" class=\"ltx_tr\">\n<th id=\"S5.T2.5.8.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Ours - Question-based</th>\n<td id=\"S5.T2.5.8.7.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">✗</td>\n<td id=\"S5.T2.5.8.7.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">✓</td>\n<td id=\"S5.T2.5.8.7.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.473</td>\n<td id=\"S5.T2.5.8.7.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.602</td>\n</tr>\n<tr id=\"S5.T2.5.9.8\" class=\"ltx_tr\">\n<th id=\"S5.T2.5.9.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Ours - Question-based</th>\n<td id=\"S5.T2.5.9.8.2\" class=\"ltx_td ltx_align_center ltx_border_r\">✓</td>\n<td id=\"S5.T2.5.9.8.3\" class=\"ltx_td ltx_align_center ltx_border_r\">✗</td>\n<td id=\"S5.T2.5.9.8.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0.134</td>\n<td id=\"S5.T2.5.9.8.5\" class=\"ltx_td ltx_align_center\">0.202</td>\n</tr>\n<tr id=\"S5.T2.5.10.9\" class=\"ltx_tr\">\n<th id=\"S5.T2.5.10.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Ours - Question-based</th>\n<td id=\"S5.T2.5.10.9.2\" class=\"ltx_td ltx_align_center ltx_border_r\">✓</td>\n<td id=\"S5.T2.5.10.9.3\" class=\"ltx_td ltx_align_center ltx_border_r\">✓</td>\n<td id=\"S5.T2.5.10.9.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0.256</td>\n<td id=\"S5.T2.5.10.9.5\" class=\"ltx_td ltx_align_center\">0.330</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "To evaluate the Visual Question Answering capabilities of our proposed method, we follow the setting of [6]. However, we do not rely on any vision-based model but rather on a fully textual question answering model based on DistilBert [27], as explained in Sec. 4.\nIn Tab. 2, we compare our approach to the one of VQA-CH [6]. It has to be noted that, contrary to [6], we do not rely on real textual descriptions, which are known to contain the answer, but we only extract information from GPT-3. This is a strong disadvantage for our method. However, we are not interested in obtaining better results than VQA-CH, but rather our goal is to demonstrate if GPT-3 can act as a substitute of textual descriptions handcrafted by domain experts.",
            "To overcome this limitation, we test the model using captions generated with out Question-based approach. By feeding the answer to GPT-3 along with the title of the artwork, the model is able to generate more specific captions. Such captions, as explained in Sec. 6 are usually shorter but are focused on the prompt. This is particularly interesting since it means that a purely text-based model is capable of addressing a vision-based task. In Tab. 2 it can be seen that for visual questions alone, our method with question-based captions performs on par or better than the vision-based VQA-CH model."
        ]
    }
}