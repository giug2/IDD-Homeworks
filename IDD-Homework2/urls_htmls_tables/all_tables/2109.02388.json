{
    "PAPER'S NUMBER OF TABLES": 1,
    "S3.T1": {
        "caption": "Table 1: Definition of studied algorithms by used local optimization algorithms and server updates and resulting properties of these algorithms. New methods proposed by us are marked with *. Communication rounds are counted per parameter update on the server.",
        "table": "<table id=\"S3.T1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.2.1.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T1.2.1.1.1.1\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"S3.T1.2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">definition</td>\n<td id=\"S3.T1.2.1.1.1.3\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"S3.T1.2.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\">properties</td>\n</tr>\n<tr id=\"S3.T1.2.1.2.2\" class=\"ltx_tr\">\n<td id=\"S3.T1.2.1.2.2.1\" class=\"ltx_td\"></td>\n<td id=\"S3.T1.2.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">local</td>\n<td id=\"S3.T1.2.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">server</td>\n<td id=\"S3.T1.2.1.2.2.4\" class=\"ltx_td\"></td>\n<td id=\"S3.T1.2.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">global</td>\n<td id=\"S3.T1.2.1.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\">global</td>\n<td id=\"S3.T1.2.1.2.2.7\" class=\"ltx_td ltx_align_center ltx_border_t\">local</td>\n<td id=\"S3.T1.2.1.2.2.8\" class=\"ltx_td ltx_align_center ltx_border_t\">#comm.</td>\n</tr>\n<tr id=\"S3.T1.2.1.3.3\" class=\"ltx_tr\">\n<td id=\"S3.T1.2.1.3.3.1\" class=\"ltx_td\"></td>\n<td id=\"S3.T1.2.1.3.3.2\" class=\"ltx_td ltx_align_center\">optimization</td>\n<td id=\"S3.T1.2.1.3.3.3\" class=\"ltx_td ltx_align_center\">update</td>\n<td id=\"S3.T1.2.1.3.3.4\" class=\"ltx_td\"></td>\n<td id=\"S3.T1.2.1.3.3.5\" class=\"ltx_td ltx_align_center\">gradient</td>\n<td id=\"S3.T1.2.1.3.3.6\" class=\"ltx_td ltx_align_center\">line search</td>\n<td id=\"S3.T1.2.1.3.3.7\" class=\"ltx_td ltx_align_center\">steps</td>\n<td id=\"S3.T1.2.1.3.3.8\" class=\"ltx_td ltx_align_center\">rounds</td>\n</tr>\n<tr id=\"S3.T1.2.1.4.4\" class=\"ltx_tr\">\n<td id=\"S3.T1.2.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_border_t\">GIANT <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_text\" style=\"font-size:90%;\">Wang et al.</span>, <a href=\"#bib.bib29\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">2018</span></a>)</cite>\n</td>\n<td id=\"S3.T1.2.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\">Alg. <a href=\"#alg2\" title=\"Algorithm 2 ‣ On Second-order Optimization Methods for Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>\n</td>\n<td id=\"S3.T1.2.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\">Alg. <a href=\"#alg7\" title=\"Algorithm 7 ‣ On Second-order Optimization Methods for Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>\n</td>\n<td id=\"S3.T1.2.1.4.4.4\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S3.T1.2.1.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\">yes</td>\n<td id=\"S3.T1.2.1.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\">yes</td>\n<td id=\"S3.T1.2.1.4.4.7\" class=\"ltx_td ltx_align_center ltx_border_t\">no</td>\n<td id=\"S3.T1.2.1.4.4.8\" class=\"ltx_td ltx_align_center ltx_border_t\">3</td>\n</tr>\n<tr id=\"S3.T1.2.1.5.5\" class=\"ltx_tr\">\n<td id=\"S3.T1.2.1.5.5.1\" class=\"ltx_td ltx_align_left\">GIANT with local steps and global line search<sup id=\"S3.T1.2.1.5.5.1.1\" class=\"ltx_sup\">*</sup>\n</td>\n<td id=\"S3.T1.2.1.5.5.2\" class=\"ltx_td ltx_align_center\">Alg. <a href=\"#alg3\" title=\"Algorithm 3 ‣ On Second-order Optimization Methods for Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>\n</td>\n<td id=\"S3.T1.2.1.5.5.3\" class=\"ltx_td ltx_align_center\">Alg. <a href=\"#alg7\" title=\"Algorithm 7 ‣ On Second-order Optimization Methods for Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>\n</td>\n<td id=\"S3.T1.2.1.5.5.4\" class=\"ltx_td\"></td>\n<td id=\"S3.T1.2.1.5.5.5\" class=\"ltx_td ltx_align_center\">yes</td>\n<td id=\"S3.T1.2.1.5.5.6\" class=\"ltx_td ltx_align_center\">yes</td>\n<td id=\"S3.T1.2.1.5.5.7\" class=\"ltx_td ltx_align_center\">yes</td>\n<td id=\"S3.T1.2.1.5.5.8\" class=\"ltx_td ltx_align_center\">3</td>\n</tr>\n<tr id=\"S3.T1.2.1.6.6\" class=\"ltx_tr\">\n<td id=\"S3.T1.2.1.6.6.1\" class=\"ltx_td ltx_align_left\">GIANT with local steps and local line search<sup id=\"S3.T1.2.1.6.6.1.1\" class=\"ltx_sup\">*</sup>\n</td>\n<td id=\"S3.T1.2.1.6.6.2\" class=\"ltx_td ltx_align_center\">Alg. <a href=\"#alg4\" title=\"Algorithm 4 ‣ On Second-order Optimization Methods for Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>\n</td>\n<td id=\"S3.T1.2.1.6.6.3\" class=\"ltx_td ltx_align_center\">Alg. <a href=\"#alg8\" title=\"Algorithm 8 ‣ On Second-order Optimization Methods for Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>\n</td>\n<td id=\"S3.T1.2.1.6.6.4\" class=\"ltx_td\"></td>\n<td id=\"S3.T1.2.1.6.6.5\" class=\"ltx_td ltx_align_center\">yes</td>\n<td id=\"S3.T1.2.1.6.6.6\" class=\"ltx_td ltx_align_center\">no</td>\n<td id=\"S3.T1.2.1.6.6.7\" class=\"ltx_td ltx_align_center\">yes</td>\n<td id=\"S3.T1.2.1.6.6.8\" class=\"ltx_td ltx_align_center\">2</td>\n</tr>\n<tr id=\"S3.T1.2.1.7.7\" class=\"ltx_tr\">\n<td id=\"S3.T1.2.1.7.7.1\" class=\"ltx_td ltx_align_left\">LocalNewton with global line search<sup id=\"S3.T1.2.1.7.7.1.1\" class=\"ltx_sup\">*</sup>\n</td>\n<td id=\"S3.T1.2.1.7.7.2\" class=\"ltx_td ltx_align_center\">Alg. <a href=\"#alg5\" title=\"Algorithm 5 ‣ On Second-order Optimization Methods for Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>\n</td>\n<td id=\"S3.T1.2.1.7.7.3\" class=\"ltx_td ltx_align_center\">Alg. <a href=\"#alg9\" title=\"Algorithm 9 ‣ On Second-order Optimization Methods for Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>\n</td>\n<td id=\"S3.T1.2.1.7.7.4\" class=\"ltx_td\"></td>\n<td id=\"S3.T1.2.1.7.7.5\" class=\"ltx_td ltx_align_center\">no</td>\n<td id=\"S3.T1.2.1.7.7.6\" class=\"ltx_td ltx_align_center\">yes</td>\n<td id=\"S3.T1.2.1.7.7.7\" class=\"ltx_td ltx_align_center\">yes</td>\n<td id=\"S3.T1.2.1.7.7.8\" class=\"ltx_td ltx_align_center\">2</td>\n</tr>\n<tr id=\"S3.T1.2.1.8.8\" class=\"ltx_tr\">\n<td id=\"S3.T1.2.1.8.8.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">LocalNewton <cite class=\"ltx_cite ltx_citemacro_citep\">(<span class=\"ltx_text\" style=\"font-size:90%;\">Gupta et al.</span>, <a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\"><span class=\"ltx_text\" style=\"font-size:90%;\">2021</span></a>)</cite>\n</td>\n<td id=\"S3.T1.2.1.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">Alg. <a href=\"#alg6\" title=\"Algorithm 6 ‣ On Second-order Optimization Methods for Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>\n</td>\n<td id=\"S3.T1.2.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">Alg. <a href=\"#alg8\" title=\"Algorithm 8 ‣ On Second-order Optimization Methods for Federated Learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>\n</td>\n<td id=\"S3.T1.2.1.8.8.4\" class=\"ltx_td ltx_border_bb\"></td>\n<td id=\"S3.T1.2.1.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">no</td>\n<td id=\"S3.T1.2.1.8.8.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">no</td>\n<td id=\"S3.T1.2.1.8.8.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">yes</td>\n<td id=\"S3.T1.2.1.8.8.8\" class=\"ltx_td ltx_align_center ltx_border_bb\">1</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Each of the possible algorithms (See Table 1 and Alg. 1) can then either use the global gradient ∇ft​(𝐰)=1|𝒮t|​∑i∈𝒮tfi​(𝐰)∇subscript𝑓𝑡𝐰1subscript𝒮𝑡subscript𝑖subscript𝒮𝑡subscript𝑓𝑖𝐰\\nabla f_{t}(\\mathbf{w})=\\frac{1}{|\\mathcal{S}_{t}|}\\sum_{i\\in\\mathcal{S}_{t}}f_{i}(\\mathbf{w}) or only the local gradient ∇fi​(𝐰)∇subscript𝑓𝑖𝐰\\nabla f_{i}(\\mathbf{w}), either use a global line search over Eq. (2) or a local line search only over Eq. (3) and all of our proposed variants use local steps.\nThe global gradient and global line search each need one more communication round than their local counterparts but lead to the inclusion of global information in the update process.\nGIANT with local steps and global line search and LocalNewton with global line search introduce an additional step size parameter for the local steps.\nThe local steps are performed with this additional step size parameter and the global line search is performed over the resulting update from multiple local steps (ui=𝐰lt−𝐰0tsubscript𝑢𝑖superscriptsubscript𝐰𝑙𝑡superscriptsubscript𝐰0𝑡u_{i}=\\mathbf{w}_{l}^{t}-\\mathbf{w}_{0}^{t}).\nWe select a new active subset of clients for the global line search of LocalNewton with global line search similar to the sampling of a new minibatch proposed for the adaption of the Armijo line-search (Armijo, 1966) in a stochastic setting (Vaswani et al., 2019).\nLocalNewton with global line search does not use the global gradient for the update calculation and we would need to calculate it only for the backtracking line search.\nThis could be done in parallel to the update computations and is therefore possibly not as expensive as for methods needing it for the update.\nInstead, we choose the step size using arg⁡minγ∈γ1,…,γl​∑i∈𝒮tfi​(w+γi​u)subscript𝛾subscript𝛾1…subscript𝛾𝑙subscript𝑖subscript𝒮𝑡subscript𝑓𝑖𝑤subscript𝛾𝑖𝑢\\arg\\min_{\\gamma\\in\\gamma_{1},\\ldots,\\gamma_{l}}\\sum_{i\\in\\mathcal{S}_{t}}f_{i}(w+\\gamma_{i}u) which can potentially choose a bigger step size than original backtracking line search would but the backtracking line search variant of Wang et al. (2018) with a fixed set of step sizes has the same problem.\nBoth variants of GIANT with local steps can use the real global gradient only in the first local step. Afterwards, they can update the global gradient only by their local gradient with 𝐠j+1=𝐠j−1|𝒮t|​∇fi​(𝐰jt)+1|𝒮t|​∇fi​(𝐰j+1t)subscript𝐠𝑗1subscript𝐠𝑗1subscript𝒮𝑡∇subscript𝑓𝑖superscriptsubscript𝐰𝑗𝑡1subscript𝒮𝑡∇subscript𝑓𝑖superscriptsubscript𝐰𝑗1𝑡\\mathbf{g}_{j+1}=\\mathbf{g}_{j}-\\frac{1}{|\\mathcal{S}_{t}|}\\nabla f_{i}(\\mathbf{w}_{j}^{t})+\\frac{1}{|\\mathcal{S}_{t}|}\\nabla f_{i}(\\mathbf{w}_{j+1}^{t}) as calculating the global gradient at the new parameters 𝐰it+1superscriptsubscript𝐰𝑖𝑡1\\mathbf{w}_{i}^{t+1} would need at least one additional communication round.",
            "Results\nFig. 1(a) and Fig. 2(a) show the advantage of adding local steps to GIANT.\nThe local steps allow the method to make more progress in one communication round and allow to choose multiple steps with a smaller stepsize when a single step with a higher step size would be too noisy.\nThe difference between Local Newton and Local Newton with global line search on an i.i.d. setting like in Fig. 1(a) is minimal but the global line search definitely helps in a non i.i.d. setting like in Fig. 1(b).\nUsing a local line search for GIANT and therefore saving one communication round is not working as can be seen in Fig. 1(a) and Fig. 2(a).\nThis method fails in nearly all experiments.\nTable 1 raises the question if one should invest the second communication round into a global gradient (GIANT with local steps and local line search) or a global line search (LocalNewton with global line search) and our experiments indicate that LocalNewton with global line search is advantageous (Fig. 1(a) and 2(c)).\nFigure 1(b) shows that the second order methods except for LocalNewton with global line search struggle with the non-i.i.d setup with client-specific means.\nThe GIANT variants with global line search are choosing steps which do not improve the overall loss which is not prevented by using another set of clients for the global line search (not shown here). The two methods with only local line search already choose a too specific first update which results in them diverging.\nLocalNewton with global line search shows among the second-order methods the best performance overall considering the number of used communication rounds.\nFigure 2(d) shows the competitiveness of Federated Averaging also in the cross-device setting when all methods have the same gradient evaluation budget as discussed in Section 1.\nAn interesting empirical observation is that the conjugate gradient methods needs an increasing number of iterations after each update step to converge to a given tolerance.\nThis makes a fair comparison with first-order methods more difficult."
        ]
    }
}