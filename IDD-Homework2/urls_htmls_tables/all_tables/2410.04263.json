{
    "id_table_1": {
        "caption": "Table 1:  Graph generation performance on the synthetic datasets: Planar, Tree and SBM. We present the results from five sampling runs, each generating 40 graphs, reported as the mean  standard deviation. Full version in  Tab.   7 .",
        "table": "S6.T1.6.6",
        "footnotes": [
            ""
        ],
        "references": [
            "In this section, we introduce the DFM framework as originally proposed by  Campbell et al. ( 2024 ) . We adapt their notation for clarity, and illustrate it under the graph setting in  Figure   1 .",
            "Since we aim to reverse a  z 1 subscript z 1 z_{1} italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT -conditional noising process  p t | 1 (  | z 1 )   Z  1 p_{t|1}(\\cdot|z_{1})\\in\\Delta^{Z-1} italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT (  | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT )  roman_ start_POSTSUPERSCRIPT italic_Z - 1 end_POSTSUPERSCRIPT  (recall  Eq.   1 ), DFM instead considers a  z 1 subscript z 1 z_{1} italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT -conditional rate matrix,  R t (  ,  | z 1 )  R Z  Z R_{t}(\\cdot\\,,\\cdot|z_{1})\\in\\mathbb{R}^{Z\\times Z} italic_R start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT (  ,  | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT )  blackboard_R start_POSTSUPERSCRIPT italic_Z  italic_Z end_POSTSUPERSCRIPT  that generates it. Under mild assumptions,  Campbell et al. ( 2024 )  present a closed-form for a valid conditional rate matrix, i.e., a matrix that verifies the corresponding Kolmogorov equation, defined as:",
            "We jointly model  D D D italic_D  variables,  ( z 1 , ... , z D ) = z 1 : D  Z D superscript z 1 ... superscript z D superscript z : 1 D superscript Z D (z^{1},\\ldots,z^{D})=z^{1:D}\\in\\mathcal{Z}^{D} ( italic_z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , ... , italic_z start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ) = italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT  caligraphic_Z start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT . The noising process defined in  Eq.   1  is performed independently for each variable, i.e.,  p t | 1  ( z t 1 : D | z 1 1 : D ) =  d = 1 D p t | 1 d  ( z t d | z 1 d ) subscript p conditional t 1 conditional subscript superscript z : 1 D t subscript superscript z : 1 D 1 subscript superscript product D d 1 subscript superscript p d conditional t 1 conditional subscript superscript z d t subscript superscript z d 1 p_{t|1}(z^{1:D}_{t}|z^{1:D}_{1})=\\prod^{D}_{d=1}p^{d}_{t|1}(z^{d}_{t}|z^{d}_{1}) italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) =  start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_d = 1 end_POSTSUBSCRIPT italic_p start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) . For the denoising process,  Campbell et al. ( 2024 )  propose the utilization of an efficient approximation for CTMC simulation on multivariate non-ordinal data, where an Euler step is applied independently to each dimension  d d d italic_d , with a finite time step    t  t \\Delta t roman_ italic_t :",
            "The resulting training and sampling processes are detailed in  Algs.   1  and  2 .",
            "We now expose DeFoGs equivariance properties in  Lemma   1 . (See  Sec.   C.2.2  for proof.)",
            "Empirically, the marginal distribution outperforms the others with a single exception, as detailed further in  Sec.   B.1 . This is supported by two key observations: first,  Vignac et al. ( 2022 )  demonstrate that, for any given distribution, the closest distribution within the aforementioned class of factorizable initial distributions is the marginal one, thus illustrating its optimality as a prior. Second, the marginal initial distribution preserves the datasets marginal properties throughout the noising process, maintaining graph-theoretical characteristics like sparsity  (Qin et al.,  2023 ) . We conjecture that this fact facilitates the denoising task for the graph transformer.",
            "From  Alg.   1 , line 4, vanilla DeFoG samples  t t t italic_t  from a uniform distribution. However, in the same vein as adjusting the noise schedule in diffusion models, we can modify this distribution to allow the model to refine its predictions differently across various time regions. We implement this procedure by sampling  t t t italic_t  from a uniform distribution and applying a  distortion function   f f f italic_f  such that  t  = f  ( t ) superscript t  f t t^{\\prime}=f(t) italic_t start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = italic_f ( italic_t ) . The specific distortions used and their corresponding distributions for  t  superscript t  t^{\\prime} italic_t start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , are detailed in  Sec.   A.1 . In contrast to prior findings in image generation, which suggest that focusing on intermediate time regions is preferable  (Esser et al.,  2024 ) , we observe that for most graph generation tasks, the best performing distortion functions particularly emphasize  t t t italic_t  approaching  1 1 1 1 . Our key insight is that, as  t t t italic_t  approaches 1, discrete structures undergo abrupt transitions between states  from 0 to 1 in a one-hot encoding  rather than the smooth, continuous refinements seen in continuous domains. Therefore, later time steps are critical for detecting errors, such as edges breaking planarity or atoms violating molecular valency rules.",
            "In DeFoGs vanilla sampling process, the discretization is performed using equally sized time steps ( Alg.   2 , line 5). Instead, we propose employing variable step sizes. This adjustment is motivated by the need for more fine-grained control during certain time intervals, for instance, to ensure that cluster structures are properly formed before focusing on intra-cluster refinements, or to prevent edge alterations that could compromise global properties once the overall structure is established. By allocating smaller, more frequent steps to these critical intervals, the generated graph can better capture the true properties of the data. To achieve this, we modify the evenly spaced step sizes using distortion functions (see  Sec.   A.1 ). Although the optimal distortion function is highly dataset-dependent, in  Sec.   B.2  we propose a method to guide the selection of the distortion function based on the observed training dynamics. Notably, in diffusion models, the design for diffusion steps such as noise schedule is typically identical for training and sampling. However, in flow models, the time distribution for training detailed in  Sec.   4.2.1  and the time steps used for sampling can be more flexibly disentangled. In practice, applying distortion only during sampling already yields notable performance improvements.",
            "with    R +  superscript R \\omega\\in\\mathbb{R}^{+} italic_  blackboard_R start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT . This adjustment biases the transitions toward the clean data state  z 1 subscript z 1 z_{1} italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT .  Lemma   10 , in  Sec.   A.2 , demonstrates that this modification introduces an  O  (  ) O  O(\\omega) italic_O ( italic_ )  violation of the Kolmogorov equation. Consequently, choosing a small value of    \\omega italic_  is experimentally shown to be highly beneficial, while a larger    \\omega italic_  restricts the distribution to regions of high probability, increasing the distance between the generated data and the training data, as indicated in  Sec.   A.4 ,  Figure   11 .",
            "The space of valid rate matrices, i.e., those that satisfy the Kolmogorov equation, is not exhausted by the original formulation of  R t   ( z t , z t + d  t | z 1 ) subscript superscript R t subscript z t conditional subscript z t d t subscript z 1 R^{*}_{t}{(z_{t},z_{t+\\mathrm{d}t}|z_{1})} italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) .  Campbell et al. ( 2024 )  investigate this and show that for any rate matrix  R t DB subscript superscript R DB t R^{\\text{DB}}_{t} italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  that satisfies the detailed balance condition,  p t | 1  ( z t | z 1 )  R t DB  ( z t , z t + d  t | z 1 ) = p t | 1  ( z t + d  t | z 1 )  R t DB  ( z t + d  t , z t | z 1 ) subscript p conditional t 1 conditional subscript z t subscript z 1 subscript superscript R DB t subscript z t conditional subscript z t d t subscript z 1 subscript p conditional t 1 conditional subscript z t d t subscript z 1 subscript superscript R DB t subscript z t d t conditional subscript z t subscript z 1 p_{t|1}(z_{t}|z_{1})R^{\\text{DB}}_{t}(z_{t},z_{t+\\text{d}t}|z_{1})=p_{t|1}(z_{% t+\\text{d}t}|z_{1})R^{\\text{DB}}_{t}(z_{t+\\text{d}t},z_{t}|z_{1}) italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_t + d italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) = italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t + d italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t + d italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , the modified rate matrix  R t  = R t  +   R t DB subscript superscript R  t subscript superscript R t  subscript superscript R DB t R^{\\eta}_{t}=R^{*}_{t}+\\eta R^{\\text{DB}}_{t} italic_R start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_ italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , with    R +  superscript R \\eta\\in\\mathbb{R}^{+} italic_  blackboard_R start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT , also satisfies the Kolmogorov equation. Increasing    \\eta italic_  introduces more stochasticity into the trajectory of the denoising process, while different designs of  R t D  B subscript superscript R D B t R^{DB}_{t} italic_R start_POSTSUPERSCRIPT italic_D italic_B end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  encode different priors for preferred transitions between states. This mechanism can be interpreted as a correction mechanism, as it enables transitions back to states that would otherwise be disallowed according to the rate matrix formulation, as described in  Sec.   A.5 . The effect of    \\eta italic_  across different datasets is illustrated in detail in  Figure   12 ,  Sec.   A.4 . To further improve the sampling performance of DeFoG, we also investigate the different formulations of  R DB superscript R DB R^{\\text{DB}} italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT  under the detailed balance condition. Additional details and discussions are provided in  Sec.   A.3 .",
            "In this section, we present novel theoretical results on general multivariate data, which are naturally extendable to our graph-based framework, as introduced in  Sec.   4.1 . Their complete versions and proofs are available in  Sec.   C.1 . We begin by presenting a theoretical result that further justifies the design choice for the loss function of DFM, and thus of DeFoG.",
            "We evaluate DeFoG using the widely adopted  Planar ,  SBM   (Martinkus et al.,  2022 ) , and  Tree  datasets  (Bergmeister et al.,  2023 ) , along with the associated evaluation methodology. In  Tab.   1 , we report the proportion of generated graphs that are valid, unique, and novel (V.U.N.), as well as the average ratio of distances between graph statistics of the generated and test sets relative to the train and test sets (Ratio) to assess sample quality.",
            "As shown in  Tab.   1 , for the Planar dataset, DeFoG achieves the best performance across both metrics. On the Tree dataset, it is only surpassed by HSpectre, which leverages a local expansion procedure particularly well-suited to hierarchical structures like trees. On the SBM dataset, DeFoG attains the highest V.U.N. score and an average ratio close to the optimal. Notably, across all datasets, DeFoG secures second place in 4 out of 6 cases while using only 5% of the sampling steps.",
            "Molecular design is a prominent real-world application of graph generation. We evaluate DeFoGs performance on this task using the QM9  (Wu et al.,  2018 ) , MOSES  (Polykovskiy et al.,  2020 ) , and Guacamol  (Brown et al.,  2019 )  datasets. For QM9, we follow the dataset split and evaluation metrics from  Vignac et al. ( 2022 ) , presenting the results in  Sec.   E.1.2 ,  Tab.   8 . For the larger MOSES and Guacamol datasets, we adhere to the training setup and evaluation metrics established by  Polykovskiy et al. ( 2020 )  and  Brown et al. ( 2019 ) , respectively, with results in  Tabs.   9  and  10 .",
            "As highlighted in  Tabs.   1 ,  2  and  8 , DeFoG attains similar performance, and in some cases even outperforms, several diffusion-based methods with just 5% or 10% of their sampling steps. This efficiency is a result of DeFoGs sampling stage flexibility, whose optimization is enabled due to the flexible and disentangled training-sampling procedure within DFM. To further demonstrate the advantages of sampling optimization,  Figure   3(a)  shows the cumulative effect of each optimization step discussed in  Sec.   4.2.2 . As illustrated, starting from a vanilla DeFoG model, which initially performs slightly worse than DiGress, we sequentially incorporate algorithmic improvements, with each stage indicated by an increasing number of  + + +  symbols. Each addition leads to a progressive improvement over the previous stage across both the Planar and QM9 datasets under various numbers of steps for sampling, with the final performance significantly surpassing the vanilla model using only 50 steps on the Planar dataset. While the benefits of each optimization component may vary across datasets, the sampling optimization pipeline remains highly efficient, enabling quick hyperparameter exploration without requiring retraining, which is a significantly more resource-intensive stage (see  Sec.   E.2 ). We provide more details on the impact of each sampling optimization strategy across datasets in  Sec.   A.4 .",
            "While our primary focus is on optimizing the sampling process given its computational efficiency, we emphasize that DeFoGs training optimization strategies can further push its performance. To illustrate this,  Figure   3(b)  shows the convergence curves for the tree and MOSES datasets. We first observe that, with the same model, leveraging only sampling distortion can further enhance its performance beyond the vanilla implementation. This suggests that optimizing the sampling procedure can be particularly useful in settings where computational resources are limited and the model is undertrained, as detailed in  Sec.   A.6 . Moreover, when an appropriate sample distortion is known (e.g.,  polydec  distortion is shown to be particularly preferred across molecular datasets), applying it to both train distortion and sample distortion typically further improves performance. Although DeFoG achieves faster convergence with both distortions, we remark that DiGress also shows good convergence speed on the Tree dataset, due to the well-tuned noise schedule employed for joint training and sampling. The mutual impact of training and sampling distortion is discussed in  Sec.   B.2 . Besides, in the same section, we discuss a heuristic on which distortion to use for each dataset according to DeFoGs vanilla dynamics. Further details on how the initial distribution affects training efficiency are provided in  Sec.   B.1 .",
            "In this section, we explore the proposed sampling optimization in more detail. We start by analysing the different time distortion functions in  Sec.   A.1 . Next, in  Sec.   A.2 , we prove that the proposed target guidance mechanism actually satisfies the Kolmogorov equation, thus yielding valid rate matrices and, in  Sec.   A.3 , we provide more details about the detailed balance equation and how it widens the design space of rate matrices. In  Sec.   A.4 , we also describe the adopted sampling optimization pipeline. Finally, in  Sec.   A.5 , we provide more details to better clarify the dynamics of the sampling process.",
            "In this section, we demonstrate that the proposed  target guidance  design for the rate matrices violates the Kolmogorov equation with an error that is linear in    \\omega italic_ . This result indicates that a small guidance factor effectively helps fit the distribution, whereas a larger guidance factor, as shown in  Figure   11 , while enhancing topological properties such as planarity, increases the distance between generated and training data on synthetic datasets according to the metrics of average ratio. Similarly, for molecular datasets, this also leads to an increase in validity and a decrease in novelty by forcing the generated data to closely resemble the training data.",
            "We denote by RHS and LHS the right-hand side and left-hand side, respectively, of  Eq.   11 . For the case in which  p t | 1  ( z t | z 1 ) > 0 subscript p conditional t 1 conditional subscript z t subscript z 1 0 p_{t|1}(z_{t}|z_{1})>0 italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) > 0 , we have:",
            "Campbell et al. ( 2024 )  show that although their  z 1 subscript z 1 z_{1} italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT -conditional formulation of  R   t superscript R t R^{*}t italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT italic_t  generates  p  t | 1 conditional p t 1 p{t|1} italic_p italic_t | 1 , it does not span the full space of valid rate matrices  those that satisfy the conditional Kolmogorov equation ( Eq.   11 ). They derive sufficient conditions for identifying other valid rate matrices. Notably, they demonstrate that matrices of the form",
            "still satisfy the Kolmogorov equation. The detailed balance condition ensures that the outflow,  p t | 1  ( z t | z 1 )  R t DB  ( z t , z t + d  t | z 1 ) subscript p conditional t 1 conditional subscript z t subscript z 1 subscript superscript R DB t subscript z t conditional subscript z t d t subscript z 1 p_{t|1}(z_{t}|z_{1})R^{\\text{DB}}_{t}(z_{t},z_{t+\\text{d}t}|z_{1}) italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_t + d italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , and inflow,  p t | 1  ( z t + d  t | z 1 )  R t DB  ( z t + d  t , z t | z 1 ) subscript p conditional t 1 conditional subscript z t d t subscript z 1 subscript superscript R DB t subscript z t d t conditional subscript z t subscript z 1 p_{t|1}(z_{t+\\text{d}t}|z_{1})R^{\\text{DB}}_{t}(z_{t+\\text{d}t},z_{t}|z_{1}) italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t + d italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t + d italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , of probability mass to any given state are perfectly balanced. Under these conditions, this additive components contribution to the Kolmogorov equation becomes null (similar to the target guidance, as shown in the proof of of  Lemma   10 , in  Sec.   A.2 ).",
            "A natural question is how to choose a suitable design for  R t DB subscript superscript R DB t R^{\\text{DB}}_{t} italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  from the infinite space of detailed balance rate matrices. As depicted in  Figure   5 , this flexibility can be leveraged to incorporate priors into the denoising model by encouraging specific transitions between states. By adjusting the sparsity of the matrix entries, additional transitions beyond those prescribed by  R t  subscript superscript R t R^{*}_{t} italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  can be introduced. In the general case, transitions between all states are possible; in the column case, a specific state centralizes all potential transitions; and in the single-entry case, only transitions between two states are permitted. These examples merely illustrate some possibilities and do not exhaust the range of potential  R t DB subscript superscript R DB t R^{\\text{DB}}_{t} italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  designs. The matrix entries can be structured by considering the following reorganization of terms of  Eq.   12 :",
            "Orthogonal to the design of  R t DB subscript superscript R DB t R^{\\text{DB}}_{t} italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , we must also consider the hyperparameter    \\eta italic_ , which regulates the magnitude of stochasticity in the denoising process. Specifically, setting   = 0  0 \\eta=0 italic_ = 0  (thereby relying solely on  R t  subscript superscript R t R^{*}_{t} italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) minimizes the expected number of jumps throughout the denoising trajectory under certain conditions, as shown by  Campbell et al. ( 2024 )  in Proposition 3.4. However, in continuous diffusion models, some level of stochasticity has been demonstrated to enhance performance  (Karras et al.,  2022 ; Cao et al.,  2023 ; Xu et al.,  2023 ) . Conversely, excessive stochasticity can negatively impact performance.  Campbell et al. ( 2024 )  propose that there exists an optimal level of stochasticity that strikes a balance between exploration and accuracy. In our experiments, we observed varied behaviors as    \\eta italic_  increases, resulting in different performance outcomes across datasets, as illustrated in  Figure   12 .",
            "To better mode detailedly illustrate the influence of each sampling optimization, we show in  Figures   8  and  10  in present the impact of varying parameter values across the synthetic datasets  Figure   8  and molecular datasets in Figure  10  used in this work.",
            "While Figures  8  and  10  are highly condensed, we provide a more fine-grained version that specifically illustrates the influence of the hyperparameters    \\eta italic_  and    \\omega italic_ . This version highlights their impact when generating with the full number of steps (500 and 1000 for molecular and synthetic data, respectively) and with 50 steps. As emphasized in Figures  11  and  12 , the influence of these hyperparameters varies across datasets and exhibits distinct behaviors depending on the number of steps used.",
            "To demonstrate the benefit of each designed optimization step, we report the step-wise improvements by sequentially adding each tuned step across the primary datasets  synthetic datasets in Figure  14  and molecular datasets in Figure  16   used in this work.",
            "derived by directly differentiating  Eq.   1 . Based on this, the possible values of  R t  subscript superscript R t R^{*}_{t} italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  for different combinations of  z t subscript z t z_{t} italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and  z t + d  t subscript z t d t z_{t+\\mathrm{d}t} italic_z start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT  are outlined in  Tab.   4 .",
            "From the first two lines of  Tab.   4 , we observe that once the system reaches the predicted state  z 1 subscript z 1 z_{1} italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , it remains there. If not,  R t  subscript superscript R t R^{*}_{t} italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  only encourages transitions to other states under two conditions: either the target state is  z 1 subscript z 1 z_{1} italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  (third line), or the corresponding entries in the initial distribution for potential next states have smaller values than the current state (fourth line). As a result, the sampling dynamics are heavily influenced by the initial distribution, as discussed further in  Sec.   B.1 .",
            "In this section, we present the performance of a model trained on the QM9 dataset and the Planar dataset using only  30 % percent 30 30\\% 30 %  of the epochs compared to the final model being reported. We employ the same hyperparameters with  Tab.   8  and  Tab.   1  for the sampling setup, as reported in  Tab.   6 .",
            "In this section, we provide a more detailed analysis of the influence of the various training optimization strategies introduced in  Sec.   4.2.1 . In  Sec.   B.1 , we empirically demonstrate the impact of selecting different initial distributions on performance, while in  Sec.   B.2 , we examine the interaction between training and sampling optimization.",
            "In  Figure   17 , we present the training curves for each initial distribution for three different datasets.",
            "We observe that the marginal distribution consistently achieves at least as good performance as the other initial distributions. This, along with the theoretical reasons outlined in  Sec.   4.2.1 , reinforces its use as the default initial distribution for DeFoG. The only dataset where marginal was surpassed was the SBM dataset, which we attribute to its inherently different nature (stochastic  vs.  deterministic). In this case, the absorbing distribution emerged as the best-performing choice. Interestingly, the absorbing distribution also tends to converge faster across datasets.",
            "To investigate these questions, we conducted a grid search. For two datasets, we trained five models, each with a different time distortion applied during training. Subsequently, we tested each model by applying the five different distortions at the sampling stage. The results are presented in  Figure   18 .",
            "To determine if the structural properties observed in datasets like the planar dataset can be detected and exploited without requiring an exhaustive sweep over all possibilities, we propose developing a metric that quantifies the difficulty of predicting the clean graph for any given time point  t  [ 0 , 1 ) t 0 1 t\\in[0,1) italic_t  [ 0 , 1 ) . For this, we perform a sweep over  t t t italic_t  for a given model, where for each  t t t italic_t , we noise a sufficiently large batch of clean graphs and evaluate the models training loss on them. This yields a curve that shows how the training loss varies as a function of  t t t italic_t . We then track how this curve evolves across epochs. To make the changes more explicit, we compute the ratio of the loss curve relative to the fully trained models values. These curves are shown in  Figure   19 .",
            "As expected, the curve of training loss as a function of  t t t italic_t  (left in  Figure   19 ) is monotonically decreasing, indicating that as graphs are decreasingly noised, the task becomes simpler. However, the most interesting insights arise from the evolution of this curve across epochs (right in  Figure   19 ). We observe that for smaller values of  t t t italic_t , the model reaches its maximum capacity early in the training process, showing no significant improvements after the initial few epochs. In contrast, for larger values of  t t t italic_t  (closer to  t = 1 t 1 t=1 italic_t = 1 ), the model exhibits substantial improvements throughout the training period. This suggests that the model can continue to refine its predictions in the time range where the task is easier. These findings align with those in  Figure   18 , reinforcing our expectation that training the model to be more precise in this range or providing more refined sampling steps will naturally enhance performance in the planar dataset.",
            "Here, we first provide the proof of  Theorem   2  in  Sec.   C.1.1 , and then the proof of  Theorem   3  in  Sec.   C.1.2 .",
            "As exposed in  Sec.   3.1 , the marginal distribution and the rate matrix of a CTMC are related by the Kolmogorov equation:",
            "By definition ( Eq.   13 ), we have:",
            "In  Eq.   16 , we use the definition of TV distance as defined in  Eq.   14  and  Eq.   17  results from direct application of Pinskers inequality. Now, we change the ordering of the sum and of the square root through the Cauchy-Schwarz inequality:",
            "As proceeded in  Sec.   C.1.1 , we start by introducing the necessary concepts that will reveal useful for the proof of the intended result.",
            "where  R t  ( z t 1 : D , z t + d  t 1 : D ) subscript R t subscript superscript z : 1 D t subscript superscript z : 1 D t d t R_{t}(z^{1:D}_{t},z^{1:D}_{t+\\mathrm{d}t}) italic_R start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT )  denotes the unconditional multivariate rate matrix defined in  Eq.   13 . This process can be simulated exactly using Gillespies Algorithm  (Gillespie,  1976 ;  1977 ) . However, such an algorithm does not scale for large  D D D italic_D   (Campbell et al.,  2022 ) . Although    \\tau italic_ -leaping is a widely adopted approximate algorithm to address this limitation  (Gillespie,  2001 ) , it requires ordinal discrete state spaces, which is suitable for cases like text or images but not for graphs. Therefore, we cannot apply it in the context of this paper. Additionally, directly replacing the infinitesimal step  d  t d t \\text{d}t d italic_t  in  Eq.   13  with a finite time step    t  t \\Delta t roman_ italic_t   a la  Euler method is inappropriate, as  R t  ( z t 1 : D , z t + d  t 1 : D ) subscript R t subscript superscript z : 1 D t subscript superscript z : 1 D t d t R_{t}(z^{1:D}_{t},z^{1:D}_{t+\\mathrm{d}t}) italic_R start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT )  prevents state transitions in more than one dimension per step under the continuous framework. Instead,  Campbell et al. ( 2024 )  propose an approximation where the Euler step is applied independently to each dimension, as seen in  Eq.   4 .",
            "Recall that  p t | 1  ( z t | z 1 ) = t    ( z t , z 1 ) + ( 1  t )  p 0  ( z t ) subscript p conditional t 1 conditional subscript z t subscript z 1 t  subscript z t subscript z 1 1 t subscript p 0 subscript z t p_{t|1}(z_{t}|z_{1})=t\\,\\delta(z_{t},z_{1})+(1-t)\\,p_{0}(z_{t}) italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) = italic_t italic_ ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) + ( 1 - italic_t ) italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  (from  Eq.   1 ). Two different cases must then be considered.",
            "From  Eq.   13 , the unconditional rate matrix is given by:",
            "where, in  Eq.   21 ,  P 1 | 0 subscript P conditional 1 0 \\mathbb{P}_{1|0} blackboard_P start_POSTSUBSCRIPT 1 | 0 end_POSTSUBSCRIPT  denotes the path measure of the exact groundtruth CTMC and the difference between limit distributions (second term from  Eq.   21 ) is zero since in flow matching the convergence to the limit distribution via linear interpolation is not asymptotic (as in diffusion models) but actually attained at  t = 0 t 0 t=0 italic_t = 0 . In  Eq.   22 , we introduce the stepwise path measure, i.e.,  P k = P t k | t k  1 subscript P k subscript P conditional subscript t k subscript t k 1 \\mathcal{P}_{k}=\\mathbb{P}_{t_{k}|t_{k-1}} caligraphic_P start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = blackboard_P start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , such that  P T | 0 = P 1  P 2  ...  P K subscript P conditional T 0 subscript P 1 subscript P 2 ... subscript P K \\mathbb{P}_{T|0}=\\mathcal{P}_{1}\\mathcal{P}_{2}\\ldots\\mathcal{P}_{K} blackboard_P start_POSTSUBSCRIPT italic_T | 0 end_POSTSUBSCRIPT = caligraphic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT caligraphic_P start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ... caligraphic_P start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT . Therefore, finding the intended upper bound amounts to establish bounds on the total variation distance for each interval  [ t k  1 , t k ] subscript t k 1 subscript t k [t_{k-1},t_{k}] [ italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ] .",
            "i.e., the square root of the right-hand side of  Eq.   15 .",
            "It remains to bound the second term from  Eq.   23 . We start by analyzing the Markov kernel corresponding to a Markov chain with  constant  rate matrix  R t k  1  subscript superscript R  subscript t k 1 R^{\\theta}_{t_{k-1}} italic_R start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT  between  t k  1 subscript t k 1 t_{k-1} italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT  and  t k subscript t k t_{k} italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT . In that case, from  Eq.   19  we obtain:",
            "where in  Eq.   30  we have the approximated transition rate matrix is computed according to  Eq.   13  but using  p 1 | t   ( z 1 d | z t 1 : D ) subscript superscript p  conditional 1 t conditional subscript superscript z d 1 subscript superscript z : 1 D t p^{\\theta}_{1|t}(z^{d}_{1}|z^{1:D}_{t}) italic_p start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 | italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  instead of  p 1 | t  ( z 1 d | z t 1 : D ) subscript p conditional 1 t conditional subscript superscript z d 1 subscript superscript z : 1 D t p_{1|t}(z^{d}_{1}|z^{1:D}_{t}) italic_p start_POSTSUBSCRIPT 1 | italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) .",
            "Therefore, we get the intended result by gathering the results from  Eq.   27 ,  Eq.   29 , and  Eq.   31 .",
            "The different components of a graph generative model have to respect different graph symmetries. For example, the permutation equivariance of the model architecture ensures the output changes consistently with any reordering of input nodes, while permutation-invariant loss evaluates the models performance consistently across isomorphic graphs, regardless of node order. We provide a proof for related properties included in Lemma  1  as follows.",
            "Here, we describe the datasets employed in our experiments and outline the specific metrics used to evaluate model performance on each dataset. Additional visualizations of example graphs from each dataset, along with generated graphs, are provided in  Figures   21 ,  23  and  25 .",
            "Each cell graph in the dataset can be mapped to a TLS (Tertiary Lymphoid Structure) embedding, denoted as   = [  0 , ... ,  5 ]  R 6  subscript  0 ... subscript  5 superscript R 6 \\kappa=[\\kappa_{0},\\ldots,\\kappa_{5}]\\in\\mathbb{R}^{6} italic_ = [ italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , ... , italic_ start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT ]  blackboard_R start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT , which quantifies its TLS content. A graph  G G G italic_G  is classified as having low TLS content if   1  ( G ) < 0.05 subscript  1 G 0.05 \\kappa_{1}(G)<0.05 italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_G ) < 0.05 , and high TLS content if   2  ( G ) > 0.05 subscript  2 G 0.05 \\kappa_{2}(G)>0.05 italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_G ) > 0.05 . Based on these criteria, the dataset is split into two subsets: high TLS and low TLS. In prior work, TLS generation accuracy was evaluated by training generative models on these subsets separately, and verifying if the generated graphs matched the corresponding TLS content label. We compute TLS accuracy as the average accuracy across both subsets. For DeFoG, we conditionally train it on both subsets simultaneously, as described in  Appendix   D , and compute TLS accuracy based on whether the generated graphs adhere to the conditioning label. Additionally, we report the V.U.N. metric (valid, unique, novel), similar to what is done for the synthetic datasets (see  Sec.   E.1.1 ). A graph is considered valid in this case if it is a connected planar graph, as the graphs in these datasets were constructed using Delaunay triangulation.",
            "The default hyperparameters for training and sampling for each dataset can be found in the provided code repository. In  Tab.   6 , we specifically highlight their values for the proposed training (see  Sec.   4.2.1 ) and sampling (see  Sec.   4.2.2 ) strategies, and conditional guidance parameter (see  Appendix   D ). As the training process is by far the most computationally costly stage, we aim to minimize changes to the default model training configuration. Nevertheless, we demonstrate the effectiveness of these modifications on certain datasets:",
            "In this section, we present additional experimental results. We begin with the complete tables showcasing results for synthetic datasets in  Sec.   F.1 . Next,  Sec.   F.2  focuses on molecular generation tasks, including results for the QM9 dataset and the full tables for MOSES and Guacamol. Finally, in  Sec.   F.3 , we analyze the time complexity of various additional features that enhance expressivity in graph diffusion models.",
            "Additionally, we provide the complete version of  Tab.   2 , presenting the results for MOSES and Guacamol separately in  Tab.   9  and  Tab.   10 , respectively. We include models from classes beyond diffusion models to better contextualize the performance achieved by DeFoG.",
            "In graph diffusion methods, the task of graph generation is decomposed into a mapping of a graph to a set of marginal probabilities for each node and edge. This problem is typically addressed using a Graph Transformer architecture, which is augmented with additional features to capture structural aspects that the base architecture might struggle to model effectively  (Vignac et al.,  2022 ; Xu et al.,  2024 ; Siraudin et al.,  2024 )  otherwise. In this section, we conduct a time complexity analysis of the additional features commonly employed. Specifically, we compare the spectral encodings and cycle encodings (up to  6 6 6 6 -cycles) proposed by  Vignac et al. ( 2022 )  to the RRWP encodings (12 steps) from  Siraudin et al. ( 2024 )  (originally from  Ma et al. ( 2023 ) ). While both cycle and RRWP encodings primarily involve matrix multiplications, spectral encodings necessitate more complex algorithms for eigenvalue and eigenvector computation. As shown in  Tab.   11 , cycle and RRWP encodings are more computationally efficient, particularly for larger graphs where eigenvalue computation becomes increasingly costly. These results support the use of RRWP encodings over the combined utilization of cycle and spectral features."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Large molecule generation performance. Only iterative denoising-based methods are reported here. Respecive full versions in  Tab.   9  (MOSES) and  Tab.   10  (Guacamol),  Sec.   F.2 .",
        "table": "S6.T2.12.12",
        "footnotes": [
            "",
            "",
            ""
        ],
        "references": [
            "and  Z t > 0 = | { z t : p t | 1 ( z t | z 1 ) > 0 } | Z^{>0}_{t}=|\\{z_{t}:p_{t|1}(z_{t}|z_{1})>0\\}| italic_Z start_POSTSUPERSCRIPT > 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = | { italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT : italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) > 0 } | . Again, normalization is performed for the case  z t = z t + d  t subscript z t subscript z t d t z_{t}=z_{t+\\mathrm{d}t} italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_z start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT . Intuitively,  R t  subscript superscript R t R^{*}_{t} italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  applies a positive rate to states needing more mass than the current state  z t subscript z t z_{t} italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  (details in  Sec.   A.5 ). Note that  R t  subscript superscript R t R^{*}_{t} italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  is just one valid instantiation of  R t subscript R t R_{t} italic_R start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , though others exist, as detailed in  Sec.   4.2.2 .",
            "The resulting training and sampling processes are detailed in  Algs.   1  and  2 .",
            "We now expose DeFoGs equivariance properties in  Lemma   1 . (See  Sec.   C.2.2  for proof.)",
            "In DeFoGs vanilla sampling process, the discretization is performed using equally sized time steps ( Alg.   2 , line 5). Instead, we propose employing variable step sizes. This adjustment is motivated by the need for more fine-grained control during certain time intervals, for instance, to ensure that cluster structures are properly formed before focusing on intra-cluster refinements, or to prevent edge alterations that could compromise global properties once the overall structure is established. By allocating smaller, more frequent steps to these critical intervals, the generated graph can better capture the true properties of the data. To achieve this, we modify the evenly spaced step sizes using distortion functions (see  Sec.   A.1 ). Although the optimal distortion function is highly dataset-dependent, in  Sec.   B.2  we propose a method to guide the selection of the distortion function based on the observed training dynamics. Notably, in diffusion models, the design for diffusion steps such as noise schedule is typically identical for training and sampling. However, in flow models, the time distribution for training detailed in  Sec.   4.2.1  and the time steps used for sampling can be more flexibly disentangled. In practice, applying distortion only during sampling already yields notable performance improvements.",
            "with    R +  superscript R \\omega\\in\\mathbb{R}^{+} italic_  blackboard_R start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT . This adjustment biases the transitions toward the clean data state  z 1 subscript z 1 z_{1} italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT .  Lemma   10 , in  Sec.   A.2 , demonstrates that this modification introduces an  O  (  ) O  O(\\omega) italic_O ( italic_ )  violation of the Kolmogorov equation. Consequently, choosing a small value of    \\omega italic_  is experimentally shown to be highly beneficial, while a larger    \\omega italic_  restricts the distribution to regions of high probability, increasing the distance between the generated data and the training data, as indicated in  Sec.   A.4 ,  Figure   11 .",
            "The space of valid rate matrices, i.e., those that satisfy the Kolmogorov equation, is not exhausted by the original formulation of  R t   ( z t , z t + d  t | z 1 ) subscript superscript R t subscript z t conditional subscript z t d t subscript z 1 R^{*}_{t}{(z_{t},z_{t+\\mathrm{d}t}|z_{1})} italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) .  Campbell et al. ( 2024 )  investigate this and show that for any rate matrix  R t DB subscript superscript R DB t R^{\\text{DB}}_{t} italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  that satisfies the detailed balance condition,  p t | 1  ( z t | z 1 )  R t DB  ( z t , z t + d  t | z 1 ) = p t | 1  ( z t + d  t | z 1 )  R t DB  ( z t + d  t , z t | z 1 ) subscript p conditional t 1 conditional subscript z t subscript z 1 subscript superscript R DB t subscript z t conditional subscript z t d t subscript z 1 subscript p conditional t 1 conditional subscript z t d t subscript z 1 subscript superscript R DB t subscript z t d t conditional subscript z t subscript z 1 p_{t|1}(z_{t}|z_{1})R^{\\text{DB}}_{t}(z_{t},z_{t+\\text{d}t}|z_{1})=p_{t|1}(z_{% t+\\text{d}t}|z_{1})R^{\\text{DB}}_{t}(z_{t+\\text{d}t},z_{t}|z_{1}) italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_t + d italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) = italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t + d italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t + d italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , the modified rate matrix  R t  = R t  +   R t DB subscript superscript R  t subscript superscript R t  subscript superscript R DB t R^{\\eta}_{t}=R^{*}_{t}+\\eta R^{\\text{DB}}_{t} italic_R start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_ italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , with    R +  superscript R \\eta\\in\\mathbb{R}^{+} italic_  blackboard_R start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT , also satisfies the Kolmogorov equation. Increasing    \\eta italic_  introduces more stochasticity into the trajectory of the denoising process, while different designs of  R t D  B subscript superscript R D B t R^{DB}_{t} italic_R start_POSTSUPERSCRIPT italic_D italic_B end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  encode different priors for preferred transitions between states. This mechanism can be interpreted as a correction mechanism, as it enables transitions back to states that would otherwise be disallowed according to the rate matrix formulation, as described in  Sec.   A.5 . The effect of    \\eta italic_  across different datasets is illustrated in detail in  Figure   12 ,  Sec.   A.4 . To further improve the sampling performance of DeFoG, we also investigate the different formulations of  R DB superscript R DB R^{\\text{DB}} italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT  under the detailed balance condition. Additional details and discussions are provided in  Sec.   A.3 .",
            "The execution of line 8 in  Alg.   2  requires computing  E p 1 | t  , d  ( z 1 d | z t 1 : D )  [ R t d  ( z t d , z t +   t d | z 1 d ) ] subscript E superscript subscript p conditional 1 t  d conditional subscript superscript z d 1 subscript superscript z : 1 D t delimited-[] subscript superscript R d t subscript superscript z d t conditional subscript superscript z d t  t subscript superscript z d 1 \\mathbb{E}_{p_{1|t}^{\\theta,d}(z^{d}_{1}|z^{1:D}_{t})}\\left[R^{d}_{t}(z^{d}_{t% },z^{d}_{t+\\Delta t}|z^{d}_{1})\\right] blackboard_E start_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT 1 | italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ , italic_d end_POSTSUPERSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT [ italic_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_ italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ] . In practice,  Campbell et al. ( 2024 )  sample  z 1 d  p 1 | t  , d  ( z 1 d | z t 1 : D ) similar-to subscript superscript z d 1 subscript superscript p  d conditional 1 t conditional subscript superscript z d 1 subscript superscript z : 1 D t z^{d}_{1}\\sim p^{\\theta,d}_{1|t}(z^{d}_{1}|z^{1:D}_{t}) italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  italic_p start_POSTSUPERSCRIPT italic_ , italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 | italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  to directly approximate the expectation with  R t d  ( z t d , z t +   t d | z 1 d ) subscript superscript R d t subscript superscript z d t conditional subscript superscript z d t  t subscript superscript z d 1 R^{d}_{t}(z^{d}_{t},z^{d}_{t+\\Delta t}|z^{d}_{1}) italic_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_ italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) . Although this procedure converges in expectation to the intended value, it introduces more stochasticity into the denoising trajectory compared to computing the exact expectation. Given that the cardinalities of each dimension are relatively small ( X X X italic_X  and  E E E italic_E ), we explore the exact computation of the expectation. This approach is especially useful in settings where precision is prioritized over diversity, ensuring high confidence in the validity of generated samples, even at the expense of reduced variability.",
            "This result parallels  Campbell et al. ( 2022 ) , who established a similar bound for continuous time discrete diffusion models using    \\tau italic_ -leaping for sampling. However,  Theorem   3  extends that result to the DFM framework, employing the independent-dimensional Euler method. Specifically, the first term of the upper bound results from the estimation error, i.e., using the neural network approximation  p 1 | t   ( z 1 d | z t 1 : D ) subscript superscript p  conditional 1 t conditional subscript superscript z d 1 subscript superscript z : 1 D t p^{\\theta}_{1|t}(z^{d}_{1}|z^{1:D}_{t}) italic_p start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 | italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  instead of the true distribution  p 1 | t  ( z 1 d | z t 1 : D ) subscript p conditional 1 t conditional subscript superscript z d 1 subscript superscript z : 1 D t p_{1|t}(z^{d}_{1}|z^{1:D}_{t}) italic_p start_POSTSUBSCRIPT 1 | italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) . As shown in  Theorem   2 , this term can be bounded. The remaining terms arise from the time discretization and approximated simulation of the CTMC, respectively. Since these terms are  O  (   t ) O  t O(\\Delta t) italic_O ( roman_ italic_t ) , their impact can be controlled by arbitrarily reducing the step size, ensuring that the generated distribution remains faithful to the ground truth.",
            "Molecular design is a prominent real-world application of graph generation. We evaluate DeFoGs performance on this task using the QM9  (Wu et al.,  2018 ) , MOSES  (Polykovskiy et al.,  2020 ) , and Guacamol  (Brown et al.,  2019 )  datasets. For QM9, we follow the dataset split and evaluation metrics from  Vignac et al. ( 2022 ) , presenting the results in  Sec.   E.1.2 ,  Tab.   8 . For the larger MOSES and Guacamol datasets, we adhere to the training setup and evaluation metrics established by  Polykovskiy et al. ( 2020 )  and  Brown et al. ( 2019 ) , respectively, with results in  Tabs.   9  and  10 .",
            "As illustrated in  Tab.   2 , DeFoG outperforms existing diffusion models. It achieving state-of the Validity while preserving high uniqueness on MOSES. On Guacamol, it consistently ranks best, followed by Cometh, another work utilizing a continuous-time framework. Notably, DeFoG approaches the performance of existing diffusion models with only 10% of the sampling steps. This result is further investigated in the following section.",
            "As highlighted in  Tabs.   1 ,  2  and  8 , DeFoG attains similar performance, and in some cases even outperforms, several diffusion-based methods with just 5% or 10% of their sampling steps. This efficiency is a result of DeFoGs sampling stage flexibility, whose optimization is enabled due to the flexible and disentangled training-sampling procedure within DFM. To further demonstrate the advantages of sampling optimization,  Figure   3(a)  shows the cumulative effect of each optimization step discussed in  Sec.   4.2.2 . As illustrated, starting from a vanilla DeFoG model, which initially performs slightly worse than DiGress, we sequentially incorporate algorithmic improvements, with each stage indicated by an increasing number of  + + +  symbols. Each addition leads to a progressive improvement over the previous stage across both the Planar and QM9 datasets under various numbers of steps for sampling, with the final performance significantly surpassing the vanilla model using only 50 steps on the Planar dataset. While the benefits of each optimization component may vary across datasets, the sampling optimization pipeline remains highly efficient, enabling quick hyperparameter exploration without requiring retraining, which is a significantly more resource-intensive stage (see  Sec.   E.2 ). We provide more details on the impact of each sampling optimization strategy across datasets in  Sec.   A.4 .",
            "While our primary focus is on optimizing the sampling process given its computational efficiency, we emphasize that DeFoGs training optimization strategies can further push its performance. To illustrate this,  Figure   3(b)  shows the convergence curves for the tree and MOSES datasets. We first observe that, with the same model, leveraging only sampling distortion can further enhance its performance beyond the vanilla implementation. This suggests that optimizing the sampling procedure can be particularly useful in settings where computational resources are limited and the model is undertrained, as detailed in  Sec.   A.6 . Moreover, when an appropriate sample distortion is known (e.g.,  polydec  distortion is shown to be particularly preferred across molecular datasets), applying it to both train distortion and sample distortion typically further improves performance. Although DeFoG achieves faster convergence with both distortions, we remark that DiGress also shows good convergence speed on the Tree dataset, due to the well-tuned noise schedule employed for joint training and sampling. The mutual impact of training and sampling distortion is discussed in  Sec.   B.2 . Besides, in the same section, we discuss a heuristic on which distortion to use for each dataset according to DeFoGs vanilla dynamics. Further details on how the initial distribution affects training efficiency are provided in  Sec.   B.1 .",
            "In this section, we explore the proposed sampling optimization in more detail. We start by analysing the different time distortion functions in  Sec.   A.1 . Next, in  Sec.   A.2 , we prove that the proposed target guidance mechanism actually satisfies the Kolmogorov equation, thus yielding valid rate matrices and, in  Sec.   A.3 , we provide more details about the detailed balance equation and how it widens the design space of rate matrices. In  Sec.   A.4 , we also describe the adopted sampling optimization pipeline. Finally, in  Sec.   A.5 , we provide more details to better clarify the dynamics of the sampling process.",
            "still satisfy the Kolmogorov equation. The detailed balance condition ensures that the outflow,  p t | 1  ( z t | z 1 )  R t DB  ( z t , z t + d  t | z 1 ) subscript p conditional t 1 conditional subscript z t subscript z 1 subscript superscript R DB t subscript z t conditional subscript z t d t subscript z 1 p_{t|1}(z_{t}|z_{1})R^{\\text{DB}}_{t}(z_{t},z_{t+\\text{d}t}|z_{1}) italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_t + d italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , and inflow,  p t | 1  ( z t + d  t | z 1 )  R t DB  ( z t + d  t , z t | z 1 ) subscript p conditional t 1 conditional subscript z t d t subscript z 1 subscript superscript R DB t subscript z t d t conditional subscript z t subscript z 1 p_{t|1}(z_{t+\\text{d}t}|z_{1})R^{\\text{DB}}_{t}(z_{t+\\text{d}t},z_{t}|z_{1}) italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t + d italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t + d italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , of probability mass to any given state are perfectly balanced. Under these conditions, this additive components contribution to the Kolmogorov equation becomes null (similar to the target guidance, as shown in the proof of of  Lemma   10 , in  Sec.   A.2 ).",
            "A natural question is how to choose a suitable design for  R t DB subscript superscript R DB t R^{\\text{DB}}_{t} italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  from the infinite space of detailed balance rate matrices. As depicted in  Figure   5 , this flexibility can be leveraged to incorporate priors into the denoising model by encouraging specific transitions between states. By adjusting the sparsity of the matrix entries, additional transitions beyond those prescribed by  R t  subscript superscript R t R^{*}_{t} italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  can be introduced. In the general case, transitions between all states are possible; in the column case, a specific state centralizes all potential transitions; and in the single-entry case, only transitions between two states are permitted. These examples merely illustrate some possibilities and do not exhaust the range of potential  R t DB subscript superscript R DB t R^{\\text{DB}}_{t} italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  designs. The matrix entries can be structured by considering the following reorganization of terms of  Eq.   12 :",
            "Orthogonal to the design of  R t DB subscript superscript R DB t R^{\\text{DB}}_{t} italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , we must also consider the hyperparameter    \\eta italic_ , which regulates the magnitude of stochasticity in the denoising process. Specifically, setting   = 0  0 \\eta=0 italic_ = 0  (thereby relying solely on  R t  subscript superscript R t R^{*}_{t} italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) minimizes the expected number of jumps throughout the denoising trajectory under certain conditions, as shown by  Campbell et al. ( 2024 )  in Proposition 3.4. However, in continuous diffusion models, some level of stochasticity has been demonstrated to enhance performance  (Karras et al.,  2022 ; Cao et al.,  2023 ; Xu et al.,  2023 ) . Conversely, excessive stochasticity can negatively impact performance.  Campbell et al. ( 2024 )  propose that there exists an optimal level of stochasticity that strikes a balance between exploration and accuracy. In our experiments, we observed varied behaviors as    \\eta italic_  increases, resulting in different performance outcomes across datasets, as illustrated in  Figure   12 .",
            "A significant advantage of flow matching methods is their inherently greater flexibility in the sampling process compared to diffusion models, as they are more disentangled from the training stage. Each of the proposed optimization strategies exposed in  Sec.   4.2.2  expands the search space for optimal performance. However, conducting a full grid search across all those methodologies is impractical for the computational resources available. To address this challenge, our sampling optimization pipeline consists of, for each of the proposed optimization strategies, all hyperparameters are held constant at their default values except for the parameter controlling the chosen strategy, over which we perform a sweep. The optimal values obtained for each strategy are combined to form the final configuration. In  Tab.   6 , we present the final hyperparameter values obtained for each dataset. This pipeline is sufficient to achieve state-of-the-art performance, which reinforces the expressivity of DeFoG. We expect to achieve even better results if a more comprehensive search of the hyperparameter space was carried out.",
            "While Figures  8  and  10  are highly condensed, we provide a more fine-grained version that specifically illustrates the influence of the hyperparameters    \\eta italic_  and    \\omega italic_ . This version highlights their impact when generating with the full number of steps (500 and 1000 for molecular and synthetic data, respectively) and with 50 steps. As emphasized in Figures  11  and  12 , the influence of these hyperparameters varies across datasets and exhibits distinct behaviors depending on the number of steps used.",
            "For instance, with the masking distribution, the fourth line facilitates transitions to states other than the virtual mask state, whereas for the uniform distribution, no transitions are allowed. For the marginal distribution, transitions are directed toward less likely states. Note that while these behaviors hold when the rate matrix consists solely of  R t  subscript superscript R t R^{*}_{t} italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , additional transitions can be introduced through  R t DB subscript superscript R DB t R^{\\mathrm{DB}}_{t} italic_R start_POSTSUPERSCRIPT roman_DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  (as detailed in  Sec.   A.3 ) or by applying target guidance (see  Sec.   A.2 ).",
            "In this section, we provide a more detailed analysis of the influence of the various training optimization strategies introduced in  Sec.   4.2.1 . In  Sec.   B.1 , we empirically demonstrate the impact of selecting different initial distributions on performance, while in  Sec.   B.2 , we examine the interaction between training and sampling optimization.",
            "We observe that the marginal distribution consistently achieves at least as good performance as the other initial distributions. This, along with the theoretical reasons outlined in  Sec.   4.2.1 , reinforces its use as the default initial distribution for DeFoG. The only dataset where marginal was surpassed was the SBM dataset, which we attribute to its inherently different nature (stochastic  vs.  deterministic). In this case, the absorbing distribution emerged as the best-performing choice. Interestingly, the absorbing distribution also tends to converge faster across datasets.",
            "Here, we first provide the proof of  Theorem   2  in  Sec.   C.1.1 , and then the proof of  Theorem   3  in  Sec.   C.1.2 .",
            "We are now prepared to proceed with the proof of  Theorem   2 .",
            "By differentiating the explicit form of  p t | 1  ( z t | z 1 ) subscript p conditional t 1 conditional subscript z t subscript z 1 p_{t|1}(z_{t}|z_{1}) italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , we have that   2 p t | 1  ( z t | z 1 ) = 0 superscript 2 subscript p conditional t 1 conditional subscript z t subscript z 1 0 \\partial^{2}p_{t|1}(z_{t}|z_{1})=0  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) = 0 . As a consequence, the numerator of  eq.   20  has zero derivative. Additionally, we also note that  Z t > 0 subscript superscript Z absent 0 t \\mathbb{Z}^{>0}_{t} blackboard_Z start_POSTSUPERSCRIPT > 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  is constant. Again, since  p t | 1  ( z t | z 1 ) subscript p conditional t 1 conditional subscript z t subscript z 1 p_{t|1}(z_{t}|z_{1}) italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT )  is a linear interpolation between  z 1 subscript z 1 z_{1} italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and  p 0 subscript p 0 p_{0} italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  and, therefore, it is impossible for  p t | 1  ( z t | z 1 ) subscript p conditional t 1 conditional subscript z t subscript z 1 p_{t|1}(z_{t}|z_{1}) italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT )  to suddenly become 0 for  t  ( 0 , 1 ) t 0 1 t\\in(0,1) italic_t  ( 0 , 1 ) .",
            "where, in  Eq.   21 ,  P 1 | 0 subscript P conditional 1 0 \\mathbb{P}_{1|0} blackboard_P start_POSTSUBSCRIPT 1 | 0 end_POSTSUBSCRIPT  denotes the path measure of the exact groundtruth CTMC and the difference between limit distributions (second term from  Eq.   21 ) is zero since in flow matching the convergence to the limit distribution via linear interpolation is not asymptotic (as in diffusion models) but actually attained at  t = 0 t 0 t=0 italic_t = 0 . In  Eq.   22 , we introduce the stepwise path measure, i.e.,  P k = P t k | t k  1 subscript P k subscript P conditional subscript t k subscript t k 1 \\mathcal{P}_{k}=\\mathbb{P}_{t_{k}|t_{k-1}} caligraphic_P start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = blackboard_P start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , such that  P T | 0 = P 1  P 2  ...  P K subscript P conditional T 0 subscript P 1 subscript P 2 ... subscript P K \\mathbb{P}_{T|0}=\\mathcal{P}_{1}\\mathcal{P}_{2}\\ldots\\mathcal{P}_{K} blackboard_P start_POSTSUBSCRIPT italic_T | 0 end_POSTSUBSCRIPT = caligraphic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT caligraphic_P start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ... caligraphic_P start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT . Therefore, finding the intended upper bound amounts to establish bounds on the total variation distance for each interval  [ t k  1 , t k ] subscript t k 1 subscript t k [t_{k-1},t_{k}] [ italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ] .",
            "where, in  Eq.   24 , we use the Mean Value Theorem, with  t c  ( t k  1 , t k ) subscript t c subscript t k 1 subscript t k t_{c}\\in(t_{k-1},t_{k}) italic_t start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT  ( italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ; in  Eq.   25 , we use the fact that there are  Z  D Z D ZD italic_Z italic_D  values of  z t + d  t 1 : D subscript superscript z : 1 D t d t z^{1:D}_{t+\\mathrm{d}t} italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT  that differ at most in only one coordinate from  z t 1 : D subscript superscript z : 1 D t z^{1:D}_{t} italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; in  Eq.   26 , we use the result from  Proposition   7  to upper bound the time derivative of the multivariate unconditional rate matrix; and finally, in  Eq.   27 , we define  B k = sup t  ( t k  1 , t k ) B t subscript B k subscript supremum t subscript t k 1 subscript t k subscript B t B_{k}=\\sup_{t\\in(t_{k-1},t_{k})}B_{t} italic_B start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = roman_sup start_POSTSUBSCRIPT italic_t  ( italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT italic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and    t k = t k  t k  1  subscript t k subscript t k subscript t k 1 \\Delta t_{k}=t_{k}-t_{k-1} roman_ italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT .",
            "where, in  Eq.   28 , we use again the fact that there are  Z  D Z D ZD italic_Z italic_D  values of  z t + d  t 1 : D subscript superscript z : 1 D t d t z^{1:D}_{t+\\mathrm{d}t} italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT  that differ at most in only one coordinate from  z t 1 : D subscript superscript z : 1 D t z^{1:D}_{t} italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  along with the estimation error upper bound from  Theorem   2 . In particular, we consider  U k = sup t  [ t k  1 , t k ] , z t 1 : D , z t + d  t 1 : D  Z D  U k z t 1 : D  z t + d  t 1 : D subscript U k t subscript t k 1 subscript t k subscript superscript z : 1 D t subscript superscript z : 1 D t d t superscript Z D supremum subscript superscript U  subscript superscript z : 1 D t subscript superscript z : 1 D t d t k U_{k}=\\underset{\\begin{subarray}{c}t\\in[t_{k-1},t_{k}],\\;\\\\ z^{1:D}_{t},\\;z^{1:D}_{t+\\mathrm{d}t}\\in\\mathcal{Z}^{D}\\end{subarray}}{\\sup}U^% {z^{1:D}_{t}\\to z^{1:D}_{t+\\mathrm{d}t}}_{k} italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = start_UNDERACCENT start_ARG start_ROW start_CELL italic_t  [ italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ] , end_CELL end_ROW start_ROW start_CELL italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT  caligraphic_Z start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_CELL end_ROW end_ARG end_UNDERACCENT start_ARG roman_sup end_ARG italic_U start_POSTSUPERSCRIPT italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , with:",
            "It remains to bound the second term from  Eq.   23 . We start by analyzing the Markov kernel corresponding to a Markov chain with  constant  rate matrix  R t k  1  subscript superscript R  subscript t k 1 R^{\\theta}_{t_{k-1}} italic_R start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT  between  t k  1 subscript t k 1 t_{k-1} italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT  and  t k subscript t k t_{k} italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT . In that case, from  Eq.   19  we obtain:",
            "Therefore, we get the intended result by gathering the results from  Eq.   27 ,  Eq.   29 , and  Eq.   31 .",
            "In  Eq.   32 , we use the definition of permuted ordered set for  x t  subscript superscript x  t x^{\\prime}_{t} italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and  x t + d  t  subscript superscript x  t d t x^{\\prime}_{t+\\mathrm{d}t} italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT , and, in  Eq.   33 , we use that  f  subscript f  f_{\\theta} italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  is equivariant.",
            "Here, we describe the datasets employed in our experiments and outline the specific metrics used to evaluate model performance on each dataset. Additional visualizations of example graphs from each dataset, along with generated graphs, are provided in  Figures   21 ,  23  and  25 .",
            "The default hyperparameters for training and sampling for each dataset can be found in the provided code repository. In  Tab.   6 , we specifically highlight their values for the proposed training (see  Sec.   4.2.1 ) and sampling (see  Sec.   4.2.2 ) strategies, and conditional guidance parameter (see  Appendix   D ). As the training process is by far the most computationally costly stage, we aim to minimize changes to the default model training configuration. Nevertheless, we demonstrate the effectiveness of these modifications on certain datasets:",
            "In this section, we present additional experimental results. We begin with the complete tables showcasing results for synthetic datasets in  Sec.   F.1 . Next,  Sec.   F.2  focuses on molecular generation tasks, including results for the QM9 dataset and the full tables for MOSES and Guacamol. Finally, in  Sec.   F.3 , we analyze the time complexity of various additional features that enhance expressivity in graph diffusion models.",
            "Additionally, we provide the complete version of  Tab.   2 , presenting the results for MOSES and Guacamol separately in  Tab.   9  and  Tab.   10 , respectively. We include models from classes beyond diffusion models to better contextualize the performance achieved by DeFoG."
        ]
    },
    "id_table_3": {
        "caption": "Table 4:   Values of  R t  subscript superscript R t R^{*}_{t} italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  for different  z t subscript z t z_{t} italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and  z t + d  t subscript z t d t z_{t+\\mathrm{d}t} italic_z start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT .",
        "table": "S6.T3.2.2",
        "footnotes": [
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Graph neural networks suffer from inherent expressivity constraints  (Xu et al.,  2019 ) . A usual approach to overcome their limited representation power consists of explicitly augmenting the inputs with features that the networks would otherwise struggle to learn. We adopt Relative Random Walk Probabilities (RRWP) encodings that are proved to be expressive for both discriminative  (Ma et al.,  2023 )  and generative settings  (Siraudin et al.,  2024 ) . RRWP encodes the likelihood of traversing from one node to another in a graph through random walks of varying lengths. In particular, given a graph with an adjacency matrix  A A A italic_A , we generate  K  1 K 1 K-1 italic_K - 1  powers of its degree-normalized adjacency matrix,  M = D  1  A M superscript D 1 A M=D^{-1}A italic_M = italic_D start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT italic_A , i.e.,  [ I , M , M 2 , ... , M K  1 ] I M superscript M 2 ... superscript M K 1 \\left[I,M,M^{2},\\ldots,M^{K-1}\\right] [ italic_I , italic_M , italic_M start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , ... , italic_M start_POSTSUPERSCRIPT italic_K - 1 end_POSTSUPERSCRIPT ] . We concatenate the diagonal entries of each power to their corresponding node embedding, while combining and appending the non-diagonal to their corresponding edge embeddings. RRWP features also stand out for being efficient to compute compared to the spectral and cycle features used in  Vignac et al. ( 2022 ) , as demonstrated in  Sec.   F.3 .",
            "The space of valid rate matrices, i.e., those that satisfy the Kolmogorov equation, is not exhausted by the original formulation of  R t   ( z t , z t + d  t | z 1 ) subscript superscript R t subscript z t conditional subscript z t d t subscript z 1 R^{*}_{t}{(z_{t},z_{t+\\mathrm{d}t}|z_{1})} italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) .  Campbell et al. ( 2024 )  investigate this and show that for any rate matrix  R t DB subscript superscript R DB t R^{\\text{DB}}_{t} italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  that satisfies the detailed balance condition,  p t | 1  ( z t | z 1 )  R t DB  ( z t , z t + d  t | z 1 ) = p t | 1  ( z t + d  t | z 1 )  R t DB  ( z t + d  t , z t | z 1 ) subscript p conditional t 1 conditional subscript z t subscript z 1 subscript superscript R DB t subscript z t conditional subscript z t d t subscript z 1 subscript p conditional t 1 conditional subscript z t d t subscript z 1 subscript superscript R DB t subscript z t d t conditional subscript z t subscript z 1 p_{t|1}(z_{t}|z_{1})R^{\\text{DB}}_{t}(z_{t},z_{t+\\text{d}t}|z_{1})=p_{t|1}(z_{% t+\\text{d}t}|z_{1})R^{\\text{DB}}_{t}(z_{t+\\text{d}t},z_{t}|z_{1}) italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_t + d italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) = italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t + d italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t + d italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , the modified rate matrix  R t  = R t  +   R t DB subscript superscript R  t subscript superscript R t  subscript superscript R DB t R^{\\eta}_{t}=R^{*}_{t}+\\eta R^{\\text{DB}}_{t} italic_R start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_ italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , with    R +  superscript R \\eta\\in\\mathbb{R}^{+} italic_  blackboard_R start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT , also satisfies the Kolmogorov equation. Increasing    \\eta italic_  introduces more stochasticity into the trajectory of the denoising process, while different designs of  R t D  B subscript superscript R D B t R^{DB}_{t} italic_R start_POSTSUPERSCRIPT italic_D italic_B end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  encode different priors for preferred transitions between states. This mechanism can be interpreted as a correction mechanism, as it enables transitions back to states that would otherwise be disallowed according to the rate matrix formulation, as described in  Sec.   A.5 . The effect of    \\eta italic_  across different datasets is illustrated in detail in  Figure   12 ,  Sec.   A.4 . To further improve the sampling performance of DeFoG, we also investigate the different formulations of  R DB superscript R DB R^{\\text{DB}} italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT  under the detailed balance condition. Additional details and discussions are provided in  Sec.   A.3 .",
            "This result parallels  Campbell et al. ( 2022 ) , who established a similar bound for continuous time discrete diffusion models using    \\tau italic_ -leaping for sampling. However,  Theorem   3  extends that result to the DFM framework, employing the independent-dimensional Euler method. Specifically, the first term of the upper bound results from the estimation error, i.e., using the neural network approximation  p 1 | t   ( z 1 d | z t 1 : D ) subscript superscript p  conditional 1 t conditional subscript superscript z d 1 subscript superscript z : 1 D t p^{\\theta}_{1|t}(z^{d}_{1}|z^{1:D}_{t}) italic_p start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 | italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  instead of the true distribution  p 1 | t  ( z 1 d | z t 1 : D ) subscript p conditional 1 t conditional subscript superscript z d 1 subscript superscript z : 1 D t p_{1|t}(z^{d}_{1}|z^{1:D}_{t}) italic_p start_POSTSUBSCRIPT 1 | italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) . As shown in  Theorem   2 , this term can be bounded. The remaining terms arise from the time discretization and approximated simulation of the CTMC, respectively. Since these terms are  O  (   t ) O  t O(\\Delta t) italic_O ( roman_ italic_t ) , their impact can be controlled by arbitrarily reducing the step size, ensuring that the generated distribution remains faithful to the ground truth.",
            "As highlighted in  Tabs.   1 ,  2  and  8 , DeFoG attains similar performance, and in some cases even outperforms, several diffusion-based methods with just 5% or 10% of their sampling steps. This efficiency is a result of DeFoGs sampling stage flexibility, whose optimization is enabled due to the flexible and disentangled training-sampling procedure within DFM. To further demonstrate the advantages of sampling optimization,  Figure   3(a)  shows the cumulative effect of each optimization step discussed in  Sec.   4.2.2 . As illustrated, starting from a vanilla DeFoG model, which initially performs slightly worse than DiGress, we sequentially incorporate algorithmic improvements, with each stage indicated by an increasing number of  + + +  symbols. Each addition leads to a progressive improvement over the previous stage across both the Planar and QM9 datasets under various numbers of steps for sampling, with the final performance significantly surpassing the vanilla model using only 50 steps on the Planar dataset. While the benefits of each optimization component may vary across datasets, the sampling optimization pipeline remains highly efficient, enabling quick hyperparameter exploration without requiring retraining, which is a significantly more resource-intensive stage (see  Sec.   E.2 ). We provide more details on the impact of each sampling optimization strategy across datasets in  Sec.   A.4 .",
            "While our primary focus is on optimizing the sampling process given its computational efficiency, we emphasize that DeFoGs training optimization strategies can further push its performance. To illustrate this,  Figure   3(b)  shows the convergence curves for the tree and MOSES datasets. We first observe that, with the same model, leveraging only sampling distortion can further enhance its performance beyond the vanilla implementation. This suggests that optimizing the sampling procedure can be particularly useful in settings where computational resources are limited and the model is undertrained, as detailed in  Sec.   A.6 . Moreover, when an appropriate sample distortion is known (e.g.,  polydec  distortion is shown to be particularly preferred across molecular datasets), applying it to both train distortion and sample distortion typically further improves performance. Although DeFoG achieves faster convergence with both distortions, we remark that DiGress also shows good convergence speed on the Tree dataset, due to the well-tuned noise schedule employed for joint training and sampling. The mutual impact of training and sampling distortion is discussed in  Sec.   B.2 . Besides, in the same section, we discuss a heuristic on which distortion to use for each dataset according to DeFoGs vanilla dynamics. Further details on how the initial distribution affects training efficiency are provided in  Sec.   B.1 .",
            "From  Tab.   3 , DeFoG significantly outperforms the unconstrained models (all but ConStruct). Notably, we outperform ConStruct on TLS validity with even  50 50 50 50  steps. For V.U.N., while ConStruct is hard-constrained to achieve 100% graph planarity, making it strongly biased toward high validity, DeFoG remarkably approaches these values without relying on such rigid constraints.",
            "Code is available at  github.com/manuelmlmadeira/DeFoG . We include detailed descriptions of the datasets, experimental setup and methodologies used in  Secs.   6  and  E . In particular, the specific hyperparameters employed in our experiments are documented in this paper in  Sec.   E.3  and in the code repository. The full version and proofs of theoretical results can be found in  Appendix   C .",
            "In this section, we explore the proposed sampling optimization in more detail. We start by analysing the different time distortion functions in  Sec.   A.1 . Next, in  Sec.   A.2 , we prove that the proposed target guidance mechanism actually satisfies the Kolmogorov equation, thus yielding valid rate matrices and, in  Sec.   A.3 , we provide more details about the detailed balance equation and how it widens the design space of rate matrices. In  Sec.   A.4 , we also describe the adopted sampling optimization pipeline. Finally, in  Sec.   A.5 , we provide more details to better clarify the dynamics of the sampling process.",
            "In this section, we aim to provide deeper intuition into the sampling dynamics imposed by the design of  R t  subscript superscript R t R^{*}_{t} italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , as proposed by  Campbell et al. ( 2024 ) . The explicit formulation of  R t  subscript superscript R t R^{*}_{t} italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  can be found in  Eq.   3 . Notably, the denominator in the expression serves as a normalizing factor, meaning the dynamics of each sampling step are primarily influenced by the values in the numerator. Specifically, we observe the following relationship:",
            "For instance, with the masking distribution, the fourth line facilitates transitions to states other than the virtual mask state, whereas for the uniform distribution, no transitions are allowed. For the marginal distribution, transitions are directed toward less likely states. Note that while these behaviors hold when the rate matrix consists solely of  R t  subscript superscript R t R^{*}_{t} italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , additional transitions can be introduced through  R t DB subscript superscript R DB t R^{\\mathrm{DB}}_{t} italic_R start_POSTSUPERSCRIPT roman_DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  (as detailed in  Sec.   A.3 ) or by applying target guidance (see  Sec.   A.2 ).",
            "Here, we first provide the proof of  Theorem   2  in  Sec.   C.1.1 , and then the proof of  Theorem   3  in  Sec.   C.1.2 .",
            "As exposed in  Sec.   3.1 , the marginal distribution and the rate matrix of a CTMC are related by the Kolmogorov equation:",
            "In the first equality,  1 : D  d : 1 D d {1:D\\setminus d} 1 : italic_D  italic_d  refers to all dimensions except  d d d italic_d  and the    \\delta italic_  term restricts contributions to rate matrices that account for at most one dimension transitioning at a time, since the probability of two or more independently noised dimensions transitioning simultaneously is zero under a continuous time framework  (Campbell et al.,  2022 ;  2024 ) . In the second equality, the unconditional rate matrix is retrieved by taking the expectation over the  z 1 subscript z 1 z_{1} italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT -conditioned rate matrices. Specifically,  R t  d  ( z t d , z t + d  t d | z 1 d ) subscript superscript R absent d t subscript superscript z d t conditional subscript superscript z d t d t superscript subscript z 1 d R^{*d}_{t}(z^{d}_{t},z^{d}_{t+\\mathrm{d}t}|z_{1}^{d}) italic_R start_POSTSUPERSCRIPT  italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT )  denotes the univariate rate matrix corresponding to dimension  d d d italic_d  (see  Eq.   3 )",
            "By definition ( Eq.   13 ), we have:",
            "where  R t  ( z t 1 : D , z t + d  t 1 : D ) subscript R t subscript superscript z : 1 D t subscript superscript z : 1 D t d t R_{t}(z^{1:D}_{t},z^{1:D}_{t+\\mathrm{d}t}) italic_R start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT )  denotes the unconditional multivariate rate matrix defined in  Eq.   13 . This process can be simulated exactly using Gillespies Algorithm  (Gillespie,  1976 ;  1977 ) . However, such an algorithm does not scale for large  D D D italic_D   (Campbell et al.,  2022 ) . Although    \\tau italic_ -leaping is a widely adopted approximate algorithm to address this limitation  (Gillespie,  2001 ) , it requires ordinal discrete state spaces, which is suitable for cases like text or images but not for graphs. Therefore, we cannot apply it in the context of this paper. Additionally, directly replacing the infinitesimal step  d  t d t \\text{d}t d italic_t  in  Eq.   13  with a finite time step    t  t \\Delta t roman_ italic_t   a la  Euler method is inappropriate, as  R t  ( z t 1 : D , z t + d  t 1 : D ) subscript R t subscript superscript z : 1 D t subscript superscript z : 1 D t d t R_{t}(z^{1:D}_{t},z^{1:D}_{t+\\mathrm{d}t}) italic_R start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT )  prevents state transitions in more than one dimension per step under the continuous framework. Instead,  Campbell et al. ( 2024 )  propose an approximation where the Euler step is applied independently to each dimension, as seen in  Eq.   4 .",
            "We are now in conditions of proceeding to the proof of  Theorem   3 . We start by first proving that, in the univariate case, the time derivatives of the  conditional  rate matrices are upper bounded.",
            "From  Eq.   13 , the unconditional rate matrix is given by:",
            "Now, we finally start the proof of  Theorem   3",
            "We first apply the same decomposition to the left-hand side of  Theorem   3 , as  Campbell et al. ( 2022 ) , Theorem 1:",
            "It remains to bound the second term from  Eq.   23 . We start by analyzing the Markov kernel corresponding to a Markov chain with  constant  rate matrix  R t k  1  subscript superscript R  subscript t k 1 R^{\\theta}_{t_{k-1}} italic_R start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT  between  t k  1 subscript t k 1 t_{k-1} italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT  and  t k subscript t k t_{k} italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT . In that case, from  Eq.   19  we obtain:",
            "where in  Eq.   30  we have the approximated transition rate matrix is computed according to  Eq.   13  but using  p 1 | t   ( z 1 d | z t 1 : D ) subscript superscript p  conditional 1 t conditional subscript superscript z d 1 subscript superscript z : 1 D t p^{\\theta}_{1|t}(z^{d}_{1}|z^{1:D}_{t}) italic_p start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 | italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  instead of  p 1 | t  ( z 1 d | z t 1 : D ) subscript p conditional 1 t conditional subscript superscript z d 1 subscript superscript z : 1 D t p_{1|t}(z^{d}_{1}|z^{1:D}_{t}) italic_p start_POSTSUBSCRIPT 1 | italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) .",
            "Therefore, we get the intended result by gathering the results from  Eq.   27 ,  Eq.   29 , and  Eq.   31 .",
            "In  Eq.   32 , we use the definition of permuted ordered set for  x t  subscript superscript x  t x^{\\prime}_{t} italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and  x t + d  t  subscript superscript x  t d t x^{\\prime}_{t+\\mathrm{d}t} italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT , and, in  Eq.   33 , we use that  f  subscript f  f_{\\theta} italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  is equivariant.",
            "Here, we describe the datasets employed in our experiments and outline the specific metrics used to evaluate model performance on each dataset. Additional visualizations of example graphs from each dataset, along with generated graphs, are provided in  Figures   21 ,  23  and  25 .",
            "In this section, we present additional experimental results. We begin with the complete tables showcasing results for synthetic datasets in  Sec.   F.1 . Next,  Sec.   F.2  focuses on molecular generation tasks, including results for the QM9 dataset and the full tables for MOSES and Guacamol. Finally, in  Sec.   F.3 , we analyze the time complexity of various additional features that enhance expressivity in graph diffusion models."
        ]
    },
    "id_table_4": {
        "caption": "Table 5:  Training and sampling time on each dataset.",
        "table": "A6.EGx1",
        "footnotes": [],
        "references": [
            "and  Z t > 0 = | { z t : p t | 1 ( z t | z 1 ) > 0 } | Z^{>0}_{t}=|\\{z_{t}:p_{t|1}(z_{t}|z_{1})>0\\}| italic_Z start_POSTSUPERSCRIPT > 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = | { italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT : italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) > 0 } | . Again, normalization is performed for the case  z t = z t + d  t subscript z t subscript z t d t z_{t}=z_{t+\\mathrm{d}t} italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_z start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT . Intuitively,  R t  subscript superscript R t R^{*}_{t} italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  applies a positive rate to states needing more mass than the current state  z t subscript z t z_{t} italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  (details in  Sec.   A.5 ). Note that  R t  subscript superscript R t R^{*}_{t} italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  is just one valid instantiation of  R t subscript R t R_{t} italic_R start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , though others exist, as detailed in  Sec.   4.2.2 .",
            "Eq.   4  employs  E p 1 | t d  ( z 1 d | z t 1 : D )  [ R t d  ( z t d , z t +   t d | z 1 d ) ] subscript E subscript superscript p d conditional 1 t conditional subscript superscript z d 1 subscript superscript z : 1 D t delimited-[] subscript superscript R d t subscript superscript z d t conditional subscript superscript z d t  t subscript superscript z d 1 \\mathbb{E}_{p^{d}_{1|t}(z^{d}_{1}|z^{1:D}_{t})}\\left[R^{d}_{t}(z^{d}_{t},z^{d}% _{t+\\Delta t}|z^{d}_{1})\\right] blackboard_E start_POSTSUBSCRIPT italic_p start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 | italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT [ italic_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_ italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ] , which requires computing the expectation under the condition  p 1 | t d  ( z 1 d | z t 1 : D ) subscript superscript p d conditional 1 t conditional subscript superscript z d 1 subscript superscript z : 1 D t p^{d}_{1|t}(z^{d}_{1}|z^{1:D}_{t}) italic_p start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 | italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) . This term consists of a prediction of the clean data  z 1 subscript z 1 z_{1} italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  from the noisy joint variable  z t 1 : D subscript superscript z : 1 D t z^{1:D}_{t} italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . However, unlike the noising process, the denoising process does not factorize across dimensions, rendering such predictions intractable in general. Instead, we use a neural network,  f  subscript f  f_{\\theta} italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT , parameterized by    \\theta italic_ , to approximate it, i.e.,  p 1 | t  , d  ( z 1 d | z t 1 : D )  p 1 | t d  ( z 1 d | z t 1 : D ) subscript superscript p  d conditional 1 t conditional subscript superscript z d 1 subscript superscript z : 1 D t subscript superscript p d conditional 1 t conditional subscript superscript z d 1 subscript superscript z : 1 D t p^{\\theta,d}_{1|t}(z^{d}_{1}|z^{1:D}_{t})\\approx p^{d}_{1|t}(z^{d}_{1}|z^{1:D}% _{t}) italic_p start_POSTSUPERSCRIPT italic_ , italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 | italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  italic_p start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 | italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) . In DFM, the network is trained by minimizing the sum of the cross-entropy losses over all variables:",
            "Generating new samples using DFM amounts to simulate the CTMC formulated with rate matrices for the denoising process. This is accomplished by sampling an initial datapoint,  z 0 1 : D subscript superscript z : 1 D 0 z^{1:D}_{0} italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , from the initial distribution  p 0 subscript p 0 p_{0} italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  and iteratively applying  Eq.   4 , using  p 1 | t  , d  ( z 1 d | z t 1 : D ) subscript superscript p  d conditional 1 t conditional subscript superscript z d 1 subscript superscript z : 1 D t p^{\\theta,d}_{1|t}(z^{d}_{1}|z^{1:D}_{t}) italic_p start_POSTSUPERSCRIPT italic_ , italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 | italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) .",
            "In DeFoGs vanilla sampling process, the discretization is performed using equally sized time steps ( Alg.   2 , line 5). Instead, we propose employing variable step sizes. This adjustment is motivated by the need for more fine-grained control during certain time intervals, for instance, to ensure that cluster structures are properly formed before focusing on intra-cluster refinements, or to prevent edge alterations that could compromise global properties once the overall structure is established. By allocating smaller, more frequent steps to these critical intervals, the generated graph can better capture the true properties of the data. To achieve this, we modify the evenly spaced step sizes using distortion functions (see  Sec.   A.1 ). Although the optimal distortion function is highly dataset-dependent, in  Sec.   B.2  we propose a method to guide the selection of the distortion function based on the observed training dynamics. Notably, in diffusion models, the design for diffusion steps such as noise schedule is typically identical for training and sampling. However, in flow models, the time distribution for training detailed in  Sec.   4.2.1  and the time steps used for sampling can be more flexibly disentangled. In practice, applying distortion only during sampling already yields notable performance improvements.",
            "with    R +  superscript R \\omega\\in\\mathbb{R}^{+} italic_  blackboard_R start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT . This adjustment biases the transitions toward the clean data state  z 1 subscript z 1 z_{1} italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT .  Lemma   10 , in  Sec.   A.2 , demonstrates that this modification introduces an  O  (  ) O  O(\\omega) italic_O ( italic_ )  violation of the Kolmogorov equation. Consequently, choosing a small value of    \\omega italic_  is experimentally shown to be highly beneficial, while a larger    \\omega italic_  restricts the distribution to regions of high probability, increasing the distance between the generated data and the training data, as indicated in  Sec.   A.4 ,  Figure   11 .",
            "The space of valid rate matrices, i.e., those that satisfy the Kolmogorov equation, is not exhausted by the original formulation of  R t   ( z t , z t + d  t | z 1 ) subscript superscript R t subscript z t conditional subscript z t d t subscript z 1 R^{*}_{t}{(z_{t},z_{t+\\mathrm{d}t}|z_{1})} italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) .  Campbell et al. ( 2024 )  investigate this and show that for any rate matrix  R t DB subscript superscript R DB t R^{\\text{DB}}_{t} italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  that satisfies the detailed balance condition,  p t | 1  ( z t | z 1 )  R t DB  ( z t , z t + d  t | z 1 ) = p t | 1  ( z t + d  t | z 1 )  R t DB  ( z t + d  t , z t | z 1 ) subscript p conditional t 1 conditional subscript z t subscript z 1 subscript superscript R DB t subscript z t conditional subscript z t d t subscript z 1 subscript p conditional t 1 conditional subscript z t d t subscript z 1 subscript superscript R DB t subscript z t d t conditional subscript z t subscript z 1 p_{t|1}(z_{t}|z_{1})R^{\\text{DB}}_{t}(z_{t},z_{t+\\text{d}t}|z_{1})=p_{t|1}(z_{% t+\\text{d}t}|z_{1})R^{\\text{DB}}_{t}(z_{t+\\text{d}t},z_{t}|z_{1}) italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_t + d italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) = italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t + d italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t + d italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , the modified rate matrix  R t  = R t  +   R t DB subscript superscript R  t subscript superscript R t  subscript superscript R DB t R^{\\eta}_{t}=R^{*}_{t}+\\eta R^{\\text{DB}}_{t} italic_R start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_ italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , with    R +  superscript R \\eta\\in\\mathbb{R}^{+} italic_  blackboard_R start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT , also satisfies the Kolmogorov equation. Increasing    \\eta italic_  introduces more stochasticity into the trajectory of the denoising process, while different designs of  R t D  B subscript superscript R D B t R^{DB}_{t} italic_R start_POSTSUPERSCRIPT italic_D italic_B end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  encode different priors for preferred transitions between states. This mechanism can be interpreted as a correction mechanism, as it enables transitions back to states that would otherwise be disallowed according to the rate matrix formulation, as described in  Sec.   A.5 . The effect of    \\eta italic_  across different datasets is illustrated in detail in  Figure   12 ,  Sec.   A.4 . To further improve the sampling performance of DeFoG, we also investigate the different formulations of  R DB superscript R DB R^{\\text{DB}} italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT  under the detailed balance condition. Additional details and discussions are provided in  Sec.   A.3 .",
            "In this section, we present novel theoretical results on general multivariate data, which are naturally extendable to our graph-based framework, as introduced in  Sec.   4.1 . Their complete versions and proofs are available in  Sec.   C.1 . We begin by presenting a theoretical result that further justifies the design choice for the loss function of DFM, and thus of DeFoG.",
            "As highlighted in  Tabs.   1 ,  2  and  8 , DeFoG attains similar performance, and in some cases even outperforms, several diffusion-based methods with just 5% or 10% of their sampling steps. This efficiency is a result of DeFoGs sampling stage flexibility, whose optimization is enabled due to the flexible and disentangled training-sampling procedure within DFM. To further demonstrate the advantages of sampling optimization,  Figure   3(a)  shows the cumulative effect of each optimization step discussed in  Sec.   4.2.2 . As illustrated, starting from a vanilla DeFoG model, which initially performs slightly worse than DiGress, we sequentially incorporate algorithmic improvements, with each stage indicated by an increasing number of  + + +  symbols. Each addition leads to a progressive improvement over the previous stage across both the Planar and QM9 datasets under various numbers of steps for sampling, with the final performance significantly surpassing the vanilla model using only 50 steps on the Planar dataset. While the benefits of each optimization component may vary across datasets, the sampling optimization pipeline remains highly efficient, enabling quick hyperparameter exploration without requiring retraining, which is a significantly more resource-intensive stage (see  Sec.   E.2 ). We provide more details on the impact of each sampling optimization strategy across datasets in  Sec.   A.4 .",
            "In this section, we explore the proposed sampling optimization in more detail. We start by analysing the different time distortion functions in  Sec.   A.1 . Next, in  Sec.   A.2 , we prove that the proposed target guidance mechanism actually satisfies the Kolmogorov equation, thus yielding valid rate matrices and, in  Sec.   A.3 , we provide more details about the detailed balance equation and how it widens the design space of rate matrices. In  Sec.   A.4 , we also describe the adopted sampling optimization pipeline. Finally, in  Sec.   A.5 , we provide more details to better clarify the dynamics of the sampling process.",
            "In  Sec.   4 , we explore the utilization of different  distortion functions , i.e., functions that are used to transform time. The key motivation for employing such functions arises from prior work on flow matching in image generation, where skewing the time distribution during training has been shown to significantly enhance empirical performance  (Esser et al.,  2024 ) . In practical terms, this implies that the model is more frequently exposed to specific time intervals. Mathematically, this transformation corresponds to introducing a time-dependent re-weighting factor in the loss function, biasing the model to achieve better performance in particular time ranges.",
            "where   t  ( t ) subscript italic- t t \\phi_{t}(t) italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_t )  and   t   ( t  ) subscript italic- superscript t  superscript t  \\phi_{t^{\\prime}}(t^{\\prime}) italic_ start_POSTSUBSCRIPT italic_t start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_t start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT )  denote the PDFs of  t t t italic_t  and  t  superscript t  t^{\\prime} italic_t start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , respectively. In our case,   t  ( t ) = 1 subscript italic- t t 1 \\phi_{t}(t)=1 italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_t ) = 1  for  t  [ 0 , 1 ] t 0 1 t\\in[0,1] italic_t  [ 0 , 1 ] . The distortion functions and their corresponding PDFs are illustrated in  Figure   4 .",
            "One of the strategies the proposed in  sampling  optimization procedure is the use of variable step sizes throughout the denoising process. This is achieved by mapping evenly spaced time points (DeFoGs vanilla version) through a transformation that follows the same constraints as the training time distortions discussed earlier. We employ the same set of time distortion functions, again not to exhaustively explore the space of applicable functions, but to gain insight into how varying step sizes affect graph generation. The expected step sizes for each distortion can be directly inferred from  Figure   4 . For instance, the polydec function leads to progressively smaller time steps, suggesting more refined graph edits in the denoising process as  t  superscript t  t^{\\prime} italic_t start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  approaches 1.",
            "A significant advantage of flow matching methods is their inherently greater flexibility in the sampling process compared to diffusion models, as they are more disentangled from the training stage. Each of the proposed optimization strategies exposed in  Sec.   4.2.2  expands the search space for optimal performance. However, conducting a full grid search across all those methodologies is impractical for the computational resources available. To address this challenge, our sampling optimization pipeline consists of, for each of the proposed optimization strategies, all hyperparameters are held constant at their default values except for the parameter controlling the chosen strategy, over which we perform a sweep. The optimal values obtained for each strategy are combined to form the final configuration. In  Tab.   6 , we present the final hyperparameter values obtained for each dataset. This pipeline is sufficient to achieve state-of-the-art performance, which reinforces the expressivity of DeFoG. We expect to achieve even better results if a more comprehensive search of the hyperparameter space was carried out.",
            "To demonstrate the benefit of each designed optimization step, we report the step-wise improvements by sequentially adding each tuned step across the primary datasets  synthetic datasets in Figure  14  and molecular datasets in Figure  16   used in this work.",
            "derived by directly differentiating  Eq.   1 . Based on this, the possible values of  R t  subscript superscript R t R^{*}_{t} italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  for different combinations of  z t subscript z t z_{t} italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and  z t + d  t subscript z t d t z_{t+\\mathrm{d}t} italic_z start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT  are outlined in  Tab.   4 .",
            "From the first two lines of  Tab.   4 , we observe that once the system reaches the predicted state  z 1 subscript z 1 z_{1} italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , it remains there. If not,  R t  subscript superscript R t R^{*}_{t} italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  only encourages transitions to other states under two conditions: either the target state is  z 1 subscript z 1 z_{1} italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  (third line), or the corresponding entries in the initial distribution for potential next states have smaller values than the current state (fourth line). As a result, the sampling dynamics are heavily influenced by the initial distribution, as discussed further in  Sec.   B.1 .",
            "In this section, we provide a more detailed analysis of the influence of the various training optimization strategies introduced in  Sec.   4.2.1 . In  Sec.   B.1 , we empirically demonstrate the impact of selecting different initial distributions on performance, while in  Sec.   B.2 , we examine the interaction between training and sampling optimization.",
            "We observe that the marginal distribution consistently achieves at least as good performance as the other initial distributions. This, along with the theoretical reasons outlined in  Sec.   4.2.1 , reinforces its use as the default initial distribution for DeFoG. The only dataset where marginal was surpassed was the SBM dataset, which we attribute to its inherently different nature (stochastic  vs.  deterministic). In this case, the absorbing distribution emerged as the best-performing choice. Interestingly, the absorbing distribution also tends to converge faster across datasets.",
            "From  Sec.   A.4 , we observe that time distortions applied during the sampling stage can significantly affect performance. This suggests that graph discrete flow models do not behave evenly across time and are more sensitive to specific time intervals, where generative performance benefits from finer updates achieved by using smaller time steps. Building on this observation, we extended our analysis to the training stage, exploring two main questions:",
            "In  Eq.   16 , we use the definition of TV distance as defined in  Eq.   14  and  Eq.   17  results from direct application of Pinskers inequality. Now, we change the ordering of the sum and of the square root through the Cauchy-Schwarz inequality:",
            "where  R t  ( z t 1 : D , z t + d  t 1 : D ) subscript R t subscript superscript z : 1 D t subscript superscript z : 1 D t d t R_{t}(z^{1:D}_{t},z^{1:D}_{t+\\mathrm{d}t}) italic_R start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT )  denotes the unconditional multivariate rate matrix defined in  Eq.   13 . This process can be simulated exactly using Gillespies Algorithm  (Gillespie,  1976 ;  1977 ) . However, such an algorithm does not scale for large  D D D italic_D   (Campbell et al.,  2022 ) . Although    \\tau italic_ -leaping is a widely adopted approximate algorithm to address this limitation  (Gillespie,  2001 ) , it requires ordinal discrete state spaces, which is suitable for cases like text or images but not for graphs. Therefore, we cannot apply it in the context of this paper. Additionally, directly replacing the infinitesimal step  d  t d t \\text{d}t d italic_t  in  Eq.   13  with a finite time step    t  t \\Delta t roman_ italic_t   a la  Euler method is inappropriate, as  R t  ( z t 1 : D , z t + d  t 1 : D ) subscript R t subscript superscript z : 1 D t subscript superscript z : 1 D t d t R_{t}(z^{1:D}_{t},z^{1:D}_{t+\\mathrm{d}t}) italic_R start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT )  prevents state transitions in more than one dimension per step under the continuous framework. Instead,  Campbell et al. ( 2024 )  propose an approximation where the Euler step is applied independently to each dimension, as seen in  Eq.   4 .",
            "where, in  Eq.   24 , we use the Mean Value Theorem, with  t c  ( t k  1 , t k ) subscript t c subscript t k 1 subscript t k t_{c}\\in(t_{k-1},t_{k}) italic_t start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT  ( italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ; in  Eq.   25 , we use the fact that there are  Z  D Z D ZD italic_Z italic_D  values of  z t + d  t 1 : D subscript superscript z : 1 D t d t z^{1:D}_{t+\\mathrm{d}t} italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT  that differ at most in only one coordinate from  z t 1 : D subscript superscript z : 1 D t z^{1:D}_{t} italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; in  Eq.   26 , we use the result from  Proposition   7  to upper bound the time derivative of the multivariate unconditional rate matrix; and finally, in  Eq.   27 , we define  B k = sup t  ( t k  1 , t k ) B t subscript B k subscript supremum t subscript t k 1 subscript t k subscript B t B_{k}=\\sup_{t\\in(t_{k-1},t_{k})}B_{t} italic_B start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = roman_sup start_POSTSUBSCRIPT italic_t  ( italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT italic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and    t k = t k  t k  1  subscript t k subscript t k subscript t k 1 \\Delta t_{k}=t_{k}-t_{k-1} roman_ italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT .",
            "On the other hand, we have from  Eq.   4  that sampling with the Euler approximation in multivariate Markov chain corresponds to:",
            "where    \\gamma italic_  denotes the guidance weight. In particular, the case with   = 1  1 \\gamma=1 italic_ = 1  corresponds to standard conditional generation, while   = 0  0 \\gamma=0 italic_ = 0  represents standard unconditional generation. As    \\gamma italic_  increases, the conditioning effect described by  ( R t   ( x , x ~ | y ) R t   ( x , x ~ ) )   1 superscript subscript superscript R  t x conditional ~ x y subscript superscript R  t x ~ x  1 {\\left(\\frac{R^{\\theta}_{t}(x,\\tilde{x}|y)}{R^{\\theta}_{t}(x,\\tilde{x})}\\right% )}^{\\gamma-1} ( divide start_ARG italic_R start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_x , over~ start_ARG italic_x end_ARG | italic_y ) end_ARG start_ARG italic_R start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_x , over~ start_ARG italic_x end_ARG ) end_ARG ) start_POSTSUPERSCRIPT italic_ - 1 end_POSTSUPERSCRIPT  is strengthened, thereby enhancing the quality of the generated samples. We observed   = 2.0  2.0 \\gamma=2.0 italic_ = 2.0  to be the best performing value for our digital pathology experiments ( Sec.   6.4 ), as detailed in  Tab.   6 .",
            "The default hyperparameters for training and sampling for each dataset can be found in the provided code repository. In  Tab.   6 , we specifically highlight their values for the proposed training (see  Sec.   4.2.1 ) and sampling (see  Sec.   4.2.2 ) strategies, and conditional guidance parameter (see  Appendix   D ). As the training process is by far the most computationally costly stage, we aim to minimize changes to the default model training configuration. Nevertheless, we demonstrate the effectiveness of these modifications on certain datasets:"
        ]
    },
    "id_table_5": {
        "caption": "Table 6:  Training and sampling parameters for full-step sampling (500 or 1000 steps for synthetic and molecular datasets respectively).",
        "table": "A6.EGx2",
        "footnotes": [],
        "references": [
            "and  Z t > 0 = | { z t : p t | 1 ( z t | z 1 ) > 0 } | Z^{>0}_{t}=|\\{z_{t}:p_{t|1}(z_{t}|z_{1})>0\\}| italic_Z start_POSTSUPERSCRIPT > 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = | { italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT : italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) > 0 } | . Again, normalization is performed for the case  z t = z t + d  t subscript z t subscript z t d t z_{t}=z_{t+\\mathrm{d}t} italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_z start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT . Intuitively,  R t  subscript superscript R t R^{*}_{t} italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  applies a positive rate to states needing more mass than the current state  z t subscript z t z_{t} italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  (details in  Sec.   A.5 ). Note that  R t  subscript superscript R t R^{*}_{t} italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  is just one valid instantiation of  R t subscript R t R_{t} italic_R start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , though others exist, as detailed in  Sec.   4.2.2 .",
            "The nodes and edges of graphs encode distinct structural information, justifying their differential treatment in a graph-specific generative model. This distinction should be reflected in the training loss function. We define  L DeFoG = E t  U  [ 0 , 1 ] , p 1  ( G 1 ) , p t | 1  ( G t | G 1 )  CE   ( G 1 , p 1 | t   ( G t ) ) subscript L DeFoG subscript E similar-to t U 0 1 subscript p 1 subscript G 1 subscript p conditional t 1 conditional subscript G t subscript G 1 subscript CE  subscript G 1 subscript superscript p  conditional 1 t subscript G t \\mathcal{L}_{\\text{DeFoG}}=\\mathbb{E}_{t\\sim\\mathcal{U}[0,1],p_{1}(G_{1}),p_{t% |1}(G_{t}|G_{1})}\\operatorname{CE}_{\\lambda}(G_{1},p^{\\theta}_{1|t}(G_{t})) caligraphic_L start_POSTSUBSCRIPT DeFoG end_POSTSUBSCRIPT = blackboard_E start_POSTSUBSCRIPT italic_t  caligraphic_U [ 0 , 1 ] , italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_G start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_G start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_G start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT roman_CE start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_G start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_p start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 | italic_t end_POSTSUBSCRIPT ( italic_G start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) )  similarly to  L DFM subscript L DFM \\mathcal{L}_{\\text{DFM}} caligraphic_L start_POSTSUBSCRIPT DFM end_POSTSUBSCRIPT , in  Eq.   5 , with  CE  subscript CE  \\operatorname{CE}_{\\lambda} roman_CE start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  defined as follows:",
            "The space of valid rate matrices, i.e., those that satisfy the Kolmogorov equation, is not exhausted by the original formulation of  R t   ( z t , z t + d  t | z 1 ) subscript superscript R t subscript z t conditional subscript z t d t subscript z 1 R^{*}_{t}{(z_{t},z_{t+\\mathrm{d}t}|z_{1})} italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) .  Campbell et al. ( 2024 )  investigate this and show that for any rate matrix  R t DB subscript superscript R DB t R^{\\text{DB}}_{t} italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  that satisfies the detailed balance condition,  p t | 1  ( z t | z 1 )  R t DB  ( z t , z t + d  t | z 1 ) = p t | 1  ( z t + d  t | z 1 )  R t DB  ( z t + d  t , z t | z 1 ) subscript p conditional t 1 conditional subscript z t subscript z 1 subscript superscript R DB t subscript z t conditional subscript z t d t subscript z 1 subscript p conditional t 1 conditional subscript z t d t subscript z 1 subscript superscript R DB t subscript z t d t conditional subscript z t subscript z 1 p_{t|1}(z_{t}|z_{1})R^{\\text{DB}}_{t}(z_{t},z_{t+\\text{d}t}|z_{1})=p_{t|1}(z_{% t+\\text{d}t}|z_{1})R^{\\text{DB}}_{t}(z_{t+\\text{d}t},z_{t}|z_{1}) italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_t + d italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) = italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t + d italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t + d italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , the modified rate matrix  R t  = R t  +   R t DB subscript superscript R  t subscript superscript R t  subscript superscript R DB t R^{\\eta}_{t}=R^{*}_{t}+\\eta R^{\\text{DB}}_{t} italic_R start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_ italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , with    R +  superscript R \\eta\\in\\mathbb{R}^{+} italic_  blackboard_R start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT , also satisfies the Kolmogorov equation. Increasing    \\eta italic_  introduces more stochasticity into the trajectory of the denoising process, while different designs of  R t D  B subscript superscript R D B t R^{DB}_{t} italic_R start_POSTSUPERSCRIPT italic_D italic_B end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  encode different priors for preferred transitions between states. This mechanism can be interpreted as a correction mechanism, as it enables transitions back to states that would otherwise be disallowed according to the rate matrix formulation, as described in  Sec.   A.5 . The effect of    \\eta italic_  across different datasets is illustrated in detail in  Figure   12 ,  Sec.   A.4 . To further improve the sampling performance of DeFoG, we also investigate the different formulations of  R DB superscript R DB R^{\\text{DB}} italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT  under the detailed balance condition. Additional details and discussions are provided in  Sec.   A.3 .",
            "By taking the expectation over  t  U  [ 0 , 1 ] similar-to t U 0 1 t\\sim\\mathcal{U}[0,1] italic_t  caligraphic_U [ 0 , 1 ]  and summing over the resulting  z t 1 : D subscript superscript z : 1 D t z^{1:D}_{t} italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , minimizing the derived upper bound with respect to    \\theta italic_  shown in the right-hand side (RHS) of  Eq.   8  corresponds directly to minimizing the DFM loss in  Eq.   5 . This further reinforces the pertinence of using such loss function, beyond its ELBO maximization motivation as proposed by  Campbell et al. ( 2024 ) .",
            "In this section, we explore the proposed sampling optimization in more detail. We start by analysing the different time distortion functions in  Sec.   A.1 . Next, in  Sec.   A.2 , we prove that the proposed target guidance mechanism actually satisfies the Kolmogorov equation, thus yielding valid rate matrices and, in  Sec.   A.3 , we provide more details about the detailed balance equation and how it widens the design space of rate matrices. In  Sec.   A.4 , we also describe the adopted sampling optimization pipeline. Finally, in  Sec.   A.5 , we provide more details to better clarify the dynamics of the sampling process.",
            "A natural question is how to choose a suitable design for  R t DB subscript superscript R DB t R^{\\text{DB}}_{t} italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  from the infinite space of detailed balance rate matrices. As depicted in  Figure   5 , this flexibility can be leveraged to incorporate priors into the denoising model by encouraging specific transitions between states. By adjusting the sparsity of the matrix entries, additional transitions beyond those prescribed by  R t  subscript superscript R t R^{*}_{t} italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  can be introduced. In the general case, transitions between all states are possible; in the column case, a specific state centralizes all potential transitions; and in the single-entry case, only transitions between two states are permitted. These examples merely illustrate some possibilities and do not exhaust the range of potential  R t DB subscript superscript R DB t R^{\\text{DB}}_{t} italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  designs. The matrix entries can be structured by considering the following reorganization of terms of  Eq.   12 :",
            "We incorporated various types of priors into  R DB superscript R DB R^{\\text{DB}} italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT  by preserving specific rows or entries in the matrix. Specifically, we experimented with retaining the column corresponding to the state with the highest marginal distribution (Column - Max Marginal), the column corresponding to the predicted  x 1 subscript x 1 x_{1} italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  states (Column -  x 1 subscript x 1 x_{1} italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ), and the columns corresponding to the state with the highest probability in  p t | 1 subscript p conditional t 1 p_{t|1} italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT . Additionally, we tested the approach of retaining only  R DB  ( x t , i ) superscript R DB subscript x t i R^{\\text{DB}}(x_{t},i) italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_i )  where  i i i italic_i  is the state with the highest marginal distribution (Entry - Max Marginal). For instance, under the absorbing initial distribution, this state is the one to which all data is absorbed at  t = 0 t 0 t=0 italic_t = 0 . We note that there remains significant space for exploration by adjusting the weights assigned to different positions within  R DB superscript R DB R^{\\text{DB}} italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT , as the only condition that must be satisfied is that symmetrical positions adhere to a specific proportionality. However, in practice, none of the specific designs illustrated in  Figure   5  showed a clear advantage over others in the settings we evaluated. As a result, we chose the general case for our experiments, as it offers the most flexibility by incorporating the least prior knowledge.",
            "Under DeFoGs framework, the noising process for each dimension is modeled as a linear interpolation between the clean data distribution (the one-hot representation of the current state) and an initial distribution,  p 0 subscript p 0 p_{0} italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT . As such, it is intuitive that different initial distributions result in varying performances, depending on the denoising dynamics they induce. In particular, they have a direct impact on the sampling dynamics through  R t  subscript superscript R t R^{*}_{t} italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  (see  Sec.   A.5 ) and may also pose tasks of varying difficulty for the graph transformer. In this paper, we explore four distinct initial distributions 2 2 2 Recall that  Z Z Z italic_Z  represents the cardinality of the state space, and   Z  1 superscript  Z 1 \\Delta^{Z-1} roman_ start_POSTSUPERSCRIPT italic_Z - 1 end_POSTSUPERSCRIPT  the associated probability simplex. :",
            "We now upper bound the time derivative of the  unconditonal  multivariate rate matrix. We use  Lemma   5  as an intermediate result to accomplish so. Additionally, we consider the following assumption.",
            "where in the first inequality triangular we apply triangular inequality; in the second inequality, we use  Lemma   5  and in  6  to upper bound  |  t R t  d ( z t d , z t + d  t d | z 1 d ) | |\\partial_{t}R^{*d}_{t}(z^{d}_{t},z^{d}_{t+\\mathrm{d}t}|z^{d}_{1})| |  start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_R start_POSTSUPERSCRIPT  italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) |  and  p  ( z 1 d | z t 1 : D ) p conditional subscript superscript z d 1 subscript superscript z : 1 D t p(z^{d}_{1}|z^{1:D}_{t}) italic_p ( italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , respectively.",
            "where, in  Eq.   24 , we use the Mean Value Theorem, with  t c  ( t k  1 , t k ) subscript t c subscript t k 1 subscript t k t_{c}\\in(t_{k-1},t_{k}) italic_t start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT  ( italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ; in  Eq.   25 , we use the fact that there are  Z  D Z D ZD italic_Z italic_D  values of  z t + d  t 1 : D subscript superscript z : 1 D t d t z^{1:D}_{t+\\mathrm{d}t} italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT  that differ at most in only one coordinate from  z t 1 : D subscript superscript z : 1 D t z^{1:D}_{t} italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; in  Eq.   26 , we use the result from  Proposition   7  to upper bound the time derivative of the multivariate unconditional rate matrix; and finally, in  Eq.   27 , we define  B k = sup t  ( t k  1 , t k ) B t subscript B k subscript supremum t subscript t k 1 subscript t k subscript B t B_{k}=\\sup_{t\\in(t_{k-1},t_{k})}B_{t} italic_B start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = roman_sup start_POSTSUBSCRIPT italic_t  ( italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT italic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and    t k = t k  t k  1  subscript t k subscript t k subscript t k 1 \\Delta t_{k}=t_{k}-t_{k-1} roman_ italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT .",
            "i.e., the square root of the right-hand side of  Eq.   15 .",
            "Here, we describe the datasets employed in our experiments and outline the specific metrics used to evaluate model performance on each dataset. Additional visualizations of example graphs from each dataset, along with generated graphs, are provided in  Figures   21 ,  23  and  25 .",
            "The training and sampling times for the different datasets explored in this paper are provided in  Tab.   5 . All the experiments in this work were run on a single NVIDIA A100-SXM4-80GB GPU."
        ]
    },
    "id_table_6": {
        "caption": "Table 7:   Graph Generation Performance on Synthetic Graphs. We present the results of DeFoG across five sampling runs, each generating 40 graphs, reported as mean  standard deviation. The remaining values are obtained from  Bergmeister et al. ( 2023 ) . Additionally, we include results for Cometh  (Siraudin et al.,  2024 )  and DisCo  (Xu et al.,  2024 ) . For the average ratio computation, we adhere to the method outlined by  Bergmeister et al. ( 2023 ) , excluding statistics where the training set MMD is zero.",
        "table": "A6.EGx3",
        "footnotes": [
            "",
            "",
            "",
            ""
        ],
        "references": [
            "While our primary focus is on optimizing the sampling process given its computational efficiency, we emphasize that DeFoGs training optimization strategies can further push its performance. To illustrate this,  Figure   3(b)  shows the convergence curves for the tree and MOSES datasets. We first observe that, with the same model, leveraging only sampling distortion can further enhance its performance beyond the vanilla implementation. This suggests that optimizing the sampling procedure can be particularly useful in settings where computational resources are limited and the model is undertrained, as detailed in  Sec.   A.6 . Moreover, when an appropriate sample distortion is known (e.g.,  polydec  distortion is shown to be particularly preferred across molecular datasets), applying it to both train distortion and sample distortion typically further improves performance. Although DeFoG achieves faster convergence with both distortions, we remark that DiGress also shows good convergence speed on the Tree dataset, due to the well-tuned noise schedule employed for joint training and sampling. The mutual impact of training and sampling distortion is discussed in  Sec.   B.2 . Besides, in the same section, we discuss a heuristic on which distortion to use for each dataset according to DeFoGs vanilla dynamics. Further details on how the initial distribution affects training efficiency are provided in  Sec.   B.1 .",
            "Code is available at  github.com/manuelmlmadeira/DeFoG . We include detailed descriptions of the datasets, experimental setup and methodologies used in  Secs.   6  and  E . In particular, the specific hyperparameters employed in our experiments are documented in this paper in  Sec.   E.3  and in the code repository. The full version and proofs of theoretical results can be found in  Appendix   C .",
            "A significant advantage of flow matching methods is their inherently greater flexibility in the sampling process compared to diffusion models, as they are more disentangled from the training stage. Each of the proposed optimization strategies exposed in  Sec.   4.2.2  expands the search space for optimal performance. However, conducting a full grid search across all those methodologies is impractical for the computational resources available. To address this challenge, our sampling optimization pipeline consists of, for each of the proposed optimization strategies, all hyperparameters are held constant at their default values except for the parameter controlling the chosen strategy, over which we perform a sweep. The optimal values obtained for each strategy are combined to form the final configuration. In  Tab.   6 , we present the final hyperparameter values obtained for each dataset. This pipeline is sufficient to achieve state-of-the-art performance, which reinforces the expressivity of DeFoG. We expect to achieve even better results if a more comprehensive search of the hyperparameter space was carried out.",
            "To demonstrate the benefit of each designed optimization step, we report the step-wise improvements by sequentially adding each tuned step across the primary datasets  synthetic datasets in Figure  14  and molecular datasets in Figure  16   used in this work.",
            "In this section, we present the performance of a model trained on the QM9 dataset and the Planar dataset using only  30 % percent 30 30\\% 30 %  of the epochs compared to the final model being reported. We employ the same hyperparameters with  Tab.   8  and  Tab.   1  for the sampling setup, as reported in  Tab.   6 .",
            "In  Eq.   16 , we use the definition of TV distance as defined in  Eq.   14  and  Eq.   17  results from direct application of Pinskers inequality. Now, we change the ordering of the sum and of the square root through the Cauchy-Schwarz inequality:",
            "For  z t 1 : D , z t + d  t 1 : D  Z D subscript superscript z : 1 D t subscript superscript z : 1 D t d t superscript Z D z^{1:D}_{t},z^{1:D}_{t+\\mathrm{d}t}\\in\\mathcal{Z}^{D} italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT  caligraphic_Z start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT  and  t  ( 0 , 1 ) t 0 1 t\\in(0,1) italic_t  ( 0 , 1 ) , under  6 , we have:",
            "where in the first inequality triangular we apply triangular inequality; in the second inequality, we use  Lemma   5  and in  6  to upper bound  |  t R t  d ( z t d , z t + d  t d | z 1 d ) | |\\partial_{t}R^{*d}_{t}(z^{d}_{t},z^{d}_{t+\\mathrm{d}t}|z^{d}_{1})| |  start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_R start_POSTSUPERSCRIPT  italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) |  and  p  ( z 1 d | z t 1 : D ) p conditional subscript superscript z d 1 subscript superscript z : 1 D t p(z^{d}_{1}|z^{1:D}_{t}) italic_p ( italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , respectively.",
            "Let  { z t 1 : D } t  [ 0 , 1 ]  Z D  [ 0 , 1 ] subscript subscript superscript z : 1 D t t 0 1 superscript Z D 0 1 \\{z^{1:D}_{t}\\}_{t\\in[0,1]}\\in\\mathcal{Z}^{D}\\times[0,1] { italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_t  [ 0 , 1 ] end_POSTSUBSCRIPT  caligraphic_Z start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT  [ 0 , 1 ]  be a CTMC starting with  p  ( z 0 1 : D ) = p  p subscript superscript z : 1 D 0 subscript p italic- p(z^{1:D}_{0})=p_{\\epsilon} italic_p ( italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = italic_p start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  and ending with  p  ( z 1 1 : D ) = p data p subscript superscript z : 1 D 1 subscript p data p(z^{1:D}_{1})=p_{\\text{data}} italic_p ( italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) = italic_p start_POSTSUBSCRIPT data end_POSTSUBSCRIPT , whose groundtruth rate matrix is  R t subscript R t R_{t} italic_R start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . Additionally, let  ( y k 1 : D ) k = 0 , 1 , ... , K subscript subscript superscript y : 1 D k k 0 1 ... K (y^{1:D}_{k})_{k=0,1,\\ldots,K} ( italic_y start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_k = 0 , 1 , ... , italic_K end_POSTSUBSCRIPT  be a Euler sampling approximation of that CTMC, with maximum step size    T = sup k   t k  T subscript supremum k  subscript t k \\Delta T=\\sup_{k}\\Delta t_{k} roman_ italic_T = roman_sup start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT roman_ italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT  and an approximate rate matrix  R t  subscript superscript R  t R^{\\theta}_{t} italic_R start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . Then, under  6 , the following total variation bound holds:",
            "where, in  Eq.   24 , we use the Mean Value Theorem, with  t c  ( t k  1 , t k ) subscript t c subscript t k 1 subscript t k t_{c}\\in(t_{k-1},t_{k}) italic_t start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT  ( italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ; in  Eq.   25 , we use the fact that there are  Z  D Z D ZD italic_Z italic_D  values of  z t + d  t 1 : D subscript superscript z : 1 D t d t z^{1:D}_{t+\\mathrm{d}t} italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT  that differ at most in only one coordinate from  z t 1 : D subscript superscript z : 1 D t z^{1:D}_{t} italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; in  Eq.   26 , we use the result from  Proposition   7  to upper bound the time derivative of the multivariate unconditional rate matrix; and finally, in  Eq.   27 , we define  B k = sup t  ( t k  1 , t k ) B t subscript B k subscript supremum t subscript t k 1 subscript t k subscript B t B_{k}=\\sup_{t\\in(t_{k-1},t_{k})}B_{t} italic_B start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = roman_sup start_POSTSUBSCRIPT italic_t  ( italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT italic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and    t k = t k  t k  1  subscript t k subscript t k subscript t k 1 \\Delta t_{k}=t_{k}-t_{k-1} roman_ italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT .",
            "where    \\gamma italic_  denotes the guidance weight. In particular, the case with   = 1  1 \\gamma=1 italic_ = 1  corresponds to standard conditional generation, while   = 0  0 \\gamma=0 italic_ = 0  represents standard unconditional generation. As    \\gamma italic_  increases, the conditioning effect described by  ( R t   ( x , x ~ | y ) R t   ( x , x ~ ) )   1 superscript subscript superscript R  t x conditional ~ x y subscript superscript R  t x ~ x  1 {\\left(\\frac{R^{\\theta}_{t}(x,\\tilde{x}|y)}{R^{\\theta}_{t}(x,\\tilde{x})}\\right% )}^{\\gamma-1} ( divide start_ARG italic_R start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_x , over~ start_ARG italic_x end_ARG | italic_y ) end_ARG start_ARG italic_R start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_x , over~ start_ARG italic_x end_ARG ) end_ARG ) start_POSTSUPERSCRIPT italic_ - 1 end_POSTSUPERSCRIPT  is strengthened, thereby enhancing the quality of the generated samples. We observed   = 2.0  2.0 \\gamma=2.0 italic_ = 2.0  to be the best performing value for our digital pathology experiments ( Sec.   6.4 ), as detailed in  Tab.   6 .",
            "The default hyperparameters for training and sampling for each dataset can be found in the provided code repository. In  Tab.   6 , we specifically highlight their values for the proposed training (see  Sec.   4.2.1 ) and sampling (see  Sec.   4.2.2 ) strategies, and conditional guidance parameter (see  Appendix   D ). As the training process is by far the most computationally costly stage, we aim to minimize changes to the default model training configuration. Nevertheless, we demonstrate the effectiveness of these modifications on certain datasets:"
        ]
    },
    "id_table_7": {
        "caption": "Table 8:  Molecule generation on QM9. We present the results over five sampling runs of 10000 generated graphs each, in the format mean  standard deviation.",
        "table": "A6.EGx4",
        "footnotes": [],
        "references": [
            "In  Figure   17 , we present the training curves for each initial distribution for three different datasets.",
            "In  Eq.   16 , we use the definition of TV distance as defined in  Eq.   14  and  Eq.   17  results from direct application of Pinskers inequality. Now, we change the ordering of the sum and of the square root through the Cauchy-Schwarz inequality:",
            "where, in  Eq.   24 , we use the Mean Value Theorem, with  t c  ( t k  1 , t k ) subscript t c subscript t k 1 subscript t k t_{c}\\in(t_{k-1},t_{k}) italic_t start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT  ( italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ; in  Eq.   25 , we use the fact that there are  Z  D Z D ZD italic_Z italic_D  values of  z t + d  t 1 : D subscript superscript z : 1 D t d t z^{1:D}_{t+\\mathrm{d}t} italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT  that differ at most in only one coordinate from  z t 1 : D subscript superscript z : 1 D t z^{1:D}_{t} italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; in  Eq.   26 , we use the result from  Proposition   7  to upper bound the time derivative of the multivariate unconditional rate matrix; and finally, in  Eq.   27 , we define  B k = sup t  ( t k  1 , t k ) B t subscript B k subscript supremum t subscript t k 1 subscript t k subscript B t B_{k}=\\sup_{t\\in(t_{k-1},t_{k})}B_{t} italic_B start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = roman_sup start_POSTSUBSCRIPT italic_t  ( italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT italic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and    t k = t k  t k  1  subscript t k subscript t k subscript t k 1 \\Delta t_{k}=t_{k}-t_{k-1} roman_ italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT .",
            "Therefore, we get the intended result by gathering the results from  Eq.   27 ,  Eq.   29 , and  Eq.   31 .",
            "This section explains the expressivity of the RRWP features used in DeFoG. We summarize the findings of  Ma et al. ( 2023 )  in  Proposition   7 , who establish that, by encoding random walk probabilities, the RRWP positional features can be used to arbitrarily approximate several essential graph properties when fed into an MLP. Specifically, point 1 shows that RRWP with  K  1 K 1 K-1 italic_K - 1  steps encodes all shortest path distances for nodes up to  K  1 K 1 K-1 italic_K - 1  hops. Additionally, points 2 and 3 indicate that RRWP features effectively capture diverse graph propagation dynamics.",
            "In  Tab.   7 , we present the full results for DeFoG for the three different datasets: planar, tree, and SBM."
        ]
    },
    "id_table_8": {
        "caption": "Table 9:  Molecule generation on MOSES.",
        "table": "A6.EGx5",
        "footnotes": [],
        "references": [
            "By taking the expectation over  t  U  [ 0 , 1 ] similar-to t U 0 1 t\\sim\\mathcal{U}[0,1] italic_t  caligraphic_U [ 0 , 1 ]  and summing over the resulting  z t 1 : D subscript superscript z : 1 D t z^{1:D}_{t} italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , minimizing the derived upper bound with respect to    \\theta italic_  shown in the right-hand side (RHS) of  Eq.   8  corresponds directly to minimizing the DFM loss in  Eq.   5 . This further reinforces the pertinence of using such loss function, beyond its ELBO maximization motivation as proposed by  Campbell et al. ( 2024 ) .",
            "with  U U U italic_U  and  B B B italic_B  representing constant upper bounds for the right-hand side of  Eq.   8  and for the denoising process relative to its noising counterpart, respectively, for any  t  [ 0 , 1 ] t 0 1 t\\in[0,1] italic_t  [ 0 , 1 ] .",
            "Molecular design is a prominent real-world application of graph generation. We evaluate DeFoGs performance on this task using the QM9  (Wu et al.,  2018 ) , MOSES  (Polykovskiy et al.,  2020 ) , and Guacamol  (Brown et al.,  2019 )  datasets. For QM9, we follow the dataset split and evaluation metrics from  Vignac et al. ( 2022 ) , presenting the results in  Sec.   E.1.2 ,  Tab.   8 . For the larger MOSES and Guacamol datasets, we adhere to the training setup and evaluation metrics established by  Polykovskiy et al. ( 2020 )  and  Brown et al. ( 2019 ) , respectively, with results in  Tabs.   9  and  10 .",
            "As highlighted in  Tabs.   1 ,  2  and  8 , DeFoG attains similar performance, and in some cases even outperforms, several diffusion-based methods with just 5% or 10% of their sampling steps. This efficiency is a result of DeFoGs sampling stage flexibility, whose optimization is enabled due to the flexible and disentangled training-sampling procedure within DFM. To further demonstrate the advantages of sampling optimization,  Figure   3(a)  shows the cumulative effect of each optimization step discussed in  Sec.   4.2.2 . As illustrated, starting from a vanilla DeFoG model, which initially performs slightly worse than DiGress, we sequentially incorporate algorithmic improvements, with each stage indicated by an increasing number of  + + +  symbols. Each addition leads to a progressive improvement over the previous stage across both the Planar and QM9 datasets under various numbers of steps for sampling, with the final performance significantly surpassing the vanilla model using only 50 steps on the Planar dataset. While the benefits of each optimization component may vary across datasets, the sampling optimization pipeline remains highly efficient, enabling quick hyperparameter exploration without requiring retraining, which is a significantly more resource-intensive stage (see  Sec.   E.2 ). We provide more details on the impact of each sampling optimization strategy across datasets in  Sec.   A.4 .",
            "To better mode detailedly illustrate the influence of each sampling optimization, we show in  Figures   8  and  10  in present the impact of varying parameter values across the synthetic datasets  Figure   8  and molecular datasets in Figure  10  used in this work.",
            "While Figures  8  and  10  are highly condensed, we provide a more fine-grained version that specifically illustrates the influence of the hyperparameters    \\eta italic_  and    \\omega italic_ . This version highlights their impact when generating with the full number of steps (500 and 1000 for molecular and synthetic data, respectively) and with 50 steps. As emphasized in Figures  11  and  12 , the influence of these hyperparameters varies across datasets and exhibits distinct behaviors depending on the number of steps used.",
            "In this section, we present the performance of a model trained on the QM9 dataset and the Planar dataset using only  30 % percent 30 30\\% 30 %  of the epochs compared to the final model being reported. We employ the same hyperparameters with  Tab.   8  and  Tab.   1  for the sampling setup, as reported in  Tab.   6 .",
            "To investigate these questions, we conducted a grid search. For two datasets, we trained five models, each with a different time distortion applied during training. Subsequently, we tested each model by applying the five different distortions at the sampling stage. The results are presented in  Figure   18 .",
            "As expected, the curve of training loss as a function of  t t t italic_t  (left in  Figure   19 ) is monotonically decreasing, indicating that as graphs are decreasingly noised, the task becomes simpler. However, the most interesting insights arise from the evolution of this curve across epochs (right in  Figure   19 ). We observe that for smaller values of  t t t italic_t , the model reaches its maximum capacity early in the training process, showing no significant improvements after the initial few epochs. In contrast, for larger values of  t t t italic_t  (closer to  t = 1 t 1 t=1 italic_t = 1 ), the model exhibits substantial improvements throughout the training period. This suggests that the model can continue to refine its predictions in the time range where the task is easier. These findings align with those in  Figure   18 , reinforcing our expectation that training the model to be more precise in this range or providing more refined sampling steps will naturally enhance performance in the planar dataset.",
            "where, in  Eq.   28 , we use again the fact that there are  Z  D Z D ZD italic_Z italic_D  values of  z t + d  t 1 : D subscript superscript z : 1 D t d t z^{1:D}_{t+\\mathrm{d}t} italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT  that differ at most in only one coordinate from  z t 1 : D subscript superscript z : 1 D t z^{1:D}_{t} italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  along with the estimation error upper bound from  Theorem   2 . In particular, we consider  U k = sup t  [ t k  1 , t k ] , z t 1 : D , z t + d  t 1 : D  Z D  U k z t 1 : D  z t + d  t 1 : D subscript U k t subscript t k 1 subscript t k subscript superscript z : 1 D t subscript superscript z : 1 D t d t superscript Z D supremum subscript superscript U  subscript superscript z : 1 D t subscript superscript z : 1 D t d t k U_{k}=\\underset{\\begin{subarray}{c}t\\in[t_{k-1},t_{k}],\\;\\\\ z^{1:D}_{t},\\;z^{1:D}_{t+\\mathrm{d}t}\\in\\mathcal{Z}^{D}\\end{subarray}}{\\sup}U^% {z^{1:D}_{t}\\to z^{1:D}_{t+\\mathrm{d}t}}_{k} italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = start_UNDERACCENT start_ARG start_ROW start_CELL italic_t  [ italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ] , end_CELL end_ROW start_ROW start_CELL italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT  caligraphic_Z start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_CELL end_ROW end_ARG end_UNDERACCENT start_ARG roman_sup end_ARG italic_U start_POSTSUPERSCRIPT italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , with:",
            "For the molecular generation tasks, we begin by examining the results for QM9, considering both implicit and explicit hydrogens  (Vignac et al.,  2022 ) . In the implicit case, hydrogen atoms are inferred to complete the valencies, while in the explicit case, hydrogens must be explicitly modeled, making it an inherently more challenging task. The results are presented in  Tab.   8 . Notably, DeFoG achieves training set validity in both scenarios, representing the theoretical maximum. Furthermore, DeFoG consistently outperforms other models in terms of FCD. Remarkably, even with only 10% of the sampling steps, DeFoG surpasses many existing methods."
        ]
    },
    "id_table_9": {
        "caption": "Table 10:  Molecule generation on GuacaMol. We present the results over five sampling runs of 10000 generated graphs each, in the format mean  standard deviation.",
        "table": "A1.T4.22.16",
        "footnotes": [],
        "references": [
            "Molecular design is a prominent real-world application of graph generation. We evaluate DeFoGs performance on this task using the QM9  (Wu et al.,  2018 ) , MOSES  (Polykovskiy et al.,  2020 ) , and Guacamol  (Brown et al.,  2019 )  datasets. For QM9, we follow the dataset split and evaluation metrics from  Vignac et al. ( 2022 ) , presenting the results in  Sec.   E.1.2 ,  Tab.   8 . For the larger MOSES and Guacamol datasets, we adhere to the training setup and evaluation metrics established by  Polykovskiy et al. ( 2020 )  and  Brown et al. ( 2019 ) , respectively, with results in  Tabs.   9  and  10 .",
            "To determine if the structural properties observed in datasets like the planar dataset can be detected and exploited without requiring an exhaustive sweep over all possibilities, we propose developing a metric that quantifies the difficulty of predicting the clean graph for any given time point  t  [ 0 , 1 ) t 0 1 t\\in[0,1) italic_t  [ 0 , 1 ) . For this, we perform a sweep over  t t t italic_t  for a given model, where for each  t t t italic_t , we noise a sufficiently large batch of clean graphs and evaluate the models training loss on them. This yields a curve that shows how the training loss varies as a function of  t t t italic_t . We then track how this curve evolves across epochs. To make the changes more explicit, we compute the ratio of the loss curve relative to the fully trained models values. These curves are shown in  Figure   19 .",
            "As expected, the curve of training loss as a function of  t t t italic_t  (left in  Figure   19 ) is monotonically decreasing, indicating that as graphs are decreasingly noised, the task becomes simpler. However, the most interesting insights arise from the evolution of this curve across epochs (right in  Figure   19 ). We observe that for smaller values of  t t t italic_t , the model reaches its maximum capacity early in the training process, showing no significant improvements after the initial few epochs. In contrast, for larger values of  t t t italic_t  (closer to  t = 1 t 1 t=1 italic_t = 1 ), the model exhibits substantial improvements throughout the training period. This suggests that the model can continue to refine its predictions in the time range where the task is easier. These findings align with those in  Figure   18 , reinforcing our expectation that training the model to be more precise in this range or providing more refined sampling steps will naturally enhance performance in the planar dataset.",
            "It remains to bound the second term from  Eq.   23 . We start by analyzing the Markov kernel corresponding to a Markov chain with  constant  rate matrix  R t k  1  subscript superscript R  subscript t k 1 R^{\\theta}_{t_{k-1}} italic_R start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT  between  t k  1 subscript t k 1 t_{k-1} italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT  and  t k subscript t k t_{k} italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT . In that case, from  Eq.   19  we obtain:",
            "Therefore, we get the intended result by gathering the results from  Eq.   27 ,  Eq.   29 , and  Eq.   31 .",
            "Additionally, we provide the complete version of  Tab.   2 , presenting the results for MOSES and Guacamol separately in  Tab.   9  and  Tab.   10 , respectively. We include models from classes beyond diffusion models to better contextualize the performance achieved by DeFoG."
        ]
    },
    "id_table_10": {
        "caption": "Table 11:  Computation time for different additional features.",
        "table": "A6.EGx6",
        "footnotes": [],
        "references": [
            "with    R +  superscript R \\omega\\in\\mathbb{R}^{+} italic_  blackboard_R start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT . This adjustment biases the transitions toward the clean data state  z 1 subscript z 1 z_{1} italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT .  Lemma   10 , in  Sec.   A.2 , demonstrates that this modification introduces an  O  (  ) O  O(\\omega) italic_O ( italic_ )  violation of the Kolmogorov equation. Consequently, choosing a small value of    \\omega italic_  is experimentally shown to be highly beneficial, while a larger    \\omega italic_  restricts the distribution to regions of high probability, increasing the distance between the generated data and the training data, as indicated in  Sec.   A.4 ,  Figure   11 .",
            "Molecular design is a prominent real-world application of graph generation. We evaluate DeFoGs performance on this task using the QM9  (Wu et al.,  2018 ) , MOSES  (Polykovskiy et al.,  2020 ) , and Guacamol  (Brown et al.,  2019 )  datasets. For QM9, we follow the dataset split and evaluation metrics from  Vignac et al. ( 2022 ) , presenting the results in  Sec.   E.1.2 ,  Tab.   8 . For the larger MOSES and Guacamol datasets, we adhere to the training setup and evaluation metrics established by  Polykovskiy et al. ( 2020 )  and  Brown et al. ( 2019 ) , respectively, with results in  Tabs.   9  and  10 .",
            "still satisfy the Kolmogorov equation. The detailed balance condition ensures that the outflow,  p t | 1  ( z t | z 1 )  R t DB  ( z t , z t + d  t | z 1 ) subscript p conditional t 1 conditional subscript z t subscript z 1 subscript superscript R DB t subscript z t conditional subscript z t d t subscript z 1 p_{t|1}(z_{t}|z_{1})R^{\\text{DB}}_{t}(z_{t},z_{t+\\text{d}t}|z_{1}) italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_t + d italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , and inflow,  p t | 1  ( z t + d  t | z 1 )  R t DB  ( z t + d  t , z t | z 1 ) subscript p conditional t 1 conditional subscript z t d t subscript z 1 subscript superscript R DB t subscript z t d t conditional subscript z t subscript z 1 p_{t|1}(z_{t+\\text{d}t}|z_{1})R^{\\text{DB}}_{t}(z_{t+\\text{d}t},z_{t}|z_{1}) italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t + d italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t + d italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , of probability mass to any given state are perfectly balanced. Under these conditions, this additive components contribution to the Kolmogorov equation becomes null (similar to the target guidance, as shown in the proof of of  Lemma   10 , in  Sec.   A.2 ).",
            "To better mode detailedly illustrate the influence of each sampling optimization, we show in  Figures   8  and  10  in present the impact of varying parameter values across the synthetic datasets  Figure   8  and molecular datasets in Figure  10  used in this work.",
            "While Figures  8  and  10  are highly condensed, we provide a more fine-grained version that specifically illustrates the influence of the hyperparameters    \\eta italic_  and    \\omega italic_ . This version highlights their impact when generating with the full number of steps (500 and 1000 for molecular and synthetic data, respectively) and with 50 steps. As emphasized in Figures  11  and  12 , the influence of these hyperparameters varies across datasets and exhibits distinct behaviors depending on the number of steps used.",
            "Additionally, we provide the complete version of  Tab.   2 , presenting the results for MOSES and Guacamol separately in  Tab.   9  and  Tab.   10 , respectively. We include models from classes beyond diffusion models to better contextualize the performance achieved by DeFoG."
        ]
    },
    "id_table_11": {
        "caption": "",
        "table": "A6.EGx7",
        "footnotes": [],
        "references": [
            "with    R +  superscript R \\omega\\in\\mathbb{R}^{+} italic_  blackboard_R start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT . This adjustment biases the transitions toward the clean data state  z 1 subscript z 1 z_{1} italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT .  Lemma   10 , in  Sec.   A.2 , demonstrates that this modification introduces an  O  (  ) O  O(\\omega) italic_O ( italic_ )  violation of the Kolmogorov equation. Consequently, choosing a small value of    \\omega italic_  is experimentally shown to be highly beneficial, while a larger    \\omega italic_  restricts the distribution to regions of high probability, increasing the distance between the generated data and the training data, as indicated in  Sec.   A.4 ,  Figure   11 .",
            "In this section, we demonstrate that the proposed  target guidance  design for the rate matrices violates the Kolmogorov equation with an error that is linear in    \\omega italic_ . This result indicates that a small guidance factor effectively helps fit the distribution, whereas a larger guidance factor, as shown in  Figure   11 , while enhancing topological properties such as planarity, increases the distance between generated and training data on synthetic datasets according to the metrics of average ratio. Similarly, for molecular datasets, this also leads to an increase in validity and a decrease in novelty by forcing the generated data to closely resemble the training data.",
            "We denote by RHS and LHS the right-hand side and left-hand side, respectively, of  Eq.   11 . For the case in which  p t | 1  ( z t | z 1 ) > 0 subscript p conditional t 1 conditional subscript z t subscript z 1 0 p_{t|1}(z_{t}|z_{1})>0 italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) > 0 , we have:",
            "Campbell et al. ( 2024 )  show that although their  z 1 subscript z 1 z_{1} italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT -conditional formulation of  R   t superscript R t R^{*}t italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT italic_t  generates  p  t | 1 conditional p t 1 p{t|1} italic_p italic_t | 1 , it does not span the full space of valid rate matrices  those that satisfy the conditional Kolmogorov equation ( Eq.   11 ). They derive sufficient conditions for identifying other valid rate matrices. Notably, they demonstrate that matrices of the form",
            "While Figures  8  and  10  are highly condensed, we provide a more fine-grained version that specifically illustrates the influence of the hyperparameters    \\eta italic_  and    \\omega italic_ . This version highlights their impact when generating with the full number of steps (500 and 1000 for molecular and synthetic data, respectively) and with 50 steps. As emphasized in Figures  11  and  12 , the influence of these hyperparameters varies across datasets and exhibits distinct behaviors depending on the number of steps used.",
            "In graph diffusion methods, the task of graph generation is decomposed into a mapping of a graph to a set of marginal probabilities for each node and edge. This problem is typically addressed using a Graph Transformer architecture, which is augmented with additional features to capture structural aspects that the base architecture might struggle to model effectively  (Vignac et al.,  2022 ; Xu et al.,  2024 ; Siraudin et al.,  2024 )  otherwise. In this section, we conduct a time complexity analysis of the additional features commonly employed. Specifically, we compare the spectral encodings and cycle encodings (up to  6 6 6 6 -cycles) proposed by  Vignac et al. ( 2022 )  to the RRWP encodings (12 steps) from  Siraudin et al. ( 2024 )  (originally from  Ma et al. ( 2023 ) ). While both cycle and RRWP encodings primarily involve matrix multiplications, spectral encodings necessitate more complex algorithms for eigenvalue and eigenvector computation. As shown in  Tab.   11 , cycle and RRWP encodings are more computationally efficient, particularly for larger graphs where eigenvalue computation becomes increasingly costly. These results support the use of RRWP encodings over the combined utilization of cycle and spectral features."
        ]
    },
    "id_table_12": {
        "caption": "",
        "table": "A6.EGx8",
        "footnotes": [],
        "references": [
            "The space of valid rate matrices, i.e., those that satisfy the Kolmogorov equation, is not exhausted by the original formulation of  R t   ( z t , z t + d  t | z 1 ) subscript superscript R t subscript z t conditional subscript z t d t subscript z 1 R^{*}_{t}{(z_{t},z_{t+\\mathrm{d}t}|z_{1})} italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) .  Campbell et al. ( 2024 )  investigate this and show that for any rate matrix  R t DB subscript superscript R DB t R^{\\text{DB}}_{t} italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  that satisfies the detailed balance condition,  p t | 1  ( z t | z 1 )  R t DB  ( z t , z t + d  t | z 1 ) = p t | 1  ( z t + d  t | z 1 )  R t DB  ( z t + d  t , z t | z 1 ) subscript p conditional t 1 conditional subscript z t subscript z 1 subscript superscript R DB t subscript z t conditional subscript z t d t subscript z 1 subscript p conditional t 1 conditional subscript z t d t subscript z 1 subscript superscript R DB t subscript z t d t conditional subscript z t subscript z 1 p_{t|1}(z_{t}|z_{1})R^{\\text{DB}}_{t}(z_{t},z_{t+\\text{d}t}|z_{1})=p_{t|1}(z_{% t+\\text{d}t}|z_{1})R^{\\text{DB}}_{t}(z_{t+\\text{d}t},z_{t}|z_{1}) italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_t + d italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) = italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t + d italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t + d italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , the modified rate matrix  R t  = R t  +   R t DB subscript superscript R  t subscript superscript R t  subscript superscript R DB t R^{\\eta}_{t}=R^{*}_{t}+\\eta R^{\\text{DB}}_{t} italic_R start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_ italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , with    R +  superscript R \\eta\\in\\mathbb{R}^{+} italic_  blackboard_R start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT , also satisfies the Kolmogorov equation. Increasing    \\eta italic_  introduces more stochasticity into the trajectory of the denoising process, while different designs of  R t D  B subscript superscript R D B t R^{DB}_{t} italic_R start_POSTSUPERSCRIPT italic_D italic_B end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  encode different priors for preferred transitions between states. This mechanism can be interpreted as a correction mechanism, as it enables transitions back to states that would otherwise be disallowed according to the rate matrix formulation, as described in  Sec.   A.5 . The effect of    \\eta italic_  across different datasets is illustrated in detail in  Figure   12 ,  Sec.   A.4 . To further improve the sampling performance of DeFoG, we also investigate the different formulations of  R DB superscript R DB R^{\\text{DB}} italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT  under the detailed balance condition. Additional details and discussions are provided in  Sec.   A.3 .",
            "A natural question is how to choose a suitable design for  R t DB subscript superscript R DB t R^{\\text{DB}}_{t} italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  from the infinite space of detailed balance rate matrices. As depicted in  Figure   5 , this flexibility can be leveraged to incorporate priors into the denoising model by encouraging specific transitions between states. By adjusting the sparsity of the matrix entries, additional transitions beyond those prescribed by  R t  subscript superscript R t R^{*}_{t} italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  can be introduced. In the general case, transitions between all states are possible; in the column case, a specific state centralizes all potential transitions; and in the single-entry case, only transitions between two states are permitted. These examples merely illustrate some possibilities and do not exhaust the range of potential  R t DB subscript superscript R DB t R^{\\text{DB}}_{t} italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  designs. The matrix entries can be structured by considering the following reorganization of terms of  Eq.   12 :",
            "Orthogonal to the design of  R t DB subscript superscript R DB t R^{\\text{DB}}_{t} italic_R start_POSTSUPERSCRIPT DB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , we must also consider the hyperparameter    \\eta italic_ , which regulates the magnitude of stochasticity in the denoising process. Specifically, setting   = 0  0 \\eta=0 italic_ = 0  (thereby relying solely on  R t  subscript superscript R t R^{*}_{t} italic_R start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) minimizes the expected number of jumps throughout the denoising trajectory under certain conditions, as shown by  Campbell et al. ( 2024 )  in Proposition 3.4. However, in continuous diffusion models, some level of stochasticity has been demonstrated to enhance performance  (Karras et al.,  2022 ; Cao et al.,  2023 ; Xu et al.,  2023 ) . Conversely, excessive stochasticity can negatively impact performance.  Campbell et al. ( 2024 )  propose that there exists an optimal level of stochasticity that strikes a balance between exploration and accuracy. In our experiments, we observed varied behaviors as    \\eta italic_  increases, resulting in different performance outcomes across datasets, as illustrated in  Figure   12 .",
            "While Figures  8  and  10  are highly condensed, we provide a more fine-grained version that specifically illustrates the influence of the hyperparameters    \\eta italic_  and    \\omega italic_ . This version highlights their impact when generating with the full number of steps (500 and 1000 for molecular and synthetic data, respectively) and with 50 steps. As emphasized in Figures  11  and  12 , the influence of these hyperparameters varies across datasets and exhibits distinct behaviors depending on the number of steps used."
        ]
    },
    "id_table_13": {
        "caption": "",
        "table": "A6.EGx9",
        "footnotes": [],
        "references": [
            "By definition ( Eq.   13 ), we have:",
            "where  R t  ( z t 1 : D , z t + d  t 1 : D ) subscript R t subscript superscript z : 1 D t subscript superscript z : 1 D t d t R_{t}(z^{1:D}_{t},z^{1:D}_{t+\\mathrm{d}t}) italic_R start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT )  denotes the unconditional multivariate rate matrix defined in  Eq.   13 . This process can be simulated exactly using Gillespies Algorithm  (Gillespie,  1976 ;  1977 ) . However, such an algorithm does not scale for large  D D D italic_D   (Campbell et al.,  2022 ) . Although    \\tau italic_ -leaping is a widely adopted approximate algorithm to address this limitation  (Gillespie,  2001 ) , it requires ordinal discrete state spaces, which is suitable for cases like text or images but not for graphs. Therefore, we cannot apply it in the context of this paper. Additionally, directly replacing the infinitesimal step  d  t d t \\text{d}t d italic_t  in  Eq.   13  with a finite time step    t  t \\Delta t roman_ italic_t   a la  Euler method is inappropriate, as  R t  ( z t 1 : D , z t + d  t 1 : D ) subscript R t subscript superscript z : 1 D t subscript superscript z : 1 D t d t R_{t}(z^{1:D}_{t},z^{1:D}_{t+\\mathrm{d}t}) italic_R start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT )  prevents state transitions in more than one dimension per step under the continuous framework. Instead,  Campbell et al. ( 2024 )  propose an approximation where the Euler step is applied independently to each dimension, as seen in  Eq.   4 .",
            "From  Eq.   13 , the unconditional rate matrix is given by:",
            "where in  Eq.   30  we have the approximated transition rate matrix is computed according to  Eq.   13  but using  p 1 | t   ( z 1 d | z t 1 : D ) subscript superscript p  conditional 1 t conditional subscript superscript z d 1 subscript superscript z : 1 D t p^{\\theta}_{1|t}(z^{d}_{1}|z^{1:D}_{t}) italic_p start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 | italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  instead of  p 1 | t  ( z 1 d | z t 1 : D ) subscript p conditional 1 t conditional subscript superscript z d 1 subscript superscript z : 1 D t p_{1|t}(z^{d}_{1}|z^{1:D}_{t}) italic_p start_POSTSUBSCRIPT 1 | italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ."
        ]
    },
    "id_table_14": {
        "caption": "",
        "table": "A6.EGx10",
        "footnotes": [],
        "references": [
            "To demonstrate the benefit of each designed optimization step, we report the step-wise improvements by sequentially adding each tuned step across the primary datasets  synthetic datasets in Figure  14  and molecular datasets in Figure  16   used in this work.",
            "In  Eq.   16 , we use the definition of TV distance as defined in  Eq.   14  and  Eq.   17  results from direct application of Pinskers inequality. Now, we change the ordering of the sum and of the square root through the Cauchy-Schwarz inequality:"
        ]
    },
    "id_table_15": {
        "caption": "",
        "table": "A6.EGx11",
        "footnotes": [],
        "references": [
            "i.e., the square root of the right-hand side of  Eq.   15 ."
        ]
    },
    "id_table_16": {
        "caption": "",
        "table": "A6.EGx12",
        "footnotes": [],
        "references": [
            "To demonstrate the benefit of each designed optimization step, we report the step-wise improvements by sequentially adding each tuned step across the primary datasets  synthetic datasets in Figure  14  and molecular datasets in Figure  16   used in this work.",
            "In  Eq.   16 , we use the definition of TV distance as defined in  Eq.   14  and  Eq.   17  results from direct application of Pinskers inequality. Now, we change the ordering of the sum and of the square root through the Cauchy-Schwarz inequality:"
        ]
    },
    "id_table_17": {
        "caption": "",
        "table": "A6.EGx13",
        "footnotes": [],
        "references": [
            "In  Figure   17 , we present the training curves for each initial distribution for three different datasets.",
            "In  Eq.   16 , we use the definition of TV distance as defined in  Eq.   14  and  Eq.   17  results from direct application of Pinskers inequality. Now, we change the ordering of the sum and of the square root through the Cauchy-Schwarz inequality:"
        ]
    },
    "id_table_18": {
        "caption": "",
        "table": "A6.EGx14",
        "footnotes": [],
        "references": [
            "To investigate these questions, we conducted a grid search. For two datasets, we trained five models, each with a different time distortion applied during training. Subsequently, we tested each model by applying the five different distortions at the sampling stage. The results are presented in  Figure   18 .",
            "As expected, the curve of training loss as a function of  t t t italic_t  (left in  Figure   19 ) is monotonically decreasing, indicating that as graphs are decreasingly noised, the task becomes simpler. However, the most interesting insights arise from the evolution of this curve across epochs (right in  Figure   19 ). We observe that for smaller values of  t t t italic_t , the model reaches its maximum capacity early in the training process, showing no significant improvements after the initial few epochs. In contrast, for larger values of  t t t italic_t  (closer to  t = 1 t 1 t=1 italic_t = 1 ), the model exhibits substantial improvements throughout the training period. This suggests that the model can continue to refine its predictions in the time range where the task is easier. These findings align with those in  Figure   18 , reinforcing our expectation that training the model to be more precise in this range or providing more refined sampling steps will naturally enhance performance in the planar dataset."
        ]
    },
    "id_table_19": {
        "caption": "",
        "table": "A6.EGx15",
        "footnotes": [],
        "references": [
            "To determine if the structural properties observed in datasets like the planar dataset can be detected and exploited without requiring an exhaustive sweep over all possibilities, we propose developing a metric that quantifies the difficulty of predicting the clean graph for any given time point  t  [ 0 , 1 ) t 0 1 t\\in[0,1) italic_t  [ 0 , 1 ) . For this, we perform a sweep over  t t t italic_t  for a given model, where for each  t t t italic_t , we noise a sufficiently large batch of clean graphs and evaluate the models training loss on them. This yields a curve that shows how the training loss varies as a function of  t t t italic_t . We then track how this curve evolves across epochs. To make the changes more explicit, we compute the ratio of the loss curve relative to the fully trained models values. These curves are shown in  Figure   19 .",
            "As expected, the curve of training loss as a function of  t t t italic_t  (left in  Figure   19 ) is monotonically decreasing, indicating that as graphs are decreasingly noised, the task becomes simpler. However, the most interesting insights arise from the evolution of this curve across epochs (right in  Figure   19 ). We observe that for smaller values of  t t t italic_t , the model reaches its maximum capacity early in the training process, showing no significant improvements after the initial few epochs. In contrast, for larger values of  t t t italic_t  (closer to  t = 1 t 1 t=1 italic_t = 1 ), the model exhibits substantial improvements throughout the training period. This suggests that the model can continue to refine its predictions in the time range where the task is easier. These findings align with those in  Figure   18 , reinforcing our expectation that training the model to be more precise in this range or providing more refined sampling steps will naturally enhance performance in the planar dataset.",
            "It remains to bound the second term from  Eq.   23 . We start by analyzing the Markov kernel corresponding to a Markov chain with  constant  rate matrix  R t k  1  subscript superscript R  subscript t k 1 R^{\\theta}_{t_{k-1}} italic_R start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT  between  t k  1 subscript t k 1 t_{k-1} italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT  and  t k subscript t k t_{k} italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT . In that case, from  Eq.   19  we obtain:"
        ]
    },
    "id_table_20": {
        "caption": "",
        "table": "A6.EGx16",
        "footnotes": [],
        "references": [
            "By differentiating the explicit form of  p t | 1  ( z t | z 1 ) subscript p conditional t 1 conditional subscript z t subscript z 1 p_{t|1}(z_{t}|z_{1}) italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , we have that   2 p t | 1  ( z t | z 1 ) = 0 superscript 2 subscript p conditional t 1 conditional subscript z t subscript z 1 0 \\partial^{2}p_{t|1}(z_{t}|z_{1})=0  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) = 0 . As a consequence, the numerator of  eq.   20  has zero derivative. Additionally, we also note that  Z t > 0 subscript superscript Z absent 0 t \\mathbb{Z}^{>0}_{t} blackboard_Z start_POSTSUPERSCRIPT > 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  is constant. Again, since  p t | 1  ( z t | z 1 ) subscript p conditional t 1 conditional subscript z t subscript z 1 p_{t|1}(z_{t}|z_{1}) italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT )  is a linear interpolation between  z 1 subscript z 1 z_{1} italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and  p 0 subscript p 0 p_{0} italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  and, therefore, it is impossible for  p t | 1  ( z t | z 1 ) subscript p conditional t 1 conditional subscript z t subscript z 1 p_{t|1}(z_{t}|z_{1}) italic_p start_POSTSUBSCRIPT italic_t | 1 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT )  to suddenly become 0 for  t  ( 0 , 1 ) t 0 1 t\\in(0,1) italic_t  ( 0 , 1 ) ."
        ]
    },
    "id_table_21": {
        "caption": "",
        "table": "A6.EGx17",
        "footnotes": [],
        "references": [
            "where, in  Eq.   21 ,  P 1 | 0 subscript P conditional 1 0 \\mathbb{P}_{1|0} blackboard_P start_POSTSUBSCRIPT 1 | 0 end_POSTSUBSCRIPT  denotes the path measure of the exact groundtruth CTMC and the difference between limit distributions (second term from  Eq.   21 ) is zero since in flow matching the convergence to the limit distribution via linear interpolation is not asymptotic (as in diffusion models) but actually attained at  t = 0 t 0 t=0 italic_t = 0 . In  Eq.   22 , we introduce the stepwise path measure, i.e.,  P k = P t k | t k  1 subscript P k subscript P conditional subscript t k subscript t k 1 \\mathcal{P}_{k}=\\mathbb{P}_{t_{k}|t_{k-1}} caligraphic_P start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = blackboard_P start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , such that  P T | 0 = P 1  P 2  ...  P K subscript P conditional T 0 subscript P 1 subscript P 2 ... subscript P K \\mathbb{P}_{T|0}=\\mathcal{P}_{1}\\mathcal{P}_{2}\\ldots\\mathcal{P}_{K} blackboard_P start_POSTSUBSCRIPT italic_T | 0 end_POSTSUBSCRIPT = caligraphic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT caligraphic_P start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ... caligraphic_P start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT . Therefore, finding the intended upper bound amounts to establish bounds on the total variation distance for each interval  [ t k  1 , t k ] subscript t k 1 subscript t k [t_{k-1},t_{k}] [ italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ] .",
            "Here, we describe the datasets employed in our experiments and outline the specific metrics used to evaluate model performance on each dataset. Additional visualizations of example graphs from each dataset, along with generated graphs, are provided in  Figures   21 ,  23  and  25 ."
        ]
    },
    "id_table_22": {
        "caption": "",
        "table": "A6.EGx18",
        "footnotes": [],
        "references": [
            "where, in  Eq.   21 ,  P 1 | 0 subscript P conditional 1 0 \\mathbb{P}_{1|0} blackboard_P start_POSTSUBSCRIPT 1 | 0 end_POSTSUBSCRIPT  denotes the path measure of the exact groundtruth CTMC and the difference between limit distributions (second term from  Eq.   21 ) is zero since in flow matching the convergence to the limit distribution via linear interpolation is not asymptotic (as in diffusion models) but actually attained at  t = 0 t 0 t=0 italic_t = 0 . In  Eq.   22 , we introduce the stepwise path measure, i.e.,  P k = P t k | t k  1 subscript P k subscript P conditional subscript t k subscript t k 1 \\mathcal{P}_{k}=\\mathbb{P}_{t_{k}|t_{k-1}} caligraphic_P start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = blackboard_P start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , such that  P T | 0 = P 1  P 2  ...  P K subscript P conditional T 0 subscript P 1 subscript P 2 ... subscript P K \\mathbb{P}_{T|0}=\\mathcal{P}_{1}\\mathcal{P}_{2}\\ldots\\mathcal{P}_{K} blackboard_P start_POSTSUBSCRIPT italic_T | 0 end_POSTSUBSCRIPT = caligraphic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT caligraphic_P start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ... caligraphic_P start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT . Therefore, finding the intended upper bound amounts to establish bounds on the total variation distance for each interval  [ t k  1 , t k ] subscript t k 1 subscript t k [t_{k-1},t_{k}] [ italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ] ."
        ]
    },
    "id_table_23": {
        "caption": "",
        "table": "A6.EGx19",
        "footnotes": [],
        "references": [
            "It remains to bound the second term from  Eq.   23 . We start by analyzing the Markov kernel corresponding to a Markov chain with  constant  rate matrix  R t k  1  subscript superscript R  subscript t k 1 R^{\\theta}_{t_{k-1}} italic_R start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT  between  t k  1 subscript t k 1 t_{k-1} italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT  and  t k subscript t k t_{k} italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT . In that case, from  Eq.   19  we obtain:",
            "Here, we describe the datasets employed in our experiments and outline the specific metrics used to evaluate model performance on each dataset. Additional visualizations of example graphs from each dataset, along with generated graphs, are provided in  Figures   21 ,  23  and  25 ."
        ]
    },
    "id_table_24": {
        "caption": "",
        "table": "A6.EGx20",
        "footnotes": [],
        "references": [
            "where, in  Eq.   24 , we use the Mean Value Theorem, with  t c  ( t k  1 , t k ) subscript t c subscript t k 1 subscript t k t_{c}\\in(t_{k-1},t_{k}) italic_t start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT  ( italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ; in  Eq.   25 , we use the fact that there are  Z  D Z D ZD italic_Z italic_D  values of  z t + d  t 1 : D subscript superscript z : 1 D t d t z^{1:D}_{t+\\mathrm{d}t} italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT  that differ at most in only one coordinate from  z t 1 : D subscript superscript z : 1 D t z^{1:D}_{t} italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; in  Eq.   26 , we use the result from  Proposition   7  to upper bound the time derivative of the multivariate unconditional rate matrix; and finally, in  Eq.   27 , we define  B k = sup t  ( t k  1 , t k ) B t subscript B k subscript supremum t subscript t k 1 subscript t k subscript B t B_{k}=\\sup_{t\\in(t_{k-1},t_{k})}B_{t} italic_B start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = roman_sup start_POSTSUBSCRIPT italic_t  ( italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT italic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and    t k = t k  t k  1  subscript t k subscript t k subscript t k 1 \\Delta t_{k}=t_{k}-t_{k-1} roman_ italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT ."
        ]
    },
    "id_table_25": {
        "caption": "",
        "table": "A6.EGx21",
        "footnotes": [],
        "references": [
            "where, in  Eq.   24 , we use the Mean Value Theorem, with  t c  ( t k  1 , t k ) subscript t c subscript t k 1 subscript t k t_{c}\\in(t_{k-1},t_{k}) italic_t start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT  ( italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ; in  Eq.   25 , we use the fact that there are  Z  D Z D ZD italic_Z italic_D  values of  z t + d  t 1 : D subscript superscript z : 1 D t d t z^{1:D}_{t+\\mathrm{d}t} italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT  that differ at most in only one coordinate from  z t 1 : D subscript superscript z : 1 D t z^{1:D}_{t} italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; in  Eq.   26 , we use the result from  Proposition   7  to upper bound the time derivative of the multivariate unconditional rate matrix; and finally, in  Eq.   27 , we define  B k = sup t  ( t k  1 , t k ) B t subscript B k subscript supremum t subscript t k 1 subscript t k subscript B t B_{k}=\\sup_{t\\in(t_{k-1},t_{k})}B_{t} italic_B start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = roman_sup start_POSTSUBSCRIPT italic_t  ( italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT italic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and    t k = t k  t k  1  subscript t k subscript t k subscript t k 1 \\Delta t_{k}=t_{k}-t_{k-1} roman_ italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT .",
            "Here, we describe the datasets employed in our experiments and outline the specific metrics used to evaluate model performance on each dataset. Additional visualizations of example graphs from each dataset, along with generated graphs, are provided in  Figures   21 ,  23  and  25 ."
        ]
    },
    "id_table_26": {
        "caption": "",
        "table": "A6.EGx22",
        "footnotes": [],
        "references": [
            "where, in  Eq.   24 , we use the Mean Value Theorem, with  t c  ( t k  1 , t k ) subscript t c subscript t k 1 subscript t k t_{c}\\in(t_{k-1},t_{k}) italic_t start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT  ( italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ; in  Eq.   25 , we use the fact that there are  Z  D Z D ZD italic_Z italic_D  values of  z t + d  t 1 : D subscript superscript z : 1 D t d t z^{1:D}_{t+\\mathrm{d}t} italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT  that differ at most in only one coordinate from  z t 1 : D subscript superscript z : 1 D t z^{1:D}_{t} italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; in  Eq.   26 , we use the result from  Proposition   7  to upper bound the time derivative of the multivariate unconditional rate matrix; and finally, in  Eq.   27 , we define  B k = sup t  ( t k  1 , t k ) B t subscript B k subscript supremum t subscript t k 1 subscript t k subscript B t B_{k}=\\sup_{t\\in(t_{k-1},t_{k})}B_{t} italic_B start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = roman_sup start_POSTSUBSCRIPT italic_t  ( italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT italic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and    t k = t k  t k  1  subscript t k subscript t k subscript t k 1 \\Delta t_{k}=t_{k}-t_{k-1} roman_ italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT ."
        ]
    },
    "id_table_27": {
        "caption": "",
        "table": "A6.EGx23",
        "footnotes": [],
        "references": [
            "where, in  Eq.   24 , we use the Mean Value Theorem, with  t c  ( t k  1 , t k ) subscript t c subscript t k 1 subscript t k t_{c}\\in(t_{k-1},t_{k}) italic_t start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT  ( italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ; in  Eq.   25 , we use the fact that there are  Z  D Z D ZD italic_Z italic_D  values of  z t + d  t 1 : D subscript superscript z : 1 D t d t z^{1:D}_{t+\\mathrm{d}t} italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT  that differ at most in only one coordinate from  z t 1 : D subscript superscript z : 1 D t z^{1:D}_{t} italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; in  Eq.   26 , we use the result from  Proposition   7  to upper bound the time derivative of the multivariate unconditional rate matrix; and finally, in  Eq.   27 , we define  B k = sup t  ( t k  1 , t k ) B t subscript B k subscript supremum t subscript t k 1 subscript t k subscript B t B_{k}=\\sup_{t\\in(t_{k-1},t_{k})}B_{t} italic_B start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = roman_sup start_POSTSUBSCRIPT italic_t  ( italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT italic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and    t k = t k  t k  1  subscript t k subscript t k subscript t k 1 \\Delta t_{k}=t_{k}-t_{k-1} roman_ italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT .",
            "Therefore, we get the intended result by gathering the results from  Eq.   27 ,  Eq.   29 , and  Eq.   31 ."
        ]
    },
    "id_table_28": {
        "caption": "",
        "table": "A6.EGx24",
        "footnotes": [],
        "references": [
            "where, in  Eq.   28 , we use again the fact that there are  Z  D Z D ZD italic_Z italic_D  values of  z t + d  t 1 : D subscript superscript z : 1 D t d t z^{1:D}_{t+\\mathrm{d}t} italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT  that differ at most in only one coordinate from  z t 1 : D subscript superscript z : 1 D t z^{1:D}_{t} italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  along with the estimation error upper bound from  Theorem   2 . In particular, we consider  U k = sup t  [ t k  1 , t k ] , z t 1 : D , z t + d  t 1 : D  Z D  U k z t 1 : D  z t + d  t 1 : D subscript U k t subscript t k 1 subscript t k subscript superscript z : 1 D t subscript superscript z : 1 D t d t superscript Z D supremum subscript superscript U  subscript superscript z : 1 D t subscript superscript z : 1 D t d t k U_{k}=\\underset{\\begin{subarray}{c}t\\in[t_{k-1},t_{k}],\\;\\\\ z^{1:D}_{t},\\;z^{1:D}_{t+\\mathrm{d}t}\\in\\mathcal{Z}^{D}\\end{subarray}}{\\sup}U^% {z^{1:D}_{t}\\to z^{1:D}_{t+\\mathrm{d}t}}_{k} italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = start_UNDERACCENT start_ARG start_ROW start_CELL italic_t  [ italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ] , end_CELL end_ROW start_ROW start_CELL italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT  caligraphic_Z start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_CELL end_ROW end_ARG end_UNDERACCENT start_ARG roman_sup end_ARG italic_U start_POSTSUPERSCRIPT italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , with:"
        ]
    },
    "id_table_29": {
        "caption": "",
        "table": "A6.EGx25",
        "footnotes": [],
        "references": [
            "Therefore, we get the intended result by gathering the results from  Eq.   27 ,  Eq.   29 , and  Eq.   31 ."
        ]
    },
    "id_table_30": {
        "caption": "",
        "table": "A6.EGx26",
        "footnotes": [],
        "references": [
            "where in  Eq.   30  we have the approximated transition rate matrix is computed according to  Eq.   13  but using  p 1 | t   ( z 1 d | z t 1 : D ) subscript superscript p  conditional 1 t conditional subscript superscript z d 1 subscript superscript z : 1 D t p^{\\theta}_{1|t}(z^{d}_{1}|z^{1:D}_{t}) italic_p start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 | italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  instead of  p 1 | t  ( z 1 d | z t 1 : D ) subscript p conditional 1 t conditional subscript superscript z d 1 subscript superscript z : 1 D t p_{1|t}(z^{d}_{1}|z^{1:D}_{t}) italic_p start_POSTSUBSCRIPT 1 | italic_t end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT 1 : italic_D end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ."
        ]
    },
    "id_table_31": {
        "caption": "",
        "table": "A6.EGx27",
        "footnotes": [],
        "references": [
            "Therefore, we get the intended result by gathering the results from  Eq.   27 ,  Eq.   29 , and  Eq.   31 ."
        ]
    },
    "id_table_32": {
        "caption": "",
        "table": "A6.EGx28",
        "footnotes": [],
        "references": [
            "In  Eq.   32 , we use the definition of permuted ordered set for  x t  subscript superscript x  t x^{\\prime}_{t} italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and  x t + d  t  subscript superscript x  t d t x^{\\prime}_{t+\\mathrm{d}t} italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT , and, in  Eq.   33 , we use that  f  subscript f  f_{\\theta} italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  is equivariant."
        ]
    },
    "id_table_33": {
        "caption": "",
        "table": "A6.EGx29",
        "footnotes": [],
        "references": [
            "In  Eq.   32 , we use the definition of permuted ordered set for  x t  subscript superscript x  t x^{\\prime}_{t} italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and  x t + d  t  subscript superscript x  t d t x^{\\prime}_{t+\\mathrm{d}t} italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + roman_d italic_t end_POSTSUBSCRIPT , and, in  Eq.   33 , we use that  f  subscript f  f_{\\theta} italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  is equivariant."
        ]
    },
    "id_table_34": {
        "caption": "",
        "table": "A5.T5.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_35": {
        "caption": "",
        "table": "A5.T6.3.3",
        "footnotes": [],
        "references": []
    },
    "id_table_36": {
        "caption": "",
        "table": "A6.T7.22.22",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": []
    },
    "id_table_37": {
        "caption": "",
        "table": "A6.T8.13.13",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": []
    },
    "id_table_38": {
        "caption": "",
        "table": "A6.T9.7.7",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": []
    },
    "id_table_39": {
        "caption": "",
        "table": "A6.T10.5.5",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": []
    },
    "id_table_40": {
        "caption": "",
        "table": "A6.T11.1.1",
        "footnotes": [],
        "references": []
    }
}