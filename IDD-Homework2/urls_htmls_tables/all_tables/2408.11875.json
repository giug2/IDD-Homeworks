{
    "id_table_1": {
        "caption": "Table 1.    Old Corpus vs. Indexed Wikicorpus.",
        "table": "S4.T2.1.1",
        "footnotes": [],
        "references": [
            "The selection and integration of knowledge retrieved from multiple sources pose various challenges. Several key issues, such as outdated information, context window length limitations, and accuracy-quantity trade-off issues, significantly impact the performance of multi-hop QA systems. Firstly, new multi-hop QA systems often rely on outdated and insufficient knowledge within localized knowledge bases. As illustrated in Figure  1(b) , a QA system searches an outdated corpus  (Karpukhin et al . ,  2020 )  to answer the query  Who is the current president of the United States? , resulting in the incorrect answer Donald Trump. Similarly, the outdated corpus cannot provide real-time updates for newly released news, movies, books, etc. For instance, it does not include the 2023 movie  Transformers: Rise of the Beasts , as shown in Figure  1(c) . Although outdated information can be mitigated through online retrieval, excessive retrieval introduces efficiency and cost issues. To address this, we first update the older corpus and construct an entity-centric, more comprehensive corpus,  Indexed Wikicorpus , based on Wikipedia, to reduce the reliance on over-searching. Additionally, to quickly understand the basic information of entities, we create a corpus containing some basic information of entities,  Profile Wikicorpus .",
            "Building on previous studies  (Press et al . ,  2022 ; Yao et al . ,  2022 ; Trivedi et al . ,  2022a ) , we decompose the original question into multi-hop questions, as illustrated in Figure  1(a) . Each sub-question is answered based on retrieved knowledge, and the answers are then integrated using the Chain-of-Thought (CoT) approach to derive the final answer.  In local retrieval, most studies, such as FLARE  (Jiang et al . ,  2023 )  and MetaRAG  (Zhou et al . ,  2024 ) , employ a retrieval method that involves searching the corpus in chunks and returning the top  n n n italic_n  most similar candidate chunks as retrieved knowledge. We refer to this method as  multi-candidate retrieval , as depicted in Figure  2(a) . Intuitively, increasing the value of  n n n italic_n  acquires more knowledge, thereby improving the probability of obtaining the correct answer to the query.  However, these approaches face two significant challenges. Firstly, the  context window limit  imposes an upper limit on the value of  n n n italic_n , preventing the unlimited expansion of the retrieved content due to the models context window size constraints. Secondly, the  accuracy-number tradeoff  arises, where increasing  n n n italic_n  also increases the amount of potentially irrelevant or redundant information in the retrieved content. This noisy data may not only mislead the LLM and exacerbate its hallucination problem but also distract its attention, causing it to miss relevant information.",
            "The importance of corpus cannot be overstated in the realm of external knowledge acquisition. The Wikipedia corpus released by DPR  (Karpukhin et al . ,  2020 )  is widely recognized as a standard resource. However, as time progresses, this corpus faces challenges such as knowledge gaps and outdated information. Moreover, to accommodate dense retrieval requirements, the corpus is designed to fragment entity information into multiple segments, often resulting in incoherent data representation.  To address these limitations, we develop a novel Wikipedia corpus called  Indexed Wikicorpus  along with a corresponding key entity profile corpus called  Profile Wikicorpus . Our approach differs significantly from its predecessor in prioritizing the coherence of entity information. In Indexed Wikicorpus, each entry represents a complete entity as shown in Figure  3 , ensuring a more comprehensive and cohesive representation of information. Building upon this foundation, we draw inspiration from the Web version of Wikipedia to extract entity profiles. These profiles, curated by Wikipedia, encapsulate the essential information about each entity, which is saved to Profile Wikicorpus. Data analysis about the number of words and entities as shown in Table  1  reveals that our new corpus contains a higher volume of entity information compared to its predecessor. Furthermore, subsequent experiments demonstrate the superior performance of our corpus over the older version.",
            "To address the challenges in retrieval-augmented generation, including the lack of deep integration between different retrieval methods and the potential introduction of noise from multi-candidate retrieval, we propose a novel framework called  HiRAG . This framework consists of five key components: Decomposer, Summarizer, Retriever, Definer, and Filter, as illustrated in Figure  4 . The detailed algorithm is presented in Algorithm  1  in Appendix  A .  Specifically, the Decomposer is designed to tackle complex questions by decomposing them into smaller, more manageable sub-questions that can be easily answered. The Definer then determines whether the question can be solved. If it can, the Summarizer leverages the sub-answers to generate a response to the original question. Otherwise, the Retriever extracts relevant information related to the sub-question through a hierarchical retrieval process at both the document and chunk levels. Finally, the Filter verifies the validity of the retrieved content, generates a sub-answer, and re-evaluates the results if they are found to be inaccurate.",
            "In Appendix  A.1 , we present experimental results investigating the relationship between model performance, as measured by the EM metric, and the number of model turns  m  t m t mt italic_m italic_t . The results are summarized in Table  7 .",
            "For clarity, we outline the workflow of the HiRAG framework in Algorithm  1 , providing a step-by-step illustration of our method. This algorithm contains the Main function, Decompose function, RewriteAndAnswerQuestion function and Retrieval function. The Decompose function and the main function complete the decomposition and final summary of the problem, the Retrieval function describes the hierarchical Retrieval and the process of the Filter, and the RewriteAndAnswerQuestion function describes our solution when the semantics are found to be incomplete."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.    Experimental results on four datasets. Without retrieval means only using the internal knowledge of LLM while With retrieval means using the external knowledge from the retriever. HiRAG is divided into HiRAG (online) and HiRAG (local) according to whether it uses Google for retrieval after semantic supplementation. The best results are in bold.",
        "table": "S4.T3.1.1",
        "footnotes": [],
        "references": [
            "Building on previous studies  (Press et al . ,  2022 ; Yao et al . ,  2022 ; Trivedi et al . ,  2022a ) , we decompose the original question into multi-hop questions, as illustrated in Figure  1(a) . Each sub-question is answered based on retrieved knowledge, and the answers are then integrated using the Chain-of-Thought (CoT) approach to derive the final answer.  In local retrieval, most studies, such as FLARE  (Jiang et al . ,  2023 )  and MetaRAG  (Zhou et al . ,  2024 ) , employ a retrieval method that involves searching the corpus in chunks and returning the top  n n n italic_n  most similar candidate chunks as retrieved knowledge. We refer to this method as  multi-candidate retrieval , as depicted in Figure  2(a) . Intuitively, increasing the value of  n n n italic_n  acquires more knowledge, thereby improving the probability of obtaining the correct answer to the query.  However, these approaches face two significant challenges. Firstly, the  context window limit  imposes an upper limit on the value of  n n n italic_n , preventing the unlimited expansion of the retrieved content due to the models context window size constraints. Secondly, the  accuracy-number tradeoff  arises, where increasing  n n n italic_n  also increases the amount of potentially irrelevant or redundant information in the retrieved content. This noisy data may not only mislead the LLM and exacerbate its hallucination problem but also distract its attention, causing it to miss relevant information.",
            "To address the challenges, we propose a novel framework called the  Hi erarchical  R etrieval- A ugmented  G eneration Model with Rethink ( HiRAG ), as illustrated in Figure  4 . HiRAG comprises five key modules, i.e., Decomposer, Definer, Retriever, Filter, and Summarizer. The Filter module further incorporates two submodules, including Verify and Rethink. Previous methods typically rely on either sparse retrieval  (Svore and Burges,  2009 ) , which focuses on lexical matching, or dense retrieval  (Zhao et al . ,  2024 ) , which leverages latent embeddings for semantic matching. While some approaches combine the results of these two retrieval methods at the output level  (Arabzadeh et al . ,  2021 ; Luan et al . ,  2021 ; Karpukhin et al . ,  2020 ) , they do not deeply integrate the strengths of each retrieval method within the retrieval process itself.  We propose the Retrieval module which incorporates a new hierarchical retrieval strategy, performing multi-level retrieval from the document level to the chunk level. Initially, we employ sparse retrieval at the document level to extract key information, such as entity names, through lexical matching. Subsequently, dense retrieval is used to retrieve specific information at the chunk level, leveraging semantic matching.  We also introduce a novel retrieval method,  single-candidate retrieval , as shown in Figure  2(b) . This method returns only the most similar chunk as the retrieved knowledge for each decomposed sub-question. However, selecting just one candidate does not guarantee the correctness of the answer. To mitigate this, we apply the Filter module to assess the answer based on the retrieved knowledge. If the answer to a sub-question is found to be incorrect, we utilize the Rethink module, in conjunction with the Retrieval module, to select another chunk as the retrieved knowledge to answer the question.  The rethinking process is iterated until the Filter module confirms the answer as correct, thereby yielding the solution to the current sub-question. This cycle of decomposition, retrieval, and answering is repeated for subsequent sub-questions until the Definer module determines that the current question, accompanied by its sub-answers, is answerable. At this point, the Summarizer module is invoked to generate the final answer to the current question.",
            "The main results are shown in Table  2  and reveal the following notable findings.  (1) Our proposed framework, HiRAG, exhibits superior performance across multiple evaluation metrics, outperforming state-of-the-art methods on three out of four datasets. Additionally, it demonstrates improvements in several metrics on the remaining dataset. The key advantage of our approach is its emphasis on the retrieval process, which consistently produces high-quality results. This highlights the crucial role of the retrieval component in achieving exceptional outcomes.  (2) In the Without Retrieval setting, HiRAG significantly outperforms the baselines, showcasing its effectiveness in question decomposition and answer summarization. This is due to the frameworks design, which enables autonomous sub-question generation and termination of the loop when the original problem is resolvable. Our approach also differs from self-ask in its segregation of subtasks and implementation of each subtask through a separate prompt, resulting in superior performance.  (3) A comparative analysis of results across datasets reveals that HiRAG achieves the most significant breakthrough in 2WikiMultiHopQA, with a notable improvement of over 12% in the EM index compared to the state-of-the-art method. This is primarily attributed to the fact that most external knowledge required by 2WikiMultiHopQA can be retrieved from Wikipedia, and the question format in this dataset is relatively standardized. However, the MuSiQue dataset poses a greater challenge due to the complexity of the questions and the inability to directly retrieve required knowledge from Wikipedia. This complexity affects our frameworks ability to evaluate retrieval results, diminishing the effectiveness of the response."
        ]
    },
    "id_table_3": {
        "caption": "Table 3.    Result of substitution of different LLMs as the backbone, i.e. the icon of ChatGPT in Figure  4 . The best results are in bold.",
        "table": "S5.T4.1.1",
        "footnotes": [],
        "references": [
            "The importance of corpus cannot be overstated in the realm of external knowledge acquisition. The Wikipedia corpus released by DPR  (Karpukhin et al . ,  2020 )  is widely recognized as a standard resource. However, as time progresses, this corpus faces challenges such as knowledge gaps and outdated information. Moreover, to accommodate dense retrieval requirements, the corpus is designed to fragment entity information into multiple segments, often resulting in incoherent data representation.  To address these limitations, we develop a novel Wikipedia corpus called  Indexed Wikicorpus  along with a corresponding key entity profile corpus called  Profile Wikicorpus . Our approach differs significantly from its predecessor in prioritizing the coherence of entity information. In Indexed Wikicorpus, each entry represents a complete entity as shown in Figure  3 , ensuring a more comprehensive and cohesive representation of information. Building upon this foundation, we draw inspiration from the Web version of Wikipedia to extract entity profiles. These profiles, curated by Wikipedia, encapsulate the essential information about each entity, which is saved to Profile Wikicorpus. Data analysis about the number of words and entities as shown in Table  1  reveals that our new corpus contains a higher volume of entity information compared to its predecessor. Furthermore, subsequent experiments demonstrate the superior performance of our corpus over the older version.",
            "To assess the generalizability of HiRAG, we conduct a series of experiments where we substitute different base models, with the results presented in Table  3 . Specifically, we evaluate the performance of HiRAG when paired with several prominent base models, including LLaMA-3-70B, LLaMA-3-8B, and Qwen2-7B. The results demonstrate that our method exhibits robustness and effectiveness across different base models, showcasing its adaptability to various model architectures. When paired with LLaMA-3-70B, our approach achieves state-of-the-art performance on most datasets, underscoring its ability to enhance the performance of strong base models and highlighting its potential for widespread applicability.   Notably, to ensure a fair comparison with existing SOTA models, like Meta-RAG, which is built on GPT-3.5-turbo, we use the same base model in our experiments, even though LLaMA-70B has shown better performance.  This allows for a more direct and meaningful comparison of our approach with prior work.",
            "We provide a case study in Figure  5  of Appendix  A.3  to demonstrate the workings of our framework. We gradually generate sub-questions, retrieve sub-questions, evaluate the retrieval results, and answer the sub-question. When the retrieval results are not ideal, we use rethink to find a more satisfactory result. Finally, we use the answers to all sub-questions to get the answer to the original question."
        ]
    },
    "id_table_4": {
        "caption": "Table 4.    Ablation experimental results for the Retriever module.",
        "table": "S5.T5.1.1",
        "footnotes": [],
        "references": [
            "To address the challenges, we propose a novel framework called the  Hi erarchical  R etrieval- A ugmented  G eneration Model with Rethink ( HiRAG ), as illustrated in Figure  4 . HiRAG comprises five key modules, i.e., Decomposer, Definer, Retriever, Filter, and Summarizer. The Filter module further incorporates two submodules, including Verify and Rethink. Previous methods typically rely on either sparse retrieval  (Svore and Burges,  2009 ) , which focuses on lexical matching, or dense retrieval  (Zhao et al . ,  2024 ) , which leverages latent embeddings for semantic matching. While some approaches combine the results of these two retrieval methods at the output level  (Arabzadeh et al . ,  2021 ; Luan et al . ,  2021 ; Karpukhin et al . ,  2020 ) , they do not deeply integrate the strengths of each retrieval method within the retrieval process itself.  We propose the Retrieval module which incorporates a new hierarchical retrieval strategy, performing multi-level retrieval from the document level to the chunk level. Initially, we employ sparse retrieval at the document level to extract key information, such as entity names, through lexical matching. Subsequently, dense retrieval is used to retrieve specific information at the chunk level, leveraging semantic matching.  We also introduce a novel retrieval method,  single-candidate retrieval , as shown in Figure  2(b) . This method returns only the most similar chunk as the retrieved knowledge for each decomposed sub-question. However, selecting just one candidate does not guarantee the correctness of the answer. To mitigate this, we apply the Filter module to assess the answer based on the retrieved knowledge. If the answer to a sub-question is found to be incorrect, we utilize the Rethink module, in conjunction with the Retrieval module, to select another chunk as the retrieved knowledge to answer the question.  The rethinking process is iterated until the Filter module confirms the answer as correct, thereby yielding the solution to the current sub-question. This cycle of decomposition, retrieval, and answering is repeated for subsequent sub-questions until the Definer module determines that the current question, accompanied by its sub-answers, is answerable. At this point, the Summarizer module is invoked to generate the final answer to the current question.",
            "To address the challenges in retrieval-augmented generation, including the lack of deep integration between different retrieval methods and the potential introduction of noise from multi-candidate retrieval, we propose a novel framework called  HiRAG . This framework consists of five key components: Decomposer, Summarizer, Retriever, Definer, and Filter, as illustrated in Figure  4 . The detailed algorithm is presented in Algorithm  1  in Appendix  A .  Specifically, the Decomposer is designed to tackle complex questions by decomposing them into smaller, more manageable sub-questions that can be easily answered. The Definer then determines whether the question can be solved. If it can, the Summarizer leverages the sub-answers to generate a response to the original question. Otherwise, the Retriever extracts relevant information related to the sub-question through a hierarchical retrieval process at both the document and chunk levels. Finally, the Filter verifies the validity of the retrieved content, generates a sub-answer, and re-evaluates the results if they are found to be inaccurate.",
            "We conduct ablation experiments to evaluate the contributions of our Retriever module, focusing on the hierarchical retrieval approach. By removing the processing during rethinking at the chunk level and document level, we investigate the impact of these components on the overall performance. The results, presented in Table  4 , demonstrate the effectiveness of our approach.  Our method is designed to improve the accuracy of retrieved results through chunk level and document level rethink. To isolate the impact of each level, we perform ablation experiments by eliminating one level of rethink at a time. Specifically, we remove chunk level rethink by considering only the highest-scoring chunk within a document and remove document level rethink by retrieving only the highest-scoring document. The results show that eliminating either level of rethink leads to a decrease in performance, confirming the importance of both chunk level and document level rethink in our hierarchical retrieval approach.  Furthermore, we investigate the influence of knowledge incorporated in the Profile WikiCorpus. Our findings indicate that removing this knowledge component leads to a decrease in the performance of HiRAG."
        ]
    },
    "id_table_5": {
        "caption": "Table 5.    Corpus experiments based on Flare, such as the corpus generated by DPR and Indexed Wikicorpus.",
        "table": "A1.T7.3.3",
        "footnotes": [],
        "references": [
            "We conduct experiments under the FLARE framework to assess the effectiveness of our corpus in improving model performance. As shown in Table  5 , replacing the external knowledge source with our corpus leads to notable advancements in Exact Match (EM) scores across four datasets, with three datasets experiencing improvements in all evaluated metrics. The comprehensive coverage and meticulous entity name segmentation in our corpus are key factors contributing to these improvements, enabling more efficient retrieval and utilization of relevant information.",
            "We provide a case study in Figure  5  of Appendix  A.3  to demonstrate the workings of our framework. We gradually generate sub-questions, retrieve sub-questions, evaluate the retrieval results, and answer the sub-question. When the retrieval results are not ideal, we use rethink to find a more satisfactory result. Finally, we use the answers to all sub-questions to get the answer to the original question.",
            "We illustrate the effectiveness of the HiRAG framework through a case study on a multi-hop question, as depicted in Figure  5 . Specifically, the original question presented in the figure is decomposed into two sub-questions, which are then retrieved and verified in a layered manner. Sub-questions that fail verification are re-evaluated through a rethink process. Notably, the second sub-question in the figure yields the correct result after multiple chunk-level rethinks. Ultimately, we combine the sub-answers to form the final answer to the original question."
        ]
    },
    "global_footnotes": []
}