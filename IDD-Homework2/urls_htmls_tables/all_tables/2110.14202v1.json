{
    "S6.T1": {
        "caption": "Table 1: Meta-test accuracies on the multimodal few-shot image classification with 2, 3, and 5 modes. The accuracy values are mean of 1000 randomly generated test tasks, and the ¬±plus-or-minus\\pm shows 95% confidence interval over tasks. 2Mode‚Ä† and 2Mode indicate the combination of mini-ImageNet with FC100 and Omniglot, respectively. * Results produced by code provided in ¬†[1]. ** Our Implementation.",
        "table": null,
        "footnotes": [],
        "references": [
            "Multimodal few-shot image classification results are shown in Table 1. Comparing MProtoNet with MMAML, we can see that replacing ProtoNet with MAML considerably improves the classification accuracy. We believe that most of the extra accuracy gained in the multimodal framework by replacing MAML with ProtoNet results from the proper training of modulation network due to improved gradient flow from meta-learner. A detailed experiment can be found in supplementary. Additionally, as the results in Table 1 suggest, the proposed KML scheme considerably improves the performance of multimodal few-shot classification with both meta-learners. The improvement in 5-shot scenarios is higher because more clues about the task provide more accurate embedding vectors to generate the modulation parameters. We have also included the detailed accuracy for 2Mode‚Ä† scenario consisting of mini-ImageNet and FC100 in table 2. This shows on average a positive knowledge transfer from mini-ImageNet to FC100 dataset in terms of increased classification accuracy compared to a single meta-learner trained and tested on FC100. The improved accuracy of proposed KML over previous modulation comes at the cost of parameter and computational overhead. A detailed discussion can be found in the supplementary. In addition to multimodal scenario, KML also brings considerable improvement in the conventional unimodal scenario (e.g., mini-ImageNet). Please see supplementary for more details and results."
        ]
    },
    "S6.T2": {
        "caption": "Table 2: Meta-test accuracies including the performance on each dataset.",
        "table": null,
        "footnotes": [],
        "references": [
            "Multimodal few-shot image classification results are shown in Table 1. Comparing MProtoNet with MMAML, we can see that replacing ProtoNet with MAML considerably improves the classification accuracy. We believe that most of the extra accuracy gained in the multimodal framework by replacing MAML with ProtoNet results from the proper training of modulation network due to improved gradient flow from meta-learner. A detailed experiment can be found in supplementary. Additionally, as the results in Table 1 suggest, the proposed KML scheme considerably improves the performance of multimodal few-shot classification with both meta-learners. The improvement in 5-shot scenarios is higher because more clues about the task provide more accurate embedding vectors to generate the modulation parameters. We have also included the detailed accuracy for 2Mode‚Ä† scenario consisting of mini-ImageNet and FC100 in table 2. This shows on average a positive knowledge transfer from mini-ImageNet to FC100 dataset in terms of increased classification accuracy compared to a single meta-learner trained and tested on FC100. The improved accuracy of proposed KML over previous modulation comes at the cost of parameter and computational overhead. A detailed discussion can be found in the supplementary. In addition to multimodal scenario, KML also brings considerable improvement in the conventional unimodal scenario (e.g., mini-ImageNet). Please see supplementary for more details and results."
        ]
    },
    "A1.T3": {
        "caption": "Table 3: Meta-test accuracies on unimodal few-shot classification for different number of shared layers.",
        "table": null,
        "footnotes": [],
        "references": [
            "Unimodal Few-Shot Classification. When compared to a multimodal scenario, in conventional unimodal few-shot setup, there could be less negative knowledge transfer between few-shot tasks (see Sec. B for an example). In this case, from the experiences in the MTL domain, we expect the performance to be increased when some layers are shared between tasks (specially earlier layers which encode low-level features ¬†[51]). The meta-test accuracies for different number of shared layers in a 4-layer CNN are shown in table 3. In this case, no shared layers means that all of the layers are modulated using task-aware KML scheme. In contrast, all layers shared means that no modulation is applied. Based on these results, proposed KML improves the unimodal few-shot classification by up to 2.5% compared to the vanilla meta-learner. Additionally,\nmodulating only third and fourth layers of the CNN on average yields the best results. The details of network architecture and hyperparameters used for this experiment can be found in Sec. J."
        ]
    },
    "A1.T4": {
        "caption": "Table 4: Meta-test accuracies on multimodal few-shot classification by including hard parameter sharing.",
        "table": null,
        "footnotes": [],
        "references": [
            "Multimodal Few-Shot Classification. In contrast, in multimodal scenario, few-shot tasks are more diverse. So, the negative transfer can happen more often due to the larger discrepancy between the characteristics of the different datasets. Therefore, here we expect the performance to degrade by sharing layers between tasks from different modes. The experimental results for sharing different number of layers in a 4-layer CNN are shown in table 4 for 2Mode multimodal classification (including mini-ImageNet and FC100). Similar results are obtained for other modes. As the results suggest, sharing layers between a diverse set of tasks degrades the performance, and the best results obtained when all of the layers are modulated."
        ]
    },
    "A3.T5": {
        "caption": "Table 5: The mean and standard deviation of cumulative reward per episode for multimodal reinforcement learning problems with 2, 4 and 6 modes reported across 3 random seeds.",
        "table": null,
        "footnotes": [],
        "references": [
            "The proposed KML idea can be extended to Reinforcement Learning (LR) environments, when using an optimization-based meta-learner like MAML. we have applied our KML algorithm on the official code of MMAML for RL experiments on three different environments used in ¬†[1]: Point Mass, Reacher, and Ant. Similar to ¬†[1], for each environment, the goals are sampled from a multimodal goal distribution, with similar environment-specific parameters as ¬†[1]. To have a fair comparison, we have kept all other hyperparameters the same as the ¬†[1]. The mean and standard deviation of cumulative reward per episode for multimodal reinforcement learning problems with 2, 4 and 6 modes are shown in table 5. Results show that our proposed KML can achieve gain over ¬†[1] in all of the RL experiment setups."
        ]
    },
    "A4.T6": {
        "caption": "Table 6: Comparison of meta-test accuracies for original implementation of MMAML with proposed new interpretation (new-MMAML) for few-shot classification on multimodal scenario.",
        "table": null,
        "footnotes": [],
        "references": [
            "Second, we compare the training performance of MMAML and new-MMAML. In the official implementation of MMAML, the affine transform of the BN layer ¬†[52] is disabled, due to the similar functionality performed by FiLM. However, since in new-MMAML we are using modulated parameters, we enable the affine transform of the BN layer. Meta-test results are shown in table 6 for different multimodal image classification modes. As the results suggest the new-MMAML has almost the same performance as the MMAML which also verifies the proposed interpretation through experiments. The minor difference between the results is due to the difference between the BN layer in two implementations (as discussed)."
        ]
    },
    "A5.T7": {
        "caption": "Table 7: Number of parameters required in each structure as modulation parameter generator ùê†œïsubscriptùê†italic-œï\\mathbf{g}_{\\phi}.",
        "table": null,
        "footnotes": [],
        "references": [
            "Following the structure proposed in ¬†[1], the base network consists of four convolutional layer with the channel size 32, 64, 128 and 256. In our KML scheme we intend to produce a modulation parameter for each parameter of the base network using task embedding ùùäùùä\\boldsymbol{\\upsilon} (with a dimension of 128) as input. Table 7 compares the number of parameters in single MLP and proposed structure when used as parameter generation network for each layer. The total number is also provided. As one can see, proposed structure reduces the number of the parameters by a factor of 152."
        ]
    },
    "A5.T8": {
        "caption": "Table 8: 2Mode Meta-test accuracy of the proposed simplified structure versus single MLP for 2Mode 5-way scenario.",
        "table": null,
        "footnotes": [],
        "references": [
            "We also compare using MLP with proposed structure in terms of convergence\nspeed. We use the same hyperparameters for training both models.\nThe accuracy of meta-validation set during meta-training on 3Mode,\n5-way 1-shot setting is plotted in figure 11.\nWe can clearly see that using the proposed structure as modulation parameter generator, the network converges faster and also yields better results in term of accuracy compared to single MLP.\nWe have also provided the meta-test accuracy results for 3Mode few-shot classification in Table 8. These results also support the improved performance of the proposed simplified structure versus single MLP."
        ]
    },
    "A6.T9": {
        "caption": "Table 9: Training time for MProtoNet and MProtoNet+KML (proposed method) for 2Mode setup.",
        "table": null,
        "footnotes": [],
        "references": [
            "Second, in terms of computational overhead, the table 9 shows the total training time for MProtoNet (existing method in ¬†[1]) and MProtoNet+KML (ours) for 2Mode (combination of Omniglot and miniImageNet), 5-way 5-shot scenario. As the results show, the computational overhead of the proposed method in training time is around 5.2%. Similar training results are obtained for the other setups (3 Mode, 5Mode). Also please note that the inference time of our method and existing method ¬†[1] are almost the same (on average 0.087 seconds for each mini-batch of few-shot tasks)."
        ]
    },
    "A7.T10": {
        "caption": "Table 10: Meta-test accuracies for 2Mode setup with different rank approximation in simplified parameter generator gœïsubscriptùëîitalic-œïg_{\\phi}.",
        "table": null,
        "footnotes": [],
        "references": [
            "The meta-test results for 2Mode classification are shown in table 10. As results suggest, the 2-rank and 3-rank approximations still have better performance compared to the MProtoNet. However, the performance is degraded compared to the 1-rank approximation. The possible reason could be overfitting of 2-rank and 3-rank versions due to more parameters."
        ]
    },
    "A8.T11": {
        "caption": "Table 11: Results of applying the proposed KML on visual reasoning dataset CLEVR.",
        "table": null,
        "footnotes": [],
        "references": [
            "We applied the KML to the CLEVR dataset by replacing KML with the FiLM in the official code of the FiLM paper ¬†[43]. The results are shown in table 11. As the results show, KML on average improves the FiLM by 0.5% in the 5 question types. Note that KML obtains this improvement while FiLM has achieved very high accuracy already."
        ]
    },
    "A10.T12": {
        "caption": "Table 12: Details of Datasets.",
        "table": null,
        "footnotes": [],
        "references": [
            "To create a meta-dataset for multi-modal few-shot classification, we utilize five popular datasets: OMNIGLOT, MINI-IMAGENET, FC100, CUB, and AIRCRAFT. The detailed information of all the datasets are summarized in Table 12. To fit the images from all the datasets to a model, we resize all the images to 84 √ó 84. The images randomly sampled from all the datasets are shown in Figure 13, demonstrating a diverse set of modes."
        ]
    },
    "A10.T13": {
        "caption": "Table 13: Hyperparameters used in the experiments. ‚Ä†‚Ä†{\\dagger} Halve every 10K Iterations.",
        "table": null,
        "footnotes": [],
        "references": [
            "The hyperparameters for all the experiments are shown in Table 13. For comparing our algorithm with previous work, we use exactly the same hyperparameters.\nWe use 15 examples per class for evaluating the post-update meta-gradient for all the experiments, following ¬†[19, 17, 1, 11]. In the training of all networks, we use the Adam optimizer with default hyperparameters."
        ]
    },
    "A11.T14": {
        "caption": "Table 14: Amount of compute in this project. The GPU hours include computations for early explorations and experiments to produce the reported values. The carbon emission values are computed using https://mlco2.github.io/.",
        "table": null,
        "footnotes": [],
        "references": [
            "Amount of compute:\nAll the results in this paper are produced by a machine with a single RTX 2080 Ti GPU. The amount of compute in this project is documented in Table¬†14. We follow submission guidelines to include the amount of compute for different experiments and CO2 emission."
        ]
    }
}