{
    "id_table_1": {
        "caption": "Table 1 :    Comparison of each data source and their issues when conducting pre-training.  While real videos enhance model performance, they have concerns related to collection cost, privacy and licenses. Synthetic videos  [ 33 ,  38 ,  84 ]  and pseudo motions by MoSI  [ 30 ]  partially resolve these issues, but they rely on the CNN architecture and its inherent inductive bias, thus failing to accurately train ViT. Note that in VPN, additional real data is required for optimal performance, therefore the asterisked issues (   \\ast  ) are not resolved. Our proposed framework is free of these issues by generating pseudo-motion videos from synthetic images.",
        "table": "S1.T1.8",
        "footnotes": [
            "",
            "",
            "",
            ""
        ],
        "references": [
            "In this paper, to mitigate video collection costs and address concerns regarding privacy, bias, and licenses, we propose a self-supervised learning framework for video transformers using synthetic images (Table  1 ). Our framework includes a Pseudo Motion Generator (PMG) module that recursively applies image transformations to static images, generating videos with diverse pseudo-motion. These videos are then used for masked video modeling. Through experiments, by using videos generated from the PMG module, we examine that video transformers can learn transferable and robust video features which are not limited to a single domain. To the best of our knowledge, we are the first to pre-train video transformers exclusively using synthetic images. Our contributions are threefold;",
            "Few attempts are made to train action recognition models using synthetic data. For example, the GATA dataset  [ 26 ] , collected from a video game, is proposed for human motion representation learning. However, this dataset is not allowed for commercial use, and the rights of game companies have not been considered. Another example is the Video Perlin Noise (VPN) dataset  [ 33 ] , which is generated from Perlin Noise  [ 53 ,  54 ] . This dataset is proposed to initialize model weights before pre-training. Zhong et al.  [ 84 ]  propose a pre-training method with both No-Human Kinetics (NH-Kinetics) and SynAPT  [ 38 ] . While these approaches contribute to model performance, they still require pre-training on real videos. Additionally, ElderSim  [ 31 ] , PHAV  [ 16 ] , and SURREAL  [ 66 ] , which are included in SynAPT, are not allowed for commercial use. As an alternative, Huang et al.  [ 30 ]  have proposed MoSI, which pre-trains models with pseudo-motion videos generated from static images. In terms of collection cost, requiring only static images for pre-training is favorable. However, because MoSIs synthesized videos lack diversity, they fail to pre-train video transformers (See  Tab.   10 ).",
            "To reduce the collection cost of video data, we propose a self-supervised framework using pseudo-motion videos generated from static images. Figure  1  shows the overview of our framework. We first generate pseudo-motion videos from static images by Pseudo Motion Generator (PMG). Then, we utilize these videos to train VideoMAE  [ 64 ,  19 ] . VideoMAE is a powerful self-supervised learning framework and can learn spatio-temporal features effectively by reconstructing masked video regions from their complementaries. Some works point out that VideoMAE has a tendency to learn low-level features such as edges, thus failing to achieve high-level alignment  [ 40 ,  56 ] . Conversely, VideoMAE does not obtain domain-specific features, leading to high transferability. We focus on and leverage this characteristic to train video transformers with pseudo-motion videos.",
            "As candidates for image transformation, we consider the following 8 image transformations. (See  Sec.   4.1.1  for the effect of each transformation)",
            "From these candidates, through experimentation, we identify the optimal set of image transformations   italic- \\phi italic_  (See  Sec.   4.1.2 ). Furthermore, to prevent overfitting to specific types of pseudo-motion videos, we apply mixup  [ 82 ]  to each frame of the generated pseudo-motion videos. This approach significantly enhances the diversity in motion and appearance of the pseudo-motion videos, facilitating more efficient learning by VideoMAE. In the supplementary material, we describe the parameters for each image augmentation.",
            "To further enhance the diversity of videos, we applied video-level augmentation to the generated pseudo-motion videos. We examined two methods: Mixup  [ 82 ]  and VideoMix  [ 81 ] .   Tab.   5  demonstrates that video-level augmentation, especially Mixup, significantly contributes to performance improvement. This is because both video augmentations diversify pseudo-motion videos, resulting in better performance. Pre-training with VideoMix results in lower accuracy compared to Mixup because the videos generated by VideoMix have non-continuous regions like CutMix, as discussed in  Sec.   4.1.2 , From here, we will utilize Mixup in our experiments.",
            "Fig.   10  shows the relationship between the number of epochs of pre-training and accuracy on HMDB51. For generating videos for pre-training, we used 10k images from Shaders1k and a frame from each of the 3k videos in HMDB51. In both datasets, the model performance improved over epochs and the difference of accuracy gradually decreased. Because our PMG allows for the generation of diverse videos, even if we have a small amount of data for pre-training, it is possible to improve performance by increasing the number of iterations.",
            "Based on the results of the previous experiment, we hypothesized that performance can be further enhanced by increasing the diversity of samples in pre-training image datasets.  Fig.   10  shows the relationship between the number of categories in the pre-training datasets we use and the classification performance on HMDB51. We set the number of training samples to 10k images, using the IN-1k and Shaders1k datasets. For IN-1k, the accuracy seems to saturate after raising the diversity to more than 50 classes. For Shaders1k, the accuracy was almost the same even when the number of categories increased. This suggests our framework scales with having more data samples, but does not require semantic diversity within the samples. These results support the fact that VideoMAE learns low-level features like the correspondence of patches between frames, rather than semantic information like categories of objects displayed.",
            "The upper part of  Tab.   10  presents the performance of existing works which pre-train using the HMDB51, UCF101, and Diving48 datasets. Existing methods like 3D-ResNet with VPN  [ 33 ] , TimeSformer with SynAPT, and PPMA have improved model performance compared to training the model from scratch. However, they still require real data, causing issues as mentioned. In contrast, our framework, despite using fewer samples which are also synthetic, achieves comparable performance on UCF101 and better performance on Diving48.",
            "We also compared with pre-training methods which only use static images (the lower part of  Tab.   10 ). MoSI works on CNN-based architectures, but it fails to pre-train a ViT model because of the lack of diversity in generated videos. Supervised pre-training (SP) on IN-21k, ExFractalDB-21k  [ 34 ]  and VisualAtom-21k slightly improves the performance in comparison with from scratch. However, our framework significantly surpasses that performance in both settings, when using real images and when using synthetic images.",
            "Following the SynAPT benchmark  [ 38 ] , we evaluate using the following six datasets: UCF101, HMDB51, MiniSSV2, Diving48, IkeaFA, and UAV-H.  Tab.   11  presents the results. Using only synthetic images, our proposed framework partially surpasses some of the results of existing works utilizing real videos and action labels. Our framework is inferior to PPMA on UCF101, HMDB51, and IkeaFA. This is because these datasets have less data than others. PPMA leverages the 150 action labels in the video datasets for pre-training, therefore having the advantage of learning action features from a small number of videos during fine-tuning. On the other hand, our framework, not having these labels beforehand, struggled to learn meaningful features with fewer labeled data. However, our framework shows better performance on less biased datasets like MiniSSV2, Diving48, and UAV-H. This suggests that scene and object biases are mitigated when using our generated synthetic videos.",
            "Tab.   13  shows the comparison of our framework with VideoMAE on K400. Although our framework outperforms the model from scratch, it falls short of the performance of VideoMAE with real videos. This shortfall is attributed to the limited diversity of pseudo-motion videos generated by PMG, especially when compared to the vast variety found in large-scale datasets. We understand our shortcoming here, but increasing the diversity of generated videos may close this gap.",
            "Finally, to support our hypothesis that VideoMAE learns the correspondence of patches between frames, we conducted a simple experiment. Here, we assume that a larger frame difference in a video makes it difficult to capture this correspondence, for instance, due to extreme camera motion. Based on this, we made three subsets from HMDB51 and UCF101 depending on the frame difference; (i) videos having the top 50% average frame difference (ii) videos ranging from the 25th to the 75th percentile in average frame difference, (iii) videos having the bottom 50% average frame difference. We then use each of these subsets for pre-training, then fine-tune on the full set. The results are shown in  Tab.   13 . Models that are pre-trained on (i) and (iii) performed worse than those pre-trained on (ii). This lends support to our hypothesis regarding what VideoMAE learns.",
            "We conducted the experiments with 8 A100 GPUs for both pre-training and fine-tuning, mostly following the settings in VideoMAE  [ 64 ] . The settings for pre-training are detailed in  Tab.   14  and those for fine-tuning are described in  Tab.   15 . We used PyTorch  [ 52 ]  to implement our framework.",
            "Fig.   10  shows the accuracy per class on UCF101. As in the patterns observed in HMDB51, model (ii) improved the performance in classes like BodyWeightSquats, CleanAndJerk, JumpRope and YoYo, where videos lack object and background cues. Additionally, model (ii) successfully differentiated between action classes involving similar objects, for instance, BasketballDunk versus Basketball, and HammerThrow versus Hammering. However, in comparison between model (ii) and (iii), we found it was difficult for model (ii) to recognize more fine-grained actions such as Handstand Walking, Nunchucks, PullUps, and WallPushups.",
            "Another limitation of our framework is that our framework does not learn high-level semantic features, because our framework focuses on low-level features and does not utilize labels during pre-training, unlike PPMA  [ 84 ] . This limitation leads to lower performance in the linear probing settings, where the weights of the encoder are frozen while only the linear layer is trained ( Tab.   16 ). Moreover, it is challenging to extend our framework to other tasks like video-text retrieval and video captioning, without additional training or extra labeled data. We will also tackle this issue in future work."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Comparison of different image augmentations.",
        "table": "S4.T3.fig1.5",
        "footnotes": [],
        "references": [
            "From these candidates, through experimentation, we identify the optimal set of image transformations   italic- \\phi italic_  (See  Sec.   4.1.2 ). Furthermore, to prevent overfitting to specific types of pseudo-motion videos, we apply mixup  [ 82 ]  to each frame of the generated pseudo-motion videos. This approach significantly enhances the diversity in motion and appearance of the pseudo-motion videos, facilitating more efficient learning by VideoMAE. In the supplementary material, we describe the parameters for each image augmentation.",
            "Figure  2  presents examples of pseudo-motion videos generated by PMG. Although the motions in these videos differ from real videos, they exhibit a wide range of motion and appearance patterns. Moreover, the clear correspondence of patches between frames makes these pseudo-motion videos particularly well-suited for VideoMAE, because it focuses on capturing low-level features rather than high-level semantic features. Notably, when pre-training VideoMAE using real videos, we can use pseudo-motion videos generated from a frame within the videos as a powerful form of data augmentation (we call this PMG Aug). We demonstrate the effect of PMG Aug through experiments (Refer to  Sec.   4.4 ).",
            "To further enhance the diversity of videos, we applied video-level augmentation to the generated pseudo-motion videos. We examined two methods: Mixup  [ 82 ]  and VideoMix  [ 81 ] .   Tab.   5  demonstrates that video-level augmentation, especially Mixup, significantly contributes to performance improvement. This is because both video augmentations diversify pseudo-motion videos, resulting in better performance. Pre-training with VideoMix results in lower accuracy compared to Mixup because the videos generated by VideoMix have non-continuous regions like CutMix, as discussed in  Sec.   4.1.2 , From here, we will utilize Mixup in our experiments."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Combination of image augmentations for PMG.",
        "table": "S4.T3.fig2.5",
        "footnotes": [],
        "references": [
            "First, we investigated the contribution of each image transformation on VideoMAE pre-training. We used HMDB51 and UCF101 for generating pseudo-motion videos for pre-training, then used videos from the respective datasets to fine-tune the model.  Tab.   3  reports the results when applying only a single variation of image augmentation. Videos generated by the Identity transformation serve as a baseline because they do not contain any motion. Compared to this baseline, videos generated with Sliding Window, Zoom-in/out, Affine Transformation, Perspective Transformation, and CutMix improve the models accuracy over the baseline. Pseudo-motion videos generated with these transformations have corresponding patches between frames, meaning that patches in one frame might slightly move but would still exist in the subsequent frame. Therefore, this supports our hypothesis that this characteristic aids the VideoMAE when learning spatio-temporal features.",
            "Tab.   3  compares the performance on HMDB51 when models are pre-trained with various combinations of image transformations. It is observed that combining multiple image transformations improves the models performance. This indicates that the model can effectively learn as long as there is sufficient diversity, even if the motion patterns in pseudo-motion videos differ from those in real videos. However, combining more image transformations did not necessarily yield better results. In particular, in most cases where we applied CutMix, the accuracy decreased. We hypothesize that this is due to the non-continuous nature of CutMix videos. From this point on, we will use Zoom-in/out and Affine Transformation as the set of image transformations   italic- \\phi italic_ . Further discussion on the failure cases of pre-training with these pseudo-motion videos is provided in the supplementary material.",
            "Fig.   3(b)  presents the accuracy transition when the number of pre-training samples is varied among  { 1  k , 5  k , 10  k , 50  k , 100  k } 1 k 5 k 10 k 50 k 100 k \\{1k,5k,10k,50k,100k\\} { 1 italic_k , 5 italic_k , 10 italic_k , 50 italic_k , 100 italic_k } . Our framework shows improvement as the number of data increased. Because we use only a small subset from PASS and Shaders1k, there is potential for more substantial performance improvement when generating from all images.",
            "Tab.   13  shows the comparison of our framework with VideoMAE on K400. Although our framework outperforms the model from scratch, it falls short of the performance of VideoMAE with real videos. This shortfall is attributed to the limited diversity of pseudo-motion videos generated by PMG, especially when compared to the vast variety found in large-scale datasets. We understand our shortcoming here, but increasing the diversity of generated videos may close this gap.",
            "Finally, to support our hypothesis that VideoMAE learns the correspondence of patches between frames, we conducted a simple experiment. Here, we assume that a larger frame difference in a video makes it difficult to capture this correspondence, for instance, due to extreme camera motion. Based on this, we made three subsets from HMDB51 and UCF101 depending on the frame difference; (i) videos having the top 50% average frame difference (ii) videos ranging from the 25th to the 75th percentile in average frame difference, (iii) videos having the bottom 50% average frame difference. We then use each of these subsets for pre-training, then fine-tune on the full set. The results are shown in  Tab.   13 . Models that are pre-trained on (i) and (iii) performed worse than those pre-trained on (ii). This lends support to our hypothesis regarding what VideoMAE learns."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :    Effects of video-level augmentation.",
        "table": "S4.T5.fig1.7",
        "footnotes": [],
        "references": [
            "As candidates for image transformation, we consider the following 8 image transformations. (See  Sec.   4.1.1  for the effect of each transformation)",
            "From these candidates, through experimentation, we identify the optimal set of image transformations   italic- \\phi italic_  (See  Sec.   4.1.2 ). Furthermore, to prevent overfitting to specific types of pseudo-motion videos, we apply mixup  [ 82 ]  to each frame of the generated pseudo-motion videos. This approach significantly enhances the diversity in motion and appearance of the pseudo-motion videos, facilitating more efficient learning by VideoMAE. In the supplementary material, we describe the parameters for each image augmentation.",
            "Figure  2  presents examples of pseudo-motion videos generated by PMG. Although the motions in these videos differ from real videos, they exhibit a wide range of motion and appearance patterns. Moreover, the clear correspondence of patches between frames makes these pseudo-motion videos particularly well-suited for VideoMAE, because it focuses on capturing low-level features rather than high-level semantic features. Notably, when pre-training VideoMAE using real videos, we can use pseudo-motion videos generated from a frame within the videos as a powerful form of data augmentation (we call this PMG Aug). We demonstrate the effect of PMG Aug through experiments (Refer to  Sec.   4.4 ).",
            "To further enhance the diversity of videos, we applied video-level augmentation to the generated pseudo-motion videos. We examined two methods: Mixup  [ 82 ]  and VideoMix  [ 81 ] .   Tab.   5  demonstrates that video-level augmentation, especially Mixup, significantly contributes to performance improvement. This is because both video augmentations diversify pseudo-motion videos, resulting in better performance. Pre-training with VideoMix results in lower accuracy compared to Mixup because the videos generated by VideoMix have non-continuous regions like CutMix, as discussed in  Sec.   4.1.2 , From here, we will utilize Mixup in our experiments.",
            "Note that VideoMAE pre-trained with VPN has low accuracy on downstream classification tasks, which suggests that VPN does not work well with VideoMAE when learning spatio-temporal features. We consider this is because VPN videos have temporal continuity, but do not possess clear correspondence of patches between frames (e.g. edges are ambiguous, and regions suddenly disappear or appear). We believe this characteristic is key for effective VideoMAE pre-training. In  Sec.   4.7 , we further experiment to support this hypothesis.",
            "In previous experiments, the full set of video datasets for fine-tuning was available. Under these conditions, pre-training with all the videos for fine-tuning yielded better performance than our framework. However, for video datasets, there is often a limited amount of training samples to fine-tune with. To assess the effectiveness of our framework in such cases, we sampled  { 1 , 5 , 10 , 25 , 50 } 1 5 10 25 50 \\{1,5,10,25,50\\} { 1 , 5 , 10 , 25 , 50 }  videos per category from HMDB51 and UCF101, respectively, and compared the performance of our framework with VideoMAE using real videos.  Fig.   4  presents the results. The model pre-trained by our framework shows higher performance compared to the model pre-trained by VideoMAE using real data. This underscores the efficacy of our framework where the available data is limited.",
            "We conducted the experiments with 8 A100 GPUs for both pre-training and fine-tuning, mostly following the settings in VideoMAE  [ 64 ] . The settings for pre-training are detailed in  Tab.   14  and those for fine-tuning are described in  Tab.   15 . We used PyTorch  [ 52 ]  to implement our framework."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 :  Transferability from other video datasets.",
        "table": "S4.T5.fig2.5",
        "footnotes": [],
        "references": [
            "To further enhance the diversity of videos, we applied video-level augmentation to the generated pseudo-motion videos. We examined two methods: Mixup  [ 82 ]  and VideoMix  [ 81 ] .   Tab.   5  demonstrates that video-level augmentation, especially Mixup, significantly contributes to performance improvement. This is because both video augmentations diversify pseudo-motion videos, resulting in better performance. Pre-training with VideoMix results in lower accuracy compared to Mixup because the videos generated by VideoMix have non-continuous regions like CutMix, as discussed in  Sec.   4.1.2 , From here, we will utilize Mixup in our experiments.",
            "To verify the transferability of our framework, we conducted experiments by pre-training models with pseudo-motion videos generated from frames in HMDB51 and then fine-tuning on UCF101 (hereafter, we refer to this as HMDB51    \\rightarrow   UCF101), and then vice versa (UCF101    \\rightarrow   HMDB51).  Tab.   5  shows the results. Comparing the accuracy when pre-training on different datasets, the difference is marginal. This suggests that our framework learns robust features that are not domain-specific. Furthermore, this appeals that our framework can effectively pre-train models even when using image datasets instead, such as ImageNet and PASS.",
            "We conducted the experiments with 8 A100 GPUs for both pre-training and fine-tuning, mostly following the settings in VideoMAE  [ 64 ] . The settings for pre-training are detailed in  Tab.   14  and those for fine-tuning are described in  Tab.   15 . We used PyTorch  [ 52 ]  to implement our framework.",
            "While the algorithm of our Pseudo Motion Generator (PMG) is detailed in the main paper, we offer Python pseudo-code for PMG in  Fig.   5  for more clarity."
        ]
    },
    "id_table_6": {
        "caption": "Table 6 :  Pre-training with ImageNet and PASS.  The term FT data indicates that the datasets used for pre-training are identical to those used in fine-tuning.",
        "table": "S4.T7.fig1.6",
        "footnotes": [],
        "references": [
            "Fig.   6  shows the examples of pseudo-motion videos generated from three synthetic image datasets; FractalDB  [ 35 ] , Shaders1k  [ 7 ] , and Visual Atom  [ 62 ] . Although the appearance and motions in these videos differ from real videos, they exhibit a wide range of motion and appearance patterns. This variety enables VideoMAE to learn effectively. Specifically, pre-training with pseudo-motion videos generated from Shaders1k improves the models performance compared to pre-training with those from the other sources. This improvement is attributed to the videos from Shaders1k having a clear correspondence of patches between frames, which suits for VideoMAE.",
            "Another limitation of our framework is that our framework does not learn high-level semantic features, because our framework focuses on low-level features and does not utilize labels during pre-training, unlike PPMA  [ 84 ] . This limitation leads to lower performance in the linear probing settings, where the weights of the encoder are frozen while only the linear layer is trained ( Tab.   16 ). Moreover, it is challenging to extend our framework to other tasks like video-text retrieval and video captioning, without additional training or extra labeled data. We will also tackle this issue in future work."
        ]
    },
    "id_table_7": {
        "caption": "Table 7 :  Pre-training on synthetic image datasets.",
        "table": "S4.T7.fig2.5",
        "footnotes": [],
        "references": [
            "In our previous experiments, we used samples with similar visual cues between pre-training and fine-tuning, namely the semantic information including objects and people. To further assess the transferability of our framework, we conducted pre-training on the ImageNet-1k and PASS, which are in different domains compared to the fine-tuning datasets (UCF101 and HMDB51). As detailed in  Tab.   7 , pre-training using ImageNet and PASS achieved comparable performance to when pre-training with the same datasets that are used when fine-tuning. Note that PASS does not include any human images. Therefore, the semantic information within pre-training datasets are not a must for effective pre-training of VideoMAE. Moreover, increasing the number of images scaled the performance. These experimental results suggest that for VideoMAE, the diversity of the data is more crucial than domain-specific information like human motion or visual cues.",
            "We then pre-trained on synthetic image datasets using our framework to verify that spatio-temporal features can be effectively learnt from synthetic images, which present completely different visual cues compared to our target action recognition datasets. For synthetic image datasets, we used FractalDB, Shaders1k, and Visual Atom. Herein, we used 10k/100k images sampled from each dataset.  Tab.   7  shows the performance on UCF101 and HMDB51 when pre-training on diverse synthetic datasets, including FractalDB, Shaders1k, and VisualAtom. Note that pre-training with Shaders1k achieved comparable results to pre-training with real images, where pre-training with FractalDB and Visual Atom lead to subpar performance. This denotes that the model struggles to correlate patches between frames of pseudo-motion videos generated from FractalDB and Visual Atom, thus failing to capture robust low-level features. On the other hand, images in Shaders1k have distinctive patches that can be correlated before and after transformations, which supports the model when capturing low-level features. This indicates that our framework can successfully replace the need for real data when pre-training the model, as long as synthetic videos have patches that can be tracked between frames. Thus, when using our framework, challenges related to real datasets such as privacy and license infringement are nonexistent.",
            "Note that VideoMAE pre-trained with VPN has low accuracy on downstream classification tasks, which suggests that VPN does not work well with VideoMAE when learning spatio-temporal features. We consider this is because VPN videos have temporal continuity, but do not possess clear correspondence of patches between frames (e.g. edges are ambiguous, and regions suddenly disappear or appear). We believe this characteristic is key for effective VideoMAE pre-training. In  Sec.   4.7 , we further experiment to support this hypothesis.",
            "To verify that VideoMAE successfully learns the reconstruction task, we visualized its output results on HMDB51 and UCF101. We compared the outputs of three models: (i) VideoMAE trained on real videos from each dataset, (ii) VideoMAE trained on pseudo-motion videos generated from frames on each video dataset, and (iii) VideoMAE trained on pseudo-motion videos from Shaders1k.  Fig.   7  and  Fig.   8  shows the results for HMDB51 and UCF101, respectively. The inputs for these models were sampled from the test set, which was not used for pre-training. Despite not being trained on real videos, VideoMAE trained on Shaders1k manages to achieve a reasonable level of accuracy in reconstructing real videos. This suggests that the method can roughly capture the complex motion and shape characteristics of the real world."
        ]
    },
    "id_table_8": {
        "caption": "Table 8 :    Effectiveness of our PMG as a video augmentation method on HMDB51 and UCF101.     results from  [ 64 ] .",
        "table": "S4.T9.4.4",
        "footnotes": [
            ""
        ],
        "references": [
            "To verify that VideoMAE successfully learns the reconstruction task, we visualized its output results on HMDB51 and UCF101. We compared the outputs of three models: (i) VideoMAE trained on real videos from each dataset, (ii) VideoMAE trained on pseudo-motion videos generated from frames on each video dataset, and (iii) VideoMAE trained on pseudo-motion videos from Shaders1k.  Fig.   7  and  Fig.   8  shows the results for HMDB51 and UCF101, respectively. The inputs for these models were sampled from the test set, which was not used for pre-training. Despite not being trained on real videos, VideoMAE trained on Shaders1k manages to achieve a reasonable level of accuracy in reconstructing real videos. This suggests that the method can roughly capture the complex motion and shape characteristics of the real world."
        ]
    },
    "id_table_9": {
        "caption": "Table 9 :    Comparison of each combination with real videos and pseudo-motion videos.     Sources of PMG Aug.",
        "table": "S4.T9.9.5",
        "footnotes": [],
        "references": [
            "As we have verified so far, our framework enables efficient pre-training with static images. This suggests that our proposed PMG can be also used as a data augmentation method during pre-training.  Tab.   9  compares the performance when pre-training solely with real videos and when pre-training with both real and pseudo-motion videos. Notably, PMG Aug boosted model accuracy by up to 2%. This suggests that synthetic motion, despite its differences to real video motion, unintuitively contributes to the models performance by increasing diversity.",
            "Next, we use image datasets as well as sources of PMG Aug during pre-training. For the image datasets (PASS and Shaders1k), we randomly sampled 10k images as input.  Tab.   9  compares the performance of the models pre-trained on HMDB51, PASS, and Shaders1k. The results show that using both image and video datasets improved the models performance. Particularly, the combination of HMDB51 and PASS enhanced the accuracy on HMDB51 by 5.4% compared to pre-training with only real videos. This indicates that using PMG Aug resolves the problem of insufficient data quantity during VideoMAE pre-training.",
            "Fig.   9  presents the accuracy per class on HMDB51. Between model (i) and (ii), model (ii) demonstrated improved performance of actions such as cartwheel, sit, and stand, which rely on motion information for recognition. However, in the comparison between model (ii) and (iii), we found that model (ii) struggled to classify actions like kiss, push, shake hands, and wave, which involve more subtle and fine-grained motion."
        ]
    },
    "id_table_10": {
        "caption": "Table 10 :    Comparison with existing methods on HMDB51, UCF101, and Diving48.  RV = Real Videos, SV = Synthetic Videos, RI = Real Images, SI = Synthetic Images, SP = Supervised Pre-training, FT data = Fine-tuning data.    Results in our replicated experiments.    Reported in  [ 38 ] .    Herein, we refer to a combination of ElderSim  [ 31 ] , SURREACT  [ 66 ] , and PHAV  [ 58 ]  as SynAPT, as proposed in   [ 38 ] .    we report only the number of videos in SynAPT.",
        "table": "S4.T10.21",
        "footnotes": [
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Few attempts are made to train action recognition models using synthetic data. For example, the GATA dataset  [ 26 ] , collected from a video game, is proposed for human motion representation learning. However, this dataset is not allowed for commercial use, and the rights of game companies have not been considered. Another example is the Video Perlin Noise (VPN) dataset  [ 33 ] , which is generated from Perlin Noise  [ 53 ,  54 ] . This dataset is proposed to initialize model weights before pre-training. Zhong et al.  [ 84 ]  propose a pre-training method with both No-Human Kinetics (NH-Kinetics) and SynAPT  [ 38 ] . While these approaches contribute to model performance, they still require pre-training on real videos. Additionally, ElderSim  [ 31 ] , PHAV  [ 16 ] , and SURREAL  [ 66 ] , which are included in SynAPT, are not allowed for commercial use. As an alternative, Huang et al.  [ 30 ]  have proposed MoSI, which pre-trains models with pseudo-motion videos generated from static images. In terms of collection cost, requiring only static images for pre-training is favorable. However, because MoSIs synthesized videos lack diversity, they fail to pre-train video transformers (See  Tab.   10 ).",
            "Fig.   10  shows the relationship between the number of epochs of pre-training and accuracy on HMDB51. For generating videos for pre-training, we used 10k images from Shaders1k and a frame from each of the 3k videos in HMDB51. In both datasets, the model performance improved over epochs and the difference of accuracy gradually decreased. Because our PMG allows for the generation of diverse videos, even if we have a small amount of data for pre-training, it is possible to improve performance by increasing the number of iterations.",
            "Based on the results of the previous experiment, we hypothesized that performance can be further enhanced by increasing the diversity of samples in pre-training image datasets.  Fig.   10  shows the relationship between the number of categories in the pre-training datasets we use and the classification performance on HMDB51. We set the number of training samples to 10k images, using the IN-1k and Shaders1k datasets. For IN-1k, the accuracy seems to saturate after raising the diversity to more than 50 classes. For Shaders1k, the accuracy was almost the same even when the number of categories increased. This suggests our framework scales with having more data samples, but does not require semantic diversity within the samples. These results support the fact that VideoMAE learns low-level features like the correspondence of patches between frames, rather than semantic information like categories of objects displayed.",
            "The upper part of  Tab.   10  presents the performance of existing works which pre-train using the HMDB51, UCF101, and Diving48 datasets. Existing methods like 3D-ResNet with VPN  [ 33 ] , TimeSformer with SynAPT, and PPMA have improved model performance compared to training the model from scratch. However, they still require real data, causing issues as mentioned. In contrast, our framework, despite using fewer samples which are also synthetic, achieves comparable performance on UCF101 and better performance on Diving48.",
            "We also compared with pre-training methods which only use static images (the lower part of  Tab.   10 ). MoSI works on CNN-based architectures, but it fails to pre-train a ViT model because of the lack of diversity in generated videos. Supervised pre-training (SP) on IN-21k, ExFractalDB-21k  [ 34 ]  and VisualAtom-21k slightly improves the performance in comparison with from scratch. However, our framework significantly surpasses that performance in both settings, when using real images and when using synthetic images.",
            "Fig.   10  shows the accuracy per class on UCF101. As in the patterns observed in HMDB51, model (ii) improved the performance in classes like BodyWeightSquats, CleanAndJerk, JumpRope and YoYo, where videos lack object and background cues. Additionally, model (ii) successfully differentiated between action classes involving similar objects, for instance, BasketballDunk versus Basketball, and HammerThrow versus Hammering. However, in comparison between model (ii) and (iii), we found it was difficult for model (ii) to recognize more fine-grained actions such as Handstand Walking, Nunchucks, PullUps, and WallPushups."
        ]
    },
    "id_table_11": {
        "caption": "Table 11 :    Results on SynAPT benchmark.     Results reported in  [ 38 ] .",
        "table": "S4.T11.3",
        "footnotes": [
            ""
        ],
        "references": [
            "Following the SynAPT benchmark  [ 38 ] , we evaluate using the following six datasets: UCF101, HMDB51, MiniSSV2, Diving48, IkeaFA, and UAV-H.  Tab.   11  presents the results. Using only synthetic images, our proposed framework partially surpasses some of the results of existing works utilizing real videos and action labels. Our framework is inferior to PPMA on UCF101, HMDB51, and IkeaFA. This is because these datasets have less data than others. PPMA leverages the 150 action labels in the video datasets for pre-training, therefore having the advantage of learning action features from a small number of videos during fine-tuning. On the other hand, our framework, not having these labels beforehand, struggled to learn meaningful features with fewer labeled data. However, our framework shows better performance on less biased datasets like MiniSSV2, Diving48, and UAV-H. This suggests that scene and object biases are mitigated when using our generated synthetic videos."
        ]
    },
    "id_table_12": {
        "caption": "Table 12 :    Results on K400.     Results from   [ 64 ] . We use 100k images from Shaders1k.",
        "table": "S4.T11.3.1.2.1",
        "footnotes": [
            ""
        ],
        "references": []
    },
    "id_table_13": {
        "caption": "Table 13 :  Comparison of accuracy on HMDB51 and UCF101 when using subsets grouped by frame difference.",
        "table": "S4.T11.3.4.3.2.1",
        "footnotes": [],
        "references": [
            "Tab.   13  shows the comparison of our framework with VideoMAE on K400. Although our framework outperforms the model from scratch, it falls short of the performance of VideoMAE with real videos. This shortfall is attributed to the limited diversity of pseudo-motion videos generated by PMG, especially when compared to the vast variety found in large-scale datasets. We understand our shortcoming here, but increasing the diversity of generated videos may close this gap.",
            "Finally, to support our hypothesis that VideoMAE learns the correspondence of patches between frames, we conducted a simple experiment. Here, we assume that a larger frame difference in a video makes it difficult to capture this correspondence, for instance, due to extreme camera motion. Based on this, we made three subsets from HMDB51 and UCF101 depending on the frame difference; (i) videos having the top 50% average frame difference (ii) videos ranging from the 25th to the 75th percentile in average frame difference, (iii) videos having the bottom 50% average frame difference. We then use each of these subsets for pre-training, then fine-tune on the full set. The results are shown in  Tab.   13 . Models that are pre-trained on (i) and (iii) performed worse than those pre-trained on (ii). This lends support to our hypothesis regarding what VideoMAE learns."
        ]
    },
    "id_table_14": {
        "caption": "Table 14 :    Pre-training setting for each dataset.",
        "table": "S4.T13.4.4",
        "footnotes": [],
        "references": [
            "We conducted the experiments with 8 A100 GPUs for both pre-training and fine-tuning, mostly following the settings in VideoMAE  [ 64 ] . The settings for pre-training are detailed in  Tab.   14  and those for fine-tuning are described in  Tab.   15 . We used PyTorch  [ 52 ]  to implement our framework."
        ]
    },
    "id_table_15": {
        "caption": "Table 15 :    Fine-tuning setting for each dataset.",
        "table": "S4.T13.fig1.5",
        "footnotes": [],
        "references": [
            "We conducted the experiments with 8 A100 GPUs for both pre-training and fine-tuning, mostly following the settings in VideoMAE  [ 64 ] . The settings for pre-training are detailed in  Tab.   14  and those for fine-tuning are described in  Tab.   15 . We used PyTorch  [ 52 ]  to implement our framework."
        ]
    },
    "id_table_16": {
        "caption": "Table 16 :    Results on SynAPT benchmark in the linear probing setting.     Results reported in  [ 38 ] .",
        "table": "Pt0.A1.T14.1",
        "footnotes": [
            ""
        ],
        "references": [
            "Another limitation of our framework is that our framework does not learn high-level semantic features, because our framework focuses on low-level features and does not utilize labels during pre-training, unlike PPMA  [ 84 ] . This limitation leads to lower performance in the linear probing settings, where the weights of the encoder are frozen while only the linear layer is trained ( Tab.   16 ). Moreover, it is challenging to extend our framework to other tasks like video-text retrieval and video captioning, without additional training or extra labeled data. We will also tackle this issue in future work."
        ]
    },
    "id_table_17": {
        "caption": "",
        "table": "Pt0.A1.T15.1",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": []
    },
    "id_table_18": {
        "caption": "",
        "table": "Pt0.A8.T16.3",
        "footnotes": [
            ""
        ],
        "references": []
    },
    "id_table_19": {
        "caption": "",
        "table": "Pt0.A8.T16.3.1.2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_20": {
        "caption": "",
        "table": "Pt0.A8.T16.3.4.3.2.1",
        "footnotes": [],
        "references": []
    }
}