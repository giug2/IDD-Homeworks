{
    "id_table_1": {
        "caption": "Table 1:  A comparison of our  NESTful  dataset to notable Tool-learning benchmarks",
        "table": "S1.T1.1.1",
        "footnotes": [
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Unfortunately, while existing datasets used for evaluating API calling capabilities will test each of the three categories, the way in which they evaluate sequencing is incomplete. That is, existing evaluation benchmarks pose sequencing as the prediction of single or multiple isolated API calls, where the output of any particular API call within that sequence is considered irrelevant. In contrast, for many real-world tasks, a sequence of API calls may be nested, i.e., the output of some API calls may be used in the arguments to subsequent API calls. Figure  1  shows an example of a nested sequence of APIs, where the first API has to be executed first and its output is used as an argument for the next two API calls.",
            "In this paper, we present  NESTful , a benchmark specifically designed to evaluate the capabilities of the models on nested API calls.  NESTful  has over 300 human-annotated high quality examples that have been split into two categories, executable and non-executable API calls. The executable samples are curated manually by crawling Rapid-APIs whereas the non-executable samples are handpicked by human annotators from synthetically generated examples using a state-of-the-art LLM. Table  1  shows how NESTful  compares against existing function calling benchmarks. We also evaluate various standard models on  NESTful  and show that existing models struggle to perform well on the nested sequencing task, thus providing a useful avenue for the community to test advancements in API calling capabilities. As our main contribution, we provide the dataset in a public github repository 2 2 2 The dataset will be released soon, we are working on getting the required legal clearances , made available under a permissive, open-source license.",
            "As an example, Figure  1  showcases an executable API scenario. We consider it as a pass, when the model predicts the gold APIs (i.e.,  Get_Country_Details_By_Country_Name ,  Coronavirus_Smartable_GetStats , and  NewsAPISearchByKeyWord ) in the correct order, calls  Get_Country_Details_By_Country_Name  to obtain the  short_name  of a country, automatically passes it to the subsequent APIs (i.e.,  Coronavirus_Smartable_GetStats  and  NewsAPISearchByKeyWord ), and, finally, successfully executes both the APIs.",
            "Variable Assignments    As discussed in Section  3 , we add variable assignments for each API in the output to manage parallel function calls, which is very common in real life applications. An example of parallel nested function calling has been provided in Section  4.1 , where the first two APIs are the same ( WeatherAPI.com_Time_Zone_API ) creating parallel functions. However, for the third API ( CipherCircuit_Math_Assistant_CalculateAllArithmeticOperations ), it is crucial to distinguish the outputs of the first two APIs to obtain the correct result. This adds complexity to the dataset, as the models are not trained with this schema and must follow the instructions precisely. Our qualitative studies (discussed in Section  5.4 ) suggest the same as well.",
            "Implicit API calling    Implicit function calling refers to a scenario where the system must invoke a specific API, along with potentially other APIs, to fulfill a user query, even though the query does not explicitly mention the task that requires that particular API. Figure  1  illustrates an example of implicit function calling, where the user query only mentions task for two APIs   Coronavirus_Smartable_GetStats  and  NewsAPISearchByKeyWord . However, when the model analyzes the specifications of both APIs, it identifies that the query parameters  location for the first API and region for the second  require the country name to be provided in its short form (e.g., IN for India). To fulfill this requirement, the model implicitly calls the  Get_Country_Details_By_Country_Name  API, which converts the full country name into its short version. This benchmark includes multiple scenarios of implicit function calling, adding to the datasets complexity and making it more challenging for models to handle."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Evaluation Result on  NESTful  with different state-of-the-art LLMs. Models are sorted based on the size. Experiments are done in one-shot and three shots settings. Best performance is highlighted in  bold , while the second best is  underlined .  Partial Sequence Match  denotes the percentage of calling the correct API sequence (API names and arguments) while  Full Sequence Match  counts the percentage of times where the model gets the entire sequence of APIs correctly. Both the scores are reported in 0 to 1 range. We also report  API Execution Pass Rate  (reported in %) for executable APIs which measures whether all the predicted APIs by the model are executable in sequence or not.",
        "table": "S1.T1.1.1.1.1.3.1",
        "footnotes": [],
        "references": [
            "Table  2  presents a comparison of different baselines on the  NESTful  dataset with one-shot and three-shots settings. As anticipated, in most of our experiments, the models are doing better when they are provided with three shot in-context learning examples in the prompt instead of one-shot example."
        ]
    },
    "id_table_3": {
        "caption": "",
        "table": "S1.T1.1.1.1.1.4.1",
        "footnotes": [],
        "references": [
            "Across all models,  Partial Sequence Match  scores are consistently higher than and  Full Sequence Match  scores, which is expected, as the Full Sequence Match is more stricter metric than the Partial. We looked into the outputs generated by the models and have identified several common issues across them. None of the five baseline models have been trained with the robust data schema discussed in Section  3 . So, as expected, these models struggle with tasks such as assigning variables, utilizing output parameter details from the API specifications, and correctly passing variable names and corresponding output parameters to subsequent APIs, even when provided with in-context learning examples. Models like xLAM-1b-fc-r and Mixtral-8x7b-Instruct-v01 also struggles with hallucination, as it sometimes predict argument values that are not present in the user query or generates natural languange texts instead of APIs, and in some cases it keeps on generating wrong API sequence until it reaches to the max token. Also, in many cases they misse the variable assignments correctly.",
            "Variable Assignments    As discussed in Section  3 , we add variable assignments for each API in the output to manage parallel function calls, which is very common in real life applications. An example of parallel nested function calling has been provided in Section  4.1 , where the first two APIs are the same ( WeatherAPI.com_Time_Zone_API ) creating parallel functions. However, for the third API ( CipherCircuit_Math_Assistant_CalculateAllArithmeticOperations ), it is crucial to distinguish the outputs of the first two APIs to obtain the correct result. This adds complexity to the dataset, as the models are not trained with this schema and must follow the instructions precisely. Our qualitative studies (discussed in Section  5.4 ) suggest the same as well.",
            "In this work we introduced  NESTful , a new benchmark for evaluating the performance of LLMs on API function calling with nested sequences of function calls (see Sections  3  and  4 ). We showed that existing LLMs perform poorly on this dataset as compared to their performance on existing benchmarks and identified their several modes of failure (see Section  5 ). In addition, we outlined the many challenges this dataset poses to LLM function calling approaches (see Section  6 ). By making this dataset available publicly under a permissive open-source license, we aim to push the capabilities of API function calling in new directions and unlock solutions to more realistic, challenging tasks."
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "S1.T1.1.1.1.1.5.1",
        "footnotes": [],
        "references": [
            "Variable Assignments    As discussed in Section  3 , we add variable assignments for each API in the output to manage parallel function calls, which is very common in real life applications. An example of parallel nested function calling has been provided in Section  4.1 , where the first two APIs are the same ( WeatherAPI.com_Time_Zone_API ) creating parallel functions. However, for the third API ( CipherCircuit_Math_Assistant_CalculateAllArithmeticOperations ), it is crucial to distinguish the outputs of the first two APIs to obtain the correct result. This adds complexity to the dataset, as the models are not trained with this schema and must follow the instructions precisely. Our qualitative studies (discussed in Section  5.4 ) suggest the same as well.",
            "In this work we introduced  NESTful , a new benchmark for evaluating the performance of LLMs on API function calling with nested sequences of function calls (see Sections  3  and  4 ). We showed that existing LLMs perform poorly on this dataset as compared to their performance on existing benchmarks and identified their several modes of failure (see Section  5 ). In addition, we outlined the many challenges this dataset poses to LLM function calling approaches (see Section  6 ). By making this dataset available publicly under a permissive open-source license, we aim to push the capabilities of API function calling in new directions and unlock solutions to more realistic, challenging tasks."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "S1.T1.1.1.1.1.6.1",
        "footnotes": [],
        "references": [
            "Variable Assignments    As discussed in Section  3 , we add variable assignments for each API in the output to manage parallel function calls, which is very common in real life applications. An example of parallel nested function calling has been provided in Section  4.1 , where the first two APIs are the same ( WeatherAPI.com_Time_Zone_API ) creating parallel functions. However, for the third API ( CipherCircuit_Math_Assistant_CalculateAllArithmeticOperations ), it is crucial to distinguish the outputs of the first two APIs to obtain the correct result. This adds complexity to the dataset, as the models are not trained with this schema and must follow the instructions precisely. Our qualitative studies (discussed in Section  5.4 ) suggest the same as well.",
            "In this work we introduced  NESTful , a new benchmark for evaluating the performance of LLMs on API function calling with nested sequences of function calls (see Sections  3  and  4 ). We showed that existing LLMs perform poorly on this dataset as compared to their performance on existing benchmarks and identified their several modes of failure (see Section  5 ). In addition, we outlined the many challenges this dataset poses to LLM function calling approaches (see Section  6 ). By making this dataset available publicly under a permissive open-source license, we aim to push the capabilities of API function calling in new directions and unlock solutions to more realistic, challenging tasks."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "S1.T1.1.1.1.1.7.1",
        "footnotes": [],
        "references": [
            "In this work we introduced  NESTful , a new benchmark for evaluating the performance of LLMs on API function calling with nested sequences of function calls (see Sections  3  and  4 ). We showed that existing LLMs perform poorly on this dataset as compared to their performance on existing benchmarks and identified their several modes of failure (see Section  5 ). In addition, we outlined the many challenges this dataset poses to LLM function calling approaches (see Section  6 ). By making this dataset available publicly under a permissive open-source license, we aim to push the capabilities of API function calling in new directions and unlock solutions to more realistic, challenging tasks."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "S5.T2.1.1",
        "footnotes": [],
        "references": []
    }
}