{
    "id_table_1": {
        "caption": "Table 1:  Table of all network architectures based on the U-Net architecture explored in this paper.",
        "table": "S2.T1.1",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "K-Origins is a neural network layer designed to improve image-based network performances when learning colour, or intensities, is beneficial. Over 250 encoder-decoder convolutional networks are trained and tested on 16-bit synthetic data, demonstrating that K-Origins improves semantic segmentation accuracy in two scenarios: object detection with low signal-to-noise ratios, and segmenting multiple objects that are identical in shape but vary in colour. K-Origins generates output features from the input features,  X , by the equation  Y k = X  J  w k subscript Y k X  J subscript w k \\textbf{Y}_{k}=\\textbf{X}-\\textbf{J}\\cdot w_{k} Y start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = X - J  italic_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT  for each trainable parameter  w k subscript w k w_{k} italic_w start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , where  J  is a matrix of ones. Additionally, networks with varying receptive fields were trained to determine optimal network depths based on the dimensions of target classes, suggesting that receptive field lengths should exceed object sizes. By ensuring a sufficient receptive field length and incorporating K-Origins, we can achieve better semantic network performance. Examples of these improvements are illustrated in Figure  1 .",
            "To determine the RFL before a layer in the network ( r l  1 subscript r l 1 r_{l-1} italic_r start_POSTSUBSCRIPT italic_l - 1 end_POSTSUBSCRIPT ) given the RFL after that layer ( r l subscript r l r_{l} italic_r start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ), use the layers stride ( s l subscript s l s_{l} italic_s start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ) and kernel size ( k l subscript k l k_{l} italic_k start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ) in Equation  1 . For semantic segmentation, start at the deepest set of features with an RFL of one ( r l = e  n  d = 1  pixel subscript r l e n d 1 pixel r_{l=end}=1\\ \\mathrm{pixel} italic_r start_POSTSUBSCRIPT italic_l = italic_e italic_n italic_d end_POSTSUBSCRIPT = 1 roman_pixel ) and work backwards to determine the RFL at each feature layer.",
            "In Section  2.1 , we discuss the data generation process for all trials. The motivating case for this study is presented in Section  2.2 , demonstrating that a CNN can struggle with simple object detection. In Section  2.3  we introduce some additional background material that is relevant for quantifying results. In Section  2.4 , we introduce K-Origins, a layer designed to help neural networks quantify colours and intensity magnitudes. Section  2.5  demonstrates that the motivating case can either be solved by using K-Origins or by increasing the depth and complexity of the network. Finally, in Section  2.6 , we test the limits of segmentation across a range of colour distributions for two types of problems: object detection and tracer segmentation.",
            "In Figure  3 , the networks RFL is calculated by setting the bottom-right feature (the deepest point) to  r l = e  n  d = 1 subscript r l e n d 1 r_{l=end}=1 italic_r start_POSTSUBSCRIPT italic_l = italic_e italic_n italic_d end_POSTSUBSCRIPT = 1  pixel and recursively determining the RFL at previous layers. Using Equation  1 , we calculate that the motivating network has an RFL of 8 pixels, which is twice the distance that gets correctly classified from the object border, plus or minus one pixel. Being twice the correct prediction distance should be expected because the deepest features in that network can \"see\" about 4-5 pixels on either side of the pixel it wishes to classify. We hypothesize that this network classifies pixels by detecting a square edge in any direction; if no edge is detected within the RF, the pixel is classified as background.",
            "In Figure  3 , the network fails for larger objects. In this section, we investigate the required RFL for various object sizes. We use a set of small encoder-decoder networks, shown in Figure  5 , with additional details in Table  1 . The six architectures used are RFL8, RFL18, RFL38, KRFL8, KRFL18, and KRFL38, where \"KRFLX\" refers to an identical architecture to \"RFLX\" with K-Origins. All networks in Table  1  use \"same\" padding, where applicable, to prevent cropping. We hypothesize that the RFL should be larger than the dominant length scale, or the minimum length required to differentiate two objects.",
            "This comparison is done for two network architectures: RFL14 and KRFL14, shown in Figure  9  with additional parameters listed in Table  1 . The key difference is the inclusion of K-Origins in KRFL14.",
            "Figure  10  shows results for  L < R  L  F L R L F L<RLF italic_L < italic_R italic_L italic_F , and Figure  11  shows results for  L > R  F  L L R F L L>RFL italic_L > italic_R italic_F italic_L . In both figures, part (a) presents the heatmap with training results for each network. These accuracies can be compared to the HD found in Figure  8 . Part (b) shows a simple example with an HD of 0.694, and two extreme cases with HDs of 0.176. The first example case has a different distribution than those discussed earlier in this work.",
            "After adding K-Origins, the accuracy heatmap is almost directly correlated to the class HDs. This is evident by comparing Figure  8  to the KRFL14 accuracy plots in Figure  10  and Figure  11 . As the HD decreases, so does the accuracy, and vice versa. This correlation is not observed in the traditional network without K-Origins.",
            "The background has   0 = 16500 subscript  0 16500 \\mu_{0}=16500 italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 16500  and noise with a standard deviation   0 = 900 subscript  0 900 \\sigma_{0}=900 italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 900  to minimize interference with the target classes. The first target class has   1 = 20000 subscript  1 20000 \\mu_{1}=20000 italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 20000  and   1 = 1000 subscript  1 1000 \\sigma_{1}=1000 italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 1000  , while the second target class varies based on     =  2   1   subscript  2 subscript  1 \\Delta\\mu=\\mu_{2}-\\mu_{1} roman_ italic_ = italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and     =  2   1   subscript  2 subscript  1 \\Delta\\sigma=\\sigma_{2}-\\sigma_{1} roman_ italic_ = italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . Results for  L < R  F  L L R F L L<RFL italic_L < italic_R italic_F italic_L  are shown in Figure  12 , and results for  L > R  F  L L R F L L>RFL italic_L > italic_R italic_F italic_L  are shown in Figure  13 . In part (a), we present the heatmaps showing validation accuracy results for RFL14 and KRFL14, which can be compared to a trials HD using Figure  8 . Part (b) provides a straightforward example followed by the two most challenging cases tested.",
            "KRFL14 consistently outperforms RFL14 in this task, especially when the standard deviation remains constant while       \\Delta\\mu roman_ italic_  varies. Accuracy increases with  L > R  F  L L R F L L>RFL italic_L > italic_R italic_F italic_L  are for the same reason as mentioned before. This shows extremely promising results, with useful segmentation even at an HD of 0.176. As in Section  2.6.1 , the accuracy plots for the network using K-Origins correlates well with the HD plot.",
            "The networks used for Figure  1  are RFL32 and KRFL32, deeper versions of RFL14 and KRFL14. The additional level is added the same way RFL8 is extended to RFL18. This depth satisfies RFL requirements, and still demonstrates that adding K-Origins is beneficial."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Figure  7 a data.",
        "table": "A3.T2.1",
        "footnotes": [
            ""
        ],
        "references": [
            "In Section  2.1 , we discuss the data generation process for all trials. The motivating case for this study is presented in Section  2.2 , demonstrating that a CNN can struggle with simple object detection. In Section  2.3  we introduce some additional background material that is relevant for quantifying results. In Section  2.4 , we introduce K-Origins, a layer designed to help neural networks quantify colours and intensity magnitudes. Section  2.5  demonstrates that the motivating case can either be solved by using K-Origins or by increasing the depth and complexity of the network. Finally, in Section  2.6 , we test the limits of segmentation across a range of colour distributions for two types of problems: object detection and tracer segmentation.",
            "For greyscale data, a pixels colour is represented by a single integer value. For 16-bit data, as used in this paper, the values range from 0 (pure black) to 65535 (pure white), with various shades of grey in between. In this work, a classs colour is represented by its intensity mean (  i subscript  i \\mu_{i} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) and the standard deviation of added Gaussian noise (  i subscript  i \\sigma_{i} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ). Figure  2  shows the integer-intensity mapping and provides examples of the synthetic data used in this paper. Data intensity distributions are illustrated using normalized histograms ( d  a  t  a = d  a  t  a / m  a  x  ( d  a  t  a ) d a t a d a t a m a x d a t a data=data/max(data) italic_d italic_a italic_t italic_a = italic_d italic_a italic_t italic_a / italic_m italic_a italic_x ( italic_d italic_a italic_t italic_a ) ).",
            "Networks are trained for 10 epochs with a batch size of 3. Learning rates are set to 1E-4 for convolution layers and 100 for K-Origins layers. K-Origins initialization follows the same method as in Section  2.5  and this time KRFL14 has fewer parameters than RFL14. There are 50 randomly placed squares for  L < R  F  L L R F L L<RFL italic_L < italic_R italic_F italic_L  and 25 for  L > R  F  L L R F L L>RFL italic_L > italic_R italic_F italic_L . All numerical results are found in Appendix  C .",
            "The background has   0 = 16500 subscript  0 16500 \\mu_{0}=16500 italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 16500  and noise with a standard deviation   0 = 900 subscript  0 900 \\sigma_{0}=900 italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 900  to minimize interference with the target classes. The first target class has   1 = 20000 subscript  1 20000 \\mu_{1}=20000 italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 20000  and   1 = 1000 subscript  1 1000 \\sigma_{1}=1000 italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 1000  , while the second target class varies based on     =  2   1   subscript  2 subscript  1 \\Delta\\mu=\\mu_{2}-\\mu_{1} roman_ italic_ = italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and     =  2   1   subscript  2 subscript  1 \\Delta\\sigma=\\sigma_{2}-\\sigma_{1} roman_ italic_ = italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . Results for  L < R  F  L L R F L L<RFL italic_L < italic_R italic_F italic_L  are shown in Figure  12 , and results for  L > R  F  L L R F L L>RFL italic_L > italic_R italic_F italic_L  are shown in Figure  13 . In part (a), we present the heatmaps showing validation accuracy results for RFL14 and KRFL14, which can be compared to a trials HD using Figure  8 . Part (b) provides a straightforward example followed by the two most challenging cases tested.",
            "KRFL14 consistently outperforms RFL14 in this task, especially when the standard deviation remains constant while       \\Delta\\mu roman_ italic_  varies. Accuracy increases with  L > R  F  L L R F L L>RFL italic_L > italic_R italic_F italic_L  are for the same reason as mentioned before. This shows extremely promising results, with useful segmentation even at an HD of 0.176. As in Section  2.6.1 , the accuracy plots for the network using K-Origins correlates well with the HD plot."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Figure  7 b data.",
        "table": "A3.T3.1",
        "footnotes": [
            ""
        ],
        "references": [
            "In Section  2.1 , we discuss the data generation process for all trials. The motivating case for this study is presented in Section  2.2 , demonstrating that a CNN can struggle with simple object detection. In Section  2.3  we introduce some additional background material that is relevant for quantifying results. In Section  2.4 , we introduce K-Origins, a layer designed to help neural networks quantify colours and intensity magnitudes. Section  2.5  demonstrates that the motivating case can either be solved by using K-Origins or by increasing the depth and complexity of the network. Finally, in Section  2.6 , we test the limits of segmentation across a range of colour distributions for two types of problems: object detection and tracer segmentation.",
            "The motivation for K-Origins and this work is shown in Figure  3 , where a small encoder-decoder network fails to classify noiseless squares from the background. The network lacks an understanding of colour magnitude; if it could recognize the lighter gray squares against the darker gray background, the task would be simple. However, the network does not directly leverage the 16-bit valuesthe greynessof the squares in its predictions. For example, a straightforward solution to this problem is to compare a pixels integer value to 25000 (the squares colour) and classify it as a square if it matches, or as background if it does not. Despite having over 70,000 trainable parameters, the network fails to learn this behavior.",
            "In Figure  3 , the networks RFL is calculated by setting the bottom-right feature (the deepest point) to  r l = e  n  d = 1 subscript r l e n d 1 r_{l=end}=1 italic_r start_POSTSUBSCRIPT italic_l = italic_e italic_n italic_d end_POSTSUBSCRIPT = 1  pixel and recursively determining the RFL at previous layers. Using Equation  1 , we calculate that the motivating network has an RFL of 8 pixels, which is twice the distance that gets correctly classified from the object border, plus or minus one pixel. Being twice the correct prediction distance should be expected because the deepest features in that network can \"see\" about 4-5 pixels on either side of the pixel it wishes to classify. We hypothesize that this network classifies pixels by detecting a square edge in any direction; if no edge is detected within the RF, the pixel is classified as background.",
            "Figure  3  shows that the network struggles to classify pixels far from the object border and that it also fails to understand intensity magnitudes. We will address both of these issues separately and will use greyscale data because the single channel results extend to additional colour channels (RGB).",
            "The weight  w 1 subscript w 1 w_{1} italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  was initialized at 50000 with a learning rate of 100 and ended at  w 1 , f  i  n  a  l = 20200 subscript w 1 f i n a l 20200 w_{1,final}=20200 italic_w start_POSTSUBSCRIPT 1 , italic_f italic_i italic_n italic_a italic_l end_POSTSUBSCRIPT = 20200  after 33 epochs. This final value lies between the intensity values of the two classes,   0 = 20000 subscript  0 20000 \\mu_{0}=20000 italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 20000  and   1 = 25000 subscript  1 25000 \\mu_{1}=25000 italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 25000 . This small network with only 5 trainable parameters achieved 100% accuracy segmenting the case from Figure  3 , whereas the encoder-decoder network with 71,042 trainable parameters achieved only a 67% accuracy. This small network was also tested with more weights on a 7-class case and achieved 100% accuracy. However, accuracy decreased when the class intensity distributions had an HD less than unity, suggesting that a combination of K-Origins and shape recognition would perform better.",
            "In Figure  3 , the network fails for larger objects. In this section, we investigate the required RFL for various object sizes. We use a set of small encoder-decoder networks, shown in Figure  5 , with additional details in Table  1 . The six architectures used are RFL8, RFL18, RFL38, KRFL8, KRFL18, and KRFL38, where \"KRFLX\" refers to an identical architecture to \"RFLX\" with K-Origins. All networks in Table  1  use \"same\" padding, where applicable, to prevent cropping. We hypothesize that the RFL should be larger than the dominant length scale, or the minimum length required to differentiate two objects.",
            "We first train the six networks on noiseless data (  0 = 20000 ,  1 = 25000 ,  0 =  1 = 0 formulae-sequence subscript  0 20000 formulae-sequence subscript  1 25000 subscript  0 subscript  1 0 \\mu_{0}=20000,\\ \\mu_{1}=25000,\\ \\sigma_{0}=\\sigma_{1}=0 italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 20000 , italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 25000 , italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0 ) containing squares with a side length of 25 pixels, similar to the scenarios in Figures  3  and  4 . We also train on noisy data (  0 = 20000 ,  1 = 25000 ,  0 =  1 = 2000 formulae-sequence subscript  0 20000 formulae-sequence subscript  1 25000 subscript  0 subscript  1 2000 \\mu_{0}=20000,\\ \\mu_{1}=25000,\\ \\sigma_{0}=\\sigma_{1}=2000 italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 20000 , italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 25000 , italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 2000 ) to simulate the failure case in Figure  4 . This shows us the effect of increasing the RFL for a fixed object size.",
            "So far we have demonstrated a solution to the motivational problem (Figure  3 ) using both network length scales (ensuring sufficient RFL) and intensity quantification (K-Origins) for a noisy and noiseless case. In the noiseless case we set     = 5000   5000 \\Delta\\mu=5000 roman_ italic_ = 5000  and   = 0  0 \\sigma=0 italic_ = 0 , giving a unity HD. In the noisy case we set       \\Delta\\mu roman_ italic_  is the same, but   = 2000  2000 \\sigma=2000 italic_ = 2000  resulting in an HD of 0.73. The next logical step is to sweep across various HDs by adjusting       \\Delta\\mu roman_ italic_  and       \\Delta\\sigma roman_ italic_  to determine the effectiveness of K-Origins for different intensity distributions.",
            "The background has   0 = 16500 subscript  0 16500 \\mu_{0}=16500 italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 16500  and noise with a standard deviation   0 = 900 subscript  0 900 \\sigma_{0}=900 italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 900  to minimize interference with the target classes. The first target class has   1 = 20000 subscript  1 20000 \\mu_{1}=20000 italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 20000  and   1 = 1000 subscript  1 1000 \\sigma_{1}=1000 italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 1000  , while the second target class varies based on     =  2   1   subscript  2 subscript  1 \\Delta\\mu=\\mu_{2}-\\mu_{1} roman_ italic_ = italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and     =  2   1   subscript  2 subscript  1 \\Delta\\sigma=\\sigma_{2}-\\sigma_{1} roman_ italic_ = italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . Results for  L < R  F  L L R F L L<RFL italic_L < italic_R italic_F italic_L  are shown in Figure  12 , and results for  L > R  F  L L R F L L>RFL italic_L > italic_R italic_F italic_L  are shown in Figure  13 . In part (a), we present the heatmaps showing validation accuracy results for RFL14 and KRFL14, which can be compared to a trials HD using Figure  8 . Part (b) provides a straightforward example followed by the two most challenging cases tested."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Figure  10 a RFL14 data (One class L<RFL).",
        "table": "A3.T4.2",
        "footnotes": [
            ""
        ],
        "references": [
            "In Section  2.1 , we discuss the data generation process for all trials. The motivating case for this study is presented in Section  2.2 , demonstrating that a CNN can struggle with simple object detection. In Section  2.3  we introduce some additional background material that is relevant for quantifying results. In Section  2.4 , we introduce K-Origins, a layer designed to help neural networks quantify colours and intensity magnitudes. Section  2.5  demonstrates that the motivating case can either be solved by using K-Origins or by increasing the depth and complexity of the network. Finally, in Section  2.6 , we test the limits of segmentation across a range of colour distributions for two types of problems: object detection and tracer segmentation.",
            "Figure  4  shows a small network that takes an input image and concatenates it with the output of a K-Origins layer with one weight,  w 1 subscript w 1 w_{1} italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . Concatenating the output of K-Origins with the input provides stable reference features for the rest of the network, which is essential for convergence as  Y k subscript Y k \\textbf{Y}_{k} Y start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT  constantly changes. The network then applies a softmax-activated 1x1 convolution with a learning rate of 1E-3 for pixel-wise predictions. Because this network has an RFL of one pixel, it can only use information from a single pixel for its predictions, extracting no spatial information.",
            "Because supervised learning problems have ground truths, K-Origins weights can be initialized based on known class distributions with learning rates of zero, or near zero. For example, in the first problem of Figure  4 b, initializing the K-Origins weight as  w 1 = 22500 subscript w 1 22500 w_{1}=22500 italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 22500 , right between both classes, achieves 100% accuracy in just one epoch. This technique is used later in this article by \"clamping\" distributions, where a weight is placed above and below the known distribution of a target class to clamp those intensities.",
            "We first train the six networks on noiseless data (  0 = 20000 ,  1 = 25000 ,  0 =  1 = 0 formulae-sequence subscript  0 20000 formulae-sequence subscript  1 25000 subscript  0 subscript  1 0 \\mu_{0}=20000,\\ \\mu_{1}=25000,\\ \\sigma_{0}=\\sigma_{1}=0 italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 20000 , italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 25000 , italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0 ) containing squares with a side length of 25 pixels, similar to the scenarios in Figures  3  and  4 . We also train on noisy data (  0 = 20000 ,  1 = 25000 ,  0 =  1 = 2000 formulae-sequence subscript  0 20000 formulae-sequence subscript  1 25000 subscript  0 subscript  1 2000 \\mu_{0}=20000,\\ \\mu_{1}=25000,\\ \\sigma_{0}=\\sigma_{1}=2000 italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 20000 , italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 25000 , italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 2000 ) to simulate the failure case in Figure  4 . This shows us the effect of increasing the RFL for a fixed object size.",
            "Additionally, it is unclear if K-Origin layers after the first impact classification results significantly. These weights likely need to be much smaller than those used in this paper and this could be explored in future studies. There is also the possibility to extend the application of K-Origins to un-supervised problems, perhaps by using a modified version of the simple colour network in Figure  4 ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Figure  10 a KRFL14 data (One class L<RFL).",
        "table": "A3.T5.2",
        "footnotes": [
            ""
        ],
        "references": [
            "In Section  2.1 , we discuss the data generation process for all trials. The motivating case for this study is presented in Section  2.2 , demonstrating that a CNN can struggle with simple object detection. In Section  2.3  we introduce some additional background material that is relevant for quantifying results. In Section  2.4 , we introduce K-Origins, a layer designed to help neural networks quantify colours and intensity magnitudes. Section  2.5  demonstrates that the motivating case can either be solved by using K-Origins or by increasing the depth and complexity of the network. Finally, in Section  2.6 , we test the limits of segmentation across a range of colour distributions for two types of problems: object detection and tracer segmentation.",
            "In Figure  3 , the network fails for larger objects. In this section, we investigate the required RFL for various object sizes. We use a set of small encoder-decoder networks, shown in Figure  5 , with additional details in Table  1 . The six architectures used are RFL8, RFL18, RFL38, KRFL8, KRFL18, and KRFL38, where \"KRFLX\" refers to an identical architecture to \"RFLX\" with K-Origins. All networks in Table  1  use \"same\" padding, where applicable, to prevent cropping. We hypothesize that the RFL should be larger than the dominant length scale, or the minimum length required to differentiate two objects.",
            "Networks are trained for 10 epochs with a batch size of 3. Learning rates are set to 1E-4 for convolution layers and 100 for K-Origins layers. K-Origins initialization follows the same method as in Section  2.5  and this time KRFL14 has fewer parameters than RFL14. There are 50 randomly placed squares for  L < R  F  L L R F L L<RFL italic_L < italic_R italic_F italic_L  and 25 for  L > R  F  L L R F L L>RFL italic_L > italic_R italic_F italic_L . All numerical results are found in Appendix  C ."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Figure  11 a RFL14 data (One class L>RFL).",
        "table": "A3.T6.2",
        "footnotes": [
            ""
        ],
        "references": [
            "In Section  2.1 , we discuss the data generation process for all trials. The motivating case for this study is presented in Section  2.2 , demonstrating that a CNN can struggle with simple object detection. In Section  2.3  we introduce some additional background material that is relevant for quantifying results. In Section  2.4 , we introduce K-Origins, a layer designed to help neural networks quantify colours and intensity magnitudes. Section  2.5  demonstrates that the motivating case can either be solved by using K-Origins or by increasing the depth and complexity of the network. Finally, in Section  2.6 , we test the limits of segmentation across a range of colour distributions for two types of problems: object detection and tracer segmentation.",
            "The results from these trials are shown in Figure  6 . Networks without K-Origins increase in accuracy as the RFL approaches the object length, achieving high accuracies when the RFL exceeds the object length. In contrast, networks with K-Origins achieve near-perfect validation accuracy regardless of their RFL, demonstrating a more efficient solution. Achieving a near-perfect accuracy without K-Origins requires about 1.4 million trainable parameters, while using K-Origins achieves the same accuracy with only 187,000 trainable parameters. Additionally, an even smaller network with K-Origins could be possible, as this test did not determine a network size lower bound.",
            "KRFL14 consistently outperforms RFL14 in this task, especially when the standard deviation remains constant while       \\Delta\\mu roman_ italic_  varies. Accuracy increases with  L > R  F  L L R F L L>RFL italic_L > italic_R italic_F italic_L  are for the same reason as mentioned before. This shows extremely promising results, with useful segmentation even at an HD of 0.176. As in Section  2.6.1 , the accuracy plots for the network using K-Origins correlates well with the HD plot."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Figure  11 a KRFL14 data (One class L>RFL).",
        "table": "A3.T7.2",
        "footnotes": [
            ""
        ],
        "references": [
            "Next, we perform a sweep of square side lengths to RFL ratios,  L / R  F  L L R F L L/RFL italic_L / italic_R italic_F italic_L , for the six networks using the same training parameters as before. This is done with and without noise. For each RFL, we examine  L / R  F  L  { 0.3 , 0.6 , 0.95 , 1.3 , 2 , 3 } L R F L 0.3 0.6 0.95 1.3 2 3 L/RFL\\approx\\{0.3,0.6,0.95,1.3,2,3\\} italic_L / italic_R italic_F italic_L  { 0.3 , 0.6 , 0.95 , 1.3 , 2 , 3 } . These fractions are approximated since side lengths may be rounded. The summary of these tests is shown in Figure  7  and in almost every case, using K-Origins increases accuracy. We also observe that accuracy decreases when  L / R  F  L L R F L L/RFL italic_L / italic_R italic_F italic_L  is small. This is because a small  L / R  F  L L R F L L/RFL italic_L / italic_R italic_F italic_L  results in very small squares, making segmentation difficult in noisy conditions regardless of the architecture used. All numerical results are found in Appendix  C ."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Figure  12 a RFL14 data (Two class L<RFL).",
        "table": "A3.T8.2",
        "footnotes": [
            ""
        ],
        "references": [
            "In this section we explore how changing the HD affects segmentation by varying       \\Delta\\mu roman_ italic_  and       \\Delta\\sigma roman_ italic_ . Setting the first class to   = 20000  20000 \\mu=20000 italic_ = 20000  and   = 1000  1000 \\sigma=1000 italic_ = 1000 , we produce the HD heatmap shown in Figure  8 . This heatmap will be used to determine HDs for the upcoming trials, allowing us to compare the accuracy of each trial with the HD.",
            "Figure  10  shows results for  L < R  L  F L R L F L<RLF italic_L < italic_R italic_L italic_F , and Figure  11  shows results for  L > R  F  L L R F L L>RFL italic_L > italic_R italic_F italic_L . In both figures, part (a) presents the heatmap with training results for each network. These accuracies can be compared to the HD found in Figure  8 . Part (b) shows a simple example with an HD of 0.694, and two extreme cases with HDs of 0.176. The first example case has a different distribution than those discussed earlier in this work.",
            "After adding K-Origins, the accuracy heatmap is almost directly correlated to the class HDs. This is evident by comparing Figure  8  to the KRFL14 accuracy plots in Figure  10  and Figure  11 . As the HD decreases, so does the accuracy, and vice versa. This correlation is not observed in the traditional network without K-Origins.",
            "The background has   0 = 16500 subscript  0 16500 \\mu_{0}=16500 italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 16500  and noise with a standard deviation   0 = 900 subscript  0 900 \\sigma_{0}=900 italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 900  to minimize interference with the target classes. The first target class has   1 = 20000 subscript  1 20000 \\mu_{1}=20000 italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 20000  and   1 = 1000 subscript  1 1000 \\sigma_{1}=1000 italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 1000  , while the second target class varies based on     =  2   1   subscript  2 subscript  1 \\Delta\\mu=\\mu_{2}-\\mu_{1} roman_ italic_ = italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and     =  2   1   subscript  2 subscript  1 \\Delta\\sigma=\\sigma_{2}-\\sigma_{1} roman_ italic_ = italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . Results for  L < R  F  L L R F L L<RFL italic_L < italic_R italic_F italic_L  are shown in Figure  12 , and results for  L > R  F  L L R F L L>RFL italic_L > italic_R italic_F italic_L  are shown in Figure  13 . In part (a), we present the heatmaps showing validation accuracy results for RFL14 and KRFL14, which can be compared to a trials HD using Figure  8 . Part (b) provides a straightforward example followed by the two most challenging cases tested."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Figure  12 a KRFL14 data (Two class L<RFL).",
        "table": "A3.T9.2",
        "footnotes": [
            ""
        ],
        "references": [
            "This comparison is done for two network architectures: RFL14 and KRFL14, shown in Figure  9  with additional parameters listed in Table  1 . The key difference is the inclusion of K-Origins in KRFL14."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:  Figure  13 a RFL14 data (Two class L>RFL).",
        "table": "A3.T10.2",
        "footnotes": [
            ""
        ],
        "references": [
            "Figure  10  shows results for  L < R  L  F L R L F L<RLF italic_L < italic_R italic_L italic_F , and Figure  11  shows results for  L > R  F  L L R F L L>RFL italic_L > italic_R italic_F italic_L . In both figures, part (a) presents the heatmap with training results for each network. These accuracies can be compared to the HD found in Figure  8 . Part (b) shows a simple example with an HD of 0.694, and two extreme cases with HDs of 0.176. The first example case has a different distribution than those discussed earlier in this work.",
            "After adding K-Origins, the accuracy heatmap is almost directly correlated to the class HDs. This is evident by comparing Figure  8  to the KRFL14 accuracy plots in Figure  10  and Figure  11 . As the HD decreases, so does the accuracy, and vice versa. This correlation is not observed in the traditional network without K-Origins."
        ]
    },
    "id_table_11": {
        "caption": "Table 11:  Figure  13 a KRFL14 data (Two class L>RFL).",
        "table": "A3.T11.2",
        "footnotes": [
            ""
        ],
        "references": [
            "Figure  10  shows results for  L < R  L  F L R L F L<RLF italic_L < italic_R italic_L italic_F , and Figure  11  shows results for  L > R  F  L L R F L L>RFL italic_L > italic_R italic_F italic_L . In both figures, part (a) presents the heatmap with training results for each network. These accuracies can be compared to the HD found in Figure  8 . Part (b) shows a simple example with an HD of 0.694, and two extreme cases with HDs of 0.176. The first example case has a different distribution than those discussed earlier in this work.",
            "After adding K-Origins, the accuracy heatmap is almost directly correlated to the class HDs. This is evident by comparing Figure  8  to the KRFL14 accuracy plots in Figure  10  and Figure  11 . As the HD decreases, so does the accuracy, and vice versa. This correlation is not observed in the traditional network without K-Origins."
        ]
    }
}