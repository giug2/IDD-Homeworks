{
    "PAPER'S NUMBER OF TABLES": 4,
    "S3.T1": {
        "caption": "Table 1: Performance of different parameter aggregation schemes with OptAlgoB for optimisation. Average values show the stability of the FL process near convergence.",
        "table": "<table id=\"S3.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_t\"></th>\n<td id=\"S3.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">Best Dice</td>\n<td id=\"S3.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">Avg Dice</td>\n<td id=\"S3.T1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">Best loss</td>\n<td id=\"S3.T1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">Avg loss</td>\n</tr>\n<tr id=\"S3.T1.1.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">FedAvg</th>\n<td id=\"S3.T1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.7771</td>\n<td id=\"S3.T1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.7417</td>\n<td id=\"S3.T1.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.3008</td>\n<td id=\"S3.T1.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.3522</td>\n</tr>\n<tr id=\"S3.T1.1.3.3\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">CostWAgg</th>\n<td id=\"S3.T1.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.7738</td>\n<td id=\"S3.T1.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.7578</td>\n<td id=\"S3.T1.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.2984</td>\n<td id=\"S3.T1.1.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.3157</td>\n</tr>\n<tr id=\"S3.T1.1.4.4\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">RoundCWAgg</th>\n<td id=\"S3.T1.1.4.4.2\" class=\"ltx_td ltx_align_center\">0.7718</td>\n<td id=\"S3.T1.1.4.4.3\" class=\"ltx_td ltx_align_center\">0.7022</td>\n<td id=\"S3.T1.1.4.4.4\" class=\"ltx_td ltx_align_center\">0.3074</td>\n<td id=\"S3.T1.1.4.4.5\" class=\"ltx_td ltx_align_center\">0.4095</td>\n</tr>\n<tr id=\"S3.T1.1.5.5\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">RegAgg</th>\n<td id=\"S3.T1.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.7783</td>\n<td id=\"S3.T1.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.7310</td>\n<td id=\"S3.T1.1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.2920</td>\n<td id=\"S3.T1.1.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.3608</td>\n</tr>\n<tr id=\"S3.T1.1.6.6\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">RegMedAgg</th>\n<td id=\"S3.T1.1.6.6.2\" class=\"ltx_td ltx_align_center\">0.7569</td>\n<td id=\"S3.T1.1.6.6.3\" class=\"ltx_td ltx_align_center\">0.7448</td>\n<td id=\"S3.T1.1.6.6.4\" class=\"ltx_td ltx_align_center\">0.3254</td>\n<td id=\"S3.T1.1.6.6.5\" class=\"ltx_td ltx_align_center\">0.3478</td>\n</tr>\n<tr id=\"S3.T1.1.7.7\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.7.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\">RegCostAgg</th>\n<td id=\"S3.T1.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">0.7848</td>\n<td id=\"S3.T1.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">0.7595</td>\n<td id=\"S3.T1.1.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">0.2900</td>\n<td id=\"S3.T1.1.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">0.3272</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We run 13 rounds of FL training and use OptAlgoB while comparing these different parameter aggregation scheme. We specifically monitor four metrics - best validation Dice across all labels till round 13, best validation loss till round 13 as well as average validation DICE and validation loss for last five rounds. The results are summarised in Table 1. While in terms of Best Dice (or loss) values many methods are comparable, the Avg Dice (or loss) tells a more relevant story as it sheds light on the stability of the overall optimisation protocol. Mindful of the fact that these numbers weren’t obtained over multiple runs, we tentatively conclude that the parameter aggregation schemes provide marginal benefits over vanilla FedAvg (with generally smaller avg loss and higher avg Dice)."
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Performance of different aggregation schemes with OptAlgoB in the fine-tuning phase of the two-phase learning protocol of RoLePro",
        "table": "<table id=\"S3.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_t\"></th>\n<th id=\"S3.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Best Dice</th>\n<th id=\"S3.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Avg. Dice</th>\n<th id=\"S3.T2.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Best loss</th>\n<th id=\"S3.T2.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Avg. loss</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T2.1.2.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">RegAgg</th>\n<td id=\"S3.T2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.8067</td>\n<td id=\"S3.T2.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.7921</td>\n<td id=\"S3.T2.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.2639</td>\n<td id=\"S3.T2.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.2821</td>\n</tr>\n<tr id=\"S3.T2.1.3.2\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">CostWAgg</th>\n<td id=\"S3.T2.1.3.2.2\" class=\"ltx_td ltx_align_center\">0.7788</td>\n<td id=\"S3.T2.1.3.2.3\" class=\"ltx_td ltx_align_center\">0.7681</td>\n<td id=\"S3.T2.1.3.2.4\" class=\"ltx_td ltx_align_center\">0.3107</td>\n<td id=\"S3.T2.1.3.2.5\" class=\"ltx_td ltx_align_center\">0.3207</td>\n</tr>\n<tr id=\"S3.T2.1.4.3\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">TrimmedMean</th>\n<td id=\"S3.T2.1.4.3.2\" class=\"ltx_td ltx_align_center\">0.7928</td>\n<td id=\"S3.T2.1.4.3.3\" class=\"ltx_td ltx_align_center\">0.7834</td>\n<td id=\"S3.T2.1.4.3.4\" class=\"ltx_td ltx_align_center\">0.2764</td>\n<td id=\"S3.T2.1.4.3.5\" class=\"ltx_td ltx_align_center\">0.2958</td>\n</tr>\n<tr id=\"S3.T2.1.5.4\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\">TopKRegCost</th>\n<td id=\"S3.T2.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_b\">0.7965</td>\n<td id=\"S3.T2.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_b\">0.7806</td>\n<td id=\"S3.T2.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_b\">0.2718</td>\n<td id=\"S3.T2.1.5.4.5\" class=\"ltx_td ltx_align_center ltx_border_b\">0.2957</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "In Table 2 and 3 we present the results for RoLePro with OptAlgoB and OptAlgoC across different aggregation schemes. We observe that both RegAgg and TopKRegCost lead to high-performing models across both OptAlgoB and OptAlgoC. It should be mentioned that apart from aggregation schemes, the optimisation protocols of OptAlgoB and OptAlgoC also enable parameter specific learning rates, therefore it can be challenging to pin-point the source of the observed gains within the complex orchestration of an FL plan."
        ]
    },
    "S3.T3": {
        "caption": "Table 3: Performance of different aggregation schemes with OptAlgoC in the fine-tuning phase of the two-phase learning protocol of RoLePro",
        "table": "<table id=\"S3.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_t\"></th>\n<th id=\"S3.T3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Best Dice</th>\n<th id=\"S3.T3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Avg. Dice</th>\n<th id=\"S3.T3.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Best loss</th>\n<th id=\"S3.T3.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Avg. loss</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T3.1.2.1\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">RegAgg</th>\n<td id=\"S3.T3.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.7958</td>\n<td id=\"S3.T3.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.7851</td>\n<td id=\"S3.T3.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.2709</td>\n<td id=\"S3.T3.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.2905</td>\n</tr>\n<tr id=\"S3.T3.1.3.2\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">CostWAgg</th>\n<td id=\"S3.T3.1.3.2.2\" class=\"ltx_td ltx_align_center\">0.7888</td>\n<td id=\"S3.T3.1.3.2.3\" class=\"ltx_td ltx_align_center\">0.7712</td>\n<td id=\"S3.T3.1.3.2.4\" class=\"ltx_td ltx_align_center\">0.2880</td>\n<td id=\"S3.T3.1.3.2.5\" class=\"ltx_td ltx_align_center\">0.3060</td>\n</tr>\n<tr id=\"S3.T3.1.4.3\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">TrimmedMean</th>\n<td id=\"S3.T3.1.4.3.2\" class=\"ltx_td ltx_align_center\">0.7588</td>\n<td id=\"S3.T3.1.4.3.3\" class=\"ltx_td ltx_align_center\">0.7517</td>\n<td id=\"S3.T3.1.4.3.4\" class=\"ltx_td ltx_align_center\">0.3218</td>\n<td id=\"S3.T3.1.4.3.5\" class=\"ltx_td ltx_align_center\">0.3314</td>\n</tr>\n<tr id=\"S3.T3.1.5.4\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\">TopKRegCost</th>\n<td id=\"S3.T3.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_b\">0.7963</td>\n<td id=\"S3.T3.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_b\">0.7700</td>\n<td id=\"S3.T3.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_b\">0.2799</td>\n<td id=\"S3.T3.1.5.4.5\" class=\"ltx_td ltx_align_center ltx_border_b\">0.3086</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "We used the insights from above for our final submission, which was specifically:",
                "Rounds 1-3 used vanilla WAvg, with the Adam optimiser on both sides. The server-side learning rate was 0.003, and on the client-side it was 0.0005.",
                "For rounds 4-16, we switched to RegAgg, with Adam optimiser on both sides and reduced learning rates of 0.002 on the server side 0.00005 on the client side. The results on the held-out FeTS 2022 test set are reported in Table ",
                "4",
                ".",
                "Adam’s parameters were set to the aforementioned values of ",
                "β",
                "1",
                "=",
                "0.9",
                "subscript",
                "𝛽",
                "1",
                "0.9",
                "\\beta_{1}=0.9",
                ", ",
                "β",
                "2",
                "=",
                "0.99",
                "subscript",
                "𝛽",
                "2",
                "0.99",
                "\\beta_{2}=0.99",
                " and ",
                "τ",
                "=",
                "0.001",
                "𝜏",
                "0.001",
                "\\tau=0.001",
                " in all rounds and on the server and all clients."
            ]
        ]
    },
    "S3.T4": {
        "caption": "Table 4: Segmentation performance of the submitted algorithm on the FeTS 2022 test set, by label\n(WT: Whole Tumour, TC: Tumour Core, ET: Expanding Tumour). The reported Communications Cost was 0.702.",
        "table": "<table id=\"S3.T4.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T4.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"></th>\n<th id=\"S3.T4.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\">Dice</th>\n<th id=\"S3.T4.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\">Hausdorff 95</th>\n</tr>\n<tr id=\"S3.T4.1.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T4.1.2.2.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r\"></th>\n<th id=\"S3.T4.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Mean (Std.)</th>\n<th id=\"S3.T4.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">Median (Quartiles)</th>\n<th id=\"S3.T4.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Mean (Std.)</th>\n<th id=\"S3.T4.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Median (Quartiles)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T4.1.3.1\" class=\"ltx_tr\">\n<th id=\"S3.T4.1.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">WT</th>\n<td id=\"S3.T4.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.7752 (0.1753)</td>\n<td id=\"S3.T4.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.8316 ([0.7105,0.8975])</td>\n<td id=\"S3.T4.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">28.93 (30.02)</td>\n<td id=\"S3.T4.1.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">14.88 ([6.428,48.51])</td>\n</tr>\n<tr id=\"S3.T4.1.4.2\" class=\"ltx_tr\">\n<th id=\"S3.T4.1.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">TC</th>\n<td id=\"S3.T4.1.4.2.2\" class=\"ltx_td ltx_align_center\">0.7785 (0.2769)</td>\n<td id=\"S3.T4.1.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">0.9041 ([0.7609,0.9449])</td>\n<td id=\"S3.T4.1.4.2.4\" class=\"ltx_td ltx_align_center\">28.81 (80.48)</td>\n<td id=\"S3.T4.1.4.2.5\" class=\"ltx_td ltx_align_center\">4.123 ([2.236,10.1])</td>\n</tr>\n<tr id=\"S3.T4.1.5.3\" class=\"ltx_tr\">\n<th id=\"S3.T4.1.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\">ET</th>\n<td id=\"S3.T4.1.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_b\">0.747 (0.2625)</td>\n<td id=\"S3.T4.1.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">0.8492 ([0.7224,0.9114])</td>\n<td id=\"S3.T4.1.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_b\">29.59 (85.79)</td>\n<td id=\"S3.T4.1.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_b\">2.236 ([1.414,7.45])</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "For rounds 4-16, we switched to RegAgg, with Adam optimiser on both sides and reduced learning rates of 0.002 on the server side 0.00005 on the client side. The results on the held-out FeTS 2022 test set are reported in Table 4."
        ]
    }
}