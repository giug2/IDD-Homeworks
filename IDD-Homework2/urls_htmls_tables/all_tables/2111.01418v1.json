{
    "S2.T1": {
        "caption": "Table 1: Comparisons of different semantic segmentation methods. Existing methods cope with either weak supervision or few-shot settings, while ours combine both. Note that [37] still requires full supervision during training, as detailed in FigureÂ 2(b).",
        "table": null,
        "footnotes": [],
        "references": [
            "Few-Shot Semantic Segmentation.\nFew-shot learning aims at learning models which would generalize to categories with only a limited amount of labeled dataÂ [12, 11]. Meta-learningÂ [13, 30, 34] has been widely applied for this task, with the core idea of adapting the learning scheme from base to novel categories. For example, metric-based meta-learning algorithms such as ProtoNetÂ [40] and RNÂ [41] learn feature embeddings that exhibit proper distance metrics for classification and generalization. Few-shot semantic segmentation, on the other hand, aims at generalizing the ability of pixel-level classification across categories, while only a limited number of images are with ground truth pixel-level labels. OSLSMÂ [36] is the first proposed method to tackle this problem, leveraging information learned from support-set images and outputs parameters for query image segmentation. PLÂ [9] adopts metric learning methodsÂ [40] to extract prototypes of each semantic class, and measures their distances between feature maps of query images; CANetÂ [54] adds an iterative optimization module to refine the predicted results; PFENetÂ [42] generates additional prior masks to enrich the extracted features. \nMoreover, PANetÂ [46], FWBÂ [27] and CRNetÂ [23] propose to further leverage information from the support set by performing segmentation in the reversed direction (i.e., segmentation of the support set) for improved model learning. To exploit knowledge from the foreground objects, DANÂ [45] and SimPropNetÂ [14] introduce attention mechanisms, PMMsÂ [52] and ASGNetÂ [20] employ multiple prototypes for a single category, and PPNetÂ [24] proposes part-aware prototypes to capture fine-grained features. While some of the existing few-shot semantic segmentation methods present results in weak supervision settings (e.g., use of bounding boxes or scribblesÂ [29, 46, 54] as guidance), they cannot produce satisfactory performance with only image-level annotation observed. More recently, [31, 4, 37] follow the few-shot setting using image-level supervision, but they still require collection of ground truth pixel-level masks for base-class images during meta-training. As depicted in FigureÂ 2 and TableÂ 1, to the best of our knowledge, we are the first to tackle few-shot semantic segmentation using only image-level annotations during both (meta) training and testing stages."
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Performance evaluation on (a) PASCAL-5isuperscript5ğ‘–5^{i} and (b) MS COCO in terms of mean-IoU (Mean) and performance difference Î”Î”\\Delta due to change of settings. The numbers before and after â€˜/â€™ indicate results under fully and weakly supervised settings, respectively. Note that [37] considers a loosely weakly supervised setting and requires ground truth pixel-level masks during training, while [42] utilizes a stronger backbone (ResNet-50) compared to others (VGG-16).",
        "table": null,
        "footnotes": [],
        "references": [
            "PASCAL-ğŸ“isuperscript5ğ‘–\\bm{5^{i}}. We first compare the performances of different methods on PASCAL-5isuperscript5ğ‘–5^{i}, with results listed in TableÂ 2a. In the first row of this table, we consider a loosely weakly supervised model of [37] (as illustrated in FigureÂ 2b). For other methods (including ours) in this table, we present results under both fully and weakly supervised settings. While our fully supervised model achieved comparable performance with state-of-the-art methods in the standard 1-way 1-shot setting, our model reported a significant improvement over PANet [46] by 15.3% (42.4% v.s. 27.1%) in the weakly supervised setting. It is worth noting that PFENetÂ [42] is trained using a stronger backbone (ResNet-50), while all other methods (including ours) utilize VGG-16. Nevertheless, our model still outperforms [42] by a considerable margin of 2.5% in the weakly supervised setting. We also observe that the performance drop between the two different settings of our model is significantly less than the others. That is, when only image-level labels (instead of ground truth pixel-level masks) are observed during both training and testing, both PFENetÂ [42] and PANetÂ [46] suffered from a >>20% performance drop while only 5.1% was reported by our model.",
            "MS COCO. As shown in TableÂ 2b, despite the increased difficulty in few-shot segmentation on MS COCO, our model is able to achieve satisfactory performances under both fully and weakly supervised settings when comparing to [46]. Specifically, we only observe a 2.7% performance drop between the two settings on the 1-way 1-shot task, while that of [46] is 12.9%. The above quantitative results support the use of our propose framework for solving few-shot semantic segmentation, especially when only image-level labels can be observed during both training and testing (i.e., the weakly supervised setting).",
            "Multi-way Segmentation. We now show that our model is applicable to the cases when there is more than one foreground object category in an image, which is more challenging since it requires more information to be learned in each episode. In the lower parts of TableÂ 2, we list the performances of different methods under the 2-way setting (i.e., two types of foreground objects exist in an image). It is worth noting that, while most existing methodsÂ [36, 38, 54, 27, 17, 42] are designed to tackle only 1-way segmentation, they typically claim such extension can be realized by forward passing Kğ¾K times with additional decision rules. On the contrary, our method can directly produce output labels of a multi-category image as the final classification by a kğ‘˜k-NN search. As shown in TableÂ 2a, our model performed favorably against previous methods by a margin of 11.5% on the PASCAL-5isuperscript5ğ‘–5^{i} 2-way 1-shot task. To the best of our knowledge, we are the first to report results of 2-way tasks for the MS COCO dataset, as shown in the last row of TableÂ 2b.",
            "From the split-wise mean-IoUs listed in TableÂ 2, we see that some data splits would generally suffer from performance drops across different models including ours. In FigureÂ 6, we show failure segmentation example results by our weakly supervised model and the fully supervised PFENetÂ [42]. As evident in this figure, the ground truth images of such categories generally contain small foreground regions (e.g., bottle), or those that cannot be easily distinguished from the background (e.g., chair). For such image categories, training with few-shot samples would not be expected to address semantic segmentation tasks well. This would be the limitation of developing solutions for few-shot semantic segmentation."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Ablation study of our model on PASCAL-5isuperscript5ğ‘–5^{i} in mean-IoUs. Ours w/o Eğ¸E denotes our framework without meta-learner encoderÂ Eğ¸E, while w/o Saliency indicates our model without applying saliency gating to process the generated pseudo masks.",
        "table": null,
        "footnotes": [],
        "references": [
            "Ablation Study. As our meta-learning process is mainly achieved by the episodic learning of encoder Eğ¸E, we now design a baseline version such that it directly predicts the output mask by a kğ‘˜k-NN search on the pixel features encoded by the DeepLabv3+ backbone (i.e., without embedding into the latent space via Eğ¸E). As shown in the first row of TableÂ 3, the results were severely degraded with a drop of up to 18%, if the model was not learned via the meta-learning objectives. This further confirms that, the promising performance achieved by our proposed method is not a direct result of using particular strong backbones. Instead, it leverages spatial and structural details captured by the backbone, upon which semantic and category-wise information is reinforced via the meta-learning process.",
            "Additionally, we provide results that are trained using pseudo masks generated without saliency gating (as mentioned in the last step of SectionÂ 3.2). As shown in the second row of TableÂ 3, a slight decrease in mean-IoU (less than 5%) was observed, while still outperforming the baseline version by a large margin. Thus, the use of saliency gating as the post-processing step for pseudo pixel-level masks would be preferable but not critical."
        ]
    }
}