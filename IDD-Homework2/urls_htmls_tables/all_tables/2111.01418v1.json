{
    "S2.T1": {
        "caption": "Table 1: Comparisons of different semantic segmentation methods. Existing methods cope with either weak supervision or few-shot settings, while ours combine both. Note that [37] still requires full supervision during training, as detailed in Figure 2(b).",
        "table": null,
        "footnotes": [],
        "references": [
            "Few-Shot Semantic Segmentation.\nFew-shot learning aims at learning models which would generalize to categories with only a limited amount of labeled data [12, 11]. Meta-learning [13, 30, 34] has been widely applied for this task, with the core idea of adapting the learning scheme from base to novel categories. For example, metric-based meta-learning algorithms such as ProtoNet [40] and RN [41] learn feature embeddings that exhibit proper distance metrics for classification and generalization. Few-shot semantic segmentation, on the other hand, aims at generalizing the ability of pixel-level classification across categories, while only a limited number of images are with ground truth pixel-level labels. OSLSM [36] is the first proposed method to tackle this problem, leveraging information learned from support-set images and outputs parameters for query image segmentation. PL [9] adopts metric learning methods [40] to extract prototypes of each semantic class, and measures their distances between feature maps of query images; CANet [54] adds an iterative optimization module to refine the predicted results; PFENet [42] generates additional prior masks to enrich the extracted features. \nMoreover, PANet [46], FWB [27] and CRNet [23] propose to further leverage information from the support set by performing segmentation in the reversed direction (i.e., segmentation of the support set) for improved model learning. To exploit knowledge from the foreground objects, DAN [45] and SimPropNet [14] introduce attention mechanisms, PMMs [52] and ASGNet [20] employ multiple prototypes for a single category, and PPNet [24] proposes part-aware prototypes to capture fine-grained features. While some of the existing few-shot semantic segmentation methods present results in weak supervision settings (e.g., use of bounding boxes or scribbles [29, 46, 54] as guidance), they cannot produce satisfactory performance with only image-level annotation observed. More recently, [31, 4, 37] follow the few-shot setting using image-level supervision, but they still require collection of ground truth pixel-level masks for base-class images during meta-training. As depicted in Figure 2 and Table 1, to the best of our knowledge, we are the first to tackle few-shot semantic segmentation using only image-level annotations during both (meta) training and testing stages."
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Performance evaluation on (a) PASCAL-5isuperscript5𝑖5^{i} and (b) MS COCO in terms of mean-IoU (Mean) and performance difference ΔΔ\\Delta due to change of settings. The numbers before and after ‘/’ indicate results under fully and weakly supervised settings, respectively. Note that [37] considers a loosely weakly supervised setting and requires ground truth pixel-level masks during training, while [42] utilizes a stronger backbone (ResNet-50) compared to others (VGG-16).",
        "table": null,
        "footnotes": [],
        "references": [
            "PASCAL-𝟓isuperscript5𝑖\\bm{5^{i}}. We first compare the performances of different methods on PASCAL-5isuperscript5𝑖5^{i}, with results listed in Table 2a. In the first row of this table, we consider a loosely weakly supervised model of [37] (as illustrated in Figure 2b). For other methods (including ours) in this table, we present results under both fully and weakly supervised settings. While our fully supervised model achieved comparable performance with state-of-the-art methods in the standard 1-way 1-shot setting, our model reported a significant improvement over PANet [46] by 15.3% (42.4% v.s. 27.1%) in the weakly supervised setting. It is worth noting that PFENet [42] is trained using a stronger backbone (ResNet-50), while all other methods (including ours) utilize VGG-16. Nevertheless, our model still outperforms [42] by a considerable margin of 2.5% in the weakly supervised setting. We also observe that the performance drop between the two different settings of our model is significantly less than the others. That is, when only image-level labels (instead of ground truth pixel-level masks) are observed during both training and testing, both PFENet [42] and PANet [46] suffered from a >>20% performance drop while only 5.1% was reported by our model.",
            "MS COCO. As shown in Table 2b, despite the increased difficulty in few-shot segmentation on MS COCO, our model is able to achieve satisfactory performances under both fully and weakly supervised settings when comparing to [46]. Specifically, we only observe a 2.7% performance drop between the two settings on the 1-way 1-shot task, while that of [46] is 12.9%. The above quantitative results support the use of our propose framework for solving few-shot semantic segmentation, especially when only image-level labels can be observed during both training and testing (i.e., the weakly supervised setting).",
            "Multi-way Segmentation. We now show that our model is applicable to the cases when there is more than one foreground object category in an image, which is more challenging since it requires more information to be learned in each episode. In the lower parts of Table 2, we list the performances of different methods under the 2-way setting (i.e., two types of foreground objects exist in an image). It is worth noting that, while most existing methods [36, 38, 54, 27, 17, 42] are designed to tackle only 1-way segmentation, they typically claim such extension can be realized by forward passing K𝐾K times with additional decision rules. On the contrary, our method can directly produce output labels of a multi-category image as the final classification by a k𝑘k-NN search. As shown in Table 2a, our model performed favorably against previous methods by a margin of 11.5% on the PASCAL-5isuperscript5𝑖5^{i} 2-way 1-shot task. To the best of our knowledge, we are the first to report results of 2-way tasks for the MS COCO dataset, as shown in the last row of Table 2b.",
            "From the split-wise mean-IoUs listed in Table 2, we see that some data splits would generally suffer from performance drops across different models including ours. In Figure 6, we show failure segmentation example results by our weakly supervised model and the fully supervised PFENet [42]. As evident in this figure, the ground truth images of such categories generally contain small foreground regions (e.g., bottle), or those that cannot be easily distinguished from the background (e.g., chair). For such image categories, training with few-shot samples would not be expected to address semantic segmentation tasks well. This would be the limitation of developing solutions for few-shot semantic segmentation."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Ablation study of our model on PASCAL-5isuperscript5𝑖5^{i} in mean-IoUs. Ours w/o E𝐸E denotes our framework without meta-learner encoder E𝐸E, while w/o Saliency indicates our model without applying saliency gating to process the generated pseudo masks.",
        "table": null,
        "footnotes": [],
        "references": [
            "Ablation Study. As our meta-learning process is mainly achieved by the episodic learning of encoder E𝐸E, we now design a baseline version such that it directly predicts the output mask by a k𝑘k-NN search on the pixel features encoded by the DeepLabv3+ backbone (i.e., without embedding into the latent space via E𝐸E). As shown in the first row of Table 3, the results were severely degraded with a drop of up to 18%, if the model was not learned via the meta-learning objectives. This further confirms that, the promising performance achieved by our proposed method is not a direct result of using particular strong backbones. Instead, it leverages spatial and structural details captured by the backbone, upon which semantic and category-wise information is reinforced via the meta-learning process.",
            "Additionally, we provide results that are trained using pseudo masks generated without saliency gating (as mentioned in the last step of Section 3.2). As shown in the second row of Table 3, a slight decrease in mean-IoU (less than 5%) was observed, while still outperforming the baseline version by a large margin. Thus, the use of saliency gating as the post-processing step for pseudo pixel-level masks would be preferable but not critical."
        ]
    }
}