{
    "id_table_1": {
        "caption": "Table 2:  Results on AlpacaEval 2.0 and Arena-Hard. LC and WR denote length-controlled and raw win rates, respectively. After four  SynPO  iterations, Mistral-Base and Llama3-Base increase LC by 27.4% and 26.7%, respectively, on AlpacaEval 2.0. In Arena-Hard,  SynPO  achieves the highest WR by the third iteration, improving both models by over 22.1%.",
        "table": "S1.F1.fig1.1.1",
        "footnotes": [],
        "references": [
            "Experimental results demonstrate that SynPO not only benefits LLM alignment with human preferences, but also improves generalist capabilities across various tasks. Trained solely on synthetic data, SynPO significantly improves the instruction-following abilities of Llama3-8B and Mistral-7B (as shown in Figure  1  and Table  1 ), achieving over a 26% length-controlled win rate improvement on AlpacaEval 2.0  (Dubois et al.,  2024 )  and a 22% to 30% improvement on Arena-hard  (Li et al.,  2024c )  (as shown in Table  2 ). Furthermore, self-boosted models achieve 3.2% to 5.0% higher average performance than SFT models on the Open LLM leaderboard  (Beeching et al.,  2023 ) , indicating SynPO also enhances general LLM performance.",
            "For the multi-turn benchmark MT-Bench, we report both the first-turn and second-turn scores (in Table  3 ) as well as a radar chart depicting performance across different question types (refer to Figure  3 ). 5 5 5 Similar results for Mistral-7B are shown in Figure  10  in Appendix  G .  The results indicate that SynPO enhances not only first-turn performance, with an increase of over 0.7 points, but also subsequent turns, with an increase of over 1.2 points. Compared to the initial model, SynPO shows improved performance across various question types, particularly in humanities, writing, STEM, and roleplaying.",
            "We have demonstrated the diversity of prompts generated by SynPO in Section  2.1 . To further validate the self-prompt generator, we compare the generated prompts with manual collected prompts  (Cui et al.,  2023 )  and Self-Instruct prompts  (Wang et al.,  2022 ) . For each prompt construction approach, we randomly sample 20k prompts and construct response pairs through both SynPO and Sampling-Ranking. We compare the single iteration results of Llama3-8B. As shown in Table  7 , whether through self-refinement or Sampling-Ranking, synthetic prompts generated by SynPO lead to better-aligned models, validating the quality of these prompts. It is worth mentioning that SynPO prompts are even more effective than the superset of its seed training data, UltraFeedback prompts. This increased effectiveness may be attributed to the greater diversity of SynPO prompts, achieved through the keyword sampling process in prompt synthesis. Results of mixing SynPO and manual collected prompts further indicate the potential of SynPO in augmenting existing prompts.",
            "We provide the overall pipeline of SynPO in Algorithm  1 .",
            "In experiments involving iterative baselines, we control various conditions to ensure fairness. We maintain the same training data size for both iterative baselines and SynPO. We adopt the SimPO loss  (Meng et al.,  2024 )  for preference optimization, as it is more effective than DPO  (Rafailov et al.,  2024 ) . We all use self-generated prompts, which have been shown to be superior to prompts generated by other methods, as validated in Section  2.1  and Section  4.1 . All preference construction processes are iterated until performance no longer improves.",
            "For instruction-following ability evaluation, Table  8  presents the detailed information for three alignment benchmarks we use, including AlpacaEval 2.0, Arena-Hard and MT-Bench. Additionally, we display the radar chart for MT-Bench scores on different prompt types (see Figure  10 )."
        ]
    },
    "id_table_2": {
        "caption": "Table 3:  Multi-turn evaluation on MT-Bench. An asterisk (*) denotes the best score across multiple iterations. For Sampling-Ranking, Llamas best is from iteration 4 and Mistrals from iteration 3. For Self-Rewarding, both are from iteration 3.  SynPO  progressively enhances the multi-turn instruction-following capabilities of LLMs.",
        "table": "A7.EGx1",
        "footnotes": [],
        "references": [
            "Experimental results demonstrate that SynPO not only benefits LLM alignment with human preferences, but also improves generalist capabilities across various tasks. Trained solely on synthetic data, SynPO significantly improves the instruction-following abilities of Llama3-8B and Mistral-7B (as shown in Figure  1  and Table  1 ), achieving over a 26% length-controlled win rate improvement on AlpacaEval 2.0  (Dubois et al.,  2024 )  and a 22% to 30% improvement on Arena-hard  (Li et al.,  2024c )  (as shown in Table  2 ). Furthermore, self-boosted models achieve 3.2% to 5.0% higher average performance than SFT models on the Open LLM leaderboard  (Beeching et al.,  2023 ) , indicating SynPO also enhances general LLM performance.",
            "SynPO is a self-boosting scheme designed to iteratively generate high-quality preference data. An overview of SynPO is presented in Figure  2 . It begins with a small set of SFT data as seed data, denoted as  { ( x i  , y i  ) } i = 0 n superscript subscript subscript superscript x i subscript superscript y i i 0 n \\left\\{(\\mathbf{x}^{*}_{i},\\mathbf{y}^{*}_{i})\\right\\}_{i=0}^{n} { ( bold_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_y start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , and the initial policy model    0 subscript  subscript  0 \\pi_{\\theta_{0}} italic_ start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT . By incorporating both the self-prompt generator and the response improver, SynPO provides sufficient prompts for iterative training and leverages the generative rewards in the synthetic preference data. This approach allows the policy model to make subtle improvements and gradually expand its boundaries.",
            "We compare the instruction-following and human preference alignment capabilities on AlpacaEval 2.0  (Dubois et al.,  2024 )  and Arena-Hard  (Li et al.,  2024c )  in Table  2 . Compared to the initial model post-SFT, SynPO shows sustained improvement over four iterations in win rate against GPT-4 Turbo or GPT-4. On AlpacaEval 2.0, Mistral-Base achieves a 27.4% increase in length-controlled win rate and a 32.8% increase in raw win rate after four iterations. Similarly, Llama3 exhibits a 26.7% rise in length-controlled win rate and a 30.5% improvement in raw win rate after the same number of iterations. In the more challenging Arena-Hard setting, SynPO reaches the highest win rate after the third iteration. Compared to the baseline methods, SynPOs iterative preference learning on synthetic data yielded more significant improvements.",
            "We have demonstrated the diversity of prompts generated by SynPO in Section  2.1 . To further validate the self-prompt generator, we compare the generated prompts with manual collected prompts  (Cui et al.,  2023 )  and Self-Instruct prompts  (Wang et al.,  2022 ) . For each prompt construction approach, we randomly sample 20k prompts and construct response pairs through both SynPO and Sampling-Ranking. We compare the single iteration results of Llama3-8B. As shown in Table  7 , whether through self-refinement or Sampling-Ranking, synthetic prompts generated by SynPO lead to better-aligned models, validating the quality of these prompts. It is worth mentioning that SynPO prompts are even more effective than the superset of its seed training data, UltraFeedback prompts. This increased effectiveness may be attributed to the greater diversity of SynPO prompts, achieved through the keyword sampling process in prompt synthesis. Results of mixing SynPO and manual collected prompts further indicate the potential of SynPO in augmenting existing prompts.",
            "In experiments involving iterative baselines, we control various conditions to ensure fairness. We maintain the same training data size for both iterative baselines and SynPO. We adopt the SimPO loss  (Meng et al.,  2024 )  for preference optimization, as it is more effective than DPO  (Rafailov et al.,  2024 ) . We all use self-generated prompts, which have been shown to be superior to prompts generated by other methods, as validated in Section  2.1  and Section  4.1 . All preference construction processes are iterated until performance no longer improves."
        ]
    },
    "id_table_3": {
        "caption": "Figure 6:  Radar chart for Llama3-8B-Base- SynPO  on MT-Bench.  SynPO  achieves notable improvements across various prompt categories, particularly in RolePlay, STEM, Reasoning, and Coding tasks.",
        "table": "S2.T2.1.1",
        "footnotes": [],
        "references": [
            "We train the LLM itself to serve as a high-quality prompt generator. For each prompt  x i  subscript superscript x i \\mathbf{x}^{*}_{i} bold_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  in seed data, we randomly extract two keywords from  x i  subscript superscript x i \\mathbf{x}^{*}_{i} bold_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and one noise keyword from  x j  subscript superscript x j \\mathbf{x}^{*}_{j} bold_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , where  j  { 1 , 2 , ... , n }  { i } j 1 2 ... n i j\\in\\{1,2,\\ldots,n\\}\\setminus\\{i\\} italic_j  { 1 , 2 , ... , italic_n }  { italic_i } . The inclusion of the noise keyword enhances the robustness of the prompt generator. It learns to filter out irrelevant keywords during training and ensure that the generated prompts are fluent. This process yields a keyword list,  k i subscript k i k_{i} italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  for  x i  subscript superscript x i \\mathbf{x}^{*}_{i} bold_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . Next, we insert  k i subscript k i k_{i} italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  into a prompt template (see Figure  3 ) to create a prompt and use  ( x i  , y i  ) subscript superscript x i subscript superscript y i (\\mathbf{x}^{*}_{i},\\mathbf{y}^{*}_{i}) ( bold_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_y start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  as the corresponding completion. This process constructs training data for the prompt generator. We then optimize   0 subscript  0 \\theta_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  through SFT to transform the model into a prompt generator  G G \\mathcal{G} caligraphic_G .  G G \\mathcal{G} caligraphic_G  possesses the capability to generate unlimited, diverse, and high-quality user instructions, controlled by the given keywords.",
            "For the multi-turn benchmark MT-Bench, we report both the first-turn and second-turn scores (in Table  3 ) as well as a radar chart depicting performance across different question types (refer to Figure  3 ). 5 5 5 Similar results for Mistral-7B are shown in Figure  10  in Appendix  G .  The results indicate that SynPO enhances not only first-turn performance, with an increase of over 0.7 points, but also subsequent turns, with an increase of over 1.2 points. Compared to the initial model, SynPO shows improved performance across various question types, particularly in humanities, writing, STEM, and roleplaying."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Open LLM Leaderboard results. TQA stands for TruthfulQA. The asterisk (*) represents the best performance across multiple iterations. Compared to the SFT versions,  SynPO  achieves an overall improvement of 3.19% for Mistral and 5.00% for Llama3 on the average score.",
        "table": "S3.T3.fig1.1.1",
        "footnotes": [],
        "references": [
            "In experiments involving iterative baselines, we control various conditions to ensure fairness. We maintain the same training data size for both iterative baselines and SynPO. We adopt the SimPO loss  (Meng et al.,  2024 )  for preference optimization, as it is more effective than DPO  (Rafailov et al.,  2024 ) . We all use self-generated prompts, which have been shown to be superior to prompts generated by other methods, as validated in Section  2.1  and Section  4.1 . All preference construction processes are iterated until performance no longer improves."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Downstream performance in each SynPO iteration on six tasks in LM Evaluation Harness. An asterisk (*) represents the best performance across multiple iterations.",
        "table": "S3.T4.1.1",
        "footnotes": [],
        "references": [
            "For further analysis, we randomly sampled 1k self-generated prompts from the Llama3-8B-Base prompt generator and used GPT-4 Turbo to classify the intentions and topics behind these prompts 1 1 1 Experimental details in Appendix  F . . The results, shown in Figure  5 , demonstrate significant diversity across various topics and user intentions. Even compared to prompts from GPT3.5-Turbo  (Ding et al.,  2023 )  or a collection of prompts from different sources  (Cui et al.,  2023 ) , as shown in Figure  5 , SynPO generated prompts exhibit lower inter-prompt similarity and greater diversity.",
            "As baselines, we use the initial supervised fine-tuned models and those optimized with data from various preference construction methods, including manual collection and iterative approaches. We recognize the UltraFeedback preference data  (Cui et al.,  2023 )  as a product of manual collection. It is gathered from six high-quality datasets and various models, with preferences annotated by GPT-4  (Achiam et al.,  2023 ) . For iterative construction approaches, we involve Sampling-Ranking and Self-Rewarding. For Sampling-Ranking, similar to  Meng et al. ( 2024 )  and  Wu et al. ( 2024b ) , we involves LLMs in sampling five responses per prompt in each iteration. The same scoring models, i.e., PairRM and ArmoRM-Llama3-8B-v0.1, are then used to select the highest and lowest scoring responses as the chosen and rejected responses, respectively. For the Self-Rewarding method  (Yuan et al.,  2024 ) , we generate preference data based on models own rewards via LLM-as-a-Judge prompting. We employ our 18k seed data as the initial instruction-following data. Given that Self-Rewarding requires additional LLM-as-a-Judge training data, we generate 16k seed data with GPT-4 Turbo. For all settings, we adopt SimPO  (Meng et al.,  2024 )  for preference optimization. More detailed are elaborated in Appendix  C.5 .",
            "The advantages above are also reflected in the results for more diverse tasks in LLM Harness, as evidenced by Table  5 . Previous works  (Wu et al.,  2024b ; Meng et al.,  2024 )  demonstrate that preference optimization can induce the alignment tax - aligning models with human preferences can improve performance for only 1  similar-to \\sim  2 iterations or even degrade overall performance on downstream tasks  (Askell et al.,  2021 ) . Our method exhibits similar behavior on MathQA; however, overall, SynPO shows improvements across more iterations on other tasks. This is because synthesizing better chosen candidates introduces additional supervision, partially mitigating the alignment tax issue and enabling LLMs to continuously enhance their capabilities on downstream tasks at the same time of alignment."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Comparison of various prompt generation methods on AlpacaEval 2.0. SynPO Mix. combines SynPO prompts with manually collected prompts.",
        "table": "S3.T5.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_7": {
        "caption": "Table 7:  Impact analysis of seed data on AlpacaEval 2.0. PO me me {}^{\\text{me}} start_FLOATSUPERSCRIPT me end_FLOATSUPERSCRIPT  refers to preference optimization over multiple epochs.",
        "table": "S4.T7.fig1.1.1",
        "footnotes": [],
        "references": [
            "We have demonstrated the diversity of prompts generated by SynPO in Section  2.1 . To further validate the self-prompt generator, we compare the generated prompts with manual collected prompts  (Cui et al.,  2023 )  and Self-Instruct prompts  (Wang et al.,  2022 ) . For each prompt construction approach, we randomly sample 20k prompts and construct response pairs through both SynPO and Sampling-Ranking. We compare the single iteration results of Llama3-8B. As shown in Table  7 , whether through self-refinement or Sampling-Ranking, synthetic prompts generated by SynPO lead to better-aligned models, validating the quality of these prompts. It is worth mentioning that SynPO prompts are even more effective than the superset of its seed training data, UltraFeedback prompts. This increased effectiveness may be attributed to the greater diversity of SynPO prompts, achieved through the keyword sampling process in prompt synthesis. Results of mixing SynPO and manual collected prompts further indicate the potential of SynPO in augmenting existing prompts.",
            "SynPO involves training LLMs solely on synthetic preference data while using seed SFT data for validation. To investigate the maximum impact of the seed SFT data, we compare SynPO with the following settings: 1)  Seed SFT:  Directly fine-tuning the LLM using seed data. 2)  Seed PO:  For each prompt in the seed SFT data, using the gold standard response in the seed data as the chosen response and the initial policy model response as the rejected response for preference optimization. 3)  Seed SFT + PO:  To avoid distribution shifts in directly using seed SFT data, we first obtain a model fine-tuned on seed data as in 1), then construct preference data using the model output and gold standard responses. 4)  Seed SFT + PO me me {}^{\\text{me}} start_FLOATSUPERSCRIPT me end_FLOATSUPERSCRIPT :  Training on data from 3) for multiple epochs. 7 7 7 We compare the results of 2 epochs and 3 epochs and select the better-performing 2 epochs. More experimental details in Appendix  D  The results on AlpacaEval 2.0 are presented in Table  7 . Among the evaluated methods except for SynPO, setting 3) is most analogous to SynPO, and proves to be the most effective. However, due to the limited quantity of seed data, the improvement is less than that achieved by iterative SynPO on synthetic data. Training under such conditions for multiple epochs does not yield further improvements and even degrades performance. These findings validate that SynPO is a promising approach to construct preference data and maximize the utilization of minimal high-quality data.",
            "The prompt template used for training and inference in the response improver is shown in Figure  7 ."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Details for three alignment benchmarks.",
        "table": "S4.T7.1.1.1",
        "footnotes": [],
        "references": [
            "Here we provide the prompt used for prompt topic and intention analysis in Figure  8 , along with a more detailed distribution bar plot for different intentions and topics in Figure  9 . The topic word list is derived from UltraChat  (Ding et al.,  2023 ) , while the intention word list was designed by us.",
            "For instruction-following ability evaluation, Table  8  presents the detailed information for three alignment benchmarks we use, including AlpacaEval 2.0, Arena-Hard and MT-Bench. Additionally, we display the radar chart for MT-Bench scores on different prompt types (see Figure  10 )."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Number of few-shot examples in Open LLM Leaderboard evaluation.",
        "table": "A7.EGx2",
        "footnotes": [],
        "references": [
            "Here we provide the prompt used for prompt topic and intention analysis in Figure  8 , along with a more detailed distribution bar plot for different intentions and topics in Figure  9 . The topic word list is derived from UltraChat  (Ding et al.,  2023 ) , while the intention word list was designed by us.",
            "As for general LLM capability evaluation, we provide the few-shot example numbers on Open LLM Leaderboard in Table  9  and a comprehensive comparison of SynPO."
        ]
    },
    "id_table_10": {
        "caption": "",
        "table": "A7.EGx3",
        "footnotes": [],
        "references": [
            "For the multi-turn benchmark MT-Bench, we report both the first-turn and second-turn scores (in Table  3 ) as well as a radar chart depicting performance across different question types (refer to Figure  3 ). 5 5 5 Similar results for Mistral-7B are shown in Figure  10  in Appendix  G .  The results indicate that SynPO enhances not only first-turn performance, with an increase of over 0.7 points, but also subsequent turns, with an increase of over 1.2 points. Compared to the initial model, SynPO shows improved performance across various question types, particularly in humanities, writing, STEM, and roleplaying.",
            "For instruction-following ability evaluation, Table  8  presents the detailed information for three alignment benchmarks we use, including AlpacaEval 2.0, Arena-Hard and MT-Bench. Additionally, we display the radar chart for MT-Bench scores on different prompt types (see Figure  10 )."
        ]
    },
    "id_table_11": {
        "caption": "",
        "table": "A7.T8.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "",
        "table": "A7.T9.1",
        "footnotes": [],
        "references": []
    }
}