{
    "id_table_1": {
        "caption": "Table 1 :  Diversity analysis results on the 1-shot setting. DALDA achieves enhanced diversity compared to other methods. The best results are highlighted in bold.",
        "table": "S4.T1.4.1",
        "footnotes": [],
        "references": [
            "Our framework is illustrated in  Fig.   1 . DALDA consists of multiple stages, including (1) measuring CLIPScore, (2) adaptive guidance scaling, and (3) synthetic data generation. Each of these stages will be explained in detail in the subsequent sections.",
            "In  Eq.   1 ,  Q Q Q italic_Q  is a matrix derived from query features  Z Z Z italic_Z  and weights  W q subscript W q W_{q} italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT .  K t subscript K t K_{t} italic_K start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and  V t subscript V t V_{t} italic_V start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  are matrices obtained from text features  c t subscript c t c_{t} italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and their respective weights  W k  t subscript W k t W_{kt} italic_W start_POSTSUBSCRIPT italic_k italic_t end_POSTSUBSCRIPT  and  W v  t subscript W v t W_{vt} italic_W start_POSTSUBSCRIPT italic_v italic_t end_POSTSUBSCRIPT , while  K i subscript K i K_{i} italic_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and  V i subscript V i V_{i} italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  are matrices derived from image features  c i subscript c i c_{i} italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and their respective weights  W k  i subscript W k i W_{ki} italic_W start_POSTSUBSCRIPT italic_k italic_i end_POSTSUBSCRIPT  and  W v  i subscript W v i W_{vi} italic_W start_POSTSUBSCRIPT italic_v italic_i end_POSTSUBSCRIPT . The weight factor denoted by    \\lambda italic_ , assigns weights to image features. A higher value of    \\lambda italic_  increases the weight of image guidance in the inference stage, while a lower value of    \\lambda italic_  relatively increases the weight of text guidance ( Fig.   2 ).",
            "To demonstrate that our method can generate diverse synthetic images in data-scarce scenarios, we measure the accuracy in few-shot classification tasks. We recognize the importance of accurate alignment between real images and class names in the generation of synthetic data. For a comprehensive and fair analysis, we select a subset of few-shot datasets from those used in  [ 10 ] , based on their CLIPScores. These CLIPScores are calculated using the same methodology described in  Sec.   3.1 . Each dataset exhibits various distributions in  Fig.   4 ; Caltech-101  [ 7 ] , which belongs to the common dataset, recorded a high average CLIPScore of 0.8406, categorizing it as a High CLIPScore (HC) group. Oxford Pets  [ 21 ] , which belongs to the fine-grained dataset, also showed a relatively high CLIPScore of 0.7782, placing it in the HC group as well. In contrast, Flowers102  [ 19 ] , another fine-grained dataset, has an average CLIPScore of 0.5548, placing it in the Low CLIPScore (LC) group.",
            "Tab.   1  shows that the diversity of synthetic images generated by our method is generally higher compared to Real Guidance and DA-Fusion. This indicates that our method can supplement diversity with only one image per class compared to existing methods. In the HC dataset, DALDA shows outstanding performance, demonstrating its ability to generate synthetic images that are diverse and rich in information. This underscores its utility as an invaluable resource for training classifiers, especially when data availability is constrained. However, as detailed in  Sec.   3.2 , when dealing with image prompts with low CLIPScore, the strategy shifts towards minimizing diversity to mitigate the risk of generating inaccurate samples. Consequently, in the LC dataset, DALDA utilizing LLM Prompt and AGS tends to limit diversity more than other methods applying RS. This cautious approach ensures that the generated sample closely fits the target distribution, aiming to maintain alignment with the target distribution, even if it does not always lead to higher accuracy."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  1-shot classification accuracy of classifiers trained on synthetic images.",
        "table": "S4.T2.4.1",
        "footnotes": [],
        "references": [
            "In  Eq.   1 ,  Q Q Q italic_Q  is a matrix derived from query features  Z Z Z italic_Z  and weights  W q subscript W q W_{q} italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT .  K t subscript K t K_{t} italic_K start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and  V t subscript V t V_{t} italic_V start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  are matrices obtained from text features  c t subscript c t c_{t} italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and their respective weights  W k  t subscript W k t W_{kt} italic_W start_POSTSUBSCRIPT italic_k italic_t end_POSTSUBSCRIPT  and  W v  t subscript W v t W_{vt} italic_W start_POSTSUBSCRIPT italic_v italic_t end_POSTSUBSCRIPT , while  K i subscript K i K_{i} italic_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and  V i subscript V i V_{i} italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  are matrices derived from image features  c i subscript c i c_{i} italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and their respective weights  W k  i subscript W k i W_{ki} italic_W start_POSTSUBSCRIPT italic_k italic_i end_POSTSUBSCRIPT  and  W v  i subscript W v i W_{vi} italic_W start_POSTSUBSCRIPT italic_v italic_i end_POSTSUBSCRIPT . The weight factor denoted by    \\lambda italic_ , assigns weights to image features. A higher value of    \\lambda italic_  increases the weight of image guidance in the inference stage, while a lower value of    \\lambda italic_  relatively increases the weight of text guidance ( Fig.   2 ).",
            "Tab.   1  shows that the diversity of synthetic images generated by our method is generally higher compared to Real Guidance and DA-Fusion. This indicates that our method can supplement diversity with only one image per class compared to existing methods. In the HC dataset, DALDA shows outstanding performance, demonstrating its ability to generate synthetic images that are diverse and rich in information. This underscores its utility as an invaluable resource for training classifiers, especially when data availability is constrained. However, as detailed in  Sec.   3.2 , when dealing with image prompts with low CLIPScore, the strategy shifts towards minimizing diversity to mitigate the risk of generating inaccurate samples. Consequently, in the LC dataset, DALDA utilizing LLM Prompt and AGS tends to limit diversity more than other methods applying RS. This cautious approach ensures that the generated sample closely fits the target distribution, aiming to maintain alignment with the target distribution, even if it does not always lead to higher accuracy.",
            "Additionally, we measure the accuracy of classifiers trained on synthetic images, as shown in  Tab.   2 . While DALDA does not outperform in the LC dataset, it consistently shows robust performance and achieves remarkable results in the HC dataset without additional fine-tuning. Additional statistical tests are provided in  Tab.   3  of  Appendix   0.A ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Comparison of average accuracy with baselines in 1-shot setting. All values in the table represent the average of 3 trials, with the standard deviation included.",
        "table": "Pt0.A1.T3.12.12",
        "footnotes": [],
        "references": [
            "In  Fig.   3 , the generated text prompts show that the LLM can create class-centric sentences that are realistic, stay within the provided information, and maintain the semantic characteristics of each class while generating scenarios in various environments. Details of the hyperparameters and actual prompts are reported in the supplementary material.",
            "To demonstrate that our method can generate diverse synthetic images in data-scarce scenarios, we measure the accuracy in few-shot classification tasks. We recognize the importance of accurate alignment between real images and class names in the generation of synthetic data. For a comprehensive and fair analysis, we select a subset of few-shot datasets from those used in  [ 10 ] , based on their CLIPScores. These CLIPScores are calculated using the same methodology described in  Sec.   3.1 . Each dataset exhibits various distributions in  Fig.   4 ; Caltech-101  [ 7 ] , which belongs to the common dataset, recorded a high average CLIPScore of 0.8406, categorizing it as a High CLIPScore (HC) group. Oxford Pets  [ 21 ] , which belongs to the fine-grained dataset, also showed a relatively high CLIPScore of 0.7782, placing it in the HC group as well. In contrast, Flowers102  [ 19 ] , another fine-grained dataset, has an average CLIPScore of 0.5548, placing it in the Low CLIPScore (LC) group.",
            "Tab.   1  shows that the diversity of synthetic images generated by our method is generally higher compared to Real Guidance and DA-Fusion. This indicates that our method can supplement diversity with only one image per class compared to existing methods. In the HC dataset, DALDA shows outstanding performance, demonstrating its ability to generate synthetic images that are diverse and rich in information. This underscores its utility as an invaluable resource for training classifiers, especially when data availability is constrained. However, as detailed in  Sec.   3.2 , when dealing with image prompts with low CLIPScore, the strategy shifts towards minimizing diversity to mitigate the risk of generating inaccurate samples. Consequently, in the LC dataset, DALDA utilizing LLM Prompt and AGS tends to limit diversity more than other methods applying RS. This cautious approach ensures that the generated sample closely fits the target distribution, aiming to maintain alignment with the target distribution, even if it does not always lead to higher accuracy.",
            "Additionally, we measure the accuracy of classifiers trained on synthetic images, as shown in  Tab.   2 . While DALDA does not outperform in the LC dataset, it consistently shows robust performance and achieves remarkable results in the HC dataset without additional fine-tuning. Additional statistical tests are provided in  Tab.   3  of  Appendix   0.A .",
            "To provide a clearer analysis of the main results, we conducted additional statistical tests comparing our method with the baselines in  Tab.   3 . In the high CLIPScore (HC) datasets, our method shows an average improvement of +11.42% in Acc-RN50 and +0.72% in Acc-CLIP compared to Real Guidance (RG), and +13.87% and +0.64% compared to DA-Fusion (DF). These improvements are also statistically significant, with p-values of p<0.01 when compared to baselines. In contrast, in the low CLIPScore (LC) datasets, our method still shows an average improvement of +1.70% in Acc-RN50 and +0.14% in Acc-CLIP compared to RG, but a slight decrease of -0.51% in Acc-RN50 and -1.43% in Acc-CLIP when compared to DF. However, the improvement over RG in LC datasets remains statistically significant (p<0.05), and the differences compared to DF are not statistically significant (p>0.1). These results demonstrate that our method provides significant performance improvements over RG across all datasets. Additionally, while DF requires a fine-tuning stage that takes longer than generating all of our synthetic images, our approach still outperforms it in HC and shows only minimal performance differences in LC, further highlighting the practicality of our method."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  Hyperparameters Settings.",
        "table": "Pt0.A2.T4.7",
        "footnotes": [],
        "references": [
            "To demonstrate that our method can generate diverse synthetic images in data-scarce scenarios, we measure the accuracy in few-shot classification tasks. We recognize the importance of accurate alignment between real images and class names in the generation of synthetic data. For a comprehensive and fair analysis, we select a subset of few-shot datasets from those used in  [ 10 ] , based on their CLIPScores. These CLIPScores are calculated using the same methodology described in  Sec.   3.1 . Each dataset exhibits various distributions in  Fig.   4 ; Caltech-101  [ 7 ] , which belongs to the common dataset, recorded a high average CLIPScore of 0.8406, categorizing it as a High CLIPScore (HC) group. Oxford Pets  [ 21 ] , which belongs to the fine-grained dataset, also showed a relatively high CLIPScore of 0.7782, placing it in the HC group as well. In contrast, Flowers102  [ 19 ] , another fine-grained dataset, has an average CLIPScore of 0.5548, placing it in the Low CLIPScore (LC) group.",
            "Implementation Details.  For all experiments on classification tasks, we use  ResNet50   [ 9 ]  pre-trained on ImageNet-1k and  CLIP-ViT-B/16 . For each training sample, the number of text prompts  M = 10 M 10 M=10 italic_M = 10 . For training the classifier, we sample the real training images and the generated images with a uniform distribution. More details on hyperparameters can be found  Tab.   4  in Appendix  0.B ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 :  Examples of dataset description. Each description was partially taken from the official websites of the respective datasets.",
        "table": "Pt0.A3.T5.4.1",
        "footnotes": [],
        "references": [
            "While the previous methods help the generated data to stay within the target distribution, they may excessively limit diversity. Additionally, these methods may struggle to supplement new semantic information beyond the real image, especially when using CLIP  [ 22 ]  templates like  a photo of a {class} . This can be effective in maintaining the consistency of synthetic images, but it can be challenging to supplement various patterns that can be expressed with its class (see  Fig.   5 ).",
            "For example, a low CLIPScore indicates that the real image and its class are not appropriately matched within the CLIP embedding. This suggests that, at the stage of generating synthetic images, even if the weight of the image prompt is decreased and the weight of the text prompt is increased, the MMDM is likely to fail in appropriately reflecting the text prompt (see red box in  Fig.   5(b) ). Conversely, a high CLIPScore indicates that the real image and class are well-matched, enabling the generation of synthetic images that accurately reflect the text prompt in the image prompt (see  Fig.   5(a) ).",
            "In  Fig.   5 , we conduct a qualitative comparison of our method, including the baselines. The visualization results analyze how synthetic images are generated for example images with low and high CLIPScore, demonstrating the effectiveness of our method in each case. Both Real Guidance and DA-Fusion fall short of the other methods in terms of diversity across the examples. Our final method, LLM Prompt + AGS, exhibits the highest diversity in high CLIPScore examples compared to the existing methods, while it shows the lowest diversity in low CLIPScore examples, excluding the two prior methods. This visually demonstrates that our intended AGS is designed to produce images with greater diversity in high CLIPScore examples while reducing diversity in low CLIPScore examples. When using high CLIPScore examples as image prompts ( Fig.   5(a) ), AGS increases the diversity of synthetic images. Conversely, when using low CLIPScore examples as image prompts ( Fig.   5(b) ), AGS decreases diversity to keep the synthetic images within the target distribution."
        ]
    }
}