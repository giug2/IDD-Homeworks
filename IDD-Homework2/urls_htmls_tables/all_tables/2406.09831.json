{
    "PAPER'S NUMBER OF TABLES": 5,
    "S3.T1": {
        "caption": "Table 1: Overview of Federated Large Language Model Architectures",
        "table": "<table id=\"S3.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S3.T1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">No.</span></th>\n<td id=\"S3.T1.1.1.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\">\n<span id=\"S3.T1.1.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.1.1.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\"><span id=\"S3.T1.1.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Contributions</span></span>\n</span>\n</td>\n<td id=\"S3.T1.1.1.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\">\n<span id=\"S3.T1.1.1.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.1.1.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\"><span id=\"S3.T1.1.1.1.3.1.1.1\" class=\"ltx_text ltx_font_bold\">Technique</span></span>\n</span>\n</td>\n<td id=\"S3.T1.1.1.1.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\">\n<span id=\"S3.T1.1.1.1.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.1.1.4.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\"><span id=\"S3.T1.1.1.1.4.1.1.1\" class=\"ltx_text ltx_font_bold\">LM used</span></span>\n</span>\n</td>\n<td id=\"S3.T1.1.1.1.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\">\n<span id=\"S3.T1.1.1.1.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.1.1.5.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\"><span id=\"S3.T1.1.1.1.5.1.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></span>\n</span>\n</td>\n<td id=\"S3.T1.1.1.1.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span id=\"S3.T1.1.1.1.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.1.1.6.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\"><span id=\"S3.T1.1.1.1.6.1.1.1\" class=\"ltx_text ltx_font_bold\">Limitations</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">1</th>\n<td id=\"S3.T1.1.2.2.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.2.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.2.2.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Implementation of N-gram models in federated settings, dealing with distributed data.</span>\n</span>\n</td>\n<td id=\"S3.T1.1.2.2.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.2.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.2.2.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Federated learning</span>\n</span>\n</td>\n<td id=\"S3.T1.1.2.2.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.2.2.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.2.2.4.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">N-gram RNN</span>\n</span>\n</td>\n<td id=\"S3.T1.1.2.2.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.2.2.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.2.2.5.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Not specified</span>\n</span>\n</td>\n<td id=\"S3.T1.1.2.2.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.2.2.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.2.2.6.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">High communication overhead, data privacy concerns.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.3.3\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">2</th>\n<td id=\"S3.T1.1.3.3.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.3.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.3.3.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Enhances privacy and reduces server communication through localized computations.</span>\n</span>\n</td>\n<td id=\"S3.T1.1.3.3.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.3.3.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.3.3.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Federated Reconstruction</span>\n</span>\n</td>\n<td id=\"S3.T1.1.3.3.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.3.3.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.3.3.4.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">LSTM-based</span>\n</span>\n</td>\n<td id=\"S3.T1.1.3.3.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.3.3.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.3.3.5.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">MovieLens</span>\n</span>\n</td>\n<td id=\"S3.T1.1.3.3.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.3.3.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.3.3.6.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Complexity in local computations, potential performance trade-offs.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.4.4\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">3</th>\n<td id=\"S3.T1.1.4.4.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.4.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.4.4.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Framework for training and deploying LLMs in federated settings, addressing scalability and efficiency.</span>\n</span>\n</td>\n<td id=\"S3.T1.1.4.4.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.4.4.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.4.4.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">FederatedScope</span>\n</span>\n</td>\n<td id=\"S3.T1.1.4.4.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.4.4.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.4.4.4.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">LLaMa-7B</span>\n</span>\n</td>\n<td id=\"S3.T1.1.4.4.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.4.4.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.4.4.5.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">HumanEval, HELM, etc</span>\n</span>\n</td>\n<td id=\"S3.T1.1.4.4.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.4.4.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.4.4.6.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Scalability issues, computational demands.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.5.5\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">4</th>\n<td id=\"S3.T1.1.5.5.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.5.5.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.5.5.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Integration of LLMs with edge computing for autonomous decision-making in connected environments.</span>\n</span>\n</td>\n<td id=\"S3.T1.1.5.5.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.5.5.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.5.5.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Edge AI, Federated Learning</span>\n</span>\n</td>\n<td id=\"S3.T1.1.5.5.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.5.5.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.5.5.4.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">GPT-3</span>\n</span>\n</td>\n<td id=\"S3.T1.1.5.5.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.5.5.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.5.5.5.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Synthetic challenging user request dataset</span>\n</span>\n</td>\n<td id=\"S3.T1.1.5.5.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.5.5.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.5.5.6.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Resource constraints on edge devices, deployment challenges.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.6.6\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.6.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\">5</th>\n<td id=\"S3.T1.1.6.6.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.6.6.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.6.6.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Techniques for reducing LLM parameter count in federated settings, ensuring efficiency.</span>\n</span>\n</td>\n<td id=\"S3.T1.1.6.6.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.6.6.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.6.6.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Model pruning, Knowledge distillation</span>\n</span>\n</td>\n<td id=\"S3.T1.1.6.6.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.6.6.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.6.6.4.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Bert-based</span>\n</span>\n</td>\n<td id=\"S3.T1.1.6.6.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.6.6.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.6.6.5.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">IMDP, Yelp</span>\n</span>\n</td>\n<td id=\"S3.T1.1.6.6.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\">\n<span id=\"S3.T1.1.6.6.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.6.6.6.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Compromise on model performance, complexity in model reduction techniques.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Chen et al. ",
                "[",
                "23",
                "]",
                " explore the adaptation of N-gram language models to federated learning environments. It discusses the challenges of maintaining model performance when training data is distributed and not centrally stored. The primary focus is on how to effectively aggregate learned N-grams from multiple nodes while preserving privacy and minimizing communication overhead.",
                "Singhal et al. ",
                "[",
                "24",
                "]",
                " introduce a novel approach named ”Federated Reconstruction,” which enhances privacy in federated learning by allowing more computation to be performed locally. The paper particularly emphasizes its application to large language models, discussing how it can reduce the amount of data required to be shared with the server, thus enhancing data privacy and reducing bandwidth requirements.",
                "Kuang et al. ",
                "[",
                "25",
                "]",
                " present FEDERATEDSCOPE, a framework designed to facilitate the training and deployment of large language models in a federated setting. It offers insights into the architectural adjustments and optimizations necessary to handle the complexities associated with training large models across decentralized data sources. The paper also explores scalability and efficiency challenges, providing solutions to maintain robust model performance.",
                "Shen et al. ",
                "[",
                "26",
                "]",
                " discusses the integration of large language models (LLMs) with edge computing devices in a federated learning context. It focuses on how LLMs can empower edge devices for autonomous decision-making, highlighting the potential for LLMs to enhance connected intelligence across various applications. The paper addresses the challenges of deploying complex models on resource-constrained edge devices.",
                "Focusing on the issue of model size, Jiang et al. ",
                "[",
                "27",
                "]",
                " propose techniques for reducing the parameter count of large language models in federated settings. It explores methods like model pruning and knowledge distillation to maintain model efficacy with fewer parameters, which is critical for efficient data transmission and quick adaptation in federated networks.",
                "These papers collectively cover a spectrum of considerations for deploying large language models in federated learning environments. Starting from the basic implementation of N-gram models in a federated manner, the survey transitions to advanced strategies like Federated Reconstruction for enhancing privacy and computational efficiency. It then moves into a discussion on frameworks and architectural adjustments necessary for scaling LLMs across decentralized networks. The integration with edge AI represents an application-focused advancement, providing a practical viewpoint on deploying reduced-parameter models in real-world settings. This logical progression from foundational concepts to practical implementations and optimizations illustrates a comprehensive view of current research in the domain of federated large language models.",
                "The integration of large language models (LLMs) in federated settings, as explored in these papers, significantly advances the field of swarm intelligence by leveraging the collective learning capabilities of decentralized networks. The first paper’s exploration of N-gram models in federated environments lays the foundational understanding of how individual nodes, like agents in a swarm, can contribute localized knowledge to build a cohesive and comprehensive model. The introduction of ",
                "[",
                "23",
                "]",
                " in the second paper enhances this concept by minimizing central coordination, thereby promoting more autonomous local decision-making—akin to the independent yet coordinated behavior seen in natural swarms. The  ",
                "[",
                "24",
                "]",
                " framework further capitalizes on this by scaling up the capabilities of LLMs across diverse and distributed datasets, mirroring how swarms adapt to varied environments. ",
                "[",
                "25",
                "]",
                " brings these concepts into the realm of edge computing, where LLMs empower edge devices to function like intelligent agents that perform tasks and make decisions in real time, enhancing the swarm’s overall responsiveness and flexibility. Lastly, the techniques discussed in ",
                "[",
                "26",
                "]",
                " for reducing the parameter count of LLMs ensure that the computational load is manageable even in resource-constrained environments, which is crucial for maintaining the efficiency and agility of a swarm. Together, these studies demonstrate how federated learning models can emulate and enhance swarm-like intelligence, promoting robust, scalable, and decentralized problem-solving capabilities in artificial systems."
            ]
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Summary of Efficient Fine-Tuning of LLMs in Federated Settings",
        "table": "<table id=\"S3.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S3.T2.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">No.</span></th>\n<td id=\"S3.T2.1.1.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\">\n<span id=\"S3.T2.1.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.1.1.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\"><span id=\"S3.T2.1.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Contributions</span></span>\n</span>\n</td>\n<td id=\"S3.T2.1.1.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\">\n<span id=\"S3.T2.1.1.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.1.1.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\"><span id=\"S3.T2.1.1.1.3.1.1.1\" class=\"ltx_text ltx_font_bold\">Technique</span></span>\n</span>\n</td>\n<td id=\"S3.T2.1.1.1.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\">\n<span id=\"S3.T2.1.1.1.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.1.1.4.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\"><span id=\"S3.T2.1.1.1.4.1.1.1\" class=\"ltx_text ltx_font_bold\">LM used</span></span>\n</span>\n</td>\n<td id=\"S3.T2.1.1.1.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\">\n<span id=\"S3.T2.1.1.1.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.1.1.5.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\"><span id=\"S3.T2.1.1.1.5.1.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></span>\n</span>\n</td>\n<td id=\"S3.T2.1.1.1.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span id=\"S3.T2.1.1.1.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.1.1.6.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\"><span id=\"S3.T2.1.1.1.6.1.1.1\" class=\"ltx_text ltx_font_bold\">Limitations</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">1</th>\n<td id=\"S3.T2.1.2.2.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.2.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.2.2.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Integrates differential privacy efficiently in federated LLM training with minimal performance loss.</span>\n</span>\n</td>\n<td id=\"S3.T2.1.2.2.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.2.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.2.2.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Differential privacy, noise addition, parameter clipping</span>\n</span>\n</td>\n<td id=\"S3.T2.1.2.2.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.2.2.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.2.2.4.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">CIFG 19M, Transformer 21M, etc.</span>\n</span>\n</td>\n<td id=\"S3.T2.1.2.2.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.2.2.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.2.2.5.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">WordPiece</span>\n</span>\n</td>\n<td id=\"S3.T2.1.2.2.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.2.2.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.2.2.6.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Potential impact on model accuracy, complexity in tuning privacy parameters.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.3.3\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">2</th>\n<td id=\"S3.T2.1.3.3.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.3.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.3.3.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Utilizes prompt tuning and adaptive optimization to reduce parameters and adapt learning rates dynamically.</span>\n</span>\n</td>\n<td id=\"S3.T2.1.3.3.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.3.3.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.3.3.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Prompt tuning, adaptive optimization</span>\n</span>\n</td>\n<td id=\"S3.T2.1.3.3.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.3.3.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.3.3.4.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">GPT-2, LLaMa, etc.</span>\n</span>\n</td>\n<td id=\"S3.T2.1.3.3.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.3.3.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.3.3.5.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">MRPC</span>\n</span>\n</td>\n<td id=\"S3.T2.1.3.3.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.3.3.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.3.3.6.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">May not reach the full model’s potential, depends heavily on prompt design.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.4.4\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">3</th>\n<td id=\"S3.T2.1.4.4.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.4.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.4.4.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Optimizes prompt tuning in federated settings with synchronization for stable and accurate model updates.</span>\n</span>\n</td>\n<td id=\"S3.T2.1.4.4.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.4.4.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.4.4.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Federated prompt tuning, synchronization mechanisms</span>\n</span>\n</td>\n<td id=\"S3.T2.1.4.4.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.4.4.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.4.4.4.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Roberta</span>\n</span>\n</td>\n<td id=\"S3.T2.1.4.4.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.4.4.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.4.4.5.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">GLUE</span>\n</span>\n</td>\n<td id=\"S3.T2.1.4.4.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.4.4.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.4.4.6.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Challenges in synchronization across diverse networks, prompt design limitations.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.5.5\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">4</th>\n<td id=\"S3.T2.1.5.5.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.5.5.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.5.5.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Benchmarks federated learning models and systems at scale, identifying and addressing bottlenecks.</span>\n</span>\n</td>\n<td id=\"S3.T2.1.5.5.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.5.5.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.5.5.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Benchmarking, model compression, robust aggregation methods</span>\n</span>\n</td>\n<td id=\"S3.T2.1.5.5.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.5.5.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.5.5.4.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Albert</span>\n</span>\n</td>\n<td id=\"S3.T2.1.5.5.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.5.5.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.5.5.5.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Reddit</span>\n</span>\n</td>\n<td id=\"S3.T2.1.5.5.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.5.5.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.5.5.6.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Scaling issues, may require significant computational resources.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.6.6\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.6.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\">5</th>\n<td id=\"S3.T2.1.6.6.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.6.6.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.6.6.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Reduces data transmission with forward gradient method, enhancing privacy and efficiency.</span>\n</span>\n</td>\n<td id=\"S3.T2.1.6.6.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.6.6.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.6.6.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Forward gradient technique, efficient gradient aggregation</span>\n</span>\n</td>\n<td id=\"S3.T2.1.6.6.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.6.6.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.6.6.4.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">LLaMa, Roberta, etc.</span>\n</span>\n</td>\n<td id=\"S3.T2.1.6.6.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.6.6.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.6.6.5.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">AGNEWS, Yelp, etc.</span>\n</span>\n</td>\n<td id=\"S3.T2.1.6.6.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\">\n<span id=\"S3.T2.1.6.6.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.6.6.6.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Potential loss of gradient information, requires careful implementation.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Ro et al. ",
                "[",
                "28",
                "]",
                " address the challenges of applying differential privacy in federated settings, particularly when fine-tuning large language models (LLMs). It introduces a novel architecture that balances the trade-offs between privacy, efficiency, and performance. The proposed model uses lightweight mechanisms to ensure differential privacy without significantly degrading the model’s utility. Key techniques include noise addition and parameter clipping during the training process to maintain privacy. The architecture is designed to be modular, allowing easy integration with existing federated learning frameworks. This study is pivotal as it demonstrates how privacy considerations can be seamlessly integrated into the federated fine-tuning of LLMs, ensuring that user data remains confidential while still contributing to the collective learning process.",
                "Che et al. ",
                "[",
                "29",
                "]",
                " explore the use of prompt tuning as a parameter-efficient method for fine-tuning large language models in federated learning environments. Prompt tuning involves adjusting a small set of parameters (prompts) while keeping the majority of the model fixed, significantly reducing the computational and communication overhead typically associated with training large models. Additionally, the paper introduces an adaptive optimization technique that dynamically adjusts learning rates based on the network conditions and convergence rates of the participating nodes. This approach not only enhances the efficiency of the federated learning process but also ensures that the model adapts optimally across diverse and potentially noisy datasets.",
                "Building on the concept of prompt tuning, Zhang et al. ",
                "[",
                "10",
                "]",
                " specifically tailor this technique for federated settings. The paper presents a framework that optimizes the deployment of prompt tuning across a distributed network, ensuring that all nodes contribute effectively to the model’s learning process without overwhelming the network’s bandwidth. The framework also includes a novel synchronization mechanism that aligns the updates from all nodes to improve the overall stability and accuracy of the model. By focusing on the efficient distribution and synchronization of prompts, this study further refines the practical application of prompt tuning in federated environments.",
                "Lai et al. ",
                "[",
                "30",
                "]",
                " offer a comprehensive benchmarking study that evaluates the performance of various federated learning models, including LLMs, across different scales and settings. The paper identifies key bottlenecks in scaling federated learning systems and proposes solutions to overcome them. Among the highlighted solutions are strategies for efficient data sampling and distribution, model compression techniques, and robust aggregation methods that can handle large-scale deployments. This benchmarking is crucial for understanding how different models and systems perform in real-world scenarios, providing a foundation for further optimization of federated learning frameworks.",
                "Xu et al. ",
                "[",
                "31",
                "]",
                " introduce a forward gradient technique that optimizes the gradient computation and transmission in federated learning of LLMs. By calculating and sharing forward gradients instead of the full gradients, this method significantly reduces the amount of data that needs to be transmitted between nodes and the central server. This is particularly beneficial in scenarios with limited bandwidth or where data privacy is a concern. The technique also includes mechanisms for aggregating these gradients efficiently, ensuring that the model converges quickly without compromising on performance.",
                "Together, these papers present a comprehensive view of current innovations in the efficient fine-tuning of large language models within federated learning frameworks. Starting from foundational privacy-preserving techniques, the discussion progresses through enhancements in parameter efficiency via prompt tuning, system-wide optimizations for handling scale, and innovative gradient management for improved communication efficiency. Each paper contributes to a layered understanding of how to optimize the training and deployment of LLMs across distributed networks, ensuring both performance and practical viability in real-world applications. This narrative arc not only highlights individual advancements but also illustrates the synergistic potential of combining these techniques to address the multifaceted challenges of federated learning.",
                "In the realm of swarm intelligence within federated learning environments, the contributions of these papers are pivotal in demonstrating how decentralized systems can collaboratively refine and fine-tune large language models (LLMs) efficiently. ",
                "[",
                "28",
                "]",
                " introduces an architecture that incorporates differential privacy directly into the federated training process, reflecting the swarm intelligence concept of achieving collective goals while maintaining individual privacy. ",
                "[",
                "29",
                "]",
                " and ",
                "[",
                "10",
                "]",
                " further the narrative by focusing on prompt tuning—modifying a small subset of model parameters—thus reducing the complexity and computational load of federating large-scale models. This approach allows each node, akin to an agent in a swarm, to contribute more effectively and efficiently to the collective intelligence, optimizing the system’s overall learning capability.",
                "[",
                "30",
                "]",
                " provides benchmarking insights that help understand the performance and system constraints when scaling up federated learning—akin to assessing the operational capacity of a swarm over varied environments. Lastly, ",
                "[",
                "31",
                "]",
                " enhances the communication efficiency between nodes using a forward gradient method, significantly reducing the bandwidth needed for model updates. This mirrors a swarm’s efficiency in using limited resources to maintain robust communication across the group. Collectively, these studies exemplify how federated learning can harness swarm intelligence principles to achieve decentralized problem-solving and learning, effectively managing resources and maintaining synchronization across distributed agents to optimize collective outcomes in the training and deployment of LLMs."
            ]
        ]
    },
    "S3.T3": {
        "caption": "Table 3: Pre-Training of LLM in Federated Learning",
        "table": "<table id=\"S3.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S3.T3.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">No.</span></th>\n<td id=\"S3.T3.1.1.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\">\n<span id=\"S3.T3.1.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.1.1.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\"><span id=\"S3.T3.1.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Contributions</span></span>\n</span>\n</td>\n<td id=\"S3.T3.1.1.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\">\n<span id=\"S3.T3.1.1.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.1.1.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\"><span id=\"S3.T3.1.1.1.3.1.1.1\" class=\"ltx_text ltx_font_bold\">Technique</span></span>\n</span>\n</td>\n<td id=\"S3.T3.1.1.1.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\">\n<span id=\"S3.T3.1.1.1.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.1.1.4.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\"><span id=\"S3.T3.1.1.1.4.1.1.1\" class=\"ltx_text ltx_font_bold\">LM used</span></span>\n</span>\n</td>\n<td id=\"S3.T3.1.1.1.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\">\n<span id=\"S3.T3.1.1.1.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.1.1.5.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\"><span id=\"S3.T3.1.1.1.5.1.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></span>\n</span>\n</td>\n<td id=\"S3.T3.1.1.1.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span id=\"S3.T3.1.1.1.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.1.1.6.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\"><span id=\"S3.T3.1.1.1.6.1.1.1\" class=\"ltx_text ltx_font_bold\">Limitations</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T3.1.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">1</th>\n<td id=\"S3.T3.1.2.2.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.1.2.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.2.2.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Adapts BERT pre-training to federated learning, maintaining data privacy across distributed datasets.</span>\n</span>\n</td>\n<td id=\"S3.T3.1.2.2.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.1.2.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.2.2.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Federated learning adaptations, privacy enhancements</span>\n</span>\n</td>\n<td id=\"S3.T3.1.2.2.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.1.2.2.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.2.2.4.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">BERT</span>\n</span>\n</td>\n<td id=\"S3.T3.1.2.2.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.1.2.2.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.2.2.5.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">GLUE</span>\n</span>\n</td>\n<td id=\"S3.T3.1.2.2.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T3.1.2.2.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.2.2.6.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Uneven data distribution, increased complexity in training.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T3.1.3.3\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">2</th>\n<td id=\"S3.T3.1.3.3.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.1.3.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.3.3.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Explores practical deployment of pretrained models in federated learning, addressing data heterogeneity.</span>\n</span>\n</td>\n<td id=\"S3.T3.1.3.3.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.1.3.3.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.3.3.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Dynamic update rates, selective parameter updating</span>\n</span>\n</td>\n<td id=\"S3.T3.1.3.3.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.1.3.3.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.3.3.4.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">DistiBERT, BART</span>\n</span>\n</td>\n<td id=\"S3.T3.1.3.3.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.1.3.3.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.3.3.5.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">SST2, OntoNotes, etc</span>\n</span>\n</td>\n<td id=\"S3.T3.1.3.3.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T3.1.3.3.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.3.3.6.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Sync issues, managing diverse data distributions.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T3.1.4.4\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">3</th>\n<td id=\"S3.T3.1.4.4.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.1.4.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.4.4.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Enhances multilingual understanding in federated learning using pretrained models across diverse languages.</span>\n</span>\n</td>\n<td id=\"S3.T3.1.4.4.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.1.4.4.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.4.4.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Multilingual model adaptation</span>\n</span>\n</td>\n<td id=\"S3.T3.1.4.4.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.1.4.4.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.4.4.4.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Multilingual BERT</span>\n</span>\n</td>\n<td id=\"S3.T3.1.4.4.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.1.4.4.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.4.4.5.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">MTNT</span>\n</span>\n</td>\n<td id=\"S3.T3.1.4.4.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T3.1.4.4.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.4.4.6.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Requires careful handling of linguistic diversity, potential bias in language representation.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T3.1.5.5\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\">4</th>\n<td id=\"S3.T3.1.5.5.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.1.5.5.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.5.5.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Reduces communication overhead by employing parameter-efficient fine-tuning techniques in federated settings.</span>\n</span>\n</td>\n<td id=\"S3.T3.1.5.5.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.1.5.5.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.5.5.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Layer-wise relevance propagation, parameter-efficient fine-tuning</span>\n</span>\n</td>\n<td id=\"S3.T3.1.5.5.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.1.5.5.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.5.5.4.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Bert-based</span>\n</span>\n</td>\n<td id=\"S3.T3.1.5.5.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.1.5.5.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.5.5.5.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">GLUE</span>\n</span>\n</td>\n<td id=\"S3.T3.1.5.5.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\">\n<span id=\"S3.T3.1.5.5.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.5.5.6.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Possible reduction in model accuracy, limited updates to model parameters.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Tian et al. ",
                "[",
                "32",
                "]",
                " explore the feasibility of applying federated learning to the pre-training phase of BERT (Bidirectional Encoder Representations from Transformers), a popular language model. Named FedBERT, this approach adapts the pre-training process to work across distributed data sets without compromising data privacy. The study demonstrates how federated learning can be effectively utilized to pre-train BERT with data from multiple decentralized sources, ensuring that the private data remains local while still contributing to the training of a robust model. The paper highlights modifications to the BERT training algorithm to accommodate the federated setting, including adjustments to handle the uneven distribution of data across nodes.",
                "Focusing on the practical aspects, Agarwal et al. ",
                "[",
                "33",
                "]",
                " provide a detailed examination of deploying pretrained language models in a federated learning framework. It discusses the challenges and solutions related to synchronization, data heterogeneity, and maintaining model performance when the pretrained model is adapted to new datasets across distributed environments. The paper proposes several optimization techniques to enhance the efficiency and effectiveness of federated learning for pretrained models, such as dynamic update rates and selective parameter updating to cope with the diverse data distributions typically encountered in federated settings.",
                "Addressing the challenge of multilingual contexts, Weller et al. ",
                "[",
                "34",
                "]",
                " introduce techniques for leveraging pretrained language models to enhance language understanding across different languages in a federated learning scenario. It explores the application of multilingual BERT models, adapting them to federated settings to improve model training across linguistically diverse data. This approach not only broadens the applicability of federated learning but also enhances the inclusivity of language technologies, allowing for effective model training without centralizing data from various language communities.",
                "Malaviya et al. ",
                "[",
                "35",
                "]",
                " address one of the significant challenges in federated learning: the high communication overhead. It presents strategies for reducing bandwidth consumption during the pre-training of language models in federated setups. By employing parameter-efficient techniques such as layer-wise relevance propagation and other fine-tuning methods that minimize the number of parameters that need to be updated and transmitted, the paper successfully decreases the network load, which is crucial for practical implementations of federated learning where bandwidth may be limited.",
                "These papers collectively cover a spectrum of methodologies for integrating pre-training phases of large language models with federated learning principles. Starting from adapting existing models like BERT to federated settings, moving through practical deployment considerations, addressing multilingual training needs, reducing communication overhead, and finally discussing comprehensive lifecycle approaches, these studies showcase a progressive enhancement in the techniques and applications of federated learning. Each paper builds on the notion that federated learning can be effectively scaled and adapted to pre-train language models in a way that respects privacy, manages resources efficiently, and embraces linguistic diversity, thus advancing the field towards more inclusive and technologically feasible solutions.",
                "The four papers significantly advance swarm intelligence principles in federated learning by demonstrating how large language models can be collaboratively pre-trained across distributed nodes. Each paper introduces a different facet of decentralized intelligence: ",
                "[",
                "32",
                "]",
                " adapts BERT for privacy-preserving collaborative training, ",
                "[",
                "33",
                "]",
                " tackles practical deployment challenges such as dynamic updating to handle data heterogeneity, ",
                "[",
                "34",
                "]",
                " extends federated learning to multilingual contexts enhancing linguistic inclusivity, and ",
                "[",
                "35",
                "]",
                " reduces communication overhead with parameter-efficient techniques. Together, these studies exemplify how federated learning can emulate swarm behavior, optimizing collaborative tasks through decentralized interactions and contributing to the development of robust, scalable models without centralized control."
            ]
        ]
    },
    "S3.T4": {
        "caption": "Table 4: Scalable LLM via Federated Learning",
        "table": "<table id=\"S3.T4.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T4.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S3.T4.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">No.</span></th>\n<td id=\"S3.T4.1.1.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\">\n<span id=\"S3.T4.1.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.1.1.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\"><span id=\"S3.T4.1.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Contributions</span></span>\n</span>\n</td>\n<td id=\"S3.T4.1.1.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\">\n<span id=\"S3.T4.1.1.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.1.1.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\"><span id=\"S3.T4.1.1.1.3.1.1.1\" class=\"ltx_text ltx_font_bold\">Technique</span></span>\n</span>\n</td>\n<td id=\"S3.T4.1.1.1.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\">\n<span id=\"S3.T4.1.1.1.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.1.1.4.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\"><span id=\"S3.T4.1.1.1.4.1.1.1\" class=\"ltx_text ltx_font_bold\">LM used</span></span>\n</span>\n</td>\n<td id=\"S3.T4.1.1.1.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\">\n<span id=\"S3.T4.1.1.1.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.1.1.5.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\"><span id=\"S3.T4.1.1.1.5.1.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></span>\n</span>\n</td>\n<td id=\"S3.T4.1.1.1.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span id=\"S3.T4.1.1.1.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.1.1.6.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\"><span id=\"S3.T4.1.1.1.6.1.1.1\" class=\"ltx_text ltx_font_bold\">Limitations</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T4.1.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T4.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">1</th>\n<td id=\"S3.T4.1.2.2.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T4.1.2.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.2.2.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Develops FATE-LLM, a framework for scalable federated learning of LLMs, with robustness against node dropout and data discrepancies.</span>\n</span>\n</td>\n<td id=\"S3.T4.1.2.2.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T4.1.2.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.2.2.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Hierarchical aggregation</span>\n</span>\n</td>\n<td id=\"S3.T4.1.2.2.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T4.1.2.2.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.2.2.4.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">LLaMa, GPT-2, etc.</span>\n</span>\n</td>\n<td id=\"S3.T4.1.2.2.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T4.1.2.2.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.2.2.5.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">AdvertiseGen</span>\n</span>\n</td>\n<td id=\"S3.T4.1.2.2.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T4.1.2.2.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.2.2.6.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">May struggle with extremely large datasets or very high node dropout rates.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T4.1.3.3\" class=\"ltx_tr\">\n<th id=\"S3.T4.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">2</th>\n<td id=\"S3.T4.1.3.3.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T4.1.3.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.3.3.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Enhances federated learning from pre-trained models using contrastive learning to maintain model quality with non-IID data.</span>\n</span>\n</td>\n<td id=\"S3.T4.1.3.3.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T4.1.3.3.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.3.3.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Contrastive learning</span>\n</span>\n</td>\n<td id=\"S3.T4.1.3.3.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T4.1.3.3.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.3.3.4.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Not specified</span>\n</span>\n</td>\n<td id=\"S3.T4.1.3.3.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T4.1.3.3.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.3.3.5.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">DomainNet, Office-10, etc.</span>\n</span>\n</td>\n<td id=\"S3.T4.1.3.3.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T4.1.3.3.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.3.3.6.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Requires careful tuning to avoid negative impacts on convergence.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T4.1.4.4\" class=\"ltx_tr\">\n<th id=\"S3.T4.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">3</th>\n<td id=\"S3.T4.1.4.4.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T4.1.4.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.4.4.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Investigates the impact of large cohorts on federated learning efficiency, proposing dynamic cohort size adjustments.</span>\n</span>\n</td>\n<td id=\"S3.T4.1.4.4.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T4.1.4.4.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.4.4.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Dynamic cohort sizing</span>\n</span>\n</td>\n<td id=\"S3.T4.1.4.4.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T4.1.4.4.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.4.4.4.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Not specified</span>\n</span>\n</td>\n<td id=\"S3.T4.1.4.4.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T4.1.4.4.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.4.4.5.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Stack Overflow, SHAKESPEARE, etc.</span>\n</span>\n</td>\n<td id=\"S3.T4.1.4.4.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T4.1.4.4.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.4.4.6.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Communication overhead can still be significant in very large networks.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T4.1.5.5\" class=\"ltx_tr\">\n<th id=\"S3.T4.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">4</th>\n<td id=\"S3.T4.1.5.5.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T4.1.5.5.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.5.5.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Presents an architecture for efficiently fine-tuning LLMs in federated settings, reducing resource demands.</span>\n</span>\n</td>\n<td id=\"S3.T4.1.5.5.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T4.1.5.5.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.5.5.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Selective parameter updating</span>\n</span>\n</td>\n<td id=\"S3.T4.1.5.5.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T4.1.5.5.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.5.5.4.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">DistilBERT</span>\n</span>\n</td>\n<td id=\"S3.T4.1.5.5.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T4.1.5.5.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.5.5.5.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">IMDB, AG News, etc.</span>\n</span>\n</td>\n<td id=\"S3.T4.1.5.5.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T4.1.5.5.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.5.5.6.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Limited by the granularity of parameter selection and update mechanisms.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T4.1.6.6\" class=\"ltx_tr\">\n<th id=\"S3.T4.1.6.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\">5</th>\n<td id=\"S3.T4.1.6.6.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\">\n<span id=\"S3.T4.1.6.6.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.6.6.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Discusses methodologies for training larger models in cross-device federated learning through model splitting and layered updates.</span>\n</span>\n</td>\n<td id=\"S3.T4.1.6.6.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\">\n<span id=\"S3.T4.1.6.6.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.6.6.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Model splitting, layered updates</span>\n</span>\n</td>\n<td id=\"S3.T4.1.6.6.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\">\n<span id=\"S3.T4.1.6.6.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.6.6.4.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">LSTM, Transformer, etc.</span>\n</span>\n</td>\n<td id=\"S3.T4.1.6.6.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\">\n<span id=\"S3.T4.1.6.6.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.6.6.5.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Stack\nOverflow</span>\n</span>\n</td>\n<td id=\"S3.T4.1.6.6.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\">\n<span id=\"S3.T4.1.6.6.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.6.6.6.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">May face challenges in ensuring model consistency and managing update synchronization.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Fan et al. ",
                "[",
                "36",
                "]",
                " introduce FATE-LLM, a framework designed for training large language models using federated learning across highly distributed and heterogeneous environments. It focuses on overcoming the scalability challenges inherent in coordinating and aggregating updates across numerous devices. FATE-LLM employs a hierarchical aggregation strategy that reduces the communication burden and latency, enabling effective scalability without compromising the learning efficiency or model accuracy. The framework also includes robustness against node dropout and data discrepancies, making it particularly suitable for real-world applications where network conditions and data distributions can be highly variable.",
                "Tan et al. ",
                "[",
                "37",
                "]",
                " explore the use of contrastive learning techniques to enhance the effectiveness of federated learning when starting from pre-trained language models. By leveraging contrastive learning, the approach aims to maximize the relevance of local updates to the global model, thus enhancing the overall model performance even when individual clients have sparse or non-IID (non-independent and identically distributed) data. The paper discusses how this technique can be particularly useful in maintaining model quality while scaling up the number of participants in a federated learning setup, addressing common issues such as catastrophic forgetting and model divergence.",
                "Charles et al. ",
                "[",
                "38",
                "]",
                " investigate the effects of increasing the cohort size (the number of participating nodes in each training round) on the efficiency and effectiveness of federated learning systems. It proposes a method for dynamically adjusting the cohort size based on real-time assessments of network bandwidth and participant availability, optimizing resource allocation and training speed. The paper provides empirical evidence that larger cohorts can lead to faster convergence and improved model performance, provided that the communication and aggregation protocols are efficiently managed.",
                "Hilmkil et al. ",
                "[",
                "39",
                "]",
                " address the challenge of fine-tuning large pre-trained language models in a federated setting. It presents a novel architecture that allows for efficient distribution of model parameters and selective updating of those parameters most relevant to specific tasks or data types. The approach helps mitigate the high resource demands typically associated with large model fine-tuning, making it feasible to scale up federated learning to handle large language models across extensive networks of distributed nodes.",
                "Focusing on cross-device scenarios, Ro et al. ",
                "[",
                "40",
                "]",
                " explore methodologies to scale up the size of language models that can be effectively trained in a federated learning framework involving a wide array of devices. It introduces techniques such as model splitting and layered updates to manage the computational and memory constraints of devices, allowing even those with limited capabilities to participate in the training process. The paper emphasizes the scalability and flexibility of federated learning approaches in accommodating large models without requiring high-end hardware on each client device.",
                "These papers collectively advance the field of federated learning by addressing various aspects of scalability when training large language models. From hierarchical aggregation and contrastive learning approaches that enhance communication efficiency and model relevance, to dynamic cohort management and architectural innovations for fine-tuning, each paper contributes to overcoming the significant challenges of scaling LLMs in distributed environments. By implementing these methodologies, federated learning can harness the power of swarm intelligence, where many distributed agents (devices) work together to solve complex problems, leading to the development of more robust and scalable language models that can operate effectively across diverse and widespread data sources.",
                "These papers provide significant insights into how swarm intelligence principles can be applied to scale large language models (LLMs) in federated learning environments. Each paper introduces innovative techniques that enhance collaborative learning across distributed networks, reflecting key aspects of swarm behavior such as decentralized decision-making and collective problem-solving. The first paper ",
                "[",
                "36",
                "]",
                ", with its hierarchical aggregation framework, exemplifies how individual nodes can efficiently contribute to a global model without centralized oversight, much like how insects in a swarm interact locally with simple rules that lead to complex group behavior. ",
                "[",
                "37",
                "]",
                "’s use of contrastive learning further leverages local computations to maintain the relevance and accuracy of the global model, ensuring that each node’s update significantly contributes to the collective knowledge. ",
                "[",
                "38",
                "]",
                "’s dynamic cohort sizing adapts to changing network conditions and resources, optimizing the learning process similar to how swarms dynamically adjust to environmental changes. Lastly, the paper ",
                "[",
                "39",
                "]",
                " on model splitting and layered updates allows even resource-constrained devices to participate in the learning process, promoting inclusivity and resilience, akin to a swarm’s ability to adapt and thrive despite individual limitations. Collectively, these studies advance the implementation of swarm intelligence in federated settings, demonstrating how decentralized, collaborative efforts can lead to robust, scalable, and efficient outcomes in the training of sophisticated models."
            ]
        ]
    },
    "S3.T5": {
        "caption": "Table 5: Security and Privacy in Federated LLM",
        "table": "<table id=\"S3.T5.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T5.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T5.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S3.T5.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">No.</span></th>\n<td id=\"S3.T5.1.1.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\">\n<span id=\"S3.T5.1.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T5.1.1.1.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\"><span id=\"S3.T5.1.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Contributions</span></span>\n</span>\n</td>\n<td id=\"S3.T5.1.1.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\">\n<span id=\"S3.T5.1.1.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T5.1.1.1.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\"><span id=\"S3.T5.1.1.1.3.1.1.1\" class=\"ltx_text ltx_font_bold\">Technique</span></span>\n</span>\n</td>\n<td id=\"S3.T5.1.1.1.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\">\n<span id=\"S3.T5.1.1.1.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T5.1.1.1.4.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\"><span id=\"S3.T5.1.1.1.4.1.1.1\" class=\"ltx_text ltx_font_bold\">LM used</span></span>\n</span>\n</td>\n<td id=\"S3.T5.1.1.1.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\">\n<span id=\"S3.T5.1.1.1.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T5.1.1.1.5.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\"><span id=\"S3.T5.1.1.1.5.1.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></span>\n</span>\n</td>\n<td id=\"S3.T5.1.1.1.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span id=\"S3.T5.1.1.1.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T5.1.1.1.6.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\"><span id=\"S3.T5.1.1.1.6.1.1.1\" class=\"ltx_text ltx_font_bold\">Limitations</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T5.1.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T5.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">1</th>\n<td id=\"S3.T5.1.2.2.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T5.1.2.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T5.1.2.2.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Analyzes potential privacy leaks and suggests enhanced cryptographic measures and robust aggregation algorithms.</span>\n</span>\n</td>\n<td id=\"S3.T5.1.2.2.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T5.1.2.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T5.1.2.2.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Cryptographic measures, robust aggregation</span>\n</span>\n</td>\n<td id=\"S3.T5.1.2.2.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T5.1.2.2.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T5.1.2.2.4.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">GPT-2, RoberTa, etc.</span>\n</span>\n</td>\n<td id=\"S3.T5.1.2.2.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T5.1.2.2.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T5.1.2.2.5.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">IMDB, Yelp, etc.</span>\n</span>\n</td>\n<td id=\"S3.T5.1.2.2.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T5.1.2.2.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T5.1.2.2.6.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">May involve high computational overhead and complexity in implementation.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T5.1.3.3\" class=\"ltx_tr\">\n<th id=\"S3.T5.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">2</th>\n<td id=\"S3.T5.1.3.3.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T5.1.3.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T5.1.3.3.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Introduces an open-source framework incorporating SMPC and homomorphic encryption to enhance security.</span>\n</span>\n</td>\n<td id=\"S3.T5.1.3.3.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T5.1.3.3.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T5.1.3.3.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">SMPC, homomorphic encryption</span>\n</span>\n</td>\n<td id=\"S3.T5.1.3.3.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T5.1.3.3.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T5.1.3.3.4.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">GPT-4, LLaMa-2, etc.</span>\n</span>\n</td>\n<td id=\"S3.T5.1.3.3.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T5.1.3.3.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T5.1.3.3.5.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Alpaca, UltraFeedback, etc.</span>\n</span>\n</td>\n<td id=\"S3.T5.1.3.3.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T5.1.3.3.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T5.1.3.3.6.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Potential performance degradation due to encryption overhead.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T5.1.4.4\" class=\"ltx_tr\">\n<th id=\"S3.T5.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">3</th>\n<td id=\"S3.T5.1.4.4.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T5.1.4.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T5.1.4.4.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Demonstrates practical attacks for recovering private text and discusses countermeasures.</span>\n</span>\n</td>\n<td id=\"S3.T5.1.4.4.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T5.1.4.4.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T5.1.4.4.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Attack simulation, countermeasure development</span>\n</span>\n</td>\n<td id=\"S3.T5.1.4.4.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T5.1.4.4.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T5.1.4.4.4.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">GPT-2</span>\n</span>\n</td>\n<td id=\"S3.T5.1.4.4.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T5.1.4.4.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T5.1.4.4.5.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">WikiText-103</span>\n</span>\n</td>\n<td id=\"S3.T5.1.4.4.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T5.1.4.4.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T5.1.4.4.6.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Countermeasures may not fully prevent leakage and could affect model efficiency.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T5.1.5.5\" class=\"ltx_tr\">\n<th id=\"S3.T5.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\">4</th>\n<td id=\"S3.T5.1.5.5.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\">\n<span id=\"S3.T5.1.5.5.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T5.1.5.5.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Investigates unintended memorization and proposes best practices to reduce data leakage.</span>\n</span>\n</td>\n<td id=\"S3.T5.1.5.5.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\">\n<span id=\"S3.T5.1.5.5.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T5.1.5.5.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Analysis of memorization, training modifications</span>\n</span>\n</td>\n<td id=\"S3.T5.1.5.5.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\">\n<span id=\"S3.T5.1.5.5.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T5.1.5.5.4.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Bert-based</span>\n</span>\n</td>\n<td id=\"S3.T5.1.5.5.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t\">\n<span id=\"S3.T5.1.5.5.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T5.1.5.5.5.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Stack Overflow</span>\n</span>\n</td>\n<td id=\"S3.T5.1.5.5.6\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\">\n<span id=\"S3.T5.1.5.5.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T5.1.5.5.6.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Best practices may not be universally effective across all model types and scenarios.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In this section, we explore the challenges and solutions associated with maintaining privacy and security while training large language models using federated learning.",
                "Vu et al. ",
                "[",
                "41",
                "]",
                " provide an in-depth analysis of how privacy can be compromised during the federated training of large language models. It identifies specific vulnerabilities where sensitive data could potentially be extracted by adversaries through model inversion attacks and other inference techniques. The study systematically examines the types of information that can be leaked and under what conditions, offering insights into the limitations of current privacy-preserving methods like differential privacy. The paper suggests enhanced cryptographic measures and more robust aggregation algorithms to mitigate these risks, setting a foundational context for the need for advanced privacy-preserving techniques in federated learning.",
                "Ye et al. ",
                "[",
                "42",
                "]",
                " introduce an open-source framework specifically designed to support the development and deployment of federated learning models with an emphasis on privacy and security. This paper describes the architecture of the framework, which includes built-in support for secure multi-party computation (SMPC) and homomorphic encryption, technologies that allow computations to be performed on encrypted data. The framework aims to provide a practical solution to the privacy concerns highlighted in the first paper, facilitating the secure aggregation of updates from multiple clients without exposing their raw data, thereby enhancing the overall security of the federated learning process.",
                "Building on the concerns about privacy leaks, Gupta et al. ",
                "[",
                "43",
                "]",
                " demonstrate a practical attack scenario where sensitive information is extracted from a federated language model. The researchers show how certain training techniques and model configurations can inadvertently lead to the memorization of private text, which can then be recovered by malicious participants. The paper tests various scenarios and configurations, providing empirical evidence of the risks involved. It also discusses potential countermeasures, such as more stringent data sanitization and the use of privacy-preserving architectures, to prevent such vulnerabilities.",
                "Thakkar et al. ",
                "[",
                "44",
                "]",
                " complement the third by further investigating how language models, especially those trained under federated learning conditions, may unintentionally memorize and expose sensitive data. It delves into the mechanics of memorization in neural networks, exploring how different factors like model size, dataset diversity, and training duration affect the risk of data leakage. The study proposes a set of best practices and modifications to the training algorithms that can help reduce the likelihood of such memorization without significantly impacting model performance.",
                "Together, these papers create a layered understanding of the privacy and security challenges in federated learning of LLMs. Starting from identifying potential privacy leaks, moving to a framework designed to address these leaks through encryption and secure computation, and finally, demonstrating practical attack vectors and offering solutions to mitigate these issues, the narrative crafted by these studies underscores the complexity of securely training language models in a distributed manner. Each paper builds on the insights of its predecessors, collectively advancing the field towards more secure and private federated learning environments. This progression not only highlights the vulnerabilities but also charts a path toward resolving them, ensuring that federated learning can be safely employed in sensitive applications.",
                "The papers in this section contribute to the principles of swarm intelligence in the context of federated learning for large language models by addressing complex challenges related to security and privacy—key aspects for collaborative and distributed systems. First, by analyzing potential privacy leaks, the initial paper lays the groundwork for understanding vulnerabilities akin to assessing environmental risks in a swarm. ",
                "[",
                "42",
                "]",
                " introduces a robust framework with advanced encryption methods, enhancing collective security without centralized control, much like a swarm’s distributed processing enhances collective resilience. ",
                "[",
                "43",
                "]",
                " showcases practical attack simulations and mitigations, reinforcing the swarm’s adaptability and response to threats. Finally, ",
                "[",
                "44",
                "]",
                "’s investigation into unintended data memorization offers strategies to minimize such risks, promoting safer learning environments. Collectively, these studies embody swarm intelligence by improving the collective learning and decision-making processes in a decentralized, secure, and private manner, ensuring the swarm (network of nodes) remains robust against internal and external adversarial influences."
            ]
        ]
    }
}