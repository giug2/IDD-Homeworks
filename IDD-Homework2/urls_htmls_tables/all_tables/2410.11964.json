{
    "id_table_1": {
        "caption": "Table 1:  Statistics for real-world datasets.",
        "table": "A3.EGx1",
        "footnotes": [],
        "references": [
            "In Figure  1 , we can see their different properties on a simple distribution over three binary variables. In particular, take  X 1 subscript X 1 X_{1} italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and  X 2 subscript X 2 X_{2} italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  to be Bernoulli and the third  X 3 subscript X 3 X_{3} italic_X start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT  to be the XOR of the first two,  X 3 = X 1  X 2 subscript X 3 direct-sum subscript X 1 subscript X 2 X_{3}=X_{1}\\oplus X_{2} italic_X start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT = italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT . All three variables are symmetric and any one or two of them is indistinguishable from a pair of random bits. Accordingly, there is no mutual information between any of the pairs. Despite one or two variables seeming totally random, knowing all three variables simultaneously is instead completely different from the case of three random bits. Accordingly, this distribution exemplifies the need to discuss purely third-order types of information.",
            "Accordingly, after fixing a chain, this formula attributes each positive drop in KL error to a single interaction set  S S S italic_S . Since the goal of distribution learning is to reduce the KL divergence to zero, this decomposition allows for extremely fine-grained control by directly corresponding each effective parameter   S superscript  S \\theta^{S} italic_ start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT  we may choose to include to a decrease in error. We discuss the implications of this for generalization performance in the coming Section  4.1 .",
            "Unfortunately, exactly computing the refined information is difficult because for degree three and higher as there is no closed form available. Instead, we must choose some heuristic measurement of the refined information gain from including a specific mode interaction within the log-linear model. Accordingly, we use the absolute value of  J S subscript J S J_{S} italic_J start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  as introduced in Section  3.1  as an easy-to-compute alternative to  R  I S R subscript I S RI_{S} italic_R italic_I start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT . We present our search and learning algorithm in Algorithm  1  which continuously alternates between adding new mode interactions to the model and using gradient descent to train with the new parameters. Each subroutine is presented in full detail in the appendix.",
            "To first get a glimpse into the theoretical properties of refined information and the sample complexity of MAHGenTa  we generate a suite of synthetic distributions by choosing random   S superscript  S \\theta^{S} italic_ start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT  and sampling data from the induced distribution, details in the appendix. We demonstrate the impact of structure learning and the value of refined information in this setting where we have full control over the structure in the ground truth. We next apply our method to three real-world distributions from UCI machine learning datasets. The three datasets used are shown in Table  1  with their numbers of samples, features, and total possible events.",
            "We next apply our method to the real-world distribution of three different UCI machine learning datasets. Testing split is generated from 50% of the data and kept fixed throughout. The remaining data is split 70%/30% into the training and the validation set. Experiments are run on a Tesla V100 32GB GPU. The three datasets used are shown in Table  1  with their numbers of samples, features, and total possible events.",
            "In Figure  10 , we depict the algebraic structure representing all possible hierarchical chains which could be selected. Each mode interaction  S  [ d ] S delimited-[] d S\\subseteq[d] italic_S  [ italic_d ]  is represented by a different color and all arrows are drawn which are possible to add while still obeying the hierarchical condition. Horizontally sorted by the size of each collection  I I {\\mathcal{I}} caligraphic_I , although redundancies are suppressed (e.g.  {  , { 1 } , { 2 } , { 1 , 2 } } 1 2 1 2 \\{\\emptyset,\\{1\\},\\{2\\},\\{1,2\\}\\} {  , { 1 } , { 2 } , { 1 , 2 } }  =  {  , 1 , 2 , 12 } 1 2 12 \\{\\emptyset,1,2,12\\} {  , 1 , 2 , 12 }  is written as  { 12 } 12 \\{12\\} { 12 } )."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  KL divergence across all real-world datasets.",
        "table": "A3.EGx2",
        "footnotes": [],
        "references": [
            "We will later consider the problem of learning a discrete distribution by minimizing the same KL-divergence objective. Accordingly, we will adopt the language mode interaction to describe variables interacting in the generative distribution, instead of the more popular feature interaction of Section  2.2  which focuses on the discriminative distribution. Accordingly, we will next review the areas of feature selection, interaction selection, and graphical selection which we will combine with the tensor viewpoint on many-body mode interactions to be able to achieve fine-grained control over the structure of the learned distribution.",
            "In Figure  2 , we show the sample complexity of training when the underlying four-dimensional distribution has low complexity, medium complexity, or high complexity (top to bottom). For each of the three data distribution, we then train models of three different complexities and evaluate their train-time and test-time KL error. In the bottom row, we see how the underspecified, low-complexity model leads to underfitting which peaks at subpar performance. In the top row, we see how the overspecified, high-complexity model leads to overfitting which learns more slowly and less efficiently than the low-complexity model (even with multiple thousands of samples). In addition to showing the importance of matching the correct structure to achieve optimal performance, these experiments also show how achieving good generation performance automatically generalizes to classification performance.",
            "We then apply these hyperparameters to our two large-scale datasets where we cannot directly calculate the KL divergence and resort to the AIS approximation discussed in Section  4.2 . For our third real-world dataset, exact KL gradients are still calculable with an event space smaller than one million. We compare against a Boltzmann machine and an independent distribution also trained with gradient descent on the same objective. In Table  2 , we compare our approach which has the capacity to learn sparse and higher-order structures against both the 1-body and 2-body log-linear models. We find that our MAHGenTa approach is able to consistently deliver improvements in generation performance in terms of the KL divergence or log-likelihood.",
            "In Algorithm  2 , we give a full description of the subroutines used by our main algorithm."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Class-wise accuracy across all real-world datasets.",
        "table": "S5.T1.6",
        "footnotes": [],
        "references": [
            "Unfortunately, the currently existing  I S subscript I S I_{S} italic_I start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  and  J S subscript J S J_{S} italic_J start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  may return positive or negative values whenever  | S |  3 S 3 |S|\\geq 3 | italic_S |  3 , diminishing the ability to interpret MMI as information content as is possible for the original mutual information. This inspires our new definition in Section  3.3  which generalizes mutual information but still returns a non-negative quantity for higher-order interactions.",
            "Unfortunately, exactly computing the refined information is difficult because for degree three and higher as there is no closed form available. Instead, we must choose some heuristic measurement of the refined information gain from including a specific mode interaction within the log-linear model. Accordingly, we use the absolute value of  J S subscript J S J_{S} italic_J start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  as introduced in Section  3.1  as an easy-to-compute alternative to  R  I S R subscript I S RI_{S} italic_R italic_I start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT . We present our search and learning algorithm in Algorithm  1  which continuously alternates between adding new mode interactions to the model and using gradient descent to train with the new parameters. Each subroutine is presented in full detail in the appendix.",
            "In Figure  3 , we plot all different values of refined information for our high complexity distribution. This gives some preliminary insights into the properties of refined information and the range of values a single interaction  S S S italic_S  can take depending on the context  I I {\\mathcal{I}} caligraphic_I . We additionally plot the 2D marginal refined information which corresponds to the classical definition of mutual information. Further discussion of its properties and applications to structure learning and generalization are saved for Appendices  B  and  C .",
            "In Table  3 , we see how the training of a generative model automatically leads to emergent capabilties in classification via the mode interactions simplifying into feature interactions. In particular, the energy-based MAHGenTa and Boltzmann machine are able to simultaneously predict across multiple classes, unlike the discriminative approaches which must be retrained for each task. Although the discriminative approaches have the advantage of reusing the dataset to learn only one of the conditional distributions at a time, the generative approaches nevertheless yield a comparable accuracy performance across a variety of tasks simultaneously."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Best validation KL error from all MIS hyperparameters and all many-body solutions.",
        "table": "S5.T2.9",
        "footnotes": [],
        "references": [
            "Accordingly, after fixing a chain, this formula attributes each positive drop in KL error to a single interaction set  S S S italic_S . Since the goal of distribution learning is to reduce the KL divergence to zero, this decomposition allows for extremely fine-grained control by directly corresponding each effective parameter   S superscript  S \\theta^{S} italic_ start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT  we may choose to include to a decrease in error. We discuss the implications of this for generalization performance in the coming Section  4.1 .",
            "A major challenge of applying this theoretical framing to real-world data distributions is the question of how to select a good collection of mode interactions which accurately describe the distribution. In order to select an appropriate set of interactions from the combinatorially explosive set of available choices, we must leverage an appropriate heuristic for choosing interacting modes amongst the  2 d superscript 2 d 2^{d} 2 start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT  possible choices leading to an ultimate  2 2 d superscript 2 superscript 2 d 2^{\\leavevmode\\resizebox{7.0pt}{}{$2^{d}$}} 2 start_POSTSUPERSCRIPT 2 start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT  candidates for the final collection. We follow similar greedy heuristics as have been explored in previous literature based on the strong or weak heredity assumption  [ 36 ,  7 ]  which allows us to only consider a polynomial number of candidate interactions. Based on our theoretical developments in Equation ( 4 ), we would like to add the   S superscript  S \\theta^{S} italic_ start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT  parameter which corresponds to the greatest amount of refined information, continuing until our validation KL error stops decreasing alongside our training KL error. Early stopping in this way is justified because every parameter of the log-linear model is an effective parameter, and sequential projections along the statistical manifold will cause our model to obey the classical underfitting-overfitting curve.",
            "Figure  4  shows the performance in terms of the capacity curves, plotting the training and validation errors as a function of the size of the interaction collection, which is a measure of the log-linear models capacity. We train significantly beyond the point of early stopping to help fully illustrate the underfitting-overfitting behavior of the log-linear model. This provides empirical support for our theoretically principled approach of early stopping as soon as the validation error stops improving alongside the training error. Further hyperparameters are provided in the appendix. Overall, we find that using the weak hierarchy of  30 % percent 30 30\\% 30 %  strength was the most effective choice for achieving minimal validation error for our MAHGenTa algorithm and we keep this choice consistent throughout.",
            "We then apply these hyperparameters to our two large-scale datasets where we cannot directly calculate the KL divergence and resort to the AIS approximation discussed in Section  4.2 . For our third real-world dataset, exact KL gradients are still calculable with an event space smaller than one million. We compare against a Boltzmann machine and an independent distribution also trained with gradient descent on the same objective. In Table  2 , we compare our approach which has the capacity to learn sparse and higher-order structures against both the 1-body and 2-body log-linear models. We find that our MAHGenTa approach is able to consistently deliver improvements in generation performance in terms of the KL divergence or log-likelihood."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "S5.T3.1",
        "footnotes": [],
        "references": [
            "In Figures  5  and  6 , we show the capacity curves across all sets of hyperparameters chosen for the experiments with the 10-dimensional subset of the 23-dimensional mushroom datasets."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "A1.T4.1",
        "footnotes": [],
        "references": [
            "In Figures  5  and  6 , we show the capacity curves across all sets of hyperparameters chosen for the experiments with the 10-dimensional subset of the 23-dimensional mushroom datasets."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "A1.T4.2",
        "footnotes": [],
        "references": [
            "In Figure  7 , we show the sample complexity of training when the underlying four-dimensional distribution has low complexity, medium complexity, or high complexity (top to bottom). On the left-hand side, we see the KL error optimized during training, whereas on the right-hand side, we see the calibrated classification score for each of the four dimensions (predicted using the other three features), automatically rising alongside improved generative performance. In the bottom row, we see how the underspecified, low-complexity model leads to underfitting which peaks at subpar performance. In the top row, we see how the overspecified, high-complexity model leads to overfitting which makes less efficient use of the finite dataset (even with multiple thousands of samples). In addition to showing the importance of matching the correct structure to achieve optimal performance, these experiments also show how achieving good generation performance automatically generalizes to classification performance."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "A3.EGx3",
        "footnotes": [],
        "references": [
            "We further make clear the relationship to structure learning with a simple example in causal structure learning. Suppose we have the distribution as induced by the causal graph depicted in Figure  8 . Although there is mutual information between the variables  B B B italic_B  and  C C C italic_C , there is no direct causal information between them. In fact, they are both controlled by the variable  A A A italic_A  and the correlations between them are thereafter induced.",
            "In the case of  d = 3 d 3 d=3 italic_d = 3  and  | S | = 2 S 2 |S|=2 | italic_S | = 2 , the refined information only takes two values (which correspond to the marginal and conditional values). In Figure  8 , we have these values calculated for all three pairs of variables. If we take a look at the mutual information between  B B B italic_B  and  C C C italic_C , we can indeed see there is a positive amount of information; however, in the presence of conditioning on  A A A italic_A , there is no refined information between these two variables."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "A3.EGx4",
        "footnotes": [],
        "references": [
            "Here we provide some additional figures to more quickly provide intuition about the algebraic structures we introduce. In Figure  9 , we depict a possible chain which is maximally refined for  d = 3 d 3 d=3 italic_d = 3 ."
        ]
    }
}