{
    "id_table_1": {
        "caption": "Table 1 :  Attributes defined in the input prompts to synthesise emotional facial images with Stable Diffusion XL  [ 42 ] . The prompt template considers three different sources of variation: the emotion, the style, and the demographic group. The latter is determined by three different demographic attributes: age, biological sex, and skin tone. We have also utilised a negative prompt, which includes all the styles that are not desired in the current synthesis.",
        "table": "S2.T1.5",
        "footnotes": [
            ""
        ],
        "references": [
            "One of the main characteristics of the Foundation Models (FMs) is that they are trained on a broad range of data, so that the resulting models can be utilised in a wide range of problems. In addition, they exploit large amounts of learning parameters. Given sufficient learning material, from a certain number of such parameters hence well trained, knowledge emerges in the FMs, and they achieve competitive performances in tasks they have not been specifically trained for. This can, however, be difficult to predict  [ 43 ] . In this paper, we aim to investigate the emergent affective capabilities of FMs. Focusing on the vision (cf.  Section   2.1 ), linguistics (cf.  Section   2.2 ), and speech (acoustics) (cf.  Section   2.3 ) modalities, we assess the capabilities of current FMs to i) generate synthetic affective samples, from which we infer the conveyed emotions with pre-trained emotion recognition classifiers 1 1 1 Note that in principle, this can lead to a closed loop, as we cannot be sure whether the data used in the pre-trained emotion classifiers has not also been used in the training of the Foundation Models, but generally, it is unlikely, as high-quality affective data are rarely freely available on the Internet due to their privacy restrictions. , and ii) analyse well-established datasets in the field in a zero-shot manner. To favour the comparability among the different modalities investigated, we focus on the Big Six Ekman emotions  [ 44 ]  (i. e., fear, anger, happiness, sadness, disgust, and surprise), in addition to the neutral state.",
            "We have leveraged one of the latest versions of SD  2 2 2 https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0   i. e., Stable Diffusion XL (SDXL)  [ 42 ]   to synthetically generate a face emotion dataset 3 3 3 For further information and access to the dataset, please contact the authors.  . This dataset is generated utilising prompts based on a fixed template with three sources of variation: i) the emotion, ii) the style (photorealistic, cartoon-painting, anime, and 3D), and iii) the demographic group. The template, along with the values explored for each attribute, are detailed in Table  1 . Table  2  presents a summary of the gathered dataset. Although our emotion model is based on the Big Six Ekman emotions  [ 44 ]  plus the neutral state, we employed these basic emotions together with a higher intensity variation, as defined in Plutchiks model  [ 48 ] , to further emphasise the desired affective states. Also, note that the generation process spans 18 different demographic groups; determined by age, biological sex, and skin tone. Visual examples for each emotion and style are provided in Figure  1  for the demographic group  < y o u n g , w o m a n , w h i t e > <young,woman,white> < italic_y italic_o italic_u italic_n italic_g , italic_w italic_o italic_m italic_a italic_n , italic_w italic_h italic_i italic_t italic_e > . For the case of the photorealistic style, we played with the background to generate some samples in realistic scenarios (e. g., outdoors, office, park). The generation process involved two experts, the SDXL base model and the SDXL refiner 4 4 4 https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/ , with a total of 40 steps (80 % in the base model, 20 % in the refiner) and a guidance value of 7.5. Together with the desired prompt, we utilised a negative prompt to highlight what we do not want to see in the output. Once the images were generated, we filtered them according to four principles: the presence of disfigurations or artifacts, nudity, the quality of the emotion generated, and the plausibility of the style. For the sake of this experiment, only one annotator conducted this data curation process.",
            "In order to evaluate the affective analytical capabilities of FMs in the image domain, we explore their performance in a zero-shot emotion recognition task under different configurations. Our experiments are conducted on the considered validation set of AffectNet  [ 33 ]  (cf.  Section   2.1.1 ), as it is balanced, in-the-wild, and manually annotated. We compare three different approaches relying on model-prompting. In the first two, we start by extracting AU-based features with OpenFace  [ 49 ] , and we provide these features in a textual format as input to a FM. Recalling from  Section   2.1.1 , OpenFace predicts both the presence and the intensity of a subset of AUs; hence, two approaches can be derived from its predictions. On the other hand, the third approach consists in directly feeding the images within the prompt of a FM. Note that the first two approaches can be addressed with a Language Foundation Model (LFM), for which we select the LLaMA2 7B model. We utilise a Multimodal Foundation Model (MFM) for the third scenario; specifically, the LLaVA1.5 7B model  7 7 7 https://huggingface.co/liuhaotian/llava-v1.5-7b   [ 53 ,  54 ] . This is an open-source MFM with visual capabilities trained by fine-tuning Vicuna 8 8 8 https://lmsys.org/blog/2023-03-30-vicuna/ , an already fine-tuned version of LLaMA, with GPT-4 generated data. The selected models allow them having the same number of parameters.",
            "For the two baselines, we employ two different architectures: a Bi-directional Long Short-Term Memory (BiLSTM) network and a fine-tuned version of the RoBERTa-base model  [ 68 ] . Both models are trained on the selected subset of the GoEmotions dataset. The BiLSTM consists of two bidirectional LSTM layers with 128 units each. It is trained with a learning rate of  5  10  3 5 superscript 10 3 5\\times 10^{-3} 5  10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT  and a batch size of 96 for 40 epochs, while RoBERTa-base was fine-tuned at a conservative learning rate of  5  10  5 5 superscript 10 5 5\\times 10^{-5} 5  10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT  and a smaller batch size of 12. The models are trained on the training partition of the dataset, and the weights yielding the highest validation Unweighted Average Recall (UAR) are selected for each model. The test scores for both baseline models (BiLSTM and RoBERTa) on the test partition of the GoEmotions dataset are shown in  Table   10 . The results are consistent with  [ 67 ] , but with some differences, given that we model the problem as a single-label classification, instead of the original multi-label classification task. Given the superior performance of RoBERTa, we utilise it for analysing the synthetically generated emotional sentences.",
            "To evaluate the performance of the various LLMs on the emotion injection task, we test the generated sentences with the RoBERTa baseline model, in addition to GPT-4 as an approximation for human evaluation, and its weaker variant GPT-3.5. GPT-4 has shown superior performance in many affective computing problems  [ 62 ] , often better than fine-tuned, specialised models, especially with problems related to sentiment or emotions.  Table   11  demonstrates the prompt templates used for the GPT models, following a similar pattern like  [ 62 ,  69 ] . The versions of GPT variants used are gpt-3.5-turbo-0125 11 11 11 https://platform.openai.com/docs/models/gpt-3-5-turbo  for GPT-3.5 and gpt-4-turbo-2024-04-09 12 12 12 https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4  for GPT-4. The results of this evaluation are depicted in  Figure   4 . Notably, these results can be considered to reflect a better agreement between models than with the ground truth labels, which are not human-annotated. However, the results of GPT-4 should be the closest to the human evaluations  [ 70 ,  62 ] .",
            "In this section, we analyse the zero-shot sentiment analysis capabilities of the following LLMs: Mistral, Mixtral, and two versions of LLaMA2 (7 billion and 13 billion parameters). We assess their zero-shot capabilities on the test partition of the GoEmotions dataset. We design a prompt that requires the selected LLMs to predict the corresponding emotion, including the neutral state (cf.  Table   11 ). Prompt engineering is crucial for influencing LLMs, as it enhances nuanced responses and ensures more accurate behaviour  [ 71 ] . To minimise randomness and increase confidence in the predictions, we reduce the temperature setting to 0.1. This lower temperature sharpens the probability distribution, ensuring that the predicted classes reflect those that the LLMs are predicting with the highest probabilities  [ 69 ] . However, the LLM outputs sometimes include irrelevant or multiple emotions. To address this without intrusively altering the models outputs, we select the first listed emotion as the most reliable prediction. This approach aligns with the operational principle of decoder-based models, where the first valid emotion is mathematically the one with the highest confidence score, thus considered the valid class prediction.",
            "Table   12  summarises the comparative performance of the tested LLMs. We include the performance of the RoBERTa baseline model trained on the GoEmotions dataset (cf.  Section   2.2.1 ) for benchmarking purposes. The first observation from the obtained results is that the UAR scores obtained by all the investigated LLMs surpass the chance level (14.3 %), underscoring the emergent affective capabilities of the LLMs tested in a zero-shot manner. Nevertheless, none of the LLMs outperforms the RoBERTa baseline model, fine-tuned on the GoEmotions dataset, which confirms the advantage of model-specific tuning. Nevertheless, it is worth highlighting that the difference in the UAR scores obtained by the best-performing GPT-3.5 and GPT-4 models in comparison to the RoBERTa baseline model is around 15 %. This is an interesting result, as these models have not been trained on the GoEmotions dataset, but still obtained a competitive performance on the emotion recognition task, reinforcing, one more time, the emergent affective capabilities of the models."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Summary of the face images generated with the Stable Diffusion XL model  [ 42 ] . The prompts for the generation are detailed in Table  1 .",
        "table": "S2.T2.33.33",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "One of the main characteristics of the Foundation Models (FMs) is that they are trained on a broad range of data, so that the resulting models can be utilised in a wide range of problems. In addition, they exploit large amounts of learning parameters. Given sufficient learning material, from a certain number of such parameters hence well trained, knowledge emerges in the FMs, and they achieve competitive performances in tasks they have not been specifically trained for. This can, however, be difficult to predict  [ 43 ] . In this paper, we aim to investigate the emergent affective capabilities of FMs. Focusing on the vision (cf.  Section   2.1 ), linguistics (cf.  Section   2.2 ), and speech (acoustics) (cf.  Section   2.3 ) modalities, we assess the capabilities of current FMs to i) generate synthetic affective samples, from which we infer the conveyed emotions with pre-trained emotion recognition classifiers 1 1 1 Note that in principle, this can lead to a closed loop, as we cannot be sure whether the data used in the pre-trained emotion classifiers has not also been used in the training of the Foundation Models, but generally, it is unlikely, as high-quality affective data are rarely freely available on the Internet due to their privacy restrictions. , and ii) analyse well-established datasets in the field in a zero-shot manner. To favour the comparability among the different modalities investigated, we focus on the Big Six Ekman emotions  [ 44 ]  (i. e., fear, anger, happiness, sadness, disgust, and surprise), in addition to the neutral state.",
            "We have leveraged one of the latest versions of SD  2 2 2 https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0   i. e., Stable Diffusion XL (SDXL)  [ 42 ]   to synthetically generate a face emotion dataset 3 3 3 For further information and access to the dataset, please contact the authors.  . This dataset is generated utilising prompts based on a fixed template with three sources of variation: i) the emotion, ii) the style (photorealistic, cartoon-painting, anime, and 3D), and iii) the demographic group. The template, along with the values explored for each attribute, are detailed in Table  1 . Table  2  presents a summary of the gathered dataset. Although our emotion model is based on the Big Six Ekman emotions  [ 44 ]  plus the neutral state, we employed these basic emotions together with a higher intensity variation, as defined in Plutchiks model  [ 48 ] , to further emphasise the desired affective states. Also, note that the generation process spans 18 different demographic groups; determined by age, biological sex, and skin tone. Visual examples for each emotion and style are provided in Figure  1  for the demographic group  < y o u n g , w o m a n , w h i t e > <young,woman,white> < italic_y italic_o italic_u italic_n italic_g , italic_w italic_o italic_m italic_a italic_n , italic_w italic_h italic_i italic_t italic_e > . For the case of the photorealistic style, we played with the background to generate some samples in realistic scenarios (e. g., outdoors, office, park). The generation process involved two experts, the SDXL base model and the SDXL refiner 4 4 4 https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/ , with a total of 40 steps (80 % in the base model, 20 % in the refiner) and a guidance value of 7.5. Together with the desired prompt, we utilised a negative prompt to highlight what we do not want to see in the output. Once the images were generated, we filtered them according to four principles: the presence of disfigurations or artifacts, nudity, the quality of the emotion generated, and the plausibility of the style. For the sake of this experiment, only one annotator conducted this data curation process.",
            "Table   5  summarises the results obtained when analysing the generated facial images with the four different styles utilising the ViT  FER pre-trained model.  Figure   2  presents the confusion matrices computed, comparing the model inferences and the ground truth. Across styles, the worst results are those of the photorealistic style, as denoted by both accuracy  Weighted Average Recall (WAR)  and Unweighted Average Recall (UAR) 6 6 6 UAR is the sum of recall per class divided by the number of classes  this reflects imbalances and is a standard measure in the field. . In contrast, the best results are obtained with the 3D style. Interestingly, in all the 4 styles both the neutral and the happiness emotions consistently obtain the best results. These classes are traditionally over-represented in face datasets (e. g., see the training set distribution of AffectNet in Table  3 ), probably due to the nature of the data sources  [ 52 ] . Consequently, it is expected for the generative model to be biased towards those emotions, which would also explain the difficulty to generate samples for classes like disgust or fear, which obtain the worst results.",
            "In order to evaluate the affective analytical capabilities of FMs in the image domain, we explore their performance in a zero-shot emotion recognition task under different configurations. Our experiments are conducted on the considered validation set of AffectNet  [ 33 ]  (cf.  Section   2.1.1 ), as it is balanced, in-the-wild, and manually annotated. We compare three different approaches relying on model-prompting. In the first two, we start by extracting AU-based features with OpenFace  [ 49 ] , and we provide these features in a textual format as input to a FM. Recalling from  Section   2.1.1 , OpenFace predicts both the presence and the intensity of a subset of AUs; hence, two approaches can be derived from its predictions. On the other hand, the third approach consists in directly feeding the images within the prompt of a FM. Note that the first two approaches can be addressed with a Language Foundation Model (LFM), for which we select the LLaMA2 7B model. We utilise a Multimodal Foundation Model (MFM) for the third scenario; specifically, the LLaVA1.5 7B model  7 7 7 https://huggingface.co/liuhaotian/llava-v1.5-7b   [ 53 ,  54 ] . This is an open-source MFM with visual capabilities trained by fine-tuning Vicuna 8 8 8 https://lmsys.org/blog/2023-03-30-vicuna/ , an already fine-tuned version of LLaMA, with GPT-4 generated data. The selected models allow them having the same number of parameters.",
            "Table   12  summarises the comparative performance of the tested LLMs. We include the performance of the RoBERTa baseline model trained on the GoEmotions dataset (cf.  Section   2.2.1 ) for benchmarking purposes. The first observation from the obtained results is that the UAR scores obtained by all the investigated LLMs surpass the chance level (14.3 %), underscoring the emergent affective capabilities of the LLMs tested in a zero-shot manner. Nevertheless, none of the LLMs outperforms the RoBERTa baseline model, fine-tuned on the GoEmotions dataset, which confirms the advantage of model-specific tuning. Nevertheless, it is worth highlighting that the difference in the UAR scores obtained by the best-performing GPT-3.5 and GPT-4 models in comparison to the RoBERTa baseline model is around 15 %. This is an interesting result, as these models have not been trained on the GoEmotions dataset, but still obtained a competitive performance on the emotion recognition task, reinforcing, one more time, the emergent affective capabilities of the models."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Summary of the face images selected from AffectNet  [ 33 ]  in the training and the validation partitions.",
        "table": "S2.T3.1.1",
        "footnotes": [
            ""
        ],
        "references": [
            "One of the main characteristics of the Foundation Models (FMs) is that they are trained on a broad range of data, so that the resulting models can be utilised in a wide range of problems. In addition, they exploit large amounts of learning parameters. Given sufficient learning material, from a certain number of such parameters hence well trained, knowledge emerges in the FMs, and they achieve competitive performances in tasks they have not been specifically trained for. This can, however, be difficult to predict  [ 43 ] . In this paper, we aim to investigate the emergent affective capabilities of FMs. Focusing on the vision (cf.  Section   2.1 ), linguistics (cf.  Section   2.2 ), and speech (acoustics) (cf.  Section   2.3 ) modalities, we assess the capabilities of current FMs to i) generate synthetic affective samples, from which we infer the conveyed emotions with pre-trained emotion recognition classifiers 1 1 1 Note that in principle, this can lead to a closed loop, as we cannot be sure whether the data used in the pre-trained emotion classifiers has not also been used in the training of the Foundation Models, but generally, it is unlikely, as high-quality affective data are rarely freely available on the Internet due to their privacy restrictions. , and ii) analyse well-established datasets in the field in a zero-shot manner. To favour the comparability among the different modalities investigated, we focus on the Big Six Ekman emotions  [ 44 ]  (i. e., fear, anger, happiness, sadness, disgust, and surprise), in addition to the neutral state.",
            "We now aim to automatically verify the affective quality of the generated facial images with Face Emotion Recognition (FER) models. We employ the manually annotated subset of the AffectNet dataset  [ 33 ]  to develop these models. The images belonging to this dataset are annotated in terms of eleven emotions. Nevertheless, we select the images corresponding to the emotions fear, anger, happiness, sadness, disgust, and surprise in addition to the neutral class. We process the selected images with OpenFace  [ 49 ]  to extract features related to a subset of the Action Units (AU) defined in the Facial Action Coding System (FACS). Specifically, OpenFace extracts 35 features per facial image, indicating the presence (0 or 1) and the intensity (in a scale from 0  not present  to 5  present with maximum intensity) of a subset of the AUs. We discard the images that OpenFace fails to process; for instance, due to the absence of a face in the image.  Table   3  summarises the resulting data in terms of the number of images per emotion in the training and the validation partitions.",
            "We start our preliminary investigation by training FER models with Support Vector Classifiers (SVC), as these are considered a standard machine learning technique with excellent results in a wide range of problems. We compare their performance when utilising a linear and a Radial Basis Function (RBF) kernel. One challenge associated with the selected dataset is the imbalanced training samples in terms of the emotional classes (cf.  Table   3 ), which impacts the performance of the trained models. To overcome this issue, we consider weighting the training data, so that the samples corresponding to the least represented classes have more importance than the samples corresponding to the most represented classes when training the models. We fine-tune our models optimising the regularisation parameter  C  [ 10  2 , 10  1 , 1 , 10 , 10 2 ] C superscript 10 2 superscript 10 1 1 10 superscript 10 2 C\\in[10^{-2},10^{-1},1,10,10^{2}] italic_C  [ 10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT , 10 start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT , 1 , 10 , 10 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] . The performance of the optimal models on the validation partition is depicted in  Table   4 .",
            "Table   5  summarises the results obtained when analysing the generated facial images with the four different styles utilising the ViT  FER pre-trained model.  Figure   2  presents the confusion matrices computed, comparing the model inferences and the ground truth. Across styles, the worst results are those of the photorealistic style, as denoted by both accuracy  Weighted Average Recall (WAR)  and Unweighted Average Recall (UAR) 6 6 6 UAR is the sum of recall per class divided by the number of classes  this reflects imbalances and is a standard measure in the field. . In contrast, the best results are obtained with the 3D style. Interestingly, in all the 4 styles both the neutral and the happiness emotions consistently obtain the best results. These classes are traditionally over-represented in face datasets (e. g., see the training set distribution of AffectNet in Table  3 ), probably due to the nature of the data sources  [ 52 ] . Consequently, it is expected for the generative model to be biased towards those emotions, which would also explain the difficulty to generate samples for classes like disgust or fear, which obtain the worst results.",
            "In this section, we aim to investigate and leverage the affective style transfer capabilities of cutting-edge open-source LLMs. For this experiment, we select LLaMA2  [ 55 ]  and Mistral  [ 64 ] , given their proven high performance in both language understanding and text generation tasks. Additionally, we include the Mixtral 9 9 9 https://huggingface.co/mistralai/Mixtral-8x7B-v0.1  LLM  [ 65 ] , which utilises the Sparse Mixture of Experts (SMoE) method  [ 66 ] . It is important to note that the implementation of SMoE in Mixtral notably influences its size. The LLaMA2 and Mistral LLMs each comprise 7 billion parameters. We start with the text generation by compiling a corpus comprised of 122 human-curated neutral phrases. The dataset encompasses various topics, ranging from mundane personal activities to formal professional interactions, thus providing a targeted platform to evaluate how well LLMs handle affective style transfer across diverse topics. Utilising the gathered corpus, we task the aforementioned LLMs (i. e., LLaMA2, Mistral, and Mixtral) to generate six emotional phrases from each original neutral phrase, conveying a specific target emotion: fear, anger, happiness, sadness, disgust, and surprise. The generation is described by the formula  M  ( t  e  x  t , e  m  o  t  i  o  n ) , t  e  x  t  N , e  m  o  t  i  o  n  E , formulae-sequence M t e x t e m o t i o n t e x t N e m o t i o n E \\mathcal{M}(text,emotion),text\\in\\mathcal{N},emotion\\in\\mathcal{E}, caligraphic_M ( italic_t italic_e italic_x italic_t , italic_e italic_m italic_o italic_t italic_i italic_o italic_n ) , italic_t italic_e italic_x italic_t  caligraphic_N , italic_e italic_m italic_o italic_t italic_i italic_o italic_n  caligraphic_E ,  where  M M \\mathcal{M} caligraphic_M  is the LLM at hand,  N N \\mathcal{N} caligraphic_N  the set of the neutral phrases, and  E = { f  e  a  r , a  n  g  e  r , h  a  p  p  i  n  e  s  s , s  a  d  n  e  s  s , d  i  s  g  u  s  t , s  u  r  p  r  i  s  e } E f e a r a n g e r h a p p i n e s s s a d n e s s d i s g u s t s u r p r i s e \\mathcal{E}=\\{fear,anger,happiness,sadness,disgust,surprise\\} caligraphic_E = { italic_f italic_e italic_a italic_r , italic_a italic_n italic_g italic_e italic_r , italic_h italic_a italic_p italic_p italic_i italic_n italic_e italic_s italic_s , italic_s italic_a italic_d italic_n italic_e italic_s italic_s , italic_d italic_i italic_s italic_g italic_u italic_s italic_t , italic_s italic_u italic_r italic_p italic_r italic_i italic_s italic_e }  (the prompted emotion). We generate three synthetic sentences for each combination of text and emotion, yielding a corpus of  3  | N |  | E | = 2 194 3 N E 2194 3*|\\mathcal{N}|*|\\mathcal{E}|=2\\,194 3  | caligraphic_N |  | caligraphic_E | = 2 194  emotional phrases 10 10 10 For further information and access to the dataset, please contact the authors.  . The data was synthesised with settings chosen to enhance variability and creativity. The generation parameters used a temperature of 0.9, top- p p p italic_p  of 0.6, and a repetition penalty of 1.2. An example of the generated sentences is shown in  Table   8 , and  Figure   3  visually summarises the followed pipeline. We observe that the models tend to exaggerate the injected emotion, adopting an over-dramatic style that results in more formal, yet affectively adapted phrases. Despite these exaggerated and dramatic adaptations, the primary objective was to investigate how these models perceive and express emotions under basic setup conditions. Our findings highlight the models propensity to amplify emotional content, which was an anticipated aspect of this exploratory study."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  Performance summary of the trained Support Vector Classifier-based models for Face Emotion Recognition on the validation partition of the considered subset of AffectNet. We also include the performance of a state-of-the-art Vision Transformer for Facial Expression Recognition (ViT  FER). We select the accuracy (ACC) as the metric to assess the model performances.",
        "table": "S2.T4.7.7",
        "footnotes": [],
        "references": [
            "We start our preliminary investigation by training FER models with Support Vector Classifiers (SVC), as these are considered a standard machine learning technique with excellent results in a wide range of problems. We compare their performance when utilising a linear and a Radial Basis Function (RBF) kernel. One challenge associated with the selected dataset is the imbalanced training samples in terms of the emotional classes (cf.  Table   3 ), which impacts the performance of the trained models. To overcome this issue, we consider weighting the training data, so that the samples corresponding to the least represented classes have more importance than the samples corresponding to the most represented classes when training the models. We fine-tune our models optimising the regularisation parameter  C  [ 10  2 , 10  1 , 1 , 10 , 10 2 ] C superscript 10 2 superscript 10 1 1 10 superscript 10 2 C\\in[10^{-2},10^{-1},1,10,10^{2}] italic_C  [ 10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT , 10 start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT , 1 , 10 , 10 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] . The performance of the optimal models on the validation partition is depicted in  Table   4 .",
            "We also contrast the performance of the SVC-based FER models with a state-of-the-art Vision Transformer  [ 50 ]  for Facial Expression Recognition 5 5 5 https://huggingface.co/trpakov/vit-face-expression  (ViT  FER), trained on the Facial Expression Recognition 2013 (FER-2013) dataset  [ 51 ] . In this case, we use the pre-trained model off-the-shelf  without fine-tuning  and evaluate its performance on the validation partition of our subset of AffectNet. The obtained results are reported in  Table   4 .",
            "The best performance on the validation partition of the AffectNet dataset (cf.  Table   4 ) is obtained with the ViT  FER model. Thus, we use this model to assess the affective quality of the generated facial images. The results obtained from the conducted experiments exemplify the breakthrough of working with end-to-end approaches, operating on the raw images instead of on the features extracted from them. Nevertheless, it is also worth mentioning that AffectNet was collected in the wild, which may complicate the estimation of the AUs from the facial images and, in turn, worsen the performance of the SVC-based models.",
            "To evaluate the performance of the various LLMs on the emotion injection task, we test the generated sentences with the RoBERTa baseline model, in addition to GPT-4 as an approximation for human evaluation, and its weaker variant GPT-3.5. GPT-4 has shown superior performance in many affective computing problems  [ 62 ] , often better than fine-tuned, specialised models, especially with problems related to sentiment or emotions.  Table   11  demonstrates the prompt templates used for the GPT models, following a similar pattern like  [ 62 ,  69 ] . The versions of GPT variants used are gpt-3.5-turbo-0125 11 11 11 https://platform.openai.com/docs/models/gpt-3-5-turbo  for GPT-3.5 and gpt-4-turbo-2024-04-09 12 12 12 https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4  for GPT-4. The results of this evaluation are depicted in  Figure   4 . Notably, these results can be considered to reflect a better agreement between models than with the ground truth labels, which are not human-annotated. However, the results of GPT-4 should be the closest to the human evaluations  [ 70 ,  62 ] ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 :  Accuracy (ACC) and Unweighted Average Recall (UAR) scores obtained when analysing the facial images generated in the four different styles with the ViT  FER pre-trained model.",
        "table": "S2.T5.2.2",
        "footnotes": [],
        "references": [
            "Table   5  summarises the results obtained when analysing the generated facial images with the four different styles utilising the ViT  FER pre-trained model.  Figure   2  presents the confusion matrices computed, comparing the model inferences and the ground truth. Across styles, the worst results are those of the photorealistic style, as denoted by both accuracy  Weighted Average Recall (WAR)  and Unweighted Average Recall (UAR) 6 6 6 UAR is the sum of recall per class divided by the number of classes  this reflects imbalances and is a standard measure in the field. . In contrast, the best results are obtained with the 3D style. Interestingly, in all the 4 styles both the neutral and the happiness emotions consistently obtain the best results. These classes are traditionally over-represented in face datasets (e. g., see the training set distribution of AffectNet in Table  3 ), probably due to the nature of the data sources  [ 52 ] . Consequently, it is expected for the generative model to be biased towards those emotions, which would also explain the difficulty to generate samples for classes like disgust or fear, which obtain the worst results.",
            "Figure   5  depicts the confusion matrices obtained with the RoBERTa baseline model on the LLaMA2-, the Mistral-, and the Mixtral-generated sets. We also include its performance on the test set of GoEmotions as a reference. A common issue with the RoBERTa model is the confusion among anger and disgust. Analysing the confusion matrices, we also observe an interesting effect: most of the models mispredictions are assigned to the neutral class."
        ]
    },
    "id_table_6": {
        "caption": "Table 6 :  Prompts employed to perform zero-shot emotion recognition with Foundation Models in different scenarios. The prompts including (i. e., first two rows) AU information are injected in LLaMA2  [ 55 ] , while the prompt including the raw image is injected to a LLaVA1.5 model  [ 54 ] .",
        "table": "S2.T6.16",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "In  Table   7 , we present the results of the aforementioned scenarios. The prompts utilised for each scenario are detailed in  Table   6 . We also include in  Table   7  the results of the ViT  FER model on the considered validation set of AffectNet for comparability purposes. Among the three zero-shot approaches, we obtain the best results when feeding the prompts with the raw images. Both AU-based prompt approaches exhibit accuracy results close to chance. The LLaVA model achieves an accuracy only 4 points below the ViT  FER model, which was explicitly trained on the FER-2013 dataset to recognise emotions. This is an interesting result, which suggests that the LLaVA1.5 model presents emergent affective capabilities, despite not being specifically trained on affective computing tasks."
        ]
    },
    "id_table_7": {
        "caption": "Table 7 :  Accuracy scores obtained with the LLaMA2  [ 55 ]  and LLaVA1.5  [ 54 ]  Foundation Models on the validation set of AffecNet  [ 33 ] . We have included as well the performance obtained with the ViT  FER model  [ 50 ]  trained on FER- 2013 2013 2013 2013   [ 51 ]  for comparison purposes.",
        "table": "S2.T7.22.20",
        "footnotes": [
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "In  Table   7 , we present the results of the aforementioned scenarios. The prompts utilised for each scenario are detailed in  Table   6 . We also include in  Table   7  the results of the ViT  FER model on the considered validation set of AffectNet for comparability purposes. Among the three zero-shot approaches, we obtain the best results when feeding the prompts with the raw images. Both AU-based prompt approaches exhibit accuracy results close to chance. The LLaVA model achieves an accuracy only 4 points below the ViT  FER model, which was explicitly trained on the FER-2013 dataset to recognise emotions. This is an interesting result, which suggests that the LLaVA1.5 model presents emergent affective capabilities, despite not being specifically trained on affective computing tasks."
        ]
    },
    "id_table_8": {
        "caption": "Table 8 :  Affective style transfer example towards the emotion surprise with the neutral phrase:  The weather is clear and sunny.",
        "table": "S2.T8.5",
        "footnotes": [],
        "references": [
            "In this section, we aim to investigate and leverage the affective style transfer capabilities of cutting-edge open-source LLMs. For this experiment, we select LLaMA2  [ 55 ]  and Mistral  [ 64 ] , given their proven high performance in both language understanding and text generation tasks. Additionally, we include the Mixtral 9 9 9 https://huggingface.co/mistralai/Mixtral-8x7B-v0.1  LLM  [ 65 ] , which utilises the Sparse Mixture of Experts (SMoE) method  [ 66 ] . It is important to note that the implementation of SMoE in Mixtral notably influences its size. The LLaMA2 and Mistral LLMs each comprise 7 billion parameters. We start with the text generation by compiling a corpus comprised of 122 human-curated neutral phrases. The dataset encompasses various topics, ranging from mundane personal activities to formal professional interactions, thus providing a targeted platform to evaluate how well LLMs handle affective style transfer across diverse topics. Utilising the gathered corpus, we task the aforementioned LLMs (i. e., LLaMA2, Mistral, and Mixtral) to generate six emotional phrases from each original neutral phrase, conveying a specific target emotion: fear, anger, happiness, sadness, disgust, and surprise. The generation is described by the formula  M  ( t  e  x  t , e  m  o  t  i  o  n ) , t  e  x  t  N , e  m  o  t  i  o  n  E , formulae-sequence M t e x t e m o t i o n t e x t N e m o t i o n E \\mathcal{M}(text,emotion),text\\in\\mathcal{N},emotion\\in\\mathcal{E}, caligraphic_M ( italic_t italic_e italic_x italic_t , italic_e italic_m italic_o italic_t italic_i italic_o italic_n ) , italic_t italic_e italic_x italic_t  caligraphic_N , italic_e italic_m italic_o italic_t italic_i italic_o italic_n  caligraphic_E ,  where  M M \\mathcal{M} caligraphic_M  is the LLM at hand,  N N \\mathcal{N} caligraphic_N  the set of the neutral phrases, and  E = { f  e  a  r , a  n  g  e  r , h  a  p  p  i  n  e  s  s , s  a  d  n  e  s  s , d  i  s  g  u  s  t , s  u  r  p  r  i  s  e } E f e a r a n g e r h a p p i n e s s s a d n e s s d i s g u s t s u r p r i s e \\mathcal{E}=\\{fear,anger,happiness,sadness,disgust,surprise\\} caligraphic_E = { italic_f italic_e italic_a italic_r , italic_a italic_n italic_g italic_e italic_r , italic_h italic_a italic_p italic_p italic_i italic_n italic_e italic_s italic_s , italic_s italic_a italic_d italic_n italic_e italic_s italic_s , italic_d italic_i italic_s italic_g italic_u italic_s italic_t , italic_s italic_u italic_r italic_p italic_r italic_i italic_s italic_e }  (the prompted emotion). We generate three synthetic sentences for each combination of text and emotion, yielding a corpus of  3  | N |  | E | = 2 194 3 N E 2194 3*|\\mathcal{N}|*|\\mathcal{E}|=2\\,194 3  | caligraphic_N |  | caligraphic_E | = 2 194  emotional phrases 10 10 10 For further information and access to the dataset, please contact the authors.  . The data was synthesised with settings chosen to enhance variability and creativity. The generation parameters used a temperature of 0.9, top- p p p italic_p  of 0.6, and a repetition penalty of 1.2. An example of the generated sentences is shown in  Table   8 , and  Figure   3  visually summarises the followed pipeline. We observe that the models tend to exaggerate the injected emotion, adopting an over-dramatic style that results in more formal, yet affectively adapted phrases. Despite these exaggerated and dramatic adaptations, the primary objective was to investigate how these models perceive and express emotions under basic setup conditions. Our findings highlight the models propensity to amplify emotional content, which was an anticipated aspect of this exploratory study."
        ]
    },
    "id_table_9": {
        "caption": "Table 9 :  Statistics of the considered subset of the GoEmotions dataset.",
        "table": "S2.T9.1",
        "footnotes": [],
        "references": [
            "In order to investigate the quality of the generated sentences by the LLMs, we implement two baseline models trained on the GoEmotions dataset  [ 67 ] , a well-renowned corpus in the field and commonly utilised for benchmarking purposes due to its comprehensive labelling and categorisation of the emotions. The GoEmotions dataset consists of English Reddit comments annotated according to 27 distinct emotions, plus the neutral state, by 3 or 5 labellers each. Due to the nature of the annotations in the GoEmotions dataset, we begin by tailoring the data to meet the specific requirements of our experiments. First, we select instances from the dataset annotated with a single emotion, in order to tackle the task as a single-label classification problem, instead of a multi-label classification one. As previously, we adopt the Big Six Ekman emotions  [ 44 ] , in addition to a seventh neutral state. This restructuring of the GoEmotions taxonomy to the Ekman taxonomy is achieved by aggregating the original labels into the targeted, broader categories  [ 67 ] . For example, emotions like annoyance and irritation, originally distinct, were grouped under anger to fit the Ekman model.  Table   9  summarises the statistics of the considered subset of the GoEmotions dataset."
        ]
    },
    "id_table_10": {
        "caption": "Table 10 :  Performance scores of the implemented models when inferring the emotions (Ekmans Big Six in addition to the neutral state) conveyed by the sentences belonging to the test set of the GoEmotions dataset.",
        "table": "S2.T10.4",
        "footnotes": [],
        "references": [
            "For the two baselines, we employ two different architectures: a Bi-directional Long Short-Term Memory (BiLSTM) network and a fine-tuned version of the RoBERTa-base model  [ 68 ] . Both models are trained on the selected subset of the GoEmotions dataset. The BiLSTM consists of two bidirectional LSTM layers with 128 units each. It is trained with a learning rate of  5  10  3 5 superscript 10 3 5\\times 10^{-3} 5  10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT  and a batch size of 96 for 40 epochs, while RoBERTa-base was fine-tuned at a conservative learning rate of  5  10  5 5 superscript 10 5 5\\times 10^{-5} 5  10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT  and a smaller batch size of 12. The models are trained on the training partition of the dataset, and the weights yielding the highest validation Unweighted Average Recall (UAR) are selected for each model. The test scores for both baseline models (BiLSTM and RoBERTa) on the test partition of the GoEmotions dataset are shown in  Table   10 . The results are consistent with  [ 67 ] , but with some differences, given that we model the problem as a single-label classification, instead of the original multi-label classification task. Given the superior performance of RoBERTa, we utilise it for analysing the synthetically generated emotional sentences."
        ]
    },
    "id_table_11": {
        "caption": "Table 11 :  Prompts to use LLMs for zero-shot emotion recognition, following a similar pattern as in  [ 62 ]  and  [ 69 ] .",
        "table": "S2.T11.4",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "To evaluate the performance of the various LLMs on the emotion injection task, we test the generated sentences with the RoBERTa baseline model, in addition to GPT-4 as an approximation for human evaluation, and its weaker variant GPT-3.5. GPT-4 has shown superior performance in many affective computing problems  [ 62 ] , often better than fine-tuned, specialised models, especially with problems related to sentiment or emotions.  Table   11  demonstrates the prompt templates used for the GPT models, following a similar pattern like  [ 62 ,  69 ] . The versions of GPT variants used are gpt-3.5-turbo-0125 11 11 11 https://platform.openai.com/docs/models/gpt-3-5-turbo  for GPT-3.5 and gpt-4-turbo-2024-04-09 12 12 12 https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4  for GPT-4. The results of this evaluation are depicted in  Figure   4 . Notably, these results can be considered to reflect a better agreement between models than with the ground truth labels, which are not human-annotated. However, the results of GPT-4 should be the closest to the human evaluations  [ 70 ,  62 ] .",
            "In this section, we analyse the zero-shot sentiment analysis capabilities of the following LLMs: Mistral, Mixtral, and two versions of LLaMA2 (7 billion and 13 billion parameters). We assess their zero-shot capabilities on the test partition of the GoEmotions dataset. We design a prompt that requires the selected LLMs to predict the corresponding emotion, including the neutral state (cf.  Table   11 ). Prompt engineering is crucial for influencing LLMs, as it enhances nuanced responses and ensures more accurate behaviour  [ 71 ] . To minimise randomness and increase confidence in the predictions, we reduce the temperature setting to 0.1. This lower temperature sharpens the probability distribution, ensuring that the predicted classes reflect those that the LLMs are predicting with the highest probabilities  [ 69 ] . However, the LLM outputs sometimes include irrelevant or multiple emotions. To address this without intrusively altering the models outputs, we select the first listed emotion as the most reliable prediction. This approach aligns with the operational principle of decoder-based models, where the first valid emotion is mathematically the one with the highest confidence score, thus considered the valid class prediction."
        ]
    },
    "id_table_12": {
        "caption": "Table 12 :  Performance scores of the different LLMs tested on a zero-shot fashion for recognising the corresponding emotion on the sentences belonging to the test partition of the GoEmotions dataset.",
        "table": "S2.T12.4.1",
        "footnotes": [],
        "references": [
            "Table   12  summarises the comparative performance of the tested LLMs. We include the performance of the RoBERTa baseline model trained on the GoEmotions dataset (cf.  Section   2.2.1 ) for benchmarking purposes. The first observation from the obtained results is that the UAR scores obtained by all the investigated LLMs surpass the chance level (14.3 %), underscoring the emergent affective capabilities of the LLMs tested in a zero-shot manner. Nevertheless, none of the LLMs outperforms the RoBERTa baseline model, fine-tuned on the GoEmotions dataset, which confirms the advantage of model-specific tuning. Nevertheless, it is worth highlighting that the difference in the UAR scores obtained by the best-performing GPT-3.5 and GPT-4 models in comparison to the RoBERTa baseline model is around 15 %. This is an interesting result, as these models have not been trained on the GoEmotions dataset, but still obtained a competitive performance on the emotion recognition task, reinforcing, one more time, the emergent affective capabilities of the models."
        ]
    }
}