{
    "id_table_1": {
        "caption": "Table 1 :  Quantitative evaluation results of Imagine yourself vs. the SOTA control-based model and the SOTA adapter-based model under head-to-head human evaluation setting.",
        "table": "S1.T1.6",
        "footnotes": [],
        "references": [
            "The previously introduced fully parallel image-text fusion pipeline (Section  3.4.3 ) can be flexibly extended to accommodate multi-subject personalization. In the two-person scenario for example, instead of passing the global embedding and patch embedding of the single reference image into the  K K \\mathbf{K} bold_K  and  V V \\mathbf{V} bold_V  components as shown in the top left branch of Figure  4 , we can concatenate the vision embedding from both reference images and passing it into the  K K \\mathbf{K} bold_K  and  V V \\mathbf{V} bold_V  components. Given this setup, through training, the network learns how to map from reference i  to subject j  in the group photo while generating prompt-induced image context accordingly. Some examples of the two-person personalization results are shown in Figure  11 .",
            "We show examples of our model generated image in Figures  6 - 10 . Our model generates visually appealing images that both preserve the identity and follow the prompt faithfully.",
            "To quantitatively evaluate Imagine yourself, we created an evaluation set consisting of two parts: (i) reference images, and (ii) eval prompts. To have a comprehensive comparison in all representative cases, we collected a total of 51 reference identities covering different gender, race, and skin tone. We created a list of 65 prompts to evaluate the model. It widely covers a wide range of usage scenarios, and also including hard prompts that require face expression or pose changes, camera motions, and stylization. These prompts help to assess the models ability to engage in more complex and nuanced interactions, diverse pose generation, and harmonization. Each identity is paired with all 65 prompts, so a total of 51x65=3315 generations for one round of human evaluation. The distributions of the prompts is shown in Figure  12 .",
            "As shown in Table  1 , Imagine yourself outperforms the two state-of-the-art methods adapter-based model and control-based model by a significant margin in most axes. Specifically, Imagine yourself is significantly better in prompt alignment, with a +45.1% and +30.8% improvement over the SOTA adapter-based model and the SOTA control-based model, respectively. However, we observed that the control-based model is better in identity preservation than Imagine yourself, due to its hard copy-pasting of the reference image at the center of the image, resulting in unnatural images despite the high identity metric."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Ablation study of different component in Imagine yourself.",
        "table": "S4.T2.1",
        "footnotes": [],
        "references": [
            "To push the boundaries of personalized image generation, our approach begins by identifying three key facets crucial to eliciting a satisfying human visual experience: identity preservation, prompt alignment, and visual appeal (Section  3.2 ). We then introduce novel techniques tailored to enhance each of these aspects. Specifically, we propose a novel synthetic paired data generation mechanism (Section  3.3 ), new fully parallel architecture that incorporates three text encoders and a trainable vision encoder for optimizing identity preservation and text-alignment (Section  3.4 ), a novel coarse-to-fine multi-stage finetuning methodology designed to progressively enhance visual appeal, thereby pushing the visual appeal boundary of generated images. (Section  3.5 ). Finally, we demonstrate Imagine yourself is generalizable to multi-subjects personalization in Section  3.6 .",
            "Figure  2  provides an illustration of the proposed model architecture. The key of using diffusion models for personalized image generation is incorporating the reference identity as an additional control signal to the diffusion model. We propose to extract the identity information from the reference image through a trainable clip patch encoder. The identity vision signal is then added to the text signals through a parallel cross attention module. To better preserve the high visual quality of the foundation model, we leveraged low-rank adapters (LoRA) to freeze the self-attention and text cross-attention modules while only fine-tuning the adapters.",
            "To quantitatively evaluate Imagine yourself, we created an evaluation set consisting of two parts: (i) reference images, and (ii) eval prompts. To have a comprehensive comparison in all representative cases, we collected a total of 51 reference identities covering different gender, race, and skin tone. We created a list of 65 prompts to evaluate the model. It widely covers a wide range of usage scenarios, and also including hard prompts that require face expression or pose changes, camera motions, and stylization. These prompts help to assess the models ability to engage in more complex and nuanced interactions, diverse pose generation, and harmonization. Each identity is paired with all 65 prompts, so a total of 51x65=3315 generations for one round of human evaluation. The distributions of the prompts is shown in Figure  12 .",
            "In our ablation study, we examined the effectiveness of various components within our proposed Imagine yourself. Main ablation results are shown in Table  2 ."
        ]
    }
}