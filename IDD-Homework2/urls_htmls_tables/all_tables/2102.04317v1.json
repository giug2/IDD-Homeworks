{
    "S4.T1": {
        "caption": "TABLE I: Experiments of quantitative comparisons. Single-scale models (including AR-GCN and PU-GAN) trained with each specific scale factor (top two rows) vs. the naive approach of arbitrary-scale upsampling (rows 3 to 5) vs. our full model (last row). The NUC scores are tested with p=0.8%ùëùpercent0.8p=0.8\\%.",
        "table": null,
        "footnotes": [],
        "references": [
            "In this experiment, we compare Meta-PU with state-of-the-art single-scale upsampling methods, including PU-GAN¬†[6] and AR-GCN¬†[7], to upsample the sparse point cloud with scale factors R‚àà[2,4,6,9,16]ùëÖ246916R\\in[2,4,6,9,16]. Their models are trained with the author-released code, and all settings are the same as stated in their papers. Since they are single-scale upsampling methods, for each scale factor, an individual model is trained. Due to the limitations of the two-stage upsampling strategy, AR-GCN can only be trained with the factors 4,9,1649164,9,16, whereas PU-GAN can be trained with all four factors. Their performance is reported in the first two rows of Table I. We surprisingly observe that our arbitrary-upscale model even outperforms their single-scale models with most scale factors. Particularly, our model performs significantly better on the F-score, NUC, mean, and std metrics than other models, and is more stable on all scales. This may be because multiple joint training tasks of different scales can benefit each other, thus improving performance. In addition, Meta-PU needs to train only once for all testings, while others need to train multiple models, which is very inefficient.",
            "A naive approach to achieve arbitrary-scale upsampling is to first use a state-of-the-art single-scale model to upsample the cloud point to a large scale, and then downsample it to a specific smaller scale. We compare our method with this naive approach. Specifically, we choose AR-GCN¬†[7] to upsample point clouds to 16x and then downsample them to 2x,4x,6x and 9x with the random sampling, disk sampling, and farthest sampling algorithms. The results are reported in 3-5th rows of Table I. We can see that random sampling gets the worst scores because it non-uniformly downsamples the points. In comparison, the more advanced sampling algorithms, including disk sampling and farthest sampling, perform better by considering uniformity. Our method is still superior to all of them because the result of a smaller scale factor in our method is not simply a subset of the large-factor one. In fact, Meta-PU can adaptively adjust the location of the output points to fit the underlying surface better and maintain uniformity according to different scale factors. This will be analyzed by the ablation study of the meta-RGC block in the next subsection. Moreover, compared to the strongest baseline (AR-GCN+disk-sampling), ours is 120 times faster (Table IV), because this advanced upsampling algorithm requiring mesh reconstruction is slow.",
            "In Table IV, we provide the comparison of the average inference time of all integer scales in (1,16]. Since Disk-sampling obtains the best performance as shown in Table¬†I, it is employed in other single-scale baselines for arbitrary-scale upsampling. We also compare the inference time of ours with the optimization-based method EAR. The running time of our method is much less than all the compared methods. Specifically, the speed of a trivial single-scale baseline is dragged down by the bottleneck of disk-sampling. We also calculated the inference time of our model at the scales of 2,4,6,9, and 16 with 2500 points. We found no difference in inference time for different scales, demonstrating the stability of our method in terms of inference time.",
            "Point Cloud Classification. In this application, we aim to demonstrate that point upsampling can potentially improve the classification results for sparse point clouds. In detail, we first train the PointNet on the ModelNet40 training set with 204820482048 input points for each model. During testing, we prepare three datasets for each model: 1) 204820482048 points uniformly sampled from the corresponding shape surface (referred to as ‚Äú2048‚Äù); 2) 512512512 points non-uniformly sampled from the 204820482048 points (referred to as ‚Äú512‚Äù); 3) the 4√ó4\\times upsampling results obtained from the 512512512 points with Meta-PU (referred as ‚Äú2048(from 512)‚Äù). As shown in Table. X, because ‚Äú512‚Äù is sparser than ‚Äú2048‚Äù, it results in lower classification accuracy. By using Meta-PU for upsampling, ‚Äú2048(from 512)‚Äù can achieve a significant performance gain and is comparable with the original ‚Äú2048‚Äù."
        ]
    },
    "S4.T2": {
        "caption": "TABLE II: Quantitative comparisons with the EAR. The NUC scores are tested with p=0.8%ùëùpercent0.8p=0.8\\%.",
        "table": null,
        "footnotes": [],
        "references": [
            "We also compare our method with the state-of-the-art optimization-based method EAR¬†[29], which is also applicable to variable scales. The results of scale 2,4,6,9,16 are provided in Table¬†II. It could be found that our method yields superior results under all metrics."
        ]
    },
    "S4.T3": {
        "caption": "TABLE III: Quantitative comparisons with MPU. Our method obtains superior results under most metrics.",
        "table": "<table id=\"S4.T3.5\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.5.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.5.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T3.5.1.1.1.1\" class=\"ltx_text\">Method</span></th>\n<th id=\"S4.T3.5.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T3.5.1.1.2.1\" class=\"ltx_text\">CD</span></th>\n<th id=\"S4.T3.5.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T3.5.1.1.3.1\" class=\"ltx_text\">EMD</span></th>\n<th id=\"S4.T3.5.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T3.5.1.1.4.1\" class=\"ltx_text\">F-score</span></th>\n<th id=\"S4.T3.5.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"5\">NUC with different p</th>\n<th id=\"S4.T3.5.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\">Deviation(1e-2)</th>\n<th id=\"S4.T3.5.1.1.7\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T3.5.1.1.7.1\" class=\"ltx_text\">Time</span></th>\n</tr>\n<tr id=\"S4.T3.5.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.5.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.2%</th>\n<th id=\"S4.T3.5.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.4%</th>\n<th id=\"S4.T3.5.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.6%</th>\n<th id=\"S4.T3.5.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.8%</th>\n<th id=\"S4.T3.5.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">1.0%</th>\n<th id=\"S4.T3.5.2.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">mean</th>\n<th id=\"S4.T3.5.2.2.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">std</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.5.3.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.5.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">MPU(2x)</td>\n<td id=\"S4.T3.5.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.5.3.1.2.1\" class=\"ltx_text ltx_font_bold\">0.0097</span></td>\n<td id=\"S4.T3.5.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.0132</td>\n<td id=\"S4.T3.5.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">52.65%</td>\n<td id=\"S4.T3.5.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.317</td>\n<td id=\"S4.T3.5.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">0.271</td>\n<td id=\"S4.T3.5.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">0.252</td>\n<td id=\"S4.T3.5.3.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\">0.241</td>\n<td id=\"S4.T3.5.3.1.9\" class=\"ltx_td ltx_align_center ltx_border_t\">0.234</td>\n<td id=\"S4.T3.5.3.1.10\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.5.3.1.10.1\" class=\"ltx_text ltx_font_bold\">0.21</span></td>\n<td id=\"S4.T3.5.3.1.11\" class=\"ltx_td ltx_align_center ltx_border_t\">0.30</td>\n<td id=\"S4.T3.5.3.1.12\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">5.16</td>\n</tr>\n<tr id=\"S4.T3.5.4.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.5.4.2.1\" class=\"ltx_td ltx_align_center\">ours(2x)</td>\n<td id=\"S4.T3.5.4.2.2\" class=\"ltx_td ltx_align_center\">0.010</td>\n<td id=\"S4.T3.5.4.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.5.4.2.3.1\" class=\"ltx_text ltx_font_bold\">0.0049</span></td>\n<td id=\"S4.T3.5.4.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.5.4.2.4.1\" class=\"ltx_text ltx_font_bold\">53.20%</span></td>\n<td id=\"S4.T3.5.4.2.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.5.4.2.5.1\" class=\"ltx_text ltx_font_bold\">0.183</span></td>\n<td id=\"S4.T3.5.4.2.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.5.4.2.6.1\" class=\"ltx_text ltx_font_bold\">0.147</span></td>\n<td id=\"S4.T3.5.4.2.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.5.4.2.7.1\" class=\"ltx_text ltx_font_bold\">0.134</span></td>\n<td id=\"S4.T3.5.4.2.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.5.4.2.8.1\" class=\"ltx_text ltx_font_bold\">0.127</span></td>\n<td id=\"S4.T3.5.4.2.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.5.4.2.9.1\" class=\"ltx_text ltx_font_bold\">0.123</span></td>\n<td id=\"S4.T3.5.4.2.10\" class=\"ltx_td ltx_align_center\">0.23</td>\n<td id=\"S4.T3.5.4.2.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.5.4.2.11.1\" class=\"ltx_text ltx_font_bold\">0.29</span></td>\n<td id=\"S4.T3.5.4.2.12\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T3.5.4.2.12.1\" class=\"ltx_text ltx_font_bold\">0.90</span></td>\n</tr>\n<tr id=\"S4.T3.5.5.3\" class=\"ltx_tr\">\n<td id=\"S4.T3.5.5.3.1\" class=\"ltx_td ltx_align_center\">MPU(4x)</td>\n<td id=\"S4.T3.5.5.3.2\" class=\"ltx_td ltx_align_center\">0.0086</td>\n<td id=\"S4.T3.5.5.3.3\" class=\"ltx_td ltx_align_center\">0.012</td>\n<td id=\"S4.T3.5.5.3.4\" class=\"ltx_td ltx_align_center\">73.16%</td>\n<td id=\"S4.T3.5.5.3.5\" class=\"ltx_td ltx_align_center\">0.321</td>\n<td id=\"S4.T3.5.5.3.6\" class=\"ltx_td ltx_align_center\">0.282</td>\n<td id=\"S4.T3.5.5.3.7\" class=\"ltx_td ltx_align_center\">0.265</td>\n<td id=\"S4.T3.5.5.3.8\" class=\"ltx_td ltx_align_center\">0.256</td>\n<td id=\"S4.T3.5.5.3.9\" class=\"ltx_td ltx_align_center\">0.249</td>\n<td id=\"S4.T3.5.5.3.10\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.5.5.3.10.1\" class=\"ltx_text ltx_font_bold\">0.22</span></td>\n<td id=\"S4.T3.5.5.3.11\" class=\"ltx_td ltx_align_center\">0.28</td>\n<td id=\"S4.T3.5.5.3.12\" class=\"ltx_td ltx_nopad_r ltx_align_center\">36.28</td>\n</tr>\n<tr id=\"S4.T3.5.6.4\" class=\"ltx_tr\">\n<td id=\"S4.T3.5.6.4.1\" class=\"ltx_td ltx_align_center\">ours(4x)</td>\n<td id=\"S4.T3.5.6.4.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.5.6.4.2.1\" class=\"ltx_text ltx_font_bold\">0.0080</span></td>\n<td id=\"S4.T3.5.6.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.5.6.4.3.1\" class=\"ltx_text ltx_font_bold\">0.0078</span></td>\n<td id=\"S4.T3.5.6.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.5.6.4.4.1\" class=\"ltx_text ltx_font_bold\">74.05%</span></td>\n<td id=\"S4.T3.5.6.4.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.5.6.4.5.1\" class=\"ltx_text ltx_font_bold\">0.245</span></td>\n<td id=\"S4.T3.5.6.4.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.5.6.4.6.1\" class=\"ltx_text ltx_font_bold\">0.213</span></td>\n<td id=\"S4.T3.5.6.4.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.5.6.4.7.1\" class=\"ltx_text ltx_font_bold\">0.200</span></td>\n<td id=\"S4.T3.5.6.4.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.5.6.4.8.1\" class=\"ltx_text ltx_font_bold\">0.192</span></td>\n<td id=\"S4.T3.5.6.4.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.5.6.4.9.1\" class=\"ltx_text ltx_font_bold\">0.187</span></td>\n<td id=\"S4.T3.5.6.4.10\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.5.6.4.10.1\" class=\"ltx_text ltx_font_bold\">0.22</span></td>\n<td id=\"S4.T3.5.6.4.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.5.6.4.11.1\" class=\"ltx_text ltx_font_bold\">0.27</span></td>\n<td id=\"S4.T3.5.6.4.12\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T3.5.6.4.12.1\" class=\"ltx_text ltx_font_bold\">0.91</span></td>\n</tr>\n<tr id=\"S4.T3.5.7.5\" class=\"ltx_tr\">\n<td id=\"S4.T3.5.7.5.1\" class=\"ltx_td ltx_align_center\">MPU(16x)</td>\n<td id=\"S4.T3.5.7.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.5.7.5.2.1\" class=\"ltx_text ltx_font_bold\">0.0078</span></td>\n<td id=\"S4.T3.5.7.5.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.5.7.5.3.1\" class=\"ltx_text ltx_font_bold\">0.023</span></td>\n<td id=\"S4.T3.5.7.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.5.7.5.4.1\" class=\"ltx_text ltx_font_bold\">76.35%</span></td>\n<td id=\"S4.T3.5.7.5.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.5.7.5.5.1\" class=\"ltx_text ltx_font_bold\">0.425</span></td>\n<td id=\"S4.T3.5.7.5.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.5.7.5.6.1\" class=\"ltx_text ltx_font_bold\">0.378</span></td>\n<td id=\"S4.T3.5.7.5.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.5.7.5.7.1\" class=\"ltx_text ltx_font_bold\">0.357</span></td>\n<td id=\"S4.T3.5.7.5.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.5.7.5.8.1\" class=\"ltx_text ltx_font_bold\">0.346</span></td>\n<td id=\"S4.T3.5.7.5.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.5.7.5.9.1\" class=\"ltx_text ltx_font_bold\">0.337</span></td>\n<td id=\"S4.T3.5.7.5.10\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.5.7.5.10.1\" class=\"ltx_text ltx_font_bold\">0.28</span></td>\n<td id=\"S4.T3.5.7.5.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.5.7.5.11.1\" class=\"ltx_text ltx_font_bold\">0.35</span></td>\n<td id=\"S4.T3.5.7.5.12\" class=\"ltx_td ltx_nopad_r ltx_align_center\">389.40</td>\n</tr>\n<tr id=\"S4.T3.5.8.6\" class=\"ltx_tr\">\n<td id=\"S4.T3.5.8.6.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">ours(16x)</td>\n<td id=\"S4.T3.5.8.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.0082</td>\n<td id=\"S4.T3.5.8.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T3.5.8.6.3.1\" class=\"ltx_text ltx_font_bold\">0.023</span></td>\n<td id=\"S4.T3.5.8.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">75.62%</td>\n<td id=\"S4.T3.5.8.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.555</td>\n<td id=\"S4.T3.5.8.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.482</td>\n<td id=\"S4.T3.5.8.6.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.448</td>\n<td id=\"S4.T3.5.8.6.8\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.428</td>\n<td id=\"S4.T3.5.8.6.9\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.414</td>\n<td id=\"S4.T3.5.8.6.10\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.29</td>\n<td id=\"S4.T3.5.8.6.11\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T3.5.8.6.11.1\" class=\"ltx_text ltx_font_bold\">0.35</span></td>\n<td id=\"S4.T3.5.8.6.12\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\"><span id=\"S4.T3.5.8.6.12.1\" class=\"ltx_text ltx_font_bold\">0.50</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Further, we compare Meta-PU with the state-of-the-art multistep upsample method MPU¬†[5] that recursively upsamples a point set, which is also applicable to scales of a power of 2,( e.g., 2,4,16). The results of scales 2,4,16 are provided in Table¬†III. It can be found that our method obtains superior results under most metrics. In addition, we provide a comparison of inference times. The running time of our method is much less at all scales, demonstrating that our method is more efficient than the recursive approach."
        ]
    },
    "S4.T4": {
        "caption": "TABLE IV: Comparison of the inference time.",
        "table": "<table id=\"S4.T4.4\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T4.4.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T4.4.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\">Method</td>\n<td id=\"S4.T4.4.1.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span id=\"S4.T4.4.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T4.4.1.1.2.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">AR-GCN + Disk-sampling</span>\n</span>\n</td>\n<td id=\"S4.T4.4.1.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span id=\"S4.T4.4.1.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T4.4.1.1.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">PU-GAN + Disk-sampling</span>\n</span>\n</td>\n<td id=\"S4.T4.4.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">EAR</td>\n<td id=\"S4.T4.4.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\">ours</td>\n</tr>\n<tr id=\"S4.T4.4.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T4.4.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">Time(s)</td>\n<td id=\"S4.T4.4.2.2.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\">\n<span id=\"S4.T4.4.2.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T4.4.2.2.2.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">10.28</span>\n</span>\n</td>\n<td id=\"S4.T4.4.2.2.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\">\n<span id=\"S4.T4.4.2.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T4.4.2.2.3.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">10.06</span>\n</span>\n</td>\n<td id=\"S4.T4.4.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">351.10</td>\n<td id=\"S4.T4.4.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S4.T4.4.2.2.5.1\" class=\"ltx_text ltx_font_bold\">0.79</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "A naive approach to achieve arbitrary-scale upsampling is to first use a state-of-the-art single-scale model to upsample the cloud point to a large scale, and then downsample it to a specific smaller scale. We compare our method with this naive approach. Specifically, we choose AR-GCN¬†[7] to upsample point clouds to 16x and then downsample them to 2x,4x,6x and 9x with the random sampling, disk sampling, and farthest sampling algorithms. The results are reported in 3-5th rows of Table I. We can see that random sampling gets the worst scores because it non-uniformly downsamples the points. In comparison, the more advanced sampling algorithms, including disk sampling and farthest sampling, perform better by considering uniformity. Our method is still superior to all of them because the result of a smaller scale factor in our method is not simply a subset of the large-factor one. In fact, Meta-PU can adaptively adjust the location of the output points to fit the underlying surface better and maintain uniformity according to different scale factors. This will be analyzed by the ablation study of the meta-RGC block in the next subsection. Moreover, compared to the strongest baseline (AR-GCN+disk-sampling), ours is 120 times faster (Table IV), because this advanced upsampling algorithm requiring mesh reconstruction is slow.",
            "In Table IV, we provide the comparison of the average inference time of all integer scales in (1,16]. Since Disk-sampling obtains the best performance as shown in Table¬†I, it is employed in other single-scale baselines for arbitrary-scale upsampling. We also compare the inference time of ours with the optimization-based method EAR. The running time of our method is much less than all the compared methods. Specifically, the speed of a trivial single-scale baseline is dragged down by the bottleneck of disk-sampling. We also calculated the inference time of our model at the scales of 2,4,6,9, and 16 with 2500 points. We found no difference in inference time for different scales, demonstrating the stability of our method in terms of inference time."
        ]
    },
    "S4.T5": {
        "caption": "TABLE V: Ablation on the meta-RGC block.",
        "table": "<table id=\"S4.T5.5\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T5.5.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T5.5.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Methods\\F-score</th>\n<th id=\"S4.T5.5.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">2x</th>\n<th id=\"S4.T5.5.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">4x</th>\n<th id=\"S4.T5.5.1.1.4\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\">6x</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T5.5.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T5.5.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">Full-model</td>\n<td id=\"S4.T5.5.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">53.20</td>\n<td id=\"S4.T5.5.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">74.05</td>\n<td id=\"S4.T5.5.2.1.4\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">74.98</td>\n</tr>\n<tr id=\"S4.T5.5.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T5.5.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">Replace Meta-RGC</td>\n<td id=\"S4.T5.5.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">52.33</td>\n<td id=\"S4.T5.5.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">73.08</td>\n<td id=\"S4.T5.5.3.2.4\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\">74.09</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Importance of the Meta-RGC Block. We design an experiment to evaluate the influence of the meta-RGC block quantitatively. We use Meta-PU to upsample point clouds to 2x, but the weights of the meta-RGC block are generated with different input RùëÖR. We measure the average F-score and CD values on the testing set for different RùëÖR, as plotted in Fig.¬†5. It can be observed that the best performance for both the F-score and CD is achieved when the input scale factor RùëÖR of the meta-RGC block equals the target upsampling scale 222. This demonstrates the meta-RGC block adapting the convolution weight properly to different scale factors. Moreover, the meta-RGC block adaptation to various scale factors is the key to makeing the output points better fit the underlying surface and keep uniform. To demonstrate this, we respectively train the full model and our model where the meta-RGC block is replaced by normal RGC-block, and show the close up of some results in Fig.7. It is obvious that the results of our full model are more precise, sharper and cleaner, especially around key positions, such as corners. Also, we conducted an ablation study to replace the meta-RGC with normal RGC, while keeping all the other parts fixed. The results are shown in Table¬†V. These results demonstrate the meta-RGC block adapts the convolutional weight properly to different scale factors and improves performance. Therefore, as we explained before, the performance improvements of our method mainly come from two aspects: the joint training of multiple scale factors with one model and the meta-RGC block."
        ]
    },
    "S4.T6": {
        "caption": "TABLE VI: Experiments on our specially designed scale tensor vs simply feeding the original scale factor. All models are tested with a scale factor of 4. Our specially designed scale tensor performs better.",
        "table": "<table id=\"S4.T6.6\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T6.6.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T6.6.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T6.6.1.1.1.1\" class=\"ltx_text\">Method</span></th>\n<th id=\"S4.T6.6.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T6.6.1.1.2.1\" class=\"ltx_text\">CD</span></th>\n<th id=\"S4.T6.6.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T6.6.1.1.3.1\" class=\"ltx_text\">EMD</span></th>\n<th id=\"S4.T6.6.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T6.6.1.1.4.1\" class=\"ltx_text\">F-score</span></th>\n<th id=\"S4.T6.6.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"5\">NUC with different p</th>\n<th id=\"S4.T6.6.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\">Deviation(1e-2)</th>\n</tr>\n<tr id=\"S4.T6.6.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T6.6.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.2%</th>\n<th id=\"S4.T6.6.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.4%</th>\n<th id=\"S4.T6.6.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.6%</th>\n<th id=\"S4.T6.6.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.8%</th>\n<th id=\"S4.T6.6.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">1.0%</th>\n<th id=\"S4.T6.6.2.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">mean</th>\n<th id=\"S4.T6.6.2.2.7\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\">std</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T6.6.3.1\" class=\"ltx_tr\">\n<td id=\"S4.T6.6.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">all-R</td>\n<td id=\"S4.T6.6.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.0081</td>\n<td id=\"S4.T6.6.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T6.6.3.1.3.1\" class=\"ltx_text ltx_font_bold\">0.0078</span></td>\n<td id=\"S4.T6.6.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">73.46%</td>\n<td id=\"S4.T6.6.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.250</td>\n<td id=\"S4.T6.6.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">0.216</td>\n<td id=\"S4.T6.6.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">0.202</td>\n<td id=\"S4.T6.6.3.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\">0.194</td>\n<td id=\"S4.T6.6.3.1.9\" class=\"ltx_td ltx_align_center ltx_border_t\">0.189</td>\n<td id=\"S4.T6.6.3.1.10\" class=\"ltx_td ltx_align_center ltx_border_t\">0.23</td>\n<td id=\"S4.T6.6.3.1.11\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">0.28</td>\n</tr>\n<tr id=\"S4.T6.6.4.2\" class=\"ltx_tr\">\n<td id=\"S4.T6.6.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">ours</td>\n<td id=\"S4.T6.6.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T6.6.4.2.2.1\" class=\"ltx_text ltx_font_bold\">0.0080</span></td>\n<td id=\"S4.T6.6.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T6.6.4.2.3.1\" class=\"ltx_text ltx_font_bold\">0.0078</span></td>\n<td id=\"S4.T6.6.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T6.6.4.2.4.1\" class=\"ltx_text ltx_font_bold\">74.05%</span></td>\n<td id=\"S4.T6.6.4.2.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T6.6.4.2.5.1\" class=\"ltx_text ltx_font_bold\">0.245</span></td>\n<td id=\"S4.T6.6.4.2.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T6.6.4.2.6.1\" class=\"ltx_text ltx_font_bold\">0.213</span></td>\n<td id=\"S4.T6.6.4.2.7\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T6.6.4.2.7.1\" class=\"ltx_text ltx_font_bold\">0.200</span></td>\n<td id=\"S4.T6.6.4.2.8\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T6.6.4.2.8.1\" class=\"ltx_text ltx_font_bold\">0.192</span></td>\n<td id=\"S4.T6.6.4.2.9\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T6.6.4.2.9.1\" class=\"ltx_text ltx_font_bold\">0.187</span></td>\n<td id=\"S4.T6.6.4.2.10\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T6.6.4.2.10.1\" class=\"ltx_text ltx_font_bold\">0.22</span></td>\n<td id=\"S4.T6.6.4.2.11\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\"><span id=\"S4.T6.6.4.2.11.1\" class=\"ltx_text ltx_font_bold\">0.27</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Importance of Specially Designed Scale Tensor. In this ablation study, we aim to show the advantages of our designed scale tensor R~~ùëÖ\\widetilde{R} with the location identifier over directly feeding RùëÖR into the meta-RGC block. In the first row of Table. VI, we fill the scale tensor with all RùëÖRs and train the model with the same setting. We can observe that the model with the specially designed scale tensor performs better, because the location identifier in our scale tensor contains extra information to guide the network to better differentiate a group of points, generated by the same seed point, from each other."
        ]
    },
    "S4.T7": {
        "caption": "TABLE VII: Experiments on our model with Sinkhorn loss vs. Chamfer Distance vs. GAN loss. All models are tested with a scale factor of 4.",
        "table": "<table id=\"S4.T7.5\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T7.5.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T7.5.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T7.5.1.1.1.1\" class=\"ltx_text\">Method</span></th>\n<th id=\"S4.T7.5.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T7.5.1.1.2.1\" class=\"ltx_text\">CD</span></th>\n<th id=\"S4.T7.5.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T7.5.1.1.3.1\" class=\"ltx_text\">EMD</span></th>\n<th id=\"S4.T7.5.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T7.5.1.1.4.1\" class=\"ltx_text\">F-score</span></th>\n<th id=\"S4.T7.5.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"5\">NUC with different p</th>\n<th id=\"S4.T7.5.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\">Deviation(1e-2)</th>\n</tr>\n<tr id=\"S4.T7.5.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T7.5.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.2%</th>\n<th id=\"S4.T7.5.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.4%</th>\n<th id=\"S4.T7.5.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.6%</th>\n<th id=\"S4.T7.5.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.8%</th>\n<th id=\"S4.T7.5.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">1.0%</th>\n<th id=\"S4.T7.5.2.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">mean</th>\n<th id=\"S4.T7.5.2.2.7\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\">std</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T7.5.3.1\" class=\"ltx_tr\">\n<td id=\"S4.T7.5.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">ours-CD</td>\n<td id=\"S4.T7.5.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.0090</td>\n<td id=\"S4.T7.5.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.0099</td>\n<td id=\"S4.T7.5.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">65.49%</td>\n<td id=\"S4.T7.5.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.256</td>\n<td id=\"S4.T7.5.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">0.230</td>\n<td id=\"S4.T7.5.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">0.218</td>\n<td id=\"S4.T7.5.3.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\">0.211</td>\n<td id=\"S4.T7.5.3.1.9\" class=\"ltx_td ltx_align_center ltx_border_t\">0.206</td>\n<td id=\"S4.T7.5.3.1.10\" class=\"ltx_td ltx_align_center ltx_border_t\">0.47</td>\n<td id=\"S4.T7.5.3.1.11\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">0.51</td>\n</tr>\n<tr id=\"S4.T7.5.4.2\" class=\"ltx_tr\">\n<td id=\"S4.T7.5.4.2.1\" class=\"ltx_td ltx_align_center\">with GAN-loss</td>\n<td id=\"S4.T7.5.4.2.2\" class=\"ltx_td ltx_align_center\">0.0081</td>\n<td id=\"S4.T7.5.4.2.3\" class=\"ltx_td ltx_align_center\">0.0081</td>\n<td id=\"S4.T7.5.4.2.4\" class=\"ltx_td ltx_align_center\">73.69%</td>\n<td id=\"S4.T7.5.4.2.5\" class=\"ltx_td ltx_align_center\">0.259</td>\n<td id=\"S4.T7.5.4.2.6\" class=\"ltx_td ltx_align_center\">0.222</td>\n<td id=\"S4.T7.5.4.2.7\" class=\"ltx_td ltx_align_center\">0.208</td>\n<td id=\"S4.T7.5.4.2.8\" class=\"ltx_td ltx_align_center\">0.199</td>\n<td id=\"S4.T7.5.4.2.9\" class=\"ltx_td ltx_align_center\">0.192</td>\n<td id=\"S4.T7.5.4.2.10\" class=\"ltx_td ltx_align_center\">0.22</td>\n<td id=\"S4.T7.5.4.2.11\" class=\"ltx_td ltx_nopad_r ltx_align_center\">0.28</td>\n</tr>\n<tr id=\"S4.T7.5.5.3\" class=\"ltx_tr\">\n<td id=\"S4.T7.5.5.3.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">ours</td>\n<td id=\"S4.T7.5.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T7.5.5.3.2.1\" class=\"ltx_text ltx_font_bold\">0.0080</span></td>\n<td id=\"S4.T7.5.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T7.5.5.3.3.1\" class=\"ltx_text ltx_font_bold\">0.0078</span></td>\n<td id=\"S4.T7.5.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T7.5.5.3.4.1\" class=\"ltx_text ltx_font_bold\">74.05%</span></td>\n<td id=\"S4.T7.5.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T7.5.5.3.5.1\" class=\"ltx_text ltx_font_bold\">0.245</span></td>\n<td id=\"S4.T7.5.5.3.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T7.5.5.3.6.1\" class=\"ltx_text ltx_font_bold\">0.213</span></td>\n<td id=\"S4.T7.5.5.3.7\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T7.5.5.3.7.1\" class=\"ltx_text ltx_font_bold\">0.200</span></td>\n<td id=\"S4.T7.5.5.3.8\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T7.5.5.3.8.1\" class=\"ltx_text ltx_font_bold\">0.192</span></td>\n<td id=\"S4.T7.5.5.3.9\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T7.5.5.3.9.1\" class=\"ltx_text ltx_font_bold\">0.187</span></td>\n<td id=\"S4.T7.5.5.3.10\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T7.5.5.3.10.1\" class=\"ltx_text ltx_font_bold\">0.22</span></td>\n<td id=\"S4.T7.5.5.3.11\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\"><span id=\"S4.T7.5.5.3.11.1\" class=\"ltx_text ltx_font_bold\">0.27</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Ablation on Loss Terms.\nTo validate our choice on the loss terms, we design two ablation experiments. In the first experiment, we replace the Sinkhorn loss with the CD distance in our method as [7] and report the performance (ours-CD) in Table VII. It clearly demonstrates the superiority of the Sinkhorn divergence reconstruction loss over CD. For example, the F-score using the Sinkhorn loss is about 7% better than that using the CD loss. Rather than just measuring the distance between every nearest point between two point sets in the CD loss, the Sinkhorn loss considers the joint probability between two point sets and encourages the distribution of the generated points to lie on the ground-truth mesh surface. In the second experiment, we add the GAN-loss to our full-model (with GAN-loss). The scores on 4x are reported in Table VII. We found the GAN loss does not further improve the performance of our model. Thus, we do not include it in our implementation."
        ]
    },
    "S4.T8": {
        "caption": "TABLE VIII: Experiments on training Meta-PU with different upscale ranges Rm‚Äãa‚ÄãxsubscriptùëÖùëöùëéùë•R_{max} and testing with scale factor 444. Training with a wider range brings performance improvements.",
        "table": "<table id=\"S4.T8.8\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T8.8.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T8.8.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T8.8.1.1.1.1\" class=\"ltx_text\">Method</span></th>\n<th id=\"S4.T8.8.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T8.8.1.1.2.1\" class=\"ltx_text\">CD</span></th>\n<th id=\"S4.T8.8.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T8.8.1.1.3.1\" class=\"ltx_text\">EMD</span></th>\n<th id=\"S4.T8.8.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T8.8.1.1.4.1\" class=\"ltx_text\">F-score</span></th>\n<th id=\"S4.T8.8.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"5\">NUC with different p</th>\n<th id=\"S4.T8.8.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\">Deviation(1e-2)</th>\n</tr>\n<tr id=\"S4.T8.8.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T8.8.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.2%</th>\n<th id=\"S4.T8.8.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.4%</th>\n<th id=\"S4.T8.8.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.6%</th>\n<th id=\"S4.T8.8.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.8%</th>\n<th id=\"S4.T8.8.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">1.0%</th>\n<th id=\"S4.T8.8.2.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">mean</th>\n<th id=\"S4.T8.8.2.2.7\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\">std</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T8.8.3.1\" class=\"ltx_tr\">\n<td id=\"S4.T8.8.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">ours(max5)</td>\n<td id=\"S4.T8.8.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T8.8.3.1.2.1\" class=\"ltx_text ltx_font_bold\">0.0079</span></td>\n<td id=\"S4.T8.8.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.015</td>\n<td id=\"S4.T8.8.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">72.85%</td>\n<td id=\"S4.T8.8.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.386</td>\n<td id=\"S4.T8.8.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">0.341</td>\n<td id=\"S4.T8.8.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">0.324</td>\n<td id=\"S4.T8.8.3.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\">0.314</td>\n<td id=\"S4.T8.8.3.1.9\" class=\"ltx_td ltx_align_center ltx_border_t\">0.305</td>\n<td id=\"S4.T8.8.3.1.10\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T8.8.3.1.10.1\" class=\"ltx_text ltx_font_bold\">0.20</span></td>\n<td id=\"S4.T8.8.3.1.11\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\"><span id=\"S4.T8.8.3.1.11.1\" class=\"ltx_text ltx_font_bold\">0.22</span></td>\n</tr>\n<tr id=\"S4.T8.8.4.2\" class=\"ltx_tr\">\n<td id=\"S4.T8.8.4.2.1\" class=\"ltx_td ltx_align_center\">ours(max9)</td>\n<td id=\"S4.T8.8.4.2.2\" class=\"ltx_td ltx_align_center\">0.0081</td>\n<td id=\"S4.T8.8.4.2.3\" class=\"ltx_td ltx_align_center\">0.011</td>\n<td id=\"S4.T8.8.4.2.4\" class=\"ltx_td ltx_align_center\">73.39%</td>\n<td id=\"S4.T8.8.4.2.5\" class=\"ltx_td ltx_align_center\">0.248</td>\n<td id=\"S4.T8.8.4.2.6\" class=\"ltx_td ltx_align_center\">0.214</td>\n<td id=\"S4.T8.8.4.2.7\" class=\"ltx_td ltx_align_center\">0.202</td>\n<td id=\"S4.T8.8.4.2.8\" class=\"ltx_td ltx_align_center\">0.193</td>\n<td id=\"S4.T8.8.4.2.9\" class=\"ltx_td ltx_align_center\">0.189</td>\n<td id=\"S4.T8.8.4.2.10\" class=\"ltx_td ltx_align_center\">0.22</td>\n<td id=\"S4.T8.8.4.2.11\" class=\"ltx_td ltx_nopad_r ltx_align_center\">0.27</td>\n</tr>\n<tr id=\"S4.T8.8.5.3\" class=\"ltx_tr\">\n<td id=\"S4.T8.8.5.3.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">ours(max16)</td>\n<td id=\"S4.T8.8.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.0080</td>\n<td id=\"S4.T8.8.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T8.8.5.3.3.1\" class=\"ltx_text ltx_font_bold\">0.0078</span></td>\n<td id=\"S4.T8.8.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T8.8.5.3.4.1\" class=\"ltx_text ltx_font_bold\">74.05%</span></td>\n<td id=\"S4.T8.8.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T8.8.5.3.5.1\" class=\"ltx_text ltx_font_bold\">0.245</span></td>\n<td id=\"S4.T8.8.5.3.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T8.8.5.3.6.1\" class=\"ltx_text ltx_font_bold\">0.213</span></td>\n<td id=\"S4.T8.8.5.3.7\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T8.8.5.3.7.1\" class=\"ltx_text ltx_font_bold\">0.200</span></td>\n<td id=\"S4.T8.8.5.3.8\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T8.8.5.3.8.1\" class=\"ltx_text ltx_font_bold\">0.192</span></td>\n<td id=\"S4.T8.8.5.3.9\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T8.8.5.3.9.1\" class=\"ltx_text ltx_font_bold\">0.187</span></td>\n<td id=\"S4.T8.8.5.3.10\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.22</td>\n<td id=\"S4.T8.8.5.3.11\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\">0.27</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Ablation on Different Scale Ranges.\nTo test the influence of the scale factor ranges in our method, we design an experiment comparing our model trained with different Rm‚Äãa‚ÄãxsubscriptùëÖùëöùëéùë•R_{max} but all tested with R=4ùëÖ4R=4. The result is reported in Table VIII, in which we compare the results of the models trained with Rm‚Äãa‚Äãx=5,9,16subscriptùëÖùëöùëéùë•5916R_{max}=5,9,16. Generally, ours(max16) trained with the widest range performs the best, while ours(max9) are better than ours(max5).\nFrom this, we can conclude that upsampling tasks with scale factors have some shared properties that allow them to benefit from each other during joint training, further allowing the models to learn some common knowledge about upsampling as well."
        ]
    },
    "S4.T9": {
        "caption": "TABLE IX: Quantitative results on SHREC15 with scale=4. The NUC scores are tested with p=0.8%ùëùpercent0.8p=0.8\\%.",
        "table": null,
        "footnotes": [],
        "references": [
            "Upsampling on an Unseen Dataset: SHREC15. We test Meta-PU on the unseen dataset SHREC15¬†[32] without fine-tuning, to further validate the generalizability of our model. The quantitative results are shown in Table IX."
        ]
    },
    "S4.T10": {
        "caption": "TABLE X: Results of point cloud classification with PointNet¬†[30] on the ModelNet40 testing set. After upsampling with Meta-PU, classification performance improved significantly.",
        "table": "<table id=\"S4.T10.10\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T10.10.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T10.10.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">#Points</th>\n<th id=\"S4.T10.10.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">512</th>\n<th id=\"S4.T10.10.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">2048</th>\n<th id=\"S4.T10.10.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">2048(from 512)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T10.10.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T10.10.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">accuracy(%)</td>\n<td id=\"S4.T10.10.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">84.85</td>\n<td id=\"S4.T10.10.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">88.61</td>\n<td id=\"S4.T10.10.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">88.09</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Point Cloud Classification. In this application, we aim to demonstrate that point upsampling can potentially improve the classification results for sparse point clouds. In detail, we first train the PointNet on the ModelNet40 training set with 204820482048 input points for each model. During testing, we prepare three datasets for each model: 1) 204820482048 points uniformly sampled from the corresponding shape surface (referred to as ‚Äú2048‚Äù); 2) 512512512 points non-uniformly sampled from the 204820482048 points (referred to as ‚Äú512‚Äù); 3) the 4√ó4\\times upsampling results obtained from the 512512512 points with Meta-PU (referred as ‚Äú2048(from 512)‚Äù). As shown in Table. X, because ‚Äú512‚Äù is sparser than ‚Äú2048‚Äù, it results in lower classification accuracy. By using Meta-PU for upsampling, ‚Äú2048(from 512)‚Äù can achieve a significant performance gain and is comparable with the original ‚Äú2048‚Äù."
        ]
    }
}