{
    "id_table_1": {
        "caption": "Table 1.  Effectiveness of manually created evaluation scenarios against open-source Retrieval Augmented Generation (RAG) solutions. Pass - evaluation scenario succeeded, Fail - evaluation scenario produced a defect, and Partial - answer correct but references incorrect.",
        "table": "S1.T1.1",
        "footnotes": [],
        "references": [
            "Retrieval Augmented Generation (RAG) has recently gained popularity for use in question and answer systems, as they better understand query context and meaning  (Jeong et al . ,  2024 ; Yan et al . ,  2024 ; Asai et al . ,  2023 ; Es et al . ,  2023 ; Saad-Falcon et al . ,  2023 ) . However, RAG has specific limitations and failure points  (Barnett et al . ,  2024 ; Es et al . ,  2023 ; Jeong et al . ,  2024 ; Wu et al . ,  2024 )  due to imprecise embedding models and uncertainties introduced when generating results with large language models. Thus, questions a human takes for granted, a RAG pipeline may struggle with i.e. answering multiple questions from either a single document or a set of documents (see  Table 1  for more examples). These limitations are uncovered and potentially addressed through an evaluation of the a) quality of answers, b) impact of domain, and c) architectural choices. Improving RAG pipelines currently is often a manual and iterative process, that can be better supported by automation of the evaluation process - similar to automated test case generation.",
            "Jack asks different variations of questions (e.g. number, date/time, multiple-choice questions, multiple questions combined in a single question) from 5 open-source RAG pipelines: 1) Quivr 1 1 1 https://github.com/QuivrHQ/quivr , 2) Danswer 2 2 2 https://github.com/danswer-ai/danswer , 3) Ragflow 3 3 3 https://github.com/infiniflow/ragflow , 4) Verba 4 4 4 https://github.com/weaviate/Verba , and 5) Rag-stack 5 5 5 https://github.com/psychic-api/rag-stack .  Table 1  shows the results of manual execution of each question. All RAG pipelines failed to provide accurate responses for all evaluation scenarios (except S2). Jack realises the critical need to evaluate and improve the robustness of these pipelines to ensure they can handle a variety of evaluation scenarios successfully. Jack was wondering how to automatically generate question-answer pairs related to his domain, as the number of questions that can be asked from a given corpus is infinite and there is no systematic way to evaluate the developed RAG pipeline. Also, the manual approach is time-consuming. Therefore, Jack started looking at existing tools/approaches to generate question-answer pairs from documents and to consider variations of questions.  Table 2  compares existing tools. Also, tracing failures within the RAG pipeline is challenging for Jack, particularly when determining which component  retrieval, indexer, prompt, LLM or generator  contributed to the failure. Lack of clear visibility into the systems internal workings complicates the process of identifying and resolving failures effectively.",
            "To address the issues discussed in the motivating example ( section 2 ), we propose a novel approach, RAGProbe, that leverages LLMs for generating context specific evaluation scenarios and evaluates a given RAG pipeline. As shown in  Figure 1 , our approach has three key components: 1) Q&A Generator, 2) RAG Evaluation Runner, and 3) Semantic Answer Evaluator. The Q&A Generator takes an existing document corpus as an input, and then applies evaluation scenario schemas to generate questions and answers. The RAG Evaluation Runner is responsible for adapting to the RAG implementation (handle authentication and mapping to API) and collecting answers to all generated questions from the RAG pipeline under evaluation. Semantic Answer Evaluator compares generated answers by Q&A Generator against RAG pipeline generated answers. To handle the ambiguity in natural language responses, we extend OpenAI evals ClosedQA template 6 6 6 https://github.com/openai/evals/blob/main/evals/registry/modelgraded/closedqa.yaml  to determine whether the RAG generated response is correct based on the expected response.",
            "As shown in  Figure 1 , Q&A Generator of RAGProbe will use the corpus of documents, provided by developers, to generate question-answer pairs. RAG Evaluation Runner takes the RAG pipeline, given by developers, adapts to its implementation (e.g. authentication, mapping to API) to generate answers for generated questions by Q&A Generator. Semantic Answer Evaluator compares the answers generated by Q&A Generator against the answers generated by the given RAG pipeline, and provides a report on passing and failing questions. Semantic Answer Evaluator allows developers to substitute other evaluation metrics.",
            "By using RAGProbe (as shown in  Figure 1 ), Jack is able to: 1) generate questions and answers relevant to his domain, 2) have a schema of evaluation scenarios, 3) have a set of evaluation scenarios to evaluate a RAG pipeline, and 4) have an automated way to evaluate RAG during development or as part of a CI/CD pipeline.",
            "For a set of corpus, we use i) Qasper  (Dasigi et al . ,  2021 ) , ii) Google Natural Questions (NQ)  (Kwiatkowski et al . ,  2019 ) , and iii) MS Marco  (Nguyen et al . ,  2016 ) . Qasper is a dataset of 1,585 scientific research papers. Google NQ corpus is a question answering dataset with 307,373 training examples. These documents are from Wikipedia pages. MS Marco is a dataset of real web documents using the Bing search engine. MS Marco dataset contains 3.2 million documents and 8.8 million passages. From all three datasets, we used documents that could be downloaded and indexed by our selected RAG pipelines (see  subsubsection 5.1.2 ). We noticed that some URLs in MS Marco dataset were obsolete and no longer valid, so we excluded those URLs when downloading documents. Qasper dataset contains of PDF files, Google NQ dataset contains of HTML files, and MS Marco dataset contains a combination of PDF and HTML files. With respect to domains, Qasper dataset belongs to academic or scientific research domain which has scientific research papers. Both Google NQ and MS Marco datasets are open-domain, meaning they do not pertain to any specific domain. All documents were converted to plain text before ingestion.",
            "When evaluating the RAG generated responses against the expected responses, we considered different evaluation metrics such as correctness, relevance, completeness, consistency, explicitness, contradiction, and no-question related information. However, other evaluation metrics such as faithfulness and bias were not included. To address this, the existing Semantic Answer Evaluator (as shown in  Figure 1 ) can be extended by defining additional metrics to evaluate the responses.",
            "As shown in  Figure 1 , developers can consider the proposed approach to evaluate RAG pipelines that they are developing, and pinpointing the failure points. As can be seen in  Table 9 , developers can apply suggested fixes based on failing question-answer pairs. Automated evaluation for RAG pipelines offers benefits, including increased efficiency by reducing manual effort, ensuring consistent and reliable evaluation by minimizing human errors, and enabling integration into CI/CD pipelines. Understanding and fixing these failure points is essential for the development of effective and robust RAG pipelines. Our approach provides a guidance for developers for evaluating RAG pipelines in information retrieval and generation contexts."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.  Comparison of existing tools for evaluating RAG pipelines",
        "table": "S2.T2.1",
        "footnotes": [],
        "references": [
            "Jack asks different variations of questions (e.g. number, date/time, multiple-choice questions, multiple questions combined in a single question) from 5 open-source RAG pipelines: 1) Quivr 1 1 1 https://github.com/QuivrHQ/quivr , 2) Danswer 2 2 2 https://github.com/danswer-ai/danswer , 3) Ragflow 3 3 3 https://github.com/infiniflow/ragflow , 4) Verba 4 4 4 https://github.com/weaviate/Verba , and 5) Rag-stack 5 5 5 https://github.com/psychic-api/rag-stack .  Table 1  shows the results of manual execution of each question. All RAG pipelines failed to provide accurate responses for all evaluation scenarios (except S2). Jack realises the critical need to evaluate and improve the robustness of these pipelines to ensure they can handle a variety of evaluation scenarios successfully. Jack was wondering how to automatically generate question-answer pairs related to his domain, as the number of questions that can be asked from a given corpus is infinite and there is no systematic way to evaluate the developed RAG pipeline. Also, the manual approach is time-consuming. Therefore, Jack started looking at existing tools/approaches to generate question-answer pairs from documents and to consider variations of questions.  Table 2  compares existing tools. Also, tracing failures within the RAG pipeline is challenging for Jack, particularly when determining which component  retrieval, indexer, prompt, LLM or generator  contributed to the failure. Lack of clear visibility into the systems internal workings complicates the process of identifying and resolving failures effectively.",
            "To address the issues discussed in the motivating example ( section 2 ), we propose a novel approach, RAGProbe, that leverages LLMs for generating context specific evaluation scenarios and evaluates a given RAG pipeline. As shown in  Figure 1 , our approach has three key components: 1) Q&A Generator, 2) RAG Evaluation Runner, and 3) Semantic Answer Evaluator. The Q&A Generator takes an existing document corpus as an input, and then applies evaluation scenario schemas to generate questions and answers. The RAG Evaluation Runner is responsible for adapting to the RAG implementation (handle authentication and mapping to API) and collecting answers to all generated questions from the RAG pipeline under evaluation. Semantic Answer Evaluator compares generated answers by Q&A Generator against RAG pipeline generated answers. To handle the ambiguity in natural language responses, we extend OpenAI evals ClosedQA template 6 6 6 https://github.com/openai/evals/blob/main/evals/registry/modelgraded/closedqa.yaml  to determine whether the RAG generated response is correct based on the expected response.",
            "For a set of corpus, we use i) Qasper  (Dasigi et al . ,  2021 ) , ii) Google Natural Questions (NQ)  (Kwiatkowski et al . ,  2019 ) , and iii) MS Marco  (Nguyen et al . ,  2016 ) . Qasper is a dataset of 1,585 scientific research papers. Google NQ corpus is a question answering dataset with 307,373 training examples. These documents are from Wikipedia pages. MS Marco is a dataset of real web documents using the Bing search engine. MS Marco dataset contains 3.2 million documents and 8.8 million passages. From all three datasets, we used documents that could be downloaded and indexed by our selected RAG pipelines (see  subsubsection 5.1.2 ). We noticed that some URLs in MS Marco dataset were obsolete and no longer valid, so we excluded those URLs when downloading documents. Qasper dataset contains of PDF files, Google NQ dataset contains of HTML files, and MS Marco dataset contains a combination of PDF and HTML files. With respect to domains, Qasper dataset belongs to academic or scientific research domain which has scientific research papers. Both Google NQ and MS Marco datasets are open-domain, meaning they do not pertain to any specific domain. All documents were converted to plain text before ingestion.",
            "Figure 2  shows the summary of automatic execution of automatically generated questions. In total, there were 150 generated questions per scenario, that were executed across 5 RAG pipelines. Scenario S5 has the highest failure rate at 91%, followed by S4 at 78% and S6 at 65%, indicating these scenarios are the most problematic. Scenarios S1 and S2 show moderate failure rates at 45% and 40% respectively. Scenario S3 has the lowest failure rate at 29%, suggesting it encounters the fewest number of failing questions across 5 RAG pipelines."
        ]
    },
    "id_table_3": {
        "caption": "Table 3.  Overview of evaluation scenarios with example question-answer pairs",
        "table": "S3.T3.1",
        "footnotes": [],
        "references": [
            "We identify 6 illustrative scenarios to demonstrate the end-to-end automation of our approach. Three scenarios were selected based on the literature: 1) questions to retrieve numbers  (Rasool et al . ,  2023 ) , 2) questions to retrieve date/time  (Wu et al . ,  2024 ) , and 3) multiple-choice questions  (Rasool et al . ,  2023 ) . The remaining three scenarios were identified from a recent work that highlighted 7 failure points of RAG systems  (Barnett et al . ,  2024 ) . These are: 4) combining multiple questions for which answers are in a single document, 5) combining multiple questions for which answers are in a set of documents, and 6) asking a question for which answer is not in the corpus. Examples of each scenario are shown in  Table 3  and we describe each scenario below.",
            "Our approach, compared to the state-of-the-art approach, exposed increased failure rates across each dataset and across each RAG pipeline. As shown in  Figure 3 , the state-of-the-art approach exposed 37%, 37%, and 42% failure rates, whereas our approach exposed 60%, 53%, and 62% failure rates for Qasper, Google NQ, and MS Marco datasets respectively. The state-of-the-art approach revealed 37%, 21%, 45%, 19%, and 71% failure rates whereas our approach revealed 45%, 55%, 63%, 45%, and 83% failure rates across Quivr, Danswer, Ragflow, Verba, and Rag-stack respectively, as can be seen in  Figure 4 . When performed the Wilcoxon signed rank test on failure rate pairs, it resulted in p-values of 0.25, and 0.0625 per dataset and per RAG pipeline respectively. Both p-values being higher than the common significance level of 0.05, suggesting that there is no significant difference in the failure rates across different datasets and across different RAG pipelines. This implies that the observed differences in failure rates might be due to random chance rather than inherent differences in the datasets or RAG pipelines being evaluated. Consequently, our evaluation shows that the performance of the RAG pipelines is relatively consistent across different datasets and pipeline implementations."
        ]
    },
    "id_table_4": {
        "caption": "Table 4.  Overview of the open-source RAG pipelines with default settings. Note: our approach is a black box testing approach.",
        "table": "S5.T4.1",
        "footnotes": [],
        "references": [
            "We selected 5 open-source RAG repositories from Github (see  Table 4 ). Our selection criteria for RAG repositories was 1) contains keywords retrieval augmented generation or rag, 2) timeline (the repository must be created in the last 5 years), 3) popularity (the repository must have at least 1000 stars), 4) activeness (the repository must have at least a commit in the last 2 years), 5) programming language (the repository doesnt necessarily have to be Python-specific), 6) functionality (the repository should support uploading documents, the repository should not test RAG pipelines, and the repository does not require code changes to upload documents), 7) purpose (the repository should not be libraries or library wrappers or API wrappers), and 8) language (the repository description should be written in English).",
            "The search term retrieval augmented generation yielded 1.2k projects, while the term RAG returned 36.4k projects. Out of these GitHub projects, repositories with more than 1000 stars and created after 2019, resulted in 53 repositories. Out of these 53 repositories, project description was written in English for 50 repositories. We manually inspected each of these repositories to check whether 1) the repository supports uploading documents, 2) the repository does not require changing code to upload documents, 3) the repository does not test RAG pipelines, and 4) the repository is not a library or a library wrapper or an API wrapper (For example, Haystack 8 8 8 https://github.com/deepset-ai/haystack , RAGatouille 9 9 9 https://github.com/bclavie/RAGatouille  were excluded because they were libraries). Our selection criteria resulted in 6 repositories, and we manually set up all these 6 repositories for evaluation. We failed to set up one repository, superagent 10 10 10 https://github.com/superagent-ai/superagent  due to complexity with indexing documents. This resulted in 5 RAG repositories, which are 1) Quivr, 2) Danswer, 3) Ragflow, 4) Verba, and 5) Rag-stack.  Table 4  shows the summary of the selected RAG repositories for evaluation. We used default settings of these repositories to execute the generated questions.",
            "Our approach, compared to the state-of-the-art approach, exposed increased failure rates across each dataset and across each RAG pipeline. As shown in  Figure 3 , the state-of-the-art approach exposed 37%, 37%, and 42% failure rates, whereas our approach exposed 60%, 53%, and 62% failure rates for Qasper, Google NQ, and MS Marco datasets respectively. The state-of-the-art approach revealed 37%, 21%, 45%, 19%, and 71% failure rates whereas our approach revealed 45%, 55%, 63%, 45%, and 83% failure rates across Quivr, Danswer, Ragflow, Verba, and Rag-stack respectively, as can be seen in  Figure 4 . When performed the Wilcoxon signed rank test on failure rate pairs, it resulted in p-values of 0.25, and 0.0625 per dataset and per RAG pipeline respectively. Both p-values being higher than the common significance level of 0.05, suggesting that there is no significant difference in the failure rates across different datasets and across different RAG pipelines. This implies that the observed differences in failure rates might be due to random chance rather than inherent differences in the datasets or RAG pipelines being evaluated. Consequently, our evaluation shows that the performance of the RAG pipelines is relatively consistent across different datasets and pipeline implementations."
        ]
    },
    "id_table_5": {
        "caption": "Table 5.  An example of failure with a question and answer",
        "table": "S5.T5.1",
        "footnotes": [],
        "references": [
            "For a set of corpus, we use i) Qasper  (Dasigi et al . ,  2021 ) , ii) Google Natural Questions (NQ)  (Kwiatkowski et al . ,  2019 ) , and iii) MS Marco  (Nguyen et al . ,  2016 ) . Qasper is a dataset of 1,585 scientific research papers. Google NQ corpus is a question answering dataset with 307,373 training examples. These documents are from Wikipedia pages. MS Marco is a dataset of real web documents using the Bing search engine. MS Marco dataset contains 3.2 million documents and 8.8 million passages. From all three datasets, we used documents that could be downloaded and indexed by our selected RAG pipelines (see  subsubsection 5.1.2 ). We noticed that some URLs in MS Marco dataset were obsolete and no longer valid, so we excluded those URLs when downloading documents. Qasper dataset contains of PDF files, Google NQ dataset contains of HTML files, and MS Marco dataset contains a combination of PDF and HTML files. With respect to domains, Qasper dataset belongs to academic or scientific research domain which has scientific research papers. Both Google NQ and MS Marco datasets are open-domain, meaning they do not pertain to any specific domain. All documents were converted to plain text before ingestion.",
            "To answer RQ1, we random sampled 10 documents from each dataset of Qasper, Google NQ, and MS Marco for generation of question-answer pairs. Then, we converted the selected documents into a text file (both PDF and HTML files). We automatically generated 30 questions for each evaluation scenario. This resulted in 180 questions to be executed across one RAG pipeline. Then, we captured the generated responses and evaluated against the expected responses as per the evaluation metrics. Finally, we calculated the failure rate per evaluation scenario. Further, we break down failure rate per evaluation scenario per RAG pipeline.  Table 5  shows an example of failure with a question and answer.",
            "As shown in  Figure 5 , MS Marco had the highest number of failing questions, whilst Google NQ had the lowest number of failing questions. 60%, 53%, and 62% failure rates were observed for Qasper, Google NQ, and MS Marco datasets respectively. The high failure rates across all datasets indicate that RAG pipelines struggle consistently with the complexity and variability of questions in both academic and open-domain datasets."
        ]
    },
    "id_table_6": {
        "caption": "Table 6.  Failure rates of RAG pipelines across evaluation scenarios",
        "table": "S5.T6.1",
        "footnotes": [],
        "references": [
            "As shown in  Table 6 , Quivr and Rag-stack exhibited the highest failure rates (100%) in scenarios S4 and S5. When asking questions for which the answer is not in the corpus, Danswer did not generate any correct responses, but performed well in multiple-choice questions. During the manual inspection, we found that Danswer generated responses by referring to the large language models trained knowledge, instead of answering The system doesnt know the answer. Ragflow had the lowest failure rate (27%) with multiple-choice questions compared to all other scenarios for this RAG pipeline. Verba had the lowest failure rate (17%) when handling multiple-choice questions compared to all other scenarios for this RAG pipeline. Compared to 5 RAG pipelines, Rag-stack had the highest failure rates in multiple scenarios (S1, S2, S3, S4, S5), indicating significant challenges in handling complex questions and specific information retrieval."
        ]
    },
    "id_table_7": {
        "caption": "Table 7.  Comparison of validity of questions by comparing similarity between pre-assigned chunks for generated questions versus retrieved chunks.",
        "table": "S5.T7.1",
        "footnotes": [],
        "references": [
            "Table 7  compares the validity of questions generated by RAGAS and our approach by evaluating the similarity between pre-assigned chunks for generated questions and retrieved chunks. For the Qasper dataset, RAGAS achieved an 87% validity rate, while our approach achieved 90%. For the Google NQ dataset, RAGAS had a 93% validity rate, compared to 98% with our approach. For the MS Marco dataset, RAGAS achieved an 85% validity rate, while our approach reached 92%. This demonstrates that our approach consistently outperforms RAGAS in generating valid questions across all datasets."
        ]
    },
    "id_table_8": {
        "caption": "Table 8.  Number of RAG pipelines failed across evaluation scenarios and datasets",
        "table": "S5.T8.1",
        "footnotes": [],
        "references": [
            "Table 8  shows the number of RAG pipelines which have failed to produce correct responses across evaluation scenarios for each dataset. In summary, Google NQ and MS Marco have the highest number of failing scenarios, where all RAG pipelines failed in 5 scenarios. Qasper shows more variability, with one RAG pipeline passing in S2 and S6 scenarios."
        ]
    },
    "id_table_9": {
        "caption": "Table 9.  Overview of evaluation scenarios with the recommended fixes based on the literature",
        "table": "S6.T9.1",
        "footnotes": [],
        "references": [
            "As shown in  Figure 1 , developers can consider the proposed approach to evaluate RAG pipelines that they are developing, and pinpointing the failure points. As can be seen in  Table 9 , developers can apply suggested fixes based on failing question-answer pairs. Automated evaluation for RAG pipelines offers benefits, including increased efficiency by reducing manual effort, ensuring consistent and reliable evaluation by minimizing human errors, and enabling integration into CI/CD pipelines. Understanding and fixing these failure points is essential for the development of effective and robust RAG pipelines. Our approach provides a guidance for developers for evaluating RAG pipelines in information retrieval and generation contexts."
        ]
    },
    "global_footnotes": [
        "https://github.com/QuivrHQ/quivr",
        "https://github.com/danswer-ai/danswer",
        "https://github.com/infiniflow/ragflow",
        "https://github.com/weaviate/Verba",
        "https://github.com/psychic-api/rag-stack",
        "https://github.com/openai/evals/blob/main/evals/registry/modelgraded/closedqa.yaml",
        "https://figshare.com/s/e0d74c0d346fd2e05d59",
        "https://github.com/deepset-ai/haystack",
        "https://github.com/bclavie/RAGatouille",
        "https://github.com/superagent-ai/superagent",
        "https://figshare.com/s/e0d74c0d346fd2e05d59"
    ]
}