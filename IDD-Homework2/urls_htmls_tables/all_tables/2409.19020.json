{
    "id_table_1": {
        "caption": "Table 1:  Data Statistics",
        "table": "S2.T1.1.1",
        "footnotes": [],
        "references": [
            "The final stage in DiaSynths pipeline is the generation of dialogues, where all the componentssubtopics, personas, and characteristicsconverge to create contextually rich and realistic conversations. This step uses an LLM as the backbone and CoT as the reasoning mechanism, allowing the model to simulate dialogues that incorporate various aspects of human interaction. DiaSynth generates dialogues by pairing all persona-subtopic combinations. The process also integrates predefined characteristics (Table  11 ) like emotional state, formality, and familiarity to guide the flow and style. These characteristics are defined in the CoT prompt, guiding the LLM to generate realistic, contextually appropriate dialogues. The importance of CoT and the lack of it affects the quality of the dialogues, which is shown quantitatively in Appendix  E .",
            "This section discusses the results of the data generated using DiaSynth (quality of the data and usability in downstream tasks) with different LLMs and varying few-shot examples. Specifically, we utilized Phi-3, InternLM-2.5, LLaMA-3 and GPT-4o as the LLM backbones, and the few-shot examples were sourced from DialogueSum and SAMSum datasets. These combinations allow us to evaluate the robustness and adaptability of DiaSynth across different models and few shot examples. In total, eight distinct datasets were generated using DiaSynth by pairing each LLM with the two sets of few-shot examples, resulting in all possible combinations. For each combination, DiaSynth was provided with the same 16 broad topics and tasked with generating 6 subtopics for each topic, followed by creating 6 personas for each subtopic. The statistics of the datasets generated using DiaSynth, including the number of dialogues, average number of turns, and average number of tokens per turn, are summarized in Table  1 . All the experiments were run on a single A100 GPU with the generation time ranging from 2 hours to 4 hours.",
            "To assess the percentage improvement and percentage coverage of the distributional characteristics of the in-domain data by the synthetically generated data, we use Equations  1a  and  1b  respectively. We use the scores of models finetuned on LLaMA-3 generated data because of its dominance in both quality and usability. Across the 24 reported results, the overall coverage percentage of the LLaMA-3 generated data is  90.48% . Notably, the QAGS scores of models fine-tuned on synthetic data surpass those of models trained on in-domain data, suggesting that synthetic data can match or even exceed in-domain data performance in some aspects. Excluding QAGS, the coverage percentage is calculated to be  77.07% . In addition to the average percentages, we also present the model wise percentage improvement and coverage in Table  8  and  8 . The results presented are with respect to the dialogues generated using LLaMA-3 and they illustrate clear improvements for every model, highlighting that even with moderate LLMs of small scale (3B - 8B), high-quality synthetic dialogue datasets can be effectively created across different domains and different dialogue formats.",
            "In addition to evaluating the quality and usability of dialogues produced by DiaSynth, we conducted a study on the phenomenon of hallucinations within the generated dialogues. Hallucinations in language models refer to instances where the output contains misleading or incorrect information or situations where the model repeats the same content. To evaluate the occurrence of hallucinations, we compared the generated dialogues with their respective summaries and assessed them using two well-known hallucination benchmarks:  SelfCheckGPT   Manakul et al. [ 2023 ]  and  ChainPoll   Friel and Sanyal [ 2023 ] . This analysis provides insights into the prevalence of hallucinations and informs strategies for improving dialogue quality in future iterations of DiaSynth. The results are presented in Tables  10  and  10 .",
            "Table  11  shows different characteristics that we let the LLMs reason and decide using CoT. Before generating the dialogues, the LLMs are prompted to first reason about the various characteristics list for the dialogue given the topic and the personas and how the LLMs reason are illustrated in Appendix  D .",
            "In this section of the appendix, we present the impact of CoT in the dialogues generated by DiaSynth. We generate two datasets without CoT, using Phi-3 using DialogSum and SAMSum as few-shot examples with 8 topics. Tables  13 ,  13 ,  15  and  15  compare the scores of dialogues generated with and without CoT for the FED score and GPTScore and it can be clearly that CoT tends to increase the quality of the dialogues generated by DiaSynth.",
            "We hypothesize that this improvement in quality is due to allowing the LLM to set diverse characteristics for the dialogue before generating the dialogue. This illustrates that either manually setting the relevant context or letting the LLM on its own to set the relevant context, we get better outputs, as adding relevant context lowers the probabilities of sequences that are not useful. The lower FED scores of CoT generated dialogues in Table  15 , might be because of the CoT generated dialogues being longer in length but it needs further research."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  FED scores",
        "table": "S3.T2.1.1",
        "footnotes": [],
        "references": [
            "The remainder of this paper is organized as follows: Section  2  reviews related work on dialogue datasets and synthetic data. Section  3  details the DiaSynth framework and methodology. Section  4  describes our experimental setup and evaluation. Section  5  compares the performance of models fine-tuned on DiaSynth data to in-domain data. Section  6  concludes with a summary of our findings and Section  7  discusses the limitations of DiaSynth and potential future directions for this research.",
            "The quality of the synthetic datasets produced by DiaSynth was evaluated using FED, GPTScore, and G-Eval metrics, as detailed in Tables  2 ,  3 , and  4 . The results reveal distinct variations in performance across different model and dataset configurations, reflecting the unique characteristics of each.",
            "FED:  The FED scores in Table  2  show that LLaMA-3 and GPT-4o achieve almost a perfect score (+1) in most of the criteria, while Phi-3 and InternLM-2.5 also have decent performances. GPT-4o has a clear advantage when it comes to generating likeable dialogues while there is not much separation on other criteria.",
            "Dataset-Specific Performance.  The contrasting performance of GPT-4o on the DialogSum and SAMSum datasets in Table  2  can be attributed to the differing structures of the dialogues in these datasets. DialogSum consists of more formal and structured dialogues, which aligns with the typical response style of GPT-4o, leading to its stronger performance. In contrast, SAMSum contains more casual, human-like conversations, which might explain GPT-4os relatively poorer performance, as it may not adapt as well to the informal, spontaneous nature of such dialogues. Overall, while GPT-4o excels in natural and engaging dialogue, LLaMA-3 offers the most versatility, and InternLM-2.5 provides a strong alternative with high coherence and groundedness.",
            "The observed superiority of LLaMA-3 over GPT-4o is surprising because an 8 billion 8-bit quantized model not only competes with but also performs better than GPT-4o in certain metrics. We hypothesize that this could be due to the way GPT-4o was trained, which might make it more constrained in its responses, whereas LLaMA-3, being an open-source model, operates with fewer restrictions. This allows LLaMA-3 to exhibit greater flexibility, diversity, and adaptability in generating dialogues, potentially explaining its better performance in certain metrics. These characteristics can be seen in criteria like inquisitiveness and likeability in Table  3  and, depth and diverse in Table  2 . These results suggest that for building human-like data generation frameworks, open-source LLMs are a more suitable choice than closed-source LLMs. The minimal constraints on response formatting during the training of open-source models enable them to generate more diverse, flexible, and human-like dialogues, making them better suited for tasks requiring natural and conversational interactions."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  GPTScore",
        "table": "S4.T3.1.1",
        "footnotes": [],
        "references": [
            "The remainder of this paper is organized as follows: Section  2  reviews related work on dialogue datasets and synthetic data. Section  3  details the DiaSynth framework and methodology. Section  4  describes our experimental setup and evaluation. Section  5  compares the performance of models fine-tuned on DiaSynth data to in-domain data. Section  6  concludes with a summary of our findings and Section  7  discusses the limitations of DiaSynth and potential future directions for this research.",
            "The quality of the synthetic datasets produced by DiaSynth was evaluated using FED, GPTScore, and G-Eval metrics, as detailed in Tables  2 ,  3 , and  4 . The results reveal distinct variations in performance across different model and dataset configurations, reflecting the unique characteristics of each.",
            "GPTScore:  Results illustrated in  3  are surprising in that GPT-4o is the worst performing model on GPTScore, which might require further research while LLaMA-3 clearly dominates the other models.",
            "The observed superiority of LLaMA-3 over GPT-4o is surprising because an 8 billion 8-bit quantized model not only competes with but also performs better than GPT-4o in certain metrics. We hypothesize that this could be due to the way GPT-4o was trained, which might make it more constrained in its responses, whereas LLaMA-3, being an open-source model, operates with fewer restrictions. This allows LLaMA-3 to exhibit greater flexibility, diversity, and adaptability in generating dialogues, potentially explaining its better performance in certain metrics. These characteristics can be seen in criteria like inquisitiveness and likeability in Table  3  and, depth and diverse in Table  2 . These results suggest that for building human-like data generation frameworks, open-source LLMs are a more suitable choice than closed-source LLMs. The minimal constraints on response formatting during the training of open-source models enable them to generate more diverse, flexible, and human-like dialogues, making them better suited for tasks requiring natural and conversational interactions.",
            "In this section of the appendix, we present the impact of CoT in the dialogues generated by DiaSynth. We generate two datasets without CoT, using Phi-3 using DialogSum and SAMSum as few-shot examples with 8 topics. Tables  13 ,  13 ,  15  and  15  compare the scores of dialogues generated with and without CoT for the FED score and GPTScore and it can be clearly that CoT tends to increase the quality of the dialogues generated by DiaSynth."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  G-EVAL",
        "table": "S5.T4.1",
        "footnotes": [],
        "references": [
            "The remainder of this paper is organized as follows: Section  2  reviews related work on dialogue datasets and synthetic data. Section  3  details the DiaSynth framework and methodology. Section  4  describes our experimental setup and evaluation. Section  5  compares the performance of models fine-tuned on DiaSynth data to in-domain data. Section  6  concludes with a summary of our findings and Section  7  discusses the limitations of DiaSynth and potential future directions for this research.",
            "The quality of the synthetic datasets produced by DiaSynth was evaluated using FED, GPTScore, and G-Eval metrics, as detailed in Tables  2 ,  3 , and  4 . The results reveal distinct variations in performance across different model and dataset configurations, reflecting the unique characteristics of each.",
            "G-Eval:  Table  4  highlights GPT-4os dominance in engagingness and naturalness with perfect scores (3.0) for DialogSum, while InternLM-2.5 stands out in coherence (2.9990) and groundedness (2.9973) for DialogSum, and coherence (2.9983) and groundedness (2.9952) for SAMSum, suggesting it maintains high factual accuracy."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Performance of models before and after finetuning on in-domain data",
        "table": "S5.T5.1",
        "footnotes": [],
        "references": [
            "The remainder of this paper is organized as follows: Section  2  reviews related work on dialogue datasets and synthetic data. Section  3  details the DiaSynth framework and methodology. Section  4  describes our experimental setup and evaluation. Section  5  compares the performance of models fine-tuned on DiaSynth data to in-domain data. Section  6  concludes with a summary of our findings and Section  7  discusses the limitations of DiaSynth and potential future directions for this research.",
            "The results presented in Tables  5  and  6  present the performance of the base models, models finetuned on in-domain data and models finetuned on DiaSynth generated data. Models finetuned on DiaSynth data generally improves the performances from the BERTScore and ROUGE-L metrics. Surprisingly, for some models (LED and BART) the QAGS scores were higher than the models finetuned on DiaSynth. On further exploration, we found out that these models extracted multiple sentences from the given dialogue instead of generating a summary which led to high QAGS scores. Comparing models finetuned on in-domain data to those finetuned on DiaSynth data reveals that DiaSynth finetuning generally enhances factual accuracy, with BERTScore and ROUGE-L scores remaining comparable. The disparity in BERTScore and ROUGE-L results may be due to format variations. Models fine-tuned on in-domain data were evaluated on summaries that matched the training format closely, while DiaSynth-fine-tuned models were trained on LLM-generated summaries and evaluated on human-generated summaries, leading to minor format mismatches. Comparison between the different LLMs from Table  6 , shows that GPT-4o is better at generating dialogues and summaries that are formal in nature while LLaMA-3 and open source LLMs would be better for generating dialogues that are informal and casual in nature.",
            "In this section of the appendix, we present the impact of CoT in the dialogues generated by DiaSynth. We generate two datasets without CoT, using Phi-3 using DialogSum and SAMSum as few-shot examples with 8 topics. Tables  13 ,  13 ,  15  and  15  compare the scores of dialogues generated with and without CoT for the FED score and GPTScore and it can be clearly that CoT tends to increase the quality of the dialogues generated by DiaSynth.",
            "We hypothesize that this improvement in quality is due to allowing the LLM to set diverse characteristics for the dialogue before generating the dialogue. This illustrates that either manually setting the relevant context or letting the LLM on its own to set the relevant context, we get better outputs, as adding relevant context lowers the probabilities of sequences that are not useful. The lower FED scores of CoT generated dialogues in Table  15 , might be because of the CoT generated dialogues being longer in length but it needs further research."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Performance after finetuning on synthetic data",
        "table": "S5.T6.1.1",
        "footnotes": [],
        "references": [
            "The remainder of this paper is organized as follows: Section  2  reviews related work on dialogue datasets and synthetic data. Section  3  details the DiaSynth framework and methodology. Section  4  describes our experimental setup and evaluation. Section  5  compares the performance of models fine-tuned on DiaSynth data to in-domain data. Section  6  concludes with a summary of our findings and Section  7  discusses the limitations of DiaSynth and potential future directions for this research.",
            "The results presented in Tables  5  and  6  present the performance of the base models, models finetuned on in-domain data and models finetuned on DiaSynth generated data. Models finetuned on DiaSynth data generally improves the performances from the BERTScore and ROUGE-L metrics. Surprisingly, for some models (LED and BART) the QAGS scores were higher than the models finetuned on DiaSynth. On further exploration, we found out that these models extracted multiple sentences from the given dialogue instead of generating a summary which led to high QAGS scores. Comparing models finetuned on in-domain data to those finetuned on DiaSynth data reveals that DiaSynth finetuning generally enhances factual accuracy, with BERTScore and ROUGE-L scores remaining comparable. The disparity in BERTScore and ROUGE-L results may be due to format variations. Models fine-tuned on in-domain data were evaluated on summaries that matched the training format closely, while DiaSynth-fine-tuned models were trained on LLM-generated summaries and evaluated on human-generated summaries, leading to minor format mismatches. Comparison between the different LLMs from Table  6 , shows that GPT-4o is better at generating dialogues and summaries that are formal in nature while LLaMA-3 and open source LLMs would be better for generating dialogues that are informal and casual in nature."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Summarization results on DialogSum",
        "table": "S5.E1",
        "footnotes": [],
        "references": [
            "The remainder of this paper is organized as follows: Section  2  reviews related work on dialogue datasets and synthetic data. Section  3  details the DiaSynth framework and methodology. Section  4  describes our experimental setup and evaluation. Section  5  compares the performance of models fine-tuned on DiaSynth data to in-domain data. Section  6  concludes with a summary of our findings and Section  7  discusses the limitations of DiaSynth and potential future directions for this research."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Summarization results on SAMSum",
        "table": "S5.T8.fig1.1",
        "footnotes": [],
        "references": [
            "To assess the percentage improvement and percentage coverage of the distributional characteristics of the in-domain data by the synthetically generated data, we use Equations  1a  and  1b  respectively. We use the scores of models finetuned on LLaMA-3 generated data because of its dominance in both quality and usability. Across the 24 reported results, the overall coverage percentage of the LLaMA-3 generated data is  90.48% . Notably, the QAGS scores of models fine-tuned on synthetic data surpass those of models trained on in-domain data, suggesting that synthetic data can match or even exceed in-domain data performance in some aspects. Excluding QAGS, the coverage percentage is calculated to be  77.07% . In addition to the average percentages, we also present the model wise percentage improvement and coverage in Table  8  and  8 . The results presented are with respect to the dialogues generated using LLaMA-3 and they illustrate clear improvements for every model, highlighting that even with moderate LLMs of small scale (3B - 8B), high-quality synthetic dialogue datasets can be effectively created across different domains and different dialogue formats."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Hallucination calculation for DialogSum few-shot data",
        "table": "S5.T8.fig2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_10": {
        "caption": "Table 10:  Hallucination calculation for SAMSum few-shot data",
        "table": "A1.T10.fig1.1",
        "footnotes": [],
        "references": [
            "In addition to evaluating the quality and usability of dialogues produced by DiaSynth, we conducted a study on the phenomenon of hallucinations within the generated dialogues. Hallucinations in language models refer to instances where the output contains misleading or incorrect information or situations where the model repeats the same content. To evaluate the occurrence of hallucinations, we compared the generated dialogues with their respective summaries and assessed them using two well-known hallucination benchmarks:  SelfCheckGPT   Manakul et al. [ 2023 ]  and  ChainPoll   Friel and Sanyal [ 2023 ] . This analysis provides insights into the prevalence of hallucinations and informs strategies for improving dialogue quality in future iterations of DiaSynth. The results are presented in Tables  10  and  10 ."
        ]
    },
    "id_table_11": {
        "caption": "Table 11:  Characteristics of the Dialogue for CoT Prompt",
        "table": "A1.T10.fig2.1",
        "footnotes": [],
        "references": [
            "The final stage in DiaSynths pipeline is the generation of dialogues, where all the componentssubtopics, personas, and characteristicsconverge to create contextually rich and realistic conversations. This step uses an LLM as the backbone and CoT as the reasoning mechanism, allowing the model to simulate dialogues that incorporate various aspects of human interaction. DiaSynth generates dialogues by pairing all persona-subtopic combinations. The process also integrates predefined characteristics (Table  11 ) like emotional state, formality, and familiarity to guide the flow and style. These characteristics are defined in the CoT prompt, guiding the LLM to generate realistic, contextually appropriate dialogues. The importance of CoT and the lack of it affects the quality of the dialogues, which is shown quantitatively in Appendix  E .",
            "Table  11  shows different characteristics that we let the LLMs reason and decide using CoT. Before generating the dialogues, the LLMs are prompted to first reason about the various characteristics list for the dialogue given the topic and the personas and how the LLMs reason are illustrated in Appendix  D ."
        ]
    },
    "id_table_12": {
        "caption": "Table 12:  FED scores of dialogues generated with and without CoT for DialogSum few-shot",
        "table": "A5.EGx2",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "Table 13:  GPTScore of dialogues generated with and without CoT for DialogSum few-shot",
        "table": "A5.EGx3",
        "footnotes": [],
        "references": [
            "In this section of the appendix, we present the impact of CoT in the dialogues generated by DiaSynth. We generate two datasets without CoT, using Phi-3 using DialogSum and SAMSum as few-shot examples with 8 topics. Tables  13 ,  13 ,  15  and  15  compare the scores of dialogues generated with and without CoT for the FED score and GPTScore and it can be clearly that CoT tends to increase the quality of the dialogues generated by DiaSynth."
        ]
    },
    "id_table_14": {
        "caption": "Table 14:  FED scores of dialogues generated with and without CoT for SAMSum few-shot",
        "table": "A5.EGx4",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "Table 15:  GPTScore of dialogues generated with and without CoT for SAMSum few-shot",
        "table": "A3.T11.1",
        "footnotes": [],
        "references": [
            "In this section of the appendix, we present the impact of CoT in the dialogues generated by DiaSynth. We generate two datasets without CoT, using Phi-3 using DialogSum and SAMSum as few-shot examples with 8 topics. Tables  13 ,  13 ,  15  and  15  compare the scores of dialogues generated with and without CoT for the FED score and GPTScore and it can be clearly that CoT tends to increase the quality of the dialogues generated by DiaSynth.",
            "We hypothesize that this improvement in quality is due to allowing the LLM to set diverse characteristics for the dialogue before generating the dialogue. This illustrates that either manually setting the relevant context or letting the LLM on its own to set the relevant context, we get better outputs, as adding relevant context lowers the probabilities of sequences that are not useful. The lower FED scores of CoT generated dialogues in Table  15 , might be because of the CoT generated dialogues being longer in length but it needs further research."
        ]
    },
    "id_table_16": {
        "caption": "",
        "table": "A5.T13.fig1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_17": {
        "caption": "",
        "table": "A5.T13.fig2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_18": {
        "caption": "",
        "table": "A5.T15.fig1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_19": {
        "caption": "",
        "table": "A5.T15.fig2.1",
        "footnotes": [],
        "references": []
    }
}