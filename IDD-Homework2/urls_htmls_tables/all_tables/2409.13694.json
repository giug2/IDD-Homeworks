{
    "id_table_1": {
        "caption": "Table 1:   Comparative analysis of RAG performance across different external knowledge configurations.",
        "table": "Sx3.T1.1.1",
        "footnotes": [],
        "references": [
            "Our RAG framework, RAG-X, depicted in Figure  1 , employs an LLM Agent-based Router to intelligently select relevant knowledge sources from a curated mix of structured and unstructured data, including web pages and a mock API. Our retrieval process avoids web searches, focusing exclusively on the provided information sources. The retrieval process is divided into three steps: broad retrieval narrows down vast external knowledge, focused retrieval uses sparse, dense, or hybrid methods to identify key information, and rank refinement ensures the final output is accurate and prioritized. After retrieval, we enhance the LLMs reasoning with noise chunks, Chain of Thought (CoT), and In-Context Learning (ICL), leading to more relevant responses.",
            "The experimental results in Table  1  show that RAG-X consistently outperforms both the CRAG baseline and the LLM-only model. Structured knowledge from the mock API notably enhances accuracy and reduces hallucinations compared to unstructured web sources. However, increasing knowledge inputs, like combining 50 web pages with mock API data, improves accuracy but slightly raises hallucination rates. This highlights the need to carefully balance external knowledge sources to optimize overall performance  (Wang et al.  2023b ; Yu et al.  2023 ; Shi et al.  2023 ) .",
            "As illustrated in Figure  10 , this example underscores a limitation in the systems ability to synthesize information when the retrieved contexts do not directly address the query. This highlights the challenge of ensuring that retrieved data is both relevant and specific enough to enable accurate responses.",
            "As shown in Figure  11 , this case highlights a significant challenge in the RAG system: the ability to discern and validate the contexts relevance to the specific query, particularly when the query contains geographical or contextual nuances that are not directly addressed by the retrieved information."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Performance comparison across different knowledge source selection methods.",
        "table": "Sx4.T2.1.1",
        "footnotes": [],
        "references": [
            "Table  2  offers a comparative analysis of performance across different knowledge sources, including internal, external, and combined scenarios. Integrating structured knowledge from the mock API significantly improves accuracy and reduces hallucination rates. In contrast, aggregating all sources can lead to conflicts and suboptimal results  (Liu et al.  2023b ) . Predefined fallback strategies, where the LLM prioritizes internal knowledge before querying external sources, boost accuracy but also raise hallucination rates, especially with multiple sources. While the agent-based dynamic selection strategy  (Li, Nie, and Liang  2023 )  adds flexibility, it doesnt consistently outperform predefined methods, underscoring the challenges of real-time decision-making. These findings highlight the importance of structured external knowledge for performance enhancement and the need for advanced selection mechanisms to balance knowledge coverage and minimize hallucinations in retrieval processes  (Rackauckas  2024 ; Yoran et al.  2024 ) .",
            "Figure  2  illustrates the trade-offs between dense and sparse retrieval configurations in a RAG system. While higher dense retrieval ratios boost accuracy due to richer semantic understanding, they also elevate hallucination rates, suggesting a risk of contextual misalignment. Conversely, increasing sparse retrieval may lower accuracy and fails to consistently reduce hallucinations, likely because of its dependence on surface-level keyword matching. These findings underscore the need for a dynamic, hybrid strategy that balances the strengths and weaknesses of both approaches, optimizing performance by mitigating the inherent limitations of each method  (Gu et al.  2018 ; Zhang et al.  2018 ; Cheng et al.  2022 ) ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Comparison of performance with and without broad retrieval.",
        "table": "Sx4.T3.1.1",
        "footnotes": [],
        "references": [
            "Table  3  compares the performance of the RAG system with and without a broad retrieval step, where each query is supplemented with 50 web pagesfar more than the 5 used in other experiments. The results demonstrate that this broad retrieval phase significantly enhances system efficiency, notably reducing processing time. Sparse retrieval methods like BM25  (Izacard et al.  2022 ; Cheng et al.  2021 )  filter out irrelevant data, allowing dense retrieval to focus on a more targeted subset of external knowledge  (Zhang et al.  2024 ) . These findings underscore the effectiveness of a two-tiered approach, where sparse retrieval narrows the search space, enabling dense retrieval to operate with greater precision and speed. This strategy is especially beneficial in scenarios involving extensive external knowledge, optimizing both processing time and relevance, and offering a more efficient design for RAG systems  (Jiang et al.  2024 ) .",
            "Figure  3  illustrates the context-dependent impact of incorporating Chain of Thought (CoT) into RAG systems. While CoT aims to enhance logical reasoning, the results indicate that it does not consistently improve performance  (Wei et al.  2023 ; Zhou et al.  2023 )  and can sometimes reduce accuracy, especially when dealing with multiple conflicting knowledge sources. This suggests that CoT may complicate the integration process, leading to higher missing rates and decreased effectiveness. Therefore, CoT should be carefully applied, proving beneficial in straightforward scenarios but potentially counterproductive in more complex contexts."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Performance of reranker configurations in RAG.",
        "table": "Sx4.T4.1",
        "footnotes": [],
        "references": [
            "Table  4  summarizes the impact of various reranker configurations on RAG system performance, focusing on different retrieval chunk sizes. The results show that as the number of retrieved chunks increases, hallucination rates rise, while accuracy remains stable, suggesting that the reranker performs better with smaller, more focused sets. Bypassing retrieval and directly passing all chunks to the reranker, as in configuration (3, All), results in similar performance to configurations with a large number of chunks, indicating that increasing chunk size beyond a certain point does not enhance effectiveness. These findings emphasize the need to balance the number of retrieved chunks and optimize the rerankers selection process  (Ma et al.  2023 ; Vu et al.  2023 ; Yu et al.  2024 ) , especially in large-scale retrieval scenarios.",
            "Figure  4  illustrates the effect of query position within the prompt on RAG system performance. The results show that placing the query after the reference information not only increases accuracy but also reduces hallucination rates, suggesting that the model benefits from having more context before addressing the query. Conversely, positioning the query before the reference leads to a higher hallucination rate, likely because the model lacks sufficient context, making it more prone to misinterpret the query, overlook critical details, and generate responses based on incomplete information. This experiment highlights the need for careful prompt structuring to balance accuracy, hallucination, and missing information in RAG systems  (Liu et al.  2023a ) ."
        ]
    },
    "id_table_5": {
        "caption": "Note:  (3, X) denotes retrieval of X chunks, with (3, ALL) indicating all chunks passed directly.",
        "table": "Sx4.T5.1.1",
        "footnotes": [],
        "references": [
            "Table  5  presents the impact of few-shot learning   (Dai et al.  2022 ; Dong et al.  2024 )  on RAG systems, particularly in identifying false premises across various domains. The results show that the model performs best under 0-shot conditions. However, as more examples are introduced, performance on these questions declines due to overfitting and noise. Despite this, overall accuracy improves with few-shot examples, which provide task-specific guidance. Cross-domain examples also enhance generalization and reduce hallucination rates, demonstrating the value of diverse examples in broadening the models adaptability across different query types.",
            "Figure  5  illustrates the impact of varying noise chunks on RAG system performance. The results show that as the number of noise chunks increases, accuracy initially dips but then improves, reaching its peak at moderate noise levels. Interestingly, while hallucination rates rise with the introduction of noise, they tend to stabilize and slightly decrease at higher noise levels. This suggests that a certain degree of noise may prompt the model to better filter out irrelevant information  (Cuconasu et al.  2024 ) . However, the consistent decrease in the missing rate as noise increases indicates that the model becomes more decisive, though not always accurately guided. The overall score declines with increasing noise but improves at specific levels, highlighting the complex interplay between noise and performance. These findings highlight the complex impact of noise in RAG systems, emphasizing the importance of carefully optimizing noise management strategies to maintain performance."
        ]
    },
    "id_table_6": {
        "caption": "Table 5:  Impact of few-shot learning on LLM reasoning.",
        "table": "Sx5.T6.1",
        "footnotes": [],
        "references": [
            "Impact of Retrieval Embedding.   As summarized in Table  6 , the experiment evaluates the performance of three embedding modelsBGE-small-en-v1.5, BGE-large-en-v1.5, and BGE-M3within a RAG system. The BGE-M3 model  (Chen et al.  2024 )  achieves the highest accuracy but also the highest hallucination rate, highlighting a trade-off between precision and error. BGE-large-en-v1.5 minimizes hallucinations but at the expense of accuracy, indicating a more conservative approach. BGE-small-en-v1.5 offers a balanced performance between these two extremes. The performance differences among the models stem from a trade-off between capturing rich semantics and overfitting.",
            "Impact of Chunk Size and Overlap on Retrieval Performance.  As shown in Figure  6  and Figure  7 , increasing chunk size generally enhances accuracy by providing more context. However, it also raises hallucination rates due to the retrieval of excessive or less relevant content. Chunk overlap further exacerbates this issue by introducing redundancy, which increases the risk of errors. Balancing chunk size and overlap is therefore crucial for optimizing accuracy while minimizing hallucinations in RAG systems."
        ]
    },
    "id_table_7": {
        "caption": "Table 6:  Performance comparison of embedding models.",
        "table": "Sx5.T7.1",
        "footnotes": [],
        "references": [
            "Impact of Chunk Size and Overlap on Retrieval Performance.  As shown in Figure  6  and Figure  7 , increasing chunk size generally enhances accuracy by providing more context. However, it also raises hallucination rates due to the retrieval of excessive or less relevant content. Chunk overlap further exacerbates this issue by introducing redundancy, which increases the risk of errors. Balancing chunk size and overlap is therefore crucial for optimizing accuracy while minimizing hallucinations in RAG systems.",
            "Impact of Large Language Model Selection.    In Table  7 , we compare the performance of different LLM backbones within a RAG system, particularly when using techniques like Chain of Thought (CoT) prompting. This experiment highlights the impact of the LLM backbone on handling complex reasoning tasks. The LLaMA 3.1 8B model demonstrates a balanced approach, achieving a lower hallucination rate and a slightly positive overall score. In contrast, the Qwen2-7B model, while delivering higher accuracy, exhibits a much higher hallucination rate, leading to a negative overall score. This emphasizes the importance of selecting an LLM that balances accuracy and reliability, especially when tackling complex queries with CoT prompting."
        ]
    },
    "id_table_8": {
        "caption": "Table 7:  Performance comparison between LLaMA 3.1 8B and Qwen2-7B with CoT prompting.",
        "table": "Sx5.T8.1",
        "footnotes": [],
        "references": [
            "Effect of Sampling parameters.    In Figure  8  and Table  8 , the impact of varying temperature and top-p settings on RAG performance is analyzed. The figure illustrates that increasing the temperature slightly improves accuracy, but it also raises the hallucination rate and decreases the overall score. Higher temperatures encourage more diverse responses, reducing the missing rate, but this comes at the cost of introducing inaccuracies. This suggests that while higher temperatures might help generate more comprehensive answers, they also increase the risk of unreliable outputs, necessitating careful tuning to balance accuracy and creativity."
        ]
    },
    "id_table_9": {
        "caption": "Table 8:  Comparison of top-p settings on RAG performance.",
        "table": "A2.T9.1",
        "footnotes": [],
        "references": [
            "To ensure the reproducibility and consistency of our experiments, we establish a base configuration for our Retrieval-Augmented Generation (RAG) model, detailed in Table  9 . The base hyperparameters were carefully selected to balance retrieval relevance and computational efficiency while maintaining deterministic outputs from the LLM.",
            "As shown in Figure  9 , the effectiveness of the RAG approach in this instance highlights the potential of retrieval-augmented methods to enhance the accuracy of answers by supplementing the models internal knowledge with precise external information."
        ]
    },
    "global_footnotes": [
        "https://github.com/USTCAGI/RAG-X",
        "Mingyue Cheng is the corresponding author."
    ]
}