{
    "id_table_1": {
        "caption": "Table 1:  Complexity Comparison between Exact (Gaussian Elimination), LiSSA, DataInf and HyperINF. Computational and memory complexities are obtained on a LoRA-tuned model with dimension  d  N d N d\\in\\mathbb{N} italic_d  roman_N  and rank  r  N r N r\\in\\mathbb{N} italic_r  roman_N . Assume the dimension of the LoRA matrices is identical across  L L L italic_L  different layers.",
        "table": "S1.T1.22.16",
        "footnotes": [],
        "references": [
            "The second-order gradients often incur intensive computations and instability on large-scale networks. Therefore, we conduct several approximations on Hessian matrix when applying  Equation 1  on LoRA-tuned models.",
            "In deep transformer-structured networks, the Hessian matrix is observed to be approximately block-wise diagonal according to  (Zhang et al.,  2024a ,  b ) . We, therefore, apply a  block-wise diagonal approximation  on the Hessian inverse in  Equation 1 . Given a neural network as a compositional function  f   ( x ) = f  L    f  1  ( x ) subscript f  x subscript f subscript  L  subscript f subscript  1 x f_{{\\bm{\\theta}}}(x)=f_{{\\bm{\\theta}}_{L}}\\circ\\cdots\\circ f_{{\\bm{\\theta}}_{1% }}(x) italic_f start_POSTSUBSCRIPT bold_italic_ end_POSTSUBSCRIPT ( italic_x ) = italic_f start_POSTSUBSCRIPT bold_italic_ start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT end_POSTSUBSCRIPT    italic_f start_POSTSUBSCRIPT bold_italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_x )  where for  l  [ L ] l delimited-[] L l\\in[L] italic_l  [ italic_L ] , we compute the hessian inverse on each parameter block which yields a sparse estimation as  diag  ( H 1  (  )  1 , ... , H L  (  )  1 ) diag subscript H 1 superscript  1 ... subscript H L superscript  1 \\mathrm{diag}(H_{1}({\\bm{\\theta}})^{-1},\\dots,H_{L}({\\bm{\\theta}})^{-1}) roman_diag ( italic_H start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( bold_italic_ ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT , ... , italic_H start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT ( bold_italic_ ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT )   (Grosse et al.,  2023b ) .",
            "The proof is provided in Appendix  A.4 . Following Lemma  1 , we further estimate a Hessian-gradient product using the GFIM, corresponding to the ( H  (   )  1    l k H superscript superscript   1 subscript   subscript l k H({\\bm{\\theta}}^{\\star})^{-1}\\nabla_{{\\bm{\\theta}}}\\ell_{k} italic_H ( bold_italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT  start_POSTSUBSCRIPT bold_italic_ end_POSTSUBSCRIPT roman_l start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) term in  Equation 1 . Given an invertible matrix  A A A italic_A , we have  ( I r  A )  1 = I r  A  1 superscript tensor-product subscript I r A 1 tensor-product subscript I r superscript A 1 (I_{r}\\otimes A)^{-1}=I_{r}\\otimes A^{-1} ( italic_I start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT  italic_A ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT = italic_I start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT  italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT . Therefore, denote the GFIM matrix as  G  (  )  ( g  g  )  R d  d  G  g superscript g top superscript R d d G({\\bm{\\theta}})\\triangleq({\\bm{g}}{\\bm{g}}^{\\top})\\in\\mathbb{R}^{d\\times d} italic_G ( bold_italic_ )  ( bold_italic_g bold_italic_g start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT )  roman_R start_POSTSUPERSCRIPT italic_d  italic_d end_POSTSUPERSCRIPT  for any matrix  v  R d  r v superscript R d r {\\bm{v}}\\in\\mathbb{R}^{d\\times r} bold_italic_v  roman_R start_POSTSUPERSCRIPT italic_d  italic_r end_POSTSUPERSCRIPT , it holds that:",
            "When  t    t t\\rightarrow\\infty italic_t    and  X 0  A  1 subscript X 0 superscript A 1 X_{0}\\approx A^{-1} italic_X start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT , it is proved that the sequence  { X t } subscript X t \\{X_{t}\\} { italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT }  will converge towards  A  1 superscript A 1 A^{-1} italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT  in a numerically stable way  (Petkovic,  1995 ; Ben-Israel,  1965 ; Bazan and Boos,  2018 ; Soderstrom and Stewart,  1974 ) . It is proved by  Ben-Israel and Cohen ( 1966 )  and  Petkovic ( 1995 )  that with a proper initialization, Schulzs method would converge to  A  1 superscript A 1 A^{-1} italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT  in the order of convergence at least  p = 2 p 2 p=2 italic_p = 2 . Compared to other conventional matrix inverse algorithms (e.g. Gaussian Elimination, Conjugate Gradient, GMRES), Schulzs method demonstrates superior accuracy in terms of error rate and significant efficiency gains from the GPU acceleration on matrix multiplications. We include more details in Appendix  F . With the simulation matrix inversion experiments (Section.  4 ), we show that starting from a small identity matrix or random Gaussian initialization could converge to a desirable error rate in finite steps ( t < 20 t 20 t<20 italic_t < 20 ), which demonstrates that the given algorithm is not sensitive to the initialization. We provide the pseudo-code according to Algorithm  1 .",
            "Compared to the original influence function formulation in  Equation 1 , the generalized fisher information matrix  G  (   )  R d  d G superscript   superscript R d d G({\\bm{\\theta}}^{\\star})\\in\\mathbb{R}^{d\\times d} italic_G ( bold_italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT )  roman_R start_POSTSUPERSCRIPT italic_d  italic_d end_POSTSUPERSCRIPT  reduces the memory complexity from  O  ( r 2  d 2 ) O superscript r 2 superscript d 2 O(r^{2}d^{2}) italic_O ( italic_r start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )  to  O  ( d 2 ) O superscript d 2 O(d^{2}) italic_O ( italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) . On computation complexity of Hessian-gradient product, the matrix multiplication between  ( G  (   ) +   I d )  1  R d  d superscript G superscript    subscript I d 1 superscript R d d \\displaystyle{(G({\\bm{\\theta}}^{\\star})+\\lambda I_{d})^{-1}\\in\\mathbb{R}^{d% \\times d}} ( italic_G ( bold_italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) + italic_ italic_I start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT  roman_R start_POSTSUPERSCRIPT italic_d  italic_d end_POSTSUPERSCRIPT  and  g k  R d  r subscript g k superscript R d r {\\bm{g}}_{k}\\in\\mathbb{R}^{d\\times r} bold_italic_g start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT  roman_R start_POSTSUPERSCRIPT italic_d  italic_r end_POSTSUPERSCRIPT  only requires  O  ( r  d 2 ) O r superscript d 2 O(rd^{2}) italic_O ( italic_r italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )  FLOPS, instead of  O  ( r 2  d 2 ) O superscript r 2 superscript d 2 O(r^{2}d^{2}) italic_O ( italic_r start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )  with flattened gradient vectors. Specifically, with LoRA rank  r = 16 r 16 r=16 italic_r = 16 ,  HyperINF  only requires  0.39 % percent 0.39 0.39\\% 0.39 %  memory complexity and  6.25 % percent 6.25 6.25\\% 6.25 %  computations comparing to original Hessian-vector product operations. We include the complexity comparison to other existing approximation methods in  Table 1 , where  HyperINF  showcases outstanding memory and computation efficiencies.",
            "We present the results from the synthetic experiments in  Figure 1 , where  HyperINF  with Schulzs algorithm demonstrates a remarkable accuracy and stability compared to the other two methods. Specifically, on high-dimensional matrices  M M M italic_M  with large  d d d italic_d , both  LiSSA  and  Datainf  tend to diverge with increasing approximation errors. For  LiSSA , the error would not converge but explode exponentially according to the number of iterations. Even when applying on a small dimension of matrix with  N = 200 N 200 N=200 italic_N = 200 ,  LiSSA  is not able to give an accurate approximation with a large error rate   10 5 similar-to absent superscript 10 5 \\sim 10^{5}  10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT . This might comes from the sensitivity of  LiSSA  algorithm to the initialization conditions, which could be hard to tune when apply on large-scale models. In comparison,  HyperINF  with Schulzs algorithm could always converge to a low error rate within finite iterations across all scales of  d d d italic_d  and  N N N italic_N . It implies that our proposed  HyperINF  could consistently achieve a satisfying accuracy on large-scale models and datasets, while both  LiSSA  and  DataInf  could significantly diverge from the exact value.",
            "In this section, we further apply  HyperINF  on influence function approximation on large-scale foundation models and demonstrate its effectiveness on various data attribution tasks. We compare  HyperINF  with two existing baseline methods  LiSSA   (Agarwal et al.,  2017 )  and  DataInf   (Kwon et al.,  2024 ) , as well as the Hessian-free method  TracIN , which replaces the second-order derivative  H  1 superscript H 1 H^{-1} italic_H start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT  in  Equation 1  with the identity matrix  I d subscript I d I_{d} italic_I start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT   (Pruthi et al.,  2020 ) . Across all mislabeled data detection, data selection for LLM fintuning and VLM pretraining,  HyperINF  shows promising performance compared to all baseline methods.",
            "It is worth noting that  HyperINF  with GFIM does not lead to performance degradation compared to FIM. According to  Figure 5 ,  HyperINF  with GFIM could consistently achieve comparable or better performance than  HyperINF  with FIM, while being  ( 1 / r ) 3 superscript 1 r 3 (1/r)^{3} ( 1 / italic_r ) start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT  more efficient in computation and  ( 1 / r ) 2 superscript 1 r 2 (1/r)^{2} ( 1 / italic_r ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  in memory ( Table 1 ).",
            "Although the theoretical analysis in Lemma  1  is inspired by LoRA finetuning, we demonstrate that data selection via  HyperINF  provides substantial benefits for dense fine-tuning as well. In this setting, we compute  HyperINF  influence score according to  Equation 5  using the gradient from the last transformer block of the language model. Specifically, the GFIM is computed as described in Lemma  1  and  Equation 4 , where  d d d italic_d  refers to the larger dimension of the given matrix.",
            "Following LLaVa  (Liu et al.,  2023c ) , we adopt the commonly used VLM architecture which consists of three components: a vision backbone  V  subscript V italic- V_{\\phi} italic_V start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT , a projector  F  subscript F  F_{\\psi} italic_F start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  and a language backbone  L  M  L subscript M  LM_{\\theta} italic_L italic_M start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT . Both the vision and language backbones are pre-trained, while the projector is randomly initialized. We follow the auto-regressive training paradigm of vision-language models using multimodal instruct-tuning datasets represented as  ( x img , x text )  D v  l  m subscript x img subscript x text subscript D v l m ({\\bm{x}}_{\\text{img}},{\\bm{x}}_{\\text{text}})\\in D_{vlm} ( bold_italic_x start_POSTSUBSCRIPT img end_POSTSUBSCRIPT , bold_italic_x start_POSTSUBSCRIPT text end_POSTSUBSCRIPT )  italic_D start_POSTSUBSCRIPT italic_v italic_l italic_m end_POSTSUBSCRIPT . In our experiments, we apply  CLIP ViT-Large   (Radford et al.,  2021 )  with a patch size of  14 14 14 14  and input resolution of  336 336 336 336 px as the vision backbone and  Llama2-7B   (Touvron et al.,  2023 )  as the language backbone. For the projector  F  subscript F  F_{\\psi} italic_F start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT , we initialize a two-layer GELU-MLP  (Hendrycks and Gimpel,  2023 ) . Along the suggested setting from  Karamcheti et al. ( 2024 ) , we freeze the vision backbone  V  subscript V italic- V_{\\phi} italic_V start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  throughout the entire training process while only tuning the projector  F  subscript F  F_{\\psi} italic_F start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  and the language backbone  L  M  L subscript M  LM_{\\theta} italic_L italic_M start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT . We provide more implementation details in Appendix  E.1 .",
            "Define the change of the parameter      ( k )  (  )     subscript  italic- superscript  k italic- superscript   \\Delta_{\\epsilon}\\coloneqq{\\bm{\\theta}}^{(k)}(\\epsilon)-{\\bm{\\theta}}^{\\star} roman_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  bold_italic_ start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_ ) - bold_italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  and notice that    superscript   {\\bm{\\theta}}^{\\star} bold_italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  does not depend on   italic- \\epsilon italic_ , the quantity we want to compute in  Equation 1  can be re-written as:",
            "From previous definition,   ( k )  (  ) superscript  k italic- {\\bm{\\theta}}^{(k)}(\\epsilon) bold_italic_ start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_ )  is the minimizer for  Equation 12 , therefore we have the first-order optimality condition:",
            "Because    superscript   {\\bm{\\theta}}^{\\star} bold_italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is the minimizer for  R  (  ) R  R({\\bm{\\theta}}) italic_R ( bold_italic_ ) , we plus    R  (   ) = 0 subscript   R superscript   0 \\nabla_{{\\bm{\\theta}}}R({\\bm{\\theta}}^{\\star})=0  start_POSTSUBSCRIPT bold_italic_ end_POSTSUBSCRIPT italic_R ( bold_italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) = 0  and drop the   italic- \\epsilon italic_ -term in the first term of the right-hand side in  Equation 16 :",
            "Lastly, combining  Equation 10  and  Equation 13  we can get:",
            "The comparisons of error and time cost are shown in  Table 8  and  Table 9  as well as  Figure 10 . Schulz achieves a similar error margin as FGE, which is better than CG and GMRES in most cases. Furthermore, Schulz also has the lowest time cost generally in different dimension settings even when  d = 4096 d 4096 d=4096 italic_d = 4096 , while other methods observe a significant increase in running time as ranks become larger(especially for Gaussian Elimination, Conjugate Gradient and GMRES). This illustrates the efficiency and stability of  HyperINF  since Schulzs method is the main part of our method."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Mislabeled Data Detection Accuracies across the GLUE Benchmark with rank  r = 16 r 16 r=16 italic_r = 16  for rsLoRA finetuning.  When probing  20 % percent 20 20\\% 20 %  and  40 % percent 40 40\\% 40 %  data points,  HyperINF  can consistently outperform other baselines by a large margin ( 7 % percent 7 7\\% 7 % - 25 %   percent 25 absent 25\\%\\uparrow 25 %  ).",
        "table": "A6.EGx1",
        "footnotes": [],
        "references": [
            "where  G  (  ) := 1 n   i = 1 n   l i    l i  assign G  1 n superscript subscript i 1 n subscript   subscript l i subscript   superscript subscript l i top G({\\bm{\\theta}}):=\\frac{1}{n}\\sum_{i=1}^{n}\\nabla_{{\\bm{\\theta}}}\\ell_{i}% \\nabla_{{\\bm{\\theta}}}\\ell_{i}^{\\top} italic_G ( bold_italic_ ) := divide start_ARG 1 end_ARG start_ARG italic_n end_ARG  start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT  start_POSTSUBSCRIPT bold_italic_ end_POSTSUBSCRIPT roman_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  start_POSTSUBSCRIPT bold_italic_ end_POSTSUBSCRIPT roman_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  stands for the Fisher Information Matrix (FIM). While the computation complexity of  Equation 2  is reduced to  O  ( d ) O d \\mathcal{O}(d) caligraphic_O ( italic_d ) , in compromise, the reverse-order operation  Equation 23  incurs a  O  ( d 2 ) O superscript d 2 \\mathcal{O}(d^{2}) caligraphic_O ( italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )  error  (Kwon et al.,  2024 ) . When applying to large-scale models, it could risk a large approximation error.",
            "We hereby provide the holistic view of the  HyperINF  algorithm for influence function estimation. Firstly, we compute the generalized fisher information  G  (  ) G  G({\\bm{\\theta}}) italic_G ( bold_italic_ )  on all tunable parameter blocks (LoRA blocks on LoRA-tuned models); Secondly, we compute the inverse of the damped GFIM  ( G  (  ) +   I d ) G   subscript I d (G({\\bm{\\theta}})+\\lambda I_{d}) ( italic_G ( bold_italic_ ) + italic_ italic_I start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT )  with Schulzs iterations ( Equation 7 ); Last, we compute the influence score with cached validation gradient  v v {\\bm{v}} bold_italic_v  and the  unflattened  gradient on each training sample, i.e.  I HyperINF  ( x k , y k ) subscript I HyperINF subscript x k subscript y k \\mathcal{I}_{{\\textsc{HyperINF}}}\\left({\\bm{x}}_{k},y_{k}\\right) caligraphic_I start_POSTSUBSCRIPT HyperINF end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT )  ( Equation 5 ). We provide the detailed pseudo-code in the Appendix (Algo.  2 ).",
            "According to  Figure 2  and  Table 2 ,  HyperINF  outperforms all baselines on 5 out of 6 tasks with better accuracy and less variance. When probe  k = 20 % k percent 20 k=20\\% italic_k = 20 %  (resp.  40 % percent 40 40\\% 40 % ) data points,  HyperINF  achieves  7 % percent 7 7\\% 7 %  (resp.  10.82 % percent 10.82 10.82\\% 10.82 % ) improvement above  Datainf  and  22.25 % percent 22.25 22.25\\% 22.25 %  (resp.  25.88 % percent 25.88 25.88\\% 25.88 % ) above  LiSSA , in terms of average recall across 6 tasks. On SST2, the accuracy of  HyperINF  is comparable to  DataInf  and  TracIN  method while the variance is largely reduced when applying  HyperINF .",
            "We adopt the two-phase pretraining scheme following LLaVa  (Liu et al.,  2023c ) . In the  alignment phase , we tune the projector  F  subscript F  F_{\\psi} italic_F start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  and LoRA modules of the language backbone on a separate alignment dataset  (Karamcheti et al.,  2024 ) . For the second instruct-tuning phase, we select the most influential data samples from a large generic multimodal instruct-tuning dataset consisting of 665K datapoints  (Karamcheti et al.,  2024 ) . We compute the influence score utilizing the gradients from the projector and LoRA modules then select the top- k % percent k k\\% italic_k %  ( k = 5 % , 20 % k percent 5 percent 20 k=5\\%,20\\% italic_k = 5 % , 20 % ) subset with the lowest (i.e.  largest negative ) scores. We train the VLM on the selected instruct-tuning subsets for one epoch and evaluate the models performance on four cross-modal reasoning tasks: VQAv2  (Goyal et al.,  2017 ) , GQA  (Hudson and Manning,  2019 ) , POPE  (Li et al.,  2023 )  and Text-VQA  (Singh et al.,  2019 ) . We provide more details on the dataset and implementation in Appendix  E.2  and  E.3 .",
            "From previous definition,   ( k )  (  ) superscript  k italic- {\\bm{\\theta}}^{(k)}(\\epsilon) bold_italic_ start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_ )  is the minimizer for  Equation 12 , therefore we have the first-order optimality condition:",
            "where  G  (  ) := 1 n   i = 1 n   l i    l i  assign G  1 n superscript subscript i 1 n subscript   subscript l i subscript   superscript subscript l i top G({\\bm{\\theta}}):=\\frac{1}{n}\\sum_{i=1}^{n}\\nabla_{{\\bm{\\theta}}}\\ell_{i}% \\nabla_{{\\bm{\\theta}}}\\ell_{i}^{\\top} italic_G ( bold_italic_ ) := divide start_ARG 1 end_ARG start_ARG italic_n end_ARG  start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT  start_POSTSUBSCRIPT bold_italic_ end_POSTSUBSCRIPT roman_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  start_POSTSUBSCRIPT bold_italic_ end_POSTSUBSCRIPT roman_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  stands for the Fisher Information Matrix (FIM). While the computation complexity of  Equation 24  is reduced to  O  ( d ) O d \\mathcal{O}(d) caligraphic_O ( italic_d ) , in compromise, the reverse-order operation  Equation 23  incurs a  O  ( d 2 ) O superscript d 2 \\mathcal{O}(d^{2}) caligraphic_O ( italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )  error  [Kwon et al.,  2024 ] . When applying to large-scale models, it could risk a large approximation error.",
            "We provide the complete pseudo algorithm using  HyperINF  in Algorithm ( 2 ) to compute influence function for each datapoint in training set  D train superscript D train \\mathcal{D}^{\\text{train}} caligraphic_D start_POSTSUPERSCRIPT train end_POSTSUPERSCRIPT  according to the impact on the validation set  D val superscript D val \\mathcal{D}^{\\text{val}} caligraphic_D start_POSTSUPERSCRIPT val end_POSTSUPERSCRIPT .",
            "When finetuning the LLM with rsLoRA technique with rank  r = 16 r 16 r=16 italic_r = 16  in  Figure 2  and  r = 64 r 64 r=64 italic_r = 64  in  Figure 3 , we apply the gradients from trainable parameters (i.e. every value and query matrix of the attention layers) to approximate influence functions. We run  HyperINF  for  25 25 25 25  iterations and run  LiSSA  for  10 10 10 10  iterations following the implementation of  Kwon et al. [ 2024 ] . The total number of tunable parameters is  1.6  M , 7.3  M 1.6 M 7.3 M 1.6M,7.3M 1.6 italic_M , 7.3 italic_M  respectively for  r = 16 , 64 r 16 64 r=16,64 italic_r = 16 , 64 .",
            "Because of the limited computation resources, we constrain our experiments on  10 % percent 10 10\\% 10 %  of instruct-tuning training dataset used in  E.2 . We compute the influence function based on the gradients from both Project and LoRA layers, then select  k = 5 % , 20 % , 40 % k percent 5 percent 20 percent 40 k=5\\%,20\\%,40\\% italic_k = 5 % , 20 % , 40 %  datapoints using various influence function-based methods from the  10 % percent 10 10\\% 10 %  training subset, which is equivalent to  0.5 % , 2 % , 4 % percent 0.5 percent 2 percent 4 0.5\\%,2\\%,4\\% 0.5 % , 2 % , 4 %  of the original  665  K 665 K 665K 665 italic_K  instruct-tuning dataset. In this experiment, we also finetune the projector and LoRA layers of the language backbone and keep other parts frozen."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Evaluation accuracies ( % percent \\% % ) for LLM data selection with  LoRA finetuning . The best results are  Bolded  and the second-best are  Underlined . On average,  HyperINF  shows the larger improvements as  k k k italic_k  increases and performs better than all other baselines. The    \\uparrow   (   \\downarrow  ) indicates the improvement (degradation) compared to the Random baseline.",
        "table": "A6.EGx2",
        "footnotes": [],
        "references": [
            "where  G  (  ) := 1 n   i = 1 n   l i    l i  assign G  1 n superscript subscript i 1 n subscript   subscript l i subscript   superscript subscript l i top G({\\bm{\\theta}}):=\\frac{1}{n}\\sum_{i=1}^{n}\\nabla_{{\\bm{\\theta}}}\\ell_{i}% \\nabla_{{\\bm{\\theta}}}\\ell_{i}^{\\top} italic_G ( bold_italic_ ) := divide start_ARG 1 end_ARG start_ARG italic_n end_ARG  start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT  start_POSTSUBSCRIPT bold_italic_ end_POSTSUBSCRIPT roman_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  start_POSTSUBSCRIPT bold_italic_ end_POSTSUBSCRIPT roman_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  stands for the Fisher Information Matrix (FIM). While the computation complexity of  Equation 2  is reduced to  O  ( d ) O d \\mathcal{O}(d) caligraphic_O ( italic_d ) , in compromise, the reverse-order operation  Equation 23  incurs a  O  ( d 2 ) O superscript d 2 \\mathcal{O}(d^{2}) caligraphic_O ( italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )  error  (Kwon et al.,  2024 ) . When applying to large-scale models, it could risk a large approximation error.",
            "Since  Equation 3  replaces the second-order derivative with stable and tractable first-order gradients, the Fisher Information Matrix (FIM) is widely adopted as a valid approximation of Hessian matrix in deep networks  (Grosse et al.,  2023a ; Kwon et al.,  2024 ; Barshan et al.,  2020 ) . We further extend the estimation incorporating the Generalized Fisher Information Matrix (GFIM)  (Hu and Li,  2024 ) , computed using matrix-form gradient multiplication without flattening the gradient vector. This can be seen as a more efficient form of using projections of the relevant vector products, as we will demonstrate in the following result, which provides a theoretical analysis for the insights of  Hu and Li ( 2024 ) .",
            "where the first equality follows from  Equation 3 .",
            "According to  Table 3 ,  HyperINF  achieves the best performance comparing to other baselines. Notably, with  5 % percent 5 5\\% 5 %  finetuning datapoints selected by  HyperINF , the reasoning accuracy outperforms the train with the full dataset, which requires  20  20\\times 20   data samples and  4  4\\times 4   FLOPs. With  20 % percent 20 20\\% 20 %   HyperINF -selected data points,  HyperINF  greatly improves the accuracy by  2.0 % percent 2.0 2.0\\% 2.0 %  above the random selection baseline.",
            "We adopt the two-phase pretraining scheme following LLaVa  (Liu et al.,  2023c ) . In the  alignment phase , we tune the projector  F  subscript F  F_{\\psi} italic_F start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  and LoRA modules of the language backbone on a separate alignment dataset  (Karamcheti et al.,  2024 ) . For the second instruct-tuning phase, we select the most influential data samples from a large generic multimodal instruct-tuning dataset consisting of 665K datapoints  (Karamcheti et al.,  2024 ) . We compute the influence score utilizing the gradients from the projector and LoRA modules then select the top- k % percent k k\\% italic_k %  ( k = 5 % , 20 % k percent 5 percent 20 k=5\\%,20\\% italic_k = 5 % , 20 % ) subset with the lowest (i.e.  largest negative ) scores. We train the VLM on the selected instruct-tuning subsets for one epoch and evaluate the models performance on four cross-modal reasoning tasks: VQAv2  (Goyal et al.,  2017 ) , GQA  (Hudson and Manning,  2019 ) , POPE  (Li et al.,  2023 )  and Text-VQA  (Singh et al.,  2019 ) . We provide more details on the dataset and implementation in Appendix  E.2  and  E.3 .",
            "Lastly, combining  Equation 10  and  Equation 13  we can get:",
            "where  G  (  ) := 1 n   i = 1 n   l i    l i  assign G  1 n superscript subscript i 1 n subscript   subscript l i subscript   superscript subscript l i top G({\\bm{\\theta}}):=\\frac{1}{n}\\sum_{i=1}^{n}\\nabla_{{\\bm{\\theta}}}\\ell_{i}% \\nabla_{{\\bm{\\theta}}}\\ell_{i}^{\\top} italic_G ( bold_italic_ ) := divide start_ARG 1 end_ARG start_ARG italic_n end_ARG  start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT  start_POSTSUBSCRIPT bold_italic_ end_POSTSUBSCRIPT roman_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  start_POSTSUBSCRIPT bold_italic_ end_POSTSUBSCRIPT roman_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  stands for the Fisher Information Matrix (FIM). While the computation complexity of  Equation 24  is reduced to  O  ( d ) O d \\mathcal{O}(d) caligraphic_O ( italic_d ) , in compromise, the reverse-order operation  Equation 23  incurs a  O  ( d 2 ) O superscript d 2 \\mathcal{O}(d^{2}) caligraphic_O ( italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )  error  [Kwon et al.,  2024 ] . When applying to large-scale models, it could risk a large approximation error.",
            "When finetuning the LLM with rsLoRA technique with rank  r = 16 r 16 r=16 italic_r = 16  in  Figure 2  and  r = 64 r 64 r=64 italic_r = 64  in  Figure 3 , we apply the gradients from trainable parameters (i.e. every value and query matrix of the attention layers) to approximate influence functions. We run  HyperINF  for  25 25 25 25  iterations and run  LiSSA  for  10 10 10 10  iterations following the implementation of  Kwon et al. [ 2024 ] . The total number of tunable parameters is  1.6  M , 7.3  M 1.6 M 7.3 M 1.6M,7.3M 1.6 italic_M , 7.3 italic_M  respectively for  r = 16 , 64 r 16 64 r=16,64 italic_r = 16 , 64 .",
            "We present the detailed statistics of evaluation results in  Table 3  and  Figure 7  for LoRA-finetuning experiments, and  Table 4  and  Figure 8  for fully-finetuning experiments.  HyperINF  significantly outperforms all baselines."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Evaluation accuracies ( % percent \\% % ) for LLM data selection with  dense finetuning . The best results are  Bolded  and the second-best are  Underlined . On average,  HyperINF  could outperform the Random baseline while the other methods fail when the selection ratio  k k k italic_k  is small. The    \\uparrow   (   \\downarrow  ) indicates the improvement (degradation) compared to the Random baseline.",
        "table": "A6.EGx3",
        "footnotes": [],
        "references": [
            "The proof is provided in Appendix  A.4 . Following Lemma  1 , we further estimate a Hessian-gradient product using the GFIM, corresponding to the ( H  (   )  1    l k H superscript superscript   1 subscript   subscript l k H({\\bm{\\theta}}^{\\star})^{-1}\\nabla_{{\\bm{\\theta}}}\\ell_{k} italic_H ( bold_italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT  start_POSTSUBSCRIPT bold_italic_ end_POSTSUBSCRIPT roman_l start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) term in  Equation 1 . Given an invertible matrix  A A A italic_A , we have  ( I r  A )  1 = I r  A  1 superscript tensor-product subscript I r A 1 tensor-product subscript I r superscript A 1 (I_{r}\\otimes A)^{-1}=I_{r}\\otimes A^{-1} ( italic_I start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT  italic_A ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT = italic_I start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT  italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT . Therefore, denote the GFIM matrix as  G  (  )  ( g  g  )  R d  d  G  g superscript g top superscript R d d G({\\bm{\\theta}})\\triangleq({\\bm{g}}{\\bm{g}}^{\\top})\\in\\mathbb{R}^{d\\times d} italic_G ( bold_italic_ )  ( bold_italic_g bold_italic_g start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT )  roman_R start_POSTSUPERSCRIPT italic_d  italic_d end_POSTSUPERSCRIPT  for any matrix  v  R d  r v superscript R d r {\\bm{v}}\\in\\mathbb{R}^{d\\times r} bold_italic_v  roman_R start_POSTSUPERSCRIPT italic_d  italic_r end_POSTSUPERSCRIPT , it holds that:",
            "Consider a LoRA-tuned model with LoRA dimension  d d d italic_d  and rank  r r r italic_r . We assume that each column in one LoRA block    W  R d  r  W superscript R d r \\Delta W\\in\\mathbb{R}^{d\\times r} roman_ italic_W  roman_R start_POSTSUPERSCRIPT italic_d  italic_r end_POSTSUPERSCRIPT , corresponding to each rank, is independent and identical. Thus, we apply  Equation 4  to approximate the original Hessian-gradient product. To further guarantee that  G  (  ) G  G({\\bm{\\theta}}) italic_G ( bold_italic_ )  is invertible, we add a damping factor    I d  subscript I d \\lambda I_{d} italic_ italic_I start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT  to the GFIM matrix following  Martens ( 2010 ) .",
            "We eliminate the constant in  Equation 4  then derive the final formula of  HyperINF  influence score. On a specific datapoint  { x k , y k }  D train subscript x k subscript y k superscript D train \\{{\\bm{x}}_{k},y_{k}\\}\\in\\mathcal{D}^{\\text{train}} { bold_italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT }  caligraphic_D start_POSTSUPERSCRIPT train end_POSTSUPERSCRIPT , denote the  unflattened  gradient on a parameter block    {\\bm{\\theta}} bold_italic_  as  g k  (  )  R d  r subscript g k  superscript R d r {\\bm{g}}_{k}({\\bm{\\theta}})\\in\\mathbb{R}^{d\\times r} bold_italic_g start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_ )  roman_R start_POSTSUPERSCRIPT italic_d  italic_r end_POSTSUPERSCRIPT , we compute:",
            "When  t    t t\\rightarrow\\infty italic_t    and  X 0  A  1 subscript X 0 superscript A 1 X_{0}\\approx A^{-1} italic_X start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT , it is proved that the sequence  { X t } subscript X t \\{X_{t}\\} { italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT }  will converge towards  A  1 superscript A 1 A^{-1} italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT  in a numerically stable way  (Petkovic,  1995 ; Ben-Israel,  1965 ; Bazan and Boos,  2018 ; Soderstrom and Stewart,  1974 ) . It is proved by  Ben-Israel and Cohen ( 1966 )  and  Petkovic ( 1995 )  that with a proper initialization, Schulzs method would converge to  A  1 superscript A 1 A^{-1} italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT  in the order of convergence at least  p = 2 p 2 p=2 italic_p = 2 . Compared to other conventional matrix inverse algorithms (e.g. Gaussian Elimination, Conjugate Gradient, GMRES), Schulzs method demonstrates superior accuracy in terms of error rate and significant efficiency gains from the GPU acceleration on matrix multiplications. We include more details in Appendix  F . With the simulation matrix inversion experiments (Section.  4 ), we show that starting from a small identity matrix or random Gaussian initialization could converge to a desirable error rate in finite steps ( t < 20 t 20 t<20 italic_t < 20 ), which demonstrates that the given algorithm is not sensitive to the initialization. We provide the pseudo-code according to Algorithm  1 .",
            "Although the theoretical analysis in Lemma  1  is inspired by LoRA finetuning, we demonstrate that data selection via  HyperINF  provides substantial benefits for dense fine-tuning as well. In this setting, we compute  HyperINF  influence score according to  Equation 5  using the gradient from the last transformer block of the language model. Specifically, the GFIM is computed as described in Lemma  1  and  Equation 4 , where  d d d italic_d  refers to the larger dimension of the given matrix.",
            "According to  Table 4 , with  5 % , 20 % , 40 % percent 5 percent 20 percent 40 5\\%,20\\%,40\\% 5 % , 20 % , 40 %  selected data points,  HyperINF  consistently improves the reasoning accuracy across all tasks above the random baseline. In contrast, all three baselines could lead to degradation when selecting a small portion of data points ( 5 , 20 % 5 percent 20 5,20\\% 5 , 20 % ). Compared to training on the full dataset (1 epoch), using  40 % percent 40 40\\% 40 %   HyperINF -selected samples improves the average accuracy by  12.9 % percent 12.9 12.9\\% 12.9 % , which also performs other baselines by a large margin.",
            "(Karamcheti et al.,  2024 )  illustrated from extensive empirical experiments that we can skip the alignment phase in VLM pretraining to achieve comparable performance as the two-phase training. To explore whether it applies to data selection, we directly apply  HyperINF ,  DataInf ,  LiSSA  and  TracIN  before alignment. Since the projector gradients are randomly initialized before the alignment phase, we only use the gradients from the last transformer block in language backbone to compute the influence scores. According to  E.4 , while the  HyperINF  could still bring slight improvement ( 0.25  1 % 0.25 percent 1 0.25-1\\% 0.25 - 1 % ) above random baseline, all the other three methods suffer from a significant degradation (  5 %  absent percent 5  absent \\geq 5\\%\\downarrow  5 %  ) on the accuracy. We hypothesise that the alignment phase is crucial to learning about the connection between the feature spaces of language and vision backbones, which is indispensable information for VLM pretraining data selection. Therefore, we suggest the practitioners apply data selection after the alignment phase.",
            "where  G  (  ) := 1 n   i = 1 n   l i    l i  assign G  1 n superscript subscript i 1 n subscript   subscript l i subscript   superscript subscript l i top G({\\bm{\\theta}}):=\\frac{1}{n}\\sum_{i=1}^{n}\\nabla_{{\\bm{\\theta}}}\\ell_{i}% \\nabla_{{\\bm{\\theta}}}\\ell_{i}^{\\top} italic_G ( bold_italic_ ) := divide start_ARG 1 end_ARG start_ARG italic_n end_ARG  start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT  start_POSTSUBSCRIPT bold_italic_ end_POSTSUBSCRIPT roman_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  start_POSTSUBSCRIPT bold_italic_ end_POSTSUBSCRIPT roman_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  stands for the Fisher Information Matrix (FIM). While the computation complexity of  Equation 24  is reduced to  O  ( d ) O d \\mathcal{O}(d) caligraphic_O ( italic_d ) , in compromise, the reverse-order operation  Equation 23  incurs a  O  ( d 2 ) O superscript d 2 \\mathcal{O}(d^{2}) caligraphic_O ( italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )  error  [Kwon et al.,  2024 ] . When applying to large-scale models, it could risk a large approximation error.",
            "We follow the proof in  Yang et al. [ 2022 ] . Let  g ( : , k ) subscript g : k g_{(:,k)} italic_g start_POSTSUBSCRIPT ( : , italic_k ) end_POSTSUBSCRIPT  be the  k k k italic_k -th column of  g g g italic_g . According to the assumption, we know that  E  [ g ( : , k ) ] = 0 d  1 ,  k = 1 , 2 , ... , r formulae-sequence E delimited-[] subscript g : k subscript 0 d 1 for-all k 1 2 ... r \\mathbb{E}[g_{(:,k)}]=0_{d\\times 1},\\forall k=1,2,...,r roman_E [ italic_g start_POSTSUBSCRIPT ( : , italic_k ) end_POSTSUBSCRIPT ] = 0 start_POSTSUBSCRIPT italic_d  1 end_POSTSUBSCRIPT ,  italic_k = 1 , 2 , ... , italic_r  and  Cov  ( g ( : , k ) , g ( : , l ) ) = 0 d  d Cov subscript g : k subscript g : l subscript 0 d d \\text{Cov}(g_{(:,k)},g_{(:,l)})=0_{d\\times d} Cov ( italic_g start_POSTSUBSCRIPT ( : , italic_k ) end_POSTSUBSCRIPT , italic_g start_POSTSUBSCRIPT ( : , italic_l ) end_POSTSUBSCRIPT ) = 0 start_POSTSUBSCRIPT italic_d  italic_d end_POSTSUBSCRIPT  if  l = k l k l\\neq k italic_l = italic_k .  vec ( g ) vec ( g )   R r  d  r  d \\operatorname{vec}(g)\\operatorname{vec}(g)^{\\top}\\in\\mathbb{R}^{rd\\times rd} roman_vec ( italic_g ) roman_vec ( italic_g ) start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  roman_R start_POSTSUPERSCRIPT italic_r italic_d  italic_r italic_d end_POSTSUPERSCRIPT  can be seen as a  r  r r r r\\times r italic_r  italic_r  block matrix with each block a  d  d d d d\\times d italic_d  italic_d  matrix. Therefore when taking the expectation, we have the off-diagonal blocks of  E [ vec ( g ) vec ( g )  ] \\mathbb{E}[\\operatorname{vec}(g)\\operatorname{vec}(g)^{\\top}] roman_E [ roman_vec ( italic_g ) roman_vec ( italic_g ) start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ]  are zero metrics and each diagonal part is equal to  Var  ( g ( : , k ) , g ( : , k ) ) Var subscript g : k subscript g : k \\operatorname{Var}(g_{(:,k)},g_{(:,k)}) roman_Var ( italic_g start_POSTSUBSCRIPT ( : , italic_k ) end_POSTSUBSCRIPT , italic_g start_POSTSUBSCRIPT ( : , italic_k ) end_POSTSUBSCRIPT )  since each column is i.i.d random vector with zero mean. For  1 r  g  g  1 r g superscript g top \\frac{1}{r}gg^{\\top} divide start_ARG 1 end_ARG start_ARG italic_r end_ARG italic_g italic_g start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , we have  1 r  g  g  = 1 r   i = 1 r g ( : , i )  g ( : , k )  1 r g superscript g top 1 r superscript subscript i 1 r subscript g : i superscript subscript g : k top \\frac{1}{r}gg^{\\top}=\\frac{1}{r}\\sum_{i=1}^{r}g_{(:,i)}g_{(:,k)}^{\\top} divide start_ARG 1 end_ARG start_ARG italic_r end_ARG italic_g italic_g start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = divide start_ARG 1 end_ARG start_ARG italic_r end_ARG  start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT italic_g start_POSTSUBSCRIPT ( : , italic_i ) end_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT ( : , italic_k ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT . As a result,  E  [ 1 r  g  g  ] = 1 r  r  Var  ( g ( : , k ) , g ( : , k ) ) = Var  ( g ( : , k ) , g ( : , k ) ) E delimited-[] 1 r g superscript g top  1 r r Var subscript g : k subscript g : k Var subscript g : k subscript g : k \\mathbb{E}[\\frac{1}{r}gg^{\\top}]=\\frac{1}{r}\\cdot r\\cdot\\operatorname{Var}(g_{% (:,k)},g_{(:,k)})=\\operatorname{Var}(g_{(:,k)},g_{(:,k)}) roman_E [ divide start_ARG 1 end_ARG start_ARG italic_r end_ARG italic_g italic_g start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ] = divide start_ARG 1 end_ARG start_ARG italic_r end_ARG  italic_r  roman_Var ( italic_g start_POSTSUBSCRIPT ( : , italic_k ) end_POSTSUBSCRIPT , italic_g start_POSTSUBSCRIPT ( : , italic_k ) end_POSTSUBSCRIPT ) = roman_Var ( italic_g start_POSTSUBSCRIPT ( : , italic_k ) end_POSTSUBSCRIPT , italic_g start_POSTSUBSCRIPT ( : , italic_k ) end_POSTSUBSCRIPT ) . The right side of  Equation 4  is equal to  I r  E  [ 1 r  g  g  ] = I r  Var  ( g ( : , k ) , g ( : , k ) ) tensor-product subscript I r E delimited-[] 1 r g superscript g top tensor-product subscript I r Var subscript g : k subscript g : k I_{r}\\otimes\\mathbb{E}[\\frac{1}{r}gg^{\\top}]=I_{r}\\otimes\\operatorname{Var}(g_% {(:,k)},g_{(:,k)}) italic_I start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT  roman_E [ divide start_ARG 1 end_ARG start_ARG italic_r end_ARG italic_g italic_g start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ] = italic_I start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT  roman_Var ( italic_g start_POSTSUBSCRIPT ( : , italic_k ) end_POSTSUBSCRIPT , italic_g start_POSTSUBSCRIPT ( : , italic_k ) end_POSTSUBSCRIPT ) , then we finished the proof.",
            "Moreover, We also experiment using the last layers gradients of  Roberta-large  to detect the mislabeled datapoints. We only tune the last layer of the model on the corrupted training dataset, then compute the influence function based on the last layers gradients. The results are shown in  Figure 4 , which indicates that the last layers gradients can also be a candidate for computing the influence function.",
            "We present the detailed statistics of evaluation results in  Table 3  and  Figure 7  for LoRA-finetuning experiments, and  Table 4  and  Figure 8  for fully-finetuning experiments.  HyperINF  significantly outperforms all baselines.",
            "In this section, we compare the efficiency of computing inverse of matrices between Schulzs method and other commonly used methods * * * https://github.com/devzhk/Pytorch-linalg , including Gaussian Elimination, Conjugate Gradient, Generalized Minimal Residual method (GMRES) and Faster Gaussian Elimination (i.e.  torch.inverse ). For the iterative methods, we all set the number of iterations to  20 20 20 20  for fair comparisons. We follow the same step in Section.  4  to construct the invertible matrix  M M M italic_M , and set the dimension of the matrix in different scales:  d  { 16 , 64 , 256 , 1024 , 4096 } d 16 64 256 1024 4096 d\\in\\{16,64,256,1024,4096\\} italic_d  { 16 , 64 , 256 , 1024 , 4096 }  and  N = 12800 N 12800 N=12800 italic_N = 12800 . We use the Frobenius Norm to measure the error between the approximated and true inverse, where we set the Gaussian Elimination as the ground truth. In addition to the error comparison, we also compare the time cost of each method in terms of efficiency aspect. We run the experiments with 3 random seeds and report the average and standard deviation of time costs. All the experiments are done with a single A100 GPU."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Downstream evaluation accuracies ( % percent \\% % ) from VLM instruct-tuning data selection experiments (after cross-modal alignment on Projector and LoRA layers). The best results are  Bolded  and the second-best are  Underlined .  Projector+LoRA  means the gradient from both the  Projector  and  LoRA  are used to compute approximated scores. Methods with  > 5 % absent percent 5 >5\\% > 5 %  accuracy degradation are marked in  Red .",
        "table": "A6.EGx4",
        "footnotes": [],
        "references": [
            "We hereby provide the holistic view of the  HyperINF  algorithm for influence function estimation. Firstly, we compute the generalized fisher information  G  (  ) G  G({\\bm{\\theta}}) italic_G ( bold_italic_ )  on all tunable parameter blocks (LoRA blocks on LoRA-tuned models); Secondly, we compute the inverse of the damped GFIM  ( G  (  ) +   I d ) G   subscript I d (G({\\bm{\\theta}})+\\lambda I_{d}) ( italic_G ( bold_italic_ ) + italic_ italic_I start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT )  with Schulzs iterations ( Equation 7 ); Last, we compute the influence score with cached validation gradient  v v {\\bm{v}} bold_italic_v  and the  unflattened  gradient on each training sample, i.e.  I HyperINF  ( x k , y k ) subscript I HyperINF subscript x k subscript y k \\mathcal{I}_{{\\textsc{HyperINF}}}\\left({\\bm{x}}_{k},y_{k}\\right) caligraphic_I start_POSTSUBSCRIPT HyperINF end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT )  ( Equation 5 ). We provide the detailed pseudo-code in the Appendix (Algo.  2 ).",
            "It is worth noting that  HyperINF  with GFIM does not lead to performance degradation compared to FIM. According to  Figure 5 ,  HyperINF  with GFIM could consistently achieve comparable or better performance than  HyperINF  with FIM, while being  ( 1 / r ) 3 superscript 1 r 3 (1/r)^{3} ( 1 / italic_r ) start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT  more efficient in computation and  ( 1 / r ) 2 superscript 1 r 2 (1/r)^{2} ( 1 / italic_r ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  in memory ( Table 1 ).",
            "Although the theoretical analysis in Lemma  1  is inspired by LoRA finetuning, we demonstrate that data selection via  HyperINF  provides substantial benefits for dense fine-tuning as well. In this setting, we compute  HyperINF  influence score according to  Equation 5  using the gradient from the last transformer block of the language model. Specifically, the GFIM is computed as described in Lemma  1  and  Equation 4 , where  d d d italic_d  refers to the larger dimension of the given matrix.",
            "We present the downstream accuracies across four reasoning tasks in  Table 5 . On average,  HyperINF  consistently outperforms all the other data selection methods and achieves a  2.3 % percent 2.3 2.3\\% 2.3 %  improvement above the random baseline with  20 % percent 20 20\\% 20 %  selected subset. In contrast, with  5 % percent 5 5\\% 5 %  selected data points,  LiSSA  shows a large ( 8 % percent 8 8\\% 8 % ) performance degradation because of the lack of accurate second-order information.",
            "To explore if using GFIM can lead to performance degradation, we compare  HyperINF  with GFIM and  HyperINF  with FIM. In this experiment, we set rank  r = 8 r 8 r=8 italic_r = 8  since larger ranks (e.g.  r = 16 , 32 , ... r 16 32 ... r=16,32,... italic_r = 16 , 32 , ... ) would cause the Out-Of-Memory error in FIM. The results are shown in  Figure 5 , where we do not observe the significantly worse performance in  HyperINF  with GFIM, and it performs even better on some datasets than FIM, such as QQP and SST2."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Hyperparameters setting for training VLM",
        "table": "A6.EGx5",
        "footnotes": [],
        "references": [
            "Because    superscript   {\\bm{\\theta}}^{\\star} bold_italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is the minimizer for  R  (  ) R  R({\\bm{\\theta}}) italic_R ( bold_italic_ ) , we plus    R  (   ) = 0 subscript   R superscript   0 \\nabla_{{\\bm{\\theta}}}R({\\bm{\\theta}}^{\\star})=0  start_POSTSUBSCRIPT bold_italic_ end_POSTSUBSCRIPT italic_R ( bold_italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) = 0  and drop the   italic- \\epsilon italic_ -term in the first term of the right-hand side in  Equation 16 :",
            "We choose two datasets, and compare the complexity of each method by the running time for computing the inverse Hessian vector product  v   G  (  ) superscript v top G  {\\bm{v}}^{\\top}G({\\bm{\\theta}}) bold_italic_v start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT italic_G ( bold_italic_ )  under different finetuning ranks  r = 1 , 2 , 4 , 8 r 1 2 4 8 r=1,2,4,8 italic_r = 1 , 2 , 4 , 8  and  16 16 16 16  by a single A100 GPU, shown in  Figure 6 . Compared to  DataInf  and  HyperINF ,  LISSA  requires more ( > 4  >4\\times > 4  ) time costs. In addition, our  HyperINF  even costs less time than  DataInf  thanks to its GPU-friendly mechanism.",
            "For LoRA-finetuning, we follow the same setting as we implement in Mislabeled Data Detection task while setting the rank  r = 64 r 64 r=64 italic_r = 64 . The hyperparameters are set as the same as in VLM experiments ( Table 6 ), while the Epoch number is set to  3 3 3 3  for fully-finetuning and  5 5 5 5  for LoRA-finetuning across  k = 5 % , 20 % , 40 % k percent 5 percent 20 percent 40 k=5\\%,20\\%,40\\% italic_k = 5 % , 20 % , 40 % . When selecting all datapoints (i.e.  k = 100 % k percent 100 k=100\\% italic_k = 100 % ), we finetune it for only 1 epoch.",
            "Specifically, we utilize the Prismatic-VLM framework * * * https://github.com/TRI-ML/prismatic-vlms?tab=readme-ov-file   [Karamcheti et al.,  2024 ]  to train the VLM. We use 6xA100 80G GPUs to train the model, and the hyperparameters are set as  Table 6 .",
            "We keep the same hyperparameter setting as in  Table 6  and adopt LoRA to the language backbone. We keep the same LoRA setting in the LLM LoRA-finetuning. In the alignment phase, we tune the projector and LoRA layers while keeping other parts frozen. We use the Vision-Language Alignment dataset  [Karamcheti et al.,  2024 ] , which consists of 558K (image, caption) pairs, where the caption is a sentence description of the corresponding image. The images are sourced from LAION  [Schuhmann et al.,  2021 ] , Conceptual Captions  [Sharma et al.,  2018 ]  and SBU Captions  [Ordonez et al.,  2011 ] . Considering the limited computation resources, we randomly select  5 % percent 5 5\\% 5 %  datapoints from the alignment dataset for the alignment phase. We leave the larger-scale experiments to future work."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Downstream evaluation accuracies ( % percent \\% % ) from VLM instruct-tuning data selection experiments (before cross-modal alignment). The best results are  Bolded  and the second-best are  Underlined . The gradient from the last layer of the language backbone is used to compute approximated scores.  HyperINF  could outperform the Random baseline while the other methods fail when selection ratios are small. The    \\uparrow   (   \\downarrow  ) indicates the improvement (degradation) compared to the Random baseline. Methods with  > 5 % absent percent 5 >5\\% > 5 %  accuracy degradation are marked in  Red .",
        "table": "A6.EGx6",
        "footnotes": [],
        "references": [
            "We hereby provide the holistic view of the  HyperINF  algorithm for influence function estimation. Firstly, we compute the generalized fisher information  G  (  ) G  G({\\bm{\\theta}}) italic_G ( bold_italic_ )  on all tunable parameter blocks (LoRA blocks on LoRA-tuned models); Secondly, we compute the inverse of the damped GFIM  ( G  (  ) +   I d ) G   subscript I d (G({\\bm{\\theta}})+\\lambda I_{d}) ( italic_G ( bold_italic_ ) + italic_ italic_I start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT )  with Schulzs iterations ( Equation 7 ); Last, we compute the influence score with cached validation gradient  v v {\\bm{v}} bold_italic_v  and the  unflattened  gradient on each training sample, i.e.  I HyperINF  ( x k , y k ) subscript I HyperINF subscript x k subscript y k \\mathcal{I}_{{\\textsc{HyperINF}}}\\left({\\bm{x}}_{k},y_{k}\\right) caligraphic_I start_POSTSUBSCRIPT HyperINF end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT )  ( Equation 5 ). We provide the detailed pseudo-code in the Appendix (Algo.  2 ).",
            "We present the detailed statistics of evaluation results in  Table 3  and  Figure 7  for LoRA-finetuning experiments, and  Table 4  and  Figure 8  for fully-finetuning experiments.  HyperINF  significantly outperforms all baselines.",
            "We present the evaluation accuracies on four multimodal downstream tasks in  Table 7 . Notably, when selecting  k = 20 % k percent 20 k=20\\% italic_k = 20 %  of datapoints,  HyperINF  improves the accuracy in average by  7.20 % percent 7.20 7.20\\% 7.20 %  above  DataInf ,  8.37 % percent 8.37 8.37\\% 8.37 %  above  LiSSA  and  9.11 % percent 9.11 9.11\\% 9.11 %  above  TracIN . However, we also note that when the selection ratio gets larger ( k > 40 % k percent 40 k>40\\% italic_k > 40 % ), the performance of other baselines will approach  HyperINF , since the impact from approximation errors on the data ranking is mitigated. Meanwhile, we observe that the random selection is a very strong baseline for all tasks, where only  HyperINF  has a small improvement above the random baseline ( 0.25 % percent 0.25 0.25\\% 0.25 % ) in average accuracy while all the other methods cause a large performance degradation ( > 5 % absent percent 5 >5\\% > 5 % ). We hypothesize that using pretrained LLM backbone without leveraging cross-modal alignment information may lead to sub-optimal results.",
            "We present detailed statistics for downstream evaluations in  Table 7  and  Figure 9 .  HyperINF  greatly improves the accuracies across all tasks above the other data selection baselines, while the random selection is a strong baseline. When selecting  20 % percent 20 20\\% 20 %  subset,  HyperINF  is the only method that could outperform random selection according to average accuracy."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Error comparisons among different methods for computing the inverse of the matrix. CG, and FGE denote the Conjugate Gradient and Faster Gaussian Elimination respectively. We reimplemented all the algorithms in  torch  if the original implementation does not support GPU acceleration.",
        "table": "A6.EGx7",
        "footnotes": [],
        "references": [
            "We present the detailed statistics of evaluation results in  Table 3  and  Figure 7  for LoRA-finetuning experiments, and  Table 4  and  Figure 8  for fully-finetuning experiments.  HyperINF  significantly outperforms all baselines.",
            "The comparisons of error and time cost are shown in  Table 8  and  Table 9  as well as  Figure 10 . Schulz achieves a similar error margin as FGE, which is better than CG and GMRES in most cases. Furthermore, Schulz also has the lowest time cost generally in different dimension settings even when  d = 4096 d 4096 d=4096 italic_d = 4096 , while other methods observe a significant increase in running time as ranks become larger(especially for Gaussian Elimination, Conjugate Gradient and GMRES). This illustrates the efficiency and stability of  HyperINF  since Schulzs method is the main part of our method."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Time cost (s) comparisons among different methods for computing the inverse of the matrix. GE, CG and FGE denote the Gaussian Elimination, Conjugate Gradient and Faster Gaussian Elimination respectively. We reimplemented all the algorithms in  torch  if the original implementation does not support GPU acceleration.",
        "table": "A6.EGx8",
        "footnotes": [],
        "references": [
            "We present detailed statistics for downstream evaluations in  Table 7  and  Figure 9 .  HyperINF  greatly improves the accuracies across all tasks above the other data selection baselines, while the random selection is a strong baseline. When selecting  20 % percent 20 20\\% 20 %  subset,  HyperINF  is the only method that could outperform random selection according to average accuracy.",
            "The comparisons of error and time cost are shown in  Table 8  and  Table 9  as well as  Figure 10 . Schulz achieves a similar error margin as FGE, which is better than CG and GMRES in most cases. Furthermore, Schulz also has the lowest time cost generally in different dimension settings even when  d = 4096 d 4096 d=4096 italic_d = 4096 , while other methods observe a significant increase in running time as ranks become larger(especially for Gaussian Elimination, Conjugate Gradient and GMRES). This illustrates the efficiency and stability of  HyperINF  since Schulzs method is the main part of our method."
        ]
    },
    "id_table_10": {
        "caption": "",
        "table": "A6.EGx9",
        "footnotes": [],
        "references": [
            "Lastly, combining  Equation 10  and  Equation 13  we can get:",
            "The comparisons of error and time cost are shown in  Table 8  and  Table 9  as well as  Figure 10 . Schulz achieves a similar error margin as FGE, which is better than CG and GMRES in most cases. Furthermore, Schulz also has the lowest time cost generally in different dimension settings even when  d = 4096 d 4096 d=4096 italic_d = 4096 , while other methods observe a significant increase in running time as ranks become larger(especially for Gaussian Elimination, Conjugate Gradient and GMRES). This illustrates the efficiency and stability of  HyperINF  since Schulzs method is the main part of our method."
        ]
    },
    "id_table_11": {
        "caption": "",
        "table": "A6.EGx10",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "",
        "table": "A6.EGx11",
        "footnotes": [],
        "references": [
            "From previous definition,   ( k )  (  ) superscript  k italic- {\\bm{\\theta}}^{(k)}(\\epsilon) bold_italic_ start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_ )  is the minimizer for  Equation 12 , therefore we have the first-order optimality condition:"
        ]
    },
    "id_table_13": {
        "caption": "",
        "table": "A6.EGx12",
        "footnotes": [],
        "references": [
            "Lastly, combining  Equation 10  and  Equation 13  we can get:"
        ]
    },
    "id_table_14": {
        "caption": "",
        "table": "A6.EGx13",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "",
        "table": "A6.EGx14",
        "footnotes": [],
        "references": []
    },
    "id_table_16": {
        "caption": "",
        "table": "A6.EGx15",
        "footnotes": [],
        "references": [
            "Because    superscript   {\\bm{\\theta}}^{\\star} bold_italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is the minimizer for  R  (  ) R  R({\\bm{\\theta}}) italic_R ( bold_italic_ ) , we plus    R  (   ) = 0 subscript   R superscript   0 \\nabla_{{\\bm{\\theta}}}R({\\bm{\\theta}}^{\\star})=0  start_POSTSUBSCRIPT bold_italic_ end_POSTSUBSCRIPT italic_R ( bold_italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) = 0  and drop the   italic- \\epsilon italic_ -term in the first term of the right-hand side in  Equation 16 :"
        ]
    },
    "id_table_17": {
        "caption": "",
        "table": "A6.EGx16",
        "footnotes": [],
        "references": []
    },
    "id_table_18": {
        "caption": "",
        "table": "A6.EGx17",
        "footnotes": [],
        "references": []
    },
    "id_table_19": {
        "caption": "",
        "table": "A6.EGx18",
        "footnotes": [],
        "references": []
    },
    "id_table_20": {
        "caption": "",
        "table": "A5.T6.1",
        "footnotes": [],
        "references": []
    },
    "id_table_21": {
        "caption": "",
        "table": "A6.T8.20",
        "footnotes": [],
        "references": []
    },
    "id_table_22": {
        "caption": "",
        "table": "A6.T9.25",
        "footnotes": [],
        "references": []
    }
}