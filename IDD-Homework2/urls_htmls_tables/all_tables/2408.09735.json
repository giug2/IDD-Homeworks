{
    "id_table_1": {
        "caption": "Table 1:  Approaches to Generate JavaDoc Summary",
        "table": "S2.T1.1.1",
        "footnotes": [],
        "references": [
            "We report our experiments, results and insights while automatically generating summary comments for Java methods within Ericsson 1 1 1 www.ericsson.com , a global telecom vendor. We baseline the results against the recent LLM-based state-of-the-art (SOTA) approach for code summarization - Automatic Semantic Augmentation of Prompts (ASAP)  [ 9 ] . It uses static program analysis to extract data flow and other information from the body of a method and augments the LLM prompt. The notion of Chain-of-Thought prompting or few-shot learning are also leveraged to present sample input methods and their existing developer written summary comments to the LLM through prompts so that it can  learn  the task of summary comment generation. The exemplar methods are not chosen in an ad-hoc manner but are those that are most similar to the input method for which the summary is desired to be generated (refer to Figure  1(a) ). The ASAP approach reports effectiveness on open source projects (including programming languages such as Python and Java).",
            "Hence, we also explore the effectiveness of simpler prompting strategies in generating good Javadoc summary comments. We propose four different simpler prompting strategies that do not have the  overhead  of the ASAP approach;  viz. , our approaches do not require static program analysis, do not require the presence of exemplar similar methods with existing developer written summaries and thus do not employ information retrieval (refer Figure  1(b) ).",
            "Armed with the above additions to the prompt, the ASAP approach formulates the problem as a completion task to the LLM, i.e., complete the summary comment for the given method, given a sample of three methods with their existing summary and an input method, with all methods having the program analysis information such as data flow. The ASAP approach is shown in Figure  1(a) , interested readers can refer to   [ 12 ,  9 ]  for more details where it is described eloquently.",
            "In the following subsections we introduce our prompt-based approaches (in ascending order of \"complexity\") with the approach name introduced in heading of each subsection, depicted in Figure  1(b)  and summarized in Table  1 .",
            "Masking approach:   For the masking experiments to answer  RQ2 , we simply replace the method name with the word MASKED. Thus, for example, in Listing  1 , the method name  remove  is replaced by  MASKED , with everything else remaining the same.",
            "Rows 6 and 7 show how our summary conveys more information than both the original developers summary and the ASAP approach. It uses information obtained from the method body to correctly add additional information in a succinct manner. In row 6, removal is not the only operation but balancing is also done. In row 7 validating is done via checking latitude, longitude and altitude which are mentioned in the summary. The method bodies corresponding to rows 6 and 7 are shown in Listing  1  and Listing  2 , respectively."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Subject Details: LOC = Lines Of Code",
        "table": "S3.T2.1",
        "footnotes": [],
        "references": [
            "The remainder of this paper is organized as follows: In Section  2  we describe our approach.  Section  3  delineates our evaluation, while in Section  4  we depict the related work and we conclude with a focus on future work in Section  5 .",
            "We consider a large Java project within our Ericsson code base and also two popular open source Java projects,  viz. , Guava  [ 14 ]   and ElasticSearch  [ 15 ] . The statistics about the projects such as number of methods and methods with a leading Javadoc comment are shown in Table  2 .",
            "In addition, we report distribution of various metrics of best performing prompt against the baseline along with the p-values for one-sided t-test and KS-tests; these metrics are also reported post masking the method name of the source code while introducing the query in the prompt (Figures  2(a) ,  2(b) ,  3 ). The distribution of best prompt across the selected 100 queries is reported in Figure  4 .",
            "Our results shown below each metric in Fig.  2  show that for CodeLlama (sub-figure  2(a) ), our mean is higher than the baseline in 5 of 8 metrics but not stochastically greater than the baseline in any of metrics. Similarly, with Llama (sub-figure  2(b) ), our best prompt mean is higher than the baseline in 7 of the 8 metrics and stochastically greater in one of them. The results validate that the proposed prompt performs better than the baseline, especially on average, across multiple metrics.",
            "Rows 6 and 7 show how our summary conveys more information than both the original developers summary and the ASAP approach. It uses information obtained from the method body to correctly add additional information in a succinct manner. In row 6, removal is not the only operation but balancing is also done. In row 7 validating is done via checking latitude, longitude and altitude which are mentioned in the summary. The method bodies corresponding to rows 6 and 7 are shown in Listing  1  and Listing  2 , respectively."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Means (and standard deviations) for best different metrics and prompts on commercial project using Codellama as LLM. Best score for each metric is in BOLD.  In case of ties in mean, score with lower standard deviation is chosen as best.",
        "table": "S3.T3.1.1",
        "footnotes": [],
        "references": [
            "Somewhat  surprisingly , we found that simpler prompting approaches worked as well or better than ASAP in Java method summary comment generation task (across a wide variety of  eight  different metrics). We also observe that this result holds with two open source projects, Guava and ElasticSearch and across two chosen LLMs as well (detailed in Section  3 )",
            "Somewhat  unsurprisingly , we found that the actual method name (such as  add ) played a role in the java doc summary comment generation. However, results show that our approach is less susceptible in a statistically significant way to method name masking than the baseline ASAP approach. This indicates that our approach is more robust to method name choices and possibly derives the summary from the body of the method. We detail these observations through various metrics and significance tests in Section  3 .",
            "The remainder of this paper is organized as follows: In Section  2  we describe our approach.  Section  3  delineates our evaluation, while in Section  4  we depict the related work and we conclude with a focus on future work in Section  5 .",
            "We report the distribution of the metrics (using mean, standard deviation) in Tables  3  and   4   for CodeLlama, Llama, respectively (using commercial datasets) and for selected prompts on open source datasets in Table  5  on Codellama, Llama and Mixtral LLMs.",
            "In addition, we report distribution of various metrics of best performing prompt against the baseline along with the p-values for one-sided t-test and KS-tests; these metrics are also reported post masking the method name of the source code while introducing the query in the prompt (Figures  2(a) ,  2(b) ,  3 ). The distribution of best prompt across the selected 100 queries is reported in Figure  4 .",
            "For each of the 100 methods in our commercial evaluation set, we generate the summary for each approach, compute all metrics scores, aggregate and report the mean and standard deviation of the metric scores (and median). The mean (and standard deviations) are shown in Table  3  and Table  4  for CodeLlama and Llama LLMs, respectively.",
            "From the tables, it can be seen that with the CodeLlama LLM Table  3 , our approach reports a slightly higher mean in seven of eight metrics. With the Llama LLM (Table  4 ), our approach had a higher mean value on  all  metrics.",
            "The Table  3  and Table 4  clearly indicate that among the 4 proposed prompts, the  wordrestrictprompt  shows improved performance over the baseline, ASAP. To validate this, we ran a set of statistical tests for our best performing prompt against the baseline. In particular, for each of the metrics we ran a one-sided t-test with the null hypothesis that the means of the two distributions ( viz.  our best prompt and baseline) are equal and the alternate hypothesis that our means is higher than the means of the baseline. We also repoert one-sided KolmogorovSmirnov test outcomes  [ 27 ]  (KS test) with the null hypothesis that the metrics for our best prompt and baseline are drawn from the same distribution and the alternate hypothesis that our metrics are stochastically greater than that of the baseline.",
            "We run similar statistical tests for understanding the robustness of the approaches to method name masking. For the baseline ASAP and our best prompt ( wordrestrictprompt ), we separately compare means and distributions with the method name available and masked approach for each of the 8 metrics and LLM models. For brevity, we show the results only for CodeLlama in Fig.  3 . We run a one-sided t-test, separately for baseline and our best prompt, with the null hypothesis that means for masked and unmasked are equal and the alternate hypothesis that the means for unmasked are higher than that of the method with the name masked. Similarly, we run a KS-test for each metric, independently for ASAP and  wordrestrictprompt , with the null hypothesis that the distribution with method name unmasked is stochastically greater than the distribution with masked.",
            "Our results (Fig.  3 ), show that our prompt is less sensitive to method name choice. In particular, for the baseline prompt shown in sub-figure  3(a)  in 6 (out of 8 metrics) of the means with method name available (i.e. unmasked) are greater than those with the method names masked. On the other hand, for none of the metrics using our prompt the means with unmasked method name is higher than masked, in a statistically significant sense. This shows that our method is more robust to method name choices and gives an indication of the fact that the summary produced by our approach is derived in a more complete manner from the method body itself.",
            "We also used Metas Llama and Codellama as the LLMs in our work based on criteria listed in Section  3 . We do not use OpenAI and other API based LLMs as they are not commercial friendly and thus we are not certain of the generalizability of our approach across all popular LLMs.",
            "We observe from Fig  4  that the performance of  WordRestrict  approach is better than that of baseline. The p-values shows that the improvement is statistically significant. From Figure  3 , we see that the decrease in terms of metric values after masking the method name is significant for baseline approach in 6 out of 8 metrics and 4 out of 8 on our best prompt approach. This implies that much of the code summarization task is influenced by the method name than the logic or flow of the code itself. This is a noteworthy observation as this implies providing correct names to methods can aid in code summarization performance. Thus, method names play a crucial role in summary comment generation as evinced by the reduced metric scores when the method name is masked. This holds even with masking of method names in the ASAP approach as well. Note that ASAP prompts are augmented with Data Flow Graphs, Parameters and local variables as well. Our results (refer Figure  4 ) suggest that simpler prompts with some fine tuning such as requesting the LLM to be concise and prefer brevity does better than the ASAP approach."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Means (and standard deviations) for best different metrics and prompts on commercial project using llama2-70b as LLM. Best score for each metric is in  BOLD.  In case of ties in mean, score with lower standard deviation is chosen as best.",
        "table": "S3.T4.1.1",
        "footnotes": [],
        "references": [
            "The remainder of this paper is organized as follows: In Section  2  we describe our approach.  Section  3  delineates our evaluation, while in Section  4  we depict the related work and we conclude with a focus on future work in Section  5 .",
            "We report the distribution of the metrics (using mean, standard deviation) in Tables  3  and   4   for CodeLlama, Llama, respectively (using commercial datasets) and for selected prompts on open source datasets in Table  5  on Codellama, Llama and Mixtral LLMs.",
            "In addition, we report distribution of various metrics of best performing prompt against the baseline along with the p-values for one-sided t-test and KS-tests; these metrics are also reported post masking the method name of the source code while introducing the query in the prompt (Figures  2(a) ,  2(b) ,  3 ). The distribution of best prompt across the selected 100 queries is reported in Figure  4 .",
            "For each of the 100 methods in our commercial evaluation set, we generate the summary for each approach, compute all metrics scores, aggregate and report the mean and standard deviation of the metric scores (and median). The mean (and standard deviations) are shown in Table  3  and Table  4  for CodeLlama and Llama LLMs, respectively.",
            "From the tables, it can be seen that with the CodeLlama LLM Table  3 , our approach reports a slightly higher mean in seven of eight metrics. With the Llama LLM (Table  4 ), our approach had a higher mean value on  all  metrics.",
            "The Table  3  and Table 4  clearly indicate that among the 4 proposed prompts, the  wordrestrictprompt  shows improved performance over the baseline, ASAP. To validate this, we ran a set of statistical tests for our best performing prompt against the baseline. In particular, for each of the metrics we ran a one-sided t-test with the null hypothesis that the means of the two distributions ( viz.  our best prompt and baseline) are equal and the alternate hypothesis that our means is higher than the means of the baseline. We also repoert one-sided KolmogorovSmirnov test outcomes  [ 27 ]  (KS test) with the null hypothesis that the metrics for our best prompt and baseline are drawn from the same distribution and the alternate hypothesis that our metrics are stochastically greater than that of the baseline.",
            "Across the 100 evaluation methods in our commercial project, which approaches (prompts) performed the best?  Figure  4  shows the distribution of the  best  prompt (approach) across the 100 methods we had in our evaluation set for the Ericsson commercial project. The top part shows the results with the CodeLlama, and the bottom part refers to results using Llama.",
            "We observe from Fig  4  that the performance of  WordRestrict  approach is better than that of baseline. The p-values shows that the improvement is statistically significant. From Figure  3 , we see that the decrease in terms of metric values after masking the method name is significant for baseline approach in 6 out of 8 metrics and 4 out of 8 on our best prompt approach. This implies that much of the code summarization task is influenced by the method name than the logic or flow of the code itself. This is a noteworthy observation as this implies providing correct names to methods can aid in code summarization performance. Thus, method names play a crucial role in summary comment generation as evinced by the reduced metric scores when the method name is masked. This holds even with masking of method names in the ASAP approach as well. Note that ASAP prompts are augmented with Data Flow Graphs, Parameters and local variables as well. Our results (refer Figure  4 ) suggest that simpler prompts with some fine tuning such as requesting the LLM to be concise and prefer brevity does better than the ASAP approach."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Means (and standard deviations) for best different metrics, prompts and LLMs on two open source datasets. We have shown results only for our best prompt (wordrestrictprompt) from the commercial dataset and baseline. The best score for each metric for each dataset and model combination is in BOLD.  In case of ties in mean, score with lower standard deviation is chosen as best.",
        "table": "S3.T5.1.1",
        "footnotes": [],
        "references": [
            "The remainder of this paper is organized as follows: In Section  2  we describe our approach.  Section  3  delineates our evaluation, while in Section  4  we depict the related work and we conclude with a focus on future work in Section  5 .",
            "We report the distribution of the metrics (using mean, standard deviation) in Tables  3  and   4   for CodeLlama, Llama, respectively (using commercial datasets) and for selected prompts on open source datasets in Table  5  on Codellama, Llama and Mixtral LLMs.",
            "To check generalizability of the previous results for proposed approaches, we repeat the experiments on two open source projects, Guava and Elasticsearch. We used only the best of our approaches,  wordrestrict prompt  to compare with ASAP. Table  5  shows the mean and standard deviation across the eight metrics across three LLMs. Note that, we used the Mixtral LLM here in addition to Llama and CodeLlama.",
            "To mitigate the above threat of lack of generalizability across LLMs, we checked the robustness of our techniques with a separate LLM altogether and we chose the recently released Mixtral 8*7B Mixture of Experts (MoE) model. Our results for these are shown in Table  5 . The results indicate robustness of our prompts across the chosen LLMs.  For few of the metrics, the baseline prompt does better on the Mixtral model - this could be due to the way this model (coming from a different organization) may expect its prompt to be structured. This needs further investigation to understand and design techniques to make prompts robust to LLM change."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Samples of Generated Summary Comments (X etc are obfuscated identifier names for confidentiality). Method name not masked. Rows 1 to 4,  blue  indicates correct objects matching the ground truth, while  red  indicates incorrect objects. Rows 6, 7, blue indicates additional useful information obtained from the body of the method by our approach.",
        "table": "S3.T6.1",
        "footnotes": [],
        "references": [
            "Table  6  juxtaposes summary comments written by the developer with the summaries generated by our approach ( WordRestrict  Prompt) and the ASAP. Note that to maintain confidentiality we have obfuscated the class names and denote them as X, Y, Z etc.",
            "Consider the example 1 in Table  6 .  Our approach correctly identifies the operation ( viz. , conversion) and the objects involved X and Y and also adds some more relevant details to the summary. In contrast, the ASAP approach misses the objects involved (A, B and C instead of X and Y), although it gets the action of conversion correct.",
            "Observe that a good summary should ideally convey more information than that can be obtained from the method name, parameters and the enclosing class name. The summary generated in rows 6 and 7 of Table  6 , achieves precisely this using approximately the same amount of words as used by ASAP i.e., our approach tends to be  concise  yet  precise ."
        ]
    },
    "global_footnotes": [
        "www.ericsson.com",
        "https://openai.com/policies/terms-of-use",
        "There was a tie between ASAP and our approach on",
        "metric."
    ]
}