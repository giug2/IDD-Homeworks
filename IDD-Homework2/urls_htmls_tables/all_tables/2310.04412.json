{
    "PAPER'S NUMBER OF TABLES": 10,
    "S3.T1": {
        "caption": "Table 1: A study on the effect of various activation functions.",
        "table": "",
        "footnotes": "",
        "references": [
            []
        ]
    },
    "S3.T2": {
        "caption": "Table 2: (a) Analysis of the effect of reducing activation functions. ‚ÄúActX‚Äù refers to the block that only keeps the activation layer after the Xth convolution layer. ‚ÄúAll‚Äù refers to keeping all activation layers within the block.\n(b) Analysis of the effect of reducing normalization functions. ‚ÄúNormY‚Äù refers to the block that only keeps the normalization layer after the Yth convolution layer. ‚ÄúNo Norm‚Äù refers to removing all normalization layers within the block.",
        "table": "<table id=\"S3.T2.st1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T2.st1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.st1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\">Block</th>\n<th id=\"S3.T2.st1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\">Act</th>\n<th id=\"S3.T2.st1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Central</th>\n<th id=\"S3.T2.st1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">FL</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T2.st1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.st1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S3.T2.st1.1.2.1.1.1\" class=\"ltx_text\">Normal</span></th>\n<th id=\"S3.T2.st1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">All</th>\n<td id=\"S3.T2.st1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">95.82</td>\n<td id=\"S3.T2.st1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">77.44</td>\n</tr>\n<tr id=\"S3.T2.st1.1.3.2\" class=\"ltx_tr\">\n<th id=\"S3.T2.st1.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">\n<span id=\"S3.T2.st1.1.3.2.1.1\" class=\"ltx_ERROR undefined\">\\cdashline</span>2-4</th>\n<th id=\"S3.T2.st1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Act1</th>\n<td id=\"S3.T2.st1.1.3.2.3\" class=\"ltx_td ltx_align_center\">95.41</td>\n<td id=\"S3.T2.st1.1.3.2.4\" class=\"ltx_td ltx_align_center\">79.24</td>\n</tr>\n<tr id=\"S3.T2.st1.1.4.3\" class=\"ltx_tr\">\n<th id=\"S3.T2.st1.1.4.3.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"></th>\n<th id=\"S3.T2.st1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Act2</th>\n<td id=\"S3.T2.st1.1.4.3.3\" class=\"ltx_td ltx_align_center\">95.41</td>\n<td id=\"S3.T2.st1.1.4.3.4\" class=\"ltx_td ltx_align_center\">80.12</td>\n</tr>\n<tr id=\"S3.T2.st1.1.5.4\" class=\"ltx_tr\">\n<th id=\"S3.T2.st1.1.5.4.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"></th>\n<th id=\"S3.T2.st1.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st1.1.5.4.2.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">Act3</span></th>\n<td id=\"S3.T2.st1.1.5.4.3\" class=\"ltx_td ltx_align_center\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st1.1.5.4.3.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">95.89</span></td>\n<td id=\"S3.T2.st1.1.5.4.4\" class=\"ltx_td ltx_align_center\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st1.1.5.4.4.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">84.33</span></td>\n</tr>\n<tr id=\"S3.T2.st1.1.6.5\" class=\"ltx_tr\">\n<th id=\"S3.T2.st1.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S3.T2.st1.1.6.5.1.1\" class=\"ltx_text\">Invert</span></th>\n<th id=\"S3.T2.st1.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">All</th>\n<td id=\"S3.T2.st1.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\">95.64</td>\n<td id=\"S3.T2.st1.1.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\">80.35</td>\n</tr>\n<tr id=\"S3.T2.st1.1.7.6\" class=\"ltx_tr\">\n<th id=\"S3.T2.st1.1.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">\n<span id=\"S3.T2.st1.1.7.6.1.1\" class=\"ltx_ERROR undefined\">\\cdashline</span>2-4</th>\n<th id=\"S3.T2.st1.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st1.1.7.6.2.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">Act1</span></th>\n<td id=\"S3.T2.st1.1.7.6.3\" class=\"ltx_td ltx_align_center\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st1.1.7.6.3.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">96.19</span></td>\n<td id=\"S3.T2.st1.1.7.6.4\" class=\"ltx_td ltx_align_center\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st1.1.7.6.4.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">84.12</span></td>\n</tr>\n<tr id=\"S3.T2.st1.1.8.7\" class=\"ltx_tr\">\n<th id=\"S3.T2.st1.1.8.7.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"></th>\n<th id=\"S3.T2.st1.1.8.7.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Act2</th>\n<td id=\"S3.T2.st1.1.8.7.3\" class=\"ltx_td ltx_align_center\">95.24</td>\n<td id=\"S3.T2.st1.1.8.7.4\" class=\"ltx_td ltx_align_center\">82.06</td>\n</tr>\n<tr id=\"S3.T2.st1.1.9.8\" class=\"ltx_tr\">\n<th id=\"S3.T2.st1.1.9.8.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"></th>\n<th id=\"S3.T2.st1.1.9.8.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Act3</th>\n<td id=\"S3.T2.st1.1.9.8.3\" class=\"ltx_td ltx_align_center\">95.46</td>\n<td id=\"S3.T2.st1.1.9.8.4\" class=\"ltx_td ltx_align_center\">78.60</td>\n</tr>\n<tr id=\"S3.T2.st1.1.10.9\" class=\"ltx_tr\">\n<th id=\"S3.T2.st1.1.10.9.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S3.T2.st1.1.10.9.1.1\" class=\"ltx_text\">InvertUp</span></th>\n<th id=\"S3.T2.st1.1.10.9.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">All</th>\n<td id=\"S3.T2.st1.1.10.9.3\" class=\"ltx_td ltx_align_center ltx_border_t\">95.76</td>\n<td id=\"S3.T2.st1.1.10.9.4\" class=\"ltx_td ltx_align_center ltx_border_t\">80.96</td>\n</tr>\n<tr id=\"S3.T2.st1.1.11.10\" class=\"ltx_tr\">\n<th id=\"S3.T2.st1.1.11.10.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">\n<span id=\"S3.T2.st1.1.11.10.1.1\" class=\"ltx_ERROR undefined\">\\cdashline</span>2-4</th>\n<th id=\"S3.T2.st1.1.11.10.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Act1</th>\n<td id=\"S3.T2.st1.1.11.10.3\" class=\"ltx_td ltx_align_center\">95.21</td>\n<td id=\"S3.T2.st1.1.11.10.4\" class=\"ltx_td ltx_align_center\">76.97</td>\n</tr>\n<tr id=\"S3.T2.st1.1.12.11\" class=\"ltx_tr\">\n<th id=\"S3.T2.st1.1.12.11.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"></th>\n<th id=\"S3.T2.st1.1.12.11.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st1.1.12.11.2.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">Act2</span></th>\n<td id=\"S3.T2.st1.1.12.11.3\" class=\"ltx_td ltx_align_center\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st1.1.12.11.3.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">95.71</span></td>\n<td id=\"S3.T2.st1.1.12.11.4\" class=\"ltx_td ltx_align_center\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st1.1.12.11.4.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">82.44</span></td>\n</tr>\n<tr id=\"S3.T2.st1.1.13.12\" class=\"ltx_tr\">\n<th id=\"S3.T2.st1.1.13.12.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_r\"></th>\n<th id=\"S3.T2.st1.1.13.12.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">Act3</th>\n<td id=\"S3.T2.st1.1.13.12.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">95.56</td>\n<td id=\"S3.T2.st1.1.13.12.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">77.46</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We hereby investigate the impact of activation function selection on model performance in the context of heterogeneous FL. Our exploration begins with GELU, the default activation function in Transformers (Dosovitskiy et¬†al., 2020; Touvron et¬†al., 2021a; Liu et¬†al., 2021). Previous studies show that GELU consistently outperforms ReLU in terms of both clean accuracy and adversarial robustness (Hendrycks & Gimpel, 2016; Elfwing et¬†al., 2018; Clevert et¬†al., 2016; Xie et¬†al., 2020). In our assessment of GELU‚Äôs efficacy for heterogeneous FL, we observe a similar conclusion: as shown in Table¬†2, replacing ReLU with GELU can lead to a significant accuracy improvement of 4.52% in COVID-FL, from 72.92% to 77.44%.",
            "Building upon the insights from (Xie et¬†al., 2020), we next explore the generalization of this improvement to smooth activation functions, which are defined as being ùíûùíû\\mathcal{C}1 smooth. Specifically, we assess five activation functions: two that are non-smooth (Parametric Rectified Linear Unit (PReLU) (He et¬†al., 2015) and Leaky ReLU (LReLU)), and three that are smooth (SoftPlus (Dugas et¬†al., 2000), Exponential Linear Unit (ELU) (Clevert et¬†al., 2016), and Sigmoid Linear Unit (SiLU) (Elfwing et¬†al., 2018)). The curves of these activation functions are shown in Figure¬†2, and their performance on COVID-FL is reported in Table¬†2. Our results show that smooth activation functions generally outperform their non-smooth counterparts in heterogeneous FL. Notably, both SiLU and ELU achieve an accuracy surpassing 78%, markedly superior to the accuracy of LReLU (73.41%) and PReLU (74.24%). Yet, there is an anomaly in our findings: SoftPlus, when replacing ReLU, fails to show a notable improvement. This suggests that smoothness alone is not sufficient for achieving strong performance in heterogeneous FL.",
            "With a closer look at these smooth activation functions, we note a key difference between SoftPlus and the others: while SoftPlus consistently yields positive outputs, the other three (GELU, SiLU, and ELU) can produce negative values for certain input ranges, potentially facilitating outputs that are more centered around zero. To quantitatively characterize this difference, we calculate the mean activation values of ResNet-M across all layers when presented with COVID-FL data. As noted in the ‚ÄúMean‚Äù column of Table¬†2, GELU, SiLU, and ELU consistently hold mean activation values close to zero, while other activation functions exhibit a much larger deviations from zero. These empirical results lead us to conclude that utilizing a smooth and near-zero-centered activation function is advantageous in heterogeneous FL.",
            "We begin our experiments by aggressively reducing the number of activation layers. Specifically, we retain only one activation layer in each block to sustain non-linearity. As highlighted in Table¬†2(a), all three block designs showcase at least one configuration that delivers substantial performance gains in heterogeneous FL. For example, the best configurations yield an improvement of 6.89% for Normal Block (from 77.44% to 84.33%), 3.77% for Invert Block (from 80.35% to 84.12%), and 1.48% for InvertUp Block (from 80.96% to 82.44%). More intriguingly, a simple rule-of-thumb design principle emerges for these best configurations: the activation function is most effective when placed subsequent to the channel-expanding convolution layer, whose output channel dimension is larger than its input dimension.",
            "Building upon our experiments above with only one best-positioned activation layer, we further investigate the impact of aggressively removing normalization layers. Similarly, we hereby are interested in keeping only one normalization layer in each block. As presented in Table¬†2(b), we observe that the effects of removing normalization layers are highly block-dependent in heterogeneous FL: it consistently hurts Normal Block, leads to improvements for the Invert Block (up to +2.07%), and consistently enhances InvertUP block (up to +3.26%).",
            "Another interesting direction to explore is by removing all normalization layers from our models. This is motivated by recent studies that demonstrate high-performance visual recognition with normalization-free CNNs (Brock et¬†al., 2021a; b). To achieve this, we train normalization-free networks using the Adaptive Gradient Clipping (AGC) technique, following (Brock et¬†al., 2021b). The results are presented in Table¬†2(b). Surprisingly, we observe that these normalization-free variants are able to achieve competitive performance compared to their best counterparts with only one activation layer and one normalization layer, e.g., 82.50% vs. 82.33% for Normal Block, 84.63% vs. 86.19% for Invert Block, and 85.65% vs. 85.70% for InvertUP Block. Additionally, a normalization-free setup offers practical advantages such as faster training speed (Singh & Shrivastava, 2019) and reduced GPU memory overhead (Bulo et¬†al., 2018). In our context, compared to the vanilla block instantiations, removing normalization layers leads to a 28.5% acceleration in training, a 38.8% cut in GPU memory, and conveniently bypasses the need to determine the best strategy for reducing normalization layers."
        ]
    },
    "S3.T2.st1": {
        "caption": "(a) ",
        "table": "<table id=\"S3.T2.st1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T2.st1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.st1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\">Block</th>\n<th id=\"S3.T2.st1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\">Act</th>\n<th id=\"S3.T2.st1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Central</th>\n<th id=\"S3.T2.st1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">FL</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T2.st1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.st1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S3.T2.st1.1.2.1.1.1\" class=\"ltx_text\">Normal</span></th>\n<th id=\"S3.T2.st1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">All</th>\n<td id=\"S3.T2.st1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">95.82</td>\n<td id=\"S3.T2.st1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">77.44</td>\n</tr>\n<tr id=\"S3.T2.st1.1.3.2\" class=\"ltx_tr\">\n<th id=\"S3.T2.st1.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">\n<span id=\"S3.T2.st1.1.3.2.1.1\" class=\"ltx_ERROR undefined\">\\cdashline</span>2-4</th>\n<th id=\"S3.T2.st1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Act1</th>\n<td id=\"S3.T2.st1.1.3.2.3\" class=\"ltx_td ltx_align_center\">95.41</td>\n<td id=\"S3.T2.st1.1.3.2.4\" class=\"ltx_td ltx_align_center\">79.24</td>\n</tr>\n<tr id=\"S3.T2.st1.1.4.3\" class=\"ltx_tr\">\n<th id=\"S3.T2.st1.1.4.3.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"></th>\n<th id=\"S3.T2.st1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Act2</th>\n<td id=\"S3.T2.st1.1.4.3.3\" class=\"ltx_td ltx_align_center\">95.41</td>\n<td id=\"S3.T2.st1.1.4.3.4\" class=\"ltx_td ltx_align_center\">80.12</td>\n</tr>\n<tr id=\"S3.T2.st1.1.5.4\" class=\"ltx_tr\">\n<th id=\"S3.T2.st1.1.5.4.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"></th>\n<th id=\"S3.T2.st1.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st1.1.5.4.2.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">Act3</span></th>\n<td id=\"S3.T2.st1.1.5.4.3\" class=\"ltx_td ltx_align_center\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st1.1.5.4.3.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">95.89</span></td>\n<td id=\"S3.T2.st1.1.5.4.4\" class=\"ltx_td ltx_align_center\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st1.1.5.4.4.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">84.33</span></td>\n</tr>\n<tr id=\"S3.T2.st1.1.6.5\" class=\"ltx_tr\">\n<th id=\"S3.T2.st1.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S3.T2.st1.1.6.5.1.1\" class=\"ltx_text\">Invert</span></th>\n<th id=\"S3.T2.st1.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">All</th>\n<td id=\"S3.T2.st1.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\">95.64</td>\n<td id=\"S3.T2.st1.1.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\">80.35</td>\n</tr>\n<tr id=\"S3.T2.st1.1.7.6\" class=\"ltx_tr\">\n<th id=\"S3.T2.st1.1.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">\n<span id=\"S3.T2.st1.1.7.6.1.1\" class=\"ltx_ERROR undefined\">\\cdashline</span>2-4</th>\n<th id=\"S3.T2.st1.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st1.1.7.6.2.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">Act1</span></th>\n<td id=\"S3.T2.st1.1.7.6.3\" class=\"ltx_td ltx_align_center\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st1.1.7.6.3.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">96.19</span></td>\n<td id=\"S3.T2.st1.1.7.6.4\" class=\"ltx_td ltx_align_center\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st1.1.7.6.4.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">84.12</span></td>\n</tr>\n<tr id=\"S3.T2.st1.1.8.7\" class=\"ltx_tr\">\n<th id=\"S3.T2.st1.1.8.7.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"></th>\n<th id=\"S3.T2.st1.1.8.7.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Act2</th>\n<td id=\"S3.T2.st1.1.8.7.3\" class=\"ltx_td ltx_align_center\">95.24</td>\n<td id=\"S3.T2.st1.1.8.7.4\" class=\"ltx_td ltx_align_center\">82.06</td>\n</tr>\n<tr id=\"S3.T2.st1.1.9.8\" class=\"ltx_tr\">\n<th id=\"S3.T2.st1.1.9.8.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"></th>\n<th id=\"S3.T2.st1.1.9.8.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Act3</th>\n<td id=\"S3.T2.st1.1.9.8.3\" class=\"ltx_td ltx_align_center\">95.46</td>\n<td id=\"S3.T2.st1.1.9.8.4\" class=\"ltx_td ltx_align_center\">78.60</td>\n</tr>\n<tr id=\"S3.T2.st1.1.10.9\" class=\"ltx_tr\">\n<th id=\"S3.T2.st1.1.10.9.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S3.T2.st1.1.10.9.1.1\" class=\"ltx_text\">InvertUp</span></th>\n<th id=\"S3.T2.st1.1.10.9.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">All</th>\n<td id=\"S3.T2.st1.1.10.9.3\" class=\"ltx_td ltx_align_center ltx_border_t\">95.76</td>\n<td id=\"S3.T2.st1.1.10.9.4\" class=\"ltx_td ltx_align_center ltx_border_t\">80.96</td>\n</tr>\n<tr id=\"S3.T2.st1.1.11.10\" class=\"ltx_tr\">\n<th id=\"S3.T2.st1.1.11.10.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">\n<span id=\"S3.T2.st1.1.11.10.1.1\" class=\"ltx_ERROR undefined\">\\cdashline</span>2-4</th>\n<th id=\"S3.T2.st1.1.11.10.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Act1</th>\n<td id=\"S3.T2.st1.1.11.10.3\" class=\"ltx_td ltx_align_center\">95.21</td>\n<td id=\"S3.T2.st1.1.11.10.4\" class=\"ltx_td ltx_align_center\">76.97</td>\n</tr>\n<tr id=\"S3.T2.st1.1.12.11\" class=\"ltx_tr\">\n<th id=\"S3.T2.st1.1.12.11.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"></th>\n<th id=\"S3.T2.st1.1.12.11.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st1.1.12.11.2.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">Act2</span></th>\n<td id=\"S3.T2.st1.1.12.11.3\" class=\"ltx_td ltx_align_center\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st1.1.12.11.3.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">95.71</span></td>\n<td id=\"S3.T2.st1.1.12.11.4\" class=\"ltx_td ltx_align_center\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st1.1.12.11.4.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">82.44</span></td>\n</tr>\n<tr id=\"S3.T2.st1.1.13.12\" class=\"ltx_tr\">\n<th id=\"S3.T2.st1.1.13.12.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_r\"></th>\n<th id=\"S3.T2.st1.1.13.12.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">Act3</th>\n<td id=\"S3.T2.st1.1.13.12.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">95.56</td>\n<td id=\"S3.T2.st1.1.13.12.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">77.46</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "To address this challenge, FedProx (Li et¬†al., 2020c) adds a proximal term in the loss function to achieve more stable and accurate convergence; FedAVG-Share (Zhao et¬†al., 2018) keeps a small globally-shared subset amongst devices; SCAFFOLD (Karimireddy et¬†al., 2020) introduces a variable to both estimate and correct update direction of each client. Beyond these, several other techniques have been explored in heterogeneous FL, including reinforcement learning (Wang et¬†al., 2020a), hierarchical clustering (Briggs et¬†al., 2020), knowledge distillation (Zhu et¬†al., 2021; Li & Wang, 2019; Qu et¬†al., 2022a), and self-supervised learning (Yan et¬†al., 2023; Zhang et¬†al., 2021).",
            "We hereby investigate the impact of activation function selection on model performance in the context of heterogeneous FL. Our exploration begins with GELU, the default activation function in Transformers (Dosovitskiy et¬†al., 2020; Touvron et¬†al., 2021a; Liu et¬†al., 2021). Previous studies show that GELU consistently outperforms ReLU in terms of both clean accuracy and adversarial robustness (Hendrycks & Gimpel, 2016; Elfwing et¬†al., 2018; Clevert et¬†al., 2016; Xie et¬†al., 2020). In our assessment of GELU‚Äôs efficacy for heterogeneous FL, we observe a similar conclusion: as shown in Table¬†2, replacing ReLU with GELU can lead to a significant accuracy improvement of 4.52% in COVID-FL, from 72.92% to 77.44%.",
            "Building upon the insights from (Xie et¬†al., 2020), we next explore the generalization of this improvement to smooth activation functions, which are defined as being ùíûùíû\\mathcal{C}1 smooth. Specifically, we assess five activation functions: two that are non-smooth (Parametric Rectified Linear Unit (PReLU) (He et¬†al., 2015) and Leaky ReLU (LReLU)), and three that are smooth (SoftPlus (Dugas et¬†al., 2000), Exponential Linear Unit (ELU) (Clevert et¬†al., 2016), and Sigmoid Linear Unit (SiLU) (Elfwing et¬†al., 2018)). The curves of these activation functions are shown in Figure¬†2, and their performance on COVID-FL is reported in Table¬†2. Our results show that smooth activation functions generally outperform their non-smooth counterparts in heterogeneous FL. Notably, both SiLU and ELU achieve an accuracy surpassing 78%, markedly superior to the accuracy of LReLU (73.41%) and PReLU (74.24%). Yet, there is an anomaly in our findings: SoftPlus, when replacing ReLU, fails to show a notable improvement. This suggests that smoothness alone is not sufficient for achieving strong performance in heterogeneous FL.",
            "With a closer look at these smooth activation functions, we note a key difference between SoftPlus and the others: while SoftPlus consistently yields positive outputs, the other three (GELU, SiLU, and ELU) can produce negative values for certain input ranges, potentially facilitating outputs that are more centered around zero. To quantitatively characterize this difference, we calculate the mean activation values of ResNet-M across all layers when presented with COVID-FL data. As noted in the ‚ÄúMean‚Äù column of Table¬†2, GELU, SiLU, and ELU consistently hold mean activation values close to zero, while other activation functions exhibit a much larger deviations from zero. These empirical results lead us to conclude that utilizing a smooth and near-zero-centered activation function is advantageous in heterogeneous FL.",
            "Transformer blocks, in contrast to traditional CNN blocks, generally incorporate fewer activation and normalization layers (Dosovitskiy et¬†al., 2020; Touvron et¬†al., 2021b; Liu et¬†al., 2021).\nPrior works show that CNNs can remain stable or even attain higher performance with fewer activation layers (Liu et¬†al., 2022) or without normalization layers (Brock et¬†al., 2021b).\nIn this section, we delve into this design choice in the context of heterogeneous FL.",
            "We begin our experiments by aggressively reducing the number of activation layers. Specifically, we retain only one activation layer in each block to sustain non-linearity. As highlighted in Table¬†2(a), all three block designs showcase at least one configuration that delivers substantial performance gains in heterogeneous FL. For example, the best configurations yield an improvement of 6.89% for Normal Block (from 77.44% to 84.33%), 3.77% for Invert Block (from 80.35% to 84.12%), and 1.48% for InvertUp Block (from 80.96% to 82.44%). More intriguingly, a simple rule-of-thumb design principle emerges for these best configurations: the activation function is most effective when placed subsequent to the channel-expanding convolution layer, whose output channel dimension is larger than its input dimension.",
            "Building upon our experiments above with only one best-positioned activation layer, we further investigate the impact of aggressively removing normalization layers. Similarly, we hereby are interested in keeping only one normalization layer in each block. As presented in Table¬†2(b), we observe that the effects of removing normalization layers are highly block-dependent in heterogeneous FL: it consistently hurts Normal Block, leads to improvements for the Invert Block (up to +2.07%), and consistently enhances InvertUP block (up to +3.26%).",
            "Another interesting direction to explore is by removing all normalization layers from our models. This is motivated by recent studies that demonstrate high-performance visual recognition with normalization-free CNNs (Brock et¬†al., 2021a; b). To achieve this, we train normalization-free networks using the Adaptive Gradient Clipping (AGC) technique, following (Brock et¬†al., 2021b). The results are presented in Table¬†2(b). Surprisingly, we observe that these normalization-free variants are able to achieve competitive performance compared to their best counterparts with only one activation layer and one normalization layer, e.g., 82.50% vs. 82.33% for Normal Block, 84.63% vs. 86.19% for Invert Block, and 85.65% vs. 85.70% for InvertUP Block. Additionally, a normalization-free setup offers practical advantages such as faster training speed (Singh & Shrivastava, 2019) and reduced GPU memory overhead (Bulo et¬†al., 2018). In our context, compared to the vanilla block instantiations, removing normalization layers leads to a 28.5% acceleration in training, a 38.8% cut in GPU memory, and conveniently bypasses the need to determine the best strategy for reducing normalization layers.",
            "CNNs and Transformers adopt different pipelines to process input data, which is known as the stem. Typically, CNNs employ a stack of convolutions to downsample images into desired-sized feature maps, while Transformers use patchify layers to directly divide images into a set of tokens. To better understand the impact of the stem layer in heterogeneous FL, we comparatively study diverse stem designs, including the default ResNet-stem, Swin-stem, and ConvStem inspired by (Xiao et¬†al., 2021). A visualization of these stem designs is provided in Figure¬†4, and the empirical results are reported in Table¬†5. We note that 1) both Swin-stem and ConvStem outperform the vanilla ResNet-stem baseline, and 2) ConvStem attains the best performance. Next, we probe potential enhancements to ResNet-stem and Swin-stem by leveraging the ‚Äúadvanced‚Äù designs in ConvStem.",
            "ResNet-stem, despite its employment of a 7√ó\\times7 convolution layer with a stride of 2 ‚Äî thereby extracting features from overlapped patches ‚Äî remarkably lags behind Swin-stem in performance. A noteworthy distinction lies in the ResNet-stem‚Äôs integration of an additional max-pooling layer to facilitate part of its downsampling; while both Swin-stem and ConvStem exclusively rely on convolution layers for this purpose. To understand the role of the max-pooling layer within ResNet-stem, we remove it and adjust the stride of the initial convolution layer from 2 to 4. As shown in Table¬†5, this modification, dubbed ‚ÄúResNet-stem (No MaxPool)‚Äù, registers an impressive 5.15% absolute accuracy improvement over the vanilla ResNet-stem. This observation suggests that employing convolutions alone (hence no pooling layers) for downsampling is important in heterogeneous FL.",
            "Our empirical results demonstrate that these seemingly simple architectural designs collectively lead to a significant performance improvement in heterogeneous FL. As shown in Table¬†4, FedConv models achieve the best performance, surpassing strong competitors such as ViT, Swin-Transformer, and ConvNeXt. The standout performer, FedConv-InvertUp, records the highest accuracy of 92.21%, outperforming the prior art, ConvNeXt, by 2.64%. These outcomes compellingly contest the assertions in (Qu et¬†al., 2022b), highlighting that a pure CNN architecture can be a competitive alternative to ViT in heterogeneous FL scenarios.",
            "Table¬†5 reports the performance on CIFAR-10 and iNaturalist datasets. As data heterogeneity increases from split1 to split3 on CIFAR-10, while FedConv only experiences a modest accuracy drop of 1.85%, other models drop the accuracy by at least 2.35%. On iNaturalist, FedConv impressively achieves an accuracy of 54.19%, surpassing the runner-up, ViT-Small, by more than 10%. These results confirm the strong generalization ability of FedConv in highly heterogeneous FL settings.",
            "The results, as reported in Table¬†6, consistently highlight the superior performance of our FedConv model across these diverse FL methods. This observation underscores FedConv‚Äôs potential to enhance a wide range of heterogeneous FL methods, enabling seamless integration and suggesting its promise for further performance improvements.",
            "As shown in Figure¬†6, our FedConv-InvertUp achieves the fastest convergence speed among all models. In CIFAR-10 split3, where high data heterogeneity exists, FedConv-InvertUp only needs 4 communication rounds to achieve the target accuracy of 90%, while ConvNeXt necessitates 7 rounds. This efficiency also translates to a marked reduction in TMS in FL, as reported in Table¬†7. In contrast, ResNet struggles to converge to the 90% accuracy threshold in the CIFAR-10 split3 setting. These results demonstrate the effectiveness of our proposed FedConv architecture in reducing communication costs and improving the overall FL performance.",
            "We experiment with FedBN(Li et¬†al., 2021), a popular algorithm that also tackles data heterogeneity from the perspective of model architecture. Its key idea is to not average BN layers in FedAVG. We apply this method on the regular ResNet-50, and a ConvNeXt-Tiny model with its normalization layer changed from LN-C to BN. As shown in Table¬†8, in split 1 and 2, where data heterogeneity is not so severe, FedBN achieves close performance compared to FedAVG. However, in a more extreme heterogeneous scenario like CIFAR-10 split3, the performance of both ResNet and ConvNeXt-BN trained by FedBN drops sharply. Specifically, from split1 to split 3, ResNet and ConvNeXt-BN shows a drop of 14.60% (96.42% to 81.82%), and 24.12% (97.99% to 73.87%), respectively. By contrast, the original ConvNeXt that chooses LN-C as its normalization layer, shows only a performance drop of 2.35% (98.20% to 95.85%). Also, our proposed FedConv achieves the best accuracy of 96.26% in split3, demonstrating the effectiveness of the normalization-free design."
        ]
    },
    "S3.T2.st2": {
        "caption": "(b) ",
        "table": "<table id=\"S3.T2.st2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T2.st2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.st2.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\">Block</th>\n<th id=\"S3.T2.st2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Act</th>\n<th id=\"S3.T2.st2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Norm</th>\n<th id=\"S3.T2.st2.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Central</th>\n<th id=\"S3.T2.st2.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">FL</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T2.st2.1.2.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.st2.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S3.T2.st2.1.2.1.1.1\" class=\"ltx_text\">Normal</span></th>\n<td id=\"S3.T2.st2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">Act3</td>\n<td id=\"S3.T2.st2.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">All</td>\n<td id=\"S3.T2.st2.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">95.89</td>\n<td id=\"S3.T2.st2.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">84.33</td>\n</tr>\n<tr id=\"S3.T2.st2.1.3.2\" class=\"ltx_tr\">\n<th id=\"S3.T2.st2.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">\n<span id=\"S3.T2.st2.1.3.2.1.1\" class=\"ltx_ERROR undefined\">\\cdashline</span>2-5</th>\n<td id=\"S3.T2.st2.1.3.2.2\" class=\"ltx_td ltx_align_center\">Act3</td>\n<td id=\"S3.T2.st2.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">Norm1</td>\n<td id=\"S3.T2.st2.1.3.2.4\" class=\"ltx_td ltx_align_center\">95.84</td>\n<td id=\"S3.T2.st2.1.3.2.5\" class=\"ltx_td ltx_align_center\">82.06</td>\n</tr>\n<tr id=\"S3.T2.st2.1.4.3\" class=\"ltx_tr\">\n<th id=\"S3.T2.st2.1.4.3.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"></th>\n<td id=\"S3.T2.st2.1.4.3.2\" class=\"ltx_td ltx_align_center\">Act3</td>\n<td id=\"S3.T2.st2.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">Norm2</td>\n<td id=\"S3.T2.st2.1.4.3.4\" class=\"ltx_td ltx_align_center\">95.36</td>\n<td id=\"S3.T2.st2.1.4.3.5\" class=\"ltx_td ltx_align_center\">82.33</td>\n</tr>\n<tr id=\"S3.T2.st2.1.5.4\" class=\"ltx_tr\">\n<th id=\"S3.T2.st2.1.5.4.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"></th>\n<td id=\"S3.T2.st2.1.5.4.2\" class=\"ltx_td ltx_align_center\">Act3</td>\n<td id=\"S3.T2.st2.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">Norm3</td>\n<td id=\"S3.T2.st2.1.5.4.4\" class=\"ltx_td ltx_align_center\">95.34</td>\n<td id=\"S3.T2.st2.1.5.4.5\" class=\"ltx_td ltx_align_center\">81.36</td>\n</tr>\n<tr id=\"S3.T2.st2.1.6.5\" class=\"ltx_tr\">\n<th id=\"S3.T2.st2.1.6.5.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"></th>\n<td id=\"S3.T2.st2.1.6.5.2\" class=\"ltx_td ltx_align_center\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st2.1.6.5.2.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">Act3</span></td>\n<td id=\"S3.T2.st2.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st2.1.6.5.3.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">No Norm</span></td>\n<td id=\"S3.T2.st2.1.6.5.4\" class=\"ltx_td ltx_align_center\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st2.1.6.5.4.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">95.29</span></td>\n<td id=\"S3.T2.st2.1.6.5.5\" class=\"ltx_td ltx_align_center\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st2.1.6.5.5.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">82.50</span></td>\n</tr>\n<tr id=\"S3.T2.st2.1.7.6\" class=\"ltx_tr\">\n<th id=\"S3.T2.st2.1.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S3.T2.st2.1.7.6.1.1\" class=\"ltx_text\">Invert</span></th>\n<td id=\"S3.T2.st2.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\">Act1</td>\n<td id=\"S3.T2.st2.1.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">All</td>\n<td id=\"S3.T2.st2.1.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_t\">96.19</td>\n<td id=\"S3.T2.st2.1.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_t\">84.12</td>\n</tr>\n<tr id=\"S3.T2.st2.1.8.7\" class=\"ltx_tr\">\n<th id=\"S3.T2.st2.1.8.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">\n<span id=\"S3.T2.st2.1.8.7.1.1\" class=\"ltx_ERROR undefined\">\\cdashline</span>2-5</th>\n<td id=\"S3.T2.st2.1.8.7.2\" class=\"ltx_td ltx_align_center\">Act1</td>\n<td id=\"S3.T2.st2.1.8.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\">Norm1</td>\n<td id=\"S3.T2.st2.1.8.7.4\" class=\"ltx_td ltx_align_center\">95.76</td>\n<td id=\"S3.T2.st2.1.8.7.5\" class=\"ltx_td ltx_align_center\">82.48</td>\n</tr>\n<tr id=\"S3.T2.st2.1.9.8\" class=\"ltx_tr\">\n<th id=\"S3.T2.st2.1.9.8.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"></th>\n<td id=\"S3.T2.st2.1.9.8.2\" class=\"ltx_td ltx_align_center\">Act1</td>\n<td id=\"S3.T2.st2.1.9.8.3\" class=\"ltx_td ltx_align_center ltx_border_r\">Norm2</td>\n<td id=\"S3.T2.st2.1.9.8.4\" class=\"ltx_td ltx_align_center\">96.04</td>\n<td id=\"S3.T2.st2.1.9.8.5\" class=\"ltx_td ltx_align_center\">83.02</td>\n</tr>\n<tr id=\"S3.T2.st2.1.10.9\" class=\"ltx_tr\">\n<th id=\"S3.T2.st2.1.10.9.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"></th>\n<td id=\"S3.T2.st2.1.10.9.2\" class=\"ltx_td ltx_align_center\">Act1</td>\n<td id=\"S3.T2.st2.1.10.9.3\" class=\"ltx_td ltx_align_center ltx_border_r\">Norm3</td>\n<td id=\"S3.T2.st2.1.10.9.4\" class=\"ltx_td ltx_align_center\">95.59</td>\n<td id=\"S3.T2.st2.1.10.9.5\" class=\"ltx_td ltx_align_center\">86.19</td>\n</tr>\n<tr id=\"S3.T2.st2.1.11.10\" class=\"ltx_tr\">\n<th id=\"S3.T2.st2.1.11.10.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"></th>\n<td id=\"S3.T2.st2.1.11.10.2\" class=\"ltx_td ltx_align_center\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st2.1.11.10.2.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">Act1</span></td>\n<td id=\"S3.T2.st2.1.11.10.3\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st2.1.11.10.3.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">No Norm</span></td>\n<td id=\"S3.T2.st2.1.11.10.4\" class=\"ltx_td ltx_align_center\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st2.1.11.10.4.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">95.94</span></td>\n<td id=\"S3.T2.st2.1.11.10.5\" class=\"ltx_td ltx_align_center\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st2.1.11.10.5.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">84.63</span></td>\n</tr>\n<tr id=\"S3.T2.st2.1.12.11\" class=\"ltx_tr\">\n<th id=\"S3.T2.st2.1.12.11.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S3.T2.st2.1.12.11.1.1\" class=\"ltx_text\">InvertUp</span></th>\n<td id=\"S3.T2.st2.1.12.11.2\" class=\"ltx_td ltx_align_center ltx_border_t\">Act2</td>\n<td id=\"S3.T2.st2.1.12.11.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">All</td>\n<td id=\"S3.T2.st2.1.12.11.4\" class=\"ltx_td ltx_align_center ltx_border_t\">95.71</td>\n<td id=\"S3.T2.st2.1.12.11.5\" class=\"ltx_td ltx_align_center ltx_border_t\">82.44</td>\n</tr>\n<tr id=\"S3.T2.st2.1.13.12\" class=\"ltx_tr\">\n<th id=\"S3.T2.st2.1.13.12.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">\n<span id=\"S3.T2.st2.1.13.12.1.1\" class=\"ltx_ERROR undefined\">\\cdashline</span>2-5</th>\n<td id=\"S3.T2.st2.1.13.12.2\" class=\"ltx_td ltx_align_center\">Act2</td>\n<td id=\"S3.T2.st2.1.13.12.3\" class=\"ltx_td ltx_align_center ltx_border_r\">Norm1</td>\n<td id=\"S3.T2.st2.1.13.12.4\" class=\"ltx_td ltx_align_center\">95.64</td>\n<td id=\"S3.T2.st2.1.13.12.5\" class=\"ltx_td ltx_align_center\">82.45</td>\n</tr>\n<tr id=\"S3.T2.st2.1.14.13\" class=\"ltx_tr\">\n<th id=\"S3.T2.st2.1.14.13.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"></th>\n<td id=\"S3.T2.st2.1.14.13.2\" class=\"ltx_td ltx_align_center\">Act2</td>\n<td id=\"S3.T2.st2.1.14.13.3\" class=\"ltx_td ltx_align_center ltx_border_r\">Norm2</td>\n<td id=\"S3.T2.st2.1.14.13.4\" class=\"ltx_td ltx_align_center\">95.64</td>\n<td id=\"S3.T2.st2.1.14.13.5\" class=\"ltx_td ltx_align_center\">83.46</td>\n</tr>\n<tr id=\"S3.T2.st2.1.15.14\" class=\"ltx_tr\">\n<th id=\"S3.T2.st2.1.15.14.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"></th>\n<td id=\"S3.T2.st2.1.15.14.2\" class=\"ltx_td ltx_align_center\">Act2</td>\n<td id=\"S3.T2.st2.1.15.14.3\" class=\"ltx_td ltx_align_center ltx_border_r\">Norm3</td>\n<td id=\"S3.T2.st2.1.15.14.4\" class=\"ltx_td ltx_align_center\">95.71</td>\n<td id=\"S3.T2.st2.1.15.14.5\" class=\"ltx_td ltx_align_center\">85.70</td>\n</tr>\n<tr id=\"S3.T2.st2.1.16.15\" class=\"ltx_tr\">\n<th id=\"S3.T2.st2.1.16.15.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_b ltx_border_r\"></th>\n<td id=\"S3.T2.st2.1.16.15.2\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st2.1.16.15.2.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">Act2</span></td>\n<td id=\"S3.T2.st2.1.16.15.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st2.1.16.15.3.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">No Norm</span></td>\n<td id=\"S3.T2.st2.1.16.15.4\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st2.1.16.15.4.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">95.74</span></td>\n<td id=\"S3.T2.st2.1.16.15.5\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"background-color:#E6E6E6;\"><span id=\"S3.T2.st2.1.16.15.5.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">85.65</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "To address this challenge, FedProx (Li et¬†al., 2020c) adds a proximal term in the loss function to achieve more stable and accurate convergence; FedAVG-Share (Zhao et¬†al., 2018) keeps a small globally-shared subset amongst devices; SCAFFOLD (Karimireddy et¬†al., 2020) introduces a variable to both estimate and correct update direction of each client. Beyond these, several other techniques have been explored in heterogeneous FL, including reinforcement learning (Wang et¬†al., 2020a), hierarchical clustering (Briggs et¬†al., 2020), knowledge distillation (Zhu et¬†al., 2021; Li & Wang, 2019; Qu et¬†al., 2022a), and self-supervised learning (Yan et¬†al., 2023; Zhang et¬†al., 2021).",
            "We hereby investigate the impact of activation function selection on model performance in the context of heterogeneous FL. Our exploration begins with GELU, the default activation function in Transformers (Dosovitskiy et¬†al., 2020; Touvron et¬†al., 2021a; Liu et¬†al., 2021). Previous studies show that GELU consistently outperforms ReLU in terms of both clean accuracy and adversarial robustness (Hendrycks & Gimpel, 2016; Elfwing et¬†al., 2018; Clevert et¬†al., 2016; Xie et¬†al., 2020). In our assessment of GELU‚Äôs efficacy for heterogeneous FL, we observe a similar conclusion: as shown in Table¬†2, replacing ReLU with GELU can lead to a significant accuracy improvement of 4.52% in COVID-FL, from 72.92% to 77.44%.",
            "Building upon the insights from (Xie et¬†al., 2020), we next explore the generalization of this improvement to smooth activation functions, which are defined as being ùíûùíû\\mathcal{C}1 smooth. Specifically, we assess five activation functions: two that are non-smooth (Parametric Rectified Linear Unit (PReLU) (He et¬†al., 2015) and Leaky ReLU (LReLU)), and three that are smooth (SoftPlus (Dugas et¬†al., 2000), Exponential Linear Unit (ELU) (Clevert et¬†al., 2016), and Sigmoid Linear Unit (SiLU) (Elfwing et¬†al., 2018)). The curves of these activation functions are shown in Figure¬†2, and their performance on COVID-FL is reported in Table¬†2. Our results show that smooth activation functions generally outperform their non-smooth counterparts in heterogeneous FL. Notably, both SiLU and ELU achieve an accuracy surpassing 78%, markedly superior to the accuracy of LReLU (73.41%) and PReLU (74.24%). Yet, there is an anomaly in our findings: SoftPlus, when replacing ReLU, fails to show a notable improvement. This suggests that smoothness alone is not sufficient for achieving strong performance in heterogeneous FL.",
            "With a closer look at these smooth activation functions, we note a key difference between SoftPlus and the others: while SoftPlus consistently yields positive outputs, the other three (GELU, SiLU, and ELU) can produce negative values for certain input ranges, potentially facilitating outputs that are more centered around zero. To quantitatively characterize this difference, we calculate the mean activation values of ResNet-M across all layers when presented with COVID-FL data. As noted in the ‚ÄúMean‚Äù column of Table¬†2, GELU, SiLU, and ELU consistently hold mean activation values close to zero, while other activation functions exhibit a much larger deviations from zero. These empirical results lead us to conclude that utilizing a smooth and near-zero-centered activation function is advantageous in heterogeneous FL.",
            "Transformer blocks, in contrast to traditional CNN blocks, generally incorporate fewer activation and normalization layers (Dosovitskiy et¬†al., 2020; Touvron et¬†al., 2021b; Liu et¬†al., 2021).\nPrior works show that CNNs can remain stable or even attain higher performance with fewer activation layers (Liu et¬†al., 2022) or without normalization layers (Brock et¬†al., 2021b).\nIn this section, we delve into this design choice in the context of heterogeneous FL.",
            "We begin our experiments by aggressively reducing the number of activation layers. Specifically, we retain only one activation layer in each block to sustain non-linearity. As highlighted in Table¬†2(a), all three block designs showcase at least one configuration that delivers substantial performance gains in heterogeneous FL. For example, the best configurations yield an improvement of 6.89% for Normal Block (from 77.44% to 84.33%), 3.77% for Invert Block (from 80.35% to 84.12%), and 1.48% for InvertUp Block (from 80.96% to 82.44%). More intriguingly, a simple rule-of-thumb design principle emerges for these best configurations: the activation function is most effective when placed subsequent to the channel-expanding convolution layer, whose output channel dimension is larger than its input dimension.",
            "Building upon our experiments above with only one best-positioned activation layer, we further investigate the impact of aggressively removing normalization layers. Similarly, we hereby are interested in keeping only one normalization layer in each block. As presented in Table¬†2(b), we observe that the effects of removing normalization layers are highly block-dependent in heterogeneous FL: it consistently hurts Normal Block, leads to improvements for the Invert Block (up to +2.07%), and consistently enhances InvertUP block (up to +3.26%).",
            "Another interesting direction to explore is by removing all normalization layers from our models. This is motivated by recent studies that demonstrate high-performance visual recognition with normalization-free CNNs (Brock et¬†al., 2021a; b). To achieve this, we train normalization-free networks using the Adaptive Gradient Clipping (AGC) technique, following (Brock et¬†al., 2021b). The results are presented in Table¬†2(b). Surprisingly, we observe that these normalization-free variants are able to achieve competitive performance compared to their best counterparts with only one activation layer and one normalization layer, e.g., 82.50% vs. 82.33% for Normal Block, 84.63% vs. 86.19% for Invert Block, and 85.65% vs. 85.70% for InvertUP Block. Additionally, a normalization-free setup offers practical advantages such as faster training speed (Singh & Shrivastava, 2019) and reduced GPU memory overhead (Bulo et¬†al., 2018). In our context, compared to the vanilla block instantiations, removing normalization layers leads to a 28.5% acceleration in training, a 38.8% cut in GPU memory, and conveniently bypasses the need to determine the best strategy for reducing normalization layers.",
            "CNNs and Transformers adopt different pipelines to process input data, which is known as the stem. Typically, CNNs employ a stack of convolutions to downsample images into desired-sized feature maps, while Transformers use patchify layers to directly divide images into a set of tokens. To better understand the impact of the stem layer in heterogeneous FL, we comparatively study diverse stem designs, including the default ResNet-stem, Swin-stem, and ConvStem inspired by (Xiao et¬†al., 2021). A visualization of these stem designs is provided in Figure¬†4, and the empirical results are reported in Table¬†5. We note that 1) both Swin-stem and ConvStem outperform the vanilla ResNet-stem baseline, and 2) ConvStem attains the best performance. Next, we probe potential enhancements to ResNet-stem and Swin-stem by leveraging the ‚Äúadvanced‚Äù designs in ConvStem.",
            "ResNet-stem, despite its employment of a 7√ó\\times7 convolution layer with a stride of 2 ‚Äî thereby extracting features from overlapped patches ‚Äî remarkably lags behind Swin-stem in performance. A noteworthy distinction lies in the ResNet-stem‚Äôs integration of an additional max-pooling layer to facilitate part of its downsampling; while both Swin-stem and ConvStem exclusively rely on convolution layers for this purpose. To understand the role of the max-pooling layer within ResNet-stem, we remove it and adjust the stride of the initial convolution layer from 2 to 4. As shown in Table¬†5, this modification, dubbed ‚ÄúResNet-stem (No MaxPool)‚Äù, registers an impressive 5.15% absolute accuracy improvement over the vanilla ResNet-stem. This observation suggests that employing convolutions alone (hence no pooling layers) for downsampling is important in heterogeneous FL.",
            "Our empirical results demonstrate that these seemingly simple architectural designs collectively lead to a significant performance improvement in heterogeneous FL. As shown in Table¬†4, FedConv models achieve the best performance, surpassing strong competitors such as ViT, Swin-Transformer, and ConvNeXt. The standout performer, FedConv-InvertUp, records the highest accuracy of 92.21%, outperforming the prior art, ConvNeXt, by 2.64%. These outcomes compellingly contest the assertions in (Qu et¬†al., 2022b), highlighting that a pure CNN architecture can be a competitive alternative to ViT in heterogeneous FL scenarios.",
            "Table¬†5 reports the performance on CIFAR-10 and iNaturalist datasets. As data heterogeneity increases from split1 to split3 on CIFAR-10, while FedConv only experiences a modest accuracy drop of 1.85%, other models drop the accuracy by at least 2.35%. On iNaturalist, FedConv impressively achieves an accuracy of 54.19%, surpassing the runner-up, ViT-Small, by more than 10%. These results confirm the strong generalization ability of FedConv in highly heterogeneous FL settings.",
            "The results, as reported in Table¬†6, consistently highlight the superior performance of our FedConv model across these diverse FL methods. This observation underscores FedConv‚Äôs potential to enhance a wide range of heterogeneous FL methods, enabling seamless integration and suggesting its promise for further performance improvements.",
            "As shown in Figure¬†6, our FedConv-InvertUp achieves the fastest convergence speed among all models. In CIFAR-10 split3, where high data heterogeneity exists, FedConv-InvertUp only needs 4 communication rounds to achieve the target accuracy of 90%, while ConvNeXt necessitates 7 rounds. This efficiency also translates to a marked reduction in TMS in FL, as reported in Table¬†7. In contrast, ResNet struggles to converge to the 90% accuracy threshold in the CIFAR-10 split3 setting. These results demonstrate the effectiveness of our proposed FedConv architecture in reducing communication costs and improving the overall FL performance.",
            "We experiment with FedBN(Li et¬†al., 2021), a popular algorithm that also tackles data heterogeneity from the perspective of model architecture. Its key idea is to not average BN layers in FedAVG. We apply this method on the regular ResNet-50, and a ConvNeXt-Tiny model with its normalization layer changed from LN-C to BN. As shown in Table¬†8, in split 1 and 2, where data heterogeneity is not so severe, FedBN achieves close performance compared to FedAVG. However, in a more extreme heterogeneous scenario like CIFAR-10 split3, the performance of both ResNet and ConvNeXt-BN trained by FedBN drops sharply. Specifically, from split1 to split 3, ResNet and ConvNeXt-BN shows a drop of 14.60% (96.42% to 81.82%), and 24.12% (97.99% to 73.87%), respectively. By contrast, the original ConvNeXt that chooses LN-C as its normalization layer, shows only a performance drop of 2.35% (98.20% to 95.85%). Also, our proposed FedConv achieves the best accuracy of 96.26% in split3, demonstrating the effectiveness of the normalization-free design."
        ]
    },
    "S3.T3": {
        "caption": "Table 3: Analysis of the effect of various stem layers. ‚Äú(Kernel Size 5)‚Äù denotes using a kernel size of 5 in convolution. ‚Äú(No MaxPool)‚Äù denotes removing the max-pooling layer and increasing the stride of the first convolution layer accordingly.",
        "table": "",
        "footnotes": "",
        "references": [
            []
        ]
    },
    "S4.T4": {
        "caption": "Table 4: Performance comparison on COVID-FL. By incorporating archiectural elements such as SiLU activation function, retaining only one activation function, the normalization-free setup, ConvStem, and a large kernel size of 9, our FedConv models consistently outperform other advanced solutions in heterogeneous FL. ",
        "table": "<table id=\"S4.T4.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">Model</th>\n<td id=\"S4.T4.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FLOPs</td>\n<th id=\"S4.T4.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">Central</th>\n<td id=\"S4.T4.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">FL</td>\n</tr>\n<tr id=\"S4.T4.1.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T4.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">ResNet50</th>\n<td id=\"S4.T4.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4.1G</td>\n<th id=\"S4.T4.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">95.66</th>\n<td id=\"S4.T4.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">73.61</td>\n</tr>\n<tr id=\"S4.T4.1.3.3\" class=\"ltx_tr\">\n<th id=\"S4.T4.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">ResNet-M</th>\n<td id=\"S4.T4.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">4.6G</td>\n<th id=\"S4.T4.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">95.82</th>\n<td id=\"S4.T4.1.3.3.4\" class=\"ltx_td ltx_align_center\">77.44</td>\n</tr>\n<tr id=\"S4.T4.1.4.4\" class=\"ltx_tr\">\n<th id=\"S4.T4.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">Swin-Tiny</th>\n<td id=\"S4.T4.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4.5G</td>\n<th id=\"S4.T4.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">95.74</th>\n<td id=\"S4.T4.1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\">88.38</td>\n</tr>\n<tr id=\"S4.T4.1.5.5\" class=\"ltx_tr\">\n<th id=\"S4.T4.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">ViT-Small</th>\n<td id=\"S4.T4.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\">4.6G</td>\n<th id=\"S4.T4.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">95.86</th>\n<td id=\"S4.T4.1.5.5.4\" class=\"ltx_td ltx_align_center\">84.89</td>\n</tr>\n<tr id=\"S4.T4.1.6.6\" class=\"ltx_tr\">\n<th id=\"S4.T4.1.6.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">ConvNeXt-Tiny</th>\n<td id=\"S4.T4.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\">4.5G</td>\n<th id=\"S4.T4.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">96.01</th>\n<td id=\"S4.T4.1.6.6.4\" class=\"ltx_td ltx_align_center\">89.57</td>\n</tr>\n<tr id=\"S4.T4.1.7.7\" class=\"ltx_tr\" style=\"background-color:#E6E6E6;\">\n<th id=\"S4.T4.1.7.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\"><span id=\"S4.T4.1.7.7.1.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">FedConv-Normal</span></th>\n<td id=\"S4.T4.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T4.1.7.7.2.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">4.6G</span></td>\n<th id=\"S4.T4.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\"><span id=\"S4.T4.1.7.7.3.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">95.84</span></th>\n<td id=\"S4.T4.1.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.1.7.7.4.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">90.61</span></td>\n</tr>\n<tr id=\"S4.T4.1.8.8\" class=\"ltx_tr\" style=\"background-color:#E6E6E6;\">\n<th id=\"S4.T4.1.8.8.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><span id=\"S4.T4.1.8.8.1.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">FedConv-Invert</span></th>\n<td id=\"S4.T4.1.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T4.1.8.8.2.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">4.6G</span></td>\n<th id=\"S4.T4.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><span id=\"S4.T4.1.8.8.3.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">96.19</span></th>\n<td id=\"S4.T4.1.8.8.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.1.8.8.4.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">91.68</span></td>\n</tr>\n<tr id=\"S4.T4.1.9.9\" class=\"ltx_tr\" style=\"background-color:#E6E6E6;\">\n<th id=\"S4.T4.1.9.9.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\"><span id=\"S4.T4.1.9.9.1.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">FedConv-InvertUp</span></th>\n<td id=\"S4.T4.1.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T4.1.9.9.2.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">4.6G</span></td>\n<th id=\"S4.T4.1.9.9.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\"><span id=\"S4.T4.1.9.9.3.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">96.04</span></th>\n<td id=\"S4.T4.1.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S4.T4.1.9.9.4.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">92.21</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Our empirical results demonstrate that these seemingly simple architectural designs collectively lead to a significant performance improvement in heterogeneous FL. As shown in Table¬†4, FedConv models achieve the best performance, surpassing strong competitors such as ViT, Swin-Transformer, and ConvNeXt. The standout performer, FedConv-InvertUp, records the highest accuracy of 92.21%, outperforming the prior art, ConvNeXt, by 2.64%. These outcomes compellingly contest the assertions in (Qu et¬†al., 2022b), highlighting that a pure CNN architecture can be a competitive alternative to ViT in heterogeneous FL scenarios."
        ]
    },
    "S4.T5": {
        "caption": "Table 5: Performance comparison on CIFAR-10 and iNaturalist. Our FedConv model consistently outperforms other models. Notably, as data heterogeneity increases, FedConv‚Äôs strong generalization becomes more evident.",
        "table": "<table id=\"S4.T5.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T5.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T5.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S4.T5.1.1.1.1.1\" class=\"ltx_text\">Model</span></th>\n<td id=\"S4.T5.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"4\">CIFAR-10</td>\n<td id=\"S4.T5.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">iNaturalist</td>\n</tr>\n<tr id=\"S4.T5.1.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T5.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\">Central</td>\n<td id=\"S4.T5.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">Split-1</td>\n<td id=\"S4.T5.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">Split-2</td>\n<td id=\"S4.T5.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Split-3</td>\n<td id=\"S4.T5.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">FL</td>\n</tr>\n<tr id=\"S4.T5.1.3.3\" class=\"ltx_tr\">\n<th id=\"S4.T5.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">ResNet50</th>\n<td id=\"S4.T5.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">97.47</td>\n<td id=\"S4.T5.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">96.69</td>\n<td id=\"S4.T5.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">95.56</td>\n<td id=\"S4.T5.1.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">87.43</td>\n<td id=\"S4.T5.1.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\">12.61</td>\n</tr>\n<tr id=\"S4.T5.1.4.4\" class=\"ltx_tr\">\n<th id=\"S4.T5.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Swin-Tiny</th>\n<td id=\"S4.T5.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\">98.31</td>\n<td id=\"S4.T5.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\">98.36</td>\n<td id=\"S4.T5.1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\">97.83</td>\n<td id=\"S4.T5.1.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">95.22</td>\n<td id=\"S4.T5.1.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\">24.57</td>\n</tr>\n<tr id=\"S4.T5.1.5.5\" class=\"ltx_tr\">\n<th id=\"S4.T5.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">ViT-Small</th>\n<td id=\"S4.T5.1.5.5.2\" class=\"ltx_td ltx_align_center\">97.99</td>\n<td id=\"S4.T5.1.5.5.3\" class=\"ltx_td ltx_align_center\">98.24</td>\n<td id=\"S4.T5.1.5.5.4\" class=\"ltx_td ltx_align_center\">97.84</td>\n<td id=\"S4.T5.1.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_r\">95.64</td>\n<td id=\"S4.T5.1.5.5.6\" class=\"ltx_td ltx_align_center\">40.30</td>\n</tr>\n<tr id=\"S4.T5.1.6.6\" class=\"ltx_tr\">\n<th id=\"S4.T5.1.6.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">ConvNeXt-Tiny</th>\n<td id=\"S4.T5.1.6.6.2\" class=\"ltx_td ltx_align_center\">98.31</td>\n<td id=\"S4.T5.1.6.6.3\" class=\"ltx_td ltx_align_center\">98.20</td>\n<td id=\"S4.T5.1.6.6.4\" class=\"ltx_td ltx_align_center\">97.67</td>\n<td id=\"S4.T5.1.6.6.5\" class=\"ltx_td ltx_align_center ltx_border_r\">95.85</td>\n<td id=\"S4.T5.1.6.6.6\" class=\"ltx_td ltx_align_center\">22.53</td>\n</tr>\n<tr id=\"S4.T5.1.7.7\" class=\"ltx_tr\" style=\"background-color:#E6E6E6;\">\n<th id=\"S4.T5.1.7.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\"><span id=\"S4.T5.1.7.7.1.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">FedConv-InvertUp</span></th>\n<td id=\"S4.T5.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S4.T5.1.7.7.2.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">98.42</span></td>\n<td id=\"S4.T5.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S4.T5.1.7.7.3.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">98.11</span></td>\n<td id=\"S4.T5.1.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S4.T5.1.7.7.4.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">97.74</span></td>\n<td id=\"S4.T5.1.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T5.1.7.7.5.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">96.26</span></td>\n<td id=\"S4.T5.1.7.7.6\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S4.T5.1.7.7.6.1\" class=\"ltx_text\" style=\"background-color:#E6E6E6;\">54.19</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "CNNs and Transformers adopt different pipelines to process input data, which is known as the stem. Typically, CNNs employ a stack of convolutions to downsample images into desired-sized feature maps, while Transformers use patchify layers to directly divide images into a set of tokens. To better understand the impact of the stem layer in heterogeneous FL, we comparatively study diverse stem designs, including the default ResNet-stem, Swin-stem, and ConvStem inspired by (Xiao et¬†al., 2021). A visualization of these stem designs is provided in Figure¬†4, and the empirical results are reported in Table¬†5. We note that 1) both Swin-stem and ConvStem outperform the vanilla ResNet-stem baseline, and 2) ConvStem attains the best performance. Next, we probe potential enhancements to ResNet-stem and Swin-stem by leveraging the ‚Äúadvanced‚Äù designs in ConvStem.",
            "ResNet-stem, despite its employment of a 7√ó\\times7 convolution layer with a stride of 2 ‚Äî thereby extracting features from overlapped patches ‚Äî remarkably lags behind Swin-stem in performance. A noteworthy distinction lies in the ResNet-stem‚Äôs integration of an additional max-pooling layer to facilitate part of its downsampling; while both Swin-stem and ConvStem exclusively rely on convolution layers for this purpose. To understand the role of the max-pooling layer within ResNet-stem, we remove it and adjust the stride of the initial convolution layer from 2 to 4. As shown in Table¬†5, this modification, dubbed ‚ÄúResNet-stem (No MaxPool)‚Äù, registers an impressive 5.15% absolute accuracy improvement over the vanilla ResNet-stem. This observation suggests that employing convolutions alone (hence no pooling layers) for downsampling is important in heterogeneous FL.",
            "Table¬†5 reports the performance on CIFAR-10 and iNaturalist datasets. As data heterogeneity increases from split1 to split3 on CIFAR-10, while FedConv only experiences a modest accuracy drop of 1.85%, other models drop the accuracy by at least 2.35%. On iNaturalist, FedConv impressively achieves an accuracy of 54.19%, surpassing the runner-up, ViT-Small, by more than 10%. These results confirm the strong generalization ability of FedConv in highly heterogeneous FL settings."
        ]
    },
    "S4.T6": {
        "caption": "Table 6: Performance comparison with different FL methods on COVID-FL. ‚ÄôShare‚Äô denotes ‚ÄôFedAVG-Share‚Äô. We note our FedConv consistently shows superior performance.",
        "table": "",
        "footnotes": "",
        "references": [
            "The results, as reported in Table¬†6, consistently highlight the superior performance of our FedConv model across these diverse FL methods. This observation underscores FedConv‚Äôs potential to enhance a wide range of heterogeneous FL methods, enabling seamless integration and suggesting its promise for further performance improvements."
        ]
    },
    "S4.T7": {
        "caption": "Table 7: Comparison based on TMS. TMS is calculated by multiplying the number of model parameters with the communication rounds needed to attain the target accuracy. We note our FedConv requires the lowest TMS to reach the target accuracy.",
        "table": "<table id=\"S4.T7.5\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T7.5.6.1\" class=\"ltx_tr\">\n<th id=\"S4.T7.5.6.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\">Model</th>\n<th id=\"S4.T7.5.6.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">ResNet50</th>\n<th id=\"S4.T7.5.6.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Swin-Tiny</th>\n<th id=\"S4.T7.5.6.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">ViT-Small</th>\n<th id=\"S4.T7.5.6.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">ConvNext-Tiny</th>\n<th id=\"S4.T7.5.6.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">FedConv-InvertUp</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T7.5.5\" class=\"ltx_tr\">\n<th id=\"S4.T7.5.5.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\">TMS</th>\n<td id=\"S4.T7.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><math id=\"S4.T7.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\infty\" display=\"inline\"><semantics id=\"S4.T7.1.1.1.m1.1a\"><mi mathvariant=\"normal\" id=\"S4.T7.1.1.1.m1.1.1\" xref=\"S4.T7.1.1.1.m1.1.1.cmml\">‚àû</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T7.1.1.1.m1.1b\"><infinity id=\"S4.T7.1.1.1.m1.1.1.cmml\" xref=\"S4.T7.1.1.1.m1.1.1\"></infinity></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T7.1.1.1.m1.1c\">\\infty</annotation></semantics></math></td>\n<td id=\"S4.T7.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">27.5M<math id=\"S4.T7.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S4.T7.2.2.2.m1.1a\"><mo id=\"S4.T7.2.2.2.m1.1.1\" xref=\"S4.T7.2.2.2.m1.1.1.cmml\">√ó</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T7.2.2.2.m1.1b\"><times id=\"S4.T7.2.2.2.m1.1.1.cmml\" xref=\"S4.T7.2.2.2.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T7.2.2.2.m1.1c\">\\times</annotation></semantics></math>10</td>\n<td id=\"S4.T7.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">21.7M<math id=\"S4.T7.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S4.T7.3.3.3.m1.1a\"><mo id=\"S4.T7.3.3.3.m1.1.1\" xref=\"S4.T7.3.3.3.m1.1.1.cmml\">√ó</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T7.3.3.3.m1.1b\"><times id=\"S4.T7.3.3.3.m1.1.1.cmml\" xref=\"S4.T7.3.3.3.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T7.3.3.3.m1.1c\">\\times</annotation></semantics></math>11</td>\n<td id=\"S4.T7.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">27.8M<math id=\"S4.T7.4.4.4.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S4.T7.4.4.4.m1.1a\"><mo id=\"S4.T7.4.4.4.m1.1.1\" xref=\"S4.T7.4.4.4.m1.1.1.cmml\">√ó</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T7.4.4.4.m1.1b\"><times id=\"S4.T7.4.4.4.m1.1.1.cmml\" xref=\"S4.T7.4.4.4.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T7.4.4.4.m1.1c\">\\times</annotation></semantics></math>8</td>\n<td id=\"S4.T7.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">25.6M<math id=\"S4.T7.5.5.5.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S4.T7.5.5.5.m1.1a\"><mo id=\"S4.T7.5.5.5.m1.1.1\" xref=\"S4.T7.5.5.5.m1.1.1.cmml\">√ó</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T7.5.5.5.m1.1b\"><times id=\"S4.T7.5.5.5.m1.1.1.cmml\" xref=\"S4.T7.5.5.5.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T7.5.5.5.m1.1c\">\\times</annotation></semantics></math>5</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "As shown in Figure¬†6, our FedConv-InvertUp achieves the fastest convergence speed among all models. In CIFAR-10 split3, where high data heterogeneity exists, FedConv-InvertUp only needs 4 communication rounds to achieve the target accuracy of 90%, while ConvNeXt necessitates 7 rounds. This efficiency also translates to a marked reduction in TMS in FL, as reported in Table¬†7. In contrast, ResNet struggles to converge to the 90% accuracy threshold in the CIFAR-10 split3 setting. These results demonstrate the effectiveness of our proposed FedConv architecture in reducing communication costs and improving the overall FL performance."
        ]
    },
    "A1.T8": {
        "caption": "Table 8: Performance comparison on CIFAR-10 dataset. ‚ÄôConvNeXt-BN‚Äô denotes ConvNeXt with LN-C changed to BN. ‚Äô¬±plus-or-minus\\pm‚Äô indicates the range of accuracy between clients. ",
        "table": "<table id=\"A1.T8.17\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T8.17.16.1\" class=\"ltx_tr\">\n<th id=\"A1.T8.17.16.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Method</th>\n<th id=\"A1.T8.17.16.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Model</th>\n<td id=\"A1.T8.17.16.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">Split1</td>\n<td id=\"A1.T8.17.16.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">Split2</td>\n<td id=\"A1.T8.17.16.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">Split3</td>\n</tr>\n<tr id=\"A1.T8.5.3\" class=\"ltx_tr\">\n<th id=\"A1.T8.5.3.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"A1.T8.5.3.4.1\" class=\"ltx_text\">FedBN</span></th>\n<th id=\"A1.T8.5.3.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">ResNet50</th>\n<td id=\"A1.T8.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">96.42<math id=\"A1.T8.3.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A1.T8.3.1.1.m1.1a\"><mo id=\"A1.T8.3.1.1.m1.1.1\" xref=\"A1.T8.3.1.1.m1.1.1.cmml\">¬±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A1.T8.3.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"A1.T8.3.1.1.m1.1.1.cmml\" xref=\"A1.T8.3.1.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T8.3.1.1.m1.1c\">\\pm</annotation></semantics></math>0.18</td>\n<td id=\"A1.T8.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">93.15<math id=\"A1.T8.4.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A1.T8.4.2.2.m1.1a\"><mo id=\"A1.T8.4.2.2.m1.1.1\" xref=\"A1.T8.4.2.2.m1.1.1.cmml\">¬±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A1.T8.4.2.2.m1.1b\"><csymbol cd=\"latexml\" id=\"A1.T8.4.2.2.m1.1.1.cmml\" xref=\"A1.T8.4.2.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T8.4.2.2.m1.1c\">\\pm</annotation></semantics></math>1.35</td>\n<td id=\"A1.T8.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">81.82<math id=\"A1.T8.5.3.3.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A1.T8.5.3.3.m1.1a\"><mo id=\"A1.T8.5.3.3.m1.1.1\" xref=\"A1.T8.5.3.3.m1.1.1.cmml\">¬±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A1.T8.5.3.3.m1.1b\"><csymbol cd=\"latexml\" id=\"A1.T8.5.3.3.m1.1.1.cmml\" xref=\"A1.T8.5.3.3.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T8.5.3.3.m1.1c\">\\pm</annotation></semantics></math>1.52</td>\n</tr>\n<tr id=\"A1.T8.8.6\" class=\"ltx_tr\">\n<th id=\"A1.T8.8.6.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">ConvNeXt-BN</th>\n<td id=\"A1.T8.6.4.1\" class=\"ltx_td ltx_align_center\">97.99<math id=\"A1.T8.6.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A1.T8.6.4.1.m1.1a\"><mo id=\"A1.T8.6.4.1.m1.1.1\" xref=\"A1.T8.6.4.1.m1.1.1.cmml\">¬±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A1.T8.6.4.1.m1.1b\"><csymbol cd=\"latexml\" id=\"A1.T8.6.4.1.m1.1.1.cmml\" xref=\"A1.T8.6.4.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T8.6.4.1.m1.1c\">\\pm</annotation></semantics></math>0.05</td>\n<td id=\"A1.T8.7.5.2\" class=\"ltx_td ltx_align_center\">95.76<math id=\"A1.T8.7.5.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A1.T8.7.5.2.m1.1a\"><mo id=\"A1.T8.7.5.2.m1.1.1\" xref=\"A1.T8.7.5.2.m1.1.1.cmml\">¬±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A1.T8.7.5.2.m1.1b\"><csymbol cd=\"latexml\" id=\"A1.T8.7.5.2.m1.1.1.cmml\" xref=\"A1.T8.7.5.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T8.7.5.2.m1.1c\">\\pm</annotation></semantics></math>0.82</td>\n<td id=\"A1.T8.8.6.3\" class=\"ltx_td ltx_align_center\">73.87<math id=\"A1.T8.8.6.3.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A1.T8.8.6.3.m1.1a\"><mo id=\"A1.T8.8.6.3.m1.1.1\" xref=\"A1.T8.8.6.3.m1.1.1.cmml\">¬±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A1.T8.8.6.3.m1.1b\"><csymbol cd=\"latexml\" id=\"A1.T8.8.6.3.m1.1.1.cmml\" xref=\"A1.T8.8.6.3.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T8.8.6.3.m1.1c\">\\pm</annotation></semantics></math>12.57</td>\n</tr>\n<tr id=\"A1.T8.11.9\" class=\"ltx_tr\">\n<th id=\"A1.T8.11.9.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"A1.T8.11.9.4.1\" class=\"ltx_text\">FedAVG</span></th>\n<th id=\"A1.T8.11.9.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">ResNet50</th>\n<td id=\"A1.T8.9.7.1\" class=\"ltx_td ltx_align_center ltx_border_t\">96.69<math id=\"A1.T8.9.7.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A1.T8.9.7.1.m1.1a\"><mo id=\"A1.T8.9.7.1.m1.1.1\" xref=\"A1.T8.9.7.1.m1.1.1.cmml\">¬±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A1.T8.9.7.1.m1.1b\"><csymbol cd=\"latexml\" id=\"A1.T8.9.7.1.m1.1.1.cmml\" xref=\"A1.T8.9.7.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T8.9.7.1.m1.1c\">\\pm</annotation></semantics></math>0.00</td>\n<td id=\"A1.T8.10.8.2\" class=\"ltx_td ltx_align_center ltx_border_t\">95.56<math id=\"A1.T8.10.8.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A1.T8.10.8.2.m1.1a\"><mo id=\"A1.T8.10.8.2.m1.1.1\" xref=\"A1.T8.10.8.2.m1.1.1.cmml\">¬±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A1.T8.10.8.2.m1.1b\"><csymbol cd=\"latexml\" id=\"A1.T8.10.8.2.m1.1.1.cmml\" xref=\"A1.T8.10.8.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T8.10.8.2.m1.1c\">\\pm</annotation></semantics></math>0.00</td>\n<td id=\"A1.T8.11.9.3\" class=\"ltx_td ltx_align_center ltx_border_t\">87.43 <math id=\"A1.T8.11.9.3.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A1.T8.11.9.3.m1.1a\"><mo id=\"A1.T8.11.9.3.m1.1.1\" xref=\"A1.T8.11.9.3.m1.1.1.cmml\">¬±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A1.T8.11.9.3.m1.1b\"><csymbol cd=\"latexml\" id=\"A1.T8.11.9.3.m1.1.1.cmml\" xref=\"A1.T8.11.9.3.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T8.11.9.3.m1.1c\">\\pm</annotation></semantics></math>0.00</td>\n</tr>\n<tr id=\"A1.T8.14.12\" class=\"ltx_tr\">\n<th id=\"A1.T8.14.12.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">ConvNeXt</th>\n<td id=\"A1.T8.12.10.1\" class=\"ltx_td ltx_align_center\">98.20<math id=\"A1.T8.12.10.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A1.T8.12.10.1.m1.1a\"><mo id=\"A1.T8.12.10.1.m1.1.1\" xref=\"A1.T8.12.10.1.m1.1.1.cmml\">¬±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A1.T8.12.10.1.m1.1b\"><csymbol cd=\"latexml\" id=\"A1.T8.12.10.1.m1.1.1.cmml\" xref=\"A1.T8.12.10.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T8.12.10.1.m1.1c\">\\pm</annotation></semantics></math>0.00</td>\n<td id=\"A1.T8.13.11.2\" class=\"ltx_td ltx_align_center\">97.67<math id=\"A1.T8.13.11.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A1.T8.13.11.2.m1.1a\"><mo id=\"A1.T8.13.11.2.m1.1.1\" xref=\"A1.T8.13.11.2.m1.1.1.cmml\">¬±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A1.T8.13.11.2.m1.1b\"><csymbol cd=\"latexml\" id=\"A1.T8.13.11.2.m1.1.1.cmml\" xref=\"A1.T8.13.11.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T8.13.11.2.m1.1c\">\\pm</annotation></semantics></math>0.00</td>\n<td id=\"A1.T8.14.12.3\" class=\"ltx_td ltx_align_center\">95.85<math id=\"A1.T8.14.12.3.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A1.T8.14.12.3.m1.1a\"><mo id=\"A1.T8.14.12.3.m1.1.1\" xref=\"A1.T8.14.12.3.m1.1.1.cmml\">¬±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A1.T8.14.12.3.m1.1b\"><csymbol cd=\"latexml\" id=\"A1.T8.14.12.3.m1.1.1.cmml\" xref=\"A1.T8.14.12.3.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T8.14.12.3.m1.1c\">\\pm</annotation></semantics></math>0.00</td>\n</tr>\n<tr id=\"A1.T8.17.15\" class=\"ltx_tr\">\n<th id=\"A1.T8.17.15.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\">FedConv-InvertUp</th>\n<td id=\"A1.T8.15.13.1\" class=\"ltx_td ltx_align_center ltx_border_b\">98.11<math id=\"A1.T8.15.13.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A1.T8.15.13.1.m1.1a\"><mo id=\"A1.T8.15.13.1.m1.1.1\" xref=\"A1.T8.15.13.1.m1.1.1.cmml\">¬±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A1.T8.15.13.1.m1.1b\"><csymbol cd=\"latexml\" id=\"A1.T8.15.13.1.m1.1.1.cmml\" xref=\"A1.T8.15.13.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T8.15.13.1.m1.1c\">\\pm</annotation></semantics></math>0.00</td>\n<td id=\"A1.T8.16.14.2\" class=\"ltx_td ltx_align_center ltx_border_b\">97.74<math id=\"A1.T8.16.14.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A1.T8.16.14.2.m1.1a\"><mo id=\"A1.T8.16.14.2.m1.1.1\" xref=\"A1.T8.16.14.2.m1.1.1.cmml\">¬±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A1.T8.16.14.2.m1.1b\"><csymbol cd=\"latexml\" id=\"A1.T8.16.14.2.m1.1.1.cmml\" xref=\"A1.T8.16.14.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T8.16.14.2.m1.1c\">\\pm</annotation></semantics></math>0.00</td>\n<td id=\"A1.T8.17.15.3\" class=\"ltx_td ltx_align_center ltx_border_b\">96.26<math id=\"A1.T8.17.15.3.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A1.T8.17.15.3.m1.1a\"><mo id=\"A1.T8.17.15.3.m1.1.1\" xref=\"A1.T8.17.15.3.m1.1.1.cmml\">¬±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A1.T8.17.15.3.m1.1b\"><csymbol cd=\"latexml\" id=\"A1.T8.17.15.3.m1.1.1.cmml\" xref=\"A1.T8.17.15.3.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T8.17.15.3.m1.1c\">\\pm</annotation></semantics></math>0.00</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We experiment with FedBN(Li et¬†al., 2021), a popular algorithm that also tackles data heterogeneity from the perspective of model architecture. Its key idea is to not average BN layers in FedAVG. We apply this method on the regular ResNet-50, and a ConvNeXt-Tiny model with its normalization layer changed from LN-C to BN. As shown in Table¬†8, in split 1 and 2, where data heterogeneity is not so severe, FedBN achieves close performance compared to FedAVG. However, in a more extreme heterogeneous scenario like CIFAR-10 split3, the performance of both ResNet and ConvNeXt-BN trained by FedBN drops sharply. Specifically, from split1 to split 3, ResNet and ConvNeXt-BN shows a drop of 14.60% (96.42% to 81.82%), and 24.12% (97.99% to 73.87%), respectively. By contrast, the original ConvNeXt that chooses LN-C as its normalization layer, shows only a performance drop of 2.35% (98.20% to 95.85%). Also, our proposed FedConv achieves the best accuracy of 96.26% in split3, demonstrating the effectiveness of the normalization-free design."
        ]
    }
}