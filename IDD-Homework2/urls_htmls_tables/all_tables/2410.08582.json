{
    "S3.T1.40": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T1.40\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.40.41.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\" id=\"S3.T1.40.41.1.1\"><span class=\"ltx_text\" id=\"S3.T1.40.41.1.1.1\" style=\"font-size:90%;\">Variant Architectures of DeBiFormer</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.40.42.2\">\n<td class=\"ltx_td ltx_border_r\" id=\"S3.T1.40.42.2.1\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.40.42.2.2\"><span class=\"ltx_text\" id=\"S3.T1.40.42.2.2.1\" style=\"font-size:90%;\">DeBi-T</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.40.42.2.3\"><span class=\"ltx_text\" id=\"S3.T1.40.42.2.3.1\" style=\"font-size:90%;\">DeBi-S</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.40.42.2.4\"><span class=\"ltx_text\" id=\"S3.T1.40.42.2.4.1\" style=\"font-size:90%;\">DeBi-B</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.10.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.1.1.1\"><span class=\"ltx_text\" id=\"S3.T1.1.1.1.1\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.1.1.1.1.1\">\n<span class=\"ltx_tr\" id=\"S3.T1.1.1.1.1.1.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.1.1.1.1.1.2.1\">Stage 1</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.1.1.1.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.1.1.1.1.1.1.1\">56<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.1.1.1.1.1.1.1.m1.1\"><semantics id=\"S3.T1.1.1.1.1.1.1.1.m1.1a\"><mo id=\"S3.T1.1.1.1.1.1.1.1.m1.1.1\" xref=\"S3.T1.1.1.1.1.1.1.1.m1.1.1.cmml\">&#215;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.1.1.1.1.1.1.1.m1.1b\"><times id=\"S3.T1.1.1.1.1.1.1.1.m1.1.1.cmml\" xref=\"S3.T1.1.1.1.1.1.1.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.1.1.1.1.1.1.1.m1.1c\">\\times</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.1.1.1.1.1.1.1.m1.1d\">&#215;</annotation></semantics></math>56</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.4.4.4\"><span class=\"ltx_text\" id=\"S3.T1.4.4.4.3\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.4.4.4.3.3\">\n<span class=\"ltx_tr\" id=\"S3.T1.2.2.2.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.2.2.2.1.1.1.1\"><math alttext=\"N_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.2.2.2.1.1.1.1.m1.1\"><semantics id=\"S3.T1.2.2.2.1.1.1.1.m1.1a\"><msub id=\"S3.T1.2.2.2.1.1.1.1.m1.1.1\" xref=\"S3.T1.2.2.2.1.1.1.1.m1.1.1.cmml\"><mi id=\"S3.T1.2.2.2.1.1.1.1.m1.1.1.2\" xref=\"S3.T1.2.2.2.1.1.1.1.m1.1.1.2.cmml\">N</mi><mn id=\"S3.T1.2.2.2.1.1.1.1.m1.1.1.3\" xref=\"S3.T1.2.2.2.1.1.1.1.m1.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.2.2.2.1.1.1.1.m1.1b\"><apply id=\"S3.T1.2.2.2.1.1.1.1.m1.1.1.cmml\" xref=\"S3.T1.2.2.2.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.2.2.2.1.1.1.1.m1.1.1.1.cmml\" xref=\"S3.T1.2.2.2.1.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.2.2.2.1.1.1.1.m1.1.1.2.cmml\" xref=\"S3.T1.2.2.2.1.1.1.1.m1.1.1.2\">&#119873;</ci><cn id=\"S3.T1.2.2.2.1.1.1.1.m1.1.1.3.cmml\" type=\"integer\" xref=\"S3.T1.2.2.2.1.1.1.1.m1.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.2.2.2.1.1.1.1.m1.1c\">N_{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.2.2.2.1.1.1.1.m1.1d\">italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>=2, C=64</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.3.3.3.2.2.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.3.3.3.2.2.2.1\">r=8, M=2, <math alttext=\"D_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.3.3.3.2.2.2.1.m1.1\"><semantics id=\"S3.T1.3.3.3.2.2.2.1.m1.1a\"><msub id=\"S3.T1.3.3.3.2.2.2.1.m1.1.1\" xref=\"S3.T1.3.3.3.2.2.2.1.m1.1.1.cmml\"><mi id=\"S3.T1.3.3.3.2.2.2.1.m1.1.1.2\" xref=\"S3.T1.3.3.3.2.2.2.1.m1.1.1.2.cmml\">D</mi><mi id=\"S3.T1.3.3.3.2.2.2.1.m1.1.1.3\" xref=\"S3.T1.3.3.3.2.2.2.1.m1.1.1.3.cmml\">r</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.3.3.3.2.2.2.1.m1.1b\"><apply id=\"S3.T1.3.3.3.2.2.2.1.m1.1.1.cmml\" xref=\"S3.T1.3.3.3.2.2.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.3.3.3.2.2.2.1.m1.1.1.1.cmml\" xref=\"S3.T1.3.3.3.2.2.2.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.3.3.3.2.2.2.1.m1.1.1.2.cmml\" xref=\"S3.T1.3.3.3.2.2.2.1.m1.1.1.2\">&#119863;</ci><ci id=\"S3.T1.3.3.3.2.2.2.1.m1.1.1.3.cmml\" xref=\"S3.T1.3.3.3.2.2.2.1.m1.1.1.3\">&#119903;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.3.3.3.2.2.2.1.m1.1c\">D_{r}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.3.3.3.2.2.2.1.m1.1d\">italic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>=3</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.4.4.4.3.3.3\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.4.4.4.3.3.3.1\">G=1, K=9, <math alttext=\"B_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.4.4.4.3.3.3.1.m1.1\"><semantics id=\"S3.T1.4.4.4.3.3.3.1.m1.1a\"><msub id=\"S3.T1.4.4.4.3.3.3.1.m1.1.1\" xref=\"S3.T1.4.4.4.3.3.3.1.m1.1.1.cmml\"><mi id=\"S3.T1.4.4.4.3.3.3.1.m1.1.1.2\" xref=\"S3.T1.4.4.4.3.3.3.1.m1.1.1.2.cmml\">B</mi><mi id=\"S3.T1.4.4.4.3.3.3.1.m1.1.1.3\" xref=\"S3.T1.4.4.4.3.3.3.1.m1.1.1.3.cmml\">r</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.4.4.4.3.3.3.1.m1.1b\"><apply id=\"S3.T1.4.4.4.3.3.3.1.m1.1.1.cmml\" xref=\"S3.T1.4.4.4.3.3.3.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.4.4.4.3.3.3.1.m1.1.1.1.cmml\" xref=\"S3.T1.4.4.4.3.3.3.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.4.4.4.3.3.3.1.m1.1.1.2.cmml\" xref=\"S3.T1.4.4.4.3.3.3.1.m1.1.1.2\">&#119861;</ci><ci id=\"S3.T1.4.4.4.3.3.3.1.m1.1.1.3.cmml\" xref=\"S3.T1.4.4.4.3.3.3.1.m1.1.1.3\">&#119903;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.4.4.4.3.3.3.1.m1.1c\">B_{r}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.4.4.4.3.3.3.1.m1.1d\">italic_B start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>=3</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.7.7.7\"><span class=\"ltx_text\" id=\"S3.T1.7.7.7.3\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.7.7.7.3.3\">\n<span class=\"ltx_tr\" id=\"S3.T1.5.5.5.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.5.5.5.1.1.1.1\"><math alttext=\"N_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.5.5.5.1.1.1.1.m1.1\"><semantics id=\"S3.T1.5.5.5.1.1.1.1.m1.1a\"><msub id=\"S3.T1.5.5.5.1.1.1.1.m1.1.1\" xref=\"S3.T1.5.5.5.1.1.1.1.m1.1.1.cmml\"><mi id=\"S3.T1.5.5.5.1.1.1.1.m1.1.1.2\" xref=\"S3.T1.5.5.5.1.1.1.1.m1.1.1.2.cmml\">N</mi><mn id=\"S3.T1.5.5.5.1.1.1.1.m1.1.1.3\" xref=\"S3.T1.5.5.5.1.1.1.1.m1.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.5.5.5.1.1.1.1.m1.1b\"><apply id=\"S3.T1.5.5.5.1.1.1.1.m1.1.1.cmml\" xref=\"S3.T1.5.5.5.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.5.5.5.1.1.1.1.m1.1.1.1.cmml\" xref=\"S3.T1.5.5.5.1.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.5.5.5.1.1.1.1.m1.1.1.2.cmml\" xref=\"S3.T1.5.5.5.1.1.1.1.m1.1.1.2\">&#119873;</ci><cn id=\"S3.T1.5.5.5.1.1.1.1.m1.1.1.3.cmml\" type=\"integer\" xref=\"S3.T1.5.5.5.1.1.1.1.m1.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.5.5.5.1.1.1.1.m1.1c\">N_{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.5.5.5.1.1.1.1.m1.1d\">italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>=4, C=64</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.6.6.6.2.2.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.6.6.6.2.2.2.1\">r=8, M=2, <math alttext=\"D_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.6.6.6.2.2.2.1.m1.1\"><semantics id=\"S3.T1.6.6.6.2.2.2.1.m1.1a\"><msub id=\"S3.T1.6.6.6.2.2.2.1.m1.1.1\" xref=\"S3.T1.6.6.6.2.2.2.1.m1.1.1.cmml\"><mi id=\"S3.T1.6.6.6.2.2.2.1.m1.1.1.2\" xref=\"S3.T1.6.6.6.2.2.2.1.m1.1.1.2.cmml\">D</mi><mi id=\"S3.T1.6.6.6.2.2.2.1.m1.1.1.3\" xref=\"S3.T1.6.6.6.2.2.2.1.m1.1.1.3.cmml\">r</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.6.6.6.2.2.2.1.m1.1b\"><apply id=\"S3.T1.6.6.6.2.2.2.1.m1.1.1.cmml\" xref=\"S3.T1.6.6.6.2.2.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.6.6.6.2.2.2.1.m1.1.1.1.cmml\" xref=\"S3.T1.6.6.6.2.2.2.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.6.6.6.2.2.2.1.m1.1.1.2.cmml\" xref=\"S3.T1.6.6.6.2.2.2.1.m1.1.1.2\">&#119863;</ci><ci id=\"S3.T1.6.6.6.2.2.2.1.m1.1.1.3.cmml\" xref=\"S3.T1.6.6.6.2.2.2.1.m1.1.1.3\">&#119903;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.6.6.6.2.2.2.1.m1.1c\">D_{r}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.6.6.6.2.2.2.1.m1.1d\">italic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>=3</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.7.7.7.3.3.3\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.7.7.7.3.3.3.1\">G=1, K=9, <math alttext=\"B_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.7.7.7.3.3.3.1.m1.1\"><semantics id=\"S3.T1.7.7.7.3.3.3.1.m1.1a\"><msub id=\"S3.T1.7.7.7.3.3.3.1.m1.1.1\" xref=\"S3.T1.7.7.7.3.3.3.1.m1.1.1.cmml\"><mi id=\"S3.T1.7.7.7.3.3.3.1.m1.1.1.2\" xref=\"S3.T1.7.7.7.3.3.3.1.m1.1.1.2.cmml\">B</mi><mi id=\"S3.T1.7.7.7.3.3.3.1.m1.1.1.3\" xref=\"S3.T1.7.7.7.3.3.3.1.m1.1.1.3.cmml\">r</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.7.7.7.3.3.3.1.m1.1b\"><apply id=\"S3.T1.7.7.7.3.3.3.1.m1.1.1.cmml\" xref=\"S3.T1.7.7.7.3.3.3.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.7.7.7.3.3.3.1.m1.1.1.1.cmml\" xref=\"S3.T1.7.7.7.3.3.3.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.7.7.7.3.3.3.1.m1.1.1.2.cmml\" xref=\"S3.T1.7.7.7.3.3.3.1.m1.1.1.2\">&#119861;</ci><ci id=\"S3.T1.7.7.7.3.3.3.1.m1.1.1.3.cmml\" xref=\"S3.T1.7.7.7.3.3.3.1.m1.1.1.3\">&#119903;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.7.7.7.3.3.3.1.m1.1c\">B_{r}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.7.7.7.3.3.3.1.m1.1d\">italic_B start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>=3</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.10.10.10\"><span class=\"ltx_text\" id=\"S3.T1.10.10.10.3\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.10.10.10.3.3\">\n<span class=\"ltx_tr\" id=\"S3.T1.8.8.8.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.8.8.8.1.1.1.1\"><math alttext=\"N_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.8.8.8.1.1.1.1.m1.1\"><semantics id=\"S3.T1.8.8.8.1.1.1.1.m1.1a\"><msub id=\"S3.T1.8.8.8.1.1.1.1.m1.1.1\" xref=\"S3.T1.8.8.8.1.1.1.1.m1.1.1.cmml\"><mi id=\"S3.T1.8.8.8.1.1.1.1.m1.1.1.2\" xref=\"S3.T1.8.8.8.1.1.1.1.m1.1.1.2.cmml\">N</mi><mn id=\"S3.T1.8.8.8.1.1.1.1.m1.1.1.3\" xref=\"S3.T1.8.8.8.1.1.1.1.m1.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.8.8.8.1.1.1.1.m1.1b\"><apply id=\"S3.T1.8.8.8.1.1.1.1.m1.1.1.cmml\" xref=\"S3.T1.8.8.8.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.8.8.8.1.1.1.1.m1.1.1.1.cmml\" xref=\"S3.T1.8.8.8.1.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.8.8.8.1.1.1.1.m1.1.1.2.cmml\" xref=\"S3.T1.8.8.8.1.1.1.1.m1.1.1.2\">&#119873;</ci><cn id=\"S3.T1.8.8.8.1.1.1.1.m1.1.1.3.cmml\" type=\"integer\" xref=\"S3.T1.8.8.8.1.1.1.1.m1.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.8.8.8.1.1.1.1.m1.1c\">N_{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.8.8.8.1.1.1.1.m1.1d\">italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>=4, C=96</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.9.9.9.2.2.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.9.9.9.2.2.2.1\">r=8, M=3, <math alttext=\"D_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.9.9.9.2.2.2.1.m1.1\"><semantics id=\"S3.T1.9.9.9.2.2.2.1.m1.1a\"><msub id=\"S3.T1.9.9.9.2.2.2.1.m1.1.1\" xref=\"S3.T1.9.9.9.2.2.2.1.m1.1.1.cmml\"><mi id=\"S3.T1.9.9.9.2.2.2.1.m1.1.1.2\" xref=\"S3.T1.9.9.9.2.2.2.1.m1.1.1.2.cmml\">D</mi><mi id=\"S3.T1.9.9.9.2.2.2.1.m1.1.1.3\" xref=\"S3.T1.9.9.9.2.2.2.1.m1.1.1.3.cmml\">r</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.9.9.9.2.2.2.1.m1.1b\"><apply id=\"S3.T1.9.9.9.2.2.2.1.m1.1.1.cmml\" xref=\"S3.T1.9.9.9.2.2.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.9.9.9.2.2.2.1.m1.1.1.1.cmml\" xref=\"S3.T1.9.9.9.2.2.2.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.9.9.9.2.2.2.1.m1.1.1.2.cmml\" xref=\"S3.T1.9.9.9.2.2.2.1.m1.1.1.2\">&#119863;</ci><ci id=\"S3.T1.9.9.9.2.2.2.1.m1.1.1.3.cmml\" xref=\"S3.T1.9.9.9.2.2.2.1.m1.1.1.3\">&#119903;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.9.9.9.2.2.2.1.m1.1c\">D_{r}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.9.9.9.2.2.2.1.m1.1d\">italic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>=3</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.10.10.10.3.3.3\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.10.10.10.3.3.3.1\">G=1, K=9, <math alttext=\"B_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.10.10.10.3.3.3.1.m1.1\"><semantics id=\"S3.T1.10.10.10.3.3.3.1.m1.1a\"><msub id=\"S3.T1.10.10.10.3.3.3.1.m1.1.1\" xref=\"S3.T1.10.10.10.3.3.3.1.m1.1.1.cmml\"><mi id=\"S3.T1.10.10.10.3.3.3.1.m1.1.1.2\" xref=\"S3.T1.10.10.10.3.3.3.1.m1.1.1.2.cmml\">B</mi><mi id=\"S3.T1.10.10.10.3.3.3.1.m1.1.1.3\" xref=\"S3.T1.10.10.10.3.3.3.1.m1.1.1.3.cmml\">r</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.10.10.10.3.3.3.1.m1.1b\"><apply id=\"S3.T1.10.10.10.3.3.3.1.m1.1.1.cmml\" xref=\"S3.T1.10.10.10.3.3.3.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.10.10.10.3.3.3.1.m1.1.1.1.cmml\" xref=\"S3.T1.10.10.10.3.3.3.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.10.10.10.3.3.3.1.m1.1.1.2.cmml\" xref=\"S3.T1.10.10.10.3.3.3.1.m1.1.1.2\">&#119861;</ci><ci id=\"S3.T1.10.10.10.3.3.3.1.m1.1.1.3.cmml\" xref=\"S3.T1.10.10.10.3.3.3.1.m1.1.1.3\">&#119903;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.10.10.10.3.3.3.1.m1.1c\">B_{r}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.10.10.10.3.3.3.1.m1.1d\">italic_B start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>=3</span></span>\n</span></span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.20.20\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.11.11.1\"><span class=\"ltx_text\" id=\"S3.T1.11.11.1.1\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.11.11.1.1.1\">\n<span class=\"ltx_tr\" id=\"S3.T1.11.11.1.1.1.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.11.11.1.1.1.2.1\">Stage 2</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.11.11.1.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.11.11.1.1.1.1.1\">28<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.11.11.1.1.1.1.1.m1.1\"><semantics id=\"S3.T1.11.11.1.1.1.1.1.m1.1a\"><mo id=\"S3.T1.11.11.1.1.1.1.1.m1.1.1\" xref=\"S3.T1.11.11.1.1.1.1.1.m1.1.1.cmml\">&#215;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.11.11.1.1.1.1.1.m1.1b\"><times id=\"S3.T1.11.11.1.1.1.1.1.m1.1.1.cmml\" xref=\"S3.T1.11.11.1.1.1.1.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.11.11.1.1.1.1.1.m1.1c\">\\times</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.11.11.1.1.1.1.1.m1.1d\">&#215;</annotation></semantics></math>28</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.14.14.4\"><span class=\"ltx_text\" id=\"S3.T1.14.14.4.3\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.14.14.4.3.3\">\n<span class=\"ltx_tr\" id=\"S3.T1.12.12.2.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.12.12.2.1.1.1.1\"><math alttext=\"N_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.12.12.2.1.1.1.1.m1.1\"><semantics id=\"S3.T1.12.12.2.1.1.1.1.m1.1a\"><msub id=\"S3.T1.12.12.2.1.1.1.1.m1.1.1\" xref=\"S3.T1.12.12.2.1.1.1.1.m1.1.1.cmml\"><mi id=\"S3.T1.12.12.2.1.1.1.1.m1.1.1.2\" xref=\"S3.T1.12.12.2.1.1.1.1.m1.1.1.2.cmml\">N</mi><mn id=\"S3.T1.12.12.2.1.1.1.1.m1.1.1.3\" xref=\"S3.T1.12.12.2.1.1.1.1.m1.1.1.3.cmml\">2</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.12.12.2.1.1.1.1.m1.1b\"><apply id=\"S3.T1.12.12.2.1.1.1.1.m1.1.1.cmml\" xref=\"S3.T1.12.12.2.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.12.12.2.1.1.1.1.m1.1.1.1.cmml\" xref=\"S3.T1.12.12.2.1.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.12.12.2.1.1.1.1.m1.1.1.2.cmml\" xref=\"S3.T1.12.12.2.1.1.1.1.m1.1.1.2\">&#119873;</ci><cn id=\"S3.T1.12.12.2.1.1.1.1.m1.1.1.3.cmml\" type=\"integer\" xref=\"S3.T1.12.12.2.1.1.1.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.12.12.2.1.1.1.1.m1.1c\">N_{2}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.12.12.2.1.1.1.1.m1.1d\">italic_N start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>=2, C=128</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.13.13.3.2.2.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.13.13.3.2.2.2.1\">r=4, M=4, <math alttext=\"D_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.13.13.3.2.2.2.1.m1.1\"><semantics id=\"S3.T1.13.13.3.2.2.2.1.m1.1a\"><msub id=\"S3.T1.13.13.3.2.2.2.1.m1.1.1\" xref=\"S3.T1.13.13.3.2.2.2.1.m1.1.1.cmml\"><mi id=\"S3.T1.13.13.3.2.2.2.1.m1.1.1.2\" xref=\"S3.T1.13.13.3.2.2.2.1.m1.1.1.2.cmml\">D</mi><mi id=\"S3.T1.13.13.3.2.2.2.1.m1.1.1.3\" xref=\"S3.T1.13.13.3.2.2.2.1.m1.1.1.3.cmml\">r</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.13.13.3.2.2.2.1.m1.1b\"><apply id=\"S3.T1.13.13.3.2.2.2.1.m1.1.1.cmml\" xref=\"S3.T1.13.13.3.2.2.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.13.13.3.2.2.2.1.m1.1.1.1.cmml\" xref=\"S3.T1.13.13.3.2.2.2.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.13.13.3.2.2.2.1.m1.1.1.2.cmml\" xref=\"S3.T1.13.13.3.2.2.2.1.m1.1.1.2\">&#119863;</ci><ci id=\"S3.T1.13.13.3.2.2.2.1.m1.1.1.3.cmml\" xref=\"S3.T1.13.13.3.2.2.2.1.m1.1.1.3\">&#119903;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.13.13.3.2.2.2.1.m1.1c\">D_{r}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.13.13.3.2.2.2.1.m1.1d\">italic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>=3</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.14.14.4.3.3.3\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.14.14.4.3.3.3.1\">G=2, K=7, <math alttext=\"B_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.14.14.4.3.3.3.1.m1.1\"><semantics id=\"S3.T1.14.14.4.3.3.3.1.m1.1a\"><msub id=\"S3.T1.14.14.4.3.3.3.1.m1.1.1\" xref=\"S3.T1.14.14.4.3.3.3.1.m1.1.1.cmml\"><mi id=\"S3.T1.14.14.4.3.3.3.1.m1.1.1.2\" xref=\"S3.T1.14.14.4.3.3.3.1.m1.1.1.2.cmml\">B</mi><mi id=\"S3.T1.14.14.4.3.3.3.1.m1.1.1.3\" xref=\"S3.T1.14.14.4.3.3.3.1.m1.1.1.3.cmml\">r</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.14.14.4.3.3.3.1.m1.1b\"><apply id=\"S3.T1.14.14.4.3.3.3.1.m1.1.1.cmml\" xref=\"S3.T1.14.14.4.3.3.3.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.14.14.4.3.3.3.1.m1.1.1.1.cmml\" xref=\"S3.T1.14.14.4.3.3.3.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.14.14.4.3.3.3.1.m1.1.1.2.cmml\" xref=\"S3.T1.14.14.4.3.3.3.1.m1.1.1.2\">&#119861;</ci><ci id=\"S3.T1.14.14.4.3.3.3.1.m1.1.1.3.cmml\" xref=\"S3.T1.14.14.4.3.3.3.1.m1.1.1.3\">&#119903;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.14.14.4.3.3.3.1.m1.1c\">B_{r}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.14.14.4.3.3.3.1.m1.1d\">italic_B start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>=3</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.17.17.7\"><span class=\"ltx_text\" id=\"S3.T1.17.17.7.3\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.17.17.7.3.3\">\n<span class=\"ltx_tr\" id=\"S3.T1.15.15.5.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.15.15.5.1.1.1.1\"><math alttext=\"N_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.15.15.5.1.1.1.1.m1.1\"><semantics id=\"S3.T1.15.15.5.1.1.1.1.m1.1a\"><msub id=\"S3.T1.15.15.5.1.1.1.1.m1.1.1\" xref=\"S3.T1.15.15.5.1.1.1.1.m1.1.1.cmml\"><mi id=\"S3.T1.15.15.5.1.1.1.1.m1.1.1.2\" xref=\"S3.T1.15.15.5.1.1.1.1.m1.1.1.2.cmml\">N</mi><mn id=\"S3.T1.15.15.5.1.1.1.1.m1.1.1.3\" xref=\"S3.T1.15.15.5.1.1.1.1.m1.1.1.3.cmml\">2</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.15.15.5.1.1.1.1.m1.1b\"><apply id=\"S3.T1.15.15.5.1.1.1.1.m1.1.1.cmml\" xref=\"S3.T1.15.15.5.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.15.15.5.1.1.1.1.m1.1.1.1.cmml\" xref=\"S3.T1.15.15.5.1.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.15.15.5.1.1.1.1.m1.1.1.2.cmml\" xref=\"S3.T1.15.15.5.1.1.1.1.m1.1.1.2\">&#119873;</ci><cn id=\"S3.T1.15.15.5.1.1.1.1.m1.1.1.3.cmml\" type=\"integer\" xref=\"S3.T1.15.15.5.1.1.1.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.15.15.5.1.1.1.1.m1.1c\">N_{2}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.15.15.5.1.1.1.1.m1.1d\">italic_N start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>=4, C=128</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.16.16.6.2.2.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.16.16.6.2.2.2.1\">r=4, M=4, <math alttext=\"D_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.16.16.6.2.2.2.1.m1.1\"><semantics id=\"S3.T1.16.16.6.2.2.2.1.m1.1a\"><msub id=\"S3.T1.16.16.6.2.2.2.1.m1.1.1\" xref=\"S3.T1.16.16.6.2.2.2.1.m1.1.1.cmml\"><mi id=\"S3.T1.16.16.6.2.2.2.1.m1.1.1.2\" xref=\"S3.T1.16.16.6.2.2.2.1.m1.1.1.2.cmml\">D</mi><mi id=\"S3.T1.16.16.6.2.2.2.1.m1.1.1.3\" xref=\"S3.T1.16.16.6.2.2.2.1.m1.1.1.3.cmml\">r</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.16.16.6.2.2.2.1.m1.1b\"><apply id=\"S3.T1.16.16.6.2.2.2.1.m1.1.1.cmml\" xref=\"S3.T1.16.16.6.2.2.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.16.16.6.2.2.2.1.m1.1.1.1.cmml\" xref=\"S3.T1.16.16.6.2.2.2.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.16.16.6.2.2.2.1.m1.1.1.2.cmml\" xref=\"S3.T1.16.16.6.2.2.2.1.m1.1.1.2\">&#119863;</ci><ci id=\"S3.T1.16.16.6.2.2.2.1.m1.1.1.3.cmml\" xref=\"S3.T1.16.16.6.2.2.2.1.m1.1.1.3\">&#119903;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.16.16.6.2.2.2.1.m1.1c\">D_{r}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.16.16.6.2.2.2.1.m1.1d\">italic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>=3</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.17.17.7.3.3.3\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.17.17.7.3.3.3.1\">G=2, K=7, <math alttext=\"B_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.17.17.7.3.3.3.1.m1.1\"><semantics id=\"S3.T1.17.17.7.3.3.3.1.m1.1a\"><msub id=\"S3.T1.17.17.7.3.3.3.1.m1.1.1\" xref=\"S3.T1.17.17.7.3.3.3.1.m1.1.1.cmml\"><mi id=\"S3.T1.17.17.7.3.3.3.1.m1.1.1.2\" xref=\"S3.T1.17.17.7.3.3.3.1.m1.1.1.2.cmml\">B</mi><mi id=\"S3.T1.17.17.7.3.3.3.1.m1.1.1.3\" xref=\"S3.T1.17.17.7.3.3.3.1.m1.1.1.3.cmml\">r</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.17.17.7.3.3.3.1.m1.1b\"><apply id=\"S3.T1.17.17.7.3.3.3.1.m1.1.1.cmml\" xref=\"S3.T1.17.17.7.3.3.3.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.17.17.7.3.3.3.1.m1.1.1.1.cmml\" xref=\"S3.T1.17.17.7.3.3.3.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.17.17.7.3.3.3.1.m1.1.1.2.cmml\" xref=\"S3.T1.17.17.7.3.3.3.1.m1.1.1.2\">&#119861;</ci><ci id=\"S3.T1.17.17.7.3.3.3.1.m1.1.1.3.cmml\" xref=\"S3.T1.17.17.7.3.3.3.1.m1.1.1.3\">&#119903;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.17.17.7.3.3.3.1.m1.1c\">B_{r}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.17.17.7.3.3.3.1.m1.1d\">italic_B start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>=3</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.20.20.10\"><span class=\"ltx_text\" id=\"S3.T1.20.20.10.3\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.20.20.10.3.3\">\n<span class=\"ltx_tr\" id=\"S3.T1.18.18.8.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.18.18.8.1.1.1.1\"><math alttext=\"N_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.18.18.8.1.1.1.1.m1.1\"><semantics id=\"S3.T1.18.18.8.1.1.1.1.m1.1a\"><msub id=\"S3.T1.18.18.8.1.1.1.1.m1.1.1\" xref=\"S3.T1.18.18.8.1.1.1.1.m1.1.1.cmml\"><mi id=\"S3.T1.18.18.8.1.1.1.1.m1.1.1.2\" xref=\"S3.T1.18.18.8.1.1.1.1.m1.1.1.2.cmml\">N</mi><mn id=\"S3.T1.18.18.8.1.1.1.1.m1.1.1.3\" xref=\"S3.T1.18.18.8.1.1.1.1.m1.1.1.3.cmml\">2</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.18.18.8.1.1.1.1.m1.1b\"><apply id=\"S3.T1.18.18.8.1.1.1.1.m1.1.1.cmml\" xref=\"S3.T1.18.18.8.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.18.18.8.1.1.1.1.m1.1.1.1.cmml\" xref=\"S3.T1.18.18.8.1.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.18.18.8.1.1.1.1.m1.1.1.2.cmml\" xref=\"S3.T1.18.18.8.1.1.1.1.m1.1.1.2\">&#119873;</ci><cn id=\"S3.T1.18.18.8.1.1.1.1.m1.1.1.3.cmml\" type=\"integer\" xref=\"S3.T1.18.18.8.1.1.1.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.18.18.8.1.1.1.1.m1.1c\">N_{2}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.18.18.8.1.1.1.1.m1.1d\">italic_N start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>=4, C=192</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.19.19.9.2.2.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.19.19.9.2.2.2.1\">r=4, M=6, <math alttext=\"D_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.19.19.9.2.2.2.1.m1.1\"><semantics id=\"S3.T1.19.19.9.2.2.2.1.m1.1a\"><msub id=\"S3.T1.19.19.9.2.2.2.1.m1.1.1\" xref=\"S3.T1.19.19.9.2.2.2.1.m1.1.1.cmml\"><mi id=\"S3.T1.19.19.9.2.2.2.1.m1.1.1.2\" xref=\"S3.T1.19.19.9.2.2.2.1.m1.1.1.2.cmml\">D</mi><mi id=\"S3.T1.19.19.9.2.2.2.1.m1.1.1.3\" xref=\"S3.T1.19.19.9.2.2.2.1.m1.1.1.3.cmml\">r</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.19.19.9.2.2.2.1.m1.1b\"><apply id=\"S3.T1.19.19.9.2.2.2.1.m1.1.1.cmml\" xref=\"S3.T1.19.19.9.2.2.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.19.19.9.2.2.2.1.m1.1.1.1.cmml\" xref=\"S3.T1.19.19.9.2.2.2.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.19.19.9.2.2.2.1.m1.1.1.2.cmml\" xref=\"S3.T1.19.19.9.2.2.2.1.m1.1.1.2\">&#119863;</ci><ci id=\"S3.T1.19.19.9.2.2.2.1.m1.1.1.3.cmml\" xref=\"S3.T1.19.19.9.2.2.2.1.m1.1.1.3\">&#119903;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.19.19.9.2.2.2.1.m1.1c\">D_{r}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.19.19.9.2.2.2.1.m1.1d\">italic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>=3</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.20.20.10.3.3.3\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.20.20.10.3.3.3.1\">G=2, K=7, <math alttext=\"B_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.20.20.10.3.3.3.1.m1.1\"><semantics id=\"S3.T1.20.20.10.3.3.3.1.m1.1a\"><msub id=\"S3.T1.20.20.10.3.3.3.1.m1.1.1\" xref=\"S3.T1.20.20.10.3.3.3.1.m1.1.1.cmml\"><mi id=\"S3.T1.20.20.10.3.3.3.1.m1.1.1.2\" xref=\"S3.T1.20.20.10.3.3.3.1.m1.1.1.2.cmml\">B</mi><mi id=\"S3.T1.20.20.10.3.3.3.1.m1.1.1.3\" xref=\"S3.T1.20.20.10.3.3.3.1.m1.1.1.3.cmml\">r</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.20.20.10.3.3.3.1.m1.1b\"><apply id=\"S3.T1.20.20.10.3.3.3.1.m1.1.1.cmml\" xref=\"S3.T1.20.20.10.3.3.3.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.20.20.10.3.3.3.1.m1.1.1.1.cmml\" xref=\"S3.T1.20.20.10.3.3.3.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.20.20.10.3.3.3.1.m1.1.1.2.cmml\" xref=\"S3.T1.20.20.10.3.3.3.1.m1.1.1.2\">&#119861;</ci><ci id=\"S3.T1.20.20.10.3.3.3.1.m1.1.1.3.cmml\" xref=\"S3.T1.20.20.10.3.3.3.1.m1.1.1.3\">&#119903;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.20.20.10.3.3.3.1.m1.1c\">B_{r}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.20.20.10.3.3.3.1.m1.1d\">italic_B start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>=3</span></span>\n</span></span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.30.30\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.21.21.1\"><span class=\"ltx_text\" id=\"S3.T1.21.21.1.1\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.21.21.1.1.1\">\n<span class=\"ltx_tr\" id=\"S3.T1.21.21.1.1.1.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.21.21.1.1.1.2.1\">Stage 3</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.21.21.1.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.21.21.1.1.1.1.1\">14<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.21.21.1.1.1.1.1.m1.1\"><semantics id=\"S3.T1.21.21.1.1.1.1.1.m1.1a\"><mo id=\"S3.T1.21.21.1.1.1.1.1.m1.1.1\" xref=\"S3.T1.21.21.1.1.1.1.1.m1.1.1.cmml\">&#215;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.21.21.1.1.1.1.1.m1.1b\"><times id=\"S3.T1.21.21.1.1.1.1.1.m1.1.1.cmml\" xref=\"S3.T1.21.21.1.1.1.1.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.21.21.1.1.1.1.1.m1.1c\">\\times</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.21.21.1.1.1.1.1.m1.1d\">&#215;</annotation></semantics></math>14</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.24.24.4\"><span class=\"ltx_text\" id=\"S3.T1.24.24.4.3\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.24.24.4.3.3\">\n<span class=\"ltx_tr\" id=\"S3.T1.22.22.2.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.22.22.2.1.1.1.1\"><math alttext=\"N_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.22.22.2.1.1.1.1.m1.1\"><semantics id=\"S3.T1.22.22.2.1.1.1.1.m1.1a\"><msub id=\"S3.T1.22.22.2.1.1.1.1.m1.1.1\" xref=\"S3.T1.22.22.2.1.1.1.1.m1.1.1.cmml\"><mi id=\"S3.T1.22.22.2.1.1.1.1.m1.1.1.2\" xref=\"S3.T1.22.22.2.1.1.1.1.m1.1.1.2.cmml\">N</mi><mn id=\"S3.T1.22.22.2.1.1.1.1.m1.1.1.3\" xref=\"S3.T1.22.22.2.1.1.1.1.m1.1.1.3.cmml\">3</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.22.22.2.1.1.1.1.m1.1b\"><apply id=\"S3.T1.22.22.2.1.1.1.1.m1.1.1.cmml\" xref=\"S3.T1.22.22.2.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.22.22.2.1.1.1.1.m1.1.1.1.cmml\" xref=\"S3.T1.22.22.2.1.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.22.22.2.1.1.1.1.m1.1.1.2.cmml\" xref=\"S3.T1.22.22.2.1.1.1.1.m1.1.1.2\">&#119873;</ci><cn id=\"S3.T1.22.22.2.1.1.1.1.m1.1.1.3.cmml\" type=\"integer\" xref=\"S3.T1.22.22.2.1.1.1.1.m1.1.1.3\">3</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.22.22.2.1.1.1.1.m1.1c\">N_{3}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.22.22.2.1.1.1.1.m1.1d\">italic_N start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT</annotation></semantics></math>=8, C=256</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.23.23.3.2.2.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.23.23.3.2.2.2.1\">r=2, M=8, <math alttext=\"D_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.23.23.3.2.2.2.1.m1.1\"><semantics id=\"S3.T1.23.23.3.2.2.2.1.m1.1a\"><msub id=\"S3.T1.23.23.3.2.2.2.1.m1.1.1\" xref=\"S3.T1.23.23.3.2.2.2.1.m1.1.1.cmml\"><mi id=\"S3.T1.23.23.3.2.2.2.1.m1.1.1.2\" xref=\"S3.T1.23.23.3.2.2.2.1.m1.1.1.2.cmml\">D</mi><mi id=\"S3.T1.23.23.3.2.2.2.1.m1.1.1.3\" xref=\"S3.T1.23.23.3.2.2.2.1.m1.1.1.3.cmml\">r</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.23.23.3.2.2.2.1.m1.1b\"><apply id=\"S3.T1.23.23.3.2.2.2.1.m1.1.1.cmml\" xref=\"S3.T1.23.23.3.2.2.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.23.23.3.2.2.2.1.m1.1.1.1.cmml\" xref=\"S3.T1.23.23.3.2.2.2.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.23.23.3.2.2.2.1.m1.1.1.2.cmml\" xref=\"S3.T1.23.23.3.2.2.2.1.m1.1.1.2\">&#119863;</ci><ci id=\"S3.T1.23.23.3.2.2.2.1.m1.1.1.3.cmml\" xref=\"S3.T1.23.23.3.2.2.2.1.m1.1.1.3\">&#119903;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.23.23.3.2.2.2.1.m1.1c\">D_{r}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.23.23.3.2.2.2.1.m1.1d\">italic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>=3</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.24.24.4.3.3.3\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.24.24.4.3.3.3.1\">G=4, K=5, <math alttext=\"B_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.24.24.4.3.3.3.1.m1.1\"><semantics id=\"S3.T1.24.24.4.3.3.3.1.m1.1a\"><msub id=\"S3.T1.24.24.4.3.3.3.1.m1.1.1\" xref=\"S3.T1.24.24.4.3.3.3.1.m1.1.1.cmml\"><mi id=\"S3.T1.24.24.4.3.3.3.1.m1.1.1.2\" xref=\"S3.T1.24.24.4.3.3.3.1.m1.1.1.2.cmml\">B</mi><mi id=\"S3.T1.24.24.4.3.3.3.1.m1.1.1.3\" xref=\"S3.T1.24.24.4.3.3.3.1.m1.1.1.3.cmml\">r</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.24.24.4.3.3.3.1.m1.1b\"><apply id=\"S3.T1.24.24.4.3.3.3.1.m1.1.1.cmml\" xref=\"S3.T1.24.24.4.3.3.3.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.24.24.4.3.3.3.1.m1.1.1.1.cmml\" xref=\"S3.T1.24.24.4.3.3.3.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.24.24.4.3.3.3.1.m1.1.1.2.cmml\" xref=\"S3.T1.24.24.4.3.3.3.1.m1.1.1.2\">&#119861;</ci><ci id=\"S3.T1.24.24.4.3.3.3.1.m1.1.1.3.cmml\" xref=\"S3.T1.24.24.4.3.3.3.1.m1.1.1.3\">&#119903;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.24.24.4.3.3.3.1.m1.1c\">B_{r}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.24.24.4.3.3.3.1.m1.1d\">italic_B start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>=3</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.27.27.7\"><span class=\"ltx_text\" id=\"S3.T1.27.27.7.3\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.27.27.7.3.3\">\n<span class=\"ltx_tr\" id=\"S3.T1.25.25.5.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.25.25.5.1.1.1.1\"><math alttext=\"N_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.25.25.5.1.1.1.1.m1.1\"><semantics id=\"S3.T1.25.25.5.1.1.1.1.m1.1a\"><msub id=\"S3.T1.25.25.5.1.1.1.1.m1.1.1\" xref=\"S3.T1.25.25.5.1.1.1.1.m1.1.1.cmml\"><mi id=\"S3.T1.25.25.5.1.1.1.1.m1.1.1.2\" xref=\"S3.T1.25.25.5.1.1.1.1.m1.1.1.2.cmml\">N</mi><mn id=\"S3.T1.25.25.5.1.1.1.1.m1.1.1.3\" xref=\"S3.T1.25.25.5.1.1.1.1.m1.1.1.3.cmml\">3</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.25.25.5.1.1.1.1.m1.1b\"><apply id=\"S3.T1.25.25.5.1.1.1.1.m1.1.1.cmml\" xref=\"S3.T1.25.25.5.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.25.25.5.1.1.1.1.m1.1.1.1.cmml\" xref=\"S3.T1.25.25.5.1.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.25.25.5.1.1.1.1.m1.1.1.2.cmml\" xref=\"S3.T1.25.25.5.1.1.1.1.m1.1.1.2\">&#119873;</ci><cn id=\"S3.T1.25.25.5.1.1.1.1.m1.1.1.3.cmml\" type=\"integer\" xref=\"S3.T1.25.25.5.1.1.1.1.m1.1.1.3\">3</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.25.25.5.1.1.1.1.m1.1c\">N_{3}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.25.25.5.1.1.1.1.m1.1d\">italic_N start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT</annotation></semantics></math>=18, C=256</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.26.26.6.2.2.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.26.26.6.2.2.2.1\">r=2, M=8, <math alttext=\"D_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.26.26.6.2.2.2.1.m1.1\"><semantics id=\"S3.T1.26.26.6.2.2.2.1.m1.1a\"><msub id=\"S3.T1.26.26.6.2.2.2.1.m1.1.1\" xref=\"S3.T1.26.26.6.2.2.2.1.m1.1.1.cmml\"><mi id=\"S3.T1.26.26.6.2.2.2.1.m1.1.1.2\" xref=\"S3.T1.26.26.6.2.2.2.1.m1.1.1.2.cmml\">D</mi><mi id=\"S3.T1.26.26.6.2.2.2.1.m1.1.1.3\" xref=\"S3.T1.26.26.6.2.2.2.1.m1.1.1.3.cmml\">r</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.26.26.6.2.2.2.1.m1.1b\"><apply id=\"S3.T1.26.26.6.2.2.2.1.m1.1.1.cmml\" xref=\"S3.T1.26.26.6.2.2.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.26.26.6.2.2.2.1.m1.1.1.1.cmml\" xref=\"S3.T1.26.26.6.2.2.2.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.26.26.6.2.2.2.1.m1.1.1.2.cmml\" xref=\"S3.T1.26.26.6.2.2.2.1.m1.1.1.2\">&#119863;</ci><ci id=\"S3.T1.26.26.6.2.2.2.1.m1.1.1.3.cmml\" xref=\"S3.T1.26.26.6.2.2.2.1.m1.1.1.3\">&#119903;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.26.26.6.2.2.2.1.m1.1c\">D_{r}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.26.26.6.2.2.2.1.m1.1d\">italic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>=3</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.27.27.7.3.3.3\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.27.27.7.3.3.3.1\">G=4, K=5, <math alttext=\"B_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.27.27.7.3.3.3.1.m1.1\"><semantics id=\"S3.T1.27.27.7.3.3.3.1.m1.1a\"><msub id=\"S3.T1.27.27.7.3.3.3.1.m1.1.1\" xref=\"S3.T1.27.27.7.3.3.3.1.m1.1.1.cmml\"><mi id=\"S3.T1.27.27.7.3.3.3.1.m1.1.1.2\" xref=\"S3.T1.27.27.7.3.3.3.1.m1.1.1.2.cmml\">B</mi><mi id=\"S3.T1.27.27.7.3.3.3.1.m1.1.1.3\" xref=\"S3.T1.27.27.7.3.3.3.1.m1.1.1.3.cmml\">r</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.27.27.7.3.3.3.1.m1.1b\"><apply id=\"S3.T1.27.27.7.3.3.3.1.m1.1.1.cmml\" xref=\"S3.T1.27.27.7.3.3.3.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.27.27.7.3.3.3.1.m1.1.1.1.cmml\" xref=\"S3.T1.27.27.7.3.3.3.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.27.27.7.3.3.3.1.m1.1.1.2.cmml\" xref=\"S3.T1.27.27.7.3.3.3.1.m1.1.1.2\">&#119861;</ci><ci id=\"S3.T1.27.27.7.3.3.3.1.m1.1.1.3.cmml\" xref=\"S3.T1.27.27.7.3.3.3.1.m1.1.1.3\">&#119903;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.27.27.7.3.3.3.1.m1.1c\">B_{r}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.27.27.7.3.3.3.1.m1.1d\">italic_B start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>=3</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.30.30.10\"><span class=\"ltx_text\" id=\"S3.T1.30.30.10.3\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.30.30.10.3.3\">\n<span class=\"ltx_tr\" id=\"S3.T1.28.28.8.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.28.28.8.1.1.1.1\"><math alttext=\"N_{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.28.28.8.1.1.1.1.m1.1\"><semantics id=\"S3.T1.28.28.8.1.1.1.1.m1.1a\"><msub id=\"S3.T1.28.28.8.1.1.1.1.m1.1.1\" xref=\"S3.T1.28.28.8.1.1.1.1.m1.1.1.cmml\"><mi id=\"S3.T1.28.28.8.1.1.1.1.m1.1.1.2\" xref=\"S3.T1.28.28.8.1.1.1.1.m1.1.1.2.cmml\">N</mi><mn id=\"S3.T1.28.28.8.1.1.1.1.m1.1.1.3\" xref=\"S3.T1.28.28.8.1.1.1.1.m1.1.1.3.cmml\">3</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.28.28.8.1.1.1.1.m1.1b\"><apply id=\"S3.T1.28.28.8.1.1.1.1.m1.1.1.cmml\" xref=\"S3.T1.28.28.8.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.28.28.8.1.1.1.1.m1.1.1.1.cmml\" xref=\"S3.T1.28.28.8.1.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.28.28.8.1.1.1.1.m1.1.1.2.cmml\" xref=\"S3.T1.28.28.8.1.1.1.1.m1.1.1.2\">&#119873;</ci><cn id=\"S3.T1.28.28.8.1.1.1.1.m1.1.1.3.cmml\" type=\"integer\" xref=\"S3.T1.28.28.8.1.1.1.1.m1.1.1.3\">3</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.28.28.8.1.1.1.1.m1.1c\">N_{3}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.28.28.8.1.1.1.1.m1.1d\">italic_N start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT</annotation></semantics></math>=18, C=384</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.29.29.9.2.2.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.29.29.9.2.2.2.1\">r=2, M=12, <math alttext=\"D_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.29.29.9.2.2.2.1.m1.1\"><semantics id=\"S3.T1.29.29.9.2.2.2.1.m1.1a\"><msub id=\"S3.T1.29.29.9.2.2.2.1.m1.1.1\" xref=\"S3.T1.29.29.9.2.2.2.1.m1.1.1.cmml\"><mi id=\"S3.T1.29.29.9.2.2.2.1.m1.1.1.2\" xref=\"S3.T1.29.29.9.2.2.2.1.m1.1.1.2.cmml\">D</mi><mi id=\"S3.T1.29.29.9.2.2.2.1.m1.1.1.3\" xref=\"S3.T1.29.29.9.2.2.2.1.m1.1.1.3.cmml\">r</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.29.29.9.2.2.2.1.m1.1b\"><apply id=\"S3.T1.29.29.9.2.2.2.1.m1.1.1.cmml\" xref=\"S3.T1.29.29.9.2.2.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.29.29.9.2.2.2.1.m1.1.1.1.cmml\" xref=\"S3.T1.29.29.9.2.2.2.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.29.29.9.2.2.2.1.m1.1.1.2.cmml\" xref=\"S3.T1.29.29.9.2.2.2.1.m1.1.1.2\">&#119863;</ci><ci id=\"S3.T1.29.29.9.2.2.2.1.m1.1.1.3.cmml\" xref=\"S3.T1.29.29.9.2.2.2.1.m1.1.1.3\">&#119903;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.29.29.9.2.2.2.1.m1.1c\">D_{r}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.29.29.9.2.2.2.1.m1.1d\">italic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>=3</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.30.30.10.3.3.3\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.30.30.10.3.3.3.1\">G=4, K=5, <math alttext=\"B_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.30.30.10.3.3.3.1.m1.1\"><semantics id=\"S3.T1.30.30.10.3.3.3.1.m1.1a\"><msub id=\"S3.T1.30.30.10.3.3.3.1.m1.1.1\" xref=\"S3.T1.30.30.10.3.3.3.1.m1.1.1.cmml\"><mi id=\"S3.T1.30.30.10.3.3.3.1.m1.1.1.2\" xref=\"S3.T1.30.30.10.3.3.3.1.m1.1.1.2.cmml\">B</mi><mi id=\"S3.T1.30.30.10.3.3.3.1.m1.1.1.3\" xref=\"S3.T1.30.30.10.3.3.3.1.m1.1.1.3.cmml\">r</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.30.30.10.3.3.3.1.m1.1b\"><apply id=\"S3.T1.30.30.10.3.3.3.1.m1.1.1.cmml\" xref=\"S3.T1.30.30.10.3.3.3.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.30.30.10.3.3.3.1.m1.1.1.1.cmml\" xref=\"S3.T1.30.30.10.3.3.3.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.30.30.10.3.3.3.1.m1.1.1.2.cmml\" xref=\"S3.T1.30.30.10.3.3.3.1.m1.1.1.2\">&#119861;</ci><ci id=\"S3.T1.30.30.10.3.3.3.1.m1.1.1.3.cmml\" xref=\"S3.T1.30.30.10.3.3.3.1.m1.1.1.3\">&#119903;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.30.30.10.3.3.3.1.m1.1c\">B_{r}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.30.30.10.3.3.3.1.m1.1d\">italic_B start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>=3</span></span>\n</span></span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.40.40\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S3.T1.31.31.1\"><span class=\"ltx_text\" id=\"S3.T1.31.31.1.1\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.31.31.1.1.1\">\n<span class=\"ltx_tr\" id=\"S3.T1.31.31.1.1.1.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.31.31.1.1.1.2.1\">Stage 4</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.31.31.1.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.31.31.1.1.1.1.1\">7<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.31.31.1.1.1.1.1.m1.1\"><semantics id=\"S3.T1.31.31.1.1.1.1.1.m1.1a\"><mo id=\"S3.T1.31.31.1.1.1.1.1.m1.1.1\" xref=\"S3.T1.31.31.1.1.1.1.1.m1.1.1.cmml\">&#215;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.31.31.1.1.1.1.1.m1.1b\"><times id=\"S3.T1.31.31.1.1.1.1.1.m1.1.1.cmml\" xref=\"S3.T1.31.31.1.1.1.1.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.31.31.1.1.1.1.1.m1.1c\">\\times</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.31.31.1.1.1.1.1.m1.1d\">&#215;</annotation></semantics></math>7</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S3.T1.34.34.4\"><span class=\"ltx_text\" id=\"S3.T1.34.34.4.3\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.34.34.4.3.3\">\n<span class=\"ltx_tr\" id=\"S3.T1.32.32.2.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.32.32.2.1.1.1.1\"><math alttext=\"N_{4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.32.32.2.1.1.1.1.m1.1\"><semantics id=\"S3.T1.32.32.2.1.1.1.1.m1.1a\"><msub id=\"S3.T1.32.32.2.1.1.1.1.m1.1.1\" xref=\"S3.T1.32.32.2.1.1.1.1.m1.1.1.cmml\"><mi id=\"S3.T1.32.32.2.1.1.1.1.m1.1.1.2\" xref=\"S3.T1.32.32.2.1.1.1.1.m1.1.1.2.cmml\">N</mi><mn id=\"S3.T1.32.32.2.1.1.1.1.m1.1.1.3\" xref=\"S3.T1.32.32.2.1.1.1.1.m1.1.1.3.cmml\">4</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.32.32.2.1.1.1.1.m1.1b\"><apply id=\"S3.T1.32.32.2.1.1.1.1.m1.1.1.cmml\" xref=\"S3.T1.32.32.2.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.32.32.2.1.1.1.1.m1.1.1.1.cmml\" xref=\"S3.T1.32.32.2.1.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.32.32.2.1.1.1.1.m1.1.1.2.cmml\" xref=\"S3.T1.32.32.2.1.1.1.1.m1.1.1.2\">&#119873;</ci><cn id=\"S3.T1.32.32.2.1.1.1.1.m1.1.1.3.cmml\" type=\"integer\" xref=\"S3.T1.32.32.2.1.1.1.1.m1.1.1.3\">4</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.32.32.2.1.1.1.1.m1.1c\">N_{4}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.32.32.2.1.1.1.1.m1.1d\">italic_N start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT</annotation></semantics></math>=2, C=512</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.33.33.3.2.2.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.33.33.3.2.2.2.1\">r=1, M=16, <math alttext=\"D_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.33.33.3.2.2.2.1.m1.1\"><semantics id=\"S3.T1.33.33.3.2.2.2.1.m1.1a\"><msub id=\"S3.T1.33.33.3.2.2.2.1.m1.1.1\" xref=\"S3.T1.33.33.3.2.2.2.1.m1.1.1.cmml\"><mi id=\"S3.T1.33.33.3.2.2.2.1.m1.1.1.2\" xref=\"S3.T1.33.33.3.2.2.2.1.m1.1.1.2.cmml\">D</mi><mi id=\"S3.T1.33.33.3.2.2.2.1.m1.1.1.3\" xref=\"S3.T1.33.33.3.2.2.2.1.m1.1.1.3.cmml\">r</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.33.33.3.2.2.2.1.m1.1b\"><apply id=\"S3.T1.33.33.3.2.2.2.1.m1.1.1.cmml\" xref=\"S3.T1.33.33.3.2.2.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.33.33.3.2.2.2.1.m1.1.1.1.cmml\" xref=\"S3.T1.33.33.3.2.2.2.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.33.33.3.2.2.2.1.m1.1.1.2.cmml\" xref=\"S3.T1.33.33.3.2.2.2.1.m1.1.1.2\">&#119863;</ci><ci id=\"S3.T1.33.33.3.2.2.2.1.m1.1.1.3.cmml\" xref=\"S3.T1.33.33.3.2.2.2.1.m1.1.1.3\">&#119903;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.33.33.3.2.2.2.1.m1.1c\">D_{r}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.33.33.3.2.2.2.1.m1.1d\">italic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>=3</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.34.34.4.3.3.3\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.34.34.4.3.3.3.1\">G=8, K=3, <math alttext=\"B_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.34.34.4.3.3.3.1.m1.1\"><semantics id=\"S3.T1.34.34.4.3.3.3.1.m1.1a\"><msub id=\"S3.T1.34.34.4.3.3.3.1.m1.1.1\" xref=\"S3.T1.34.34.4.3.3.3.1.m1.1.1.cmml\"><mi id=\"S3.T1.34.34.4.3.3.3.1.m1.1.1.2\" xref=\"S3.T1.34.34.4.3.3.3.1.m1.1.1.2.cmml\">B</mi><mi id=\"S3.T1.34.34.4.3.3.3.1.m1.1.1.3\" xref=\"S3.T1.34.34.4.3.3.3.1.m1.1.1.3.cmml\">r</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.34.34.4.3.3.3.1.m1.1b\"><apply id=\"S3.T1.34.34.4.3.3.3.1.m1.1.1.cmml\" xref=\"S3.T1.34.34.4.3.3.3.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.34.34.4.3.3.3.1.m1.1.1.1.cmml\" xref=\"S3.T1.34.34.4.3.3.3.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.34.34.4.3.3.3.1.m1.1.1.2.cmml\" xref=\"S3.T1.34.34.4.3.3.3.1.m1.1.1.2\">&#119861;</ci><ci id=\"S3.T1.34.34.4.3.3.3.1.m1.1.1.3.cmml\" xref=\"S3.T1.34.34.4.3.3.3.1.m1.1.1.3\">&#119903;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.34.34.4.3.3.3.1.m1.1c\">B_{r}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.34.34.4.3.3.3.1.m1.1d\">italic_B start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>=3</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S3.T1.37.37.7\"><span class=\"ltx_text\" id=\"S3.T1.37.37.7.3\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.37.37.7.3.3\">\n<span class=\"ltx_tr\" id=\"S3.T1.35.35.5.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.35.35.5.1.1.1.1\"><math alttext=\"N_{4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.35.35.5.1.1.1.1.m1.1\"><semantics id=\"S3.T1.35.35.5.1.1.1.1.m1.1a\"><msub id=\"S3.T1.35.35.5.1.1.1.1.m1.1.1\" xref=\"S3.T1.35.35.5.1.1.1.1.m1.1.1.cmml\"><mi id=\"S3.T1.35.35.5.1.1.1.1.m1.1.1.2\" xref=\"S3.T1.35.35.5.1.1.1.1.m1.1.1.2.cmml\">N</mi><mn id=\"S3.T1.35.35.5.1.1.1.1.m1.1.1.3\" xref=\"S3.T1.35.35.5.1.1.1.1.m1.1.1.3.cmml\">4</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.35.35.5.1.1.1.1.m1.1b\"><apply id=\"S3.T1.35.35.5.1.1.1.1.m1.1.1.cmml\" xref=\"S3.T1.35.35.5.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.35.35.5.1.1.1.1.m1.1.1.1.cmml\" xref=\"S3.T1.35.35.5.1.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.35.35.5.1.1.1.1.m1.1.1.2.cmml\" xref=\"S3.T1.35.35.5.1.1.1.1.m1.1.1.2\">&#119873;</ci><cn id=\"S3.T1.35.35.5.1.1.1.1.m1.1.1.3.cmml\" type=\"integer\" xref=\"S3.T1.35.35.5.1.1.1.1.m1.1.1.3\">4</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.35.35.5.1.1.1.1.m1.1c\">N_{4}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.35.35.5.1.1.1.1.m1.1d\">italic_N start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT</annotation></semantics></math>=6, C=512</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.36.36.6.2.2.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.36.36.6.2.2.2.1\">r=1, M=16, <math alttext=\"D_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.36.36.6.2.2.2.1.m1.1\"><semantics id=\"S3.T1.36.36.6.2.2.2.1.m1.1a\"><msub id=\"S3.T1.36.36.6.2.2.2.1.m1.1.1\" xref=\"S3.T1.36.36.6.2.2.2.1.m1.1.1.cmml\"><mi id=\"S3.T1.36.36.6.2.2.2.1.m1.1.1.2\" xref=\"S3.T1.36.36.6.2.2.2.1.m1.1.1.2.cmml\">D</mi><mi id=\"S3.T1.36.36.6.2.2.2.1.m1.1.1.3\" xref=\"S3.T1.36.36.6.2.2.2.1.m1.1.1.3.cmml\">r</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.36.36.6.2.2.2.1.m1.1b\"><apply id=\"S3.T1.36.36.6.2.2.2.1.m1.1.1.cmml\" xref=\"S3.T1.36.36.6.2.2.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.36.36.6.2.2.2.1.m1.1.1.1.cmml\" xref=\"S3.T1.36.36.6.2.2.2.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.36.36.6.2.2.2.1.m1.1.1.2.cmml\" xref=\"S3.T1.36.36.6.2.2.2.1.m1.1.1.2\">&#119863;</ci><ci id=\"S3.T1.36.36.6.2.2.2.1.m1.1.1.3.cmml\" xref=\"S3.T1.36.36.6.2.2.2.1.m1.1.1.3\">&#119903;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.36.36.6.2.2.2.1.m1.1c\">D_{r}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.36.36.6.2.2.2.1.m1.1d\">italic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>=1</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.37.37.7.3.3.3\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.37.37.7.3.3.3.1\">G=8, K=3, <math alttext=\"B_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.37.37.7.3.3.3.1.m1.1\"><semantics id=\"S3.T1.37.37.7.3.3.3.1.m1.1a\"><msub id=\"S3.T1.37.37.7.3.3.3.1.m1.1.1\" xref=\"S3.T1.37.37.7.3.3.3.1.m1.1.1.cmml\"><mi id=\"S3.T1.37.37.7.3.3.3.1.m1.1.1.2\" xref=\"S3.T1.37.37.7.3.3.3.1.m1.1.1.2.cmml\">B</mi><mi id=\"S3.T1.37.37.7.3.3.3.1.m1.1.1.3\" xref=\"S3.T1.37.37.7.3.3.3.1.m1.1.1.3.cmml\">r</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.37.37.7.3.3.3.1.m1.1b\"><apply id=\"S3.T1.37.37.7.3.3.3.1.m1.1.1.cmml\" xref=\"S3.T1.37.37.7.3.3.3.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.37.37.7.3.3.3.1.m1.1.1.1.cmml\" xref=\"S3.T1.37.37.7.3.3.3.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.37.37.7.3.3.3.1.m1.1.1.2.cmml\" xref=\"S3.T1.37.37.7.3.3.3.1.m1.1.1.2\">&#119861;</ci><ci id=\"S3.T1.37.37.7.3.3.3.1.m1.1.1.3.cmml\" xref=\"S3.T1.37.37.7.3.3.3.1.m1.1.1.3\">&#119903;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.37.37.7.3.3.3.1.m1.1c\">B_{r}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.37.37.7.3.3.3.1.m1.1d\">italic_B start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>=2</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S3.T1.40.40.10\"><span class=\"ltx_text\" id=\"S3.T1.40.40.10.3\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.40.40.10.3.3\">\n<span class=\"ltx_tr\" id=\"S3.T1.38.38.8.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.38.38.8.1.1.1.1\"><math alttext=\"N_{4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.38.38.8.1.1.1.1.m1.1\"><semantics id=\"S3.T1.38.38.8.1.1.1.1.m1.1a\"><msub id=\"S3.T1.38.38.8.1.1.1.1.m1.1.1\" xref=\"S3.T1.38.38.8.1.1.1.1.m1.1.1.cmml\"><mi id=\"S3.T1.38.38.8.1.1.1.1.m1.1.1.2\" xref=\"S3.T1.38.38.8.1.1.1.1.m1.1.1.2.cmml\">N</mi><mn id=\"S3.T1.38.38.8.1.1.1.1.m1.1.1.3\" xref=\"S3.T1.38.38.8.1.1.1.1.m1.1.1.3.cmml\">4</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.38.38.8.1.1.1.1.m1.1b\"><apply id=\"S3.T1.38.38.8.1.1.1.1.m1.1.1.cmml\" xref=\"S3.T1.38.38.8.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.38.38.8.1.1.1.1.m1.1.1.1.cmml\" xref=\"S3.T1.38.38.8.1.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.38.38.8.1.1.1.1.m1.1.1.2.cmml\" xref=\"S3.T1.38.38.8.1.1.1.1.m1.1.1.2\">&#119873;</ci><cn id=\"S3.T1.38.38.8.1.1.1.1.m1.1.1.3.cmml\" type=\"integer\" xref=\"S3.T1.38.38.8.1.1.1.1.m1.1.1.3\">4</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.38.38.8.1.1.1.1.m1.1c\">N_{4}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.38.38.8.1.1.1.1.m1.1d\">italic_N start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT</annotation></semantics></math>=4, C=768</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.39.39.9.2.2.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.39.39.9.2.2.2.1\">r=1, M=24, <math alttext=\"D_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.39.39.9.2.2.2.1.m1.1\"><semantics id=\"S3.T1.39.39.9.2.2.2.1.m1.1a\"><msub id=\"S3.T1.39.39.9.2.2.2.1.m1.1.1\" xref=\"S3.T1.39.39.9.2.2.2.1.m1.1.1.cmml\"><mi id=\"S3.T1.39.39.9.2.2.2.1.m1.1.1.2\" xref=\"S3.T1.39.39.9.2.2.2.1.m1.1.1.2.cmml\">D</mi><mi id=\"S3.T1.39.39.9.2.2.2.1.m1.1.1.3\" xref=\"S3.T1.39.39.9.2.2.2.1.m1.1.1.3.cmml\">r</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.39.39.9.2.2.2.1.m1.1b\"><apply id=\"S3.T1.39.39.9.2.2.2.1.m1.1.1.cmml\" xref=\"S3.T1.39.39.9.2.2.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.39.39.9.2.2.2.1.m1.1.1.1.cmml\" xref=\"S3.T1.39.39.9.2.2.2.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.39.39.9.2.2.2.1.m1.1.1.2.cmml\" xref=\"S3.T1.39.39.9.2.2.2.1.m1.1.1.2\">&#119863;</ci><ci id=\"S3.T1.39.39.9.2.2.2.1.m1.1.1.3.cmml\" xref=\"S3.T1.39.39.9.2.2.2.1.m1.1.1.3\">&#119903;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.39.39.9.2.2.2.1.m1.1c\">D_{r}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.39.39.9.2.2.2.1.m1.1d\">italic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>=3</span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.40.40.10.3.3.3\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.40.40.10.3.3.3.1\">G=8, K=3, <math alttext=\"B_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.40.40.10.3.3.3.1.m1.1\"><semantics id=\"S3.T1.40.40.10.3.3.3.1.m1.1a\"><msub id=\"S3.T1.40.40.10.3.3.3.1.m1.1.1\" xref=\"S3.T1.40.40.10.3.3.3.1.m1.1.1.cmml\"><mi id=\"S3.T1.40.40.10.3.3.3.1.m1.1.1.2\" xref=\"S3.T1.40.40.10.3.3.3.1.m1.1.1.2.cmml\">B</mi><mi id=\"S3.T1.40.40.10.3.3.3.1.m1.1.1.3\" xref=\"S3.T1.40.40.10.3.3.3.1.m1.1.1.3.cmml\">r</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.40.40.10.3.3.3.1.m1.1b\"><apply id=\"S3.T1.40.40.10.3.3.3.1.m1.1.1.cmml\" xref=\"S3.T1.40.40.10.3.3.3.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S3.T1.40.40.10.3.3.3.1.m1.1.1.1.cmml\" xref=\"S3.T1.40.40.10.3.3.3.1.m1.1.1\">subscript</csymbol><ci id=\"S3.T1.40.40.10.3.3.3.1.m1.1.1.2.cmml\" xref=\"S3.T1.40.40.10.3.3.3.1.m1.1.1.2\">&#119861;</ci><ci id=\"S3.T1.40.40.10.3.3.3.1.m1.1.1.3.cmml\" xref=\"S3.T1.40.40.10.3.3.3.1.m1.1.1.3\">&#119903;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.40.40.10.3.3.3.1.m1.1c\">B_{r}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.40.40.10.3.3.3.1.m1.1d\">italic_B start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT</annotation></semantics></math>=3</span></span>\n</span></span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 1:  DeBiFormer model architecture specifications.  N i subscript 𝑁 𝑖 N_{i} italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT : Number of blocks at stage  i 𝑖 i italic_i .  C 𝐶 C italic_C : Base channels in each block.  r 𝑟 r italic_r : Downsample ratio of deformed points.  M 𝑀 M italic_M : Number of attention heads in DBRMHA.  G 𝐺 G italic_G : Number of offset groups in DBRMHA.  D r subscript 𝐷 𝑟 D_{r} italic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT : Deformable level MLP expansion ratio.  B r subscript 𝐵 𝑟 B_{r} italic_B start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT : Bi level MLP expansion ratio.  K 𝐾 K italic_K : Kernel size of offset module.",
        "footnotes": [],
        "references": [
            "Leveraging DBRA as a fundamental building block, we introduce a novel vision transformer called DeBiFormer. As depicted in Figure3, we adhere to the recent state-of-the-art Vision Transformers [14, 29, 56, 47], using a four-stage pyramid structure.\nIn stage i𝑖iitalic_i, we utilize an overlapped patch embedding in the first stage and a patch merging module [26, 34] in the second to fourth stages. This is done to decrease the input spatial resolution while increasing the number of channels. Subsequently, Nisubscript𝑁𝑖N_{i}italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT consecutive DeBiFormer blocks are used to transform the features. Within each DeBiFormer block, we adhere to recent methodologies [26, 40, 56] by using a 3×3333\\times 33 × 3 depthwise convolution at the outset. This is done to implicitly encode relative position information. Following that, we sequentially use a DBRA module with a 2-ConvFFN module with an expansion ratio e𝑒eitalic_e for cross-location relation modeling and per-location embedding, respectively.\nDeBiFormer is instantiated in three distinct model sizes, achieved by scaling the network width and depth as outlined in Table 1.\nEach attention head comprises 32 channels, and we use a bi-level ConvFFN and deformable-level ConvFFN with an MLP expansion ratio of e=3𝑒3e=3italic_e = 3. For the BRA, we use t⁢o⁢p⁢k=1,4,16,S2𝑡𝑜𝑝𝑘1416superscript𝑆2topk=1,4,16,S^{2}italic_t italic_o italic_p italic_k = 1 , 4 , 16 , italic_S start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, and for the DBRA, we use t⁢o⁢p⁢k=4,8,16,S2𝑡𝑜𝑝𝑘4816superscript𝑆2topk=4,8,16,S^{2}italic_t italic_o italic_p italic_k = 4 , 8 , 16 , italic_S start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT for the four stages. Moreover, we set the region partition factor S𝑆Sitalic_S to specific values: S=7𝑆7S=7italic_S = 7 for classification, S=8𝑆8S=8italic_S = 8 for semantic segmentation, and S=20𝑆20S=20italic_S = 20 for object detection tasks."
        ]
    },
    "S4.T2.3": {
        "table": "<table class=\"ltx_tabular ltx_figure_panel ltx_minipage ltx_align_top\" id=\"S4.T2.3\" style=\"width:203.8pt;\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.3.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T2.3.1.1.1\"><span class=\"ltx_text\" id=\"S4.T2.3.1.1.1.1\" style=\"font-size:90%;\">Model</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.3.1.1.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.3.1.1.2.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.3.1.1.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.3.1.1.2.1.1.1\"><span class=\"ltx_text\" id=\"S4.T2.3.1.1.2.1.1.1.1\" style=\"font-size:90%;\">FLOPs</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.1.1.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.3.1.1.2.1.2.1\"><span class=\"ltx_text\" id=\"S4.T2.3.1.1.2.1.2.1.1\" style=\"font-size:90%;\">(G)</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.3.1.1.3\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.3.1.1.3.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.3.1.1.3.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.3.1.1.3.1.1.1\"><span class=\"ltx_text\" id=\"S4.T2.3.1.1.3.1.1.1.1\" style=\"font-size:90%;\">Params</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.1.1.3.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.3.1.1.3.1.2.1\"><span class=\"ltx_text\" id=\"S4.T2.3.1.1.3.1.2.1.1\" style=\"font-size:90%;\">(M)</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.3.1.1.4\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.3.1.1.4.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.3.1.1.4.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.3.1.1.4.1.1.1\"><span class=\"ltx_text\" id=\"S4.T2.3.1.1.4.1.1.1.1\" style=\"font-size:90%;\">Top-1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.1.1.4.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.3.1.1.4.1.2.1\"><span class=\"ltx_text\" id=\"S4.T2.3.1.1.4.1.2.1.1\" style=\"font-size:90%;\">(%)</span></td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T2.3.2.2.1\">\n<span class=\"ltx_text\" id=\"S4.T2.3.2.2.1.1\" style=\"font-size:90%;\">ResNet-18 </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.3.2.2.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib20\" title=\"\">20</a><span class=\"ltx_text\" id=\"S4.T2.3.2.2.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.3.2.2.2\"><span class=\"ltx_text\" id=\"S4.T2.3.2.2.2.1\" style=\"font-size:90%;\">1.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.3.2.2.3\"><span class=\"ltx_text\" id=\"S4.T2.3.2.2.3.1\" style=\"font-size:90%;\">11.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.3.2.2.4\"><span class=\"ltx_text\" id=\"S4.T2.3.2.2.4.1\" style=\"font-size:90%;\">69.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.3.3.3.1\">\n<span class=\"ltx_text\" id=\"S4.T2.3.3.3.1.1\" style=\"font-size:90%;\">PVTv2-b1 </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.3.3.3.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib42\" title=\"\">42</a><span class=\"ltx_text\" id=\"S4.T2.3.3.3.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.3.3.2\"><span class=\"ltx_text\" id=\"S4.T2.3.3.3.2.1\" style=\"font-size:90%;\">2.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.3.3.3\"><span class=\"ltx_text\" id=\"S4.T2.3.3.3.3.1\" style=\"font-size:90%;\">13.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.3.3.4\"><span class=\"ltx_text\" id=\"S4.T2.3.3.3.4.1\" style=\"font-size:90%;\">78.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.3.4.4.1\">\n<span class=\"ltx_text\" id=\"S4.T2.3.4.4.1.1\" style=\"font-size:90%;\">Shunted-T </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.3.4.4.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib34\" title=\"\">34</a><span class=\"ltx_text\" id=\"S4.T2.3.4.4.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.4.4.2\"><span class=\"ltx_text\" id=\"S4.T2.3.4.4.2.1\" style=\"font-size:90%;\">2.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.4.4.3\"><span class=\"ltx_text\" id=\"S4.T2.3.4.4.3.1\" style=\"font-size:90%;\">11.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.4.4.4\"><span class=\"ltx_text\" id=\"S4.T2.3.4.4.4.1\" style=\"font-size:90%;\">79.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.3.5.5.1\">\n<span class=\"ltx_text\" id=\"S4.T2.3.5.5.1.1\" style=\"font-size:90%;\">QuadTree-B-b1 </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.3.5.5.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib37\" title=\"\">37</a><span class=\"ltx_text\" id=\"S4.T2.3.5.5.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.5.5.2\"><span class=\"ltx_text\" id=\"S4.T2.3.5.5.2.1\" style=\"font-size:90%;\">2.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.5.5.3\"><span class=\"ltx_text\" id=\"S4.T2.3.5.5.3.1\" style=\"font-size:90%;\">13.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.5.5.4\"><span class=\"ltx_text\" id=\"S4.T2.3.5.5.4.1\" style=\"font-size:90%;\">80.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.3.6.6.1\">\n<span class=\"ltx_text\" id=\"S4.T2.3.6.6.1.1\" style=\"font-size:90%;\">BiFormer-T </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.3.6.6.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib56\" title=\"\">56</a><span class=\"ltx_text\" id=\"S4.T2.3.6.6.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.6.6.2\"><span class=\"ltx_text\" id=\"S4.T2.3.6.6.2.1\" style=\"font-size:90%;\">2.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.6.6.3\"><span class=\"ltx_text\" id=\"S4.T2.3.6.6.3.1\" style=\"font-size:90%;\">13.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.6.6.4\"><span class=\"ltx_text\" id=\"S4.T2.3.6.6.4.1\" style=\"font-size:90%;\">81.4</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.7.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.3.7.7.1\">\n<span class=\"ltx_text\" id=\"S4.T2.3.7.7.1.1\" style=\"font-size:90%;\">Conv2Former-N </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.3.7.7.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib22\" title=\"\">22</a><span class=\"ltx_text\" id=\"S4.T2.3.7.7.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.7.7.2\"><span class=\"ltx_text\" id=\"S4.T2.3.7.7.2.1\" style=\"font-size:90%;\">2.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.7.7.3\"><span class=\"ltx_text\" id=\"S4.T2.3.7.7.3.1\" style=\"font-size:90%;\">15.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.7.7.4\"><span class=\"ltx_text\" id=\"S4.T2.3.7.7.4.1\" style=\"font-size:90%;\">81.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.8.8\" style=\"background-color:#96FFFB;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.3.8.8.1\"><span class=\"ltx_text\" id=\"S4.T2.3.8.8.1.1\" style=\"font-size:90%;background-color:#96FFFB;\">DeBiFormer-T</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.8.8.2\"><span class=\"ltx_text\" id=\"S4.T2.3.8.8.2.1\" style=\"font-size:90%;background-color:#96FFFB;\">2.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.8.8.3\"><span class=\"ltx_text\" id=\"S4.T2.3.8.8.3.1\" style=\"font-size:90%;background-color:#96FFFB;\">21.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.8.8.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.3.8.8.4.1\" style=\"font-size:90%;background-color:#96FFFB;\">81.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.9.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T2.3.9.9.1\">\n<span class=\"ltx_text\" id=\"S4.T2.3.9.9.1.1\" style=\"font-size:90%;\">PVTv2-B3 </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.3.9.9.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib44\" title=\"\">44</a><span class=\"ltx_text\" id=\"S4.T2.3.9.9.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.3.9.9.2\"><span class=\"ltx_text\" id=\"S4.T2.3.9.9.2.1\" style=\"font-size:90%;\">6.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.3.9.9.3\"><span class=\"ltx_text\" id=\"S4.T2.3.9.9.3.1\" style=\"font-size:90%;\">45</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.3.9.9.4\"><span class=\"ltx_text\" id=\"S4.T2.3.9.9.4.1\" style=\"font-size:90%;\">81.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.10.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.3.10.10.1\">\n<span class=\"ltx_text\" id=\"S4.T2.3.10.10.1.1\" style=\"font-size:90%;\">Swin-T </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.3.10.10.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib29\" title=\"\">29</a><span class=\"ltx_text\" id=\"S4.T2.3.10.10.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.10.10.2\"><span class=\"ltx_text\" id=\"S4.T2.3.10.10.2.1\" style=\"font-size:90%;\">4.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.10.10.3\"><span class=\"ltx_text\" id=\"S4.T2.3.10.10.3.1\" style=\"font-size:90%;\">29</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.10.10.4\"><span class=\"ltx_text\" id=\"S4.T2.3.10.10.4.1\" style=\"font-size:90%;\">81.3</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.11.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.3.11.11.1\">\n<span class=\"ltx_text\" id=\"S4.T2.3.11.11.1.1\" style=\"font-size:90%;\">CSWin-T </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.3.11.11.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib14\" title=\"\">14</a><span class=\"ltx_text\" id=\"S4.T2.3.11.11.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.11.11.2\"><span class=\"ltx_text\" id=\"S4.T2.3.11.11.2.1\" style=\"font-size:90%;\">4.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.11.11.3\"><span class=\"ltx_text\" id=\"S4.T2.3.11.11.3.1\" style=\"font-size:90%;\">23</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.11.11.4\"><span class=\"ltx_text\" id=\"S4.T2.3.11.11.4.1\" style=\"font-size:90%;\">82.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.12.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.3.12.12.1\">\n<span class=\"ltx_text\" id=\"S4.T2.3.12.12.1.1\" style=\"font-size:90%;\">DAT-T </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.3.12.12.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib47\" title=\"\">47</a><span class=\"ltx_text\" id=\"S4.T2.3.12.12.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.12.12.2\"><span class=\"ltx_text\" id=\"S4.T2.3.12.12.2.1\" style=\"font-size:90%;\">4.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.12.12.3\"><span class=\"ltx_text\" id=\"S4.T2.3.12.12.3.1\" style=\"font-size:90%;\">29</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.12.12.4\"><span class=\"ltx_text\" id=\"S4.T2.3.12.12.4.1\" style=\"font-size:90%;\">82.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.13.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.3.13.13.1\">\n<span class=\"ltx_text\" id=\"S4.T2.3.13.13.1.1\" style=\"font-size:90%;\">CrossFormer-S </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.3.13.13.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib45\" title=\"\">45</a><span class=\"ltx_text\" id=\"S4.T2.3.13.13.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.13.13.2\"><span class=\"ltx_text\" id=\"S4.T2.3.13.13.2.1\" style=\"font-size:90%;\">5.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.13.13.3\"><span class=\"ltx_text\" id=\"S4.T2.3.13.13.3.1\" style=\"font-size:90%;\">31</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.13.13.4\"><span class=\"ltx_text\" id=\"S4.T2.3.13.13.4.1\" style=\"font-size:90%;\">82.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.14.14\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.3.14.14.1\">\n<span class=\"ltx_text\" id=\"S4.T2.3.14.14.1.1\" style=\"font-size:90%;\">RegionViT-S+ </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.3.14.14.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib2\" title=\"\">2</a><span class=\"ltx_text\" id=\"S4.T2.3.14.14.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.14.14.2\"><span class=\"ltx_text\" id=\"S4.T2.3.14.14.2.1\" style=\"font-size:90%;\">5.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.14.14.3\"><span class=\"ltx_text\" id=\"S4.T2.3.14.14.3.1\" style=\"font-size:90%;\">31</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.14.14.4\"><span class=\"ltx_text\" id=\"S4.T2.3.14.14.4.1\" style=\"font-size:90%;\">83.3</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.15.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.3.15.15.1\">\n<span class=\"ltx_text\" id=\"S4.T2.3.15.15.1.1\" style=\"font-size:90%;\">QuadTree-B-b3 </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.3.15.15.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib37\" title=\"\">37</a><span class=\"ltx_text\" id=\"S4.T2.3.15.15.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.15.15.2\"><span class=\"ltx_text\" id=\"S4.T2.3.15.15.2.1\" style=\"font-size:90%;\">7.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.15.15.3\"><span class=\"ltx_text\" id=\"S4.T2.3.15.15.3.1\" style=\"font-size:90%;\">46</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.15.15.4\"><span class=\"ltx_text\" id=\"S4.T2.3.15.15.4.1\" style=\"font-size:90%;\">83.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.16.16\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.3.16.16.1\">\n<span class=\"ltx_text\" id=\"S4.T2.3.16.16.1.1\" style=\"font-size:90%;\">MaxViT-T </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.3.16.16.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib40\" title=\"\">40</a><span class=\"ltx_text\" id=\"S4.T2.3.16.16.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.16.16.2\"><span class=\"ltx_text\" id=\"S4.T2.3.16.16.2.1\" style=\"font-size:90%;\">5.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.16.16.3\"><span class=\"ltx_text\" id=\"S4.T2.3.16.16.3.1\" style=\"font-size:90%;\">31</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.16.16.4\"><span class=\"ltx_text\" id=\"S4.T2.3.16.16.4.1\" style=\"font-size:90%;\">83.6</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.17.17\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.3.17.17.1\">\n<span class=\"ltx_text\" id=\"S4.T2.3.17.17.1.1\" style=\"font-size:90%;\">InternImage-T </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.3.17.17.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib43\" title=\"\">43</a><span class=\"ltx_text\" id=\"S4.T2.3.17.17.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.17.17.2\"><span class=\"ltx_text\" id=\"S4.T2.3.17.17.2.1\" style=\"font-size:90%;\">5.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.17.17.3\"><span class=\"ltx_text\" id=\"S4.T2.3.17.17.3.1\" style=\"font-size:90%;\">30</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.17.17.4\"><span class=\"ltx_text\" id=\"S4.T2.3.17.17.4.1\" style=\"font-size:90%;\">83.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.18.18\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.3.18.18.1\">\n<span class=\"ltx_text\" id=\"S4.T2.3.18.18.1.1\" style=\"font-size:90%;\">MixFormer-B4 </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.3.18.18.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib4\" title=\"\">4</a><span class=\"ltx_text\" id=\"S4.T2.3.18.18.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.18.18.2\"><span class=\"ltx_text\" id=\"S4.T2.3.18.18.2.1\" style=\"font-size:90%;\">3.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.18.18.3\"><span class=\"ltx_text\" id=\"S4.T2.3.18.18.3.1\" style=\"font-size:90%;\">35</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.18.18.4\"><span class=\"ltx_text\" id=\"S4.T2.3.18.18.4.1\" style=\"font-size:90%;\">83.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.19.19\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.3.19.19.1\">\n<span class=\"ltx_text\" id=\"S4.T2.3.19.19.1.1\" style=\"font-size:90%;\">BiFormer-S </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.3.19.19.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib56\" title=\"\">56</a><span class=\"ltx_text\" id=\"S4.T2.3.19.19.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.19.19.2\"><span class=\"ltx_text\" id=\"S4.T2.3.19.19.2.1\" style=\"font-size:90%;\">4.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.19.19.3\"><span class=\"ltx_text\" id=\"S4.T2.3.19.19.3.1\" style=\"font-size:90%;\">26</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.19.19.4\"><span class=\"ltx_text\" id=\"S4.T2.3.19.19.4.1\" style=\"font-size:90%;\">83.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.20.20\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.3.20.20.1\">\n<span class=\"ltx_text\" id=\"S4.T2.3.20.20.1.1\" style=\"font-size:90%;\">UniRepLKNet-T </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.3.20.20.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" id=\"S4.T2.3.20.20.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.20.20.2\"><span class=\"ltx_text\" id=\"S4.T2.3.20.20.2.1\" style=\"font-size:90%;\">4.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.20.20.3\"><span class=\"ltx_text\" id=\"S4.T2.3.20.20.3.1\" style=\"font-size:90%;\">31</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.3.20.20.4\"><span class=\"ltx_text\" id=\"S4.T2.3.20.20.4.1\" style=\"font-size:90%;\">83.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.21.21\" style=\"background-color:#96FFFB;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\" id=\"S4.T2.3.21.21.1\"><span class=\"ltx_text\" id=\"S4.T2.3.21.21.1.1\" style=\"font-size:90%;background-color:#96FFFB;\">DeBiFormer-S</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T2.3.21.21.2\"><span class=\"ltx_text\" id=\"S4.T2.3.21.21.2.1\" style=\"font-size:90%;background-color:#96FFFB;\">5.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T2.3.21.21.3\"><span class=\"ltx_text\" id=\"S4.T2.3.21.21.3.1\" style=\"font-size:90%;background-color:#96FFFB;\">44</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T2.3.21.21.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.3.21.21.4.1\" style=\"font-size:90%;background-color:#96FFFB;\">83.9</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 2:  Evaluating and comparing different backbones on ImageNet-1K on images with resolution of  224 × 224 224 224 224\\times 224 224 × 224 .",
        "footnotes": [
            "[20] \nHe, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition (2015)\n\n",
            "[42] \nWang, S., Li, B.Z., Khabsa, M., Fang, H., Ma, H.: Linformer: Self-attention with linear complexity (2020)\n\n",
            "[34] \nRen, S., Zhou, D., He, S., Feng, J., Wang, X.: Shunted self-attention via multi-scale token aggregation (2021)\n\n",
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n",
            "[22] \nHou, Q., Lu, C.Z., Cheng, M.M., Feng, J.: Conv2former: A simple transformer-style convnet for visual recognition. arXiv preprint arXiv:2211.11943 (2022)\n\n",
            "[44] \nWang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 568–578 (2021)\n\n",
            "[29] \nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021)\n\n",
            "[14] \nDong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin transformer: A general vision transformer backbone with cross-shaped windows. In: Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR). pp. 12124–12134 (2022)\n\n",
            "[47] \nXia, Z., Pan, X., Song, S., Li, L.E., Huang, G.: Vision transformer with deformable attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4794–4803 (June 2022)\n\n",
            "[45] \nWang, W., Yao, L., Chen, L., Lin, B., Cai, D., He, X., Liu, W.: Crossformer: A versatile vision transformer hinging on cross-scale attention. In: Proceedings of the International Conference on Learning Representations (ICLR) (2022),  https://openreview.net/forum?id=_PHymLIxuI \n",
            "[2] \nChen, C.F., Panda, R., Fan, Q.: Regionvit: Regional-to-local attention for vision transformers. arXiv preprint arXiv:2106.02689 (2021)\n\n",
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[40] \nTu, Z., Talebi, H., Zhang, H., Yang, F., Milanfar, P., Bovik, A., Li, Y.: Maxvit: Multi-axis vision transformer. ECCV (2022)\n\n",
            "[43] \nWang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu, L., Li, H., et al.: Internimage: Exploring large-scale vision foundation models with deformable convolutions. arXiv preprint arXiv:2211.05778 (2022)\n\n",
            "[4] \nChen, Q., Wu, Q., Wang, J., Hu, Q., Hu, T., Ding, E., Cheng, J., Wang, J.: Mixformer: Mixing features across windows and dimensions (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n",
            "[12] \nDing, X., Zhang, Y., Ge, Y., Zhao, S., Song, L., Yue, X., Shan, Y.: Unireplknet: A universal perception large-kernel convnet for audio, video, point cloud, time-series and image recognition. arXiv preprint arXiv:2311.15599 (2023)\n\n",
            "[30] \nLiu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for the 2020s. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 11976–11986 (June 2022)\n\n",
            "[27] \nLiu, S., Chen, T., Chen, X., Chen, X., Xiao, Q., Wu, B., Pechenizkiy, M., Mocanu, D., Wang, Z.: More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity. arXiv preprint arXiv:2207.03620 (2022)\n\n",
            "[7] \nChu, X., Tian, Z., Wang, Y., Zhang, B., Ren, H., Wei, X., Xia, H., Shen, C.: Twins: Revisiting the design of spatial attention in vision transformers. In: NeurIPS 2021 (2021),  https://openreview.net/forum?id=5kTlVBkzSRx \n",
            "[44] \nWang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 568–578 (2021)\n\n",
            "[29] \nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021)\n\n",
            "[49] \nYang, J., Li, C., Zhang, P., Dai, X., Xiao, B., Yuan, L., Gao, J.: Focal attention for long-range interactions in vision transformers. In: Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., Vaughan, J.W. (eds.) Advances in Neural Information Processing Systems. vol. 34, pp. 30008–30022. Curran Associates, Inc. (2021)\n\n",
            "[14] \nDong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin transformer: A general vision transformer backbone with cross-shaped windows. In: Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR). pp. 12124–12134 (2022)\n\n",
            "[34] \nRen, S., Zhou, D., He, S., Feng, J., Wang, X.: Shunted self-attention via multi-scale token aggregation (2021)\n\n",
            "[26] \nLi, K., Wang, Y., Gao, P., Song, G., Liu, Y., Li, H., Qiao, Y.: Uniformer: Unified transformer for efficient spatiotemporal representation learning (2022)\n\n",
            "[51] \nYang, R., Ma, H., Wu, J., Tang, Y., Xiao, X., Zheng, M., Li, X.: Scalablevit: Rethinking the context-oriented generalization of vision transformer (2022)\n\n",
            "[33] \nPan, X., Ye, T., Xia, Z., Song, S., Huang, G.: Slide-transformer: Hierarchical vision transformer with local self-attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 2082–2091 (June 2023)\n\n",
            "[47] \nXia, Z., Pan, X., Song, S., Li, L.E., Huang, G.: Vision transformer with deformable attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4794–4803 (June 2022)\n\n",
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[45] \nWang, W., Yao, L., Chen, L., Lin, B., Cai, D., He, X., Liu, W.: Crossformer: A versatile vision transformer hinging on cross-scale attention. In: Proceedings of the International Conference on Learning Representations (ICLR) (2022),  https://openreview.net/forum?id=_PHymLIxuI \n",
            "[2] \nChen, C.F., Panda, R., Fan, Q.: Regionvit: Regional-to-local attention for vision transformers. arXiv preprint arXiv:2106.02689 (2021)\n\n",
            "[43] \nWang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu, L., Li, H., et al.: Internimage: Exploring large-scale vision foundation models with deformable convolutions. arXiv preprint arXiv:2211.05778 (2022)\n\n",
            "[4] \nChen, Q., Wu, Q., Wang, J., Hu, Q., Hu, T., Ding, E., Cheng, J., Wang, J.: Mixformer: Mixing features across windows and dimensions (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n",
            "[12] \nDing, X., Zhang, Y., Ge, Y., Zhao, S., Song, L., Yue, X., Shan, Y.: Unireplknet: A universal perception large-kernel convnet for audio, video, point cloud, time-series and image recognition. arXiv preprint arXiv:2311.15599 (2023)\n\n"
        ],
        "references": [
            "Settings.\nWe conducted image classification experiments on the ImageNet-1K [35] dataset, following the experimental settings of DeiT [39] for a fair comparison. Specifically, each model was trained for 300 epochs on 8 V100 GPUs with an input size of 224×224224224224\\times 224224 × 224. We used AdamW as the optimizer with a weight decay of 0.050.050.050.05 and used a cosine decay learning rate schedule with an initial learning rate of 0.0010.0010.0010.001, while the first five epochs were used for linear warm-up. The batch size was set to 1024. To avoid overfitting, we used regularization techniques including RandAugment [9] (rand-m9-mstd0.5-inc1), MixUp [54] (prob = 0.8), CutMix [52] (prob = 1.0), Random Erasing (prob = 0.25), and increasing stochastic depth [23] (prob = 0.1/0.2/0.4 for DeBiFormer-T/S/B, respectively).\n\nResults.\nWe report our results in Table 2 showing the top-1 accuracy with similar computational complexities. Our DeBiFormer outperformed the Swin Transformer [29], PVT [44], DeiT [39], DAT[47], and Biformer [56] in all three scales. Without inserting convolutions in Transformer blocks or using overlapped convolutions in patch embeddings, DeBiFormer achieved gains of 0.5pt, 0.1pt and 0.1pt over BiFormer [56] counterparts."
        ]
    },
    "S4.T2.3.1.1.2.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.3.1.1.2.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.3.1.1.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.3.1.1.2.1.1.1\"><span class=\"ltx_text\" id=\"S4.T2.3.1.1.2.1.1.1.1\" style=\"font-size:90%;\">FLOPs</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.1.1.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.3.1.1.2.1.2.1\"><span class=\"ltx_text\" id=\"S4.T2.3.1.1.2.1.2.1.1\" style=\"font-size:90%;\">(G)</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 2:  Evaluating and comparing different backbones on ImageNet-1K on images with resolution of  224 × 224 224 224 224\\times 224 224 × 224 .",
        "footnotes": [
            "[20] \nHe, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition (2015)\n\n",
            "[42] \nWang, S., Li, B.Z., Khabsa, M., Fang, H., Ma, H.: Linformer: Self-attention with linear complexity (2020)\n\n",
            "[34] \nRen, S., Zhou, D., He, S., Feng, J., Wang, X.: Shunted self-attention via multi-scale token aggregation (2021)\n\n",
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n",
            "[22] \nHou, Q., Lu, C.Z., Cheng, M.M., Feng, J.: Conv2former: A simple transformer-style convnet for visual recognition. arXiv preprint arXiv:2211.11943 (2022)\n\n",
            "[44] \nWang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 568–578 (2021)\n\n",
            "[29] \nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021)\n\n",
            "[14] \nDong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin transformer: A general vision transformer backbone with cross-shaped windows. In: Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR). pp. 12124–12134 (2022)\n\n",
            "[47] \nXia, Z., Pan, X., Song, S., Li, L.E., Huang, G.: Vision transformer with deformable attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4794–4803 (June 2022)\n\n",
            "[45] \nWang, W., Yao, L., Chen, L., Lin, B., Cai, D., He, X., Liu, W.: Crossformer: A versatile vision transformer hinging on cross-scale attention. In: Proceedings of the International Conference on Learning Representations (ICLR) (2022),  https://openreview.net/forum?id=_PHymLIxuI \n",
            "[2] \nChen, C.F., Panda, R., Fan, Q.: Regionvit: Regional-to-local attention for vision transformers. arXiv preprint arXiv:2106.02689 (2021)\n\n",
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[40] \nTu, Z., Talebi, H., Zhang, H., Yang, F., Milanfar, P., Bovik, A., Li, Y.: Maxvit: Multi-axis vision transformer. ECCV (2022)\n\n",
            "[43] \nWang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu, L., Li, H., et al.: Internimage: Exploring large-scale vision foundation models with deformable convolutions. arXiv preprint arXiv:2211.05778 (2022)\n\n",
            "[4] \nChen, Q., Wu, Q., Wang, J., Hu, Q., Hu, T., Ding, E., Cheng, J., Wang, J.: Mixformer: Mixing features across windows and dimensions (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n",
            "[12] \nDing, X., Zhang, Y., Ge, Y., Zhao, S., Song, L., Yue, X., Shan, Y.: Unireplknet: A universal perception large-kernel convnet for audio, video, point cloud, time-series and image recognition. arXiv preprint arXiv:2311.15599 (2023)\n\n",
            "[30] \nLiu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for the 2020s. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 11976–11986 (June 2022)\n\n",
            "[27] \nLiu, S., Chen, T., Chen, X., Chen, X., Xiao, Q., Wu, B., Pechenizkiy, M., Mocanu, D., Wang, Z.: More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity. arXiv preprint arXiv:2207.03620 (2022)\n\n",
            "[7] \nChu, X., Tian, Z., Wang, Y., Zhang, B., Ren, H., Wei, X., Xia, H., Shen, C.: Twins: Revisiting the design of spatial attention in vision transformers. In: NeurIPS 2021 (2021),  https://openreview.net/forum?id=5kTlVBkzSRx \n",
            "[44] \nWang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 568–578 (2021)\n\n",
            "[29] \nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021)\n\n",
            "[49] \nYang, J., Li, C., Zhang, P., Dai, X., Xiao, B., Yuan, L., Gao, J.: Focal attention for long-range interactions in vision transformers. In: Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., Vaughan, J.W. (eds.) Advances in Neural Information Processing Systems. vol. 34, pp. 30008–30022. Curran Associates, Inc. (2021)\n\n",
            "[14] \nDong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin transformer: A general vision transformer backbone with cross-shaped windows. In: Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR). pp. 12124–12134 (2022)\n\n",
            "[34] \nRen, S., Zhou, D., He, S., Feng, J., Wang, X.: Shunted self-attention via multi-scale token aggregation (2021)\n\n",
            "[26] \nLi, K., Wang, Y., Gao, P., Song, G., Liu, Y., Li, H., Qiao, Y.: Uniformer: Unified transformer for efficient spatiotemporal representation learning (2022)\n\n",
            "[51] \nYang, R., Ma, H., Wu, J., Tang, Y., Xiao, X., Zheng, M., Li, X.: Scalablevit: Rethinking the context-oriented generalization of vision transformer (2022)\n\n",
            "[33] \nPan, X., Ye, T., Xia, Z., Song, S., Huang, G.: Slide-transformer: Hierarchical vision transformer with local self-attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 2082–2091 (June 2023)\n\n",
            "[47] \nXia, Z., Pan, X., Song, S., Li, L.E., Huang, G.: Vision transformer with deformable attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4794–4803 (June 2022)\n\n",
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[45] \nWang, W., Yao, L., Chen, L., Lin, B., Cai, D., He, X., Liu, W.: Crossformer: A versatile vision transformer hinging on cross-scale attention. In: Proceedings of the International Conference on Learning Representations (ICLR) (2022),  https://openreview.net/forum?id=_PHymLIxuI \n",
            "[2] \nChen, C.F., Panda, R., Fan, Q.: Regionvit: Regional-to-local attention for vision transformers. arXiv preprint arXiv:2106.02689 (2021)\n\n",
            "[43] \nWang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu, L., Li, H., et al.: Internimage: Exploring large-scale vision foundation models with deformable convolutions. arXiv preprint arXiv:2211.05778 (2022)\n\n",
            "[4] \nChen, Q., Wu, Q., Wang, J., Hu, Q., Hu, T., Ding, E., Cheng, J., Wang, J.: Mixformer: Mixing features across windows and dimensions (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n",
            "[12] \nDing, X., Zhang, Y., Ge, Y., Zhao, S., Song, L., Yue, X., Shan, Y.: Unireplknet: A universal perception large-kernel convnet for audio, video, point cloud, time-series and image recognition. arXiv preprint arXiv:2311.15599 (2023)\n\n"
        ],
        "references": [
            "Settings.\nWe conducted image classification experiments on the ImageNet-1K [35] dataset, following the experimental settings of DeiT [39] for a fair comparison. Specifically, each model was trained for 300 epochs on 8 V100 GPUs with an input size of 224×224224224224\\times 224224 × 224. We used AdamW as the optimizer with a weight decay of 0.050.050.050.05 and used a cosine decay learning rate schedule with an initial learning rate of 0.0010.0010.0010.001, while the first five epochs were used for linear warm-up. The batch size was set to 1024. To avoid overfitting, we used regularization techniques including RandAugment [9] (rand-m9-mstd0.5-inc1), MixUp [54] (prob = 0.8), CutMix [52] (prob = 1.0), Random Erasing (prob = 0.25), and increasing stochastic depth [23] (prob = 0.1/0.2/0.4 for DeBiFormer-T/S/B, respectively).\n\nResults.\nWe report our results in Table 2 showing the top-1 accuracy with similar computational complexities. Our DeBiFormer outperformed the Swin Transformer [29], PVT [44], DeiT [39], DAT[47], and Biformer [56] in all three scales. Without inserting convolutions in Transformer blocks or using overlapped convolutions in patch embeddings, DeBiFormer achieved gains of 0.5pt, 0.1pt and 0.1pt over BiFormer [56] counterparts."
        ]
    },
    "S4.T2.3.1.1.3.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.3.1.1.3.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.3.1.1.3.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.3.1.1.3.1.1.1\"><span class=\"ltx_text\" id=\"S4.T2.3.1.1.3.1.1.1.1\" style=\"font-size:90%;\">Params</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.1.1.3.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.3.1.1.3.1.2.1\"><span class=\"ltx_text\" id=\"S4.T2.3.1.1.3.1.2.1.1\" style=\"font-size:90%;\">(M)</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 2:  Evaluating and comparing different backbones on ImageNet-1K on images with resolution of  224 × 224 224 224 224\\times 224 224 × 224 .",
        "footnotes": [
            "[20] \nHe, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition (2015)\n\n",
            "[42] \nWang, S., Li, B.Z., Khabsa, M., Fang, H., Ma, H.: Linformer: Self-attention with linear complexity (2020)\n\n",
            "[34] \nRen, S., Zhou, D., He, S., Feng, J., Wang, X.: Shunted self-attention via multi-scale token aggregation (2021)\n\n",
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n",
            "[22] \nHou, Q., Lu, C.Z., Cheng, M.M., Feng, J.: Conv2former: A simple transformer-style convnet for visual recognition. arXiv preprint arXiv:2211.11943 (2022)\n\n",
            "[44] \nWang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 568–578 (2021)\n\n",
            "[29] \nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021)\n\n",
            "[14] \nDong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin transformer: A general vision transformer backbone with cross-shaped windows. In: Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR). pp. 12124–12134 (2022)\n\n",
            "[47] \nXia, Z., Pan, X., Song, S., Li, L.E., Huang, G.: Vision transformer with deformable attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4794–4803 (June 2022)\n\n",
            "[45] \nWang, W., Yao, L., Chen, L., Lin, B., Cai, D., He, X., Liu, W.: Crossformer: A versatile vision transformer hinging on cross-scale attention. In: Proceedings of the International Conference on Learning Representations (ICLR) (2022),  https://openreview.net/forum?id=_PHymLIxuI \n",
            "[2] \nChen, C.F., Panda, R., Fan, Q.: Regionvit: Regional-to-local attention for vision transformers. arXiv preprint arXiv:2106.02689 (2021)\n\n",
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[40] \nTu, Z., Talebi, H., Zhang, H., Yang, F., Milanfar, P., Bovik, A., Li, Y.: Maxvit: Multi-axis vision transformer. ECCV (2022)\n\n",
            "[43] \nWang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu, L., Li, H., et al.: Internimage: Exploring large-scale vision foundation models with deformable convolutions. arXiv preprint arXiv:2211.05778 (2022)\n\n",
            "[4] \nChen, Q., Wu, Q., Wang, J., Hu, Q., Hu, T., Ding, E., Cheng, J., Wang, J.: Mixformer: Mixing features across windows and dimensions (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n",
            "[12] \nDing, X., Zhang, Y., Ge, Y., Zhao, S., Song, L., Yue, X., Shan, Y.: Unireplknet: A universal perception large-kernel convnet for audio, video, point cloud, time-series and image recognition. arXiv preprint arXiv:2311.15599 (2023)\n\n",
            "[30] \nLiu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for the 2020s. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 11976–11986 (June 2022)\n\n",
            "[27] \nLiu, S., Chen, T., Chen, X., Chen, X., Xiao, Q., Wu, B., Pechenizkiy, M., Mocanu, D., Wang, Z.: More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity. arXiv preprint arXiv:2207.03620 (2022)\n\n",
            "[7] \nChu, X., Tian, Z., Wang, Y., Zhang, B., Ren, H., Wei, X., Xia, H., Shen, C.: Twins: Revisiting the design of spatial attention in vision transformers. In: NeurIPS 2021 (2021),  https://openreview.net/forum?id=5kTlVBkzSRx \n",
            "[44] \nWang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 568–578 (2021)\n\n",
            "[29] \nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021)\n\n",
            "[49] \nYang, J., Li, C., Zhang, P., Dai, X., Xiao, B., Yuan, L., Gao, J.: Focal attention for long-range interactions in vision transformers. In: Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., Vaughan, J.W. (eds.) Advances in Neural Information Processing Systems. vol. 34, pp. 30008–30022. Curran Associates, Inc. (2021)\n\n",
            "[14] \nDong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin transformer: A general vision transformer backbone with cross-shaped windows. In: Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR). pp. 12124–12134 (2022)\n\n",
            "[34] \nRen, S., Zhou, D., He, S., Feng, J., Wang, X.: Shunted self-attention via multi-scale token aggregation (2021)\n\n",
            "[26] \nLi, K., Wang, Y., Gao, P., Song, G., Liu, Y., Li, H., Qiao, Y.: Uniformer: Unified transformer for efficient spatiotemporal representation learning (2022)\n\n",
            "[51] \nYang, R., Ma, H., Wu, J., Tang, Y., Xiao, X., Zheng, M., Li, X.: Scalablevit: Rethinking the context-oriented generalization of vision transformer (2022)\n\n",
            "[33] \nPan, X., Ye, T., Xia, Z., Song, S., Huang, G.: Slide-transformer: Hierarchical vision transformer with local self-attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 2082–2091 (June 2023)\n\n",
            "[47] \nXia, Z., Pan, X., Song, S., Li, L.E., Huang, G.: Vision transformer with deformable attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4794–4803 (June 2022)\n\n",
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[45] \nWang, W., Yao, L., Chen, L., Lin, B., Cai, D., He, X., Liu, W.: Crossformer: A versatile vision transformer hinging on cross-scale attention. In: Proceedings of the International Conference on Learning Representations (ICLR) (2022),  https://openreview.net/forum?id=_PHymLIxuI \n",
            "[2] \nChen, C.F., Panda, R., Fan, Q.: Regionvit: Regional-to-local attention for vision transformers. arXiv preprint arXiv:2106.02689 (2021)\n\n",
            "[43] \nWang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu, L., Li, H., et al.: Internimage: Exploring large-scale vision foundation models with deformable convolutions. arXiv preprint arXiv:2211.05778 (2022)\n\n",
            "[4] \nChen, Q., Wu, Q., Wang, J., Hu, Q., Hu, T., Ding, E., Cheng, J., Wang, J.: Mixformer: Mixing features across windows and dimensions (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n",
            "[12] \nDing, X., Zhang, Y., Ge, Y., Zhao, S., Song, L., Yue, X., Shan, Y.: Unireplknet: A universal perception large-kernel convnet for audio, video, point cloud, time-series and image recognition. arXiv preprint arXiv:2311.15599 (2023)\n\n"
        ],
        "references": [
            "Settings.\nWe conducted image classification experiments on the ImageNet-1K [35] dataset, following the experimental settings of DeiT [39] for a fair comparison. Specifically, each model was trained for 300 epochs on 8 V100 GPUs with an input size of 224×224224224224\\times 224224 × 224. We used AdamW as the optimizer with a weight decay of 0.050.050.050.05 and used a cosine decay learning rate schedule with an initial learning rate of 0.0010.0010.0010.001, while the first five epochs were used for linear warm-up. The batch size was set to 1024. To avoid overfitting, we used regularization techniques including RandAugment [9] (rand-m9-mstd0.5-inc1), MixUp [54] (prob = 0.8), CutMix [52] (prob = 1.0), Random Erasing (prob = 0.25), and increasing stochastic depth [23] (prob = 0.1/0.2/0.4 for DeBiFormer-T/S/B, respectively).\n\nResults.\nWe report our results in Table 2 showing the top-1 accuracy with similar computational complexities. Our DeBiFormer outperformed the Swin Transformer [29], PVT [44], DeiT [39], DAT[47], and Biformer [56] in all three scales. Without inserting convolutions in Transformer blocks or using overlapped convolutions in patch embeddings, DeBiFormer achieved gains of 0.5pt, 0.1pt and 0.1pt over BiFormer [56] counterparts."
        ]
    },
    "S4.T2.3.1.1.4.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.3.1.1.4.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.3.1.1.4.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.3.1.1.4.1.1.1\"><span class=\"ltx_text\" id=\"S4.T2.3.1.1.4.1.1.1.1\" style=\"font-size:90%;\">Top-1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.3.1.1.4.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.3.1.1.4.1.2.1\"><span class=\"ltx_text\" id=\"S4.T2.3.1.1.4.1.2.1.1\" style=\"font-size:90%;\">(%)</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 2:  Evaluating and comparing different backbones on ImageNet-1K on images with resolution of  224 × 224 224 224 224\\times 224 224 × 224 .",
        "footnotes": [
            "[20] \nHe, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition (2015)\n\n",
            "[42] \nWang, S., Li, B.Z., Khabsa, M., Fang, H., Ma, H.: Linformer: Self-attention with linear complexity (2020)\n\n",
            "[34] \nRen, S., Zhou, D., He, S., Feng, J., Wang, X.: Shunted self-attention via multi-scale token aggregation (2021)\n\n",
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n",
            "[22] \nHou, Q., Lu, C.Z., Cheng, M.M., Feng, J.: Conv2former: A simple transformer-style convnet for visual recognition. arXiv preprint arXiv:2211.11943 (2022)\n\n",
            "[44] \nWang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 568–578 (2021)\n\n",
            "[29] \nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021)\n\n",
            "[14] \nDong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin transformer: A general vision transformer backbone with cross-shaped windows. In: Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR). pp. 12124–12134 (2022)\n\n",
            "[47] \nXia, Z., Pan, X., Song, S., Li, L.E., Huang, G.: Vision transformer with deformable attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4794–4803 (June 2022)\n\n",
            "[45] \nWang, W., Yao, L., Chen, L., Lin, B., Cai, D., He, X., Liu, W.: Crossformer: A versatile vision transformer hinging on cross-scale attention. In: Proceedings of the International Conference on Learning Representations (ICLR) (2022),  https://openreview.net/forum?id=_PHymLIxuI \n",
            "[2] \nChen, C.F., Panda, R., Fan, Q.: Regionvit: Regional-to-local attention for vision transformers. arXiv preprint arXiv:2106.02689 (2021)\n\n",
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[40] \nTu, Z., Talebi, H., Zhang, H., Yang, F., Milanfar, P., Bovik, A., Li, Y.: Maxvit: Multi-axis vision transformer. ECCV (2022)\n\n",
            "[43] \nWang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu, L., Li, H., et al.: Internimage: Exploring large-scale vision foundation models with deformable convolutions. arXiv preprint arXiv:2211.05778 (2022)\n\n",
            "[4] \nChen, Q., Wu, Q., Wang, J., Hu, Q., Hu, T., Ding, E., Cheng, J., Wang, J.: Mixformer: Mixing features across windows and dimensions (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n",
            "[12] \nDing, X., Zhang, Y., Ge, Y., Zhao, S., Song, L., Yue, X., Shan, Y.: Unireplknet: A universal perception large-kernel convnet for audio, video, point cloud, time-series and image recognition. arXiv preprint arXiv:2311.15599 (2023)\n\n",
            "[30] \nLiu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for the 2020s. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 11976–11986 (June 2022)\n\n",
            "[27] \nLiu, S., Chen, T., Chen, X., Chen, X., Xiao, Q., Wu, B., Pechenizkiy, M., Mocanu, D., Wang, Z.: More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity. arXiv preprint arXiv:2207.03620 (2022)\n\n",
            "[7] \nChu, X., Tian, Z., Wang, Y., Zhang, B., Ren, H., Wei, X., Xia, H., Shen, C.: Twins: Revisiting the design of spatial attention in vision transformers. In: NeurIPS 2021 (2021),  https://openreview.net/forum?id=5kTlVBkzSRx \n",
            "[44] \nWang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 568–578 (2021)\n\n",
            "[29] \nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021)\n\n",
            "[49] \nYang, J., Li, C., Zhang, P., Dai, X., Xiao, B., Yuan, L., Gao, J.: Focal attention for long-range interactions in vision transformers. In: Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., Vaughan, J.W. (eds.) Advances in Neural Information Processing Systems. vol. 34, pp. 30008–30022. Curran Associates, Inc. (2021)\n\n",
            "[14] \nDong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin transformer: A general vision transformer backbone with cross-shaped windows. In: Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR). pp. 12124–12134 (2022)\n\n",
            "[34] \nRen, S., Zhou, D., He, S., Feng, J., Wang, X.: Shunted self-attention via multi-scale token aggregation (2021)\n\n",
            "[26] \nLi, K., Wang, Y., Gao, P., Song, G., Liu, Y., Li, H., Qiao, Y.: Uniformer: Unified transformer for efficient spatiotemporal representation learning (2022)\n\n",
            "[51] \nYang, R., Ma, H., Wu, J., Tang, Y., Xiao, X., Zheng, M., Li, X.: Scalablevit: Rethinking the context-oriented generalization of vision transformer (2022)\n\n",
            "[33] \nPan, X., Ye, T., Xia, Z., Song, S., Huang, G.: Slide-transformer: Hierarchical vision transformer with local self-attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 2082–2091 (June 2023)\n\n",
            "[47] \nXia, Z., Pan, X., Song, S., Li, L.E., Huang, G.: Vision transformer with deformable attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4794–4803 (June 2022)\n\n",
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[45] \nWang, W., Yao, L., Chen, L., Lin, B., Cai, D., He, X., Liu, W.: Crossformer: A versatile vision transformer hinging on cross-scale attention. In: Proceedings of the International Conference on Learning Representations (ICLR) (2022),  https://openreview.net/forum?id=_PHymLIxuI \n",
            "[2] \nChen, C.F., Panda, R., Fan, Q.: Regionvit: Regional-to-local attention for vision transformers. arXiv preprint arXiv:2106.02689 (2021)\n\n",
            "[43] \nWang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu, L., Li, H., et al.: Internimage: Exploring large-scale vision foundation models with deformable convolutions. arXiv preprint arXiv:2211.05778 (2022)\n\n",
            "[4] \nChen, Q., Wu, Q., Wang, J., Hu, Q., Hu, T., Ding, E., Cheng, J., Wang, J.: Mixformer: Mixing features across windows and dimensions (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n",
            "[12] \nDing, X., Zhang, Y., Ge, Y., Zhao, S., Song, L., Yue, X., Shan, Y.: Unireplknet: A universal perception large-kernel convnet for audio, video, point cloud, time-series and image recognition. arXiv preprint arXiv:2311.15599 (2023)\n\n"
        ],
        "references": [
            "Settings.\nWe conducted image classification experiments on the ImageNet-1K [35] dataset, following the experimental settings of DeiT [39] for a fair comparison. Specifically, each model was trained for 300 epochs on 8 V100 GPUs with an input size of 224×224224224224\\times 224224 × 224. We used AdamW as the optimizer with a weight decay of 0.050.050.050.05 and used a cosine decay learning rate schedule with an initial learning rate of 0.0010.0010.0010.001, while the first five epochs were used for linear warm-up. The batch size was set to 1024. To avoid overfitting, we used regularization techniques including RandAugment [9] (rand-m9-mstd0.5-inc1), MixUp [54] (prob = 0.8), CutMix [52] (prob = 1.0), Random Erasing (prob = 0.25), and increasing stochastic depth [23] (prob = 0.1/0.2/0.4 for DeBiFormer-T/S/B, respectively).\n\nResults.\nWe report our results in Table 2 showing the top-1 accuracy with similar computational complexities. Our DeBiFormer outperformed the Swin Transformer [29], PVT [44], DeiT [39], DAT[47], and Biformer [56] in all three scales. Without inserting convolutions in Transformer blocks or using overlapped convolutions in patch embeddings, DeBiFormer achieved gains of 0.5pt, 0.1pt and 0.1pt over BiFormer [56] counterparts."
        ]
    },
    "S4.T2.4": {
        "table": "<table class=\"ltx_tabular ltx_figure_panel ltx_minipage ltx_align_top\" id=\"S4.T2.4\" style=\"width:203.8pt;\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T2.4.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T2.4.1.1.1\"><span class=\"ltx_text\" id=\"S4.T2.4.1.1.1.1\" style=\"font-size:90%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.4.1.1.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.4.1.1.2.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.4.1.1.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.4.1.1.2.1.1.1\"><span class=\"ltx_text\" id=\"S4.T2.4.1.1.2.1.1.1.1\" style=\"font-size:90%;\">FLOPs</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.1.1.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.4.1.1.2.1.2.1\"><span class=\"ltx_text\" id=\"S4.T2.4.1.1.2.1.2.1.1\" style=\"font-size:90%;\">(G)</span></td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.4.1.1.3\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.4.1.1.3.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.4.1.1.3.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.4.1.1.3.1.1.1\"><span class=\"ltx_text\" id=\"S4.T2.4.1.1.3.1.1.1.1\" style=\"font-size:90%;\">Params</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.1.1.3.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.4.1.1.3.1.2.1\"><span class=\"ltx_text\" id=\"S4.T2.4.1.1.3.1.2.1.1\" style=\"font-size:90%;\">(M)</span></td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.4.1.1.4\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.4.1.1.4.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.4.1.1.4.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.4.1.1.4.1.1.1\"><span class=\"ltx_text\" id=\"S4.T2.4.1.1.4.1.1.1.1\" style=\"font-size:90%;\">Top-1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.1.1.4.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.4.1.1.4.1.2.1\"><span class=\"ltx_text\" id=\"S4.T2.4.1.1.4.1.2.1.1\" style=\"font-size:90%;\">(%)</span></td>\n</tr>\n</table>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.4.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T2.4.2.1.1\">\n<span class=\"ltx_text\" id=\"S4.T2.4.2.1.1.1\" style=\"font-size:90%;\">ConvNeXt-B </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.4.2.1.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib30\" title=\"\">30</a><span class=\"ltx_text\" id=\"S4.T2.4.2.1.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.4.2.1.2\"><span class=\"ltx_text\" id=\"S4.T2.4.2.1.2.1\" style=\"font-size:90%;\">15.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.4.2.1.3\"><span class=\"ltx_text\" id=\"S4.T2.4.2.1.3.1\" style=\"font-size:90%;\">89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.4.2.1.4\"><span class=\"ltx_text\" id=\"S4.T2.4.2.1.4.1\" style=\"font-size:90%;\">83.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.4.3.2.1\">\n<span class=\"ltx_text\" id=\"S4.T2.4.3.2.1.1\" style=\"font-size:90%;\">SLaK-S </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.4.3.2.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib27\" title=\"\">27</a><span class=\"ltx_text\" id=\"S4.T2.4.3.2.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.3.2.2\"><span class=\"ltx_text\" id=\"S4.T2.4.3.2.2.1\" style=\"font-size:90%;\">9.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.3.2.3\"><span class=\"ltx_text\" id=\"S4.T2.4.3.2.3.1\" style=\"font-size:90%;\">55</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.3.2.4\"><span class=\"ltx_text\" id=\"S4.T2.4.3.2.4.1\" style=\"font-size:90%;\">83.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.4.4.3.1\">\n<span class=\"ltx_text\" id=\"S4.T2.4.4.3.1.1\" style=\"font-size:90%;\">Twins-SVT-L </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.4.4.3.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib7\" title=\"\">7</a><span class=\"ltx_text\" id=\"S4.T2.4.4.3.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.4.3.2\"><span class=\"ltx_text\" id=\"S4.T2.4.4.3.2.1\" style=\"font-size:90%;\">14.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.4.3.3\"><span class=\"ltx_text\" id=\"S4.T2.4.4.3.3.1\" style=\"font-size:90%;\">99</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.4.3.4\"><span class=\"ltx_text\" id=\"S4.T2.4.4.3.4.1\" style=\"font-size:90%;\">83.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.4.5.4.1\">\n<span class=\"ltx_text\" id=\"S4.T2.4.5.4.1.1\" style=\"font-size:90%;\">PVTv2-B5 </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.4.5.4.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib44\" title=\"\">44</a><span class=\"ltx_text\" id=\"S4.T2.4.5.4.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.5.4.2\"><span class=\"ltx_text\" id=\"S4.T2.4.5.4.2.1\" style=\"font-size:90%;\">11.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.5.4.3\"><span class=\"ltx_text\" id=\"S4.T2.4.5.4.3.1\" style=\"font-size:90%;\">82</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.5.4.4\"><span class=\"ltx_text\" id=\"S4.T2.4.5.4.4.1\" style=\"font-size:90%;\">83.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.6.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.4.6.5.1\">\n<span class=\"ltx_text\" id=\"S4.T2.4.6.5.1.1\" style=\"font-size:90%;\">Swin-B </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.4.6.5.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib29\" title=\"\">29</a><span class=\"ltx_text\" id=\"S4.T2.4.6.5.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.6.5.2\"><span class=\"ltx_text\" id=\"S4.T2.4.6.5.2.1\" style=\"font-size:90%;\">15.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.6.5.3\"><span class=\"ltx_text\" id=\"S4.T2.4.6.5.3.1\" style=\"font-size:90%;\">88</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.6.5.4\"><span class=\"ltx_text\" id=\"S4.T2.4.6.5.4.1\" style=\"font-size:90%;\">83.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.7.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.4.7.6.1\">\n<span class=\"ltx_text\" id=\"S4.T2.4.7.6.1.1\" style=\"font-size:90%;\">Focal-B </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.4.7.6.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib49\" title=\"\">49</a><span class=\"ltx_text\" id=\"S4.T2.4.7.6.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.7.6.2\"><span class=\"ltx_text\" id=\"S4.T2.4.7.6.2.1\" style=\"font-size:90%;\">16.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.7.6.3\"><span class=\"ltx_text\" id=\"S4.T2.4.7.6.3.1\" style=\"font-size:90%;\">90</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.7.6.4\"><span class=\"ltx_text\" id=\"S4.T2.4.7.6.4.1\" style=\"font-size:90%;\">84.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.8.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.4.8.7.1\">\n<span class=\"ltx_text\" id=\"S4.T2.4.8.7.1.1\" style=\"font-size:90%;\">CSWin-B </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.4.8.7.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib14\" title=\"\">14</a><span class=\"ltx_text\" id=\"S4.T2.4.8.7.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.8.7.2\"><span class=\"ltx_text\" id=\"S4.T2.4.8.7.2.1\" style=\"font-size:90%;\">15.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.8.7.3\"><span class=\"ltx_text\" id=\"S4.T2.4.8.7.3.1\" style=\"font-size:90%;\">78</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.8.7.4\"><span class=\"ltx_text\" id=\"S4.T2.4.8.7.4.1\" style=\"font-size:90%;\">84.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.9.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.4.9.8.1\">\n<span class=\"ltx_text\" id=\"S4.T2.4.9.8.1.1\" style=\"font-size:90%;\">Shunted-B </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.4.9.8.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib34\" title=\"\">34</a><span class=\"ltx_text\" id=\"S4.T2.4.9.8.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.9.8.2\"><span class=\"ltx_text\" id=\"S4.T2.4.9.8.2.1\" style=\"font-size:90%;\">8.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.9.8.3\"><span class=\"ltx_text\" id=\"S4.T2.4.9.8.3.1\" style=\"font-size:90%;\">39</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.9.8.4\"><span class=\"ltx_text\" id=\"S4.T2.4.9.8.4.1\" style=\"font-size:90%;\">84.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.10.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.4.10.9.1\">\n<span class=\"ltx_text\" id=\"S4.T2.4.10.9.1.1\" style=\"font-size:90%;\">UniFormer-B </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.4.10.9.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib26\" title=\"\">26</a><span class=\"ltx_text\" id=\"S4.T2.4.10.9.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.10.9.2\"><span class=\"ltx_text\" id=\"S4.T2.4.10.9.2.1\" style=\"font-size:90%;\">8.3</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.10.9.3\"><span class=\"ltx_text\" id=\"S4.T2.4.10.9.3.1\" style=\"font-size:90%;\">50.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.10.9.4\"><span class=\"ltx_text\" id=\"S4.T2.4.10.9.4.1\" style=\"font-size:90%;\">83.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.11.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.4.11.10.1\">\n<span class=\"ltx_text\" id=\"S4.T2.4.11.10.1.1\" style=\"font-size:90%;\">ScalableViT-B </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.4.11.10.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib51\" title=\"\">51</a><span class=\"ltx_text\" id=\"S4.T2.4.11.10.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.11.10.2\"><span class=\"ltx_text\" id=\"S4.T2.4.11.10.2.1\" style=\"font-size:90%;\">8.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.11.10.3\"><span class=\"ltx_text\" id=\"S4.T2.4.11.10.3.1\" style=\"font-size:90%;\">81</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.11.10.4\"><span class=\"ltx_text\" id=\"S4.T2.4.11.10.4.1\" style=\"font-size:90%;\">84.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.12.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.4.12.11.1\">\n<span class=\"ltx_text\" id=\"S4.T2.4.12.11.1.1\" style=\"font-size:90%;\">Slide-Swin-B </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.4.12.11.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib33\" title=\"\">33</a><span class=\"ltx_text\" id=\"S4.T2.4.12.11.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.12.11.2\"><span class=\"ltx_text\" id=\"S4.T2.4.12.11.2.1\" style=\"font-size:90%;\">15.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.12.11.3\"><span class=\"ltx_text\" id=\"S4.T2.4.12.11.3.1\" style=\"font-size:90%;\">89.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.12.11.4\"><span class=\"ltx_text\" id=\"S4.T2.4.12.11.4.1\" style=\"font-size:90%;\">84.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.13.12\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.4.13.12.1\">\n<span class=\"ltx_text\" id=\"S4.T2.4.13.12.1.1\" style=\"font-size:90%;\">DAT-S </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.4.13.12.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib47\" title=\"\">47</a><span class=\"ltx_text\" id=\"S4.T2.4.13.12.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.13.12.2\"><span class=\"ltx_text\" id=\"S4.T2.4.13.12.2.1\" style=\"font-size:90%;\">9.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.13.12.3\"><span class=\"ltx_text\" id=\"S4.T2.4.13.12.3.1\" style=\"font-size:90%;\">50</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.13.12.4\"><span class=\"ltx_text\" id=\"S4.T2.4.13.12.4.1\" style=\"font-size:90%;\">83.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.14.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.4.14.13.1\">\n<span class=\"ltx_text\" id=\"S4.T2.4.14.13.1.1\" style=\"font-size:90%;\">QuadTree-B-b4</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.4.14.13.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib37\" title=\"\">37</a><span class=\"ltx_text\" id=\"S4.T2.4.14.13.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.14.13.2\"><span class=\"ltx_text\" id=\"S4.T2.4.14.13.2.1\" style=\"font-size:90%;\">11.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.14.13.3\"><span class=\"ltx_text\" id=\"S4.T2.4.14.13.3.1\" style=\"font-size:90%;\">64</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.14.13.4\"><span class=\"ltx_text\" id=\"S4.T2.4.14.13.4.1\" style=\"font-size:90%;\">84.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.15.14\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.4.15.14.1\">\n<span class=\"ltx_text\" id=\"S4.T2.4.15.14.1.1\" style=\"font-size:90%;\">CrossFormer-L </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.4.15.14.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib45\" title=\"\">45</a><span class=\"ltx_text\" id=\"S4.T2.4.15.14.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.15.14.2\"><span class=\"ltx_text\" id=\"S4.T2.4.15.14.2.1\" style=\"font-size:90%;\">16.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.15.14.3\"><span class=\"ltx_text\" id=\"S4.T2.4.15.14.3.1\" style=\"font-size:90%;\">92</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.15.14.4\"><span class=\"ltx_text\" id=\"S4.T2.4.15.14.4.1\" style=\"font-size:90%;\">84.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.16.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.4.16.15.1\">\n<span class=\"ltx_text\" id=\"S4.T2.4.16.15.1.1\" style=\"font-size:90%;\">RegionViT-B+ </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.4.16.15.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib2\" title=\"\">2</a><span class=\"ltx_text\" id=\"S4.T2.4.16.15.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.16.15.2\"><span class=\"ltx_text\" id=\"S4.T2.4.16.15.2.1\" style=\"font-size:90%;\">13.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.16.15.3\"><span class=\"ltx_text\" id=\"S4.T2.4.16.15.3.1\" style=\"font-size:90%;\">74</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.16.15.4\"><span class=\"ltx_text\" id=\"S4.T2.4.16.15.4.1\" style=\"font-size:90%;\">83.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.17.16\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.4.17.16.1\">\n<span class=\"ltx_text\" id=\"S4.T2.4.17.16.1.1\" style=\"font-size:90%;\">InternImage-S </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.4.17.16.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib43\" title=\"\">43</a><span class=\"ltx_text\" id=\"S4.T2.4.17.16.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.17.16.2\"><span class=\"ltx_text\" id=\"S4.T2.4.17.16.2.1\" style=\"font-size:90%;\">8.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.17.16.3\"><span class=\"ltx_text\" id=\"S4.T2.4.17.16.3.1\" style=\"font-size:90%;\">50</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.17.16.4\"><span class=\"ltx_text\" id=\"S4.T2.4.17.16.4.1\" style=\"font-size:90%;\">84.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.18.17\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.4.18.17.1\">\n<span class=\"ltx_text\" id=\"S4.T2.4.18.17.1.1\" style=\"font-size:90%;\">MixFormer-B6 </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.4.18.17.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib4\" title=\"\">4</a><span class=\"ltx_text\" id=\"S4.T2.4.18.17.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.18.17.2\"><span class=\"ltx_text\" id=\"S4.T2.4.18.17.2.1\" style=\"font-size:90%;\">12.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.18.17.3\"><span class=\"ltx_text\" id=\"S4.T2.4.18.17.3.1\" style=\"font-size:90%;\">119</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.18.17.4\"><span class=\"ltx_text\" id=\"S4.T2.4.18.17.4.1\" style=\"font-size:90%;\">83.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.19.18\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.4.19.18.1\">\n<span class=\"ltx_text\" id=\"S4.T2.4.19.18.1.1\" style=\"font-size:90%;\">BiFormer-B </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.4.19.18.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib56\" title=\"\">56</a><span class=\"ltx_text\" id=\"S4.T2.4.19.18.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.19.18.2\"><span class=\"ltx_text\" id=\"S4.T2.4.19.18.2.1\" style=\"font-size:90%;\">9.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.19.18.3\"><span class=\"ltx_text\" id=\"S4.T2.4.19.18.3.1\" style=\"font-size:90%;\">57</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.19.18.4\"><span class=\"ltx_text\" id=\"S4.T2.4.19.18.4.1\" style=\"font-size:90%;\">84.3</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.20.19\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.4.20.19.1\">\n<span class=\"ltx_text\" id=\"S4.T2.4.20.19.1.1\" style=\"font-size:90%;\">UniRepLKNet-S </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S4.T2.4.20.19.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" id=\"S4.T2.4.20.19.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.20.19.2\"><span class=\"ltx_text\" id=\"S4.T2.4.20.19.2.1\" style=\"font-size:90%;\">9.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.20.19.3\"><span class=\"ltx_text\" id=\"S4.T2.4.20.19.3.1\" style=\"font-size:90%;\">56</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.20.19.4\"><span class=\"ltx_text\" id=\"S4.T2.4.20.19.4.1\" style=\"font-size:90%;\">83.9</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.21.20\" style=\"background-color:#96FFFB;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\" id=\"S4.T2.4.21.20.1\"><span class=\"ltx_text\" id=\"S4.T2.4.21.20.1.1\" style=\"font-size:90%;background-color:#96FFFB;\">DeBiFormer-B</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T2.4.21.20.2\"><span class=\"ltx_text\" id=\"S4.T2.4.21.20.2.1\" style=\"font-size:90%;background-color:#96FFFB;\">11.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T2.4.21.20.3\"><span class=\"ltx_text\" id=\"S4.T2.4.21.20.3.1\" style=\"font-size:90%;background-color:#96FFFB;\">77</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T2.4.21.20.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.4.21.20.4.1\" style=\"font-size:90%;background-color:#96FFFB;\">84.4</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 2:  Evaluating and comparing different backbones on ImageNet-1K on images with resolution of  224 × 224 224 224 224\\times 224 224 × 224 .",
        "footnotes": [
            "[20] \nHe, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition (2015)\n\n",
            "[42] \nWang, S., Li, B.Z., Khabsa, M., Fang, H., Ma, H.: Linformer: Self-attention with linear complexity (2020)\n\n",
            "[34] \nRen, S., Zhou, D., He, S., Feng, J., Wang, X.: Shunted self-attention via multi-scale token aggregation (2021)\n\n",
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n",
            "[22] \nHou, Q., Lu, C.Z., Cheng, M.M., Feng, J.: Conv2former: A simple transformer-style convnet for visual recognition. arXiv preprint arXiv:2211.11943 (2022)\n\n",
            "[44] \nWang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 568–578 (2021)\n\n",
            "[29] \nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021)\n\n",
            "[14] \nDong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin transformer: A general vision transformer backbone with cross-shaped windows. In: Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR). pp. 12124–12134 (2022)\n\n",
            "[47] \nXia, Z., Pan, X., Song, S., Li, L.E., Huang, G.: Vision transformer with deformable attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4794–4803 (June 2022)\n\n",
            "[45] \nWang, W., Yao, L., Chen, L., Lin, B., Cai, D., He, X., Liu, W.: Crossformer: A versatile vision transformer hinging on cross-scale attention. In: Proceedings of the International Conference on Learning Representations (ICLR) (2022),  https://openreview.net/forum?id=_PHymLIxuI \n",
            "[2] \nChen, C.F., Panda, R., Fan, Q.: Regionvit: Regional-to-local attention for vision transformers. arXiv preprint arXiv:2106.02689 (2021)\n\n",
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[40] \nTu, Z., Talebi, H., Zhang, H., Yang, F., Milanfar, P., Bovik, A., Li, Y.: Maxvit: Multi-axis vision transformer. ECCV (2022)\n\n",
            "[43] \nWang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu, L., Li, H., et al.: Internimage: Exploring large-scale vision foundation models with deformable convolutions. arXiv preprint arXiv:2211.05778 (2022)\n\n",
            "[4] \nChen, Q., Wu, Q., Wang, J., Hu, Q., Hu, T., Ding, E., Cheng, J., Wang, J.: Mixformer: Mixing features across windows and dimensions (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n",
            "[12] \nDing, X., Zhang, Y., Ge, Y., Zhao, S., Song, L., Yue, X., Shan, Y.: Unireplknet: A universal perception large-kernel convnet for audio, video, point cloud, time-series and image recognition. arXiv preprint arXiv:2311.15599 (2023)\n\n",
            "[30] \nLiu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for the 2020s. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 11976–11986 (June 2022)\n\n",
            "[27] \nLiu, S., Chen, T., Chen, X., Chen, X., Xiao, Q., Wu, B., Pechenizkiy, M., Mocanu, D., Wang, Z.: More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity. arXiv preprint arXiv:2207.03620 (2022)\n\n",
            "[7] \nChu, X., Tian, Z., Wang, Y., Zhang, B., Ren, H., Wei, X., Xia, H., Shen, C.: Twins: Revisiting the design of spatial attention in vision transformers. In: NeurIPS 2021 (2021),  https://openreview.net/forum?id=5kTlVBkzSRx \n",
            "[44] \nWang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 568–578 (2021)\n\n",
            "[29] \nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021)\n\n",
            "[49] \nYang, J., Li, C., Zhang, P., Dai, X., Xiao, B., Yuan, L., Gao, J.: Focal attention for long-range interactions in vision transformers. In: Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., Vaughan, J.W. (eds.) Advances in Neural Information Processing Systems. vol. 34, pp. 30008–30022. Curran Associates, Inc. (2021)\n\n",
            "[14] \nDong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin transformer: A general vision transformer backbone with cross-shaped windows. In: Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR). pp. 12124–12134 (2022)\n\n",
            "[34] \nRen, S., Zhou, D., He, S., Feng, J., Wang, X.: Shunted self-attention via multi-scale token aggregation (2021)\n\n",
            "[26] \nLi, K., Wang, Y., Gao, P., Song, G., Liu, Y., Li, H., Qiao, Y.: Uniformer: Unified transformer for efficient spatiotemporal representation learning (2022)\n\n",
            "[51] \nYang, R., Ma, H., Wu, J., Tang, Y., Xiao, X., Zheng, M., Li, X.: Scalablevit: Rethinking the context-oriented generalization of vision transformer (2022)\n\n",
            "[33] \nPan, X., Ye, T., Xia, Z., Song, S., Huang, G.: Slide-transformer: Hierarchical vision transformer with local self-attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 2082–2091 (June 2023)\n\n",
            "[47] \nXia, Z., Pan, X., Song, S., Li, L.E., Huang, G.: Vision transformer with deformable attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4794–4803 (June 2022)\n\n",
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[45] \nWang, W., Yao, L., Chen, L., Lin, B., Cai, D., He, X., Liu, W.: Crossformer: A versatile vision transformer hinging on cross-scale attention. In: Proceedings of the International Conference on Learning Representations (ICLR) (2022),  https://openreview.net/forum?id=_PHymLIxuI \n",
            "[2] \nChen, C.F., Panda, R., Fan, Q.: Regionvit: Regional-to-local attention for vision transformers. arXiv preprint arXiv:2106.02689 (2021)\n\n",
            "[43] \nWang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu, L., Li, H., et al.: Internimage: Exploring large-scale vision foundation models with deformable convolutions. arXiv preprint arXiv:2211.05778 (2022)\n\n",
            "[4] \nChen, Q., Wu, Q., Wang, J., Hu, Q., Hu, T., Ding, E., Cheng, J., Wang, J.: Mixformer: Mixing features across windows and dimensions (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n",
            "[12] \nDing, X., Zhang, Y., Ge, Y., Zhao, S., Song, L., Yue, X., Shan, Y.: Unireplknet: A universal perception large-kernel convnet for audio, video, point cloud, time-series and image recognition. arXiv preprint arXiv:2311.15599 (2023)\n\n"
        ],
        "references": [
            "Settings.\nWe conducted image classification experiments on the ImageNet-1K [35] dataset, following the experimental settings of DeiT [39] for a fair comparison. Specifically, each model was trained for 300 epochs on 8 V100 GPUs with an input size of 224×224224224224\\times 224224 × 224. We used AdamW as the optimizer with a weight decay of 0.050.050.050.05 and used a cosine decay learning rate schedule with an initial learning rate of 0.0010.0010.0010.001, while the first five epochs were used for linear warm-up. The batch size was set to 1024. To avoid overfitting, we used regularization techniques including RandAugment [9] (rand-m9-mstd0.5-inc1), MixUp [54] (prob = 0.8), CutMix [52] (prob = 1.0), Random Erasing (prob = 0.25), and increasing stochastic depth [23] (prob = 0.1/0.2/0.4 for DeBiFormer-T/S/B, respectively).\n\nResults.\nWe report our results in Table 2 showing the top-1 accuracy with similar computational complexities. Our DeBiFormer outperformed the Swin Transformer [29], PVT [44], DeiT [39], DAT[47], and Biformer [56] in all three scales. Without inserting convolutions in Transformer blocks or using overlapped convolutions in patch embeddings, DeBiFormer achieved gains of 0.5pt, 0.1pt and 0.1pt over BiFormer [56] counterparts."
        ]
    },
    "S4.T2.4.1.1.2.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.4.1.1.2.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.4.1.1.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.4.1.1.2.1.1.1\"><span class=\"ltx_text\" id=\"S4.T2.4.1.1.2.1.1.1.1\" style=\"font-size:90%;\">FLOPs</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.1.1.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.4.1.1.2.1.2.1\"><span class=\"ltx_text\" id=\"S4.T2.4.1.1.2.1.2.1.1\" style=\"font-size:90%;\">(G)</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 2:  Evaluating and comparing different backbones on ImageNet-1K on images with resolution of  224 × 224 224 224 224\\times 224 224 × 224 .",
        "footnotes": [
            "[20] \nHe, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition (2015)\n\n",
            "[42] \nWang, S., Li, B.Z., Khabsa, M., Fang, H., Ma, H.: Linformer: Self-attention with linear complexity (2020)\n\n",
            "[34] \nRen, S., Zhou, D., He, S., Feng, J., Wang, X.: Shunted self-attention via multi-scale token aggregation (2021)\n\n",
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n",
            "[22] \nHou, Q., Lu, C.Z., Cheng, M.M., Feng, J.: Conv2former: A simple transformer-style convnet for visual recognition. arXiv preprint arXiv:2211.11943 (2022)\n\n",
            "[44] \nWang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 568–578 (2021)\n\n",
            "[29] \nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021)\n\n",
            "[14] \nDong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin transformer: A general vision transformer backbone with cross-shaped windows. In: Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR). pp. 12124–12134 (2022)\n\n",
            "[47] \nXia, Z., Pan, X., Song, S., Li, L.E., Huang, G.: Vision transformer with deformable attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4794–4803 (June 2022)\n\n",
            "[45] \nWang, W., Yao, L., Chen, L., Lin, B., Cai, D., He, X., Liu, W.: Crossformer: A versatile vision transformer hinging on cross-scale attention. In: Proceedings of the International Conference on Learning Representations (ICLR) (2022),  https://openreview.net/forum?id=_PHymLIxuI \n",
            "[2] \nChen, C.F., Panda, R., Fan, Q.: Regionvit: Regional-to-local attention for vision transformers. arXiv preprint arXiv:2106.02689 (2021)\n\n",
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[40] \nTu, Z., Talebi, H., Zhang, H., Yang, F., Milanfar, P., Bovik, A., Li, Y.: Maxvit: Multi-axis vision transformer. ECCV (2022)\n\n",
            "[43] \nWang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu, L., Li, H., et al.: Internimage: Exploring large-scale vision foundation models with deformable convolutions. arXiv preprint arXiv:2211.05778 (2022)\n\n",
            "[4] \nChen, Q., Wu, Q., Wang, J., Hu, Q., Hu, T., Ding, E., Cheng, J., Wang, J.: Mixformer: Mixing features across windows and dimensions (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n",
            "[12] \nDing, X., Zhang, Y., Ge, Y., Zhao, S., Song, L., Yue, X., Shan, Y.: Unireplknet: A universal perception large-kernel convnet for audio, video, point cloud, time-series and image recognition. arXiv preprint arXiv:2311.15599 (2023)\n\n",
            "[30] \nLiu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for the 2020s. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 11976–11986 (June 2022)\n\n",
            "[27] \nLiu, S., Chen, T., Chen, X., Chen, X., Xiao, Q., Wu, B., Pechenizkiy, M., Mocanu, D., Wang, Z.: More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity. arXiv preprint arXiv:2207.03620 (2022)\n\n",
            "[7] \nChu, X., Tian, Z., Wang, Y., Zhang, B., Ren, H., Wei, X., Xia, H., Shen, C.: Twins: Revisiting the design of spatial attention in vision transformers. In: NeurIPS 2021 (2021),  https://openreview.net/forum?id=5kTlVBkzSRx \n",
            "[44] \nWang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 568–578 (2021)\n\n",
            "[29] \nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021)\n\n",
            "[49] \nYang, J., Li, C., Zhang, P., Dai, X., Xiao, B., Yuan, L., Gao, J.: Focal attention for long-range interactions in vision transformers. In: Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., Vaughan, J.W. (eds.) Advances in Neural Information Processing Systems. vol. 34, pp. 30008–30022. Curran Associates, Inc. (2021)\n\n",
            "[14] \nDong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin transformer: A general vision transformer backbone with cross-shaped windows. In: Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR). pp. 12124–12134 (2022)\n\n",
            "[34] \nRen, S., Zhou, D., He, S., Feng, J., Wang, X.: Shunted self-attention via multi-scale token aggregation (2021)\n\n",
            "[26] \nLi, K., Wang, Y., Gao, P., Song, G., Liu, Y., Li, H., Qiao, Y.: Uniformer: Unified transformer for efficient spatiotemporal representation learning (2022)\n\n",
            "[51] \nYang, R., Ma, H., Wu, J., Tang, Y., Xiao, X., Zheng, M., Li, X.: Scalablevit: Rethinking the context-oriented generalization of vision transformer (2022)\n\n",
            "[33] \nPan, X., Ye, T., Xia, Z., Song, S., Huang, G.: Slide-transformer: Hierarchical vision transformer with local self-attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 2082–2091 (June 2023)\n\n",
            "[47] \nXia, Z., Pan, X., Song, S., Li, L.E., Huang, G.: Vision transformer with deformable attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4794–4803 (June 2022)\n\n",
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[45] \nWang, W., Yao, L., Chen, L., Lin, B., Cai, D., He, X., Liu, W.: Crossformer: A versatile vision transformer hinging on cross-scale attention. In: Proceedings of the International Conference on Learning Representations (ICLR) (2022),  https://openreview.net/forum?id=_PHymLIxuI \n",
            "[2] \nChen, C.F., Panda, R., Fan, Q.: Regionvit: Regional-to-local attention for vision transformers. arXiv preprint arXiv:2106.02689 (2021)\n\n",
            "[43] \nWang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu, L., Li, H., et al.: Internimage: Exploring large-scale vision foundation models with deformable convolutions. arXiv preprint arXiv:2211.05778 (2022)\n\n",
            "[4] \nChen, Q., Wu, Q., Wang, J., Hu, Q., Hu, T., Ding, E., Cheng, J., Wang, J.: Mixformer: Mixing features across windows and dimensions (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n",
            "[12] \nDing, X., Zhang, Y., Ge, Y., Zhao, S., Song, L., Yue, X., Shan, Y.: Unireplknet: A universal perception large-kernel convnet for audio, video, point cloud, time-series and image recognition. arXiv preprint arXiv:2311.15599 (2023)\n\n"
        ],
        "references": [
            "Settings.\nWe conducted image classification experiments on the ImageNet-1K [35] dataset, following the experimental settings of DeiT [39] for a fair comparison. Specifically, each model was trained for 300 epochs on 8 V100 GPUs with an input size of 224×224224224224\\times 224224 × 224. We used AdamW as the optimizer with a weight decay of 0.050.050.050.05 and used a cosine decay learning rate schedule with an initial learning rate of 0.0010.0010.0010.001, while the first five epochs were used for linear warm-up. The batch size was set to 1024. To avoid overfitting, we used regularization techniques including RandAugment [9] (rand-m9-mstd0.5-inc1), MixUp [54] (prob = 0.8), CutMix [52] (prob = 1.0), Random Erasing (prob = 0.25), and increasing stochastic depth [23] (prob = 0.1/0.2/0.4 for DeBiFormer-T/S/B, respectively).\n\nResults.\nWe report our results in Table 2 showing the top-1 accuracy with similar computational complexities. Our DeBiFormer outperformed the Swin Transformer [29], PVT [44], DeiT [39], DAT[47], and Biformer [56] in all three scales. Without inserting convolutions in Transformer blocks or using overlapped convolutions in patch embeddings, DeBiFormer achieved gains of 0.5pt, 0.1pt and 0.1pt over BiFormer [56] counterparts."
        ]
    },
    "S4.T2.4.1.1.3.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.4.1.1.3.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.4.1.1.3.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.4.1.1.3.1.1.1\"><span class=\"ltx_text\" id=\"S4.T2.4.1.1.3.1.1.1.1\" style=\"font-size:90%;\">Params</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.1.1.3.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.4.1.1.3.1.2.1\"><span class=\"ltx_text\" id=\"S4.T2.4.1.1.3.1.2.1.1\" style=\"font-size:90%;\">(M)</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 2:  Evaluating and comparing different backbones on ImageNet-1K on images with resolution of  224 × 224 224 224 224\\times 224 224 × 224 .",
        "footnotes": [
            "[20] \nHe, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition (2015)\n\n",
            "[42] \nWang, S., Li, B.Z., Khabsa, M., Fang, H., Ma, H.: Linformer: Self-attention with linear complexity (2020)\n\n",
            "[34] \nRen, S., Zhou, D., He, S., Feng, J., Wang, X.: Shunted self-attention via multi-scale token aggregation (2021)\n\n",
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n",
            "[22] \nHou, Q., Lu, C.Z., Cheng, M.M., Feng, J.: Conv2former: A simple transformer-style convnet for visual recognition. arXiv preprint arXiv:2211.11943 (2022)\n\n",
            "[44] \nWang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 568–578 (2021)\n\n",
            "[29] \nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021)\n\n",
            "[14] \nDong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin transformer: A general vision transformer backbone with cross-shaped windows. In: Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR). pp. 12124–12134 (2022)\n\n",
            "[47] \nXia, Z., Pan, X., Song, S., Li, L.E., Huang, G.: Vision transformer with deformable attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4794–4803 (June 2022)\n\n",
            "[45] \nWang, W., Yao, L., Chen, L., Lin, B., Cai, D., He, X., Liu, W.: Crossformer: A versatile vision transformer hinging on cross-scale attention. In: Proceedings of the International Conference on Learning Representations (ICLR) (2022),  https://openreview.net/forum?id=_PHymLIxuI \n",
            "[2] \nChen, C.F., Panda, R., Fan, Q.: Regionvit: Regional-to-local attention for vision transformers. arXiv preprint arXiv:2106.02689 (2021)\n\n",
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[40] \nTu, Z., Talebi, H., Zhang, H., Yang, F., Milanfar, P., Bovik, A., Li, Y.: Maxvit: Multi-axis vision transformer. ECCV (2022)\n\n",
            "[43] \nWang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu, L., Li, H., et al.: Internimage: Exploring large-scale vision foundation models with deformable convolutions. arXiv preprint arXiv:2211.05778 (2022)\n\n",
            "[4] \nChen, Q., Wu, Q., Wang, J., Hu, Q., Hu, T., Ding, E., Cheng, J., Wang, J.: Mixformer: Mixing features across windows and dimensions (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n",
            "[12] \nDing, X., Zhang, Y., Ge, Y., Zhao, S., Song, L., Yue, X., Shan, Y.: Unireplknet: A universal perception large-kernel convnet for audio, video, point cloud, time-series and image recognition. arXiv preprint arXiv:2311.15599 (2023)\n\n",
            "[30] \nLiu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for the 2020s. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 11976–11986 (June 2022)\n\n",
            "[27] \nLiu, S., Chen, T., Chen, X., Chen, X., Xiao, Q., Wu, B., Pechenizkiy, M., Mocanu, D., Wang, Z.: More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity. arXiv preprint arXiv:2207.03620 (2022)\n\n",
            "[7] \nChu, X., Tian, Z., Wang, Y., Zhang, B., Ren, H., Wei, X., Xia, H., Shen, C.: Twins: Revisiting the design of spatial attention in vision transformers. In: NeurIPS 2021 (2021),  https://openreview.net/forum?id=5kTlVBkzSRx \n",
            "[44] \nWang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 568–578 (2021)\n\n",
            "[29] \nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021)\n\n",
            "[49] \nYang, J., Li, C., Zhang, P., Dai, X., Xiao, B., Yuan, L., Gao, J.: Focal attention for long-range interactions in vision transformers. In: Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., Vaughan, J.W. (eds.) Advances in Neural Information Processing Systems. vol. 34, pp. 30008–30022. Curran Associates, Inc. (2021)\n\n",
            "[14] \nDong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin transformer: A general vision transformer backbone with cross-shaped windows. In: Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR). pp. 12124–12134 (2022)\n\n",
            "[34] \nRen, S., Zhou, D., He, S., Feng, J., Wang, X.: Shunted self-attention via multi-scale token aggregation (2021)\n\n",
            "[26] \nLi, K., Wang, Y., Gao, P., Song, G., Liu, Y., Li, H., Qiao, Y.: Uniformer: Unified transformer for efficient spatiotemporal representation learning (2022)\n\n",
            "[51] \nYang, R., Ma, H., Wu, J., Tang, Y., Xiao, X., Zheng, M., Li, X.: Scalablevit: Rethinking the context-oriented generalization of vision transformer (2022)\n\n",
            "[33] \nPan, X., Ye, T., Xia, Z., Song, S., Huang, G.: Slide-transformer: Hierarchical vision transformer with local self-attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 2082–2091 (June 2023)\n\n",
            "[47] \nXia, Z., Pan, X., Song, S., Li, L.E., Huang, G.: Vision transformer with deformable attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4794–4803 (June 2022)\n\n",
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[45] \nWang, W., Yao, L., Chen, L., Lin, B., Cai, D., He, X., Liu, W.: Crossformer: A versatile vision transformer hinging on cross-scale attention. In: Proceedings of the International Conference on Learning Representations (ICLR) (2022),  https://openreview.net/forum?id=_PHymLIxuI \n",
            "[2] \nChen, C.F., Panda, R., Fan, Q.: Regionvit: Regional-to-local attention for vision transformers. arXiv preprint arXiv:2106.02689 (2021)\n\n",
            "[43] \nWang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu, L., Li, H., et al.: Internimage: Exploring large-scale vision foundation models with deformable convolutions. arXiv preprint arXiv:2211.05778 (2022)\n\n",
            "[4] \nChen, Q., Wu, Q., Wang, J., Hu, Q., Hu, T., Ding, E., Cheng, J., Wang, J.: Mixformer: Mixing features across windows and dimensions (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n",
            "[12] \nDing, X., Zhang, Y., Ge, Y., Zhao, S., Song, L., Yue, X., Shan, Y.: Unireplknet: A universal perception large-kernel convnet for audio, video, point cloud, time-series and image recognition. arXiv preprint arXiv:2311.15599 (2023)\n\n"
        ],
        "references": [
            "Settings.\nWe conducted image classification experiments on the ImageNet-1K [35] dataset, following the experimental settings of DeiT [39] for a fair comparison. Specifically, each model was trained for 300 epochs on 8 V100 GPUs with an input size of 224×224224224224\\times 224224 × 224. We used AdamW as the optimizer with a weight decay of 0.050.050.050.05 and used a cosine decay learning rate schedule with an initial learning rate of 0.0010.0010.0010.001, while the first five epochs were used for linear warm-up. The batch size was set to 1024. To avoid overfitting, we used regularization techniques including RandAugment [9] (rand-m9-mstd0.5-inc1), MixUp [54] (prob = 0.8), CutMix [52] (prob = 1.0), Random Erasing (prob = 0.25), and increasing stochastic depth [23] (prob = 0.1/0.2/0.4 for DeBiFormer-T/S/B, respectively).\n\nResults.\nWe report our results in Table 2 showing the top-1 accuracy with similar computational complexities. Our DeBiFormer outperformed the Swin Transformer [29], PVT [44], DeiT [39], DAT[47], and Biformer [56] in all three scales. Without inserting convolutions in Transformer blocks or using overlapped convolutions in patch embeddings, DeBiFormer achieved gains of 0.5pt, 0.1pt and 0.1pt over BiFormer [56] counterparts."
        ]
    },
    "S4.T2.4.1.1.4.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.4.1.1.4.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.4.1.1.4.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.4.1.1.4.1.1.1\"><span class=\"ltx_text\" id=\"S4.T2.4.1.1.4.1.1.1.1\" style=\"font-size:90%;\">Top-1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.1.1.4.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T2.4.1.1.4.1.2.1\"><span class=\"ltx_text\" id=\"S4.T2.4.1.1.4.1.2.1.1\" style=\"font-size:90%;\">(%)</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 2:  Evaluating and comparing different backbones on ImageNet-1K on images with resolution of  224 × 224 224 224 224\\times 224 224 × 224 .",
        "footnotes": [
            "[20] \nHe, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition (2015)\n\n",
            "[42] \nWang, S., Li, B.Z., Khabsa, M., Fang, H., Ma, H.: Linformer: Self-attention with linear complexity (2020)\n\n",
            "[34] \nRen, S., Zhou, D., He, S., Feng, J., Wang, X.: Shunted self-attention via multi-scale token aggregation (2021)\n\n",
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n",
            "[22] \nHou, Q., Lu, C.Z., Cheng, M.M., Feng, J.: Conv2former: A simple transformer-style convnet for visual recognition. arXiv preprint arXiv:2211.11943 (2022)\n\n",
            "[44] \nWang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 568–578 (2021)\n\n",
            "[29] \nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021)\n\n",
            "[14] \nDong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin transformer: A general vision transformer backbone with cross-shaped windows. In: Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR). pp. 12124–12134 (2022)\n\n",
            "[47] \nXia, Z., Pan, X., Song, S., Li, L.E., Huang, G.: Vision transformer with deformable attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4794–4803 (June 2022)\n\n",
            "[45] \nWang, W., Yao, L., Chen, L., Lin, B., Cai, D., He, X., Liu, W.: Crossformer: A versatile vision transformer hinging on cross-scale attention. In: Proceedings of the International Conference on Learning Representations (ICLR) (2022),  https://openreview.net/forum?id=_PHymLIxuI \n",
            "[2] \nChen, C.F., Panda, R., Fan, Q.: Regionvit: Regional-to-local attention for vision transformers. arXiv preprint arXiv:2106.02689 (2021)\n\n",
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[40] \nTu, Z., Talebi, H., Zhang, H., Yang, F., Milanfar, P., Bovik, A., Li, Y.: Maxvit: Multi-axis vision transformer. ECCV (2022)\n\n",
            "[43] \nWang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu, L., Li, H., et al.: Internimage: Exploring large-scale vision foundation models with deformable convolutions. arXiv preprint arXiv:2211.05778 (2022)\n\n",
            "[4] \nChen, Q., Wu, Q., Wang, J., Hu, Q., Hu, T., Ding, E., Cheng, J., Wang, J.: Mixformer: Mixing features across windows and dimensions (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n",
            "[12] \nDing, X., Zhang, Y., Ge, Y., Zhao, S., Song, L., Yue, X., Shan, Y.: Unireplknet: A universal perception large-kernel convnet for audio, video, point cloud, time-series and image recognition. arXiv preprint arXiv:2311.15599 (2023)\n\n",
            "[30] \nLiu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for the 2020s. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 11976–11986 (June 2022)\n\n",
            "[27] \nLiu, S., Chen, T., Chen, X., Chen, X., Xiao, Q., Wu, B., Pechenizkiy, M., Mocanu, D., Wang, Z.: More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity. arXiv preprint arXiv:2207.03620 (2022)\n\n",
            "[7] \nChu, X., Tian, Z., Wang, Y., Zhang, B., Ren, H., Wei, X., Xia, H., Shen, C.: Twins: Revisiting the design of spatial attention in vision transformers. In: NeurIPS 2021 (2021),  https://openreview.net/forum?id=5kTlVBkzSRx \n",
            "[44] \nWang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 568–578 (2021)\n\n",
            "[29] \nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021)\n\n",
            "[49] \nYang, J., Li, C., Zhang, P., Dai, X., Xiao, B., Yuan, L., Gao, J.: Focal attention for long-range interactions in vision transformers. In: Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., Vaughan, J.W. (eds.) Advances in Neural Information Processing Systems. vol. 34, pp. 30008–30022. Curran Associates, Inc. (2021)\n\n",
            "[14] \nDong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin transformer: A general vision transformer backbone with cross-shaped windows. In: Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR). pp. 12124–12134 (2022)\n\n",
            "[34] \nRen, S., Zhou, D., He, S., Feng, J., Wang, X.: Shunted self-attention via multi-scale token aggregation (2021)\n\n",
            "[26] \nLi, K., Wang, Y., Gao, P., Song, G., Liu, Y., Li, H., Qiao, Y.: Uniformer: Unified transformer for efficient spatiotemporal representation learning (2022)\n\n",
            "[51] \nYang, R., Ma, H., Wu, J., Tang, Y., Xiao, X., Zheng, M., Li, X.: Scalablevit: Rethinking the context-oriented generalization of vision transformer (2022)\n\n",
            "[33] \nPan, X., Ye, T., Xia, Z., Song, S., Huang, G.: Slide-transformer: Hierarchical vision transformer with local self-attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 2082–2091 (June 2023)\n\n",
            "[47] \nXia, Z., Pan, X., Song, S., Li, L.E., Huang, G.: Vision transformer with deformable attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4794–4803 (June 2022)\n\n",
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[45] \nWang, W., Yao, L., Chen, L., Lin, B., Cai, D., He, X., Liu, W.: Crossformer: A versatile vision transformer hinging on cross-scale attention. In: Proceedings of the International Conference on Learning Representations (ICLR) (2022),  https://openreview.net/forum?id=_PHymLIxuI \n",
            "[2] \nChen, C.F., Panda, R., Fan, Q.: Regionvit: Regional-to-local attention for vision transformers. arXiv preprint arXiv:2106.02689 (2021)\n\n",
            "[43] \nWang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu, L., Li, H., et al.: Internimage: Exploring large-scale vision foundation models with deformable convolutions. arXiv preprint arXiv:2211.05778 (2022)\n\n",
            "[4] \nChen, Q., Wu, Q., Wang, J., Hu, Q., Hu, T., Ding, E., Cheng, J., Wang, J.: Mixformer: Mixing features across windows and dimensions (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n",
            "[12] \nDing, X., Zhang, Y., Ge, Y., Zhao, S., Song, L., Yue, X., Shan, Y.: Unireplknet: A universal perception large-kernel convnet for audio, video, point cloud, time-series and image recognition. arXiv preprint arXiv:2311.15599 (2023)\n\n"
        ],
        "references": [
            "Settings.\nWe conducted image classification experiments on the ImageNet-1K [35] dataset, following the experimental settings of DeiT [39] for a fair comparison. Specifically, each model was trained for 300 epochs on 8 V100 GPUs with an input size of 224×224224224224\\times 224224 × 224. We used AdamW as the optimizer with a weight decay of 0.050.050.050.05 and used a cosine decay learning rate schedule with an initial learning rate of 0.0010.0010.0010.001, while the first five epochs were used for linear warm-up. The batch size was set to 1024. To avoid overfitting, we used regularization techniques including RandAugment [9] (rand-m9-mstd0.5-inc1), MixUp [54] (prob = 0.8), CutMix [52] (prob = 1.0), Random Erasing (prob = 0.25), and increasing stochastic depth [23] (prob = 0.1/0.2/0.4 for DeBiFormer-T/S/B, respectively).\n\nResults.\nWe report our results in Table 2 showing the top-1 accuracy with similar computational complexities. Our DeBiFormer outperformed the Swin Transformer [29], PVT [44], DeiT [39], DAT[47], and Biformer [56] in all three scales. Without inserting convolutions in Transformer blocks or using overlapped convolutions in patch embeddings, DeBiFormer achieved gains of 0.5pt, 0.1pt and 0.1pt over BiFormer [56] counterparts."
        ]
    },
    "S4.T3.1": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T3.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T3.1.1.1.1.1\">Backbone</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.1.1.2\">Semantic-FPN</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.1.3\">UperNet</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.2.2.1\">mIoU(%)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.2.2.2\">mIoU(%)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.3.3.1\">Swin-T <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib29\" title=\"\">29</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.3.3.2\">41.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.3.3.3\">44.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.4.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.4.4.1\">DAT-T <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib47\" title=\"\">47</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.4.4.2\">42.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.4.4.3\">45.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.5.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.5.5.1\">CSWin-T <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib14\" title=\"\">14</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.5.5.2\">48.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.5.5.3\">49.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.6.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.6.6.1\">RegionViT-S+ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib2\" title=\"\">2</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.6.6.2\">45.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.6.6.3\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.7.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.7.7.1\">InternImage-T <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib43\" title=\"\">43</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.7.7.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.7.7.3\">47.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.8.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.8.8.1\">CrossFormer-S <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib45\" title=\"\">45</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.8.8.2\">46.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.8.8.3\">47.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.9.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.9.9.1\">Uniformer-S <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib26\" title=\"\">26</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.9.9.2\">46.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.9.9.3\">47.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.10.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.10.10.1\">Shunted-S <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib34\" title=\"\">34</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.10.10.2\">48.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.10.10.3\">48.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.11.11\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.11.11.1\">BiFormer-S <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib56\" title=\"\">56</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.11.11.2\">48.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.11.11.3\">49.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.12.12\" style=\"background-color:#96FFFB;\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.12.12.1\"><span class=\"ltx_text\" id=\"S4.T3.1.12.12.1.1\" style=\"background-color:#96FFFB;\">DeBiFormer-S</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.12.12.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.12.12.2.1\" style=\"background-color:#96FFFB;\">49.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.12.12.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.12.12.3.1\" style=\"background-color:#96FFFB;\">50.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.13.13\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.13.13.1\">Swin-S <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib29\" title=\"\">29</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T3.1.13.13.2\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.13.13.3\">47.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.14.14\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.14.14.1\">DAT-S <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib47\" title=\"\">47</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.14.14.2\">46.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.14.14.3\">48.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.15.15\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.15.15.1\">CSWin-S <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib14\" title=\"\">14</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.15.15.2\">49.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.15.15.3\">50.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.16.16\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.16.16.1\">RegionViT-B+ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib2\" title=\"\">2</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.16.16.2\">47.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.16.16.3\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.17.17\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.17.17.1\">InternImage-S <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib43\" title=\"\">43</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.17.17.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.17.17.3\">50.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.18.18\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.18.18.1\">CrossFormer-B <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib45\" title=\"\">45</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.18.18.2\">47.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.18.18.3\">49.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.19.19\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.19.19.1\">Uniformer-B <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib26\" title=\"\">26</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.19.19.2\">48.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.19.19.3\">50.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.20.20\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.20.20.1\">BiFormer-B <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib56\" title=\"\">56</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T3.1.20.20.2\">49.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.20.20.3\">51.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.21.21\" style=\"background-color:#96FFFB;\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T3.1.21.21.1\"><span class=\"ltx_text\" id=\"S4.T3.1.21.21.1.1\" style=\"background-color:#96FFFB;\">DeBiFormer-B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T3.1.21.21.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.21.21.2.1\" style=\"background-color:#96FFFB;\">50.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T3.1.21.21.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.21.21.3.1\" style=\"background-color:#96FFFB;\">51.4</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 3:  Evaluating DeBiFormer on semantic segmentation with two segmentation heads (Semantic FPN and UpperNet) on ADE20K dataset.",
        "footnotes": [
            "[29] \nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021)\n\n",
            "[47] \nXia, Z., Pan, X., Song, S., Li, L.E., Huang, G.: Vision transformer with deformable attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4794–4803 (June 2022)\n\n",
            "[14] \nDong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin transformer: A general vision transformer backbone with cross-shaped windows. In: Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR). pp. 12124–12134 (2022)\n\n",
            "[2] \nChen, C.F., Panda, R., Fan, Q.: Regionvit: Regional-to-local attention for vision transformers. arXiv preprint arXiv:2106.02689 (2021)\n\n",
            "[43] \nWang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu, L., Li, H., et al.: Internimage: Exploring large-scale vision foundation models with deformable convolutions. arXiv preprint arXiv:2211.05778 (2022)\n\n",
            "[45] \nWang, W., Yao, L., Chen, L., Lin, B., Cai, D., He, X., Liu, W.: Crossformer: A versatile vision transformer hinging on cross-scale attention. In: Proceedings of the International Conference on Learning Representations (ICLR) (2022),  https://openreview.net/forum?id=_PHymLIxuI \n",
            "[26] \nLi, K., Wang, Y., Gao, P., Song, G., Liu, Y., Li, H., Qiao, Y.: Uniformer: Unified transformer for efficient spatiotemporal representation learning (2022)\n\n",
            "[34] \nRen, S., Zhou, D., He, S., Feng, J., Wang, X.: Shunted self-attention via multi-scale token aggregation (2021)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n",
            "[29] \nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021)\n\n",
            "[47] \nXia, Z., Pan, X., Song, S., Li, L.E., Huang, G.: Vision transformer with deformable attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4794–4803 (June 2022)\n\n",
            "[14] \nDong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin transformer: A general vision transformer backbone with cross-shaped windows. In: Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR). pp. 12124–12134 (2022)\n\n",
            "[2] \nChen, C.F., Panda, R., Fan, Q.: Regionvit: Regional-to-local attention for vision transformers. arXiv preprint arXiv:2106.02689 (2021)\n\n",
            "[43] \nWang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu, L., Li, H., et al.: Internimage: Exploring large-scale vision foundation models with deformable convolutions. arXiv preprint arXiv:2211.05778 (2022)\n\n",
            "[45] \nWang, W., Yao, L., Chen, L., Lin, B., Cai, D., He, X., Liu, W.: Crossformer: A versatile vision transformer hinging on cross-scale attention. In: Proceedings of the International Conference on Learning Representations (ICLR) (2022),  https://openreview.net/forum?id=_PHymLIxuI \n",
            "[26] \nLi, K., Wang, Y., Gao, P., Song, G., Liu, Y., Li, H., Qiao, Y.: Uniformer: Unified transformer for efficient spatiotemporal representation learning (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n"
        ],
        "references": []
    },
    "S4.T4.12.12": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T4.12.12\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T4.12.12.13.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T4.12.12.13.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T4.12.12.13.1.1.1\">Backbone</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"6\" id=\"S4.T4.12.12.13.1.2\">RetinaNet 1&#215; schedule</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"6\" id=\"S4.T4.12.12.13.1.3\">Mask R-CNN 1&#215; schedule</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.12.12.12\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.1.1.1.1\"><math alttext=\"mAP\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.1.1.1.1.m1.1\"><semantics id=\"S4.T4.1.1.1.1.m1.1a\"><mrow id=\"S4.T4.1.1.1.1.m1.1.1\" xref=\"S4.T4.1.1.1.1.m1.1.1.cmml\"><mi id=\"S4.T4.1.1.1.1.m1.1.1.2\" xref=\"S4.T4.1.1.1.1.m1.1.1.2.cmml\">m</mi><mo id=\"S4.T4.1.1.1.1.m1.1.1.1\" xref=\"S4.T4.1.1.1.1.m1.1.1.1.cmml\">&#8290;</mo><mi id=\"S4.T4.1.1.1.1.m1.1.1.3\" xref=\"S4.T4.1.1.1.1.m1.1.1.3.cmml\">A</mi><mo id=\"S4.T4.1.1.1.1.m1.1.1.1a\" xref=\"S4.T4.1.1.1.1.m1.1.1.1.cmml\">&#8290;</mo><mi id=\"S4.T4.1.1.1.1.m1.1.1.4\" xref=\"S4.T4.1.1.1.1.m1.1.1.4.cmml\">P</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.1.1.1.1.m1.1b\"><apply id=\"S4.T4.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T4.1.1.1.1.m1.1.1\"><times id=\"S4.T4.1.1.1.1.m1.1.1.1.cmml\" xref=\"S4.T4.1.1.1.1.m1.1.1.1\"/><ci id=\"S4.T4.1.1.1.1.m1.1.1.2.cmml\" xref=\"S4.T4.1.1.1.1.m1.1.1.2\">&#119898;</ci><ci id=\"S4.T4.1.1.1.1.m1.1.1.3.cmml\" xref=\"S4.T4.1.1.1.1.m1.1.1.3\">&#119860;</ci><ci id=\"S4.T4.1.1.1.1.m1.1.1.4.cmml\" xref=\"S4.T4.1.1.1.1.m1.1.1.4\">&#119875;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.1.1.1.1.m1.1c\">mAP</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T4.1.1.1.1.m1.1d\">italic_m italic_A italic_P</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.2.2.2\"><math alttext=\"AP_{50}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.2.2.2.2.m1.1\"><semantics id=\"S4.T4.2.2.2.2.m1.1a\"><mrow id=\"S4.T4.2.2.2.2.m1.1.1\" xref=\"S4.T4.2.2.2.2.m1.1.1.cmml\"><mi id=\"S4.T4.2.2.2.2.m1.1.1.2\" xref=\"S4.T4.2.2.2.2.m1.1.1.2.cmml\">A</mi><mo id=\"S4.T4.2.2.2.2.m1.1.1.1\" xref=\"S4.T4.2.2.2.2.m1.1.1.1.cmml\">&#8290;</mo><msub id=\"S4.T4.2.2.2.2.m1.1.1.3\" xref=\"S4.T4.2.2.2.2.m1.1.1.3.cmml\"><mi id=\"S4.T4.2.2.2.2.m1.1.1.3.2\" xref=\"S4.T4.2.2.2.2.m1.1.1.3.2.cmml\">P</mi><mn id=\"S4.T4.2.2.2.2.m1.1.1.3.3\" xref=\"S4.T4.2.2.2.2.m1.1.1.3.3.cmml\">50</mn></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.2.2.2.2.m1.1b\"><apply id=\"S4.T4.2.2.2.2.m1.1.1.cmml\" xref=\"S4.T4.2.2.2.2.m1.1.1\"><times id=\"S4.T4.2.2.2.2.m1.1.1.1.cmml\" xref=\"S4.T4.2.2.2.2.m1.1.1.1\"/><ci id=\"S4.T4.2.2.2.2.m1.1.1.2.cmml\" xref=\"S4.T4.2.2.2.2.m1.1.1.2\">&#119860;</ci><apply id=\"S4.T4.2.2.2.2.m1.1.1.3.cmml\" xref=\"S4.T4.2.2.2.2.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.T4.2.2.2.2.m1.1.1.3.1.cmml\" xref=\"S4.T4.2.2.2.2.m1.1.1.3\">subscript</csymbol><ci id=\"S4.T4.2.2.2.2.m1.1.1.3.2.cmml\" xref=\"S4.T4.2.2.2.2.m1.1.1.3.2\">&#119875;</ci><cn id=\"S4.T4.2.2.2.2.m1.1.1.3.3.cmml\" type=\"integer\" xref=\"S4.T4.2.2.2.2.m1.1.1.3.3\">50</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.2.2.2.2.m1.1c\">AP_{50}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T4.2.2.2.2.m1.1d\">italic_A italic_P start_POSTSUBSCRIPT 50 end_POSTSUBSCRIPT</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.3.3.3.3\"><math alttext=\"AP_{75}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.3.3.3.3.m1.1\"><semantics id=\"S4.T4.3.3.3.3.m1.1a\"><mrow id=\"S4.T4.3.3.3.3.m1.1.1\" xref=\"S4.T4.3.3.3.3.m1.1.1.cmml\"><mi id=\"S4.T4.3.3.3.3.m1.1.1.2\" xref=\"S4.T4.3.3.3.3.m1.1.1.2.cmml\">A</mi><mo id=\"S4.T4.3.3.3.3.m1.1.1.1\" xref=\"S4.T4.3.3.3.3.m1.1.1.1.cmml\">&#8290;</mo><msub id=\"S4.T4.3.3.3.3.m1.1.1.3\" xref=\"S4.T4.3.3.3.3.m1.1.1.3.cmml\"><mi id=\"S4.T4.3.3.3.3.m1.1.1.3.2\" xref=\"S4.T4.3.3.3.3.m1.1.1.3.2.cmml\">P</mi><mn id=\"S4.T4.3.3.3.3.m1.1.1.3.3\" xref=\"S4.T4.3.3.3.3.m1.1.1.3.3.cmml\">75</mn></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.3.3.3.3.m1.1b\"><apply id=\"S4.T4.3.3.3.3.m1.1.1.cmml\" xref=\"S4.T4.3.3.3.3.m1.1.1\"><times id=\"S4.T4.3.3.3.3.m1.1.1.1.cmml\" xref=\"S4.T4.3.3.3.3.m1.1.1.1\"/><ci id=\"S4.T4.3.3.3.3.m1.1.1.2.cmml\" xref=\"S4.T4.3.3.3.3.m1.1.1.2\">&#119860;</ci><apply id=\"S4.T4.3.3.3.3.m1.1.1.3.cmml\" xref=\"S4.T4.3.3.3.3.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.T4.3.3.3.3.m1.1.1.3.1.cmml\" xref=\"S4.T4.3.3.3.3.m1.1.1.3\">subscript</csymbol><ci id=\"S4.T4.3.3.3.3.m1.1.1.3.2.cmml\" xref=\"S4.T4.3.3.3.3.m1.1.1.3.2\">&#119875;</ci><cn id=\"S4.T4.3.3.3.3.m1.1.1.3.3.cmml\" type=\"integer\" xref=\"S4.T4.3.3.3.3.m1.1.1.3.3\">75</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.3.3.3.3.m1.1c\">AP_{75}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T4.3.3.3.3.m1.1d\">italic_A italic_P start_POSTSUBSCRIPT 75 end_POSTSUBSCRIPT</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.4.4.4.4\"><math alttext=\"AP_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.4.4.4.4.m1.1\"><semantics id=\"S4.T4.4.4.4.4.m1.1a\"><mrow id=\"S4.T4.4.4.4.4.m1.1.1\" xref=\"S4.T4.4.4.4.4.m1.1.1.cmml\"><mi id=\"S4.T4.4.4.4.4.m1.1.1.2\" xref=\"S4.T4.4.4.4.4.m1.1.1.2.cmml\">A</mi><mo id=\"S4.T4.4.4.4.4.m1.1.1.1\" xref=\"S4.T4.4.4.4.4.m1.1.1.1.cmml\">&#8290;</mo><msub id=\"S4.T4.4.4.4.4.m1.1.1.3\" xref=\"S4.T4.4.4.4.4.m1.1.1.3.cmml\"><mi id=\"S4.T4.4.4.4.4.m1.1.1.3.2\" xref=\"S4.T4.4.4.4.4.m1.1.1.3.2.cmml\">P</mi><mi id=\"S4.T4.4.4.4.4.m1.1.1.3.3\" xref=\"S4.T4.4.4.4.4.m1.1.1.3.3.cmml\">S</mi></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.4.4.4.4.m1.1b\"><apply id=\"S4.T4.4.4.4.4.m1.1.1.cmml\" xref=\"S4.T4.4.4.4.4.m1.1.1\"><times id=\"S4.T4.4.4.4.4.m1.1.1.1.cmml\" xref=\"S4.T4.4.4.4.4.m1.1.1.1\"/><ci id=\"S4.T4.4.4.4.4.m1.1.1.2.cmml\" xref=\"S4.T4.4.4.4.4.m1.1.1.2\">&#119860;</ci><apply id=\"S4.T4.4.4.4.4.m1.1.1.3.cmml\" xref=\"S4.T4.4.4.4.4.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.T4.4.4.4.4.m1.1.1.3.1.cmml\" xref=\"S4.T4.4.4.4.4.m1.1.1.3\">subscript</csymbol><ci id=\"S4.T4.4.4.4.4.m1.1.1.3.2.cmml\" xref=\"S4.T4.4.4.4.4.m1.1.1.3.2\">&#119875;</ci><ci id=\"S4.T4.4.4.4.4.m1.1.1.3.3.cmml\" xref=\"S4.T4.4.4.4.4.m1.1.1.3.3\">&#119878;</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.4.4.4.4.m1.1c\">AP_{S}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T4.4.4.4.4.m1.1d\">italic_A italic_P start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.5.5.5.5\"><math alttext=\"AP_{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.5.5.5.5.m1.1\"><semantics id=\"S4.T4.5.5.5.5.m1.1a\"><mrow id=\"S4.T4.5.5.5.5.m1.1.1\" xref=\"S4.T4.5.5.5.5.m1.1.1.cmml\"><mi id=\"S4.T4.5.5.5.5.m1.1.1.2\" xref=\"S4.T4.5.5.5.5.m1.1.1.2.cmml\">A</mi><mo id=\"S4.T4.5.5.5.5.m1.1.1.1\" xref=\"S4.T4.5.5.5.5.m1.1.1.1.cmml\">&#8290;</mo><msub id=\"S4.T4.5.5.5.5.m1.1.1.3\" xref=\"S4.T4.5.5.5.5.m1.1.1.3.cmml\"><mi id=\"S4.T4.5.5.5.5.m1.1.1.3.2\" xref=\"S4.T4.5.5.5.5.m1.1.1.3.2.cmml\">P</mi><mi id=\"S4.T4.5.5.5.5.m1.1.1.3.3\" xref=\"S4.T4.5.5.5.5.m1.1.1.3.3.cmml\">M</mi></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.5.5.5.5.m1.1b\"><apply id=\"S4.T4.5.5.5.5.m1.1.1.cmml\" xref=\"S4.T4.5.5.5.5.m1.1.1\"><times id=\"S4.T4.5.5.5.5.m1.1.1.1.cmml\" xref=\"S4.T4.5.5.5.5.m1.1.1.1\"/><ci id=\"S4.T4.5.5.5.5.m1.1.1.2.cmml\" xref=\"S4.T4.5.5.5.5.m1.1.1.2\">&#119860;</ci><apply id=\"S4.T4.5.5.5.5.m1.1.1.3.cmml\" xref=\"S4.T4.5.5.5.5.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.T4.5.5.5.5.m1.1.1.3.1.cmml\" xref=\"S4.T4.5.5.5.5.m1.1.1.3\">subscript</csymbol><ci id=\"S4.T4.5.5.5.5.m1.1.1.3.2.cmml\" xref=\"S4.T4.5.5.5.5.m1.1.1.3.2\">&#119875;</ci><ci id=\"S4.T4.5.5.5.5.m1.1.1.3.3.cmml\" xref=\"S4.T4.5.5.5.5.m1.1.1.3.3\">&#119872;</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.5.5.5.5.m1.1c\">AP_{M}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T4.5.5.5.5.m1.1d\">italic_A italic_P start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.6.6.6.6\"><math alttext=\"AP_{L}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.6.6.6.6.m1.1\"><semantics id=\"S4.T4.6.6.6.6.m1.1a\"><mrow id=\"S4.T4.6.6.6.6.m1.1.1\" xref=\"S4.T4.6.6.6.6.m1.1.1.cmml\"><mi id=\"S4.T4.6.6.6.6.m1.1.1.2\" xref=\"S4.T4.6.6.6.6.m1.1.1.2.cmml\">A</mi><mo id=\"S4.T4.6.6.6.6.m1.1.1.1\" xref=\"S4.T4.6.6.6.6.m1.1.1.1.cmml\">&#8290;</mo><msub id=\"S4.T4.6.6.6.6.m1.1.1.3\" xref=\"S4.T4.6.6.6.6.m1.1.1.3.cmml\"><mi id=\"S4.T4.6.6.6.6.m1.1.1.3.2\" xref=\"S4.T4.6.6.6.6.m1.1.1.3.2.cmml\">P</mi><mi id=\"S4.T4.6.6.6.6.m1.1.1.3.3\" xref=\"S4.T4.6.6.6.6.m1.1.1.3.3.cmml\">L</mi></msub></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.6.6.6.6.m1.1b\"><apply id=\"S4.T4.6.6.6.6.m1.1.1.cmml\" xref=\"S4.T4.6.6.6.6.m1.1.1\"><times id=\"S4.T4.6.6.6.6.m1.1.1.1.cmml\" xref=\"S4.T4.6.6.6.6.m1.1.1.1\"/><ci id=\"S4.T4.6.6.6.6.m1.1.1.2.cmml\" xref=\"S4.T4.6.6.6.6.m1.1.1.2\">&#119860;</ci><apply id=\"S4.T4.6.6.6.6.m1.1.1.3.cmml\" xref=\"S4.T4.6.6.6.6.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.T4.6.6.6.6.m1.1.1.3.1.cmml\" xref=\"S4.T4.6.6.6.6.m1.1.1.3\">subscript</csymbol><ci id=\"S4.T4.6.6.6.6.m1.1.1.3.2.cmml\" xref=\"S4.T4.6.6.6.6.m1.1.1.3.2\">&#119875;</ci><ci id=\"S4.T4.6.6.6.6.m1.1.1.3.3.cmml\" xref=\"S4.T4.6.6.6.6.m1.1.1.3.3\">&#119871;</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.6.6.6.6.m1.1c\">AP_{L}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T4.6.6.6.6.m1.1d\">italic_A italic_P start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.7.7.7.7\"><math alttext=\"mAP^{b}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.7.7.7.7.m1.1\"><semantics id=\"S4.T4.7.7.7.7.m1.1a\"><mrow id=\"S4.T4.7.7.7.7.m1.1.1\" xref=\"S4.T4.7.7.7.7.m1.1.1.cmml\"><mi id=\"S4.T4.7.7.7.7.m1.1.1.2\" xref=\"S4.T4.7.7.7.7.m1.1.1.2.cmml\">m</mi><mo id=\"S4.T4.7.7.7.7.m1.1.1.1\" xref=\"S4.T4.7.7.7.7.m1.1.1.1.cmml\">&#8290;</mo><mi id=\"S4.T4.7.7.7.7.m1.1.1.3\" xref=\"S4.T4.7.7.7.7.m1.1.1.3.cmml\">A</mi><mo id=\"S4.T4.7.7.7.7.m1.1.1.1a\" xref=\"S4.T4.7.7.7.7.m1.1.1.1.cmml\">&#8290;</mo><msup id=\"S4.T4.7.7.7.7.m1.1.1.4\" xref=\"S4.T4.7.7.7.7.m1.1.1.4.cmml\"><mi id=\"S4.T4.7.7.7.7.m1.1.1.4.2\" xref=\"S4.T4.7.7.7.7.m1.1.1.4.2.cmml\">P</mi><mi id=\"S4.T4.7.7.7.7.m1.1.1.4.3\" xref=\"S4.T4.7.7.7.7.m1.1.1.4.3.cmml\">b</mi></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.7.7.7.7.m1.1b\"><apply id=\"S4.T4.7.7.7.7.m1.1.1.cmml\" xref=\"S4.T4.7.7.7.7.m1.1.1\"><times id=\"S4.T4.7.7.7.7.m1.1.1.1.cmml\" xref=\"S4.T4.7.7.7.7.m1.1.1.1\"/><ci id=\"S4.T4.7.7.7.7.m1.1.1.2.cmml\" xref=\"S4.T4.7.7.7.7.m1.1.1.2\">&#119898;</ci><ci id=\"S4.T4.7.7.7.7.m1.1.1.3.cmml\" xref=\"S4.T4.7.7.7.7.m1.1.1.3\">&#119860;</ci><apply id=\"S4.T4.7.7.7.7.m1.1.1.4.cmml\" xref=\"S4.T4.7.7.7.7.m1.1.1.4\"><csymbol cd=\"ambiguous\" id=\"S4.T4.7.7.7.7.m1.1.1.4.1.cmml\" xref=\"S4.T4.7.7.7.7.m1.1.1.4\">superscript</csymbol><ci id=\"S4.T4.7.7.7.7.m1.1.1.4.2.cmml\" xref=\"S4.T4.7.7.7.7.m1.1.1.4.2\">&#119875;</ci><ci id=\"S4.T4.7.7.7.7.m1.1.1.4.3.cmml\" xref=\"S4.T4.7.7.7.7.m1.1.1.4.3\">&#119887;</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.7.7.7.7.m1.1c\">mAP^{b}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T4.7.7.7.7.m1.1d\">italic_m italic_A italic_P start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.8.8.8.8\"><math alttext=\"AP^{b}_{50}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.8.8.8.8.m1.1\"><semantics id=\"S4.T4.8.8.8.8.m1.1a\"><mrow id=\"S4.T4.8.8.8.8.m1.1.1\" xref=\"S4.T4.8.8.8.8.m1.1.1.cmml\"><mi id=\"S4.T4.8.8.8.8.m1.1.1.2\" xref=\"S4.T4.8.8.8.8.m1.1.1.2.cmml\">A</mi><mo id=\"S4.T4.8.8.8.8.m1.1.1.1\" xref=\"S4.T4.8.8.8.8.m1.1.1.1.cmml\">&#8290;</mo><msubsup id=\"S4.T4.8.8.8.8.m1.1.1.3\" xref=\"S4.T4.8.8.8.8.m1.1.1.3.cmml\"><mi id=\"S4.T4.8.8.8.8.m1.1.1.3.2.2\" xref=\"S4.T4.8.8.8.8.m1.1.1.3.2.2.cmml\">P</mi><mn id=\"S4.T4.8.8.8.8.m1.1.1.3.3\" xref=\"S4.T4.8.8.8.8.m1.1.1.3.3.cmml\">50</mn><mi id=\"S4.T4.8.8.8.8.m1.1.1.3.2.3\" xref=\"S4.T4.8.8.8.8.m1.1.1.3.2.3.cmml\">b</mi></msubsup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.8.8.8.8.m1.1b\"><apply id=\"S4.T4.8.8.8.8.m1.1.1.cmml\" xref=\"S4.T4.8.8.8.8.m1.1.1\"><times id=\"S4.T4.8.8.8.8.m1.1.1.1.cmml\" xref=\"S4.T4.8.8.8.8.m1.1.1.1\"/><ci id=\"S4.T4.8.8.8.8.m1.1.1.2.cmml\" xref=\"S4.T4.8.8.8.8.m1.1.1.2\">&#119860;</ci><apply id=\"S4.T4.8.8.8.8.m1.1.1.3.cmml\" xref=\"S4.T4.8.8.8.8.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.T4.8.8.8.8.m1.1.1.3.1.cmml\" xref=\"S4.T4.8.8.8.8.m1.1.1.3\">subscript</csymbol><apply id=\"S4.T4.8.8.8.8.m1.1.1.3.2.cmml\" xref=\"S4.T4.8.8.8.8.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.T4.8.8.8.8.m1.1.1.3.2.1.cmml\" xref=\"S4.T4.8.8.8.8.m1.1.1.3\">superscript</csymbol><ci id=\"S4.T4.8.8.8.8.m1.1.1.3.2.2.cmml\" xref=\"S4.T4.8.8.8.8.m1.1.1.3.2.2\">&#119875;</ci><ci id=\"S4.T4.8.8.8.8.m1.1.1.3.2.3.cmml\" xref=\"S4.T4.8.8.8.8.m1.1.1.3.2.3\">&#119887;</ci></apply><cn id=\"S4.T4.8.8.8.8.m1.1.1.3.3.cmml\" type=\"integer\" xref=\"S4.T4.8.8.8.8.m1.1.1.3.3\">50</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.8.8.8.8.m1.1c\">AP^{b}_{50}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T4.8.8.8.8.m1.1d\">italic_A italic_P start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 50 end_POSTSUBSCRIPT</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.9.9.9.9\"><math alttext=\"AP^{b}_{75}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.9.9.9.9.m1.1\"><semantics id=\"S4.T4.9.9.9.9.m1.1a\"><mrow id=\"S4.T4.9.9.9.9.m1.1.1\" xref=\"S4.T4.9.9.9.9.m1.1.1.cmml\"><mi id=\"S4.T4.9.9.9.9.m1.1.1.2\" xref=\"S4.T4.9.9.9.9.m1.1.1.2.cmml\">A</mi><mo id=\"S4.T4.9.9.9.9.m1.1.1.1\" xref=\"S4.T4.9.9.9.9.m1.1.1.1.cmml\">&#8290;</mo><msubsup id=\"S4.T4.9.9.9.9.m1.1.1.3\" xref=\"S4.T4.9.9.9.9.m1.1.1.3.cmml\"><mi id=\"S4.T4.9.9.9.9.m1.1.1.3.2.2\" xref=\"S4.T4.9.9.9.9.m1.1.1.3.2.2.cmml\">P</mi><mn id=\"S4.T4.9.9.9.9.m1.1.1.3.3\" xref=\"S4.T4.9.9.9.9.m1.1.1.3.3.cmml\">75</mn><mi id=\"S4.T4.9.9.9.9.m1.1.1.3.2.3\" xref=\"S4.T4.9.9.9.9.m1.1.1.3.2.3.cmml\">b</mi></msubsup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.9.9.9.9.m1.1b\"><apply id=\"S4.T4.9.9.9.9.m1.1.1.cmml\" xref=\"S4.T4.9.9.9.9.m1.1.1\"><times id=\"S4.T4.9.9.9.9.m1.1.1.1.cmml\" xref=\"S4.T4.9.9.9.9.m1.1.1.1\"/><ci id=\"S4.T4.9.9.9.9.m1.1.1.2.cmml\" xref=\"S4.T4.9.9.9.9.m1.1.1.2\">&#119860;</ci><apply id=\"S4.T4.9.9.9.9.m1.1.1.3.cmml\" xref=\"S4.T4.9.9.9.9.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.T4.9.9.9.9.m1.1.1.3.1.cmml\" xref=\"S4.T4.9.9.9.9.m1.1.1.3\">subscript</csymbol><apply id=\"S4.T4.9.9.9.9.m1.1.1.3.2.cmml\" xref=\"S4.T4.9.9.9.9.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.T4.9.9.9.9.m1.1.1.3.2.1.cmml\" xref=\"S4.T4.9.9.9.9.m1.1.1.3\">superscript</csymbol><ci id=\"S4.T4.9.9.9.9.m1.1.1.3.2.2.cmml\" xref=\"S4.T4.9.9.9.9.m1.1.1.3.2.2\">&#119875;</ci><ci id=\"S4.T4.9.9.9.9.m1.1.1.3.2.3.cmml\" xref=\"S4.T4.9.9.9.9.m1.1.1.3.2.3\">&#119887;</ci></apply><cn id=\"S4.T4.9.9.9.9.m1.1.1.3.3.cmml\" type=\"integer\" xref=\"S4.T4.9.9.9.9.m1.1.1.3.3\">75</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.9.9.9.9.m1.1c\">AP^{b}_{75}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T4.9.9.9.9.m1.1d\">italic_A italic_P start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 75 end_POSTSUBSCRIPT</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.10.10.10.10\"><math alttext=\"mAP^{m}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.10.10.10.10.m1.1\"><semantics id=\"S4.T4.10.10.10.10.m1.1a\"><mrow id=\"S4.T4.10.10.10.10.m1.1.1\" xref=\"S4.T4.10.10.10.10.m1.1.1.cmml\"><mi id=\"S4.T4.10.10.10.10.m1.1.1.2\" xref=\"S4.T4.10.10.10.10.m1.1.1.2.cmml\">m</mi><mo id=\"S4.T4.10.10.10.10.m1.1.1.1\" xref=\"S4.T4.10.10.10.10.m1.1.1.1.cmml\">&#8290;</mo><mi id=\"S4.T4.10.10.10.10.m1.1.1.3\" xref=\"S4.T4.10.10.10.10.m1.1.1.3.cmml\">A</mi><mo id=\"S4.T4.10.10.10.10.m1.1.1.1a\" xref=\"S4.T4.10.10.10.10.m1.1.1.1.cmml\">&#8290;</mo><msup id=\"S4.T4.10.10.10.10.m1.1.1.4\" xref=\"S4.T4.10.10.10.10.m1.1.1.4.cmml\"><mi id=\"S4.T4.10.10.10.10.m1.1.1.4.2\" xref=\"S4.T4.10.10.10.10.m1.1.1.4.2.cmml\">P</mi><mi id=\"S4.T4.10.10.10.10.m1.1.1.4.3\" xref=\"S4.T4.10.10.10.10.m1.1.1.4.3.cmml\">m</mi></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.10.10.10.10.m1.1b\"><apply id=\"S4.T4.10.10.10.10.m1.1.1.cmml\" xref=\"S4.T4.10.10.10.10.m1.1.1\"><times id=\"S4.T4.10.10.10.10.m1.1.1.1.cmml\" xref=\"S4.T4.10.10.10.10.m1.1.1.1\"/><ci id=\"S4.T4.10.10.10.10.m1.1.1.2.cmml\" xref=\"S4.T4.10.10.10.10.m1.1.1.2\">&#119898;</ci><ci id=\"S4.T4.10.10.10.10.m1.1.1.3.cmml\" xref=\"S4.T4.10.10.10.10.m1.1.1.3\">&#119860;</ci><apply id=\"S4.T4.10.10.10.10.m1.1.1.4.cmml\" xref=\"S4.T4.10.10.10.10.m1.1.1.4\"><csymbol cd=\"ambiguous\" id=\"S4.T4.10.10.10.10.m1.1.1.4.1.cmml\" xref=\"S4.T4.10.10.10.10.m1.1.1.4\">superscript</csymbol><ci id=\"S4.T4.10.10.10.10.m1.1.1.4.2.cmml\" xref=\"S4.T4.10.10.10.10.m1.1.1.4.2\">&#119875;</ci><ci id=\"S4.T4.10.10.10.10.m1.1.1.4.3.cmml\" xref=\"S4.T4.10.10.10.10.m1.1.1.4.3\">&#119898;</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.10.10.10.10.m1.1c\">mAP^{m}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T4.10.10.10.10.m1.1d\">italic_m italic_A italic_P start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.11.11.11.11\"><math alttext=\"AP^{m}_{50}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.11.11.11.11.m1.1\"><semantics id=\"S4.T4.11.11.11.11.m1.1a\"><mrow id=\"S4.T4.11.11.11.11.m1.1.1\" xref=\"S4.T4.11.11.11.11.m1.1.1.cmml\"><mi id=\"S4.T4.11.11.11.11.m1.1.1.2\" xref=\"S4.T4.11.11.11.11.m1.1.1.2.cmml\">A</mi><mo id=\"S4.T4.11.11.11.11.m1.1.1.1\" xref=\"S4.T4.11.11.11.11.m1.1.1.1.cmml\">&#8290;</mo><msubsup id=\"S4.T4.11.11.11.11.m1.1.1.3\" xref=\"S4.T4.11.11.11.11.m1.1.1.3.cmml\"><mi id=\"S4.T4.11.11.11.11.m1.1.1.3.2.2\" xref=\"S4.T4.11.11.11.11.m1.1.1.3.2.2.cmml\">P</mi><mn id=\"S4.T4.11.11.11.11.m1.1.1.3.3\" xref=\"S4.T4.11.11.11.11.m1.1.1.3.3.cmml\">50</mn><mi id=\"S4.T4.11.11.11.11.m1.1.1.3.2.3\" xref=\"S4.T4.11.11.11.11.m1.1.1.3.2.3.cmml\">m</mi></msubsup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.11.11.11.11.m1.1b\"><apply id=\"S4.T4.11.11.11.11.m1.1.1.cmml\" xref=\"S4.T4.11.11.11.11.m1.1.1\"><times id=\"S4.T4.11.11.11.11.m1.1.1.1.cmml\" xref=\"S4.T4.11.11.11.11.m1.1.1.1\"/><ci id=\"S4.T4.11.11.11.11.m1.1.1.2.cmml\" xref=\"S4.T4.11.11.11.11.m1.1.1.2\">&#119860;</ci><apply id=\"S4.T4.11.11.11.11.m1.1.1.3.cmml\" xref=\"S4.T4.11.11.11.11.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.T4.11.11.11.11.m1.1.1.3.1.cmml\" xref=\"S4.T4.11.11.11.11.m1.1.1.3\">subscript</csymbol><apply id=\"S4.T4.11.11.11.11.m1.1.1.3.2.cmml\" xref=\"S4.T4.11.11.11.11.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.T4.11.11.11.11.m1.1.1.3.2.1.cmml\" xref=\"S4.T4.11.11.11.11.m1.1.1.3\">superscript</csymbol><ci id=\"S4.T4.11.11.11.11.m1.1.1.3.2.2.cmml\" xref=\"S4.T4.11.11.11.11.m1.1.1.3.2.2\">&#119875;</ci><ci id=\"S4.T4.11.11.11.11.m1.1.1.3.2.3.cmml\" xref=\"S4.T4.11.11.11.11.m1.1.1.3.2.3\">&#119898;</ci></apply><cn id=\"S4.T4.11.11.11.11.m1.1.1.3.3.cmml\" type=\"integer\" xref=\"S4.T4.11.11.11.11.m1.1.1.3.3\">50</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.11.11.11.11.m1.1c\">AP^{m}_{50}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T4.11.11.11.11.m1.1d\">italic_A italic_P start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 50 end_POSTSUBSCRIPT</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.12.12\"><math alttext=\"AP^{m}_{75}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.12.12.12.12.m1.1\"><semantics id=\"S4.T4.12.12.12.12.m1.1a\"><mrow id=\"S4.T4.12.12.12.12.m1.1.1\" xref=\"S4.T4.12.12.12.12.m1.1.1.cmml\"><mi id=\"S4.T4.12.12.12.12.m1.1.1.2\" xref=\"S4.T4.12.12.12.12.m1.1.1.2.cmml\">A</mi><mo id=\"S4.T4.12.12.12.12.m1.1.1.1\" xref=\"S4.T4.12.12.12.12.m1.1.1.1.cmml\">&#8290;</mo><msubsup id=\"S4.T4.12.12.12.12.m1.1.1.3\" xref=\"S4.T4.12.12.12.12.m1.1.1.3.cmml\"><mi id=\"S4.T4.12.12.12.12.m1.1.1.3.2.2\" xref=\"S4.T4.12.12.12.12.m1.1.1.3.2.2.cmml\">P</mi><mn id=\"S4.T4.12.12.12.12.m1.1.1.3.3\" xref=\"S4.T4.12.12.12.12.m1.1.1.3.3.cmml\">75</mn><mi id=\"S4.T4.12.12.12.12.m1.1.1.3.2.3\" xref=\"S4.T4.12.12.12.12.m1.1.1.3.2.3.cmml\">m</mi></msubsup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.12.12.12.12.m1.1b\"><apply id=\"S4.T4.12.12.12.12.m1.1.1.cmml\" xref=\"S4.T4.12.12.12.12.m1.1.1\"><times id=\"S4.T4.12.12.12.12.m1.1.1.1.cmml\" xref=\"S4.T4.12.12.12.12.m1.1.1.1\"/><ci id=\"S4.T4.12.12.12.12.m1.1.1.2.cmml\" xref=\"S4.T4.12.12.12.12.m1.1.1.2\">&#119860;</ci><apply id=\"S4.T4.12.12.12.12.m1.1.1.3.cmml\" xref=\"S4.T4.12.12.12.12.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.T4.12.12.12.12.m1.1.1.3.1.cmml\" xref=\"S4.T4.12.12.12.12.m1.1.1.3\">subscript</csymbol><apply id=\"S4.T4.12.12.12.12.m1.1.1.3.2.cmml\" xref=\"S4.T4.12.12.12.12.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.T4.12.12.12.12.m1.1.1.3.2.1.cmml\" xref=\"S4.T4.12.12.12.12.m1.1.1.3\">superscript</csymbol><ci id=\"S4.T4.12.12.12.12.m1.1.1.3.2.2.cmml\" xref=\"S4.T4.12.12.12.12.m1.1.1.3.2.2\">&#119875;</ci><ci id=\"S4.T4.12.12.12.12.m1.1.1.3.2.3.cmml\" xref=\"S4.T4.12.12.12.12.m1.1.1.3.2.3\">&#119898;</ci></apply><cn id=\"S4.T4.12.12.12.12.m1.1.1.3.3.cmml\" type=\"integer\" xref=\"S4.T4.12.12.12.12.m1.1.1.3.3\">75</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.12.12.12.12.m1.1c\">AP^{m}_{75}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T4.12.12.12.12.m1.1d\">italic_A italic_P start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 75 end_POSTSUBSCRIPT</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.12.12.14.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T4.12.12.14.2.1\">Swin-T <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib29\" title=\"\">29</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.12.12.14.2.2\">41.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.12.12.14.2.3\">62.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.12.12.14.2.4\">44.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.12.12.14.2.5\">25.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.12.12.14.2.6\">44.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.12.12.14.2.7\">55.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.12.12.14.2.8\">42.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.12.12.14.2.9\">64.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.12.12.14.2.10\">46.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.12.12.14.2.11\">39.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.12.12.14.2.12\">61.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.12.12.14.2.13\">42.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.12.12.15.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T4.12.12.15.3.1\">DAT-T <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib47\" title=\"\">47</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.15.3.2\">42.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.15.3.3\">64.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.15.3.4\">45.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.15.3.5\">28.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.15.3.6\">45.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.12.12.15.3.7\">57.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.15.3.8\">44.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.15.3.9\">67.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.15.3.10\">48.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.15.3.11\">40.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.15.3.12\">64.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.15.3.13\">43.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.12.12.16.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T4.12.12.16.4.1\">CSWin-T <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib14\" title=\"\">14</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.16.4.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.16.4.3\">-</td>\n<td class=\"ltx_td\" id=\"S4.T4.12.12.16.4.4\"/>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.16.4.5\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.16.4.6\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.12.12.16.4.7\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.16.4.8\">46.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.16.4.9\">68.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.16.4.10\">51.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.16.4.11\">42.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.16.4.12\">65.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.16.4.13\">45.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.12.12.17.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T4.12.12.17.5.1\">RegionViT-S+ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib2\" title=\"\">2</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.17.5.2\">43.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.17.5.3\">65.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.17.5.4\">47.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.17.5.5\">28.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.17.5.6\">47.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.12.12.17.5.7\">58.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.17.5.8\">44.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.17.5.9\">67.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.17.5.10\">48.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.17.5.11\">40.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.17.5.12\">64.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.17.5.13\">44.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.12.12.18.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T4.12.12.18.6.1\">MixFormer-B4 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib4\" title=\"\">4</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.18.6.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.18.6.3\">-</td>\n<td class=\"ltx_td\" id=\"S4.T4.12.12.18.6.4\"/>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.18.6.5\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.18.6.6\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.12.12.18.6.7\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.18.6.8\">45.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.18.6.9\">67.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.18.6.10\">49.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.18.6.11\">41.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.18.6.12\">64.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.18.6.13\">44.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.12.12.19.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T4.12.12.19.7.1\">CrossFormer-S <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib45\" title=\"\">45</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.19.7.2\">44.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.19.7.3\">55.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.19.7.4\">38.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.19.7.5\">19.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.19.7.6\">40.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.12.12.19.7.7\">48.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.19.7.8\">45.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.19.7.9\">68.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.19.7.10\">49.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.19.7.11\">41.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.19.7.12\">64.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.19.7.13\">44.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.12.12.20.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T4.12.12.20.8.1\">QuadTree-B2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib37\" title=\"\">37</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.20.8.2\">46.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.20.8.3\">67.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.20.8.4\">49.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.20.8.5\">29.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.20.8.6\">50.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.12.12.20.8.7\">61.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.20.8.8\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.20.8.9\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.20.8.10\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.20.8.11\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.20.8.12\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.20.8.13\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.12.12.21.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T4.12.12.21.9.1\">InternImage-T <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib43\" title=\"\">43</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.21.9.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.21.9.3\">-</td>\n<td class=\"ltx_td\" id=\"S4.T4.12.12.21.9.4\"/>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.21.9.5\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.21.9.6\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.12.12.21.9.7\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.21.9.8\">47.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.21.9.9\">69.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.21.9.10\">52.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.21.9.11\">42.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.21.9.12\">66.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.21.9.13\">45.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.12.12.22.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T4.12.12.22.10.1\">Agent-Swin-T <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib18\" title=\"\">18</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.22.10.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.22.10.3\">-</td>\n<td class=\"ltx_td\" id=\"S4.T4.12.12.22.10.4\"/>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.22.10.5\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.22.10.6\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.12.12.22.10.7\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.22.10.8\">44.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.22.10.9\">67.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.22.10.10\">48.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.22.10.11\">40.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.22.10.12\">64.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.22.10.13\">43.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.12.12.23.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T4.12.12.23.11.1\">BiFormer-S <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib56\" title=\"\">56</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.23.11.2\">45.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.23.11.3\">66.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.23.11.4\">49.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.23.11.5\">30.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.23.11.6\">49.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.12.12.23.11.7\">61.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.23.11.8\">47.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.23.11.9\">69.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.23.11.10\">52.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.23.11.11\">43.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.23.11.12\">66.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.23.11.13\">46.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.12.12.24.12\" style=\"background-color:#96FFFB;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T4.12.12.24.12.1\"><span class=\"ltx_text\" id=\"S4.T4.12.12.24.12.1.1\" style=\"background-color:#96FFFB;\">DeBiFormer-S</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.24.12.2\"><span class=\"ltx_text\" id=\"S4.T4.12.12.24.12.2.1\" style=\"background-color:#96FFFB;\">45.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.24.12.3\"><span class=\"ltx_text\" id=\"S4.T4.12.12.24.12.3.1\" style=\"background-color:#96FFFB;\">66.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.24.12.4\"><span class=\"ltx_text\" id=\"S4.T4.12.12.24.12.4.1\" style=\"background-color:#96FFFB;\">48.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.24.12.5\"><span class=\"ltx_text\" id=\"S4.T4.12.12.24.12.5.1\" style=\"background-color:#96FFFB;\">28.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.24.12.6\"><span class=\"ltx_text\" id=\"S4.T4.12.12.24.12.6.1\" style=\"background-color:#96FFFB;\">49.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.12.12.24.12.7\"><span class=\"ltx_text\" id=\"S4.T4.12.12.24.12.7.1\" style=\"background-color:#96FFFB;\">61.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.24.12.8\"><span class=\"ltx_text\" id=\"S4.T4.12.12.24.12.8.1\" style=\"background-color:#96FFFB;\">47.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.24.12.9\"><span class=\"ltx_text\" id=\"S4.T4.12.12.24.12.9.1\" style=\"background-color:#96FFFB;\">69.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.24.12.10\"><span class=\"ltx_text\" id=\"S4.T4.12.12.24.12.10.1\" style=\"background-color:#96FFFB;\">52.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.24.12.11\"><span class=\"ltx_text\" id=\"S4.T4.12.12.24.12.11.1\" style=\"background-color:#96FFFB;\">42.5</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.24.12.12\"><span class=\"ltx_text\" id=\"S4.T4.12.12.24.12.12.1\" style=\"background-color:#96FFFB;\">66.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.24.12.13\"><span class=\"ltx_text\" id=\"S4.T4.12.12.24.12.13.1\" style=\"background-color:#96FFFB;\">45.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.12.12.25.13\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T4.12.12.25.13.1\">Swin-S <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib29\" title=\"\">29</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.12.12.25.13.2\">44.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.12.12.25.13.3\">65.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.12.12.25.13.4\">47.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.12.12.25.13.5\">27.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.12.12.25.13.6\">48.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T4.12.12.25.13.7\">59.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.12.12.25.13.8\">44.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.12.12.25.13.9\">66.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.12.12.25.13.10\">48.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.12.12.25.13.11\">40.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.12.12.25.13.12\">63.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.12.12.25.13.13\">44.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.12.12.26.14\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T4.12.12.26.14.1\">DAT-S <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib47\" title=\"\">47</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.26.14.2\">45.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.26.14.3\">67.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.26.14.4\">48.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.26.14.5\">30.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.26.14.6\">49.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.12.12.26.14.7\">61.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.26.14.8\">47.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.26.14.9\">69.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.26.14.10\">51.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.26.14.11\">42.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.26.14.12\">66.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.26.14.13\">45.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.12.12.27.15\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T4.12.12.27.15.1\">CSWin-S <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib14\" title=\"\">14</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.27.15.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.27.15.3\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.27.15.4\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.27.15.5\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.27.15.6\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.12.12.27.15.7\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.27.15.8\">47.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.27.15.9\">70.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.27.15.10\">52.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.27.15.11\">43.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.27.15.12\">67.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.27.15.13\">46.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.12.12.28.16\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T4.12.12.28.16.1\">RegionViT-B+ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib2\" title=\"\">2</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.28.16.2\">44.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.28.16.3\">66.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.28.16.4\">47.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.28.16.5\">29.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.28.16.6\">47.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.12.12.28.16.7\">59.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.28.16.8\">45.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.28.16.9\">68.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.28.16.10\">49.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.28.16.11\">41.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.28.16.12\">65.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.28.16.13\">44.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.12.12.29.17\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T4.12.12.29.17.1\">CrossFormer-B <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib45\" title=\"\">45</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.29.17.2\">46.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.29.17.3\">67.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.29.17.4\">49.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.29.17.5\">30.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.29.17.6\">49.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.12.12.29.17.7\">61.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.29.17.8\">47.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.29.17.9\">69.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.29.17.10\">51.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.29.17.11\">42.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.29.17.12\">66.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.29.17.13\">46.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.12.12.30.18\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T4.12.12.30.18.1\">QuadTree-B3 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib37\" title=\"\">37</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.30.18.2\">47.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.30.18.3\">68.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.30.18.4\">50.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.30.18.5\">30.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.30.18.6\">51.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.12.12.30.18.7\">62.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.30.18.8\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.30.18.9\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.30.18.10\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.30.18.11\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.30.18.12\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.30.18.13\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.12.12.31.19\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T4.12.12.31.19.1\">InternImage-S <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib43\" title=\"\">43</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.31.19.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.31.19.3\">-</td>\n<td class=\"ltx_td\" id=\"S4.T4.12.12.31.19.4\"/>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.31.19.5\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.31.19.6\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.12.12.31.19.7\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.31.19.8\">47.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.31.19.9\">69.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.31.19.10\">52.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.31.19.11\">43.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.31.19.12\">67.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.31.19.13\">46.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.12.12.32.20\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T4.12.12.32.20.1\">Agent-Swin-S <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib18\" title=\"\">18</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.32.20.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.32.20.3\">-</td>\n<td class=\"ltx_td\" id=\"S4.T4.12.12.32.20.4\"/>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.32.20.5\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.32.20.6\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.12.12.32.20.7\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.32.20.8\">47.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.32.20.9\">69.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.32.20.10\">52.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.32.20.11\">42.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.32.20.12\">66.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.32.20.13\">45.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.12.12.33.21\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T4.12.12.33.21.1\">BiFormer-B <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib56\" title=\"\">56</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.33.21.2\">47.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.33.21.3\">68.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.33.21.4\">50.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.33.21.5\">31.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.33.21.6\">50.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T4.12.12.33.21.7\">62.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.33.21.8\">48.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.33.21.9\">70.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.33.21.10\">53.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.33.21.11\">43.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.33.21.12\">67.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.12.12.33.21.13\">47.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.12.12.34.22\" style=\"background-color:#96FFFB;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\" id=\"S4.T4.12.12.34.22.1\"><span class=\"ltx_text\" id=\"S4.T4.12.12.34.22.1.1\" style=\"background-color:#96FFFB;\">DeBiFormer-B</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T4.12.12.34.22.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.12.12.34.22.2.1\" style=\"background-color:#96FFFB;\">47.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T4.12.12.34.22.3\"><span class=\"ltx_text\" id=\"S4.T4.12.12.34.22.3.1\" style=\"background-color:#96FFFB;\">68.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T4.12.12.34.22.4\"><span class=\"ltx_text\" id=\"S4.T4.12.12.34.22.4.1\" style=\"background-color:#96FFFB;\">50.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T4.12.12.34.22.5\"><span class=\"ltx_text\" id=\"S4.T4.12.12.34.22.5.1\" style=\"background-color:#96FFFB;\">30.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T4.12.12.34.22.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.12.12.34.22.6.1\" style=\"background-color:#96FFFB;\">51.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T4.12.12.34.22.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.12.12.34.22.7.1\" style=\"background-color:#96FFFB;\">63.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T4.12.12.34.22.8\"><span class=\"ltx_text\" id=\"S4.T4.12.12.34.22.8.1\" style=\"background-color:#96FFFB;\">48.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T4.12.12.34.22.9\"><span class=\"ltx_text\" id=\"S4.T4.12.12.34.22.9.1\" style=\"background-color:#96FFFB;\">70.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T4.12.12.34.22.10\"><span class=\"ltx_text\" id=\"S4.T4.12.12.34.22.10.1\" style=\"background-color:#96FFFB;\">53.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T4.12.12.34.22.11\"><span class=\"ltx_text\" id=\"S4.T4.12.12.34.22.11.1\" style=\"background-color:#96FFFB;\">43.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T4.12.12.34.22.12\"><span class=\"ltx_text\" id=\"S4.T4.12.12.34.22.12.1\" style=\"background-color:#96FFFB;\">67.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T4.12.12.34.22.13\"><span class=\"ltx_text\" id=\"S4.T4.12.12.34.22.13.1\" style=\"background-color:#96FFFB;\">46.4</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 4:  Results on object detection (left group) and instance segmentation (right group) tasks, performed on COCO 2017 dataset.",
        "footnotes": [
            "[29] \nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021)\n\n",
            "[47] \nXia, Z., Pan, X., Song, S., Li, L.E., Huang, G.: Vision transformer with deformable attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4794–4803 (June 2022)\n\n",
            "[14] \nDong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin transformer: A general vision transformer backbone with cross-shaped windows. In: Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR). pp. 12124–12134 (2022)\n\n",
            "[2] \nChen, C.F., Panda, R., Fan, Q.: Regionvit: Regional-to-local attention for vision transformers. arXiv preprint arXiv:2106.02689 (2021)\n\n",
            "[4] \nChen, Q., Wu, Q., Wang, J., Hu, Q., Hu, T., Ding, E., Cheng, J., Wang, J.: Mixformer: Mixing features across windows and dimensions (2022)\n\n",
            "[45] \nWang, W., Yao, L., Chen, L., Lin, B., Cai, D., He, X., Liu, W.: Crossformer: A versatile vision transformer hinging on cross-scale attention. In: Proceedings of the International Conference on Learning Representations (ICLR) (2022),  https://openreview.net/forum?id=_PHymLIxuI \n",
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[43] \nWang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu, L., Li, H., et al.: Internimage: Exploring large-scale vision foundation models with deformable convolutions. arXiv preprint arXiv:2211.05778 (2022)\n\n",
            "[18] \nHan, D., Ye, T., Han, Y., Xia, Z., Song, S., Huang, G.: Agent attention: On the integration of softmax and linear attention. arXiv preprint arXiv:2312.08874 (2023)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n",
            "[29] \nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021)\n\n",
            "[47] \nXia, Z., Pan, X., Song, S., Li, L.E., Huang, G.: Vision transformer with deformable attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4794–4803 (June 2022)\n\n",
            "[14] \nDong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin transformer: A general vision transformer backbone with cross-shaped windows. In: Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR). pp. 12124–12134 (2022)\n\n",
            "[2] \nChen, C.F., Panda, R., Fan, Q.: Regionvit: Regional-to-local attention for vision transformers. arXiv preprint arXiv:2106.02689 (2021)\n\n",
            "[45] \nWang, W., Yao, L., Chen, L., Lin, B., Cai, D., He, X., Liu, W.: Crossformer: A versatile vision transformer hinging on cross-scale attention. In: Proceedings of the International Conference on Learning Representations (ICLR) (2022),  https://openreview.net/forum?id=_PHymLIxuI \n",
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[43] \nWang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu, L., Li, H., et al.: Internimage: Exploring large-scale vision foundation models with deformable convolutions. arXiv preprint arXiv:2211.05778 (2022)\n\n",
            "[18] \nHan, D., Ye, T., Han, Y., Xia, Z., Song, S., Huang, G.: Agent attention: On the integration of softmax and linear attention. arXiv preprint arXiv:2312.08874 (2023)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n"
        ],
        "references": [
            "Settings.\nWe used our DeBiFormer as the backbone in the Mask RCNN [19] and RetinaNet [16] frameworks to evaluate the effectiveness of models for object detection and instance segmentation on COCO 2017 [17]. The experiments were conducted with the MMDetection [3] toolbox. Before training on COCO, we initialized the backbone with weights pre-trained on ImageNet-1K and followed the same training strategies as BiFormer [56] to compare our methods fairly. Note that due to device limitations, we set mini batch size as 4 for these experiments, while in BiFormer this value is 16. For details on the specific settings of the experiment, please refer to the supplementary paper.\n\nResults.\nWe list the results in Table 4. For object detection with RetinaNet, we report the mean average precision (mAP) and the average precision (AP) at different IoU thresholds (50%, 75%) for three object sizes (i.e., small, medium, and large (S/M/L)). From the results, we can see that while the overall performance of DeBiFormer was only comparable to some of the most competitive existing methods, the performance on large objects (A⁢PL𝐴subscript𝑃𝐿AP_{L}italic_A italic_P start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT) outperformed these methods although we use a limited resources. This may be because the DBRA allocates deformable points more reasonably. These points are not to focus only on small things, but to focus on important things in the image. Therefore the attention is not limited to a small area, which improves the detection accuracy of large objects. For instance segmentation with Mask R-CNN, we report the bounding box and mask the average precision (A⁢Pb𝐴subscript𝑃𝑏AP_{b}italic_A italic_P start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT and A⁢Pm𝐴subscript𝑃𝑚AP_{m}italic_A italic_P start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT) at different IoU thresholds (50%, 75%). Note that our DeBiFormer still achieved great performance under the device limitation of mini batch size. We believe that we could achieve better results if the mini batch size could be the same to other methods since it has been proved on semantic segmentation tasks."
        ]
    },
    "S4.T5.1": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T5.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T5.1.1.1.1\">Sparse Attention</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T5.1.1.1.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T5.1.1.1.2.1\">\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.1.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T5.1.1.1.2.1.1.1\">IN1K</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.1.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T5.1.1.1.2.1.2.1\">Top1(%)</td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T5.1.1.1.3\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T5.1.1.1.3.1\">\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.1.3.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T5.1.1.1.3.1.1.1\">ADE20K</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.1.3.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T5.1.1.1.3.1.2.1\">mIoU(%)</td>\n</tr>\n</table>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T5.1.2.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T5.1.2.1.1\">Shifted window <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib29\" title=\"\">29</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T5.1.2.1.2\">81.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.2.1.3\">41.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.3.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T5.1.3.2.1\">Spatially sep <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib7\" title=\"\">7</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T5.1.3.2.2\">81.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.3.2.3\">42.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.4.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T5.1.4.3.1\">Sequential axial <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib21\" title=\"\">21</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T5.1.4.3.2\">81.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.4.3.3\">39.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.5.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T5.1.5.4.1\">Criss-cross <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib45\" title=\"\">45</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T5.1.5.4.2\">81.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.5.4.3\">43.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.6.5\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T5.1.6.5.1\">Cross-shaped window <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib14\" title=\"\">14</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T5.1.6.5.2\">82.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.6.5.3\">43.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.7.6\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T5.1.7.6.1\">Deformable <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib47\" title=\"\">47</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T5.1.7.6.2\">82.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.7.6.3\">42.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.8.7\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T5.1.8.7.1\">Block-grid <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib40\" title=\"\">40</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T5.1.8.7.2\">81.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.8.7.3\">42.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.9.8\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S4.T5.1.9.8.1\">Bi-level routing <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib56\" title=\"\">56</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T5.1.9.8.2\">82.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T5.1.9.8.3\">44.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.10.9\" style=\"background-color:#96FFFB;\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\" id=\"S4.T5.1.10.9.1\"><span class=\"ltx_text\" id=\"S4.T5.1.10.9.1.1\" style=\"background-color:#96FFFB;\">Deformable bi-level routing</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T5.1.10.9.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.10.9.2.1\" style=\"background-color:#96FFFB;\">82.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T5.1.10.9.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.10.9.3.1\" style=\"background-color:#96FFFB;\">48.0</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 5:  Ablation study on different attention mechanisms. All models follow the architecture design of the Swin-T model.",
        "footnotes": [
            "[29] \nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021)\n\n",
            "[7] \nChu, X., Tian, Z., Wang, Y., Zhang, B., Ren, H., Wei, X., Xia, H., Shen, C.: Twins: Revisiting the design of spatial attention in vision transformers. In: NeurIPS 2021 (2021),  https://openreview.net/forum?id=5kTlVBkzSRx \n",
            "[21] \nHo, J., Kalchbrenner, N., Weissenborn, D., Salimans, T.: Axial attention in multidimensional transformers (2019)\n\n",
            "[45] \nWang, W., Yao, L., Chen, L., Lin, B., Cai, D., He, X., Liu, W.: Crossformer: A versatile vision transformer hinging on cross-scale attention. In: Proceedings of the International Conference on Learning Representations (ICLR) (2022),  https://openreview.net/forum?id=_PHymLIxuI \n",
            "[14] \nDong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin transformer: A general vision transformer backbone with cross-shaped windows. In: Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR). pp. 12124–12134 (2022)\n\n",
            "[47] \nXia, Z., Pan, X., Song, S., Li, L.E., Huang, G.: Vision transformer with deformable attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4794–4803 (June 2022)\n\n",
            "[40] \nTu, Z., Talebi, H., Zhang, H., Yang, F., Milanfar, P., Bovik, A., Li, Y.: Maxvit: Multi-axis vision transformer. ECCV (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n"
        ],
        "references": [
            "Effectiveness of DBRA.\nWe compared DBRA with several existing sparse attention mechanisms. Following CSWIN [14], we aligned macro architecture designs with Swin-T [29] for fair comparison. Specifically, we used 2,2,6,222622,2,6,22 , 2 , 6 , 2 blocks for the four stages and non-overlapped patch embedding, and we set the initial patch embedding dimension to C=96𝐶96C=96italic_C = 96 and MLP expansion ratio to e=4𝑒4e=4italic_e = 4. The results are reported in Table 5. Our Deformable Bi-level Routing Attention had significantly better performance than the existing sparse attention mechanisms, in terms of both image classification and semantic segmentation."
        ]
    },
    "S4.T5.1.1.1.2.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T5.1.1.1.2.1\">\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.1.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T5.1.1.1.2.1.1.1\">IN1K</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.1.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T5.1.1.1.2.1.2.1\">Top1(%)</td>\n</tr>\n</table>\n\n",
        "caption": "Table 5:  Ablation study on different attention mechanisms. All models follow the architecture design of the Swin-T model.",
        "footnotes": [
            "[29] \nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021)\n\n",
            "[7] \nChu, X., Tian, Z., Wang, Y., Zhang, B., Ren, H., Wei, X., Xia, H., Shen, C.: Twins: Revisiting the design of spatial attention in vision transformers. In: NeurIPS 2021 (2021),  https://openreview.net/forum?id=5kTlVBkzSRx \n",
            "[21] \nHo, J., Kalchbrenner, N., Weissenborn, D., Salimans, T.: Axial attention in multidimensional transformers (2019)\n\n",
            "[45] \nWang, W., Yao, L., Chen, L., Lin, B., Cai, D., He, X., Liu, W.: Crossformer: A versatile vision transformer hinging on cross-scale attention. In: Proceedings of the International Conference on Learning Representations (ICLR) (2022),  https://openreview.net/forum?id=_PHymLIxuI \n",
            "[14] \nDong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin transformer: A general vision transformer backbone with cross-shaped windows. In: Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR). pp. 12124–12134 (2022)\n\n",
            "[47] \nXia, Z., Pan, X., Song, S., Li, L.E., Huang, G.: Vision transformer with deformable attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4794–4803 (June 2022)\n\n",
            "[40] \nTu, Z., Talebi, H., Zhang, H., Yang, F., Milanfar, P., Bovik, A., Li, Y.: Maxvit: Multi-axis vision transformer. ECCV (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n"
        ],
        "references": [
            "Effectiveness of DBRA.\nWe compared DBRA with several existing sparse attention mechanisms. Following CSWIN [14], we aligned macro architecture designs with Swin-T [29] for fair comparison. Specifically, we used 2,2,6,222622,2,6,22 , 2 , 6 , 2 blocks for the four stages and non-overlapped patch embedding, and we set the initial patch embedding dimension to C=96𝐶96C=96italic_C = 96 and MLP expansion ratio to e=4𝑒4e=4italic_e = 4. The results are reported in Table 5. Our Deformable Bi-level Routing Attention had significantly better performance than the existing sparse attention mechanisms, in terms of both image classification and semantic segmentation."
        ]
    },
    "S4.T5.1.1.1.3.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T5.1.1.1.3.1\">\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.1.3.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T5.1.1.1.3.1.1.1\">ADE20K</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.1.3.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T5.1.1.1.3.1.2.1\">mIoU(%)</td>\n</tr>\n</table>\n\n",
        "caption": "Table 5:  Ablation study on different attention mechanisms. All models follow the architecture design of the Swin-T model.",
        "footnotes": [
            "[29] \nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021)\n\n",
            "[7] \nChu, X., Tian, Z., Wang, Y., Zhang, B., Ren, H., Wei, X., Xia, H., Shen, C.: Twins: Revisiting the design of spatial attention in vision transformers. In: NeurIPS 2021 (2021),  https://openreview.net/forum?id=5kTlVBkzSRx \n",
            "[21] \nHo, J., Kalchbrenner, N., Weissenborn, D., Salimans, T.: Axial attention in multidimensional transformers (2019)\n\n",
            "[45] \nWang, W., Yao, L., Chen, L., Lin, B., Cai, D., He, X., Liu, W.: Crossformer: A versatile vision transformer hinging on cross-scale attention. In: Proceedings of the International Conference on Learning Representations (ICLR) (2022),  https://openreview.net/forum?id=_PHymLIxuI \n",
            "[14] \nDong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin transformer: A general vision transformer backbone with cross-shaped windows. In: Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR). pp. 12124–12134 (2022)\n\n",
            "[47] \nXia, Z., Pan, X., Song, S., Li, L.E., Huang, G.: Vision transformer with deformable attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4794–4803 (June 2022)\n\n",
            "[40] \nTu, Z., Talebi, H., Zhang, H., Yang, F., Milanfar, P., Bovik, A., Li, Y.: Maxvit: Multi-axis vision transformer. ECCV (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n"
        ],
        "references": [
            "Effectiveness of DBRA.\nWe compared DBRA with several existing sparse attention mechanisms. Following CSWIN [14], we aligned macro architecture designs with Swin-T [29] for fair comparison. Specifically, we used 2,2,6,222622,2,6,22 , 2 , 6 , 2 blocks for the four stages and non-overlapped patch embedding, and we set the initial patch embedding dimension to C=96𝐶96C=96italic_C = 96 and MLP expansion ratio to e=4𝑒4e=4italic_e = 4. The results are reported in Table 5. Our Deformable Bi-level Routing Attention had significantly better performance than the existing sparse attention mechanisms, in terms of both image classification and semantic segmentation."
        ]
    },
    "S4.T6.1": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T6.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T6.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T6.1.1.2\">#partition factor</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T6.1.1.1\"><math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T6.1.1.1.m1.1\"><semantics id=\"S4.T6.1.1.1.m1.1a\"><mi id=\"S4.T6.1.1.1.m1.1.1\" xref=\"S4.T6.1.1.1.m1.1.1.cmml\">k</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T6.1.1.1.m1.1b\"><ci id=\"S4.T6.1.1.1.m1.1.1.cmml\" xref=\"S4.T6.1.1.1.m1.1.1\">&#119896;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T6.1.1.1.m1.1c\">k</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T6.1.1.1.m1.1d\">italic_k</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T6.1.1.3\">Bi-level Routing tokens to attend</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S4.T6.1.1.4\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T6.1.1.4.1\">\n<tr class=\"ltx_tr\" id=\"S4.T6.1.1.4.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T6.1.1.4.1.1.1\">CoreML</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.1.4.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T6.1.1.4.1.2.1\">Latency(ms)</td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T6.1.1.5\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T6.1.1.5.1\">\n<tr class=\"ltx_tr\" id=\"S4.T6.1.1.5.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T6.1.1.5.1.1.1\">Top-1 Acc</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.1.5.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T6.1.1.5.1.2.1\">(%)</td>\n</tr>\n</table>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T6.1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T6.1.2.1.1\">7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T6.1.2.1.2\">1,4,16,49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T6.1.2.1.3\">64, 64, 64, 49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T6.1.2.1.4\">291</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.1.2.1.5\">81.81</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T6.1.3.2.1\">7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T6.1.3.2.2\">2,8,32,49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T6.1.3.2.3\">128, 128, 128, 49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T6.1.3.2.4\">459</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.1.3.2.5\">81.74</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T6.1.4.3.1\">7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T6.1.4.3.2\">4,8,16,32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T6.1.4.3.3\">256, 128, 64, 32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T6.1.4.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.4.3.4.1\">276</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.1.4.3.5\">81.47</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T6.1.5.4.1\">7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T6.1.5.4.2\">8,16,32,49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T6.1.5.4.3\">512, 256, 128, 49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T6.1.5.4.4\">498</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.1.5.4.5\">81.60</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S4.T6.1.6.5.1\">7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S4.T6.1.6.5.2\">4,8,16,49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S4.T6.1.6.5.3\">256, 128, 64, 49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S4.T6.1.6.5.4\">382</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S4.T6.1.6.5.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.6.5.5.1\">81.90</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 6:  Ablation study on top-k selection.The four numbers in the k column represent the top-k values of the four stages and the same to the tokens to attend column. The CoreML Latency is conducted by MacBook Pro M1 using CPU and Nerual Engine. ",
        "footnotes": [],
        "references": []
    },
    "S4.T6.1.1.4.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T6.1.1.4.1\">\n<tr class=\"ltx_tr\" id=\"S4.T6.1.1.4.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T6.1.1.4.1.1.1\">CoreML</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.1.4.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T6.1.1.4.1.2.1\">Latency(ms)</td>\n</tr>\n</table>\n\n",
        "caption": "Table 6:  Ablation study on top-k selection.The four numbers in the k column represent the top-k values of the four stages and the same to the tokens to attend column. The CoreML Latency is conducted by MacBook Pro M1 using CPU and Nerual Engine. ",
        "footnotes": [],
        "references": []
    },
    "S4.T6.1.1.5.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T6.1.1.5.1\">\n<tr class=\"ltx_tr\" id=\"S4.T6.1.1.5.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T6.1.1.5.1.1.1\">Top-1 Acc</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.1.5.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T6.1.1.5.1.2.1\">(%)</td>\n</tr>\n</table>\n\n",
        "caption": "Table 6:  Ablation study on top-k selection.The four numbers in the k column represent the top-k values of the four stages and the same to the tokens to attend column. The CoreML Latency is conducted by MacBook Pro M1 using CPU and Nerual Engine. ",
        "footnotes": [],
        "references": []
    },
    "S4.T7.1": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T7.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T7.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"4\" id=\"S4.T7.1.1.1.1\">Stage Configurations</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\" id=\"S4.T7.1.1.1.2\">Backbone</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.1.1.1.3\">IN-1K</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.1.2.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.2.2.1\">1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.2.2.2\">2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.2.2.3\">3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T7.1.2.2.4\">4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.2.2.5\">FLOPs</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T7.1.2.2.6\">#Param</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.2.2.7\">Acc.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.1.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.1.3.3.1\">BB</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.1.3.3.2\">BB</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.1.3.3.3\">BB</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T7.1.3.3.4\">BB</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.1.3.3.5\">2.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T7.1.3.3.6\">13.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T7.1.3.3.7\">81.37</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.1.4.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.4.4.1\">BB</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.4.4.2\">BB</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.4.4.3\">BB</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T7.1.4.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.4.4.4.1\">B*</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.4.4.5\">2.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T7.1.4.4.6\">15.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.4.4.7\">81.58</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.1.5.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.5.5.1\">BB</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.5.5.2\">BB</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.5.5.3\">BB</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T7.1.5.5.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.5.5.4.1\">**</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.5.5.5\">2.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T7.1.5.5.6\">18.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.5.5.7\">81.63</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.1.6.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.6.6.1\">BB</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.6.6.2\">BB</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.6.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.6.6.3.1\">B*</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T7.1.6.6.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.6.6.4.1\">**</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.6.6.5\">2.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T7.1.6.6.6\">21.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.6.6.7\">81.84</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.1.7.7\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.7.7.1\">BB</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.7.7.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.7.7.2.1\">B*</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.7.7.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.7.7.3.1\">B*</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T7.1.7.7.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.7.7.4.1\">**</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.7.7.5\">2.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T7.1.7.7.6\">21.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T7.1.7.7.7\">81.87</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T7.1.8.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S4.T7.1.8.8.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.8.8.1.1\">B*</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S4.T7.1.8.8.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.8.8.2.1\">B*</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S4.T7.1.8.8.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.8.8.3.1\">B*</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S4.T7.1.8.8.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.8.8.4.1\">**</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S4.T7.1.8.8.5\">2.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S4.T7.1.8.8.6\">21.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S4.T7.1.8.8.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.1.8.8.7.1\">81.90</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 7:  Configuration  B* : this stage is constructed with successive bi-level routing attention and DBRMHA blocks, while BB and  **  denotes that stage uses the same blocks as bi-level routing attention and DBRMHA, respectively.",
        "footnotes": [],
        "references": [
            "We systematically adjusted k𝑘kitalic_k to ensure a reasonable number of tokens attended to deformable queries as the region size diminished in later stages. Exploring various combinations of k𝑘kitalic_k is a viable option. In Table 9, we present ablation results on IN-1K, following DeBiFormer-STL (“STL” denotes Swin-T Layout). A crucial observation from these experiments is that augmenting the number of tokens paid attention to the deformable queries had a detrimental effect on accuracy and latency, and increasing the number of tokens paid attention in stages 1 and 2 had an effect on accuracy.\n\nDeformable Bi-level Routing Multi-Head Attention (DBRMHA) at different stages.\nTo evaluate the impact of design choices, we systematically replaced bi-level routing attention blocks with DBRMHA blocks across different stages, as shown in Table7. Initially, all stages used bi-level routing attention, similar to BiFormer-T [56], achieving 81.3% accuracy in image classification. Replacing just one block in the 4th stage with DBRMHA immediately boosted accuracy by +0.21. Replacing all blocks in the 4th stage added another +0.05. Further DBRMHA replacements in the 3rd stage continued to improve performance across tasks. While gains tapered off with earlier stage replacements, we settled on a final version—DeBiFormer—where all stages use Deformable Bi-level Routing Attention for simplicity."
        ]
    },
    "S7.T8.1": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S7.T8.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S7.T8.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S7.T8.1.1.1.1\">Backbone</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S7.T8.1.1.1.2\">mIoU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S7.T8.1.1.1.3\">MS mIoU</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T8.1.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S7.T8.1.2.2.1\">Swin-S <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib29\" title=\"\">29</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S7.T8.1.2.2.2\">47.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T8.1.2.2.3\">49.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T8.1.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S7.T8.1.3.3.1\">DAT-S <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib47\" title=\"\">47</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S7.T8.1.3.3.2\">48.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T8.1.3.3.3\">49.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T8.1.4.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S7.T8.1.4.4.1\">CSWin-S <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib14\" title=\"\">14</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S7.T8.1.4.4.2\">50.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T8.1.4.4.3\">51.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T8.1.5.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S7.T8.1.5.5.1\">InternImage-S <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib43\" title=\"\">43</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S7.T8.1.5.5.2\">50.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T8.1.5.5.3\">50.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T8.1.6.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S7.T8.1.6.6.1\">CrossFormer-B <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib45\" title=\"\">45</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S7.T8.1.6.6.2\">49.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T8.1.6.6.3\">50.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T8.1.7.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S7.T8.1.7.7.1\">Uniformer-B <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib26\" title=\"\">26</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S7.T8.1.7.7.2\">50.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T8.1.7.7.3\">50.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T8.1.8.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S7.T8.1.8.8.1\">BiFormer-B <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib56\" title=\"\">56</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S7.T8.1.8.8.2\">51.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T8.1.8.8.3\">51.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T8.1.9.9\" style=\"background-color:#96FFFB;\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S7.T8.1.9.9.1\"><span class=\"ltx_text\" id=\"S7.T8.1.9.9.1.1\" style=\"background-color:#96FFFB;\">DeBiFormer-B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S7.T8.1.9.9.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S7.T8.1.9.9.2.1\" style=\"background-color:#96FFFB;\">51.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S7.T8.1.9.9.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S7.T8.1.9.9.3.1\" style=\"background-color:#96FFFB;\">52.0</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 8:  Evaluating DeBiFormer on semantic segmentation with two segmentation heads (UpperNet With MS IoU Scale) on ADE20K dataset.",
        "footnotes": [
            "[29] \nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021)\n\n",
            "[47] \nXia, Z., Pan, X., Song, S., Li, L.E., Huang, G.: Vision transformer with deformable attention. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4794–4803 (June 2022)\n\n",
            "[14] \nDong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin transformer: A general vision transformer backbone with cross-shaped windows. In: Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR). pp. 12124–12134 (2022)\n\n",
            "[43] \nWang, W., Dai, J., Chen, Z., Huang, Z., Li, Z., Zhu, X., Hu, X., Lu, T., Lu, L., Li, H., et al.: Internimage: Exploring large-scale vision foundation models with deformable convolutions. arXiv preprint arXiv:2211.05778 (2022)\n\n",
            "[45] \nWang, W., Yao, L., Chen, L., Lin, B., Cai, D., He, X., Liu, W.: Crossformer: A versatile vision transformer hinging on cross-scale attention. In: Proceedings of the International Conference on Learning Representations (ICLR) (2022),  https://openreview.net/forum?id=_PHymLIxuI \n",
            "[26] \nLi, K., Wang, Y., Gao, P., Song, G., Liu, Y., Li, H., Qiao, Y.: Uniformer: Unified transformer for efficient spatiotemporal representation learning (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n"
        ],
        "references": [
            "Settings.\nThe same as existing works, we used our DeBiFormer on SemanticFPN [46] and UperNet [48]. In both cases, the backbone was initialized with ImageNet-1K pretrained weights. The optimizer was AdamW [31], and the batch size was 32. For a fair comparison, we followed the same setting as PVT [44] to train the model with 80k steps and Swin Transformer [29] to train the model with 160k steps.\n\nResults.\nTable 8 shows the results of the two different frameworks. It shows that with the Semantic FPN framework, our DeBiFormer-S/B achieved 49.2/50.6 mIoU, respectively, improving BiFormer by 0.3pt./0.7pt. A similar performance gain for the UperNet framework was also observed. By utilizing the DBRA module, our DeBiFormer could caputure the most semantic key-value pairs, which makes the attention selection more reasonable and achieve higher performance on downstream semantic tasks.",
            "For ADE20K, we utilized the AdamW optimizer with\nan initial learning rate of 0.00006, a weight decay of 0.01, and a mini batch size of 16 for all models trained for 160K iterations. In terms of testing, we reported the results using both single-scale (SS) and multi-scale (MS) testing in the main comparisons. For multi-scale testing, we experimented with resolutions ranging from 0.5 to 1.75 times that of the training resolution. To set the path drop rates in different models, we used the same hyper-parameters as those used for object detection and instance segmentation. Table 8 shows the results of the Upernet frameworks with the single and Multi-Scale IoU."
        ]
    },
    "S7.T9.1": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S7.T9.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S7.T9.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"S7.T9.1.1.1.1\">#Backbone</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S7.T9.1.1.1.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S7.T9.1.1.1.2.1\">\n<tr class=\"ltx_tr\" id=\"S7.T9.1.1.1.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S7.T9.1.1.1.2.1.1.1\">Token to attend</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T9.1.1.1.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S7.T9.1.1.1.2.1.2.1\">for each query</td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S7.T9.1.1.1.3\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S7.T9.1.1.1.3.1\">\n<tr class=\"ltx_tr\" id=\"S7.T9.1.1.1.3.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S7.T9.1.1.1.3.1.1.1\">IN1K</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T9.1.1.1.3.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S7.T9.1.1.1.3.1.2.1\">Top1(%)</td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S7.T9.1.1.1.4\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S7.T9.1.1.1.4.1\">\n<tr class=\"ltx_tr\" id=\"S7.T9.1.1.1.4.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S7.T9.1.1.1.4.1.1.1\">ADE20K</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T9.1.1.1.4.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S7.T9.1.1.1.4.1.2.1\">mIoU(%)</td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S7.T9.1.1.1.5\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S7.T9.1.1.1.5.1\">\n<tr class=\"ltx_tr\" id=\"S7.T9.1.1.1.5.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S7.T9.1.1.1.5.1.1.1\">COCO</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T9.1.1.1.5.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S7.T9.1.1.1.5.1.2.1\">mAP(%)</td>\n</tr>\n</table>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S7.T9.1.2.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S7.T9.1.2.1.1\">QuadTree-B3 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib37\" title=\"\">37</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S7.T9.1.2.1.2\">2048, 512, 128, 32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S7.T9.1.2.1.3\">83.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S7.T9.1.2.1.4\">50.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S7.T9.1.2.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S7.T9.1.2.1.5.1\">47.3</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T9.1.3.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S7.T9.1.3.2.1\">BiFormer-B <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.08582v1#bib.bib56\" title=\"\">56</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S7.T9.1.3.2.2\">64, 64, 64, 49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S7.T9.1.3.2.3\">84.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S7.T9.1.3.2.4\">49.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T9.1.3.2.5\">47.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T9.1.4.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\" id=\"S7.T9.1.4.3.1\">Debiformer-B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S7.T9.1.4.3.2\">1, 1, 1, 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S7.T9.1.4.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S7.T9.1.4.3.3.1\">84.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S7.T9.1.4.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S7.T9.1.4.3.4.1\">50.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S7.T9.1.4.3.5\">47.1</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 9:  Comparing different backbones Token to attend for each query in on ImageNet-1K, ADE20K, COCO.",
        "footnotes": [
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n"
        ],
        "references": [
            "We systematically adjusted k𝑘kitalic_k to ensure a reasonable number of tokens attended to deformable queries as the region size diminished in later stages. Exploring various combinations of k𝑘kitalic_k is a viable option. In Table 9, we present ablation results on IN-1K, following DeBiFormer-STL (“STL” denotes Swin-T Layout). A crucial observation from these experiments is that augmenting the number of tokens paid attention to the deformable queries had a detrimental effect on accuracy and latency, and increasing the number of tokens paid attention in stages 1 and 2 had an effect on accuracy.\n\nDeformable Bi-level Routing Multi-Head Attention (DBRMHA) at different stages.\nTo evaluate the impact of design choices, we systematically replaced bi-level routing attention blocks with DBRMHA blocks across different stages, as shown in Table7. Initially, all stages used bi-level routing attention, similar to BiFormer-T [56], achieving 81.3% accuracy in image classification. Replacing just one block in the 4th stage with DBRMHA immediately boosted accuracy by +0.21. Replacing all blocks in the 4th stage added another +0.05. Further DBRMHA replacements in the 3rd stage continued to improve performance across tasks. While gains tapered off with earlier stage replacements, we settled on a final version—DeBiFormer—where all stages use Deformable Bi-level Routing Attention for simplicity.",
            "In Table 9, we present the token to attend to the query and the token to attend to the deformable point. Compared with other methods, DeBiFormer has the fewest tokens to attend for each query but has a high performance in Imagenet1K, ADE20K(S-FPN head) and COCO(Retina head)."
        ]
    },
    "S7.T9.1.1.1.2.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S7.T9.1.1.1.2.1\">\n<tr class=\"ltx_tr\" id=\"S7.T9.1.1.1.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S7.T9.1.1.1.2.1.1.1\">Token to attend</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T9.1.1.1.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S7.T9.1.1.1.2.1.2.1\">for each query</td>\n</tr>\n</table>\n\n",
        "caption": "Table 9:  Comparing different backbones Token to attend for each query in on ImageNet-1K, ADE20K, COCO.",
        "footnotes": [
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n"
        ],
        "references": [
            "We systematically adjusted k𝑘kitalic_k to ensure a reasonable number of tokens attended to deformable queries as the region size diminished in later stages. Exploring various combinations of k𝑘kitalic_k is a viable option. In Table 9, we present ablation results on IN-1K, following DeBiFormer-STL (“STL” denotes Swin-T Layout). A crucial observation from these experiments is that augmenting the number of tokens paid attention to the deformable queries had a detrimental effect on accuracy and latency, and increasing the number of tokens paid attention in stages 1 and 2 had an effect on accuracy.\n\nDeformable Bi-level Routing Multi-Head Attention (DBRMHA) at different stages.\nTo evaluate the impact of design choices, we systematically replaced bi-level routing attention blocks with DBRMHA blocks across different stages, as shown in Table7. Initially, all stages used bi-level routing attention, similar to BiFormer-T [56], achieving 81.3% accuracy in image classification. Replacing just one block in the 4th stage with DBRMHA immediately boosted accuracy by +0.21. Replacing all blocks in the 4th stage added another +0.05. Further DBRMHA replacements in the 3rd stage continued to improve performance across tasks. While gains tapered off with earlier stage replacements, we settled on a final version—DeBiFormer—where all stages use Deformable Bi-level Routing Attention for simplicity.",
            "In Table 9, we present the token to attend to the query and the token to attend to the deformable point. Compared with other methods, DeBiFormer has the fewest tokens to attend for each query but has a high performance in Imagenet1K, ADE20K(S-FPN head) and COCO(Retina head)."
        ]
    },
    "S7.T9.1.1.1.3.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S7.T9.1.1.1.3.1\">\n<tr class=\"ltx_tr\" id=\"S7.T9.1.1.1.3.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S7.T9.1.1.1.3.1.1.1\">IN1K</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T9.1.1.1.3.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S7.T9.1.1.1.3.1.2.1\">Top1(%)</td>\n</tr>\n</table>\n\n",
        "caption": "Table 9:  Comparing different backbones Token to attend for each query in on ImageNet-1K, ADE20K, COCO.",
        "footnotes": [
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n"
        ],
        "references": [
            "We systematically adjusted k𝑘kitalic_k to ensure a reasonable number of tokens attended to deformable queries as the region size diminished in later stages. Exploring various combinations of k𝑘kitalic_k is a viable option. In Table 9, we present ablation results on IN-1K, following DeBiFormer-STL (“STL” denotes Swin-T Layout). A crucial observation from these experiments is that augmenting the number of tokens paid attention to the deformable queries had a detrimental effect on accuracy and latency, and increasing the number of tokens paid attention in stages 1 and 2 had an effect on accuracy.\n\nDeformable Bi-level Routing Multi-Head Attention (DBRMHA) at different stages.\nTo evaluate the impact of design choices, we systematically replaced bi-level routing attention blocks with DBRMHA blocks across different stages, as shown in Table7. Initially, all stages used bi-level routing attention, similar to BiFormer-T [56], achieving 81.3% accuracy in image classification. Replacing just one block in the 4th stage with DBRMHA immediately boosted accuracy by +0.21. Replacing all blocks in the 4th stage added another +0.05. Further DBRMHA replacements in the 3rd stage continued to improve performance across tasks. While gains tapered off with earlier stage replacements, we settled on a final version—DeBiFormer—where all stages use Deformable Bi-level Routing Attention for simplicity.",
            "In Table 9, we present the token to attend to the query and the token to attend to the deformable point. Compared with other methods, DeBiFormer has the fewest tokens to attend for each query but has a high performance in Imagenet1K, ADE20K(S-FPN head) and COCO(Retina head)."
        ]
    },
    "S7.T9.1.1.1.4.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S7.T9.1.1.1.4.1\">\n<tr class=\"ltx_tr\" id=\"S7.T9.1.1.1.4.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S7.T9.1.1.1.4.1.1.1\">ADE20K</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T9.1.1.1.4.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S7.T9.1.1.1.4.1.2.1\">mIoU(%)</td>\n</tr>\n</table>\n\n",
        "caption": "Table 9:  Comparing different backbones Token to attend for each query in on ImageNet-1K, ADE20K, COCO.",
        "footnotes": [
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n"
        ],
        "references": [
            "We systematically adjusted k𝑘kitalic_k to ensure a reasonable number of tokens attended to deformable queries as the region size diminished in later stages. Exploring various combinations of k𝑘kitalic_k is a viable option. In Table 9, we present ablation results on IN-1K, following DeBiFormer-STL (“STL” denotes Swin-T Layout). A crucial observation from these experiments is that augmenting the number of tokens paid attention to the deformable queries had a detrimental effect on accuracy and latency, and increasing the number of tokens paid attention in stages 1 and 2 had an effect on accuracy.\n\nDeformable Bi-level Routing Multi-Head Attention (DBRMHA) at different stages.\nTo evaluate the impact of design choices, we systematically replaced bi-level routing attention blocks with DBRMHA blocks across different stages, as shown in Table7. Initially, all stages used bi-level routing attention, similar to BiFormer-T [56], achieving 81.3% accuracy in image classification. Replacing just one block in the 4th stage with DBRMHA immediately boosted accuracy by +0.21. Replacing all blocks in the 4th stage added another +0.05. Further DBRMHA replacements in the 3rd stage continued to improve performance across tasks. While gains tapered off with earlier stage replacements, we settled on a final version—DeBiFormer—where all stages use Deformable Bi-level Routing Attention for simplicity.",
            "In Table 9, we present the token to attend to the query and the token to attend to the deformable point. Compared with other methods, DeBiFormer has the fewest tokens to attend for each query but has a high performance in Imagenet1K, ADE20K(S-FPN head) and COCO(Retina head)."
        ]
    },
    "S7.T9.1.1.1.5.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S7.T9.1.1.1.5.1\">\n<tr class=\"ltx_tr\" id=\"S7.T9.1.1.1.5.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S7.T9.1.1.1.5.1.1.1\">COCO</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T9.1.1.1.5.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S7.T9.1.1.1.5.1.2.1\">mAP(%)</td>\n</tr>\n</table>\n\n",
        "caption": "Table 9:  Comparing different backbones Token to attend for each query in on ImageNet-1K, ADE20K, COCO.",
        "footnotes": [
            "[37] \nTang, S., Zhang, J., Zhu, S., Tan, P.: Quadtree attention for vision transformers. ICLR (2022)\n\n",
            "[56] \nZhu, L., Wang, X., Ke, Z., Zhang, W., Lau, R.: Biformer: Vision transformer with bi-level routing attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023)\n\n"
        ],
        "references": [
            "We systematically adjusted k𝑘kitalic_k to ensure a reasonable number of tokens attended to deformable queries as the region size diminished in later stages. Exploring various combinations of k𝑘kitalic_k is a viable option. In Table 9, we present ablation results on IN-1K, following DeBiFormer-STL (“STL” denotes Swin-T Layout). A crucial observation from these experiments is that augmenting the number of tokens paid attention to the deformable queries had a detrimental effect on accuracy and latency, and increasing the number of tokens paid attention in stages 1 and 2 had an effect on accuracy.\n\nDeformable Bi-level Routing Multi-Head Attention (DBRMHA) at different stages.\nTo evaluate the impact of design choices, we systematically replaced bi-level routing attention blocks with DBRMHA blocks across different stages, as shown in Table7. Initially, all stages used bi-level routing attention, similar to BiFormer-T [56], achieving 81.3% accuracy in image classification. Replacing just one block in the 4th stage with DBRMHA immediately boosted accuracy by +0.21. Replacing all blocks in the 4th stage added another +0.05. Further DBRMHA replacements in the 3rd stage continued to improve performance across tasks. While gains tapered off with earlier stage replacements, we settled on a final version—DeBiFormer—where all stages use Deformable Bi-level Routing Attention for simplicity.",
            "In Table 9, we present the token to attend to the query and the token to attend to the deformable point. Compared with other methods, DeBiFormer has the fewest tokens to attend for each query but has a high performance in Imagenet1K, ADE20K(S-FPN head) and COCO(Retina head)."
        ]
    },
    "S7.T10.6": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S7.T10.6\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S7.T10.6.7.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S7.T10.6.7.1.1\"><span class=\"ltx_text\" id=\"S7.T10.6.7.1.1.1\" style=\"font-size:90%;\">Settings</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S7.T10.6.7.1.2\"><span class=\"ltx_text\" id=\"S7.T10.6.7.1.2.1\" style=\"font-size:90%;\">DeBi-T</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S7.T10.6.7.1.3\"><span class=\"ltx_text\" id=\"S7.T10.6.7.1.3.1\" style=\"font-size:90%;\">DeBi-S</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S7.T10.6.7.1.4\"><span class=\"ltx_text\" id=\"S7.T10.6.7.1.4.1\" style=\"font-size:90%;\">DeBi-B</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T10.6.8.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S7.T10.6.8.2.1\"><span class=\"ltx_text\" id=\"S7.T10.6.8.2.1.1\" style=\"font-size:90%;\">Input resolution</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S7.T10.6.8.2.2\"><span class=\"ltx_text\" id=\"S7.T10.6.8.2.2.1\" style=\"font-size:90%;\">224</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S7.T10.6.8.2.3\"><span class=\"ltx_text\" id=\"S7.T10.6.8.2.3.1\" style=\"font-size:90%;\">224</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S7.T10.6.8.2.4\"><span class=\"ltx_text\" id=\"S7.T10.6.8.2.4.1\" style=\"font-size:90%;\">224</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T10.6.9.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S7.T10.6.9.3.1\"><span class=\"ltx_text\" id=\"S7.T10.6.9.3.1.1\" style=\"font-size:90%;\">Batch size</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.9.3.2\"><span class=\"ltx_text\" id=\"S7.T10.6.9.3.2.1\" style=\"font-size:90%;\">1024</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.9.3.3\"><span class=\"ltx_text\" id=\"S7.T10.6.9.3.3.1\" style=\"font-size:90%;\">512</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.9.3.4\"><span class=\"ltx_text\" id=\"S7.T10.6.9.3.4.1\" style=\"font-size:90%;\">512</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T10.6.10.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S7.T10.6.10.4.1\"><span class=\"ltx_text\" id=\"S7.T10.6.10.4.1.1\" style=\"font-size:90%;\">Optimizer</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.10.4.2\"><span class=\"ltx_text\" id=\"S7.T10.6.10.4.2.1\" style=\"font-size:90%;\">AdamW</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.10.4.3\"><span class=\"ltx_text\" id=\"S7.T10.6.10.4.3.1\" style=\"font-size:90%;\">AdamW</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.10.4.4\"><span class=\"ltx_text\" id=\"S7.T10.6.10.4.4.1\" style=\"font-size:90%;\">AdamW</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T10.3.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S7.T10.3.3.4\"><span class=\"ltx_text\" id=\"S7.T10.3.3.4.1\" style=\"font-size:90%;\">Learning rate</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.1.1.1\"><math alttext=\"1\\times 10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T10.1.1.1.m1.1\"><semantics id=\"S7.T10.1.1.1.m1.1a\"><mrow id=\"S7.T10.1.1.1.m1.1.1\" xref=\"S7.T10.1.1.1.m1.1.1.cmml\"><mn id=\"S7.T10.1.1.1.m1.1.1.2\" mathsize=\"90%\" xref=\"S7.T10.1.1.1.m1.1.1.2.cmml\">1</mn><mo id=\"S7.T10.1.1.1.m1.1.1.1\" lspace=\"0.222em\" mathsize=\"90%\" rspace=\"0.222em\" xref=\"S7.T10.1.1.1.m1.1.1.1.cmml\">&#215;</mo><msup id=\"S7.T10.1.1.1.m1.1.1.3\" xref=\"S7.T10.1.1.1.m1.1.1.3.cmml\"><mn id=\"S7.T10.1.1.1.m1.1.1.3.2\" mathsize=\"90%\" xref=\"S7.T10.1.1.1.m1.1.1.3.2.cmml\">10</mn><mrow id=\"S7.T10.1.1.1.m1.1.1.3.3\" xref=\"S7.T10.1.1.1.m1.1.1.3.3.cmml\"><mo id=\"S7.T10.1.1.1.m1.1.1.3.3a\" mathsize=\"90%\" xref=\"S7.T10.1.1.1.m1.1.1.3.3.cmml\">&#8722;</mo><mn id=\"S7.T10.1.1.1.m1.1.1.3.3.2\" mathsize=\"90%\" xref=\"S7.T10.1.1.1.m1.1.1.3.3.2.cmml\">3</mn></mrow></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S7.T10.1.1.1.m1.1b\"><apply id=\"S7.T10.1.1.1.m1.1.1.cmml\" xref=\"S7.T10.1.1.1.m1.1.1\"><times id=\"S7.T10.1.1.1.m1.1.1.1.cmml\" xref=\"S7.T10.1.1.1.m1.1.1.1\"/><cn id=\"S7.T10.1.1.1.m1.1.1.2.cmml\" type=\"integer\" xref=\"S7.T10.1.1.1.m1.1.1.2\">1</cn><apply id=\"S7.T10.1.1.1.m1.1.1.3.cmml\" xref=\"S7.T10.1.1.1.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S7.T10.1.1.1.m1.1.1.3.1.cmml\" xref=\"S7.T10.1.1.1.m1.1.1.3\">superscript</csymbol><cn id=\"S7.T10.1.1.1.m1.1.1.3.2.cmml\" type=\"integer\" xref=\"S7.T10.1.1.1.m1.1.1.3.2\">10</cn><apply id=\"S7.T10.1.1.1.m1.1.1.3.3.cmml\" xref=\"S7.T10.1.1.1.m1.1.1.3.3\"><minus id=\"S7.T10.1.1.1.m1.1.1.3.3.1.cmml\" xref=\"S7.T10.1.1.1.m1.1.1.3.3\"/><cn id=\"S7.T10.1.1.1.m1.1.1.3.3.2.cmml\" type=\"integer\" xref=\"S7.T10.1.1.1.m1.1.1.3.3.2\">3</cn></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T10.1.1.1.m1.1c\">1\\times 10^{-3}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S7.T10.1.1.1.m1.1d\">1 &#215; 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.2.2.2\"><math alttext=\"5\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T10.2.2.2.m1.1\"><semantics id=\"S7.T10.2.2.2.m1.1a\"><mrow id=\"S7.T10.2.2.2.m1.1.1\" xref=\"S7.T10.2.2.2.m1.1.1.cmml\"><mn id=\"S7.T10.2.2.2.m1.1.1.2\" mathsize=\"90%\" xref=\"S7.T10.2.2.2.m1.1.1.2.cmml\">5</mn><mo id=\"S7.T10.2.2.2.m1.1.1.1\" lspace=\"0.222em\" mathsize=\"90%\" rspace=\"0.222em\" xref=\"S7.T10.2.2.2.m1.1.1.1.cmml\">&#215;</mo><msup id=\"S7.T10.2.2.2.m1.1.1.3\" xref=\"S7.T10.2.2.2.m1.1.1.3.cmml\"><mn id=\"S7.T10.2.2.2.m1.1.1.3.2\" mathsize=\"90%\" xref=\"S7.T10.2.2.2.m1.1.1.3.2.cmml\">10</mn><mrow id=\"S7.T10.2.2.2.m1.1.1.3.3\" xref=\"S7.T10.2.2.2.m1.1.1.3.3.cmml\"><mo id=\"S7.T10.2.2.2.m1.1.1.3.3a\" mathsize=\"90%\" xref=\"S7.T10.2.2.2.m1.1.1.3.3.cmml\">&#8722;</mo><mn id=\"S7.T10.2.2.2.m1.1.1.3.3.2\" mathsize=\"90%\" xref=\"S7.T10.2.2.2.m1.1.1.3.3.2.cmml\">4</mn></mrow></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S7.T10.2.2.2.m1.1b\"><apply id=\"S7.T10.2.2.2.m1.1.1.cmml\" xref=\"S7.T10.2.2.2.m1.1.1\"><times id=\"S7.T10.2.2.2.m1.1.1.1.cmml\" xref=\"S7.T10.2.2.2.m1.1.1.1\"/><cn id=\"S7.T10.2.2.2.m1.1.1.2.cmml\" type=\"integer\" xref=\"S7.T10.2.2.2.m1.1.1.2\">5</cn><apply id=\"S7.T10.2.2.2.m1.1.1.3.cmml\" xref=\"S7.T10.2.2.2.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S7.T10.2.2.2.m1.1.1.3.1.cmml\" xref=\"S7.T10.2.2.2.m1.1.1.3\">superscript</csymbol><cn id=\"S7.T10.2.2.2.m1.1.1.3.2.cmml\" type=\"integer\" xref=\"S7.T10.2.2.2.m1.1.1.3.2\">10</cn><apply id=\"S7.T10.2.2.2.m1.1.1.3.3.cmml\" xref=\"S7.T10.2.2.2.m1.1.1.3.3\"><minus id=\"S7.T10.2.2.2.m1.1.1.3.3.1.cmml\" xref=\"S7.T10.2.2.2.m1.1.1.3.3\"/><cn id=\"S7.T10.2.2.2.m1.1.1.3.3.2.cmml\" type=\"integer\" xref=\"S7.T10.2.2.2.m1.1.1.3.3.2\">4</cn></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T10.2.2.2.m1.1c\">5\\times 10^{-4}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S7.T10.2.2.2.m1.1d\">5 &#215; 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.3.3.3\"><math alttext=\"5\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T10.3.3.3.m1.1\"><semantics id=\"S7.T10.3.3.3.m1.1a\"><mrow id=\"S7.T10.3.3.3.m1.1.1\" xref=\"S7.T10.3.3.3.m1.1.1.cmml\"><mn id=\"S7.T10.3.3.3.m1.1.1.2\" mathsize=\"90%\" xref=\"S7.T10.3.3.3.m1.1.1.2.cmml\">5</mn><mo id=\"S7.T10.3.3.3.m1.1.1.1\" lspace=\"0.222em\" mathsize=\"90%\" rspace=\"0.222em\" xref=\"S7.T10.3.3.3.m1.1.1.1.cmml\">&#215;</mo><msup id=\"S7.T10.3.3.3.m1.1.1.3\" xref=\"S7.T10.3.3.3.m1.1.1.3.cmml\"><mn id=\"S7.T10.3.3.3.m1.1.1.3.2\" mathsize=\"90%\" xref=\"S7.T10.3.3.3.m1.1.1.3.2.cmml\">10</mn><mrow id=\"S7.T10.3.3.3.m1.1.1.3.3\" xref=\"S7.T10.3.3.3.m1.1.1.3.3.cmml\"><mo id=\"S7.T10.3.3.3.m1.1.1.3.3a\" mathsize=\"90%\" xref=\"S7.T10.3.3.3.m1.1.1.3.3.cmml\">&#8722;</mo><mn id=\"S7.T10.3.3.3.m1.1.1.3.3.2\" mathsize=\"90%\" xref=\"S7.T10.3.3.3.m1.1.1.3.3.2.cmml\">4</mn></mrow></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S7.T10.3.3.3.m1.1b\"><apply id=\"S7.T10.3.3.3.m1.1.1.cmml\" xref=\"S7.T10.3.3.3.m1.1.1\"><times id=\"S7.T10.3.3.3.m1.1.1.1.cmml\" xref=\"S7.T10.3.3.3.m1.1.1.1\"/><cn id=\"S7.T10.3.3.3.m1.1.1.2.cmml\" type=\"integer\" xref=\"S7.T10.3.3.3.m1.1.1.2\">5</cn><apply id=\"S7.T10.3.3.3.m1.1.1.3.cmml\" xref=\"S7.T10.3.3.3.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S7.T10.3.3.3.m1.1.1.3.1.cmml\" xref=\"S7.T10.3.3.3.m1.1.1.3\">superscript</csymbol><cn id=\"S7.T10.3.3.3.m1.1.1.3.2.cmml\" type=\"integer\" xref=\"S7.T10.3.3.3.m1.1.1.3.2\">10</cn><apply id=\"S7.T10.3.3.3.m1.1.1.3.3.cmml\" xref=\"S7.T10.3.3.3.m1.1.1.3.3\"><minus id=\"S7.T10.3.3.3.m1.1.1.3.3.1.cmml\" xref=\"S7.T10.3.3.3.m1.1.1.3.3\"/><cn id=\"S7.T10.3.3.3.m1.1.1.3.3.2.cmml\" type=\"integer\" xref=\"S7.T10.3.3.3.m1.1.1.3.3.2\">4</cn></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T10.3.3.3.m1.1c\">5\\times 10^{-4}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S7.T10.3.3.3.m1.1d\">5 &#215; 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T10.6.11.5\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S7.T10.6.11.5.1\"><span class=\"ltx_text\" id=\"S7.T10.6.11.5.1.1\" style=\"font-size:90%;\">LR schedule</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.11.5.2\"><span class=\"ltx_text\" id=\"S7.T10.6.11.5.2.1\" style=\"font-size:90%;\">cosine</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.11.5.3\"><span class=\"ltx_text\" id=\"S7.T10.6.11.5.3.1\" style=\"font-size:90%;\">cosine</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.11.5.4\"><span class=\"ltx_text\" id=\"S7.T10.6.11.5.4.1\" style=\"font-size:90%;\">cosine</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T10.6.6\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S7.T10.6.6.4\"><span class=\"ltx_text\" id=\"S7.T10.6.6.4.1\" style=\"font-size:90%;\">Weight decay</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.4.4.1\"><math alttext=\"5\\times 10^{-2}\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T10.4.4.1.m1.1\"><semantics id=\"S7.T10.4.4.1.m1.1a\"><mrow id=\"S7.T10.4.4.1.m1.1.1\" xref=\"S7.T10.4.4.1.m1.1.1.cmml\"><mn id=\"S7.T10.4.4.1.m1.1.1.2\" mathsize=\"90%\" xref=\"S7.T10.4.4.1.m1.1.1.2.cmml\">5</mn><mo id=\"S7.T10.4.4.1.m1.1.1.1\" lspace=\"0.222em\" mathsize=\"90%\" rspace=\"0.222em\" xref=\"S7.T10.4.4.1.m1.1.1.1.cmml\">&#215;</mo><msup id=\"S7.T10.4.4.1.m1.1.1.3\" xref=\"S7.T10.4.4.1.m1.1.1.3.cmml\"><mn id=\"S7.T10.4.4.1.m1.1.1.3.2\" mathsize=\"90%\" xref=\"S7.T10.4.4.1.m1.1.1.3.2.cmml\">10</mn><mrow id=\"S7.T10.4.4.1.m1.1.1.3.3\" xref=\"S7.T10.4.4.1.m1.1.1.3.3.cmml\"><mo id=\"S7.T10.4.4.1.m1.1.1.3.3a\" mathsize=\"90%\" xref=\"S7.T10.4.4.1.m1.1.1.3.3.cmml\">&#8722;</mo><mn id=\"S7.T10.4.4.1.m1.1.1.3.3.2\" mathsize=\"90%\" xref=\"S7.T10.4.4.1.m1.1.1.3.3.2.cmml\">2</mn></mrow></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S7.T10.4.4.1.m1.1b\"><apply id=\"S7.T10.4.4.1.m1.1.1.cmml\" xref=\"S7.T10.4.4.1.m1.1.1\"><times id=\"S7.T10.4.4.1.m1.1.1.1.cmml\" xref=\"S7.T10.4.4.1.m1.1.1.1\"/><cn id=\"S7.T10.4.4.1.m1.1.1.2.cmml\" type=\"integer\" xref=\"S7.T10.4.4.1.m1.1.1.2\">5</cn><apply id=\"S7.T10.4.4.1.m1.1.1.3.cmml\" xref=\"S7.T10.4.4.1.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S7.T10.4.4.1.m1.1.1.3.1.cmml\" xref=\"S7.T10.4.4.1.m1.1.1.3\">superscript</csymbol><cn id=\"S7.T10.4.4.1.m1.1.1.3.2.cmml\" type=\"integer\" xref=\"S7.T10.4.4.1.m1.1.1.3.2\">10</cn><apply id=\"S7.T10.4.4.1.m1.1.1.3.3.cmml\" xref=\"S7.T10.4.4.1.m1.1.1.3.3\"><minus id=\"S7.T10.4.4.1.m1.1.1.3.3.1.cmml\" xref=\"S7.T10.4.4.1.m1.1.1.3.3\"/><cn id=\"S7.T10.4.4.1.m1.1.1.3.3.2.cmml\" type=\"integer\" xref=\"S7.T10.4.4.1.m1.1.1.3.3.2\">2</cn></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T10.4.4.1.m1.1c\">5\\times 10^{-2}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S7.T10.4.4.1.m1.1d\">5 &#215; 10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.5.5.2\"><math alttext=\"5\\times 10^{-2}\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T10.5.5.2.m1.1\"><semantics id=\"S7.T10.5.5.2.m1.1a\"><mrow id=\"S7.T10.5.5.2.m1.1.1\" xref=\"S7.T10.5.5.2.m1.1.1.cmml\"><mn id=\"S7.T10.5.5.2.m1.1.1.2\" mathsize=\"90%\" xref=\"S7.T10.5.5.2.m1.1.1.2.cmml\">5</mn><mo id=\"S7.T10.5.5.2.m1.1.1.1\" lspace=\"0.222em\" mathsize=\"90%\" rspace=\"0.222em\" xref=\"S7.T10.5.5.2.m1.1.1.1.cmml\">&#215;</mo><msup id=\"S7.T10.5.5.2.m1.1.1.3\" xref=\"S7.T10.5.5.2.m1.1.1.3.cmml\"><mn id=\"S7.T10.5.5.2.m1.1.1.3.2\" mathsize=\"90%\" xref=\"S7.T10.5.5.2.m1.1.1.3.2.cmml\">10</mn><mrow id=\"S7.T10.5.5.2.m1.1.1.3.3\" xref=\"S7.T10.5.5.2.m1.1.1.3.3.cmml\"><mo id=\"S7.T10.5.5.2.m1.1.1.3.3a\" mathsize=\"90%\" xref=\"S7.T10.5.5.2.m1.1.1.3.3.cmml\">&#8722;</mo><mn id=\"S7.T10.5.5.2.m1.1.1.3.3.2\" mathsize=\"90%\" xref=\"S7.T10.5.5.2.m1.1.1.3.3.2.cmml\">2</mn></mrow></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S7.T10.5.5.2.m1.1b\"><apply id=\"S7.T10.5.5.2.m1.1.1.cmml\" xref=\"S7.T10.5.5.2.m1.1.1\"><times id=\"S7.T10.5.5.2.m1.1.1.1.cmml\" xref=\"S7.T10.5.5.2.m1.1.1.1\"/><cn id=\"S7.T10.5.5.2.m1.1.1.2.cmml\" type=\"integer\" xref=\"S7.T10.5.5.2.m1.1.1.2\">5</cn><apply id=\"S7.T10.5.5.2.m1.1.1.3.cmml\" xref=\"S7.T10.5.5.2.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S7.T10.5.5.2.m1.1.1.3.1.cmml\" xref=\"S7.T10.5.5.2.m1.1.1.3\">superscript</csymbol><cn id=\"S7.T10.5.5.2.m1.1.1.3.2.cmml\" type=\"integer\" xref=\"S7.T10.5.5.2.m1.1.1.3.2\">10</cn><apply id=\"S7.T10.5.5.2.m1.1.1.3.3.cmml\" xref=\"S7.T10.5.5.2.m1.1.1.3.3\"><minus id=\"S7.T10.5.5.2.m1.1.1.3.3.1.cmml\" xref=\"S7.T10.5.5.2.m1.1.1.3.3\"/><cn id=\"S7.T10.5.5.2.m1.1.1.3.3.2.cmml\" type=\"integer\" xref=\"S7.T10.5.5.2.m1.1.1.3.3.2\">2</cn></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T10.5.5.2.m1.1c\">5\\times 10^{-2}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S7.T10.5.5.2.m1.1d\">5 &#215; 10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.6.3\"><math alttext=\"5\\times 10^{-2}\" class=\"ltx_Math\" display=\"inline\" id=\"S7.T10.6.6.3.m1.1\"><semantics id=\"S7.T10.6.6.3.m1.1a\"><mrow id=\"S7.T10.6.6.3.m1.1.1\" xref=\"S7.T10.6.6.3.m1.1.1.cmml\"><mn id=\"S7.T10.6.6.3.m1.1.1.2\" mathsize=\"90%\" xref=\"S7.T10.6.6.3.m1.1.1.2.cmml\">5</mn><mo id=\"S7.T10.6.6.3.m1.1.1.1\" lspace=\"0.222em\" mathsize=\"90%\" rspace=\"0.222em\" xref=\"S7.T10.6.6.3.m1.1.1.1.cmml\">&#215;</mo><msup id=\"S7.T10.6.6.3.m1.1.1.3\" xref=\"S7.T10.6.6.3.m1.1.1.3.cmml\"><mn id=\"S7.T10.6.6.3.m1.1.1.3.2\" mathsize=\"90%\" xref=\"S7.T10.6.6.3.m1.1.1.3.2.cmml\">10</mn><mrow id=\"S7.T10.6.6.3.m1.1.1.3.3\" xref=\"S7.T10.6.6.3.m1.1.1.3.3.cmml\"><mo id=\"S7.T10.6.6.3.m1.1.1.3.3a\" mathsize=\"90%\" xref=\"S7.T10.6.6.3.m1.1.1.3.3.cmml\">&#8722;</mo><mn id=\"S7.T10.6.6.3.m1.1.1.3.3.2\" mathsize=\"90%\" xref=\"S7.T10.6.6.3.m1.1.1.3.3.2.cmml\">2</mn></mrow></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S7.T10.6.6.3.m1.1b\"><apply id=\"S7.T10.6.6.3.m1.1.1.cmml\" xref=\"S7.T10.6.6.3.m1.1.1\"><times id=\"S7.T10.6.6.3.m1.1.1.1.cmml\" xref=\"S7.T10.6.6.3.m1.1.1.1\"/><cn id=\"S7.T10.6.6.3.m1.1.1.2.cmml\" type=\"integer\" xref=\"S7.T10.6.6.3.m1.1.1.2\">5</cn><apply id=\"S7.T10.6.6.3.m1.1.1.3.cmml\" xref=\"S7.T10.6.6.3.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S7.T10.6.6.3.m1.1.1.3.1.cmml\" xref=\"S7.T10.6.6.3.m1.1.1.3\">superscript</csymbol><cn id=\"S7.T10.6.6.3.m1.1.1.3.2.cmml\" type=\"integer\" xref=\"S7.T10.6.6.3.m1.1.1.3.2\">10</cn><apply id=\"S7.T10.6.6.3.m1.1.1.3.3.cmml\" xref=\"S7.T10.6.6.3.m1.1.1.3.3\"><minus id=\"S7.T10.6.6.3.m1.1.1.3.3.1.cmml\" xref=\"S7.T10.6.6.3.m1.1.1.3.3\"/><cn id=\"S7.T10.6.6.3.m1.1.1.3.3.2.cmml\" type=\"integer\" xref=\"S7.T10.6.6.3.m1.1.1.3.3.2\">2</cn></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T10.6.6.3.m1.1c\">5\\times 10^{-2}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S7.T10.6.6.3.m1.1d\">5 &#215; 10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T10.6.12.6\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S7.T10.6.12.6.1\"><span class=\"ltx_text\" id=\"S7.T10.6.12.6.1.1\" style=\"font-size:90%;\">Warmup epochs</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.12.6.2\"><span class=\"ltx_text\" id=\"S7.T10.6.12.6.2.1\" style=\"font-size:90%;\">20</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.12.6.3\"><span class=\"ltx_text\" id=\"S7.T10.6.12.6.3.1\" style=\"font-size:90%;\">20</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.12.6.4\"><span class=\"ltx_text\" id=\"S7.T10.6.12.6.4.1\" style=\"font-size:90%;\">20</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T10.6.13.7\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S7.T10.6.13.7.1\"><span class=\"ltx_text\" id=\"S7.T10.6.13.7.1.1\" style=\"font-size:90%;\">Epochs</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.13.7.2\"><span class=\"ltx_text\" id=\"S7.T10.6.13.7.2.1\" style=\"font-size:90%;\">300</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.13.7.3\"><span class=\"ltx_text\" id=\"S7.T10.6.13.7.3.1\" style=\"font-size:90%;\">300</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.13.7.4\"><span class=\"ltx_text\" id=\"S7.T10.6.13.7.4.1\" style=\"font-size:90%;\">300</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T10.6.14.8\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S7.T10.6.14.8.1\"><span class=\"ltx_text\" id=\"S7.T10.6.14.8.1.1\" style=\"font-size:90%;\">Horizontal flip</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S7.T10.6.14.8.2\"><span class=\"ltx_text\" id=\"S7.T10.6.14.8.2.1\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S7.T10.6.14.8.3\"><span class=\"ltx_text\" id=\"S7.T10.6.14.8.3.1\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S7.T10.6.14.8.4\"><span class=\"ltx_text\" id=\"S7.T10.6.14.8.4.1\" style=\"font-size:90%;\">&#10003;</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T10.6.15.9\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S7.T10.6.15.9.1\"><span class=\"ltx_text\" id=\"S7.T10.6.15.9.1.1\" style=\"font-size:90%;\">Random resize</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.15.9.2\"><span class=\"ltx_text\" id=\"S7.T10.6.15.9.2.1\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.15.9.3\"><span class=\"ltx_text\" id=\"S7.T10.6.15.9.3.1\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.15.9.4\"><span class=\"ltx_text\" id=\"S7.T10.6.15.9.4.1\" style=\"font-size:90%;\">&#10003;</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T10.6.16.10\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S7.T10.6.16.10.1\"><span class=\"ltx_text\" id=\"S7.T10.6.16.10.1.1\" style=\"font-size:90%;\">AutoAugment</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.16.10.2\"><span class=\"ltx_text\" id=\"S7.T10.6.16.10.2.1\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.16.10.3\"><span class=\"ltx_text\" id=\"S7.T10.6.16.10.3.1\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.16.10.4\"><span class=\"ltx_text\" id=\"S7.T10.6.16.10.4.1\" style=\"font-size:90%;\">&#10003;</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T10.6.17.11\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S7.T10.6.17.11.1\"><span class=\"ltx_text\" id=\"S7.T10.6.17.11.1.1\" style=\"font-size:90%;\">Mixup alpha</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.17.11.2\"><span class=\"ltx_text\" id=\"S7.T10.6.17.11.2.1\" style=\"font-size:90%;\">0.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.17.11.3\"><span class=\"ltx_text\" id=\"S7.T10.6.17.11.3.1\" style=\"font-size:90%;\">0.8</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.17.11.4\"><span class=\"ltx_text\" id=\"S7.T10.6.17.11.4.1\" style=\"font-size:90%;\">0.8</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T10.6.18.12\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S7.T10.6.18.12.1\"><span class=\"ltx_text\" id=\"S7.T10.6.18.12.1.1\" style=\"font-size:90%;\">Cutmix alpha</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.18.12.2\"><span class=\"ltx_text\" id=\"S7.T10.6.18.12.2.1\" style=\"font-size:90%;\">1.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.18.12.3\"><span class=\"ltx_text\" id=\"S7.T10.6.18.12.3.1\" style=\"font-size:90%;\">1.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.18.12.4\"><span class=\"ltx_text\" id=\"S7.T10.6.18.12.4.1\" style=\"font-size:90%;\">1.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T10.6.19.13\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S7.T10.6.19.13.1\"><span class=\"ltx_text\" id=\"S7.T10.6.19.13.1.1\" style=\"font-size:90%;\">Random erasing prob.</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.19.13.2\"><span class=\"ltx_text\" id=\"S7.T10.6.19.13.2.1\" style=\"font-size:90%;\">0.25</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.19.13.3\"><span class=\"ltx_text\" id=\"S7.T10.6.19.13.3.1\" style=\"font-size:90%;\">0.25</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.19.13.4\"><span class=\"ltx_text\" id=\"S7.T10.6.19.13.4.1\" style=\"font-size:90%;\">0.25</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T10.6.20.14\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S7.T10.6.20.14.1\"><span class=\"ltx_text\" id=\"S7.T10.6.20.14.1.1\" style=\"font-size:90%;\">Color jitter</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.20.14.2\"><span class=\"ltx_text\" id=\"S7.T10.6.20.14.2.1\" style=\"font-size:90%;\">0.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.20.14.3\"><span class=\"ltx_text\" id=\"S7.T10.6.20.14.3.1\" style=\"font-size:90%;\">0.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.20.14.4\"><span class=\"ltx_text\" id=\"S7.T10.6.20.14.4.1\" style=\"font-size:90%;\">0.4</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T10.6.21.15\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S7.T10.6.21.15.1\"><span class=\"ltx_text\" id=\"S7.T10.6.21.15.1.1\" style=\"font-size:90%;\">Label smoothing</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S7.T10.6.21.15.2\"><span class=\"ltx_text\" id=\"S7.T10.6.21.15.2.1\" style=\"font-size:90%;\">0.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S7.T10.6.21.15.3\"><span class=\"ltx_text\" id=\"S7.T10.6.21.15.3.1\" style=\"font-size:90%;\">0.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S7.T10.6.21.15.4\"><span class=\"ltx_text\" id=\"S7.T10.6.21.15.4.1\" style=\"font-size:90%;\">0.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T10.6.22.16\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S7.T10.6.22.16.1\"><span class=\"ltx_text\" id=\"S7.T10.6.22.16.1.1\" style=\"font-size:90%;\">Droppath rate</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.22.16.2\"><span class=\"ltx_text\" id=\"S7.T10.6.22.16.2.1\" style=\"font-size:90%;\">0.</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.22.16.3\"><span class=\"ltx_text\" id=\"S7.T10.6.22.16.3.1\" style=\"font-size:90%;\">0.2</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T10.6.22.16.4\"><span class=\"ltx_text\" id=\"S7.T10.6.22.16.4.1\" style=\"font-size:90%;\">0.4</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T10.6.23.17\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\" id=\"S7.T10.6.23.17.1\"><span class=\"ltx_text\" id=\"S7.T10.6.23.17.1.1\" style=\"font-size:90%;\">Grad. clipping</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S7.T10.6.23.17.2\"><span class=\"ltx_text\" id=\"S7.T10.6.23.17.2.1\" style=\"font-size:90%;\">5.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S7.T10.6.23.17.3\"><span class=\"ltx_text\" id=\"S7.T10.6.23.17.3.1\" style=\"font-size:90%;\">5.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S7.T10.6.23.17.4\"><span class=\"ltx_text\" id=\"S7.T10.6.23.17.4.1\" style=\"font-size:90%;\">5.0</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 10:  Image Classification Training Settings",
        "footnotes": [],
        "references": [
            "As introduced in the main paper, each model was trained for 300 epochs on 8 V100 GPU with an input size of 224×224224224224\\times 224224 × 224. The experimental settings are strictly follow the DeiT[39] for a fair comparison. For more details, please refer to the Table 10 provided."
        ]
    },
    "S7.T11.1": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S7.T11.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S7.T11.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S7.T11.1.1.1.1\"><span class=\"ltx_text\" id=\"S7.T11.1.1.1.1.1\" style=\"font-size:90%;\">Config</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S7.T11.1.1.1.2\"><span class=\"ltx_text\" id=\"S7.T11.1.1.1.2.1\" style=\"font-size:90%;\">Value</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T11.1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S7.T11.1.2.2.1\"><span class=\"ltx_text\" id=\"S7.T11.1.2.2.1.1\" style=\"font-size:90%;\">Optimizer</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S7.T11.1.2.2.2\"><span class=\"ltx_text\" id=\"S7.T11.1.2.2.2.1\" style=\"font-size:90%;\">AdamW</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T11.1.3.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S7.T11.1.3.3.1\"><span class=\"ltx_text\" id=\"S7.T11.1.3.3.1.1\" style=\"font-size:90%;\">LR</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T11.1.3.3.2\"><span class=\"ltx_text\" id=\"S7.T11.1.3.3.2.1\" style=\"font-size:90%;\">0.0002</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T11.1.4.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S7.T11.1.4.4.1\"><span class=\"ltx_text\" id=\"S7.T11.1.4.4.1.1\" style=\"font-size:90%;\">weight decay</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T11.1.4.4.2\"><span class=\"ltx_text\" id=\"S7.T11.1.4.4.2.1\" style=\"font-size:90%;\">0.05</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T11.1.5.5\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S7.T11.1.5.5.1\"><span class=\"ltx_text\" id=\"S7.T11.1.5.5.1.1\" style=\"font-size:90%;\">batch size</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T11.1.5.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S7.T11.1.5.5.2.1\" style=\"font-size:90%;\">4</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T11.1.6.6\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S7.T11.1.6.6.1\"><span class=\"ltx_text\" id=\"S7.T11.1.6.6.1.1\" style=\"font-size:90%;\">LR schedule</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T11.1.6.6.2\"><span class=\"ltx_text\" id=\"S7.T11.1.6.6.2.1\" style=\"font-size:90%;\">steps:[8, 11]</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T11.1.7.7\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S7.T11.1.7.7.1\"><span class=\"ltx_text\" id=\"S7.T11.1.7.7.1.1\" style=\"font-size:90%;\">training</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T11.1.7.7.2\"><span class=\"ltx_text\" id=\"S7.T11.1.7.7.2.1\" style=\"font-size:90%;\">epochs 12</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T11.1.8.8\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S7.T11.1.8.8.1\"><span class=\"ltx_text\" id=\"S7.T11.1.8.8.1.1\" style=\"font-size:90%;\">scales</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S7.T11.1.8.8.2\"><span class=\"ltx_text\" id=\"S7.T11.1.8.8.2.1\" style=\"font-size:90%;\">(800, 1333)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S7.T11.1.9.9\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\" id=\"S7.T11.1.9.9.1\"><span class=\"ltx_text\" id=\"S7.T11.1.9.9.1.1\" style=\"font-size:90%;\">drop path</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S7.T11.1.9.9.2\"><span class=\"ltx_text\" id=\"S7.T11.1.9.9.2.1\" style=\"font-size:90%;\">0.2 (Small), 0.3 (Base)</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 11:   Object Detection and Instance Segmentation Training Settings",
        "footnotes": [],
        "references": [
            "When fine-tuning our DeBiFormer to object detection and instance segmentation on COCO [17], we have considered two common frameworks: Mask R-CNN [19], RetinaNet [16]. For optimization, we adopt the AdamW optimizer with an initial learning rate of 0.0002 and a mini batch size of 4 due to the limitation of devices. When training models of different sizes, we adjust the training settings according to the settings used in image classification. The detailed hyper-parameters used in training models are presented in Table 11."
        ]
    }
}