{
    "id_table_1": {
        "caption": "Table 1:  Fraction of each hallucination amongst all the observed hallucinations in  M s  f  t subscript M s f t \\mathcal{M}_{sft} caligraphic_M start_POSTSUBSCRIPT italic_s italic_f italic_t end_POSTSUBSCRIPT  (40,985), with possible overlap.  w i subscript w i w_{i} italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  shows the severity computation of each hallucination.  I condition subscript I condition I_{\\text{condition}} italic_I start_POSTSUBSCRIPT condition end_POSTSUBSCRIPT  = 1 if condition is True otherwise it is 0. See  Fig.   5  for the detailed breakdown of the last three errors.",
        "table": "S4.T1.5.5",
        "footnotes": [],
        "references": [
            "To address the challenges of recall consolidation and gamification in existing evaluation metrics, we propose new metrics that measure sample-wise recall score based on the fraction of gold claims that can be obtained from  D D D italic_D . Specifically, this involves computing  | A G  A D | subscript A G subscript A D |A_{G}\\cap A_{D}| | italic_A start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT  italic_A start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT | , which measures the exact match (EM) recall after calibrating the gold claims. This approach sets a maximum recall limit of 1 for all models. For dataset-wide scoring, we consolidate per-sample EM recall scores using two methods: 1)  EM AC  subscript superscript absent  AC {}^{\\alpha}_{\\text{AC}} start_FLOATSUPERSCRIPT italic_ end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT : The average recall score across samples  answered  by the LLM, i.e., samples where  A R =  subscript A R A_{R}\\neq\\emptyset italic_A start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT =  ; 2)  EM AC  subscript superscript absent  AC {}^{\\beta}_{\\text{AC}} start_FLOATSUPERSCRIPT italic_ end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT : The average recall score across samples that are  answerable , i.e., samples where  A G  A D =  subscript A G subscript A D {A_{G}\\cap A_{D}}\\neq\\emptyset italic_A start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT  italic_A start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT =  3 3 3 Notably, both  EM AC  subscript superscript absent  AC {}^{\\alpha}_{\\text{AC}} start_FLOATSUPERSCRIPT italic_ end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  and  EM AC  subscript superscript absent  AC {}^{\\beta}_{\\text{AC}} start_FLOATSUPERSCRIPT italic_ end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  sum over samples that are both answered and answerable, differing primarily in their normalization values. . These metrics, illustrated in  Fig.   1 , are then combined into a single score,  EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT , which serves as a comprehensive measure of how well the LLM grounds its claims on the document  D D D italic_D . This combined metric not only facilitates the consolidation of recall but also addresses issues related to recall gamification.",
            "An important capability of an LLM in RAG is its ability to identify when a response is unanswerable based on the provided documents  D D D italic_D . To measure this, we introduce a metric called Grounded Refusals. This metric evaluates the models refusal performance by calculating dataset-wide precision and recall for both ground-truth answerable cases and refusals. These values are then combined into their respective F1 scores,  F1 ref  for refusals and  F1 ans  for answerable cases. The final score,  F1 RG RG {}_{\\text{RG}} start_FLOATSUBSCRIPT RG end_FLOATSUBSCRIPT , is the average of these two F1 scores, as shown in  Figure   1 .",
            "While Response Truthfulness metrics like  EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  and  F1 CG CG {}_{\\text{CG}} start_FLOATSUBSCRIPT CG end_FLOATSUBSCRIPT  evaluate the quality of generated claims, it is equally important to measure how well these statements are supported by relevant citationswhat we call Attribution Groundedness. To this end, we adopt two sub-metrics from  Gao et al. ( 2023b ) : Citation Recall ( CR ) and Citation Precision ( CP ). To compute  CR , we first determine if a generated statement  s i subscript s i s_{i} italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is supported by its cited documents using an NLI model 4 4 4 An NLI model checks if the cited document entails the statement. , thus obtaining sample-wise recall scores  CR s i subscript s i s_{i} italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . Then we take the mean across all samples to obtain the final  CR  score ( Figure   1 ). To compute  CP , we first score each citation  c i , j subscript c i j c_{i,j} italic_c start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT  of a statement  s i subscript s i s_{i} italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , followed by computing the average across citations in a response  S S S italic_S  (sample-wise score). The dataset-wide citation score is computed by averaging the citation scores across all the samples. To provide a single metric for Attribution Groundedness, we calculate the harmonic mean of  CP  and  CR , resulting in the final score,  F1 CG CG {}_{\\text{CG}} start_FLOATSUBSCRIPT CG end_FLOATSUBSCRIPT .",
            "We relegate a detailed computation on the metric to  Appendix   D  and  Figure   1 .",
            "To generate preferred responses, by prompting GPT-4, we stitch together the gold claims and citations 6 6 6 Prompt template can be found at  Table   21 . . For unanswerable questions, we assign a ground truth refusal response. To obtain quality negative (unpreferred) responses, we fine-tune LLaMA-2-7b on the source datasets, creating  M s  f  t subscript M s f t \\mathcal{M}_{sft} caligraphic_M start_POSTSUBSCRIPT italic_s italic_f italic_t end_POSTSUBSCRIPT . Testing  M s  f  t subscript M s f t \\mathcal{M}_{sft} caligraphic_M start_POSTSUBSCRIPT italic_s italic_f italic_t end_POSTSUBSCRIPT  on the 70K dataset identified 40K responses with hallucinations.  Table   1  shows hallucination severity ( e i subscript e i e_{i} italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) and frequency ( w i subscript w i w_{i} italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ). To obtain good negative samples, we first rank each of the 40K responses according to their severity score  e q subscript e q e_{q} italic_e start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT . We then select the top 50% 7 7 7 See  Section   F.8  for more details on this hyperparameter.  of the corresponding samples for both answerable and unanswerable responses. We perform DPO using this set of 19k samples to obtain the final aligned model.",
            "Following  Huang et al. ( 2024a ) , we use ExpertQA  (Malaviya et al.,  2024 )  to assess our models generalizability. As shown in  Section   6.1.3 , the open-source ICL models perform significantly worse w.r.t.  Trust-Score  as compared to the proprietary models, with a 9.79% gap between ICL-LLaMA-3-8b and ICL-GPT-4.  Trust-Align  not only closes this gap but establishes a lead: the tuned LLaMA-3-8b model achieves the highest TRUST score of 54.85, surpassing 54.69 of GPT-4.",
            "For an LLM used for RAG task, it is important to study the tendency of LLM towards grounding its knowledge on the provided documents. To partially quantify this, we compute EM score for questions that are unanswerable by the provided documents; thus a fraction of cases where  A G  A D =  subscript A G subscript A D A_{G}\\cap A_{D}=\\emptyset italic_A start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT  italic_A start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT =   but  A G =  subscript A G A_{G}\\neq\\emptyset italic_A start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT =   (more details on the metric in  Section   F.2 ). In  Table   9 , our analysis reveals that responsive models (high AR%) tend to rely on parametric knowledge more frequently (high  P score subscript P score \\text{P}_{\\text{score}} P start_POSTSUBSCRIPT score end_POSTSUBSCRIPT ). Notably, closed-source models like GPT-4 exhibit higher parametric knowledge usage compared to open-source and  Trust-Align  models. However,  P score subscript P score \\text{P}_{\\text{score}} P start_POSTSUBSCRIPT score end_POSTSUBSCRIPT  only partially captures the models utilization of parametric knowledge. For instance, it does not account for cases where the document contains the answer and the model still relied on the parametric knowledge to generate the correct answer (also present in the document). This phenomenon is evident in  Table   11 , where on ASQA, GPT-4 achieves a significantly higher  EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  than our models, yet its attribution groundedness score  F1 CG  is five points lower.",
            "We develop an automated data labeling pipeline that synthesizes natural responses from gold claims and maps each statement to the corresponding documents for embedded in-line citations. The gold claims are obtained from the source datasets (ASQA, QAMPARI, ELI5) and calibrated to the provided documents, i.e., filtering out claims that cannot be derived from  D D D italic_D . We first split the questions into answerable and unanswerable samples based on whether the provided documents entail the gold claims. For an answerable sample, consisting of a question  q q q italic_q , a set of documents  D D D italic_D , and a list of (calibrated) gold claims, we prompt GPT-4 to generate a natural response by stitching together the gold claims using a template ( Table   21 ). Please refer to the subsection below for more details on how the prompt is structured for each dataset. The prompt template asks GPT-4 to label each gold claim used with its index from the provided list (e.g., [Gold Claim X]), allowing for later matching of claims to documents. For unanswerable questions, a refusal response is assigned. To generate citations corresponding to each statement generated, we map the [Gold Claim X] labels to the appropriate documents. First, we extract all such labels from a sentence (which may contain multiple claims and labels). Then, we greedily identify the smallest combination of documents that covers these claims, minimizing over-citation. Details of this process is illustrated in  Fig.   4 .",
            "In  Table   9 , our analysis reveals that responsive models tend to rely on parametric knowledge more frequently. Notably, closed-source models like GPT-4 exhibit higher parametric knowledge usage compared to our models. However, this metric only partially captures the models utilization of parametric knowledge. For instance, cases where models correctly generate gold claims without proper grounding may also indicate reliance on parametric knowledge. This phenomenon is evident in  Table   11 , where on ASQA, GPT-4 achieves a significantly higher  EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  than our models, yet its attribution groundedness score  F1 CG  is five points lower.",
            "The findings are presented in  Table   10 . Our analysis reveals that, with the exception of LLaMA-2 7B which provides no responses, all other ICL-based models exhibit a higher tendency to produce erroneous answers based on their parametric knowledge compared to our models. Notably, Claude-3.5 demonstrates a more frequent reliance on its parametric knowledge, which elucidates its significantly lower  Trust-Score  score in  Table   11 .",
            "We continue our comparison of trustworthiness against competitive closed-source models utilizing in-context learning techniques. As shown in  Table   11 , our aligned models outperform GPT-3.5 ( 69.23 69.23 69.23 69.23  vs.  67.64 67.64 67.64 67.64 ) and Claude-3.5 ( 69.23 69.23 69.23 69.23  vs.  64.36 64.36 64.36 64.36 ) on the ASQA dataset, and substantially outperform GPT-3.5 ( 55.31 55.31 55.31 55.31  vs.  38.95 38.95 38.95 38.95 ), GPT-4 ( 55.31 55.31 55.31 55.31  vs.  40.35 40.35 40.35 40.35 ), and Claude-3.5 ( 55.31 55.31 55.31 55.31  vs.  39.78 39.78 39.78 39.78 ) on QAMPARI. However, the responsiveness of current closed-source models remains much higher than that of our models: even with a refusal prompt, ICL-GPT-4 still answers a significant fraction of questions (86.81% on ASQA, 73.40% on QAMPARI). As discussed in  Section   6 , this tendency allows GPT-4 to achieve higher  EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  scores on ASQA, but it negatively impacts its attribution groundedness: its  F1 CG CG {}_{\\text{CG}} start_FLOATSUBSCRIPT CG end_FLOATSUBSCRIPT  scores on both datasets are lower than those of our models. Similarly, GPT-4s  F1 RG RG {}_{\\text{RG}} start_FLOATSUBSCRIPT RG end_FLOATSUBSCRIPT  scores on both datasets are also lower. On QAMPARI, the  EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  scores of all closed-source models are lower than those of our models.",
            "Moreover, there still remains a gap between our models and the closed-source models on the ELI5 dataset. Our models  Trust-Score  is 2.45 points lower than that of the advanced ICL-GPT-4, and specifically, the  EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  and  F1 CG CG {}_{\\text{CG}} start_FLOATSUBSCRIPT CG end_FLOATSUBSCRIPT  scores are lower. For higher  EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT , as discussed in  Section   6 , it is due to a higher number of its answered answerable questions with comparable EM AC  subscript superscript absent  AC {}^{\\alpha}_{\\text{AC}} start_FLOATSUPERSCRIPT italic_ end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT . As for higher  F1 CG CG {}_{\\text{CG}} start_FLOATSUBSCRIPT CG end_FLOATSUBSCRIPT , We hypothesize that this gap could be attributed to the information density of the extracted claims utilized in constructing the alignment data ( Section   4 ). Specifically, the three claims derived from the decomposition process may either be redundant or inadequate to fully encapsulate the information inherent in the original labelled response. In some cases, the decomposed claims may even fail to align with the original facts. First, insufficient information can lead the model to learn to extract fewer facts from the document, thereby reducing the answerability by covering fewer correct answers after training. Second, redundant information can impair grounded citation learning, as it repeats the same information across different claims, making the model less capable of performing precise citations from the corresponding documents. This issue is illustrated in the case study presented in  Table   12 .",
            "To demonstrate the robustness of our synthesized alignment data across different training methods,  Table   13  also includes the performance of SFT and SIMPO  Meng et al. ( 2024 )  methods. Compared to the SFT baseline, which only utilizes the positive data points in the alignment pairs to fine-tune the base model, preference optimization methods, such as DPO and SIMPO, consistently show performance improvements, highlighting the versatility of our data pipeline. Unlike the SFT approach, DPO and SIMPO demonstrate improved TRUST scores, albeit with a reduction in responsiveness. This decrease in responsiveness is actually a favorable outcome, as it indicates that the models are less likely to attempt to answer questions for which they lack sufficient information.",
            "The determination of question answerability in our dataset is based on a combination of substring matching and TRUE criteria, as detailed in  Section   2 . Additionally, we developed an alternative version of the evaluation data that relies solely on substring matching, disregarding the TRUE criterion. This relaxation of answerability constraints results in an increased number of answerable questions. The findings from this analysis are presented in  Table   14 . It is worth noting that the overall trends observed in this analysis align with those reported in  Section   6 , which employs the combined approach of substring matching followed by TRUE verification.",
            "Following  Liu et al. ( 2023 ); Gao et al. ( 2023b ) , to form  D D D italic_D , we divide large text documents into 100-word passages and limit the number of citations  C i subscript C i \\mathcal{C}_{i} caligraphic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  for each claim to a maximum of three. If the response is empty, it is excluded from evaluation. We provide statistics of our evaluation in  Table   15 .",
            "We employed two methods to measure refusals robustly. In a refusal prompt, models were explicitly instructed to respond only with the phrase:  I apologize, but I couldnt find an answer to your question in the search results.  without providing any further explanation. As the models generally complied with this pattern, we were able to apply fuzzy matching 15 15 15 Fuzz Partial Ratio  was used to mitigate the impact of string length.  to detect the phrase above indicating refusal. For models responding to a default prompt, refusals did not adhere to a fixed pattern, making detection more challenging. Two human annotators verified that fuzzy matching yielded poor performance  Table   16 . Hence, GPT-4o was employed as an evaluator to classify whether an answer should be considered a refusal. The specific prompt used is provided in  Table   25 .",
            "Table   17 ,  Table   18 ,  Table   19  and  Table   20  show the full results of our experiments.",
            "Table   21 ,  Table   22 ,  Table   23 ,  Table   24  and  Table   25  show the prompts used in our experiments."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  LLaMA family evaluated on the ASQA, QAMPARI, and ELI5 datasets. Best  Trust-Score  values within the baseline and  Trust-Align  sets are marked in  bold  (corresponding rows  highlighted ), with the overall best  Trust-Score  values  underlined .    \\Delta roman_  values indicate the difference between the best baseline and best  Trust-Align  rows.  AR%  := Answered Ratio in %;  EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  := Exact Match F1 (Calibrated);  F1 RG RG {}_{\\text{RG}} start_FLOATSUBSCRIPT RG end_FLOATSUBSCRIPT  := Grounded Refusals F1;  F1 CG CG {}_{\\text{CG}} start_FLOATSUBSCRIPT CG end_FLOATSUBSCRIPT  := Citation Grounded F1;  TRUST  :=  Trust-Score ;  Resp.  := Responsiveness;  Att-Grd.  := Attribution Groundedness.",
        "table": "S6.22.22.22",
        "footnotes": [],
        "references": [
            "Our investigation in  Section   6.2  shows that many state-of-the-art systems, including GPT-4 and Claude-3.5-Sonnet, heavily rely on their parametric knowledge to answer questions  (OpenAI,  2023 ; Anthropic,  2024 ) . This reliance limits their suitability for RAG tasks, where models should base responses solely on the provided documents, resulting in a low  Trust-Score . Additionally, prompting approaches intended to enhance model groundability have proven ineffective, as models become overly sensitive to the prompt, leading to exaggerated refusals or excessive responsiveness shown in  Section   F.4 . To enhance the groundedness of LLMs, i.e., achieve a higher  Trust-Score , we propose an alignment method,  Trust-Align . This approach first constructs an alignment dataset consisting of 19K questions, documents, positive (preferred) responses, and negative (unpreferred) responses. The dataset covers a range of LLM errorsInaccurate Answer, Over-Responsiveness, Excessive Refusal, Over-Citation, and Improper Citation. We regard these errors as LLM hallucinations within an RAG framework.",
            "We propose  Trust-Align , an alignment approach designed to improve the trustworthiness of LLMs in RAG ( Figure   2 ). It first creates an alignment dataset of 19K samples with paired positive and negative responses, followed by aligning the model using direct preference optimization (DPO)  (Rafailov et al.,  2024b ) .",
            "To align LLMs towards trustworthiness, we propose a new approach,  Trust-Align . The approach constructs an LLM trustworthiness alignment dataset, where each sample in the dataset consists of a question  q q q italic_q , a set of retrieved documents  D D D italic_D , and a pair of positive (preferred) and negative (unpreferred) responses ( r + superscript r r^{+} italic_r start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ,  r  superscript r r^{-} italic_r start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT ). The positive response corresponds to an answer that encompasses expected gold claims for  q q q italic_q  and corresponding citations referring to the documents. If  D D D italic_D  is not sufficient to answer  q q q italic_q ,  r + superscript r r^{+} italic_r start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT  is assigned a refusal response, while  r  superscript r r^{-} italic_r start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT  is its non-refusal counterpart. We build the dataset in multiple steps: 1) Obtain a set of high-quality and diverse questions, 2) Obtain documents for each question, 3) Augmenting  ( q , D ) q D (q,D) ( italic_q , italic_D )  pairs that cover diverse hallucination types, 4) Construct positive responses entailing gold claims, and 5) Construct negative (unpreferred) responses by prompting a fine-tuned model and observing its hallucinations. We relegate fine-grained details about the dataset to  Figure   2  and  Appendix   E .",
            "Using the questions and oracle documents, we create diverse samples (i.e., varying combinations of relevant and irrelevant documents) to trigger multiple hallucinations from LLMs ( Section   2.3 ). The document order is shuffled to avoid citation bias. To construct unanswerable questions, we select documents similar to those entailing gold claims but still irrelevant to  q q q italic_q . This process results in approximately 70K question-document pairs.",
            "To generate preferred responses, by prompting GPT-4, we stitch together the gold claims and citations 6 6 6 Prompt template can be found at  Table   21 . . For unanswerable questions, we assign a ground truth refusal response. To obtain quality negative (unpreferred) responses, we fine-tune LLaMA-2-7b on the source datasets, creating  M s  f  t subscript M s f t \\mathcal{M}_{sft} caligraphic_M start_POSTSUBSCRIPT italic_s italic_f italic_t end_POSTSUBSCRIPT . Testing  M s  f  t subscript M s f t \\mathcal{M}_{sft} caligraphic_M start_POSTSUBSCRIPT italic_s italic_f italic_t end_POSTSUBSCRIPT  on the 70K dataset identified 40K responses with hallucinations.  Table   1  shows hallucination severity ( e i subscript e i e_{i} italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) and frequency ( w i subscript w i w_{i} italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ). To obtain good negative samples, we first rank each of the 40K responses according to their severity score  e q subscript e q e_{q} italic_e start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT . We then select the top 50% 7 7 7 See  Section   F.8  for more details on this hyperparameter.  of the corresponding samples for both answerable and unanswerable responses. We perform DPO using this set of 19k samples to obtain the final aligned model.",
            "We evaluate on the test-set of attributable factoid and long-form question-answering tasks from ASQA  (Stelmakh et al.,  2023 ) , QAMPARI  (Amouyal et al.,  2023 ) , and ELI5  (Fan et al.,  2019 ) . Additionally, we include ExpertQA  (Malaviya et al.,  2024 )  for OOD evaluations. For each question, we append the top 5 retrieved documents. For ELI5 and ExpertQA, the ground truth answers are decomposed into three claims. The dataset statistics are detailed in  Section   H.2 .",
            "However, in ASQA, our models underperform in EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  due to the overwhelmingly adverse impact of EM AC  subscript superscript absent  AC {}^{\\beta}_{\\text{AC}} start_FLOATSUPERSCRIPT italic_ end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT . The recall of answerable questions ( R ans subscript R ans \\text{R}_{\\text{ans}} R start_POSTSUBSCRIPT ans end_POSTSUBSCRIPT ) is lower for our model compared to baselines (89.02% for Llama-3.2-3b-DPO vs. 98.69% for FRONT-3.2-3b), which rarely refuse questions. As a result, fewer terms are summed in the numerator of EM AC  subscript superscript absent  AC {}^{\\beta}_{\\text{AC}} start_FLOATSUPERSCRIPT italic_ end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT , while the denominator remains constant (the number of answerable questions). This leads to a lower overall EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  score. To further analyze the baseline models performance, we investigated how much of their answering ability relies on parametric knowledge versus document-based information, as discussed in  Section   6.2  and  Section   F.3 .",
            "We also observe that EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  is higher for LLaMA-3-8b in the answerable-only set compared to the set with refusals. As discussed in main results, this is because EM AC  subscript superscript absent  AC {}^{\\beta}_{\\text{AC}} start_FLOATSUPERSCRIPT italic_ end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  favors over-responsive models, which artificially inflates EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT . When refusal samples are excluded, responsiveness (AR%) reaches 100%, meaning the models answer all questions, even without supporting documents. This suggests that the models rely more on ungrounded parametric knowledge, as discussed in  Section   6.2 .",
            "In LLaMA-3-8B, using  Trust-Align  results in a 16.59% improvement in grounded refusal judgment (F1 RG RG {}_{\\text{RG}} start_FLOATSUBSCRIPT RG end_FLOATSUBSCRIPT ) compared to ICL-LLaMA-3-8B and significantly outperforms GPT-3.5 and Claude 3.5 in both grounded citation generation (F1 CG CG {}_{\\text{CG}} start_FLOATSUBSCRIPT CG end_FLOATSUBSCRIPT ) and refusal judgment (F1 RG RG {}_{\\text{RG}} start_FLOATSUBSCRIPT RG end_FLOATSUBSCRIPT ). Although GPT-3.5 and GPT-4 achieve higher EM AC F1 superscript AC F1 {}^{\\text{F1}}{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT AC  scores, indicating better document understanding and answer extraction, they rely heavily on parametric knowledge ( Section   6.2  and  Section   F.3 ). This leads to higher responsiveness, which can result in less precise and trustworthy responses, as reflected in their  Trust-Score  scores. In contrast, our models superior performance in F1 CG CG {}_{\\text{CG}} start_FLOATSUBSCRIPT CG end_FLOATSUBSCRIPT  and F1 RG RG {}_{\\text{RG}} start_FLOATSUBSCRIPT RG end_FLOATSUBSCRIPT  demonstrates its strength in refusal and grounding, making it more reliable. The trends remain the same for the other language model backbones.",
            "For an LLM used for RAG task, it is important to study the tendency of LLM towards grounding its knowledge on the provided documents. To partially quantify this, we compute EM score for questions that are unanswerable by the provided documents; thus a fraction of cases where  A G  A D =  subscript A G subscript A D A_{G}\\cap A_{D}=\\emptyset italic_A start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT  italic_A start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT =   but  A G =  subscript A G A_{G}\\neq\\emptyset italic_A start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT =   (more details on the metric in  Section   F.2 ). In  Table   9 , our analysis reveals that responsive models (high AR%) tend to rely on parametric knowledge more frequently (high  P score subscript P score \\text{P}_{\\text{score}} P start_POSTSUBSCRIPT score end_POSTSUBSCRIPT ). Notably, closed-source models like GPT-4 exhibit higher parametric knowledge usage compared to open-source and  Trust-Align  models. However,  P score subscript P score \\text{P}_{\\text{score}} P start_POSTSUBSCRIPT score end_POSTSUBSCRIPT  only partially captures the models utilization of parametric knowledge. For instance, it does not account for cases where the document contains the answer and the model still relied on the parametric knowledge to generate the correct answer (also present in the document). This phenomenon is evident in  Table   11 , where on ASQA, GPT-4 achieves a significantly higher  EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  than our models, yet its attribution groundedness score  F1 CG  is five points lower.",
            "Typically, LLMs are designed to perform question-answering tasks, where response generation heavily relies on the parametric (internal) knowledge acquired during their pre-training, tuning, and alignment phases  OpenAI ( 2023 ); Anthropic ( 2024 ) . Thus, most of their knowledge is grounded in parametric memory. This makes them inherently less suitable for RAG applications, where the knowledge generated by the LLM is expected to be grounded in input documents. RAG is analogous to a reading comprehension task, where the answers must come from the provided passage (documents in RAG) rather than the prior knowledge of the person taking the test. Thus, any reliance on parametric knowledge can result in statements that are not fully grounded in the documents, including providing answers to unanswerable questions.  Our investigation shows that state-of-the-art models, such as GPT-4 and Claude-3.5-Sonnet, overtly rely on parametric knowledge even when used in a RAG setting. 10 10 10 We show a detailed analysis in  Sections   F.2  and  F.3 .",
            "The dataset construction begins by collecting a set of high-quality (challenging) and diverse questions from source datasets i.e. ASQA, QAMPARI, and ELI5referred to as  seed samples . To collect such samples, we first divide the questions in a dataset into  k k k italic_k  clusters using a Hugginface pipeline 11 11 11 https://github.com/huggingface/text-clustering/ . After identifying the diverse clusters, we use Mixtral-8x7B with the prompt described in  Table   22  to assign each a quality score ranging from 1 to 7. The quality of a cluster is determined by how difficult it is to answer the questions without requiring additional information i.e. a higher score corresponds to a high difficulty. We then select clusters with a quality score of 4 or higher and sample the desired number of questions from these top clusters. Suppose we have three clusters,  C 1 , C 2 , C 3 subscript C 1 subscript C 2 subscript C 3 C_{1},C_{2},C_{3} italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_C start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , with respective sizes  N 1 , N 2 , N 3 subscript N 1 subscript N 2 subscript N 3 N_{1},N_{2},N_{3} italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_N start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_N start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , where  N c = N 1 + N 2 + N 3 subscript N c subscript N 1 subscript N 2 subscript N 3 N_{c}=N_{1}+N_{2}+N_{3} italic_N start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT = italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_N start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + italic_N start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT . To sample  N s subscript N s N_{s} italic_N start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  questions from the clusters, we sample  N s  C i N c subscript N s subscript C i subscript N c N_{s}\\times\\frac{C_{i}}{N_{c}} italic_N start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  divide start_ARG italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_ARG italic_N start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT end_ARG  questions from cluster  C i subscript C i C_{i} italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . If this number exceeds the available questions in the cluster, we randomly sample the remaining questions from the filtered-out clusters (those with a quality score below 4). This process ensures that the seed set prioritizes both high quality and diversity. For this paper, we set  N s subscript N s N_{s} italic_N start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  to 3K, 3K, and 4K for ASQA, QAMPARI, and ELI5, respectively, resulting in approximately 10K questions in the seed set.",
            "Now that we have the questions and the most relevant (oracle) documents, our goal is to create samples of diverse types (i.e., different proportions of relevant documents for the same question) that can trigger multiple hallucinations from LLMs ( Section   2.3 ). As illustrated in  Fig.   3 , for answerable questions, we first utilize the identified entailment patterns to generate all possible combinations of documents, then select  k k k italic_k  combinations that cover diverse patterns. To create samples with unanswerable questions, we select documents that are similar to gold-claim-entailing documents but do not entail any gold claims. To minimize the risk of introducing bias in citation indices, we shuffle the order of documents in each sample. As a result, we generate approximately 70K question-document pairs.",
            "We develop an automated data labeling pipeline that synthesizes natural responses from gold claims and maps each statement to the corresponding documents for embedded in-line citations. The gold claims are obtained from the source datasets (ASQA, QAMPARI, ELI5) and calibrated to the provided documents, i.e., filtering out claims that cannot be derived from  D D D italic_D . We first split the questions into answerable and unanswerable samples based on whether the provided documents entail the gold claims. For an answerable sample, consisting of a question  q q q italic_q , a set of documents  D D D italic_D , and a list of (calibrated) gold claims, we prompt GPT-4 to generate a natural response by stitching together the gold claims using a template ( Table   21 ). Please refer to the subsection below for more details on how the prompt is structured for each dataset. The prompt template asks GPT-4 to label each gold claim used with its index from the provided list (e.g., [Gold Claim X]), allowing for later matching of claims to documents. For unanswerable questions, a refusal response is assigned. To generate citations corresponding to each statement generated, we map the [Gold Claim X] labels to the appropriate documents. First, we extract all such labels from a sentence (which may contain multiple claims and labels). Then, we greedily identify the smallest combination of documents that covers these claims, minimizing over-citation. Details of this process is illustrated in  Fig.   4 .",
            "Moreover, there still remains a gap between our models and the closed-source models on the ELI5 dataset. Our models  Trust-Score  is 2.45 points lower than that of the advanced ICL-GPT-4, and specifically, the  EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  and  F1 CG CG {}_{\\text{CG}} start_FLOATSUBSCRIPT CG end_FLOATSUBSCRIPT  scores are lower. For higher  EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT , as discussed in  Section   6 , it is due to a higher number of its answered answerable questions with comparable EM AC  subscript superscript absent  AC {}^{\\alpha}_{\\text{AC}} start_FLOATSUPERSCRIPT italic_ end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT . As for higher  F1 CG CG {}_{\\text{CG}} start_FLOATSUBSCRIPT CG end_FLOATSUBSCRIPT , We hypothesize that this gap could be attributed to the information density of the extracted claims utilized in constructing the alignment data ( Section   4 ). Specifically, the three claims derived from the decomposition process may either be redundant or inadequate to fully encapsulate the information inherent in the original labelled response. In some cases, the decomposed claims may even fail to align with the original facts. First, insufficient information can lead the model to learn to extract fewer facts from the document, thereby reducing the answerability by covering fewer correct answers after training. Second, redundant information can impair grounded citation learning, as it repeats the same information across different claims, making the model less capable of performing precise citations from the corresponding documents. This issue is illustrated in the case study presented in  Table   12 .",
            "The determination of question answerability in our dataset is based on a combination of substring matching and TRUE criteria, as detailed in  Section   2 . Additionally, we developed an alternative version of the evaluation data that relies solely on substring matching, disregarding the TRUE criterion. This relaxation of answerability constraints results in an increased number of answerable questions. The findings from this analysis are presented in  Table   14 . It is worth noting that the overall trends observed in this analysis align with those reported in  Section   6 , which employs the combined approach of substring matching followed by TRUE verification.",
            "For the GPT-4 data pipeline, we employ GPT-4 to simulate a critic that performs two key tasks in succession. First, it identifies and revises mistakes or supplements missing information in the given response based on correct answers. Second, it validates the attribution of statement-level citations and corrects them accordingly. The detailed instruction is provided in  Table   24 .",
            "We employed two methods to measure refusals robustly. In a refusal prompt, models were explicitly instructed to respond only with the phrase:  I apologize, but I couldnt find an answer to your question in the search results.  without providing any further explanation. As the models generally complied with this pattern, we were able to apply fuzzy matching 15 15 15 Fuzz Partial Ratio  was used to mitigate the impact of string length.  to detect the phrase above indicating refusal. For models responding to a default prompt, refusals did not adhere to a fixed pattern, making detection more challenging. Two human annotators verified that fuzzy matching yielded poor performance  Table   16 . Hence, GPT-4o was employed as an evaluator to classify whether an answer should be considered a refusal. The specific prompt used is provided in  Table   25 .",
            "Table   17 ,  Table   18 ,  Table   19  and  Table   20  show the full results of our experiments.",
            "Table   21 ,  Table   22 ,  Table   23 ,  Table   24  and  Table   25  show the prompts used in our experiments."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Qwen2.5 and Phi3.5 families evaluated on the three datasets. Best  Trust-Score  values within the baseline and  Trust-Align  sets are marked in  bold  (corresponding rows  highlighted ), with the overall best  Trust-Score  values  underlined .    \\Delta roman_  values indicate the difference between the best baseline and best  Trust-Align  rows.",
        "table": "S6.SS0.SSS0.Px5.22.22.22",
        "footnotes": [],
        "references": [
            "Using the questions and oracle documents, we create diverse samples (i.e., varying combinations of relevant and irrelevant documents) to trigger multiple hallucinations from LLMs ( Section   2.3 ). The document order is shuffled to avoid citation bias. To construct unanswerable questions, we select documents similar to those entailing gold claims but still irrelevant to  q q q italic_q . This process results in approximately 70K question-document pairs.",
            "However, in ASQA, our models underperform in EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  due to the overwhelmingly adverse impact of EM AC  subscript superscript absent  AC {}^{\\beta}_{\\text{AC}} start_FLOATSUPERSCRIPT italic_ end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT . The recall of answerable questions ( R ans subscript R ans \\text{R}_{\\text{ans}} R start_POSTSUBSCRIPT ans end_POSTSUBSCRIPT ) is lower for our model compared to baselines (89.02% for Llama-3.2-3b-DPO vs. 98.69% for FRONT-3.2-3b), which rarely refuse questions. As a result, fewer terms are summed in the numerator of EM AC  subscript superscript absent  AC {}^{\\beta}_{\\text{AC}} start_FLOATSUPERSCRIPT italic_ end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT , while the denominator remains constant (the number of answerable questions). This leads to a lower overall EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  score. To further analyze the baseline models performance, we investigated how much of their answering ability relies on parametric knowledge versus document-based information, as discussed in  Section   6.2  and  Section   F.3 .",
            "Following  Huang et al. ( 2024a ) , we use ExpertQA  (Malaviya et al.,  2024 )  to assess our models generalizability. As shown in  Section   6.1.3 , the open-source ICL models perform significantly worse w.r.t.  Trust-Score  as compared to the proprietary models, with a 9.79% gap between ICL-LLaMA-3-8b and ICL-GPT-4.  Trust-Align  not only closes this gap but establishes a lead: the tuned LLaMA-3-8b model achieves the highest TRUST score of 54.85, surpassing 54.69 of GPT-4.",
            "In LLaMA-3-8B, using  Trust-Align  results in a 16.59% improvement in grounded refusal judgment (F1 RG RG {}_{\\text{RG}} start_FLOATSUBSCRIPT RG end_FLOATSUBSCRIPT ) compared to ICL-LLaMA-3-8B and significantly outperforms GPT-3.5 and Claude 3.5 in both grounded citation generation (F1 CG CG {}_{\\text{CG}} start_FLOATSUBSCRIPT CG end_FLOATSUBSCRIPT ) and refusal judgment (F1 RG RG {}_{\\text{RG}} start_FLOATSUBSCRIPT RG end_FLOATSUBSCRIPT ). Although GPT-3.5 and GPT-4 achieve higher EM AC F1 superscript AC F1 {}^{\\text{F1}}{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT AC  scores, indicating better document understanding and answer extraction, they rely heavily on parametric knowledge ( Section   6.2  and  Section   F.3 ). This leads to higher responsiveness, which can result in less precise and trustworthy responses, as reflected in their  Trust-Score  scores. In contrast, our models superior performance in F1 CG CG {}_{\\text{CG}} start_FLOATSUBSCRIPT CG end_FLOATSUBSCRIPT  and F1 RG RG {}_{\\text{RG}} start_FLOATSUBSCRIPT RG end_FLOATSUBSCRIPT  demonstrates its strength in refusal and grounding, making it more reliable. The trends remain the same for the other language model backbones.",
            "Typically, LLMs are designed to perform question-answering tasks, where response generation heavily relies on the parametric (internal) knowledge acquired during their pre-training, tuning, and alignment phases  OpenAI ( 2023 ); Anthropic ( 2024 ) . Thus, most of their knowledge is grounded in parametric memory. This makes them inherently less suitable for RAG applications, where the knowledge generated by the LLM is expected to be grounded in input documents. RAG is analogous to a reading comprehension task, where the answers must come from the provided passage (documents in RAG) rather than the prior knowledge of the person taking the test. Thus, any reliance on parametric knowledge can result in statements that are not fully grounded in the documents, including providing answers to unanswerable questions.  Our investigation shows that state-of-the-art models, such as GPT-4 and Claude-3.5-Sonnet, overtly rely on parametric knowledge even when used in a RAG setting. 10 10 10 We show a detailed analysis in  Sections   F.2  and  F.3 .",
            "Now that we have the questions and the most relevant (oracle) documents, our goal is to create samples of diverse types (i.e., different proportions of relevant documents for the same question) that can trigger multiple hallucinations from LLMs ( Section   2.3 ). As illustrated in  Fig.   3 , for answerable questions, we first utilize the identified entailment patterns to generate all possible combinations of documents, then select  k k k italic_k  combinations that cover diverse patterns. To create samples with unanswerable questions, we select documents that are similar to gold-claim-entailing documents but do not entail any gold claims. To minimize the risk of introducing bias in citation indices, we shuffle the order of documents in each sample. As a result, we generate approximately 70K question-document pairs.",
            "To demonstrate the robustness of our synthesized alignment data across different training methods,  Table   13  also includes the performance of SFT and SIMPO  Meng et al. ( 2024 )  methods. Compared to the SFT baseline, which only utilizes the positive data points in the alignment pairs to fine-tune the base model, preference optimization methods, such as DPO and SIMPO, consistently show performance improvements, highlighting the versatility of our data pipeline. Unlike the SFT approach, DPO and SIMPO demonstrate improved TRUST scores, albeit with a reduction in responsiveness. This decrease in responsiveness is actually a favorable outcome, as it indicates that the models are less likely to attempt to answer questions for which they lack sufficient information.",
            "Table   21 ,  Table   22 ,  Table   23 ,  Table   24  and  Table   25  show the prompts used in our experiments."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Ablations of data synthesis techniques for LLaMA-2-7b on three evaluation datasets using refusal prompting;  AR%  := Answered Ratio in %;  EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  := Exact Match F1 (Calibrated);  F1 RG RG {}_{\\text{RG}} start_FLOATSUBSCRIPT RG end_FLOATSUBSCRIPT  := Grounded Refusals F1;  F1 CG CG {}_{\\text{CG}} start_FLOATSUBSCRIPT CG end_FLOATSUBSCRIPT  := Citation Grounded F1;  TRUST  := TRUST score;  HT  := hallucination types;  Resp.  := Responsiveness;  Att-Grd.  := Attribution Groundedness. The original error types in  Section   2.3  were summarized into three main classes: answer-related (Inaccurate Answer), citation-related (Overcitation, Improper Citation), refusal-related (Over Responsiveness, Excessive Refusal).",
        "table": "S6.T4.9.9",
        "footnotes": [],
        "references": [
            "Our investigation in  Section   6.2  shows that many state-of-the-art systems, including GPT-4 and Claude-3.5-Sonnet, heavily rely on their parametric knowledge to answer questions  (OpenAI,  2023 ; Anthropic,  2024 ) . This reliance limits their suitability for RAG tasks, where models should base responses solely on the provided documents, resulting in a low  Trust-Score . Additionally, prompting approaches intended to enhance model groundability have proven ineffective, as models become overly sensitive to the prompt, leading to exaggerated refusals or excessive responsiveness shown in  Section   F.4 . To enhance the groundedness of LLMs, i.e., achieve a higher  Trust-Score , we propose an alignment method,  Trust-Align . This approach first constructs an alignment dataset consisting of 19K questions, documents, positive (preferred) responses, and negative (unpreferred) responses. The dataset covers a range of LLM errorsInaccurate Answer, Over-Responsiveness, Excessive Refusal, Over-Citation, and Improper Citation. We regard these errors as LLM hallucinations within an RAG framework.",
            "Table   4  demonstrates the effectiveness of our data construction approach. Adding augmented prompts targeting five hallucination types improves  Trust-Score  by 1.50% (ASQA), 1.78% (QAMPARI), and 2.23% (ELI5), highlighting the value of our synthetic data in improving trustworthiness and reducing hallucinations. Similarly, dropping data for specific hallucination types causes  Trust-Score  to consistently drop, suggesting the importance of each subtype. In particular, removing refusal-related hallucinations significantly affects F1 RG RG {}_{\\text{RG}} start_FLOATSUBSCRIPT RG end_FLOATSUBSCRIPT , with decreases of 2.79% for ASQA, 0.46% for QAMPARI, and 2.03% for ELI5, showing that refusal-related data is critical for improving a models ability to decide when to answer, thereby enhancing trustworthiness.",
            "We develop an automated data labeling pipeline that synthesizes natural responses from gold claims and maps each statement to the corresponding documents for embedded in-line citations. The gold claims are obtained from the source datasets (ASQA, QAMPARI, ELI5) and calibrated to the provided documents, i.e., filtering out claims that cannot be derived from  D D D italic_D . We first split the questions into answerable and unanswerable samples based on whether the provided documents entail the gold claims. For an answerable sample, consisting of a question  q q q italic_q , a set of documents  D D D italic_D , and a list of (calibrated) gold claims, we prompt GPT-4 to generate a natural response by stitching together the gold claims using a template ( Table   21 ). Please refer to the subsection below for more details on how the prompt is structured for each dataset. The prompt template asks GPT-4 to label each gold claim used with its index from the provided list (e.g., [Gold Claim X]), allowing for later matching of claims to documents. For unanswerable questions, a refusal response is assigned. To generate citations corresponding to each statement generated, we map the [Gold Claim X] labels to the appropriate documents. First, we extract all such labels from a sentence (which may contain multiple claims and labels). Then, we greedily identify the smallest combination of documents that covers these claims, minimizing over-citation. Details of this process is illustrated in  Fig.   4 .",
            "Moreover, there still remains a gap between our models and the closed-source models on the ELI5 dataset. Our models  Trust-Score  is 2.45 points lower than that of the advanced ICL-GPT-4, and specifically, the  EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  and  F1 CG CG {}_{\\text{CG}} start_FLOATSUBSCRIPT CG end_FLOATSUBSCRIPT  scores are lower. For higher  EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT , as discussed in  Section   6 , it is due to a higher number of its answered answerable questions with comparable EM AC  subscript superscript absent  AC {}^{\\alpha}_{\\text{AC}} start_FLOATSUPERSCRIPT italic_ end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT . As for higher  F1 CG CG {}_{\\text{CG}} start_FLOATSUBSCRIPT CG end_FLOATSUBSCRIPT , We hypothesize that this gap could be attributed to the information density of the extracted claims utilized in constructing the alignment data ( Section   4 ). Specifically, the three claims derived from the decomposition process may either be redundant or inadequate to fully encapsulate the information inherent in the original labelled response. In some cases, the decomposed claims may even fail to align with the original facts. First, insufficient information can lead the model to learn to extract fewer facts from the document, thereby reducing the answerability by covering fewer correct answers after training. Second, redundant information can impair grounded citation learning, as it repeats the same information across different claims, making the model less capable of performing precise citations from the corresponding documents. This issue is illustrated in the case study presented in  Table   12 .",
            "The determination of question answerability in our dataset is based on a combination of substring matching and TRUE criteria, as detailed in  Section   2 . Additionally, we developed an alternative version of the evaluation data that relies solely on substring matching, disregarding the TRUE criterion. This relaxation of answerability constraints results in an increased number of answerable questions. The findings from this analysis are presented in  Table   14 . It is worth noting that the overall trends observed in this analysis align with those reported in  Section   6 , which employs the combined approach of substring matching followed by TRUE verification.",
            "For the GPT-4 data pipeline, we employ GPT-4 to simulate a critic that performs two key tasks in succession. First, it identifies and revises mistakes or supplements missing information in the given response based on correct answers. Second, it validates the attribution of statement-level citations and corrects them accordingly. The detailed instruction is provided in  Table   24 .",
            "Table   21 ,  Table   22 ,  Table   23 ,  Table   24  and  Table   25  show the prompts used in our experiments."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Effect of adding refusal samples on the ASQA.",
        "table": "S6.T5.3.3",
        "footnotes": [],
        "references": [
            "Table   5  underscores the importance of including refusal samples during fine-tuning in ASQA. Training with refusal samples in  Trust-Align  achieves the highest  Trust-Score  score of 69.23%. Removing all unanswerable questions from the training set creates a set without refusals or refusal-related hallucination types. Without refusal samples,  Trust-Score  scores drop significantlyby 10.2% for LLaMA-3-8b and 11.41% for LLaMA-2-7b. This decline is particularly pronounced in F1 RG RG {}_{\\text{RG}} start_FLOATSUBSCRIPT RG end_FLOATSUBSCRIPT  (down 26.34% for LLaMA-3-8b and 26.97% for LLaMA-2-7b) and F1 CG CG {}_{\\text{CG}} start_FLOATSUBSCRIPT CG end_FLOATSUBSCRIPT  (down 6.87% for LLaMA-3-8b and 6.57% for LLaMA-2-7b).",
            "Following  Liu et al. ( 2023 ); Gao et al. ( 2023b ) , to form  D D D italic_D , we divide large text documents into 100-word passages and limit the number of citations  C i subscript C i \\mathcal{C}_{i} caligraphic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  for each claim to a maximum of three. If the response is empty, it is excluded from evaluation. We provide statistics of our evaluation in  Table   15 .",
            "We employed two methods to measure refusals robustly. In a refusal prompt, models were explicitly instructed to respond only with the phrase:  I apologize, but I couldnt find an answer to your question in the search results.  without providing any further explanation. As the models generally complied with this pattern, we were able to apply fuzzy matching 15 15 15 Fuzz Partial Ratio  was used to mitigate the impact of string length.  to detect the phrase above indicating refusal. For models responding to a default prompt, refusals did not adhere to a fixed pattern, making detection more challenging. Two human annotators verified that fuzzy matching yielded poor performance  Table   16 . Hence, GPT-4o was employed as an evaluator to classify whether an answer should be considered a refusal. The specific prompt used is provided in  Table   25 .",
            "Table   21 ,  Table   22 ,  Table   23 ,  Table   24  and  Table   25  show the prompts used in our experiments."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Generalization test results on ExpertQA using refusal prompting.",
        "table": "S6.SS1.SSS3.3.3.3",
        "footnotes": [],
        "references": [
            "Our investigation in  Section   6.2  shows that many state-of-the-art systems, including GPT-4 and Claude-3.5-Sonnet, heavily rely on their parametric knowledge to answer questions  (OpenAI,  2023 ; Anthropic,  2024 ) . This reliance limits their suitability for RAG tasks, where models should base responses solely on the provided documents, resulting in a low  Trust-Score . Additionally, prompting approaches intended to enhance model groundability have proven ineffective, as models become overly sensitive to the prompt, leading to exaggerated refusals or excessive responsiveness shown in  Section   F.4 . To enhance the groundedness of LLMs, i.e., achieve a higher  Trust-Score , we propose an alignment method,  Trust-Align . This approach first constructs an alignment dataset consisting of 19K questions, documents, positive (preferred) responses, and negative (unpreferred) responses. The dataset covers a range of LLM errorsInaccurate Answer, Over-Responsiveness, Excessive Refusal, Over-Citation, and Improper Citation. We regard these errors as LLM hallucinations within an RAG framework.",
            "Our best  Trust-Align ed models significantly outperform the best-performing baselines by 13.44% (ASQA), 28.89% (QAMPARI), and 13.66% (ELI5) on  Trust-Score  (see    \\Delta roman_  row and highlighted rows in  Section   6 ). This improvement suggests that our models are more capable of generating responses grounded in the documents.",
            "However, in ASQA, our models underperform in EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  due to the overwhelmingly adverse impact of EM AC  subscript superscript absent  AC {}^{\\beta}_{\\text{AC}} start_FLOATSUPERSCRIPT italic_ end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT . The recall of answerable questions ( R ans subscript R ans \\text{R}_{\\text{ans}} R start_POSTSUBSCRIPT ans end_POSTSUBSCRIPT ) is lower for our model compared to baselines (89.02% for Llama-3.2-3b-DPO vs. 98.69% for FRONT-3.2-3b), which rarely refuse questions. As a result, fewer terms are summed in the numerator of EM AC  subscript superscript absent  AC {}^{\\beta}_{\\text{AC}} start_FLOATSUPERSCRIPT italic_ end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT , while the denominator remains constant (the number of answerable questions). This leads to a lower overall EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  score. To further analyze the baseline models performance, we investigated how much of their answering ability relies on parametric knowledge versus document-based information, as discussed in  Section   6.2  and  Section   F.3 .",
            "Taking LLaMA-3.2-3b as an example, DPO models outperform SFT models in  Trust-Score  by 6.70% (ASQA), 3.96% (QAMPARI), and 0.04% (ELI5). Across models shown in  Section   6 , DPO also attains significantly better F1 CG CG {}_{\\text{CG}} start_FLOATSUBSCRIPT CG end_FLOATSUBSCRIPT  compared to SFT (eg 84.21% vs. 75.63% in LLaMA-3.2-3b ASQA), indicating that training with DPO produces a model that has better citation quality. Despite DPOs mixed performance on EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  and F1 RG RG {}_{\\text{RG}} start_FLOATSUBSCRIPT RG end_FLOATSUBSCRIPT  relative to SFT, DPO models achieve better overall  Trust-Score  scores.",
            "As seen in  Section   6 , our experiments across various sizes and architectures demonstrate that the  Trust-Align  approach consistently improves the models  Trust-Score . In small models, we observe a drastic improvement in  Trust-Score  compared to ICL baselines after applying  Trust-Align  (Qwen-2.5-0.5b ASQA improves from 22.83% to 54.76%). A similar trend is seen in bigger models (Qwen-2.5-7b ASQA improves from 62.91% to 68.28%), demonstrating the scalability of  Trust-Align  across model sizes. We note that  Trust-Align  yields the largest improvements on small models. Applying  Trust-Align  on Phi3.5 yields significant  Trust-Score  improvement compared to ICL baseline- 18.98% (ASQA), 31.90% (QAMPARI), and 14.51% (ELI5). This demonstrates that our findings are robust across different architectures.",
            "We also observe that EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  is higher for LLaMA-3-8b in the answerable-only set compared to the set with refusals. As discussed in main results, this is because EM AC  subscript superscript absent  AC {}^{\\beta}_{\\text{AC}} start_FLOATSUPERSCRIPT italic_ end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  favors over-responsive models, which artificially inflates EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT . When refusal samples are excluded, responsiveness (AR%) reaches 100%, meaning the models answer all questions, even without supporting documents. This suggests that the models rely more on ungrounded parametric knowledge, as discussed in  Section   6.2 .",
            "Following  Huang et al. ( 2024a ) , we use ExpertQA  (Malaviya et al.,  2024 )  to assess our models generalizability. As shown in  Section   6.1.3 , the open-source ICL models perform significantly worse w.r.t.  Trust-Score  as compared to the proprietary models, with a 9.79% gap between ICL-LLaMA-3-8b and ICL-GPT-4.  Trust-Align  not only closes this gap but establishes a lead: the tuned LLaMA-3-8b model achieves the highest TRUST score of 54.85, surpassing 54.69 of GPT-4.",
            "In LLaMA-3-8B, using  Trust-Align  results in a 16.59% improvement in grounded refusal judgment (F1 RG RG {}_{\\text{RG}} start_FLOATSUBSCRIPT RG end_FLOATSUBSCRIPT ) compared to ICL-LLaMA-3-8B and significantly outperforms GPT-3.5 and Claude 3.5 in both grounded citation generation (F1 CG CG {}_{\\text{CG}} start_FLOATSUBSCRIPT CG end_FLOATSUBSCRIPT ) and refusal judgment (F1 RG RG {}_{\\text{RG}} start_FLOATSUBSCRIPT RG end_FLOATSUBSCRIPT ). Although GPT-3.5 and GPT-4 achieve higher EM AC F1 superscript AC F1 {}^{\\text{F1}}{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT AC  scores, indicating better document understanding and answer extraction, they rely heavily on parametric knowledge ( Section   6.2  and  Section   F.3 ). This leads to higher responsiveness, which can result in less precise and trustworthy responses, as reflected in their  Trust-Score  scores. In contrast, our models superior performance in F1 CG CG {}_{\\text{CG}} start_FLOATSUBSCRIPT CG end_FLOATSUBSCRIPT  and F1 RG RG {}_{\\text{RG}} start_FLOATSUBSCRIPT RG end_FLOATSUBSCRIPT  demonstrates its strength in refusal and grounding, making it more reliable. The trends remain the same for the other language model backbones.",
            "Fig.   6  shows the performance of our models on EM regular and EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT . As seen across the models, EM regular tends to unduly penalize our models for refusals, resulting in baselines performing disproportionately better than our models. By measuring EM on both the answerable and answered set to arrive at EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT , we arrive at an EM metric that is more fair in the presence of refusals. By correcting for the bias toward answering at all costs, we are able to reveal more balanced perspective on model performance as demonstrated by a reduction in the performance gap (eg.. in LLaMA-2-7b, LLaMA-3-8b, LLaMA-3.2-1b, LLaMA-3.2-3b) or even revealing our models stronger performance as compared to baseline (e.g. Phi-3.5 mini, Qwen-2.5-1.5b, Qwen2.5-3b). The ability to grade the model more fairly underscores the need for our calibrated metrics.",
            "We continue our comparison of trustworthiness against competitive closed-source models utilizing in-context learning techniques. As shown in  Table   11 , our aligned models outperform GPT-3.5 ( 69.23 69.23 69.23 69.23  vs.  67.64 67.64 67.64 67.64 ) and Claude-3.5 ( 69.23 69.23 69.23 69.23  vs.  64.36 64.36 64.36 64.36 ) on the ASQA dataset, and substantially outperform GPT-3.5 ( 55.31 55.31 55.31 55.31  vs.  38.95 38.95 38.95 38.95 ), GPT-4 ( 55.31 55.31 55.31 55.31  vs.  40.35 40.35 40.35 40.35 ), and Claude-3.5 ( 55.31 55.31 55.31 55.31  vs.  39.78 39.78 39.78 39.78 ) on QAMPARI. However, the responsiveness of current closed-source models remains much higher than that of our models: even with a refusal prompt, ICL-GPT-4 still answers a significant fraction of questions (86.81% on ASQA, 73.40% on QAMPARI). As discussed in  Section   6 , this tendency allows GPT-4 to achieve higher  EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  scores on ASQA, but it negatively impacts its attribution groundedness: its  F1 CG CG {}_{\\text{CG}} start_FLOATSUBSCRIPT CG end_FLOATSUBSCRIPT  scores on both datasets are lower than those of our models. Similarly, GPT-4s  F1 RG RG {}_{\\text{RG}} start_FLOATSUBSCRIPT RG end_FLOATSUBSCRIPT  scores on both datasets are also lower. On QAMPARI, the  EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  scores of all closed-source models are lower than those of our models.",
            "Moreover, there still remains a gap between our models and the closed-source models on the ELI5 dataset. Our models  Trust-Score  is 2.45 points lower than that of the advanced ICL-GPT-4, and specifically, the  EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  and  F1 CG CG {}_{\\text{CG}} start_FLOATSUBSCRIPT CG end_FLOATSUBSCRIPT  scores are lower. For higher  EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT , as discussed in  Section   6 , it is due to a higher number of its answered answerable questions with comparable EM AC  subscript superscript absent  AC {}^{\\alpha}_{\\text{AC}} start_FLOATSUPERSCRIPT italic_ end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT . As for higher  F1 CG CG {}_{\\text{CG}} start_FLOATSUBSCRIPT CG end_FLOATSUBSCRIPT , We hypothesize that this gap could be attributed to the information density of the extracted claims utilized in constructing the alignment data ( Section   4 ). Specifically, the three claims derived from the decomposition process may either be redundant or inadequate to fully encapsulate the information inherent in the original labelled response. In some cases, the decomposed claims may even fail to align with the original facts. First, insufficient information can lead the model to learn to extract fewer facts from the document, thereby reducing the answerability by covering fewer correct answers after training. Second, redundant information can impair grounded citation learning, as it repeats the same information across different claims, making the model less capable of performing precise citations from the corresponding documents. This issue is illustrated in the case study presented in  Table   12 .",
            "The determination of question answerability in our dataset is based on a combination of substring matching and TRUE criteria, as detailed in  Section   2 . Additionally, we developed an alternative version of the evaluation data that relies solely on substring matching, disregarding the TRUE criterion. This relaxation of answerability constraints results in an increased number of answerable questions. The findings from this analysis are presented in  Table   14 . It is worth noting that the overall trends observed in this analysis align with those reported in  Section   6 , which employs the combined approach of substring matching followed by TRUE verification.",
            "We employed two methods to measure refusals robustly. In a refusal prompt, models were explicitly instructed to respond only with the phrase:  I apologize, but I couldnt find an answer to your question in the search results.  without providing any further explanation. As the models generally complied with this pattern, we were able to apply fuzzy matching 15 15 15 Fuzz Partial Ratio  was used to mitigate the impact of string length.  to detect the phrase above indicating refusal. For models responding to a default prompt, refusals did not adhere to a fixed pattern, making detection more challenging. Two human annotators verified that fuzzy matching yielded poor performance  Table   16 . Hence, GPT-4o was employed as an evaluator to classify whether an answer should be considered a refusal. The specific prompt used is provided in  Table   25 ."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Case study showcasing the limitations of substring matching and necessity of TRUE judgement.",
        "table": "A2.T7.1",
        "footnotes": [],
        "references": [
            "Prior works  Liu et al. ( 2023 ); Gao et al. ( 2023b ); Ye et al. ( 2024 ); Huang et al. ( 2024a ); Li et al. ( 2024a )  have employed substring matching to indicate entailment. While this syntactic approach is fast, it often proves inadequate in complex, long contexts. A case study is presented in  Table   7 . To address the limitations of this superficial entailment, we adopt a TRUE-based method  Honovich et al. ( 2022 ) , which combines the strengths of both syntactic and semantic approaches. Specifically, we enhance the process by using the TRUE model, a T5-11B model  Raffel et al. ( 2020 )  fine-tuned for the NLI task, to verify, from a semantic perspective, whether a substring match corresponds to meaningful entailment within document passages. The input to the TRUE model is the concatenation of a premise and a hypothesis, and the output is an entailment score between 0 and 1, indicating the degree to which the premise entails the hypothesis. We treat the corresponding documents as the premise, and to minimize ambiguity, the associated question is concatenated with each gold answer as the hypothesis. In cases where the TRUE model does not yield a positive entailment score despite a substring match, we rely on the TRUE judgment as the final label. However, if the substring match fails, we bypass TRUE calculation, thus reducing the computational cost of relying solely on TRUE for semantic entailment.",
            "During the bulk of our experiments, we chose to utilize the top 50% of augmented samples to form the training dataset for DPO alignment due to time constraints. We then investigated whether varying the quantity and difficulty of the samples would influence the final model performance.  Fig.   7  shows how the amount of training samples affect the final performance of  Trust-Align  model. Notably, selecting the top 25% of augmented samples achieved the highest performance on QAMPARI (57.73) and ELI5 (47.55). The absence of a clear trend suggesting that more data is better can be attributed to the nature of the data itself. Although document recombination resulted in a substantial increase in sample size, the generated samples exhibit significant information overlap, as the top 100 retrieved documents contain highly similar content. Consequently, the positive and negative responses produced through our response construction pipeline are often quite similar across samples. When training with the top 50% of augmented samples, the model may be experiencing overfitting, which could explain the observed decline in performance. Therefore, it is likely that even better performance could be attained by carefully tuning the amount of augmented data used. This finding underscores a limitation of our pipeline, revealing that the diversity of document content plays a crucial role in determining the quality of the augmented samples.",
            "Table   17 ,  Table   18 ,  Table   19  and  Table   20  show the full results of our experiments."
        ]
    },
    "id_table_8": {
        "caption": "Figure 3:  Document recombination process in augmented prompt curation.",
        "table": "A5.T8.5.5",
        "footnotes": [],
        "references": [
            "To generate preferred responses, by prompting GPT-4, we stitch together the gold claims and citations 6 6 6 Prompt template can be found at  Table   21 . . For unanswerable questions, we assign a ground truth refusal response. To obtain quality negative (unpreferred) responses, we fine-tune LLaMA-2-7b on the source datasets, creating  M s  f  t subscript M s f t \\mathcal{M}_{sft} caligraphic_M start_POSTSUBSCRIPT italic_s italic_f italic_t end_POSTSUBSCRIPT . Testing  M s  f  t subscript M s f t \\mathcal{M}_{sft} caligraphic_M start_POSTSUBSCRIPT italic_s italic_f italic_t end_POSTSUBSCRIPT  on the 70K dataset identified 40K responses with hallucinations.  Table   1  shows hallucination severity ( e i subscript e i e_{i} italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) and frequency ( w i subscript w i w_{i} italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ). To obtain good negative samples, we first rank each of the 40K responses according to their severity score  e q subscript e q e_{q} italic_e start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT . We then select the top 50% 7 7 7 See  Section   F.8  for more details on this hyperparameter.  of the corresponding samples for both answerable and unanswerable responses. We perform DPO using this set of 19k samples to obtain the final aligned model.",
            "To create high-quality preference data, we aim to obtain quality negative (unpreferred) responses. We first fine-tune LLaMA-2-7b on the training set of the source datasets 12 12 12 Seed questions, corresponding oracle documents, and the gold answers ( r + superscript r r^{+} italic_r start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ). , creating  M s  f  t subscript M s f t \\mathcal{M}_{sft} caligraphic_M start_POSTSUBSCRIPT italic_s italic_f italic_t end_POSTSUBSCRIPT . We then test  M s  f  t subscript M s f t \\mathcal{M}_{sft} caligraphic_M start_POSTSUBSCRIPT italic_s italic_f italic_t end_POSTSUBSCRIPT  on the above-obtained dataset with approximately 70K questions and identify that 40K responses exhibit hallucinations.  Table   8  shows the severity computation ( e i subscript e i e_{i} italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) and the frequency of each hallucination type ( w i subscript w i w_{i} italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ). Thus, we can compute hallucination severity for each sample as",
            "Table   17 ,  Table   18 ,  Table   19  and  Table   20  show the full results of our experiments."
        ]
    },
    "id_table_9": {
        "caption": "Figure 4:  Claim-document-mapping process.",
        "table": "A6.T9.3.3",
        "footnotes": [],
        "references": [
            "We observe a notable increase in EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  for QAMPARI (32.64%) and ELI5 (5.56%) but a decrease of 3.37% for ASQA. The mixed performance in ASQA can be explained by the composition of EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT , which is derived from EM AC  subscript superscript absent  AC {}^{\\alpha}_{\\text{AC}} start_FLOATSUPERSCRIPT italic_ end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  and EM AC  subscript superscript absent  AC {}^{\\beta}_{\\text{AC}} start_FLOATSUPERSCRIPT italic_ end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  ( Eq.   9 ). As shown in  Appendix   I , our models generally achieve higher EM AC  subscript superscript absent  AC {}^{\\alpha}_{\\text{AC}} start_FLOATSUPERSCRIPT italic_ end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  compared to baselines (54.63% for Llama-3.2-3b-DPO vs. 52.94% for FRONT-3.2-3b) despite having a lower AR% (77.85% for Llama-3.2-3b-DPO vs. 95.25% for FRONT-3.2-3b). This suggests that our models have a higher expected value for  EM q q q italic_q AC  (per-sample EM recall), as the denominator depends on the number of answered questions. This trend is observed across models and datasets as shown in  Appendix   I .",
            "For an LLM used for RAG task, it is important to study the tendency of LLM towards grounding its knowledge on the provided documents. To partially quantify this, we compute EM score for questions that are unanswerable by the provided documents; thus a fraction of cases where  A G  A D =  subscript A G subscript A D A_{G}\\cap A_{D}=\\emptyset italic_A start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT  italic_A start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT =   but  A G =  subscript A G A_{G}\\neq\\emptyset italic_A start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT =   (more details on the metric in  Section   F.2 ). In  Table   9 , our analysis reveals that responsive models (high AR%) tend to rely on parametric knowledge more frequently (high  P score subscript P score \\text{P}_{\\text{score}} P start_POSTSUBSCRIPT score end_POSTSUBSCRIPT ). Notably, closed-source models like GPT-4 exhibit higher parametric knowledge usage compared to open-source and  Trust-Align  models. However,  P score subscript P score \\text{P}_{\\text{score}} P start_POSTSUBSCRIPT score end_POSTSUBSCRIPT  only partially captures the models utilization of parametric knowledge. For instance, it does not account for cases where the document contains the answer and the model still relied on the parametric knowledge to generate the correct answer (also present in the document). This phenomenon is evident in  Table   11 , where on ASQA, GPT-4 achieves a significantly higher  EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  than our models, yet its attribution groundedness score  F1 CG  is five points lower.",
            "In  Table   9 , our analysis reveals that responsive models tend to rely on parametric knowledge more frequently. Notably, closed-source models like GPT-4 exhibit higher parametric knowledge usage compared to our models. However, this metric only partially captures the models utilization of parametric knowledge. For instance, cases where models correctly generate gold claims without proper grounding may also indicate reliance on parametric knowledge. This phenomenon is evident in  Table   11 , where on ASQA, GPT-4 achieves a significantly higher  EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  than our models, yet its attribution groundedness score  F1 CG  is five points lower.",
            "Table   17 ,  Table   18 ,  Table   19  and  Table   20  show the full results of our experiments."
        ]
    },
    "id_table_10": {
        "caption": "Table 8:  Fraction of each hallucination amongst all the observed hallucinations in  M s  f  t subscript M s f t \\mathcal{M}_{sft} caligraphic_M start_POSTSUBSCRIPT italic_s italic_f italic_t end_POSTSUBSCRIPT  (40,985), with possible overlap.  w i subscript w i w_{i} italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  shows the severity computation of each hallucination.  I condition subscript I condition I_{\\text{condition}} italic_I start_POSTSUBSCRIPT condition end_POSTSUBSCRIPT  = 1 if condition is True otherwise it is 0. See  Fig.   5  for the detailed breakdown of the last three errors.",
        "table": "A6.T10.1.1",
        "footnotes": [],
        "references": [
            "The findings are presented in  Table   10 . Our analysis reveals that, with the exception of LLaMA-2 7B which provides no responses, all other ICL-based models exhibit a higher tendency to produce erroneous answers based on their parametric knowledge compared to our models. Notably, Claude-3.5 demonstrates a more frequent reliance on its parametric knowledge, which elucidates its significantly lower  Trust-Score  score in  Table   11 ."
        ]
    },
    "id_table_11": {
        "caption": "Figure 5:  Statistics of hallucinations from the output of LLaMA-2-7b SFT model prompted using 70K  ( q , D ) q D (q,D) ( italic_q , italic_D )  samples obtained in Step-2 of  Trust-Align .",
        "table": "A6.T11.9.9",
        "footnotes": [],
        "references": [
            "For an LLM used for RAG task, it is important to study the tendency of LLM towards grounding its knowledge on the provided documents. To partially quantify this, we compute EM score for questions that are unanswerable by the provided documents; thus a fraction of cases where  A G  A D =  subscript A G subscript A D A_{G}\\cap A_{D}=\\emptyset italic_A start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT  italic_A start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT =   but  A G =  subscript A G A_{G}\\neq\\emptyset italic_A start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT =   (more details on the metric in  Section   F.2 ). In  Table   9 , our analysis reveals that responsive models (high AR%) tend to rely on parametric knowledge more frequently (high  P score subscript P score \\text{P}_{\\text{score}} P start_POSTSUBSCRIPT score end_POSTSUBSCRIPT ). Notably, closed-source models like GPT-4 exhibit higher parametric knowledge usage compared to open-source and  Trust-Align  models. However,  P score subscript P score \\text{P}_{\\text{score}} P start_POSTSUBSCRIPT score end_POSTSUBSCRIPT  only partially captures the models utilization of parametric knowledge. For instance, it does not account for cases where the document contains the answer and the model still relied on the parametric knowledge to generate the correct answer (also present in the document). This phenomenon is evident in  Table   11 , where on ASQA, GPT-4 achieves a significantly higher  EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  than our models, yet its attribution groundedness score  F1 CG  is five points lower.",
            "In  Table   9 , our analysis reveals that responsive models tend to rely on parametric knowledge more frequently. Notably, closed-source models like GPT-4 exhibit higher parametric knowledge usage compared to our models. However, this metric only partially captures the models utilization of parametric knowledge. For instance, cases where models correctly generate gold claims without proper grounding may also indicate reliance on parametric knowledge. This phenomenon is evident in  Table   11 , where on ASQA, GPT-4 achieves a significantly higher  EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  than our models, yet its attribution groundedness score  F1 CG  is five points lower.",
            "The findings are presented in  Table   10 . Our analysis reveals that, with the exception of LLaMA-2 7B which provides no responses, all other ICL-based models exhibit a higher tendency to produce erroneous answers based on their parametric knowledge compared to our models. Notably, Claude-3.5 demonstrates a more frequent reliance on its parametric knowledge, which elucidates its significantly lower  Trust-Score  score in  Table   11 .",
            "We continue our comparison of trustworthiness against competitive closed-source models utilizing in-context learning techniques. As shown in  Table   11 , our aligned models outperform GPT-3.5 ( 69.23 69.23 69.23 69.23  vs.  67.64 67.64 67.64 67.64 ) and Claude-3.5 ( 69.23 69.23 69.23 69.23  vs.  64.36 64.36 64.36 64.36 ) on the ASQA dataset, and substantially outperform GPT-3.5 ( 55.31 55.31 55.31 55.31  vs.  38.95 38.95 38.95 38.95 ), GPT-4 ( 55.31 55.31 55.31 55.31  vs.  40.35 40.35 40.35 40.35 ), and Claude-3.5 ( 55.31 55.31 55.31 55.31  vs.  39.78 39.78 39.78 39.78 ) on QAMPARI. However, the responsiveness of current closed-source models remains much higher than that of our models: even with a refusal prompt, ICL-GPT-4 still answers a significant fraction of questions (86.81% on ASQA, 73.40% on QAMPARI). As discussed in  Section   6 , this tendency allows GPT-4 to achieve higher  EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  scores on ASQA, but it negatively impacts its attribution groundedness: its  F1 CG CG {}_{\\text{CG}} start_FLOATSUBSCRIPT CG end_FLOATSUBSCRIPT  scores on both datasets are lower than those of our models. Similarly, GPT-4s  F1 RG RG {}_{\\text{RG}} start_FLOATSUBSCRIPT RG end_FLOATSUBSCRIPT  scores on both datasets are also lower. On QAMPARI, the  EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  scores of all closed-source models are lower than those of our models."
        ]
    },
    "id_table_12": {
        "caption": "Figure 6:  Comparison of EM regular and EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  across models on ASQA.",
        "table": "A6.T12.1",
        "footnotes": [],
        "references": [
            "Moreover, there still remains a gap between our models and the closed-source models on the ELI5 dataset. Our models  Trust-Score  is 2.45 points lower than that of the advanced ICL-GPT-4, and specifically, the  EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  and  F1 CG CG {}_{\\text{CG}} start_FLOATSUBSCRIPT CG end_FLOATSUBSCRIPT  scores are lower. For higher  EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT , as discussed in  Section   6 , it is due to a higher number of its answered answerable questions with comparable EM AC  subscript superscript absent  AC {}^{\\alpha}_{\\text{AC}} start_FLOATSUPERSCRIPT italic_ end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT . As for higher  F1 CG CG {}_{\\text{CG}} start_FLOATSUBSCRIPT CG end_FLOATSUBSCRIPT , We hypothesize that this gap could be attributed to the information density of the extracted claims utilized in constructing the alignment data ( Section   4 ). Specifically, the three claims derived from the decomposition process may either be redundant or inadequate to fully encapsulate the information inherent in the original labelled response. In some cases, the decomposed claims may even fail to align with the original facts. First, insufficient information can lead the model to learn to extract fewer facts from the document, thereby reducing the answerability by covering fewer correct answers after training. Second, redundant information can impair grounded citation learning, as it repeats the same information across different claims, making the model less capable of performing precise citations from the corresponding documents. This issue is illustrated in the case study presented in  Table   12 ."
        ]
    },
    "id_table_13": {
        "caption": "Table 9:  Detection of parametric knowledge usage under refusal prompting.",
        "table": "A6.T13.3.3",
        "footnotes": [],
        "references": [
            "To demonstrate the robustness of our synthesized alignment data across different training methods,  Table   13  also includes the performance of SFT and SIMPO  Meng et al. ( 2024 )  methods. Compared to the SFT baseline, which only utilizes the positive data points in the alignment pairs to fine-tune the base model, preference optimization methods, such as DPO and SIMPO, consistently show performance improvements, highlighting the versatility of our data pipeline. Unlike the SFT approach, DPO and SIMPO demonstrate improved TRUST scores, albeit with a reduction in responsiveness. This decrease in responsiveness is actually a favorable outcome, as it indicates that the models are less likely to attempt to answer questions for which they lack sufficient information."
        ]
    },
    "id_table_14": {
        "caption": "Table 10:  The proportions of erroneous answers present in or absent from the documents.",
        "table": "A6.T14.6.6",
        "footnotes": [],
        "references": [
            "The determination of question answerability in our dataset is based on a combination of substring matching and TRUE criteria, as detailed in  Section   2 . Additionally, we developed an alternative version of the evaluation data that relies solely on substring matching, disregarding the TRUE criterion. This relaxation of answerability constraints results in an increased number of answerable questions. The findings from this analysis are presented in  Table   14 . It is worth noting that the overall trends observed in this analysis align with those reported in  Section   6 , which employs the combined approach of substring matching followed by TRUE verification."
        ]
    },
    "id_table_15": {
        "caption": "Table 11:  Our models vs closed source:  AR%  := Answered Ratio in %;  EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  := Exact Match F1 (Calibrated);  F1 RG RG {}_{\\text{RG}} start_FLOATSUBSCRIPT RG end_FLOATSUBSCRIPT  := Grounded refusals F1;  F1 CG CG {}_{\\text{CG}} start_FLOATSUBSCRIPT CG end_FLOATSUBSCRIPT  := Citation Grounded F1;  TRUST  := TRUST score.  R  := Refusal prompt is used.  D  := Default prompt is used.",
        "table": "A8.T15.1.1",
        "footnotes": [],
        "references": [
            "Following  Liu et al. ( 2023 ); Gao et al. ( 2023b ) , to form  D D D italic_D , we divide large text documents into 100-word passages and limit the number of citations  C i subscript C i \\mathcal{C}_{i} caligraphic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  for each claim to a maximum of three. If the response is empty, it is excluded from evaluation. We provide statistics of our evaluation in  Table   15 ."
        ]
    },
    "id_table_16": {
        "caption": "Table 12:    A case study of the failure of decomposition.",
        "table": "A8.T16.1",
        "footnotes": [],
        "references": [
            "We employed two methods to measure refusals robustly. In a refusal prompt, models were explicitly instructed to respond only with the phrase:  I apologize, but I couldnt find an answer to your question in the search results.  without providing any further explanation. As the models generally complied with this pattern, we were able to apply fuzzy matching 15 15 15 Fuzz Partial Ratio  was used to mitigate the impact of string length.  to detect the phrase above indicating refusal. For models responding to a default prompt, refusals did not adhere to a fixed pattern, making detection more challenging. Two human annotators verified that fuzzy matching yielded poor performance  Table   16 . Hence, GPT-4o was employed as an evaluator to classify whether an answer should be considered a refusal. The specific prompt used is provided in  Table   25 ."
        ]
    },
    "id_table_17": {
        "caption": "Table 13:  Results using different alignment methods on the ASQA dataset.",
        "table": "A9.T17.10.10",
        "footnotes": [],
        "references": [
            "Table   17 ,  Table   18 ,  Table   19  and  Table   20  show the full results of our experiments."
        ]
    },
    "id_table_18": {
        "caption": "Table 14:  Results on ASQA, QAMPARI, and ELI5 evaluation datasets where the data are created without using TRUE;  AR%  := Answered Ratio in %;  EM AC F1 subscript superscript absent F1 AC {}^{\\text{F1}}_{\\text{AC}} start_FLOATSUPERSCRIPT F1 end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT AC end_POSTSUBSCRIPT  := Exact Match F1 (Calibrated);  F1 RG RG {}_{\\text{RG}} start_FLOATSUBSCRIPT RG end_FLOATSUBSCRIPT  := Grounded refusals F1;  F1 CG CG {}_{\\text{CG}} start_FLOATSUBSCRIPT CG end_FLOATSUBSCRIPT  := Citation Grounded F1;  TRUST  :=  Trust-Score .  R  := Refusal prompt is used.  D  := Default prompt is used.",
        "table": "A9.T18.9.9",
        "footnotes": [],
        "references": [
            "Table   17 ,  Table   18 ,  Table   19  and  Table   20  show the full results of our experiments."
        ]
    },
    "id_table_19": {
        "caption": "Figure 7:  Influence of the proportion of augmented training samples on  Trust-Score  in LLaMA-3-8b.",
        "table": "A9.T19.10.10",
        "footnotes": [],
        "references": [
            "Table   17 ,  Table   18 ,  Table   19  and  Table   20  show the full results of our experiments."
        ]
    },
    "id_table_20": {
        "caption": "Table 15:  Statistics of Evaluation Dataset",
        "table": "A9.T20.10.10",
        "footnotes": [],
        "references": [
            "Table   17 ,  Table   18 ,  Table   19  and  Table   20  show the full results of our experiments."
        ]
    },
    "id_table_21": {
        "caption": "Table 16:    A case study illustrating the necessity of GPT-4o matching for detecting refusals in models prompted with default prompt.",
        "table": "A10.T21.1",
        "footnotes": [],
        "references": [
            "To generate preferred responses, by prompting GPT-4, we stitch together the gold claims and citations 6 6 6 Prompt template can be found at  Table   21 . . For unanswerable questions, we assign a ground truth refusal response. To obtain quality negative (unpreferred) responses, we fine-tune LLaMA-2-7b on the source datasets, creating  M s  f  t subscript M s f t \\mathcal{M}_{sft} caligraphic_M start_POSTSUBSCRIPT italic_s italic_f italic_t end_POSTSUBSCRIPT . Testing  M s  f  t subscript M s f t \\mathcal{M}_{sft} caligraphic_M start_POSTSUBSCRIPT italic_s italic_f italic_t end_POSTSUBSCRIPT  on the 70K dataset identified 40K responses with hallucinations.  Table   1  shows hallucination severity ( e i subscript e i e_{i} italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) and frequency ( w i subscript w i w_{i} italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ). To obtain good negative samples, we first rank each of the 40K responses according to their severity score  e q subscript e q e_{q} italic_e start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT . We then select the top 50% 7 7 7 See  Section   F.8  for more details on this hyperparameter.  of the corresponding samples for both answerable and unanswerable responses. We perform DPO using this set of 19k samples to obtain the final aligned model.",
            "We develop an automated data labeling pipeline that synthesizes natural responses from gold claims and maps each statement to the corresponding documents for embedded in-line citations. The gold claims are obtained from the source datasets (ASQA, QAMPARI, ELI5) and calibrated to the provided documents, i.e., filtering out claims that cannot be derived from  D D D italic_D . We first split the questions into answerable and unanswerable samples based on whether the provided documents entail the gold claims. For an answerable sample, consisting of a question  q q q italic_q , a set of documents  D D D italic_D , and a list of (calibrated) gold claims, we prompt GPT-4 to generate a natural response by stitching together the gold claims using a template ( Table   21 ). Please refer to the subsection below for more details on how the prompt is structured for each dataset. The prompt template asks GPT-4 to label each gold claim used with its index from the provided list (e.g., [Gold Claim X]), allowing for later matching of claims to documents. For unanswerable questions, a refusal response is assigned. To generate citations corresponding to each statement generated, we map the [Gold Claim X] labels to the appropriate documents. First, we extract all such labels from a sentence (which may contain multiple claims and labels). Then, we greedily identify the smallest combination of documents that covers these claims, minimizing over-citation. Details of this process is illustrated in  Fig.   4 .",
            "Table   21 ,  Table   22 ,  Table   23 ,  Table   24  and  Table   25  show the prompts used in our experiments."
        ]
    },
    "id_table_22": {
        "caption": "Table 17:  Detailed ASQA results. Best  Trust-Score  values within the open-source baseline, closed-source baseline, and  Trust-Align  sets are marked in  bold  (corresponding rows  highlighted ), with the overall best  Trust-Score  values  underlined .",
        "table": "A10.T22.1.1",
        "footnotes": [],
        "references": [
            "The dataset construction begins by collecting a set of high-quality (challenging) and diverse questions from source datasets i.e. ASQA, QAMPARI, and ELI5referred to as  seed samples . To collect such samples, we first divide the questions in a dataset into  k k k italic_k  clusters using a Hugginface pipeline 11 11 11 https://github.com/huggingface/text-clustering/ . After identifying the diverse clusters, we use Mixtral-8x7B with the prompt described in  Table   22  to assign each a quality score ranging from 1 to 7. The quality of a cluster is determined by how difficult it is to answer the questions without requiring additional information i.e. a higher score corresponds to a high difficulty. We then select clusters with a quality score of 4 or higher and sample the desired number of questions from these top clusters. Suppose we have three clusters,  C 1 , C 2 , C 3 subscript C 1 subscript C 2 subscript C 3 C_{1},C_{2},C_{3} italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_C start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , with respective sizes  N 1 , N 2 , N 3 subscript N 1 subscript N 2 subscript N 3 N_{1},N_{2},N_{3} italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_N start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_N start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , where  N c = N 1 + N 2 + N 3 subscript N c subscript N 1 subscript N 2 subscript N 3 N_{c}=N_{1}+N_{2}+N_{3} italic_N start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT = italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_N start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + italic_N start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT . To sample  N s subscript N s N_{s} italic_N start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  questions from the clusters, we sample  N s  C i N c subscript N s subscript C i subscript N c N_{s}\\times\\frac{C_{i}}{N_{c}} italic_N start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  divide start_ARG italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_ARG italic_N start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT end_ARG  questions from cluster  C i subscript C i C_{i} italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . If this number exceeds the available questions in the cluster, we randomly sample the remaining questions from the filtered-out clusters (those with a quality score below 4). This process ensures that the seed set prioritizes both high quality and diversity. For this paper, we set  N s subscript N s N_{s} italic_N start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  to 3K, 3K, and 4K for ASQA, QAMPARI, and ELI5, respectively, resulting in approximately 10K questions in the seed set.",
            "Table   21 ,  Table   22 ,  Table   23 ,  Table   24  and  Table   25  show the prompts used in our experiments."
        ]
    },
    "id_table_23": {
        "caption": "Table 18:  Detailed QAMPARI results. Best  Trust-Score  values within the open-source baseline, closed-source baseline, and  Trust-Align  sets are marked in  bold  (corresponding rows  highlighted ), with the overall best  Trust-Score  values  underlined .",
        "table": "A10.T23.1.1",
        "footnotes": [],
        "references": [
            "Table   21 ,  Table   22 ,  Table   23 ,  Table   24  and  Table   25  show the prompts used in our experiments."
        ]
    },
    "id_table_24": {
        "caption": "Table 19:  Detailed ELI5 results. Best  Trust-Score  values within the open-source baseline, closed-source baseline, and  Trust-Align  sets are marked in  bold  (corresponding rows  highlighted ), with the overall best  Trust-Score  values  underlined .",
        "table": "A10.T24.1.1",
        "footnotes": [],
        "references": [
            "For the GPT-4 data pipeline, we employ GPT-4 to simulate a critic that performs two key tasks in succession. First, it identifies and revises mistakes or supplements missing information in the given response based on correct answers. Second, it validates the attribution of statement-level citations and corrects them accordingly. The detailed instruction is provided in  Table   24 .",
            "Table   21 ,  Table   22 ,  Table   23 ,  Table   24  and  Table   25  show the prompts used in our experiments."
        ]
    },
    "id_table_25": {
        "caption": "Table 20:  Detailed ExpertQA results. Best  Trust-Score  values within the open-source baseline, closed-source baseline, and  Trust-Align  sets are marked in  bold  (corresponding rows  highlighted ), with the overall best  Trust-Score  values  underlined .",
        "table": "A10.T25.1.1",
        "footnotes": [],
        "references": [
            "We employed two methods to measure refusals robustly. In a refusal prompt, models were explicitly instructed to respond only with the phrase:  I apologize, but I couldnt find an answer to your question in the search results.  without providing any further explanation. As the models generally complied with this pattern, we were able to apply fuzzy matching 15 15 15 Fuzz Partial Ratio  was used to mitigate the impact of string length.  to detect the phrase above indicating refusal. For models responding to a default prompt, refusals did not adhere to a fixed pattern, making detection more challenging. Two human annotators verified that fuzzy matching yielded poor performance  Table   16 . Hence, GPT-4o was employed as an evaluator to classify whether an answer should be considered a refusal. The specific prompt used is provided in  Table   25 .",
            "Table   21 ,  Table   22 ,  Table   23 ,  Table   24  and  Table   25  show the prompts used in our experiments."
        ]
    },
    "global_footnotes": [
        "While there are many applications where retrieved documents are used to assist LLMs in providing better responses, and LLMs are expected to use their parametric knowledge, in this paper, we study the problem of complete groundednessi.e., all claims should be documents derivable, typically making this an IR task.",
        "In this paper, we use LLM groundedness and trustworthiness interchangeably in the context of RAG.",
        "Notably, both",
        "and",
        "sum over samples that are both answered and answerable, differing primarily in their normalization values.",
        "An NLI model checks if the cited document entails the statement.",
        "Clustering and document retrieval details are in",
        ".",
        "Prompt template can be found at",
        ".",
        "See",
        "for more details on this hyperparameter.",
        "All models used are instruct tuned or chat versions.",
        "For EM, the bias is that a",
        "is answerable if an exact match for claims is present in",
        ".",
        "Seed questions, corresponding oracle documents, and the gold answers (",
        ").",
        "We utilize the latest version on the AzureOpenAI Service:",
        "was used to mitigate the impact of string length."
    ]
}