{
    "id_table_1": {
        "caption": "Table 1.  Performance benchmark across 200 Synthetic user queries",
        "table": "S5.T1.1.1",
        "footnotes": [],
        "references": [
            "For each of the synthetic user queries generated, we ran a comparison prompt that includes the context retrieved as part of each approach, together with their final answers. We prompted Claude 3 Sonnet to rate each of the metrics on a scale from 0 to 100, together with a justification text. An example of the evaluation response is provided in Appendix E. The obtained metrics are then averaged across all queries and displayed below in Figure 2. We observe a clear benefit across all metrics but the precision of the retrieved documents by our two proposed QA-based methodologies. The lack of strong improvement over the precision metric is consistent with the usage of a single encoding model and show that few documents are considered completely irrelevant. Specifically, we note a significant performance boost in both the breadth and the depth of the final LLM response. This result shows that the MK Summary is providing additional information that is leveraged by the query augmentation step. Finally, the contribution of the MK summary to the conditioning of the search itself appears statistically significant across all metrics but the precision of the retriever ( p < 0.01 p 0.01 p<0.01 italic_p < 0.01  between the augmented QA search and the MK-Augmented QA search)(see Table 1). We observe that the the proposed methodology significantly improves the breadth of the search (by more than 20%, compared to traditional naive search with chunking approaches), which aligns to the intuition that our proposal allows for effectively synthetizing more information from the content of the database, and leveraging its content more extensively."
        ]
    },
    "id_table_2": {
        "caption": "",
        "table": "S5.T1.2.1",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": [
        "The dataset was filtered using the following categories on the Arxiv API: stat.ML, stat.TH, stat.AP, stat.ME, math.ST, cs.AI, cs.LG, econ.EM. Thank you to arXiv for use of its open access interoperability."
    ]
}