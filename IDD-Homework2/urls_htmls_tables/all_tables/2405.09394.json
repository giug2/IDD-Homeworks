{
    "PAPER'S NUMBER OF TABLES": 5,
    "S3.T1": {
        "caption": "Table 1: Brief description of datasets, models and hyper-parameters.",
        "table": "<table id=\"S3.T1.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S3.T1.2.1\" class=\"ltx_tr\">\n<td id=\"S3.T1.2.1.1\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\"><span id=\"S3.T1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td id=\"S3.T1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">CIFAR-10</td>\n<td id=\"S3.T1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">Medical Face</td>\n</tr>\n<tr id=\"S3.T1.2.2\" class=\"ltx_tr\">\n<td id=\"S3.T1.2.2.1\" class=\"ltx_td ltx_align_right ltx_border_r\"><span id=\"S3.T1.2.2.1.1\" class=\"ltx_text ltx_font_bold\">Task</span></td>\n<td id=\"S3.T1.2.2.2\" class=\"ltx_td ltx_align_center\">\n<span id=\"S3.T1.2.2.2.1\" class=\"ltx_text\"></span> <span id=\"S3.T1.2.2.2.2\" class=\"ltx_text\">\n<span id=\"S3.T1.2.2.2.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S3.T1.2.2.2.2.1.1\" class=\"ltx_tr\">\n<span id=\"S3.T1.2.2.2.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Multi-Class</span></span>\n<span id=\"S3.T1.2.2.2.2.1.2\" class=\"ltx_tr\">\n<span id=\"S3.T1.2.2.2.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Classification</span></span>\n</span></span><span id=\"S3.T1.2.2.2.3\" class=\"ltx_text\"></span>\n</td>\n<td id=\"S3.T1.2.2.3\" class=\"ltx_td ltx_align_center\">\n<span id=\"S3.T1.2.2.3.1\" class=\"ltx_text\"></span> <span id=\"S3.T1.2.2.3.2\" class=\"ltx_text\">\n<span id=\"S3.T1.2.2.3.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S3.T1.2.2.3.2.1.1\" class=\"ltx_tr\">\n<span id=\"S3.T1.2.2.3.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Multi-Label</span></span>\n<span id=\"S3.T1.2.2.3.2.1.2\" class=\"ltx_tr\">\n<span id=\"S3.T1.2.2.3.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Classification</span></span>\n</span></span><span id=\"S3.T1.2.2.3.3\" class=\"ltx_text\"></span>\n</td>\n</tr>\n<tr id=\"S3.T1.2.3\" class=\"ltx_tr\">\n<td id=\"S3.T1.2.3.1\" class=\"ltx_td ltx_align_right ltx_border_r\"><span id=\"S3.T1.2.3.1.1\" class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td id=\"S3.T1.2.3.2\" class=\"ltx_td ltx_align_center\">Swin-T</td>\n<td id=\"S3.T1.2.3.3\" class=\"ltx_td ltx_align_center\">Query2Label</td>\n</tr>\n<tr id=\"S3.T1.2.4\" class=\"ltx_tr\">\n<td id=\"S3.T1.2.4.1\" class=\"ltx_td ltx_align_right ltx_border_r\"><span id=\"S3.T1.2.4.1.1\" class=\"ltx_text ltx_font_bold\">Size(MB)</span></td>\n<td id=\"S3.T1.2.4.2\" class=\"ltx_td ltx_align_center\">105.01</td>\n<td id=\"S3.T1.2.4.3\" class=\"ltx_td ltx_align_center\">180.61</td>\n</tr>\n<tr id=\"S3.T1.2.5\" class=\"ltx_tr\">\n<td id=\"S3.T1.2.5.1\" class=\"ltx_td ltx_align_right ltx_border_r\"><span id=\"S3.T1.2.5.1.1\" class=\"ltx_text ltx_font_bold\">Optimizer</span></td>\n<td id=\"S3.T1.2.5.2\" class=\"ltx_td ltx_align_center\">SGD</td>\n<td id=\"S3.T1.2.5.3\" class=\"ltx_td ltx_align_center\">Adam</td>\n</tr>\n<tr id=\"S3.T1.2.6\" class=\"ltx_tr\">\n<td id=\"S3.T1.2.6.1\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\"><span id=\"S3.T1.2.6.1.1\" class=\"ltx_text ltx_font_bold\">Learning rate</span></td>\n<td id=\"S3.T1.2.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.03</td>\n<td id=\"S3.T1.2.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.0001</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "To validate the efficiency of SA-FedLora, we conduct experiments on two transformer models: Swin-T Liu et al. (2021b) model and Query2Label Liu et al. (2021a) model on CIFAR-10 dataset Krizhevsky et al. (2009) and a real-world private medical Face dataset, respectively. To further validate our FL method in more challenging settings, we construct a healthcare dataset of Face data with multi-label disease tasks, including 9 types of diseases (e.g., hyperlipidemia, malignancy lung). Each disease can be binarized the labels to positive and negative. The CIFAR-10 dataset and Face dataset are manually partitioned into 5 and 4 clients, respectively. For a fair comparison, all models used in this paper are pre-trained on ImageNet Deng et al. (2009). The datasets and corresponding models are summarized in Table 1.",
            "Experiments were conducted using Pytorch 2.1.0 with Python 3.11 on NVIDIA A40 GPUs. For the CIFAR-10 dataset, we choose an SGD optimizer with a learning rate of 0.03 for the Swin-T model. The maximal communication round of initiating stage T1subscriptùëá1T_{1} and annealing stage T2subscriptùëá2T_{2} is set to 3 and 80, respectively. The maximal local epoch of initiating stage E1subscriptùê∏1E_{1} and annealing stage E2subscriptùê∏2E_{2} is set to 30 and 2, respectively. For the Face dataset, we choose an Adam optimizer with a learning rate of 0.0001 for the Query2Label model. The maximal communication round T1subscriptùëá1T_{1} and T2subscriptùëá2T_{2} is set to 3 and 50, respectively. The maximal local epoch E1subscriptùê∏1E_{1} and E2subscriptùê∏2E_{2} are set to 30 and 2, respectively. Their specific values will be dynamically tuned based to the experimental design. Some important implementations are also detailed in Table 1."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: The main result of experiments on the CIFAR-10 dataset. The converged cost (CC) is the communication cost when the algorithm reaches convergence. The target cost (TC) is the communication cost when the algorithm reaches an accuracy of 0.85.",
        "table": "<table id=\"S4.T2.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S4.T2.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">Method</td>\n<td id=\"S4.T2.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">CC (GB)</td>\n<td id=\"S4.T2.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">TC (GB)</td>\n<td id=\"S4.T2.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">ACC</td>\n</tr>\n<tr id=\"S4.T2.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Local</td>\n<td id=\"S4.T2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">/</td>\n<td id=\"S4.T2.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">/</td>\n<td id=\"S4.T2.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.5858</td>\n</tr>\n<tr id=\"S4.T2.2.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.3.1\" class=\"ltx_td ltx_align_center ltx_border_r\">FedAvg</td>\n<td id=\"S4.T2.2.3.2\" class=\"ltx_td ltx_align_center\">37.94</td>\n<td id=\"S4.T2.2.3.3\" class=\"ltx_td ltx_align_center\">37.94</td>\n<td id=\"S4.T2.2.3.4\" class=\"ltx_td ltx_align_center\">0.8510</td>\n</tr>\n<tr id=\"S4.T2.2.4\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.4.1\" class=\"ltx_td ltx_align_center ltx_border_r\">FedPer</td>\n<td id=\"S4.T2.2.4.2\" class=\"ltx_td ltx_align_center\">41.01</td>\n<td id=\"S4.T2.2.4.3\" class=\"ltx_td ltx_align_center\">30.76</td>\n<td id=\"S4.T2.2.4.4\" class=\"ltx_td ltx_align_center\">0.8656</td>\n</tr>\n<tr id=\"S4.T2.2.5\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.5.1\" class=\"ltx_td ltx_align_center ltx_border_r\">FedFFT</td>\n<td id=\"S4.T2.2.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.2.5.2.1\" class=\"ltx_text ltx_font_bold\">32.82</span></td>\n<td id=\"S4.T2.2.5.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.2.5.3.1\" class=\"ltx_text ltx_font_bold\">13.33</span></td>\n<td id=\"S4.T2.2.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.2.5.4.1\" class=\"ltx_text ltx_font_bold\">0.9154</span></td>\n</tr>\n<tr id=\"S4.T2.2.6\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.6.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedBit</td>\n<td id=\"S4.T2.2.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.2.6.2.1\" class=\"ltx_text ltx_font_bold\">2.22</span></td>\n<td id=\"S4.T2.2.6.3\" class=\"ltx_td ltx_align_center ltx_border_t\">/</td>\n<td id=\"S4.T2.2.6.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.8239</td>\n</tr>\n<tr id=\"S4.T2.2.7\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.7.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S4.T2.2.7.1.1\" class=\"ltx_text ltx_font_bold\">SA-FedLora</span></td>\n<td id=\"S4.T2.2.7.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">2.69</td>\n<td id=\"S4.T2.2.7.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.2.7.3.1\" class=\"ltx_text ltx_font_bold\">2.42</span></td>\n<td id=\"S4.T2.2.7.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.2.7.4.1\" class=\"ltx_text ltx_font_bold\">0.9050</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Our study conducts experiments on the natural CIFAR-10 dataset to verify the efficiency and effectiveness. As the main results in Table 2 show, the converged cost (CC) is the communication cost when the algorithm reaches convergence. The target cost (TC) is the communication cost when the algorithm reaches an accuracy of 0.85. We divide algorithms into two groups based on whether they belong to the PEFT methods. Among all methods, FedBit and SA-FedLora are included in the PEFT. Our proposed SA-FedLora is a more efficient FL framework with lower communication costs compared to all algorithms except FedFFT. For instance, SA-FedLora is greater than FedAvg with improvement of accuracy by 6.35% and decline of the CC by 92.91% and TC by 93.62%. We can observe from the Figure 3 that FedBit even can not achieve the target accuracy due to the few trainable bias terms, although it‚Äôs initialized by the pre-trained model and saves the communication costs. In addition, the Local method performs worst due to the scarcity of available data. FedPer reduces the communication overhead due to its preservation of personalized layers. FedFFT is the upper bound among all algorithms since it fine-tunes the full model parameters based on the pre-trained weights and biases."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: The main result of experiments on the Face dataset. ",
        "table": "<table id=\"S4.T3.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S4.T3.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">Method</td>\n<td id=\"S4.T3.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">CC (GB)</td>\n<td id=\"S4.T3.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">AUC</td>\n<td id=\"S4.T3.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">ACC</td>\n</tr>\n<tr id=\"S4.T3.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Local</td>\n<td id=\"S4.T3.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">/</td>\n<td id=\"S4.T3.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.6800</td>\n<td id=\"S4.T3.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.8545</td>\n</tr>\n<tr id=\"S4.T3.2.3\" class=\"ltx_tr\">\n<td id=\"S4.T3.2.3.1\" class=\"ltx_td ltx_align_center ltx_border_r\">FedAvg</td>\n<td id=\"S4.T3.2.3.2\" class=\"ltx_td ltx_align_center\">39.51</td>\n<td id=\"S4.T3.2.3.3\" class=\"ltx_td ltx_align_center\">0.7944</td>\n<td id=\"S4.T3.2.3.4\" class=\"ltx_td ltx_align_center\">0.8908</td>\n</tr>\n<tr id=\"S4.T3.2.4\" class=\"ltx_tr\">\n<td id=\"S4.T3.2.4.1\" class=\"ltx_td ltx_align_center ltx_border_r\">FedPer</td>\n<td id=\"S4.T3.2.4.2\" class=\"ltx_td ltx_align_center\">29.94</td>\n<td id=\"S4.T3.2.4.3\" class=\"ltx_td ltx_align_center\">0.8378</td>\n<td id=\"S4.T3.2.4.4\" class=\"ltx_td ltx_align_center\">0.8927</td>\n</tr>\n<tr id=\"S4.T3.2.5\" class=\"ltx_tr\">\n<td id=\"S4.T3.2.5.1\" class=\"ltx_td ltx_align_center ltx_border_r\">FedFFT</td>\n<td id=\"S4.T3.2.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.2.5.2.1\" class=\"ltx_text ltx_font_bold\">29.63</span></td>\n<td id=\"S4.T3.2.5.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.2.5.3.1\" class=\"ltx_text ltx_font_bold\">0.8689</span></td>\n<td id=\"S4.T3.2.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.2.5.4.1\" class=\"ltx_text ltx_font_bold\">0.8962</span></td>\n</tr>\n<tr id=\"S4.T3.2.6\" class=\"ltx_tr\">\n<td id=\"S4.T3.2.6.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedBit</td>\n<td id=\"S4.T3.2.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.2.6.2.1\" class=\"ltx_text ltx_font_bold\">0.08</span></td>\n<td id=\"S4.T3.2.6.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.6611</td>\n<td id=\"S4.T3.2.6.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.8685</td>\n</tr>\n<tr id=\"S4.T3.2.7\" class=\"ltx_tr\">\n<td id=\"S4.T3.2.7.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S4.T3.2.7.1.1\" class=\"ltx_text ltx_font_bold\">SA-FedLora</span></td>\n<td id=\"S4.T3.2.7.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">3.45</td>\n<td id=\"S4.T3.2.7.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T3.2.7.3.1\" class=\"ltx_text ltx_font_bold\">0.8600</span></td>\n<td id=\"S4.T3.2.7.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T3.2.7.4.1\" class=\"ltx_text ltx_font_bold\">0.8892</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "To further demonstrate the efficiency of SA-FedLora in more complex scenarios, we compare the performance of different algorithms on a healthcare Face dataset with the multi-label disease task, where AUC is a more common indicator after the training convergence to diagnose diseases in the medical field. As the Table 3 illustrated, SA-FedLora reduces the CC by 91.27% and improves the AUC of FedAvg by 8.26%, achieving the comparable performance in ACC simultaneously. Results in Figure 4 reveal the optimal AUC and ACC of the FedFFT among all methods. FedPer outperforms the Fedavg resulting from the mitigation of heterogeneity in FL."
        ]
    },
    "S4.T4": {
        "caption": "Table 4: Ablation study of the initiating stage and annealing stage.",
        "table": "<table id=\"S4.T4.4\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S4.T4.4.5\" class=\"ltx_tr\">\n<td id=\"S4.T4.4.5.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T4.4.5.1.1\" class=\"ltx_text\">Mode</span></td>\n<td id=\"S4.T4.4.5.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T4.4.5.2.1\" class=\"ltx_text\"><span id=\"S4.T4.4.5.2.1.1\" class=\"ltx_text\"></span> <span id=\"S4.T4.4.5.2.1.2\" class=\"ltx_text\">\n<span id=\"S4.T4.4.5.2.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S4.T4.4.5.2.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S4.T4.4.5.2.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Initiating</span></span>\n<span id=\"S4.T4.4.5.2.1.2.1.2\" class=\"ltx_tr\">\n<span id=\"S4.T4.4.5.2.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Stage</span></span>\n</span></span> <span id=\"S4.T4.4.5.2.1.3\" class=\"ltx_text\"></span></span></td>\n<td id=\"S4.T4.4.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T4.4.5.3.1\" class=\"ltx_text\"><span id=\"S4.T4.4.5.3.1.1\" class=\"ltx_text\"></span> <span id=\"S4.T4.4.5.3.1.2\" class=\"ltx_text\">\n<span id=\"S4.T4.4.5.3.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S4.T4.4.5.3.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S4.T4.4.5.3.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Annealing</span></span>\n<span id=\"S4.T4.4.5.3.1.2.1.2\" class=\"ltx_tr\">\n<span id=\"S4.T4.4.5.3.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Stage</span></span>\n</span></span> <span id=\"S4.T4.4.5.3.1.3\" class=\"ltx_text\"></span></span></td>\n<td id=\"S4.T4.4.5.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\">CIFAR-10</td>\n<td id=\"S4.T4.4.5.5\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">Face</td>\n</tr>\n<tr id=\"S4.T4.4.6\" class=\"ltx_tr\">\n<td id=\"S4.T4.4.6.1\" class=\"ltx_td ltx_align_center ltx_border_t\">CC (GB)</td>\n<td id=\"S4.T4.4.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">ACC</td>\n<td id=\"S4.T4.4.6.3\" class=\"ltx_td ltx_align_center ltx_border_t\">CC (GB)</td>\n<td id=\"S4.T4.4.6.4\" class=\"ltx_td ltx_align_center ltx_border_t\">AUC</td>\n</tr>\n<tr id=\"S4.T4.4.7\" class=\"ltx_tr\">\n<td id=\"S4.T4.4.7.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">M1 (FedLora)</td>\n<td id=\"S4.T4.4.7.2\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S4.T4.4.7.3\" class=\"ltx_td ltx_border_r ltx_border_t\"></td>\n<td id=\"S4.T4.4.7.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.4.7.4.1\" class=\"ltx_text ltx_font_bold\">1.66</span></td>\n<td id=\"S4.T4.4.7.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.8650</td>\n<td id=\"S4.T4.4.7.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.4.7.6.1\" class=\"ltx_text ltx_font_bold\">1.96</span></td>\n<td id=\"S4.T4.4.7.7\" class=\"ltx_td ltx_align_center ltx_border_t\">0.8556</td>\n</tr>\n<tr id=\"S4.T4.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.1.2\" class=\"ltx_td ltx_align_left ltx_border_r\">M2</td>\n<td id=\"S4.T4.1.1.3\" class=\"ltx_td\"></td>\n<td id=\"S4.T4.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><math id=\"S4.T4.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\surd\" display=\"inline\"><semantics id=\"S4.T4.1.1.1.m1.1a\"><mo id=\"S4.T4.1.1.1.m1.1.1\" xref=\"S4.T4.1.1.1.m1.1.1.cmml\">‚àö</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.1.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T4.1.1.1.m1.1.1.cmml\" xref=\"S4.T4.1.1.1.m1.1.1\">square-root</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.1.1.1.m1.1c\">\\surd</annotation></semantics></math></td>\n<td id=\"S4.T4.1.1.4\" class=\"ltx_td ltx_align_center\">1.71</td>\n<td id=\"S4.T4.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_r\">0.8828</td>\n<td id=\"S4.T4.1.1.6\" class=\"ltx_td ltx_align_center\">2.04</td>\n<td id=\"S4.T4.1.1.7\" class=\"ltx_td ltx_align_center\">0.8567</td>\n</tr>\n<tr id=\"S4.T4.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T4.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_r\">M3</td>\n<td id=\"S4.T4.2.2.1\" class=\"ltx_td ltx_align_center\"><math id=\"S4.T4.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\surd\" display=\"inline\"><semantics id=\"S4.T4.2.2.1.m1.1a\"><mo id=\"S4.T4.2.2.1.m1.1.1\" xref=\"S4.T4.2.2.1.m1.1.1.cmml\">‚àö</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.2.2.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T4.2.2.1.m1.1.1.cmml\" xref=\"S4.T4.2.2.1.m1.1.1\">square-root</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.2.2.1.m1.1c\">\\surd</annotation></semantics></math></td>\n<td id=\"S4.T4.2.2.3\" class=\"ltx_td ltx_border_r\"></td>\n<td id=\"S4.T4.2.2.4\" class=\"ltx_td ltx_align_center\">2.64</td>\n<td id=\"S4.T4.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\">0.8978</td>\n<td id=\"S4.T4.2.2.6\" class=\"ltx_td ltx_align_center\">3.37</td>\n<td id=\"S4.T4.2.2.7\" class=\"ltx_td ltx_align_center\">0.8583</td>\n</tr>\n<tr id=\"S4.T4.4.4\" class=\"ltx_tr\">\n<td id=\"S4.T4.4.4.3\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span id=\"S4.T4.4.4.3.1\" class=\"ltx_text ltx_font_bold\">M4 (SA-FedLora)</span></td>\n<td id=\"S4.T4.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_bb\"><math id=\"S4.T4.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\surd\" display=\"inline\"><semantics id=\"S4.T4.3.3.1.m1.1a\"><mo id=\"S4.T4.3.3.1.m1.1.1\" xref=\"S4.T4.3.3.1.m1.1.1.cmml\">‚àö</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.3.3.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T4.3.3.1.m1.1.1.cmml\" xref=\"S4.T4.3.3.1.m1.1.1\">square-root</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.3.3.1.m1.1c\">\\surd</annotation></semantics></math></td>\n<td id=\"S4.T4.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><math id=\"S4.T4.4.4.2.m1.1\" class=\"ltx_Math\" alttext=\"\\surd\" display=\"inline\"><semantics id=\"S4.T4.4.4.2.m1.1a\"><mo id=\"S4.T4.4.4.2.m1.1.1\" xref=\"S4.T4.4.4.2.m1.1.1.cmml\">‚àö</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.4.4.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T4.4.4.2.m1.1.1.cmml\" xref=\"S4.T4.4.4.2.m1.1.1\">square-root</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.4.4.2.m1.1c\">\\surd</annotation></semantics></math></td>\n<td id=\"S4.T4.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">2.69</td>\n<td id=\"S4.T4.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S4.T4.4.4.5.1\" class=\"ltx_text ltx_font_bold\">0.9050</span></td>\n<td id=\"S4.T4.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">3.45</td>\n<td id=\"S4.T4.4.4.7\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T4.4.4.7.1\" class=\"ltx_text ltx_font_bold\">0.8600</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "In this subsection, we conduct an ablation study to discuss the impact of the initiating stage and annealing stage. The M1 mode is the standard FedLora with a fixed parameter budget, and the M4 mode is our SA-FedLora with additional parameter regularization and annealing parameter budget. As illustrated in Table 4, SA-FedLora outperforms the FedLora with the imoprovement of ACC by 4.62% for the CIFAR-10 dataset and that of AUC by 5.14% for the Face dataset. The initiating stage of M3 and M4 improves the performance due to the parameter regularization mitigating the client drift, but increases the communication cost simultaneously owing to the shared all model parameters. Note that the rising cost of the initiating stage is acceptable since it is negligible compared to the total cost of traditional FedAvg. The annealing stage of M2 and M4 also has a positive effect on the performance due to the dynamical search for global optima, but puts more communication overhead owing to more parameter budget in the early heating phase."
        ]
    },
    "S4.T5": {
        "caption": "Table 5: Analysis of cubic, linear and cosine scheduler on the CIFAR-10 dataset.",
        "table": "<table id=\"S4.T5.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S4.T5.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T5.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">Scheduler</td>\n<td id=\"S4.T5.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">CC (GB)</td>\n<td id=\"S4.T5.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">ACC</td>\n</tr>\n<tr id=\"S4.T5.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T5.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T5.2.2.1.1\" class=\"ltx_text ltx_font_bold\">Cubic</span></td>\n<td id=\"S4.T5.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T5.2.2.2.1\" class=\"ltx_text ltx_font_bold\">2.69</span></td>\n<td id=\"S4.T5.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T5.2.2.3.1\" class=\"ltx_text ltx_font_bold\">0.9050</span></td>\n</tr>\n<tr id=\"S4.T5.2.3\" class=\"ltx_tr\">\n<td id=\"S4.T5.2.3.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Linear</td>\n<td id=\"S4.T5.2.3.2\" class=\"ltx_td ltx_align_center\">2.77</td>\n<td id=\"S4.T5.2.3.3\" class=\"ltx_td ltx_align_center\">0.8962</td>\n</tr>\n<tr id=\"S4.T5.2.4\" class=\"ltx_tr\">\n<td id=\"S4.T5.2.4.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">Cosine</td>\n<td id=\"S4.T5.2.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">2.86</td>\n<td id=\"S4.T5.2.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.8956</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We analyze the performance of three schedulers on the CIFAR-10 dataset. Different schedulers own different rank durations, thus influencing the parameter budget allocation. For example, we can see from Figure 2 that the cubic scheduler between round TasubscriptùëáùëéT_{a} and TbsubscriptùëáùëèT_{b} is a convex function and the cosine scheduler is a concave function. Therefore, the cubic scheduler drops rapidly and takes more time on the lower rank. Results in Table 5 indicate that the cubic scheduler improves the ACC of the worst cosine scheduler by 1.05% and reduces the converged cost (CC) by 0.17GB, which demonstrates that decreasing to the low rank earlier is more beneficial for the parameter allocation of the SA-FedLora."
        ]
    }
}