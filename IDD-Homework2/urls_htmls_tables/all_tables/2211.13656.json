{
    "PAPER'S NUMBER OF TABLES": 6,
    "S2.T1": {
        "caption": "TABLE I: Related work on FL hyper-parameter optimization. We tag if (1) the work can run in an online and single trail manner and (2) the work targets system overheads of FL training. ",
        "table": "",
        "footnotes": "\n\n\n\n\n\nWork\nDescription\nSingle trial\nSystem\n\n\n\nFTS [21]\noptimize client models\n✗\n✗\n\nDP-FTS-DE [22]\ntrade-off privacy and utility\n✗\n✗\n\nAuto-FedRL [23]\nimprove model accuracy\n✓\n✗\n\n[24]\nimprove training robustness\n✓\n✗\n\nFedEx [25]\nNAS based framework\n✗\n✗\n\nFLoRA [26]\nNAS based framework\n✓\n✗\n\nFedTune (Ours)\na lightweight framework\n✓\n✓\n\n\n",
        "references": [
            "Designing HPO methods for FL is a new research area. Only a few approaches have touched FL HPO problems. Table I lists several representative approaches, where we highlight whether the work can execute in an online and single trial manner and whether the work targets system overhead in FL training.\nFor example, BO has been integrated with FL to improve different client models [21] and strength client privacy [22]. Several approaches apply reinforcement learning to adjust FL hyper-parameters [23, 24], which introduces extra complexity and loss of generality. FedEx is a general framework to optimize the round-to-accuracy of FL by exploiting the Neural Architecture Search (NAS) techniques of weight-sharing, which improves the baseline by several percentage points [25]; FLoRA determines the global hyper-parameters by selecting the hyper-parameters that have good performances in local clients [26]. Recently, a benchmark suite for federated\nhyper-parameter optimization [31] is designed, whose effectiveness remains to be investigated."
        ]
    },
    "S3.T2": {
        "caption": "TABLE II: Different models used for the measurement study.",
        "table": "<table id=\"S3.T2.2.2\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T2.2.2.3.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.2.2.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row\">Model</th>\n<th id=\"S3.T2.2.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">ResNet-10</th>\n<th id=\"S3.T2.2.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">ResNet-18</th>\n<th id=\"S3.T2.2.2.3.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">ResNet-26</th>\n<th id=\"S3.T2.2.2.3.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">ResNet-34</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T2.2.2.4.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.2.2.4.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\">#BasicBlock</th>\n<td id=\"S3.T2.2.2.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">[1, 1, 1, 1]</td>\n<td id=\"S3.T2.2.2.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">[2, 2, 2, 2]</td>\n<td id=\"S3.T2.2.2.4.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">[3, 3, 3, 3]</td>\n<td id=\"S3.T2.2.2.4.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\">[3, 4, 6, 3]</td>\n</tr>\n<tr id=\"S3.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">#FLOP (<math id=\"S3.T2.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times 10^{6}\" display=\"inline\"><semantics id=\"S3.T2.1.1.1.1.m1.1a\"><mrow id=\"S3.T2.1.1.1.1.m1.1.1\" xref=\"S3.T2.1.1.1.1.m1.1.1.cmml\"><mi id=\"S3.T2.1.1.1.1.m1.1.1.2\" xref=\"S3.T2.1.1.1.1.m1.1.1.2.cmml\"></mi><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S3.T2.1.1.1.1.m1.1.1.1\" xref=\"S3.T2.1.1.1.1.m1.1.1.1.cmml\">×</mo><msup id=\"S3.T2.1.1.1.1.m1.1.1.3\" xref=\"S3.T2.1.1.1.1.m1.1.1.3.cmml\"><mn id=\"S3.T2.1.1.1.1.m1.1.1.3.2\" xref=\"S3.T2.1.1.1.1.m1.1.1.3.2.cmml\">10</mn><mn id=\"S3.T2.1.1.1.1.m1.1.1.3.3\" xref=\"S3.T2.1.1.1.1.m1.1.1.3.3.cmml\">6</mn></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.T2.1.1.1.1.m1.1b\"><apply id=\"S3.T2.1.1.1.1.m1.1.1.cmml\" xref=\"S3.T2.1.1.1.1.m1.1.1\"><times id=\"S3.T2.1.1.1.1.m1.1.1.1.cmml\" xref=\"S3.T2.1.1.1.1.m1.1.1.1\"></times><csymbol cd=\"latexml\" id=\"S3.T2.1.1.1.1.m1.1.1.2.cmml\" xref=\"S3.T2.1.1.1.1.m1.1.1.2\">absent</csymbol><apply id=\"S3.T2.1.1.1.1.m1.1.1.3.cmml\" xref=\"S3.T2.1.1.1.1.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S3.T2.1.1.1.1.m1.1.1.3.1.cmml\" xref=\"S3.T2.1.1.1.1.m1.1.1.3\">superscript</csymbol><cn type=\"integer\" id=\"S3.T2.1.1.1.1.m1.1.1.3.2.cmml\" xref=\"S3.T2.1.1.1.1.m1.1.1.3.2\">10</cn><cn type=\"integer\" id=\"S3.T2.1.1.1.1.m1.1.1.3.3.cmml\" xref=\"S3.T2.1.1.1.1.m1.1.1.3.3\">6</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T2.1.1.1.1.m1.1c\">\\times 10^{6}</annotation></semantics></math>)</th>\n<td id=\"S3.T2.1.1.1.2\" class=\"ltx_td ltx_align_center\">12.5</td>\n<td id=\"S3.T2.1.1.1.3\" class=\"ltx_td ltx_align_center\">26.8</td>\n<td id=\"S3.T2.1.1.1.4\" class=\"ltx_td ltx_align_center\">41.1</td>\n<td id=\"S3.T2.1.1.1.5\" class=\"ltx_td ltx_align_center\">60.1</td>\n</tr>\n<tr id=\"S3.T2.2.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T2.2.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">#Params (<math id=\"S3.T2.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times 10^{3}\" display=\"inline\"><semantics id=\"S3.T2.2.2.2.1.m1.1a\"><mrow id=\"S3.T2.2.2.2.1.m1.1.1\" xref=\"S3.T2.2.2.2.1.m1.1.1.cmml\"><mi id=\"S3.T2.2.2.2.1.m1.1.1.2\" xref=\"S3.T2.2.2.2.1.m1.1.1.2.cmml\"></mi><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S3.T2.2.2.2.1.m1.1.1.1\" xref=\"S3.T2.2.2.2.1.m1.1.1.1.cmml\">×</mo><msup id=\"S3.T2.2.2.2.1.m1.1.1.3\" xref=\"S3.T2.2.2.2.1.m1.1.1.3.cmml\"><mn id=\"S3.T2.2.2.2.1.m1.1.1.3.2\" xref=\"S3.T2.2.2.2.1.m1.1.1.3.2.cmml\">10</mn><mn id=\"S3.T2.2.2.2.1.m1.1.1.3.3\" xref=\"S3.T2.2.2.2.1.m1.1.1.3.3.cmml\">3</mn></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.T2.2.2.2.1.m1.1b\"><apply id=\"S3.T2.2.2.2.1.m1.1.1.cmml\" xref=\"S3.T2.2.2.2.1.m1.1.1\"><times id=\"S3.T2.2.2.2.1.m1.1.1.1.cmml\" xref=\"S3.T2.2.2.2.1.m1.1.1.1\"></times><csymbol cd=\"latexml\" id=\"S3.T2.2.2.2.1.m1.1.1.2.cmml\" xref=\"S3.T2.2.2.2.1.m1.1.1.2\">absent</csymbol><apply id=\"S3.T2.2.2.2.1.m1.1.1.3.cmml\" xref=\"S3.T2.2.2.2.1.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S3.T2.2.2.2.1.m1.1.1.3.1.cmml\" xref=\"S3.T2.2.2.2.1.m1.1.1.3\">superscript</csymbol><cn type=\"integer\" id=\"S3.T2.2.2.2.1.m1.1.1.3.2.cmml\" xref=\"S3.T2.2.2.2.1.m1.1.1.3.2\">10</cn><cn type=\"integer\" id=\"S3.T2.2.2.2.1.m1.1.1.3.3.cmml\" xref=\"S3.T2.2.2.2.1.m1.1.1.3.3\">3</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T2.2.2.2.1.m1.1c\">\\times 10^{3}</annotation></semantics></math>)</th>\n<td id=\"S3.T2.2.2.2.2\" class=\"ltx_td ltx_align_center\">79.7</td>\n<td id=\"S3.T2.2.2.2.3\" class=\"ltx_td ltx_align_center\">177.2</td>\n<td id=\"S3.T2.2.2.2.4\" class=\"ltx_td ltx_align_center\">274.6</td>\n<td id=\"S3.T2.2.2.2.5\" class=\"ltx_td ltx_align_center\">515.6</td>\n</tr>\n<tr id=\"S3.T2.2.2.5.2\" class=\"ltx_tr\">\n<th id=\"S3.T2.2.2.5.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\">Accuracy</th>\n<td id=\"S3.T2.2.2.5.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.88</td>\n<td id=\"S3.T2.2.2.5.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.90</td>\n<td id=\"S3.T2.2.2.5.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.90</td>\n<td id=\"S3.T2.2.2.5.2.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.92</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Model complexity. We also investigate how the model complexity influences the training overheads if a target accuracy is met. Although it is common knowledge that smaller models have better time and computation performance in other paradigms of model training, we are the first to report the four system overhead versus model complexity in the FL setting.\nWe use ResNet [33] to build different models, as listed in Table II.\n",
            "Model Complexity.\nTable II tabulates the models for comparing training overheads versus model complexity. In this experiment, we select one participant (M=1𝑀1M=1) to train one pass (E=1𝐸1E=1) on each training round.\nFig. 5 shows the normalized CompT, TransT, CompL, and TransL for different models. The x-axis is the target model accuracy, and the y-axis is the corresponding overhead to reach that model accuracy. Since only one client and one training pass are used on each round, CompT and CompL have the same normalized comparison, and so are TransT and TransL. The results show that smaller models are better in terms of all training aspects. In addition, it is interesting to note that heavier models have higher increase rates of overheads versus model accuracy. This means that model selection is especially essential for high model accuracy applications."
        ]
    },
    "S3.T3": {
        "caption": "TABLE III: System overheads versus the number of participants M𝑀M, the number of training passes E𝐸E, and model complexity. ‘<<’, ‘==’, and ‘>>’ means the smaller the better, does not matter, and the larger the better, respectively.",
        "table": "",
        "footnotes": "\n\n\n\n\n\nTraining aspect\n\n\nM𝑀M\n\n\n\nE𝐸E\n\nModel complexity\n\n\n\nCompT\n\n\n>>\n\n\n\n<<\n\n<<\n\nCompL\n\n\n<<\n\n\n\n<<\n\n<<\n\nTransT\n\n\n>>\n\n\n\n>>\n\n<<\n\nTransL\n\n\n<<\n\n\n\n>>\n\n<<\n\nModel Accuracy\n\n\n==\n\n\n\n==\n\n>>\n\n\n",
        "references": [
            "Based on our measurement study, we summarize systems overheads versus FL hyper-parameters in Table III. As we can see, CompT, TransT, CompL, and TransL conflict with each other in selecting the optimal M𝑀M and E𝐸E.\nRegarding model complexity, smaller models have better system overheads if the model accuracy is satisfied. Please note that Table III is consistent with existing work (e.g., [12]), but is more comprehensive.\nThus, Table III is also valid for other datasets and ML models.",
            "We illustrate how to approximate Δ​MΔ𝑀\\Delta M. The process for Δ​EΔ𝐸\\Delta E is similar.\nConsidering that each step makes a small adjustment of M𝑀M, ∂tn​x​t/∂Msubscript𝑡𝑛𝑥𝑡𝑀\\partial t_{nxt}/\\partial M can be represented by (+1)×|tn​x​t−tc​u​r|1subscript𝑡𝑛𝑥𝑡subscript𝑡𝑐𝑢𝑟(+1)\\times|t_{nxt}-t_{cur}|, where (+1)1(+1) means CompT prefers larger M𝑀M according to Table III. To estimate |tn​x​t−tc​u​r|subscript𝑡𝑛𝑥𝑡subscript𝑡𝑐𝑢𝑟|t_{nxt}-t_{cur}|, we apply a linear function ηt−1×|tc​u​r−tp​r​v|subscript𝜂𝑡1subscript𝑡𝑐𝑢𝑟subscript𝑡𝑝𝑟𝑣\\eta_{t-1}\\times|t_{cur}-t_{prv}| where ηt−1=|tc​u​r−tp​r​v||tp​r​v−tp​r​v​p​r​v|subscript𝜂𝑡1subscript𝑡𝑐𝑢𝑟subscript𝑡𝑝𝑟𝑣subscript𝑡𝑝𝑟𝑣subscript𝑡𝑝𝑟𝑣𝑝𝑟𝑣\\eta_{t-1}=\\frac{|t_{cur}-t_{prv}|}{|t_{prv}-t_{prvprv}|} (tp​r​v​p​r​vsubscript𝑡𝑝𝑟𝑣𝑝𝑟𝑣t_{prvprv} is the CompT at two steps before). Thus, ηt−1subscript𝜂𝑡1\\eta_{t-1} represents the slope of the linear function. Similarly, we have ηq−1subscript𝜂𝑞1\\eta_{q-1}, ηz−1subscript𝜂𝑧1\\eta_{z-1}, ηv−1subscript𝜂𝑣1\\eta_{v-1} for TransT, CompL, and TransL when calculating their derivatives over M𝑀M.\nAs a result, Δ​MΔ𝑀\\Delta M can be approximated as"
        ]
    },
    "S4.T4": {
        "caption": "TABLE IV: Performance of FedTune for the speech-to-command dataset when FedAdagrad is used for aggregation. \n‘++’ is improvement and ‘−-’ is degradation. Standard deviation in parentheses. ",
        "table": "",
        "footnotes": "\n\n\n\n\n\nα𝛼\\alpha\nβ𝛽\\beta\nγ𝛾\\gamma\nδ𝛿\\delta\nCompT (1012superscript101210^{12})\nTransT (106superscript10610^{6})\nCompL (1012superscript101210^{12})\nTransL (106superscript10610^{6})\nFinal M\nFinal E\nOverall\n\n\n\n-\n-\n-\n-\n0.94 (0.01)\n11.61 (0.10)\n5.97 (0.04)\n232.24 (1.99)\n20\n20\n-\n\n1.0\n0.0\n0.0\n0.0\n0.42 (0.02)\n50.19 (2.57)\n4.57 (0.22)\n2418.71 (240.91)\n57.33 (4.50)\n1.00 (0.00)\n+55.23% (2.22%)\n\n0.0\n1.0\n0.0\n0.0\n1.34 (0.22)\n7.68 (1.12)\n14.99 (2.73)\n289.82 (46.98)\n48.00 (2.16)\n48.00 (2.16)\n+33.87% (9.67%)\n\n0.0\n0.0\n1.0\n0.0\n1.02 (0.10)\n615.98 (97.52)\n1.76 (0.16)\n672.21 (91.62)\n1.00 (0.00)\n1.00 (0.00)\n+70.51% (2.75%)\n\n0.0\n0.0\n0.0\n1.0\n2.18 (0.47)\n35.47 (7.51)\n3.30 (0.22)\n76.47 (1.68)\n1.00 (0.00)\n46.67 (3.30)\n+67.07% (0.72%)\n\n0.5\n0.5\n0.0\n0.0\n0.82 (0.13)\n9.17 (1.26)\n9.13 (1.66)\n347.11 (54.31)\n47.33 (2.05)\n21.33 (4.78)\n+16.97% (9.68%)\n\n0.5\n0.0\n0.5\n0.0\n0.48 (0.04)\n81.42 (9.83)\n3.23 (0.14)\n1875.99 (155.21)\n25.00 (1.63)\n1.00 (0.00)\n+47.57% (3.43%)\n\n0.5\n0.0\n0.0\n0.5\n0.79 (0.10)\n11.59 (0.55)\n5.04 (0.89)\n241.86 (68.65)\n22.33 (5.79)\n15.67 (4.50)\n+5.82% (11.28%)\n\n0.0\n0.5\n0.5\n0.0\n0.83 (0.03)\n10.66 (0.15)\n5.16 (0.31)\n207.79 (6.08)\n21.00 (1.41)\n21.00 (1.41)\n+10.87% (2.83%)\n\n0.0\n0.5\n0.0\n0.5\n1.54 (0.16)\n11.48 (3.83)\n9.59 (3.52)\n190.52 (61.53)\n19.67 (14.82)\n49.00 (0.00)\n+9.55% (7.08%)\n\n0.0\n0.0\n0.5\n0.5\n1.69 (0.26)\n50.14 (8.21)\n2.70 (0.26)\n93.21 (8.48)\n1.00 (0.00)\n23.33 (2.49)\n+57.32% (3.76%)\n\n0.33\n0.33\n0.33\n0.0\n0.82 (0.07)\n11.59 (1.01)\n5.65 (0.27)\n255.35 (9.65)\n22.33 (2.62)\n15.67 (1.25)\n+6.09% (6.67%)\n\n0.33\n0.33\n0.0\n0.33\n1.06 (0.08)\n10.07 (0.90)\n8.10 (0.34)\n247.54 (29.18)\n26.33 (2.05)\n27.00 (2.16)\n-1.93% (7.40%)\n\n0.33\n0.0\n0.33\n0.33\n0.91 (0.19)\n18.23 (5.83)\n4.15 (1.13)\n229.26 (63.40)\n12.00 (1.41)\n14.00 (5.72)\n+11.66% (11.76%)\n\n0.0\n0.33\n0.33\n0.33\n1.13 (0.13)\n16.16 (3.36)\n4.51 (0.59)\n169.93 (25.84)\n9.00 (5.35)\n23.00 (4.55)\n+3.99% (6.19%)\n\n0.25\n0.25\n0.25\n0.25\n0.91 (0.10)\n9.73 (1.81)\n6.19 (0.76)\n207.34 (3.34)\n23.33 (5.44)\n22.67 (3.30)\n+6.51% (6.13%)\n\n\n",
        "references": [
            "Benchmarks and Baseline.\nWe evaluate FedTune on three datasets: speech-to-command [32], EMNIST [34], and Cifar-100 [35], and three aggregation methods: FedAvg [8], FedNova [12], and FedAdagrad [36]. We set equal values for the combination of training preferences α𝛼\\alpha, β𝛽\\beta, γ𝛾\\gamma and δ𝛿\\delta (see the first column in Table IV). Therefore, for each dataset, we conduct 15 combinations of training preferences. We set target model accuracy for each dataset and measure CompT, TransT, CompL, and TransL for reaching the target model accuracy. We regard the practice of using fixed M𝑀M and E𝐸E as the baseline and compare FedTune to the baseline by calculating Eq. (6). In the evaluation, the positive performance means FedTune reduces the system overheads and the negative performance means the degradation.\nWe implemented FedTune in PyTorch. All the experiments are conducted in a server with 24-GB Nvidia RTX A5000 GPUs.",
            "We present the details of traces when the speech-to-command dataset and the FedAdagrad aggregation method are used.\nTable IV tabulates the results, where we show the application preference (α𝛼\\alpha, β𝛽\\beta, γ𝛾\\gamma, and δ𝛿\\delta), the system overheads (CompT, TransT, CompL, and TransL), the final M𝑀M and E𝐸E when the training is finished, and the overall performance.\nWe report the average performance, as well as their standard deviations in parentheses. The first row is the baseline, which does not change hyper-parameters during the FL training.\nAs we can see from Table IV, FedTune can adapt to different training preferences. Specifically, FedTune reduces the system overhead up to 70.51% when the application only cares about computation load (i.e., γ=1𝛾1\\gamma=1).\nOnly one preference (0.33, 0.33, 0, 0.33) results in a slightly degraded performance. On average, FedTune improves the overall performance by 26.75% for the speech-to-command dataset and the FedAdagrad aggregation method."
        ]
    },
    "S5.T5": {
        "caption": "TABLE V: Performance of FedTune for diverse datasets when FedAvg aggregation method is applied. ",
        "table": "",
        "footnotes": "\n\n\n\n\nDataset\nSpeech-command\nEMNIST\nCifar-100\n\nData Feature\nVoice\nHandwriting\nImage\n\nML Model\nResNet-10\n2-layer MLP\nResNet-10\n\nPerformance\n+22.48% (17.97%)\n+8.48% (5.51%)\n+9.33% (5.47%)\n\n",
        "references": [
            "Results for Diverse Datasets.\nTable V shows the overall performance of FedTune for different datasets when FedAvg is applied. We set the learning rate to 0.01 for the speech-to-command dataset and the EMNIST dataset, and 0.1 for the Cifar-100 dataset, all with the momentum of 0.9. We show the standard deviation in parenthesis. As shown, FedTune consistently improves the system performance across all the three datasets. In particular, FedTune reduces 22.48% system overhead of the speech-to-command dataset compared to the baseline by averaging the 15 combinations of training preferences.\nWe also observe that the FL training benefits more from FedTune if the training process needs more training rounds to converge. Our experiments with EMNIST (small model) and Cifar100 (low target accuracy) only require a few dozens of training rounds to reach their target model accuracy, and thus their performance gains from FedTune are not significant. The observation is consistent with the decision-making process in FedTune, which increases/decreases hyper-parameters by only one at each step. We leave it as future work to augment FedTune to change hyper-parameters with adaptive degrees."
        ]
    },
    "S5.T6": {
        "caption": "TABLE VI: Performance of FedTune for diverse aggregation algorithms. Speech-to-command dataset and ResNet-10 are used in this experiment.",
        "table": "",
        "footnotes": "\n\n\n\n\nAggregator\nFedAvg\nFedNova\nFedAdagrad\n\nPerformance\n+22.48% (17.97%)\n+23.53% (6.64%)\n+26.75% (6.10%)\n\n",
        "references": [
            "Results for Different Aggregation Methods. Table VI shows the overall performance of FedTune for different aggregation methods when we use the speech-to-command dataset and the ResNet-10 model. We set the learning rate to 0.1, β1subscript𝛽1\\beta_{1} to 0, and τ𝜏\\tau to 1e-3 in FedAdagrad. As shown, FedTune achieves consistent performance gain for diverse aggregation methods. In particular, FedTune reduces the system overhead of FedAdagrad by 26.75%."
        ]
    }
}