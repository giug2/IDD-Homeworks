{
    "id_table_1": {
        "caption": "Table 1:  Full Results for the QA datasets. There is overwhelming agreement between all metrics on the ranking between models trained on different synthetic fractions. EM: Exact Match, Inc: String Inclusion, R Inc: Reverse String Inclusion",
        "table": "A5.T1.1",
        "footnotes": [],
        "references": [
            "Results:  Across all datasets, using purely synthetic data typically leads to worse performance than the same amount of real data (Figure  1 ). This confirms that despite advances in synthetic generation, human annotation yields more useful data.",
            "Synthetic Replacement Experiments:  For figures in the main text where only one task is shown (Figure  1  and Figure  2 ), we provide the complete figures with both tasks (Figure  5  and Figure  6 ). We also provide the individual performance curves for these experiments (Figure  7  and Figure  8 ).",
            "To verify the robustness of the results, we show that the QA results are not an artifact of the choice of metric (Figure  1  and Figure  2 ), and that the same trends can be seen (Figure  9 , Figure  10  and Figure  11 ) when using a different fine-tuning model (Mistral-7B), a more powerful prompting model (GPT-4-Turbo) and a more sophisticated prompting strategy (Chain-of-Thought Prompting). Across all configurations, we see a consistent decrease in performance when moving from 95% to 100% synthetic data, confirming that models trained on purely synthetic data can be improved by including just  125  real data points. For Chain-of-Thought Prompting, the authors manually annotated 3 examples with rationales per dataset to serve as the prompts. The complete examples and pipeline will be provided with the code once the review period is concluded.",
            "We additionally show that these trends hold across data scales (Figure  12  and Figure  13 ), replicating the experiment with n=3000 and n=1000. While the trend is clearly visible in both cases, the results for n=1000 have more variance and hence have a minority of cases where the relationship does not hold.",
            "Tradeoff Experiment:  The main text shows results for the experiment detailed in Section  4  on the WANLI dataset (Figure  3 ), here we show results on the remaining three datasets (Figure  14 ) and provide (Figure  3 )the number of additional synthetic points needed to match the performance gains of 200 additional real points (average, median and standard deviation for each dataset). ROPES shows similar results to WANLI, however FairyTaleQA and FEVER present different trends. On FEVER, we are able to reach the saturation point, after which additional data (whether synthetic or real) does not increase performance. Even in this case, we are able to reach this point of diminishing marginal return more rapidly when using a small amount of synthetic data. On a base synthetic training set of size  3000 3000 3000 3000 , adding  200 200 200 200  real data points drives the test accuracy to  89.25 % percent 89.25 89.25\\% 89.25 % , a score that is only matched once we add at least 2000 synthetic data points (an order or magnitude larger). On FairyTaleQA, we get enormous estimates for the number of additional synthetic points needed (a mean of 2.8e5). We do not interpret these numbers literally, rather seeing this as a sign that human generated data may occasionally boost performance to an extent that could be fundamentally unachievable by purely synthetic data.",
            "In our implementation (Figure  15 ), we use few-shot learning with  k = 3 k 3 k=3 italic_k = 3 , i.e., three examples per query, with each example drawn randomly (with replacement) from the training set of the specific dataset.",
            "To compute the extent to which the evidence sentences contains the questions, answers, and claims, we measured the BLEU of the generation with  each individual sentence  of the evidence texts, plotting the maximum of these BLEU scores in Figure  17 . We find that synthetic generations have a far higher n-gram overlap with the evidence sentences than human generations. This suggests that synthetic data generation produces data points that are more extractive, while humans are more likely to abstract from the evidence. We also use the position of the evidence sentence that achieves the highest BLEU score as a proxy for the source location of the synthetic generation, and find that synthetic data generation chooses more diverse sources for the question and answer content, with human annotation overwhelmingly more likely to create questions whose answers lie in the start of the evidence texts (Figure  18 ). Finally, the main text shows the size length comparison for a single dataset, here we provide a larger sample (Figure  16 ). We explored the errors created by the models trained on 0% and 100% data, searching for trends or divergences between the input instances that achieve a low prediction accuracy or score. Our investigation found no major distinguishing factors between them, leaving a more fine-grained study of the effect of purely synthetic data on model decision-making to future work."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Results on  n = 5000 n 5000 n=5000 italic_n = 5000  from 95% to 100% for the QA datasets. There is overwhelming agreement between all metrics on the ranking between models trained on different synthetic fractions.",
        "table": "A5.T2.3",
        "footnotes": [],
        "references": [
            "The performance decline is not uniform as we increase the synthetic proportion. On all datasets, there is only a minor degradation up until 90% replacement, after which the performance drops considerably. We zoom in on the 90%-100% interval, fixing the amount of training data at  n = 5000 n 5000 n=5000 italic_n = 5000  (500 for SciFact) and training on datasets with 95%, 97.5% and 100% synthetic data (Figure  2 ). Surprisingly, the results show that across all datasets (and 3 random seeds), there is a significant difference between the performance of models on 97.5% and 100% synthetic data. These trends hold robustly over choice of fine-tuning model (Mistral7B), prompt model (GPT4), prompting strategy (Chain-of-Thought) and data scale (Appendix  A ). The same trend is seen in all cases; when using synthetic training sets, the addition of just  125  (2.5% of 5000) human generated datapoints reliably improved the performance of FV and QA models.",
            "Synthetic Replacement Experiments:  For figures in the main text where only one task is shown (Figure  1  and Figure  2 ), we provide the complete figures with both tasks (Figure  5  and Figure  6 ). We also provide the individual performance curves for these experiments (Figure  7  and Figure  8 ).",
            "To verify the robustness of the results, we show that the QA results are not an artifact of the choice of metric (Figure  1  and Figure  2 ), and that the same trends can be seen (Figure  9 , Figure  10  and Figure  11 ) when using a different fine-tuning model (Mistral-7B), a more powerful prompting model (GPT-4-Turbo) and a more sophisticated prompting strategy (Chain-of-Thought Prompting). Across all configurations, we see a consistent decrease in performance when moving from 95% to 100% synthetic data, confirming that models trained on purely synthetic data can be improved by including just  125  real data points. For Chain-of-Thought Prompting, the authors manually annotated 3 examples with rationales per dataset to serve as the prompts. The complete examples and pipeline will be provided with the code once the review period is concluded.",
            "We additionally show that these trends hold across data scales (Figure  12  and Figure  13 ), replicating the experiment with n=3000 and n=1000. While the trend is clearly visible in both cases, the results for n=1000 have more variance and hence have a minority of cases where the relationship does not hold."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Summary statistics for the number of additional synthetic data points needed to match performance gain of 200 human data points (aggregated over various fixed operating points of synthetic data). High values for FairyTaleQA suggests that human generated data may unlock performance that is unachievable with purely synthetic data. Negative values for FEVER are due to a saturation of the performance gains, however human data helps reach the saturation point much faster (see Section  A  and Figure  14 ).",
        "table": "A5.T3.1",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "Results:  Across all datasets, adding  200  human data points is comparable to adding at least an order of magnitude (often multiple orders of magnitude) more synthetic data points. On WANLI (Figure  3 ), more than  17000 17000 17000 17000  additional synthetic points are needed to achieve the performance gains of  200 200 200 200  human points. If the price of a synthetic point for WANLI exceeds  73 73 73 73  times the price of a human generated point, then an incremental amount of human annotation would be the more cost-effective solution. In the extreme case, the equation learned on FairyTaleQA suggests that it takes  2  e  5 2 e 5 2e5 2 italic_e 5  additional synthetic points to match the performance gain of 200 additional real data points. Rather than interpret these numbers literally, we take them to suggest that human data could have unique value in some settings, enabling performance levels that are impossible with purely synthetic datasets. See below (Appendix  A ) for more results and details.",
            "We additionally show that these trends hold across data scales (Figure  12  and Figure  13 ), replicating the experiment with n=3000 and n=1000. While the trend is clearly visible in both cases, the results for n=1000 have more variance and hence have a minority of cases where the relationship does not hold.",
            "Tradeoff Experiment:  The main text shows results for the experiment detailed in Section  4  on the WANLI dataset (Figure  3 ), here we show results on the remaining three datasets (Figure  14 ) and provide (Figure  3 )the number of additional synthetic points needed to match the performance gains of 200 additional real points (average, median and standard deviation for each dataset). ROPES shows similar results to WANLI, however FairyTaleQA and FEVER present different trends. On FEVER, we are able to reach the saturation point, after which additional data (whether synthetic or real) does not increase performance. Even in this case, we are able to reach this point of diminishing marginal return more rapidly when using a small amount of synthetic data. On a base synthetic training set of size  3000 3000 3000 3000 , adding  200 200 200 200  real data points drives the test accuracy to  89.25 % percent 89.25 89.25\\% 89.25 % , a score that is only matched once we add at least 2000 synthetic data points (an order or magnitude larger). On FairyTaleQA, we get enormous estimates for the number of additional synthetic points needed (a mean of 2.8e5). We do not interpret these numbers literally, rather seeing this as a sign that human generated data may occasionally boost performance to an extent that could be fundamentally unachievable by purely synthetic data."
        ]
    }
}