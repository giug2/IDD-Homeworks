{
    "S5.T1": {
        "caption": "Table 1: Accuracy of Question Relevance models on the QRPE test set. We find that premise-aware models consistently outperform alternative models.",
        "table": "<table id=\"S5.T1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">Models</td>\n<td id=\"S5.T1.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">Overall</td>\n<td id=\"S5.T1.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">First Order</td>\n<td id=\"S5.T1.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">Second Order</td>\n</tr>\n<tr id=\"S5.T1.1.1.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">VQA-Bin</td>\n<td id=\"S5.T1.1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">66.50</td>\n<td id=\"S5.T1.1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">67.36</td>\n<td id=\"S5.T1.1.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">53.00</td>\n</tr>\n<tr id=\"S5.T1.1.1.3.3\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.1.3.3.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">VQA-Bin-Prem</td>\n<td id=\"S5.T1.1.1.3.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">66.77</td>\n<td id=\"S5.T1.1.1.3.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">67.04</td>\n<td id=\"S5.T1.1.1.3.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">54.38</td>\n</tr>\n<tr id=\"S5.T1.1.1.4.4\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">HieCoAtt-Bin</td>\n<td id=\"S5.T1.1.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">70.74</td>\n<td id=\"S5.T1.1.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">71.35</td>\n<td id=\"S5.T1.1.1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.5pt;padding-right:8.5pt;\"><span id=\"S5.T1.1.1.4.4.4.1\" class=\"ltx_text ltx_font_bold\">61.54</span></td>\n</tr>\n<tr id=\"S5.T1.1.1.5.5\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.1.5.5.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">HieCoAtt-Bin-Prem</td>\n<td id=\"S5.T1.1.1.5.5.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">73.34</td>\n<td id=\"S5.T1.1.1.5.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">73.97</td>\n<td id=\"S5.T1.1.1.5.5.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">60.35</td>\n</tr>\n<tr id=\"S5.T1.1.1.6.6\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.1.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">QC-Sim</td>\n<td id=\"S5.T1.1.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">74.35</td>\n<td id=\"S5.T1.1.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">75.82</td>\n<td id=\"S5.T1.1.1.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">55.12</td>\n</tr>\n<tr id=\"S5.T1.1.1.7.7\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.1.7.7.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">PC-Sim</td>\n<td id=\"S5.T1.1.1.7.7.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">75.05</td>\n<td id=\"S5.T1.1.1.7.7.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">76.47</td>\n<td id=\"S5.T1.1.1.7.7.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">56.04</td>\n</tr>\n<tr id=\"S5.T1.1.1.8.8\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.1.8.8.1\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">QPC-Sim</td>\n<td id=\"S5.T1.1.1.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.5pt;padding-right:8.5pt;\"><span id=\"S5.T1.1.1.8.8.2.1\" class=\"ltx_text ltx_font_bold\">75.31</span></td>\n<td id=\"S5.T1.1.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.5pt;padding-right:8.5pt;\"><span id=\"S5.T1.1.1.8.8.3.1\" class=\"ltx_text ltx_font_bold\">76.67</span></td>\n<td id=\"S5.T1.1.1.8.8.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.5pt;padding-right:8.5pt;\">55.95</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "We train each model on the QRPE train split and report results on the test set in Table 1. As the dataset is balanced in the label space, random accuracy stands at 50%. We find that the simple VQA-Bin model achieves 66.5% accuracy while the attention based model HieCoAtt-Bin attains 70.74% accuracy. Surprisingly, the caption-similarity based QC-Sim model significantly outperforms these baseline, obtaining an accuracy of 74.35% while only reasoning about relevancy from textual descriptions of images. We note that the caption similarity based approaches use a large amount of outside data during pretraining of the captioning model and the word2vec embeddings, which may have contributed to the effectiveness of these methods."
        ]
    },
    "S6.1": {
        "caption": "Table 2:  Answer type distribution of source and premise questions on the Compositional VQA train set.",
        "table": "<table id=\"S6.1.1.1.p1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S6.1.1.1.p1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S6.1.1.1.p1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Training Data</th>\n<th id=\"S6.1.1.1.p1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Other</th>\n<th id=\"S6.1.1.1.p1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Number</th>\n<th id=\"S6.1.1.1.p1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Yes</th>\n<th id=\"S6.1.1.1.p1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">No</th>\n<th id=\"S6.1.1.1.p1.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Total</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.1.1.1.p1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S6.1.1.1.p1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Source</td>\n<td id=\"S6.1.1.1.p1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">123,817</td>\n<td id=\"S6.1.1.1.p1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">29,698</td>\n<td id=\"S6.1.1.1.p1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">57217</td>\n<td id=\"S6.1.1.1.p1.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">35842</td>\n<td id=\"S6.1.1.1.p1.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">246,574</td>\n</tr>\n<tr id=\"S6.1.1.1.p1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S6.1.1.1.p1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Premise</td>\n<td id=\"S6.1.1.1.p1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">137,483</td>\n<td id=\"S6.1.1.1.p1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">1,850</td>\n<td id=\"S6.1.1.1.p1.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">387,941</td>\n<td id=\"S6.1.1.1.p1.1.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0</td>\n<td id=\"S6.1.1.1.p1.1.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">527,274</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": []
    },
    "S6.T2": {
        "caption": "Table 2:  Answer type distribution of source and premise questions on the Compositional VQA train set.",
        "table": "",
        "footnotes": "",
        "references": []
    },
    "S6.T3": {
        "caption": "Table 3: Accuracy on the standard and compositional VQA validation sets for different augmentation strategies for DeeperLSTMAntol et al. (2015).",
        "table": "<table id=\"S6.T3.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S6.T3.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S6.T3.1.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" style=\"padding:1.25pt 3.0pt;\"></th>\n<th id=\"S6.T3.1.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding:1.25pt 3.0pt;\">Augmentation</th>\n<th id=\"S6.T3.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.25pt 3.0pt;\">Overall</th>\n<th id=\"S6.T3.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.25pt 3.0pt;\">Other</th>\n<th id=\"S6.T3.1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.25pt 3.0pt;\">Number</th>\n<th id=\"S6.T3.1.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.25pt 3.0pt;\">Yes/No</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T3.1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S6.T3.1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.25pt 3.0pt;\" rowspan=\"3\"><span id=\"S6.T3.1.1.2.1.1.1\" class=\"ltx_text\">\n<span id=\"S6.T3.1.1.2.1.1.1.1\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:6.9pt;height:40pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:40.0pt;transform:translate(-16.54pt,-16.54pt) rotate(-90deg) ;\">\n<span id=\"S6.T3.1.1.2.1.1.1.1.1\" class=\"ltx_p\">Standard</span>\n</span></span></span></th>\n<th id=\"S6.T3.1.1.2.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.25pt 3.0pt;\">None</th>\n<td id=\"S6.T3.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.25pt 3.0pt;\">54.23</td>\n<td id=\"S6.T3.1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.25pt 3.0pt;\">40.34</td>\n<td id=\"S6.T3.1.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.25pt 3.0pt;\">33.27</td>\n<td id=\"S6.T3.1.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.25pt 3.0pt;\">79.82</td>\n</tr>\n<tr id=\"S6.T3.1.1.3.2\" class=\"ltx_tr\">\n<th id=\"S6.T3.1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.25pt 3.0pt;\">All</th>\n<td id=\"S6.T3.1.1.3.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding:1.25pt 3.0pt;\">53.74</td>\n<td id=\"S6.T3.1.1.3.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding:1.25pt 3.0pt;\">39.28</td>\n<td id=\"S6.T3.1.1.3.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding:1.25pt 3.0pt;\"><span id=\"S6.T3.1.1.3.2.4.1\" class=\"ltx_text ltx_font_bold\">33.38</span></td>\n<td id=\"S6.T3.1.1.3.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding:1.25pt 3.0pt;\">79.89</td>\n</tr>\n<tr id=\"S6.T3.1.1.4.3\" class=\"ltx_tr\">\n<th id=\"S6.T3.1.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.25pt 3.0pt;\">Top-1k-A</th>\n<td id=\"S6.T3.1.1.4.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding:1.25pt 3.0pt;\"><span id=\"S6.T3.1.1.4.3.2.1\" class=\"ltx_text ltx_font_bold\">54.47</span></td>\n<td id=\"S6.T3.1.1.4.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding:1.25pt 3.0pt;\"><span id=\"S6.T3.1.1.4.3.3.1\" class=\"ltx_text ltx_font_bold\">40.56</span></td>\n<td id=\"S6.T3.1.1.4.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding:1.25pt 3.0pt;\">33.24</td>\n<td id=\"S6.T3.1.1.4.3.5\" class=\"ltx_td ltx_align_center\" style=\"padding:1.25pt 3.0pt;\"><span id=\"S6.T3.1.1.4.3.5.1\" class=\"ltx_text ltx_font_bold\">80.19</span></td>\n</tr>\n<tr id=\"S6.T3.1.1.5.4\" class=\"ltx_tr\">\n<th id=\"S6.T3.1.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" style=\"padding:1.25pt 3.0pt;\" rowspan=\"3\"><span id=\"S6.T3.1.1.5.4.1.1\" class=\"ltx_text\">\n<span id=\"S6.T3.1.1.5.4.1.1.1\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.8pt;height:28.9pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:28.9pt;transform:translate(-10.06pt,-9.08pt) rotate(-90deg) ;\">\n<span id=\"S6.T3.1.1.5.4.1.1.1.1\" class=\"ltx_p\">Comp.</span>\n</span></span></span></th>\n<th id=\"S6.T3.1.1.5.4.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.25pt 3.0pt;\">None</th>\n<td id=\"S6.T3.1.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.25pt 3.0pt;\">46.69</td>\n<td id=\"S6.T3.1.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.25pt 3.0pt;\">31.92</td>\n<td id=\"S6.T3.1.1.5.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.25pt 3.0pt;\">29.73</td>\n<td id=\"S6.T3.1.1.5.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.25pt 3.0pt;\">70.49</td>\n</tr>\n<tr id=\"S6.T3.1.1.6.5\" class=\"ltx_tr\">\n<th id=\"S6.T3.1.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.25pt 3.0pt;\">All</th>\n<td id=\"S6.T3.1.1.6.5.2\" class=\"ltx_td ltx_align_center\" style=\"padding:1.25pt 3.0pt;\">47.63</td>\n<td id=\"S6.T3.1.1.6.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding:1.25pt 3.0pt;\">31.97</td>\n<td id=\"S6.T3.1.1.6.5.4\" class=\"ltx_td ltx_align_center\" style=\"padding:1.25pt 3.0pt;\"><span id=\"S6.T3.1.1.6.5.4.1\" class=\"ltx_text ltx_font_bold\">30.77</span></td>\n<td id=\"S6.T3.1.1.6.5.5\" class=\"ltx_td ltx_align_center\" style=\"padding:1.25pt 3.0pt;\"><span id=\"S6.T3.1.1.6.5.5.1\" class=\"ltx_text ltx_font_bold\">72.52</span></td>\n</tr>\n<tr id=\"S6.T3.1.1.7.6\" class=\"ltx_tr\">\n<th id=\"S6.T3.1.1.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding:1.25pt 3.0pt;\">Top-1k-A</th>\n<td id=\"S6.T3.1.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.25pt 3.0pt;\"><span id=\"S6.T3.1.1.7.6.2.1\" class=\"ltx_text ltx_font_bold\">47.85</span></td>\n<td id=\"S6.T3.1.1.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.25pt 3.0pt;\"><span id=\"S6.T3.1.1.7.6.3.1\" class=\"ltx_text ltx_font_bold\">32.58</span></td>\n<td id=\"S6.T3.1.1.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.25pt 3.0pt;\">30.59</td>\n<td id=\"S6.T3.1.1.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.25pt 3.0pt;\">72.38</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": []
    },
    "S6.T4": {
        "caption": "Table 4: Overall accuracy of different VQA models on the Compositional VQA test split using Top-1k-A augmentation.",
        "table": "<table id=\"S6.T4.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T4.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S6.T4.1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding:1.25pt 2.0pt;\">VQA Model</th>\n<th id=\"S6.T4.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.25pt 2.0pt;\">Baseline</th>\n<th id=\"S6.T4.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding:1.25pt 2.0pt;\">+Premises</th>\n<td id=\"S6.T4.1.1.1.1.4\" class=\"ltx_td ltx_border_tt\" style=\"padding:1.25pt 2.0pt;\"></td>\n</tr>\n<tr id=\"S6.T4.1.1.2.2\" class=\"ltx_tr\">\n<th id=\"S6.T4.1.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:1.25pt 2.0pt;\">DeeperLSTM<cite class=\"ltx_cite ltx_citemacro_cite\">Lu et al. (<a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">2015</a>)</cite>\n</th>\n<td id=\"S6.T4.1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.25pt 2.0pt;\">46.69</td>\n<td id=\"S6.T4.1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1.25pt 2.0pt;\"><span id=\"S6.T4.1.1.2.2.3.1\" class=\"ltx_text ltx_font_bold\">47.85</span></td>\n<td id=\"S6.T4.1.1.2.2.4\" class=\"ltx_td ltx_border_t\" style=\"padding:1.25pt 2.0pt;\"></td>\n</tr>\n<tr id=\"S6.T4.1.1.3.3\" class=\"ltx_tr\">\n<th id=\"S6.T4.1.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.25pt 2.0pt;\">HieCoAtt<cite class=\"ltx_cite ltx_citemacro_cite\">Lu et al. (<a href=\"#bib.bib15\" title=\"\" class=\"ltx_ref\">2016</a>)</cite>\n</th>\n<td id=\"S6.T4.1.1.3.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding:1.25pt 2.0pt;\"><span id=\"S6.T4.1.1.3.3.2.1\" class=\"ltx_text ltx_font_bold\">50.17</span></td>\n<td id=\"S6.T4.1.1.3.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding:1.25pt 2.0pt;\">49.98</td>\n<td id=\"S6.T4.1.1.3.3.4\" class=\"ltx_td\" style=\"padding:1.25pt 2.0pt;\"></td>\n</tr>\n<tr id=\"S6.T4.1.1.4.4\" class=\"ltx_tr\">\n<th id=\"S6.T4.1.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:1.25pt 2.0pt;\">NMN<cite class=\"ltx_cite ltx_citemacro_cite\">Andreas et al. (<a href=\"#bib.bib3\" title=\"\" class=\"ltx_ref\">2016</a>)</cite>\n</th>\n<td id=\"S6.T4.1.1.4.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding:1.25pt 2.0pt;\"><span id=\"S6.T4.1.1.4.4.2.1\" class=\"ltx_text ltx_font_bold\">49.05</span></td>\n<td id=\"S6.T4.1.1.4.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding:1.25pt 2.0pt;\">48.43</td>\n<td id=\"S6.T4.1.1.4.4.4\" class=\"ltx_td\" style=\"padding:1.25pt 2.0pt;\"></td>\n</tr>\n<tr id=\"S6.T4.1.1.5.5\" class=\"ltx_tr\">\n<th id=\"S6.T4.1.1.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding:1.25pt 2.0pt;\">MCB<cite class=\"ltx_cite ltx_citemacro_cite\">Fukui et al. (<a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">2016</a>)</cite>\n</th>\n<td id=\"S6.T4.1.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.25pt 2.0pt;\">50.13</td>\n<td id=\"S6.T4.1.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:1.25pt 2.0pt;\"><span id=\"S6.T4.1.1.5.5.3.1\" class=\"ltx_text ltx_font_bold\">50.57</span></td>\n<td id=\"S6.T4.1.1.5.5.4\" class=\"ltx_td ltx_border_bb\" style=\"padding:1.25pt 2.0pt;\"></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Fig. 7 shows sample premise questions produced from source VQA questions using our pipeline. We note that the distribution of premise questions varies drastically from the source VQA distribution (see Table 5).",
            "A random selection of premise questions generated from the VQA dataset can be seen in Fig. 7. The answer type distribution of generated premise questions can be seen in Table 5. We find that generated premise questions are twice in number as compared to source questions. We generate relatively few ‘Number’ questions – very few second-order tuples of this type occur in the premises we extract, as questions about multiple number of objects at a time are rare in the VQA dataset. By design, we generate only ‘Yes’ questions and zero ‘No’ questions. The reason for that is twofold – first, we only generate premise questions from true premises, and second, first order premises are the most frequent premises in source questions (first order premises generate ‘Yes’ questions)."
        ]
    },
    "A2.1": {
        "caption": "Table 5:  Answer type distribution of source and premise questions on the Compositional VQA train set.",
        "table": "<table id=\"A2.1.1.1.p1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A2.1.1.1.p1.1.1.1\" class=\"ltx_tr\">\n<th id=\"A2.1.1.1.p1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Training Data</th>\n<th id=\"A2.1.1.1.p1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Other</th>\n<th id=\"A2.1.1.1.p1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Number</th>\n<th id=\"A2.1.1.1.p1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Yes</th>\n<th id=\"A2.1.1.1.p1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">No</th>\n<th id=\"A2.1.1.1.p1.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Total</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A2.1.1.1.p1.1.2.1\" class=\"ltx_tr\">\n<td id=\"A2.1.1.1.p1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Source</td>\n<td id=\"A2.1.1.1.p1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">123,817</td>\n<td id=\"A2.1.1.1.p1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">29,698</td>\n<td id=\"A2.1.1.1.p1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">57217</td>\n<td id=\"A2.1.1.1.p1.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">35842</td>\n<td id=\"A2.1.1.1.p1.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">246,574</td>\n</tr>\n<tr id=\"A2.1.1.1.p1.1.3.2\" class=\"ltx_tr\">\n<td id=\"A2.1.1.1.p1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Premise</td>\n<td id=\"A2.1.1.1.p1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">137,483</td>\n<td id=\"A2.1.1.1.p1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">1,850</td>\n<td id=\"A2.1.1.1.p1.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">387,941</td>\n<td id=\"A2.1.1.1.p1.1.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0</td>\n<td id=\"A2.1.1.1.p1.1.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">527,274</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "We evaluate the Deeper LSTM model of Lu et al. (2015)\non the standard and compositional splits with two augmentation strategies - All which includes the entire set of premise questions and Top-1k-A which includes only questions with answers in the top 1000 most common VQA answers. The results are listed in Table 6. We find minor improvement of 0.34% on the standard split under Top-1k-A premise question augmentation. On the compositional split, we observe a 1.16% gain with Top-1k-A augmentation over no augmentation. In this setting, explicitly reasoning about objects and attributes seen in the questions seems to help the model disentangle objects from their common characteristics.",
            "Table 6 shows the VQA accuracy of the DeeperLSTM model for these different dataset augmentation strategies. While all settings show some improvements over the standard training set, we find the largest increase with the Top1k-A setting. By restricting the additional question to those having answers in the top-1000 most commonly occurring answers from the standard VQA set, the added data does not significantly shift the types of answers the model learns are likely. Some examples where the augmented DeeperLSTM model performs better than a non-augmented model are shown in Fig. 8."
        ]
    },
    "A2.T5": {
        "caption": "Table 5:  Answer type distribution of source and premise questions on the Compositional VQA train set.",
        "table": "",
        "footnotes": "",
        "references": [
            "Fig. 7 shows sample premise questions produced from source VQA questions using our pipeline. We note that the distribution of premise questions varies drastically from the source VQA distribution (see Table 5).",
            "A random selection of premise questions generated from the VQA dataset can be seen in Fig. 7. The answer type distribution of generated premise questions can be seen in Table 5. We find that generated premise questions are twice in number as compared to source questions. We generate relatively few ‘Number’ questions – very few second-order tuples of this type occur in the premises we extract, as questions about multiple number of objects at a time are rare in the VQA dataset. By design, we generate only ‘Yes’ questions and zero ‘No’ questions. The reason for that is twofold – first, we only generate premise questions from true premises, and second, first order premises are the most frequent premises in source questions (first order premises generate ‘Yes’ questions).",
            "Keeping the Top1k-A data augmentation setting, we extend our experiments to additional VQA models. Table 7 shows the results of these experiments. We find that while this data augmentation technique results in improvements for some models, it fails to consistently deliver significantly better performance overall."
        ]
    },
    "A2.T6": {
        "caption": "Table 6: Performance of DeeperLSTM Antol et al. (2015) on Compositional VQA test split with different augmentations.",
        "table": "<table id=\"A2.T6.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A2.T6.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"A2.T6.1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Data Ablation</th>\n<th id=\"A2.T6.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Overall</th>\n<th id=\"A2.T6.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Other</th>\n<th id=\"A2.T6.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Number</th>\n<th id=\"A2.T6.1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Yes/No</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A2.T6.1.1.2.1\" class=\"ltx_tr\">\n<th id=\"A2.T6.1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Baseline</th>\n<td id=\"A2.T6.1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">46.69</td>\n<td id=\"A2.T6.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">31.92</td>\n<td id=\"A2.T6.1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">29.73</td>\n<td id=\"A2.T6.1.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">70.49</td>\n</tr>\n<tr id=\"A2.T6.1.1.3.2\" class=\"ltx_tr\">\n<th id=\"A2.T6.1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">All</th>\n<td id=\"A2.T6.1.1.3.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">47.63</td>\n<td id=\"A2.T6.1.1.3.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">31.97</td>\n<td id=\"A2.T6.1.1.3.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span id=\"A2.T6.1.1.3.2.4.1\" class=\"ltx_text ltx_font_bold\">30.77</span></td>\n<td id=\"A2.T6.1.1.3.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span id=\"A2.T6.1.1.3.2.5.1\" class=\"ltx_text ltx_font_bold\">72.52</span></td>\n</tr>\n<tr id=\"A2.T6.1.1.4.3\" class=\"ltx_tr\">\n<th id=\"A2.T6.1.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Only-Binary</th>\n<td id=\"A2.T6.1.1.4.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">47.25</td>\n<td id=\"A2.T6.1.1.4.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">32.45</td>\n<td id=\"A2.T6.1.1.4.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">29.65</td>\n<td id=\"A2.T6.1.1.4.3.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">71.30</td>\n</tr>\n<tr id=\"A2.T6.1.1.5.4\" class=\"ltx_tr\">\n<th id=\"A2.T6.1.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">No-Other</th>\n<td id=\"A2.T6.1.1.5.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">47.33</td>\n<td id=\"A2.T6.1.1.5.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">32.47</td>\n<td id=\"A2.T6.1.1.5.4.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">29.85</td>\n<td id=\"A2.T6.1.1.5.4.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">71.42</td>\n</tr>\n<tr id=\"A2.T6.1.1.6.5\" class=\"ltx_tr\">\n<th id=\"A2.T6.1.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">No-Binary</th>\n<td id=\"A2.T6.1.1.6.5.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">46.76</td>\n<td id=\"A2.T6.1.1.6.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">31.69</td>\n<td id=\"A2.T6.1.1.6.5.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">29.39</td>\n<td id=\"A2.T6.1.1.6.5.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">71.09</td>\n</tr>\n<tr id=\"A2.T6.1.1.7.6\" class=\"ltx_tr\">\n<th id=\"A2.T6.1.1.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Comm-Other</th>\n<td id=\"A2.T6.1.1.7.6.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">47.53</td>\n<td id=\"A2.T6.1.1.7.6.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">32.41</td>\n<td id=\"A2.T6.1.1.7.6.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">28.88</td>\n<td id=\"A2.T6.1.1.7.6.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">72.33</td>\n</tr>\n<tr id=\"A2.T6.1.1.8.7\" class=\"ltx_tr\">\n<th id=\"A2.T6.1.1.8.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Top1k-A</th>\n<td id=\"A2.T6.1.1.8.7.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span id=\"A2.T6.1.1.8.7.2.1\" class=\"ltx_text ltx_font_bold\">47.85</span></td>\n<td id=\"A2.T6.1.1.8.7.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span id=\"A2.T6.1.1.8.7.3.1\" class=\"ltx_text ltx_font_bold\">32.58</span></td>\n<td id=\"A2.T6.1.1.8.7.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">30.59</td>\n<td id=\"A2.T6.1.1.8.7.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">72.38</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "We evaluate the Deeper LSTM model of Lu et al. (2015)\non the standard and compositional splits with two augmentation strategies - All which includes the entire set of premise questions and Top-1k-A which includes only questions with answers in the top 1000 most common VQA answers. The results are listed in Table 6. We find minor improvement of 0.34% on the standard split under Top-1k-A premise question augmentation. On the compositional split, we observe a 1.16% gain with Top-1k-A augmentation over no augmentation. In this setting, explicitly reasoning about objects and attributes seen in the questions seems to help the model disentangle objects from their common characteristics.",
            "Table 6 shows the VQA accuracy of the DeeperLSTM model for these different dataset augmentation strategies. While all settings show some improvements over the standard training set, we find the largest increase with the Top1k-A setting. By restricting the additional question to those having answers in the top-1000 most commonly occurring answers from the standard VQA set, the added data does not significantly shift the types of answers the model learns are likely. Some examples where the augmented DeeperLSTM model performs better than a non-augmented model are shown in Fig. 8."
        ]
    },
    "A2.T7": {
        "caption": "Table 7: Accuracy of different VQA models on the Compositional VQA test split using Top1k-A augmentation.",
        "table": "<table id=\"A2.T7.1.fig1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"A2.T7.1.fig1.1.1.1\" class=\"ltx_tr\">\n<th id=\"A2.T7.1.fig1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">VQA Model</th>\n<th id=\"A2.T7.1.fig1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Baseline</th>\n<th id=\"A2.T7.1.fig1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">With Premises</th>\n<td id=\"A2.T7.1.fig1.1.1.1.4\" class=\"ltx_td ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n</tr>\n<tr id=\"A2.T7.1.fig1.1.2.2\" class=\"ltx_tr\">\n<th id=\"A2.T7.1.fig1.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">DeeperLSTM<cite class=\"ltx_cite ltx_citemacro_cite\">Antol et al. (<a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\">2015</a>)</cite>\n</th>\n<td id=\"A2.T7.1.fig1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">46.69</td>\n<td id=\"A2.T7.1.fig1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">47.85</td>\n<td id=\"A2.T7.1.fig1.1.2.2.4\" class=\"ltx_td ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n</tr>\n<tr id=\"A2.T7.1.fig1.1.3.3\" class=\"ltx_tr\">\n<th id=\"A2.T7.1.fig1.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">HieCoAtt<cite class=\"ltx_cite ltx_citemacro_cite\">Lu et al. (<a href=\"#bib.bib15\" title=\"\" class=\"ltx_ref\">2016</a>)</cite>\n</th>\n<td id=\"A2.T7.1.fig1.1.3.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">50.17</td>\n<td id=\"A2.T7.1.fig1.1.3.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">49.98</td>\n<td id=\"A2.T7.1.fig1.1.3.3.4\" class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n</tr>\n<tr id=\"A2.T7.1.fig1.1.4.4\" class=\"ltx_tr\">\n<th id=\"A2.T7.1.fig1.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">NMN<cite class=\"ltx_cite ltx_citemacro_cite\">Andreas et al. (<a href=\"#bib.bib3\" title=\"\" class=\"ltx_ref\">2016</a>)</cite>\n</th>\n<td id=\"A2.T7.1.fig1.1.4.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">49.05</td>\n<td id=\"A2.T7.1.fig1.1.4.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">48.43</td>\n<td id=\"A2.T7.1.fig1.1.4.4.4\" class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n</tr>\n<tr id=\"A2.T7.1.fig1.1.5.5\" class=\"ltx_tr\">\n<th id=\"A2.T7.1.fig1.1.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">MCB<cite class=\"ltx_cite ltx_citemacro_cite\">Fukui et al. (<a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">2016</a>)</cite>\n</th>\n<td id=\"A2.T7.1.fig1.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">50.13</td>\n<td id=\"A2.T7.1.fig1.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">50.57</td>\n<td id=\"A2.T7.1.fig1.1.5.5.4\" class=\"ltx_td ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Keeping the Top1k-A data augmentation setting, we extend our experiments to additional VQA models. Table 7 shows the results of these experiments. We find that while this data augmentation technique results in improvements for some models, it fails to consistently deliver significantly better performance overall."
        ]
    }
}