{
    "PAPER'S NUMBER OF TABLES": 7,
    "S3.T1": {
        "caption": "Table 1: Non-IID learning scenarios in Federated Learning, and the strategies that could potentially solve each situation. Strategies that deal with changes in both the input space and the behaviour are placed only in the last column, and not in the previous ones.",
        "table": "",
        "footnotes": "",
        "references": [
            "On the whole, this gives us a total of four different situations to account for. We represent them in TableÂ 1, along with the different works that deal with each situation. There are lots of techniques that could fit the cell of IID data, such as FedAvgÂ [11]. However, that situation is out of the scope of our work, and we will focus on the non-IID scenarios. Most of the works that consider heterogeneous data problems do not provide a classification of non-IID data, neither worry about the kind of heterogeneity they are trying to deal with. However, we locate them in TableÂ 1 according to the kind of non-IID data situation that they can solve. For this reason, we establish two possible classifications of the FL non-IID research.",
            "Once we have explained the existing personalization methods in FL, we now want to deepen into the other classification, based strictly on the kind of non-IID data the different works face. Going back to TableÂ 1, we classify these methods into 222 categories: those who work with changes in the input space throughout clients, i.e., changes in Pâ€‹(x)ğ‘ƒğ‘¥\\mathit{P}(x); and those who work with changes in the behaviour throughout clients, i.e., changes in Pâ€‹(y|x)ğ‘ƒconditionalğ‘¦ğ‘¥\\mathit{P}(y|x).",
            "In real-life problems, data distributions can vary in a bunch of different ways. Clients in a federated setting are expected to collect their own data samples, under particular conditions, leading to statistically unequal datasets. These differences can rely on the inputs each client perceive, Piâ€‹(x)â‰ Pjâ€‹(x)subscriptğ‘ƒğ‘–ğ‘¥subscriptğ‘ƒğ‘—ğ‘¥\\mathit{P}_{i}(x)\\neq\\mathit{P}_{j}(x), as well as on the label associated with their inputs, Piâ€‹(y|x)â‰ Pjâ€‹(y|x)subscriptğ‘ƒğ‘–conditionalğ‘¦ğ‘¥subscriptğ‘ƒğ‘—conditionalğ‘¦ğ‘¥\\mathit{P}_{i}(y|x)\\neq\\mathit{P}_{j}(y|x) (see TableÂ 1). If we desire the model to be adapted to the particularities of the training participants, standard FL techniques will not be enough. Moreover, the process of collecting data and solving a task takes a certain amount of time, so the desired model should be able to evolve and adjust to future situations. Data will be collected during a long period of time, leading to changes in the input space, Ptâ€‹(x)â‰ Pt+kâ€‹(x)superscriptğ‘ƒğ‘¡ğ‘¥superscriptğ‘ƒğ‘¡ğ‘˜ğ‘¥\\mathit{P}^{t}(x)\\neq\\mathit{P}^{t+k}(x) and also in the labels, Ptâ€‹(y|x)â‰ Pt+kâ€‹(y|x)superscriptğ‘ƒğ‘¡conditionalğ‘¦ğ‘¥superscriptğ‘ƒğ‘¡ğ‘˜conditionalğ‘¦ğ‘¥\\mathit{P}^{t}(y|x)\\neq\\mathit{P}^{t+k}(y|x) (see Fig.Â 4)."
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Summary of the datasets employed in the works presented in SectionÂ 3.2. Datasets marked with an asterisk are modified in different ways, making it impossible to fairly compare each other. Some of the datasets mentioned were not referenced so far: OmniglotÂ [58], OTBÂ [59], VOT2014Â [60], ShakespeareÂ [2] and Fashion MNISTÂ [61]. ",
        "table": "<table id=\"S3.T2.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T2.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r\">Article</th>\n<th id=\"S3.T2.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" colspan=\"3\">Datasets used in experiments</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T2.1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib36\" title=\"\" class=\"ltx_ref\">36</a>]</cite></th>\n<td id=\"S3.T2.1.1.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_t\">MNIST;</td>\n<td id=\"S3.T2.1.1.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_t\">CIFAR-10</td>\n<td id=\"S3.T2.1.1.2.1.4\" class=\"ltx_td ltx_border_t\"></td>\n</tr>\n<tr id=\"S3.T2.1.1.3.2\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib38\" title=\"\" class=\"ltx_ref\">38</a>]</cite></th>\n<td id=\"S3.T2.1.1.3.2.2\" class=\"ltx_td ltx_align_left\">MiniImageNet;</td>\n<td id=\"S3.T2.1.1.3.2.3\" class=\"ltx_td ltx_align_left\">Omniglot</td>\n<td id=\"S3.T2.1.1.3.2.4\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T2.1.1.4.3\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">39</a>]</cite></th>\n<td id=\"S3.T2.1.1.4.3.2\" class=\"ltx_td ltx_align_left\">MNIST*;</td>\n<td id=\"S3.T2.1.1.4.3.3\" class=\"ltx_td ltx_align_left\">CIFAR-10*</td>\n<td id=\"S3.T2.1.1.4.3.4\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T2.1.1.5.4\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib40\" title=\"\" class=\"ltx_ref\">40</a>]</cite></th>\n<td id=\"S3.T2.1.1.5.4.2\" class=\"ltx_td ltx_align_left\">CASAS</td>\n<td id=\"S3.T2.1.1.5.4.3\" class=\"ltx_td\"></td>\n<td id=\"S3.T2.1.1.5.4.4\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T2.1.1.6.5\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib42\" title=\"\" class=\"ltx_ref\">42</a>]</cite></th>\n<td id=\"S3.T2.1.1.6.5.2\" class=\"ltx_td ltx_align_left\">CIFAR-100;</td>\n<td id=\"S3.T2.1.1.6.5.3\" class=\"ltx_td ltx_align_left\">FLICKR-AES</td>\n<td id=\"S3.T2.1.1.6.5.4\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T2.1.1.7.6\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.1.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">43</a>]</cite></th>\n<td id=\"S3.T2.1.1.7.6.2\" class=\"ltx_td ltx_align_left\">OTB;</td>\n<td id=\"S3.T2.1.1.7.6.3\" class=\"ltx_td ltx_align_left\">VOT2014</td>\n<td id=\"S3.T2.1.1.7.6.4\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T2.1.1.8.7\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.1.8.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib45\" title=\"\" class=\"ltx_ref\">45</a>]</cite></th>\n<td id=\"S3.T2.1.1.8.7.2\" class=\"ltx_td ltx_align_left\">MNIST;</td>\n<td id=\"S3.T2.1.1.8.7.3\" class=\"ltx_td ltx_align_left\">FEMNIST;</td>\n<td id=\"S3.T2.1.1.8.7.4\" class=\"ltx_td ltx_align_left\">Shakespeare</td>\n</tr>\n<tr id=\"S3.T2.1.1.9.8\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.1.9.8.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib46\" title=\"\" class=\"ltx_ref\">46</a>]</cite></th>\n<td id=\"S3.T2.1.1.9.8.2\" class=\"ltx_td ltx_align_left\">MNIST*;</td>\n<td id=\"S3.T2.1.1.9.8.3\" class=\"ltx_td ltx_align_left\">CIFAR-10*;</td>\n<td id=\"S3.T2.1.1.9.8.4\" class=\"ltx_td ltx_align_left\">EMNIST</td>\n</tr>\n<tr id=\"S3.T2.1.1.10.9\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.1.10.9.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib51\" title=\"\" class=\"ltx_ref\">51</a>]</cite></th>\n<td id=\"S3.T2.1.1.10.9.2\" class=\"ltx_td ltx_align_left\">MNIST*;</td>\n<td id=\"S3.T2.1.1.10.9.3\" class=\"ltx_td ltx_align_left\">CIFAR-100*</td>\n<td id=\"S3.T2.1.1.10.9.4\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T2.1.1.11.10\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.1.11.10.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Â <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib52\" title=\"\" class=\"ltx_ref\">52</a>]</cite>\n</th>\n<td id=\"S3.T2.1.1.11.10.2\" class=\"ltx_td ltx_align_left\">MNIST*;</td>\n<td id=\"S3.T2.1.1.11.10.3\" class=\"ltx_td ltx_align_left\">FEMNIST</td>\n<td id=\"S3.T2.1.1.11.10.4\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T2.1.1.12.11\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.1.12.11.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib55\" title=\"\" class=\"ltx_ref\">55</a>]</cite></th>\n<td id=\"S3.T2.1.1.12.11.2\" class=\"ltx_td ltx_align_left\">Fashion MNIST;</td>\n<td id=\"S3.T2.1.1.12.11.3\" class=\"ltx_td ltx_align_left\">EMNIST</td>\n<td id=\"S3.T2.1.1.12.11.4\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T2.1.1.13.12\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.1.13.12.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib56\" title=\"\" class=\"ltx_ref\">56</a>]</cite></th>\n<td id=\"S3.T2.1.1.13.12.2\" class=\"ltx_td ltx_align_left\">Fashion MNIST;</td>\n<td id=\"S3.T2.1.1.13.12.3\" class=\"ltx_td ltx_align_left\">EMNIST</td>\n<td id=\"S3.T2.1.1.13.12.4\" class=\"ltx_td\"></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Concerning the experimental results,Â [51, 52, 53] perform training on MNIST, FEMNIST, and CIFAR-100Â [50] benchmark datasets, dividing the samples among the clients and swapping labels in some of them to produce different behaviours, Pâ€‹(y|x)ğ‘ƒconditionalğ‘¦ğ‘¥\\mathit{P}(y|x). They achieve higher accuracy and convergence ratio than standard FedAvg. However, these methods do not compare themselves with any other algorithm than FedAvg, which is designed to tackle problems only in IID scenarios. On the other hand,Â [54] improves the result obtained by FedAvg on the task of digit image recognition using the MNIST dataset, in a centralized framework.Â [55, 56] compare themselves with FedAvg in decentralized settings using the datasets of Fashion MNIST and Extended MNIST (EMNIST)Â [57], and they obtain a similar accuracy. On the whole, the most remarkable improvement accomplished with these kinds of personalization methods so far is their convergence speed. In TableÂ 2 we summarize the datasets used in these works."
        ]
    },
    "S3.T3": {
        "caption": "Table 3:  Summary of the Datasets employed in the works presented in SectionÂ 3.3.1. Asterisks indicate that the datasets have been modified in particular ways, making it impossible to fairly compare each other. Some of the datasets mentioned were not referenced so far: Bing-caltech256Â [95], COREL5000Â [96], ImageNetÂ [97] and NYUDÂ [98]. ",
        "table": "<table id=\"S3.T3.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T3.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r\">Article</th>\n<th id=\"S3.T3.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" colspan=\"3\">Datasets used in experiments</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T3.1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib62\" title=\"\" class=\"ltx_ref\">62</a>]</cite></th>\n<td id=\"S3.T3.1.1.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_t\">MNIST + SVHN + USPS</td>\n<td id=\"S3.T3.1.1.2.1.3\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S3.T3.1.1.2.1.4\" class=\"ltx_td ltx_border_t\"></td>\n</tr>\n<tr id=\"S3.T3.1.1.3.2\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib64\" title=\"\" class=\"ltx_ref\">64</a>]</cite></th>\n<td id=\"S3.T3.1.1.3.2.2\" class=\"ltx_td ltx_align_left\">Office-31;</td>\n<td id=\"S3.T3.1.1.3.2.3\" class=\"ltx_td ltx_align_left\">Bing-caltech256</td>\n<td id=\"S3.T3.1.1.3.2.4\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T3.1.1.4.3\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib67\" title=\"\" class=\"ltx_ref\">67</a>]</cite></th>\n<td id=\"S3.T3.1.1.4.3.2\" class=\"ltx_td ltx_align_left\">COREL5000;</td>\n<td id=\"S3.T3.1.1.4.3.3\" class=\"ltx_td ltx_align_left\">Trecvid2005<span id=\"footnotex1\" class=\"ltx_note ltx_role_footnote\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Available at http://www-nlpir.nist.gov/projects/trecvid</span></span></span>\n</td>\n<td id=\"S3.T3.1.1.4.3.4\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T3.1.1.5.4\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib68\" title=\"\" class=\"ltx_ref\">68</a>]</cite></th>\n<td id=\"S3.T3.1.1.5.4.2\" class=\"ltx_td ltx_align_left\">MNIST*;</td>\n<td id=\"S3.T3.1.1.5.4.3\" class=\"ltx_td ltx_align_left\">Olivetti FR<span id=\"footnotex2\" class=\"ltx_note ltx_role_footnote\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Available at http://www.uk.research.att.com/facedatabase.html.</span></span></span>\n</td>\n<td id=\"S3.T3.1.1.5.4.4\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T3.1.1.6.5\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib70\" title=\"\" class=\"ltx_ref\">70</a>]</cite></th>\n<td id=\"S3.T3.1.1.6.5.2\" class=\"ltx_td ltx_align_left\">MNIST + SVHN</td>\n<td id=\"S3.T3.1.1.6.5.3\" class=\"ltx_td\"></td>\n<td id=\"S3.T3.1.1.6.5.4\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T3.1.1.7.6\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.1.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib71\" title=\"\" class=\"ltx_ref\">71</a>]</cite></th>\n<td id=\"S3.T3.1.1.7.6.2\" class=\"ltx_td ltx_align_left\">Office-31;</td>\n<td id=\"S3.T3.1.1.7.6.3\" class=\"ltx_td ltx_align_left\">ImageNet;</td>\n<td id=\"S3.T3.1.1.7.6.4\" class=\"ltx_td ltx_align_left\">VisDA2017</td>\n</tr>\n<tr id=\"S3.T3.1.1.8.7\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.1.8.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib72\" title=\"\" class=\"ltx_ref\">72</a>]</cite></th>\n<td id=\"S3.T3.1.1.8.7.2\" class=\"ltx_td ltx_align_left\">Office-31;</td>\n<td id=\"S3.T3.1.1.8.7.3\" class=\"ltx_td ltx_align_left\">Image CLEF-DA</td>\n<td id=\"S3.T3.1.1.8.7.4\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T3.1.1.9.8\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.1.9.8.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib73\" title=\"\" class=\"ltx_ref\">73</a>]</cite></th>\n<td id=\"S3.T3.1.1.9.8.2\" class=\"ltx_td ltx_align_left\">Digit5;</td>\n<td id=\"S3.T3.1.1.9.8.3\" class=\"ltx_td ltx_align_left\">Office-31</td>\n<td id=\"S3.T3.1.1.9.8.4\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T3.1.1.10.9\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.1.10.9.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib74\" title=\"\" class=\"ltx_ref\">74</a>]</cite></th>\n<td id=\"S3.T3.1.1.10.9.2\" class=\"ltx_td ltx_align_left\">MNIST + MNIST-M + USPS;</td>\n<td id=\"S3.T3.1.1.10.9.3\" class=\"ltx_td ltx_align_left\">VisDA2017</td>\n<td id=\"S3.T3.1.1.10.9.4\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T3.1.1.11.10\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.1.11.10.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib78\" title=\"\" class=\"ltx_ref\">78</a>]</cite></th>\n<td id=\"S3.T3.1.1.11.10.2\" class=\"ltx_td ltx_align_left\">MNIST + SVHN + USPS;</td>\n<td id=\"S3.T3.1.1.11.10.3\" class=\"ltx_td ltx_align_left\">Office-31;</td>\n<td id=\"S3.T3.1.1.11.10.4\" class=\"ltx_td ltx_align_left\">NYUD</td>\n</tr>\n<tr id=\"S3.T3.1.1.12.11\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.1.12.11.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib79\" title=\"\" class=\"ltx_ref\">79</a>]</cite></th>\n<td id=\"S3.T3.1.1.12.11.2\" class=\"ltx_td ltx_align_left\">Office-31;</td>\n<td id=\"S3.T3.1.1.12.11.3\" class=\"ltx_td ltx_align_left\">Image CLEF-DA<span id=\"footnotex3\" class=\"ltx_note ltx_role_footnote\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>Available at http://imageclef.org/2014/adaptation</span></span></span>\n</td>\n<td id=\"S3.T3.1.1.12.11.4\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T3.1.1.13.12\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.1.13.12.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib80\" title=\"\" class=\"ltx_ref\">80</a>]</cite></th>\n<td id=\"S3.T3.1.1.13.12.2\" class=\"ltx_td ltx_align_left\">Office-Home;</td>\n<td id=\"S3.T3.1.1.13.12.3\" class=\"ltx_td ltx_align_left\">VisDA2017</td>\n<td id=\"S3.T3.1.1.13.12.4\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S3.T3.1.1.14.13\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.1.14.13.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib81\" title=\"\" class=\"ltx_ref\">81</a>]</cite></th>\n<td id=\"S3.T3.1.1.14.13.2\" class=\"ltx_td ltx_align_left\">MNIST + SVHN + USPS;</td>\n<td id=\"S3.T3.1.1.14.13.3\" class=\"ltx_td ltx_align_left\">Office-31</td>\n<td id=\"S3.T3.1.1.14.13.4\" class=\"ltx_td\"></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "In general, comparing the different methods is a tough issue. Each work is free to choose different synthetic datasets to perform experimental results, and also modify them to generate the required heterogeneity that they want to face (see TableÂ 3). For these reasons, it is impossible to fairly compare the diverse strategies we presented. However, there are some remarkable results that we would like to highlight: regarding Domain Transformation methods, the experimental results of two of the works stand outÂ [64, 68]. They present a complete variety of experiments and contrast their results with other well-known methods, getting significantly better error ratios and accuracies. On the other hand, the most outstanding results achieved with Domain Adaptation methods are the ones fromÂ [74, 80, 81]. The first one,Â [74] propose their method SimNet and experimentally compare their results with some other methods like DAN, RTN and a baseline method over the datasets of MNIST, Office-31 and VisDA2017. It improves the accuracy obtained by every other method in the three cases. ConcerningÂ [80, 81], they both employ the Office-31 dataset, and obtain impressive results compared to the other methods they test."
        ]
    },
    "S5.T4": {
        "caption": "Table 4: Spatial and Temporal heterogeneity learning scenarios, and the strategies that could potentially solve each situation. Strategies that deal with changes in both the input space and the behaviour are placed only in the last row/column, and not in the previous ones.",
        "table": "",
        "footnotes": "",
        "references": [
            "On the whole, there are 444 feasible scenarios for each spatial and temporal data, and they may appear combined with each other in realistic tasks. The global data distribution DGtâ€‹(x,y)superscriptsubscriptğ·ğºğ‘¡ğ‘¥ğ‘¦D_{G}^{t}(x,y), which includes both spatial and temporal heterogeneity, can evolve following 161616 different courses. We represent all of the possibilities in TableÂ 4, as well as some of the strategies and algorithms that focus on solving some of those possibilities. Notice that we include IID data to consider all of the possible combinations of heterogeneity.",
            "Notice that if we determine effective approaches to solve each of the scenarios corresponding to the first row and column in TableÂ 4, then we will be able to solve the situation of any cell by combining the algorithms from its corresponding row (already addressed in SectionÂ 3.3.1) and column, as long as they are compatible. Thus, from now on we will consider scenarios where data is IID in the spatial axis. This corresponds to pure CL. In the following sections, we are going to present the existing solutions to deal with temporal non-IID data, classify those strategies according to their shared characteristics, and compare their experimental results when possible.",
            "To be able to apply federated learning in the different scenarios depicted in Table 4, data has to fulfill certain requirements. In this subsection we will describe these restrictions, which are summarized in Table 7.",
            "Considering all of the scenarios presented in TableÂ 4, some of them are solvable using sophisticated techniques without imposing additional restrictions, but some others may need to verify certain conditions that neither standard FL nor CL demand. As we saw in SectionsÂ 3 andÂ 5, facing variations in the marginal input probabilities Pâ€‹(x)ğ‘ƒğ‘¥\\mathit{P}(x), either in the spatial or temporal dimension, is possible without any supplementary informationÂ [62, 67], i.e, unsupervised learning techniques can also be useful in these scenarios. On the contrary, if we seek to detect changes in the conditional probability Pâ€‹(y|x)ğ‘ƒconditionalğ‘¦ğ‘¥\\mathit{P}(y|x), a certain amount of labeled data is requiredÂ [100, 142], because these kinds of changes can only be measured with the error committed. To be more precise, we establish three restrictions that need to be satisfied to face some scenarios, and denote them as Restrictions 1P-MT, AP-1T, and AP-MT in TableÂ 7:",
            "The kind of heterogeneity that can be handled without any additional restriction is by far the most studied one in the literature (see the number of works cited in TableÂ 4). Situations where some additional condition is required are less studied, not because the tasks that fit these scenarios are uncommon, but because it is harder to work under the restrictions we just settled."
        ]
    },
    "S5.T5": {
        "caption": "Table 5:  Summary of the datasets employed in the works presented in SectionÂ 5.1. Asterisks indicate that the datasets have been modified in particular ways, making it impossible to fairly compare each other.",
        "table": "<table id=\"S5.T5.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T5.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r\">Article</th>\n<th id=\"S5.T5.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" colspan=\"2\">Datasets used in experiments</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T5.1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib133\" title=\"\" class=\"ltx_ref\">133</a>]</cite></th>\n<td id=\"S5.T5.1.1.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_t\">MNIST* + CIFAR-10*;</td>\n<td id=\"S5.T5.1.1.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_t\">CUBS</td>\n</tr>\n<tr id=\"S5.T5.1.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib134\" title=\"\" class=\"ltx_ref\">134</a>]</cite></th>\n<td id=\"S5.T5.1.1.3.2.2\" class=\"ltx_td ltx_align_left\">MNIST + SVHN</td>\n<td id=\"S5.T5.1.1.3.2.3\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S5.T5.1.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib135\" title=\"\" class=\"ltx_ref\">135</a>]</cite></th>\n<td id=\"S5.T5.1.1.4.3.2\" class=\"ltx_td ltx_align_left\">MNIST*</td>\n<td id=\"S5.T5.1.1.4.3.3\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S5.T5.1.1.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib136\" title=\"\" class=\"ltx_ref\">136</a>]</cite></th>\n<td id=\"S5.T5.1.1.5.4.2\" class=\"ltx_td ltx_align_left\">MNIST*</td>\n<td id=\"S5.T5.1.1.5.4.3\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S5.T5.1.1.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib138\" title=\"\" class=\"ltx_ref\">138</a>]</cite></th>\n<td id=\"S5.T5.1.1.6.5.2\" class=\"ltx_td ltx_align_left\">MNIST* + SVHN + CIFAR-10</td>\n<td id=\"S5.T5.1.1.6.5.3\" class=\"ltx_td\"></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Concerning the experimental results, we find the same problem of having very different datasets (see TableÂ 5), making it difficult to establish relations between the different strategies results. In this case, nonetheless, most of the works we just presented compare their methods with EWCÂ [136], using it as a baseline. InÂ [136], the MNIST dataset is employed to simulate the different input data distributions. They take a random permutation of pixels and apply that permutation to all of the images to create each input domain. With this strategy, they only need one dataset, denoted Permuted MNIST, to simulate any number of domains. InÂ [137], authors compare the accuracy obtained with their learning method (P&C) in each domain, and also the mean accuracy, with EWC, showing that P&C achieves slightly better results avoiding catastrophic forgetting. That also happens inÂ [132], where CLEAR method is compared with both EWC and P&C. CLEAR outperforms EWC in most situations, and attain very similar results to P&C.Â [133, 138] also use the permuted MNIST dataset and improve the mean accuracy of EWC, and some other methods they compare with. Lastly, generative strategies likeÂ [134] prove to be efficient to prevent catastrophic forgetting. They use the datasets MNIST and SVHN to show that the performance is barely affected when they change the input dataset."
        ]
    },
    "S5.T6": {
        "caption": "Table 6:  Summary of the datasets employed in the works presented in SectionÂ 5.2. Asterisks indicate that the datasets have been modified in particular ways. Some of the datasets mentioned were not referenced so far: Atari gamesÂ [146] ",
        "table": "<table id=\"S5.T6.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T6.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T6.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r\">Article</th>\n<th id=\"S5.T6.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" colspan=\"3\">Datasets used in experiments</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T6.1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T6.1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib139\" title=\"\" class=\"ltx_ref\">139</a>]</cite></th>\n<td id=\"S5.T6.1.1.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_t\">MNIST*;</td>\n<td id=\"S5.T6.1.1.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_t\">CIFAR-10*</td>\n<td id=\"S5.T6.1.1.2.1.4\" class=\"ltx_td ltx_border_t\"></td>\n</tr>\n<tr id=\"S5.T6.1.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T6.1.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib140\" title=\"\" class=\"ltx_ref\">140</a>]</cite></th>\n<td id=\"S5.T6.1.1.3.2.2\" class=\"ltx_td ltx_align_left\">MNIST*;</td>\n<td id=\"S5.T6.1.1.3.2.3\" class=\"ltx_td ltx_align_left\">CIFAR-100</td>\n<td id=\"S5.T6.1.1.3.2.4\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S5.T6.1.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T6.1.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib141\" title=\"\" class=\"ltx_ref\">141</a>]</cite></th>\n<td id=\"S5.T6.1.1.4.3.2\" class=\"ltx_td ltx_align_left\">MNIST*</td>\n<td id=\"S5.T6.1.1.4.3.3\" class=\"ltx_td\"></td>\n<td id=\"S5.T6.1.1.4.3.4\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S5.T6.1.1.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T6.1.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib142\" title=\"\" class=\"ltx_ref\">142</a>]</cite></th>\n<td id=\"S5.T6.1.1.5.4.2\" class=\"ltx_td ltx_align_left\">ImagenNet;</td>\n<td id=\"S5.T6.1.1.5.4.3\" class=\"ltx_td ltx_align_left\">CUBS;</td>\n<td id=\"S5.T6.1.1.5.4.4\" class=\"ltx_td ltx_align_left\">Oxford102Flowers</td>\n</tr>\n<tr id=\"S5.T6.1.1.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T6.1.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib143\" title=\"\" class=\"ltx_ref\">143</a>]</cite></th>\n<td id=\"S5.T6.1.1.6.5.2\" class=\"ltx_td ltx_align_left\">ImagenNet;</td>\n<td id=\"S5.T6.1.1.6.5.3\" class=\"ltx_td ltx_align_left\">CUBS;</td>\n<td id=\"S5.T6.1.1.6.5.4\" class=\"ltx_td ltx_align_left\">Oxford102Flowers</td>\n</tr>\n<tr id=\"S5.T6.1.1.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T6.1.1.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib144\" title=\"\" class=\"ltx_ref\">144</a>]</cite></th>\n<td id=\"S5.T6.1.1.7.6.2\" class=\"ltx_td ltx_align_left\">ImagenNet;</td>\n<td id=\"S5.T6.1.1.7.6.3\" class=\"ltx_td ltx_align_left\">CUBS;</td>\n<td id=\"S5.T6.1.1.7.6.4\" class=\"ltx_td ltx_align_left\">Oxford102Flowers</td>\n</tr>\n<tr id=\"S5.T6.1.1.8.7\" class=\"ltx_tr\">\n<th id=\"S5.T6.1.1.8.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib145\" title=\"\" class=\"ltx_ref\">145</a>]</cite></th>\n<td id=\"S5.T6.1.1.8.7.2\" class=\"ltx_td ltx_align_left\">Atari games</td>\n<td id=\"S5.T6.1.1.8.7.3\" class=\"ltx_td\"></td>\n<td id=\"S5.T6.1.1.8.7.4\" class=\"ltx_td\"></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "These kinds of methods present a lot of differences in the way they implement their experimental results. Some of them pay attention to the accuracy obtained, while others are more concerned about the error they got, and some others concentrate on the level of forgetting they commit. For instance, inÂ [139] the authors employ the MNIST dataset with pixel permutation, like inÂ [136], and also exchanges some class labels in parts of the dataset to simulate the different behaviours. They compare their results with EWC, LwFÂ [147], and improve their results. However, they emphasize that this form of simulating the different tasks and behaviours is quite unrealistic. Surprisingly,Â [142, 143, 144] employ the same datasets to perform their experiments: the ImageNet datasetÂ [97], used for training the pre-trained ImageNet-VGG-16 neural network; the CUBS datasetÂ [148], and the Oxford102Flowers datasetÂ [149] (see TableÂ 6). This is, as we have seen in this paper, very rare."
        ]
    },
    "S5.T7": {
        "caption": "Table 7: Required restrictions for the non-IID learning scenarios.",
        "table": "",
        "footnotes": "",
        "references": [
            "To be able to apply federated learning in the different scenarios depicted in Table 4, data has to fulfill certain requirements. In this subsection we will describe these restrictions, which are summarized in Table 7.",
            "Considering all of the scenarios presented in TableÂ 4, some of them are solvable using sophisticated techniques without imposing additional restrictions, but some others may need to verify certain conditions that neither standard FL nor CL demand. As we saw in SectionsÂ 3 andÂ 5, facing variations in the marginal input probabilities Pâ€‹(x)ğ‘ƒğ‘¥\\mathit{P}(x), either in the spatial or temporal dimension, is possible without any supplementary informationÂ [62, 67], i.e, unsupervised learning techniques can also be useful in these scenarios. On the contrary, if we seek to detect changes in the conditional probability Pâ€‹(y|x)ğ‘ƒconditionalğ‘¦ğ‘¥\\mathit{P}(y|x), a certain amount of labeled data is requiredÂ [100, 142], because these kinds of changes can only be measured with the error committed. To be more precise, we establish three restrictions that need to be satisfied to face some scenarios, and denote them as Restrictions 1P-MT, AP-1T, and AP-MT in TableÂ 7:",
            "If the clients behaviour change with time but does not change among the devices, i.e., data fit the cells marked as Restriction 1P-MT (one participant, many times) in TableÂ 7, then we will need to have enough labeled data of at least one participant from time to time. Knowing the behaviour of one participant is enough since in these scenarios the behaviours of all of the other participants will be the same. When a Real Concept Drift occurs, that client labeled data will allow the model to detect that drift and properly react to it.",
            "If, on the contrary, data fit the cells marked as Restriction AP-1T (all participants, one time) in TableÂ 7, then the clients are allowed to present different conditional probabilities, but they will remain constant in time. In that situation, enough labeled data from all of the participants will be required at the beginning of the training process, so we can determine their behaviour. Once their behaviour is settled, it is not possible for it to change, so no more labeled data is required.",
            "Lastly, if conditional probabilities vary both in the spatial and temporal axis, which corresponds to cells marked as Restriction AP-MT (all participants, many times) in TableÂ 7, then we need enough labeled data from all of the participants, from time to time, so we can conclude when drifts occur and act in consequence. This restriction provides strictly more information than the other ones, so any other scenario considered in TableÂ 7 will also be solvable under this requirement. However, it could be very unrealistic to assume that we could have this information in real-world problems."
        ]
    }
}