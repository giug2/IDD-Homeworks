{
    "A1.T2": {
        "caption": "Table 2: Details of six different neural networks",
        "table": "<table id=\"A1.T2.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T2.1.1.1\" class=\"ltx_tr\">\n<td id=\"A1.T2.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Hidden Layers</td>\n<td id=\"A1.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Nodes per Hidden Layer</td>\n<td id=\"A1.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Total Parameters</td>\n</tr>\n<tr id=\"A1.T2.1.2.2\" class=\"ltx_tr\">\n<td id=\"A1.T2.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">1</td>\n<td id=\"A1.T2.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">192</td>\n<td id=\"A1.T2.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1347</td>\n</tr>\n<tr id=\"A1.T2.1.3.3\" class=\"ltx_tr\">\n<td id=\"A1.T2.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">2</td>\n<td id=\"A1.T2.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">33</td>\n<td id=\"A1.T2.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1356</td>\n</tr>\n<tr id=\"A1.T2.1.4.4\" class=\"ltx_tr\">\n<td id=\"A1.T2.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">3</td>\n<td id=\"A1.T2.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">24</td>\n<td id=\"A1.T2.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1371</td>\n</tr>\n<tr id=\"A1.T2.1.5.5\" class=\"ltx_tr\">\n<td id=\"A1.T2.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">4</td>\n<td id=\"A1.T2.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">20</td>\n<td id=\"A1.T2.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1403</td>\n</tr>\n<tr id=\"A1.T2.1.6.6\" class=\"ltx_tr\">\n<td id=\"A1.T2.1.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">5</td>\n<td id=\"A1.T2.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">17</td>\n<td id=\"A1.T2.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1346</td>\n</tr>\n<tr id=\"A1.T2.1.7.7\" class=\"ltx_tr\">\n<td id=\"A1.T2.1.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">6</td>\n<td id=\"A1.T2.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">15</td>\n<td id=\"A1.T2.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1308</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "As discussed in Sec.\u00a0III.2, our fiducial neural network model assumes two hidden layers (Fig.\u00a01). In this Appendix, we provide results from tests we performed with larger numbers of hidden layers. To determine if additional hidden layers would be advantageous for our data set, we performed a numerical experiment where we trained neural networks with different numbers of hidden layers and compared their accuracy on noiseless data that was not in the training set. We note that, unlike the two-hidden layer neural network depicted in Fig. 1, which has more nodes in the first layer than the second, the hidden layers for the neural networks in these tests used the same number of nodes for simplicity. In order to ensure that each neural network has roughly the same number of parameters, we set the number of nodes in each hidden layer to be fewer for neural networks with more hidden layers. Table 2 shows exactly how many free parameters were in each neural network. This was an important step because keeping the number of nodes per layer the same would naturally advantage models with more hidden layers, as those models would have many times more free parameters than the models with fewer hidden layers."
        ]
    },
    "S4.T1": {
        "caption": "Table 1: Memory Consumption Results (20,000 synthetic data points). [Associated dataset available at https://doi.org/10.5281/zenodo.8221343]Desai et\u00a0al. (2023c)\n",
        "table": "<table id=\"S4.T1.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.1.1.1\" class=\"ltx_td ltx_border_l ltx_border_r ltx_border_t\"></td>\n<td id=\"S4.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">SVR</td>\n<td id=\"S4.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">NN</td>\n<td id=\"S4.T1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">GPR</td>\n</tr>\n<tr id=\"S4.T1.1.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">Ave GPU Memory Utilization (GiB)</td>\n<td id=\"S4.T1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1.9</td>\n<td id=\"S4.T1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1.1</td>\n<td id=\"S4.T1.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">10.7</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Table\u00a01 summarizes the average GPU memory consumption of the three different ML models on 20,000 synthetic data points. The NN had the lowest memory consumption while GPR consumed the most memory. All results are for single precision. In tests with double precision (not shown), GPR consumed about twice as much GPU memory while SVR and NN consumed about the same amount of GPU memory as single precision."
        ]
    }
}