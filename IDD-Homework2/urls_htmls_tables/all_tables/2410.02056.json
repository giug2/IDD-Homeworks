{
    "id_table_1": {
        "caption": "Table 1:  Result comparison of Synthio with baselines on 10 datasets and 4 small-scale settings.  n n n italic_n  refers to the number of samples in the small-scale dataset augmented with synthetic data. Synthio outperforms our baselines by 0.1% - 39%. We also highlight the relative improvements by Synthio compared to the Gold-only.",
        "table": "S5.T1.3.1",
        "footnotes": [],
        "references": [
            "Diffusion Models.  Diffusion models consist of two main processes: a forward process and a reverse process. Given a data point  x 0 subscript x 0 x_{0} italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  with probability distribution  p  ( x 0 ) p subscript x 0 p(x_{0}) italic_p ( italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) , the forward diffusion process gradually adds Gaussian noise to  x 0 subscript x 0 x_{0} italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  according to a pre-set variance schedule   1 ,  ,  T subscript  1  subscript  T \\beta_{1},\\cdots,\\beta_{T} italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,  , italic_ start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  and degrades the structure of the data. At the time step  t t t italic_t , the latent variable  x t subscript x t {x_{t}} italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  is only determined by the  x t  1 subscript x t 1 x_{t-1} italic_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT  due to its discrete-time Markov process nature. At inference time, the diffusion model iteratively executes the reverse process  T T T italic_T  times starting from a randomly sampled Gaussian Noise  (   N  ( 0 , I ) ) similar-to italic- N 0 I (\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I})) ( italic_  caligraphic_N ( 0 , bold_I ) ) . For more details on diffusion models, we request our readers to refer to Appendix  A.1 .",
            "Main Results.  Table  1  showcases the performance comparison between Synthio and the baseline methods. Synthio consistently outperforms all baselines by 0.1%-39%, achieving notable improvements in overall classification accuracy compared to Gold-only. The highest gains are observed on USD8K, while the least is on Vocal Sound, likely due to the T2A datasets heavy representation of music compared to the more sparse vocal sounds. Performance gains tend to decrease as the number of gold samples  n n n italic_n  in  D small subscript D small \\mathcal{D}_{\\text{small}} caligraphic_D start_POSTSUBSCRIPT small end_POSTSUBSCRIPT  grows, aligning with observed trends in prior studies. Although Vanilla Synthetic Augmentations emerge as the strongest baseline, they lag behind Synthio by 3.5%.",
            "A.1  Background on Diffusion Models",
            "where  p   ( x T ) = N  ( x T ; 0 , I ) subscript p  subscript x T N subscript x T 0 I p_{\\theta}(x_{T})=\\mathcal{N}(x_{T};0,I) italic_p start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) = caligraphic_N ( italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ; 0 , italic_I ) . At inference time, the diffusion model iteratively executes the reverse process (Eq.  17 )  T T T italic_T  times starting from a randomly sampled Gaussian Noise  (   N  ( 0 , I ) ) similar-to italic- N 0 I (\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I})) ( italic_  caligraphic_N ( 0 , bold_I ) ) .",
            "Table  7  presents the performance comparison of Synthio on the full original dataset splits (where the entire training set is used without any downsampling). While Synthio outperforms all baselines, traditional augmentation methods prove to be much more competitive in this scenario. This contrasts with the results in Table  1  where traditional augmentations showed minimal improvements in performance.",
            "To offer an alternative perspective on the distributional consistency between the generated augmentations and the ground-truth small-scale dataset, we compare the Frechet Audio Distance (FAD) scores  (Kilgour et al.,  2018 ) . For this experiment, we use Synthio with Template Captions. Table  10  presents a comparison of FAD scores between Synthio and other baselines. Synthio achieves the highest FAD score, indicating that it produces the most consistent audio augmentations."
        ]
    },
    "id_table_2": {
        "caption": "Table 6:  Examples of generated and revised captions from the Synthio methodology.",
        "table": "S6.T2.2.2",
        "footnotes": [],
        "references": [
            "To address these issues, we propose the concept of  aligning teaching with learning preferences . Our approach assumes that the classification model (viewed as the student) performs better when trained on synthetic audio that closely matches the inherent acoustic properties of our high-quality and human-labeled  D small subscript D small \\mathcal{D}_{\\text{small}} caligraphic_D start_POSTSUBSCRIPT small end_POSTSUBSCRIPT . Thus, we align the generations of the T2A model (viewed as the teacher) to  D small subscript D small \\mathcal{D}_{\\text{small}} caligraphic_D start_POSTSUBSCRIPT small end_POSTSUBSCRIPT , ensuring that the generated augmentations align with the desired characteristics and  sound similar , ultimately enhancing the student models ability to generalize to similarly characterized test data. As shown in Fig.  2 , we achieve this using preference optimization (DPO in our case) and align generations of  T  superscript T  \\mathcal{T}^{\\theta} caligraphic_T start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  with  D small subscript D small \\mathcal{D}_{\\text{small}} caligraphic_D start_POSTSUBSCRIPT small end_POSTSUBSCRIPT . Unlike standard fine-tuning, which can lead to less diverse outputs and overfitting due to a narrow focus on minimizing loss, preference optimization encourages greater exploration in the models output space, preventing mode collapse and fostering more diverse augmentations. Additionally, DPO leverages pairwise learning, offering richer training signals compared to the independent outputs used in standard fine-tuning, further mitigating overfitting risks. We detail our two-step approach for DPO optimization below:",
            "We propose  MixCap , a prompt generation method that creates diverse and effective captions in three steps: First, we employ GAMA  (Ghosh et al.,  2024a )  to caption all audio files in  D small subscript D small \\mathcal{D}_{\\text{small}} caligraphic_D start_POSTSUBSCRIPT small end_POSTSUBSCRIPT . Next, we prompt an LLM to extract phrases describing the acoustic components of the audio. These components correspond to the acoustic elements such as backgrounds and foreground events, and their attributes and relations, etc (see prompt in Appendix  A.2 ). Finally, for each training instance in  D small subscript D small \\mathcal{D}_{\\text{small}} caligraphic_D start_POSTSUBSCRIPT small end_POSTSUBSCRIPT , we prompt the LLM with the ground-truth label and the extracted components from all instances to generate  N N N italic_N  diverse audio captions that blend existing and new components. This approach prevents overemphasis on events already present in  D small subscript D small \\mathcal{D}_{\\text{small}} caligraphic_D start_POSTSUBSCRIPT small end_POSTSUBSCRIPT  and encourages diversity in the generated audio. These captions are then used to prompt the aligned T2A model to generate synthetic data  D syn subscript D syn \\mathcal{D}_{\\text{syn}} caligraphic_D start_POSTSUBSCRIPT syn end_POSTSUBSCRIPT .",
            "Filtering.  After generating captions and their corresponding audio, we filter the audio for label consistency. While LLMs can generate diverse captions, the audio produced must remain aligned with the ground-truth label. To ensure this, we use CLAP  to evaluate the generated audio, accepting those that meet a similarity threshold of  p % percent p p\\% italic_p %  and rejecting the rest. We denote the accepted audios as  D syn acc superscript subscript D syn acc \\mathcal{D}_{\\text{syn}}^{\\text{acc}} caligraphic_D start_POSTSUBSCRIPT syn end_POSTSUBSCRIPT start_POSTSUPERSCRIPT acc end_POSTSUPERSCRIPT  and the rejected ones as  D syn rej superscript subscript D syn rej \\mathcal{D}_{\\text{syn}}^{\\text{rej}} caligraphic_D start_POSTSUBSCRIPT syn end_POSTSUBSCRIPT start_POSTSUPERSCRIPT rej end_POSTSUPERSCRIPT . The CLAP model is pre-trained on  D a-c subscript D a-c \\mathcal{D}_{\\text{a-c}} caligraphic_D start_POSTSUBSCRIPT a-c end_POSTSUBSCRIPT  and fine-tuned on  D small subscript D small \\mathcal{D}_{\\text{small}} caligraphic_D start_POSTSUBSCRIPT small end_POSTSUBSCRIPT  to adapt to the target dataset. Example captions are in Table  6 , and prompts are in Appendix  A.2 .",
            "Fig.  4  compares the distributions of pitch and various spectral features between generated audios in  D syn subscript D syn \\mathcal{D}_{\\text{syn}} caligraphic_D start_POSTSUBSCRIPT syn end_POSTSUBSCRIPT  and real audios in  D small subscript D small \\mathcal{D}_{\\text{small}} caligraphic_D start_POSTSUBSCRIPT small end_POSTSUBSCRIPT  across different methods on the USD8K and NSynth datasets. The features analyzed include Pitch Salience (clarity of the main pitch)  (Ricard,  2004 ) , Spectral Flatness (tonal vs. noise-like quality)  (Peeters,  2004 ) , Flux (rate of spectral change)  (Tzanetakis & Cook,  1999 ) , and Complexity (level of sound detail)  (Laurier et al.,  2010 ) . Notably, Synthio-generated audios closely replicate the spectral features of the original audios, showing the best alignment among all methods and demonstrating Synthios ability to generate consistent augmentations. Table  2  presents CLAP similarity scores between ground-truth audios and their  N N N italic_N  generated augmentations, averaged across all dataset instances. Audios generated with Synthio achieve the highest compositional diversity for generated audios among all baselines. Table  8  shows that audios generated using Synthio have the highest similarity with the ground-truth category label.",
            "A.2  Prompts"
        ]
    },
    "id_table_3": {
        "caption": "Table 7:  Comparison of Synthio and other baselines on the full original dataset splits (using all samples from the original training set as  D small subscript D small \\mathcal{D}_{\\text{small}} caligraphic_D start_POSTSUBSCRIPT small end_POSTSUBSCRIPT ).",
        "table": "S6.T3.3.1",
        "footnotes": [],
        "references": [
            "Step 2: Preference Optimization Using DPO.  After constructing  D pref subscript D pref \\mathcal{D}_{\\text{pref}} caligraphic_D start_POSTSUBSCRIPT pref end_POSTSUBSCRIPT , we train our T2A model on this dataset with DPO using the approach outlined in Section  3 . The resulting aligned model is referred to as  T aln  subscript superscript T  aln \\mathcal{T}^{\\theta}_{\\text{aln}} caligraphic_T start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT aln end_POSTSUBSCRIPT . Details of the hyper-parameters used for training are provided in Section  5 .",
            "Consistent with prior findings in vision  (He et al.,  2023 ) , we observe that synthetic data alone performs sub-optimally compared to human-annotated data. However, our results show that enhancing the consistency and diversity of synthetic data aided by a small-scale version of the target dataset significantly improves model performance. Table  3  compares models trained exclusively on synthetic data with our baselines (i.e., only  D syn subscript D syn \\mathcal{D}_{\\text{syn}} caligraphic_D start_POSTSUBSCRIPT syn end_POSTSUBSCRIPT  is used for training AST). Synthio outperforms all baselines by 0.1%-26.25%, with DPO-based alignment driving the improvements.",
            "A.3  Examples"
        ]
    },
    "id_table_4": {
        "caption": "Table 8:  CLAP score between generated audios and the label.",
        "table": "S6.T4.4.4",
        "footnotes": [],
        "references": [
            "Fig.  4  compares the distributions of pitch and various spectral features between generated audios in  D syn subscript D syn \\mathcal{D}_{\\text{syn}} caligraphic_D start_POSTSUBSCRIPT syn end_POSTSUBSCRIPT  and real audios in  D small subscript D small \\mathcal{D}_{\\text{small}} caligraphic_D start_POSTSUBSCRIPT small end_POSTSUBSCRIPT  across different methods on the USD8K and NSynth datasets. The features analyzed include Pitch Salience (clarity of the main pitch)  (Ricard,  2004 ) , Spectral Flatness (tonal vs. noise-like quality)  (Peeters,  2004 ) , Flux (rate of spectral change)  (Tzanetakis & Cook,  1999 ) , and Complexity (level of sound detail)  (Laurier et al.,  2010 ) . Notably, Synthio-generated audios closely replicate the spectral features of the original audios, showing the best alignment among all methods and demonstrating Synthios ability to generate consistent augmentations. Table  2  presents CLAP similarity scores between ground-truth audios and their  N N N italic_N  generated augmentations, averaged across all dataset instances. Audios generated with Synthio achieve the highest compositional diversity for generated audios among all baselines. Table  8  shows that audios generated using Synthio have the highest similarity with the ground-truth category label.",
            "Training and evaluation were conducted using the EnCLAP framework  (Kim et al.,  2024 ) , and the dataset was enriched with 4 synthetic samples. As shown in Table  4 , Synthio significantly outperforms baseline settings, with improvements largely due to better alignment w/ DPO. However, manual inspection revealed that generated audios occasionally do not match their captions compositionally, reflecting limitations of the current T2A model. While this issue does not affect classification, it poses challenges for captioning. We will explore more advanced methods as part of future work.",
            "A.4  Extra Results"
        ]
    },
    "id_table_5": {
        "caption": "Table 9:  Comparison of our trained Stable Diffusion model on AudioCaps test set",
        "table": "S6.T5.3.1",
        "footnotes": [],
        "references": [
            "Step 1: Construction of the Preference Dataset.  To create our preference dataset  D pref = { ( a 1 w , a 1 l ) ,  , ( a j w , a j l ) } subscript D pref subscript superscript a w 1 subscript superscript a l 1  subscript superscript a w j subscript superscript a l j \\mathcal{D}_{\\text{pref}}=\\{(a^{w}_{1},a^{l}_{1}),\\cdots,(a^{w}_{j},a^{l}_{j})\\} caligraphic_D start_POSTSUBSCRIPT pref end_POSTSUBSCRIPT = { ( italic_a start_POSTSUPERSCRIPT italic_w end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_a start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ,  , ( italic_a start_POSTSUPERSCRIPT italic_w end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_a start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) } , we first generate template-based captions for each instance in  D small subscript D small \\mathcal{D}_{\\text{small}} caligraphic_D start_POSTSUBSCRIPT small end_POSTSUBSCRIPT  in the form: Sound of a  label , where  label  is the category associated with the audio. For each instance, we prompt the T2A model  j j j italic_j  times, with all generations starting from randomly initialized Gaussian noise (generation configuration is detailed in Section  5 ). Each generated audio is then paired with the corresponding ground-truth audio from the gold dataset. This resulting  D pref subscript D pref \\mathcal{D}_{\\text{pref}} caligraphic_D start_POSTSUBSCRIPT pref end_POSTSUBSCRIPT  dataset has  n  j n j n\\times j italic_n  italic_j  instances, where the generated audio is treated as the loser and the ground-truth audio as the winner. This simple approach has proven highly effective in aligning generations by generative models by prior work  (Majumder et al.,  2024 ; Tian et al.,  2024 ) .",
            "Step 2: Preference Optimization Using DPO.  After constructing  D pref subscript D pref \\mathcal{D}_{\\text{pref}} caligraphic_D start_POSTSUBSCRIPT pref end_POSTSUBSCRIPT , we train our T2A model on this dataset with DPO using the approach outlined in Section  3 . The resulting aligned model is referred to as  T aln  subscript superscript T  aln \\mathcal{T}^{\\theta}_{\\text{aln}} caligraphic_T start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT aln end_POSTSUBSCRIPT . Details of the hyper-parameters used for training are provided in Section  5 .",
            "Table  5  compares the performance of Synthio, SpecAugment, and Vanilla Synthetic Augmentations across different scaling factors  N N N italic_N  = {1, 2, 3, 4, 5}, where  N N N italic_N  represents the number of synthetic samples generated per original sample in the small-scale dataset. As observed, SpecAugment, a traditional augmentation method, cannot scale with increasing  N N N italic_N , and the performance of Vanilla Synthetic Augmentations plateaus at higher  N N N italic_N . A similar saturation occurs with Synthio when MixCap is not used. Even without DPO, Synthio maintains better scalability, though with reduced overall performance. These results highlight that MixCaps ability to generate diverse captions is crucial for Synthios scalability.",
            "Figure  5  shows the classification accuracy on four underrepresented categories in the ESC-50 and NSynth datasets, comparing performance before and after applying Synthio augmentations. We selected categories with the lowest frequency in the datasets, such as  flute  and  guitar , which appear only once in the down-sampled sets. Synthio significantly boosts accuracy, with improvements up to 48%. Notably, categorie labels like  flute  and  guitar , which originally had 0% accuracy, show substantial gains with Synthio augmentation. This demonstrates Synthios effectiveness in enhancing performance on long-tail labels, a common challenge in real-world datasets  (Zhang et al.,  2023 ) .",
            "All codes will be open-sourced upon paper acceptance, including all T2A checkpoints. All experimental details, including training parameters and hyper-parameters, are provided in Section  5 .",
            "A.5  Dataset Details"
        ]
    },
    "id_table_6": {
        "caption": "Table 10:  Comparison of FAD score of Vaniall Syn. Aug. and Stable Audio VECaps (ours).",
        "table": "A1.T6.1.1",
        "footnotes": [],
        "references": [
            "Filtering.  After generating captions and their corresponding audio, we filter the audio for label consistency. While LLMs can generate diverse captions, the audio produced must remain aligned with the ground-truth label. To ensure this, we use CLAP  to evaluate the generated audio, accepting those that meet a similarity threshold of  p % percent p p\\% italic_p %  and rejecting the rest. We denote the accepted audios as  D syn acc superscript subscript D syn acc \\mathcal{D}_{\\text{syn}}^{\\text{acc}} caligraphic_D start_POSTSUBSCRIPT syn end_POSTSUBSCRIPT start_POSTSUPERSCRIPT acc end_POSTSUPERSCRIPT  and the rejected ones as  D syn rej superscript subscript D syn rej \\mathcal{D}_{\\text{syn}}^{\\text{rej}} caligraphic_D start_POSTSUBSCRIPT syn end_POSTSUBSCRIPT start_POSTSUPERSCRIPT rej end_POSTSUPERSCRIPT . The CLAP model is pre-trained on  D a-c subscript D a-c \\mathcal{D}_{\\text{a-c}} caligraphic_D start_POSTSUBSCRIPT a-c end_POSTSUBSCRIPT  and fine-tuned on  D small subscript D small \\mathcal{D}_{\\text{small}} caligraphic_D start_POSTSUBSCRIPT small end_POSTSUBSCRIPT  to adapt to the target dataset. Example captions are in Table  6 , and prompts are in Appendix  A.2 .",
            "Self-Reflection.  For the rejected audios in  D syn rej superscript subscript D syn rej \\mathcal{D}_{\\text{syn}}^{\\text{rej}} caligraphic_D start_POSTSUBSCRIPT syn end_POSTSUBSCRIPT start_POSTSUPERSCRIPT rej end_POSTSUPERSCRIPT , we prompt the LLM to reflect on its generated captions and revise them to better align with the target label. Precisely, we feed the LLM with the original caption of each rejected audio along with extracted components from all accepted captions in  D syn acc superscript subscript D syn acc \\mathcal{D}_{\\text{syn}}^{\\text{acc}} caligraphic_D start_POSTSUBSCRIPT syn end_POSTSUBSCRIPT start_POSTSUPERSCRIPT acc end_POSTSUPERSCRIPT  and task it to rewrite the rejected captions. The revised captions are then used to generate new audio, which is again filtered using CLAP. Audios that meet the threshold are added to  D syn acc superscript subscript D syn acc \\mathcal{D}_{\\text{syn}}^{\\text{acc}} caligraphic_D start_POSTSUBSCRIPT syn end_POSTSUBSCRIPT start_POSTSUPERSCRIPT acc end_POSTSUPERSCRIPT ; those that do not are returned to  D syn rej superscript subscript D syn rej \\mathcal{D}_{\\text{syn}}^{\\text{rej}} caligraphic_D start_POSTSUBSCRIPT syn end_POSTSUBSCRIPT start_POSTSUPERSCRIPT rej end_POSTSUPERSCRIPT . This process repeats for  i i i italic_i  iterations or until  D syn rej superscript subscript D syn rej \\mathcal{D}_{\\text{syn}}^{\\text{rej}} caligraphic_D start_POSTSUBSCRIPT syn end_POSTSUBSCRIPT start_POSTSUPERSCRIPT rej end_POSTSUPERSCRIPT  is empty. We illustrate examples of generated and revised captions in Table  6 .",
            "Fig.  6 ,   7 ,   8  and   9  illustrate all the prompts used in our experiments. For all experiments, we prompt GPT-4-Turbo (GPT-4-turbo-2024-04-09) with top-p=0.5 and temperature=0.7.",
            "Table  6  presents examples of captions generated by the Synthio framework, along with their revised versions for captions that were initially rejected."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "A1.T7.3",
        "footnotes": [],
        "references": [
            "where  p   ( x T ) = N  ( x T ; 0 , I ) subscript p  subscript x T N subscript x T 0 I p_{\\theta}(x_{T})=\\mathcal{N}(x_{T};0,I) italic_p start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) = caligraphic_N ( italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ; 0 , italic_I ) . At inference time, the diffusion model iteratively executes the reverse process (Eq.  17 )  T T T italic_T  times starting from a randomly sampled Gaussian Noise  (   N  ( 0 , I ) ) similar-to italic- N 0 I (\\epsilon\\sim\\mathcal{N}(0,\\mathbf{I})) ( italic_  caligraphic_N ( 0 , bold_I ) ) .",
            "Fig.  6 ,   7 ,   8  and   9  illustrate all the prompts used in our experiments. For all experiments, we prompt GPT-4-Turbo (GPT-4-turbo-2024-04-09) with top-p=0.5 and temperature=0.7.",
            "Table  7  presents the performance comparison of Synthio on the full original dataset splits (where the entire training set is used without any downsampling). While Synthio outperforms all baselines, traditional augmentation methods prove to be much more competitive in this scenario. This contrasts with the results in Table  1  where traditional augmentations showed minimal improvements in performance."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "A1.T8.1.1",
        "footnotes": [],
        "references": [
            "Fig.  4  compares the distributions of pitch and various spectral features between generated audios in  D syn subscript D syn \\mathcal{D}_{\\text{syn}} caligraphic_D start_POSTSUBSCRIPT syn end_POSTSUBSCRIPT  and real audios in  D small subscript D small \\mathcal{D}_{\\text{small}} caligraphic_D start_POSTSUBSCRIPT small end_POSTSUBSCRIPT  across different methods on the USD8K and NSynth datasets. The features analyzed include Pitch Salience (clarity of the main pitch)  (Ricard,  2004 ) , Spectral Flatness (tonal vs. noise-like quality)  (Peeters,  2004 ) , Flux (rate of spectral change)  (Tzanetakis & Cook,  1999 ) , and Complexity (level of sound detail)  (Laurier et al.,  2010 ) . Notably, Synthio-generated audios closely replicate the spectral features of the original audios, showing the best alignment among all methods and demonstrating Synthios ability to generate consistent augmentations. Table  2  presents CLAP similarity scores between ground-truth audios and their  N N N italic_N  generated augmentations, averaged across all dataset instances. Audios generated with Synthio achieve the highest compositional diversity for generated audios among all baselines. Table  8  shows that audios generated using Synthio have the highest similarity with the ground-truth category label.",
            "Fig.  6 ,   7 ,   8  and   9  illustrate all the prompts used in our experiments. For all experiments, we prompt GPT-4-Turbo (GPT-4-turbo-2024-04-09) with top-p=0.5 and temperature=0.7."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "A1.T9.4.4",
        "footnotes": [],
        "references": [
            "Fig.  6 ,   7 ,   8  and   9  illustrate all the prompts used in our experiments. For all experiments, we prompt GPT-4-Turbo (GPT-4-turbo-2024-04-09) with top-p=0.5 and temperature=0.7.",
            "Table  9  presents a comparison of audio generation results across several evaluation metrics. We evaluate our trained Stable Diffusion model (used in our experiments, including a version further fine-tuned on AudioCaps) against other available models and baselines from the literature. Notably, our model performs competitively with other fully open-source models across most metrics."
        ]
    },
    "id_table_10": {
        "caption": "",
        "table": "A1.T10.1",
        "footnotes": [],
        "references": [
            "To offer an alternative perspective on the distributional consistency between the generated augmentations and the ground-truth small-scale dataset, we compare the Frechet Audio Distance (FAD) scores  (Kilgour et al.,  2018 ) . For this experiment, we use Synthio with Template Captions. Table  10  presents a comparison of FAD scores between Synthio and other baselines. Synthio achieves the highest FAD score, indicating that it produces the most consistent audio augmentations."
        ]
    }
}