{
    "id_table_1": {
        "caption": "Table 1 :  The configurations of DVP and the baseline models.",
        "table": "S4.T1.4",
        "footnotes": [],
        "references": [
            "DVP is a neurosymbolic architecture comprising three components (Fig.  1 ):",
            "The DSL allows expressing a number of programs. For the purpose of this study, we are interested in programs with the signature Latent    \\rightarrow   Scene, where the result is passed to the Renderer. One of the programs used in the experiments in Sec.  4  is shown in Fig.  1  and has the form:",
            "We consider two categories of Perception modules (backbones), i.e. subnetworks that map the raster image to a fixed-dimensional latent vector (Sec.  2 ): pre-trained and not pre-trained ones (Table  1 ). Our pre-trained architecture of choice is ConvNeXt-B  [ 17 ] , a large modern model that proved very effective at many computer vision tasks  [ 9 ] . In the non-pretrained case, Perception is trained from scratch alongside the rest of the model. For this variant, Perception is a 6-layer CNN (CNN1) followed by an MLP (see  Appendix 1  for details).",
            "Our baseline model is MONet  [ 2 ] , outlined in Sec.  3 . To provide for possibly fair comparison, we devise its pre-trained and non-pretrained variant: in the former, we combine it with ConvNeXt-B serving as part of the feature extraction backbone network (the counterpart of Perception in DVP); in the latter it is the original CNN used in MONet (CNN2 in Table  1 ).",
            "Perception module . The perception module is composed of CNN used for feature extraction and a submodule used for mapping those features to latent vector  z z z italic_z  (Fig.  1 )."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Reconstruction accuracy of DVP and the baselines. The best results are highlighted in bold.",
        "table": "S4.T2.4",
        "footnotes": [],
        "references": [
            "This paper is organized as follows. Section  2  presents DVP. Section  3  discusses the related work. Section  4  covers the results of extensive experimenting with DVP and its juxtaposition with several baseline architectures. Section  5  concludes the paper and points to possible future research directions.",
            "Each learnable function contains a separate dense neural network (MLP) that determines the outcome of functions calculation in direct or indirect manner. The direct mode is used by all but the last function in the above list. For instance, Rotation is an MLP with the number of inputs equal to the dimensionality of  z z z italic_z  and two outputs that encode the perceived objects predicted rotation angle   italic- \\phi italic_  as  ( cos   , sin   ) italic- italic- (\\cos\\phi,\\sin\\phi) ( roman_cos italic_ , roman_sin italic_ ) ; Scaling is an MLP that returns a single positive scalar to be interpreted as a magnification factor of the object; DescribeShape contains an MLP that learns to retrieve the shape of the input object observed by Perception and conveyed by the latent  z z z italic_z , and encode it as a vector of Elliptic Fourier Descriptors (EFD,  [ 15 ] ), detailed in  Appendix 2 . Technically, the output of the MLP is interpreted as real and imaginary parts of complex coefficients that form the spectrum which is mapped with inverse Fourier Transform to objects contour represented as complex time series  ( x t , y t ) subscript x t subscript y t (x_{t},y_{t}) ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) .",
            "Configurations of DVP and baselines . We compare DVP architectures that feature two types of DSL programs, those based on  direct  inference of the object shape from the latent (DVP-D), and those based on object  prototypes  (DVP-P). In the former, we employ the program  P P P italic_P  presented in Sec.  2 . In DVP-P, we replace in  P P P italic_P  the Describe( z z z italic_z ) call with Prototype( z z z italic_z ).",
            "We consider two categories of Perception modules (backbones), i.e. subnetworks that map the raster image to a fixed-dimensional latent vector (Sec.  2 ): pre-trained and not pre-trained ones (Table  1 ). Our pre-trained architecture of choice is ConvNeXt-B  [ 17 ] , a large modern model that proved very effective at many computer vision tasks  [ 9 ] . In the non-pretrained case, Perception is trained from scratch alongside the rest of the model. For this variant, Perception is a 6-layer CNN (CNN1) followed by an MLP (see  Appendix 1  for details).",
            "In Table  2 , we juxtapose the test-set reconstruction accuracy of DVP with the reference configs using commonly used metrics: Mean Square Error (MSE), Structural Similarity Measure (SSIM,  [ 20 ] ), Intersection over Union (IoU), and Adjusted Rand Index (ARI 2 2 2 ARI measures the similarity of two clusterings by counting the numbers of pairs of examples that belong to the same/different clusters in them and adjusting those numbers for the odds of chance agreement. Here, the examples are pixels, and there are two clusters: the background and the foreground. ). While MSE is calculated directly from the RGB values of the input image and models rendering (scaled to the  [ 0 , 1 ] 0 1 [0,1] [ 0 , 1 ]  interval), IoU and ARI require the rendered pixels to be unambiguously assigned to objects or the background. We achieve that by forcing the models to render scenes with white objects on a black background, which results in binary masks representing the assignment of pixels 3 3 3 Our implementation reuses object masks produced in the RGB rendering process.  (in contrast to complete rendering, where the model controls also the colors).",
            "In Fig.  2 , we present the rendering of selected test-set examples produced by one of the DVP models (DVP-P  ) and compare it with one of the baselines (MONet  ). As the best renderings produced by all configurations are virtually indistinguishable from the input image, we focus on the worst cases, i.e. the 6 examples rendered with the largest MSE error by DVP-P  . For the models trained on 5% of data, the differences between the reconstructions produced by DVP and MONet can be traced back to their different operating principles: MONet is better at reproducing colors, but worse at modeling the shape of the objects. On the other hand, DVP can occasionally fail to predict the correct rotation of the object. For the models trained on 1% of data, DVP can mangle the shape, while MONet may struggle with figure-ground separation, producing incorrect masks that blur the object and the background. It is important to note that the examples with the largest MSE error contain large objects, as pixel-wise metrics roughly correlate with object size.",
            "Explanatory capacity . Figure  5  shows the visual representation of the prototypes formed by DVP-P   in its learnable 8-element embedding. The EFDs represent them as closed curves (Sec.  Appendix 2 ), which may occasionally coil (e.g. #5 and #6 at the bottom of Fig.  5 ). All presented models, including the one trained on just 1% of data, learned prototypes that correctly capture shape categories. The remaining embedding slots contain random curves, used sparingly and contributing only marginally to the predicted shape, as evidenced by the normalized sum of embedding weights visualized in color. Models usually allocate a single embedding slot per category, except for hearts, for which they often form two prototypes. Given that these prototypes are rotated in opposite (or almost opposite) directions and used alternatively (notice lower weights), we posit that in the early stages of learning, the hearts prototype is an equilateral triangle, and Rotation co-adapts to the 120 invariance of this shape by generating a limited range of rotation angles. Once the prototype shape becomes more accurate, that invariance is lost, and it is easier to form a second prototype than to re-learn Rotate.",
            "By assigning labels to the identified categories, we can use a DVP model as a classifier that points to the predicted category with the  arg  max \\arg\\max roman_arg roman_max  over the outputs of the MLP in the Prototype function. We determined that all DVP models presented in Table  2 , when queried in this mode, achieve classification accuracy of 99.7% or more when queried on the test set.",
            "Out-of-sample generalization . To determine if the disentanglement of image formation aspects helps DVP to generalize well beyond the training distribution, we query selected variants of DVP and the baseline configurations on shapes from previously unseen categories: hourglass, triangle, L-shape. Table  3  and Fig.  4  summarize the quality of reconstruction. As expected, the metrics are worse than in Table  2 ; however, visual inspection of the reconstructed scenes reveals that DVP not only correctly models the background and the foreground colors, but also makes reasonably good predictions about object scale/size and orientation. Shape is the only aspect that is not modeled well enough. Interestingly, while DVP-D small  substantially outperforms DVP-D   on metrics, the latter is more faithful at reconstructing shape. Overall, the results confirm that DVP effectively disentangles the visual aspects also when faced with new types of objects.",
            "Discussion . While conventional DL models still maintain the upper hand when compared with DVP on pixel-wise metrics (Table  2 ), it is important to emphasize that the precise reconstruction of all minutiae of the image content is not the primary goal here. Reconstruction error serves here only as the guidance for the learning process, and in most use cases robust information about scene structure and composition will be of more value than attention to detail. Moreover, having a correctly inferred scene structure significantly facilitates further processing, like precise segmentation of individual objects with conventional techniques.",
            "Training DVP models in a generic way, i.e. starting from default random initialization of all parameters and using bare MSE as the loss function, leads on average to worse results than those reported in Table  2 . To attain the reported level of accuracy, DVP needs additional guidance, particularly in its prototype-based variant DVP-P. While these aspects are usually not critical for progress in training and its convergence, we cover them in this section for completeness."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :   Comparison of models on out-of-sample shape categories. The best results are highlighted in bold.",
        "table": "S4.T3.4",
        "footnotes": [],
        "references": [
            "This paper is organized as follows. Section  2  presents DVP. Section  3  discusses the related work. Section  4  covers the results of extensive experimenting with DVP and its juxtaposition with several baseline architectures. Section  5  concludes the paper and points to possible future research directions.",
            "Our baseline model is MONet  [ 2 ] , outlined in Sec.  3 . To provide for possibly fair comparison, we devise its pre-trained and non-pretrained variant: in the former, we combine it with ConvNeXt-B serving as part of the feature extraction backbone network (the counterpart of Perception in DVP); in the latter it is the original CNN used in MONet (CNN2 in Table  1 ).",
            "Robustness . Figure  3  presents how the metrics of the models trained on all data (100%) degrade with the increasing standard deviation   2 superscript  2 \\sigma^{2} italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  of normally distributed white noise added to pixels of test-set images. While MONet exhibits the best robustness on MSE, DVP is better on the qualitative metrics (IoU, SSIM and ARI), degrading more gracefully.",
            "Out-of-sample generalization . To determine if the disentanglement of image formation aspects helps DVP to generalize well beyond the training distribution, we query selected variants of DVP and the baseline configurations on shapes from previously unseen categories: hourglass, triangle, L-shape. Table  3  and Fig.  4  summarize the quality of reconstruction. As expected, the metrics are worse than in Table  2 ; however, visual inspection of the reconstructed scenes reveals that DVP not only correctly models the background and the foreground colors, but also makes reasonably good predictions about object scale/size and orientation. Shape is the only aspect that is not modeled well enough. Interestingly, while DVP-D small  substantially outperforms DVP-D   on metrics, the latter is more faithful at reconstructing shape. Overall, the results confirm that DVP effectively disentangles the visual aspects also when faced with new types of objects."
        ]
    }
}