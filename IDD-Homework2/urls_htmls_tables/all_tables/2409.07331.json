{
    "id_table_1": {
        "caption": "Table 1:  Model Performance on the OK-VQA dataset. Knowledge source abbreviations: C: ConceptNet; CC: CC12M; V2: VQA-2; W: Wikipedia; WD: WikiData; WIT: Wikipedia Image-Text; GS: Google Search; GI: Google Images; ICE: In-context Examples; PNP: Plug-and-Play VQA captioner  (Tiong et al.  2022 ) . In RACC- homo , the hyperMLLM and baseMLLM share the same structures and weights, while in RACC- hetero , they differ in either structure or weights. In the last two rows of results of the RACC- hetero , the hyperMLLM used is InstructBLIP-FlanT5XL.",
        "table": "Sx2.T1.1",
        "footnotes": [],
        "references": [
            "Building on the above setup, we propose RACC,  i.e.   R etrieval- A ugmented MLLM with  C ompress  C ontexts. In this section, we first delineate our frameworks overall workflow and then discuss the strategies we have designed to optimize it in accordance with the task-specific characteristics. The framework structure of RACC is depicted in Figure  1 .",
            "First of all, RACC outperforms many competitive baselines. The performance comparison of RACC and other competitive baselines on the OK-VQA dataset is presented in Table  1 . Based on InstructBLIP-FlanT5XL, RACC- homo  with GS as the knowledge source reaches an accuracy of 59.65%. With WIT as the knowledge source, our framework achieves 59.17%. When adopting RACC- hetero , with InstructBLIP-FlanT5XL as the hyperMLLM and InstructBLIP-Vicuna7B as the baseMLLM, we achieve a state-of-the-art (SOTA) accuracy of 62.9%."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  The results on the AOK-VQA dataset. We use the GS knowledge for AOK-VQA here.",
        "table": "Sx4.T2.1",
        "footnotes": [],
        "references": [
            "The results of AOK-VQA are shown in Table  2 . Since the GS knowledge source we use for AOK-VQA is not designed for it, the documents in GS may not provide the required knowledge for all questions in AOK-VQA. However, RACC- homo  based on InstructBLIP-FlanT5XL still achieves a state-of-the-art (SOTA) accuracy of 62.1% on the validation set. The performance on the test set is 58.1%."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Comparison of inference efficiency between RAVQA-v2 and RACC when adopting the WIT knowledge source. Eval time and Disk Space are metrics measured for a single image-question pair input, while VQA Accuracy represents the performance on the OK-VQA test set. w pre indicates that pre-saving the compressed prompts of retrieved documents before inference. The MLLM used in both two frameworks is InstructBLIP-FlanT5XL.",
        "table": "Sx4.T3.1",
        "footnotes": [],
        "references": [
            "The inference efficiency is the main concern of this paper.  RACC demonstrates significant advantages in inference efficiency compared to RAVQA-v2  (Lin et al.  2024a ) . RACC not only significantly reduces inference latency but also minimizes disk space usage by pre-saving compressed prompts corresponding to the documents of the knowledge source. We present a comparison of the inference efficiency between RACC and RAVQA-v2  (Lin et al.  2024a )  in Table  3 . When pre-saving compressed prompts, we achieve a substantial reduction of 59.7% in inference latency and 91.0% in disk space usage. Even without pre-saved compressed prompts, the inference latency can still be reduced by 22.0%."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  The results of ablation studies on the design of our aggregator module. The GS knowledge source is adopted here. The ablation experiments are conducted based on RACC- homo  with InstructBLIP-FlanT5XL.",
        "table": "Sx4.T4.1",
        "footnotes": [],
        "references": [
            "We propose four strategies to improve the aggregation process of compressed contexts and conduct ablation studies to verify their effectiveness. The settings and results of ablation studies are depicted in Table  4  and Table  5 . Note that we adopt RACC- homo  with the GS knowledge source in the ablation studies, where the hyperMLLM and baseMLLM are both initialized from InstructBLIP-FlanT5XL.",
            "Firstly, comparing lines 2 and 3, as well as lines 7 and 8, we can observe that the PIPE strategy brings improvements of 0.31% and 0.42% under different settings. From the difference between lines 3 and 4 in Table  4 , we observe that the DCVQ strategy brings an improvement of 0.77%. On the other hand, the RGCA strategy results in a performance gain of 0.37%, as shown in lines 3 and 5. Last but not least, the performance difference between lines 6 and 8 shows that the PRDB strategy leads to a performance gain of 0.54%."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  RACC- homo s results of the comparative experiments on the length of the predefined learnable prompts   v  q subscript  v q \\theta_{vq} italic_ start_POSTSUBSCRIPT italic_v italic_q end_POSTSUBSCRIPT  and   d subscript  d \\theta_{d} italic_ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT . w/o PIPE means that the learnable prompts are randomly initialized here.",
        "table": "Sx4.T5.2",
        "footnotes": [],
        "references": [
            "We propose four strategies to improve the aggregation process of compressed contexts and conduct ablation studies to verify their effectiveness. The settings and results of ablation studies are depicted in Table  4  and Table  5 . Note that we adopt RACC- homo  with the GS knowledge source in the ablation studies, where the hyperMLLM and baseMLLM are both initialized from InstructBLIP-FlanT5XL.",
            "We also explore how to set the length of learnable prompts ( i.e.  L  (  v  q ) L subscript  v q L(\\theta_{vq}) italic_L ( italic_ start_POSTSUBSCRIPT italic_v italic_q end_POSTSUBSCRIPT )  and  L  (  d ) L subscript  d L(\\theta_{d}) italic_L ( italic_ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) ), and the results are shown in Table  5 . We select the best configuration, setting  L  (  v  q ) L subscript  v q L(\\theta_{vq}) italic_L ( italic_ start_POSTSUBSCRIPT italic_v italic_q end_POSTSUBSCRIPT )  and  L  (  d ) L subscript  d L(\\theta_{d}) italic_L ( italic_ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT )  to 12 and 16, respectively. All other experiments in this paper are conducted using this configuration. In the supplementary materials, we provide additional results of ablation studies and comparative experiments on the hyperparameter  K K K italic_K ."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  RACC- hetero s experimental results on OK-VQA using different MLLMs as the baseMLLM. The hyperMLLM is fixed as InstructBLIP-FlanT5XL here.",
        "table": "Sx4.T6.1",
        "footnotes": [],
        "references": [
            "3. RACC can be applied to any off-the-shelf MLLMs. In the setup of RACC- homo , the hyperMLLM, and the baseMLLM are identical, which means that the MLLM learns to compress contexts for itself. For RACC- hetero , the hyperMLLM and the baseMLLM differ in either structure or weights. We conduct experiments under this setup and present the results in Table  6 . RACC- hetero  also performs well across different baseMLLMs. The setup of RACC- hetero  is also of practical significance:  When it is not feasible to directly fine-tune the baseMLLM due to resource constraints, our framework can still work by adopting a much smaller hyperMLLM to help produce P-Tuningv2 modulations to adapt the frozen baseMLLM."
        ]
    },
    "global_footnotes": []
}