{
    "PAPER'S NUMBER OF TABLES": 1,
    "S2.T1": {
        "caption": "Table 1: Approximation error in MSE with different decoder structures and training schemes, where nùëõn is the number of residual blocks in the decoding sub-network. With nùëõn increasing to 333 in the end-to-end training scheme, the error will not basically change any more.",
        "table": "<table id=\"S2.T1.7\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S2.T1.7.1.1\" class=\"ltx_tr\">\n<td id=\"S2.T1.7.1.1.1\" class=\"ltx_td ltx_border_l ltx_border_r ltx_border_t\"></td>\n<td id=\"S2.T1.7.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S2.T1.7.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">freezing decoder</span></td>\n<td id=\"S2.T1.7.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S2.T1.7.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">freezing encoder</span></td>\n<td id=\"S2.T1.7.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" colspan=\"3\"><span id=\"S2.T1.7.1.1.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">end-to-end</span></td>\n</tr>\n<tr id=\"S2.T1.7.2.2\" class=\"ltx_tr\">\n<td id=\"S2.T1.7.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S2.T1.7.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">n</span></td>\n<td id=\"S2.T1.7.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S2.T1.7.2.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">3</span></td>\n<td id=\"S2.T1.7.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S2.T1.7.2.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">3</span></td>\n<td id=\"S2.T1.7.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S2.T1.7.2.2.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">0</span></td>\n<td id=\"S2.T1.7.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S2.T1.7.2.2.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">3</span></td>\n<td id=\"S2.T1.7.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S2.T1.7.2.2.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">7</span></td>\n</tr>\n<tr id=\"S2.T1.7.3.3\" class=\"ltx_tr\">\n<td id=\"S2.T1.7.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S2.T1.7.3.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">MSE</span></td>\n<td id=\"S2.T1.7.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S2.T1.7.3.3.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.0112</span></td>\n<td id=\"S2.T1.7.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S2.T1.7.3.3.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.00232</span></td>\n<td id=\"S2.T1.7.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S2.T1.7.3.3.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.00210</span></td>\n<td id=\"S2.T1.7.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S2.T1.7.3.3.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.00166</span></td>\n<td id=\"S2.T1.7.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S2.T1.7.3.3.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.00168</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "The mean square error (MSE) was used to measure the approximation error. As shown in Table 1, it is observed that the end-to-end training produced lower approximation errors than solely training either the encoder or decoder. Moderately increasing the depth of the decoding sub-network can decrease the approximation error. However, the MSE becomes stable and will not change any more when the number nùëõn is larger than 333. The approximation error mainly stems from the lossy compression during encoding, which also indicates that the aggregation of the input data is hardly to be exactly recovered due to data encryption. Luckily, the main concern in the federated learning is the model accuracy that will not be affected by the slight approximation error."
        ]
    }
}