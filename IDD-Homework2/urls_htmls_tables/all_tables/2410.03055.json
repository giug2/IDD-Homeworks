{
    "id_table_1": {
        "caption": "TABLE I :    Results on the synthetic key-value dataset assuming a perfect recall retriever, which always retrieves the relevant items while also retrieving some irrelevant ones, with person ID as the key, and person SSN and DoB as values.   k k k italic_k NN-LM prediction uses   = 0.5  0.5 \\gamma=0.5 italic_ = 0.5 .  We report mean and one standard deviation for macro-averaged precision and recall.",
        "table": "S4.F6.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.p3.pic1.2.2.2.1.1",
        "footnotes": [],
        "references": [
            "In this paper, we propose a novel approach to propagate more  permissive  information-flow labels in LLM-based applications which we will refer to as an influence-based label propagator (LP).  The key idea of our approach is to propagate only the labels of the samples that were  influential  in generating the models outputand drop the labels of the inputs that are not required.  Specifically, for a given context and fixed tolerance    \\lambda italic_ , we identify all subcontexts for which the model achieves utility that is at most    \\lambda italic_  below the utility of the full context.  Within those subcontexts, we then select the one with the most permissive label.  We prove that, under idealizing assumptions, our algorithm identifies the most permissive label(s) possible.  An example of our approach is shown in Figure  1 .",
            "We demonstrate the effectiveness of an influence-based label propagator by evaluating it on three datasets:        (i)   a synthetic dataset containing specific personal details testing the label propagators ability to handle a large number of inputs,      (ii)   a news article dataset assessing whether the LP is able to handle long free-form natural language, and      (iii)   a dataset consisting of LLM agent conversations as depicted in Figure  1 .",
            "However, the LLM does not necessarily need all documents in the context to generate the output.  In the example in Figure  1 , the LLM did not need the web search result from the untrusted website. Labeling the output as untrusted would be overly pessimistic and possibly inhibit the system from using the output further, e.g., as the input to another tool that requires trusted data.",
            "Definition  1  leaves the choice of the utility function  U U U italic_U  and the language model  p LM  ( y | x , C ) subscript p LM conditional y x C p_{\\text{LM}}(y|x,C) italic_p start_POSTSUBSCRIPT LM end_POSTSUBSCRIPT ( italic_y | italic_x , italic_C )  unspecified, as different applications require custom choices of these functions.  In this paper, we focus on language modeling where  perplexity  is a common way to measure utility  [ 25 ] .  Hence, for the remainder of this paper, we compute utility as the negative perplexity:",
            "Algorithm  1  describes this idea in pseudocode.  We represent the powerset of possible labels as a directed acyclic graph (DAG) where nodes represent labels and edges represent the lattice order   square-image-of-or-equals \\sqsubseteq   (see Figure  2  for an illustration).  Starting from the root node  L =  c  C l  ( c ) L subscript square-union c C l c L=\\bigsqcup_{c\\in C}\\ell(c) italic_L =  start_POSTSUBSCRIPT italic_c  italic_C end_POSTSUBSCRIPT roman_l ( italic_c )  that corresponds to the full context, we traverse the DAG depth-first to identify    \\lambda italic_ -similar labels.  For each label  L L L italic_L , the function  minimal_labels( )  returns    \\Lambda roman_ , the set of    \\lambda italic_ -similar labels at or below  L L L italic_L  (i.e., at least as permissive as  L L L italic_L ).",
            "The set of labels returned by Algorithm  1  is minimal in that elements are pairwise incomparable with respect to the lattice order, i.e., no label is more restrictive than the other (and hence redundant).  The algorithm achieves this by recursing on each child with a more permissive but    \\lambda italic_ -similar label than the parent node, and adding a nodes label only if there is no such child.  If there is a total order over the labels (i.e., all elements are pairwise comparable) then only a single label is returned ( |  | = 1  1 |\\Lambda|=1 | roman_ | = 1 ).  Note that the labels do not need to form a tree, so Algorithm  1  may visit a node multiple times.  This can be avoided by keeping track of visited nodes, which we forgo for simplicity of presentation.",
            "Algorithm  1  improves over the naive solution by iterating only over full  L L L italic_L -subcontexts (and not over all subsets). If a child label  L  superscript L  L^{\\prime} italic_L start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is not    \\lambda italic_ -similar, we prune the search assuming that also none of the children of  L  superscript L  L^{\\prime} italic_L start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  will be    \\lambda italic_ -similar.  If the models utility is  monotonous  in the context, as in",
            "i.e., adding more documents to the context never decreases the utility, we can guarantee that Algorithm  1  identifies all minimal labels.  Proposition  1  summarizes these guarantees.",
            "Algorithm  1  always terminates and returns a minimal set of    \\lambda italic_ -similar labels.  If the utility function is monotonous,  then Algorithm  1  returns  all  minimal    \\lambda italic_ -similar labels.",
            "As utility functions are not necessarily monotonous  [ 26 ] , Algorithm  1  is a heuristic in practice. In Section  V , we evaluate how closely it matches the statement in Proposition  1 .",
            "We briefly discuss how to integrate Algorithm  1  into existing model architectures and systems.  For this, we consider two architectures for augmenting language models with retrieved information: prompt-based retrieval augmentation  [ 19 ,  21 ]  and  k k k italic_k NN language models  [ 20 ] .",
            "Compute the pessimistic label  L L L italic_L  of  C C C italic_C , and run Algorithm  1  on  ( C , L , x , y ) C L x y (C,L,x,y) ( italic_C , italic_L , italic_x , italic_y )  to obtain a set    \\Lambda roman_  of labels that are    \\lambda italic_ -similar to  L L L italic_L .",
            "However, the number of LLM queries is always bounded by the size of the lattice.  When the lattice forms a totally ordered set (e.g. two-element lattices distinguishing between trusted vs untrusted or confidential vs public data), Algorithm  1  stops as soon as it finds a subcontext whose utility drops below a    \\lambda italic_  difference w.r.t. the utility of the full context.  For richer lattices describing more fine-grained security policies, Algorithm  1  visits a small subset of the lattice in typical queries, either because only a few labels are represented in the context or because the utility drops below the tolerance    \\lambda italic_ .  Therefore, computational costs are not a major concern for several fundamental lattices.",
            "While caching can save computation in processing prompt tokens, it does not impact the efficiency of the memory-bound decoding phase.  However, we can use an algorithm-specific optimization to decode only a subset of the tokens that a naive implementation of Algorithm  1  would decode.  Since we use negative perplexity as the utility metric, during the decoding phase of an LLM call, we can keep a running calculation of the cumulative likelihood of the tokens decoded so far and stop decoding as soon as the utility drops below a difference    \\lambda italic_  of the utility of the full context.",
            "The goal of the label propagator is to return all minimal    \\lambda italic_ -similar labels.  As shown in Proposition  1 , this is achieved under monotonicity assumptions that are typically not satisfied in practice.  Therefore, our evaluation aims to quantify how closely the empirical results match this goal.  Specifically, we aim to answer the following research questions:",
            "Research questions RQ1 and RQ2 focus on the performance of Algorithm  1  directly.  Research question RQ3 focuses on the quality of the final output of the end-to-end system introduced in Section  III-D .",
            "We now describe the metrics that we use to answer the aforementioned research questions.  Throughout, we use    \\Lambda roman_  to denote the set of labels returned by Algorithm  1  and    superscript   \\Lambda^{\\star} roman_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  as ground truth, i.e., the correct set of minimal labels.",
            "We use the target response in the dataset as the completion  y y y italic_y  for Algorithm  1 , i.e., we do  not  rely on the model for generating the output unless mentioned otherwise.  This is motivated by the fact that the target label computed in the dataset is only correct w.r.t. the target response.  Therefore, an incorrect completion  y y y italic_y  from the model would render the ground-truth label set    superscript   \\Lambda^{\\star} roman_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  incorrect.  We quantify the implications of this decision in Section  V-C , where we compare the difference in performance between the dataset target and the model-generated output.",
            "Algorithm  1  identifies sub-contexts with labels that are more permissive than that of the full context. However, whenever the desired output label  L L L italic_L  can be determined  up-front , it is possible to side-step the search over    \\lambda italic_ -similar sub-contexts and directly restrict the retrieval component to documents at or below  L L L italic_L .",
            "Lastly, our approach naturally allows users to  endorse  information.  Lets consider a setup as illustrated in Figure  1 .  Initially, there might only be an untrusted source answering the initial query.  The LP system would correctly output an untrusted label to the potentially dangerous command.  However, after a trusted authority (e.g., a university department IT admin) confirms the suggestion in the untrusted source, the LP recognizes that both sources are similarly influential and assign a trusted label to the output.  Therefore, by quoting or repeating untrusted information a trusted source can  endorse  the information.",
            "Our main focus has been on predicting the least conservative label while ensuring that utility is not compromised beyond a certain threshold    \\lambda italic_  ( Definition 1 ).  However, our proposal is flexible enough to accommodate various use cases.  For instance, it can be reversed to determine the utility for a specific target label.  Alternatively, a more complex use case would be to make the system dynamic with respect to    \\lambda italic_ , which means the system is able to compromise more utility to achieve a better or less conservative label.",
            "Algorithm  1  relies only on    \\lambda italic_  to produce the minimal set of labels.  However, in the cases where documents share similarities and provide important information regarding the model output such as the format (which is the case for our synthetic key-value dataset  IV-E ), the overall contribution of irrelevant documents goes up.  Therefore, additional irrelevant documents are introduced in the predicted label set due to being    \\lambda italic_ -similar.",
            "We visualize the results for the 2D grid search by considering both perplexity tolerance    \\lambda italic_  as well as Shapley threshold on the synthetic key-value dataset in Figure  10 , and the news article dataset in Figure  11 ."
        ]
    },
    "id_table_2": {
        "caption": "TABLE II :    Results on the news article dataset with prompt-based label search.  The perfect retriever row assumes that the retriever always retrieves the relevant items while also retrieving some irrelevant ones randomly.  The realistic retriever uses a cosine similarity-based nearest-neighbour search and may not retrieve all relevant items, thereby reducing the label improvement metric of even a perfect label search.",
        "table": "S5.T1.14",
        "footnotes": [],
        "references": [
            "Product lattices.  Figure  2  shows an example of a product lattice for the case of two dimensions: reliability ( HiInt , LoInt HiInt LoInt \\textnormal{HiInt},\\textnormal{LoInt} HiInt , LoInt ) and timestamps ( LastMonth , LastWeek , Today LastMonth LastWeek Today \\textnormal{LastMonth},\\textnormal{LastWeek},\\textnormal{Today} LastMonth , LastWeek , Today ).  The top of the lattice  ( LoInt , LastMonth ) LoInt LastMonth (\\textnormal{LoInt},\\textnormal{LastMonth}) ( LoInt , LastMonth )  represents the least reliable and least recent documents, while the bottom of the lattice  ( HiInt , Today ) HiInt Today (\\textnormal{HiInt},\\textnormal{Today}) ( HiInt , Today )  represents the most reliable and most recent documents.",
            "Algorithm  1  describes this idea in pseudocode.  We represent the powerset of possible labels as a directed acyclic graph (DAG) where nodes represent labels and edges represent the lattice order   square-image-of-or-equals \\sqsubseteq   (see Figure  2  for an illustration).  Starting from the root node  L =  c  C l  ( c ) L subscript square-union c C l c L=\\bigsqcup_{c\\in C}\\ell(c) italic_L =  start_POSTSUBSCRIPT italic_c  italic_C end_POSTSUBSCRIPT roman_l ( italic_c )  that corresponds to the full context, we traverse the DAG depth-first to identify    \\lambda italic_ -similar labels.  For each label  L L L italic_L , the function  minimal_labels( )  returns    \\Lambda roman_ , the set of    \\lambda italic_ -similar labels at or below  L L L italic_L  (i.e., at least as permissive as  L L L italic_L )."
        ]
    },
    "id_table_3": {
        "caption": "TABLE III :    Results on the LLM agent dataset.  The utility target column indicates which output  y y y italic_y  we are using to compute our target utility  U  ( p LM  ( y | x , C ) ) U subscript p LM conditional y x C U(p_{\\textnormal{LM}}(y|x,C)) italic_U ( italic_p start_POSTSUBSCRIPT LM end_POSTSUBSCRIPT ( italic_y | italic_x , italic_C ) )  in equation  1 .  In practice and in the absence of ground truth, the utility target is the model-generated output  y y y italic_y , but we also compare with the ground truth  y  superscript y  y^{\\star} italic_y start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  to understand the maximum possible utility assuming a perfect LLM.  The difference in the numbers between the cases indicates that all the errors in the case of model-generated output are artifacts of the model generating a suboptimal response which causes incorrect propagation of labels.",
        "table": "S5.T2.2",
        "footnotes": [],
        "references": [
            "In practice, our algorithm is an AI-based heuristic that can make mistakes or be misled adversarially (e.g., failing to identify an influential document, or over-estimating the influence of a document).  However, these mistakes do not affect the  safety  of the propagated labels.  By rerunning the model on the new context  C | L  C_{|L^{\\prime}} italic_C start_POSTSUBSCRIPT | italic_L start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT end_POSTSUBSCRIPT  (step  3 ) and returning only  y  superscript y  y^{\\prime} italic_y start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  (step  4 ), our approach guarantees that the returned completion depends only on documents at or below  L  superscript L  L^{\\prime} italic_L start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT .  This safety property holds even in the case of adversarial input documents (e.g., prompt injection) because it is enforced by the system, rather than the model.",
            "Since each document has a unique label, it corresponds to a node at the bottom of the lattice (similar to the example shown in Figure  3 ).  Each set of documents (i.e., subset of the context) therefore corresponds to a unique element in the lattice.  The documents and question-answer pairs are designed such that the question can be answered from different combinations of subsets in the context.  Identifying all minimal labels corresponds to identifying all these subsets."
        ]
    },
    "id_table_4": {
        "caption": "TABLE IV :    Model output alignment with the dataset ground truth computed using ROUGE-L F-score on the LLM agent dataset.  Utility computation is only applicable to the prompt-based label propagator.  ROUGE-L ( y , y  ) y superscript y  (y,y^{\\star}) ( italic_y , italic_y start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT )  refers to the alignment of the full context output (no regeneration required), while ROUGE-L ( y  , y  ) superscript y  superscript y  (y^{\\prime},y^{\\star}) ( italic_y start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , italic_y start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT )  represents the alignment of the regenerated response after label improvement.  Interestingly, we see that the regenerated response is more aligned with the ground truth response suggesting that in some cases label propagation can improve utility.  ROUGE-L ( y  , y  )  limit-from superscript y  superscript y  (y^{\\prime},y^{\\star})- ( italic_y start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , italic_y start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) -  ROUGE-L ( y , y  ) y superscript y  (y,y^{\\star}) ( italic_y , italic_y start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT )  compares the difference in alignment between the full context and the subcontext of the inferred label.  When label improvement is possible, the difference in alignment is close to nil indicating that the regenerated response  y  superscript y  y^{\\prime} italic_y start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is highly aligned with the ground truth response  y  superscript y  y^{\\star} italic_y start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT .  Otherwise, the difference in alignment is highly negative, indicating a significant drop in model alignment with  y  superscript y  y^{\\star} italic_y start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT .",
        "table": "S5.T3.4",
        "footnotes": [],
        "references": [
            "In practice, our algorithm is an AI-based heuristic that can make mistakes or be misled adversarially (e.g., failing to identify an influential document, or over-estimating the influence of a document).  However, these mistakes do not affect the  safety  of the propagated labels.  By rerunning the model on the new context  C | L  C_{|L^{\\prime}} italic_C start_POSTSUBSCRIPT | italic_L start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT end_POSTSUBSCRIPT  (step  3 ) and returning only  y  superscript y  y^{\\prime} italic_y start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  (step  4 ), our approach guarantees that the returned completion depends only on documents at or below  L  superscript L  L^{\\prime} italic_L start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT .  This safety property holds even in the case of adversarial input documents (e.g., prompt injection) because it is enforced by the system, rather than the model.",
            "We create a set of key-value pairs that contain hypothetical individuals IDs as keys, and their Social Security Numbers (SSNs) and dates of birth (DoB) as values.  We randomly split, distribute, and replicate the key-value pairs across multiple documents, and we attach a unique label to each document.  We design the questions such that obtaining the correct answer requires identifying the different combinations of documents that contain all the necessary values.  An example of the generated documents and QA pairs is shown in Figure  4 .  In this example, the question can be answered with access to documents  A A A italic_A ,  B B B italic_B , and  C C C italic_C  or with access to documents  A A A italic_A  and  D D D italic_D .  Consequently, the label search should yield  { A  B  C = A  B  C , A  D = A  D } formulae-sequence A B C square-union A B C A D square-union A D \\{ABC=A\\sqcup B\\sqcup C,AD=A\\sqcup D\\} { italic_A italic_B italic_C = italic_A  italic_B  italic_C , italic_A italic_D = italic_A  italic_D } ."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "S5.T4.4",
        "footnotes": [],
        "references": [
            "Starting from an existing fake news dataset  [ 34 ] , we create pairs of high and low-integrity news articles that discuss similar topics to each other.  Contrary to the previous dataset, we focus on a simpler lattice but more complex language in this case.  Using GPT-4  [ 1 ] , we generated QA pairs based on these articles, where some of the answers depended only on the LoInt document, others only on the HiInt document, and some on both documents.  An example document pair and QA pair is shown in Figure  5 .  The dataset in total contains 240 document pairs of corresponding LoInt and HiInt documents, as well as 3465 QA pairs.  Compared to a naive label propagator, it is possible to improve the label on 647 of these QA pairs.  Since there is a total order in the lattice, we report label improvement metrics for this dataset."
        ]
    },
    "global_footnotes": [
        "Without loss of generality, we use the term",
        "to refer to an individual piece of textual data in the context. This could be a document, webpage, email, a previous output of the LLM in a multi-turn interaction, etc. In this work, we consider the context to be an unordered set."
    ]
}