{
    "id_table_1": {
        "caption": "Table 1:  Single-Turn classifier model performance on six in-domain training products and three out-of-domain evaluation products",
        "table": "S4.T1.1",
        "footnotes": [],
        "references": [
            "Given a support case subject, description, and product name, our system generates recommended solutions based on corpora of support documents. Our system consists of four major components as illustrated in Figure  1 : an encoder-only transformer  classifier , a  query generation  system, a  retriever  system, and an  answer generator  system.",
            "For our application, we prioritized correctly predicting single-turn cases (positive class) versus multi-turn cases (negative) emphasizing recall over precision. Table  1  presents the final fine-tuned model performance on the held-out evaluation set of the six products when using a classification threshold of 0.1. We found that performance varies widely depending on the product, ranging from F1 of 0.27 to 0.62 and recall from 0.75 to 0.98. The lower performance can be explained in part by the varying class imbalance across products (positive class proportion from 11% to 44%) as well as the products differing inter-annotator agreement (See Section 6). Despite this, the model still substantially outperforms random guessing of the classes. Hyper-parameters including batch size, learning rate, and dropout were determined based on a small grid search."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Comparison of BertScore F1 and ROUGE-L F1 for different models on query generation task. BertScore is based on roberta-large embeddings.    *Falcon-40B scores are not comparable to the other models because the prompt used was different, and it was used to create the initial questions that were validated or edited by SMEs to create the ground truth.",
        "table": "S4.T2.1.1",
        "footnotes": [],
        "references": [
            "In order to create a concise query that could be used by the retriever, we generated a single sentence question based on the case subject and description. Our experiments over various open-source generative models (Table  2 ) and model availability in the clients services led us to choose Mixtral-8x7b-Instruct as the model for query generation which reliably reproduced the ground truth queries despite being a relatively small model with no domain knowledge. Note that the results are skewed for Falcon-40B  (Almazrouei et al.  2023 )  as Falcon-40B generated the first pass of silver ground truth queries that were then edited by subject matter experts.",
            "Re-ranking this first set allows us to use fine-tuning to improve performance without maintaining a parallel set of indexes.  For this final task-specific fine-tuning stage, we used training data based on 1,430 questions with up to three matching passages per question extracted from documents identified in user interactions, together with negative examples found using BM25 search. The resulting IBM Slate-125M model was then distilled into the deployed IBM Slate-30M model. To evaluate its effectiveness, we present recall before and after re-ranking with Google Search as a baseline in Figure  2 .",
            "The major bottleneck in RAG systems is the retrieval component. As shown in Table  4 , when given the correct context, LLMs can typically generate responses that match the ground truth answers. However, we cannot expect to generate the correct answer if given the wrong contexts which happens for around 60% of the cases (Figure  2 ). For comparison, Google search limited to the corresponding domains indexed by the Milvus database performed worse at 30% R@3 compared to our method at 43% R@3 (See Figure  2 ). This implies, as other researchers have suggested, that the retrieval component in RAG is not a solved problem by any means.  (Petroni et al.  2024 ; Cuconasu et al.  2024 )"
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  AB testing human evaluation of retrieved links",
        "table": "S4.T3.1",
        "footnotes": [],
        "references": [
            "We also conducted an AB test in which support agents of two products were provided with a link retrieved by the tool and a link provided by a subject matter expert. The source of the link was randomized as Source A or Source B so that, for example, Source A could be either our tool or an SME for any given case. The support agents were asked to rate each link as shown in Table  3  and to pick the better of the two solutions.",
            "The tool is currently integrated into the ticketing system but silently deployed (not visible to agents) for testing purposes. We are currently working on refinements and integration and plan to deploy the system before the end of the year. We will incorporate feedback buttons for the tool once it is deployed online and visible to agents. In the customer support portal user interface, for a given case, support agents will see a suggested solution and link. This will include five-star ratings for accuracy and readability as well as a drop-down menu for feedback including the options: useful, somewhat useful, no useful suggestion, and need more client info. See Figure  3  for a working mock-up of the user interface."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Comparison of BertScore F1 and ROUGE-L F1 for different models performing the answer generation task. BertScore based on roberta-large embeddings",
        "table": "S4.T4.1",
        "footnotes": [],
        "references": [
            "To evaluate the answers, we used the subject matter experts annotated ground truth answers and ground truth documents verified to contain the answer to the question. We compared the answers generated by the answer extractor using the ground truth document to the ground truth answer using BertScore  (Zhang et al.  2020 )  and ROUGE-L F1. We evaluated different models and prompts to find the optimal combination and present the results of the models assessed in Table  4 . While BertScore (roberta-large) F1 is rather low (in practice it ranges between 0.85-0.95), ROUGE-L F1, traditionally a rather strict metric, shows promising results for Mixtral-8x7b-Instruct with a score of 0.41. Mixtral-8x7b-Instructs outperforms of GPT-4o, included as a baseline for larger models, in all three metrics, despite having substantially less parameters. Likewise, Granite-13B-Chat-v2 is not far behind GPT-4o despite its merely 13 billion parameters compared to GPT-4os rumored hundreds of billions or even trillions of parameters. This suggests that the RAG approach of smaller models leveraging retrieved context is a viable solution for IT incident resolution recommendation systems.",
            "The major bottleneck in RAG systems is the retrieval component. As shown in Table  4 , when given the correct context, LLMs can typically generate responses that match the ground truth answers. However, we cannot expect to generate the correct answer if given the wrong contexts which happens for around 60% of the cases (Figure  2 ). For comparison, Google search limited to the corresponding domains indexed by the Milvus database performed worse at 30% R@3 compared to our method at 43% R@3 (See Figure  2 ). This implies, as other researchers have suggested, that the retrieval component in RAG is not a solved problem by any means.  (Petroni et al.  2024 ; Cuconasu et al.  2024 )"
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Comparison of analytic rubric scores for different models on answer generation task.",
        "table": "S4.T5.1",
        "footnotes": [],
        "references": [
            "In Table  5 , we present the final score for each model as the average of human annotators scores across 40 question-answer pairs of which InstructLab-IT emerged as the best model. While Llama-3.1-8b-Instruct performed slightly better than GPT-4o, the improvement in results with InstructLab-IT was very noticeable over both models. This is especially significant considering the model sizes: GPT-4o (over 1 trillion parameters and 1.5 TB), Llama-3.1-8b-Instruct (8 billion parameters and 16 GB), and InstructLab-IT (7 billion parameters and 28 GB). These results signal that a smaller, domain-specific model tuned for a specific set of use cases may better meet client requirements."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Inter-Annotator Agreement: Proportion of labels that 3 annotators agreed on. Total N in parenthesis. For question and link labels, proportions only calculated based on cases for which all 3 annotators labeled as single turn and evaluated quality of corresponding question and link.",
        "table": "S6.T6.1",
        "footnotes": [],
        "references": [
            "Three SMEs labeled a subset of twenty cases to determine inter-annotator agreement. The results in Table  6  show that labeling cases as single vs. multi-turn is not a trivial task and for most products, SMEs disagreed widely. Of the cases in which all three annotators agreed to be single-turn, agreement on the question and link quality was better but still raises questions about the validity of the training and evaluation data. In particular, the low agreement of the provided links can be explained by the fact that more than one link can potentially solve the same question and so neither annotator is necessarily wrong. This suggests that for ground truth data, we should consider a list of correct links instead of a single ground truth link for each question. The low agreement of single-turn vs. multi-turn labels also potentially explains the lower performance of the classifier model if the model is attempting to learn from potentially conflicting information."
        ]
    },
    "global_footnotes": []
}