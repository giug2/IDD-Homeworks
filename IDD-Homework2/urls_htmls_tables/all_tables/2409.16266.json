{
    "id_table_1": {
        "caption": "TABLE I :  Performance comparison of ITA methods across three single-objective optimization (SOO) scenarios.   indicates higher values are better,  indicates lower values are better. Orange highlights indicate best results and those within 10% of the best.",
        "table": "S5.T1.4.1",
        "footnotes": [],
        "references": [
            "To address the above challenges, we propose  REBEL , an LLM-based method that introduces a rule-based learning approach to enhance ITA, as illustrated in Fig.  1 . LLMs have been utilized to generate and refine user-centric guidelines across various domains, producing driving policies aligned with human styles  [ 19 ,  14 ]  and patient-specific treatment plans in healthcare  [ 20 ] . In our framework, LLMs generate and iteratively update allocation rules for each single objective specified by interacting with the environment and testing different ITA strategies, while storing experiences for future reference. We then utilize Retrieval-Augmented Generation (RAG) to dynamically retrieve instructional rules and prior experiential data for either single-objective optimization (SOO) or MOO as additional contextual information. REBEL benefits from this learning-retrieval pattern, enabling it to handle unseen scenarios, reflect human preferences, and accommodate last-minute changes in team composition. The primary contributions of this work are as summarized follows:",
            "The  Rule Generation  stage is the cornerstone of the  KA  phase. This stage aims to generate prescriptive objective-specific rules that guide the ITA process within MH-MR teams. Aside from providing background information,  G , B G B G,B italic_G , italic_B  in Lines 1, 2 in Algorithm  1 , rules are created through zero-shot prompting of the LLM agent, which draws upon its pre-trained knowledge to generate task-specific rules based on mission objectives. Following rule generation, the LLM engages in self-reflection by assessing the outcomes from the  Experience Generation  stage, allowing it to refine and improve the rules iteratively.",
            "The  Experience Generation  stage is where the LLM agent acquires practical knowledge by simulating various MH-MR scenarios. Each simulation introduces distinct mission objectives and dynamic environmental conditions, providing a rich context for the LLM to refine its decision-making capabilities. The primary goal of this stage is to enable the LLM to gain hands-on experience in applying the rules generated in Stage 1, as well as adapting to real-world complexities such as task difficulty, changes in team composition, and unforeseen operational challenges. Only the rules  R R R italic_R  relevant for objective  o o o italic_o  are utilized in each mission as shown in Algorithm  1 .",
            "The  Inferencing  stage evaluates the LLMs ITA performance once it has been equipped with both the rules and experience data from the previous stages, as seen in Algorithm  1 . First, the user provides an instruction  I I I italic_I , which consists of the first three sections of the SPF, to REBEL. To maximize the utility of the acquired knowledge, we implement two distinct RAG workflows. The first workflow is managed by the Rule-based Learning RAG module, which embeds the user-specified objectives from the instruction, retrieves the most relevant rules  R I subscript R I R_{I} italic_R start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT  using an Ensemble Retriever  [ 23 ] , and appends these rules to the LLMs prompt. The Ensemble Retriever combines Sparse and Dense retrieval methods to capture both keyword-based and semantic similarities, ensuring the most contextually appropriate rules are retrieved."
        ]
    },
    "id_table_2": {
        "caption": "TABLE II :  Task Performance comparison of methods in situational awareness settings.",
        "table": "S5.T1.4.1.1.1.2.1",
        "footnotes": [],
        "references": [
            "As shown in Fig.  2 , the REBEL framework for ITA in MH-MR teams consists of three stages:  Rule Generation ,  Experience Generation , and  Inferencing . The first two stages comprise the  Knowledge Acquisition (KA)  phase, where objective-specific rules are created and refined. Additionally, experience data is generated by recording the simulation outputs of an LLM agents ITA plans. This acquired knowledge serves as a valuable resource for decision-making. In the  Inferencing  stage, the same LLM agent, or a new one adapted to specific scenarios provided by the user, dynamically leverages this knowledge using RAG  [ 21 ] . This enhances decision-making, enabling better SOO performance, adaptation to user preferences in MOO, and an improvement in the systems ability to respond to situational changes in team composition."
        ]
    },
    "id_table_3": {
        "caption": "",
        "table": "S5.T1.4.1.1.1.3.1",
        "footnotes": [],
        "references": [
            "For the experiments, we utilized a benchmark simulation environment for ITA in MH-MR teams  [ 12 ] . The environment spans a  2  k  m  2  k  m 2 k m 2 k m 2~{}km\\times 2~{}km 2 italic_k italic_m  2 italic_k italic_m  area requiring hazard surveillance by an MH-MR team and is populated with multiple POIs. Each POI corresponds to a building, and the color of each building indicates its inherent complexity level for hazard evaluation. A visual representation of this environment is provided in Fig.  3 ."
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "S5.T1.4.1.1.1.4.1",
        "footnotes": [],
        "references": [
            "Fig.  4  showcases the performance of different methods in MOO settings, with normalized values for each objective, task performance (TP), mission time (MT), and human workload (HW), to illustrate preference alignment. The blue, orange, and green lines represent scenarios where TP, MT, and HW were prioritized by users, respectively. The actual prioritized objectives for each method are highlighted in a dotted red box.",
            "From the results, we observe that REBEL exhibits superior sensitivity to user-specified objective preferences compared to the baselines. When a particular objective is prioritized, REBEL consistently aligns the performance of that objective with the users preferences, as demonstrated by the highest normalized performance value (1.0) in the blue, orange, and green lines for TP, MT, and HW, respectively. This alignment is particularly clear in the red boxed regions in Fig.  4 , where REBEL demonstrates its ability to meet user-defined goals effectively."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "S5.T2.4",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": []
}