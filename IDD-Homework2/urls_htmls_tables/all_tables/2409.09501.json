{
    "id_table_1": {
        "caption": "Table 1:  Encoder-Only Models and Their Fine-tuned Datasets",
        "table": "S2.T1.1",
        "footnotes": [
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Existing systems generate clinical letters primarily by integrating structured data, while there are not many studies on how to use Natural Language Generation ( NLG ) models for this task  [ HUSKE-KRAUS, 2003 ,  Amin-Nejad et al., 2020a ,  Tang et al., 2023 ] . NLG attempts to combine clinical knowledge with general linguistic expressions and aims to generate clinical letters that are both readable and medically sound. However, NLG technology is not yet mature enough for widespread use in healthcare systems. Additionally, it faces numerous challenges, including medical accuracy, format normalisation, and de-identification  [ HUSKE-KRAUS, 2003 ] . Therefore, this investigation focuses on how NLG technology can be used to generate reliable and anonymous clinical letters, which can benefit medical research, clinical education, and clinical decision-making. The main aim of our work is to  generate de-identified clinical letters  that can  preserve clinical information  while  differing from the original  letters. A brief example of our objective is shown in Figure  1 . Based on this objective, different generation models will be explored as a preliminary attempt. Then we select the best models and try various techniques to improve the quality of the synthetic letters. The synthetic letters are evaluated not only with quantitative and qualitative methods, but also in downstream tasks, i.e., Named Entity Recognition (NER). We hope this work will contribute to addressing the challenge of insufficient data in the clinical domain.",
            "Table  1  summarises the  encoder-only models  used in our work and their corresponding fine-tuning datasets.",
            "As shown in Fig  10 , each raw letter is split into sentences first. We used the pre-trained models provided by the NLTK library, which combines statistical and machine-learning approaches to identify sentence boundaries. Each clinical letter is treated as a separate processing unit, with the first sentence automatically assigned to the first text block (chunk). To control the length of each chunk, we set a  maximum line count  parameter (max_lines). If the first sentence already meets the value of max_lines, the chunk will only contain this one sentence. Otherwise, subsequent sentences will be added to the chunk until the line count up to the max_lines.",
            "Extra care is needed when handling text with specific formats, such as medication dosage descriptions, as shown in Fig  11 . Because there is no clear sentence boundary, these sentences may exceed the tokens limitation. To address this, we first check whether the sentence being processed exceeds the token limit (max_tokens). If it does not, the sentence will be added to the current chunk. Otherwise, the sentence should be split into smaller chunks, each no longer than max_tokens. This operation helps  balance processing efficiency  while maintaining semantic integrity. In the example shown in  11 , although using line breaks to split the text seems to be more flexible, considering time complexity and the requirement to index the annotated entities, this method was not chosen.",
            "Since we aim to generate de-identified clinical letters that can preserve clinical narratives during masking and generation, it is necessary to extract certain features beforehand. We extracted the following features, with an example provided in Figure  12  and Table  5 .",
            "The process of generating synthetic letters with encoder-decoder models is very similar to that with encoder-only models. The difference is that, unlike the BERT family, which automatically masks tokens and replaces them with  <mask> , the T5 family models  do not have any built-in masking function . As a result, we identified the words that needed to be masked by  index  and removed them, which are represented as extra_id_x in the T5 family models. The text, with these words removed, was then used for the generation, which we refer to as  text with blanks . To maintain consistency in the format, we later replaced extra_id_x with  <mask>  when displaying the masked text. Additionally, the T5 family models  require a prompt  as part of the input. For this task, the complete input was structured as Fill in the blanks in the following sentence in the clinical background + text with blanks. In this project, we used T5-base  [ Raffel et al., 2020b ] , Clinical-T5-Base  [ Eric and Johnson, 2023 ,  Goldberger et al., 2000 ] , Clinical-T5-Sci  [ Eric and Johnson, 2023 ,  Goldberger et al., 2000 ] , and Clinical-T5-Scratch  [ Eric and Johnson, 2023 ,  Goldberger et al., 2000 ]  for comparison. The comparison of encoder-only and encoder-decoder model architectures is shown in Fig  13 .",
            "Both quantitative and qualitative methods will be used to evaluate the performance. Additionally, a downstream task (NER) is employed to assess whether the synthetic clinical letters can replace the original raw data. The evaluation methods pipeline is illustrated in Fig  14 .",
            "In this downstream NER task, as shown in Fig  15 , we initially extracted entities from letters using ScispaCy. Subsequently, these entities were used to train a base spaCy model. The trained model was then employed to extract entities from the testing set. Finally, we can compare these newly extracted entities with those originally extracted by ScispaCy, and the evaluation scores can be calculated. These steps were performed on both original clinical letters and synthetic letters, to assess whether the synthetic letters can potentially replace the original ones.",
            "As described in Section  3.1 , the dataset we used has been de-identified. All private information is replaced by three underscores ___. We hope that the synthetic clinical letters can maintain a certain degree of clinical integrity without revealing any private patient information. To address this, a post-processing step was added to the synthetic results. This process involves masking the three underscores (___) detected and using PLMs to predict the masked part again. For example, if the original text is ___ caught a cold. The post-processing result should ideally be John caught a cold or Patient caught a cold. Such synthetic clinical letters can better support clinical model training and teaching.",
            "In this section, we introduced the experimental design and subsequent implementation steps: from project requirement, data collection to environmental setup, pre-processing, masking and generating, post-processing, downstream NER task, and both qualitative and quantitative evaluation. An example of the entire process flow is shown in Figure  16 .",
            "The original sentence is displayed in Fig  17 . After the feature extraction, the resulting structure is shown in Fig  19 . As detailed in Table  6 , certain manually annotated entities are excluded from masking. The output of this masking process can be seen in Fig  18 .",
            "The results from medicalai/ClinicalBERT and Clinical-Longformer are shown in Fig  21  and Fig  22 . All three clinical-related models correctly predicted r from the input context. Medicalai/ClinicalBERT performs  comparably  to Bio_ClinicalBERT, despite adding an extra comma, which did not affect the texts clarity. However, Clinical-Longformers predictions, while understandable, were  repetitive  and less satisfactory. Importantly, none of these three models altered the original meaning.",
            "We first calculated representative quantitative metrics at the sentence level, matching the sample sentence used in Subsection  4.1 . This approach allows for a better integration of quantitative and qualitative evaluations. Although SMOG is typically suited for medical datasets, it is less appropriate for sentence-level analysis, so the Flesch Reading Ease was used here. The results are shown in table  7 .",
            "We observed that clinical-related encoder-only models generally outperform RoBERTa-base in qualitative evaluation (see Subsection  4.1 ). However, from the quantitative perspective, RoBERTa-base shows mediocre performance across most metrics except for BERTScore. In contrast, Bio_ClinicalBERT, despite no word overlap in this sample sentence, achieves a reasonable clinical context and the highest BERTScore among the models. Both Medicalai/Clinical BERT and Bio_ClinicalBERT excel in Flesch Reading Ease, likely because they tend to predict tokens with fewer syllables words that preserve the original meaning.",
            "Table  10  shows that the higher masking ratio, the lower the similarity (metrics scores) will be. As we expected, all evaluation values are higher than the baseline, but still below 1. This means the model can understand the clinical context and generate understandable text. It is surprising that with the masking ratio of 1.0, BERTScore increased from the baseline (0.29) to 0.63. Although this score is not very high, it still reflects that Bio_ClinicalBERT can generate clinical text effectively.",
            "In Table  11 , we calculated three  readability  metrics, which were mentioned in Section  3.5 . All these metrics have not shown significant changes from the original ones. However, it is strange that the SMOG and Flesh-Kincaid Grade are not always between the original baseline and mask baseline. When the masking ratio is high, the evaluation values even fall below both the masking and the original baseline. This may be because a  higher masking ratio leads to a lower valid prediction rate . If the predicted words include many spaces or punctuation marks, the readability will decrease obviously.",
            "In Table  12 , considering the perplexity, the masking baseline is very high, while the values for synthetic letters are close to the original ones. This indicates that the synthetic letters are useful for training clinical models. For information entropy, regardless of the masking ratio, it can  effectively preserve the amount of information . As for subjectivity, since all the values are close, we dont need to worry that the synthetic letters will be biased.",
            "As shown in Table  13 ,  inference time  for the entire dataset consistently ranges between 3 to 4 hours. However, it decreases with either very high or very low masking ratios. A mid-range masking ratio of approximately 0.6 results in longer inference times, likely because lower ratios reduce the number of masked tokens to process, while higher ratios provide less context, reducing the computational load. This lack of effective context also increases the invalid prediction rate. Conversely, with a masking ratio of 0, even a small number of prediction errors can significantly impact the overall accuracy due to the few masked tokens.",
            "As mentioned in Subsection  3.4.4 , we set max_lines as a variable and the max_tokens equal to 256. A series of increasing max_lines were tested until the average tokens per chunk reached a peak. We initially did this on a small sample (7 letters). The results are shown in Table  14 .",
            "As shown in Table  15 ,  the fewer nouns we mask, the better all these metrics perform . This trend is consistent with random masking. When the noun masking ratio is 1.0, meaning all nouns are masked, BERTScore increases from a baseline of 0.70 to 0.89. This means the  model predicted meaningful nouns . A similar trend is observed in the ROUGE scores. All evaluations are higher than the baseline but lower than 1. However, ROUGE scores show a smaller improvement than BERTScore. This may be because the model generates synonyms or paraphrases that retain the original meaning. With the increase in nouns masking ratio, the BERTScore decreases significantly.",
            "Table  16  shows a similar trend for masking verbs as observed with other masking strategies in standard NLG metrics. However, it is surprising that as the masking ratio increases, both the invalid prediction rate and NLG metrics decrease. This phenomenon can be attributed to two main reasons.  First , the model seems to prioritise predicting meaningful tokens (rather than punctuation, spaces, etc.) to generate coherent sentences. Contextual relevance is only considered after the sentence structure is complete. This may be due to the important role of verbs in sentences.  Second , the original raw data may contain fewer verbs than nouns. Therefore, the number of actual masking tokens changes slightly when verbs are masked, making the model less sensitive to them. This is also reflected in BERTScore. If all verbs are masked, the BERTScore remains high at 0.95, whereas if all nouns are masked, the BERTScore drops to 0.89.",
            "As mentioned in Subsection  3.4.3 , masking stopwords aims to reduce noise for model understanding while introducing variation in synthetic clinical letters. Table  17  shows that  masking only stopwords follows a similar trend to random masking , where a higher masking ratio leads to lower ROUGE Score and BERTScore. Additionally, the Invalid Prediction Rate is at its lowest with a medium masking ratio. This is because higher masking ratios always result in more information loss. On the other hand, lower masking ratios lead to fewer tokens being masked, which makes small prediction errors more influential. The results show an overall low Invalid Prediction Rate and high BERTScore, indicating that  stopwords have only a limited influence on the models understanding of context . This is not because the original raw letters contain very few stopwords. In fact, there are even more stopwords than nouns and verbs, as seen in sample texts.",
            "To further observe how different masking strategies influence the generation of clinical letters, we compared the results using the same actual masking ratios but with different strategies. In other words, the number of masked tokens is fixed, so the only variable is  the type of tokens being masked . Table  18  shows the results with a 0.04 actual masking ratio, and Table  19  shows the results with a 0.1 actual masking ratio.",
            "To further explore whether keeping entities is useful for our task, we compared our results with a baseline that does not retain any entities. The baseline was trained with four epochs of fine-tuning on our dataset. Specifically, 0.4 of nouns from all tokens were randomly masked during the baseline training. In contrast, in our experiments, only eligible tokensexcluding clinical informationwere selected for masking. The comparisons are shown in Table  21 .",
            "One example text without post-processing is shown in Fig  31 . After filling in the blanks, the results with BERT-base and Bio_ClinicalBERT are shown in Fig  32  and Fig  33  separately. We can see that both models can partially achieve the goal of making the text more complete. However, neither of them created a coherent story to fill in these blanks. They just used general terms like hospital and clinic. Perhaps other decoder-only models, more suitable for generating stories like GPT, could perform better and should be explored in the future."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  The T5 Family Models used in our work",
        "table": "S2.T2.1",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Some language models fine-tuned with T5 on specific datasets, such as SciFive (fine-tuned in some science literature)  [ Phan et al., 2021 ]  and ClinicalT5 (fine-tuned in clinical dataset MIMIC-III notes)  [ Lu et al., 2022 ] , have shown excellent performance in their respective fields. The  T5 family models  used in this project and their corresponding fine-tuned datasets are summarised in Table  2 .",
            "LT3  [ Belkadi et al., 2023 ]  uses an encoder-decoder architecture to generate synthetic text from labels. As shown in Fig  2 , labels such as medications are the input of the encoder, which can generate corresponding feature representations. The decoder generates prescription sequences based on these features. The pre-trained BERT tokenizer is used to split the input sequence into sub-words. LT3 is trained from scratch. Instead of using traditional greedy decoding, which may miss the global optimum, the authors proposed Beam Search Decoding with Backtracking ( B2SD ). This approach broadens the search range through a backtracking mechanism, preserving possible candidates for the optimal solution. To reduce time complexity, they used a probability difference function to avoid searching for low-probability words. Additionally, the algorithm penalises repeated sub-sequences and employs a logarithmic heuristic to guide the exploration of generation paths. The authors test LT3 on the 2018-n2c2 dataset, and evaluate the results using both quantitative metrics and downstream tasks. It was demonstrated that this model outperforms T5 in label-to-text generation.  3 3 3 LT3 has shown significant improvements over existing label-to-text generation models. Unfortunately, when we tried applying B2SD to generate clinical letters, the results were somehow disappointing. This may be due to the length of clinical letters, B2SD consumes a lot of time on long text generation. Despite this, it still shows great potential in generating clinical data.",
            "Due to the sensitivity of clinical information, many clinical datasets are not accessible. As mentioned in Section  2 , numerous studies use NLG techniques to generate clinical letters, and evaluate the feasibility of replacing the original raw clinical letters with synthetic letters. Most existing research involves fine-tuning PLMs or training Transformer-based models from scratch on their datasets through supervising learning. These studies explore different ways to learn the mapping from original raw text to synthetic text and work on generating synthetic data that are similar (or even identical) to the original ones. Our work, however, aims to find a method that can generate clinical letters that can  keep the original clinical story, while not exactly being the same as the original letters . To achieve this objective, we employed various models and masking strategies to generate clinical letters. The experiment will follow these steps:",
            "Since we aim to generate de-identified clinical letters that can preserve clinical narratives during masking and generation, it is necessary to extract certain features beforehand. We extracted the following features, with an example provided in Figure  12  and Table  5 .",
            "As discussed in Section  2.3  and Section  2.4 , decoder-only models struggle with processing long texts that require contextual understanding  [ Amin-Nejad et al., 2020b ] . Additionally, deploying them requires substantial computing resources and time. Therefore, we explored various pre-trained language models (PLMs), including both encoder-only and encoder-decoder models in this project. After evaluating their ability to generate synthetic letters from our dataset, we focused on  Bio_ClinicalBERT  - a well-performed model in our task, to experiment with different masking strategies. Additionally, from the discussion in Section  3.3 , we need to split the text into various-length-chunks. So the appropriate  length of these chunks  is also experimented with Bio_ClinicalBERT.",
            "As mentioned earlier, the primary method for this project involves masking and generation. We focused extensively on encoder-only models because of their advantage in bi-directional semantic comprehension. These encoder-only models, including BERT, RoBERTa, and Longformerdetailed in Section  2.3 were compared for their performance. Given the clinical focus of this task, we particularly explored model variants that were fine-tuned on clinical or biological datasets. However, as no clinically fine-tuned RoBERTa  [ Zhuang et al., 2021 ]  variant was available, the  RoBERTa-base  was used for comparisons. Specifically, the encoder-only models we explored include Bio_ClinicalBERT  [ Alsentzer et al., 2019 ] , medicalai/ClinicalBERT  [ Wang et al., 2023 ] , RoBERTa-base  [ Zhuang et al., 2021 ] , and Clinical-Longformer  [ Li et al., 2023 ] .",
            "The generated text using Bio_ClinicalBERT is displayed in Fig  20 . For management of open fracture, the model produced r, which is commonly used to denote right in clinical contexts, showing a relevant and logical prediction. Furthermore, the models input R ankle, despite not being in the figure due to space constraints, provided context for predicting r instead of left. Interestingly, the term admitted was generated even though it was not in the input, indicating the models understanding of clinical context. Although the phrase from 6 stairs, from home differs significantly from the original one, it remains contextually appropriate.",
            "The results from medicalai/ClinicalBERT and Clinical-Longformer are shown in Fig  21  and Fig  22 . All three clinical-related models correctly predicted r from the input context. Medicalai/ClinicalBERT performs  comparably  to Bio_ClinicalBERT, despite adding an extra comma, which did not affect the texts clarity. However, Clinical-Longformers predictions, while understandable, were  repetitive  and less satisfactory. Importantly, none of these three models altered the original meaning.",
            "The result generated by RoBERTa-base is shown in Fig  23 . While the generated text initially seems reasonable, the predicted word years shifts the focus to a temporal context, which was not intended. This is likely because RoBERTa is pre-trained on a general corpus and lacks sufficient clinical knowledge for accurate text generation, or it could simply be a coincidence based on this specific sentence, where RoBERTa-base inferred years from its training data.",
            "Additionally,  GPT-4o  was used for comparison, with the prompt Replace  <mask>  with words in the following sentence: . The results, shown in Fig  24 , are satisfactory. As discussed in Section  2.3 , decoder-only models excel in few-shot learning  [ Wu, 2024 ] , which is confirmed by this experiment. However, its performance may decline with long clinical letters  [ Amin-Nejad et al., 2020b ] .",
            "To further evaluate different PLMs in generating synthetic letters, we tested the T5 Family models. The generated results for the same sentence are shown in Fig  25 , Fig  26 , Fig  27 , and Fig  28 .",
            "T5-base performs the best among these tested models. However, the results are still  not fully rational , as it generated open is a ___ yo male. The other three models tend to use de-identification ( DEID ) tags to replace the masked words, as these tags are part of their corpora. Furthermore, the T5 family models may predict multiple words for each token, aligning with findings in Section  2.3",
            "In Table  12 , considering the perplexity, the masking baseline is very high, while the values for synthetic letters are close to the original ones. This indicates that the synthetic letters are useful for training clinical models. For information entropy, regardless of the masking ratio, it can  effectively preserve the amount of information . As for subjectivity, since all the values are close, we dont need to worry that the synthetic letters will be biased.",
            "There is a random selection when masking tokens at certain ratios. Masking different types of tokens will lead to different results, as shown in Fig  29  and Fig  30 . This variability is understandable since the encoder-only models use bidirectional attention, as mentioned in Section  2.3 .  These models need to predict the masked tokens based on the context . Therefore, it is necessary to experiment with different masking strategies based on the types of tokens. We used  POS  tagging and  stopwords  to observe how these strategies influence the quality of synthetic letters.",
            "As discussed in Subsection  4.2 , BERTScore should be the primary evaluation metric for our objective. Additionally, the invalid prediction rate is useful for assessing the models ability to generate informative predictions, and ROUGE scores help evaluate literal diversity. Therefore, these quantitative metrics, calculated using different masking strategies, will be shown in this section. Similar to Subsection  4.2 , we experimented with different masking ratios calculated from the eligible tokens (masked tokens divided by eligible tokens). The ratios are increased in increments of 0.1, ranging from 0.0 to 1.0. Due to space constraints, only metrics with increments of 0.2 will be shown here. A comparison with the same actual masking ratio (masked tokens divided by total tokens in the text) will also be presented in this subsection.",
            "After comparing different strategies with the same actual masking ratio, we explored hybrid masking strategies and compared them with other strategies at the same actual ratio. The results are shown in Table  20 . The first three columns have the same actual masking ratio. Masking only stopwords achieved the strongest performance among these strategies. However, when nouns are also masked along with stopwords, performance decreases, as masking nouns negatively affects the results. Despite this, it still performs better than random masking, indicating that stopwords have a greater influence than nouns. Next, we compared the last two columns. If 0.5 of nouns and 0.5 of stopwords are masked, adding an additional 0.5 of masked verbs leads to worse performance, showing that verbs also negatively influence the models performance.",
            "To further explore whether keeping entities is useful for our task, we compared our results with a baseline that does not retain any entities. The baseline was trained with four epochs of fine-tuning on our dataset. Specifically, 0.4 of nouns from all tokens were randomly masked during the baseline training. In contrast, in our experiments, only eligible tokensexcluding clinical informationwere selected for masking. The comparisons are shown in Table  21 .",
            "As shown in Table  22 , spaCy models trained on original and synthetic letters showed  similar evaluation scores . They even achieved F1 scores comparable to ScispaCys score of 0.843. Therefore, the unmasked context does not significantly influence model understanding. Consequently,  our synthetic letters can be used in NER tasks to replace real-world clinical letters, thereby further protecting sensitive information .",
            "One example text without post-processing is shown in Fig  31 . After filling in the blanks, the results with BERT-base and Bio_ClinicalBERT are shown in Fig  32  and Fig  33  separately. We can see that both models can partially achieve the goal of making the text more complete. However, neither of them created a coherent story to fill in these blanks. They just used general terms like hospital and clinic. Perhaps other decoder-only models, more suitable for generating stories like GPT, could perform better and should be explored in the future."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Extracted Entities and Their Details",
        "table": "S3.T3.1",
        "footnotes": [],
        "references": [
            "? ) compare the performance of the Vanilla Transformer and GPT-2 using the MIMIC-III dataset in seq2seq tasks. Specifically, they input a series of structured patient information as conditions, as shown in Fig  3 , to generate discharge summaries. They demonstrate that the augmented data outperforms the original data in downstream tasks (e.g. readmission prediction). Furthermore, they prove that Vanilla Transformer performs better with large samples, while GPT-2 excels in few-shot scenarios. However, GPT-2 is not suitable for augmenting long texts. Additionally, they used Bio_ClinicalBERT for the downstream tasks, and discovered that Bio_ClinicalBERT significantly outperformed the baseline model (BERT) in almost all experiments. It suggests that Bio_ClinicalBERT can potentially replace BERT in the biomedical field. Interestingly, although the synthetic data have a low score on internal metrics (such as ROUGE and BLEU), the performance on downstream tasks is notably enhanced. This may be because augmenting text can effectively introduce noise into the original text, improving the models generalisation to unseen data.",
            "Initially, we merged the clinical letters file and annotations file into a new DataFrame. The method is detailed in Appendix  LABEL:Appendix:_Method_for_Dataset_Merging_in_Preprocessing . After this, we extracted manually annotated entities based on their index. An excerpt from an original letter is shown in Fig  9 , and the manually annotated entities are listed in Table  3 .",
            "As discussed in Section  2.3  and Section  2.4 , decoder-only models struggle with processing long texts that require contextual understanding  [ Amin-Nejad et al., 2020b ] . Additionally, deploying them requires substantial computing resources and time. Therefore, we explored various pre-trained language models (PLMs), including both encoder-only and encoder-decoder models in this project. After evaluating their ability to generate synthetic letters from our dataset, we focused on  Bio_ClinicalBERT  - a well-performed model in our task, to experiment with different masking strategies. Additionally, from the discussion in Section  3.3 , we need to split the text into various-length-chunks. So the appropriate  length of these chunks  is also experimented with Bio_ClinicalBERT.",
            "As mentioned earlier, the primary method for this project involves masking and generation. We focused extensively on encoder-only models because of their advantage in bi-directional semantic comprehension. These encoder-only models, including BERT, RoBERTa, and Longformerdetailed in Section  2.3 were compared for their performance. Given the clinical focus of this task, we particularly explored model variants that were fine-tuned on clinical or biological datasets. However, as no clinically fine-tuned RoBERTa  [ Zhuang et al., 2021 ]  variant was available, the  RoBERTa-base  was used for comparisons. Specifically, the encoder-only models we explored include Bio_ClinicalBERT  [ Alsentzer et al., 2019 ] , medicalai/ClinicalBERT  [ Wang et al., 2023 ] , RoBERTa-base  [ Zhuang et al., 2021 ] , and Clinical-Longformer  [ Li et al., 2023 ] .",
            "The process of generating synthetic letters with encoder-decoder models is very similar to that with encoder-only models. The difference is that, unlike the BERT family, which automatically masks tokens and replaces them with  <mask> , the T5 family models  do not have any built-in masking function . As a result, we identified the words that needed to be masked by  index  and removed them, which are represented as extra_id_x in the T5 family models. The text, with these words removed, was then used for the generation, which we refer to as  text with blanks . To maintain consistency in the format, we later replaced extra_id_x with  <mask>  when displaying the masked text. Additionally, the T5 family models  require a prompt  as part of the input. For this task, the complete input was structured as Fill in the blanks in the following sentence in the clinical background + text with blanks. In this project, we used T5-base  [ Raffel et al., 2020b ] , Clinical-T5-Base  [ Eric and Johnson, 2023 ,  Goldberger et al., 2000 ] , Clinical-T5-Sci  [ Eric and Johnson, 2023 ,  Goldberger et al., 2000 ] , and Clinical-T5-Scratch  [ Eric and Johnson, 2023 ,  Goldberger et al., 2000 ]  for comparison. The comparison of encoder-only and encoder-decoder model architectures is shown in Fig  13 .",
            "As mentioned in Section  3.3 , we utilise two parameters in our chunk segment procedure: max_lines and max_tokens. max_lines represents the desired length of each chunk, while max_tokens is related to the computing resources and model limitations. These two parameters determine the final length of each chunk together. Although most models we used have a limit of 512 tokens (except for the Longformer, which can process up to 4096 tokens), we set 256 as the value for  max_tokens  due to the computing resources constraints.",
            "As described in Section  3.1 , the dataset we used has been de-identified. All private information is replaced by three underscores ___. We hope that the synthetic clinical letters can maintain a certain degree of clinical integrity without revealing any private patient information. To address this, a post-processing step was added to the synthetic results. This process involves masking the three underscores (___) detected and using PLMs to predict the masked part again. For example, if the original text is ___ caught a cold. The post-processing result should ideally be John caught a cold or Patient caught a cold. Such synthetic clinical letters can better support clinical model training and teaching.",
            "We employed both the encoder-only and encoder-decoder models to mask and generate the data, yielding numerous interesting results for human evaluation. Given space constraints, only a simple example is provided here. Following the masking principles in Section  3.4 , the eligible tokens were randomly selected for masking. Although the initial intention was to mask 50% of tokens, the actual masking ratio was lower due to the requirement to preserve certain entities and structures.",
            "The result generated by RoBERTa-base is shown in Fig  23 . While the generated text initially seems reasonable, the predicted word years shifts the focus to a temporal context, which was not intended. This is likely because RoBERTa is pre-trained on a general corpus and lacks sufficient clinical knowledge for accurate text generation, or it could simply be a coincidence based on this specific sentence, where RoBERTa-base inferred years from its training data.",
            "Additionally,  GPT-4o  was used for comparison, with the prompt Replace  <mask>  with words in the following sentence: . The results, shown in Fig  24 , are satisfactory. As discussed in Section  2.3 , decoder-only models excel in few-shot learning  [ Wu, 2024 ] , which is confirmed by this experiment. However, its performance may decline with long clinical letters  [ Amin-Nejad et al., 2020b ] .",
            "T5-base performs the best among these tested models. However, the results are still  not fully rational , as it generated open is a ___ yo male. The other three models tend to use de-identification ( DEID ) tags to replace the masked words, as these tags are part of their corpora. Furthermore, the T5 family models may predict multiple words for each token, aligning with findings in Section  2.3",
            "We will now explore how different  masking ratios  affect the quality of synthetic clinical letters. For each model, we generated data with masking ratios from 0.0 to 1.0, in increments of 0.1 (the masking ratios here refer only to the eligible tokens, as described in Subsection  3.4.3 , and do not represent the actual overall masking ratio). Due to space limitations, we will present only the results for Bio_ClinicalBERT with a 0.2 increment here.",
            "In Table  11 , we calculated three  readability  metrics, which were mentioned in Section  3.5 . All these metrics have not shown significant changes from the original ones. However, it is strange that the SMOG and Flesh-Kincaid Grade are not always between the original baseline and mask baseline. When the masking ratio is high, the evaluation values even fall below both the masking and the original baseline. This may be because a  higher masking ratio leads to a lower valid prediction rate . If the predicted words include many spaces or punctuation marks, the readability will decrease obviously.",
            "As shown in Table  13 ,  inference time  for the entire dataset consistently ranges between 3 to 4 hours. However, it decreases with either very high or very low masking ratios. A mid-range masking ratio of approximately 0.6 results in longer inference times, likely because lower ratios reduce the number of masked tokens to process, while higher ratios provide less context, reducing the computational load. This lack of effective context also increases the invalid prediction rate. Conversely, with a masking ratio of 0, even a small number of prediction errors can significantly impact the overall accuracy due to the few masked tokens.",
            "As mentioned in Subsection  3.4.4 , we set max_lines as a variable and the max_tokens equal to 256. A series of increasing max_lines were tested until the average tokens per chunk reached a peak. We initially did this on a small sample (7 letters). The results are shown in Table  14 .",
            "There is a random selection when masking tokens at certain ratios. Masking different types of tokens will lead to different results, as shown in Fig  29  and Fig  30 . This variability is understandable since the encoder-only models use bidirectional attention, as mentioned in Section  2.3 .  These models need to predict the masked tokens based on the context . Therefore, it is necessary to experiment with different masking strategies based on the types of tokens. We used  POS  tagging and  stopwords  to observe how these strategies influence the quality of synthetic letters.",
            "As mentioned in Subsection  3.4.3 , masking stopwords aims to reduce noise for model understanding while introducing variation in synthetic clinical letters. Table  17  shows that  masking only stopwords follows a similar trend to random masking , where a higher masking ratio leads to lower ROUGE Score and BERTScore. Additionally, the Invalid Prediction Rate is at its lowest with a medium masking ratio. This is because higher masking ratios always result in more information loss. On the other hand, lower masking ratios lead to fewer tokens being masked, which makes small prediction errors more influential. The results show an overall low Invalid Prediction Rate and high BERTScore, indicating that  stopwords have only a limited influence on the models understanding of context . This is not because the original raw letters contain very few stopwords. In fact, there are even more stopwords than nouns and verbs, as seen in sample texts.",
            "One example text without post-processing is shown in Fig  31 . After filling in the blanks, the results with BERT-base and Bio_ClinicalBERT are shown in Fig  32  and Fig  33  separately. We can see that both models can partially achieve the goal of making the text more complete. However, neither of them created a coherent story to fill in these blanks. They just used general terms like hospital and clinic. Perhaps other decoder-only models, more suitable for generating stories like GPT, could perform better and should be explored in the future.",
            "Fig  34  shows that if the incorrect words are masked, the models may be able to correct the misspelled tokens by predicting them. However, the masking process is random. Additionally, sometimes the predicted words will be incorrect because some models tokenize the sentence into word-pieces. Therefore, a post-processing step is necessary for correcting spelling.",
            "As shown in Fig  35 , Tooltik TextBlob  [ Loria, 2024 ]  can successfully correct misspelled words (healhty) in our sample text. However, if clinical entities are not preserved during the pre-processing step, TextBlob  [ Loria, 2024 ]  may misidentify some clinical terms as spelling errors. It may be because TextBlob  [ Loria, 2024 ]  was developed on the general corpus, not a clinical one. Additionally, its corrections are limited to the word level and do not consider any context. Therefore, if words are misspelled deliberately, they could be processed incorrectly. Thus,  developing a clinical misspelling correction toolkit is a promising  research direction in the future."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Comparison of Tokenization Methods for Different LMs on Sentence   Patient is a ___ yo male previously healthy presenting w/ fall from 6 feet, from ladder.",
        "table": "S3.T4.1",
        "footnotes": [],
        "references": [
            "As shown in Fig  4 , the authors used clinical notes from MIMIC-III as input, and incorporated a one-shot summary along with clinical guidance as prompts to generate discharge summaries by GPT-4-turbo. Initially, five sample synthetic summaries were evaluated by a clinician. Based on the feedback, the clinical guidance was revised to adapt to the generation task. Through iterative optimisation, the revised guidance, combined with the original one-shot sample, became the new prompt. Then the authors generated 53 discharge summaries using this method and invited 11 clinicians to do a final manual quantitative evaluation. Clinicians were invited to evaluate the error rate at the section level (e.g., Diagnoses, Social Context, etc). It includes four dimensions:",
            "For the tokenization aimed at feature extraction, we used the word_tokenize method from the NLTK library. It is helpful to preserve the original features of the words, which is especially important for retaining clinical entities. For instance, in the sentence Patient is a ___ yo male previously healthy presenting w/ fall from 6 feet, from ladder. Word boundaries such as spaces can be automatically detected for tokenization. The results of different tokenization methods are shown in the Table  4 .",
            "As for the tokenization used for masking and generating, we retained the original models tokenization methods. The specific tokenization approach varies by model, as shown in Table  4 . For example, BERT family models use Word-Piece tokenization, which initially splits text by spaces and then further divides the words into sub-words  [ Zhuang et al., 2021 ] . This approach is particularly effective for handling words that are not in the pre-training vocabulary and is especially useful for predicting masked words. For complex clinical terms, however, these models rely heavily on a predefined dictionary, which can result in unsatisfactory tokenization and hinder the models understanding. For instance, the word COVID-19 is tokenized by BERT into [co, ##vid, -, 19]. In contrast, the T5 family models use Sentence-Piece tokenization. It does not rely on space to split the text. Instead, this method tokenizes directly from the raw text, making it better suited for handling abbreviations and non-standard characters (e.g. COVID-19), which are common in clinical letters.",
            "It is important to note that although all BERT family models use Word-Piece tokenization, the results can still differ. This is because different models use different vocabularies during pre-training, leading to variations in tokenization granularity. The tokenization methods for each model are detailed in Table  4 . Each tokenization approach has its own advantages and disadvantages for processing clinical letters. Therefore, exploring how these models impact the clinical letter generation is also a requirement of my project.",
            "As discussed in Section  2.3  and Section  2.4 , decoder-only models struggle with processing long texts that require contextual understanding  [ Amin-Nejad et al., 2020b ] . Additionally, deploying them requires substantial computing resources and time. Therefore, we explored various pre-trained language models (PLMs), including both encoder-only and encoder-decoder models in this project. After evaluating their ability to generate synthetic letters from our dataset, we focused on  Bio_ClinicalBERT  - a well-performed model in our task, to experiment with different masking strategies. Additionally, from the discussion in Section  3.3 , we need to split the text into various-length-chunks. So the appropriate  length of these chunks  is also experimented with Bio_ClinicalBERT.",
            "Both quantitative and qualitative methods will be used to evaluate the performance. Additionally, a downstream task (NER) is employed to assess whether the synthetic clinical letters can replace the original raw data. The evaluation methods pipeline is illustrated in Fig  14 .",
            "We employed both the encoder-only and encoder-decoder models to mask and generate the data, yielding numerous interesting results for human evaluation. Given space constraints, only a simple example is provided here. Following the masking principles in Section  3.4 , the eligible tokens were randomly selected for masking. Although the initial intention was to mask 50% of tokens, the actual masking ratio was lower due to the requirement to preserve certain entities and structures.",
            "Additionally,  GPT-4o  was used for comparison, with the prompt Replace  <mask>  with words in the following sentence: . The results, shown in Fig  24 , are satisfactory. As discussed in Section  2.3 , decoder-only models excel in few-shot learning  [ Wu, 2024 ] , which is confirmed by this experiment. However, its performance may decline with long clinical letters  [ Amin-Nejad et al., 2020b ] .",
            "We first calculated representative quantitative metrics at the sentence level, matching the sample sentence used in Subsection  4.1 . This approach allows for a better integration of quantitative and qualitative evaluations. Although SMOG is typically suited for medical datasets, it is less appropriate for sentence-level analysis, so the Flesch Reading Ease was used here. The results are shown in table  7 .",
            "We observed that clinical-related encoder-only models generally outperform RoBERTa-base in qualitative evaluation (see Subsection  4.1 ). However, from the quantitative perspective, RoBERTa-base shows mediocre performance across most metrics except for BERTScore. In contrast, Bio_ClinicalBERT, despite no word overlap in this sample sentence, achieves a reasonable clinical context and the highest BERTScore among the models. Both Medicalai/Clinical BERT and Bio_ClinicalBERT excel in Flesch Reading Ease, likely because they tend to predict tokens with fewer syllables words that preserve the original meaning.",
            "We will now explore how different  masking ratios  affect the quality of synthetic clinical letters. For each model, we generated data with masking ratios from 0.0 to 1.0, in increments of 0.1 (the masking ratios here refer only to the eligible tokens, as described in Subsection  3.4.3 , and do not represent the actual overall masking ratio). Due to space limitations, we will present only the results for Bio_ClinicalBERT with a 0.2 increment here.",
            "As mentioned in Subsection  3.4.4 , we set max_lines as a variable and the max_tokens equal to 256. A series of increasing max_lines were tested until the average tokens per chunk reached a peak. We initially did this on a small sample (7 letters). The results are shown in Table  14 .",
            "As discussed in Subsection  4.2 , BERTScore should be the primary evaluation metric for our objective. Additionally, the invalid prediction rate is useful for assessing the models ability to generate informative predictions, and ROUGE scores help evaluate literal diversity. Therefore, these quantitative metrics, calculated using different masking strategies, will be shown in this section. Similar to Subsection  4.2 , we experimented with different masking ratios calculated from the eligible tokens (masked tokens divided by eligible tokens). The ratios are increased in increments of 0.1, ranging from 0.0 to 1.0. Due to space constraints, only metrics with increments of 0.2 will be shown here. A comparison with the same actual masking ratio (masked tokens divided by total tokens in the text) will also be presented in this subsection.",
            "As mentioned in Subsection  3.4.3 , masking stopwords aims to reduce noise for model understanding while introducing variation in synthetic clinical letters. Table  17  shows that  masking only stopwords follows a similar trend to random masking , where a higher masking ratio leads to lower ROUGE Score and BERTScore. Additionally, the Invalid Prediction Rate is at its lowest with a medium masking ratio. This is because higher masking ratios always result in more information loss. On the other hand, lower masking ratios lead to fewer tokens being masked, which makes small prediction errors more influential. The results show an overall low Invalid Prediction Rate and high BERTScore, indicating that  stopwords have only a limited influence on the models understanding of context . This is not because the original raw letters contain very few stopwords. In fact, there are even more stopwords than nouns and verbs, as seen in sample texts.",
            "Fig  34  shows that if the incorrect words are masked, the models may be able to correct the misspelled tokens by predicting them. However, the masking process is random. Additionally, sometimes the predicted words will be incorrect because some models tokenize the sentence into word-pieces. Therefore, a post-processing step is necessary for correcting spelling.",
            "Spelling Correction:  As mentioned in Section  4.6 , there are very few toolkits available for spelling correction in the clinical domain. Standard spelling correction tools may misidentify clinical terms as misspelled words. Therefore, it is necessary to develop a specialised spell-checking tool adapted to the clinical domain."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Example: Summary of Feature Extraction Operations and Extracted Features",
        "table": "S3.T5.1",
        "footnotes": [],
        "references": [
            "? ) compared masked language modelling (MLM, including BERT, RoBERTa, BiomedNLP-PubMedBERT) and causal language modelling (CLM, including T5, BART, SciFive-large-Pubmed_PMC) across various datasets in masking and text generation tasks. They used qualitative and quantitative evaluations, as well as downstream tasks, to assess the quality of the synthetic texts. Their workflow is shown in Fig  5 . Based on these evaluations, the study yielded the following results:",
            "Since we aim to generate de-identified clinical letters that can preserve clinical narratives during masking and generation, it is necessary to extract certain features beforehand. We extracted the following features, with an example provided in Figure  12  and Table  5 .",
            "In this downstream NER task, as shown in Fig  15 , we initially extracted entities from letters using ScispaCy. Subsequently, these entities were used to train a base spaCy model. The trained model was then employed to extract entities from the testing set. Finally, we can compare these newly extracted entities with those originally extracted by ScispaCy, and the evaluation scores can be calculated. These steps were performed on both original clinical letters and synthetic letters, to assess whether the synthetic letters can potentially replace the original ones.",
            "To further evaluate different PLMs in generating synthetic letters, we tested the T5 Family models. The generated results for the same sentence are shown in Fig  25 , Fig  26 , Fig  27 , and Fig  28 .",
            "In Table  11 , we calculated three  readability  metrics, which were mentioned in Section  3.5 . All these metrics have not shown significant changes from the original ones. However, it is strange that the SMOG and Flesh-Kincaid Grade are not always between the original baseline and mask baseline. When the masking ratio is high, the evaluation values even fall below both the masking and the original baseline. This may be because a  higher masking ratio leads to a lower valid prediction rate . If the predicted words include many spaces or punctuation marks, the readability will decrease obviously.",
            "As shown in Table  15 ,  the fewer nouns we mask, the better all these metrics perform . This trend is consistent with random masking. When the noun masking ratio is 1.0, meaning all nouns are masked, BERTScore increases from a baseline of 0.70 to 0.89. This means the  model predicted meaningful nouns . A similar trend is observed in the ROUGE scores. All evaluations are higher than the baseline but lower than 1. However, ROUGE scores show a smaller improvement than BERTScore. This may be because the model generates synonyms or paraphrases that retain the original meaning. With the increase in nouns masking ratio, the BERTScore decreases significantly.",
            "As shown in Fig  35 , Tooltik TextBlob  [ Loria, 2024 ]  can successfully correct misspelled words (healhty) in our sample text. However, if clinical entities are not preserved during the pre-processing step, TextBlob  [ Loria, 2024 ]  may misidentify some clinical terms as spelling errors. It may be because TextBlob  [ Loria, 2024 ]  was developed on the general corpus, not a clinical one. Additionally, its corrections are limited to the word level and do not consider any context. Therefore, if words are misspelled deliberately, they could be processed incorrectly. Thus,  developing a clinical misspelling correction toolkit is a promising  research direction in the future."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Annotated Entities Extracted from the Example Sentence (They should be preserved from masking)",
        "table": "S4.T6.1",
        "footnotes": [],
        "references": [
            "An overall investigation workflow is shown in Fig  6 .",
            "In this section, we introduced the experimental design and subsequent implementation steps: from project requirement, data collection to environmental setup, pre-processing, masking and generating, post-processing, downstream NER task, and both qualitative and quantitative evaluation. An example of the entire process flow is shown in Figure  16 .",
            "The original sentence is displayed in Fig  17 . After the feature extraction, the resulting structure is shown in Fig  19 . As detailed in Table  6 , certain manually annotated entities are excluded from masking. The output of this masking process can be seen in Fig  18 .",
            "To further evaluate different PLMs in generating synthetic letters, we tested the T5 Family models. The generated results for the same sentence are shown in Fig  25 , Fig  26 , Fig  27 , and Fig  28 .",
            "Table  16  shows a similar trend for masking verbs as observed with other masking strategies in standard NLG metrics. However, it is surprising that as the masking ratio increases, both the invalid prediction rate and NLG metrics decrease. This phenomenon can be attributed to two main reasons.  First , the model seems to prioritise predicting meaningful tokens (rather than punctuation, spaces, etc.) to generate coherent sentences. Contextual relevance is only considered after the sentence structure is complete. This may be due to the important role of verbs in sentences.  Second , the original raw data may contain fewer verbs than nouns. Therefore, the number of actual masking tokens changes slightly when verbs are masked, making the model less sensitive to them. This is also reflected in BERTScore. If all verbs are masked, the BERTScore remains high at 0.95, whereas if all nouns are masked, the BERTScore drops to 0.89.",
            "Spelling Correction:  As mentioned in Section  4.6 , there are very few toolkits available for spelling correction in the clinical domain. Standard spelling correction tools may misidentify clinical terms as misspelled words. Therefore, it is necessary to develop a specialised spell-checking tool adapted to the clinical domain."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Encoder-Only Models Comparison at the Sentence Level (The Baseline without annotations was calculated by comparing masked text to the original text)",
        "table": "S4.T7.1",
        "footnotes": [],
        "references": [
            "An example of text excerpted from the original letter is shown in Figure  7 . It contains the document structure and some free text. According to the dataset, document structure often corresponds to capital letters and colons :. Our primary goal is to mask the context that is neither part of the document structure nor annotated entities, and then generate a new letter, as both  structure  and clinical  entities  are essential for understanding clinical information  [ Meystre et al., 2014 ] .",
            "The original sentence is displayed in Fig  17 . After the feature extraction, the resulting structure is shown in Fig  19 . As detailed in Table  6 , certain manually annotated entities are excluded from masking. The output of this masking process can be seen in Fig  18 .",
            "To further evaluate different PLMs in generating synthetic letters, we tested the T5 Family models. The generated results for the same sentence are shown in Fig  25 , Fig  26 , Fig  27 , and Fig  28 .",
            "We first calculated representative quantitative metrics at the sentence level, matching the sample sentence used in Subsection  4.1 . This approach allows for a better integration of quantitative and qualitative evaluations. Although SMOG is typically suited for medical datasets, it is less appropriate for sentence-level analysis, so the Flesch Reading Ease was used here. The results are shown in table  7 .",
            "As mentioned in Subsection  3.4.3 , masking stopwords aims to reduce noise for model understanding while introducing variation in synthetic clinical letters. Table  17  shows that  masking only stopwords follows a similar trend to random masking , where a higher masking ratio leads to lower ROUGE Score and BERTScore. Additionally, the Invalid Prediction Rate is at its lowest with a medium masking ratio. This is because higher masking ratios always result in more information loss. On the other hand, lower masking ratios lead to fewer tokens being masked, which makes small prediction errors more influential. The results show an overall low Invalid Prediction Rate and high BERTScore, indicating that  stopwords have only a limited influence on the models understanding of context . This is not because the original raw letters contain very few stopwords. In fact, there are even more stopwords than nouns and verbs, as seen in sample texts."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Encoder-Decoder Models Comparison at the Sentence Level (The Baseline without annotations was calculated by comparing masked text to the original text)",
        "table": "S4.T8.1",
        "footnotes": [],
        "references": [
            "The collected dataset involves different files and is entirely raw data. It is necessary to pre-process them before using them in generation tasks. The pre-processing of this system contains five steps: Merge dataset based on note_id, Annotated Entity Recognition, Split Letters in Chunks, Word Tokenization and Feature Extraction. The pre-processing pipeline is shown in Fig  8 .",
            "The original sentence is displayed in Fig  17 . After the feature extraction, the resulting structure is shown in Fig  19 . As detailed in Table  6 , certain manually annotated entities are excluded from masking. The output of this masking process can be seen in Fig  18 .",
            "To further evaluate different PLMs in generating synthetic letters, we tested the T5 Family models. The generated results for the same sentence are shown in Fig  25 , Fig  26 , Fig  27 , and Fig  28 .",
            "The evaluations for the encoder-decoder models, as shown in Table  8 , generally underperform on most metrics compared to encoder-only models, except for METEOR. Interestingly, while the Flesch Reading Ease scores suggest a minimal impact on readability, the BERTScores are significantly lower than the baseline, indicating major deviations from the original meaning. This is consistent with our qualitative observations that the outputs from encoder-decoder models are largely unintelligible.",
            "To further observe how different masking strategies influence the generation of clinical letters, we compared the results using the same actual masking ratios but with different strategies. In other words, the number of masked tokens is fixed, so the only variable is  the type of tokens being masked . Table  18  shows the results with a 0.04 actual masking ratio, and Table  19  shows the results with a 0.1 actual masking ratio."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Encoder-Only Models Comparison on the Full Dataset with Masking Ratio 0.4 (The Baseline was calculated by comparing masked text to the original text)",
        "table": "S4.T9.1",
        "footnotes": [],
        "references": [
            "Initially, we merged the clinical letters file and annotations file into a new DataFrame. The method is detailed in Appendix  LABEL:Appendix:_Method_for_Dataset_Merging_in_Preprocessing . After this, we extracted manually annotated entities based on their index. An excerpt from an original letter is shown in Fig  9 , and the manually annotated entities are listed in Table  3 .",
            "The original sentence is displayed in Fig  17 . After the feature extraction, the resulting structure is shown in Fig  19 . As detailed in Table  6 , certain manually annotated entities are excluded from masking. The output of this masking process can be seen in Fig  18 .",
            "Based on the findings above, we expect a higher BERTScore and a lower ROUGE Score. We used the 0.4 masking ratio to illustrate the model comparison on the full dataset in Table  9 . The other masking ratios show similar trends. Surprisingly, all encoder-only models this time showed comparable results, which contradicts our hypothesis that Clinical-related models would outperform base models. This suggests that  training on the clinical dataset does not significantly impact the quality of synthetic letters . This may be because most clinical-related tokens are preserved, with only the remaining tokens being eligible for masking. Consequently, the normal encoder-only models can effectively understand the context and predict appropriate words while preserving clinical information. This differs slightly from the sentence-level comparisons, likely because the evaluation of a single sentence cannot fully represent the overall results. Despite this, BERTScore as a primary evaluation metric remains useful, as the correspondence between qualitative and quantitative evaluation is consistent, whether at the sentence or dataset level.",
            "There is a random selection when masking tokens at certain ratios. Masking different types of tokens will lead to different results, as shown in Fig  29  and Fig  30 . This variability is understandable since the encoder-only models use bidirectional attention, as mentioned in Section  2.3 .  These models need to predict the masked tokens based on the context . Therefore, it is necessary to experiment with different masking strategies based on the types of tokens. We used  POS  tagging and  stopwords  to observe how these strategies influence the quality of synthetic letters.",
            "To further observe how different masking strategies influence the generation of clinical letters, we compared the results using the same actual masking ratios but with different strategies. In other words, the number of masked tokens is fixed, so the only variable is  the type of tokens being masked . Table  18  shows the results with a 0.04 actual masking ratio, and Table  19  shows the results with a 0.1 actual masking ratio."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:  Standard NLG Metrics Across Different Masking Ratios Using Bio_ClinicalBERT (The Baseline was calculated by comparing masked text to the original text)",
        "table": "S4.T10.1",
        "footnotes": [],
        "references": [
            "As shown in Fig  10 , each raw letter is split into sentences first. We used the pre-trained models provided by the NLTK library, which combines statistical and machine-learning approaches to identify sentence boundaries. Each clinical letter is treated as a separate processing unit, with the first sentence automatically assigned to the first text block (chunk). To control the length of each chunk, we set a  maximum line count  parameter (max_lines). If the first sentence already meets the value of max_lines, the chunk will only contain this one sentence. Otherwise, subsequent sentences will be added to the chunk until the line count up to the max_lines.",
            "Table  10  shows that the higher masking ratio, the lower the similarity (metrics scores) will be. As we expected, all evaluation values are higher than the baseline, but still below 1. This means the model can understand the clinical context and generate understandable text. It is surprising that with the masking ratio of 1.0, BERTScore increased from the baseline (0.29) to 0.63. Although this score is not very high, it still reflects that Bio_ClinicalBERT can generate clinical text effectively."
        ]
    },
    "id_table_11": {
        "caption": "Table 11:  Readability Metrics Across Different Masking Ratios Using Bio_ClinicalBERT (The Baseline without annotations was calculated by comparing masked text to the original text)",
        "table": "S4.T11.1",
        "footnotes": [],
        "references": [
            "Extra care is needed when handling text with specific formats, such as medication dosage descriptions, as shown in Fig  11 . Because there is no clear sentence boundary, these sentences may exceed the tokens limitation. To address this, we first check whether the sentence being processed exceeds the token limit (max_tokens). If it does not, the sentence will be added to the current chunk. Otherwise, the sentence should be split into smaller chunks, each no longer than max_tokens. This operation helps  balance processing efficiency  while maintaining semantic integrity. In the example shown in  11 , although using line breaks to split the text seems to be more flexible, considering time complexity and the requirement to index the annotated entities, this method was not chosen.",
            "In Table  11 , we calculated three  readability  metrics, which were mentioned in Section  3.5 . All these metrics have not shown significant changes from the original ones. However, it is strange that the SMOG and Flesh-Kincaid Grade are not always between the original baseline and mask baseline. When the masking ratio is high, the evaluation values even fall below both the masking and the original baseline. This may be because a  higher masking ratio leads to a lower valid prediction rate . If the predicted words include many spaces or punctuation marks, the readability will decrease obviously."
        ]
    },
    "id_table_12": {
        "caption": "Table 12:  Advanced Text Quality Metrics Across Different Masking Ratios Using Bio_ClinicalBERT (The Baseline without annotations was calculated by comparing masked text to the original text)",
        "table": "S4.T12.1",
        "footnotes": [],
        "references": [
            "Since we aim to generate de-identified clinical letters that can preserve clinical narratives during masking and generation, it is necessary to extract certain features beforehand. We extracted the following features, with an example provided in Figure  12  and Table  5 .",
            "In Table  12 , considering the perplexity, the masking baseline is very high, while the values for synthetic letters are close to the original ones. This indicates that the synthetic letters are useful for training clinical models. For information entropy, regardless of the masking ratio, it can  effectively preserve the amount of information . As for subjectivity, since all the values are close, we dont need to worry that the synthetic letters will be biased."
        ]
    },
    "id_table_13": {
        "caption": "Table 13:  Inference Time and Invalid Prediction Rate Across Different Masking Ratios Using Bio_ClinicalBERT",
        "table": "S4.T13.1",
        "footnotes": [],
        "references": [
            "The process of generating synthetic letters with encoder-decoder models is very similar to that with encoder-only models. The difference is that, unlike the BERT family, which automatically masks tokens and replaces them with  <mask> , the T5 family models  do not have any built-in masking function . As a result, we identified the words that needed to be masked by  index  and removed them, which are represented as extra_id_x in the T5 family models. The text, with these words removed, was then used for the generation, which we refer to as  text with blanks . To maintain consistency in the format, we later replaced extra_id_x with  <mask>  when displaying the masked text. Additionally, the T5 family models  require a prompt  as part of the input. For this task, the complete input was structured as Fill in the blanks in the following sentence in the clinical background + text with blanks. In this project, we used T5-base  [ Raffel et al., 2020b ] , Clinical-T5-Base  [ Eric and Johnson, 2023 ,  Goldberger et al., 2000 ] , Clinical-T5-Sci  [ Eric and Johnson, 2023 ,  Goldberger et al., 2000 ] , and Clinical-T5-Scratch  [ Eric and Johnson, 2023 ,  Goldberger et al., 2000 ]  for comparison. The comparison of encoder-only and encoder-decoder model architectures is shown in Fig  13 .",
            "As shown in Table  13 ,  inference time  for the entire dataset consistently ranges between 3 to 4 hours. However, it decreases with either very high or very low masking ratios. A mid-range masking ratio of approximately 0.6 results in longer inference times, likely because lower ratios reduce the number of masked tokens to process, while higher ratios provide less context, reducing the computational load. This lack of effective context also increases the invalid prediction rate. Conversely, with a masking ratio of 0, even a small number of prediction errors can significantly impact the overall accuracy due to the few masked tokens."
        ]
    },
    "id_table_14": {
        "caption": "Table 14:  Comparison for different Chunk Size",
        "table": "S4.T14.1.1",
        "footnotes": [],
        "references": [
            "Both quantitative and qualitative methods will be used to evaluate the performance. Additionally, a downstream task (NER) is employed to assess whether the synthetic clinical letters can replace the original raw data. The evaluation methods pipeline is illustrated in Fig  14 .",
            "As mentioned in Subsection  3.4.4 , we set max_lines as a variable and the max_tokens equal to 256. A series of increasing max_lines were tested until the average tokens per chunk reached a peak. We initially did this on a small sample (7 letters). The results are shown in Table  14 ."
        ]
    },
    "id_table_15": {
        "caption": "Table 15:  Quantitative Comparisons of Nouns Masking Ratios (The Baseline was calculated by comparing masked text to the original text)",
        "table": "S4.T15.1",
        "footnotes": [],
        "references": [
            "In this downstream NER task, as shown in Fig  15 , we initially extracted entities from letters using ScispaCy. Subsequently, these entities were used to train a base spaCy model. The trained model was then employed to extract entities from the testing set. Finally, we can compare these newly extracted entities with those originally extracted by ScispaCy, and the evaluation scores can be calculated. These steps were performed on both original clinical letters and synthetic letters, to assess whether the synthetic letters can potentially replace the original ones.",
            "As shown in Table  15 ,  the fewer nouns we mask, the better all these metrics perform . This trend is consistent with random masking. When the noun masking ratio is 1.0, meaning all nouns are masked, BERTScore increases from a baseline of 0.70 to 0.89. This means the  model predicted meaningful nouns . A similar trend is observed in the ROUGE scores. All evaluations are higher than the baseline but lower than 1. However, ROUGE scores show a smaller improvement than BERTScore. This may be because the model generates synonyms or paraphrases that retain the original meaning. With the increase in nouns masking ratio, the BERTScore decreases significantly."
        ]
    },
    "id_table_16": {
        "caption": "Table 16:  Quantitative Comparisons of Verb Masking Ratios (The Baseline was calculated by comparing masked text to the original text)",
        "table": "S4.T16.1",
        "footnotes": [],
        "references": [
            "In this section, we introduced the experimental design and subsequent implementation steps: from project requirement, data collection to environmental setup, pre-processing, masking and generating, post-processing, downstream NER task, and both qualitative and quantitative evaluation. An example of the entire process flow is shown in Figure  16 .",
            "Table  16  shows a similar trend for masking verbs as observed with other masking strategies in standard NLG metrics. However, it is surprising that as the masking ratio increases, both the invalid prediction rate and NLG metrics decrease. This phenomenon can be attributed to two main reasons.  First , the model seems to prioritise predicting meaningful tokens (rather than punctuation, spaces, etc.) to generate coherent sentences. Contextual relevance is only considered after the sentence structure is complete. This may be due to the important role of verbs in sentences.  Second , the original raw data may contain fewer verbs than nouns. Therefore, the number of actual masking tokens changes slightly when verbs are masked, making the model less sensitive to them. This is also reflected in BERTScore. If all verbs are masked, the BERTScore remains high at 0.95, whereas if all nouns are masked, the BERTScore drops to 0.89."
        ]
    },
    "id_table_17": {
        "caption": "Table 17:  Quantitative Comparisons of Stopwords Masking Ratios (The Baseline was calculated by comparing masked text to the original text)",
        "table": "S4.T17.1",
        "footnotes": [],
        "references": [
            "The original sentence is displayed in Fig  17 . After the feature extraction, the resulting structure is shown in Fig  19 . As detailed in Table  6 , certain manually annotated entities are excluded from masking. The output of this masking process can be seen in Fig  18 .",
            "As mentioned in Subsection  3.4.3 , masking stopwords aims to reduce noise for model understanding while introducing variation in synthetic clinical letters. Table  17  shows that  masking only stopwords follows a similar trend to random masking , where a higher masking ratio leads to lower ROUGE Score and BERTScore. Additionally, the Invalid Prediction Rate is at its lowest with a medium masking ratio. This is because higher masking ratios always result in more information loss. On the other hand, lower masking ratios lead to fewer tokens being masked, which makes small prediction errors more influential. The results show an overall low Invalid Prediction Rate and high BERTScore, indicating that  stopwords have only a limited influence on the models understanding of context . This is not because the original raw letters contain very few stopwords. In fact, there are even more stopwords than nouns and verbs, as seen in sample texts."
        ]
    },
    "id_table_18": {
        "caption": "Table 18:  Quantitative Comparison of Different Masking Strategies at a 0.04 Actual Masking Ratio (The Baseline was calculated by comparing masked text to the original text)",
        "table": "S4.T18.1",
        "footnotes": [],
        "references": [
            "The original sentence is displayed in Fig  17 . After the feature extraction, the resulting structure is shown in Fig  19 . As detailed in Table  6 , certain manually annotated entities are excluded from masking. The output of this masking process can be seen in Fig  18 .",
            "To further observe how different masking strategies influence the generation of clinical letters, we compared the results using the same actual masking ratios but with different strategies. In other words, the number of masked tokens is fixed, so the only variable is  the type of tokens being masked . Table  18  shows the results with a 0.04 actual masking ratio, and Table  19  shows the results with a 0.1 actual masking ratio."
        ]
    },
    "id_table_19": {
        "caption": "Table 19:  Quantitative Comparisons of 0.1 Actual Masking Ratio (The Baseline was calculated by comparing masked text to the original text)",
        "table": "S4.T19.1",
        "footnotes": [],
        "references": [
            "The original sentence is displayed in Fig  17 . After the feature extraction, the resulting structure is shown in Fig  19 . As detailed in Table  6 , certain manually annotated entities are excluded from masking. The output of this masking process can be seen in Fig  18 .",
            "To further observe how different masking strategies influence the generation of clinical letters, we compared the results using the same actual masking ratios but with different strategies. In other words, the number of masked tokens is fixed, so the only variable is  the type of tokens being masked . Table  18  shows the results with a 0.04 actual masking ratio, and Table  19  shows the results with a 0.1 actual masking ratio."
        ]
    },
    "id_table_20": {
        "caption": "Table 20:  Quantitative Comparisons for Hybrid Masking (The Baseline was calculated by comparing masked text to the original text)",
        "table": "S4.T20.1",
        "footnotes": [],
        "references": [
            "The generated text using Bio_ClinicalBERT is displayed in Fig  20 . For management of open fracture, the model produced r, which is commonly used to denote right in clinical contexts, showing a relevant and logical prediction. Furthermore, the models input R ankle, despite not being in the figure due to space constraints, provided context for predicting r instead of left. Interestingly, the term admitted was generated even though it was not in the input, indicating the models understanding of clinical context. Although the phrase from 6 stairs, from home differs significantly from the original one, it remains contextually appropriate.",
            "After comparing different strategies with the same actual masking ratio, we explored hybrid masking strategies and compared them with other strategies at the same actual ratio. The results are shown in Table  20 . The first three columns have the same actual masking ratio. Masking only stopwords achieved the strongest performance among these strategies. However, when nouns are also masked along with stopwords, performance decreases, as masking nouns negatively affects the results. Despite this, it still performs better than random masking, indicating that stopwords have a greater influence than nouns. Next, we compared the last two columns. If 0.5 of nouns and 0.5 of stopwords are masked, adding an additional 0.5 of masked verbs leads to worse performance, showing that verbs also negatively influence the models performance."
        ]
    },
    "id_table_21": {
        "caption": "Table 21:  Comparison with and without Entity Preservation Using Bio_ClinicalBERT",
        "table": "S4.T21.1",
        "footnotes": [],
        "references": [
            "The results from medicalai/ClinicalBERT and Clinical-Longformer are shown in Fig  21  and Fig  22 . All three clinical-related models correctly predicted r from the input context. Medicalai/ClinicalBERT performs  comparably  to Bio_ClinicalBERT, despite adding an extra comma, which did not affect the texts clarity. However, Clinical-Longformers predictions, while understandable, were  repetitive  and less satisfactory. Importantly, none of these three models altered the original meaning.",
            "To further explore whether keeping entities is useful for our task, we compared our results with a baseline that does not retain any entities. The baseline was trained with four epochs of fine-tuning on our dataset. Specifically, 0.4 of nouns from all tokens were randomly masked during the baseline training. In contrast, in our experiments, only eligible tokensexcluding clinical informationwere selected for masking. The comparisons are shown in Table  21 ."
        ]
    },
    "id_table_22": {
        "caption": "Table 22:  Comparisons on Downstream NER Task",
        "table": "S4.T22.1",
        "footnotes": [],
        "references": [
            "The results from medicalai/ClinicalBERT and Clinical-Longformer are shown in Fig  21  and Fig  22 . All three clinical-related models correctly predicted r from the input context. Medicalai/ClinicalBERT performs  comparably  to Bio_ClinicalBERT, despite adding an extra comma, which did not affect the texts clarity. However, Clinical-Longformers predictions, while understandable, were  repetitive  and less satisfactory. Importantly, none of these three models altered the original meaning.",
            "As shown in Table  22 , spaCy models trained on original and synthetic letters showed  similar evaluation scores . They even achieved F1 scores comparable to ScispaCys score of 0.843. Therefore, the unmasked context does not significantly influence model understanding. Consequently,  our synthetic letters can be used in NER tasks to replace real-world clinical letters, thereby further protecting sensitive information ."
        ]
    }
}