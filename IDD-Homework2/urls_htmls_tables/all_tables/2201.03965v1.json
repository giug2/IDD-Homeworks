{
    "S4.T1": {
        "caption": "Table 1: VQA accuracy of ViLBERT (Lu etÂ al., 2019) with different number of region proposals. Accuracies are computed over all the question/image pairs in the VQA-HAT (Das etÂ al., 2016) validation set.",
        "table": "<table id=\"S4.T1.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.4.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.4.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Method</th>\n<th id=\"S4.T1.4.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">VQA Accuracy</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.4.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.4.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">ViLBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu etÂ al., <a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">2019</a>)</cite> (36 Region Proposals)</td>\n<td id=\"S4.T1.4.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">76.57</td>\n</tr>\n<tr id=\"S4.T1.4.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.4.3.2.1\" class=\"ltx_td ltx_align_center\">ViLBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu etÂ al., <a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">2019</a>)</cite> (72 Region Proposals)</td>\n<td id=\"S4.T1.4.3.2.2\" class=\"ltx_td ltx_align_center\">79.39</td>\n</tr>\n<tr id=\"S4.T1.4.4.3\" class=\"ltx_tr\">\n<td id=\"S4.T1.4.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">ViLBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu etÂ al., <a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">2019</a>)</cite> (108 Region Proposals)</td>\n<td id=\"S4.T1.4.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">80.83</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Increasing the number of region proposals led layers 3-6 of the model to attend to regions more similar to those attended by humans. The increased context due to more region proposals also improved the modelâ€™s VQA accuracy (TableÂ 1 and examples in Fig.Â 4). The region proposals are generated using Faster RCNN (Ren etÂ al., 2016), an object detection architecture. Therefore, even in the first co-attention layer, which has little interaction with the language stream, the rank-correlation of the modelâ€™s visual attention with human attention is well above chance. The correlation in the lower layers is likely due to the observation that the majority of the questions in the VQA dataset (Antol etÂ al., 2015) focus either on object categories or object attributes that are salient in terms of basic visual features."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: VQA accuracy of ViLBERT (Lu etÂ al., 2019) in different controls. Note that the reported accuracy is over question/image pairs in VQA-HAT (Das etÂ al., 2016) validation set. Refer sectionÂ 4.2 for more details.",
        "table": "<table id=\"S4.T2.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.4.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.4.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Method</th>\n<th id=\"S4.T2.4.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">VQA Accuracy</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.4.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.4.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">ViLBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu etÂ al., <a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">2019</a>)</cite> (Normal)</th>\n<td id=\"S4.T2.4.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">76.57</td>\n</tr>\n<tr id=\"S4.T2.4.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.4.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">ViLBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu etÂ al., <a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">2019</a>)</cite> (Shuffled Words)</th>\n<td id=\"S4.T2.4.3.2.2\" class=\"ltx_td ltx_align_center\">60.2</td>\n</tr>\n<tr id=\"S4.T2.4.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T2.4.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\">ViLBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu etÂ al., <a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">2019</a>)</cite> (Unrelated Question/Image Pair)</th>\n<td id=\"S4.T2.4.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">10.8</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "The modelâ€™s VQA accuracy dropped considerably after shuffling the words (TableÂ 2). Thus, while attention seems to be largely independent of grammar and semantics, the ability to answer the questions correctly does require some notion of grammar and/or semantic information."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Accuracy for different VQA models on the VQA test-std set as reported in (Yang etÂ al., 2016; Lu etÂ al., 2016, 2019). Error bars in rank-correlation here show standard error of means.",
        "table": "<table id=\"S4.T3.4\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.4.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\">Method</td>\n<td id=\"S4.T3.4.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">Rank-Correlation</td>\n<td id=\"S4.T3.4.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">VQA Accuracy</td>\n</tr>\n<tr id=\"S4.T3.4.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\">Random</td>\n<td id=\"S4.T3.4.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.000 Â± 0.001</td>\n<td id=\"S4.T3.4.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n</tr>\n<tr id=\"S4.T3.4.3.3\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\">SAN-2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang etÂ al., <a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">2016</a>)</cite>\n</td>\n<td id=\"S4.T3.4.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.249 Â± 0.004</td>\n<td id=\"S4.T3.4.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">58.9</td>\n</tr>\n<tr id=\"S4.T3.4.4.4\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_t\">HieCoAtt-W <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu etÂ al., <a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">2016</a>)</cite>\n</td>\n<td id=\"S4.T3.4.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.246 Â± 0.004</td>\n<td id=\"S4.T3.4.4.4.3\" class=\"ltx_td ltx_border_t\"></td>\n</tr>\n<tr id=\"S4.T3.4.5.5\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.5.5.1\" class=\"ltx_td ltx_align_center\">HieCoAtt-P <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu etÂ al., <a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">2016</a>)</cite>\n</td>\n<td id=\"S4.T3.4.5.5.2\" class=\"ltx_td ltx_align_center\">0.256 Â± 0.004</td>\n<td id=\"S4.T3.4.5.5.3\" class=\"ltx_td ltx_align_center\">62.1</td>\n</tr>\n<tr id=\"S4.T3.4.6.6\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.6.6.1\" class=\"ltx_td ltx_align_center\">HieCoAtt-Q <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu etÂ al., <a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">2016</a>)</cite>\n</td>\n<td id=\"S4.T3.4.6.6.2\" class=\"ltx_td ltx_align_center\">0.264 Â± 0.004</td>\n<td id=\"S4.T3.4.6.6.3\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S4.T3.4.7.7\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_t\">ViLBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Lu etÂ al., <a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>\n</td>\n<td id=\"S4.T3.4.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.7.7.2.1\" class=\"ltx_text ltx_font_bold\">0.434 Â± 0.006</span></td>\n<td id=\"S4.T3.4.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.7.7.3.1\" class=\"ltx_text ltx_font_bold\">70.92</span></td>\n</tr>\n<tr id=\"S4.T3.4.8.8\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.8.8.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">Human</td>\n<td id=\"S4.T3.4.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">0.618 Â± 0.006</td>\n<td id=\"S4.T3.4.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">-</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Following the example in Fig.Â 6, row 1, the same image but using the question â€œIs this singles or doubles?â€ (instead of â€œWhat color is the floor?â€), led to the erroneous answer â€œsinglesâ€ and Ï=0.02ğœŒ0.02\\rho=0.02 (cf. Ï=0.548ğœŒ0.548\\rho=0.548 for the correct question/image pair). The similarity with human attention was largely independent of the layer number but remained well above chance levels in the case of Unrelated Question/Image Pair (Fig.Â 5). Visual attention alone is sufficient to drive the rank-correlation with humans. Interestingly, even the unrelated question case shows higher similarity than previous benchmarks that combined visual and correct language information (TableÂ 3). For layers 3-6, the similarity with human attention dropped considerably with respect to the correct question condition. Thus, attention is largely dictated by visual information, combined with focused co-attention driven by the presence of specific key words irrespective of their ordering.",
            "In TableÂ 3, we show the VQA accuracy and rank-correlation of the modelâ€™s attention maps and human attention maps for the following networks: ViLBERT (Lu etÂ al., 2019), Stacked Attention Network (Yang etÂ al., 2016) with 2 attention layers (SAN-2), Hierarchical Co-Attention Network (Lu etÂ al., 2016) with Word-Level (HieCoAtt-W), Phrase-Level (HieCoAtt-P), and Question-Level (HieCoAtt-Q). ViLBERT (Lu etÂ al., 2019) uses a multi-modal transformer architecture while SAN-2 (Yang etÂ al., 2016) and HieCoAtt (Lu etÂ al., 2016) are based on CNN and LSTM architectures. The rank-correlation for the CNN/LSTM based models is considerably lower than the transformer-based model indicating a superior co-attention mechanism and better fusion of vision and language information in multi-modal transformers. Finally, itâ€™s interesting also to note that an increase in the VQA accuracy is accompanied by a better correlation with human attention."
        ]
    }
}