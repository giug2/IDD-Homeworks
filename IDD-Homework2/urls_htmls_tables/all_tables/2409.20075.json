{
    "id_table_1": {
        "caption": "Table 1:  Comparison of Product Question Answering (PQA) datasets.  The document types are classified into Product Reviews (PR), Product Information (PI), and  Product Analysis from professional users (PA)  .",
        "table": "S3.T1.7.7",
        "footnotes": [],
        "references": [
            "C2 (framework) : How to design a suitable RAG framework for domain adaptation? As shown in Figure  1  (a), existing works typically adopt separate modules for retrieval and generation  Shi et al. ( 2023b ); Lin et al. ( 2023 ); Zhang et al. ( 2023 ); langchain ( 2023 ); Liu ( 2022 ) , for example, using BERT-like  Devlin et al. ( 2019 )  embedding models for retrieving documents and an LLM for generating answers.  In this design, retrieval and generation operate independently, hindering their potential to achieve optimal performance. It is often challenging for the retriever to leverage the continuous pre-training of the generator, while the generators performance may be constrained by retrieved documents that, although similar to the query, are less effective in addressing the questions (Section  5.2.3 ).  Although we can build the retriever and generator both on a fully shared LLM and optimize it with multi-task learning as shown in Figure  1  (b), it can suffer from negative transfer between tasks,  i.e. , performance decrease due to the potential conflicts in the retrieval task and the generation task. Moreover, balancing the retrieval loss and the generation loss is non-trivial and may lead to an effort-taking hyperparameter search.",
            "To address the two challenges, we first propose the WorthBuying dataset with 735K high-quality documents, 50K Question-Document-Answer (QDA) tuples, and human annotated test data of relevant documents for 1K questions and 500 QA pairs ( C1 ).  The knowledge base in our dataset comes from professional users, reducing conflicts and errors, and is more informative, with 1.1K words per document rather than a few dozen words as in existing e-commerce knowledge bases. We also annotate high-quality QA pairs with GPT-4  Achiam et al. ( 2023 )  and manually review the test set.  We then propose a specifically designed BSharedRAG framework that effectively adapts RAG to the e-commerce domain ( C2 ).  As shown in Figure  1  (c) we apply two task-specific modules to independently minimize the retrieval loss and generation loss, which avoids effort-taking loss balancing.  With the shared backbone that benefits from domain-specific continual pre-training, the retriever and the generator can benefit from each other to improve both retrieval and generation performance.",
            "Separate RAG . Traditional RAG frameworks treat retrieval and generation as two distinct tasks, where the retriever and generator share no parameters (Figure  1 (a)).  Formally, they optimize two groups of parameters separately:",
            "where   r subscript  r \\theta_{r} italic_ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT  and  L r subscript L r \\mathcal{L}_{r} caligraphic_L start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT  are the parameters and loss for retrieval model, and   g subscript  g \\theta_{g} italic_ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT  and  L g subscript L g L_{g} italic_L start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT  are those for generation model.  This paradigm cannot benefit from the shared information between the two related tasks as discussed in Section  1 , which limits the performance of both retrieval and generation.",
            "Fully Shared RAG .  One straightforward method to overcome the problems is to build a shared model that can be used for retrieval and generation based on multi-task learning (Figure  1  (b)):",
            "As indicated in Table  1 , previous datasets consist of undetailed short documents.  Their questions and answers are directly collected from brief user QA comments without verification, leading to noise and even incorrect answers.  Our WorthBuying dataset contains much longer documents with more than 50k QDA tuples grounded on 735k informative documents.  We also contain the most detailed product categories than existing datasets.",
            "Seperate RAG  has separate parts for retrieval and generation tasks, as shown in  1 (a). To make sure the same parameters as BSharedRAG in two stages, we train Llama3-8b  Dubey et al. ( 2024 ) , a powerful foundation LLM and eCeLLM-M(7B)  Peng et al. ( 2024 ) , a powerful e-commerce domain LLM with the same data, and configuration as BSharedRAG-retriever in retrieval stage. For the generation, we use the BSharedRAG-generator.",
            "Fully Shared RAG  integrates retrieval and generation into a single model, as shown  1 (b).  Using the same base model, data, and configuration as BSharedRAG, it employs a single LoRA to fully share retrieval and generation parameters."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Comparing retrievers of different RAG frameworks. CPT denotes continual pre-training and HN denotes using hard negative samples.  Our BShared-RAG Retriever outperforms all baselines by a large margin. CPT fails to help the BGE adapt to the e-commerce domain and even hurts the performance. FullShared-RAG performs the worst, showing that sharing all parameters between retrieval and generation leads to severe performance degradation.",
        "table": "S5.T2.1.1",
        "footnotes": [],
        "references": [
            "C2 (framework) : How to design a suitable RAG framework for domain adaptation? As shown in Figure  1  (a), existing works typically adopt separate modules for retrieval and generation  Shi et al. ( 2023b ); Lin et al. ( 2023 ); Zhang et al. ( 2023 ); langchain ( 2023 ); Liu ( 2022 ) , for example, using BERT-like  Devlin et al. ( 2019 )  embedding models for retrieving documents and an LLM for generating answers.  In this design, retrieval and generation operate independently, hindering their potential to achieve optimal performance. It is often challenging for the retriever to leverage the continuous pre-training of the generator, while the generators performance may be constrained by retrieved documents that, although similar to the query, are less effective in addressing the questions (Section  5.2.3 ).  Although we can build the retriever and generator both on a fully shared LLM and optimize it with multi-task learning as shown in Figure  1  (b), it can suffer from negative transfer between tasks,  i.e. , performance decrease due to the potential conflicts in the retrieval task and the generation task. Moreover, balancing the retrieval loss and the generation loss is non-trivial and may lead to an effort-taking hyperparameter search.",
            "As shown in Figure  2 ,  we adopt the following three training stages to optimize Equation ( 3 ).",
            "During inference, the retriever is first employed to obtain documents, which are subsequently given to the generator for processing (Figure  2 ).  Specifically, we first prepare all embeddings for the documents in the domain-specific knowledge base by using the BSharedRAG retriever offline. In the online stage, we only extract the embedding of the question and use it to retrieve the most relevant  N N N italic_N  documents ( N = 3 N 3 N=3 italic_N = 3 ). Finally, we use the BSharedRAG generator to obtain an answer to the question by taking the top documents as input.",
            "Results in Table  2  indicate that our  BShared-RAG Retriever  significantly outperforms all baselines in all metrics upon both datasets.   BGE-large-zh+HN , which is fine-tuned on our dataset using hard negative contrastive learning, serves as the strongest baseline. However, our model still achieves improvements ranging from 5% to 17% over this baseline.  The  FullyShared-RAG  performs much worse than ours and most baselines in separate RAGs. This indicates that it is difficult to optimize two objectives with fully shared parameters. Compared to BERT-like retrievers,  e.g. , BGE-large-zh, our proposed method can effectively benefit from continual pre-training. In contrast,  BGE-large-zh+CPT+HN  has significant drops in all metrics when applying continual pre-training.  In our analysis, there is evidence indicating that embedding models based on architectures like BGE require more fine-tuning training data after CPT, potentially reaching the terabyte (TB) level  Wang et al. ( 2024a ) . Fine-tuning with the same amount of data as BSharedRAG is insufficient for the BGE model, which could lead to a decline in performance.  We conduct ablation studies to analyze the detailed contributions of each module.  Both CPT and HN contribute to significant and consistent improvements, while CPT brings more gains."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Evaluation of generation results based on different retrievers on the WorthBuying-PQA test set. RAG-IT denotes retrieval augmented instruction tuning. The FullySharedRAG method performs worse because the generation objective may conflict with the retrieval objective. Compared with Baichuan2-7b series of baselines, our model achieves the best performance, demonstrating both CPT and RAG-IT contribute to the final performance.",
        "table": "S5.T3.1",
        "footnotes": [],
        "references": [
            "C2 (framework) : How to design a suitable RAG framework for domain adaptation? As shown in Figure  1  (a), existing works typically adopt separate modules for retrieval and generation  Shi et al. ( 2023b ); Lin et al. ( 2023 ); Zhang et al. ( 2023 ); langchain ( 2023 ); Liu ( 2022 ) , for example, using BERT-like  Devlin et al. ( 2019 )  embedding models for retrieving documents and an LLM for generating answers.  In this design, retrieval and generation operate independently, hindering their potential to achieve optimal performance. It is often challenging for the retriever to leverage the continuous pre-training of the generator, while the generators performance may be constrained by retrieved documents that, although similar to the query, are less effective in addressing the questions (Section  5.2.3 ).  Although we can build the retriever and generator both on a fully shared LLM and optimize it with multi-task learning as shown in Figure  1  (b), it can suffer from negative transfer between tasks,  i.e. , performance decrease due to the potential conflicts in the retrieval task and the generation task. Moreover, balancing the retrieval loss and the generation loss is non-trivial and may lead to an effort-taking hyperparameter search.",
            "As shown in Figure  2 ,  we adopt the following three training stages to optimize Equation ( 3 ).",
            "Results in Table  3  show that our model performs the best among open-source models. In terms of ROUGE-L and BertScore, our model is even slightly better than GPT-3.5.  FullyShared-RAG performs the worst because it cannot well balance the retrieval and generation tasks.  Baichuan2-7b-base has no instruction following abilities and thus it performs lowest in QA testing. After RAG-IT, it can obtain instruction following abilities and thus its performance is significantly improved,  e.g. , from 76.81% to 82.91% in Accuracy. A strong chat model, such as Baichuan2-7b-chat, can be also improved by RAG-IT because it brings domain-specific knowledge.  Our proposed BShared-RAG generator performs better than  Baichuan-7b-base + RAG-IT  by 37% in BLEU-3 and 31% in ROUGE-L.  These results demonstrate CPT and RAG-IT are both necessary and effective to improve generators.  Meanwhile, the Separate-RAG model, which employs Llama3 and EcellM as retrievers, maintains the same parameters as BSharedRAG but exhibits lower performance. This result underscores the importance of sharing a unified backbone, confirming its necessity for enhanced performance.",
            "We further conduct some experiments to analyze how retrievers influence generation. We compare the accuracy of generated results by models using BGE-large-zh, our BShared-RAG retriever and BShared-RAG w/o CPT. Results in Figure  3  indicate that our BShared-RAG Retriever can significantly improve generation accuracy no matter using GPT-3.5/GPT-4 or open-source models. Without CPT, the performance has a large drop. This underscores how important it is to bring continual pre-training into retrieval models."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Measuring whether a retriever favors a sentence, which is easy to generate from a prompt of a question. We calculate Kendalls tau and Pearsons correlation between two ranked lists, one by a retrieval model and the other by the generation probability estimated by BSharedRAG Generator. The larger the better.",
        "table": "S5.T4.1.1",
        "footnotes": [],
        "references": [
            "BSharedRAG facilitates better preference alignment between the retriever and generator . To verify this, we conducted an experiment. For each question, we gathered the ground-truth document title and top non-ground-truth titles returned by two methods. We then concatenated each question with a document title and calculated the generation probability using our BSharedRAG Generator. Next, we computed Kendalls tau and Pearson correlation between ranked lists from either BGE-large-zh or our retriever, and from the generator. As shown in Table  4 , our method achieves significantly higher correlations, indicating that the BSharedRAG Retriever favors texts with higher generation probabilities, helping it outperform separate RAG baselines.",
            "As illustrated in Figure  4 , BGE-large-zh prioritizes titles closely matching the question, whereas our BShared-RAG Retriever ranks titles that better answer the question.  For example, our method ranks Equipped with a new 33 million pixel sensor, Sony officially released the full-frame mirrorless A7M4 higher, which is like a more relevant answer to the question.  In contrast, BGE-large-zh ranks Sony A7M4 after buying user experience without pixel information higher, or another document the second, which is hard for the generator to capture due to its isolation in the middle  Liu et al. ( 2024 ) ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Comparing the space costs of different RAG frameworks, SeperateRAG (bge-zh-large) and SeperateRAG (llama3-retriever) respectively utilize the bge-zh-large and finetuned llama3 models, as retrievers, while sharing the same generator as BSharedRAG.",
        "table": "S5.T5.1.1",
        "footnotes": [],
        "references": [
            "C2 (framework) : How to design a suitable RAG framework for domain adaptation? As shown in Figure  1  (a), existing works typically adopt separate modules for retrieval and generation  Shi et al. ( 2023b ); Lin et al. ( 2023 ); Zhang et al. ( 2023 ); langchain ( 2023 ); Liu ( 2022 ) , for example, using BERT-like  Devlin et al. ( 2019 )  embedding models for retrieving documents and an LLM for generating answers.  In this design, retrieval and generation operate independently, hindering their potential to achieve optimal performance. It is often challenging for the retriever to leverage the continuous pre-training of the generator, while the generators performance may be constrained by retrieved documents that, although similar to the query, are less effective in addressing the questions (Section  5.2.3 ).  Although we can build the retriever and generator both on a fully shared LLM and optimize it with multi-task learning as shown in Figure  1  (b), it can suffer from negative transfer between tasks,  i.e. , performance decrease due to the potential conflicts in the retrieval task and the generation task. Moreover, balancing the retrieval loss and the generation loss is non-trivial and may lead to an effort-taking hyperparameter search.",
            "We evaluate the inference space cost and time of our BSharedRAG framework.  As demonstrated in Table  5 , BSharedRAG has a lower space cost than SeperateRAG due to eliminating the need for an additional retrieval component.  In the retrieval part, BSharedRAG integrates a shared backbone model, which results in a marginal increase of retriever parameters compared to the BERT-based model. However, this does not lead to a significantly longer inference time. As shown in Table  6 , the overall time increase is just 0.34 seconds, which remains within an acceptable range."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:    We calculate the average inference time over 1000 questions randomly sampled from WorthyBuying Dataset. Both BSharedRAG retriever and generator are accelerated by vllm  Kwon et al. ( 2023 )",
        "table": "S5.T6.1.1",
        "footnotes": [],
        "references": [
            "We evaluate the inference space cost and time of our BSharedRAG framework.  As demonstrated in Table  5 , BSharedRAG has a lower space cost than SeperateRAG due to eliminating the need for an additional retrieval component.  In the retrieval part, BSharedRAG integrates a shared backbone model, which results in a marginal increase of retriever parameters compared to the BERT-based model. However, this does not lead to a significantly longer inference time. As shown in Table  6 , the overall time increase is just 0.34 seconds, which remains within an acceptable range.",
            "BShareRAG retriever can better distinguish hard negatives. We analyze the different behavior between two retrievers,  i.e. , BGE-large-zh and BShared-RAG Retriever. For each question, we first calculate the gap of similarity scores between a ground-truth document (it surely contains answers) and the top returned not-ground-truth document. The gap score ranges from -1 to 1. A larger gap means a retriever can better rank the ground-truth document among others. Then we draw histograms of the gap scores for the two retrievers, as shown in Figure  6 . It indicates that our BShared-RAG retriever can better distinguish the ground-truth document from other similar documents as the gap scores are positive for more questions. In contrast,  BGE-large-zh  has a more flat distribution, which means it cannot estimate a larger similarity of the ground-truth document for many questions."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Training data of Continual pretraining",
        "table": "A2.T7.1",
        "footnotes": [],
        "references": [
            "The detailed training datasets are listed in Table  7 . For the e-commerce domain dataset, besides our WorthBuying dataset we collect e-commerce relevant notes from Xiaohongshu and Zhihu websites. For the generation domain data, we use the open-source pertaining dataset from Linly-OpenLLaMA  4 4 4 https://github.com/CVI-SZU/Linly  and WanJuan Corpus  Qiu et al. ( 2024 )"
        ]
    },
    "global_footnotes": [
        "."
    ]
}