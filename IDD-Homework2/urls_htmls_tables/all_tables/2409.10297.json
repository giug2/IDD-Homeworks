{
    "id_table_1": {
        "caption": "Table 1:  Descriptors for texture prompts.   \\emptyset   indicates an empty string.",
        "table": "Sx3.T1.4",
        "footnotes": [],
        "references": [
            "In this work, we introduce an extensible methodology for generating high-quality, representative, diverse texture images capable of supporting a broad range of texture-based tasks. Our approach: (1) constructs prompts from a range of descriptors to serve as input to text-to-image models, (2) adopts and adapts Stable Diffusion pipelines, which generates and filters the corresponding images created from our prompts, and (3) further filters the generated textures down to the highest-quality samples through CLIP scores. With this, we create the Prompted Textures Dataset (PTD): a dataset of 362,880 texture images (examples in Figure  1(b) ) across 56 different texture classes.",
            "An overview of all the descriptor categories and the specific words used in each category are shown in Table  1 . We produce prompts combining one word from each category, in standard English adjective ordering:",
            "With these adaptations made to the Stable Diffusion pipeline, we then generate 5 images for each of our 96,768 prompts. This results in the creation of 483,840 images before any additional filtering, not including the images flagged by the safety filter. A randomly selected subset of the images we generate can be seen in Figure  1(b) .",
            "When observing differences between images that passed the safety filter and the images that did not (e.g., images from Figure  1(b)  vs. the images in Figure  3 ), we find that the flagged images are typically smoother, more dull in color, and appear noisier. Besides these observations, we do not find any clear and obvious differences between the image sets or any indication as to why these images are flagged."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Top and bottom 5 CLIP scores across descriptor word pairs.",
        "table": "Sx3.T2.1",
        "footnotes": [],
        "references": [
            "To address the gaps in current texture image datasets, we introduce our methodology for creating high quality and diverse texture data, and how we apply this methodology to create the Prompted Textures Dataset (PTD). An overview of our methodology is shown in Figure  2 . All experiments in the following subsections were run on 12 A100 GPUs with 40GB of memory each using CUDA version 11.8.",
            "Table  2  shows the top and bottom 5 mean CLIP scores across all descriptor word pairs. Among the prompt pairs that do tend toward the top or bottom, we see some word pairing clusters. The texture woven appears to lead to higher quality images when paired with basic colors such as red, green, and blue. In contrast, more subtle textures such as gauzy and veined seem to result in lower CLIP scores when paired with descriptors that are designed to make the image more subtle, such as muted and earthy. From this, we find that some word pairs are more compatible than others and that this can influence resulting image quality."
        ]
    }
}