{
    "S4.T1": {
        "caption": "Table 1: Average PEHE and its standard error. N0ssuperscriptsubscriptğ‘0sN_{0}^{\\rm{s}} and N1ssuperscriptsubscriptğ‘1sN_{1}^{\\rm{s}} are number of untreated and treated support instances. Values in bold are not statistically different at 5% level from best performing method in each dataset by a paired t-test.",
        "table": null,
        "footnotes": [],
        "references": [
            "TableÂ 1 presents the average PEHE of each method.\nOur proposed method achieved the best performance in every case.\nThe PEHE of DR-CFS was higher than our proposed method, although\nboth methods used the same model architecture.\nThis is because DR-CFS performed meta-learning by minimizing the regression\nand classification losses. On the other hand,\nthe proposed method performs meta-learning by maximizing the CATE estimation\nperformance using pseudo CATEs.\nAlthough DR-ML and Meta-CI also use pseudo CATEs\nsince they did not obtain the optimal task-specific parameters\nin a closed form, their PEHEs were higher than our method.\nThis result indicates the effectiveness of\nour strategy that optimizes the task-specific parameters with closed-form solvers.",
            "We evaluated the performance of our method when using other CATE estimation methods\nthan the DR-Learner. In particular, we tested\nthe RA-Learner (RA)Â [10]\nand plugin learners (Plugin) using the same model architectures as the DR-Learner.\nAs described in SectionÂ 3.5.1, with all these CATE estimation methods,\nour proposed method offers differentiable closed-form solvers for task adaptation.\nTableÂ 2 shows the results.\nAlthough all the learners were comparable on\nthe Synth dataset with Ns=6,10superscriptğ‘s610N^{\\rm{s}}=6,10\nand IHDP dataset with Ns=14superscriptğ‘s14N^{\\rm{s}}=14,\nthe DR-Learner achieved better performance on the IHDP dataset with Ns=6superscriptğ‘s6N^{\\rm{s}}=6.\nCompared with the performance of the baseline methods in TableÂ 1,\nour proposed method with all the learners achieved good performance,\nwhich indicates the effectiveness of closed-form solvers\nfor task adaptation.",
            "TableÂ 3 shows the average PEHE\nwhen using Gaussian processes (GPs) for task adaptation\nin the proposed method as described in SectionÂ 3.5.2.\nLinear is the proposed method using linear models for task adaptation\nas described in SectionÂ 3.3,\nwhich corresponds to Ours in TableÂ 1.\nThe performance with GPs was comparable to that with linear models.\nThis result indicates that the proposed method also works well when GPs\nare used for task adaptation instead of linear models."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Average PEHE and its standard error of proposed method with DR-, RA-, and Plugin-Learners. Values in bold are not statistically different at 5% level from best performing method in each dataset by a paired t-test.",
        "table": null,
        "footnotes": [],
        "references": [
            "We evaluated the performance of our method when using other CATE estimation methods\nthan the DR-Learner. In particular, we tested\nthe RA-Learner (RA)Â [10]\nand plugin learners (Plugin) using the same model architectures as the DR-Learner.\nAs described in SectionÂ 3.5.1, with all these CATE estimation methods,\nour proposed method offers differentiable closed-form solvers for task adaptation.\nTableÂ 2 shows the results.\nAlthough all the learners were comparable on\nthe Synth dataset with Ns=6,10superscriptğ‘s610N^{\\rm{s}}=6,10\nand IHDP dataset with Ns=14superscriptğ‘s14N^{\\rm{s}}=14,\nthe DR-Learner achieved better performance on the IHDP dataset with Ns=6superscriptğ‘s6N^{\\rm{s}}=6.\nCompared with the performance of the baseline methods in TableÂ 1,\nour proposed method with all the learners achieved good performance,\nwhich indicates the effectiveness of closed-form solvers\nfor task adaptation."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Average PEHE and its standard error of proposed method with linear models (Linear) and Gaussian processes (GP) for task adaptation. Values in bold are not statistically different at 5% level from best performing method in each dataset by a paired t-test.",
        "table": null,
        "footnotes": [],
        "references": [
            "TableÂ 3 shows the average PEHE\nwhen using Gaussian processes (GPs) for task adaptation\nin the proposed method as described in SectionÂ 3.5.2.\nLinear is the proposed method using linear models for task adaptation\nas described in SectionÂ 3.3,\nwhich corresponds to Ours in TableÂ 1.\nThe performance with GPs was comparable to that with linear models.\nThis result indicates that the proposed method also works well when GPs\nare used for task adaptation instead of linear models."
        ]
    },
    "S4.T4": {
        "caption": "Table 4: Computation time in seconds for meta-training with Ns=6superscriptğ‘s6N^{\\rm{s}}=6.",
        "table": "<table id=\"S4.T4.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T4.3.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.3.1.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_t\"/>\n<th id=\"S4.T4.3.1.1.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\">Ours</th>\n<th id=\"S4.T4.3.1.1.3\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\">DR-CFS</th>\n<th id=\"S4.T4.3.1.1.4\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\">DR-ML</th>\n<th id=\"S4.T4.3.1.1.5\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\">Meta-CI</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T4.3.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.3.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Synth</th>\n<td id=\"S4.T4.3.2.1.2\" class=\"ltx_td ltx_align_right ltx_border_t\">1402.7</td>\n<td id=\"S4.T4.3.2.1.3\" class=\"ltx_td ltx_align_right ltx_border_t\">1454.6</td>\n<td id=\"S4.T4.3.2.1.4\" class=\"ltx_td ltx_align_right ltx_border_t\">2009.5</td>\n<td id=\"S4.T4.3.2.1.5\" class=\"ltx_td ltx_align_right ltx_border_t\">2634.1</td>\n</tr>\n<tr id=\"S4.T4.3.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T4.3.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\">IHDP</th>\n<td id=\"S4.T4.3.3.2.2\" class=\"ltx_td ltx_align_right ltx_border_b\">442.3</td>\n<td id=\"S4.T4.3.3.2.3\" class=\"ltx_td ltx_align_right ltx_border_b\">489.9</td>\n<td id=\"S4.T4.3.3.2.4\" class=\"ltx_td ltx_align_right ltx_border_b\">1003.8</td>\n<td id=\"S4.T4.3.3.2.5\" class=\"ltx_td ltx_align_right ltx_border_b\">1751.9</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "To see how computationally efficient these closed-form solvers are, we evaluated\nthe computation time that is required to perform meta-learning\nusing computers with CentOS, Xeon Gold 6130 2.10GHz CPU, and 256GB memory.\nTableÂ 4 shows the results.\nCompared with the methods that perform iterative optimization for adaptation (i.e., DR-ML and Meta-CI),\nthose using closed-form solvers (i.e., Ours and DR-CFS)\nneeded much shorter times, implying that adapting task-specific parameters with closed-form solvers\nis far more computationally efficient."
        ]
    }
}