{
    "PAPER'S NUMBER OF TABLES": 3,
    "S1.T1": {
        "caption": "Table 1. Types of Defenses and Their Implementations Against Specific Attacks",
        "table": "<table id=\"S1.T1.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S1.T1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S1.T1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Type of Defenses</span></td>\n<td id=\"S1.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S1.T1.1.2.1.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Implementations</span></td>\n<td id=\"S1.T1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S1.T1.1.2.1.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Type of Attacks Against</span></td>\n</tr>\n<tr id=\"S1.T1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"7\"><span id=\"S1.T1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Before-aggregation defenses</span></td>\n<td id=\"S1.T1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S1.T1.1.3.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">SLSGD </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.3.2.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Xie et al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.3.2.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib89\" title=\"\" class=\"ltx_ref\">2020</a><span id=\"S1.T1.1.3.2.2.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S1.T1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S1.T1.1.3.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Data poisoning attacks, e.g., label flipping backdoor attack </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.3.2.3.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Tolpegin et al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.3.2.3.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib81\" title=\"\" class=\"ltx_ref\">2020</a><span id=\"S1.T1.1.3.2.3.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S1.T1.1.3.2.3.5\" class=\"ltx_text\" style=\"font-size:80%;\">,</span>\n</td>\n</tr>\n<tr id=\"S1.T1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S1.T1.1.4.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Residual Reweighting Defense </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.4.3.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Fu et al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.4.3.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S1.T1.1.4.3.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S1.T1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S1.T1.1.4.3.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Backdoor attacks</span></td>\n</tr>\n<tr id=\"S1.T1.1.5.4\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S1.T1.1.5.4.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Foolsgold </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.5.4.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Fung et al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.5.4.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">2020</a><span id=\"S1.T1.1.5.4.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S1.T1.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S1.T1.1.5.4.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Backdoor attacks and Byzantine attacks</span></td>\n</tr>\n<tr id=\"S1.T1.1.1\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S1.T1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Krum </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Blanchard et al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S1.T1.1.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S1.T1.1.1.1.5\" class=\"ltx_text\" style=\"font-size:80%;\">  </span><math id=\"S1.T1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"m\" display=\"inline\"><semantics id=\"S1.T1.1.1.1.m1.1a\"><mi mathsize=\"80%\" id=\"S1.T1.1.1.1.m1.1.1\" xref=\"S1.T1.1.1.1.m1.1.1.cmml\">m</mi><annotation-xml encoding=\"MathML-Content\" id=\"S1.T1.1.1.1.m1.1b\"><ci id=\"S1.T1.1.1.1.m1.1.1.cmml\" xref=\"S1.T1.1.1.1.m1.1.1\">𝑚</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.T1.1.1.1.m1.1c\">m</annotation></semantics></math><span id=\"S1.T1.1.1.1.6\" class=\"ltx_text\" style=\"font-size:80%;\">-Krum </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.1.1.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Blanchard et al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.1.1.8.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S1.T1.1.1.1.9.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S1.T1.1.1.1.10\" class=\"ltx_text\" style=\"font-size:80%;\"> CClip </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.1.1.11.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Karimireddy et al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.1.1.12.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">2020</a><span id=\"S1.T1.1.1.1.13.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S1.T1.1.1.1.14\" class=\"ltx_text\" style=\"font-size:80%;\"> weak DP </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.1.1.15.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Sun et al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.1.1.16.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib79\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S1.T1.1.1.1.17.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S1.T1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"4\"><span id=\"S1.T1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Model poisoning attacks, <span id=\"S1.T1.1.1.2.1.1\" class=\"ltx_text ltx_font_italic\">e</span>.<span id=\"S1.T1.1.1.2.1.2\" class=\"ltx_text ltx_font_italic\">g</span>., Byzantine attacks <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">2017</a>; Fang et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">2020</a>; Lin et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib54\" title=\"\" class=\"ltx_ref\">2019</a>)</cite> or Backdoor attacks that attack by poisoning model updates  <cite class=\"ltx_cite ltx_citemacro_citep\">(Bagdasaryan et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\">2020</a>)</cite></span></td>\n</tr>\n<tr id=\"S1.T1.1.6.5\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_border_r\">\n<span id=\"S1.T1.1.6.5.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Norm Clipping </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.6.5.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Sun et al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.6.5.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib79\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S1.T1.1.6.5.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S1.T1.1.6.5.1.5\" class=\"ltx_text\" style=\"font-size:80%;\"> Fl-wbc </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.6.5.1.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Sun et al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.6.5.1.7.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib78\" title=\"\" class=\"ltx_ref\">2021</a><span id=\"S1.T1.1.6.5.1.8.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S1.T1.1.6.5.1.9\" class=\"ltx_text\" style=\"font-size:80%;\">  Bulyan Defense </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.6.5.1.10.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Guerraoui et al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.6.5.1.11.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib32\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S1.T1.1.6.5.1.12.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n</tr>\n<tr id=\"S1.T1.1.7.6\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.7.6.1\" class=\"ltx_td ltx_align_center ltx_border_r\">\n<span id=\"S1.T1.1.7.6.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">coordinate-wise median </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.7.6.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Yin et al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.7.6.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib93\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S1.T1.1.7.6.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n</tr>\n<tr id=\"S1.T1.1.8.7\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.8.7.1\" class=\"ltx_td ltx_align_center ltx_border_r\">\n<span id=\"S1.T1.1.8.7.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">coordinate-wise trimmed mean </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.8.7.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Yin et al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.8.7.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib93\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S1.T1.1.8.7.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n</tr>\n<tr id=\"S1.T1.1.9.8\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.9.8.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"4\"><span id=\"S1.T1.1.9.8.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">On-aggregation defenses</span></td>\n<td id=\"S1.T1.1.9.8.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S1.T1.1.9.8.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Robust Learning Rate </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.9.8.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Ozdayi et al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.9.8.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib65\" title=\"\" class=\"ltx_ref\">2021</a><span id=\"S1.T1.1.9.8.2.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S1.T1.1.9.8.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S1.T1.1.9.8.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Backdoor attacks</span></td>\n</tr>\n<tr id=\"S1.T1.1.10.9\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.10.9.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S1.T1.1.10.9.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">SLSGD </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.10.9.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Xie et al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.10.9.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib89\" title=\"\" class=\"ltx_ref\">2020</a><span id=\"S1.T1.1.10.9.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S1.T1.1.10.9.2\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S1.T1.1.10.9.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Data poisoning attacks, e.g., label flipping backdoor attack </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.10.9.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Tolpegin et al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.10.9.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib81\" title=\"\" class=\"ltx_ref\">2020</a><span id=\"S1.T1.1.10.9.2.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S1.T1.1.10.9.2.5\" class=\"ltx_text\" style=\"font-size:80%;\">,</span>\n</td>\n</tr>\n<tr id=\"S1.T1.1.11.10\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.11.10.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S1.T1.1.11.10.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">geometric median </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.11.10.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Chen et al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.11.10.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">2017</a><span id=\"S1.T1.1.11.10.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S1.T1.1.11.10.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S1.T1.1.11.10.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Byzantine attacks</span></td>\n</tr>\n<tr id=\"S1.T1.1.12.11\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.12.11.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S1.T1.1.12.11.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">RFA </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.12.11.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Pillutla et al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.12.11.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib66\" title=\"\" class=\"ltx_ref\">2022</a><span id=\"S1.T1.1.12.11.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S1.T1.1.12.11.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S1.T1.1.12.11.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Byzantine attacks</span></td>\n</tr>\n<tr id=\"S1.T1.1.13.12\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.13.12.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S1.T1.1.13.12.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">After-aggregation defenses</span></td>\n<td id=\"S1.T1.1.13.12.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S1.T1.1.13.12.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">CClip </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.13.12.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Karimireddy et al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.13.12.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">2020</a><span id=\"S1.T1.1.13.12.2.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S1.T1.1.13.12.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S1.T1.1.13.12.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Byzantine attacks or backdoor attacks</span></td>\n</tr>\n<tr id=\"S1.T1.1.14.13\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.14.13.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">\n<span id=\"S1.T1.1.14.13.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">CRFL </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.1.14.13.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Xie et al<span class=\"ltx_text\">.</span><span id=\"S1.T1.1.14.13.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib88\" title=\"\" class=\"ltx_ref\">2021</a><span id=\"S1.T1.1.14.13.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite>\n</td>\n<td id=\"S1.T1.1.14.13.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S1.T1.1.14.13.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Backdoor attacks</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Federated Learning (FL) ",
                "(McMahan et al",
                ".",
                ", ",
                "2017a",
                ")",
                " facilitates training across distributed data and empowers individual clients to utilize their local data to collaboratively train machine learning models. Instead of collecting data to a centralized server, FL clients train models on their local data and share the local models with the FL server, where the local models are aggregated into a global model.",
                "FL has attracted considerable attention across various domains and has been utilized in numerous areas such as next-word prediction ",
                "(Hard et al",
                ".",
                ", ",
                "2018",
                "; Chen et al",
                ".",
                ", ",
                "2019",
                "; Ramaswamy et al",
                ".",
                ", ",
                "2019",
                ")",
                ", hot-word detection ",
                "(Leroy et al",
                ".",
                ", ",
                "2019",
                ")",
                ", financial risk assessment ",
                "(Byrd and Polychroniadou, ",
                "2020",
                ")",
                ", and cancer risk prediction ",
                "(Chowdhury et al",
                ".",
                ", ",
                "2022",
                ")",
                ", demonstrating its wide-ranging versatility.\nRecently, FL has found applications in large language models (LLMs) that expand its use cases. Referred to as ",
                "federated LLMs",
                ", these models utilize FL during pre-training and finetuning as well as for prompt engineering ",
                "(Chen et al",
                ".",
                ", ",
                "2023",
                ")",
                ". Currently, there are industry products that utilize FL (or distributed training) to train LLMs, including Deepspeed ZeRO ",
                "(Rajbhandari et al",
                ".",
                ", ",
                "2020",
                "; Wang et al",
                ".",
                ", ",
                "2023",
                ")",
                ", HuggingFace Accelerate ",
                "(Gugger, ",
                "2021",
                ")",
                ", Pytorch Lightning Fabric ",
                "(Antiga, ",
                "2023",
                ")",
                ". FL can facilitate LLM training due to the following reasons: ",
                "i",
                ") ",
                "Distributed nature of LLM training data:",
                " LLMs are pre-trained using large amounts of data, which often reside in different locations. Collecting such data to a central server is expensive and may also leak sensitive user information, while a viable way is to train LLMs in a federated manner.\n",
                "ii",
                ") ",
                "Scalability and efficiency:",
                " LLMs, such as GPT-3 ",
                "(Brown et al",
                ".",
                ", ",
                "2020",
                ")",
                ", have an extremely large number of parameters. Training LLMs on a single machine is infeasible and inflexible, while FL can be a good choice.\n",
                "iii",
                ") ",
                "Continuous improvement with user data:",
                " LLMs can be deployed in a federated manner and local instances of the models can be further finetuned based on the local data,\nenabling the global model to improve over time based on users’ data without ever having direct access to that data. This is particularly relevant for privacy-sensitive fields such as healthcare or personal communications.",
                "FL, as well as federated LLMs, ",
                "aims to",
                " maintain privacy and security of client data by allowing clients to train locally without spreading their data to other parties.\nHowever, its decentralized and collaborative nature might inadvertently introduce privacy and security vulnerabilities.\n",
                "Recent works have spotlighted specific attack mechanisms in FL. Adversarial clients compromise the integrity of global model by submitting spurious models to prevent the global model from converging ",
                "(Chen et al",
                ".",
                ", ",
                "2017",
                "; Fang et al",
                ".",
                ", ",
                "2020",
                "; Lin et al",
                ".",
                ", ",
                "2019",
                "; Baruch et al",
                ".",
                ", ",
                "2019",
                "; Bagdasaryan et al",
                ".",
                ", ",
                "2020",
                "; Wang, ",
                "2022",
                "; Fraboni et al",
                ".",
                ", ",
                "2021",
                ")",
                "), manipulating data samples to induce the global model to mis-classify specific samples ",
                "(Tolpegin et al",
                ".",
                ", ",
                "2020",
                "; Wang et al",
                ".",
                ", ",
                "2020b",
                ")",
                ", and / or planting backdoors ",
                "(Baruch et al",
                ".",
                ", ",
                "2019",
                "; Bagdasaryan et al",
                ".",
                ", ",
                "2020",
                "; Tolpegin et al",
                ".",
                ", ",
                "2020",
                "; Wang et al",
                ".",
                ", ",
                "2020b",
                ")",
                ". Adversaries can also reconstruct private data from shared model updates ",
                "(Zhu et al",
                ".",
                ", ",
                "2019",
                "; Geiping et al",
                ".",
                ", ",
                "2020",
                "; Dang et al",
                ".",
                ", ",
                "2021",
                ")",
                ". ",
                "\nMeanwhile, a wide range of defense mechanisms has emerged to mitigate the impact of these attacks ",
                "(Li et al",
                ".",
                ", ",
                "2022",
                "; Kumari et al",
                ".",
                ", ",
                "2023",
                "; Sun et al",
                ".",
                ", ",
                "2019",
                "; Ozdayi et al",
                ".",
                ", ",
                "2021",
                "; Blanchard et al",
                ".",
                ", ",
                "2017",
                "; Xie et al",
                ".",
                ", ",
                "2020",
                "; Chen et al",
                ".",
                ", ",
                "2017",
                "; Sun et al",
                ".",
                ", ",
                "2019",
                "; Karimireddy et al",
                ".",
                ", ",
                "2020",
                "; Yin et al",
                ".",
                ", ",
                "2018",
                "; Pillutla et al",
                ".",
                ", ",
                "2022",
                "; Fung et al",
                ".",
                ", ",
                "2020",
                "; Xie et al",
                ".",
                ", ",
                "2021",
                "; Yin et al",
                ".",
                ", ",
                "2018",
                "; Ma et al",
                ".",
                ", ",
                "2022",
                "; Kumar et al",
                ".",
                ", ",
                "2022",
                "; Chen et al",
                ".",
                ", ",
                "2022",
                ")",
                ".\nDespite the efforts for addressing the vulnerability of FL systems, there still lacks a comprehensive benchmark for comparing approaches under unified sittings.\nMoreover, while existing works have explored effectiveness of attacks and defenses on small-scale models, there remains a significant gap in understanding how these mechanisms perform against large-scale models, such as LLMs. Given that LLMs possess a large number of parameters and are trained on complex datasets obtained from unregulated sources, the effectiveness of attacks and defenses may be diminished when applied to them.\nThese motivate an urgent need for a standardized and comprehensive benchmark to evaluate baseline attack and defense mechanisms in the context of FL and federated LLMs.",
                "This paper introduces FedMLSecurity",
                "1",
                "1",
                "1",
                "Code: https://github.com/FedML-AI/FedML/tree/master/python/fedml/core/security",
                ", a benchmark that simulates attacks and defenses in FedML ",
                "(He et al",
                ".",
                ", ",
                "2020b",
                ")",
                ".\nFedMLSecurity comprises two primary components: FedMLAttacker and FedMLDefender.\nFedMLAttacker simulates attacks in FL to help understand and prepare for potential security risks, while\nFedMLDefender is equipped with ",
                "state-of-the-arts",
                " defense mechanisms to counteract the attacks injected by FedMLAttacker. We summarize our contributions as follows:\n",
                "i",
                ") Enabling benchmarking of several different attacks and defenses in FL",
                ". FedMLSecurity implements attacks and defenses that are widely considered in the literature. ",
                "We summarize the defenses and the attacks in Table ",
                "1",
                " and Table ",
                "2",
                ", respectively.",
                "ii",
                ") Supporting flexible configuration and customization. ",
                "\nFedMLSecurity supports configurations using a .yaml file. Sample configurations for attacks and defenses are shown in Figures ",
                "1(a)",
                "\nand Figures ",
                "1(b)",
                ", respectively. FedMLSecurity also provides APIs to enable customizing attacks and defenses.",
                "iii",
                ") Supporting various models and FL optimizers.",
                "\nFedMLSecurity can be utilized with a wide range of models, including Logistic Regression, LeNet ",
                "(LeCun et al",
                ".",
                ", ",
                "1998",
                ")",
                ", ResNet ",
                "(He et al",
                ".",
                ", ",
                "2015",
                ")",
                ", CNN ",
                "(LeCun et al",
                ".",
                ", ",
                "1989",
                ")",
                ", RNN ",
                "(Rumelhart et al",
                ".",
                ", ",
                "1986",
                ")",
                ", GAN ",
                "(Goodfellow et al",
                ".",
                ", ",
                "2014",
                ")",
                ", and so on. FedMLSecurity is compatible with various FL optimizers, such as FedAVG ",
                "(McMahan et al",
                ".",
                ", ",
                "2016",
                ")",
                ", FedSGD ",
                "(Shokri and Shmatikov, ",
                "2015",
                ")",
                ", FedOPT ",
                "(Reddi et al",
                ".",
                ", ",
                "2021",
                ")",
                ", FedPROX ",
                "(Li et al",
                ".",
                ", ",
                "2020",
                ")",
                ", FedGKT ",
                "(He et al",
                ".",
                ", ",
                "2020a",
                ")",
                ", FedGAN ",
                "(Rasouli et al",
                ".",
                ", ",
                "2020",
                ")",
                ", FedNAS ",
                "(He et al",
                ".",
                ", ",
                "2021",
                ")",
                ", FedNOVA ",
                "(Wang et al",
                ".",
                ", ",
                "2020a",
                ")",
                ", etc.",
                "iv",
                ") Extensions to federated LLMs and real-world applications.",
                "\nFedMLSecurity can simulate attacks and defenses during training of federated LLMs. It can also be integrated with real-world FL applications; see ",
                "Exp 7",
                ", where we utilize edge devices from Theta Network ",
                "(Theta Network., ",
                "2023",
                ")",
                " instead of simulations.",
                "Key takeaways",
                ": ",
                "i",
                ")\nWhile defense mechanisms can help mitigate attacks, it might also bring a potential loss of accuracy to the aggregation results. Therefore, when integrating defenses into FL applications, it’s crucial to weigh the benefits against potential drawbacks.\n",
                "ii",
                ") Nearly all existing defense mechanisms are impractical in real-world FL applications, as they compromise accuracy even if no attack happened. A defense that is practical for real-world systems is in need, where the defense should satisfy: 1) it must detect if attacks have happened, and only activates defensive mechanisms when attacks are detected; and 2) it must identify malicious clients accurately without harming benign local models. "
            ]
        ]
    },
    "S1.T2": {
        "caption": "Table 2. Attacks Implemented in FedMLSecurity",
        "table": "<table id=\"S1.T2.6\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S1.T2.6.1.1\" class=\"ltx_tr\">\n<th id=\"S1.T2.6.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\">\n<span id=\"S1.T2.6.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T2.6.1.1.1.1.1\" class=\"ltx_p\" style=\"width:74.0pt;\"><span id=\"S1.T2.6.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Type of Attacks</span></span>\n</span>\n</th>\n<th id=\"S1.T2.6.1.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span id=\"S1.T2.6.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T2.6.1.1.2.1.1\" class=\"ltx_p\" style=\"width:153.6pt;\"><span id=\"S1.T2.6.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Implementations</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S1.T2.6.2.1\" class=\"ltx_tr\">\n<th id=\"S1.T2.6.2.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"4\">\n<span id=\"S1.T2.6.2.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T2.6.2.1.1.1.1\" class=\"ltx_p\" style=\"width:74.0pt;\"><span id=\"S1.T2.6.2.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Model poisoning attacks</span></span>\n</span>\n</th>\n<td id=\"S1.T2.6.2.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S1.T2.6.2.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T2.6.2.1.2.1.1\" class=\"ltx_p\" style=\"width:153.6pt;\"><span id=\"S1.T2.6.2.1.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Byzantine attack </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T2.6.2.1.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Chen et al<span class=\"ltx_text\">.</span><span id=\"S1.T2.6.2.1.2.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">2017</a>; Fang et al<span class=\"ltx_text\">.</span><span id=\"S1.T2.6.2.1.2.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">2020</a>; Lin et al<span class=\"ltx_text\">.</span><span id=\"S1.T2.6.2.1.2.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib54\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S1.T2.6.2.1.2.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite><span id=\"S1.T2.6.2.1.2.1.1.5\" class=\"ltx_text\" style=\"font-size:80%;\">: (1) zero mode (2) random mode (3) flipping mode</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S1.T2.6.3.2\" class=\"ltx_tr\">\n<td id=\"S1.T2.6.3.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S1.T2.6.3.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T2.6.3.2.1.1.1\" class=\"ltx_p\" style=\"width:153.6pt;\"><span id=\"S1.T2.6.3.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Minimizing Distance Backdoor Attack </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T2.6.3.2.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Baruch et al<span class=\"ltx_text\">.</span><span id=\"S1.T2.6.3.2.1.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib5\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S1.T2.6.3.2.1.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>\n</span>\n</td>\n</tr>\n<tr id=\"S1.T2.6.4.3\" class=\"ltx_tr\">\n<td id=\"S1.T2.6.4.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S1.T2.6.4.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T2.6.4.3.1.1.1\" class=\"ltx_p\" style=\"width:153.6pt;\"><span id=\"S1.T2.6.4.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Model Replacement Backdoor Attack </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T2.6.4.3.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Bagdasaryan et al<span class=\"ltx_text\">.</span><span id=\"S1.T2.6.4.3.1.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\">2020</a><span id=\"S1.T2.6.4.3.1.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>\n</span>\n</td>\n</tr>\n<tr id=\"S1.T2.6.5.4\" class=\"ltx_tr\">\n<td id=\"S1.T2.6.5.4.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S1.T2.6.5.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T2.6.5.4.1.1.1\" class=\"ltx_p\" style=\"width:153.6pt;\"><span id=\"S1.T2.6.5.4.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Lazy Worker (or Free Rider) Attack </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T2.6.5.4.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Wang<span id=\"S1.T2.6.5.4.1.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib85\" title=\"\" class=\"ltx_ref\">2022</a>; Fraboni et al<span class=\"ltx_text\">.</span><span id=\"S1.T2.6.5.4.1.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">2021</a><span id=\"S1.T2.6.5.4.1.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>\n</span>\n</td>\n</tr>\n<tr id=\"S1.T2.6.6.5\" class=\"ltx_tr\">\n<th id=\"S1.T2.6.6.5.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"2\">\n<span id=\"S1.T2.6.6.5.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T2.6.6.5.1.1.1\" class=\"ltx_p\" style=\"width:74.0pt;\"><span id=\"S1.T2.6.6.5.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Data poisoning attacks</span></span>\n</span>\n</th>\n<td id=\"S1.T2.6.6.5.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S1.T2.6.6.5.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T2.6.6.5.2.1.1\" class=\"ltx_p\" style=\"width:153.6pt;\"><span id=\"S1.T2.6.6.5.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Label Flipping Backdoor attack </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T2.6.6.5.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Tolpegin et al<span class=\"ltx_text\">.</span><span id=\"S1.T2.6.6.5.2.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib81\" title=\"\" class=\"ltx_ref\">2020</a><span id=\"S1.T2.6.6.5.2.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>\n</span>\n</td>\n</tr>\n<tr id=\"S1.T2.6.7.6\" class=\"ltx_tr\">\n<td id=\"S1.T2.6.7.6.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S1.T2.6.7.6.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T2.6.7.6.1.1.1\" class=\"ltx_p\" style=\"width:153.6pt;\"><span id=\"S1.T2.6.7.6.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Edge Case Backdoor Attack </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T2.6.7.6.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Wang et al<span class=\"ltx_text\">.</span><span id=\"S1.T2.6.7.6.1.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib84\" title=\"\" class=\"ltx_ref\">2020b</a><span id=\"S1.T2.6.7.6.1.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>\n</span>\n</td>\n</tr>\n<tr id=\"S1.T2.6.8.7\" class=\"ltx_tr\">\n<th id=\"S1.T2.6.8.7.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" rowspan=\"3\">\n<span id=\"S1.T2.6.8.7.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T2.6.8.7.1.1.1\" class=\"ltx_p\" style=\"width:74.0pt;\"><span id=\"S1.T2.6.8.7.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Data reconstruction attacks</span></span>\n</span>\n</th>\n<td id=\"S1.T2.6.8.7.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S1.T2.6.8.7.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T2.6.8.7.2.1.1\" class=\"ltx_p\" style=\"width:153.6pt;\"><span id=\"S1.T2.6.8.7.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Deep Leakage Attack </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T2.6.8.7.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Zhu et al<span class=\"ltx_text\">.</span><span id=\"S1.T2.6.8.7.2.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib96\" title=\"\" class=\"ltx_ref\">2019</a><span id=\"S1.T2.6.8.7.2.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>\n</span>\n</td>\n</tr>\n<tr id=\"S1.T2.6.9.8\" class=\"ltx_tr\">\n<td id=\"S1.T2.6.9.8.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S1.T2.6.9.8.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T2.6.9.8.1.1.1\" class=\"ltx_p\" style=\"width:153.6pt;\"><span id=\"S1.T2.6.9.8.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Inverting Gradient Attack </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T2.6.9.8.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Geiping et al<span class=\"ltx_text\">.</span><span id=\"S1.T2.6.9.8.1.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">2020</a><span id=\"S1.T2.6.9.8.1.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>\n</span>\n</td>\n</tr>\n<tr id=\"S1.T2.6.10.9\" class=\"ltx_tr\">\n<td id=\"S1.T2.6.10.9.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\">\n<span id=\"S1.T2.6.10.9.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T2.6.10.9.1.1.1\" class=\"ltx_p\" style=\"width:153.6pt;\"><span id=\"S1.T2.6.10.9.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Revealing Labels Attack </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T2.6.10.9.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">(</span>Dang et al<span class=\"ltx_text\">.</span><span id=\"S1.T2.6.10.9.1.1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">, </span><a href=\"#bib.bib19\" title=\"\" class=\"ltx_ref\">2021</a><span id=\"S1.T2.6.10.9.1.1.1.4.3\" class=\"ltx_text\" style=\"font-size:80%;\">)</span></cite></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Federated Learning (FL) ",
                "(McMahan et al",
                ".",
                ", ",
                "2017a",
                ")",
                " facilitates training across distributed data and empowers individual clients to utilize their local data to collaboratively train machine learning models. Instead of collecting data to a centralized server, FL clients train models on their local data and share the local models with the FL server, where the local models are aggregated into a global model.",
                "FL has attracted considerable attention across various domains and has been utilized in numerous areas such as next-word prediction ",
                "(Hard et al",
                ".",
                ", ",
                "2018",
                "; Chen et al",
                ".",
                ", ",
                "2019",
                "; Ramaswamy et al",
                ".",
                ", ",
                "2019",
                ")",
                ", hot-word detection ",
                "(Leroy et al",
                ".",
                ", ",
                "2019",
                ")",
                ", financial risk assessment ",
                "(Byrd and Polychroniadou, ",
                "2020",
                ")",
                ", and cancer risk prediction ",
                "(Chowdhury et al",
                ".",
                ", ",
                "2022",
                ")",
                ", demonstrating its wide-ranging versatility.\nRecently, FL has found applications in large language models (LLMs) that expand its use cases. Referred to as ",
                "federated LLMs",
                ", these models utilize FL during pre-training and finetuning as well as for prompt engineering ",
                "(Chen et al",
                ".",
                ", ",
                "2023",
                ")",
                ". Currently, there are industry products that utilize FL (or distributed training) to train LLMs, including Deepspeed ZeRO ",
                "(Rajbhandari et al",
                ".",
                ", ",
                "2020",
                "; Wang et al",
                ".",
                ", ",
                "2023",
                ")",
                ", HuggingFace Accelerate ",
                "(Gugger, ",
                "2021",
                ")",
                ", Pytorch Lightning Fabric ",
                "(Antiga, ",
                "2023",
                ")",
                ". FL can facilitate LLM training due to the following reasons: ",
                "i",
                ") ",
                "Distributed nature of LLM training data:",
                " LLMs are pre-trained using large amounts of data, which often reside in different locations. Collecting such data to a central server is expensive and may also leak sensitive user information, while a viable way is to train LLMs in a federated manner.\n",
                "ii",
                ") ",
                "Scalability and efficiency:",
                " LLMs, such as GPT-3 ",
                "(Brown et al",
                ".",
                ", ",
                "2020",
                ")",
                ", have an extremely large number of parameters. Training LLMs on a single machine is infeasible and inflexible, while FL can be a good choice.\n",
                "iii",
                ") ",
                "Continuous improvement with user data:",
                " LLMs can be deployed in a federated manner and local instances of the models can be further finetuned based on the local data,\nenabling the global model to improve over time based on users’ data without ever having direct access to that data. This is particularly relevant for privacy-sensitive fields such as healthcare or personal communications.",
                "FL, as well as federated LLMs, ",
                "aims to",
                " maintain privacy and security of client data by allowing clients to train locally without spreading their data to other parties.\nHowever, its decentralized and collaborative nature might inadvertently introduce privacy and security vulnerabilities.\n",
                "Recent works have spotlighted specific attack mechanisms in FL. Adversarial clients compromise the integrity of global model by submitting spurious models to prevent the global model from converging ",
                "(Chen et al",
                ".",
                ", ",
                "2017",
                "; Fang et al",
                ".",
                ", ",
                "2020",
                "; Lin et al",
                ".",
                ", ",
                "2019",
                "; Baruch et al",
                ".",
                ", ",
                "2019",
                "; Bagdasaryan et al",
                ".",
                ", ",
                "2020",
                "; Wang, ",
                "2022",
                "; Fraboni et al",
                ".",
                ", ",
                "2021",
                ")",
                "), manipulating data samples to induce the global model to mis-classify specific samples ",
                "(Tolpegin et al",
                ".",
                ", ",
                "2020",
                "; Wang et al",
                ".",
                ", ",
                "2020b",
                ")",
                ", and / or planting backdoors ",
                "(Baruch et al",
                ".",
                ", ",
                "2019",
                "; Bagdasaryan et al",
                ".",
                ", ",
                "2020",
                "; Tolpegin et al",
                ".",
                ", ",
                "2020",
                "; Wang et al",
                ".",
                ", ",
                "2020b",
                ")",
                ". Adversaries can also reconstruct private data from shared model updates ",
                "(Zhu et al",
                ".",
                ", ",
                "2019",
                "; Geiping et al",
                ".",
                ", ",
                "2020",
                "; Dang et al",
                ".",
                ", ",
                "2021",
                ")",
                ". ",
                "\nMeanwhile, a wide range of defense mechanisms has emerged to mitigate the impact of these attacks ",
                "(Li et al",
                ".",
                ", ",
                "2022",
                "; Kumari et al",
                ".",
                ", ",
                "2023",
                "; Sun et al",
                ".",
                ", ",
                "2019",
                "; Ozdayi et al",
                ".",
                ", ",
                "2021",
                "; Blanchard et al",
                ".",
                ", ",
                "2017",
                "; Xie et al",
                ".",
                ", ",
                "2020",
                "; Chen et al",
                ".",
                ", ",
                "2017",
                "; Sun et al",
                ".",
                ", ",
                "2019",
                "; Karimireddy et al",
                ".",
                ", ",
                "2020",
                "; Yin et al",
                ".",
                ", ",
                "2018",
                "; Pillutla et al",
                ".",
                ", ",
                "2022",
                "; Fung et al",
                ".",
                ", ",
                "2020",
                "; Xie et al",
                ".",
                ", ",
                "2021",
                "; Yin et al",
                ".",
                ", ",
                "2018",
                "; Ma et al",
                ".",
                ", ",
                "2022",
                "; Kumar et al",
                ".",
                ", ",
                "2022",
                "; Chen et al",
                ".",
                ", ",
                "2022",
                ")",
                ".\nDespite the efforts for addressing the vulnerability of FL systems, there still lacks a comprehensive benchmark for comparing approaches under unified sittings.\nMoreover, while existing works have explored effectiveness of attacks and defenses on small-scale models, there remains a significant gap in understanding how these mechanisms perform against large-scale models, such as LLMs. Given that LLMs possess a large number of parameters and are trained on complex datasets obtained from unregulated sources, the effectiveness of attacks and defenses may be diminished when applied to them.\nThese motivate an urgent need for a standardized and comprehensive benchmark to evaluate baseline attack and defense mechanisms in the context of FL and federated LLMs.",
                "This paper introduces FedMLSecurity",
                "1",
                "1",
                "1",
                "Code: https://github.com/FedML-AI/FedML/tree/master/python/fedml/core/security",
                ", a benchmark that simulates attacks and defenses in FedML ",
                "(He et al",
                ".",
                ", ",
                "2020b",
                ")",
                ".\nFedMLSecurity comprises two primary components: FedMLAttacker and FedMLDefender.\nFedMLAttacker simulates attacks in FL to help understand and prepare for potential security risks, while\nFedMLDefender is equipped with ",
                "state-of-the-arts",
                " defense mechanisms to counteract the attacks injected by FedMLAttacker. We summarize our contributions as follows:\n",
                "i",
                ") Enabling benchmarking of several different attacks and defenses in FL",
                ". FedMLSecurity implements attacks and defenses that are widely considered in the literature. ",
                "We summarize the defenses and the attacks in Table ",
                "1",
                " and Table ",
                "2",
                ", respectively.",
                "ii",
                ") Supporting flexible configuration and customization. ",
                "\nFedMLSecurity supports configurations using a .yaml file. Sample configurations for attacks and defenses are shown in Figures ",
                "1(a)",
                "\nand Figures ",
                "1(b)",
                ", respectively. FedMLSecurity also provides APIs to enable customizing attacks and defenses.",
                "iii",
                ") Supporting various models and FL optimizers.",
                "\nFedMLSecurity can be utilized with a wide range of models, including Logistic Regression, LeNet ",
                "(LeCun et al",
                ".",
                ", ",
                "1998",
                ")",
                ", ResNet ",
                "(He et al",
                ".",
                ", ",
                "2015",
                ")",
                ", CNN ",
                "(LeCun et al",
                ".",
                ", ",
                "1989",
                ")",
                ", RNN ",
                "(Rumelhart et al",
                ".",
                ", ",
                "1986",
                ")",
                ", GAN ",
                "(Goodfellow et al",
                ".",
                ", ",
                "2014",
                ")",
                ", and so on. FedMLSecurity is compatible with various FL optimizers, such as FedAVG ",
                "(McMahan et al",
                ".",
                ", ",
                "2016",
                ")",
                ", FedSGD ",
                "(Shokri and Shmatikov, ",
                "2015",
                ")",
                ", FedOPT ",
                "(Reddi et al",
                ".",
                ", ",
                "2021",
                ")",
                ", FedPROX ",
                "(Li et al",
                ".",
                ", ",
                "2020",
                ")",
                ", FedGKT ",
                "(He et al",
                ".",
                ", ",
                "2020a",
                ")",
                ", FedGAN ",
                "(Rasouli et al",
                ".",
                ", ",
                "2020",
                ")",
                ", FedNAS ",
                "(He et al",
                ".",
                ", ",
                "2021",
                ")",
                ", FedNOVA ",
                "(Wang et al",
                ".",
                ", ",
                "2020a",
                ")",
                ", etc.",
                "iv",
                ") Extensions to federated LLMs and real-world applications.",
                "\nFedMLSecurity can simulate attacks and defenses during training of federated LLMs. It can also be integrated with real-world FL applications; see ",
                "Exp 7",
                ", where we utilize edge devices from Theta Network ",
                "(Theta Network., ",
                "2023",
                ")",
                " instead of simulations.",
                "Key takeaways",
                ": ",
                "i",
                ")\nWhile defense mechanisms can help mitigate attacks, it might also bring a potential loss of accuracy to the aggregation results. Therefore, when integrating defenses into FL applications, it’s crucial to weigh the benefits against potential drawbacks.\n",
                "ii",
                ") Nearly all existing defense mechanisms are impractical in real-world FL applications, as they compromise accuracy even if no attack happened. A defense that is practical for real-world systems is in need, where the defense should satisfy: 1) it must detect if attacks have happened, and only activates defensive mechanisms when attacks are detected; and 2) it must identify malicious clients accurately without harming benign local models. "
            ]
        ]
    },
    "S4.T3": {
        "caption": "Table 3. Models and datasets for evaluations.",
        "table": "<table id=\"S4.T3.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.4.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.4.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S4.T3.4.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></th>\n<td id=\"S4.T3.4.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S4.T3.4.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">ResNet20 </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T3.4.1.1.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>He et al<span class=\"ltx_text\">.</span><span id=\"S4.T3.4.1.1.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">2016</a><span id=\"S4.T3.4.1.1.2.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td id=\"S4.T3.4.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S4.T3.4.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">ResNet56 </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T3.4.1.1.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>He et al<span class=\"ltx_text\">.</span><span id=\"S4.T3.4.1.1.3.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">2016</a><span id=\"S4.T3.4.1.1.3.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td id=\"S4.T3.4.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S4.T3.4.1.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">CNN </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T3.4.1.1.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>McMahan et al<span class=\"ltx_text\">.</span><span id=\"S4.T3.4.1.1.4.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib60\" title=\"\" class=\"ltx_ref\">2017a</a><span id=\"S4.T3.4.1.1.4.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td id=\"S4.T3.4.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S4.T3.4.1.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">RNN (bi-LSTM) </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T3.4.1.1.5.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>McMahan et al<span class=\"ltx_text\">.</span><span id=\"S4.T3.4.1.1.5.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib60\" title=\"\" class=\"ltx_ref\">2017a</a><span id=\"S4.T3.4.1.1.5.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td id=\"S4.T3.4.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S4.T3.4.1.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">BERT </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T3.4.1.1.6.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>Devlin et al<span class=\"ltx_text\">.</span><span id=\"S4.T3.4.1.1.6.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib20\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S4.T3.4.1.1.6.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td id=\"S4.T3.4.1.1.7\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S4.T3.4.1.1.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">Pythia-1B </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T3.4.1.1.7.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>Biderman et al<span class=\"ltx_text\">.</span><span id=\"S4.T3.4.1.1.7.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">2023</a><span id=\"S4.T3.4.1.1.7.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n</tr>\n<tr id=\"S4.T3.4.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.4.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S4.T3.4.2.2.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Dataset</span></th>\n<td id=\"S4.T3.4.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">\n<span id=\"S4.T3.4.2.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">CIFAR10 </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T3.4.2.2.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>Krizhevsky et al<span class=\"ltx_text\">.</span><span id=\"S4.T3.4.2.2.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib44\" title=\"\" class=\"ltx_ref\">2009</a><span id=\"S4.T3.4.2.2.2.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td id=\"S4.T3.4.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">\n<span id=\"S4.T3.4.2.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">CIFAR100 </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T3.4.2.2.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>Krizhevsky et al<span class=\"ltx_text\">.</span><span id=\"S4.T3.4.2.2.3.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib44\" title=\"\" class=\"ltx_ref\">2009</a><span id=\"S4.T3.4.2.2.3.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td id=\"S4.T3.4.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">\n<span id=\"S4.T3.4.2.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">FEMNIST </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T3.4.2.2.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>Caldas et al<span class=\"ltx_text\">.</span><span id=\"S4.T3.4.2.2.4.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">2018</a><span id=\"S4.T3.4.2.2.4.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td id=\"S4.T3.4.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">\n<span id=\"S4.T3.4.2.2.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Shakespeare </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T3.4.2.2.5.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>McMahan et al<span class=\"ltx_text\">.</span><span id=\"S4.T3.4.2.2.5.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib61\" title=\"\" class=\"ltx_ref\">2017b</a><span id=\"S4.T3.4.2.2.5.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td id=\"S4.T3.4.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">\n<span id=\"S4.T3.4.2.2.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">20News </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T3.4.2.2.6.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>Lang<span id=\"S4.T3.4.2.2.6.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib48\" title=\"\" class=\"ltx_ref\">1995</a><span id=\"S4.T3.4.2.2.6.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n<td id=\"S4.T3.4.2.2.7\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">\n<span id=\"S4.T3.4.2.2.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">PubMedQA </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S4.T3.4.2.2.7.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>Luo et al<span class=\"ltx_text\">.</span><span id=\"S4.T3.4.2.2.7.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib57\" title=\"\" class=\"ltx_ref\">2022</a><span id=\"S4.T3.4.2.2.7.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Experimental setting. \nA summary of datasets and models for evaluations can be found in Table 3. We utilize FedAVG in our experiments. By default, we employ ResNet20 and the non-i.i.d. CIFAR10 dataset (partition parameter α=0.5𝛼0.5\\alpha=0.5), as the non-i.i.d. setting closely captures real-world scenarios. We further extend our evaluations to i.i.d. cases and various other models and datasets.\nFor evaluations on LLMs, we utilize FedLLM (FedML Inc., 2023) that trains LLMs in a federated manner. We employ the Pythia-1B model (Biderman et al., 2023) and PubMedQA (Jin et al., 2019), a non-i.i.d. biomedical research dataset that contains 212,269 questions for question answering. We utilize the “artificial” subset for training and the “labelled” subset for testing.\nEvaluations are conducted on a server with 8 NVIDIA A100-SXM4-80GB GPUs."
        ]
    }
}