{
    "S5.T1": {
        "caption": "Table 1: Test Accuracy (%) on RotatedMNIST and DiverseDigits with 3-layer MLP and LeNet respectivly. Each column is a test dataset, and MetaMD is trained on the other datasets.",
        "table": null,
        "footnotes": [],
        "references": [
            "RotatedMNIST and MLPs  We first evaluate optimiser learning for neural networks using the RotatedMNIST dataset and a 3-layer MLP architecture. RotatedMNIST defines 6 domains by rotating the original MNIST dataset by 0, 15, 30, 45, 60 and 75 degrees. We use 5 domains for meta-training, and train MetaMD to convergence in the inner loop, and evaluate the performance on the held-out domain. This process is repeated, holding out each domain in turn as meta-test. The convergence curve is shown in Fig. 6(left), and the testing performance in Table 1(top). We can see that MetaMD converges rapidly and trains models with strong testing performance. The hyperparameter tuning protocol for this and other experiments in this section is explained in Appendix E.",
            "The results averaged over 3 meta-test trials are shown as testing performance at convergence in Table 1(bottom) and selected meta-test learning curves in Fig. 3, with the remaining learning curves given in Appendix F. We can see that MetaMD is clearly faster than SGD and SGD-M in training convergence (Fig. 3), while typically producing models with the strongest generalisation error (Table 1). It is noteworthy that MetaMD exhibits strong cross-dataset generalisation here, corroborating our Theorem 11 on cross-task optimiser generalisation."
        ]
    },
    "A4.T2": {
        "caption": "Table 2: Test Accuracy (%) on RotatedMNIST with linear model",
        "table": null,
        "footnotes": [],
        "references": [
            "We study the convex setting when the base model is linear. In Table 2, we can see that in this setting all the optimisers have very similar performance due to the single global minima caused by convexity, but converge at different speeds shown in Fig 6. SGD converges in a slower ratio than others."
        ]
    }
}