{
    "id_table_1": {
        "caption": "Table 1 :  Composition of the area of law for each question in our dataset.",
        "table": "S3.T1.4",
        "footnotes": [],
        "references": [
            "Little prior work focuses on optimizing legal AI systems from start to finish for factors that matter to laypeople. Among these factors are accessibility of the services due to cost, factual correctness, and ease of understanding. In this paper, we propose an end-to-end  human-centeric legal AI  framework, which covers data sourcing, training/inference, and evaluation to improve these factors; importantly, we put laypeople first by ensuring each step of the process is backed by high-quality data from legal experts (see Figure  1 ). To our knowledge, this type of  human-centric legal framework  is the first of its kind.",
            "To build a source of structured and expert-approved legal data that is effective at  retrieval time , we construct a new dataset from real legal questions and ask law professors and law students to answer these questions. Then, we use this data during our retrieval process to ground model answers in citations vetted by legal experts. During the evaluation process, we also ground our evaluations with this dataset, establishing an end-to-end legal-expert-driven framework (see Figure  1 ).",
            "Specifically, we source questions from an online community 1 1 1 https://www.reddit.com/r/legaladvice/ , collected from January 2021 to October 2022. These questions are specific (e.g., Table  2 ) situations that real laypeople have, not hypotheticals 2 2 2 As per the legal advice community guidelines, the questions must be real questions, not hypothetical questions . For instance, the example sample in Table  2  outlines a specific scenario. This real-world focus allows our evaluations to be closer to a target domain that is helpful to laypeople. Then, we ask law professors and law students to provide golden answers to these questions. Since this research was done in Canada, the legal experts we worked with were knowledgeable primarily in Canadian law. Therefore, we asked these annotators to answer these legal questions according to Canadian law. Human answers are typically concise (shorter than the questions) and under 100 tokens (see Figure  2 ). Each answer contains a citation with more information relevant to the question. To perform a rigorous performance analysis across legal areas of practice, we classify each question into six categories relevant to laypeople, shown in Table  1 . The classification was done through a zero-shot classification approach  (Laurer et al.,  2023 )  and manually inspected for correctness. To aid the community in evaluating existing LLMs, we release our evaluation dataset ( n = 323 n 323 n=323 italic_n = 323 ) publicly 3 3 3 https://huggingface.co/datasets/jonathanli/law_qa_eval .",
            "As shown in Figure  3(b) , we embed both the context and the question using an existing state-of-the-art embedding model,  BAAI/bge-large-en-v1.5   (Xiao et al.,  2023 ) . Then, we compute the dot product between the question and each document, selecting the document with the greatest dot product as the most relevant sample. Then we provide this document in the context of an existing language model ( GPT-3.5-turbo ), using prompts containing both the context and question. Unlike prior work, we evaluate retrieval from only legal expert sources rather than the entirety of the internet. This constitutes our model inference part in Figure  1 . We call this legal retrieval."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Example question (source) and provided answers and citation.",
        "table": "S3.T2.4",
        "footnotes": [],
        "references": [
            "Specifically, we source questions from an online community 1 1 1 https://www.reddit.com/r/legaladvice/ , collected from January 2021 to October 2022. These questions are specific (e.g., Table  2 ) situations that real laypeople have, not hypotheticals 2 2 2 As per the legal advice community guidelines, the questions must be real questions, not hypothetical questions . For instance, the example sample in Table  2  outlines a specific scenario. This real-world focus allows our evaluations to be closer to a target domain that is helpful to laypeople. Then, we ask law professors and law students to provide golden answers to these questions. Since this research was done in Canada, the legal experts we worked with were knowledgeable primarily in Canadian law. Therefore, we asked these annotators to answer these legal questions according to Canadian law. Human answers are typically concise (shorter than the questions) and under 100 tokens (see Figure  2 ). Each answer contains a citation with more information relevant to the question. To perform a rigorous performance analysis across legal areas of practice, we classify each question into six categories relevant to laypeople, shown in Table  1 . The classification was done through a zero-shot classification approach  (Laurer et al.,  2023 )  and manually inspected for correctness. To aid the community in evaluating existing LLMs, we release our evaluation dataset ( n = 323 n 323 n=323 italic_n = 323 ) publicly 3 3 3 https://huggingface.co/datasets/jonathanli/law_qa_eval ."
        ]
    },
    "global_footnotes": [
        "As per the legal advice community guidelines, the questions must be real questions, not hypothetical questions",
        "This was roughly calculated based on the hundreds of billions of pages that Google indexes",
        "compared to our dataset of under a thousand samples."
    ]
}