{
    "id_table_1": {
        "caption": "Table 1:  Comparison of Baseline and SimpleStrat Average KL Divergence for Different Models on CoverageQA-Curated and CoverageQA-Wikipedia. Smaller numbers reflect closer alignment to uniform distribution.",
        "table": "S5.T1.1",
        "footnotes": [],
        "references": [
            "Naively, increasing temperature, a parameter that controllably flattens an LLMs softmax, can improve an LLMs generation diversity. However, temperature introduces two problems. First, higher temperatures degrades generation quality. Recent evidence suggests removing temperature scaling is desirable for multi-step reasoning to reduce errors compounding  (Zhang et al.,  2024 ) . This is especially critical in syntax sensitive settings like code generation where low temperatures (  0.15 absent 0.15 \\leq 0.15  0.15 ) are often used. Second, controlling for temperature does not necessarily improve diversity in the answer space. In Figure  1 , we illustrate increasing temperature doesnt lead to meaningful increase in diversity if the model is excessively confident and suffers from mode collapse. When asked to \"Name a US State,\" the model heavily skews towards answering \"California\", high temperature only marginally softens the skew while surfacing incorrect answers and hurting instruction following.",
            "We propose SimpleStrat, a training-free sampling approach to increase diversity. SimpleStrat improves LLM generation diversity without degradation to generation quality while ensuring that an LLMs outputs are aligned with the true distribution of answers. SimpleStrat consist of three stages: auto-stratification, heuristic estimation, and probabilistic prompting. Even if a language model cannot generate diverse solutions, we find that it can be prompted to identify useful partitions of the solution space based on the user request. We call this process  auto-stratification . In Fig.  1 , SimpleStrat identifies two semantically significant strata from user request, \"Name a US State\": \"(East/West) of the Mississippi River\" and \"(North/South) of the Missouri Compromise Line.\"",
            "Next, the heuristic estimation computes the joint probabilities across all strata. Back to Fig.  1 , SimpleStrat then outputs the probability for all four possible regions in US. Finally, SimpleStrat samples from the joint probability distribution to augment the original user prompt with the selected stratas. We note that this approach to diversity is orthogonal to increasing temperature and hence does not affect generation quality.",
            "Post heuristic estimation, a set of statum is sampled from the joint probability distribution in Eqn.  1 . This implicitly forms a  probabilistic prompt , which specifies a distribution over concrete language model prompts. After a prompt is sampled, the LLM is then used to sample from within the stratum. Back to Fig.  2 , East and South are sampled from the Mississippi and Missouri strata respectively, augmenting the final prompt with diverse specifications.",
            "This approach has several advantages: 1) it allows us to create a diverse and extensive benchmark that can be easily updated with weekly updates to Wikidata, 2) it allows us to arbitrarily specify the size of the solution space as constraints can be added or removed to form; and 3) this process in principle can curate a large dataset with little manual effort or supervision. In the initial instantiation of CoverageQA dataset, we publish 105 questions across 4 domains, each corresponding to a different initial seed item-property pair. To ensure quality, we employ both automatic filters (e.g., excluding certain generic properties) and manual curation to remove redundant or unsuitable questions. This dataset can be substantially expanded as we only used 4 domains, but we leave this for future work. For a details on the dataset breakdown and details on the question generation process, please refer to Appendix   A.1 .",
            "In Fig.  6 , we see an example of the response distribution for Llama 3.1 with and without the correction due to SimpleStrat. The base model distribution is strongly biased towards its most desired output. As illustrated in Fig.  1 , California is preferred by a long margin. Thus, its not surprising that we observed little diversity when by simply increasing temperature. In contrast, SimpleStrat provides a much more uniform distribution. The over represented solutions are adjusted to be lower and the under represented solutions are adjusted to be higher. We note that the distributions are still not perfectly uniform. For more examples, see App.  E . Although Fig.  6  shows the larger model has a closer to ideal distribution, the results in Table  1  indicate larger models have on average worse diversity. This may be a result of memorization exacerbated in larger models."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  CoverageQA Domains",
        "table": "A2.T2.1",
        "footnotes": [],
        "references": [
            "As illustrated in  2 , SimpleStrat consist of three stages, 1) auto-stratification, 2) heuristic estimation, and 3) probabilistic prompting. For each unique user prompt, the outputs of the first two stages can be cached to avoid recomputing feed-forwards.",
            "We ask the LLM for each  l j subscript l j l_{j} italic_l start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , to estimate the marginal proportion of solutions this holds for. As this may not add up to 1, we normalize the estimates to form a proper probability distribution. For simplicity, we focus in this work on settings where all solution in the solution space is equally like. As noted in Sec.  3.2 , we encourage the LLM to propose balanced partitions. However, heuristic estimation allow us to support imbalanced partitions by reweighing the sampling to favor stratum with more potential solutions. More details on prompting in App.  D . In Fig.  2 , the LLM determines the joint probabilities across two stratas, the Mississippi River and the Missouri Compromise Line.",
            "Post heuristic estimation, a set of statum is sampled from the joint probability distribution in Eqn.  1 . This implicitly forms a  probabilistic prompt , which specifies a distribution over concrete language model prompts. After a prompt is sampled, the LLM is then used to sample from within the stratum. Back to Fig.  2 , East and South are sampled from the Mississippi and Missouri strata respectively, augmenting the final prompt with diverse specifications.",
            "We wish to evaluate generation diversity in settings where 1) user requests have more than one distinct correct answer, 2) and answers are equally likely, and 3) answers do not require hidden or implicit context to verify. These three properties allow us to measure diversity in the sense of whether the language model will provide coverage over the full solution space when resampled. Unfortunately existing benchmarks discussed in Sec.  2  do not satisfy these properties. We introduce CoverageQA to assess the language model generation diversity. The dataset consists of two splits: CoverageQA-Curated, manually curated naturally underspecified questions, and CoverageQA-Wikipedia, and auto-generated dataset of underspecified questions.",
            "For our baseline, we prompt the models and directly compute  P  r  [ s | P  r  o  m  p  t  ( l  ) ] P r delimited-[] conditional s P r o m p t  l Pr[s|Prompt(\\vec{l})] italic_P italic_r [ italic_s | italic_P italic_r italic_o italic_m italic_p italic_t ( over start_ARG italic_l end_ARG ) ]  for each solution,  s s s italic_s . This is simply the product of the individual next-token probabilities. For SimpleStrat, the probability involves the next-token probability conditioned on the prompt weighted by the probability the prompt is selected. Concretely, the probability an answer is sampled by SimpleStrat can be computed based on Eqn. 2 . The next-token probability based response distribution  P  r  [ s | P  r  o  m  p  t  ( l  ) ] P r delimited-[] conditional s P r o m p t  l Pr[s|Prompt(\\vec{l})] italic_P italic_r [ italic_s | italic_P italic_r italic_o italic_m italic_p italic_t ( over start_ARG italic_l end_ARG ) ]  computed just as the baseline, and we do a sum weighted by the joint probabilities assigned in heuristic estimation. We assign remaining probability density to an \"Invalid\" category to form a proper distribution. The probabilistic formulation allows us to easily compute the response distribution of SimpleStrat."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Full prompt for Auto-stratification.",
        "table": "A3.T3.1",
        "footnotes": [],
        "references": [
            "We ask the LLM for each  l j subscript l j l_{j} italic_l start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , to estimate the marginal proportion of solutions this holds for. As this may not add up to 1, we normalize the estimates to form a proper probability distribution. For simplicity, we focus in this work on settings where all solution in the solution space is equally like. As noted in Sec.  3.2 , we encourage the LLM to propose balanced partitions. However, heuristic estimation allow us to support imbalanced partitions by reweighing the sampling to favor stratum with more potential solutions. More details on prompting in App.  D . In Fig.  2 , the LLM determines the joint probabilities across two stratas, the Mississippi River and the Missouri Compromise Line.",
            "Consider the question \"Name one Great Lake in the United States.\" as shown in Fig.  3 . We see that temperature scaling with GPT-4o results in a strong preference/bias for Lake Erie. This is certainly a correct continuation and under the language modeling objective should be incentivized. Increasing the temperature helps sample the next most likely candidate solutions more often. However, even when increasing the temperature past 1 there is still low coverage over the solutions space. Specifically, Huron is only seen once out of 100 samples at 1.5 temperature, and Lake Ontario is never observed. This is undesirable if the data is used to proposing candidate plans, generating test cases, or generating training data. Not only is there insufficient coverage over all possible solutions, but the model consistently has a strong preference for Lake Erie. This undesired biases in generations may lead to problems in downstream use cases.",
            "In Fig.  3 , we observe a much more uniform distribution over valid solutions when using SimpleStrat. Notably, we observe full coverage over all 5 Great Lakes. At lower temperatures, there is still a preference of a single lake over the others, in this case Lake Huron. However, this is less pronounced at higher temperatures and is a significant improvement over GPT-4o without SimpleStrat.",
            "Our quantitative results are consistent with the Great Lakes example in Fig.  3 . Scaling temperature alone does not lead to as much coverage diversity as combining with SimpleStrat. The recall importantly does not come at the expense of quality as measure by precision as shown in App.   B .",
            "We provide the full prompt in Tbl.  3 . To improve prompt adherence, we provide one in context example in the form of one simulated round of multi-turn conversation, i.e. we provide an example set of reasoning following the template."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Prompt for Partition-specific Heuristic Estimation.",
        "table": "A4.T4.1",
        "footnotes": [],
        "references": [
            "We first assess coverage diversity, specifically, the models ability to recall all the valid solutions upon resampling. This measure is clearly impacted by temperature as temperature zero or greedy decoding of LLMs leads to a single deterministic result. We compare the coverage diversity (recall) of SimpleStrat, GPT-4o, and Claude 3.5 Sonnet as a function of temperature. We sweep over temperatures from 0.15 to 1.5. Although not shown in the evaluation, note that SimpleStrat has the advantage of providing diversity even when sampled at temperature zero. SimpleStrat with GPT-4o leads to an improvement to recall across all temperatures as shown in Fig. 4 . On the CoverageQA-Curated, we see a consistent 0.2 increase in recall over the same base model, GPT-4o. On CoverageQA-Wikipedia, we see as much as 0.05 increase in recall",
            "We first take each partition function from auto-stratification and estimate a starting probability with the prompt in Table  4 . This prompt is heavily inspired by  Halawi et al. ( 2024 ) . We then collect all the proportions and pass it through a final Heuristic Estimation prompt to remove redundant properties (negations for instance) and give the model a chance to correct any incorrect probabilities. See Table  5  for full prompt. Finally, we ask the model to select at most 3."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Prompt for Final Heuristic Estimation.",
        "table": "A4.T5.1",
        "footnotes": [],
        "references": [
            "Additionally, we analyze the KL divergence on a per-question basis to identify where our method achieves improvement (a more uniform distribution). The scatter plot in Fig   5(b)  shows KL divergence values for SimpleStrat (y-axis) versus the baseline (x-axis) for each question in the CoverageQA Wikipedia dataset. Points above the diagonal line represent questions where SimpleStrat outperforms the baseline by yielding a lower KL divergence. The vast majority of points fall on or above this line, indicating that SimpleStrat consistently produces improvement in generating more uniform distributions across the questions in the dataset.",
            "We first take each partition function from auto-stratification and estimate a starting probability with the prompt in Table  4 . This prompt is heavily inspired by  Halawi et al. ( 2024 ) . We then collect all the proportions and pass it through a final Heuristic Estimation prompt to remove redundant properties (negations for instance) and give the model a chance to correct any incorrect probabilities. See Table  5  for full prompt. Finally, we ask the model to select at most 3."
        ]
    }
}