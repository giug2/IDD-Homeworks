{
    "id_table_1": {
        "caption": "Table 1:  Results of the experiment on the sampled CAIL2018 dataset",
        "table": "S4.T1.1",
        "footnotes": [],
        "references": [
            "We aim to leverage LLMs with RAG to improve their performance on LJP tasks. First, for each accusation, we create a generated descriptive example with the help of LLMs. Second, the generated example can be used to construct semantic embeddings for downstream similarity searching. Finally, we retrieve top accusation candidates and prompt the LLMs to make judgment predictions on provided legal cases. A detailed demonstration of our method is shown in Fig.  1 .",
            "Legal judgment prediction can be defined as the process of forecasting the judgment outcome of a particular legal case. Lets consider a dataset  L L L italic_L  comprising diverse legal cases  l 1 , l 2 , ... , l n subscript l 1 subscript l 2 ... subscript l n {l_{1},l_{2},...,l_{n}} italic_l start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_l start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , italic_l start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT  and another set  J J J italic_J  representing potential judgments  j 1 , j 2 , ... , j n subscript j 1 subscript j 2 ... subscript j n {j_{1},j_{2},...,j_{n}} italic_j start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_j start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , italic_j start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT  (referred to as an accusation list). The Legal Judgment Prediction (LJP) task, as depicted in Equation  1 , involves establishing a functional mapping  f f f italic_f  that links the space of legal case datasets  L L L italic_L  to the accusation list  J J J italic_J .",
            "The results are detailed in Table  1 . Initially, for relatively smaller models like GPT-3.5, all performances exhibit similarly low scores. This could be due to both the limited contextual ability and reasoning ability of the LLM. Second, with the increase in model capability from GPT-3.5-turbo to GPT-4o, the accuracy increases consistently, indicating the importance of foundation models. Third, both zero-shot-CoT and legal syllogism prompting outperform the baseline, for powerful models like GPT-4o. Notably, Athena significantly outperforms all other methods with the GPT-4 series. This highlights the effectiveness and necessity of prompt engineering, especially RAG.",
            "We define the average accuracy and weight accuracy as follows. For each accusation  c i subscript c i c_{i} italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  in the set  { c 1 , c 2 , ... , c m } subscript c 1 subscript c 2 ... subscript c m \\{c_{1},c_{2},...,c_{m}\\} { italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , italic_c start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT } , where  m m m italic_m  is the size of accusation set for  L l  a  r  g  e subscript L l a r g e L_{large} italic_L start_POSTSUBSCRIPT italic_l italic_a italic_r italic_g italic_e end_POSTSUBSCRIPT , the accuracy is denoted as  acc  ( c i ) acc subscript c i \\text{acc}(c_{i}) acc ( italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  and the total number of corresponding legal cases in  L l  a  r  g  e subscript L l a r g e L_{large} italic_L start_POSTSUBSCRIPT italic_l italic_a italic_r italic_g italic_e end_POSTSUBSCRIPT  is denoted as  | L c i | subscript L subscript c i |L_{c_{i}}| | italic_L start_POSTSUBSCRIPT italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT | . The average accuracy and weight accuracy are calculated on all  acc  ( c i ) acc subscript c i \\text{acc}(c_{i}) acc ( italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  accordingly, as shown in Equation.  9  and Equation.  10 . Note that the weighted accuracy is more representative in a natural environment when no specified accusations are considered pivotal, while the average accuracy can provide a more fine-grained analysis of the weakness in accusations."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  The ablation study results of in-context window size  k k k italic_k , all with the Athena method.  Top0  indicates no usage of context from the knowledge base, hence zero hits for every legal case",
        "table": "S4.T2.5",
        "footnotes": [],
        "references": [
            "With the introduction of large language models, the functional mapping can be implemented as an  LLM , as illustrated in Equation  2 . The LLM inference engine predicts relevant accusations for a specified legal case by leveraging its prior knowledge of general laws.",
            "The overall framework of Athena, as demonstrated in Fig.  2 , consists of two major workflows: the prompting workflow and the knowledge retrieval workflow. These two workflows split on the given legal case and converge at the prompt template.",
            "As clearly shown in Table  2 , the overall performance of Athenas LLMs follows a pattern where with an increase in the in-context window size  k k k italic_k , the performance of Athena initially increases, then declines, and eventually stabilizes at  a moderate window size  k k k italic_k .",
            "The Hit Rate in Table  2  reflects the overall likelihood across the entire sampled dataset of retrieving the correct accusation candidate for the LLM, as illustrated in Equation  8 . Here,  C C C italic_C  denotes the collection of the retrieved accusation candidates,  c true subscript c true c_{\\text{true}} italic_c start_POSTSUBSCRIPT true end_POSTSUBSCRIPT  represents the correct accusation candidate (equal to the label of the given legal case),  N N N italic_N  stands for the total number of legal cases in the dataset, and  hits i subscript hits i \\text{hits}_{i} hits start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is the hits value for the  i i i italic_i -th case."
        ]
    },
    "global_footnotes": []
}