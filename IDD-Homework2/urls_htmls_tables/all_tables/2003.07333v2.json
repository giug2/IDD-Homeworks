{
    "S2.T1": {
        "caption": "TABLE I: Thresholds for size attributes according to the dataset scale. When dealing with low resolution data, visible objects of interest are larger. To deal with this disparity, we adapt the size thresholds to the resolution of the images.",
        "table": "<table id=\"S2.T1.12\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S2.T1.12.13.1\" class=\"ltx_tr\">\n<th id=\"S2.T1.12.13.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">Scale</th>\n<th id=\"S2.T1.12.13.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Small</th>\n<th id=\"S2.T1.12.13.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Medium</th>\n<th id=\"S2.T1.12.13.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Large</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S2.T1.6.6\" class=\"ltx_tr\">\n<td id=\"S2.T1.6.6.7\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Low resolution</td>\n<td id=\"S2.T1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<math id=\"S2.T1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"&lt;\" display=\"inline\"><semantics id=\"S2.T1.1.1.1.m1.1a\"><mo id=\"S2.T1.1.1.1.m1.1.1\" xref=\"S2.T1.1.1.1.m1.1.1.cmml\">&lt;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.1.1.1.m1.1b\"><lt id=\"S2.T1.1.1.1.m1.1.1.cmml\" xref=\"S2.T1.1.1.1.m1.1.1\"></lt></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.1.1.1.m1.1c\">&lt;</annotation></semantics></math> 3000m<sup id=\"S2.T1.2.2.2.1\" class=\"ltx_sup\">2</sup>\n</td>\n<td id=\"S2.T1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<math id=\"S2.T1.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"&lt;\" display=\"inline\"><semantics id=\"S2.T1.3.3.3.m1.1a\"><mo id=\"S2.T1.3.3.3.m1.1.1\" xref=\"S2.T1.3.3.3.m1.1.1.cmml\">&lt;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.3.3.3.m1.1b\"><lt id=\"S2.T1.3.3.3.m1.1.1.cmml\" xref=\"S2.T1.3.3.3.m1.1.1\"></lt></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.3.3.3.m1.1c\">&lt;</annotation></semantics></math> 10000m<sup id=\"S2.T1.4.4.4.1\" class=\"ltx_sup\">2</sup>\n</td>\n<td id=\"S2.T1.6.6.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<math id=\"S2.T1.5.5.5.m1.1\" class=\"ltx_Math\" alttext=\"\\geq\" display=\"inline\"><semantics id=\"S2.T1.5.5.5.m1.1a\"><mo id=\"S2.T1.5.5.5.m1.1.1\" xref=\"S2.T1.5.5.5.m1.1.1.cmml\">≥</mo><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.5.5.5.m1.1b\"><geq id=\"S2.T1.5.5.5.m1.1.1.cmml\" xref=\"S2.T1.5.5.5.m1.1.1\"></geq></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.5.5.5.m1.1c\">\\geq</annotation></semantics></math> 10000m<sup id=\"S2.T1.6.6.6.1\" class=\"ltx_sup\">2</sup>\n</td>\n</tr>\n<tr id=\"S2.T1.12.12\" class=\"ltx_tr\">\n<td id=\"S2.T1.12.12.7\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">High resolution</td>\n<td id=\"S2.T1.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">\n<math id=\"S2.T1.7.7.1.m1.1\" class=\"ltx_Math\" alttext=\"&lt;\" display=\"inline\"><semantics id=\"S2.T1.7.7.1.m1.1a\"><mo id=\"S2.T1.7.7.1.m1.1.1\" xref=\"S2.T1.7.7.1.m1.1.1.cmml\">&lt;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.7.7.1.m1.1b\"><lt id=\"S2.T1.7.7.1.m1.1.1.cmml\" xref=\"S2.T1.7.7.1.m1.1.1\"></lt></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.7.7.1.m1.1c\">&lt;</annotation></semantics></math> 100m<sup id=\"S2.T1.8.8.2.1\" class=\"ltx_sup\">2</sup>\n</td>\n<td id=\"S2.T1.10.10.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">\n<math id=\"S2.T1.9.9.3.m1.1\" class=\"ltx_Math\" alttext=\"&lt;\" display=\"inline\"><semantics id=\"S2.T1.9.9.3.m1.1a\"><mo id=\"S2.T1.9.9.3.m1.1.1\" xref=\"S2.T1.9.9.3.m1.1.1.cmml\">&lt;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.9.9.3.m1.1b\"><lt id=\"S2.T1.9.9.3.m1.1.1.cmml\" xref=\"S2.T1.9.9.3.m1.1.1\"></lt></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.9.9.3.m1.1c\">&lt;</annotation></semantics></math> 500m<sup id=\"S2.T1.10.10.4.1\" class=\"ltx_sup\">2</sup>\n</td>\n<td id=\"S2.T1.12.12.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">\n<math id=\"S2.T1.11.11.5.m1.1\" class=\"ltx_Math\" alttext=\"\\geq\" display=\"inline\"><semantics id=\"S2.T1.11.11.5.m1.1a\"><mo id=\"S2.T1.11.11.5.m1.1.1\" xref=\"S2.T1.11.11.5.m1.1.1.cmml\">≥</mo><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.11.11.5.m1.1b\"><geq id=\"S2.T1.11.11.5.m1.1.1.cmml\" xref=\"S2.T1.11.11.5.m1.1.1\"></geq></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.11.11.5.m1.1c\">\\geq</annotation></semantics></math> 500m<sup id=\"S2.T1.12.12.6.1\" class=\"ltx_sup\">2</sup>\n</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Size:\nusing hard thresholds on the surface area, elements can be considered ”small”, ”medium” or ”large”. As we are interested in information at different scales in the two datasets, we use different threshold values, which are described in Table I."
        ]
    },
    "S4.T2": {
        "caption": "TABLE II: Results on the test set of the low resolution dataset. The standard deviation is reported in brackets.",
        "table": "<table id=\"S4.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">Type</th>\n<th id=\"S4.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Accuracy</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.1.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Count</td>\n<td id=\"S4.T2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">67.01% (0.59%)</td>\n</tr>\n<tr id=\"S4.T2.1.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">Presence</td>\n<td id=\"S4.T2.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">87.46% (0.06%)</td>\n</tr>\n<tr id=\"S4.T2.1.4.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">Comparison</td>\n<td id=\"S4.T2.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">81.50% (0.03%)</td>\n</tr>\n<tr id=\"S4.T2.1.5.4\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">Rural/Urban</td>\n<td id=\"S4.T2.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">90.00% (1.41%)</td>\n</tr>\n<tr id=\"S4.T2.1.6.5\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">AA</td>\n<td id=\"S4.T2.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">81.49% (0.49%)</td>\n</tr>\n<tr id=\"S4.T2.1.7.6\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.7.6.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">OA</td>\n<td id=\"S4.T2.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">79.08% (0.20%)</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "We show some predictions of the model on the different test sets in Figure 8 and Figure 9 to qualitatively assess the results. Numerical performance of the proposed model on the LR dataset is reported in Table II and the confusion matrix is shown in Figure 10. The performance on both tests sets of the HR dataset are reported in Table III and the confusion matrices are shown in Figure 11.",
            "General accuracy assessment:\nThe proposed model achieves an overall accuracy of 79% on the low resolution dataset (see Table II) and of 83% on the first test set of the high resolution dataset (Table III), indicating that the task of automatically answering question based on remote sensing images is possible. When looking at the accuracies per question type (in Tables II and III), it can be noted that the model performs inconsistently with respect to the task the question is tackling: while a question about the presence of an object is generally well answered (87.46% in the LR dataset, 90.43% in the first test set of the HR dataset), counting questions gives poorer performances (67.01% and 68.63% respectively). This can be explained by the fact that presence questions can be seen as simplified counting questions to which the answers are restricted to two options: ”0” or ”1 or more”. Classical VQA models are known to struggle with the counting task [38]. An issue which partly explains these performances in the counting task is the separation of connected instances. This problem has been raised for the case of buildings in [33] and is illustrated in Figure 8(f), where the ground truth is indicating three buildings, which could also be only one. We found another illustration of this phenomenon in the second test set in Figure 8(i). This issue mostly arises when counting roads or buildings."
        ]
    },
    "S4.T3": {
        "caption": "TABLE III: Results on both test sets of the high resolution dataset. The standard deviation is reported in brackets.",
        "table": "<table id=\"S4.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">Type</th>\n<th id=\"S4.T3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Accuracy</th>\n<th id=\"S4.T3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Accuracy</th>\n</tr>\n<tr id=\"S4.T3.1.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.2.2.1\" class=\"ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r\"></th>\n<th id=\"S4.T3.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">Test set 1</th>\n<th id=\"S4.T3.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">Test set 2</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.1.3.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Count</td>\n<td id=\"S4.T3.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">68.63% (0.11%)</td>\n<td id=\"S4.T3.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">61.47% (0.08%)</td>\n</tr>\n<tr id=\"S4.T3.1.4.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">Presence</td>\n<td id=\"S4.T3.1.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">90.43% (0.04%)</td>\n<td id=\"S4.T3.1.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">86.26% (0.47%)</td>\n</tr>\n<tr id=\"S4.T3.1.5.3\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.5.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">Comparison</td>\n<td id=\"S4.T3.1.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">88.19% (0.08%)</td>\n<td id=\"S4.T3.1.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">85.94% (0.12%)</td>\n</tr>\n<tr id=\"S4.T3.1.6.4\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.6.4.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">Area</td>\n<td id=\"S4.T3.1.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">85.24% (0.05%)</td>\n<td id=\"S4.T3.1.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">76.33% (0.50%)</td>\n</tr>\n<tr id=\"S4.T3.1.7.5\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.7.5.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">AA</td>\n<td id=\"S4.T3.1.7.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">83.12% (0.03%)</td>\n<td id=\"S4.T3.1.7.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">77.50% (0.29%)</td>\n</tr>\n<tr id=\"S4.T3.1.8.6\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.8.6.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">OA</td>\n<td id=\"S4.T3.1.8.6.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">83.23% (0.02%)</td>\n<td id=\"S4.T3.1.8.6.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">78.23% (0.25%)</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "We show some predictions of the model on the different test sets in Figure 8 and Figure 9 to qualitatively assess the results. Numerical performance of the proposed model on the LR dataset is reported in Table II and the confusion matrix is shown in Figure 10. The performance on both tests sets of the HR dataset are reported in Table III and the confusion matrices are shown in Figure 11.",
            "General accuracy assessment:\nThe proposed model achieves an overall accuracy of 79% on the low resolution dataset (see Table II) and of 83% on the first test set of the high resolution dataset (Table III), indicating that the task of automatically answering question based on remote sensing images is possible. When looking at the accuracies per question type (in Tables II and III), it can be noted that the model performs inconsistently with respect to the task the question is tackling: while a question about the presence of an object is generally well answered (87.46% in the LR dataset, 90.43% in the first test set of the HR dataset), counting questions gives poorer performances (67.01% and 68.63% respectively). This can be explained by the fact that presence questions can be seen as simplified counting questions to which the answers are restricted to two options: ”0” or ”1 or more”. Classical VQA models are known to struggle with the counting task [38]. An issue which partly explains these performances in the counting task is the separation of connected instances. This problem has been raised for the case of buildings in [33] and is illustrated in Figure 8(f), where the ground truth is indicating three buildings, which could also be only one. We found another illustration of this phenomenon in the second test set in Figure 8(i). This issue mostly arises when counting roads or buildings.",
            "Importance of the number of training samples:\nWe show in Figure 12 the evolution of the accuracies when the model is trained with a fraction of the HR training samples. When using only 1% of the available training samples, the model already gets 65% in average accuracy (vs 83% for the model trained on the whole training set). However, it can be seen that, for numerical tasks (counts and area estimation), larger amounts of samples are needed to achieve the performances reported in Table III. This experiment also shows that the performances start to plateau after 10% of the training data is used: this indicates that the proposed model would not profit substantially from a larger dataset."
        ]
    }
}