{
    "id_table_1": {
        "caption": "Table 1 .   Comparison of existing performance capture technologies with our proposed method.",
        "table": "S1.T1.46",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "In this work, we tackle the problem of accurate, robust and adaptable performance capture of the whole human. Our approach eliminates the need for specialist hardware, supports any number of cameras without needing calibration, can achieve state-of-the-art results fully automatically, and significantly lowers the amount of expert manual labor required to reach production-grade performance capture results. We demonstrate holistic performance capture of the face, hands and body simultaneously in  Figure 1 .",
            "In summary, we combine the benefits of model fitting and ML-based methods to achieve:  (1) Robustness  of DNNs which directly regress body pose and shape.  (2) Accuracy  of DNNs predicting image-space features.  (3) Adaptability  of model fitting to support arbitrary cameras. A comparison of the capabilities of our method with other performance capture technologies is shown in  Table 1 . Specifically, in this work we present the following:",
            "Following  Zhang et al .  ( 2023b ) , we use a HRNet backbone  (Sun et al . ,  2019 ) , pretrained on ImageNet  (Deng et al . ,  2009 ) , to extract visual features from the input image. As shown in  Figure 10 , the compressed representation computed with HRNet then serves as input to three MLP-based heads to predict probabilistic landmarks, pose, and shape.",
            "We use a number of priors to regularize the optimization  (Wood et al . ,  2022 ; Pavlakos et al . ,  2019 ) .  E shape subscript E shape E_{\\textrm{shape}} italic_E start_POSTSUBSCRIPT shape end_POSTSUBSCRIPT  is a combination of GMM priors; minimizing the log probability of the given body and face shape,    \\boldsymbol{\\beta} bold_italic_  and    \\boldsymbol{\\gamma} bold_italic_ . For faces we fit a GMM to the shape parameters of the training scans described in  Section 3.1 , for bodies we fit a GMM to a library of SMPL-H identities ( Section 3.2 ).  E exp subscript E exp E_{\\textrm{exp}} italic_E start_POSTSUBSCRIPT exp end_POSTSUBSCRIPT  is an  L 1 subscript L 1 L_{1} italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  loss on expression parameters to promote sparsity, combined with quartic barrier loss to keep coefficients in the range  [ 0 , 1 ] 0 1 [0,1] [ 0 , 1 ] .  E temp subscript E temp E_{\\textrm{temp}} italic_E start_POSTSUBSCRIPT temp end_POSTSUBSCRIPT  is an  L 2 subscript L 2 L_{2} italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  loss on frame-to-frame distance between 3D vertex locations, this helps to reduce jitter that might arise from noise in the predicted 2D landmarks.  E intsct subscript E intsct E_{\\textrm{intsct}} italic_E start_POSTSUBSCRIPT intsct end_POSTSUBSCRIPT  is to prevent intersection of the eyeballs, teeth and tongue with the surface of the face  (Wood et al . ,  2022 ) .  E cam subscript E cam E_{\\textrm{cam}} italic_E start_POSTSUBSCRIPT cam end_POSTSUBSCRIPT  is an  L 2 subscript L 2 L_{2} italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  loss on the position of the cameras compared to their initial location, to prevent them moving too far from the initialization. It is trivial to extend the system with additional priors in the case where we have further information on the scene. For example we can add an  L 2 subscript L 2 L_{2} italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  loss on the height of the subject to reduce ambiguity in scale, or restrict the shape prior based on the gender of the subject.",
            "To evaluate performance capture for the hands we use the FrieHAND dataset  (Zimmermann et al . ,  2019 ) . We fit just the MANO hand model  (Romero et al . ,  2017 )  using the prediction of the hand DNN and  E pose subscript E pose E_{\\textrm{pose}} italic_E start_POSTSUBSCRIPT pose end_POSTSUBSCRIPT  only. Quantitative results are shown in  Table 2 , where our method performs comparably to recent approaches. Note that ours is the only method that is  not  trained on the FreiHAND training set and that recent non-parametric methods  (Moon and Lee,  2020 ; Lin et al . ,  2021b )  lead to superior metrics but are not useful for performance capture. Qualitative results and comparison to recent work are shown in  Figure 11  and the supplementary material.",
            "We evaluate facial performance capture on the NoW benchmark  (Sanyal et al . ,  2019 )  by fitting just the face of the SOMA model, omitting any unnecessary energy terms; results are shown in  Table 3 . Most full-body benchmarks have low-quality face ground truth, so this lets us compare our face model with other recent methods, including the model of  Wood et al .  ( 2021 ) , more directly. We also evaluate a variant of the NoW benchmark where we reconstruct the face shape of each subject using  all  images of that subject together as input to our system, i.e., the multi-view scenario. While our method does not outperform  Zhang et al .  ( 2023a )  in the single-view case, our approach extends to multi-view input which provides more accurate reconstructions. Qualitative results and comparison to recent work are shown in  Figure 12  and the supplementary material. The NoW challenge only considers the face, not the full head, so improved quality of head/ear shape is not captured in the quantitative results.",
            "Results for the EHF dataset  (Pavlakos et al . ,  2019 )  are shown in  Table 4 . We outperform recent methods in most metrics or are on par with concurrent works, and qualitatively achieve significantly more accurate results as shown in  Figure 13 . We can easily introduce additional supervisory signals to our method; for example if we introduce a single measurement of the actors height we achieve a full-body MPVPE ( without  pelvis alignment) of 43.1mm, as the scale ambiguity inherent to the single-view scenario is removed.",
            "Results for multi-view reconstruction on the Human3.6M dataset  (Ionescu et al . ,  2014 )  are shown in  Table 6 . Our method significantly outperforms other multi-view methods which are also not trained on Human3.6M. Given the limited variety of the data there is a high chance that other methods over-fit; this is supported by the results of  Choudhury et al .  ( 2023 )  reproduced in  Table 6 . The Human3.6M benchmark evaluates only joint positions which do not fully specify human pose, leaving some degrees of freedom (e.g., rotation about a limb) undefined  (Zhang et al . ,  2021 ) . Our method, which recovers the full body mesh, often leads to more compelling results, supported by the qualitative results shown in  Figure 13 .",
            "To construct the template mesh,  T     T \\mathbf{\\overline{T}} over  start_ARG bold_T end_ARG , we manually align the template of  Wood et al .  ( 2021 )  to the head of the SMPL-H template. Once aligned, the head of SMPL-H and lower neck of the new head are removed and the two partial meshes merged. The topology around the join is hand-crafted to create a smooth transition given the different density of the two meshes, see  Figure 14 . A new UV-map is also hand-authored based on the SMPL-H UV space.",
            "Examples showing the poor semantic consistency in the mouth region described in the main paper, and improvements resulting from the above process, are shown in  Figure 15 .",
            "Figure 16  shows some results for faces including visible tongues. In many cases the reconstruction is of high quality, though we note failures due to lack of expressivity of the blendshapes and failures of the landmark DNN to accurately regress landmarks on the tongue. Common failure cases include confusion of the tongue for thicker lips, and failure to identify the visibility of the tongue at all. Addition of the tongue blendshapes also demonstrates how we could quite easily add further blendshapes based on quality gaps noticed in reconstruction quality for a given capture, or based on an enrolment sequence for a specific actor with idiosyncratic facial motion.",
            "Results for the single-view case on the Human3.6M dataset  (Ionescu et al . ,  2014 )  are shown in  Table 7 . Our method performs comparatively to other methods despite ours being the only method  not  trained on the Human3.6M training set. As highlighted in the main paper, the qualitative results shown in the main paper and in  Figure 17  demonstrate that our method recovers significantly higher-quality body meshes. This however, is not reflected in the MPJPE metric calculated on sparse ground-truth annotations.",
            "Further qualitative comparisons for full-body reconstruction on the EHF  (Pavlakos et al . ,  2019 )  and Human3.6M  (Ionescu et al . ,  2014 )  datasets are shown in  Figure 17 . One failure case of our method is for complex interlocking hand poses. We suspect that with higher-quality pose data (e.g.,  Moon et al .  ( 2020 ) ) and consequently better synthetic data (e.g.,  Moon et al .  ( 2023 ) ) we might improve robustness in these cases. It is possible though that the model fitting method itself may need to be augmented to adequately deal with these scenarios, e.g., to account for intersection or contact points  (Taheri et al . ,  2020 ,  2022 ) .",
            "Qualitative results of our method for face reconstruction on the NoW benchmark  (Sanyal et al . ,  2019 )  are shown in  Figure 18 . Qualitative results of our method for hand reconstruction on the FreiHAND dataset  (Zimmermann et al . ,  2019 ) , including some failure cases, are shown in  Figure 19 .",
            "In  Table 8 ,  Table 9  and  Table 10  we compare the performance without DNN initialization to the full method. When not using DNN initialization we initialize with T-pose and template shape. In the single-view case ( Table 8 ,  Table 9 ), for the same number of iterations, we see significantly worse results, demonstrating that the initialization is critical to enable to optimizer to find a good minimum. Results are far less stable in the absence of initialization, drastically affecting the robustness of the method. In the multi-view case ( Table 10 ), the impact of initialization seems to be minor, likely because there is a great deal more data informing the optimization and therefore much less ambiguity for the optimizer to deal with.",
            "We make use of dense probabilistic landmarks in our method to provide additional freedom to the optimizer based on the confidence of the DNN prediction. In  Table 8 ,  Table 9  and  Table 10  we show the effect of ignoring these confidence predictions and treating all landmarks with uniform certainty during optimization. In all cases this has a negative impact on results, as the optimizer is distracted by poor landmark predictions and unable to fine-tune for those landmarks that are highly accurate. In the multi-view case ( Table 10 ) the benefit of probabilistic landmarks appears to be greater, allowing the optimizer to use the best available viewpoint, while in the single-view case ( Table 9 ) the benefit is less significant.",
            "Figure 21  shows the benefit of probabilistic landmarks in dealing with self-occlusions due to variation in body pose. Parts that are occluded can be informed more strongly by observations from other views, or priors, leading to more plausible results in these cases.",
            "Table 9  and  Table 10  show the impact of the pose, shape and temporal priors on registration quality in monocular and multi-view scenarios. In the monocular case all of these priors have a small beneficial effect on results. In the multi-view case, however, the impact is negligible and in the case of the temporal prior it actually increases the MPJPE very slightly. This is not unexpected; in the multi-view case there is sufficient data from the dense landmarks to almost completely specify the result, and we need to rely very little on priors. In the monocular case there is often self-occlusion or depth ambiguity where we rely more heavily on the pose prior, or scale ambiguity where we rely more on the shape prior. The temporal prior promotes smoothness of the motion, something that is perceptually important for performance capture, but not reflected at all in the MPJPE metric."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 .  Single-view hand reconstruction errors on the FreiHAND dataset  (Zimmermann et al . ,  2019 ) ,  ours is the only method not trained on FreiHAND . Qualitative results are shown in  Figure 11 .",
        "table": "S5.T2.3.3",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "The pipeline starts with a parametric model which captures face and body shape, body and hand pose and facial expression, including articulation of the tongue. To construct this, we build on the popular SMPL-H body model  (Romero et al . ,  2017 )  and the face model of  Wood et al .  ( 2021 ) . We then add texture, hair and clothing from a large and diverse asset library. Finally, we situate the body in an indoor 3D scene or an outdoor scene with environmental lighting from an HDR image  (Debevec,  2006 ) . We render the scene using the Cycles rendering engine  (Blender Foundation,  2021 ) .  Figure 2  shows how we construct a render, as well as some example ground-truth annotations output by our pipeline. Examples of images rendered using our pipeline can be seen in  Figure 3 . All data originating from captures of real people used in the pipeline is obtained with explicit consent. Below we give a brief summary of the parametric model and data generation pipeline; for greater detail, please consult the supplementary material.",
            "As outlined in  Figure 2 , we build up a unique render by sampling shape, pose, hair and clothing assets, texture and environment. Face and body shape are sampled from GMMs fit to, or directly from, the face training data and a library of 3572 body scans from various sources  (Yang et al . ,  2014 ; Mahmood et al . ,  2019 ; Renderpeople,  2024 ; Ten24 Media,  2024 ) . Example identities are shown in  Figure 5 .",
            "The shape head predicts SOMA shape parameters for which we compute the  L 1 subscript L 1 L_{1} italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  error between the ground truth shape parameters and the prediction,  L shape =       1 subscript L shape subscript norm  superscript   1 \\mathcal{L}_{\\textrm{shape}}=\\|\\boldsymbol{\\beta}-\\boldsymbol{\\beta}^{\\prime}% \\|_{1} caligraphic_L start_POSTSUBSCRIPT shape end_POSTSUBSCRIPT =  bold_italic_ - bold_italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . Note that shape parameters are also implicitly optimized in  Equation 2 . Our final loss function is therefore",
            "We use a number of priors to regularize the optimization  (Wood et al . ,  2022 ; Pavlakos et al . ,  2019 ) .  E shape subscript E shape E_{\\textrm{shape}} italic_E start_POSTSUBSCRIPT shape end_POSTSUBSCRIPT  is a combination of GMM priors; minimizing the log probability of the given body and face shape,    \\boldsymbol{\\beta} bold_italic_  and    \\boldsymbol{\\gamma} bold_italic_ . For faces we fit a GMM to the shape parameters of the training scans described in  Section 3.1 , for bodies we fit a GMM to a library of SMPL-H identities ( Section 3.2 ).  E exp subscript E exp E_{\\textrm{exp}} italic_E start_POSTSUBSCRIPT exp end_POSTSUBSCRIPT  is an  L 1 subscript L 1 L_{1} italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  loss on expression parameters to promote sparsity, combined with quartic barrier loss to keep coefficients in the range  [ 0 , 1 ] 0 1 [0,1] [ 0 , 1 ] .  E temp subscript E temp E_{\\textrm{temp}} italic_E start_POSTSUBSCRIPT temp end_POSTSUBSCRIPT  is an  L 2 subscript L 2 L_{2} italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  loss on frame-to-frame distance between 3D vertex locations, this helps to reduce jitter that might arise from noise in the predicted 2D landmarks.  E intsct subscript E intsct E_{\\textrm{intsct}} italic_E start_POSTSUBSCRIPT intsct end_POSTSUBSCRIPT  is to prevent intersection of the eyeballs, teeth and tongue with the surface of the face  (Wood et al . ,  2022 ) .  E cam subscript E cam E_{\\textrm{cam}} italic_E start_POSTSUBSCRIPT cam end_POSTSUBSCRIPT  is an  L 2 subscript L 2 L_{2} italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  loss on the position of the cameras compared to their initial location, to prevent them moving too far from the initialization. It is trivial to extend the system with additional priors in the case where we have further information on the scene. For example we can add an  L 2 subscript L 2 L_{2} italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  loss on the height of the subject to reduce ambiguity in scale, or restrict the shape prior based on the gender of the subject.",
            "To evaluate performance capture for the hands we use the FrieHAND dataset  (Zimmermann et al . ,  2019 ) . We fit just the MANO hand model  (Romero et al . ,  2017 )  using the prediction of the hand DNN and  E pose subscript E pose E_{\\textrm{pose}} italic_E start_POSTSUBSCRIPT pose end_POSTSUBSCRIPT  only. Quantitative results are shown in  Table 2 , where our method performs comparably to recent approaches. Note that ours is the only method that is  not  trained on the FreiHAND training set and that recent non-parametric methods  (Moon and Lee,  2020 ; Lin et al . ,  2021b )  lead to superior metrics but are not useful for performance capture. Qualitative results and comparison to recent work are shown in  Figure 11  and the supplementary material.",
            "We evaluate facial performance capture on the NoW benchmark  (Sanyal et al . ,  2019 )  by fitting just the face of the SOMA model, omitting any unnecessary energy terms; results are shown in  Table 3 . Most full-body benchmarks have low-quality face ground truth, so this lets us compare our face model with other recent methods, including the model of  Wood et al .  ( 2021 ) , more directly. We also evaluate a variant of the NoW benchmark where we reconstruct the face shape of each subject using  all  images of that subject together as input to our system, i.e., the multi-view scenario. While our method does not outperform  Zhang et al .  ( 2023a )  in the single-view case, our approach extends to multi-view input which provides more accurate reconstructions. Qualitative results and comparison to recent work are shown in  Figure 12  and the supplementary material. The NoW challenge only considers the face, not the full head, so improved quality of head/ear shape is not captured in the quantitative results.",
            "In  Table 8  and  Table 9  we compare the performance of the DNN only (i.e., the method without optimization) to the full method. The impact of optimization is generally small as the DNNs are able to directly produce quite accurate results. The models are robust to diverse poses and image conditions, and appear to have learned a strong prior over both body pose and shape. The optimization serves to fine-tune the precision of the results in terms of image-space alignment, fine-grained pose and shape detail and temporal stability. This makes a large difference to perceived quality of the results, though this is not clearly expressed in the metrics used, see  Figure 20 .",
            "Our method uses dedicated DNNs to refine the face and hand predictions.  Figure 20  shows results when not performing this refinement and just using the body DNN in isolation. This significantly degrades performance for the face and hands.",
            "Figure 21  shows the benefit of probabilistic landmarks in dealing with self-occlusions due to variation in body pose. Parts that are occluded can be informed more strongly by observations from other views, or priors, leading to more plausible results in these cases.",
            "We use the Human3.6M benchmark  (Ionescu et al . ,  2014 )  to evaluate the impact of number of views on our method. As our approach can take arbitrary input viewpoints, we run the reconstruction process with subsets of the four views available in the dataset. As shown in  Figure 22 , results improve with each camera added, although the benefit diminishes with greater numbers of cameras. In the monocular case there is a significant scale/translation ambiguity present, so it is no surprise that world-space (not pelvis-aligned) MPJPE is high in this case. Adding a second view gives a strong cue for scale so drastically improves results. After this each camera can help to reduce the impact of self-occlusions and further refine results, but with far more limited effect. Going beyond four cameras we would expect the impact to further diminish as it is largely redundant information that is being added to the objective function."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 .  Face shape reconstruction results on the NoW Challenge  (Sanyal et al . ,  2019 )  validation and test sets. Qualitative results are shown in  Figure 12 .",
        "table": "S5.F11.4",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "The pipeline starts with a parametric model which captures face and body shape, body and hand pose and facial expression, including articulation of the tongue. To construct this, we build on the popular SMPL-H body model  (Romero et al . ,  2017 )  and the face model of  Wood et al .  ( 2021 ) . We then add texture, hair and clothing from a large and diverse asset library. Finally, we situate the body in an indoor 3D scene or an outdoor scene with environmental lighting from an HDR image  (Debevec,  2006 ) . We render the scene using the Cycles rendering engine  (Blender Foundation,  2021 ) .  Figure 2  shows how we construct a render, as well as some example ground-truth annotations output by our pipeline. Examples of images rendered using our pipeline can be seen in  Figure 3 . All data originating from captures of real people used in the pipeline is obtained with explicit consent. Below we give a brief summary of the parametric model and data generation pipeline; for greater detail, please consult the supplementary material.",
            "To train the DNNs used in our method, we generate three datasets, each containing approximately 100,000 images, one featuring the entire body, one the face, and one the left hand. We call these datasets  SynthBody ,  SynthFace  and  SynthHand . These datasets will be released publicly with sparse annotations upon publication. Example images from each dataset can be seen in  Figure 3 .  SynthBody  and  SynthFace  are rendered at  512  512 512 512 512\\times 512 512  512  pixel resolution, both include 2D landmark and semantic segmentation annotations.  SynthBody  also includes 3D joint locations and SMPL-H pose and shape parameters while  SynthFace  also includes head pose annotations.  SynthHand  is rendered at  256  256 256 256 256\\times 256 256  256  pixel resolution and includes 2D and 3D joint locations and MANO parameter annotations. All datasets also include camera parameters, download instructions can be found at  https://aka.ms/SynthMoCap . Note that all results presented below are from DNNs trained  exclusively  on these synthetic datasets.",
            "We employ a multi-stage technique to fit the SOMA model to image or video data. The first stage regresses information including 2D landmarks from the input image(s) using DNNs trained on synthetic data generated using the pipeline described in  Section 3 . The second stage uses conventional optimization to fit our parametric model to the observed 2D landmarks by minimizing the reprojection error of the 3D mesh vertices into the image(s). The optimization process involves a series of energy terms to encode prior knowledge of the scenario, as detailed below. We find that using 2D landmarks alone does not lead to robust results, and that it is critical to initialize the optimization with a good approximation of the body shape and pose. As such, we also train the DNNs to predict body pose and shape from the input image(s). The method can therefore also be run in real-time without using optimization to provide a solution for pre-visualization of results.  Figure 8  provides a graphical representation of our pipeline.",
            "We use a number of priors to regularize the optimization  (Wood et al . ,  2022 ; Pavlakos et al . ,  2019 ) .  E shape subscript E shape E_{\\textrm{shape}} italic_E start_POSTSUBSCRIPT shape end_POSTSUBSCRIPT  is a combination of GMM priors; minimizing the log probability of the given body and face shape,    \\boldsymbol{\\beta} bold_italic_  and    \\boldsymbol{\\gamma} bold_italic_ . For faces we fit a GMM to the shape parameters of the training scans described in  Section 3.1 , for bodies we fit a GMM to a library of SMPL-H identities ( Section 3.2 ).  E exp subscript E exp E_{\\textrm{exp}} italic_E start_POSTSUBSCRIPT exp end_POSTSUBSCRIPT  is an  L 1 subscript L 1 L_{1} italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  loss on expression parameters to promote sparsity, combined with quartic barrier loss to keep coefficients in the range  [ 0 , 1 ] 0 1 [0,1] [ 0 , 1 ] .  E temp subscript E temp E_{\\textrm{temp}} italic_E start_POSTSUBSCRIPT temp end_POSTSUBSCRIPT  is an  L 2 subscript L 2 L_{2} italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  loss on frame-to-frame distance between 3D vertex locations, this helps to reduce jitter that might arise from noise in the predicted 2D landmarks.  E intsct subscript E intsct E_{\\textrm{intsct}} italic_E start_POSTSUBSCRIPT intsct end_POSTSUBSCRIPT  is to prevent intersection of the eyeballs, teeth and tongue with the surface of the face  (Wood et al . ,  2022 ) .  E cam subscript E cam E_{\\textrm{cam}} italic_E start_POSTSUBSCRIPT cam end_POSTSUBSCRIPT  is an  L 2 subscript L 2 L_{2} italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  loss on the position of the cameras compared to their initial location, to prevent them moving too far from the initialization. It is trivial to extend the system with additional priors in the case where we have further information on the scene. For example we can add an  L 2 subscript L 2 L_{2} italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  loss on the height of the subject to reduce ambiguity in scale, or restrict the shape prior based on the gender of the subject.",
            "We evaluate facial performance capture on the NoW benchmark  (Sanyal et al . ,  2019 )  by fitting just the face of the SOMA model, omitting any unnecessary energy terms; results are shown in  Table 3 . Most full-body benchmarks have low-quality face ground truth, so this lets us compare our face model with other recent methods, including the model of  Wood et al .  ( 2021 ) , more directly. We also evaluate a variant of the NoW benchmark where we reconstruct the face shape of each subject using  all  images of that subject together as input to our system, i.e., the multi-view scenario. While our method does not outperform  Zhang et al .  ( 2023a )  in the single-view case, our approach extends to multi-view input which provides more accurate reconstructions. Qualitative results and comparison to recent work are shown in  Figure 12  and the supplementary material. The NoW challenge only considers the face, not the full head, so improved quality of head/ear shape is not captured in the quantitative results.",
            "Results for the EHF dataset  (Pavlakos et al . ,  2019 )  are shown in  Table 4 . We outperform recent methods in most metrics or are on par with concurrent works, and qualitatively achieve significantly more accurate results as shown in  Figure 13 . We can easily introduce additional supervisory signals to our method; for example if we introduce a single measurement of the actors height we achieve a full-body MPVPE ( without  pelvis alignment) of 43.1mm, as the scale ambiguity inherent to the single-view scenario is removed.",
            "Results for multi-view reconstruction on the Human3.6M dataset  (Ionescu et al . ,  2014 )  are shown in  Table 6 . Our method significantly outperforms other multi-view methods which are also not trained on Human3.6M. Given the limited variety of the data there is a high chance that other methods over-fit; this is supported by the results of  Choudhury et al .  ( 2023 )  reproduced in  Table 6 . The Human3.6M benchmark evaluates only joint positions which do not fully specify human pose, leaving some degrees of freedom (e.g., rotation about a limb) undefined  (Zhang et al . ,  2021 ) . Our method, which recovers the full body mesh, often leads to more compelling results, supported by the qualitative results shown in  Figure 13 ."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 .  Single-view holistic reconstruction errors on the EHF dataset  (Pavlakos et al . ,  2019 ) . MPVPE is pelvis aligned, PA-MPVPE is aligned with Procrustes analysis. Qualitative results are shown in  Figure 13 .",
        "table": "S5.T3.11",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "Tongue articulation is achieved through addition of 12 blendshapes solely to control the movement of the tongue, covering a large proportion of possible articulations of the tongue. Given that the tongue is such a complex muscle, it would be almost impossible to capture its  full  range of motion using a practical number of blendshapes. Rather than introducing a complex joint based rig which would involve a more challenging fitting procedure, we instead compromise on the expressivity of our model and select a minimal set of blendshapes that provide reasonable coverage, these are visualized in  Figure 4 .",
            "Results for the EHF dataset  (Pavlakos et al . ,  2019 )  are shown in  Table 4 . We outperform recent methods in most metrics or are on par with concurrent works, and qualitatively achieve significantly more accurate results as shown in  Figure 13 . We can easily introduce additional supervisory signals to our method; for example if we introduce a single measurement of the actors height we achieve a full-body MPVPE ( without  pelvis alignment) of 43.1mm, as the scale ambiguity inherent to the single-view scenario is removed.",
            "To construct the template mesh,  T     T \\mathbf{\\overline{T}} over  start_ARG bold_T end_ARG , we manually align the template of  Wood et al .  ( 2021 )  to the head of the SMPL-H template. Once aligned, the head of SMPL-H and lower neck of the new head are removed and the two partial meshes merged. The topology around the join is hand-crafted to create a smooth transition given the different density of the two meshes, see  Figure 14 . A new UV-map is also hand-authored based on the SMPL-H UV space."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 .  Single-view body shape reconstruction errors on the SSP-3D dataset  (Sengupta et al . ,  2020 ) .",
        "table": "S5.F12.2.1",
        "footnotes": [
            ""
        ],
        "references": [
            "As outlined in  Figure 2 , we build up a unique render by sampling shape, pose, hair and clothing assets, texture and environment. Face and body shape are sampled from GMMs fit to, or directly from, the face training data and a library of 3572 body scans from various sources  (Yang et al . ,  2014 ; Mahmood et al . ,  2019 ; Renderpeople,  2024 ; Ten24 Media,  2024 ) . Example identities are shown in  Figure 5 .",
            "Results for shape reconstruction on the SSP-3D dataset are shown in  Table 5 . Our method outperforms the closest method also trained exclusively on synthetic data  (Black et al . ,  2023 ) , as well as beating the overall state-of-the-art approach  (Sengupta et al . ,  2021a ) .",
            "Examples showing the poor semantic consistency in the mouth region described in the main paper, and improvements resulting from the above process, are shown in  Figure 15 ."
        ]
    },
    "id_table_6": {
        "caption": "Table 6 .   Multi-view body pose errors on the Human3.6M validation set  (Ionescu et al . ,  2014 )  excluding sequences with incorrect ground-truth annotations following  Iskakov et al .  ( 2019 ) . MPJPE is  not  pelvis aligned.",
        "table": "S5.F12.4.1",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "For facial expression we re-register the library of  Wood et al .  ( 2021 )  to improve the quality and add tongue articulation, as well as adding additional artist generated sequences including the tongue. For body and hand pose we sample from a large library including AMASS  (Mahmood et al . ,  2019 ) , MANO  (Romero et al . ,  2017 )  and other motion capture data processed using MoSh  (Loper et al . ,  2014 ) . Example poses and expressions are shown in  Figure 6 .",
            "Results for multi-view reconstruction on the Human3.6M dataset  (Ionescu et al . ,  2014 )  are shown in  Table 6 . Our method significantly outperforms other multi-view methods which are also not trained on Human3.6M. Given the limited variety of the data there is a high chance that other methods over-fit; this is supported by the results of  Choudhury et al .  ( 2023 )  reproduced in  Table 6 . The Human3.6M benchmark evaluates only joint positions which do not fully specify human pose, leaving some degrees of freedom (e.g., rotation about a limb) undefined  (Zhang et al . ,  2021 ) . Our method, which recovers the full body mesh, often leads to more compelling results, supported by the qualitative results shown in  Figure 13 .",
            "Figure 16  shows some results for faces including visible tongues. In many cases the reconstruction is of high quality, though we note failures due to lack of expressivity of the blendshapes and failures of the landmark DNN to accurately regress landmarks on the tongue. Common failure cases include confusion of the tongue for thicker lips, and failure to identify the visibility of the tongue at all. Addition of the tongue blendshapes also demonstrates how we could quite easily add further blendshapes based on quality gaps noticed in reconstruction quality for a given capture, or based on an enrolment sequence for a specific actor with idiosyncratic facial motion."
        ]
    },
    "id_table_7": {
        "caption": "Table 7 .  Single-view body pose errors on the Human3.6M validation set  (Ionescu et al . ,  2014 ) . MPJPE is pelvis aligned, PA-MPJPE is aligned with Procrustes-analysis.  Note that ours is the only method not trained on the Human3.6M training set.  Qualitative results are shown in the main paper and  Figure 17 .",
        "table": "S5.F12.6.1",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "Once we have a posed body mesh, we add hair, clothing and accessories and render the result in a realistic environment. For this, we construct a large library of artist-created assets, from which we can sample to produce a diverse range of realistic scenes. We use displacement maps to model basic clothing, and mesh assets for glasses and headwear. Hair is represented using strands, with separate assets for the scalp hair, facial hair, eyebrows and eyelashes. We use a mixture of HDRIs and 3D room environments to provide a wide variety of environmental lighting and background content. Example assets can be seen in  Figure 7  and further details of the asset library are given in the supplementary material.",
            "Results for the single-view case on the Human3.6M dataset  (Ionescu et al . ,  2014 )  are shown in  Table 7 . Our method performs comparatively to other methods despite ours being the only method  not  trained on the Human3.6M training set. As highlighted in the main paper, the qualitative results shown in the main paper and in  Figure 17  demonstrate that our method recovers significantly higher-quality body meshes. This however, is not reflected in the MPJPE metric calculated on sparse ground-truth annotations.",
            "Further qualitative comparisons for full-body reconstruction on the EHF  (Pavlakos et al . ,  2019 )  and Human3.6M  (Ionescu et al . ,  2014 )  datasets are shown in  Figure 17 . One failure case of our method is for complex interlocking hand poses. We suspect that with higher-quality pose data (e.g.,  Moon et al .  ( 2020 ) ) and consequently better synthetic data (e.g.,  Moon et al .  ( 2023 ) ) we might improve robustness in these cases. It is possible though that the model fitting method itself may need to be augmented to adequately deal with these scenarios, e.g., to account for intersection or contact points  (Taheri et al . ,  2020 ,  2022 ) ."
        ]
    },
    "id_table_8": {
        "caption": "Table 8 .  Ablations on the EHF dataset  (Pavlakos et al . ,  2019 ) . MPVPE is pelvis aligned.",
        "table": "S5.T4.3",
        "footnotes": [
            ""
        ],
        "references": [
            "We employ a multi-stage technique to fit the SOMA model to image or video data. The first stage regresses information including 2D landmarks from the input image(s) using DNNs trained on synthetic data generated using the pipeline described in  Section 3 . The second stage uses conventional optimization to fit our parametric model to the observed 2D landmarks by minimizing the reprojection error of the 3D mesh vertices into the image(s). The optimization process involves a series of energy terms to encode prior knowledge of the scenario, as detailed below. We find that using 2D landmarks alone does not lead to robust results, and that it is critical to initialize the optimization with a good approximation of the body shape and pose. As such, we also train the DNNs to predict body pose and shape from the input image(s). The method can therefore also be run in real-time without using optimization to provide a solution for pre-visualization of results.  Figure 8  provides a graphical representation of our pipeline.",
            "Qualitative results of our method for face reconstruction on the NoW benchmark  (Sanyal et al . ,  2019 )  are shown in  Figure 18 . Qualitative results of our method for hand reconstruction on the FreiHAND dataset  (Zimmermann et al . ,  2019 ) , including some failure cases, are shown in  Figure 19 .",
            "In  Table 8 ,  Table 9  and  Table 10  we compare the performance without DNN initialization to the full method. When not using DNN initialization we initialize with T-pose and template shape. In the single-view case ( Table 8 ,  Table 9 ), for the same number of iterations, we see significantly worse results, demonstrating that the initialization is critical to enable to optimizer to find a good minimum. Results are far less stable in the absence of initialization, drastically affecting the robustness of the method. In the multi-view case ( Table 10 ), the impact of initialization seems to be minor, likely because there is a great deal more data informing the optimization and therefore much less ambiguity for the optimizer to deal with.",
            "In  Table 8  and  Table 9  we compare the performance of the DNN only (i.e., the method without optimization) to the full method. The impact of optimization is generally small as the DNNs are able to directly produce quite accurate results. The models are robust to diverse poses and image conditions, and appear to have learned a strong prior over both body pose and shape. The optimization serves to fine-tune the precision of the results in terms of image-space alignment, fine-grained pose and shape detail and temporal stability. This makes a large difference to perceived quality of the results, though this is not clearly expressed in the metrics used, see  Figure 20 .",
            "We make use of dense probabilistic landmarks in our method to provide additional freedom to the optimizer based on the confidence of the DNN prediction. In  Table 8 ,  Table 9  and  Table 10  we show the effect of ignoring these confidence predictions and treating all landmarks with uniform certainty during optimization. In all cases this has a negative impact on results, as the optimizer is distracted by poor landmark predictions and unable to fine-tune for those landmarks that are highly accurate. In the multi-view case ( Table 10 ) the benefit of probabilistic landmarks appears to be greater, allowing the optimizer to use the best available viewpoint, while in the single-view case ( Table 9 ) the benefit is less significant."
        ]
    },
    "id_table_9": {
        "caption": "Table 9 .  Single-view ablations on the Human3.6M validation set  (Ionescu et al . ,  2014 ) . MPJPE is pelvis aligned.",
        "table": "S5.T5.10",
        "footnotes": [
            ""
        ],
        "references": [
            "Our model-fitting pipeline relies on full-body dense landmarks as well as an initialization for pose and shape. High density of the landmarks is critical to ensure accurate shape reconstruction of the surface of the human  (Wood et al . ,  2022 ) ; we show our landmark set on the template meshes in  Figure 9 . We observe that DNNs trained to directly regress parametric model parameters from images are very robust and learn a strong implicit prior over the body pose, though struggle to achieve very precise results on their own. Starting from a reasonable estimate of body pose, however, helps the optimizer to find a better solution, particularly in the under-constrained single-view case.",
            "We train the body DNN on the  SynthBody  dataset using the full architecture and losses described above. The model predicts 1428 landmarks on the body (see  Figure 9 ), body pose,    R 23  6  superscript R 23 6 \\boldsymbol{\\xi}\\in\\mathbb{R}^{23\\times 6} bold_italic_  blackboard_R start_POSTSUPERSCRIPT 23  6 end_POSTSUPERSCRIPT , (excluding the pelvis and hands) and the first 10 components of the SOMA body shape parameters which explain the majority of shape variance. The hand DNN is trained on the  SynthHand  dataset with the shape head and corresponding loss omitted (  s = 0 subscript  s 0 \\alpha_{s}=0 italic_ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT = 0 ). We train only a single DNN for the left hand. During inference, we mirror the input image and predicted landmarks in the  x x x italic_x -direction and mirror the predicted pose around the  y y y italic_y -axis for the right hand. The hand DNN predicts 141 dense landmarks on the hand (see  Figure 9 ) and 15 joints for the hand pose (excluding the wrist). The face DNN is trained on the  SynthFace  dataset and directly regresses 744 landmarks on the face (see  Figure 9 ). We omit the pose and shape heads (  s =  p =  t =  p = 0 subscript  s subscript  p subscript  t subscript  p 0 \\alpha_{s}=\\alpha_{p}=\\alpha_{t}=\\alpha_{p}=0 italic_ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT = italic_ start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_ start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = 0 ) as variation in the facial appearance due to shape and expression is of far lower magnitude than for the body, and the optimizer is able to fully determine these in the model fitting stage.",
            "Qualitative results of our method for face reconstruction on the NoW benchmark  (Sanyal et al . ,  2019 )  are shown in  Figure 18 . Qualitative results of our method for hand reconstruction on the FreiHAND dataset  (Zimmermann et al . ,  2019 ) , including some failure cases, are shown in  Figure 19 .",
            "In  Table 8 ,  Table 9  and  Table 10  we compare the performance without DNN initialization to the full method. When not using DNN initialization we initialize with T-pose and template shape. In the single-view case ( Table 8 ,  Table 9 ), for the same number of iterations, we see significantly worse results, demonstrating that the initialization is critical to enable to optimizer to find a good minimum. Results are far less stable in the absence of initialization, drastically affecting the robustness of the method. In the multi-view case ( Table 10 ), the impact of initialization seems to be minor, likely because there is a great deal more data informing the optimization and therefore much less ambiguity for the optimizer to deal with.",
            "In  Table 8  and  Table 9  we compare the performance of the DNN only (i.e., the method without optimization) to the full method. The impact of optimization is generally small as the DNNs are able to directly produce quite accurate results. The models are robust to diverse poses and image conditions, and appear to have learned a strong prior over both body pose and shape. The optimization serves to fine-tune the precision of the results in terms of image-space alignment, fine-grained pose and shape detail and temporal stability. This makes a large difference to perceived quality of the results, though this is not clearly expressed in the metrics used, see  Figure 20 .",
            "We make use of dense probabilistic landmarks in our method to provide additional freedom to the optimizer based on the confidence of the DNN prediction. In  Table 8 ,  Table 9  and  Table 10  we show the effect of ignoring these confidence predictions and treating all landmarks with uniform certainty during optimization. In all cases this has a negative impact on results, as the optimizer is distracted by poor landmark predictions and unable to fine-tune for those landmarks that are highly accurate. In the multi-view case ( Table 10 ) the benefit of probabilistic landmarks appears to be greater, allowing the optimizer to use the best available viewpoint, while in the single-view case ( Table 9 ) the benefit is less significant.",
            "Table 9  and  Table 10  show the impact of the pose, shape and temporal priors on registration quality in monocular and multi-view scenarios. In the monocular case all of these priors have a small beneficial effect on results. In the multi-view case, however, the impact is negligible and in the case of the temporal prior it actually increases the MPJPE very slightly. This is not unexpected; in the multi-view case there is sufficient data from the dense landmarks to almost completely specify the result, and we need to rely very little on priors. In the monocular case there is often self-occlusion or depth ambiguity where we rely more heavily on the pose prior, or scale ambiguity where we rely more on the shape prior. The temporal prior promotes smoothness of the motion, something that is perceptually important for performance capture, but not reflected at all in the MPJPE metric."
        ]
    },
    "id_table_10": {
        "caption": "Table 10 .  Multi-view ablations on the Human3.6M validation set  (Ionescu et al . ,  2014 )  excluding sequences with incorrect ground-truth annotations following  Iskakov et al .  ( 2019 ) . MPJPE is  not  pelvis aligned.",
        "table": "S5.T6.3",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "Following  Zhang et al .  ( 2023b ) , we use a HRNet backbone  (Sun et al . ,  2019 ) , pretrained on ImageNet  (Deng et al . ,  2009 ) , to extract visual features from the input image. As shown in  Figure 10 , the compressed representation computed with HRNet then serves as input to three MLP-based heads to predict probabilistic landmarks, pose, and shape.",
            "In  Table 8 ,  Table 9  and  Table 10  we compare the performance without DNN initialization to the full method. When not using DNN initialization we initialize with T-pose and template shape. In the single-view case ( Table 8 ,  Table 9 ), for the same number of iterations, we see significantly worse results, demonstrating that the initialization is critical to enable to optimizer to find a good minimum. Results are far less stable in the absence of initialization, drastically affecting the robustness of the method. In the multi-view case ( Table 10 ), the impact of initialization seems to be minor, likely because there is a great deal more data informing the optimization and therefore much less ambiguity for the optimizer to deal with.",
            "We make use of dense probabilistic landmarks in our method to provide additional freedom to the optimizer based on the confidence of the DNN prediction. In  Table 8 ,  Table 9  and  Table 10  we show the effect of ignoring these confidence predictions and treating all landmarks with uniform certainty during optimization. In all cases this has a negative impact on results, as the optimizer is distracted by poor landmark predictions and unable to fine-tune for those landmarks that are highly accurate. In the multi-view case ( Table 10 ) the benefit of probabilistic landmarks appears to be greater, allowing the optimizer to use the best available viewpoint, while in the single-view case ( Table 9 ) the benefit is less significant.",
            "Table 9  and  Table 10  show the impact of the pose, shape and temporal priors on registration quality in monocular and multi-view scenarios. In the monocular case all of these priors have a small beneficial effect on results. In the multi-view case, however, the impact is negligible and in the case of the temporal prior it actually increases the MPJPE very slightly. This is not unexpected; in the multi-view case there is sufficient data from the dense landmarks to almost completely specify the result, and we need to rely very little on priors. In the monocular case there is often self-occlusion or depth ambiguity where we rely more heavily on the pose prior, or scale ambiguity where we rely more on the shape prior. The temporal prior promotes smoothness of the motion, something that is perceptually important for performance capture, but not reflected at all in the MPJPE metric."
        ]
    },
    "id_table_11": {
        "caption": "",
        "table": "S5.F13.6",
        "footnotes": [
            "",
            "",
            "",
            ""
        ],
        "references": [
            "To evaluate performance capture for the hands we use the FrieHAND dataset  (Zimmermann et al . ,  2019 ) . We fit just the MANO hand model  (Romero et al . ,  2017 )  using the prediction of the hand DNN and  E pose subscript E pose E_{\\textrm{pose}} italic_E start_POSTSUBSCRIPT pose end_POSTSUBSCRIPT  only. Quantitative results are shown in  Table 2 , where our method performs comparably to recent approaches. Note that ours is the only method that is  not  trained on the FreiHAND training set and that recent non-parametric methods  (Moon and Lee,  2020 ; Lin et al . ,  2021b )  lead to superior metrics but are not useful for performance capture. Qualitative results and comparison to recent work are shown in  Figure 11  and the supplementary material."
        ]
    },
    "id_table_12": {
        "caption": "",
        "table": "A8.T7.13",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "We evaluate facial performance capture on the NoW benchmark  (Sanyal et al . ,  2019 )  by fitting just the face of the SOMA model, omitting any unnecessary energy terms; results are shown in  Table 3 . Most full-body benchmarks have low-quality face ground truth, so this lets us compare our face model with other recent methods, including the model of  Wood et al .  ( 2021 ) , more directly. We also evaluate a variant of the NoW benchmark where we reconstruct the face shape of each subject using  all  images of that subject together as input to our system, i.e., the multi-view scenario. While our method does not outperform  Zhang et al .  ( 2023a )  in the single-view case, our approach extends to multi-view input which provides more accurate reconstructions. Qualitative results and comparison to recent work are shown in  Figure 12  and the supplementary material. The NoW challenge only considers the face, not the full head, so improved quality of head/ear shape is not captured in the quantitative results."
        ]
    },
    "id_table_13": {
        "caption": "",
        "table": "A8.F17.6",
        "footnotes": [
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Results for the EHF dataset  (Pavlakos et al . ,  2019 )  are shown in  Table 4 . We outperform recent methods in most metrics or are on par with concurrent works, and qualitatively achieve significantly more accurate results as shown in  Figure 13 . We can easily introduce additional supervisory signals to our method; for example if we introduce a single measurement of the actors height we achieve a full-body MPVPE ( without  pelvis alignment) of 43.1mm, as the scale ambiguity inherent to the single-view scenario is removed.",
            "Results for multi-view reconstruction on the Human3.6M dataset  (Ionescu et al . ,  2014 )  are shown in  Table 6 . Our method significantly outperforms other multi-view methods which are also not trained on Human3.6M. Given the limited variety of the data there is a high chance that other methods over-fit; this is supported by the results of  Choudhury et al .  ( 2023 )  reproduced in  Table 6 . The Human3.6M benchmark evaluates only joint positions which do not fully specify human pose, leaving some degrees of freedom (e.g., rotation about a limb) undefined  (Zhang et al . ,  2021 ) . Our method, which recovers the full body mesh, often leads to more compelling results, supported by the qualitative results shown in  Figure 13 ."
        ]
    },
    "id_table_14": {
        "caption": "",
        "table": "A9.T8.10",
        "footnotes": [],
        "references": [
            "To construct the template mesh,  T     T \\mathbf{\\overline{T}} over  start_ARG bold_T end_ARG , we manually align the template of  Wood et al .  ( 2021 )  to the head of the SMPL-H template. Once aligned, the head of SMPL-H and lower neck of the new head are removed and the two partial meshes merged. The topology around the join is hand-crafted to create a smooth transition given the different density of the two meshes, see  Figure 14 . A new UV-map is also hand-authored based on the SMPL-H UV space."
        ]
    },
    "id_table_15": {
        "caption": "",
        "table": "A9.T9.10",
        "footnotes": [],
        "references": [
            "Examples showing the poor semantic consistency in the mouth region described in the main paper, and improvements resulting from the above process, are shown in  Figure 15 ."
        ]
    },
    "id_table_16": {
        "caption": "",
        "table": "A9.T10.15",
        "footnotes": [],
        "references": [
            "Figure 16  shows some results for faces including visible tongues. In many cases the reconstruction is of high quality, though we note failures due to lack of expressivity of the blendshapes and failures of the landmark DNN to accurately regress landmarks on the tongue. Common failure cases include confusion of the tongue for thicker lips, and failure to identify the visibility of the tongue at all. Addition of the tongue blendshapes also demonstrates how we could quite easily add further blendshapes based on quality gaps noticed in reconstruction quality for a given capture, or based on an enrolment sequence for a specific actor with idiosyncratic facial motion."
        ]
    },
    "id_table_17": {
        "caption": "",
        "table": "A9.F20.4",
        "footnotes": [],
        "references": [
            "Results for the single-view case on the Human3.6M dataset  (Ionescu et al . ,  2014 )  are shown in  Table 7 . Our method performs comparatively to other methods despite ours being the only method  not  trained on the Human3.6M training set. As highlighted in the main paper, the qualitative results shown in the main paper and in  Figure 17  demonstrate that our method recovers significantly higher-quality body meshes. This however, is not reflected in the MPJPE metric calculated on sparse ground-truth annotations.",
            "Further qualitative comparisons for full-body reconstruction on the EHF  (Pavlakos et al . ,  2019 )  and Human3.6M  (Ionescu et al . ,  2014 )  datasets are shown in  Figure 17 . One failure case of our method is for complex interlocking hand poses. We suspect that with higher-quality pose data (e.g.,  Moon et al .  ( 2020 ) ) and consequently better synthetic data (e.g.,  Moon et al .  ( 2023 ) ) we might improve robustness in these cases. It is possible though that the model fitting method itself may need to be augmented to adequately deal with these scenarios, e.g., to account for intersection or contact points  (Taheri et al . ,  2020 ,  2022 ) ."
        ]
    }
}