{
    "S4.T1": {
        "caption": "Table 1: Accuracy on SHAPES.",
        "table": "",
        "footnotes": "",
        "references": [
            "In this part, we will look into the influence of the volume of the data on our model‚Äôs generalization, and compare the performance of SIG-G2, MF-G2 and LBP-G2 models. We find it is difficult for all 3 models to generalize with the same amount of data as [1]. This may because [1] uses a parser to understand the queries with guaranteed correctness on this dataset, while we have to train the GRU to understand the queries from scratch. The parser-based method may not perform well in more general tasks such as the VQA dataset, since they found using fewer modules on the VQA dataset turned out to be better, but our RNN-based approach should generalize better with enough training data. So we generate more data with the same answer distributions for each query as [1] to train and test our model, and re-trained their model using the released code. We name the original dataset small, and the newly generated datasets with 2 and 3 times as much data in both training and test sets as medium and large respectively. We find with more training data, our model becomes competitive with [1] and the MF-G2 model surpasses it on both medium and large, as shown in Table 1. Our models are extremely good at handling length-4 queries, which looks for object arrangements in 4-neighborhood, as demonstrated in Fig. 3. The high accuracy also implies the model is capable of set theory reasoning, since it achieved high test accuracy with length-3 queries which contain self-conflict queries such as is red green, which aims to find an object that is both red and green. For complex queries, such as is red below below green, which aims to find a red object below another object that is below a green object, it is not as competitive as [1], probably because the GRU in our model has not generalized to higher order logic."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Accuracy on CLEVR. CI, QA, CA stand for Count Integer, Query Attribute and Compare Attribute respectively. The top half uses ResNet-152 features and the bottom half uses ResNet-101 features. Our best model uses the same visual feature as [13].",
        "table": "",
        "footnotes": "",
        "references": [
            "In this part, we study the role of different visual features and different kinds of attentions on the performance, and test our best models on the test set, as shown in Table 2. Our best model surpasses the best baseline model in [13] by more than 9.5% on the test set. Both MF and LBP outperform SM and SIG, demonstrating the effectiveness of our method. The maximum margin of MF/LBP vs. SIG on overall accuracy is 2.62% and 1.33% with the ResNet-152 and ResNet-101 features respectively. The most significant improvement of MF/LBP over other models is on Compare Attribute, which involves comparing specific attributes of objects specified by spatial relations with other objects. This also proves that our model alleviates the problem of previous methods of ignoring arrangements of regions. Further, we show the receptive fields of the 2 selected layers on the test set in Fig. 4. In each image, we choose the feature vector closest to the center, where there is a higher chance for objects to appear. Still, both of them only occupy a small portion of the image, indicating the importance of considering the structure of regions. Overall, the performance with res4b22 features is better than that with res5c features. From the ERF point of view, the ERF of res5c has more than twice the area as the ERF of res4b22. As a result, the feature vector of res5c may require more than twice the number of parameters to represent same amount of information in this region as res4b22, but its has only twice."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Results of the Open Ended task on test-dev.",
        "table": "",
        "footnotes": "",
        "references": [
            "Since we have found MF-SIG and LBP-SIG are the best on CLEVR, in this part, we mainly compare the two models with different TùëáT. Notice now the total number of glimpses is the same as MCB [8] and MLB [17], and both of them use res5c features and better feature pooling methods. The optimal choice in these experiments is MF-SIG-T3, which is 0.92% higher in overall accuracy than the previous best method [17], and outperforms previous methods on all 3 general categories of questions. We then use external data from Visual Genome to train MF-SIG-T3 and MF-T3, in which MF-SIG surpassed MLB under the same condition by 1.35%. The accuracy boost of our model is higher than MCB and MLB, showing that our model has higher capacity. The LBP models, which performs better than MF layers on CLEVR, turns out to be worse on this dataset, and T=1ùëá1T=1 is the optimal choice for LBP. We also find the single MF attention model, which should not be as powerful as MF-SIG, achieved 67.17% accuracy with augmentation. These might be caused by the bias of the current VQA dataset [3], where there are questions with fixed answers across all involved images. We also show the results on test, as shown in Table 4. Our model is the best among published methods without external data. With an ensemble of 3 MF-T3 and 4 MF-SIG-T3 models, we achieve 68.18% accuracy on test, 1.25% higher than best published ensemble model on the Open Ended task. By the date of submission, we rank the second on the leaderboard of Open Ended task and the first on that of the Multiple Choice task. The champion on Open Ended has an accuracy of 69.94% but the method is not published. We have also recorded our model‚Äôs performance on the test-dev2017 and test2017 of VQA2.0 in Table 3 and 4. Accuracy on test2017 is achieved with 8 snapshots from 4 models with different learning rates."
        ]
    },
    "S4.T4": {
        "caption": "Table 4: Results of the Open Ended and Multiple Choice tasks on test. We compare the accuracy of single models (without augmentation) and ensemble models with published methods.",
        "table": "",
        "footnotes": "",
        "references": [
            "Since we have found MF-SIG and LBP-SIG are the best on CLEVR, in this part, we mainly compare the two models with different TùëáT. Notice now the total number of glimpses is the same as MCB [8] and MLB [17], and both of them use res5c features and better feature pooling methods. The optimal choice in these experiments is MF-SIG-T3, which is 0.92% higher in overall accuracy than the previous best method [17], and outperforms previous methods on all 3 general categories of questions. We then use external data from Visual Genome to train MF-SIG-T3 and MF-T3, in which MF-SIG surpassed MLB under the same condition by 1.35%. The accuracy boost of our model is higher than MCB and MLB, showing that our model has higher capacity. The LBP models, which performs better than MF layers on CLEVR, turns out to be worse on this dataset, and T=1ùëá1T=1 is the optimal choice for LBP. We also find the single MF attention model, which should not be as powerful as MF-SIG, achieved 67.17% accuracy with augmentation. These might be caused by the bias of the current VQA dataset [3], where there are questions with fixed answers across all involved images. We also show the results on test, as shown in Table 4. Our model is the best among published methods without external data. With an ensemble of 3 MF-T3 and 4 MF-SIG-T3 models, we achieve 68.18% accuracy on test, 1.25% higher than best published ensemble model on the Open Ended task. By the date of submission, we rank the second on the leaderboard of Open Ended task and the first on that of the Multiple Choice task. The champion on Open Ended has an accuracy of 69.94% but the method is not published. We have also recorded our model‚Äôs performance on the test-dev2017 and test2017 of VQA2.0 in Table 3 and 4. Accuracy on test2017 is achieved with 8 snapshots from 4 models with different learning rates."
        ]
    }
}