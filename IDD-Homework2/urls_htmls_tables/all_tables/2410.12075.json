{
    "id_table_1": {
        "caption": "TABLE I :  Domain generalization performance (mIoU (%)) of state-of-arts methods using ResNet-50 and MiT-B5 as encoders . The compared methods are retrained with Cityscapes dataset. Evaluations are performed on ACDC, BDD100K, and DarkZurich datasets that feature adverse conditions, such as snow, rain, fog, and low-light scenarios.",
        "table": "S5.T1.2.1",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "We evaluate WeatherDG against state-of-the-art domain generalization models using ResNet-50  [ 40 ]  and MiT-B5  [ 41 ]  as encoders. As shown in  Table   I , for models using ResNet-50 as encoder, our model consistently outperforms state-of-the-art methods with the highest average mIoU score. With MiT-B5 as backbone, our model exceeds the second-best model MIC [ 10 ]  on the ACDC and DarkZurich datasets by over 10% and on the BDD100K dataset by 4.3% in mIoU performance, achieving the best semantic segmentation performance. In addition, we visualize our models semantic segmentation results under challenging conditions and compare them with MIC. As shown in  Figure   1 , our model correctly segments sidewalks and sky in foggy and nighttime scenes. In the rainy scene, reflections on the road that are falsely recognized as vehicles by MIC are alleviated by our model. In the snowy scenario, our model successfully detects pedestrians on the road that MIC fails to recognize. These findings indicate our models superior generalization capabilities compared to state-of-the-art methods in real-world challenging weather and lighting conditions."
        ]
    },
    "id_table_2": {
        "caption": "TABLE II :  Comparison of mIoU performance of UDA methods  trained using the labeled Cityscapes dataset as the source dataset and our generated unlabeled images as the target dataset.",
        "table": "S5.T2.2.1",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Data augmentation for domain generalization aims to generate new, diverse and realistic synthetic images for augmenting the training data. Previous methods commonly adopt simulators  [ 11 ,  12 ,  13 ]  or image translation models  [ 14 ,  15 ]  to generate new samples. Despite their effectiveness, these methods still suffer from diversity and authenticity, particularly in generating samples with adverse weather conditions (shown in  Figure   2 ). In recent years, Stable Diffusion (SD)  [ 16 ]  has shown its strong ability to generate realistic, diverse, and high-quality images. This inspires us to leverage SD to solve the drawbacks of previous data augmentation methods in domain generalization. However, directly applying SD in our task will lead to a critical issue: the styles and layouts are very different from the driving-screen samples (see  Figure   2 b )). Hence, training with such synthetic samples will hamper the performance of the model on unseen domains. This problem is mainly caused by that the SD was trained with various types of samples, such as natural images and artistic images, instead of specifically with the driving-screen samples. As such, the SD cannot well generate on-the-hand driving-screen-aware samples without a detailed and specific guide."
        ]
    },
    "id_table_3": {
        "caption": "TABLE III :  Comparison of mIoU performance of UDA techniques across typical weather and lighting conditions.",
        "table": "S5.T3.2.1",
        "footnotes": [
            "",
            "",
            ""
        ],
        "references": [
            "WeatherDG aims to generate images tailored to weather-specific autonomous driving scenes, enhancing semantic segmentation in challenging conditions. We begin by fine-tuning a diffusion model to adapt scene priors from the source domain, ensuring the generated images are within a driving scene ( Section   III-B ). Next, we employ a procedural prompt generation method to create detailed prompts that enable the diffusion model to produce realistic and diverse weather and lighting effects ( Section   III-C ). Additionally, we incorporate a probability-oriented sampling strategy for prompt generation. Following this, we use UDA training methods to leverage these generated images for semantic segmentation training ( Section   III-D ). The overview of the proposed approach is shown in  Figure   3 ."
        ]
    },
    "id_table_4": {
        "caption": "TABLE IV :  Comparison of mIoU performance of models trained on datasets generated with and without fine-tuned model.",
        "table": "S5.T4.2",
        "footnotes": [],
        "references": [
            "Recently, diffusion models have advanced the field of generative domain adaptation, showcasing exceptional capabilities in producing photo-realistic images conditioned on text  [ 30 ] . However, directly applying diffusion models in autonomous driving setting presents challenges due to shifts in scene priors like style and layout. For example, the model often generates artistic images or adopt a birds eye view perspective, which is different from the images in real-world autonomous driving datasets, as shown in Figure  4(b) . When these images are incorporated during training, they can disrupt the knowledge the model has acquired from the labeled source domain, thereby harming the semantic segmentation performance. To address these issues, we finetune a diffusion model  [ 16 ]  to generate diverse images that retain content and layouts relevant to source domain. The input consists of a clean image paired with a corresponding prompt text, while the output is an image tailored to the autonomous driving domain.",
            "We follow similar text-to-image diffusion model training procedures described in the relevant work  [ 34 ,  27 ] . Specifically, we use a unique identifier in the prompt to link priors to one single image choosen from the source domain dataset. For instance, the prompt A photo of  V  superscript V V^{*} italic_V start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  driving scene associates with image patches cropped from the selected image, where  V  superscript V V^{*} italic_V start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  identifies the scene, and driving scene describes it broadly. This prevents language drift and improves model performance  [ 34 ] . The training procedure is presented in  Figure   4(a) . After training, the model captures scene priors through the unique identifier  V  subscript V V_{*} italic_V start_POSTSUBSCRIPT  end_POSTSUBSCRIPT  and can generate new instances in similar contexts within autonomous driving datasets, as shown in Figure  4(b) ."
        ]
    },
    "id_table_5": {
        "caption": "TABLE V :  Comparison of mIoU performance for models trained on datasets generated by introducing different LLM agents.",
        "table": "S5.T5.7",
        "footnotes": [],
        "references": [
            "Intuitively, to enable the generation of images covering a wide range of weather effects and different times of the day, we can design the prompt template as A photo of <CLS >, <WEATHER >, <TIME > to describe the image to be generated. Here, <CLS > refers to the classes in the typical autonomous driving dataset, <WEATHER > specifies the weather conditions, and <TIME > indicates the time of the day. To enhance the balance and diversity of the generated dataset, we include three common weather conditions: snowy, rainy, and foggy, along with two distinct times of day: daytime and nighttime. Each condition and time period is equally represented in the dataset to ensure comprehensive coverage and variability. However, merely incorporating categories of weather and time category does not guarantee the desired diversity in the effects, it often generates a singular subject with minimal environmental context and a limited range of effects in relatively simple scenes. As shown in Figure  5 , with the prompt A photo of motorcycle, rainy, daytime generated by  E SC subscript E SC \\mathcal{E}_{\\mathit{SC}} caligraphic_E start_POSTSUBSCRIPT italic_SC end_POSTSUBSCRIPT , the model adds reflections on the road in daytime lighting, indicating a humid, rainy day, but the effect is subtle and the scene details are limited. Although the result does not fully meet expectations, it provides a solid foundation for further improvement."
        ]
    }
}