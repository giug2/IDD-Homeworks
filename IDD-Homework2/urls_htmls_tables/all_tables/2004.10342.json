{
    "PAPER'S NUMBER OF TABLES": 4,
    "S5.T1": {
        "caption": "Table 1: Precision@1 (%) on CIFAR-10and CIFAR-100.",
        "table": "<table id=\"S5.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Dataset</span></th>\n<th id=\"S5.T1.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Model</span></th>\n<td id=\"S5.T1.1.1.1.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">Baseline-1</span></td>\n<td id=\"S5.T1.1.1.1.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.1.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">Baseline-2</span></td>\n<td id=\"S5.T1.1.1.1.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.1.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">FedAwS</span></td>\n<td id=\"S5.T1.1.1.1.6\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S5.T1.1.1.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">Softmax (Oracle)</span></td>\n</tr>\n<tr id=\"S5.T1.1.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.2.2.1.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">CIFAR-10</span></th>\n<th id=\"S5.T1.1.2.2.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">\n<span id=\"S5.T1.1.2.2.2.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">ResNet</span><span id=\"S5.T1.1.2.2.2.2\" class=\"ltx_text\" style=\"font-size:90%;\">-8</span>\n</th>\n<td id=\"S5.T1.1.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.2.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">10.7</span></td>\n<td id=\"S5.T1.1.2.2.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.2.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">83.3</span></td>\n<td id=\"S5.T1.1.2.2.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.2.2.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">86.3</span></td>\n<td id=\"S5.T1.1.2.2.6\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S5.T1.1.2.2.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">88.4</span></td>\n</tr>\n<tr id=\"S5.T1.1.3.3\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.3.3.1.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">CIFAR-10</span></th>\n<th id=\"S5.T1.1.3.3.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">\n<span id=\"S5.T1.1.3.3.2.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">ResNet</span><span id=\"S5.T1.1.3.3.2.2\" class=\"ltx_text\" style=\"font-size:90%;\">-32</span>\n</th>\n<td id=\"S5.T1.1.3.3.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.3.3.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">9.8</span></td>\n<td id=\"S5.T1.1.3.3.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.3.3.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">92.1</span></td>\n<td id=\"S5.T1.1.3.3.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.3.3.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">92.4</span></td>\n<td id=\"S5.T1.1.3.3.6\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S5.T1.1.3.3.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">92.4</span></td>\n</tr>\n<tr id=\"S5.T1.1.4.4\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.4.4.1.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">CIFAR-100</span></th>\n<th id=\"S5.T1.1.4.4.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">\n<span id=\"S5.T1.1.4.4.2.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">ResNet</span><span id=\"S5.T1.1.4.4.2.2\" class=\"ltx_text\" style=\"font-size:90%;\">-32</span>\n</th>\n<td id=\"S5.T1.1.4.4.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.4.4.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.0</span></td>\n<td id=\"S5.T1.1.4.4.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.4.4.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">65.1</span></td>\n<td id=\"S5.T1.1.4.4.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.4.4.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">67.9</span></td>\n<td id=\"S5.T1.1.4.4.6\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S5.T1.1.4.4.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">68.0</span></td>\n</tr>\n<tr id=\"S5.T1.1.5.5\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.5.5.1.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">CIFAR-100</span></th>\n<th id=\"S5.T1.1.5.5.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"S5.T1.1.5.5.2.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">ResNet</span><span id=\"S5.T1.1.5.5.2.2\" class=\"ltx_text\" style=\"font-size:90%;\">-56</span>\n</th>\n<td id=\"S5.T1.1.5.5.3\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.5.5.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.1</span></td>\n<td id=\"S5.T1.1.5.5.4\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.5.5.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">67.5</span></td>\n<td id=\"S5.T1.1.5.5.5\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.5.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">69.6</span></td>\n<td id=\"S5.T1.1.5.5.6\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\"><span id=\"S5.T1.1.5.5.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">70.0</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "From Table1,\nwe see that\non both CIFAR-10and CIFAR-100, FedAwS almost matches or comes very close to the performance of the oracle method which has access to all labels.\nThe first baseline method, training with only positive squared hinge loss does not lead to any meaningful precision values. In this case, as discussed above the model collapses into a degenerate solution."
        ]
    },
    "S6.T2": {
        "caption": "Table 2: Summary of the datasets used in the paper. #I/L is the number of instances per label, and #L/I is the number of labels per instance.",
        "table": "<table id=\"S6.T2.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T2.1.1.1\" class=\"ltx_tr\">\n<td id=\"S6.T2.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S6.T2.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Dataset</span></td>\n<td id=\"S6.T2.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S6.T2.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">#Features</span></td>\n<td id=\"S6.T2.1.1.1.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S6.T2.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">#Labels</span></td>\n<td id=\"S6.T2.1.1.1.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S6.T2.1.1.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">#TrainPoints</span></td>\n<td id=\"S6.T2.1.1.1.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S6.T2.1.1.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">#TestPoints</span></td>\n<td id=\"S6.T2.1.1.1.6\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S6.T2.1.1.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">Avg. #I/L</span></td>\n<td id=\"S6.T2.1.1.1.7\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S6.T2.1.1.1.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">Avg. #L/I</span></td>\n</tr>\n<tr id=\"S6.T2.1.2.2\" class=\"ltx_tr\">\n<td id=\"S6.T2.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S6.T2.1.2.2.1.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">AmazonCat</span></td>\n<td id=\"S6.T2.1.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S6.T2.1.2.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">203,882</span></td>\n<td id=\"S6.T2.1.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S6.T2.1.2.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">13,330</span></td>\n<td id=\"S6.T2.1.2.2.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S6.T2.1.2.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">1,186,239</span></td>\n<td id=\"S6.T2.1.2.2.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S6.T2.1.2.2.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">306,782</span></td>\n<td id=\"S6.T2.1.2.2.6\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S6.T2.1.2.2.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">448.57</span></td>\n<td id=\"S6.T2.1.2.2.7\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S6.T2.1.2.2.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">5.04</span></td>\n</tr>\n<tr id=\"S6.T2.1.3.3\" class=\"ltx_tr\">\n<td id=\"S6.T2.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S6.T2.1.3.3.1.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">WikiLSHTC</span></td>\n<td id=\"S6.T2.1.3.3.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S6.T2.1.3.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">1,617,899</span></td>\n<td id=\"S6.T2.1.3.3.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S6.T2.1.3.3.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">325,056</span></td>\n<td id=\"S6.T2.1.3.3.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S6.T2.1.3.3.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">1,778,351</span></td>\n<td id=\"S6.T2.1.3.3.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S6.T2.1.3.3.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">587,084</span></td>\n<td id=\"S6.T2.1.3.3.6\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S6.T2.1.3.3.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">17.46</span></td>\n<td id=\"S6.T2.1.3.3.7\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S6.T2.1.3.3.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">3.19</span></td>\n</tr>\n<tr id=\"S6.T2.1.4.4\" class=\"ltx_tr\">\n<td id=\"S6.T2.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S6.T2.1.4.4.1.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:90%;\">Amazon670K</span></td>\n<td id=\"S6.T2.1.4.4.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S6.T2.1.4.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">135,909</span></td>\n<td id=\"S6.T2.1.4.4.3\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S6.T2.1.4.4.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">670,091</span></td>\n<td id=\"S6.T2.1.4.4.4\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S6.T2.1.4.4.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">490,449</span></td>\n<td id=\"S6.T2.1.4.4.5\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S6.T2.1.4.4.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">153,025</span></td>\n<td id=\"S6.T2.1.4.4.6\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S6.T2.1.4.4.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">3.99</span></td>\n<td id=\"S6.T2.1.4.4.7\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\"><span id=\"S6.T2.1.4.4.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">5.45</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Datasets.\nWe test the proposed approach on standard extreme multilabel classification datasets(Varma, 2018). These datasets have a large number of classes, and therefore are a good representatives of the applications of federated learning with only positive labels. Similar to (Reddi etal., 2019), because these datasets are multi-label, we uniformly sample positive labels to obtain datasets corresponding to multi-class classification problems. The datasets and their statistics are summarized in Table 2."
        ]
    },
    "S6.T3": {
        "caption": "Table 3: P@1,3,5 (%) of different methods on AmazonCat, Amazon670K and WikiLSHTC.",
        "table": "<table id=\"S6.T3.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T3.1.1.1\" class=\"ltx_tr\">\n<td id=\"S6.T3.1.1.1.1\" class=\"ltx_td\" colspan=\"2\"></td>\n<td id=\"S6.T3.1.1.1.2\" class=\"ltx_td ltx_align_center\" colspan=\"3\"><span id=\"S6.T3.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Federated Learning with Only Positives</span></td>\n<td id=\"S6.T3.1.1.1.3\" class=\"ltx_td ltx_align_center\" colspan=\"2\"><span id=\"S6.T3.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Oracle</span></td>\n</tr>\n<tr id=\"S6.T3.1.2.2\" class=\"ltx_tr\">\n<td id=\"S6.T3.1.2.2.1\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S6.T3.1.2.2.2\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S6.T3.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">Baseline-1</td>\n<td id=\"S6.T3.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">Baseline-2</td>\n<td id=\"S6.T3.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedAwS</td>\n<td id=\"S6.T3.1.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\">Softmax</td>\n<td id=\"S6.T3.1.2.2.7\" class=\"ltx_td ltx_align_center ltx_border_t\">SLEEC</td>\n</tr>\n<tr id=\"S6.T3.1.3.3\" class=\"ltx_tr\">\n<td id=\"S6.T3.1.3.3.1\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S6.T3.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">P@1</td>\n<td id=\"S6.T3.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">3.4</td>\n<td id=\"S6.T3.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">64.1</td>\n<td id=\"S6.T3.1.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">92.1</td>\n<td id=\"S6.T3.1.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\">92.1</td>\n<td id=\"S6.T3.1.3.3.7\" class=\"ltx_td ltx_align_center ltx_border_t\">90.5</td>\n</tr>\n<tr id=\"S6.T3.1.4.4\" class=\"ltx_tr\">\n<td id=\"S6.T3.1.4.4.1\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T3.1.4.4.1.1\" class=\"ltx_text ltx_font_smallcaps\">AmazonCat</span></td>\n<td id=\"S6.T3.1.4.4.2\" class=\"ltx_td ltx_align_center\">P@3</td>\n<td id=\"S6.T3.1.4.4.3\" class=\"ltx_td ltx_align_center\">3.2</td>\n<td id=\"S6.T3.1.4.4.4\" class=\"ltx_td ltx_align_center\">46.8</td>\n<td id=\"S6.T3.1.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_r\">70.8</td>\n<td id=\"S6.T3.1.4.4.6\" class=\"ltx_td ltx_align_center\">77.9</td>\n<td id=\"S6.T3.1.4.4.7\" class=\"ltx_td ltx_align_center\">76.3</td>\n</tr>\n<tr id=\"S6.T3.1.5.5\" class=\"ltx_tr\">\n<td id=\"S6.T3.1.5.5.1\" class=\"ltx_td\"></td>\n<td id=\"S6.T3.1.5.5.2\" class=\"ltx_td ltx_align_center\">P@5</td>\n<td id=\"S6.T3.1.5.5.3\" class=\"ltx_td ltx_align_center\">3.1</td>\n<td id=\"S6.T3.1.5.5.4\" class=\"ltx_td ltx_align_center\">32.6</td>\n<td id=\"S6.T3.1.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_r\">58.7</td>\n<td id=\"S6.T3.1.5.5.6\" class=\"ltx_td ltx_align_center\">62.3</td>\n<td id=\"S6.T3.1.5.5.7\" class=\"ltx_td ltx_align_center\">61.5</td>\n</tr>\n<tr id=\"S6.T3.1.6.6\" class=\"ltx_tr\">\n<td id=\"S6.T3.1.6.6.1\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S6.T3.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\">P@1</td>\n<td id=\"S6.T3.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.0</td>\n<td id=\"S6.T3.1.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_t\">4.3</td>\n<td id=\"S6.T3.1.6.6.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">33.1</td>\n<td id=\"S6.T3.1.6.6.6\" class=\"ltx_td ltx_align_center ltx_border_t\">35.2</td>\n<td id=\"S6.T3.1.6.6.7\" class=\"ltx_td ltx_align_center ltx_border_t\">35.1</td>\n</tr>\n<tr id=\"S6.T3.1.7.7\" class=\"ltx_tr\">\n<td id=\"S6.T3.1.7.7.1\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T3.1.7.7.1.1\" class=\"ltx_text ltx_font_smallcaps\">Amazon670K</span></td>\n<td id=\"S6.T3.1.7.7.2\" class=\"ltx_td ltx_align_center\">P@3</td>\n<td id=\"S6.T3.1.7.7.3\" class=\"ltx_td ltx_align_center\">0.0</td>\n<td id=\"S6.T3.1.7.7.4\" class=\"ltx_td ltx_align_center\">2.8</td>\n<td id=\"S6.T3.1.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_r\">29.6</td>\n<td id=\"S6.T3.1.7.7.6\" class=\"ltx_td ltx_align_center\">31.6</td>\n<td id=\"S6.T3.1.7.7.7\" class=\"ltx_td ltx_align_center\">31.3</td>\n</tr>\n<tr id=\"S6.T3.1.8.8\" class=\"ltx_tr\">\n<td id=\"S6.T3.1.8.8.1\" class=\"ltx_td\"></td>\n<td id=\"S6.T3.1.8.8.2\" class=\"ltx_td ltx_align_center\">P@5</td>\n<td id=\"S6.T3.1.8.8.3\" class=\"ltx_td ltx_align_center\">0.0</td>\n<td id=\"S6.T3.1.8.8.4\" class=\"ltx_td ltx_align_center\">2.2</td>\n<td id=\"S6.T3.1.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_r\">27.4</td>\n<td id=\"S6.T3.1.8.8.6\" class=\"ltx_td ltx_align_center\">29.5</td>\n<td id=\"S6.T3.1.8.8.7\" class=\"ltx_td ltx_align_center\">28.6</td>\n</tr>\n<tr id=\"S6.T3.1.9.9\" class=\"ltx_tr\">\n<td id=\"S6.T3.1.9.9.1\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S6.T3.1.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_t\">P@1</td>\n<td id=\"S6.T3.1.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_t\">7.6</td>\n<td id=\"S6.T3.1.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_t\">7.9</td>\n<td id=\"S6.T3.1.9.9.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">37.2</td>\n<td id=\"S6.T3.1.9.9.6\" class=\"ltx_td ltx_align_center ltx_border_t\">54.1</td>\n<td id=\"S6.T3.1.9.9.7\" class=\"ltx_td ltx_align_center ltx_border_t\">54.8</td>\n</tr>\n<tr id=\"S6.T3.1.10.10\" class=\"ltx_tr\">\n<td id=\"S6.T3.1.10.10.1\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T3.1.10.10.1.1\" class=\"ltx_text ltx_font_smallcaps\">WikiLSHTC</span></td>\n<td id=\"S6.T3.1.10.10.2\" class=\"ltx_td ltx_align_center\">P@3</td>\n<td id=\"S6.T3.1.10.10.3\" class=\"ltx_td ltx_align_center\">4.5</td>\n<td id=\"S6.T3.1.10.10.4\" class=\"ltx_td ltx_align_center\">3.4</td>\n<td id=\"S6.T3.1.10.10.5\" class=\"ltx_td ltx_align_center ltx_border_r\">22.6</td>\n<td id=\"S6.T3.1.10.10.6\" class=\"ltx_td ltx_align_center\">38.8</td>\n<td id=\"S6.T3.1.10.10.7\" class=\"ltx_td ltx_align_center\">33.4</td>\n</tr>\n<tr id=\"S6.T3.1.11.11\" class=\"ltx_tr\">\n<td id=\"S6.T3.1.11.11.1\" class=\"ltx_td ltx_border_b\"></td>\n<td id=\"S6.T3.1.11.11.2\" class=\"ltx_td ltx_align_center ltx_border_b\">P@5</td>\n<td id=\"S6.T3.1.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_b\">2.8</td>\n<td id=\"S6.T3.1.11.11.4\" class=\"ltx_td ltx_align_center ltx_border_b\">2.6</td>\n<td id=\"S6.T3.1.11.11.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">16.2</td>\n<td id=\"S6.T3.1.11.11.6\" class=\"ltx_td ltx_align_center ltx_border_b\">29.9</td>\n<td id=\"S6.T3.1.11.11.7\" class=\"ltx_td ltx_align_center ltx_border_b\">23.9</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Results.\nWe report precision@kk for k{1,3,4}134k\\in\\{1,3,4\\}\nin Table 3.\nOn all the datasets, FedAwS largely outperforms the two baseline methods of training with only positive labels. On both AmazonCatand Amazon670K, it matches or comes very close to the performance of Softmax and SLEEC. Baseline-2 gives reasonable (although quite sub-optimal) performance on AmazonCat; but does not work on Amazon670Kand WikiLSHTCwhich have larger number of classes. Thus, randomly initialized class embeddings are not ideal in the situation of many classes, and it is crucial to train the class embeddings with the rest of the model.",
            "Meta parameters. There are two meta parameters in the proposed method: the learning rate multiplier of the spreadout loss 位\\lambda (cf.Algorithm1), and the number top confusing labels considered in each round kk (cf.(8)).\nTo make a fair comparison with other methods which do not have these meta parameters, in all of our other experiments in Table 3, we simply use k=1010k=10 and 位=1010\\lambda=10."
        ]
    },
    "S6.T4": {
        "caption": "Table 4: P@1,3,5 (%) of different meta parameters on AmazonCat.",
        "table": "<table id=\"S6.T4.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S6.T4.3.3\" class=\"ltx_tr\">\n<th id=\"S6.T4.3.3.4\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_t\"></th>\n<th id=\"S6.T4.3.3.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Baseline-1</th>\n<th id=\"S6.T4.3.3.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Baseline-2</th>\n<th id=\"S6.T4.3.3.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">k = 10</th>\n<th id=\"S6.T4.3.3.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">k = 100</th>\n<th id=\"S6.T4.3.3.9\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">k = 500</th>\n<th id=\"S6.T4.3.3.10\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">k = all</th>\n<th id=\"S6.T4.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\">\n<math id=\"S6.T4.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\lambda\" display=\"inline\"><semantics id=\"S6.T4.1.1.1.m1.1a\"><mi id=\"S6.T4.1.1.1.m1.1.1\" xref=\"S6.T4.1.1.1.m1.1.1.cmml\">位</mi><annotation-xml encoding=\"MathML-Content\" id=\"S6.T4.1.1.1.m1.1b\"><ci id=\"S6.T4.1.1.1.m1.1.1.cmml\" xref=\"S6.T4.1.1.1.m1.1.1\"></ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T4.1.1.1.m1.1c\">\\lambda</annotation></semantics></math> = 1</th>\n<th id=\"S6.T4.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<math id=\"S6.T4.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\lambda\" display=\"inline\"><semantics id=\"S6.T4.2.2.2.m1.1a\"><mi id=\"S6.T4.2.2.2.m1.1.1\" xref=\"S6.T4.2.2.2.m1.1.1.cmml\">位</mi><annotation-xml encoding=\"MathML-Content\" id=\"S6.T4.2.2.2.m1.1b\"><ci id=\"S6.T4.2.2.2.m1.1.1.cmml\" xref=\"S6.T4.2.2.2.m1.1.1\"></ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T4.2.2.2.m1.1c\">\\lambda</annotation></semantics></math> = 10</th>\n<th id=\"S6.T4.3.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math id=\"S6.T4.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"\\lambda=100\" display=\"inline\"><semantics id=\"S6.T4.3.3.3.m1.1a\"><mrow id=\"S6.T4.3.3.3.m1.1.1\" xref=\"S6.T4.3.3.3.m1.1.1.cmml\"><mi id=\"S6.T4.3.3.3.m1.1.1.2\" xref=\"S6.T4.3.3.3.m1.1.1.2.cmml\">位</mi><mo id=\"S6.T4.3.3.3.m1.1.1.1\" xref=\"S6.T4.3.3.3.m1.1.1.1.cmml\">=</mo><mn id=\"S6.T4.3.3.3.m1.1.1.3\" xref=\"S6.T4.3.3.3.m1.1.1.3.cmml\">100</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S6.T4.3.3.3.m1.1b\"><apply id=\"S6.T4.3.3.3.m1.1.1.cmml\" xref=\"S6.T4.3.3.3.m1.1.1\"><eq id=\"S6.T4.3.3.3.m1.1.1.1.cmml\" xref=\"S6.T4.3.3.3.m1.1.1.1\"></eq><ci id=\"S6.T4.3.3.3.m1.1.1.2.cmml\" xref=\"S6.T4.3.3.3.m1.1.1.2\"></ci><cn type=\"integer\" id=\"S6.T4.3.3.3.m1.1.1.3.cmml\" xref=\"S6.T4.3.3.3.m1.1.1.3\">100</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T4.3.3.3.m1.1c\">\\lambda=100</annotation></semantics></math></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T4.3.4.1\" class=\"ltx_tr\">\n<th id=\"S6.T4.3.4.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">P@1</th>\n<td id=\"S6.T4.3.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">3.4</td>\n<td id=\"S6.T4.3.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">64.1</td>\n<td id=\"S6.T4.3.4.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">26.3</td>\n<td id=\"S6.T4.3.4.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">92.1</td>\n<td id=\"S6.T4.3.4.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">86.9</td>\n<td id=\"S6.T4.3.4.1.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">87.7</td>\n<th id=\"S6.T4.3.4.1.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">73.2</th>\n<td id=\"S6.T4.3.4.1.9\" class=\"ltx_td ltx_align_center ltx_border_t\">92.1</td>\n<td id=\"S6.T4.3.4.1.10\" class=\"ltx_td ltx_align_center ltx_border_t\">92.2</td>\n</tr>\n<tr id=\"S6.T4.3.5.2\" class=\"ltx_tr\">\n<th id=\"S6.T4.3.5.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">P@3</th>\n<td id=\"S6.T4.3.5.2.2\" class=\"ltx_td ltx_align_center\">3.2</td>\n<td id=\"S6.T4.3.5.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">46.8</td>\n<td id=\"S6.T4.3.5.2.4\" class=\"ltx_td ltx_align_center\">21.5</td>\n<td id=\"S6.T4.3.5.2.5\" class=\"ltx_td ltx_align_center\">70.8</td>\n<td id=\"S6.T4.3.5.2.6\" class=\"ltx_td ltx_align_center\">66.1</td>\n<td id=\"S6.T4.3.5.2.7\" class=\"ltx_td ltx_align_center ltx_border_r\">69.7</td>\n<th id=\"S6.T4.3.5.2.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">50.2</th>\n<td id=\"S6.T4.3.5.2.9\" class=\"ltx_td ltx_align_center\">70.8</td>\n<td id=\"S6.T4.3.5.2.10\" class=\"ltx_td ltx_align_center\">71.7</td>\n</tr>\n<tr id=\"S6.T4.3.6.3\" class=\"ltx_tr\">\n<th id=\"S6.T4.3.6.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\">P@5</th>\n<td id=\"S6.T4.3.6.3.2\" class=\"ltx_td ltx_align_center ltx_border_b\">3.1</td>\n<td id=\"S6.T4.3.6.3.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">32.6</td>\n<td id=\"S6.T4.3.6.3.4\" class=\"ltx_td ltx_align_center ltx_border_b\">18.2</td>\n<td id=\"S6.T4.3.6.3.5\" class=\"ltx_td ltx_align_center ltx_border_b\">58.7</td>\n<td id=\"S6.T4.3.6.3.6\" class=\"ltx_td ltx_align_center ltx_border_b\">49.3</td>\n<td id=\"S6.T4.3.6.3.7\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">52.2</td>\n<th id=\"S6.T4.3.6.3.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\">40.4</th>\n<td id=\"S6.T4.3.6.3.9\" class=\"ltx_td ltx_align_center ltx_border_b\">58.7</td>\n<td id=\"S6.T4.3.6.3.10\" class=\"ltx_td ltx_align_center ltx_border_b\">57.9</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We perform an analysis of these two parameters in Table 4 on the AmazonCatdataset. A very large kk leads to worse performance, verifying the benefit and requirement of stochastic negative mining. The reason for the bad performance for a small kk is that most of the picked labels are in fact positives in this setting (due to the inherent multi-label nature of the dataset), and over spreading the positive classes is not desirable. Regarding 位\\lambda, a relatively large value such as 10 or 100 is necessary to ensure that the class embeddings are sufficiently spreadout."
        ]
    }
}