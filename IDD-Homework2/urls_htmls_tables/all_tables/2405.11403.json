{
    "id_table_1": {
        "caption": "Features in code generation prompt techniques.",
        "table": [
            [
                "<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.1.1\">\n       <img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_landscape\" height=\"78\" id=\"S2.T1.1.1.1.g1\" src=\"/html/2405.11403/assets/x2.png\" width=\"226\">\n      </td>\n     \n"
            ]
        ],
        "footnotes": [],
        "references": [
            "Prompting LLMs:        As indicated in Section               1             , LLM prompting can be summarized into three categories: retrieval         Yasunaga et al. (           2023          ); Parvez et al. (           2023          ,           2021          )        ; planning         (Wei et al.,           2022b          ; Jiang et al.,           2023b          )        ; debugging         (Ridnik et al.,           2024          ; Chen et al.,           2023          ,           2022          ; Le et al.,           2022          )        apart from the direct code generation approaches. In contrast, we combine all these paradigms and bridge their gaps (See Table               1             ). Among others, in different contexts of generic problem-solving, Tree-of-thoughts         Yao et al. (           2023          )        , and Cumulative reasoning         Zhang et al. (           2023          )        approaches consider a tree traversal approach to explore different sub-steps towards a solution while our code generation approach mirrors the human programming cycle through various LLM agents. Notably, our traversal does not rely on sub-steps toward the solution but instead utilizes different forms of complete solutions."
        ]
    },
    "id_table_2": {
        "caption": "Pass@1 results for different approaches. The results of the yellow and blue colored cells are obtained fromand, respectively. The results of the Self-collaborationpaper are collected from their paper. The green texts indicate the state-of-the-art results, and the red text is gain over Direct Prompting approach.",
        "table": [
            [
                "<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.1\">\n        <img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_landscape\" height=\"248\" id=\"S4.T2.1.1.1.g1\" src=\"/html/2405.11403/assets/x6.png\" width=\"456\">\n       </td>\n      \n"
            ]
        ],
        "footnotes": [],
        "references": [
            "In this section, we evaluate the code generation capabilities of our framework,         MapCoder        , for competitive problem solving. Our experimental results are reported in Table               2             .Overall,         MapCoder        shows a tremendous excellence in code generation, significantly outperforms all baselines, and achieves new state-of-the-art results in all benchmarks. In general the scales with GPT-4 are higher than ChatGPT.",
            "The highest scale of performance (Pass@1) scores are observed in simple program synthesis tasks like HumanEval, MBPP in Table                  2                .Though with the simpler problem (non-contests) datasets such as HumanEval, HumanEval-ET, the current state-of-the-art method, Reflexion           Shinn et al. (             2023            )          perform reasonably well, this approach does not generalize across varying datasets depicting a wide variety of problems.Self-reflection techniques enhance GPT-4’s performance on HumanEval but result in a 3% decrease on the MBPP dataset. Similarly, with ChatGPT, there’s a notable 26.3% drop in performance where in several cases their AI generated test cases are incorrect. We observe that 8% of failures in HumanEval and 15% in MBPP is caused by their AI generates incorrect test cases while our approach is independent of AI test cases, and consistently improves code generations in general. Consequently, even in HumanEval, with GPT-4, our Pass@1 surpasses Reflexion by                          ∼                               similar-to                              \\sim                       3%. On top, in all four simple programming datasets,           MapCoder          enhances the Direct prompting significantly with a maximum of 88% on HumanEvalET by ChatGPT."
        ]
    },
    "id_table_3": {
        "caption": "Pass@5 results on CodeContest dataset. AlphCodium result are from. The green cells indicate the SoTA and the red text indicates improvement w.r.t Direct approach.",
        "table": [
            [
                "<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.1.1\">\n        <img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_landscape\" height=\"70\" id=\"S5.T3.1.1.1.g1\" src=\"/html/2405.11403/assets/x7.png\" width=\"221\">\n       </td>\n      \n"
            ]
        ],
        "footnotes": [],
        "references": [
            "The significance of           MapCoder          shines through clearly when evaluated in competitive problem-solving contexts. Across datasets such as APPS, xCodeEval, and CodeContests,           MapCoder          demonstrates substantial enhancements over Direct prompting methods, with improvements of 41.3%, 52.6%, and 132.8% for ChatGPT, and 73.7%, 41.2%, and 135.1% for GPT4, respectively. Notably, the most challenging datasets are APPS and CodeContest, where           MapCoder          ’s performance stands out prominently. We deliberately compare against strong baselines on these datasets, regardless of whether they are prompt-based or not.Importantly, on CodeContest our Pass@1 results match the Pass@5 scores of the concurrent state-of-the-art model AlphaCodium           (Ridnik et al.,             2024            )          : 28.5% vs. their 29% (see Table                  3                ). Furthermore, our Pass@5 results demonstrate an additional improvement of 12.8%. On APPS,           MapCoder          consistently surpasses the Pass@1 scores of all baseline prompts for both ChatGPT and GPT-4."
        ]
    },
    "id_table_4": {
        "caption": "Pass@1 results with using Gemini Pro. The red text is gain over Direct Prompting approach.",
        "table": [
            [
                "<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.1.1\">\n        <img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_landscape\" height=\"52\" id=\"S5.T4.1.1.1.g1\" src=\"/html/2405.11403/assets/x8.png\" width=\"207\">\n       </td>\n      \n"
            ]
        ],
        "footnotes": [],
        "references": [
            "To show the robustness of           MapCoder          across various LLMs, we evaluate           MapCoder          using Gemini Pro, a different family of SoTA LLM in Table                  4                . We also evaluate           MapCoder          using an open-source LLM Mistral-7B instruct in Table                  5                . As expected, our method shows performance gains over other baseline approaches in equitable trends on both simple (HumanEval) and contest-level problems (CodeContest)."
        ]
    },
    "id_table_5": {
        "caption": "Pass@1 results with using Mistral-7B-instruct. The red text is gain over Direct Prompting approach.",
        "table": [
            [
                "<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.1\">\n        <img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_landscape\" height=\"52\" id=\"S5.T5.1.1.1.g1\" src=\"/html/2405.11403/assets/x9.png\" width=\"207\">\n       </td>\n      \n"
            ]
        ],
        "footnotes": [],
        "references": [
            "To show the robustness of           MapCoder          across various LLMs, we evaluate           MapCoder          using Gemini Pro, a different family of SoTA LLM in Table                  4                . We also evaluate           MapCoder          using an open-source LLM Mistral-7B instruct in Table                  5                . As expected, our method shows performance gains over other baseline approaches in equitable trends on both simple (HumanEval) and contest-level problems (CodeContest)."
        ]
    },
    "id_table_6": {
        "caption": "Pass@1 results for different versions of(by using ChatGPT on HumanEval dataset).",
        "table": [
            [
                "<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.1.1.1\">\n          <img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_landscape\" height=\"91\" id=\"S6.T6.1.1.1.1.g1\" src=\"/html/2405.11403/assets/x10.png\" width=\"221\">\n         </td>\n        \n"
            ]
        ],
        "footnotes": [],
        "references": [
            "We have also conducted a study by excluding certain agents from our           MapCoder          , which helps us investigate each agent’s impact in our whole pipeline. As expected, the results (Table                  6                ) show that every agent has its role in the pipeline as turning off any agent decreases the performance of           MapCoder          . Furthermore, we observe that the Debugging Agent has the most significant impact on the pipeline, as evidenced by a performance drop of 17.5% when excluding this agent exclusively, and an avg performance drop of 24.83% in all cases. The           Planning agent          has the second best important with avg drop of 16.7% in all cases.In Table                  6                ), we perform an ablation study of our multi-agent framework investigate each agent’s impact in our whole pipeline."
        ]
    },
    "id_table_7": {
        "caption": "Pass@1 results by varyingand.",
        "table": [
            [
                "<td class=\"ltx_td ltx_align_center\" id=\"S6.T7.1.1.1.1\">\n          <img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_landscape\" height=\"58\" id=\"S6.T7.1.1.1.1.g1\" src=\"/html/2405.11403/assets/x11.png\" width=\"221\">\n         </td>\n        \n"
            ]
        ],
        "footnotes": [],
        "references": [
            "MapCoder          involves two hyper-parameters: the number of self-retrieved exemplars,                          k                               𝑘                              k                       , and the number of debugging attempts,                          t                               𝑡                              t                       . Our findings (Table                  7                ) reveal that higher                          k                               𝑘                              k                       ,                          t                               𝑡                              t                       is proportionate performance gain at the expense of time.",
            "Among the limitations of our work, firstly,         MapCoder        generates a large number of tokens, which may pose challenges in resource-constrained environments. Table               8             shows the number of average API calls and token consumption with the default                      k                           𝑘                          k                   and                      t                           𝑡                          t                   (i.e., with respect to the reported performance) while Table               7             ) shows how                      k                           𝑘                          k                   ,                      t                           𝑡                          t                   can be adjusted to proportionate the performance gain at the expense of time/token. We have not addressed the problem of minimizing tokens/API-calls in this paper and leave it for future works. Secondly, our method currently relies on sample input-output (I/O) pairs for bug fixing. Although sample I/Os provide valuable insights for LLMs’ code generation, their limited number may not always capture the full spectrum of possible test cases. Consequently, enhancing the quality of additional test case generation could reduce our reliance on sample I/Os and further improve the robustness of our approach. Additionally, future exploration of open-source code generation models, such as CodeLLaMa, LLaMa3, Mixtral 8x7B could offer valuable insights and potential enhancements to our approach. Another important concern is that while running machine-generated code, it is advisable to run it inside a sandbox to avoid any potential risks."
        ]
    },
    "id_table_8": {
        "caption": "Average number of API calls, thousands of tokens used, required time in minutes to get the API response.",
        "table": [
            [
                "<td class=\"ltx_td ltx_align_center\" id=\"S6.T8.1.1.1\">\n        <img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_landscape\" height=\"169\" id=\"S6.T8.1.1.1.g1\" src=\"/html/2405.11403/assets/x12.png\" width=\"368\">\n       </td>\n      \n"
            ]
        ],
        "footnotes": [],
        "references": [
            "Among the limitations of our work, firstly,         MapCoder        generates a large number of tokens, which may pose challenges in resource-constrained environments. Table               8             shows the number of average API calls and token consumption with the default                      k                           𝑘                          k                   and                      t                           𝑡                          t                   (i.e., with respect to the reported performance) while Table               7             ) shows how                      k                           𝑘                          k                   ,                      t                           𝑡                          t                   can be adjusted to proportionate the performance gain at the expense of time/token. We have not addressed the problem of minimizing tokens/API-calls in this paper and leave it for future works. Secondly, our method currently relies on sample input-output (I/O) pairs for bug fixing. Although sample I/Os provide valuable insights for LLMs’ code generation, their limited number may not always capture the full spectrum of possible test cases. Consequently, enhancing the quality of additional test case generation could reduce our reliance on sample I/Os and further improve the robustness of our approach. Additionally, future exploration of open-source code generation models, such as CodeLLaMa, LLaMa3, Mixtral 8x7B could offer valuable insights and potential enhancements to our approach. Another important concern is that while running machine-generated code, it is advisable to run it inside a sandbox to avoid any potential risks."
        ]
    }
}