{
    "PAPER'S NUMBER OF TABLES": 25,
    "S4.T1": {
        "caption": "Table 1: Main result 1 (single-domain data distribution): Each client has one source domain in its local data. The proposed StableFDG achieves the best generalization, underscoring its effectiveness.",
        "table": "<table id=\"S4.T1.st1.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.st1.2.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.st1.2.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_border_rr ltx_border_tt\"></th>\n<th id=\"S4.T1.st1.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"5\"><span id=\"S4.T1.st1.2.1.1.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">PACS</span></th>\n<th id=\"S4.T1.st1.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"5\"><span id=\"S4.T1.st1.2.1.1.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">VLCS</span></th>\n</tr>\n<tr id=\"S4.T1.st1.2.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T1.st1.2.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr\"><span id=\"S4.T1.st1.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Methods</span></th>\n<th id=\"S4.T1.st1.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.st1.2.2.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">Art</span></th>\n<th id=\"S4.T1.st1.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.st1.2.2.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">Cartoon</span></th>\n<th id=\"S4.T1.st1.2.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.st1.2.2.2.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">Photo</span></th>\n<th id=\"S4.T1.st1.2.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T1.st1.2.2.2.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">Sketch</span></th>\n<th id=\"S4.T1.st1.2.2.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t\"><span id=\"S4.T1.st1.2.2.2.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">Avg.</span></th>\n<th id=\"S4.T1.st1.2.2.2.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.st1.2.2.2.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">Caltech</span></th>\n<th id=\"S4.T1.st1.2.2.2.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.st1.2.2.2.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">LabelMe</span></th>\n<th id=\"S4.T1.st1.2.2.2.9\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.st1.2.2.2.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">Pascal</span></th>\n<th id=\"S4.T1.st1.2.2.2.10\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T1.st1.2.2.2.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">Sun</span></th>\n<th id=\"S4.T1.st1.2.2.2.11\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.st1.2.2.2.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">Avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.st1.2.3.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.st1.2.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">\n<span id=\"S4.T1.st1.2.3.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedAvg </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T1.st1.2.3.1.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">26</a><span id=\"S4.T1.st1.2.3.1.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T1.st1.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.st1.2.3.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.67</span></td>\n<td id=\"S4.T1.st1.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.st1.2.3.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">70.87</span></td>\n<td id=\"S4.T1.st1.2.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.st1.2.3.1.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">90.27</span></td>\n<td id=\"S4.T1.st1.2.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T1.st1.2.3.1.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">55.70</span></td>\n<td id=\"S4.T1.st1.2.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\"><span id=\"S4.T1.st1.2.3.1.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.63</span></td>\n<td id=\"S4.T1.st1.2.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.st1.2.3.1.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">93.75</span></td>\n<td id=\"S4.T1.st1.2.3.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.st1.2.3.1.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">59.30</span></td>\n<td id=\"S4.T1.st1.2.3.1.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.st1.2.3.1.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">70.05</span></td>\n<td id=\"S4.T1.st1.2.3.1.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T1.st1.2.3.1.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">69.90</span></td>\n<td id=\"S4.T1.st1.2.3.1.11\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.st1.2.3.1.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.25</span></td>\n</tr>\n<tr id=\"S4.T1.st1.2.4.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.st1.2.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T1.st1.2.4.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedBN </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T1.st1.2.4.2.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">22</a><span id=\"S4.T1.st1.2.4.2.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T1.st1.2.4.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.4.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">78.42</span></td>\n<td id=\"S4.T1.st1.2.4.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.4.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">70.9</span></td>\n<td id=\"S4.T1.st1.2.4.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.4.2.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">90.96</span></td>\n<td id=\"S4.T1.st1.2.4.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st1.2.4.2.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">54.07</span></td>\n<td id=\"S4.T1.st1.2.4.2.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T1.st1.2.4.2.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.59</span></td>\n<td id=\"S4.T1.st1.2.4.2.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.4.2.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">94.81</span></td>\n<td id=\"S4.T1.st1.2.4.2.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.4.2.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">58.59</span></td>\n<td id=\"S4.T1.st1.2.4.2.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.4.2.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.06</span></td>\n<td id=\"S4.T1.st1.2.4.2.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st1.2.4.2.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">70.36</span></td>\n<td id=\"S4.T1.st1.2.4.2.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.4.2.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.96</span></td>\n</tr>\n<tr id=\"S4.T1.st1.2.5.3\" class=\"ltx_tr\">\n<td id=\"S4.T1.st1.2.5.3.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T1.st1.2.5.3.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">MixStyle </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T1.st1.2.5.3.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">39</a><span id=\"S4.T1.st1.2.5.3.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T1.st1.2.5.3.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.5.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">79.10</span></td>\n<td id=\"S4.T1.st1.2.5.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.5.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">76.30</span></td>\n<td id=\"S4.T1.st1.2.5.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.5.3.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">90.10</span></td>\n<td id=\"S4.T1.st1.2.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st1.2.5.3.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">60.63</span></td>\n<td id=\"S4.T1.st1.2.5.3.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T1.st1.2.5.3.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">76.53</span></td>\n<td id=\"S4.T1.st1.2.5.3.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.5.3.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">95.20</span></td>\n<td id=\"S4.T1.st1.2.5.3.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.5.3.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">60.40</span></td>\n<td id=\"S4.T1.st1.2.5.3.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.5.3.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.10</span></td>\n<td id=\"S4.T1.st1.2.5.3.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st1.2.5.3.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">69.93</span></td>\n<td id=\"S4.T1.st1.2.5.3.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.5.3.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.41</span></td>\n</tr>\n<tr id=\"S4.T1.st1.2.6.4\" class=\"ltx_tr\">\n<td id=\"S4.T1.st1.2.6.4.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T1.st1.2.6.4.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">DSU </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T1.st1.2.6.4.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">21</a><span id=\"S4.T1.st1.2.6.4.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T1.st1.2.6.4.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.6.4.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">80.43</span></td>\n<td id=\"S4.T1.st1.2.6.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.6.4.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">75.70</span></td>\n<td id=\"S4.T1.st1.2.6.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.6.4.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">92.60</span></td>\n<td id=\"S4.T1.st1.2.6.4.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st1.2.6.4.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">69.87</span></td>\n<td id=\"S4.T1.st1.2.6.4.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T1.st1.2.6.4.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">79.65</span></td>\n<td id=\"S4.T1.st1.2.6.4.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.6.4.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">96.13</span></td>\n<td id=\"S4.T1.st1.2.6.4.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.6.4.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">58.77</span></td>\n<td id=\"S4.T1.st1.2.6.4.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.6.4.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">71.80</span></td>\n<td id=\"S4.T1.st1.2.6.4.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st1.2.6.4.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">71.87</span></td>\n<td id=\"S4.T1.st1.2.6.4.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.6.4.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.64</span></td>\n</tr>\n<tr id=\"S4.T1.st1.2.7.5\" class=\"ltx_tr\">\n<td id=\"S4.T1.st1.2.7.5.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T1.st1.2.7.5.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">CCST </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T1.st1.2.7.5.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">2</a><span id=\"S4.T1.st1.2.7.5.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T1.st1.2.7.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.7.5.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">71.35</span></td>\n<td id=\"S4.T1.st1.2.7.5.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.7.5.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.40</span></td>\n<td id=\"S4.T1.st1.2.7.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.7.5.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">88.65</span></td>\n<td id=\"S4.T1.st1.2.7.5.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st1.2.7.5.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">64.10</span></td>\n<td id=\"S4.T1.st1.2.7.5.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T1.st1.2.7.5.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.13</span></td>\n<td id=\"S4.T1.st1.2.7.5.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.7.5.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">92.50</span></td>\n<td id=\"S4.T1.st1.2.7.5.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.7.5.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">61.20</span></td>\n<td id=\"S4.T1.st1.2.7.5.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.7.5.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">68.20</span></td>\n<td id=\"S4.T1.st1.2.7.5.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st1.2.7.5.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">66.50</span></td>\n<td id=\"S4.T1.st1.2.7.5.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.7.5.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.10</span></td>\n</tr>\n<tr id=\"S4.T1.st1.2.8.6\" class=\"ltx_tr\">\n<td id=\"S4.T1.st1.2.8.6.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T1.st1.2.8.6.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedDG </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T1.st1.2.8.6.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">25</a><span id=\"S4.T1.st1.2.8.6.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T1.st1.2.8.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.8.6.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">71.20</span></td>\n<td id=\"S4.T1.st1.2.8.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.8.6.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">71.40</span></td>\n<td id=\"S4.T1.st1.2.8.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.8.6.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">90.70</span></td>\n<td id=\"S4.T1.st1.2.8.6.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st1.2.8.6.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">59.20</span></td>\n<td id=\"S4.T1.st1.2.8.6.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T1.st1.2.8.6.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.13</span></td>\n<td id=\"S4.T1.st1.2.8.6.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.8.6.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">95.3</span></td>\n<td id=\"S4.T1.st1.2.8.6.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.8.6.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">57.5</span></td>\n<td id=\"S4.T1.st1.2.8.6.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.8.6.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.8</span></td>\n<td id=\"S4.T1.st1.2.8.6.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st1.2.8.6.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">69.8</span></td>\n<td id=\"S4.T1.st1.2.8.6.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.8.6.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.85</span></td>\n</tr>\n<tr id=\"S4.T1.st1.2.9.7\" class=\"ltx_tr\">\n<td id=\"S4.T1.st1.2.9.7.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T1.st1.2.9.7.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedSR </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T1.st1.2.9.7.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">28</a><span id=\"S4.T1.st1.2.9.7.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T1.st1.2.9.7.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.9.7.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">76.40</span></td>\n<td id=\"S4.T1.st1.2.9.7.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.9.7.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">71.25</span></td>\n<td id=\"S4.T1.st1.2.9.7.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.9.7.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">93.25</span></td>\n<td id=\"S4.T1.st1.2.9.7.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st1.2.9.7.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">60.55</span></td>\n<td id=\"S4.T1.st1.2.9.7.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T1.st1.2.9.7.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">75.36</span></td>\n<td id=\"S4.T1.st1.2.9.7.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.9.7.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">92.10</span></td>\n<td id=\"S4.T1.st1.2.9.7.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.9.7.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">60.50</span></td>\n<td id=\"S4.T1.st1.2.9.7.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.9.7.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">70.75</span></td>\n<td id=\"S4.T1.st1.2.9.7.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st1.2.9.7.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">71.65</span></td>\n<td id=\"S4.T1.st1.2.9.7.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.9.7.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.75</span></td>\n</tr>\n<tr id=\"S4.T1.st1.2.10.8\" class=\"ltx_tr\">\n<td id=\"S4.T1.st1.2.10.8.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_rr\">\n<span id=\"S4.T1.st1.2.10.8.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">StableFDG</span><span id=\"S4.T1.st1.2.10.8.1.2\" class=\"ltx_text\" style=\"font-size:70%;\"> (ours)</span>\n</td>\n<td id=\"S4.T1.st1.2.10.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.st1.2.10.8.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">84.10</span></td>\n<td id=\"S4.T1.st1.2.10.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.st1.2.10.8.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">78.57</span></td>\n<td id=\"S4.T1.st1.2.10.8.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.st1.2.10.8.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">95.40</span></td>\n<td id=\"S4.T1.st1.2.10.8.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S4.T1.st1.2.10.8.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.73</span></td>\n<td id=\"S4.T1.st1.2.10.8.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_rr\"><span id=\"S4.T1.st1.2.10.8.6.1\" class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">82.70</span></td>\n<td id=\"S4.T1.st1.2.10.8.7\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.st1.2.10.8.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">98.13</span></td>\n<td id=\"S4.T1.st1.2.10.8.8\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.st1.2.10.8.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">59.20</span></td>\n<td id=\"S4.T1.st1.2.10.8.9\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.st1.2.10.8.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.60</span></td>\n<td id=\"S4.T1.st1.2.10.8.10\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S4.T1.st1.2.10.8.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">70.27</span></td>\n<td id=\"S4.T1.st1.2.10.8.11\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.st1.2.10.8.11.1\" class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">75.30</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Single-domain data distribution. Table 1 shows our results in a single-domain data distribution setup. We have the following observations. Compared to the previous results provided in the centralized DG works [39, 21], the performance of each method is generally lower. This is due to the limited numbers of styles and data samples in each FL client, which restricts the generalization performance of individual client models. It can be seen that most of the baselines perform better than FedAvg and FedBN that do not tackle the DG problem.\nThe proposed StableFDG achieves the best average accuracy for all benchmark datasets, where the gain is especially large in PACS having large shifts between domains. In contrast to our scheme, the prior works [2, 25, 28] targeting federated DG show marginal performance gains relative to FedAvg in our practical experimental setup with (i) more clients (which results in less data in each client) and (ii) partial client participations.",
            "Multi-domain data distribution. In Table 2, we report the results in a multi-domain data distribution scenario. Compared to the results in Table 1, most of the schemes achieve improved performance in Table 2. This is because each client has multiple source domains, and thus providing a better platform for each client model to gain generalization ability. The proposed StableFDG still performs the best, demonstrating the effectiveness of our style and attention based learning strategy for federated DG.",
            "We also compare the effect of number of styles received at each client in Table 10 using PACS dataset in a multi-domain data distribution setup. The performance increases as the number of received styles increases, with small additional communication load (the vector length of the style information is 128, which is negligible compared to the number of model parameters, which is 11,180,103).",
            "In Section 3.1, we utilized the k𝑘k-means++ as a tool for facilitating our key idea in style shifting, which is to effectively balance between the original source domain and the new source domain for better generalization; k-means++ plays a role to select the B/2𝐵2B/2 styles that are similar to the remaining B/2𝐵2B/2 styles in the mini-batch. By doing so, the model can explore new styles while not losing the performance on the original styles. To see this effect, we compare k𝑘k-means++ vs. random sampling when selecting B/2 samples to be shifted, in Table 11. The results show that strategically selecting the B/2 samples to be shifted achieves better performance especially in the Sketch domain (1.89% gain) that has a large style gap with other domains. We believe that these results motivate and justify our design choice.",
            "In our main paper, we performed the class-balanced oversampling in the feature space to alleviate the class-imbalance issue during style exploration. To confirm the effectiveness of the class-balanced oversampling, we compare it with random oversampling under the same condition where only the style exploration is applied without other components. Table 12 shows the results on Office-Home dataset, where the class distribution is highly imbalanced in a multi-domain data distribution scenario. It can be seen that our class-balanced oversampling achieves higher performance over the simple random sampling, which validates the efficacy of mitigating the class imbalance problem in FL clients.",
            "We conduct additional ablation studies on the probability value (defined as p𝑝p here) utilized to control the operation of the style sharing/shifting and style exploration modules. Larger p𝑝p means that our scheme is more likely to be activated. In Table 13, we provide results on various probability values in a single-domain data distribution scenario using PACS dataset. From the results, it is confirm that for all p𝑝p values, the proposed StableFDG outperforms existing baselines, demonstrating that our scheme can work well with an arbitrarily chosen probability p𝑝p. In detail, when p𝑝p ranges from 0.3 to 0.7, the high performance is maintained while the performance decreases at both extreme probabilities (p=0.1𝑝0.1p=0.1 and 0.9). Therefore, it is recommended for practitioners to select the p𝑝p in an appropriate range, avoiding extreme cases.",
            "In our main paper, the similarity metric in equation (6) adopts cross-attention, while the metric in equation (8) combines cross-attention and self-attention. When applying only the cross-attention-based metric in equation (6), we found that the similarity value could become low even when the two samples belong to the same class, in special cases. We handled this issue by adding the self-attention component as in equation (8). Intuitively, by doing this, the attention module is learning to extract the important features across images (via cross-attention), and within the image (via self-attention). Table 15 compares the performance of our StableFDG when using (i) self-attention alone, (ii) cross-attention alone (equation (6)), and (iii) both self and cross attentions at the same time (equation (8)), confirming the advantage of using self-attention and cross-attention together.",
            "Our attention module requires 0.44% of additional model parameters to perform the attention-based learning. For a fair comparison to see the effect of our attention-based learning, we consider a different baseline with the same model size but without attention-based learning. Specifically, the baseline computes the attention score map using only additional convolutional operations and take the weighted average of the feature zisubscript𝑧𝑖z_{i} based on the attention score map. Table 16 shows the results using PACS dataset in a single-domain data distribution scenario. The results demonstrate that our attention-based learning achieves performance improvements on all four domains while playing a key role in capturing essential parts of the features.",
            "Now we provide answer to the following question: Instead of the FL setup we focused on, can attention provide benefits in the centralized DG setup? Table 17 shows the results with/without attention module in a centralized setup using PACS dataset. The results show that attention still provides performance improvements in the centralized setup by learning domain-invariant features, although the gain is slightly lower than the gain in the FL setup as shown in Table 3 of the main manuscript. These results indicate that the proposed attention-based learning indeed captures the domain-invariant characteristics of samples, while the scheme provides more benefits in the FL setup where each client is prone to overfitting due to lack of data."
        ]
    },
    "S4.T1.st1": {
        "caption": "(a) PACS and VLCS datasets.",
        "table": "<table id=\"S4.T1.st1.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.st1.2.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.st1.2.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_border_rr ltx_border_tt\"></th>\n<th id=\"S4.T1.st1.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"5\"><span id=\"S4.T1.st1.2.1.1.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">PACS</span></th>\n<th id=\"S4.T1.st1.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"5\"><span id=\"S4.T1.st1.2.1.1.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">VLCS</span></th>\n</tr>\n<tr id=\"S4.T1.st1.2.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T1.st1.2.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr\"><span id=\"S4.T1.st1.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Methods</span></th>\n<th id=\"S4.T1.st1.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.st1.2.2.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">Art</span></th>\n<th id=\"S4.T1.st1.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.st1.2.2.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">Cartoon</span></th>\n<th id=\"S4.T1.st1.2.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.st1.2.2.2.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">Photo</span></th>\n<th id=\"S4.T1.st1.2.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T1.st1.2.2.2.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">Sketch</span></th>\n<th id=\"S4.T1.st1.2.2.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t\"><span id=\"S4.T1.st1.2.2.2.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">Avg.</span></th>\n<th id=\"S4.T1.st1.2.2.2.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.st1.2.2.2.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">Caltech</span></th>\n<th id=\"S4.T1.st1.2.2.2.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.st1.2.2.2.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">LabelMe</span></th>\n<th id=\"S4.T1.st1.2.2.2.9\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.st1.2.2.2.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">Pascal</span></th>\n<th id=\"S4.T1.st1.2.2.2.10\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T1.st1.2.2.2.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">Sun</span></th>\n<th id=\"S4.T1.st1.2.2.2.11\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.st1.2.2.2.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">Avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.st1.2.3.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.st1.2.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">\n<span id=\"S4.T1.st1.2.3.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedAvg </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T1.st1.2.3.1.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">26</a><span id=\"S4.T1.st1.2.3.1.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T1.st1.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.st1.2.3.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.67</span></td>\n<td id=\"S4.T1.st1.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.st1.2.3.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">70.87</span></td>\n<td id=\"S4.T1.st1.2.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.st1.2.3.1.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">90.27</span></td>\n<td id=\"S4.T1.st1.2.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T1.st1.2.3.1.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">55.70</span></td>\n<td id=\"S4.T1.st1.2.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\"><span id=\"S4.T1.st1.2.3.1.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.63</span></td>\n<td id=\"S4.T1.st1.2.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.st1.2.3.1.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">93.75</span></td>\n<td id=\"S4.T1.st1.2.3.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.st1.2.3.1.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">59.30</span></td>\n<td id=\"S4.T1.st1.2.3.1.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.st1.2.3.1.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">70.05</span></td>\n<td id=\"S4.T1.st1.2.3.1.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T1.st1.2.3.1.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">69.90</span></td>\n<td id=\"S4.T1.st1.2.3.1.11\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.st1.2.3.1.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.25</span></td>\n</tr>\n<tr id=\"S4.T1.st1.2.4.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.st1.2.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T1.st1.2.4.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedBN </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T1.st1.2.4.2.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">22</a><span id=\"S4.T1.st1.2.4.2.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T1.st1.2.4.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.4.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">78.42</span></td>\n<td id=\"S4.T1.st1.2.4.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.4.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">70.9</span></td>\n<td id=\"S4.T1.st1.2.4.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.4.2.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">90.96</span></td>\n<td id=\"S4.T1.st1.2.4.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st1.2.4.2.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">54.07</span></td>\n<td id=\"S4.T1.st1.2.4.2.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T1.st1.2.4.2.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.59</span></td>\n<td id=\"S4.T1.st1.2.4.2.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.4.2.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">94.81</span></td>\n<td id=\"S4.T1.st1.2.4.2.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.4.2.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">58.59</span></td>\n<td id=\"S4.T1.st1.2.4.2.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.4.2.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.06</span></td>\n<td id=\"S4.T1.st1.2.4.2.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st1.2.4.2.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">70.36</span></td>\n<td id=\"S4.T1.st1.2.4.2.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.4.2.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.96</span></td>\n</tr>\n<tr id=\"S4.T1.st1.2.5.3\" class=\"ltx_tr\">\n<td id=\"S4.T1.st1.2.5.3.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T1.st1.2.5.3.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">MixStyle </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T1.st1.2.5.3.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">39</a><span id=\"S4.T1.st1.2.5.3.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T1.st1.2.5.3.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.5.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">79.10</span></td>\n<td id=\"S4.T1.st1.2.5.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.5.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">76.30</span></td>\n<td id=\"S4.T1.st1.2.5.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.5.3.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">90.10</span></td>\n<td id=\"S4.T1.st1.2.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st1.2.5.3.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">60.63</span></td>\n<td id=\"S4.T1.st1.2.5.3.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T1.st1.2.5.3.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">76.53</span></td>\n<td id=\"S4.T1.st1.2.5.3.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.5.3.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">95.20</span></td>\n<td id=\"S4.T1.st1.2.5.3.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.5.3.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">60.40</span></td>\n<td id=\"S4.T1.st1.2.5.3.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.5.3.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.10</span></td>\n<td id=\"S4.T1.st1.2.5.3.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st1.2.5.3.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">69.93</span></td>\n<td id=\"S4.T1.st1.2.5.3.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.5.3.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.41</span></td>\n</tr>\n<tr id=\"S4.T1.st1.2.6.4\" class=\"ltx_tr\">\n<td id=\"S4.T1.st1.2.6.4.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T1.st1.2.6.4.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">DSU </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T1.st1.2.6.4.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">21</a><span id=\"S4.T1.st1.2.6.4.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T1.st1.2.6.4.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.6.4.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">80.43</span></td>\n<td id=\"S4.T1.st1.2.6.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.6.4.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">75.70</span></td>\n<td id=\"S4.T1.st1.2.6.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.6.4.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">92.60</span></td>\n<td id=\"S4.T1.st1.2.6.4.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st1.2.6.4.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">69.87</span></td>\n<td id=\"S4.T1.st1.2.6.4.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T1.st1.2.6.4.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">79.65</span></td>\n<td id=\"S4.T1.st1.2.6.4.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.6.4.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">96.13</span></td>\n<td id=\"S4.T1.st1.2.6.4.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.6.4.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">58.77</span></td>\n<td id=\"S4.T1.st1.2.6.4.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.6.4.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">71.80</span></td>\n<td id=\"S4.T1.st1.2.6.4.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st1.2.6.4.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">71.87</span></td>\n<td id=\"S4.T1.st1.2.6.4.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.6.4.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.64</span></td>\n</tr>\n<tr id=\"S4.T1.st1.2.7.5\" class=\"ltx_tr\">\n<td id=\"S4.T1.st1.2.7.5.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T1.st1.2.7.5.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">CCST </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T1.st1.2.7.5.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">2</a><span id=\"S4.T1.st1.2.7.5.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T1.st1.2.7.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.7.5.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">71.35</span></td>\n<td id=\"S4.T1.st1.2.7.5.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.7.5.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.40</span></td>\n<td id=\"S4.T1.st1.2.7.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.7.5.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">88.65</span></td>\n<td id=\"S4.T1.st1.2.7.5.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st1.2.7.5.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">64.10</span></td>\n<td id=\"S4.T1.st1.2.7.5.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T1.st1.2.7.5.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.13</span></td>\n<td id=\"S4.T1.st1.2.7.5.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.7.5.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">92.50</span></td>\n<td id=\"S4.T1.st1.2.7.5.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.7.5.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">61.20</span></td>\n<td id=\"S4.T1.st1.2.7.5.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.7.5.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">68.20</span></td>\n<td id=\"S4.T1.st1.2.7.5.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st1.2.7.5.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">66.50</span></td>\n<td id=\"S4.T1.st1.2.7.5.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.7.5.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.10</span></td>\n</tr>\n<tr id=\"S4.T1.st1.2.8.6\" class=\"ltx_tr\">\n<td id=\"S4.T1.st1.2.8.6.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T1.st1.2.8.6.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedDG </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T1.st1.2.8.6.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">25</a><span id=\"S4.T1.st1.2.8.6.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T1.st1.2.8.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.8.6.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">71.20</span></td>\n<td id=\"S4.T1.st1.2.8.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.8.6.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">71.40</span></td>\n<td id=\"S4.T1.st1.2.8.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.8.6.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">90.70</span></td>\n<td id=\"S4.T1.st1.2.8.6.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st1.2.8.6.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">59.20</span></td>\n<td id=\"S4.T1.st1.2.8.6.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T1.st1.2.8.6.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.13</span></td>\n<td id=\"S4.T1.st1.2.8.6.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.8.6.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">95.3</span></td>\n<td id=\"S4.T1.st1.2.8.6.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.8.6.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">57.5</span></td>\n<td id=\"S4.T1.st1.2.8.6.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.8.6.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.8</span></td>\n<td id=\"S4.T1.st1.2.8.6.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st1.2.8.6.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">69.8</span></td>\n<td id=\"S4.T1.st1.2.8.6.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.8.6.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.85</span></td>\n</tr>\n<tr id=\"S4.T1.st1.2.9.7\" class=\"ltx_tr\">\n<td id=\"S4.T1.st1.2.9.7.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T1.st1.2.9.7.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedSR </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T1.st1.2.9.7.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">28</a><span id=\"S4.T1.st1.2.9.7.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T1.st1.2.9.7.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.9.7.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">76.40</span></td>\n<td id=\"S4.T1.st1.2.9.7.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.9.7.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">71.25</span></td>\n<td id=\"S4.T1.st1.2.9.7.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.9.7.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">93.25</span></td>\n<td id=\"S4.T1.st1.2.9.7.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st1.2.9.7.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">60.55</span></td>\n<td id=\"S4.T1.st1.2.9.7.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T1.st1.2.9.7.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">75.36</span></td>\n<td id=\"S4.T1.st1.2.9.7.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.9.7.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">92.10</span></td>\n<td id=\"S4.T1.st1.2.9.7.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.9.7.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">60.50</span></td>\n<td id=\"S4.T1.st1.2.9.7.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.9.7.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">70.75</span></td>\n<td id=\"S4.T1.st1.2.9.7.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st1.2.9.7.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">71.65</span></td>\n<td id=\"S4.T1.st1.2.9.7.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st1.2.9.7.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.75</span></td>\n</tr>\n<tr id=\"S4.T1.st1.2.10.8\" class=\"ltx_tr\">\n<td id=\"S4.T1.st1.2.10.8.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_rr\">\n<span id=\"S4.T1.st1.2.10.8.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">StableFDG</span><span id=\"S4.T1.st1.2.10.8.1.2\" class=\"ltx_text\" style=\"font-size:70%;\"> (ours)</span>\n</td>\n<td id=\"S4.T1.st1.2.10.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.st1.2.10.8.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">84.10</span></td>\n<td id=\"S4.T1.st1.2.10.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.st1.2.10.8.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">78.57</span></td>\n<td id=\"S4.T1.st1.2.10.8.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.st1.2.10.8.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">95.40</span></td>\n<td id=\"S4.T1.st1.2.10.8.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S4.T1.st1.2.10.8.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.73</span></td>\n<td id=\"S4.T1.st1.2.10.8.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_rr\"><span id=\"S4.T1.st1.2.10.8.6.1\" class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">82.70</span></td>\n<td id=\"S4.T1.st1.2.10.8.7\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.st1.2.10.8.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">98.13</span></td>\n<td id=\"S4.T1.st1.2.10.8.8\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.st1.2.10.8.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">59.20</span></td>\n<td id=\"S4.T1.st1.2.10.8.9\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.st1.2.10.8.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.60</span></td>\n<td id=\"S4.T1.st1.2.10.8.10\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S4.T1.st1.2.10.8.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">70.27</span></td>\n<td id=\"S4.T1.st1.2.10.8.11\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.st1.2.10.8.11.1\" class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">75.30</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Traditional federated learning (FL) algorithms operate under the assumption that the data distributions at training (source domains) and testing (target domain) are the same. The fact that domain shifts often occur in practice necessitates equipping FL methods with a domain generalization (DG) capability. However, existing DG algorithms face fundamental challenges in FL setups due to the lack of samples/domains in each client’s local dataset. In this paper, we propose StableFDG, a style and attention based learning strategy for accomplishing federated domain generalization, introducing two key contributions. The first is style-based learning, which enables each client to explore novel styles beyond the original source domains in its local dataset, improving domain diversity based on the proposed style sharing, shifting, and exploration strategies. Our second contribution is an attention-based feature highlighter, which captures the similarities between the features of data samples in the same class, and emphasizes the important/common characteristics to better learn the domain-invariant characteristics of each class in data-poor FL scenarios. Experimental results show that StableFDG outperforms existing baselines on various DG benchmark datasets, demonstrating its efficacy.",
            "Contributions. In this paper, we propose StableFDG, a style and attention based learning strategy tailored to federated domain generalization. StableFDG tackles the fundamental challenges in federated DG that arise due to the lack of data/styles in each FL client, with two novel characteristics:",
            "The two suggested schemes work in a complementary fashion, each providing one necessary component for federated DG: our style-based learning\nimproves domain diversity, while the attention-based feature highlighter learns domain-invariant characteristics of each class. Experiments on various FL setups using DG benchmarks confirm the advantage of StableFDG over (i) the baselines that directly apply DG methods to FL and (ii) the baselines that are specifically designed for federated DG.",
            "Overview of approach. Fig. 1 provides an overview of the problem setup and our StableFDG algorithm. As in conventional FL, the training process consists of multiple global rounds, which we index t=1,2,…,T𝑡12…𝑇t=1,2,\\dots,T. In the beginning of round t𝑡t, a selected set of clients download the current global model 𝐰tsubscript𝐰𝑡\\mathbf{w}_{t} from the server. Before local training begins, each client n𝑛n computes its own style information Φn=[μn,σn,Σn​(μ),Σn​(σ)]subscriptΦ𝑛subscript𝜇𝑛subscript𝜎𝑛subscriptΣ𝑛𝜇subscriptΣ𝑛𝜎\\Phi_{n}=[\\mu_{n},\\sigma_{n},\\Sigma_{n}(\\mu),\\Sigma_{n}(\\sigma)] using its local dataset according to (2), which will be clarified soon. This information is sent to the server, and the server shares these information with other clients to compensate for the lack of styles or domains in each client.\nDuring the local update process, each client selectively shifts the styles of the original data in the mini-batch to the new style (received from the server) via adaptive instance normalization (AdaIN) [10], to improve domain diversity (inner box in Fig. 2(b)). After this style sharing and shifting process, each client performs style exploration via feature-level oversampling to further expose the model to novel styles beyond the current source domains of each client (outer box in Fig. 2(b)). Finally, at the output of the feature extractor, we apply our attention-based feature highlighter to extract common/important feature information within each class and emphasize them for better generalization (Fig. 3). When local updates are finished, the server aggregates the client models and proceeds to the next round.",
            "to avoid performance degradation when there is little commonality between key and query images. In other words, StableFDG takes advantage of both cross-attention and self-attention, enabling the model to extract and learn important characteristics across images (via cross-attention), and within the image (via self-attention). A more detailed analysis on (8) can be found in Appendix.",
            "Finally, we put together StableFDG. In each FL round, the clients first download the global model from the server and perform style sharing, shifting, and exploration according to 3.1, which are done in the early layers of CNNs where the style information is preserved. Then, at the output of the feature extractor, attention-based weighted averaging is applied according to Sec. 3.2. These two components have their own roles and work in a complementary fashion to handle the challenging DG problem in FL; our style-based strategy is effective in improving the domain diversity, while our attention-based method can directly capture the domain-invariant characteristics of each class. After the local update process, the server aggregates the client models and proceeds to the next round.",
            "Remark 4 (Computational complexity). The computational complexity of StableFDG depends on the oversampling size in Sec. 3.1 and the attention module size in Sec. 3.2, which could be controlled depending on the resource constraints of clients. We show in Sec. 4 that StableFDG achieves the state-of-the-art performance with (i) minimal oversampling size and (ii) negligible cost of attention module. A more detailed discussion on the computational complexity is in Appendix.",
            "Single-domain data distribution. Table 1 shows our results in a single-domain data distribution setup. We have the following observations. Compared to the previous results provided in the centralized DG works [39, 21], the performance of each method is generally lower. This is due to the limited numbers of styles and data samples in each FL client, which restricts the generalization performance of individual client models. It can be seen that most of the baselines perform better than FedAvg and FedBN that do not tackle the DG problem.\nThe proposed StableFDG achieves the best average accuracy for all benchmark datasets, where the gain is especially large in PACS having large shifts between domains. In contrast to our scheme, the prior works [2, 25, 28] targeting federated DG show marginal performance gains relative to FedAvg in our practical experimental setup with (i) more clients (which results in less data in each client) and (ii) partial client participations.",
            "Multi-domain data distribution. In Table 2, we report the results in a multi-domain data distribution scenario. Compared to the results in Table 1, most of the schemes achieve improved performance in Table 2. This is because each client has multiple source domains, and thus providing a better platform for each client model to gain generalization ability. The proposed StableFDG still performs the best, demonstrating the effectiveness of our style and attention based learning strategy for federated DG.",
            "Effect of each component. To see the effect of each component of StableFDG, in Table 3, we apply our style-based learning and attention-based learning one-by-one in a multi-domain data distribution setup using PACS. We compare our results with style-augmentation DG baselines, MixStyle [39] and DSU [21]. By applying only our style-based learning, StableFDG already outperforms prior style-augmentation methods.\nFurthermore, by adopting only one of the proposed components, our scheme performs better than all the baselines in Table 2. Additional ablation studies using other datasets are reported in Appendix.",
            "Effect of hyperparameters. In DG setups, it is generally impractical to tune the hyperparameter using the target domain, because there is no information on the target domain during training. Hence, we used a fixed exploration level α=3𝛼3\\alpha=3 throughout all experiments without tuning. In Fig. 4, we observe how the hyperparameters affect the target domain performance on PACS. In the first plot of Fig. 4, if α𝛼\\alpha is too small, the performance is relatively low since\nthe model is not able to explore novel styles\nbeyond the client’s source domains. If α𝛼\\alpha is too large, the performance could be slightly degraded because the model would explore too many redundant styles. The overall results show that StableFDG still performs better than the baselines with an arbitrarily chosen α𝛼\\alpha, which is a significant advantage of our scheme in the DG setup where hyperparameter tuning is challenging. The second plot of Fig. 4 shows how the oversampling size (introduced in Step 3 of Sec. 3.1) affects the DG performance. StableFDG still outperforms the baseline with minimal oversampling size,\nindicating that other components of our solution (style sharing/shifting and attention-based components) are already strong enough. The size of oversampling can be determined depending on the clients’ computation/memory constraints, with the cost of improved generalization.",
            "Performance in a centralized setup. Although our scheme is tailored to federated DG, the ideas of StableFDG can be also utilized in a centralized setup. In Table 4(a), we study the effects of our style and attention based strategies in a centralized DG setting using PACS, while the other settings are the same as in the FL setup. The results demonstrate that the proposed ideas are not only specific to data-poor FL scenarios but also have potentials to be utilized in centralized DG settings.",
            "Performance with ResNet-50. In Table 4(b), we also conduct experiments using ResNet-50 on PACS dataset in the multi-domain data distribution scenario. Other settings are exactly the same as in Table 2. The results further confirm the advantage of StableFDG with larger models.",
            "Despite the practical significance, the field of federated domain generalization is still in the early stage of research. In this paper, we proposed StableFDG, a new training strategy tailored to this unexplored area. Our style-based strategy enables the model to get exposed to various novel styles beyond each client’s source domains, while our attention-based method captures and emphasizes the important/common characteristics of each class. Extensive experimental results confirmed the advantage of our StableFDG for federated domain generalization with data-poor FL clients.",
            "Limitations and future works. StableFDG requires 0.45 % more communication load compared to FedAvg for sharing the attention module and style statistics, which is the cost for a better DG performance.\nFurther developing our idea to tailor to centralized DG and extending our attention\nstrategy to segmentation/detection DG tasks are also interesting directions for future research.",
            "To demonstrate the effectiveness of our StableFDG on a larger dataset, we performed experiments on DomainNet dataset in a single-domain data distribution scenario. To this end, we utilize ResNet-50 pretrained on ImageNet. The number of global rounds and mini-batch size are set to 60 and 64, respectively. The remaining settings are the same as those in our main paper. The results in Table 5 show that our StableFDG consistently outperforms not only the centralized DG works but also the prior works on federated DG even with a more complex dataset.",
            "Table 6 compares the communication, computation, and average accuracy of different schemes on PACS in a multi-domain data distribution scenario. ResNet-18 is adopted as in our main manuscript. We first compare the uplink communication load of each client in a specific global round. Compared to FedAvg that only transmits the model in each round, our scheme requires additional communication burden for transmitting the style statistics and the attention module, which are negligible. We also compare the computation time by measuring the time required for local update at each client using an GTX 1080 Ti GPU. CCST [2] and FedDG [25] require large computation due to the increased amounts of data samples or multiple backpropagations for meta training. Our scheme requires additional computation caused by style exploration, attention module update, etc., which are the costs for better generalization to the unseen domain.",
            "Different from the prior works [2, 28] for federated DG adopting the usual setting where the number of clients equals the number of source domains, in our main paper, we introduce a more practical experimental setting for federated DG where the source data is distributed to more clients than the number of source domains. For a comparison with them in the same setting, we also provide additional experimental results in the setup with number of clients = number of source domains. Table 7 shows the results on PACS and Office-Home datasets with three clients in a single-domain data distribution scenario. It is confirmed from the results that our StableFDG also achieves better performance compared to the existing works [2, 28] in this simple setting.",
            "As mentioned in the main manuscript, we provide further ablation studies on the effect of each component in StableFDG using VLCS dataset. Table 8 shows that each component individually brings performance gain compared to FedAvg. Using both strategies achieves greater performance gains, confirming that the two proposed schemes work in a complementary fashion.",
            "In our style based learning, style sharing among clients is performed at random. However, one can think of the strategy where client n𝑛n receives the style information Φn′subscriptΦsuperscript𝑛′\\Phi_{n^{\\prime}} that has the largest distance with its own style information Φn′subscriptΦsuperscript𝑛′\\Phi_{n^{\\prime}} in the style space. Table 9 shows the corresponding result using PACS dataset in a multi-domain data distribution setup. Interestingly, it can be seen that the random selection adopted in this paper performs better, since most of the users generally tend to receive the same style statistics when using the largest distance strategy.",
            "We also compare the effect of number of styles received at each client in Table 10 using PACS dataset in a multi-domain data distribution setup. The performance increases as the number of received styles increases, with small additional communication load (the vector length of the style information is 128, which is negligible compared to the number of model parameters, which is 11,180,103).",
            "In Section 3.1, we utilized the k𝑘k-means++ as a tool for facilitating our key idea in style shifting, which is to effectively balance between the original source domain and the new source domain for better generalization; k-means++ plays a role to select the B/2𝐵2B/2 styles that are similar to the remaining B/2𝐵2B/2 styles in the mini-batch. By doing so, the model can explore new styles while not losing the performance on the original styles. To see this effect, we compare k𝑘k-means++ vs. random sampling when selecting B/2 samples to be shifted, in Table 11. The results show that strategically selecting the B/2 samples to be shifted achieves better performance especially in the Sketch domain (1.89% gain) that has a large style gap with other domains. We believe that these results motivate and justify our design choice.",
            "In our main paper, we performed the class-balanced oversampling in the feature space to alleviate the class-imbalance issue during style exploration. To confirm the effectiveness of the class-balanced oversampling, we compare it with random oversampling under the same condition where only the style exploration is applied without other components. Table 12 shows the results on Office-Home dataset, where the class distribution is highly imbalanced in a multi-domain data distribution scenario. It can be seen that our class-balanced oversampling achieves higher performance over the simple random sampling, which validates the efficacy of mitigating the class imbalance problem in FL clients.",
            "We conduct additional ablation studies on the probability value (defined as p𝑝p here) utilized to control the operation of the style sharing/shifting and style exploration modules. Larger p𝑝p means that our scheme is more likely to be activated. In Table 13, we provide results on various probability values in a single-domain data distribution scenario using PACS dataset. From the results, it is confirm that for all p𝑝p values, the proposed StableFDG outperforms existing baselines, demonstrating that our scheme can work well with an arbitrarily chosen probability p𝑝p. In detail, when p𝑝p ranges from 0.3 to 0.7, the high performance is maintained while the performance decreases at both extreme probabilities (p=0.1𝑝0.1p=0.1 and 0.9). Therefore, it is recommended for practitioners to select the p𝑝p in an appropriate range, avoiding extreme cases.",
            "For implementation, style-based learning is applied only in the 1st, 2nd, 3rd blocks among 4 residual blocks in ResNet-18. Note that at the output of the 4th block, label information is dominant rather than style information, which results in degraded performance when style-based schemes are applied. This is confirmed by our new experiments in the table below. It can be seen from the results that if we consider the 4th residual block to apply our style-based learning, the performance gets degraded. This result confirms the intuition that style-based learning should be conducted at the earlier layers where style information is preserved.",
            "In our main paper, the similarity metric in equation (6) adopts cross-attention, while the metric in equation (8) combines cross-attention and self-attention. When applying only the cross-attention-based metric in equation (6), we found that the similarity value could become low even when the two samples belong to the same class, in special cases. We handled this issue by adding the self-attention component as in equation (8). Intuitively, by doing this, the attention module is learning to extract the important features across images (via cross-attention), and within the image (via self-attention). Table 15 compares the performance of our StableFDG when using (i) self-attention alone, (ii) cross-attention alone (equation (6)), and (iii) both self and cross attentions at the same time (equation (8)), confirming the advantage of using self-attention and cross-attention together.",
            "Our attention module requires 0.44% of additional model parameters to perform the attention-based learning. For a fair comparison to see the effect of our attention-based learning, we consider a different baseline with the same model size but without attention-based learning. Specifically, the baseline computes the attention score map using only additional convolutional operations and take the weighted average of the feature zisubscript𝑧𝑖z_{i} based on the attention score map. Table 16 shows the results using PACS dataset in a single-domain data distribution scenario. The results demonstrate that our attention-based learning achieves performance improvements on all four domains while playing a key role in capturing essential parts of the features.",
            "Now we provide answer to the following question: Instead of the FL setup we focused on, can attention provide benefits in the centralized DG setup? Table 17 shows the results with/without attention module in a centralized setup using PACS dataset. The results show that attention still provides performance improvements in the centralized setup by learning domain-invariant features, although the gain is slightly lower than the gain in the FL setup as shown in Table 3 of the main manuscript. These results indicate that the proposed attention-based learning indeed captures the domain-invariant characteristics of samples, while the scheme provides more benefits in the FL setup where each client is prone to overfitting due to lack of data.",
            "More detailed description on oversampling: Let sn∈ℝB×C×H×Wsuperscript𝑠𝑛superscriptℝ𝐵𝐶𝐻𝑊s^{n}\\in\\mathbb{R}^{B\\times C\\times H\\times W} be a mini-batch of features in client n𝑛n at a specific layer, obtained after Steps 1 and 2 in the main manuscript. Now given a fixed oversampling size, we oversample the features in the mini-batch to obtain s~nsuperscript~𝑠𝑛\\tilde{s}^{n}, so that the concatenated mini-batch s^n=[sn,s~n]superscript^𝑠𝑛superscript𝑠𝑛superscript~𝑠𝑛\\hat{s}^{n}=[s^{n},\\tilde{s}^{n}] becomes class-balanced as much as possible. Consider a toy example where the number of samples for classes a𝑎a, b𝑏b, c𝑐c are 3, 2, 1, respectively in the mini-batch snsuperscript𝑠𝑛s^{n}. In this example, if the oversampling size is 3, we randomly choose one data point from class b𝑏b\nand two data points from class c𝑐c (in this case, the same data point is selected for two times with duplication) to obtain s~nsuperscript~𝑠𝑛\\tilde{s}^{n}, so that the concatenated mini-batch s^n=[sn,s~n]superscript^𝑠𝑛superscript𝑠𝑛superscript~𝑠𝑛\\hat{s}^{n}=[s^{n},\\tilde{s}^{n}] becomes class-balanced. If the oversampling size is 1, we oversample one data point in class c𝑐c to make the concatenated mini-batch to be balanced as much as possible. If the oversampling size is 6, we oversample 1, 2, 3 samples from classes a𝑎a, b𝑏b, c𝑐c, respectively to construct s~nsuperscript~𝑠𝑛\\tilde{s}^{n}. The concatenated mini-batch s^n=[sn,s~n]superscript^𝑠𝑛superscript𝑠𝑛superscript~𝑠𝑛\\hat{s}^{n}=[s^{n},\\tilde{s}^{n}] is utilized for style-based learning and updating the model. This process not only mitigates the class-imbalance issue in each client but also provides a good platform for style exploration by oversampling the features. In our work, we reported the results with oversampling size of B𝐵B (which is equal to the mini-batch size), while the effect of the oversampling size is also reported in Fig. 4 of the main manuscript: A larger oversampling size leads to a better performance, and more importantly, our StableFDG outperforms the baseline even without any oversampling.",
            "Algorithm 1 summarizes the overall process of our StableFDG."
        ]
    },
    "S4.T1.st2": {
        "caption": "(b) Office-Home and Digits-DG datasets.",
        "table": "<table id=\"S4.T1.st2.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.st2.2.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.st2.2.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_border_rr ltx_border_tt\"></th>\n<th id=\"S4.T1.st2.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"5\"><span id=\"S4.T1.st2.2.1.1.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Office-Home</span></th>\n<th id=\"S4.T1.st2.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"5\"><span id=\"S4.T1.st2.2.1.1.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Digits-DG</span></th>\n</tr>\n<tr id=\"S4.T1.st2.2.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T1.st2.2.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr\"><span id=\"S4.T1.st2.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Methods</span></th>\n<th id=\"S4.T1.st2.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.st2.2.2.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">Art</span></th>\n<th id=\"S4.T1.st2.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.st2.2.2.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">Clipart</span></th>\n<th id=\"S4.T1.st2.2.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.st2.2.2.2.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">Product</span></th>\n<th id=\"S4.T1.st2.2.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T1.st2.2.2.2.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">Real</span></th>\n<th id=\"S4.T1.st2.2.2.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t\"><span id=\"S4.T1.st2.2.2.2.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">Avg.</span></th>\n<th id=\"S4.T1.st2.2.2.2.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.st2.2.2.2.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">MNIST</span></th>\n<th id=\"S4.T1.st2.2.2.2.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.st2.2.2.2.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">MNIST-M</span></th>\n<th id=\"S4.T1.st2.2.2.2.9\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.st2.2.2.2.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">SVHN</span></th>\n<th id=\"S4.T1.st2.2.2.2.10\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T1.st2.2.2.2.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">SYN</span></th>\n<th id=\"S4.T1.st2.2.2.2.11\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.st2.2.2.2.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">Avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.st2.2.3.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.st2.2.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">\n<span id=\"S4.T1.st2.2.3.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedAvg </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T1.st2.2.3.1.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">26</a><span id=\"S4.T1.st2.2.3.1.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T1.st2.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.st2.2.3.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">57.27</span></td>\n<td id=\"S4.T1.st2.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.st2.2.3.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">48.23</span></td>\n<td id=\"S4.T1.st2.2.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.st2.2.3.1.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.77</span></td>\n<td id=\"S4.T1.st2.2.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T1.st2.2.3.1.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.60</span></td>\n<td id=\"S4.T1.st2.2.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\"><span id=\"S4.T1.st2.2.3.1.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">63.22</span></td>\n<td id=\"S4.T1.st2.2.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.st2.2.3.1.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">98.05</span></td>\n<td id=\"S4.T1.st2.2.3.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.st2.2.3.1.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">70.95</span></td>\n<td id=\"S4.T1.st2.2.3.1.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.st2.2.3.1.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">68.95</span></td>\n<td id=\"S4.T1.st2.2.3.1.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T1.st2.2.3.1.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">86.40</span></td>\n<td id=\"S4.T1.st2.2.3.1.11\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.st2.2.3.1.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">81.09</span></td>\n</tr>\n<tr id=\"S4.T1.st2.2.4.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.st2.2.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T1.st2.2.4.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedBN </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T1.st2.2.4.2.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">22</a><span id=\"S4.T1.st2.2.4.2.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T1.st2.2.4.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.4.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">57.56</span></td>\n<td id=\"S4.T1.st2.2.4.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.4.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">48.13</span></td>\n<td id=\"S4.T1.st2.2.4.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.4.2.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.65</span></td>\n<td id=\"S4.T1.st2.2.4.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st2.2.4.2.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.57</span></td>\n<td id=\"S4.T1.st2.2.4.2.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T1.st2.2.4.2.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">63.23</span></td>\n<td id=\"S4.T1.st2.2.4.2.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.4.2.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">97.33</span></td>\n<td id=\"S4.T1.st2.2.4.2.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.4.2.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.68</span></td>\n<td id=\"S4.T1.st2.2.4.2.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.4.2.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">71.77</span></td>\n<td id=\"S4.T1.st2.2.4.2.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st2.2.4.2.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">85.36</span></td>\n<td id=\"S4.T1.st2.2.4.2.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.4.2.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">81.79</span></td>\n</tr>\n<tr id=\"S4.T1.st2.2.5.3\" class=\"ltx_tr\">\n<td id=\"S4.T1.st2.2.5.3.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T1.st2.2.5.3.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">MixStyle </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T1.st2.2.5.3.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">39</a><span id=\"S4.T1.st2.2.5.3.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T1.st2.2.5.3.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.5.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">56.05</span></td>\n<td id=\"S4.T1.st2.2.5.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.5.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">51.55</span></td>\n<td id=\"S4.T1.st2.2.5.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.5.3.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">70.95</span></td>\n<td id=\"S4.T1.st2.2.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st2.2.5.3.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.25</span></td>\n<td id=\"S4.T1.st2.2.5.3.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T1.st2.2.5.3.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">62.95</span></td>\n<td id=\"S4.T1.st2.2.5.3.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.5.3.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">97.75</span></td>\n<td id=\"S4.T1.st2.2.5.3.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.5.3.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.25</span></td>\n<td id=\"S4.T1.st2.2.5.3.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.5.3.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">70.85</span></td>\n<td id=\"S4.T1.st2.2.5.3.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st2.2.5.3.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">85.50</span></td>\n<td id=\"S4.T1.st2.2.5.3.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.5.3.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">82.09</span></td>\n</tr>\n<tr id=\"S4.T1.st2.2.6.4\" class=\"ltx_tr\">\n<td id=\"S4.T1.st2.2.6.4.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T1.st2.2.6.4.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">DSU </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T1.st2.2.6.4.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">21</a><span id=\"S4.T1.st2.2.6.4.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T1.st2.2.6.4.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.6.4.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">58.55</span></td>\n<td id=\"S4.T1.st2.2.6.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.6.4.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">52.60</span></td>\n<td id=\"S4.T1.st2.2.6.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.6.4.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">71.60</span></td>\n<td id=\"S4.T1.st2.2.6.4.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st2.2.6.4.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.15</span></td>\n<td id=\"S4.T1.st2.2.6.4.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T1.st2.2.6.4.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">63.98</span></td>\n<td id=\"S4.T1.st2.2.6.4.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.6.4.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">98.10</span></td>\n<td id=\"S4.T1.st2.2.6.4.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.6.4.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">75.60</span></td>\n<td id=\"S4.T1.st2.2.6.4.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.6.4.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">70.47</span></td>\n<td id=\"S4.T1.st2.2.6.4.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st2.2.6.4.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">85.80</span></td>\n<td id=\"S4.T1.st2.2.6.4.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.6.4.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">82.49</span></td>\n</tr>\n<tr id=\"S4.T1.st2.2.7.5\" class=\"ltx_tr\">\n<td id=\"S4.T1.st2.2.7.5.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T1.st2.2.7.5.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">CCST </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T1.st2.2.7.5.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">2</a><span id=\"S4.T1.st2.2.7.5.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T1.st2.2.7.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.7.5.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">51.3</span></td>\n<td id=\"S4.T1.st2.2.7.5.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.7.5.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">51.75</span></td>\n<td id=\"S4.T1.st2.2.7.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.7.5.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">70.2</span></td>\n<td id=\"S4.T1.st2.2.7.5.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st2.2.7.5.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">70.3</span></td>\n<td id=\"S4.T1.st2.2.7.5.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T1.st2.2.7.5.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">60.89</span></td>\n<td id=\"S4.T1.st2.2.7.5.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.7.5.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">95.10</span></td>\n<td id=\"S4.T1.st2.2.7.5.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.7.5.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">62.80</span></td>\n<td id=\"S4.T1.st2.2.7.5.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.7.5.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">56.60</span></td>\n<td id=\"S4.T1.st2.2.7.5.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st2.2.7.5.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.90</span></td>\n<td id=\"S4.T1.st2.2.7.5.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.7.5.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.35</span></td>\n</tr>\n<tr id=\"S4.T1.st2.2.8.6\" class=\"ltx_tr\">\n<td id=\"S4.T1.st2.2.8.6.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T1.st2.2.8.6.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedDG </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T1.st2.2.8.6.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">25</a><span id=\"S4.T1.st2.2.8.6.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T1.st2.2.8.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.8.6.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">57.6</span></td>\n<td id=\"S4.T1.st2.2.8.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.8.6.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">48.1</span></td>\n<td id=\"S4.T1.st2.2.8.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.8.6.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.55</span></td>\n<td id=\"S4.T1.st2.2.8.6.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st2.2.8.6.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.33</span></td>\n<td id=\"S4.T1.st2.2.8.6.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T1.st2.2.8.6.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">63.15</span></td>\n<td id=\"S4.T1.st2.2.8.6.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.8.6.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">97.97</span></td>\n<td id=\"S4.T1.st2.2.8.6.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.8.6.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.13</span></td>\n<td id=\"S4.T1.st2.2.8.6.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.8.6.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">71.03</span></td>\n<td id=\"S4.T1.st2.2.8.6.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st2.2.8.6.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">87.87</span></td>\n<td id=\"S4.T1.st2.2.8.6.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.8.6.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">82.25</span></td>\n</tr>\n<tr id=\"S4.T1.st2.2.9.7\" class=\"ltx_tr\">\n<td id=\"S4.T1.st2.2.9.7.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T1.st2.2.9.7.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedSR </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T1.st2.2.9.7.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">28</a><span id=\"S4.T1.st2.2.9.7.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T1.st2.2.9.7.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.9.7.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">57.8</span></td>\n<td id=\"S4.T1.st2.2.9.7.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.9.7.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">48.1</span></td>\n<td id=\"S4.T1.st2.2.9.7.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.9.7.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.1</span></td>\n<td id=\"S4.T1.st2.2.9.7.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st2.2.9.7.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.2</span></td>\n<td id=\"S4.T1.st2.2.9.7.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T1.st2.2.9.7.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">63.05</span></td>\n<td id=\"S4.T1.st2.2.9.7.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.9.7.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">98.00</span></td>\n<td id=\"S4.T1.st2.2.9.7.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.9.7.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.00</span></td>\n<td id=\"S4.T1.st2.2.9.7.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.9.7.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">68.50</span></td>\n<td id=\"S4.T1.st2.2.9.7.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.st2.2.9.7.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">86.70</span></td>\n<td id=\"S4.T1.st2.2.9.7.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.st2.2.9.7.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">81.55</span></td>\n</tr>\n<tr id=\"S4.T1.st2.2.10.8\" class=\"ltx_tr\">\n<td id=\"S4.T1.st2.2.10.8.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_rr\">\n<span id=\"S4.T1.st2.2.10.8.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">StableFDG</span><span id=\"S4.T1.st2.2.10.8.1.2\" class=\"ltx_text\" style=\"font-size:70%;\"> (ours)</span>\n</td>\n<td id=\"S4.T1.st2.2.10.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.st2.2.10.8.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">57.57</span></td>\n<td id=\"S4.T1.st2.2.10.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.st2.2.10.8.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">54.30</span></td>\n<td id=\"S4.T1.st2.2.10.8.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.st2.2.10.8.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.33</span></td>\n<td id=\"S4.T1.st2.2.10.8.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S4.T1.st2.2.10.8.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.97</span></td>\n<td id=\"S4.T1.st2.2.10.8.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_rr\"><span id=\"S4.T1.st2.2.10.8.6.1\" class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">64.79</span></td>\n<td id=\"S4.T1.st2.2.10.8.7\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.st2.2.10.8.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">97.23</span></td>\n<td id=\"S4.T1.st2.2.10.8.8\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.st2.2.10.8.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.53</span></td>\n<td id=\"S4.T1.st2.2.10.8.9\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.st2.2.10.8.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.95</span></td>\n<td id=\"S4.T1.st2.2.10.8.10\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S4.T1.st2.2.10.8.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">85.85</span></td>\n<td id=\"S4.T1.st2.2.10.8.11\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.st2.2.10.8.11.1\" class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">82.64</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Traditional federated learning (FL) algorithms operate under the assumption that the data distributions at training (source domains) and testing (target domain) are the same. The fact that domain shifts often occur in practice necessitates equipping FL methods with a domain generalization (DG) capability. However, existing DG algorithms face fundamental challenges in FL setups due to the lack of samples/domains in each client’s local dataset. In this paper, we propose StableFDG, a style and attention based learning strategy for accomplishing federated domain generalization, introducing two key contributions. The first is style-based learning, which enables each client to explore novel styles beyond the original source domains in its local dataset, improving domain diversity based on the proposed style sharing, shifting, and exploration strategies. Our second contribution is an attention-based feature highlighter, which captures the similarities between the features of data samples in the same class, and emphasizes the important/common characteristics to better learn the domain-invariant characteristics of each class in data-poor FL scenarios. Experimental results show that StableFDG outperforms existing baselines on various DG benchmark datasets, demonstrating its efficacy.",
            "Contributions. In this paper, we propose StableFDG, a style and attention based learning strategy tailored to federated domain generalization. StableFDG tackles the fundamental challenges in federated DG that arise due to the lack of data/styles in each FL client, with two novel characteristics:",
            "The two suggested schemes work in a complementary fashion, each providing one necessary component for federated DG: our style-based learning\nimproves domain diversity, while the attention-based feature highlighter learns domain-invariant characteristics of each class. Experiments on various FL setups using DG benchmarks confirm the advantage of StableFDG over (i) the baselines that directly apply DG methods to FL and (ii) the baselines that are specifically designed for federated DG.",
            "Overview of approach. Fig. 1 provides an overview of the problem setup and our StableFDG algorithm. As in conventional FL, the training process consists of multiple global rounds, which we index t=1,2,…,T𝑡12…𝑇t=1,2,\\dots,T. In the beginning of round t𝑡t, a selected set of clients download the current global model 𝐰tsubscript𝐰𝑡\\mathbf{w}_{t} from the server. Before local training begins, each client n𝑛n computes its own style information Φn=[μn,σn,Σn​(μ),Σn​(σ)]subscriptΦ𝑛subscript𝜇𝑛subscript𝜎𝑛subscriptΣ𝑛𝜇subscriptΣ𝑛𝜎\\Phi_{n}=[\\mu_{n},\\sigma_{n},\\Sigma_{n}(\\mu),\\Sigma_{n}(\\sigma)] using its local dataset according to (2), which will be clarified soon. This information is sent to the server, and the server shares these information with other clients to compensate for the lack of styles or domains in each client.\nDuring the local update process, each client selectively shifts the styles of the original data in the mini-batch to the new style (received from the server) via adaptive instance normalization (AdaIN) [10], to improve domain diversity (inner box in Fig. 2(b)). After this style sharing and shifting process, each client performs style exploration via feature-level oversampling to further expose the model to novel styles beyond the current source domains of each client (outer box in Fig. 2(b)). Finally, at the output of the feature extractor, we apply our attention-based feature highlighter to extract common/important feature information within each class and emphasize them for better generalization (Fig. 3). When local updates are finished, the server aggregates the client models and proceeds to the next round.",
            "to avoid performance degradation when there is little commonality between key and query images. In other words, StableFDG takes advantage of both cross-attention and self-attention, enabling the model to extract and learn important characteristics across images (via cross-attention), and within the image (via self-attention). A more detailed analysis on (8) can be found in Appendix.",
            "Finally, we put together StableFDG. In each FL round, the clients first download the global model from the server and perform style sharing, shifting, and exploration according to 3.1, which are done in the early layers of CNNs where the style information is preserved. Then, at the output of the feature extractor, attention-based weighted averaging is applied according to Sec. 3.2. These two components have their own roles and work in a complementary fashion to handle the challenging DG problem in FL; our style-based strategy is effective in improving the domain diversity, while our attention-based method can directly capture the domain-invariant characteristics of each class. After the local update process, the server aggregates the client models and proceeds to the next round.",
            "Remark 4 (Computational complexity). The computational complexity of StableFDG depends on the oversampling size in Sec. 3.1 and the attention module size in Sec. 3.2, which could be controlled depending on the resource constraints of clients. We show in Sec. 4 that StableFDG achieves the state-of-the-art performance with (i) minimal oversampling size and (ii) negligible cost of attention module. A more detailed discussion on the computational complexity is in Appendix.",
            "Single-domain data distribution. Table 1 shows our results in a single-domain data distribution setup. We have the following observations. Compared to the previous results provided in the centralized DG works [39, 21], the performance of each method is generally lower. This is due to the limited numbers of styles and data samples in each FL client, which restricts the generalization performance of individual client models. It can be seen that most of the baselines perform better than FedAvg and FedBN that do not tackle the DG problem.\nThe proposed StableFDG achieves the best average accuracy for all benchmark datasets, where the gain is especially large in PACS having large shifts between domains. In contrast to our scheme, the prior works [2, 25, 28] targeting federated DG show marginal performance gains relative to FedAvg in our practical experimental setup with (i) more clients (which results in less data in each client) and (ii) partial client participations.",
            "Multi-domain data distribution. In Table 2, we report the results in a multi-domain data distribution scenario. Compared to the results in Table 1, most of the schemes achieve improved performance in Table 2. This is because each client has multiple source domains, and thus providing a better platform for each client model to gain generalization ability. The proposed StableFDG still performs the best, demonstrating the effectiveness of our style and attention based learning strategy for federated DG.",
            "Effect of each component. To see the effect of each component of StableFDG, in Table 3, we apply our style-based learning and attention-based learning one-by-one in a multi-domain data distribution setup using PACS. We compare our results with style-augmentation DG baselines, MixStyle [39] and DSU [21]. By applying only our style-based learning, StableFDG already outperforms prior style-augmentation methods.\nFurthermore, by adopting only one of the proposed components, our scheme performs better than all the baselines in Table 2. Additional ablation studies using other datasets are reported in Appendix.",
            "Effect of hyperparameters. In DG setups, it is generally impractical to tune the hyperparameter using the target domain, because there is no information on the target domain during training. Hence, we used a fixed exploration level α=3𝛼3\\alpha=3 throughout all experiments without tuning. In Fig. 4, we observe how the hyperparameters affect the target domain performance on PACS. In the first plot of Fig. 4, if α𝛼\\alpha is too small, the performance is relatively low since\nthe model is not able to explore novel styles\nbeyond the client’s source domains. If α𝛼\\alpha is too large, the performance could be slightly degraded because the model would explore too many redundant styles. The overall results show that StableFDG still performs better than the baselines with an arbitrarily chosen α𝛼\\alpha, which is a significant advantage of our scheme in the DG setup where hyperparameter tuning is challenging. The second plot of Fig. 4 shows how the oversampling size (introduced in Step 3 of Sec. 3.1) affects the DG performance. StableFDG still outperforms the baseline with minimal oversampling size,\nindicating that other components of our solution (style sharing/shifting and attention-based components) are already strong enough. The size of oversampling can be determined depending on the clients’ computation/memory constraints, with the cost of improved generalization.",
            "Performance in a centralized setup. Although our scheme is tailored to federated DG, the ideas of StableFDG can be also utilized in a centralized setup. In Table 4(a), we study the effects of our style and attention based strategies in a centralized DG setting using PACS, while the other settings are the same as in the FL setup. The results demonstrate that the proposed ideas are not only specific to data-poor FL scenarios but also have potentials to be utilized in centralized DG settings.",
            "Performance with ResNet-50. In Table 4(b), we also conduct experiments using ResNet-50 on PACS dataset in the multi-domain data distribution scenario. Other settings are exactly the same as in Table 2. The results further confirm the advantage of StableFDG with larger models.",
            "Despite the practical significance, the field of federated domain generalization is still in the early stage of research. In this paper, we proposed StableFDG, a new training strategy tailored to this unexplored area. Our style-based strategy enables the model to get exposed to various novel styles beyond each client’s source domains, while our attention-based method captures and emphasizes the important/common characteristics of each class. Extensive experimental results confirmed the advantage of our StableFDG for federated domain generalization with data-poor FL clients.",
            "Limitations and future works. StableFDG requires 0.45 % more communication load compared to FedAvg for sharing the attention module and style statistics, which is the cost for a better DG performance.\nFurther developing our idea to tailor to centralized DG and extending our attention\nstrategy to segmentation/detection DG tasks are also interesting directions for future research.",
            "To demonstrate the effectiveness of our StableFDG on a larger dataset, we performed experiments on DomainNet dataset in a single-domain data distribution scenario. To this end, we utilize ResNet-50 pretrained on ImageNet. The number of global rounds and mini-batch size are set to 60 and 64, respectively. The remaining settings are the same as those in our main paper. The results in Table 5 show that our StableFDG consistently outperforms not only the centralized DG works but also the prior works on federated DG even with a more complex dataset.",
            "Table 6 compares the communication, computation, and average accuracy of different schemes on PACS in a multi-domain data distribution scenario. ResNet-18 is adopted as in our main manuscript. We first compare the uplink communication load of each client in a specific global round. Compared to FedAvg that only transmits the model in each round, our scheme requires additional communication burden for transmitting the style statistics and the attention module, which are negligible. We also compare the computation time by measuring the time required for local update at each client using an GTX 1080 Ti GPU. CCST [2] and FedDG [25] require large computation due to the increased amounts of data samples or multiple backpropagations for meta training. Our scheme requires additional computation caused by style exploration, attention module update, etc., which are the costs for better generalization to the unseen domain.",
            "Different from the prior works [2, 28] for federated DG adopting the usual setting where the number of clients equals the number of source domains, in our main paper, we introduce a more practical experimental setting for federated DG where the source data is distributed to more clients than the number of source domains. For a comparison with them in the same setting, we also provide additional experimental results in the setup with number of clients = number of source domains. Table 7 shows the results on PACS and Office-Home datasets with three clients in a single-domain data distribution scenario. It is confirmed from the results that our StableFDG also achieves better performance compared to the existing works [2, 28] in this simple setting.",
            "As mentioned in the main manuscript, we provide further ablation studies on the effect of each component in StableFDG using VLCS dataset. Table 8 shows that each component individually brings performance gain compared to FedAvg. Using both strategies achieves greater performance gains, confirming that the two proposed schemes work in a complementary fashion.",
            "In our style based learning, style sharing among clients is performed at random. However, one can think of the strategy where client n𝑛n receives the style information Φn′subscriptΦsuperscript𝑛′\\Phi_{n^{\\prime}} that has the largest distance with its own style information Φn′subscriptΦsuperscript𝑛′\\Phi_{n^{\\prime}} in the style space. Table 9 shows the corresponding result using PACS dataset in a multi-domain data distribution setup. Interestingly, it can be seen that the random selection adopted in this paper performs better, since most of the users generally tend to receive the same style statistics when using the largest distance strategy.",
            "We also compare the effect of number of styles received at each client in Table 10 using PACS dataset in a multi-domain data distribution setup. The performance increases as the number of received styles increases, with small additional communication load (the vector length of the style information is 128, which is negligible compared to the number of model parameters, which is 11,180,103).",
            "In Section 3.1, we utilized the k𝑘k-means++ as a tool for facilitating our key idea in style shifting, which is to effectively balance between the original source domain and the new source domain for better generalization; k-means++ plays a role to select the B/2𝐵2B/2 styles that are similar to the remaining B/2𝐵2B/2 styles in the mini-batch. By doing so, the model can explore new styles while not losing the performance on the original styles. To see this effect, we compare k𝑘k-means++ vs. random sampling when selecting B/2 samples to be shifted, in Table 11. The results show that strategically selecting the B/2 samples to be shifted achieves better performance especially in the Sketch domain (1.89% gain) that has a large style gap with other domains. We believe that these results motivate and justify our design choice.",
            "In our main paper, we performed the class-balanced oversampling in the feature space to alleviate the class-imbalance issue during style exploration. To confirm the effectiveness of the class-balanced oversampling, we compare it with random oversampling under the same condition where only the style exploration is applied without other components. Table 12 shows the results on Office-Home dataset, where the class distribution is highly imbalanced in a multi-domain data distribution scenario. It can be seen that our class-balanced oversampling achieves higher performance over the simple random sampling, which validates the efficacy of mitigating the class imbalance problem in FL clients.",
            "We conduct additional ablation studies on the probability value (defined as p𝑝p here) utilized to control the operation of the style sharing/shifting and style exploration modules. Larger p𝑝p means that our scheme is more likely to be activated. In Table 13, we provide results on various probability values in a single-domain data distribution scenario using PACS dataset. From the results, it is confirm that for all p𝑝p values, the proposed StableFDG outperforms existing baselines, demonstrating that our scheme can work well with an arbitrarily chosen probability p𝑝p. In detail, when p𝑝p ranges from 0.3 to 0.7, the high performance is maintained while the performance decreases at both extreme probabilities (p=0.1𝑝0.1p=0.1 and 0.9). Therefore, it is recommended for practitioners to select the p𝑝p in an appropriate range, avoiding extreme cases.",
            "For implementation, style-based learning is applied only in the 1st, 2nd, 3rd blocks among 4 residual blocks in ResNet-18. Note that at the output of the 4th block, label information is dominant rather than style information, which results in degraded performance when style-based schemes are applied. This is confirmed by our new experiments in the table below. It can be seen from the results that if we consider the 4th residual block to apply our style-based learning, the performance gets degraded. This result confirms the intuition that style-based learning should be conducted at the earlier layers where style information is preserved.",
            "In our main paper, the similarity metric in equation (6) adopts cross-attention, while the metric in equation (8) combines cross-attention and self-attention. When applying only the cross-attention-based metric in equation (6), we found that the similarity value could become low even when the two samples belong to the same class, in special cases. We handled this issue by adding the self-attention component as in equation (8). Intuitively, by doing this, the attention module is learning to extract the important features across images (via cross-attention), and within the image (via self-attention). Table 15 compares the performance of our StableFDG when using (i) self-attention alone, (ii) cross-attention alone (equation (6)), and (iii) both self and cross attentions at the same time (equation (8)), confirming the advantage of using self-attention and cross-attention together.",
            "Our attention module requires 0.44% of additional model parameters to perform the attention-based learning. For a fair comparison to see the effect of our attention-based learning, we consider a different baseline with the same model size but without attention-based learning. Specifically, the baseline computes the attention score map using only additional convolutional operations and take the weighted average of the feature zisubscript𝑧𝑖z_{i} based on the attention score map. Table 16 shows the results using PACS dataset in a single-domain data distribution scenario. The results demonstrate that our attention-based learning achieves performance improvements on all four domains while playing a key role in capturing essential parts of the features.",
            "Now we provide answer to the following question: Instead of the FL setup we focused on, can attention provide benefits in the centralized DG setup? Table 17 shows the results with/without attention module in a centralized setup using PACS dataset. The results show that attention still provides performance improvements in the centralized setup by learning domain-invariant features, although the gain is slightly lower than the gain in the FL setup as shown in Table 3 of the main manuscript. These results indicate that the proposed attention-based learning indeed captures the domain-invariant characteristics of samples, while the scheme provides more benefits in the FL setup where each client is prone to overfitting due to lack of data.",
            "More detailed description on oversampling: Let sn∈ℝB×C×H×Wsuperscript𝑠𝑛superscriptℝ𝐵𝐶𝐻𝑊s^{n}\\in\\mathbb{R}^{B\\times C\\times H\\times W} be a mini-batch of features in client n𝑛n at a specific layer, obtained after Steps 1 and 2 in the main manuscript. Now given a fixed oversampling size, we oversample the features in the mini-batch to obtain s~nsuperscript~𝑠𝑛\\tilde{s}^{n}, so that the concatenated mini-batch s^n=[sn,s~n]superscript^𝑠𝑛superscript𝑠𝑛superscript~𝑠𝑛\\hat{s}^{n}=[s^{n},\\tilde{s}^{n}] becomes class-balanced as much as possible. Consider a toy example where the number of samples for classes a𝑎a, b𝑏b, c𝑐c are 3, 2, 1, respectively in the mini-batch snsuperscript𝑠𝑛s^{n}. In this example, if the oversampling size is 3, we randomly choose one data point from class b𝑏b\nand two data points from class c𝑐c (in this case, the same data point is selected for two times with duplication) to obtain s~nsuperscript~𝑠𝑛\\tilde{s}^{n}, so that the concatenated mini-batch s^n=[sn,s~n]superscript^𝑠𝑛superscript𝑠𝑛superscript~𝑠𝑛\\hat{s}^{n}=[s^{n},\\tilde{s}^{n}] becomes class-balanced. If the oversampling size is 1, we oversample one data point in class c𝑐c to make the concatenated mini-batch to be balanced as much as possible. If the oversampling size is 6, we oversample 1, 2, 3 samples from classes a𝑎a, b𝑏b, c𝑐c, respectively to construct s~nsuperscript~𝑠𝑛\\tilde{s}^{n}. The concatenated mini-batch s^n=[sn,s~n]superscript^𝑠𝑛superscript𝑠𝑛superscript~𝑠𝑛\\hat{s}^{n}=[s^{n},\\tilde{s}^{n}] is utilized for style-based learning and updating the model. This process not only mitigates the class-imbalance issue in each client but also provides a good platform for style exploration by oversampling the features. In our work, we reported the results with oversampling size of B𝐵B (which is equal to the mini-batch size), while the effect of the oversampling size is also reported in Fig. 4 of the main manuscript: A larger oversampling size leads to a better performance, and more importantly, our StableFDG outperforms the baseline even without any oversampling.",
            "Algorithm 1 summarizes the overall process of our StableFDG."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Main result 2 (multi-domain data distribution): Each client has multiple source domains in its local dataset. The results are consistent with the single-domain scenario in Table 1.",
        "table": "<table id=\"S4.T2.st1.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.st1.2.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.st1.2.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_border_rr ltx_border_tt\"></th>\n<th id=\"S4.T2.st1.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"5\"><span id=\"S4.T2.st1.2.1.1.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Office-Home</span></th>\n<th id=\"S4.T2.st1.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"5\"><span id=\"S4.T2.st1.2.1.1.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">VLCS</span></th>\n</tr>\n<tr id=\"S4.T2.st1.2.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.st1.2.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr\"><span id=\"S4.T2.st1.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Methods</span></th>\n<th id=\"S4.T2.st1.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">Art</span></th>\n<th id=\"S4.T2.st1.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">Clipart</span></th>\n<th id=\"S4.T2.st1.2.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">Product</span></th>\n<th id=\"S4.T2.st1.2.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">Real</span></th>\n<th id=\"S4.T2.st1.2.2.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">Avg.</span></th>\n<th id=\"S4.T2.st1.2.2.2.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">Caltech</span></th>\n<th id=\"S4.T2.st1.2.2.2.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">LabelMe</span></th>\n<th id=\"S4.T2.st1.2.2.2.9\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">Pascal</span></th>\n<th id=\"S4.T2.st1.2.2.2.10\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">Sun</span></th>\n<th id=\"S4.T2.st1.2.2.2.11\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">Avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.st1.2.3.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">\n<span id=\"S4.T2.st1.2.3.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedAvg </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T2.st1.2.3.1.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">26</a><span id=\"S4.T2.st1.2.3.1.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T2.st1.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">57.70</span></td>\n<td id=\"S4.T2.st1.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">48.30</span></td>\n<td id=\"S4.T2.st1.2.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.87</span></td>\n<td id=\"S4.T2.st1.2.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">75.33</span></td>\n<td id=\"S4.T2.st1.2.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">63.55</span></td>\n<td id=\"S4.T2.st1.2.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">93.65</span></td>\n<td id=\"S4.T2.st1.2.3.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">61.10</span></td>\n<td id=\"S4.T2.st1.2.3.1.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.55</span></td>\n<td id=\"S4.T2.st1.2.3.1.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">65.40</span></td>\n<td id=\"S4.T2.st1.2.3.1.11\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.18</span></td>\n</tr>\n<tr id=\"S4.T2.st1.2.4.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T2.st1.2.4.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedBN </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T2.st1.2.4.2.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">22</a><span id=\"S4.T2.st1.2.4.2.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T2.st1.2.4.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.4.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">57.07</span></td>\n<td id=\"S4.T2.st1.2.4.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.4.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">48.32</span></td>\n<td id=\"S4.T2.st1.2.4.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.4.2.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.31</span></td>\n<td id=\"S4.T2.st1.2.4.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.4.2.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.57</span></td>\n<td id=\"S4.T2.st1.2.4.2.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T2.st1.2.4.2.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">63.07</span></td>\n<td id=\"S4.T2.st1.2.4.2.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.4.2.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">94.34</span></td>\n<td id=\"S4.T2.st1.2.4.2.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.4.2.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">62.61</span></td>\n<td id=\"S4.T2.st1.2.4.2.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.4.2.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">69.89</span></td>\n<td id=\"S4.T2.st1.2.4.2.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.4.2.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">69.04</span></td>\n<td id=\"S4.T2.st1.2.4.2.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.4.2.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.97</span></td>\n</tr>\n<tr id=\"S4.T2.st1.2.5.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.5.3.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T2.st1.2.5.3.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">MixStyle </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T2.st1.2.5.3.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">39</a><span id=\"S4.T2.st1.2.5.3.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T2.st1.2.5.3.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.5.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">55.87</span></td>\n<td id=\"S4.T2.st1.2.5.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.5.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">52.03</span></td>\n<td id=\"S4.T2.st1.2.5.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.5.3.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">71.10</span></td>\n<td id=\"S4.T2.st1.2.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.5.3.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.20</span></td>\n<td id=\"S4.T2.st1.2.5.3.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T2.st1.2.5.3.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">63.30</span></td>\n<td id=\"S4.T2.st1.2.5.3.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.5.3.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">95.20</span></td>\n<td id=\"S4.T2.st1.2.5.3.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.5.3.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">60.77</span></td>\n<td id=\"S4.T2.st1.2.5.3.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.5.3.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.90</span></td>\n<td id=\"S4.T2.st1.2.5.3.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.5.3.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">66.73</span></td>\n<td id=\"S4.T2.st1.2.5.3.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.5.3.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.15</span></td>\n</tr>\n<tr id=\"S4.T2.st1.2.6.4\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.6.4.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T2.st1.2.6.4.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">DSU </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T2.st1.2.6.4.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">21</a><span id=\"S4.T2.st1.2.6.4.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T2.st1.2.6.4.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.6.4.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">58.60</span></td>\n<td id=\"S4.T2.st1.2.6.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.6.4.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">52.80</span></td>\n<td id=\"S4.T2.st1.2.6.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.6.4.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">71.63</span></td>\n<td id=\"S4.T2.st1.2.6.4.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.6.4.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.00</span></td>\n<td id=\"S4.T2.st1.2.6.4.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T2.st1.2.6.4.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">64.26</span></td>\n<td id=\"S4.T2.st1.2.6.4.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.6.4.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">96.87</span></td>\n<td id=\"S4.T2.st1.2.6.4.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.6.4.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">60.23</span></td>\n<td id=\"S4.T2.st1.2.6.4.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.6.4.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.97</span></td>\n<td id=\"S4.T2.st1.2.6.4.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.6.4.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">68.97</span></td>\n<td id=\"S4.T2.st1.2.6.4.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.6.4.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.76</span></td>\n</tr>\n<tr id=\"S4.T2.st1.2.7.5\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.7.5.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T2.st1.2.7.5.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">CCST </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T2.st1.2.7.5.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">2</a><span id=\"S4.T2.st1.2.7.5.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T2.st1.2.7.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.7.5.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">52.2</span></td>\n<td id=\"S4.T2.st1.2.7.5.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.7.5.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">52.2</span></td>\n<td id=\"S4.T2.st1.2.7.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.7.5.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">70.6</span></td>\n<td id=\"S4.T2.st1.2.7.5.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.7.5.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.3</span></td>\n<td id=\"S4.T2.st1.2.7.5.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T2.st1.2.7.5.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">61.83</span></td>\n<td id=\"S4.T2.st1.2.7.5.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.7.5.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">96.70</span></td>\n<td id=\"S4.T2.st1.2.7.5.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.7.5.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">60.40</span></td>\n<td id=\"S4.T2.st1.2.7.5.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.7.5.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">71.40</span></td>\n<td id=\"S4.T2.st1.2.7.5.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.7.5.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">65.00</span></td>\n<td id=\"S4.T2.st1.2.7.5.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.7.5.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.38</span></td>\n</tr>\n<tr id=\"S4.T2.st1.2.8.6\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.8.6.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T2.st1.2.8.6.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedDG </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T2.st1.2.8.6.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">25</a><span id=\"S4.T2.st1.2.8.6.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T2.st1.2.8.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.8.6.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">57.9</span></td>\n<td id=\"S4.T2.st1.2.8.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.8.6.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">48.6</span></td>\n<td id=\"S4.T2.st1.2.8.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.8.6.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.2</span></td>\n<td id=\"S4.T2.st1.2.8.6.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.8.6.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">75.0</span></td>\n<td id=\"S4.T2.st1.2.8.6.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T2.st1.2.8.6.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">63.68</span></td>\n<td id=\"S4.T2.st1.2.8.6.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.8.6.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">96.2</span></td>\n<td id=\"S4.T2.st1.2.8.6.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.8.6.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">60.7</span></td>\n<td id=\"S4.T2.st1.2.8.6.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.8.6.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.4</span></td>\n<td id=\"S4.T2.st1.2.8.6.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.8.6.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">67.3</span></td>\n<td id=\"S4.T2.st1.2.8.6.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.8.6.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.15</span></td>\n</tr>\n<tr id=\"S4.T2.st1.2.9.7\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.9.7.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T2.st1.2.9.7.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedSR </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T2.st1.2.9.7.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">28</a><span id=\"S4.T2.st1.2.9.7.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T2.st1.2.9.7.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.9.7.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">58.1</span></td>\n<td id=\"S4.T2.st1.2.9.7.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.9.7.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">48.2</span></td>\n<td id=\"S4.T2.st1.2.9.7.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.9.7.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.5</span></td>\n<td id=\"S4.T2.st1.2.9.7.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.9.7.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">75.4</span></td>\n<td id=\"S4.T2.st1.2.9.7.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T2.st1.2.9.7.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">63.55</span></td>\n<td id=\"S4.T2.st1.2.9.7.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.9.7.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">92.60</span></td>\n<td id=\"S4.T2.st1.2.9.7.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.9.7.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">60.80</span></td>\n<td id=\"S4.T2.st1.2.9.7.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.9.7.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.15</span></td>\n<td id=\"S4.T2.st1.2.9.7.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.9.7.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">68.30</span></td>\n<td id=\"S4.T2.st1.2.9.7.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.9.7.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.46</span></td>\n</tr>\n<tr id=\"S4.T2.st1.2.10.8\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.10.8.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_rr\">\n<span id=\"S4.T2.st1.2.10.8.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">StableFDG</span><span id=\"S4.T2.st1.2.10.8.1.2\" class=\"ltx_text\" style=\"font-size:70%;\"> (ours)</span>\n</td>\n<td id=\"S4.T2.st1.2.10.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.st1.2.10.8.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">57.87</span></td>\n<td id=\"S4.T2.st1.2.10.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.st1.2.10.8.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">54.20</span></td>\n<td id=\"S4.T2.st1.2.10.8.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.st1.2.10.8.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.10</span></td>\n<td id=\"S4.T2.st1.2.10.8.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S4.T2.st1.2.10.8.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">75.00</span></td>\n<td id=\"S4.T2.st1.2.10.8.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_rr\"><span id=\"S4.T2.st1.2.10.8.6.1\" class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">65.04</span></td>\n<td id=\"S4.T2.st1.2.10.8.7\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.st1.2.10.8.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">98.50</span></td>\n<td id=\"S4.T2.st1.2.10.8.8\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.st1.2.10.8.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">60.07</span></td>\n<td id=\"S4.T2.st1.2.10.8.9\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.st1.2.10.8.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.40</span></td>\n<td id=\"S4.T2.st1.2.10.8.10\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S4.T2.st1.2.10.8.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">69.43</span></td>\n<td id=\"S4.T2.st1.2.10.8.11\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.st1.2.10.8.11.1\" class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">75.60</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Multi-domain data distribution. In Table 2, we report the results in a multi-domain data distribution scenario. Compared to the results in Table 1, most of the schemes achieve improved performance in Table 2. This is because each client has multiple source domains, and thus providing a better platform for each client model to gain generalization ability. The proposed StableFDG still performs the best, demonstrating the effectiveness of our style and attention based learning strategy for federated DG.",
            "Effect of each component. To see the effect of each component of StableFDG, in Table 3, we apply our style-based learning and attention-based learning one-by-one in a multi-domain data distribution setup using PACS. We compare our results with style-augmentation DG baselines, MixStyle [39] and DSU [21]. By applying only our style-based learning, StableFDG already outperforms prior style-augmentation methods.\nFurthermore, by adopting only one of the proposed components, our scheme performs better than all the baselines in Table 2. Additional ablation studies using other datasets are reported in Appendix.",
            "Performance with ResNet-50. In Table 4(b), we also conduct experiments using ResNet-50 on PACS dataset in the multi-domain data distribution scenario. Other settings are exactly the same as in Table 2. The results further confirm the advantage of StableFDG with larger models."
        ]
    },
    "S4.T2.st1": {
        "caption": "(a) Office-Home and VLCS datasets.",
        "table": "<table id=\"S4.T2.st1.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.st1.2.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.st1.2.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_border_rr ltx_border_tt\"></th>\n<th id=\"S4.T2.st1.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"5\"><span id=\"S4.T2.st1.2.1.1.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Office-Home</span></th>\n<th id=\"S4.T2.st1.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"5\"><span id=\"S4.T2.st1.2.1.1.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">VLCS</span></th>\n</tr>\n<tr id=\"S4.T2.st1.2.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.st1.2.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr\"><span id=\"S4.T2.st1.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Methods</span></th>\n<th id=\"S4.T2.st1.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">Art</span></th>\n<th id=\"S4.T2.st1.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">Clipart</span></th>\n<th id=\"S4.T2.st1.2.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">Product</span></th>\n<th id=\"S4.T2.st1.2.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">Real</span></th>\n<th id=\"S4.T2.st1.2.2.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">Avg.</span></th>\n<th id=\"S4.T2.st1.2.2.2.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">Caltech</span></th>\n<th id=\"S4.T2.st1.2.2.2.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">LabelMe</span></th>\n<th id=\"S4.T2.st1.2.2.2.9\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">Pascal</span></th>\n<th id=\"S4.T2.st1.2.2.2.10\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">Sun</span></th>\n<th id=\"S4.T2.st1.2.2.2.11\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T2.st1.2.2.2.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">Avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.st1.2.3.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">\n<span id=\"S4.T2.st1.2.3.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedAvg </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T2.st1.2.3.1.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">26</a><span id=\"S4.T2.st1.2.3.1.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T2.st1.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">57.70</span></td>\n<td id=\"S4.T2.st1.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">48.30</span></td>\n<td id=\"S4.T2.st1.2.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.87</span></td>\n<td id=\"S4.T2.st1.2.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">75.33</span></td>\n<td id=\"S4.T2.st1.2.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">63.55</span></td>\n<td id=\"S4.T2.st1.2.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">93.65</span></td>\n<td id=\"S4.T2.st1.2.3.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">61.10</span></td>\n<td id=\"S4.T2.st1.2.3.1.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.55</span></td>\n<td id=\"S4.T2.st1.2.3.1.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">65.40</span></td>\n<td id=\"S4.T2.st1.2.3.1.11\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.st1.2.3.1.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.18</span></td>\n</tr>\n<tr id=\"S4.T2.st1.2.4.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T2.st1.2.4.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedBN </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T2.st1.2.4.2.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">22</a><span id=\"S4.T2.st1.2.4.2.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T2.st1.2.4.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.4.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">57.07</span></td>\n<td id=\"S4.T2.st1.2.4.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.4.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">48.32</span></td>\n<td id=\"S4.T2.st1.2.4.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.4.2.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.31</span></td>\n<td id=\"S4.T2.st1.2.4.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.4.2.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.57</span></td>\n<td id=\"S4.T2.st1.2.4.2.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T2.st1.2.4.2.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">63.07</span></td>\n<td id=\"S4.T2.st1.2.4.2.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.4.2.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">94.34</span></td>\n<td id=\"S4.T2.st1.2.4.2.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.4.2.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">62.61</span></td>\n<td id=\"S4.T2.st1.2.4.2.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.4.2.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">69.89</span></td>\n<td id=\"S4.T2.st1.2.4.2.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.4.2.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">69.04</span></td>\n<td id=\"S4.T2.st1.2.4.2.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.4.2.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.97</span></td>\n</tr>\n<tr id=\"S4.T2.st1.2.5.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.5.3.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T2.st1.2.5.3.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">MixStyle </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T2.st1.2.5.3.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">39</a><span id=\"S4.T2.st1.2.5.3.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T2.st1.2.5.3.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.5.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">55.87</span></td>\n<td id=\"S4.T2.st1.2.5.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.5.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">52.03</span></td>\n<td id=\"S4.T2.st1.2.5.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.5.3.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">71.10</span></td>\n<td id=\"S4.T2.st1.2.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.5.3.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.20</span></td>\n<td id=\"S4.T2.st1.2.5.3.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T2.st1.2.5.3.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">63.30</span></td>\n<td id=\"S4.T2.st1.2.5.3.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.5.3.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">95.20</span></td>\n<td id=\"S4.T2.st1.2.5.3.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.5.3.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">60.77</span></td>\n<td id=\"S4.T2.st1.2.5.3.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.5.3.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.90</span></td>\n<td id=\"S4.T2.st1.2.5.3.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.5.3.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">66.73</span></td>\n<td id=\"S4.T2.st1.2.5.3.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.5.3.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.15</span></td>\n</tr>\n<tr id=\"S4.T2.st1.2.6.4\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.6.4.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T2.st1.2.6.4.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">DSU </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T2.st1.2.6.4.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">21</a><span id=\"S4.T2.st1.2.6.4.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T2.st1.2.6.4.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.6.4.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">58.60</span></td>\n<td id=\"S4.T2.st1.2.6.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.6.4.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">52.80</span></td>\n<td id=\"S4.T2.st1.2.6.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.6.4.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">71.63</span></td>\n<td id=\"S4.T2.st1.2.6.4.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.6.4.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.00</span></td>\n<td id=\"S4.T2.st1.2.6.4.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T2.st1.2.6.4.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">64.26</span></td>\n<td id=\"S4.T2.st1.2.6.4.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.6.4.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">96.87</span></td>\n<td id=\"S4.T2.st1.2.6.4.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.6.4.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">60.23</span></td>\n<td id=\"S4.T2.st1.2.6.4.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.6.4.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.97</span></td>\n<td id=\"S4.T2.st1.2.6.4.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.6.4.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">68.97</span></td>\n<td id=\"S4.T2.st1.2.6.4.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.6.4.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.76</span></td>\n</tr>\n<tr id=\"S4.T2.st1.2.7.5\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.7.5.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T2.st1.2.7.5.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">CCST </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T2.st1.2.7.5.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">2</a><span id=\"S4.T2.st1.2.7.5.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T2.st1.2.7.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.7.5.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">52.2</span></td>\n<td id=\"S4.T2.st1.2.7.5.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.7.5.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">52.2</span></td>\n<td id=\"S4.T2.st1.2.7.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.7.5.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">70.6</span></td>\n<td id=\"S4.T2.st1.2.7.5.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.7.5.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.3</span></td>\n<td id=\"S4.T2.st1.2.7.5.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T2.st1.2.7.5.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">61.83</span></td>\n<td id=\"S4.T2.st1.2.7.5.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.7.5.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">96.70</span></td>\n<td id=\"S4.T2.st1.2.7.5.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.7.5.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">60.40</span></td>\n<td id=\"S4.T2.st1.2.7.5.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.7.5.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">71.40</span></td>\n<td id=\"S4.T2.st1.2.7.5.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.7.5.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">65.00</span></td>\n<td id=\"S4.T2.st1.2.7.5.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.7.5.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.38</span></td>\n</tr>\n<tr id=\"S4.T2.st1.2.8.6\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.8.6.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T2.st1.2.8.6.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedDG </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T2.st1.2.8.6.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">25</a><span id=\"S4.T2.st1.2.8.6.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T2.st1.2.8.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.8.6.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">57.9</span></td>\n<td id=\"S4.T2.st1.2.8.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.8.6.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">48.6</span></td>\n<td id=\"S4.T2.st1.2.8.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.8.6.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.2</span></td>\n<td id=\"S4.T2.st1.2.8.6.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.8.6.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">75.0</span></td>\n<td id=\"S4.T2.st1.2.8.6.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T2.st1.2.8.6.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">63.68</span></td>\n<td id=\"S4.T2.st1.2.8.6.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.8.6.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">96.2</span></td>\n<td id=\"S4.T2.st1.2.8.6.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.8.6.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">60.7</span></td>\n<td id=\"S4.T2.st1.2.8.6.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.8.6.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.4</span></td>\n<td id=\"S4.T2.st1.2.8.6.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.8.6.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">67.3</span></td>\n<td id=\"S4.T2.st1.2.8.6.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.8.6.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.15</span></td>\n</tr>\n<tr id=\"S4.T2.st1.2.9.7\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.9.7.1\" class=\"ltx_td ltx_align_center ltx_border_rr\">\n<span id=\"S4.T2.st1.2.9.7.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedSR </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T2.st1.2.9.7.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">28</a><span id=\"S4.T2.st1.2.9.7.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"S4.T2.st1.2.9.7.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.9.7.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">58.1</span></td>\n<td id=\"S4.T2.st1.2.9.7.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.9.7.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">48.2</span></td>\n<td id=\"S4.T2.st1.2.9.7.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.9.7.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.5</span></td>\n<td id=\"S4.T2.st1.2.9.7.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.9.7.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">75.4</span></td>\n<td id=\"S4.T2.st1.2.9.7.6\" class=\"ltx_td ltx_align_center ltx_border_rr\"><span id=\"S4.T2.st1.2.9.7.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">63.55</span></td>\n<td id=\"S4.T2.st1.2.9.7.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.9.7.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">92.60</span></td>\n<td id=\"S4.T2.st1.2.9.7.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.9.7.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">60.80</span></td>\n<td id=\"S4.T2.st1.2.9.7.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.9.7.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">72.15</span></td>\n<td id=\"S4.T2.st1.2.9.7.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st1.2.9.7.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">68.30</span></td>\n<td id=\"S4.T2.st1.2.9.7.11\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st1.2.9.7.11.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.46</span></td>\n</tr>\n<tr id=\"S4.T2.st1.2.10.8\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.10.8.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_rr\">\n<span id=\"S4.T2.st1.2.10.8.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">StableFDG</span><span id=\"S4.T2.st1.2.10.8.1.2\" class=\"ltx_text\" style=\"font-size:70%;\"> (ours)</span>\n</td>\n<td id=\"S4.T2.st1.2.10.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.st1.2.10.8.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">57.87</span></td>\n<td id=\"S4.T2.st1.2.10.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.st1.2.10.8.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">54.20</span></td>\n<td id=\"S4.T2.st1.2.10.8.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.st1.2.10.8.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.10</span></td>\n<td id=\"S4.T2.st1.2.10.8.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S4.T2.st1.2.10.8.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">75.00</span></td>\n<td id=\"S4.T2.st1.2.10.8.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_rr\"><span id=\"S4.T2.st1.2.10.8.6.1\" class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">65.04</span></td>\n<td id=\"S4.T2.st1.2.10.8.7\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.st1.2.10.8.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">98.50</span></td>\n<td id=\"S4.T2.st1.2.10.8.8\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.st1.2.10.8.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">60.07</span></td>\n<td id=\"S4.T2.st1.2.10.8.9\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.st1.2.10.8.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.40</span></td>\n<td id=\"S4.T2.st1.2.10.8.10\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S4.T2.st1.2.10.8.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">69.43</span></td>\n<td id=\"S4.T2.st1.2.10.8.11\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.st1.2.10.8.11.1\" class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">75.60</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Traditional federated learning (FL) algorithms operate under the assumption that the data distributions at training (source domains) and testing (target domain) are the same. The fact that domain shifts often occur in practice necessitates equipping FL methods with a domain generalization (DG) capability. However, existing DG algorithms face fundamental challenges in FL setups due to the lack of samples/domains in each client’s local dataset. In this paper, we propose StableFDG, a style and attention based learning strategy for accomplishing federated domain generalization, introducing two key contributions. The first is style-based learning, which enables each client to explore novel styles beyond the original source domains in its local dataset, improving domain diversity based on the proposed style sharing, shifting, and exploration strategies. Our second contribution is an attention-based feature highlighter, which captures the similarities between the features of data samples in the same class, and emphasizes the important/common characteristics to better learn the domain-invariant characteristics of each class in data-poor FL scenarios. Experimental results show that StableFDG outperforms existing baselines on various DG benchmark datasets, demonstrating its efficacy.",
            "Contributions. In this paper, we propose StableFDG, a style and attention based learning strategy tailored to federated domain generalization. StableFDG tackles the fundamental challenges in federated DG that arise due to the lack of data/styles in each FL client, with two novel characteristics:",
            "The two suggested schemes work in a complementary fashion, each providing one necessary component for federated DG: our style-based learning\nimproves domain diversity, while the attention-based feature highlighter learns domain-invariant characteristics of each class. Experiments on various FL setups using DG benchmarks confirm the advantage of StableFDG over (i) the baselines that directly apply DG methods to FL and (ii) the baselines that are specifically designed for federated DG.",
            "Overview of approach. Fig. 1 provides an overview of the problem setup and our StableFDG algorithm. As in conventional FL, the training process consists of multiple global rounds, which we index t=1,2,…,T𝑡12…𝑇t=1,2,\\dots,T. In the beginning of round t𝑡t, a selected set of clients download the current global model 𝐰tsubscript𝐰𝑡\\mathbf{w}_{t} from the server. Before local training begins, each client n𝑛n computes its own style information Φn=[μn,σn,Σn​(μ),Σn​(σ)]subscriptΦ𝑛subscript𝜇𝑛subscript𝜎𝑛subscriptΣ𝑛𝜇subscriptΣ𝑛𝜎\\Phi_{n}=[\\mu_{n},\\sigma_{n},\\Sigma_{n}(\\mu),\\Sigma_{n}(\\sigma)] using its local dataset according to (2), which will be clarified soon. This information is sent to the server, and the server shares these information with other clients to compensate for the lack of styles or domains in each client.\nDuring the local update process, each client selectively shifts the styles of the original data in the mini-batch to the new style (received from the server) via adaptive instance normalization (AdaIN) [10], to improve domain diversity (inner box in Fig. 2(b)). After this style sharing and shifting process, each client performs style exploration via feature-level oversampling to further expose the model to novel styles beyond the current source domains of each client (outer box in Fig. 2(b)). Finally, at the output of the feature extractor, we apply our attention-based feature highlighter to extract common/important feature information within each class and emphasize them for better generalization (Fig. 3). When local updates are finished, the server aggregates the client models and proceeds to the next round.",
            "to avoid performance degradation when there is little commonality between key and query images. In other words, StableFDG takes advantage of both cross-attention and self-attention, enabling the model to extract and learn important characteristics across images (via cross-attention), and within the image (via self-attention). A more detailed analysis on (8) can be found in Appendix.",
            "Finally, we put together StableFDG. In each FL round, the clients first download the global model from the server and perform style sharing, shifting, and exploration according to 3.1, which are done in the early layers of CNNs where the style information is preserved. Then, at the output of the feature extractor, attention-based weighted averaging is applied according to Sec. 3.2. These two components have their own roles and work in a complementary fashion to handle the challenging DG problem in FL; our style-based strategy is effective in improving the domain diversity, while our attention-based method can directly capture the domain-invariant characteristics of each class. After the local update process, the server aggregates the client models and proceeds to the next round.",
            "Remark 4 (Computational complexity). The computational complexity of StableFDG depends on the oversampling size in Sec. 3.1 and the attention module size in Sec. 3.2, which could be controlled depending on the resource constraints of clients. We show in Sec. 4 that StableFDG achieves the state-of-the-art performance with (i) minimal oversampling size and (ii) negligible cost of attention module. A more detailed discussion on the computational complexity is in Appendix.",
            "Single-domain data distribution. Table 1 shows our results in a single-domain data distribution setup. We have the following observations. Compared to the previous results provided in the centralized DG works [39, 21], the performance of each method is generally lower. This is due to the limited numbers of styles and data samples in each FL client, which restricts the generalization performance of individual client models. It can be seen that most of the baselines perform better than FedAvg and FedBN that do not tackle the DG problem.\nThe proposed StableFDG achieves the best average accuracy for all benchmark datasets, where the gain is especially large in PACS having large shifts between domains. In contrast to our scheme, the prior works [2, 25, 28] targeting federated DG show marginal performance gains relative to FedAvg in our practical experimental setup with (i) more clients (which results in less data in each client) and (ii) partial client participations.",
            "Multi-domain data distribution. In Table 2, we report the results in a multi-domain data distribution scenario. Compared to the results in Table 1, most of the schemes achieve improved performance in Table 2. This is because each client has multiple source domains, and thus providing a better platform for each client model to gain generalization ability. The proposed StableFDG still performs the best, demonstrating the effectiveness of our style and attention based learning strategy for federated DG.",
            "Effect of each component. To see the effect of each component of StableFDG, in Table 3, we apply our style-based learning and attention-based learning one-by-one in a multi-domain data distribution setup using PACS. We compare our results with style-augmentation DG baselines, MixStyle [39] and DSU [21]. By applying only our style-based learning, StableFDG already outperforms prior style-augmentation methods.\nFurthermore, by adopting only one of the proposed components, our scheme performs better than all the baselines in Table 2. Additional ablation studies using other datasets are reported in Appendix.",
            "Effect of hyperparameters. In DG setups, it is generally impractical to tune the hyperparameter using the target domain, because there is no information on the target domain during training. Hence, we used a fixed exploration level α=3𝛼3\\alpha=3 throughout all experiments without tuning. In Fig. 4, we observe how the hyperparameters affect the target domain performance on PACS. In the first plot of Fig. 4, if α𝛼\\alpha is too small, the performance is relatively low since\nthe model is not able to explore novel styles\nbeyond the client’s source domains. If α𝛼\\alpha is too large, the performance could be slightly degraded because the model would explore too many redundant styles. The overall results show that StableFDG still performs better than the baselines with an arbitrarily chosen α𝛼\\alpha, which is a significant advantage of our scheme in the DG setup where hyperparameter tuning is challenging. The second plot of Fig. 4 shows how the oversampling size (introduced in Step 3 of Sec. 3.1) affects the DG performance. StableFDG still outperforms the baseline with minimal oversampling size,\nindicating that other components of our solution (style sharing/shifting and attention-based components) are already strong enough. The size of oversampling can be determined depending on the clients’ computation/memory constraints, with the cost of improved generalization.",
            "Performance in a centralized setup. Although our scheme is tailored to federated DG, the ideas of StableFDG can be also utilized in a centralized setup. In Table 4(a), we study the effects of our style and attention based strategies in a centralized DG setting using PACS, while the other settings are the same as in the FL setup. The results demonstrate that the proposed ideas are not only specific to data-poor FL scenarios but also have potentials to be utilized in centralized DG settings.",
            "Performance with ResNet-50. In Table 4(b), we also conduct experiments using ResNet-50 on PACS dataset in the multi-domain data distribution scenario. Other settings are exactly the same as in Table 2. The results further confirm the advantage of StableFDG with larger models.",
            "Despite the practical significance, the field of federated domain generalization is still in the early stage of research. In this paper, we proposed StableFDG, a new training strategy tailored to this unexplored area. Our style-based strategy enables the model to get exposed to various novel styles beyond each client’s source domains, while our attention-based method captures and emphasizes the important/common characteristics of each class. Extensive experimental results confirmed the advantage of our StableFDG for federated domain generalization with data-poor FL clients.",
            "Limitations and future works. StableFDG requires 0.45 % more communication load compared to FedAvg for sharing the attention module and style statistics, which is the cost for a better DG performance.\nFurther developing our idea to tailor to centralized DG and extending our attention\nstrategy to segmentation/detection DG tasks are also interesting directions for future research.",
            "To demonstrate the effectiveness of our StableFDG on a larger dataset, we performed experiments on DomainNet dataset in a single-domain data distribution scenario. To this end, we utilize ResNet-50 pretrained on ImageNet. The number of global rounds and mini-batch size are set to 60 and 64, respectively. The remaining settings are the same as those in our main paper. The results in Table 5 show that our StableFDG consistently outperforms not only the centralized DG works but also the prior works on federated DG even with a more complex dataset.",
            "Table 6 compares the communication, computation, and average accuracy of different schemes on PACS in a multi-domain data distribution scenario. ResNet-18 is adopted as in our main manuscript. We first compare the uplink communication load of each client in a specific global round. Compared to FedAvg that only transmits the model in each round, our scheme requires additional communication burden for transmitting the style statistics and the attention module, which are negligible. We also compare the computation time by measuring the time required for local update at each client using an GTX 1080 Ti GPU. CCST [2] and FedDG [25] require large computation due to the increased amounts of data samples or multiple backpropagations for meta training. Our scheme requires additional computation caused by style exploration, attention module update, etc., which are the costs for better generalization to the unseen domain.",
            "Different from the prior works [2, 28] for federated DG adopting the usual setting where the number of clients equals the number of source domains, in our main paper, we introduce a more practical experimental setting for federated DG where the source data is distributed to more clients than the number of source domains. For a comparison with them in the same setting, we also provide additional experimental results in the setup with number of clients = number of source domains. Table 7 shows the results on PACS and Office-Home datasets with three clients in a single-domain data distribution scenario. It is confirmed from the results that our StableFDG also achieves better performance compared to the existing works [2, 28] in this simple setting.",
            "As mentioned in the main manuscript, we provide further ablation studies on the effect of each component in StableFDG using VLCS dataset. Table 8 shows that each component individually brings performance gain compared to FedAvg. Using both strategies achieves greater performance gains, confirming that the two proposed schemes work in a complementary fashion.",
            "In our style based learning, style sharing among clients is performed at random. However, one can think of the strategy where client n𝑛n receives the style information Φn′subscriptΦsuperscript𝑛′\\Phi_{n^{\\prime}} that has the largest distance with its own style information Φn′subscriptΦsuperscript𝑛′\\Phi_{n^{\\prime}} in the style space. Table 9 shows the corresponding result using PACS dataset in a multi-domain data distribution setup. Interestingly, it can be seen that the random selection adopted in this paper performs better, since most of the users generally tend to receive the same style statistics when using the largest distance strategy.",
            "We also compare the effect of number of styles received at each client in Table 10 using PACS dataset in a multi-domain data distribution setup. The performance increases as the number of received styles increases, with small additional communication load (the vector length of the style information is 128, which is negligible compared to the number of model parameters, which is 11,180,103).",
            "In Section 3.1, we utilized the k𝑘k-means++ as a tool for facilitating our key idea in style shifting, which is to effectively balance between the original source domain and the new source domain for better generalization; k-means++ plays a role to select the B/2𝐵2B/2 styles that are similar to the remaining B/2𝐵2B/2 styles in the mini-batch. By doing so, the model can explore new styles while not losing the performance on the original styles. To see this effect, we compare k𝑘k-means++ vs. random sampling when selecting B/2 samples to be shifted, in Table 11. The results show that strategically selecting the B/2 samples to be shifted achieves better performance especially in the Sketch domain (1.89% gain) that has a large style gap with other domains. We believe that these results motivate and justify our design choice.",
            "In our main paper, we performed the class-balanced oversampling in the feature space to alleviate the class-imbalance issue during style exploration. To confirm the effectiveness of the class-balanced oversampling, we compare it with random oversampling under the same condition where only the style exploration is applied without other components. Table 12 shows the results on Office-Home dataset, where the class distribution is highly imbalanced in a multi-domain data distribution scenario. It can be seen that our class-balanced oversampling achieves higher performance over the simple random sampling, which validates the efficacy of mitigating the class imbalance problem in FL clients.",
            "We conduct additional ablation studies on the probability value (defined as p𝑝p here) utilized to control the operation of the style sharing/shifting and style exploration modules. Larger p𝑝p means that our scheme is more likely to be activated. In Table 13, we provide results on various probability values in a single-domain data distribution scenario using PACS dataset. From the results, it is confirm that for all p𝑝p values, the proposed StableFDG outperforms existing baselines, demonstrating that our scheme can work well with an arbitrarily chosen probability p𝑝p. In detail, when p𝑝p ranges from 0.3 to 0.7, the high performance is maintained while the performance decreases at both extreme probabilities (p=0.1𝑝0.1p=0.1 and 0.9). Therefore, it is recommended for practitioners to select the p𝑝p in an appropriate range, avoiding extreme cases.",
            "For implementation, style-based learning is applied only in the 1st, 2nd, 3rd blocks among 4 residual blocks in ResNet-18. Note that at the output of the 4th block, label information is dominant rather than style information, which results in degraded performance when style-based schemes are applied. This is confirmed by our new experiments in the table below. It can be seen from the results that if we consider the 4th residual block to apply our style-based learning, the performance gets degraded. This result confirms the intuition that style-based learning should be conducted at the earlier layers where style information is preserved.",
            "In our main paper, the similarity metric in equation (6) adopts cross-attention, while the metric in equation (8) combines cross-attention and self-attention. When applying only the cross-attention-based metric in equation (6), we found that the similarity value could become low even when the two samples belong to the same class, in special cases. We handled this issue by adding the self-attention component as in equation (8). Intuitively, by doing this, the attention module is learning to extract the important features across images (via cross-attention), and within the image (via self-attention). Table 15 compares the performance of our StableFDG when using (i) self-attention alone, (ii) cross-attention alone (equation (6)), and (iii) both self and cross attentions at the same time (equation (8)), confirming the advantage of using self-attention and cross-attention together.",
            "Our attention module requires 0.44% of additional model parameters to perform the attention-based learning. For a fair comparison to see the effect of our attention-based learning, we consider a different baseline with the same model size but without attention-based learning. Specifically, the baseline computes the attention score map using only additional convolutional operations and take the weighted average of the feature zisubscript𝑧𝑖z_{i} based on the attention score map. Table 16 shows the results using PACS dataset in a single-domain data distribution scenario. The results demonstrate that our attention-based learning achieves performance improvements on all four domains while playing a key role in capturing essential parts of the features.",
            "Now we provide answer to the following question: Instead of the FL setup we focused on, can attention provide benefits in the centralized DG setup? Table 17 shows the results with/without attention module in a centralized setup using PACS dataset. The results show that attention still provides performance improvements in the centralized setup by learning domain-invariant features, although the gain is slightly lower than the gain in the FL setup as shown in Table 3 of the main manuscript. These results indicate that the proposed attention-based learning indeed captures the domain-invariant characteristics of samples, while the scheme provides more benefits in the FL setup where each client is prone to overfitting due to lack of data.",
            "More detailed description on oversampling: Let sn∈ℝB×C×H×Wsuperscript𝑠𝑛superscriptℝ𝐵𝐶𝐻𝑊s^{n}\\in\\mathbb{R}^{B\\times C\\times H\\times W} be a mini-batch of features in client n𝑛n at a specific layer, obtained after Steps 1 and 2 in the main manuscript. Now given a fixed oversampling size, we oversample the features in the mini-batch to obtain s~nsuperscript~𝑠𝑛\\tilde{s}^{n}, so that the concatenated mini-batch s^n=[sn,s~n]superscript^𝑠𝑛superscript𝑠𝑛superscript~𝑠𝑛\\hat{s}^{n}=[s^{n},\\tilde{s}^{n}] becomes class-balanced as much as possible. Consider a toy example where the number of samples for classes a𝑎a, b𝑏b, c𝑐c are 3, 2, 1, respectively in the mini-batch snsuperscript𝑠𝑛s^{n}. In this example, if the oversampling size is 3, we randomly choose one data point from class b𝑏b\nand two data points from class c𝑐c (in this case, the same data point is selected for two times with duplication) to obtain s~nsuperscript~𝑠𝑛\\tilde{s}^{n}, so that the concatenated mini-batch s^n=[sn,s~n]superscript^𝑠𝑛superscript𝑠𝑛superscript~𝑠𝑛\\hat{s}^{n}=[s^{n},\\tilde{s}^{n}] becomes class-balanced. If the oversampling size is 1, we oversample one data point in class c𝑐c to make the concatenated mini-batch to be balanced as much as possible. If the oversampling size is 6, we oversample 1, 2, 3 samples from classes a𝑎a, b𝑏b, c𝑐c, respectively to construct s~nsuperscript~𝑠𝑛\\tilde{s}^{n}. The concatenated mini-batch s^n=[sn,s~n]superscript^𝑠𝑛superscript𝑠𝑛superscript~𝑠𝑛\\hat{s}^{n}=[s^{n},\\tilde{s}^{n}] is utilized for style-based learning and updating the model. This process not only mitigates the class-imbalance issue in each client but also provides a good platform for style exploration by oversampling the features. In our work, we reported the results with oversampling size of B𝐵B (which is equal to the mini-batch size), while the effect of the oversampling size is also reported in Fig. 4 of the main manuscript: A larger oversampling size leads to a better performance, and more importantly, our StableFDG outperforms the baseline even without any oversampling.",
            "Algorithm 1 summarizes the overall process of our StableFDG."
        ]
    },
    "S4.T2.st2": {
        "caption": "(b) PACS dataset.",
        "table": "<table id=\"S4.T2.st2.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.st2.2.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.st2.2.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_rr ltx_border_tt\"></th>\n<th id=\"S4.T2.st2.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T2.st2.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">Art</span></th>\n<th id=\"S4.T2.st2.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T2.st2.2.1.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">Cartoon</span></th>\n<th id=\"S4.T2.st2.2.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T2.st2.2.1.1.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">Photo</span></th>\n<th id=\"S4.T2.st2.2.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"S4.T2.st2.2.1.1.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">Sketch</span></th>\n<th id=\"S4.T2.st2.2.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T2.st2.2.1.1.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">Avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.st2.2.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.st2.2.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_t\">\n<span id=\"S4.T2.st2.2.2.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedAvg </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T2.st2.2.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">26</a><span id=\"S4.T2.st2.2.2.1.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</th>\n<td id=\"S4.T2.st2.2.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.st2.2.2.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.87</span></td>\n<td id=\"S4.T2.st2.2.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.st2.2.2.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.53</span></td>\n<td id=\"S4.T2.st2.2.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.st2.2.2.1.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">95.30</span></td>\n<td id=\"S4.T2.st2.2.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T2.st2.2.2.1.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">63.37</span></td>\n<td id=\"S4.T2.st2.2.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.st2.2.2.1.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">77.02</span></td>\n</tr>\n<tr id=\"S4.T2.st2.2.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.st2.2.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr\">\n<span id=\"S4.T2.st2.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedBN </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T2.st2.2.3.2.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">22</a><span id=\"S4.T2.st2.2.3.2.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</th>\n<td id=\"S4.T2.st2.2.3.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.3.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">77.00</span></td>\n<td id=\"S4.T2.st2.2.3.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.3.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">76.83</span></td>\n<td id=\"S4.T2.st2.2.3.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.3.2.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">95.45</span></td>\n<td id=\"S4.T2.st2.2.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st2.2.3.2.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">67.01</span></td>\n<td id=\"S4.T2.st2.2.3.2.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.3.2.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">79.07</span></td>\n</tr>\n<tr id=\"S4.T2.st2.2.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T2.st2.2.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr\">\n<span id=\"S4.T2.st2.2.4.3.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">MixStyle </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T2.st2.2.4.3.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">39</a><span id=\"S4.T2.st2.2.4.3.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</th>\n<td id=\"S4.T2.st2.2.4.3.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.4.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">80.07</span></td>\n<td id=\"S4.T2.st2.2.4.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.4.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">77.53</span></td>\n<td id=\"S4.T2.st2.2.4.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.4.3.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">96.23</span></td>\n<td id=\"S4.T2.st2.2.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st2.2.4.3.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">67.40</span></td>\n<td id=\"S4.T2.st2.2.4.3.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.4.3.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">80.31</span></td>\n</tr>\n<tr id=\"S4.T2.st2.2.5.4\" class=\"ltx_tr\">\n<th id=\"S4.T2.st2.2.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr\">\n<span id=\"S4.T2.st2.2.5.4.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">DSU </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T2.st2.2.5.4.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">21</a><span id=\"S4.T2.st2.2.5.4.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</th>\n<td id=\"S4.T2.st2.2.5.4.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.5.4.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">80.53</span></td>\n<td id=\"S4.T2.st2.2.5.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.5.4.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">76.30</span></td>\n<td id=\"S4.T2.st2.2.5.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.5.4.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">95.37</span></td>\n<td id=\"S4.T2.st2.2.5.4.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st2.2.5.4.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">70.93</span></td>\n<td id=\"S4.T2.st2.2.5.4.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.5.4.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">80.78</span></td>\n</tr>\n<tr id=\"S4.T2.st2.2.6.5\" class=\"ltx_tr\">\n<th id=\"S4.T2.st2.2.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr\">\n<span id=\"S4.T2.st2.2.6.5.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">CCST </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T2.st2.2.6.5.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">2</a><span id=\"S4.T2.st2.2.6.5.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</th>\n<td id=\"S4.T2.st2.2.6.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.6.5.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">75.53</span></td>\n<td id=\"S4.T2.st2.2.6.5.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.6.5.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">75.80</span></td>\n<td id=\"S4.T2.st2.2.6.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.6.5.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">93.53</span></td>\n<td id=\"S4.T2.st2.2.6.5.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st2.2.6.5.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">71.13</span></td>\n<td id=\"S4.T2.st2.2.6.5.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.6.5.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">79.00</span></td>\n</tr>\n<tr id=\"S4.T2.st2.2.7.6\" class=\"ltx_tr\">\n<th id=\"S4.T2.st2.2.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr\">\n<span id=\"S4.T2.st2.2.7.6.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedDG </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T2.st2.2.7.6.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">25</a><span id=\"S4.T2.st2.2.7.6.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</th>\n<td id=\"S4.T2.st2.2.7.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.7.6.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">75.35</span></td>\n<td id=\"S4.T2.st2.2.7.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.7.6.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">75.85</span></td>\n<td id=\"S4.T2.st2.2.7.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.7.6.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">95.65</span></td>\n<td id=\"S4.T2.st2.2.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st2.2.7.6.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">61.05</span></td>\n<td id=\"S4.T2.st2.2.7.6.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.7.6.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">76.98</span></td>\n</tr>\n<tr id=\"S4.T2.st2.2.8.7\" class=\"ltx_tr\">\n<th id=\"S4.T2.st2.2.8.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr\">\n<span id=\"S4.T2.st2.2.8.7.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FedSR </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T2.st2.2.8.7.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">28</a><span id=\"S4.T2.st2.2.8.7.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</th>\n<td id=\"S4.T2.st2.2.8.7.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.8.7.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">73.83</span></td>\n<td id=\"S4.T2.st2.2.8.7.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.8.7.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.83</span></td>\n<td id=\"S4.T2.st2.2.8.7.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.8.7.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">95.53</span></td>\n<td id=\"S4.T2.st2.2.8.7.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.st2.2.8.7.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">66.03</span></td>\n<td id=\"S4.T2.st2.2.8.7.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.st2.2.8.7.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">77.56</span></td>\n</tr>\n<tr id=\"S4.T2.st2.2.9.8\" class=\"ltx_tr\">\n<th id=\"S4.T2.st2.2.9.8.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_rr\">\n<span id=\"S4.T2.st2.2.9.8.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">StableFDG</span><span id=\"S4.T2.st2.2.9.8.1.2\" class=\"ltx_text\" style=\"font-size:70%;\"> (ours)</span>\n</th>\n<td id=\"S4.T2.st2.2.9.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.st2.2.9.8.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">83.97</span></td>\n<td id=\"S4.T2.st2.2.9.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.st2.2.9.8.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">79.10</span></td>\n<td id=\"S4.T2.st2.2.9.8.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.st2.2.9.8.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">96.27</span></td>\n<td id=\"S4.T2.st2.2.9.8.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S4.T2.st2.2.9.8.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">75.67</span></td>\n<td id=\"S4.T2.st2.2.9.8.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.st2.2.9.8.6.1\" class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">83.75</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Traditional federated learning (FL) algorithms operate under the assumption that the data distributions at training (source domains) and testing (target domain) are the same. The fact that domain shifts often occur in practice necessitates equipping FL methods with a domain generalization (DG) capability. However, existing DG algorithms face fundamental challenges in FL setups due to the lack of samples/domains in each client’s local dataset. In this paper, we propose StableFDG, a style and attention based learning strategy for accomplishing federated domain generalization, introducing two key contributions. The first is style-based learning, which enables each client to explore novel styles beyond the original source domains in its local dataset, improving domain diversity based on the proposed style sharing, shifting, and exploration strategies. Our second contribution is an attention-based feature highlighter, which captures the similarities between the features of data samples in the same class, and emphasizes the important/common characteristics to better learn the domain-invariant characteristics of each class in data-poor FL scenarios. Experimental results show that StableFDG outperforms existing baselines on various DG benchmark datasets, demonstrating its efficacy.",
            "Contributions. In this paper, we propose StableFDG, a style and attention based learning strategy tailored to federated domain generalization. StableFDG tackles the fundamental challenges in federated DG that arise due to the lack of data/styles in each FL client, with two novel characteristics:",
            "The two suggested schemes work in a complementary fashion, each providing one necessary component for federated DG: our style-based learning\nimproves domain diversity, while the attention-based feature highlighter learns domain-invariant characteristics of each class. Experiments on various FL setups using DG benchmarks confirm the advantage of StableFDG over (i) the baselines that directly apply DG methods to FL and (ii) the baselines that are specifically designed for federated DG.",
            "Overview of approach. Fig. 1 provides an overview of the problem setup and our StableFDG algorithm. As in conventional FL, the training process consists of multiple global rounds, which we index t=1,2,…,T𝑡12…𝑇t=1,2,\\dots,T. In the beginning of round t𝑡t, a selected set of clients download the current global model 𝐰tsubscript𝐰𝑡\\mathbf{w}_{t} from the server. Before local training begins, each client n𝑛n computes its own style information Φn=[μn,σn,Σn​(μ),Σn​(σ)]subscriptΦ𝑛subscript𝜇𝑛subscript𝜎𝑛subscriptΣ𝑛𝜇subscriptΣ𝑛𝜎\\Phi_{n}=[\\mu_{n},\\sigma_{n},\\Sigma_{n}(\\mu),\\Sigma_{n}(\\sigma)] using its local dataset according to (2), which will be clarified soon. This information is sent to the server, and the server shares these information with other clients to compensate for the lack of styles or domains in each client.\nDuring the local update process, each client selectively shifts the styles of the original data in the mini-batch to the new style (received from the server) via adaptive instance normalization (AdaIN) [10], to improve domain diversity (inner box in Fig. 2(b)). After this style sharing and shifting process, each client performs style exploration via feature-level oversampling to further expose the model to novel styles beyond the current source domains of each client (outer box in Fig. 2(b)). Finally, at the output of the feature extractor, we apply our attention-based feature highlighter to extract common/important feature information within each class and emphasize them for better generalization (Fig. 3). When local updates are finished, the server aggregates the client models and proceeds to the next round.",
            "to avoid performance degradation when there is little commonality between key and query images. In other words, StableFDG takes advantage of both cross-attention and self-attention, enabling the model to extract and learn important characteristics across images (via cross-attention), and within the image (via self-attention). A more detailed analysis on (8) can be found in Appendix.",
            "Finally, we put together StableFDG. In each FL round, the clients first download the global model from the server and perform style sharing, shifting, and exploration according to 3.1, which are done in the early layers of CNNs where the style information is preserved. Then, at the output of the feature extractor, attention-based weighted averaging is applied according to Sec. 3.2. These two components have their own roles and work in a complementary fashion to handle the challenging DG problem in FL; our style-based strategy is effective in improving the domain diversity, while our attention-based method can directly capture the domain-invariant characteristics of each class. After the local update process, the server aggregates the client models and proceeds to the next round.",
            "Remark 4 (Computational complexity). The computational complexity of StableFDG depends on the oversampling size in Sec. 3.1 and the attention module size in Sec. 3.2, which could be controlled depending on the resource constraints of clients. We show in Sec. 4 that StableFDG achieves the state-of-the-art performance with (i) minimal oversampling size and (ii) negligible cost of attention module. A more detailed discussion on the computational complexity is in Appendix.",
            "Single-domain data distribution. Table 1 shows our results in a single-domain data distribution setup. We have the following observations. Compared to the previous results provided in the centralized DG works [39, 21], the performance of each method is generally lower. This is due to the limited numbers of styles and data samples in each FL client, which restricts the generalization performance of individual client models. It can be seen that most of the baselines perform better than FedAvg and FedBN that do not tackle the DG problem.\nThe proposed StableFDG achieves the best average accuracy for all benchmark datasets, where the gain is especially large in PACS having large shifts between domains. In contrast to our scheme, the prior works [2, 25, 28] targeting federated DG show marginal performance gains relative to FedAvg in our practical experimental setup with (i) more clients (which results in less data in each client) and (ii) partial client participations.",
            "Multi-domain data distribution. In Table 2, we report the results in a multi-domain data distribution scenario. Compared to the results in Table 1, most of the schemes achieve improved performance in Table 2. This is because each client has multiple source domains, and thus providing a better platform for each client model to gain generalization ability. The proposed StableFDG still performs the best, demonstrating the effectiveness of our style and attention based learning strategy for federated DG.",
            "Effect of each component. To see the effect of each component of StableFDG, in Table 3, we apply our style-based learning and attention-based learning one-by-one in a multi-domain data distribution setup using PACS. We compare our results with style-augmentation DG baselines, MixStyle [39] and DSU [21]. By applying only our style-based learning, StableFDG already outperforms prior style-augmentation methods.\nFurthermore, by adopting only one of the proposed components, our scheme performs better than all the baselines in Table 2. Additional ablation studies using other datasets are reported in Appendix.",
            "Effect of hyperparameters. In DG setups, it is generally impractical to tune the hyperparameter using the target domain, because there is no information on the target domain during training. Hence, we used a fixed exploration level α=3𝛼3\\alpha=3 throughout all experiments without tuning. In Fig. 4, we observe how the hyperparameters affect the target domain performance on PACS. In the first plot of Fig. 4, if α𝛼\\alpha is too small, the performance is relatively low since\nthe model is not able to explore novel styles\nbeyond the client’s source domains. If α𝛼\\alpha is too large, the performance could be slightly degraded because the model would explore too many redundant styles. The overall results show that StableFDG still performs better than the baselines with an arbitrarily chosen α𝛼\\alpha, which is a significant advantage of our scheme in the DG setup where hyperparameter tuning is challenging. The second plot of Fig. 4 shows how the oversampling size (introduced in Step 3 of Sec. 3.1) affects the DG performance. StableFDG still outperforms the baseline with minimal oversampling size,\nindicating that other components of our solution (style sharing/shifting and attention-based components) are already strong enough. The size of oversampling can be determined depending on the clients’ computation/memory constraints, with the cost of improved generalization.",
            "Performance in a centralized setup. Although our scheme is tailored to federated DG, the ideas of StableFDG can be also utilized in a centralized setup. In Table 4(a), we study the effects of our style and attention based strategies in a centralized DG setting using PACS, while the other settings are the same as in the FL setup. The results demonstrate that the proposed ideas are not only specific to data-poor FL scenarios but also have potentials to be utilized in centralized DG settings.",
            "Performance with ResNet-50. In Table 4(b), we also conduct experiments using ResNet-50 on PACS dataset in the multi-domain data distribution scenario. Other settings are exactly the same as in Table 2. The results further confirm the advantage of StableFDG with larger models.",
            "Despite the practical significance, the field of federated domain generalization is still in the early stage of research. In this paper, we proposed StableFDG, a new training strategy tailored to this unexplored area. Our style-based strategy enables the model to get exposed to various novel styles beyond each client’s source domains, while our attention-based method captures and emphasizes the important/common characteristics of each class. Extensive experimental results confirmed the advantage of our StableFDG for federated domain generalization with data-poor FL clients.",
            "Limitations and future works. StableFDG requires 0.45 % more communication load compared to FedAvg for sharing the attention module and style statistics, which is the cost for a better DG performance.\nFurther developing our idea to tailor to centralized DG and extending our attention\nstrategy to segmentation/detection DG tasks are also interesting directions for future research.",
            "To demonstrate the effectiveness of our StableFDG on a larger dataset, we performed experiments on DomainNet dataset in a single-domain data distribution scenario. To this end, we utilize ResNet-50 pretrained on ImageNet. The number of global rounds and mini-batch size are set to 60 and 64, respectively. The remaining settings are the same as those in our main paper. The results in Table 5 show that our StableFDG consistently outperforms not only the centralized DG works but also the prior works on federated DG even with a more complex dataset.",
            "Table 6 compares the communication, computation, and average accuracy of different schemes on PACS in a multi-domain data distribution scenario. ResNet-18 is adopted as in our main manuscript. We first compare the uplink communication load of each client in a specific global round. Compared to FedAvg that only transmits the model in each round, our scheme requires additional communication burden for transmitting the style statistics and the attention module, which are negligible. We also compare the computation time by measuring the time required for local update at each client using an GTX 1080 Ti GPU. CCST [2] and FedDG [25] require large computation due to the increased amounts of data samples or multiple backpropagations for meta training. Our scheme requires additional computation caused by style exploration, attention module update, etc., which are the costs for better generalization to the unseen domain.",
            "Different from the prior works [2, 28] for federated DG adopting the usual setting where the number of clients equals the number of source domains, in our main paper, we introduce a more practical experimental setting for federated DG where the source data is distributed to more clients than the number of source domains. For a comparison with them in the same setting, we also provide additional experimental results in the setup with number of clients = number of source domains. Table 7 shows the results on PACS and Office-Home datasets with three clients in a single-domain data distribution scenario. It is confirmed from the results that our StableFDG also achieves better performance compared to the existing works [2, 28] in this simple setting.",
            "As mentioned in the main manuscript, we provide further ablation studies on the effect of each component in StableFDG using VLCS dataset. Table 8 shows that each component individually brings performance gain compared to FedAvg. Using both strategies achieves greater performance gains, confirming that the two proposed schemes work in a complementary fashion.",
            "In our style based learning, style sharing among clients is performed at random. However, one can think of the strategy where client n𝑛n receives the style information Φn′subscriptΦsuperscript𝑛′\\Phi_{n^{\\prime}} that has the largest distance with its own style information Φn′subscriptΦsuperscript𝑛′\\Phi_{n^{\\prime}} in the style space. Table 9 shows the corresponding result using PACS dataset in a multi-domain data distribution setup. Interestingly, it can be seen that the random selection adopted in this paper performs better, since most of the users generally tend to receive the same style statistics when using the largest distance strategy.",
            "We also compare the effect of number of styles received at each client in Table 10 using PACS dataset in a multi-domain data distribution setup. The performance increases as the number of received styles increases, with small additional communication load (the vector length of the style information is 128, which is negligible compared to the number of model parameters, which is 11,180,103).",
            "In Section 3.1, we utilized the k𝑘k-means++ as a tool for facilitating our key idea in style shifting, which is to effectively balance between the original source domain and the new source domain for better generalization; k-means++ plays a role to select the B/2𝐵2B/2 styles that are similar to the remaining B/2𝐵2B/2 styles in the mini-batch. By doing so, the model can explore new styles while not losing the performance on the original styles. To see this effect, we compare k𝑘k-means++ vs. random sampling when selecting B/2 samples to be shifted, in Table 11. The results show that strategically selecting the B/2 samples to be shifted achieves better performance especially in the Sketch domain (1.89% gain) that has a large style gap with other domains. We believe that these results motivate and justify our design choice.",
            "In our main paper, we performed the class-balanced oversampling in the feature space to alleviate the class-imbalance issue during style exploration. To confirm the effectiveness of the class-balanced oversampling, we compare it with random oversampling under the same condition where only the style exploration is applied without other components. Table 12 shows the results on Office-Home dataset, where the class distribution is highly imbalanced in a multi-domain data distribution scenario. It can be seen that our class-balanced oversampling achieves higher performance over the simple random sampling, which validates the efficacy of mitigating the class imbalance problem in FL clients.",
            "We conduct additional ablation studies on the probability value (defined as p𝑝p here) utilized to control the operation of the style sharing/shifting and style exploration modules. Larger p𝑝p means that our scheme is more likely to be activated. In Table 13, we provide results on various probability values in a single-domain data distribution scenario using PACS dataset. From the results, it is confirm that for all p𝑝p values, the proposed StableFDG outperforms existing baselines, demonstrating that our scheme can work well with an arbitrarily chosen probability p𝑝p. In detail, when p𝑝p ranges from 0.3 to 0.7, the high performance is maintained while the performance decreases at both extreme probabilities (p=0.1𝑝0.1p=0.1 and 0.9). Therefore, it is recommended for practitioners to select the p𝑝p in an appropriate range, avoiding extreme cases.",
            "For implementation, style-based learning is applied only in the 1st, 2nd, 3rd blocks among 4 residual blocks in ResNet-18. Note that at the output of the 4th block, label information is dominant rather than style information, which results in degraded performance when style-based schemes are applied. This is confirmed by our new experiments in the table below. It can be seen from the results that if we consider the 4th residual block to apply our style-based learning, the performance gets degraded. This result confirms the intuition that style-based learning should be conducted at the earlier layers where style information is preserved.",
            "In our main paper, the similarity metric in equation (6) adopts cross-attention, while the metric in equation (8) combines cross-attention and self-attention. When applying only the cross-attention-based metric in equation (6), we found that the similarity value could become low even when the two samples belong to the same class, in special cases. We handled this issue by adding the self-attention component as in equation (8). Intuitively, by doing this, the attention module is learning to extract the important features across images (via cross-attention), and within the image (via self-attention). Table 15 compares the performance of our StableFDG when using (i) self-attention alone, (ii) cross-attention alone (equation (6)), and (iii) both self and cross attentions at the same time (equation (8)), confirming the advantage of using self-attention and cross-attention together.",
            "Our attention module requires 0.44% of additional model parameters to perform the attention-based learning. For a fair comparison to see the effect of our attention-based learning, we consider a different baseline with the same model size but without attention-based learning. Specifically, the baseline computes the attention score map using only additional convolutional operations and take the weighted average of the feature zisubscript𝑧𝑖z_{i} based on the attention score map. Table 16 shows the results using PACS dataset in a single-domain data distribution scenario. The results demonstrate that our attention-based learning achieves performance improvements on all four domains while playing a key role in capturing essential parts of the features.",
            "Now we provide answer to the following question: Instead of the FL setup we focused on, can attention provide benefits in the centralized DG setup? Table 17 shows the results with/without attention module in a centralized setup using PACS dataset. The results show that attention still provides performance improvements in the centralized setup by learning domain-invariant features, although the gain is slightly lower than the gain in the FL setup as shown in Table 3 of the main manuscript. These results indicate that the proposed attention-based learning indeed captures the domain-invariant characteristics of samples, while the scheme provides more benefits in the FL setup where each client is prone to overfitting due to lack of data.",
            "More detailed description on oversampling: Let sn∈ℝB×C×H×Wsuperscript𝑠𝑛superscriptℝ𝐵𝐶𝐻𝑊s^{n}\\in\\mathbb{R}^{B\\times C\\times H\\times W} be a mini-batch of features in client n𝑛n at a specific layer, obtained after Steps 1 and 2 in the main manuscript. Now given a fixed oversampling size, we oversample the features in the mini-batch to obtain s~nsuperscript~𝑠𝑛\\tilde{s}^{n}, so that the concatenated mini-batch s^n=[sn,s~n]superscript^𝑠𝑛superscript𝑠𝑛superscript~𝑠𝑛\\hat{s}^{n}=[s^{n},\\tilde{s}^{n}] becomes class-balanced as much as possible. Consider a toy example where the number of samples for classes a𝑎a, b𝑏b, c𝑐c are 3, 2, 1, respectively in the mini-batch snsuperscript𝑠𝑛s^{n}. In this example, if the oversampling size is 3, we randomly choose one data point from class b𝑏b\nand two data points from class c𝑐c (in this case, the same data point is selected for two times with duplication) to obtain s~nsuperscript~𝑠𝑛\\tilde{s}^{n}, so that the concatenated mini-batch s^n=[sn,s~n]superscript^𝑠𝑛superscript𝑠𝑛superscript~𝑠𝑛\\hat{s}^{n}=[s^{n},\\tilde{s}^{n}] becomes class-balanced. If the oversampling size is 1, we oversample one data point in class c𝑐c to make the concatenated mini-batch to be balanced as much as possible. If the oversampling size is 6, we oversample 1, 2, 3 samples from classes a𝑎a, b𝑏b, c𝑐c, respectively to construct s~nsuperscript~𝑠𝑛\\tilde{s}^{n}. The concatenated mini-batch s^n=[sn,s~n]superscript^𝑠𝑛superscript𝑠𝑛superscript~𝑠𝑛\\hat{s}^{n}=[s^{n},\\tilde{s}^{n}] is utilized for style-based learning and updating the model. This process not only mitigates the class-imbalance issue in each client but also provides a good platform for style exploration by oversampling the features. In our work, we reported the results with oversampling size of B𝐵B (which is equal to the mini-batch size), while the effect of the oversampling size is also reported in Fig. 4 of the main manuscript: A larger oversampling size leads to a better performance, and more importantly, our StableFDG outperforms the baseline even without any oversampling.",
            "Algorithm 1 summarizes the overall process of our StableFDG."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Effect of each component of StableFDG.",
        "table": "<table id=\"S4.T3.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.2.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.2.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S4.T3.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">Methods</span></th>\n<th id=\"S4.T3.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T3.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">Art</span></th>\n<th id=\"S4.T3.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T3.2.1.1.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">Cartoon</span></th>\n<th id=\"S4.T3.2.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T3.2.1.1.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">Photo</span></th>\n<th id=\"S4.T3.2.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"S4.T3.2.1.1.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">Sketch</span></th>\n<th id=\"S4.T3.2.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T3.2.1.1.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">Avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.2.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.2.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">\n<span id=\"S4.T3.2.2.1.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">MixStyle </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T3.2.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">[</span><a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">39</a><span id=\"S4.T3.2.2.1.1.3.2\" class=\"ltx_text\" style=\"font-size:50%;\">]</span></cite>\n</th>\n<td id=\"S4.T3.2.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.2.2.1.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">80.07</span></td>\n<td id=\"S4.T3.2.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.2.2.1.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">77.53</span></td>\n<td id=\"S4.T3.2.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.2.2.1.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">96.23</span></td>\n<td id=\"S4.T3.2.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.2.2.1.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">67.40</span></td>\n<td id=\"S4.T3.2.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.2.2.1.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">80.31</span></td>\n</tr>\n<tr id=\"S4.T3.2.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.2.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">\n<span id=\"S4.T3.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">DSU </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T3.2.3.2.1.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">[</span><a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">21</a><span id=\"S4.T3.2.3.2.1.3.2\" class=\"ltx_text\" style=\"font-size:50%;\">]</span></cite>\n</th>\n<td id=\"S4.T3.2.3.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.2.3.2.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">80.53</span></td>\n<td id=\"S4.T3.2.3.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.2.3.2.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">76.30</span></td>\n<td id=\"S4.T3.2.3.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.2.3.2.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">95.37</span></td>\n<td id=\"S4.T3.2.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.2.3.2.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">70.93</span></td>\n<td id=\"S4.T3.2.3.2.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.2.3.2.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">80.78</span></td>\n</tr>\n<tr id=\"S4.T3.2.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T3.2.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S4.T3.2.4.3.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">StableFDG (only style)</span></th>\n<td id=\"S4.T3.2.4.3.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.2.4.3.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">82.62</span></td>\n<td id=\"S4.T3.2.4.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.2.4.3.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">79.01</span></td>\n<td id=\"S4.T3.2.4.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.2.4.3.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">95.57</span></td>\n<td id=\"S4.T3.2.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.2.4.3.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">74.47</span></td>\n<td id=\"S4.T3.2.4.3.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.2.4.3.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">82.92</span></td>\n</tr>\n<tr id=\"S4.T3.2.5.4\" class=\"ltx_tr\">\n<th id=\"S4.T3.2.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S4.T3.2.5.4.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">StableFDG (only attention)</span></th>\n<td id=\"S4.T3.2.5.4.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.2.5.4.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">79.98</span></td>\n<td id=\"S4.T3.2.5.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.2.5.4.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">78.58</span></td>\n<td id=\"S4.T3.2.5.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.2.5.4.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">95.75</span></td>\n<td id=\"S4.T3.2.5.4.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.2.5.4.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">71.35</span></td>\n<td id=\"S4.T3.2.5.4.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.2.5.4.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">81.41</span></td>\n</tr>\n<tr id=\"S4.T3.2.6.5\" class=\"ltx_tr\">\n<th id=\"S4.T3.2.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span id=\"S4.T3.2.6.5.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">StableFDG (both)</span></th>\n<td id=\"S4.T3.2.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T3.2.6.5.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">83.97</span></td>\n<td id=\"S4.T3.2.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T3.2.6.5.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">79.10</span></td>\n<td id=\"S4.T3.2.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T3.2.6.5.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">96.27</span></td>\n<td id=\"S4.T3.2.6.5.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S4.T3.2.6.5.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">75.67</span></td>\n<td id=\"S4.T3.2.6.5.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T3.2.6.5.6.1\" class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:50%;\">83.75</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Effect of each component. To see the effect of each component of StableFDG, in Table 3, we apply our style-based learning and attention-based learning one-by-one in a multi-domain data distribution setup using PACS. We compare our results with style-augmentation DG baselines, MixStyle [39] and DSU [21]. By applying only our style-based learning, StableFDG already outperforms prior style-augmentation methods.\nFurthermore, by adopting only one of the proposed components, our scheme performs better than all the baselines in Table 2. Additional ablation studies using other datasets are reported in Appendix.",
            "Now we provide answer to the following question: Instead of the FL setup we focused on, can attention provide benefits in the centralized DG setup? Table 17 shows the results with/without attention module in a centralized setup using PACS dataset. The results show that attention still provides performance improvements in the centralized setup by learning domain-invariant features, although the gain is slightly lower than the gain in the FL setup as shown in Table 3 of the main manuscript. These results indicate that the proposed attention-based learning indeed captures the domain-invariant characteristics of samples, while the scheme provides more benefits in the FL setup where each client is prone to overfitting due to lack of data."
        ]
    },
    "S4.T4": {
        "caption": "Table 4: The applicability of StableFDG in a centralized DG setup (Table 4(a)) and performance using a larger model (Table 4(b)) on the PACS dataset.",
        "table": "<table id=\"S4.T4.st1.2\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T4.st1.2.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.st1.2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S4.T4.st1.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">Methods</span></th>\n<th id=\"S4.T4.st1.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T4.st1.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">Art</span></th>\n<th id=\"S4.T4.st1.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T4.st1.2.1.1.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">Cartoon</span></th>\n<th id=\"S4.T4.st1.2.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T4.st1.2.1.1.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">Photo</span></th>\n<th id=\"S4.T4.st1.2.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"S4.T4.st1.2.1.1.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">Sketch</span></th>\n<th id=\"S4.T4.st1.2.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T4.st1.2.1.1.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">Avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T4.st1.2.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.st1.2.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">\n<span id=\"S4.T4.st1.2.2.1.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">MixStyle </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T4.st1.2.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">[</span><a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">39</a><span id=\"S4.T4.st1.2.2.1.1.3.2\" class=\"ltx_text\" style=\"font-size:50%;\">]</span></cite>\n</th>\n<td id=\"S4.T4.st1.2.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.st1.2.2.1.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">81.61</span></td>\n<td id=\"S4.T4.st1.2.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.st1.2.2.1.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">78.83</span></td>\n<td id=\"S4.T4.st1.2.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.st1.2.2.1.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">96.77</span></td>\n<td id=\"S4.T4.st1.2.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T4.st1.2.2.1.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">72.29</span></td>\n<td id=\"S4.T4.st1.2.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.st1.2.2.1.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">82.38</span></td>\n</tr>\n<tr id=\"S4.T4.st1.2.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T4.st1.2.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span id=\"S4.T4.st1.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">DSU </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T4.st1.2.3.2.1.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">[</span><a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">21</a><span id=\"S4.T4.st1.2.3.2.1.3.2\" class=\"ltx_text\" style=\"font-size:50%;\">]</span></cite>\n</th>\n<td id=\"S4.T4.st1.2.3.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.st1.2.3.2.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">78.84</span></td>\n<td id=\"S4.T4.st1.2.3.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.st1.2.3.2.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">79.56</span></td>\n<td id=\"S4.T4.st1.2.3.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.st1.2.3.2.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">95.21</span></td>\n<td id=\"S4.T4.st1.2.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T4.st1.2.3.2.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">79.39</span></td>\n<td id=\"S4.T4.st1.2.3.2.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.st1.2.3.2.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">83.25</span></td>\n</tr>\n<tr id=\"S4.T4.st1.2.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T4.st1.2.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">\n<span id=\"S4.T4.st1.2.4.3.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:50%;\">StableFDG</span><span id=\"S4.T4.st1.2.4.3.1.2\" class=\"ltx_text\" style=\"font-size:50%;\"> (ours)</span>\n</th>\n<td id=\"S4.T4.st1.2.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T4.st1.2.4.3.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">85.02</span></td>\n<td id=\"S4.T4.st1.2.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T4.st1.2.4.3.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">79.65</span></td>\n<td id=\"S4.T4.st1.2.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T4.st1.2.4.3.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">96.38</span></td>\n<td id=\"S4.T4.st1.2.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S4.T4.st1.2.4.3.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">78.35</span></td>\n<td id=\"S4.T4.st1.2.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T4.st1.2.4.3.6.1\" class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:50%;\">84.85</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Performance in a centralized setup. Although our scheme is tailored to federated DG, the ideas of StableFDG can be also utilized in a centralized setup. In Table 4(a), we study the effects of our style and attention based strategies in a centralized DG setting using PACS, while the other settings are the same as in the FL setup. The results demonstrate that the proposed ideas are not only specific to data-poor FL scenarios but also have potentials to be utilized in centralized DG settings.",
            "Performance with ResNet-50. In Table 4(b), we also conduct experiments using ResNet-50 on PACS dataset in the multi-domain data distribution scenario. Other settings are exactly the same as in Table 2. The results further confirm the advantage of StableFDG with larger models."
        ]
    },
    "S4.T4.st1": {
        "caption": "(a) Performance in a centralized DG setup.",
        "table": "<table id=\"S4.T4.st1.2\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T4.st1.2.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.st1.2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S4.T4.st1.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">Methods</span></th>\n<th id=\"S4.T4.st1.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T4.st1.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">Art</span></th>\n<th id=\"S4.T4.st1.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T4.st1.2.1.1.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">Cartoon</span></th>\n<th id=\"S4.T4.st1.2.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T4.st1.2.1.1.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">Photo</span></th>\n<th id=\"S4.T4.st1.2.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"S4.T4.st1.2.1.1.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">Sketch</span></th>\n<th id=\"S4.T4.st1.2.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T4.st1.2.1.1.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">Avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T4.st1.2.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.st1.2.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">\n<span id=\"S4.T4.st1.2.2.1.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">MixStyle </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T4.st1.2.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">[</span><a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">39</a><span id=\"S4.T4.st1.2.2.1.1.3.2\" class=\"ltx_text\" style=\"font-size:50%;\">]</span></cite>\n</th>\n<td id=\"S4.T4.st1.2.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.st1.2.2.1.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">81.61</span></td>\n<td id=\"S4.T4.st1.2.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.st1.2.2.1.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">78.83</span></td>\n<td id=\"S4.T4.st1.2.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.st1.2.2.1.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">96.77</span></td>\n<td id=\"S4.T4.st1.2.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T4.st1.2.2.1.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">72.29</span></td>\n<td id=\"S4.T4.st1.2.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.st1.2.2.1.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">82.38</span></td>\n</tr>\n<tr id=\"S4.T4.st1.2.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T4.st1.2.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span id=\"S4.T4.st1.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">DSU </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T4.st1.2.3.2.1.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">[</span><a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">21</a><span id=\"S4.T4.st1.2.3.2.1.3.2\" class=\"ltx_text\" style=\"font-size:50%;\">]</span></cite>\n</th>\n<td id=\"S4.T4.st1.2.3.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.st1.2.3.2.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">78.84</span></td>\n<td id=\"S4.T4.st1.2.3.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.st1.2.3.2.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">79.56</span></td>\n<td id=\"S4.T4.st1.2.3.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.st1.2.3.2.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">95.21</span></td>\n<td id=\"S4.T4.st1.2.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T4.st1.2.3.2.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">79.39</span></td>\n<td id=\"S4.T4.st1.2.3.2.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.st1.2.3.2.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">83.25</span></td>\n</tr>\n<tr id=\"S4.T4.st1.2.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T4.st1.2.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">\n<span id=\"S4.T4.st1.2.4.3.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:50%;\">StableFDG</span><span id=\"S4.T4.st1.2.4.3.1.2\" class=\"ltx_text\" style=\"font-size:50%;\"> (ours)</span>\n</th>\n<td id=\"S4.T4.st1.2.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T4.st1.2.4.3.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">85.02</span></td>\n<td id=\"S4.T4.st1.2.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T4.st1.2.4.3.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">79.65</span></td>\n<td id=\"S4.T4.st1.2.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T4.st1.2.4.3.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">96.38</span></td>\n<td id=\"S4.T4.st1.2.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S4.T4.st1.2.4.3.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">78.35</span></td>\n<td id=\"S4.T4.st1.2.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T4.st1.2.4.3.6.1\" class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:50%;\">84.85</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Traditional federated learning (FL) algorithms operate under the assumption that the data distributions at training (source domains) and testing (target domain) are the same. The fact that domain shifts often occur in practice necessitates equipping FL methods with a domain generalization (DG) capability. However, existing DG algorithms face fundamental challenges in FL setups due to the lack of samples/domains in each client’s local dataset. In this paper, we propose StableFDG, a style and attention based learning strategy for accomplishing federated domain generalization, introducing two key contributions. The first is style-based learning, which enables each client to explore novel styles beyond the original source domains in its local dataset, improving domain diversity based on the proposed style sharing, shifting, and exploration strategies. Our second contribution is an attention-based feature highlighter, which captures the similarities between the features of data samples in the same class, and emphasizes the important/common characteristics to better learn the domain-invariant characteristics of each class in data-poor FL scenarios. Experimental results show that StableFDG outperforms existing baselines on various DG benchmark datasets, demonstrating its efficacy.",
            "Contributions. In this paper, we propose StableFDG, a style and attention based learning strategy tailored to federated domain generalization. StableFDG tackles the fundamental challenges in federated DG that arise due to the lack of data/styles in each FL client, with two novel characteristics:",
            "The two suggested schemes work in a complementary fashion, each providing one necessary component for federated DG: our style-based learning\nimproves domain diversity, while the attention-based feature highlighter learns domain-invariant characteristics of each class. Experiments on various FL setups using DG benchmarks confirm the advantage of StableFDG over (i) the baselines that directly apply DG methods to FL and (ii) the baselines that are specifically designed for federated DG.",
            "Overview of approach. Fig. 1 provides an overview of the problem setup and our StableFDG algorithm. As in conventional FL, the training process consists of multiple global rounds, which we index t=1,2,…,T𝑡12…𝑇t=1,2,\\dots,T. In the beginning of round t𝑡t, a selected set of clients download the current global model 𝐰tsubscript𝐰𝑡\\mathbf{w}_{t} from the server. Before local training begins, each client n𝑛n computes its own style information Φn=[μn,σn,Σn​(μ),Σn​(σ)]subscriptΦ𝑛subscript𝜇𝑛subscript𝜎𝑛subscriptΣ𝑛𝜇subscriptΣ𝑛𝜎\\Phi_{n}=[\\mu_{n},\\sigma_{n},\\Sigma_{n}(\\mu),\\Sigma_{n}(\\sigma)] using its local dataset according to (2), which will be clarified soon. This information is sent to the server, and the server shares these information with other clients to compensate for the lack of styles or domains in each client.\nDuring the local update process, each client selectively shifts the styles of the original data in the mini-batch to the new style (received from the server) via adaptive instance normalization (AdaIN) [10], to improve domain diversity (inner box in Fig. 2(b)). After this style sharing and shifting process, each client performs style exploration via feature-level oversampling to further expose the model to novel styles beyond the current source domains of each client (outer box in Fig. 2(b)). Finally, at the output of the feature extractor, we apply our attention-based feature highlighter to extract common/important feature information within each class and emphasize them for better generalization (Fig. 3). When local updates are finished, the server aggregates the client models and proceeds to the next round.",
            "to avoid performance degradation when there is little commonality between key and query images. In other words, StableFDG takes advantage of both cross-attention and self-attention, enabling the model to extract and learn important characteristics across images (via cross-attention), and within the image (via self-attention). A more detailed analysis on (8) can be found in Appendix.",
            "Finally, we put together StableFDG. In each FL round, the clients first download the global model from the server and perform style sharing, shifting, and exploration according to 3.1, which are done in the early layers of CNNs where the style information is preserved. Then, at the output of the feature extractor, attention-based weighted averaging is applied according to Sec. 3.2. These two components have their own roles and work in a complementary fashion to handle the challenging DG problem in FL; our style-based strategy is effective in improving the domain diversity, while our attention-based method can directly capture the domain-invariant characteristics of each class. After the local update process, the server aggregates the client models and proceeds to the next round.",
            "Remark 4 (Computational complexity). The computational complexity of StableFDG depends on the oversampling size in Sec. 3.1 and the attention module size in Sec. 3.2, which could be controlled depending on the resource constraints of clients. We show in Sec. 4 that StableFDG achieves the state-of-the-art performance with (i) minimal oversampling size and (ii) negligible cost of attention module. A more detailed discussion on the computational complexity is in Appendix.",
            "Single-domain data distribution. Table 1 shows our results in a single-domain data distribution setup. We have the following observations. Compared to the previous results provided in the centralized DG works [39, 21], the performance of each method is generally lower. This is due to the limited numbers of styles and data samples in each FL client, which restricts the generalization performance of individual client models. It can be seen that most of the baselines perform better than FedAvg and FedBN that do not tackle the DG problem.\nThe proposed StableFDG achieves the best average accuracy for all benchmark datasets, where the gain is especially large in PACS having large shifts between domains. In contrast to our scheme, the prior works [2, 25, 28] targeting federated DG show marginal performance gains relative to FedAvg in our practical experimental setup with (i) more clients (which results in less data in each client) and (ii) partial client participations.",
            "Multi-domain data distribution. In Table 2, we report the results in a multi-domain data distribution scenario. Compared to the results in Table 1, most of the schemes achieve improved performance in Table 2. This is because each client has multiple source domains, and thus providing a better platform for each client model to gain generalization ability. The proposed StableFDG still performs the best, demonstrating the effectiveness of our style and attention based learning strategy for federated DG.",
            "Effect of each component. To see the effect of each component of StableFDG, in Table 3, we apply our style-based learning and attention-based learning one-by-one in a multi-domain data distribution setup using PACS. We compare our results with style-augmentation DG baselines, MixStyle [39] and DSU [21]. By applying only our style-based learning, StableFDG already outperforms prior style-augmentation methods.\nFurthermore, by adopting only one of the proposed components, our scheme performs better than all the baselines in Table 2. Additional ablation studies using other datasets are reported in Appendix.",
            "Effect of hyperparameters. In DG setups, it is generally impractical to tune the hyperparameter using the target domain, because there is no information on the target domain during training. Hence, we used a fixed exploration level α=3𝛼3\\alpha=3 throughout all experiments without tuning. In Fig. 4, we observe how the hyperparameters affect the target domain performance on PACS. In the first plot of Fig. 4, if α𝛼\\alpha is too small, the performance is relatively low since\nthe model is not able to explore novel styles\nbeyond the client’s source domains. If α𝛼\\alpha is too large, the performance could be slightly degraded because the model would explore too many redundant styles. The overall results show that StableFDG still performs better than the baselines with an arbitrarily chosen α𝛼\\alpha, which is a significant advantage of our scheme in the DG setup where hyperparameter tuning is challenging. The second plot of Fig. 4 shows how the oversampling size (introduced in Step 3 of Sec. 3.1) affects the DG performance. StableFDG still outperforms the baseline with minimal oversampling size,\nindicating that other components of our solution (style sharing/shifting and attention-based components) are already strong enough. The size of oversampling can be determined depending on the clients’ computation/memory constraints, with the cost of improved generalization.",
            "Performance in a centralized setup. Although our scheme is tailored to federated DG, the ideas of StableFDG can be also utilized in a centralized setup. In Table 4(a), we study the effects of our style and attention based strategies in a centralized DG setting using PACS, while the other settings are the same as in the FL setup. The results demonstrate that the proposed ideas are not only specific to data-poor FL scenarios but also have potentials to be utilized in centralized DG settings.",
            "Performance with ResNet-50. In Table 4(b), we also conduct experiments using ResNet-50 on PACS dataset in the multi-domain data distribution scenario. Other settings are exactly the same as in Table 2. The results further confirm the advantage of StableFDG with larger models.",
            "Despite the practical significance, the field of federated domain generalization is still in the early stage of research. In this paper, we proposed StableFDG, a new training strategy tailored to this unexplored area. Our style-based strategy enables the model to get exposed to various novel styles beyond each client’s source domains, while our attention-based method captures and emphasizes the important/common characteristics of each class. Extensive experimental results confirmed the advantage of our StableFDG for federated domain generalization with data-poor FL clients.",
            "Limitations and future works. StableFDG requires 0.45 % more communication load compared to FedAvg for sharing the attention module and style statistics, which is the cost for a better DG performance.\nFurther developing our idea to tailor to centralized DG and extending our attention\nstrategy to segmentation/detection DG tasks are also interesting directions for future research.",
            "To demonstrate the effectiveness of our StableFDG on a larger dataset, we performed experiments on DomainNet dataset in a single-domain data distribution scenario. To this end, we utilize ResNet-50 pretrained on ImageNet. The number of global rounds and mini-batch size are set to 60 and 64, respectively. The remaining settings are the same as those in our main paper. The results in Table 5 show that our StableFDG consistently outperforms not only the centralized DG works but also the prior works on federated DG even with a more complex dataset.",
            "Table 6 compares the communication, computation, and average accuracy of different schemes on PACS in a multi-domain data distribution scenario. ResNet-18 is adopted as in our main manuscript. We first compare the uplink communication load of each client in a specific global round. Compared to FedAvg that only transmits the model in each round, our scheme requires additional communication burden for transmitting the style statistics and the attention module, which are negligible. We also compare the computation time by measuring the time required for local update at each client using an GTX 1080 Ti GPU. CCST [2] and FedDG [25] require large computation due to the increased amounts of data samples or multiple backpropagations for meta training. Our scheme requires additional computation caused by style exploration, attention module update, etc., which are the costs for better generalization to the unseen domain.",
            "Different from the prior works [2, 28] for federated DG adopting the usual setting where the number of clients equals the number of source domains, in our main paper, we introduce a more practical experimental setting for federated DG where the source data is distributed to more clients than the number of source domains. For a comparison with them in the same setting, we also provide additional experimental results in the setup with number of clients = number of source domains. Table 7 shows the results on PACS and Office-Home datasets with three clients in a single-domain data distribution scenario. It is confirmed from the results that our StableFDG also achieves better performance compared to the existing works [2, 28] in this simple setting.",
            "As mentioned in the main manuscript, we provide further ablation studies on the effect of each component in StableFDG using VLCS dataset. Table 8 shows that each component individually brings performance gain compared to FedAvg. Using both strategies achieves greater performance gains, confirming that the two proposed schemes work in a complementary fashion.",
            "In our style based learning, style sharing among clients is performed at random. However, one can think of the strategy where client n𝑛n receives the style information Φn′subscriptΦsuperscript𝑛′\\Phi_{n^{\\prime}} that has the largest distance with its own style information Φn′subscriptΦsuperscript𝑛′\\Phi_{n^{\\prime}} in the style space. Table 9 shows the corresponding result using PACS dataset in a multi-domain data distribution setup. Interestingly, it can be seen that the random selection adopted in this paper performs better, since most of the users generally tend to receive the same style statistics when using the largest distance strategy.",
            "We also compare the effect of number of styles received at each client in Table 10 using PACS dataset in a multi-domain data distribution setup. The performance increases as the number of received styles increases, with small additional communication load (the vector length of the style information is 128, which is negligible compared to the number of model parameters, which is 11,180,103).",
            "In Section 3.1, we utilized the k𝑘k-means++ as a tool for facilitating our key idea in style shifting, which is to effectively balance between the original source domain and the new source domain for better generalization; k-means++ plays a role to select the B/2𝐵2B/2 styles that are similar to the remaining B/2𝐵2B/2 styles in the mini-batch. By doing so, the model can explore new styles while not losing the performance on the original styles. To see this effect, we compare k𝑘k-means++ vs. random sampling when selecting B/2 samples to be shifted, in Table 11. The results show that strategically selecting the B/2 samples to be shifted achieves better performance especially in the Sketch domain (1.89% gain) that has a large style gap with other domains. We believe that these results motivate and justify our design choice.",
            "In our main paper, we performed the class-balanced oversampling in the feature space to alleviate the class-imbalance issue during style exploration. To confirm the effectiveness of the class-balanced oversampling, we compare it with random oversampling under the same condition where only the style exploration is applied without other components. Table 12 shows the results on Office-Home dataset, where the class distribution is highly imbalanced in a multi-domain data distribution scenario. It can be seen that our class-balanced oversampling achieves higher performance over the simple random sampling, which validates the efficacy of mitigating the class imbalance problem in FL clients.",
            "We conduct additional ablation studies on the probability value (defined as p𝑝p here) utilized to control the operation of the style sharing/shifting and style exploration modules. Larger p𝑝p means that our scheme is more likely to be activated. In Table 13, we provide results on various probability values in a single-domain data distribution scenario using PACS dataset. From the results, it is confirm that for all p𝑝p values, the proposed StableFDG outperforms existing baselines, demonstrating that our scheme can work well with an arbitrarily chosen probability p𝑝p. In detail, when p𝑝p ranges from 0.3 to 0.7, the high performance is maintained while the performance decreases at both extreme probabilities (p=0.1𝑝0.1p=0.1 and 0.9). Therefore, it is recommended for practitioners to select the p𝑝p in an appropriate range, avoiding extreme cases.",
            "For implementation, style-based learning is applied only in the 1st, 2nd, 3rd blocks among 4 residual blocks in ResNet-18. Note that at the output of the 4th block, label information is dominant rather than style information, which results in degraded performance when style-based schemes are applied. This is confirmed by our new experiments in the table below. It can be seen from the results that if we consider the 4th residual block to apply our style-based learning, the performance gets degraded. This result confirms the intuition that style-based learning should be conducted at the earlier layers where style information is preserved.",
            "In our main paper, the similarity metric in equation (6) adopts cross-attention, while the metric in equation (8) combines cross-attention and self-attention. When applying only the cross-attention-based metric in equation (6), we found that the similarity value could become low even when the two samples belong to the same class, in special cases. We handled this issue by adding the self-attention component as in equation (8). Intuitively, by doing this, the attention module is learning to extract the important features across images (via cross-attention), and within the image (via self-attention). Table 15 compares the performance of our StableFDG when using (i) self-attention alone, (ii) cross-attention alone (equation (6)), and (iii) both self and cross attentions at the same time (equation (8)), confirming the advantage of using self-attention and cross-attention together.",
            "Our attention module requires 0.44% of additional model parameters to perform the attention-based learning. For a fair comparison to see the effect of our attention-based learning, we consider a different baseline with the same model size but without attention-based learning. Specifically, the baseline computes the attention score map using only additional convolutional operations and take the weighted average of the feature zisubscript𝑧𝑖z_{i} based on the attention score map. Table 16 shows the results using PACS dataset in a single-domain data distribution scenario. The results demonstrate that our attention-based learning achieves performance improvements on all four domains while playing a key role in capturing essential parts of the features.",
            "Now we provide answer to the following question: Instead of the FL setup we focused on, can attention provide benefits in the centralized DG setup? Table 17 shows the results with/without attention module in a centralized setup using PACS dataset. The results show that attention still provides performance improvements in the centralized setup by learning domain-invariant features, although the gain is slightly lower than the gain in the FL setup as shown in Table 3 of the main manuscript. These results indicate that the proposed attention-based learning indeed captures the domain-invariant characteristics of samples, while the scheme provides more benefits in the FL setup where each client is prone to overfitting due to lack of data.",
            "More detailed description on oversampling: Let sn∈ℝB×C×H×Wsuperscript𝑠𝑛superscriptℝ𝐵𝐶𝐻𝑊s^{n}\\in\\mathbb{R}^{B\\times C\\times H\\times W} be a mini-batch of features in client n𝑛n at a specific layer, obtained after Steps 1 and 2 in the main manuscript. Now given a fixed oversampling size, we oversample the features in the mini-batch to obtain s~nsuperscript~𝑠𝑛\\tilde{s}^{n}, so that the concatenated mini-batch s^n=[sn,s~n]superscript^𝑠𝑛superscript𝑠𝑛superscript~𝑠𝑛\\hat{s}^{n}=[s^{n},\\tilde{s}^{n}] becomes class-balanced as much as possible. Consider a toy example where the number of samples for classes a𝑎a, b𝑏b, c𝑐c are 3, 2, 1, respectively in the mini-batch snsuperscript𝑠𝑛s^{n}. In this example, if the oversampling size is 3, we randomly choose one data point from class b𝑏b\nand two data points from class c𝑐c (in this case, the same data point is selected for two times with duplication) to obtain s~nsuperscript~𝑠𝑛\\tilde{s}^{n}, so that the concatenated mini-batch s^n=[sn,s~n]superscript^𝑠𝑛superscript𝑠𝑛superscript~𝑠𝑛\\hat{s}^{n}=[s^{n},\\tilde{s}^{n}] becomes class-balanced. If the oversampling size is 1, we oversample one data point in class c𝑐c to make the concatenated mini-batch to be balanced as much as possible. If the oversampling size is 6, we oversample 1, 2, 3 samples from classes a𝑎a, b𝑏b, c𝑐c, respectively to construct s~nsuperscript~𝑠𝑛\\tilde{s}^{n}. The concatenated mini-batch s^n=[sn,s~n]superscript^𝑠𝑛superscript𝑠𝑛superscript~𝑠𝑛\\hat{s}^{n}=[s^{n},\\tilde{s}^{n}] is utilized for style-based learning and updating the model. This process not only mitigates the class-imbalance issue in each client but also provides a good platform for style exploration by oversampling the features. In our work, we reported the results with oversampling size of B𝐵B (which is equal to the mini-batch size), while the effect of the oversampling size is also reported in Fig. 4 of the main manuscript: A larger oversampling size leads to a better performance, and more importantly, our StableFDG outperforms the baseline even without any oversampling.",
            "Algorithm 1 summarizes the overall process of our StableFDG."
        ]
    },
    "S4.T4.st2": {
        "caption": "(b) Performance using ResNet-50. ",
        "table": "<table id=\"S4.T4.st2.2\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T4.st2.2.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.st2.2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S4.T4.st2.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">Methods</span></th>\n<th id=\"S4.T4.st2.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T4.st2.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">Art</span></th>\n<th id=\"S4.T4.st2.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T4.st2.2.1.1.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">Cartoon</span></th>\n<th id=\"S4.T4.st2.2.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T4.st2.2.1.1.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">Photo</span></th>\n<th id=\"S4.T4.st2.2.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"S4.T4.st2.2.1.1.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">Sketch</span></th>\n<th id=\"S4.T4.st2.2.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T4.st2.2.1.1.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">Avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T4.st2.2.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.st2.2.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">\n<span id=\"S4.T4.st2.2.2.1.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">MixStyle </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T4.st2.2.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">[</span><a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">39</a><span id=\"S4.T4.st2.2.2.1.1.3.2\" class=\"ltx_text\" style=\"font-size:50%;\">]</span></cite>\n</th>\n<td id=\"S4.T4.st2.2.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.st2.2.2.1.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">87.51</span></td>\n<td id=\"S4.T4.st2.2.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.st2.2.2.1.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">81.12</span></td>\n<td id=\"S4.T4.st2.2.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.st2.2.2.1.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">97.48</span></td>\n<td id=\"S4.T4.st2.2.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T4.st2.2.2.1.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">68.39</span></td>\n<td id=\"S4.T4.st2.2.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.st2.2.2.1.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">83.63</span></td>\n</tr>\n<tr id=\"S4.T4.st2.2.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T4.st2.2.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span id=\"S4.T4.st2.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">DSU </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"S4.T4.st2.2.3.2.1.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">[</span><a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">21</a><span id=\"S4.T4.st2.2.3.2.1.3.2\" class=\"ltx_text\" style=\"font-size:50%;\">]</span></cite>\n</th>\n<td id=\"S4.T4.st2.2.3.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.st2.2.3.2.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">86.48</span></td>\n<td id=\"S4.T4.st2.2.3.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.st2.2.3.2.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">81.22</span></td>\n<td id=\"S4.T4.st2.2.3.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.st2.2.3.2.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">97.21</span></td>\n<td id=\"S4.T4.st2.2.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T4.st2.2.3.2.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">73.99</span></td>\n<td id=\"S4.T4.st2.2.3.2.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.st2.2.3.2.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">84.73</span></td>\n</tr>\n<tr id=\"S4.T4.st2.2.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T4.st2.2.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">\n<span id=\"S4.T4.st2.2.4.3.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:50%;\">StableFDG</span><span id=\"S4.T4.st2.2.4.3.1.2\" class=\"ltx_text\" style=\"font-size:50%;\"> (ours)</span>\n</th>\n<td id=\"S4.T4.st2.2.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T4.st2.2.4.3.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">90.01</span></td>\n<td id=\"S4.T4.st2.2.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T4.st2.2.4.3.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">83.29</span></td>\n<td id=\"S4.T4.st2.2.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T4.st2.2.4.3.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">98.02</span></td>\n<td id=\"S4.T4.st2.2.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S4.T4.st2.2.4.3.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">79.47</span></td>\n<td id=\"S4.T4.st2.2.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T4.st2.2.4.3.6.1\" class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:50%;\">87.70</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Traditional federated learning (FL) algorithms operate under the assumption that the data distributions at training (source domains) and testing (target domain) are the same. The fact that domain shifts often occur in practice necessitates equipping FL methods with a domain generalization (DG) capability. However, existing DG algorithms face fundamental challenges in FL setups due to the lack of samples/domains in each client’s local dataset. In this paper, we propose StableFDG, a style and attention based learning strategy for accomplishing federated domain generalization, introducing two key contributions. The first is style-based learning, which enables each client to explore novel styles beyond the original source domains in its local dataset, improving domain diversity based on the proposed style sharing, shifting, and exploration strategies. Our second contribution is an attention-based feature highlighter, which captures the similarities between the features of data samples in the same class, and emphasizes the important/common characteristics to better learn the domain-invariant characteristics of each class in data-poor FL scenarios. Experimental results show that StableFDG outperforms existing baselines on various DG benchmark datasets, demonstrating its efficacy.",
            "Contributions. In this paper, we propose StableFDG, a style and attention based learning strategy tailored to federated domain generalization. StableFDG tackles the fundamental challenges in federated DG that arise due to the lack of data/styles in each FL client, with two novel characteristics:",
            "The two suggested schemes work in a complementary fashion, each providing one necessary component for federated DG: our style-based learning\nimproves domain diversity, while the attention-based feature highlighter learns domain-invariant characteristics of each class. Experiments on various FL setups using DG benchmarks confirm the advantage of StableFDG over (i) the baselines that directly apply DG methods to FL and (ii) the baselines that are specifically designed for federated DG.",
            "Overview of approach. Fig. 1 provides an overview of the problem setup and our StableFDG algorithm. As in conventional FL, the training process consists of multiple global rounds, which we index t=1,2,…,T𝑡12…𝑇t=1,2,\\dots,T. In the beginning of round t𝑡t, a selected set of clients download the current global model 𝐰tsubscript𝐰𝑡\\mathbf{w}_{t} from the server. Before local training begins, each client n𝑛n computes its own style information Φn=[μn,σn,Σn​(μ),Σn​(σ)]subscriptΦ𝑛subscript𝜇𝑛subscript𝜎𝑛subscriptΣ𝑛𝜇subscriptΣ𝑛𝜎\\Phi_{n}=[\\mu_{n},\\sigma_{n},\\Sigma_{n}(\\mu),\\Sigma_{n}(\\sigma)] using its local dataset according to (2), which will be clarified soon. This information is sent to the server, and the server shares these information with other clients to compensate for the lack of styles or domains in each client.\nDuring the local update process, each client selectively shifts the styles of the original data in the mini-batch to the new style (received from the server) via adaptive instance normalization (AdaIN) [10], to improve domain diversity (inner box in Fig. 2(b)). After this style sharing and shifting process, each client performs style exploration via feature-level oversampling to further expose the model to novel styles beyond the current source domains of each client (outer box in Fig. 2(b)). Finally, at the output of the feature extractor, we apply our attention-based feature highlighter to extract common/important feature information within each class and emphasize them for better generalization (Fig. 3). When local updates are finished, the server aggregates the client models and proceeds to the next round.",
            "to avoid performance degradation when there is little commonality between key and query images. In other words, StableFDG takes advantage of both cross-attention and self-attention, enabling the model to extract and learn important characteristics across images (via cross-attention), and within the image (via self-attention). A more detailed analysis on (8) can be found in Appendix.",
            "Finally, we put together StableFDG. In each FL round, the clients first download the global model from the server and perform style sharing, shifting, and exploration according to 3.1, which are done in the early layers of CNNs where the style information is preserved. Then, at the output of the feature extractor, attention-based weighted averaging is applied according to Sec. 3.2. These two components have their own roles and work in a complementary fashion to handle the challenging DG problem in FL; our style-based strategy is effective in improving the domain diversity, while our attention-based method can directly capture the domain-invariant characteristics of each class. After the local update process, the server aggregates the client models and proceeds to the next round.",
            "Remark 4 (Computational complexity). The computational complexity of StableFDG depends on the oversampling size in Sec. 3.1 and the attention module size in Sec. 3.2, which could be controlled depending on the resource constraints of clients. We show in Sec. 4 that StableFDG achieves the state-of-the-art performance with (i) minimal oversampling size and (ii) negligible cost of attention module. A more detailed discussion on the computational complexity is in Appendix.",
            "Single-domain data distribution. Table 1 shows our results in a single-domain data distribution setup. We have the following observations. Compared to the previous results provided in the centralized DG works [39, 21], the performance of each method is generally lower. This is due to the limited numbers of styles and data samples in each FL client, which restricts the generalization performance of individual client models. It can be seen that most of the baselines perform better than FedAvg and FedBN that do not tackle the DG problem.\nThe proposed StableFDG achieves the best average accuracy for all benchmark datasets, where the gain is especially large in PACS having large shifts between domains. In contrast to our scheme, the prior works [2, 25, 28] targeting federated DG show marginal performance gains relative to FedAvg in our practical experimental setup with (i) more clients (which results in less data in each client) and (ii) partial client participations.",
            "Multi-domain data distribution. In Table 2, we report the results in a multi-domain data distribution scenario. Compared to the results in Table 1, most of the schemes achieve improved performance in Table 2. This is because each client has multiple source domains, and thus providing a better platform for each client model to gain generalization ability. The proposed StableFDG still performs the best, demonstrating the effectiveness of our style and attention based learning strategy for federated DG.",
            "Effect of each component. To see the effect of each component of StableFDG, in Table 3, we apply our style-based learning and attention-based learning one-by-one in a multi-domain data distribution setup using PACS. We compare our results with style-augmentation DG baselines, MixStyle [39] and DSU [21]. By applying only our style-based learning, StableFDG already outperforms prior style-augmentation methods.\nFurthermore, by adopting only one of the proposed components, our scheme performs better than all the baselines in Table 2. Additional ablation studies using other datasets are reported in Appendix.",
            "Effect of hyperparameters. In DG setups, it is generally impractical to tune the hyperparameter using the target domain, because there is no information on the target domain during training. Hence, we used a fixed exploration level α=3𝛼3\\alpha=3 throughout all experiments without tuning. In Fig. 4, we observe how the hyperparameters affect the target domain performance on PACS. In the first plot of Fig. 4, if α𝛼\\alpha is too small, the performance is relatively low since\nthe model is not able to explore novel styles\nbeyond the client’s source domains. If α𝛼\\alpha is too large, the performance could be slightly degraded because the model would explore too many redundant styles. The overall results show that StableFDG still performs better than the baselines with an arbitrarily chosen α𝛼\\alpha, which is a significant advantage of our scheme in the DG setup where hyperparameter tuning is challenging. The second plot of Fig. 4 shows how the oversampling size (introduced in Step 3 of Sec. 3.1) affects the DG performance. StableFDG still outperforms the baseline with minimal oversampling size,\nindicating that other components of our solution (style sharing/shifting and attention-based components) are already strong enough. The size of oversampling can be determined depending on the clients’ computation/memory constraints, with the cost of improved generalization.",
            "Performance in a centralized setup. Although our scheme is tailored to federated DG, the ideas of StableFDG can be also utilized in a centralized setup. In Table 4(a), we study the effects of our style and attention based strategies in a centralized DG setting using PACS, while the other settings are the same as in the FL setup. The results demonstrate that the proposed ideas are not only specific to data-poor FL scenarios but also have potentials to be utilized in centralized DG settings.",
            "Performance with ResNet-50. In Table 4(b), we also conduct experiments using ResNet-50 on PACS dataset in the multi-domain data distribution scenario. Other settings are exactly the same as in Table 2. The results further confirm the advantage of StableFDG with larger models.",
            "Despite the practical significance, the field of federated domain generalization is still in the early stage of research. In this paper, we proposed StableFDG, a new training strategy tailored to this unexplored area. Our style-based strategy enables the model to get exposed to various novel styles beyond each client’s source domains, while our attention-based method captures and emphasizes the important/common characteristics of each class. Extensive experimental results confirmed the advantage of our StableFDG for federated domain generalization with data-poor FL clients.",
            "Limitations and future works. StableFDG requires 0.45 % more communication load compared to FedAvg for sharing the attention module and style statistics, which is the cost for a better DG performance.\nFurther developing our idea to tailor to centralized DG and extending our attention\nstrategy to segmentation/detection DG tasks are also interesting directions for future research.",
            "To demonstrate the effectiveness of our StableFDG on a larger dataset, we performed experiments on DomainNet dataset in a single-domain data distribution scenario. To this end, we utilize ResNet-50 pretrained on ImageNet. The number of global rounds and mini-batch size are set to 60 and 64, respectively. The remaining settings are the same as those in our main paper. The results in Table 5 show that our StableFDG consistently outperforms not only the centralized DG works but also the prior works on federated DG even with a more complex dataset.",
            "Table 6 compares the communication, computation, and average accuracy of different schemes on PACS in a multi-domain data distribution scenario. ResNet-18 is adopted as in our main manuscript. We first compare the uplink communication load of each client in a specific global round. Compared to FedAvg that only transmits the model in each round, our scheme requires additional communication burden for transmitting the style statistics and the attention module, which are negligible. We also compare the computation time by measuring the time required for local update at each client using an GTX 1080 Ti GPU. CCST [2] and FedDG [25] require large computation due to the increased amounts of data samples or multiple backpropagations for meta training. Our scheme requires additional computation caused by style exploration, attention module update, etc., which are the costs for better generalization to the unseen domain.",
            "Different from the prior works [2, 28] for federated DG adopting the usual setting where the number of clients equals the number of source domains, in our main paper, we introduce a more practical experimental setting for federated DG where the source data is distributed to more clients than the number of source domains. For a comparison with them in the same setting, we also provide additional experimental results in the setup with number of clients = number of source domains. Table 7 shows the results on PACS and Office-Home datasets with three clients in a single-domain data distribution scenario. It is confirmed from the results that our StableFDG also achieves better performance compared to the existing works [2, 28] in this simple setting.",
            "As mentioned in the main manuscript, we provide further ablation studies on the effect of each component in StableFDG using VLCS dataset. Table 8 shows that each component individually brings performance gain compared to FedAvg. Using both strategies achieves greater performance gains, confirming that the two proposed schemes work in a complementary fashion.",
            "In our style based learning, style sharing among clients is performed at random. However, one can think of the strategy where client n𝑛n receives the style information Φn′subscriptΦsuperscript𝑛′\\Phi_{n^{\\prime}} that has the largest distance with its own style information Φn′subscriptΦsuperscript𝑛′\\Phi_{n^{\\prime}} in the style space. Table 9 shows the corresponding result using PACS dataset in a multi-domain data distribution setup. Interestingly, it can be seen that the random selection adopted in this paper performs better, since most of the users generally tend to receive the same style statistics when using the largest distance strategy.",
            "We also compare the effect of number of styles received at each client in Table 10 using PACS dataset in a multi-domain data distribution setup. The performance increases as the number of received styles increases, with small additional communication load (the vector length of the style information is 128, which is negligible compared to the number of model parameters, which is 11,180,103).",
            "In Section 3.1, we utilized the k𝑘k-means++ as a tool for facilitating our key idea in style shifting, which is to effectively balance between the original source domain and the new source domain for better generalization; k-means++ plays a role to select the B/2𝐵2B/2 styles that are similar to the remaining B/2𝐵2B/2 styles in the mini-batch. By doing so, the model can explore new styles while not losing the performance on the original styles. To see this effect, we compare k𝑘k-means++ vs. random sampling when selecting B/2 samples to be shifted, in Table 11. The results show that strategically selecting the B/2 samples to be shifted achieves better performance especially in the Sketch domain (1.89% gain) that has a large style gap with other domains. We believe that these results motivate and justify our design choice.",
            "In our main paper, we performed the class-balanced oversampling in the feature space to alleviate the class-imbalance issue during style exploration. To confirm the effectiveness of the class-balanced oversampling, we compare it with random oversampling under the same condition where only the style exploration is applied without other components. Table 12 shows the results on Office-Home dataset, where the class distribution is highly imbalanced in a multi-domain data distribution scenario. It can be seen that our class-balanced oversampling achieves higher performance over the simple random sampling, which validates the efficacy of mitigating the class imbalance problem in FL clients.",
            "We conduct additional ablation studies on the probability value (defined as p𝑝p here) utilized to control the operation of the style sharing/shifting and style exploration modules. Larger p𝑝p means that our scheme is more likely to be activated. In Table 13, we provide results on various probability values in a single-domain data distribution scenario using PACS dataset. From the results, it is confirm that for all p𝑝p values, the proposed StableFDG outperforms existing baselines, demonstrating that our scheme can work well with an arbitrarily chosen probability p𝑝p. In detail, when p𝑝p ranges from 0.3 to 0.7, the high performance is maintained while the performance decreases at both extreme probabilities (p=0.1𝑝0.1p=0.1 and 0.9). Therefore, it is recommended for practitioners to select the p𝑝p in an appropriate range, avoiding extreme cases.",
            "For implementation, style-based learning is applied only in the 1st, 2nd, 3rd blocks among 4 residual blocks in ResNet-18. Note that at the output of the 4th block, label information is dominant rather than style information, which results in degraded performance when style-based schemes are applied. This is confirmed by our new experiments in the table below. It can be seen from the results that if we consider the 4th residual block to apply our style-based learning, the performance gets degraded. This result confirms the intuition that style-based learning should be conducted at the earlier layers where style information is preserved.",
            "In our main paper, the similarity metric in equation (6) adopts cross-attention, while the metric in equation (8) combines cross-attention and self-attention. When applying only the cross-attention-based metric in equation (6), we found that the similarity value could become low even when the two samples belong to the same class, in special cases. We handled this issue by adding the self-attention component as in equation (8). Intuitively, by doing this, the attention module is learning to extract the important features across images (via cross-attention), and within the image (via self-attention). Table 15 compares the performance of our StableFDG when using (i) self-attention alone, (ii) cross-attention alone (equation (6)), and (iii) both self and cross attentions at the same time (equation (8)), confirming the advantage of using self-attention and cross-attention together.",
            "Our attention module requires 0.44% of additional model parameters to perform the attention-based learning. For a fair comparison to see the effect of our attention-based learning, we consider a different baseline with the same model size but without attention-based learning. Specifically, the baseline computes the attention score map using only additional convolutional operations and take the weighted average of the feature zisubscript𝑧𝑖z_{i} based on the attention score map. Table 16 shows the results using PACS dataset in a single-domain data distribution scenario. The results demonstrate that our attention-based learning achieves performance improvements on all four domains while playing a key role in capturing essential parts of the features.",
            "Now we provide answer to the following question: Instead of the FL setup we focused on, can attention provide benefits in the centralized DG setup? Table 17 shows the results with/without attention module in a centralized setup using PACS dataset. The results show that attention still provides performance improvements in the centralized setup by learning domain-invariant features, although the gain is slightly lower than the gain in the FL setup as shown in Table 3 of the main manuscript. These results indicate that the proposed attention-based learning indeed captures the domain-invariant characteristics of samples, while the scheme provides more benefits in the FL setup where each client is prone to overfitting due to lack of data.",
            "More detailed description on oversampling: Let sn∈ℝB×C×H×Wsuperscript𝑠𝑛superscriptℝ𝐵𝐶𝐻𝑊s^{n}\\in\\mathbb{R}^{B\\times C\\times H\\times W} be a mini-batch of features in client n𝑛n at a specific layer, obtained after Steps 1 and 2 in the main manuscript. Now given a fixed oversampling size, we oversample the features in the mini-batch to obtain s~nsuperscript~𝑠𝑛\\tilde{s}^{n}, so that the concatenated mini-batch s^n=[sn,s~n]superscript^𝑠𝑛superscript𝑠𝑛superscript~𝑠𝑛\\hat{s}^{n}=[s^{n},\\tilde{s}^{n}] becomes class-balanced as much as possible. Consider a toy example where the number of samples for classes a𝑎a, b𝑏b, c𝑐c are 3, 2, 1, respectively in the mini-batch snsuperscript𝑠𝑛s^{n}. In this example, if the oversampling size is 3, we randomly choose one data point from class b𝑏b\nand two data points from class c𝑐c (in this case, the same data point is selected for two times with duplication) to obtain s~nsuperscript~𝑠𝑛\\tilde{s}^{n}, so that the concatenated mini-batch s^n=[sn,s~n]superscript^𝑠𝑛superscript𝑠𝑛superscript~𝑠𝑛\\hat{s}^{n}=[s^{n},\\tilde{s}^{n}] becomes class-balanced. If the oversampling size is 1, we oversample one data point in class c𝑐c to make the concatenated mini-batch to be balanced as much as possible. If the oversampling size is 6, we oversample 1, 2, 3 samples from classes a𝑎a, b𝑏b, c𝑐c, respectively to construct s~nsuperscript~𝑠𝑛\\tilde{s}^{n}. The concatenated mini-batch s^n=[sn,s~n]superscript^𝑠𝑛superscript𝑠𝑛superscript~𝑠𝑛\\hat{s}^{n}=[s^{n},\\tilde{s}^{n}] is utilized for style-based learning and updating the model. This process not only mitigates the class-imbalance issue in each client but also provides a good platform for style exploration by oversampling the features. In our work, we reported the results with oversampling size of B𝐵B (which is equal to the mini-batch size), while the effect of the oversampling size is also reported in Fig. 4 of the main manuscript: A larger oversampling size leads to a better performance, and more importantly, our StableFDG outperforms the baseline even without any oversampling.",
            "Algorithm 1 summarizes the overall process of our StableFDG."
        ]
    },
    "A1.T5": {
        "caption": "Table 5: Results on DomainNet dataset in a single-domain data distribution scenario. ",
        "table": "<table id=\"A1.T5.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A1.T5.2.1.1\" class=\"ltx_tr\">\n<th id=\"A1.T5.2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"A1.T5.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Methods</span></th>\n<th id=\"A1.T5.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A1.T5.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Clipart</span></th>\n<th id=\"A1.T5.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A1.T5.2.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">Inforgraph</span></th>\n<th id=\"A1.T5.2.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A1.T5.2.1.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">Painting</span></th>\n<th id=\"A1.T5.2.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A1.T5.2.1.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Quickdraw</span></th>\n<th id=\"A1.T5.2.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A1.T5.2.1.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">Real</span></th>\n<th id=\"A1.T5.2.1.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"A1.T5.2.1.1.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">Sketch</span></th>\n<th id=\"A1.T5.2.1.1.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A1.T5.2.1.1.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">Avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T5.2.2.1\" class=\"ltx_tr\">\n<th id=\"A1.T5.2.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">\n<span id=\"A1.T5.2.2.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">FedAvg </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A1.T5.2.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">[</span><a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">26</a><span id=\"A1.T5.2.2.1.1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td id=\"A1.T5.2.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T5.2.2.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">61.52</span></td>\n<td id=\"A1.T5.2.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T5.2.2.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">24.75</span></td>\n<td id=\"A1.T5.2.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T5.2.2.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">50.83</span></td>\n<td id=\"A1.T5.2.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T5.2.2.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">12.08</span></td>\n<td id=\"A1.T5.2.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T5.2.2.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">60.00</span></td>\n<td id=\"A1.T5.2.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"A1.T5.2.2.1.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">49.92</span></td>\n<td id=\"A1.T5.2.2.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T5.2.2.1.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">43.18</span></td>\n</tr>\n<tr id=\"A1.T5.2.3.2\" class=\"ltx_tr\">\n<th id=\"A1.T5.2.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span id=\"A1.T5.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">FedBN </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A1.T5.2.3.2.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">[</span><a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">22</a><span id=\"A1.T5.2.3.2.1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td id=\"A1.T5.2.3.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.3.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">60.23</span></td>\n<td id=\"A1.T5.2.3.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.3.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">24.40</span></td>\n<td id=\"A1.T5.2.3.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.3.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">50.43</span></td>\n<td id=\"A1.T5.2.3.2.5\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.3.2.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">11.99</span></td>\n<td id=\"A1.T5.2.3.2.6\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.3.2.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">59.57</span></td>\n<td id=\"A1.T5.2.3.2.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"A1.T5.2.3.2.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">49.66</span></td>\n<td id=\"A1.T5.2.3.2.8\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.3.2.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">42.71</span></td>\n</tr>\n<tr id=\"A1.T5.2.4.3\" class=\"ltx_tr\">\n<th id=\"A1.T5.2.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span id=\"A1.T5.2.4.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">MixStyle </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A1.T5.2.4.3.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">[</span><a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">39</a><span id=\"A1.T5.2.4.3.1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td id=\"A1.T5.2.4.3.2\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.4.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">61.39</span></td>\n<td id=\"A1.T5.2.4.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.4.3.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">24.33</span></td>\n<td id=\"A1.T5.2.4.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.4.3.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">51.41</span></td>\n<td id=\"A1.T5.2.4.3.5\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.4.3.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">13.07</span></td>\n<td id=\"A1.T5.2.4.3.6\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.4.3.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">57.90</span></td>\n<td id=\"A1.T5.2.4.3.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"A1.T5.2.4.3.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">51.40</span></td>\n<td id=\"A1.T5.2.4.3.8\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.4.3.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">43.25</span></td>\n</tr>\n<tr id=\"A1.T5.2.5.4\" class=\"ltx_tr\">\n<th id=\"A1.T5.2.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span id=\"A1.T5.2.5.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">DSU </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A1.T5.2.5.4.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">[</span><a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">21</a><span id=\"A1.T5.2.5.4.1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td id=\"A1.T5.2.5.4.2\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.5.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">62.43</span></td>\n<td id=\"A1.T5.2.5.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.5.4.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">24.30</span></td>\n<td id=\"A1.T5.2.5.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.5.4.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">51.87</span></td>\n<td id=\"A1.T5.2.5.4.5\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.5.4.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">13.65</span></td>\n<td id=\"A1.T5.2.5.4.6\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.5.4.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">58.75</span></td>\n<td id=\"A1.T5.2.5.4.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"A1.T5.2.5.4.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">52.40</span></td>\n<td id=\"A1.T5.2.5.4.8\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.5.4.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">43.90</span></td>\n</tr>\n<tr id=\"A1.T5.2.6.5\" class=\"ltx_tr\">\n<th id=\"A1.T5.2.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span id=\"A1.T5.2.6.5.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">FedDG </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A1.T5.2.6.5.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">[</span><a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">25</a><span id=\"A1.T5.2.6.5.1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td id=\"A1.T5.2.6.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.6.5.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">62.49</span></td>\n<td id=\"A1.T5.2.6.5.3\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.6.5.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">23.71</span></td>\n<td id=\"A1.T5.2.6.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.6.5.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">48.42</span></td>\n<td id=\"A1.T5.2.6.5.5\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.6.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">12.45</span></td>\n<td id=\"A1.T5.2.6.5.6\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.6.5.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">61.44</span></td>\n<td id=\"A1.T5.2.6.5.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"A1.T5.2.6.5.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">49.11</span></td>\n<td id=\"A1.T5.2.6.5.8\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.6.5.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">42.94</span></td>\n</tr>\n<tr id=\"A1.T5.2.7.6\" class=\"ltx_tr\">\n<th id=\"A1.T5.2.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span id=\"A1.T5.2.7.6.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">FedSR </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A1.T5.2.7.6.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">[</span><a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">28</a><span id=\"A1.T5.2.7.6.1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td id=\"A1.T5.2.7.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.7.6.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">61.91</span></td>\n<td id=\"A1.T5.2.7.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.7.6.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">25.37</span></td>\n<td id=\"A1.T5.2.7.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.7.6.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">50.54</span></td>\n<td id=\"A1.T5.2.7.6.5\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.7.6.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">11.59</span></td>\n<td id=\"A1.T5.2.7.6.6\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.7.6.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">62.03</span></td>\n<td id=\"A1.T5.2.7.6.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"A1.T5.2.7.6.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">50.13</span></td>\n<td id=\"A1.T5.2.7.6.8\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.2.7.6.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">43.60</span></td>\n</tr>\n<tr id=\"A1.T5.2.8.7\" class=\"ltx_tr\">\n<th id=\"A1.T5.2.8.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span id=\"A1.T5.2.8.7.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">StableFDG</span></th>\n<td id=\"A1.T5.2.8.7.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A1.T5.2.8.7.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">62.58</span></td>\n<td id=\"A1.T5.2.8.7.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A1.T5.2.8.7.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">24.12</span></td>\n<td id=\"A1.T5.2.8.7.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A1.T5.2.8.7.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">52.23</span></td>\n<td id=\"A1.T5.2.8.7.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A1.T5.2.8.7.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">14.87</span></td>\n<td id=\"A1.T5.2.8.7.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A1.T5.2.8.7.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">60.60</span></td>\n<td id=\"A1.T5.2.8.7.7\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"A1.T5.2.8.7.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">52.50</span></td>\n<td id=\"A1.T5.2.8.7.8\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A1.T5.2.8.7.8.1\" class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">44.48</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "To demonstrate the effectiveness of our StableFDG on a larger dataset, we performed experiments on DomainNet dataset in a single-domain data distribution scenario. To this end, we utilize ResNet-50 pretrained on ImageNet. The number of global rounds and mini-batch size are set to 60 and 64, respectively. The remaining settings are the same as those in our main paper. The results in Table 5 show that our StableFDG consistently outperforms not only the centralized DG works but also the prior works on federated DG even with a more complex dataset."
        ]
    },
    "A2.T6": {
        "caption": "Table 6: Computation and communication cost comparison. ",
        "table": "<table id=\"A2.T6.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A2.T6.2.1.1\" class=\"ltx_tr\">\n<th id=\"A2.T6.2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"A2.T6.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Methods</span></th>\n<th id=\"A2.T6.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A2.T6.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Communication load</span></th>\n<th id=\"A2.T6.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A2.T6.2.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">Computation time</span></th>\n<th id=\"A2.T6.2.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A2.T6.2.1.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">Achievable average accuracy</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A2.T6.2.2.1\" class=\"ltx_tr\">\n<th id=\"A2.T6.2.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">\n<span id=\"A2.T6.2.2.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">FedAvg </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A2.T6.2.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">[</span><a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">26</a><span id=\"A2.T6.2.2.1.1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td id=\"A2.T6.2.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T6.2.2.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">44.98 MB</span></td>\n<td id=\"A2.T6.2.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T6.2.2.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">4.57 sec</span></td>\n<td id=\"A2.T6.2.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T6.2.2.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">77.02 %</span></td>\n</tr>\n<tr id=\"A2.T6.2.3.2\" class=\"ltx_tr\">\n<th id=\"A2.T6.2.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span id=\"A2.T6.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">CCST </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A2.T6.2.3.2.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">[</span><a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">2</a><span id=\"A2.T6.2.3.2.1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td id=\"A2.T6.2.3.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T6.2.3.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">44.98 MB</span></td>\n<td id=\"A2.T6.2.3.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T6.2.3.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">9.03 sec</span></td>\n<td id=\"A2.T6.2.3.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T6.2.3.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">79.00 %</span></td>\n</tr>\n<tr id=\"A2.T6.2.4.3\" class=\"ltx_tr\">\n<th id=\"A2.T6.2.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span id=\"A2.T6.2.4.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">FedDG </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A2.T6.2.4.3.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">[</span><a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">25</a><span id=\"A2.T6.2.4.3.1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td id=\"A2.T6.2.4.3.2\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T6.2.4.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">44.98 MB</span></td>\n<td id=\"A2.T6.2.4.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T6.2.4.3.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">22.44 sec</span></td>\n<td id=\"A2.T6.2.4.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T6.2.4.3.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">76.98 %</span></td>\n</tr>\n<tr id=\"A2.T6.2.5.4\" class=\"ltx_tr\">\n<th id=\"A2.T6.2.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span id=\"A2.T6.2.5.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">FedSR </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A2.T6.2.5.4.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">[</span><a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">28</a><span id=\"A2.T6.2.5.4.1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td id=\"A2.T6.2.5.4.2\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T6.2.5.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">44.98 MB</span></td>\n<td id=\"A2.T6.2.5.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T6.2.5.4.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">4.59 sec</span></td>\n<td id=\"A2.T6.2.5.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T6.2.5.4.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">77.56 %</span></td>\n</tr>\n<tr id=\"A2.T6.2.6.5\" class=\"ltx_tr\">\n<th id=\"A2.T6.2.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span id=\"A2.T6.2.6.5.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">StableFDG</span></th>\n<td id=\"A2.T6.2.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A2.T6.2.6.5.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">45.00 MB</span></td>\n<td id=\"A2.T6.2.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A2.T6.2.6.5.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">7.39 sec</span></td>\n<td id=\"A2.T6.2.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"A2.T6.2.6.5.4.1\" class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">83.75</span><span id=\"A2.T6.2.6.5.4.2\" class=\"ltx_text\" style=\"font-size:90%;\"> %</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table 6 compares the communication, computation, and average accuracy of different schemes on PACS in a multi-domain data distribution scenario. ResNet-18 is adopted as in our main manuscript. We first compare the uplink communication load of each client in a specific global round. Compared to FedAvg that only transmits the model in each round, our scheme requires additional communication burden for transmitting the style statistics and the attention module, which are negligible. We also compare the computation time by measuring the time required for local update at each client using an GTX 1080 Ti GPU. CCST [2] and FedDG [25] require large computation due to the increased amounts of data samples or multiple backpropagations for meta training. Our scheme requires additional computation caused by style exploration, attention module update, etc., which are the costs for better generalization to the unseen domain."
        ]
    },
    "A3.T7": {
        "caption": "Table 7: Results in a single-domain data distribution scenario with three clients.",
        "table": "<table id=\"A3.T7.st1.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A3.T7.st1.2.1.1\" class=\"ltx_tr\">\n<th id=\"A3.T7.st1.2.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_rr ltx_border_tt\"><span id=\"A3.T7.st1.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Methods</span></th>\n<th id=\"A3.T7.st1.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A3.T7.st1.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Art</span></th>\n<th id=\"A3.T7.st1.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A3.T7.st1.2.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">Cartoon</span></th>\n<th id=\"A3.T7.st1.2.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A3.T7.st1.2.1.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">Photo</span></th>\n<th id=\"A3.T7.st1.2.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"A3.T7.st1.2.1.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Sketch</span></th>\n<th id=\"A3.T7.st1.2.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A3.T7.st1.2.1.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">Avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A3.T7.st1.2.2.1\" class=\"ltx_tr\">\n<th id=\"A3.T7.st1.2.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_t\">\n<span id=\"A3.T7.st1.2.2.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">CCST </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A3.T7.st1.2.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">[</span><a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">2</a><span id=\"A3.T7.st1.2.2.1.1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td id=\"A3.T7.st1.2.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A3.T7.st1.2.2.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">81.25</span></td>\n<td id=\"A3.T7.st1.2.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A3.T7.st1.2.2.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">73.34</span></td>\n<td id=\"A3.T7.st1.2.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A3.T7.st1.2.2.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">95.21</span></td>\n<td id=\"A3.T7.st1.2.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"A3.T7.st1.2.2.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">80.27</span></td>\n<td id=\"A3.T7.st1.2.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A3.T7.st1.2.2.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">82.52</span></td>\n</tr>\n<tr id=\"A3.T7.st1.2.3.2\" class=\"ltx_tr\">\n<th id=\"A3.T7.st1.2.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr\">\n<span id=\"A3.T7.st1.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">FedSR </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A3.T7.st1.2.3.2.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">[</span><a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">28</a><span id=\"A3.T7.st1.2.3.2.1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td id=\"A3.T7.st1.2.3.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"A3.T7.st1.2.3.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">83.20</span></td>\n<td id=\"A3.T7.st1.2.3.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"A3.T7.st1.2.3.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">76.00</span></td>\n<td id=\"A3.T7.st1.2.3.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"A3.T7.st1.2.3.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">93.80</span></td>\n<td id=\"A3.T7.st1.2.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"A3.T7.st1.2.3.2.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">81.90</span></td>\n<td id=\"A3.T7.st1.2.3.2.6\" class=\"ltx_td ltx_align_center\"><span id=\"A3.T7.st1.2.3.2.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">83.70</span></td>\n</tr>\n<tr id=\"A3.T7.st1.2.4.3\" class=\"ltx_tr\">\n<th id=\"A3.T7.st1.2.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_rr\"><span id=\"A3.T7.st1.2.4.3.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">StableFDG</span></th>\n<td id=\"A3.T7.st1.2.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A3.T7.st1.2.4.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">83.01</span></td>\n<td id=\"A3.T7.st1.2.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A3.T7.st1.2.4.3.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">79.31</span></td>\n<td id=\"A3.T7.st1.2.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A3.T7.st1.2.4.3.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">94.85</span></td>\n<td id=\"A3.T7.st1.2.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"A3.T7.st1.2.4.3.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">79.76</span></td>\n<td id=\"A3.T7.st1.2.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A3.T7.st1.2.4.3.6.1\" class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">84.23</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Different from the prior works [2, 28] for federated DG adopting the usual setting where the number of clients equals the number of source domains, in our main paper, we introduce a more practical experimental setting for federated DG where the source data is distributed to more clients than the number of source domains. For a comparison with them in the same setting, we also provide additional experimental results in the setup with number of clients = number of source domains. Table 7 shows the results on PACS and Office-Home datasets with three clients in a single-domain data distribution scenario. It is confirmed from the results that our StableFDG also achieves better performance compared to the existing works [2, 28] in this simple setting."
        ]
    },
    "A3.T7.st1": {
        "caption": "(a) Results on PACS dataset.",
        "table": "<table id=\"A3.T7.st1.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A3.T7.st1.2.1.1\" class=\"ltx_tr\">\n<th id=\"A3.T7.st1.2.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_rr ltx_border_tt\"><span id=\"A3.T7.st1.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Methods</span></th>\n<th id=\"A3.T7.st1.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A3.T7.st1.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Art</span></th>\n<th id=\"A3.T7.st1.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A3.T7.st1.2.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">Cartoon</span></th>\n<th id=\"A3.T7.st1.2.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A3.T7.st1.2.1.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">Photo</span></th>\n<th id=\"A3.T7.st1.2.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"A3.T7.st1.2.1.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Sketch</span></th>\n<th id=\"A3.T7.st1.2.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A3.T7.st1.2.1.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">Avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A3.T7.st1.2.2.1\" class=\"ltx_tr\">\n<th id=\"A3.T7.st1.2.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_t\">\n<span id=\"A3.T7.st1.2.2.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">CCST </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A3.T7.st1.2.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">[</span><a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">2</a><span id=\"A3.T7.st1.2.2.1.1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td id=\"A3.T7.st1.2.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A3.T7.st1.2.2.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">81.25</span></td>\n<td id=\"A3.T7.st1.2.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A3.T7.st1.2.2.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">73.34</span></td>\n<td id=\"A3.T7.st1.2.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A3.T7.st1.2.2.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">95.21</span></td>\n<td id=\"A3.T7.st1.2.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"A3.T7.st1.2.2.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">80.27</span></td>\n<td id=\"A3.T7.st1.2.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A3.T7.st1.2.2.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">82.52</span></td>\n</tr>\n<tr id=\"A3.T7.st1.2.3.2\" class=\"ltx_tr\">\n<th id=\"A3.T7.st1.2.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr\">\n<span id=\"A3.T7.st1.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">FedSR </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A3.T7.st1.2.3.2.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">[</span><a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">28</a><span id=\"A3.T7.st1.2.3.2.1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td id=\"A3.T7.st1.2.3.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"A3.T7.st1.2.3.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">83.20</span></td>\n<td id=\"A3.T7.st1.2.3.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"A3.T7.st1.2.3.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">76.00</span></td>\n<td id=\"A3.T7.st1.2.3.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"A3.T7.st1.2.3.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">93.80</span></td>\n<td id=\"A3.T7.st1.2.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"A3.T7.st1.2.3.2.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">81.90</span></td>\n<td id=\"A3.T7.st1.2.3.2.6\" class=\"ltx_td ltx_align_center\"><span id=\"A3.T7.st1.2.3.2.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">83.70</span></td>\n</tr>\n<tr id=\"A3.T7.st1.2.4.3\" class=\"ltx_tr\">\n<th id=\"A3.T7.st1.2.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_rr\"><span id=\"A3.T7.st1.2.4.3.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">StableFDG</span></th>\n<td id=\"A3.T7.st1.2.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A3.T7.st1.2.4.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">83.01</span></td>\n<td id=\"A3.T7.st1.2.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A3.T7.st1.2.4.3.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">79.31</span></td>\n<td id=\"A3.T7.st1.2.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A3.T7.st1.2.4.3.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">94.85</span></td>\n<td id=\"A3.T7.st1.2.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"A3.T7.st1.2.4.3.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">79.76</span></td>\n<td id=\"A3.T7.st1.2.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A3.T7.st1.2.4.3.6.1\" class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">84.23</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Traditional federated learning (FL) algorithms operate under the assumption that the data distributions at training (source domains) and testing (target domain) are the same. The fact that domain shifts often occur in practice necessitates equipping FL methods with a domain generalization (DG) capability. However, existing DG algorithms face fundamental challenges in FL setups due to the lack of samples/domains in each client’s local dataset. In this paper, we propose StableFDG, a style and attention based learning strategy for accomplishing federated domain generalization, introducing two key contributions. The first is style-based learning, which enables each client to explore novel styles beyond the original source domains in its local dataset, improving domain diversity based on the proposed style sharing, shifting, and exploration strategies. Our second contribution is an attention-based feature highlighter, which captures the similarities between the features of data samples in the same class, and emphasizes the important/common characteristics to better learn the domain-invariant characteristics of each class in data-poor FL scenarios. Experimental results show that StableFDG outperforms existing baselines on various DG benchmark datasets, demonstrating its efficacy.",
            "Contributions. In this paper, we propose StableFDG, a style and attention based learning strategy tailored to federated domain generalization. StableFDG tackles the fundamental challenges in federated DG that arise due to the lack of data/styles in each FL client, with two novel characteristics:",
            "The two suggested schemes work in a complementary fashion, each providing one necessary component for federated DG: our style-based learning\nimproves domain diversity, while the attention-based feature highlighter learns domain-invariant characteristics of each class. Experiments on various FL setups using DG benchmarks confirm the advantage of StableFDG over (i) the baselines that directly apply DG methods to FL and (ii) the baselines that are specifically designed for federated DG.",
            "Overview of approach. Fig. 1 provides an overview of the problem setup and our StableFDG algorithm. As in conventional FL, the training process consists of multiple global rounds, which we index t=1,2,…,T𝑡12…𝑇t=1,2,\\dots,T. In the beginning of round t𝑡t, a selected set of clients download the current global model 𝐰tsubscript𝐰𝑡\\mathbf{w}_{t} from the server. Before local training begins, each client n𝑛n computes its own style information Φn=[μn,σn,Σn​(μ),Σn​(σ)]subscriptΦ𝑛subscript𝜇𝑛subscript𝜎𝑛subscriptΣ𝑛𝜇subscriptΣ𝑛𝜎\\Phi_{n}=[\\mu_{n},\\sigma_{n},\\Sigma_{n}(\\mu),\\Sigma_{n}(\\sigma)] using its local dataset according to (2), which will be clarified soon. This information is sent to the server, and the server shares these information with other clients to compensate for the lack of styles or domains in each client.\nDuring the local update process, each client selectively shifts the styles of the original data in the mini-batch to the new style (received from the server) via adaptive instance normalization (AdaIN) [10], to improve domain diversity (inner box in Fig. 2(b)). After this style sharing and shifting process, each client performs style exploration via feature-level oversampling to further expose the model to novel styles beyond the current source domains of each client (outer box in Fig. 2(b)). Finally, at the output of the feature extractor, we apply our attention-based feature highlighter to extract common/important feature information within each class and emphasize them for better generalization (Fig. 3). When local updates are finished, the server aggregates the client models and proceeds to the next round.",
            "to avoid performance degradation when there is little commonality between key and query images. In other words, StableFDG takes advantage of both cross-attention and self-attention, enabling the model to extract and learn important characteristics across images (via cross-attention), and within the image (via self-attention). A more detailed analysis on (8) can be found in Appendix.",
            "Finally, we put together StableFDG. In each FL round, the clients first download the global model from the server and perform style sharing, shifting, and exploration according to 3.1, which are done in the early layers of CNNs where the style information is preserved. Then, at the output of the feature extractor, attention-based weighted averaging is applied according to Sec. 3.2. These two components have their own roles and work in a complementary fashion to handle the challenging DG problem in FL; our style-based strategy is effective in improving the domain diversity, while our attention-based method can directly capture the domain-invariant characteristics of each class. After the local update process, the server aggregates the client models and proceeds to the next round.",
            "Remark 4 (Computational complexity). The computational complexity of StableFDG depends on the oversampling size in Sec. 3.1 and the attention module size in Sec. 3.2, which could be controlled depending on the resource constraints of clients. We show in Sec. 4 that StableFDG achieves the state-of-the-art performance with (i) minimal oversampling size and (ii) negligible cost of attention module. A more detailed discussion on the computational complexity is in Appendix.",
            "Single-domain data distribution. Table 1 shows our results in a single-domain data distribution setup. We have the following observations. Compared to the previous results provided in the centralized DG works [39, 21], the performance of each method is generally lower. This is due to the limited numbers of styles and data samples in each FL client, which restricts the generalization performance of individual client models. It can be seen that most of the baselines perform better than FedAvg and FedBN that do not tackle the DG problem.\nThe proposed StableFDG achieves the best average accuracy for all benchmark datasets, where the gain is especially large in PACS having large shifts between domains. In contrast to our scheme, the prior works [2, 25, 28] targeting federated DG show marginal performance gains relative to FedAvg in our practical experimental setup with (i) more clients (which results in less data in each client) and (ii) partial client participations.",
            "Multi-domain data distribution. In Table 2, we report the results in a multi-domain data distribution scenario. Compared to the results in Table 1, most of the schemes achieve improved performance in Table 2. This is because each client has multiple source domains, and thus providing a better platform for each client model to gain generalization ability. The proposed StableFDG still performs the best, demonstrating the effectiveness of our style and attention based learning strategy for federated DG.",
            "Effect of each component. To see the effect of each component of StableFDG, in Table 3, we apply our style-based learning and attention-based learning one-by-one in a multi-domain data distribution setup using PACS. We compare our results with style-augmentation DG baselines, MixStyle [39] and DSU [21]. By applying only our style-based learning, StableFDG already outperforms prior style-augmentation methods.\nFurthermore, by adopting only one of the proposed components, our scheme performs better than all the baselines in Table 2. Additional ablation studies using other datasets are reported in Appendix.",
            "Effect of hyperparameters. In DG setups, it is generally impractical to tune the hyperparameter using the target domain, because there is no information on the target domain during training. Hence, we used a fixed exploration level α=3𝛼3\\alpha=3 throughout all experiments without tuning. In Fig. 4, we observe how the hyperparameters affect the target domain performance on PACS. In the first plot of Fig. 4, if α𝛼\\alpha is too small, the performance is relatively low since\nthe model is not able to explore novel styles\nbeyond the client’s source domains. If α𝛼\\alpha is too large, the performance could be slightly degraded because the model would explore too many redundant styles. The overall results show that StableFDG still performs better than the baselines with an arbitrarily chosen α𝛼\\alpha, which is a significant advantage of our scheme in the DG setup where hyperparameter tuning is challenging. The second plot of Fig. 4 shows how the oversampling size (introduced in Step 3 of Sec. 3.1) affects the DG performance. StableFDG still outperforms the baseline with minimal oversampling size,\nindicating that other components of our solution (style sharing/shifting and attention-based components) are already strong enough. The size of oversampling can be determined depending on the clients’ computation/memory constraints, with the cost of improved generalization.",
            "Performance in a centralized setup. Although our scheme is tailored to federated DG, the ideas of StableFDG can be also utilized in a centralized setup. In Table 4(a), we study the effects of our style and attention based strategies in a centralized DG setting using PACS, while the other settings are the same as in the FL setup. The results demonstrate that the proposed ideas are not only specific to data-poor FL scenarios but also have potentials to be utilized in centralized DG settings.",
            "Performance with ResNet-50. In Table 4(b), we also conduct experiments using ResNet-50 on PACS dataset in the multi-domain data distribution scenario. Other settings are exactly the same as in Table 2. The results further confirm the advantage of StableFDG with larger models.",
            "Despite the practical significance, the field of federated domain generalization is still in the early stage of research. In this paper, we proposed StableFDG, a new training strategy tailored to this unexplored area. Our style-based strategy enables the model to get exposed to various novel styles beyond each client’s source domains, while our attention-based method captures and emphasizes the important/common characteristics of each class. Extensive experimental results confirmed the advantage of our StableFDG for federated domain generalization with data-poor FL clients.",
            "Limitations and future works. StableFDG requires 0.45 % more communication load compared to FedAvg for sharing the attention module and style statistics, which is the cost for a better DG performance.\nFurther developing our idea to tailor to centralized DG and extending our attention\nstrategy to segmentation/detection DG tasks are also interesting directions for future research.",
            "To demonstrate the effectiveness of our StableFDG on a larger dataset, we performed experiments on DomainNet dataset in a single-domain data distribution scenario. To this end, we utilize ResNet-50 pretrained on ImageNet. The number of global rounds and mini-batch size are set to 60 and 64, respectively. The remaining settings are the same as those in our main paper. The results in Table 5 show that our StableFDG consistently outperforms not only the centralized DG works but also the prior works on federated DG even with a more complex dataset.",
            "Table 6 compares the communication, computation, and average accuracy of different schemes on PACS in a multi-domain data distribution scenario. ResNet-18 is adopted as in our main manuscript. We first compare the uplink communication load of each client in a specific global round. Compared to FedAvg that only transmits the model in each round, our scheme requires additional communication burden for transmitting the style statistics and the attention module, which are negligible. We also compare the computation time by measuring the time required for local update at each client using an GTX 1080 Ti GPU. CCST [2] and FedDG [25] require large computation due to the increased amounts of data samples or multiple backpropagations for meta training. Our scheme requires additional computation caused by style exploration, attention module update, etc., which are the costs for better generalization to the unseen domain.",
            "Different from the prior works [2, 28] for federated DG adopting the usual setting where the number of clients equals the number of source domains, in our main paper, we introduce a more practical experimental setting for federated DG where the source data is distributed to more clients than the number of source domains. For a comparison with them in the same setting, we also provide additional experimental results in the setup with number of clients = number of source domains. Table 7 shows the results on PACS and Office-Home datasets with three clients in a single-domain data distribution scenario. It is confirmed from the results that our StableFDG also achieves better performance compared to the existing works [2, 28] in this simple setting.",
            "As mentioned in the main manuscript, we provide further ablation studies on the effect of each component in StableFDG using VLCS dataset. Table 8 shows that each component individually brings performance gain compared to FedAvg. Using both strategies achieves greater performance gains, confirming that the two proposed schemes work in a complementary fashion.",
            "In our style based learning, style sharing among clients is performed at random. However, one can think of the strategy where client n𝑛n receives the style information Φn′subscriptΦsuperscript𝑛′\\Phi_{n^{\\prime}} that has the largest distance with its own style information Φn′subscriptΦsuperscript𝑛′\\Phi_{n^{\\prime}} in the style space. Table 9 shows the corresponding result using PACS dataset in a multi-domain data distribution setup. Interestingly, it can be seen that the random selection adopted in this paper performs better, since most of the users generally tend to receive the same style statistics when using the largest distance strategy.",
            "We also compare the effect of number of styles received at each client in Table 10 using PACS dataset in a multi-domain data distribution setup. The performance increases as the number of received styles increases, with small additional communication load (the vector length of the style information is 128, which is negligible compared to the number of model parameters, which is 11,180,103).",
            "In Section 3.1, we utilized the k𝑘k-means++ as a tool for facilitating our key idea in style shifting, which is to effectively balance between the original source domain and the new source domain for better generalization; k-means++ plays a role to select the B/2𝐵2B/2 styles that are similar to the remaining B/2𝐵2B/2 styles in the mini-batch. By doing so, the model can explore new styles while not losing the performance on the original styles. To see this effect, we compare k𝑘k-means++ vs. random sampling when selecting B/2 samples to be shifted, in Table 11. The results show that strategically selecting the B/2 samples to be shifted achieves better performance especially in the Sketch domain (1.89% gain) that has a large style gap with other domains. We believe that these results motivate and justify our design choice.",
            "In our main paper, we performed the class-balanced oversampling in the feature space to alleviate the class-imbalance issue during style exploration. To confirm the effectiveness of the class-balanced oversampling, we compare it with random oversampling under the same condition where only the style exploration is applied without other components. Table 12 shows the results on Office-Home dataset, where the class distribution is highly imbalanced in a multi-domain data distribution scenario. It can be seen that our class-balanced oversampling achieves higher performance over the simple random sampling, which validates the efficacy of mitigating the class imbalance problem in FL clients.",
            "We conduct additional ablation studies on the probability value (defined as p𝑝p here) utilized to control the operation of the style sharing/shifting and style exploration modules. Larger p𝑝p means that our scheme is more likely to be activated. In Table 13, we provide results on various probability values in a single-domain data distribution scenario using PACS dataset. From the results, it is confirm that for all p𝑝p values, the proposed StableFDG outperforms existing baselines, demonstrating that our scheme can work well with an arbitrarily chosen probability p𝑝p. In detail, when p𝑝p ranges from 0.3 to 0.7, the high performance is maintained while the performance decreases at both extreme probabilities (p=0.1𝑝0.1p=0.1 and 0.9). Therefore, it is recommended for practitioners to select the p𝑝p in an appropriate range, avoiding extreme cases.",
            "For implementation, style-based learning is applied only in the 1st, 2nd, 3rd blocks among 4 residual blocks in ResNet-18. Note that at the output of the 4th block, label information is dominant rather than style information, which results in degraded performance when style-based schemes are applied. This is confirmed by our new experiments in the table below. It can be seen from the results that if we consider the 4th residual block to apply our style-based learning, the performance gets degraded. This result confirms the intuition that style-based learning should be conducted at the earlier layers where style information is preserved.",
            "In our main paper, the similarity metric in equation (6) adopts cross-attention, while the metric in equation (8) combines cross-attention and self-attention. When applying only the cross-attention-based metric in equation (6), we found that the similarity value could become low even when the two samples belong to the same class, in special cases. We handled this issue by adding the self-attention component as in equation (8). Intuitively, by doing this, the attention module is learning to extract the important features across images (via cross-attention), and within the image (via self-attention). Table 15 compares the performance of our StableFDG when using (i) self-attention alone, (ii) cross-attention alone (equation (6)), and (iii) both self and cross attentions at the same time (equation (8)), confirming the advantage of using self-attention and cross-attention together.",
            "Our attention module requires 0.44% of additional model parameters to perform the attention-based learning. For a fair comparison to see the effect of our attention-based learning, we consider a different baseline with the same model size but without attention-based learning. Specifically, the baseline computes the attention score map using only additional convolutional operations and take the weighted average of the feature zisubscript𝑧𝑖z_{i} based on the attention score map. Table 16 shows the results using PACS dataset in a single-domain data distribution scenario. The results demonstrate that our attention-based learning achieves performance improvements on all four domains while playing a key role in capturing essential parts of the features.",
            "Now we provide answer to the following question: Instead of the FL setup we focused on, can attention provide benefits in the centralized DG setup? Table 17 shows the results with/without attention module in a centralized setup using PACS dataset. The results show that attention still provides performance improvements in the centralized setup by learning domain-invariant features, although the gain is slightly lower than the gain in the FL setup as shown in Table 3 of the main manuscript. These results indicate that the proposed attention-based learning indeed captures the domain-invariant characteristics of samples, while the scheme provides more benefits in the FL setup where each client is prone to overfitting due to lack of data.",
            "More detailed description on oversampling: Let sn∈ℝB×C×H×Wsuperscript𝑠𝑛superscriptℝ𝐵𝐶𝐻𝑊s^{n}\\in\\mathbb{R}^{B\\times C\\times H\\times W} be a mini-batch of features in client n𝑛n at a specific layer, obtained after Steps 1 and 2 in the main manuscript. Now given a fixed oversampling size, we oversample the features in the mini-batch to obtain s~nsuperscript~𝑠𝑛\\tilde{s}^{n}, so that the concatenated mini-batch s^n=[sn,s~n]superscript^𝑠𝑛superscript𝑠𝑛superscript~𝑠𝑛\\hat{s}^{n}=[s^{n},\\tilde{s}^{n}] becomes class-balanced as much as possible. Consider a toy example where the number of samples for classes a𝑎a, b𝑏b, c𝑐c are 3, 2, 1, respectively in the mini-batch snsuperscript𝑠𝑛s^{n}. In this example, if the oversampling size is 3, we randomly choose one data point from class b𝑏b\nand two data points from class c𝑐c (in this case, the same data point is selected for two times with duplication) to obtain s~nsuperscript~𝑠𝑛\\tilde{s}^{n}, so that the concatenated mini-batch s^n=[sn,s~n]superscript^𝑠𝑛superscript𝑠𝑛superscript~𝑠𝑛\\hat{s}^{n}=[s^{n},\\tilde{s}^{n}] becomes class-balanced. If the oversampling size is 1, we oversample one data point in class c𝑐c to make the concatenated mini-batch to be balanced as much as possible. If the oversampling size is 6, we oversample 1, 2, 3 samples from classes a𝑎a, b𝑏b, c𝑐c, respectively to construct s~nsuperscript~𝑠𝑛\\tilde{s}^{n}. The concatenated mini-batch s^n=[sn,s~n]superscript^𝑠𝑛superscript𝑠𝑛superscript~𝑠𝑛\\hat{s}^{n}=[s^{n},\\tilde{s}^{n}] is utilized for style-based learning and updating the model. This process not only mitigates the class-imbalance issue in each client but also provides a good platform for style exploration by oversampling the features. In our work, we reported the results with oversampling size of B𝐵B (which is equal to the mini-batch size), while the effect of the oversampling size is also reported in Fig. 4 of the main manuscript: A larger oversampling size leads to a better performance, and more importantly, our StableFDG outperforms the baseline even without any oversampling.",
            "Algorithm 1 summarizes the overall process of our StableFDG."
        ]
    },
    "A3.T7.st2": {
        "caption": "(b) Results on Office-Home dataset.",
        "table": "<table id=\"A3.T7.st2.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A3.T7.st2.2.1.1\" class=\"ltx_tr\">\n<th id=\"A3.T7.st2.2.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_rr ltx_border_tt\"><span id=\"A3.T7.st2.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Methods</span></th>\n<th id=\"A3.T7.st2.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A3.T7.st2.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Art</span></th>\n<th id=\"A3.T7.st2.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A3.T7.st2.2.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">Clipart</span></th>\n<th id=\"A3.T7.st2.2.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A3.T7.st2.2.1.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">Product</span></th>\n<th id=\"A3.T7.st2.2.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"A3.T7.st2.2.1.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Real</span></th>\n<th id=\"A3.T7.st2.2.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A3.T7.st2.2.1.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">Avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A3.T7.st2.2.2.1\" class=\"ltx_tr\">\n<th id=\"A3.T7.st2.2.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_t\">\n<span id=\"A3.T7.st2.2.2.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">CCST </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A3.T7.st2.2.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">[</span><a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">2</a><span id=\"A3.T7.st2.2.2.1.1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td id=\"A3.T7.st2.2.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A3.T7.st2.2.2.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">59.05</span></td>\n<td id=\"A3.T7.st2.2.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A3.T7.st2.2.2.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">50.06</span></td>\n<td id=\"A3.T7.st2.2.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A3.T7.st2.2.2.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">72.97</span></td>\n<td id=\"A3.T7.st2.2.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"A3.T7.st2.2.2.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">71.67</span></td>\n<td id=\"A3.T7.st2.2.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A3.T7.st2.2.2.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">63.56</span></td>\n</tr>\n<tr id=\"A3.T7.st2.2.3.2\" class=\"ltx_tr\">\n<th id=\"A3.T7.st2.2.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr\">\n<span id=\"A3.T7.st2.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">FedSR </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A3.T7.st2.2.3.2.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">[</span><a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">28</a><span id=\"A3.T7.st2.2.3.2.1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td id=\"A3.T7.st2.2.3.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"A3.T7.st2.2.3.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">57.93</span></td>\n<td id=\"A3.T7.st2.2.3.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"A3.T7.st2.2.3.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">50.45</span></td>\n<td id=\"A3.T7.st2.2.3.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"A3.T7.st2.2.3.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">73.33</span></td>\n<td id=\"A3.T7.st2.2.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"A3.T7.st2.2.3.2.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">75.51</span></td>\n<td id=\"A3.T7.st2.2.3.2.6\" class=\"ltx_td ltx_align_center\"><span id=\"A3.T7.st2.2.3.2.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">64.31</span></td>\n</tr>\n<tr id=\"A3.T7.st2.2.4.3\" class=\"ltx_tr\">\n<th id=\"A3.T7.st2.2.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_rr\"><span id=\"A3.T7.st2.2.4.3.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">StableFDG</span></th>\n<td id=\"A3.T7.st2.2.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A3.T7.st2.2.4.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">57.19</span></td>\n<td id=\"A3.T7.st2.2.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A3.T7.st2.2.4.3.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">57.94</span></td>\n<td id=\"A3.T7.st2.2.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A3.T7.st2.2.4.3.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">72.76</span></td>\n<td id=\"A3.T7.st2.2.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"A3.T7.st2.2.4.3.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">72.16</span></td>\n<td id=\"A3.T7.st2.2.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A3.T7.st2.2.4.3.6.1\" class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">65.01</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Traditional federated learning (FL) algorithms operate under the assumption that the data distributions at training (source domains) and testing (target domain) are the same. The fact that domain shifts often occur in practice necessitates equipping FL methods with a domain generalization (DG) capability. However, existing DG algorithms face fundamental challenges in FL setups due to the lack of samples/domains in each client’s local dataset. In this paper, we propose StableFDG, a style and attention based learning strategy for accomplishing federated domain generalization, introducing two key contributions. The first is style-based learning, which enables each client to explore novel styles beyond the original source domains in its local dataset, improving domain diversity based on the proposed style sharing, shifting, and exploration strategies. Our second contribution is an attention-based feature highlighter, which captures the similarities between the features of data samples in the same class, and emphasizes the important/common characteristics to better learn the domain-invariant characteristics of each class in data-poor FL scenarios. Experimental results show that StableFDG outperforms existing baselines on various DG benchmark datasets, demonstrating its efficacy.",
            "Contributions. In this paper, we propose StableFDG, a style and attention based learning strategy tailored to federated domain generalization. StableFDG tackles the fundamental challenges in federated DG that arise due to the lack of data/styles in each FL client, with two novel characteristics:",
            "The two suggested schemes work in a complementary fashion, each providing one necessary component for federated DG: our style-based learning\nimproves domain diversity, while the attention-based feature highlighter learns domain-invariant characteristics of each class. Experiments on various FL setups using DG benchmarks confirm the advantage of StableFDG over (i) the baselines that directly apply DG methods to FL and (ii) the baselines that are specifically designed for federated DG.",
            "Overview of approach. Fig. 1 provides an overview of the problem setup and our StableFDG algorithm. As in conventional FL, the training process consists of multiple global rounds, which we index t=1,2,…,T𝑡12…𝑇t=1,2,\\dots,T. In the beginning of round t𝑡t, a selected set of clients download the current global model 𝐰tsubscript𝐰𝑡\\mathbf{w}_{t} from the server. Before local training begins, each client n𝑛n computes its own style information Φn=[μn,σn,Σn​(μ),Σn​(σ)]subscriptΦ𝑛subscript𝜇𝑛subscript𝜎𝑛subscriptΣ𝑛𝜇subscriptΣ𝑛𝜎\\Phi_{n}=[\\mu_{n},\\sigma_{n},\\Sigma_{n}(\\mu),\\Sigma_{n}(\\sigma)] using its local dataset according to (2), which will be clarified soon. This information is sent to the server, and the server shares these information with other clients to compensate for the lack of styles or domains in each client.\nDuring the local update process, each client selectively shifts the styles of the original data in the mini-batch to the new style (received from the server) via adaptive instance normalization (AdaIN) [10], to improve domain diversity (inner box in Fig. 2(b)). After this style sharing and shifting process, each client performs style exploration via feature-level oversampling to further expose the model to novel styles beyond the current source domains of each client (outer box in Fig. 2(b)). Finally, at the output of the feature extractor, we apply our attention-based feature highlighter to extract common/important feature information within each class and emphasize them for better generalization (Fig. 3). When local updates are finished, the server aggregates the client models and proceeds to the next round.",
            "to avoid performance degradation when there is little commonality between key and query images. In other words, StableFDG takes advantage of both cross-attention and self-attention, enabling the model to extract and learn important characteristics across images (via cross-attention), and within the image (via self-attention). A more detailed analysis on (8) can be found in Appendix.",
            "Finally, we put together StableFDG. In each FL round, the clients first download the global model from the server and perform style sharing, shifting, and exploration according to 3.1, which are done in the early layers of CNNs where the style information is preserved. Then, at the output of the feature extractor, attention-based weighted averaging is applied according to Sec. 3.2. These two components have their own roles and work in a complementary fashion to handle the challenging DG problem in FL; our style-based strategy is effective in improving the domain diversity, while our attention-based method can directly capture the domain-invariant characteristics of each class. After the local update process, the server aggregates the client models and proceeds to the next round.",
            "Remark 4 (Computational complexity). The computational complexity of StableFDG depends on the oversampling size in Sec. 3.1 and the attention module size in Sec. 3.2, which could be controlled depending on the resource constraints of clients. We show in Sec. 4 that StableFDG achieves the state-of-the-art performance with (i) minimal oversampling size and (ii) negligible cost of attention module. A more detailed discussion on the computational complexity is in Appendix.",
            "Single-domain data distribution. Table 1 shows our results in a single-domain data distribution setup. We have the following observations. Compared to the previous results provided in the centralized DG works [39, 21], the performance of each method is generally lower. This is due to the limited numbers of styles and data samples in each FL client, which restricts the generalization performance of individual client models. It can be seen that most of the baselines perform better than FedAvg and FedBN that do not tackle the DG problem.\nThe proposed StableFDG achieves the best average accuracy for all benchmark datasets, where the gain is especially large in PACS having large shifts between domains. In contrast to our scheme, the prior works [2, 25, 28] targeting federated DG show marginal performance gains relative to FedAvg in our practical experimental setup with (i) more clients (which results in less data in each client) and (ii) partial client participations.",
            "Multi-domain data distribution. In Table 2, we report the results in a multi-domain data distribution scenario. Compared to the results in Table 1, most of the schemes achieve improved performance in Table 2. This is because each client has multiple source domains, and thus providing a better platform for each client model to gain generalization ability. The proposed StableFDG still performs the best, demonstrating the effectiveness of our style and attention based learning strategy for federated DG.",
            "Effect of each component. To see the effect of each component of StableFDG, in Table 3, we apply our style-based learning and attention-based learning one-by-one in a multi-domain data distribution setup using PACS. We compare our results with style-augmentation DG baselines, MixStyle [39] and DSU [21]. By applying only our style-based learning, StableFDG already outperforms prior style-augmentation methods.\nFurthermore, by adopting only one of the proposed components, our scheme performs better than all the baselines in Table 2. Additional ablation studies using other datasets are reported in Appendix.",
            "Effect of hyperparameters. In DG setups, it is generally impractical to tune the hyperparameter using the target domain, because there is no information on the target domain during training. Hence, we used a fixed exploration level α=3𝛼3\\alpha=3 throughout all experiments without tuning. In Fig. 4, we observe how the hyperparameters affect the target domain performance on PACS. In the first plot of Fig. 4, if α𝛼\\alpha is too small, the performance is relatively low since\nthe model is not able to explore novel styles\nbeyond the client’s source domains. If α𝛼\\alpha is too large, the performance could be slightly degraded because the model would explore too many redundant styles. The overall results show that StableFDG still performs better than the baselines with an arbitrarily chosen α𝛼\\alpha, which is a significant advantage of our scheme in the DG setup where hyperparameter tuning is challenging. The second plot of Fig. 4 shows how the oversampling size (introduced in Step 3 of Sec. 3.1) affects the DG performance. StableFDG still outperforms the baseline with minimal oversampling size,\nindicating that other components of our solution (style sharing/shifting and attention-based components) are already strong enough. The size of oversampling can be determined depending on the clients’ computation/memory constraints, with the cost of improved generalization.",
            "Performance in a centralized setup. Although our scheme is tailored to federated DG, the ideas of StableFDG can be also utilized in a centralized setup. In Table 4(a), we study the effects of our style and attention based strategies in a centralized DG setting using PACS, while the other settings are the same as in the FL setup. The results demonstrate that the proposed ideas are not only specific to data-poor FL scenarios but also have potentials to be utilized in centralized DG settings.",
            "Performance with ResNet-50. In Table 4(b), we also conduct experiments using ResNet-50 on PACS dataset in the multi-domain data distribution scenario. Other settings are exactly the same as in Table 2. The results further confirm the advantage of StableFDG with larger models.",
            "Despite the practical significance, the field of federated domain generalization is still in the early stage of research. In this paper, we proposed StableFDG, a new training strategy tailored to this unexplored area. Our style-based strategy enables the model to get exposed to various novel styles beyond each client’s source domains, while our attention-based method captures and emphasizes the important/common characteristics of each class. Extensive experimental results confirmed the advantage of our StableFDG for federated domain generalization with data-poor FL clients.",
            "Limitations and future works. StableFDG requires 0.45 % more communication load compared to FedAvg for sharing the attention module and style statistics, which is the cost for a better DG performance.\nFurther developing our idea to tailor to centralized DG and extending our attention\nstrategy to segmentation/detection DG tasks are also interesting directions for future research.",
            "To demonstrate the effectiveness of our StableFDG on a larger dataset, we performed experiments on DomainNet dataset in a single-domain data distribution scenario. To this end, we utilize ResNet-50 pretrained on ImageNet. The number of global rounds and mini-batch size are set to 60 and 64, respectively. The remaining settings are the same as those in our main paper. The results in Table 5 show that our StableFDG consistently outperforms not only the centralized DG works but also the prior works on federated DG even with a more complex dataset.",
            "Table 6 compares the communication, computation, and average accuracy of different schemes on PACS in a multi-domain data distribution scenario. ResNet-18 is adopted as in our main manuscript. We first compare the uplink communication load of each client in a specific global round. Compared to FedAvg that only transmits the model in each round, our scheme requires additional communication burden for transmitting the style statistics and the attention module, which are negligible. We also compare the computation time by measuring the time required for local update at each client using an GTX 1080 Ti GPU. CCST [2] and FedDG [25] require large computation due to the increased amounts of data samples or multiple backpropagations for meta training. Our scheme requires additional computation caused by style exploration, attention module update, etc., which are the costs for better generalization to the unseen domain.",
            "Different from the prior works [2, 28] for federated DG adopting the usual setting where the number of clients equals the number of source domains, in our main paper, we introduce a more practical experimental setting for federated DG where the source data is distributed to more clients than the number of source domains. For a comparison with them in the same setting, we also provide additional experimental results in the setup with number of clients = number of source domains. Table 7 shows the results on PACS and Office-Home datasets with three clients in a single-domain data distribution scenario. It is confirmed from the results that our StableFDG also achieves better performance compared to the existing works [2, 28] in this simple setting.",
            "As mentioned in the main manuscript, we provide further ablation studies on the effect of each component in StableFDG using VLCS dataset. Table 8 shows that each component individually brings performance gain compared to FedAvg. Using both strategies achieves greater performance gains, confirming that the two proposed schemes work in a complementary fashion.",
            "In our style based learning, style sharing among clients is performed at random. However, one can think of the strategy where client n𝑛n receives the style information Φn′subscriptΦsuperscript𝑛′\\Phi_{n^{\\prime}} that has the largest distance with its own style information Φn′subscriptΦsuperscript𝑛′\\Phi_{n^{\\prime}} in the style space. Table 9 shows the corresponding result using PACS dataset in a multi-domain data distribution setup. Interestingly, it can be seen that the random selection adopted in this paper performs better, since most of the users generally tend to receive the same style statistics when using the largest distance strategy.",
            "We also compare the effect of number of styles received at each client in Table 10 using PACS dataset in a multi-domain data distribution setup. The performance increases as the number of received styles increases, with small additional communication load (the vector length of the style information is 128, which is negligible compared to the number of model parameters, which is 11,180,103).",
            "In Section 3.1, we utilized the k𝑘k-means++ as a tool for facilitating our key idea in style shifting, which is to effectively balance between the original source domain and the new source domain for better generalization; k-means++ plays a role to select the B/2𝐵2B/2 styles that are similar to the remaining B/2𝐵2B/2 styles in the mini-batch. By doing so, the model can explore new styles while not losing the performance on the original styles. To see this effect, we compare k𝑘k-means++ vs. random sampling when selecting B/2 samples to be shifted, in Table 11. The results show that strategically selecting the B/2 samples to be shifted achieves better performance especially in the Sketch domain (1.89% gain) that has a large style gap with other domains. We believe that these results motivate and justify our design choice.",
            "In our main paper, we performed the class-balanced oversampling in the feature space to alleviate the class-imbalance issue during style exploration. To confirm the effectiveness of the class-balanced oversampling, we compare it with random oversampling under the same condition where only the style exploration is applied without other components. Table 12 shows the results on Office-Home dataset, where the class distribution is highly imbalanced in a multi-domain data distribution scenario. It can be seen that our class-balanced oversampling achieves higher performance over the simple random sampling, which validates the efficacy of mitigating the class imbalance problem in FL clients.",
            "We conduct additional ablation studies on the probability value (defined as p𝑝p here) utilized to control the operation of the style sharing/shifting and style exploration modules. Larger p𝑝p means that our scheme is more likely to be activated. In Table 13, we provide results on various probability values in a single-domain data distribution scenario using PACS dataset. From the results, it is confirm that for all p𝑝p values, the proposed StableFDG outperforms existing baselines, demonstrating that our scheme can work well with an arbitrarily chosen probability p𝑝p. In detail, when p𝑝p ranges from 0.3 to 0.7, the high performance is maintained while the performance decreases at both extreme probabilities (p=0.1𝑝0.1p=0.1 and 0.9). Therefore, it is recommended for practitioners to select the p𝑝p in an appropriate range, avoiding extreme cases.",
            "For implementation, style-based learning is applied only in the 1st, 2nd, 3rd blocks among 4 residual blocks in ResNet-18. Note that at the output of the 4th block, label information is dominant rather than style information, which results in degraded performance when style-based schemes are applied. This is confirmed by our new experiments in the table below. It can be seen from the results that if we consider the 4th residual block to apply our style-based learning, the performance gets degraded. This result confirms the intuition that style-based learning should be conducted at the earlier layers where style information is preserved.",
            "In our main paper, the similarity metric in equation (6) adopts cross-attention, while the metric in equation (8) combines cross-attention and self-attention. When applying only the cross-attention-based metric in equation (6), we found that the similarity value could become low even when the two samples belong to the same class, in special cases. We handled this issue by adding the self-attention component as in equation (8). Intuitively, by doing this, the attention module is learning to extract the important features across images (via cross-attention), and within the image (via self-attention). Table 15 compares the performance of our StableFDG when using (i) self-attention alone, (ii) cross-attention alone (equation (6)), and (iii) both self and cross attentions at the same time (equation (8)), confirming the advantage of using self-attention and cross-attention together.",
            "Our attention module requires 0.44% of additional model parameters to perform the attention-based learning. For a fair comparison to see the effect of our attention-based learning, we consider a different baseline with the same model size but without attention-based learning. Specifically, the baseline computes the attention score map using only additional convolutional operations and take the weighted average of the feature zisubscript𝑧𝑖z_{i} based on the attention score map. Table 16 shows the results using PACS dataset in a single-domain data distribution scenario. The results demonstrate that our attention-based learning achieves performance improvements on all four domains while playing a key role in capturing essential parts of the features.",
            "Now we provide answer to the following question: Instead of the FL setup we focused on, can attention provide benefits in the centralized DG setup? Table 17 shows the results with/without attention module in a centralized setup using PACS dataset. The results show that attention still provides performance improvements in the centralized setup by learning domain-invariant features, although the gain is slightly lower than the gain in the FL setup as shown in Table 3 of the main manuscript. These results indicate that the proposed attention-based learning indeed captures the domain-invariant characteristics of samples, while the scheme provides more benefits in the FL setup where each client is prone to overfitting due to lack of data.",
            "More detailed description on oversampling: Let sn∈ℝB×C×H×Wsuperscript𝑠𝑛superscriptℝ𝐵𝐶𝐻𝑊s^{n}\\in\\mathbb{R}^{B\\times C\\times H\\times W} be a mini-batch of features in client n𝑛n at a specific layer, obtained after Steps 1 and 2 in the main manuscript. Now given a fixed oversampling size, we oversample the features in the mini-batch to obtain s~nsuperscript~𝑠𝑛\\tilde{s}^{n}, so that the concatenated mini-batch s^n=[sn,s~n]superscript^𝑠𝑛superscript𝑠𝑛superscript~𝑠𝑛\\hat{s}^{n}=[s^{n},\\tilde{s}^{n}] becomes class-balanced as much as possible. Consider a toy example where the number of samples for classes a𝑎a, b𝑏b, c𝑐c are 3, 2, 1, respectively in the mini-batch snsuperscript𝑠𝑛s^{n}. In this example, if the oversampling size is 3, we randomly choose one data point from class b𝑏b\nand two data points from class c𝑐c (in this case, the same data point is selected for two times with duplication) to obtain s~nsuperscript~𝑠𝑛\\tilde{s}^{n}, so that the concatenated mini-batch s^n=[sn,s~n]superscript^𝑠𝑛superscript𝑠𝑛superscript~𝑠𝑛\\hat{s}^{n}=[s^{n},\\tilde{s}^{n}] becomes class-balanced. If the oversampling size is 1, we oversample one data point in class c𝑐c to make the concatenated mini-batch to be balanced as much as possible. If the oversampling size is 6, we oversample 1, 2, 3 samples from classes a𝑎a, b𝑏b, c𝑐c, respectively to construct s~nsuperscript~𝑠𝑛\\tilde{s}^{n}. The concatenated mini-batch s^n=[sn,s~n]superscript^𝑠𝑛superscript𝑠𝑛superscript~𝑠𝑛\\hat{s}^{n}=[s^{n},\\tilde{s}^{n}] is utilized for style-based learning and updating the model. This process not only mitigates the class-imbalance issue in each client but also provides a good platform for style exploration by oversampling the features. In our work, we reported the results with oversampling size of B𝐵B (which is equal to the mini-batch size), while the effect of the oversampling size is also reported in Fig. 4 of the main manuscript: A larger oversampling size leads to a better performance, and more importantly, our StableFDG outperforms the baseline even without any oversampling.",
            "Algorithm 1 summarizes the overall process of our StableFDG."
        ]
    },
    "A4.T8": {
        "caption": "Table 8: Effect of each component on VLCS dataset in a multi-domain data distribution scenario. The reported results indicate (mean ±plus-or-minus\\pm 95% confidence interval) over 3 random trials.",
        "table": "<table id=\"A4.T8.20\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A4.T8.20.21.1\" class=\"ltx_tr\">\n<th id=\"A4.T8.20.21.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"A4.T8.20.21.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Methods</span></th>\n<th id=\"A4.T8.20.21.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A4.T8.20.21.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Caltech</span></th>\n<th id=\"A4.T8.20.21.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A4.T8.20.21.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">Labelme</span></th>\n<th id=\"A4.T8.20.21.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A4.T8.20.21.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">Pascal</span></th>\n<th id=\"A4.T8.20.21.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"A4.T8.20.21.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Sun</span></th>\n<th id=\"A4.T8.20.21.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A4.T8.20.21.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">Avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A4.T8.5.5\" class=\"ltx_tr\">\n<th id=\"A4.T8.5.5.6\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">\n<span id=\"A4.T8.5.5.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">FedAvg </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A4.T8.5.5.6.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">[</span><a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">26</a><span id=\"A4.T8.5.5.6.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td id=\"A4.T8.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"A4.T8.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">93.65 </span><math id=\"A4.T8.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A4.T8.1.1.1.m1.1a\"><mo mathsize=\"90%\" id=\"A4.T8.1.1.1.m1.1.1\" xref=\"A4.T8.1.1.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A4.T8.1.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"A4.T8.1.1.1.m1.1.1.cmml\" xref=\"A4.T8.1.1.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T8.1.1.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"A4.T8.1.1.1.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 1.59</span>\n</td>\n<td id=\"A4.T8.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"A4.T8.2.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">61.10 </span><math id=\"A4.T8.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A4.T8.2.2.2.m1.1a\"><mo mathsize=\"90%\" id=\"A4.T8.2.2.2.m1.1.1\" xref=\"A4.T8.2.2.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A4.T8.2.2.2.m1.1b\"><csymbol cd=\"latexml\" id=\"A4.T8.2.2.2.m1.1.1.cmml\" xref=\"A4.T8.2.2.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T8.2.2.2.m1.1c\">\\pm</annotation></semantics></math><span id=\"A4.T8.2.2.2.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 2.08</span>\n</td>\n<td id=\"A4.T8.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"A4.T8.3.3.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">72.55 </span><math id=\"A4.T8.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A4.T8.3.3.3.m1.1a\"><mo mathsize=\"90%\" id=\"A4.T8.3.3.3.m1.1.1\" xref=\"A4.T8.3.3.3.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A4.T8.3.3.3.m1.1b\"><csymbol cd=\"latexml\" id=\"A4.T8.3.3.3.m1.1.1.cmml\" xref=\"A4.T8.3.3.3.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T8.3.3.3.m1.1c\">\\pm</annotation></semantics></math><span id=\"A4.T8.3.3.3.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 0.90</span>\n</td>\n<td id=\"A4.T8.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"A4.T8.4.4.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">65.40 </span><math id=\"A4.T8.4.4.4.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A4.T8.4.4.4.m1.1a\"><mo mathsize=\"90%\" id=\"A4.T8.4.4.4.m1.1.1\" xref=\"A4.T8.4.4.4.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A4.T8.4.4.4.m1.1b\"><csymbol cd=\"latexml\" id=\"A4.T8.4.4.4.m1.1.1.cmml\" xref=\"A4.T8.4.4.4.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T8.4.4.4.m1.1c\">\\pm</annotation></semantics></math><span id=\"A4.T8.4.4.4.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 0.28</span>\n</td>\n<td id=\"A4.T8.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"A4.T8.5.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">73.18 </span><math id=\"A4.T8.5.5.5.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A4.T8.5.5.5.m1.1a\"><mo mathsize=\"90%\" id=\"A4.T8.5.5.5.m1.1.1\" xref=\"A4.T8.5.5.5.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A4.T8.5.5.5.m1.1b\"><csymbol cd=\"latexml\" id=\"A4.T8.5.5.5.m1.1.1.cmml\" xref=\"A4.T8.5.5.5.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T8.5.5.5.m1.1c\">\\pm</annotation></semantics></math><span id=\"A4.T8.5.5.5.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 0.27</span>\n</td>\n</tr>\n<tr id=\"A4.T8.10.10\" class=\"ltx_tr\">\n<th id=\"A4.T8.10.10.6\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"A4.T8.10.10.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">StableFDG (only style)</span></th>\n<td id=\"A4.T8.6.6.1\" class=\"ltx_td ltx_align_center\">\n<span id=\"A4.T8.6.6.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">98.19 </span><math id=\"A4.T8.6.6.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A4.T8.6.6.1.m1.1a\"><mo mathsize=\"90%\" id=\"A4.T8.6.6.1.m1.1.1\" xref=\"A4.T8.6.6.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A4.T8.6.6.1.m1.1b\"><csymbol cd=\"latexml\" id=\"A4.T8.6.6.1.m1.1.1.cmml\" xref=\"A4.T8.6.6.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T8.6.6.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"A4.T8.6.6.1.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 0.81</span>\n</td>\n<td id=\"A4.T8.7.7.2\" class=\"ltx_td ltx_align_center\">\n<span id=\"A4.T8.7.7.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">58.93 </span><math id=\"A4.T8.7.7.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A4.T8.7.7.2.m1.1a\"><mo mathsize=\"90%\" id=\"A4.T8.7.7.2.m1.1.1\" xref=\"A4.T8.7.7.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A4.T8.7.7.2.m1.1b\"><csymbol cd=\"latexml\" id=\"A4.T8.7.7.2.m1.1.1.cmml\" xref=\"A4.T8.7.7.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T8.7.7.2.m1.1c\">\\pm</annotation></semantics></math><span id=\"A4.T8.7.7.2.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 1.16</span>\n</td>\n<td id=\"A4.T8.8.8.3\" class=\"ltx_td ltx_align_center\">\n<span id=\"A4.T8.8.8.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">75.19 </span><math id=\"A4.T8.8.8.3.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A4.T8.8.8.3.m1.1a\"><mo mathsize=\"90%\" id=\"A4.T8.8.8.3.m1.1.1\" xref=\"A4.T8.8.8.3.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A4.T8.8.8.3.m1.1b\"><csymbol cd=\"latexml\" id=\"A4.T8.8.8.3.m1.1.1.cmml\" xref=\"A4.T8.8.8.3.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T8.8.8.3.m1.1c\">\\pm</annotation></semantics></math><span id=\"A4.T8.8.8.3.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 0.67</span>\n</td>\n<td id=\"A4.T8.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_r\">\n<span id=\"A4.T8.9.9.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">68.69 </span><math id=\"A4.T8.9.9.4.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A4.T8.9.9.4.m1.1a\"><mo mathsize=\"90%\" id=\"A4.T8.9.9.4.m1.1.1\" xref=\"A4.T8.9.9.4.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A4.T8.9.9.4.m1.1b\"><csymbol cd=\"latexml\" id=\"A4.T8.9.9.4.m1.1.1.cmml\" xref=\"A4.T8.9.9.4.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T8.9.9.4.m1.1c\">\\pm</annotation></semantics></math><span id=\"A4.T8.9.9.4.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 0.73</span>\n</td>\n<td id=\"A4.T8.10.10.5\" class=\"ltx_td ltx_align_center\">\n<span id=\"A4.T8.10.10.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">75.25 </span><math id=\"A4.T8.10.10.5.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A4.T8.10.10.5.m1.1a\"><mo mathsize=\"90%\" id=\"A4.T8.10.10.5.m1.1.1\" xref=\"A4.T8.10.10.5.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A4.T8.10.10.5.m1.1b\"><csymbol cd=\"latexml\" id=\"A4.T8.10.10.5.m1.1.1.cmml\" xref=\"A4.T8.10.10.5.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T8.10.10.5.m1.1c\">\\pm</annotation></semantics></math><span id=\"A4.T8.10.10.5.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 0.20</span>\n</td>\n</tr>\n<tr id=\"A4.T8.15.15\" class=\"ltx_tr\">\n<th id=\"A4.T8.15.15.6\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"A4.T8.15.15.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">StableFDG (only attetnion)</span></th>\n<td id=\"A4.T8.11.11.1\" class=\"ltx_td ltx_align_center\">\n<span id=\"A4.T8.11.11.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">94.10 </span><math id=\"A4.T8.11.11.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A4.T8.11.11.1.m1.1a\"><mo mathsize=\"90%\" id=\"A4.T8.11.11.1.m1.1.1\" xref=\"A4.T8.11.11.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A4.T8.11.11.1.m1.1b\"><csymbol cd=\"latexml\" id=\"A4.T8.11.11.1.m1.1.1.cmml\" xref=\"A4.T8.11.11.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T8.11.11.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"A4.T8.11.11.1.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 1.22</span>\n</td>\n<td id=\"A4.T8.12.12.2\" class=\"ltx_td ltx_align_center\">\n<span id=\"A4.T8.12.12.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">62.02 </span><math id=\"A4.T8.12.12.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A4.T8.12.12.2.m1.1a\"><mo mathsize=\"90%\" id=\"A4.T8.12.12.2.m1.1.1\" xref=\"A4.T8.12.12.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A4.T8.12.12.2.m1.1b\"><csymbol cd=\"latexml\" id=\"A4.T8.12.12.2.m1.1.1.cmml\" xref=\"A4.T8.12.12.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T8.12.12.2.m1.1c\">\\pm</annotation></semantics></math><span id=\"A4.T8.12.12.2.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 1.55</span>\n</td>\n<td id=\"A4.T8.13.13.3\" class=\"ltx_td ltx_align_center\">\n<span id=\"A4.T8.13.13.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">72.26 </span><math id=\"A4.T8.13.13.3.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A4.T8.13.13.3.m1.1a\"><mo mathsize=\"90%\" id=\"A4.T8.13.13.3.m1.1.1\" xref=\"A4.T8.13.13.3.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A4.T8.13.13.3.m1.1b\"><csymbol cd=\"latexml\" id=\"A4.T8.13.13.3.m1.1.1.cmml\" xref=\"A4.T8.13.13.3.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T8.13.13.3.m1.1c\">\\pm</annotation></semantics></math><span id=\"A4.T8.13.13.3.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 0.78</span>\n</td>\n<td id=\"A4.T8.14.14.4\" class=\"ltx_td ltx_align_center ltx_border_r\">\n<span id=\"A4.T8.14.14.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">66.16 </span><math id=\"A4.T8.14.14.4.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A4.T8.14.14.4.m1.1a\"><mo mathsize=\"90%\" id=\"A4.T8.14.14.4.m1.1.1\" xref=\"A4.T8.14.14.4.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A4.T8.14.14.4.m1.1b\"><csymbol cd=\"latexml\" id=\"A4.T8.14.14.4.m1.1.1.cmml\" xref=\"A4.T8.14.14.4.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T8.14.14.4.m1.1c\">\\pm</annotation></semantics></math><span id=\"A4.T8.14.14.4.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 2.58</span>\n</td>\n<td id=\"A4.T8.15.15.5\" class=\"ltx_td ltx_align_center\">\n<span id=\"A4.T8.15.15.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">73.64 </span><math id=\"A4.T8.15.15.5.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A4.T8.15.15.5.m1.1a\"><mo mathsize=\"90%\" id=\"A4.T8.15.15.5.m1.1.1\" xref=\"A4.T8.15.15.5.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A4.T8.15.15.5.m1.1b\"><csymbol cd=\"latexml\" id=\"A4.T8.15.15.5.m1.1.1.cmml\" xref=\"A4.T8.15.15.5.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T8.15.15.5.m1.1c\">\\pm</annotation></semantics></math><span id=\"A4.T8.15.15.5.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 0.89</span>\n</td>\n</tr>\n<tr id=\"A4.T8.20.20\" class=\"ltx_tr\">\n<th id=\"A4.T8.20.20.6\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span id=\"A4.T8.20.20.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">StableFDG (both)</span></th>\n<td id=\"A4.T8.16.16.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"A4.T8.16.16.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">98.50 </span><math id=\"A4.T8.16.16.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A4.T8.16.16.1.m1.1a\"><mo mathsize=\"90%\" id=\"A4.T8.16.16.1.m1.1.1\" xref=\"A4.T8.16.16.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A4.T8.16.16.1.m1.1b\"><csymbol cd=\"latexml\" id=\"A4.T8.16.16.1.m1.1.1.cmml\" xref=\"A4.T8.16.16.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T8.16.16.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"A4.T8.16.16.1.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 0.17</span>\n</td>\n<td id=\"A4.T8.17.17.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"A4.T8.17.17.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">60.07 </span><math id=\"A4.T8.17.17.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A4.T8.17.17.2.m1.1a\"><mo mathsize=\"90%\" id=\"A4.T8.17.17.2.m1.1.1\" xref=\"A4.T8.17.17.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A4.T8.17.17.2.m1.1b\"><csymbol cd=\"latexml\" id=\"A4.T8.17.17.2.m1.1.1.cmml\" xref=\"A4.T8.17.17.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T8.17.17.2.m1.1c\">\\pm</annotation></semantics></math><span id=\"A4.T8.17.17.2.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 0.79</span>\n</td>\n<td id=\"A4.T8.18.18.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"A4.T8.18.18.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">74.40 </span><math id=\"A4.T8.18.18.3.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A4.T8.18.18.3.m1.1a\"><mo mathsize=\"90%\" id=\"A4.T8.18.18.3.m1.1.1\" xref=\"A4.T8.18.18.3.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A4.T8.18.18.3.m1.1b\"><csymbol cd=\"latexml\" id=\"A4.T8.18.18.3.m1.1.1.cmml\" xref=\"A4.T8.18.18.3.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T8.18.18.3.m1.1c\">\\pm</annotation></semantics></math><span id=\"A4.T8.18.18.3.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 1.88</span>\n</td>\n<td id=\"A4.T8.19.19.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">\n<span id=\"A4.T8.19.19.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">69.43 </span><math id=\"A4.T8.19.19.4.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A4.T8.19.19.4.m1.1a\"><mo mathsize=\"90%\" id=\"A4.T8.19.19.4.m1.1.1\" xref=\"A4.T8.19.19.4.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A4.T8.19.19.4.m1.1b\"><csymbol cd=\"latexml\" id=\"A4.T8.19.19.4.m1.1.1.cmml\" xref=\"A4.T8.19.19.4.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T8.19.19.4.m1.1c\">\\pm</annotation></semantics></math><span id=\"A4.T8.19.19.4.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 1.11</span>\n</td>\n<td id=\"A4.T8.20.20.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"A4.T8.20.20.5.1\" class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">75.61</span><span id=\"A4.T8.20.20.5.2\" class=\"ltx_text\" style=\"font-size:90%;\"> </span><math id=\"A4.T8.20.20.5.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A4.T8.20.20.5.m1.1a\"><mo mathsize=\"90%\" id=\"A4.T8.20.20.5.m1.1.1\" xref=\"A4.T8.20.20.5.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A4.T8.20.20.5.m1.1b\"><csymbol cd=\"latexml\" id=\"A4.T8.20.20.5.m1.1.1.cmml\" xref=\"A4.T8.20.20.5.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T8.20.20.5.m1.1c\">\\pm</annotation></semantics></math><span id=\"A4.T8.20.20.5.3\" class=\"ltx_text\" style=\"font-size:90%;\"> 0.71</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "As mentioned in the main manuscript, we provide further ablation studies on the effect of each component in StableFDG using VLCS dataset. Table 8 shows that each component individually brings performance gain compared to FedAvg. Using both strategies achieves greater performance gains, confirming that the two proposed schemes work in a complementary fashion."
        ]
    },
    "A5.T9": {
        "caption": "Table 9: Random sharing vs receiving the style that has the largest distance in style space. The reported results indicate (mean ±plus-or-minus\\pm 95% confidence interval) over 3 random trials.",
        "table": "<table id=\"A5.T9.10\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A5.T9.10.11.1\" class=\"ltx_tr\">\n<th id=\"A5.T9.10.11.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"A5.T9.10.11.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Methods</span></th>\n<th id=\"A5.T9.10.11.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A5.T9.10.11.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">Art</span></th>\n<th id=\"A5.T9.10.11.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A5.T9.10.11.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">Cartoon</span></th>\n<th id=\"A5.T9.10.11.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A5.T9.10.11.1.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">Photo</span></th>\n<th id=\"A5.T9.10.11.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"A5.T9.10.11.1.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">Sketch</span></th>\n<th id=\"A5.T9.10.11.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A5.T9.10.11.1.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">Avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A5.T9.5.5\" class=\"ltx_tr\">\n<th id=\"A5.T9.5.5.6\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A5.T9.5.5.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">Random sharing (current manuscript)</span></th>\n<td id=\"A5.T9.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"A5.T9.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">83.97 </span><math id=\"A5.T9.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A5.T9.1.1.1.m1.1a\"><mo mathsize=\"70%\" id=\"A5.T9.1.1.1.m1.1.1\" xref=\"A5.T9.1.1.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A5.T9.1.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"A5.T9.1.1.1.m1.1.1.cmml\" xref=\"A5.T9.1.1.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T9.1.1.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"A5.T9.1.1.1.2\" class=\"ltx_text\" style=\"font-size:70%;\"> 1.25</span>\n</td>\n<td id=\"A5.T9.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"A5.T9.2.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">79.10 </span><math id=\"A5.T9.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A5.T9.2.2.2.m1.1a\"><mo mathsize=\"70%\" id=\"A5.T9.2.2.2.m1.1.1\" xref=\"A5.T9.2.2.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A5.T9.2.2.2.m1.1b\"><csymbol cd=\"latexml\" id=\"A5.T9.2.2.2.m1.1.1.cmml\" xref=\"A5.T9.2.2.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T9.2.2.2.m1.1c\">\\pm</annotation></semantics></math><span id=\"A5.T9.2.2.2.2\" class=\"ltx_text\" style=\"font-size:70%;\"> 0.45</span>\n</td>\n<td id=\"A5.T9.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"A5.T9.3.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">96.27 </span><math id=\"A5.T9.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A5.T9.3.3.3.m1.1a\"><mo mathsize=\"70%\" id=\"A5.T9.3.3.3.m1.1.1\" xref=\"A5.T9.3.3.3.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A5.T9.3.3.3.m1.1b\"><csymbol cd=\"latexml\" id=\"A5.T9.3.3.3.m1.1.1.cmml\" xref=\"A5.T9.3.3.3.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T9.3.3.3.m1.1c\">\\pm</annotation></semantics></math><span id=\"A5.T9.3.3.3.2\" class=\"ltx_text\" style=\"font-size:70%;\"> 0.36</span>\n</td>\n<td id=\"A5.T9.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"A5.T9.4.4.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">75.67 </span><math id=\"A5.T9.4.4.4.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A5.T9.4.4.4.m1.1a\"><mo mathsize=\"70%\" id=\"A5.T9.4.4.4.m1.1.1\" xref=\"A5.T9.4.4.4.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A5.T9.4.4.4.m1.1b\"><csymbol cd=\"latexml\" id=\"A5.T9.4.4.4.m1.1.1.cmml\" xref=\"A5.T9.4.4.4.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T9.4.4.4.m1.1c\">\\pm</annotation></semantics></math><span id=\"A5.T9.4.4.4.2\" class=\"ltx_text\" style=\"font-size:70%;\"> 0.58</span>\n</td>\n<td id=\"A5.T9.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"A5.T9.5.5.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">83.75 </span><math id=\"A5.T9.5.5.5.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A5.T9.5.5.5.m1.1a\"><mo mathsize=\"70%\" id=\"A5.T9.5.5.5.m1.1.1\" xref=\"A5.T9.5.5.5.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A5.T9.5.5.5.m1.1b\"><csymbol cd=\"latexml\" id=\"A5.T9.5.5.5.m1.1.1.cmml\" xref=\"A5.T9.5.5.5.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T9.5.5.5.m1.1c\">\\pm</annotation></semantics></math><span id=\"A5.T9.5.5.5.2\" class=\"ltx_text\" style=\"font-size:70%;\"> 0.23</span>\n</td>\n</tr>\n<tr id=\"A5.T9.10.10\" class=\"ltx_tr\">\n<th id=\"A5.T9.10.10.6\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span id=\"A5.T9.10.10.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">Large distance</span></th>\n<td id=\"A5.T9.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"A5.T9.6.6.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">83.01 </span><math id=\"A5.T9.6.6.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A5.T9.6.6.1.m1.1a\"><mo mathsize=\"70%\" id=\"A5.T9.6.6.1.m1.1.1\" xref=\"A5.T9.6.6.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A5.T9.6.6.1.m1.1b\"><csymbol cd=\"latexml\" id=\"A5.T9.6.6.1.m1.1.1.cmml\" xref=\"A5.T9.6.6.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T9.6.6.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"A5.T9.6.6.1.2\" class=\"ltx_text\" style=\"font-size:70%;\"> 1.43</span>\n</td>\n<td id=\"A5.T9.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"A5.T9.7.7.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">78.33 </span><math id=\"A5.T9.7.7.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A5.T9.7.7.2.m1.1a\"><mo mathsize=\"70%\" id=\"A5.T9.7.7.2.m1.1.1\" xref=\"A5.T9.7.7.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A5.T9.7.7.2.m1.1b\"><csymbol cd=\"latexml\" id=\"A5.T9.7.7.2.m1.1.1.cmml\" xref=\"A5.T9.7.7.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T9.7.7.2.m1.1c\">\\pm</annotation></semantics></math><span id=\"A5.T9.7.7.2.2\" class=\"ltx_text\" style=\"font-size:70%;\"> 0.51</span>\n</td>\n<td id=\"A5.T9.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"A5.T9.8.8.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">96.19 </span><math id=\"A5.T9.8.8.3.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A5.T9.8.8.3.m1.1a\"><mo mathsize=\"70%\" id=\"A5.T9.8.8.3.m1.1.1\" xref=\"A5.T9.8.8.3.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A5.T9.8.8.3.m1.1b\"><csymbol cd=\"latexml\" id=\"A5.T9.8.8.3.m1.1.1.cmml\" xref=\"A5.T9.8.8.3.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T9.8.8.3.m1.1c\">\\pm</annotation></semantics></math><span id=\"A5.T9.8.8.3.2\" class=\"ltx_text\" style=\"font-size:70%;\"> 0.76</span>\n</td>\n<td id=\"A5.T9.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">\n<span id=\"A5.T9.9.9.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.63 </span><math id=\"A5.T9.9.9.4.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A5.T9.9.9.4.m1.1a\"><mo mathsize=\"70%\" id=\"A5.T9.9.9.4.m1.1.1\" xref=\"A5.T9.9.9.4.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A5.T9.9.9.4.m1.1b\"><csymbol cd=\"latexml\" id=\"A5.T9.9.9.4.m1.1.1.cmml\" xref=\"A5.T9.9.9.4.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T9.9.9.4.m1.1c\">\\pm</annotation></semantics></math><span id=\"A5.T9.9.9.4.2\" class=\"ltx_text\" style=\"font-size:70%;\"> 0.65</span>\n</td>\n<td id=\"A5.T9.10.10.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"A5.T9.10.10.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">83.04 </span><math id=\"A5.T9.10.10.5.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A5.T9.10.10.5.m1.1a\"><mo mathsize=\"70%\" id=\"A5.T9.10.10.5.m1.1.1\" xref=\"A5.T9.10.10.5.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A5.T9.10.10.5.m1.1b\"><csymbol cd=\"latexml\" id=\"A5.T9.10.10.5.m1.1.1.cmml\" xref=\"A5.T9.10.10.5.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T9.10.10.5.m1.1c\">\\pm</annotation></semantics></math><span id=\"A5.T9.10.10.5.2\" class=\"ltx_text\" style=\"font-size:70%;\"> 0.30</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "In our style based learning, style sharing among clients is performed at random. However, one can think of the strategy where client n𝑛n receives the style information Φn′subscriptΦsuperscript𝑛′\\Phi_{n^{\\prime}} that has the largest distance with its own style information Φn′subscriptΦsuperscript𝑛′\\Phi_{n^{\\prime}} in the style space. Table 9 shows the corresponding result using PACS dataset in a multi-domain data distribution setup. Interestingly, it can be seen that the random selection adopted in this paper performs better, since most of the users generally tend to receive the same style statistics when using the largest distance strategy."
        ]
    },
    "A5.T10": {
        "caption": "Table 10: Effect of the number of shared styles. The reported results indicate (mean ±plus-or-minus\\pm 95% confidence interval) over 3 random trials. ",
        "table": "<table id=\"A5.T10.15\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A5.T10.15.16.1\" class=\"ltx_tr\">\n<th id=\"A5.T10.15.16.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"A5.T10.15.16.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Methods</span></th>\n<th id=\"A5.T10.15.16.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A5.T10.15.16.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">Art</span></th>\n<th id=\"A5.T10.15.16.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A5.T10.15.16.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">Cartoon</span></th>\n<th id=\"A5.T10.15.16.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A5.T10.15.16.1.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">Photo</span></th>\n<th id=\"A5.T10.15.16.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"A5.T10.15.16.1.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">Sketch</span></th>\n<th id=\"A5.T10.15.16.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A5.T10.15.16.1.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">Avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A5.T10.5.5\" class=\"ltx_tr\">\n<th id=\"A5.T10.5.5.6\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A5.T10.5.5.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">No style sharing</span></th>\n<td id=\"A5.T10.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"A5.T10.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">83.16 </span><math id=\"A5.T10.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A5.T10.1.1.1.m1.1a\"><mo mathsize=\"70%\" id=\"A5.T10.1.1.1.m1.1.1\" xref=\"A5.T10.1.1.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A5.T10.1.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"A5.T10.1.1.1.m1.1.1.cmml\" xref=\"A5.T10.1.1.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T10.1.1.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"A5.T10.1.1.1.2\" class=\"ltx_text\" style=\"font-size:70%;\"> 0.18</span>\n</td>\n<td id=\"A5.T10.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"A5.T10.2.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">78.54 </span><math id=\"A5.T10.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A5.T10.2.2.2.m1.1a\"><mo mathsize=\"70%\" id=\"A5.T10.2.2.2.m1.1.1\" xref=\"A5.T10.2.2.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A5.T10.2.2.2.m1.1b\"><csymbol cd=\"latexml\" id=\"A5.T10.2.2.2.m1.1.1.cmml\" xref=\"A5.T10.2.2.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T10.2.2.2.m1.1c\">\\pm</annotation></semantics></math><span id=\"A5.T10.2.2.2.2\" class=\"ltx_text\" style=\"font-size:70%;\"> 1.11</span>\n</td>\n<td id=\"A5.T10.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"A5.T10.3.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">95.56 </span><math id=\"A5.T10.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A5.T10.3.3.3.m1.1a\"><mo mathsize=\"70%\" id=\"A5.T10.3.3.3.m1.1.1\" xref=\"A5.T10.3.3.3.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A5.T10.3.3.3.m1.1b\"><csymbol cd=\"latexml\" id=\"A5.T10.3.3.3.m1.1.1.cmml\" xref=\"A5.T10.3.3.3.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T10.3.3.3.m1.1c\">\\pm</annotation></semantics></math><span id=\"A5.T10.3.3.3.2\" class=\"ltx_text\" style=\"font-size:70%;\"> 0.92</span>\n</td>\n<td id=\"A5.T10.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"A5.T10.4.4.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">74.64 </span><math id=\"A5.T10.4.4.4.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A5.T10.4.4.4.m1.1a\"><mo mathsize=\"70%\" id=\"A5.T10.4.4.4.m1.1.1\" xref=\"A5.T10.4.4.4.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A5.T10.4.4.4.m1.1b\"><csymbol cd=\"latexml\" id=\"A5.T10.4.4.4.m1.1.1.cmml\" xref=\"A5.T10.4.4.4.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T10.4.4.4.m1.1c\">\\pm</annotation></semantics></math><span id=\"A5.T10.4.4.4.2\" class=\"ltx_text\" style=\"font-size:70%;\"> 1.06</span>\n</td>\n<td id=\"A5.T10.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"A5.T10.5.5.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">82.97 </span><math id=\"A5.T10.5.5.5.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A5.T10.5.5.5.m1.1a\"><mo mathsize=\"70%\" id=\"A5.T10.5.5.5.m1.1.1\" xref=\"A5.T10.5.5.5.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A5.T10.5.5.5.m1.1b\"><csymbol cd=\"latexml\" id=\"A5.T10.5.5.5.m1.1.1.cmml\" xref=\"A5.T10.5.5.5.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T10.5.5.5.m1.1c\">\\pm</annotation></semantics></math><span id=\"A5.T10.5.5.5.2\" class=\"ltx_text\" style=\"font-size:70%;\"> 0.36</span>\n</td>\n</tr>\n<tr id=\"A5.T10.10.10\" class=\"ltx_tr\">\n<th id=\"A5.T10.10.10.6\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"A5.T10.10.10.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">Receive 1 style (current manuscript)</span></th>\n<td id=\"A5.T10.6.6.1\" class=\"ltx_td ltx_align_center\">\n<span id=\"A5.T10.6.6.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">83.97 </span><math id=\"A5.T10.6.6.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A5.T10.6.6.1.m1.1a\"><mo mathsize=\"70%\" id=\"A5.T10.6.6.1.m1.1.1\" xref=\"A5.T10.6.6.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A5.T10.6.6.1.m1.1b\"><csymbol cd=\"latexml\" id=\"A5.T10.6.6.1.m1.1.1.cmml\" xref=\"A5.T10.6.6.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T10.6.6.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"A5.T10.6.6.1.2\" class=\"ltx_text\" style=\"font-size:70%;\"> 1.25</span>\n</td>\n<td id=\"A5.T10.7.7.2\" class=\"ltx_td ltx_align_center\">\n<span id=\"A5.T10.7.7.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">79.10 </span><math id=\"A5.T10.7.7.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A5.T10.7.7.2.m1.1a\"><mo mathsize=\"70%\" id=\"A5.T10.7.7.2.m1.1.1\" xref=\"A5.T10.7.7.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A5.T10.7.7.2.m1.1b\"><csymbol cd=\"latexml\" id=\"A5.T10.7.7.2.m1.1.1.cmml\" xref=\"A5.T10.7.7.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T10.7.7.2.m1.1c\">\\pm</annotation></semantics></math><span id=\"A5.T10.7.7.2.2\" class=\"ltx_text\" style=\"font-size:70%;\"> 0.45</span>\n</td>\n<td id=\"A5.T10.8.8.3\" class=\"ltx_td ltx_align_center\">\n<span id=\"A5.T10.8.8.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">96.27 </span><math id=\"A5.T10.8.8.3.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A5.T10.8.8.3.m1.1a\"><mo mathsize=\"70%\" id=\"A5.T10.8.8.3.m1.1.1\" xref=\"A5.T10.8.8.3.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A5.T10.8.8.3.m1.1b\"><csymbol cd=\"latexml\" id=\"A5.T10.8.8.3.m1.1.1.cmml\" xref=\"A5.T10.8.8.3.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T10.8.8.3.m1.1c\">\\pm</annotation></semantics></math><span id=\"A5.T10.8.8.3.2\" class=\"ltx_text\" style=\"font-size:70%;\"> 0.36</span>\n</td>\n<td id=\"A5.T10.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_r\">\n<span id=\"A5.T10.9.9.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">75.67 </span><math id=\"A5.T10.9.9.4.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A5.T10.9.9.4.m1.1a\"><mo mathsize=\"70%\" id=\"A5.T10.9.9.4.m1.1.1\" xref=\"A5.T10.9.9.4.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A5.T10.9.9.4.m1.1b\"><csymbol cd=\"latexml\" id=\"A5.T10.9.9.4.m1.1.1.cmml\" xref=\"A5.T10.9.9.4.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T10.9.9.4.m1.1c\">\\pm</annotation></semantics></math><span id=\"A5.T10.9.9.4.2\" class=\"ltx_text\" style=\"font-size:70%;\"> 0.58</span>\n</td>\n<td id=\"A5.T10.10.10.5\" class=\"ltx_td ltx_align_center\">\n<span id=\"A5.T10.10.10.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">83.75 </span><math id=\"A5.T10.10.10.5.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A5.T10.10.10.5.m1.1a\"><mo mathsize=\"70%\" id=\"A5.T10.10.10.5.m1.1.1\" xref=\"A5.T10.10.10.5.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A5.T10.10.10.5.m1.1b\"><csymbol cd=\"latexml\" id=\"A5.T10.10.10.5.m1.1.1.cmml\" xref=\"A5.T10.10.10.5.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T10.10.10.5.m1.1c\">\\pm</annotation></semantics></math><span id=\"A5.T10.10.10.5.2\" class=\"ltx_text\" style=\"font-size:70%;\"> 0.23</span>\n</td>\n</tr>\n<tr id=\"A5.T10.15.15\" class=\"ltx_tr\">\n<th id=\"A5.T10.15.15.6\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span id=\"A5.T10.15.15.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">Receive 3 styles</span></th>\n<td id=\"A5.T10.11.11.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"A5.T10.11.11.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">84.55 </span><math id=\"A5.T10.11.11.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A5.T10.11.11.1.m1.1a\"><mo mathsize=\"70%\" id=\"A5.T10.11.11.1.m1.1.1\" xref=\"A5.T10.11.11.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A5.T10.11.11.1.m1.1b\"><csymbol cd=\"latexml\" id=\"A5.T10.11.11.1.m1.1.1.cmml\" xref=\"A5.T10.11.11.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T10.11.11.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"A5.T10.11.11.1.2\" class=\"ltx_text\" style=\"font-size:70%;\"> 1.20</span>\n</td>\n<td id=\"A5.T10.12.12.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"A5.T10.12.12.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">78.67 </span><math id=\"A5.T10.12.12.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A5.T10.12.12.2.m1.1a\"><mo mathsize=\"70%\" id=\"A5.T10.12.12.2.m1.1.1\" xref=\"A5.T10.12.12.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A5.T10.12.12.2.m1.1b\"><csymbol cd=\"latexml\" id=\"A5.T10.12.12.2.m1.1.1.cmml\" xref=\"A5.T10.12.12.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T10.12.12.2.m1.1c\">\\pm</annotation></semantics></math><span id=\"A5.T10.12.12.2.2\" class=\"ltx_text\" style=\"font-size:70%;\"> 0.59</span>\n</td>\n<td id=\"A5.T10.13.13.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"A5.T10.13.13.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">95.75 </span><math id=\"A5.T10.13.13.3.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A5.T10.13.13.3.m1.1a\"><mo mathsize=\"70%\" id=\"A5.T10.13.13.3.m1.1.1\" xref=\"A5.T10.13.13.3.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A5.T10.13.13.3.m1.1b\"><csymbol cd=\"latexml\" id=\"A5.T10.13.13.3.m1.1.1.cmml\" xref=\"A5.T10.13.13.3.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T10.13.13.3.m1.1c\">\\pm</annotation></semantics></math><span id=\"A5.T10.13.13.3.2\" class=\"ltx_text\" style=\"font-size:70%;\"> 0.58</span>\n</td>\n<td id=\"A5.T10.14.14.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">\n<span id=\"A5.T10.14.14.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">76.77 </span><math id=\"A5.T10.14.14.4.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A5.T10.14.14.4.m1.1a\"><mo mathsize=\"70%\" id=\"A5.T10.14.14.4.m1.1.1\" xref=\"A5.T10.14.14.4.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A5.T10.14.14.4.m1.1b\"><csymbol cd=\"latexml\" id=\"A5.T10.14.14.4.m1.1.1.cmml\" xref=\"A5.T10.14.14.4.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T10.14.14.4.m1.1c\">\\pm</annotation></semantics></math><span id=\"A5.T10.14.14.4.2\" class=\"ltx_text\" style=\"font-size:70%;\"> 0.27</span>\n</td>\n<td id=\"A5.T10.15.15.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"A5.T10.15.15.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">83.94 </span><math id=\"A5.T10.15.15.5.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A5.T10.15.15.5.m1.1a\"><mo mathsize=\"70%\" id=\"A5.T10.15.15.5.m1.1.1\" xref=\"A5.T10.15.15.5.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A5.T10.15.15.5.m1.1b\"><csymbol cd=\"latexml\" id=\"A5.T10.15.15.5.m1.1.1.cmml\" xref=\"A5.T10.15.15.5.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T10.15.15.5.m1.1c\">\\pm</annotation></semantics></math><span id=\"A5.T10.15.15.5.2\" class=\"ltx_text\" style=\"font-size:70%;\"> 0.07</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We also compare the effect of number of styles received at each client in Table 10 using PACS dataset in a multi-domain data distribution setup. The performance increases as the number of received styles increases, with small additional communication load (the vector length of the style information is 128, which is negligible compared to the number of model parameters, which is 11,180,103)."
        ]
    },
    "A5.T11": {
        "caption": "Table 11: Effect of k𝑘k-means++ in style shifting on PACS dataset. ",
        "table": "<table id=\"A5.T11.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A5.T11.3.4.1\" class=\"ltx_tr\">\n<th id=\"A5.T11.3.4.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"A5.T11.3.4.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Methods</span></th>\n<th id=\"A5.T11.3.4.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A5.T11.3.4.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Art</span></th>\n<th id=\"A5.T11.3.4.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A5.T11.3.4.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">Cartoon</span></th>\n<th id=\"A5.T11.3.4.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A5.T11.3.4.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">Photo</span></th>\n<th id=\"A5.T11.3.4.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"A5.T11.3.4.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Sketch</span></th>\n<th id=\"A5.T11.3.4.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A5.T11.3.4.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">Avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A5.T11.1.1\" class=\"ltx_tr\">\n<th id=\"A5.T11.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">\n<span id=\"A5.T11.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Shifting </span><math id=\"A5.T11.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"B/2\" display=\"inline\"><semantics id=\"A5.T11.1.1.1.m1.1a\"><mrow id=\"A5.T11.1.1.1.m1.1.1\" xref=\"A5.T11.1.1.1.m1.1.1.cmml\"><mi mathsize=\"90%\" id=\"A5.T11.1.1.1.m1.1.1.2\" xref=\"A5.T11.1.1.1.m1.1.1.2.cmml\">B</mi><mo maxsize=\"90%\" minsize=\"90%\" stretchy=\"true\" symmetric=\"true\" id=\"A5.T11.1.1.1.m1.1.1.1\" xref=\"A5.T11.1.1.1.m1.1.1.1.cmml\">/</mo><mn mathsize=\"90%\" id=\"A5.T11.1.1.1.m1.1.1.3\" xref=\"A5.T11.1.1.1.m1.1.1.3.cmml\">2</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A5.T11.1.1.1.m1.1b\"><apply id=\"A5.T11.1.1.1.m1.1.1.cmml\" xref=\"A5.T11.1.1.1.m1.1.1\"><divide id=\"A5.T11.1.1.1.m1.1.1.1.cmml\" xref=\"A5.T11.1.1.1.m1.1.1.1\"></divide><ci id=\"A5.T11.1.1.1.m1.1.1.2.cmml\" xref=\"A5.T11.1.1.1.m1.1.1.2\">𝐵</ci><cn type=\"integer\" id=\"A5.T11.1.1.1.m1.1.1.3.cmml\" xref=\"A5.T11.1.1.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T11.1.1.1.m1.1c\">B/2</annotation></semantics></math><span id=\"A5.T11.1.1.1.2\" class=\"ltx_text\" style=\"font-size:90%;\"> random styles</span>\n</th>\n<td id=\"A5.T11.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A5.T11.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">83.69</span></td>\n<td id=\"A5.T11.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A5.T11.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">79.61</span></td>\n<td id=\"A5.T11.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A5.T11.1.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">95.99</span></td>\n<td id=\"A5.T11.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"A5.T11.1.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">73.78</span></td>\n<td id=\"A5.T11.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A5.T11.1.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">83.27</span></td>\n</tr>\n<tr id=\"A5.T11.3.3\" class=\"ltx_tr\">\n<th id=\"A5.T11.3.3.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">\n<span id=\"A5.T11.3.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Shifting </span><math id=\"A5.T11.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"B/2\" display=\"inline\"><semantics id=\"A5.T11.2.2.1.m1.1a\"><mrow id=\"A5.T11.2.2.1.m1.1.1\" xref=\"A5.T11.2.2.1.m1.1.1.cmml\"><mi mathsize=\"90%\" id=\"A5.T11.2.2.1.m1.1.1.2\" xref=\"A5.T11.2.2.1.m1.1.1.2.cmml\">B</mi><mo maxsize=\"90%\" minsize=\"90%\" stretchy=\"true\" symmetric=\"true\" id=\"A5.T11.2.2.1.m1.1.1.1\" xref=\"A5.T11.2.2.1.m1.1.1.1.cmml\">/</mo><mn mathsize=\"90%\" id=\"A5.T11.2.2.1.m1.1.1.3\" xref=\"A5.T11.2.2.1.m1.1.1.3.cmml\">2</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A5.T11.2.2.1.m1.1b\"><apply id=\"A5.T11.2.2.1.m1.1.1.cmml\" xref=\"A5.T11.2.2.1.m1.1.1\"><divide id=\"A5.T11.2.2.1.m1.1.1.1.cmml\" xref=\"A5.T11.2.2.1.m1.1.1.1\"></divide><ci id=\"A5.T11.2.2.1.m1.1.1.2.cmml\" xref=\"A5.T11.2.2.1.m1.1.1.2\">𝐵</ci><cn type=\"integer\" id=\"A5.T11.2.2.1.m1.1.1.3.cmml\" xref=\"A5.T11.2.2.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T11.2.2.1.m1.1c\">B/2</annotation></semantics></math><span id=\"A5.T11.3.3.2.2\" class=\"ltx_text\" style=\"font-size:90%;\"> styles via </span><math id=\"A5.T11.3.3.2.m2.1\" class=\"ltx_Math\" alttext=\"k\" display=\"inline\"><semantics id=\"A5.T11.3.3.2.m2.1a\"><mi mathsize=\"90%\" id=\"A5.T11.3.3.2.m2.1.1\" xref=\"A5.T11.3.3.2.m2.1.1.cmml\">k</mi><annotation-xml encoding=\"MathML-Content\" id=\"A5.T11.3.3.2.m2.1b\"><ci id=\"A5.T11.3.3.2.m2.1.1.cmml\" xref=\"A5.T11.3.3.2.m2.1.1\">𝑘</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T11.3.3.2.m2.1c\">k</annotation></semantics></math><span id=\"A5.T11.3.3.2.3\" class=\"ltx_text\" style=\"font-size:90%;\">-means++</span>\n</th>\n<td id=\"A5.T11.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A5.T11.3.3.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">83.97</span></td>\n<td id=\"A5.T11.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A5.T11.3.3.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">79.10</span></td>\n<td id=\"A5.T11.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A5.T11.3.3.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">96.27</span></td>\n<td id=\"A5.T11.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"A5.T11.3.3.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">75.67</span></td>\n<td id=\"A5.T11.3.3.7\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A5.T11.3.3.7.1\" class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">83.75</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "In Section 3.1, we utilized the k𝑘k-means++ as a tool for facilitating our key idea in style shifting, which is to effectively balance between the original source domain and the new source domain for better generalization; k-means++ plays a role to select the B/2𝐵2B/2 styles that are similar to the remaining B/2𝐵2B/2 styles in the mini-batch. By doing so, the model can explore new styles while not losing the performance on the original styles. To see this effect, we compare k𝑘k-means++ vs. random sampling when selecting B/2 samples to be shifted, in Table 11. The results show that strategically selecting the B/2 samples to be shifted achieves better performance especially in the Sketch domain (1.89% gain) that has a large style gap with other domains. We believe that these results motivate and justify our design choice."
        ]
    },
    "A5.T12": {
        "caption": "Table 12: Effect of the class-balanced oversampling on Office-Home dataset in a multi-domain data distribution scenario. ",
        "table": "<table id=\"A5.T12.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A5.T12.2.1.1\" class=\"ltx_tr\">\n<th id=\"A5.T12.2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"A5.T12.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Methods</span></th>\n<th id=\"A5.T12.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A5.T12.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Art</span></th>\n<th id=\"A5.T12.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A5.T12.2.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">Cartoon</span></th>\n<th id=\"A5.T12.2.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A5.T12.2.1.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">Photo</span></th>\n<th id=\"A5.T12.2.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"A5.T12.2.1.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Sketch</span></th>\n<th id=\"A5.T12.2.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A5.T12.2.1.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">Avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A5.T12.2.2.1\" class=\"ltx_tr\">\n<th id=\"A5.T12.2.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A5.T12.2.2.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Random oversampling</span></th>\n<td id=\"A5.T12.2.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A5.T12.2.2.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">56.21</span></td>\n<td id=\"A5.T12.2.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A5.T12.2.2.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">54.43</span></td>\n<td id=\"A5.T12.2.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A5.T12.2.2.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">69.37</span></td>\n<td id=\"A5.T12.2.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"A5.T12.2.2.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">72.29</span></td>\n<td id=\"A5.T12.2.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A5.T12.2.2.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">63.08</span></td>\n</tr>\n<tr id=\"A5.T12.2.3.2\" class=\"ltx_tr\">\n<th id=\"A5.T12.2.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span id=\"A5.T12.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Class-balanced oversampling</span></th>\n<td id=\"A5.T12.2.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A5.T12.2.3.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">57.20</span></td>\n<td id=\"A5.T12.2.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A5.T12.2.3.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">53.15</span></td>\n<td id=\"A5.T12.2.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A5.T12.2.3.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">71.2</span></td>\n<td id=\"A5.T12.2.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"A5.T12.2.3.2.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">74.23</span></td>\n<td id=\"A5.T12.2.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A5.T12.2.3.2.6.1\" class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">64.10</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "In our main paper, we performed the class-balanced oversampling in the feature space to alleviate the class-imbalance issue during style exploration. To confirm the effectiveness of the class-balanced oversampling, we compare it with random oversampling under the same condition where only the style exploration is applied without other components. Table 12 shows the results on Office-Home dataset, where the class distribution is highly imbalanced in a multi-domain data distribution scenario. It can be seen that our class-balanced oversampling achieves higher performance over the simple random sampling, which validates the efficacy of mitigating the class imbalance problem in FL clients."
        ]
    },
    "A5.T13": {
        "caption": "Table 13: Effect of the probability value in our style-based learning on PACS in a single-domain data distribution scenario.",
        "table": "<table id=\"A5.T13.5\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A5.T13.5.6.1\" class=\"ltx_tr\">\n<th id=\"A5.T13.5.6.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"A5.T13.5.6.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Methods</span></th>\n<th id=\"A5.T13.5.6.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A5.T13.5.6.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Art</span></th>\n<th id=\"A5.T13.5.6.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A5.T13.5.6.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">Cartoon</span></th>\n<th id=\"A5.T13.5.6.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A5.T13.5.6.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">Photo</span></th>\n<th id=\"A5.T13.5.6.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"A5.T13.5.6.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Sketch</span></th>\n<th id=\"A5.T13.5.6.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A5.T13.5.6.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">Avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A5.T13.5.7.1\" class=\"ltx_tr\">\n<th id=\"A5.T13.5.7.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">\n<span id=\"A5.T13.5.7.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">MixStyle </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A5.T13.5.7.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">[</span><a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">39</a><span id=\"A5.T13.5.7.1.1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td id=\"A5.T13.5.7.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A5.T13.5.7.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">79.10</span></td>\n<td id=\"A5.T13.5.7.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A5.T13.5.7.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">76.30</span></td>\n<td id=\"A5.T13.5.7.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A5.T13.5.7.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">90.10</span></td>\n<td id=\"A5.T13.5.7.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"A5.T13.5.7.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">60.63</span></td>\n<td id=\"A5.T13.5.7.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A5.T13.5.7.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">76.53</span></td>\n</tr>\n<tr id=\"A5.T13.5.8.2\" class=\"ltx_tr\">\n<th id=\"A5.T13.5.8.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span id=\"A5.T13.5.8.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">DSU </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A5.T13.5.8.2.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">[</span><a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">21</a><span id=\"A5.T13.5.8.2.1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<td id=\"A5.T13.5.8.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"A5.T13.5.8.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">80.43</span></td>\n<td id=\"A5.T13.5.8.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"A5.T13.5.8.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">75.70</span></td>\n<td id=\"A5.T13.5.8.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"A5.T13.5.8.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">92.60</span></td>\n<td id=\"A5.T13.5.8.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"A5.T13.5.8.2.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">69.87</span></td>\n<td id=\"A5.T13.5.8.2.6\" class=\"ltx_td ltx_align_center\"><span id=\"A5.T13.5.8.2.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">79.65</span></td>\n</tr>\n<tr id=\"A5.T13.1.1\" class=\"ltx_tr\">\n<th id=\"A5.T13.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span id=\"A5.T13.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">StableFDG</span><span id=\"A5.T13.1.1.1.2\" class=\"ltx_text\" style=\"font-size:90%;\"> (</span><math id=\"A5.T13.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"p=0.1\" display=\"inline\"><semantics id=\"A5.T13.1.1.1.m1.1a\"><mrow id=\"A5.T13.1.1.1.m1.1.1\" xref=\"A5.T13.1.1.1.m1.1.1.cmml\"><mi mathsize=\"90%\" id=\"A5.T13.1.1.1.m1.1.1.2\" xref=\"A5.T13.1.1.1.m1.1.1.2.cmml\">p</mi><mo mathsize=\"90%\" id=\"A5.T13.1.1.1.m1.1.1.1\" xref=\"A5.T13.1.1.1.m1.1.1.1.cmml\">=</mo><mn mathsize=\"90%\" id=\"A5.T13.1.1.1.m1.1.1.3\" xref=\"A5.T13.1.1.1.m1.1.1.3.cmml\">0.1</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A5.T13.1.1.1.m1.1b\"><apply id=\"A5.T13.1.1.1.m1.1.1.cmml\" xref=\"A5.T13.1.1.1.m1.1.1\"><eq id=\"A5.T13.1.1.1.m1.1.1.1.cmml\" xref=\"A5.T13.1.1.1.m1.1.1.1\"></eq><ci id=\"A5.T13.1.1.1.m1.1.1.2.cmml\" xref=\"A5.T13.1.1.1.m1.1.1.2\">𝑝</ci><cn type=\"float\" id=\"A5.T13.1.1.1.m1.1.1.3.cmml\" xref=\"A5.T13.1.1.1.m1.1.1.3\">0.1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T13.1.1.1.m1.1c\">p=0.1</annotation></semantics></math><span id=\"A5.T13.1.1.1.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span>\n</th>\n<td id=\"A5.T13.1.1.2\" class=\"ltx_td ltx_align_center\"><span id=\"A5.T13.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">82.70</span></td>\n<td id=\"A5.T13.1.1.3\" class=\"ltx_td ltx_align_center\"><span id=\"A5.T13.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">78.30</span></td>\n<td id=\"A5.T13.1.1.4\" class=\"ltx_td ltx_align_center\"><span id=\"A5.T13.1.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">95.30</span></td>\n<td id=\"A5.T13.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"A5.T13.1.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">75.30</span></td>\n<td id=\"A5.T13.1.1.6\" class=\"ltx_td ltx_align_center\"><span id=\"A5.T13.1.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">82.90</span></td>\n</tr>\n<tr id=\"A5.T13.2.2\" class=\"ltx_tr\">\n<th id=\"A5.T13.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span id=\"A5.T13.2.2.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">StableFDG</span><span id=\"A5.T13.2.2.1.2\" class=\"ltx_text\" style=\"font-size:90%;\"> (</span><math id=\"A5.T13.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"p=0.3\" display=\"inline\"><semantics id=\"A5.T13.2.2.1.m1.1a\"><mrow id=\"A5.T13.2.2.1.m1.1.1\" xref=\"A5.T13.2.2.1.m1.1.1.cmml\"><mi mathsize=\"90%\" id=\"A5.T13.2.2.1.m1.1.1.2\" xref=\"A5.T13.2.2.1.m1.1.1.2.cmml\">p</mi><mo mathsize=\"90%\" id=\"A5.T13.2.2.1.m1.1.1.1\" xref=\"A5.T13.2.2.1.m1.1.1.1.cmml\">=</mo><mn mathsize=\"90%\" id=\"A5.T13.2.2.1.m1.1.1.3\" xref=\"A5.T13.2.2.1.m1.1.1.3.cmml\">0.3</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A5.T13.2.2.1.m1.1b\"><apply id=\"A5.T13.2.2.1.m1.1.1.cmml\" xref=\"A5.T13.2.2.1.m1.1.1\"><eq id=\"A5.T13.2.2.1.m1.1.1.1.cmml\" xref=\"A5.T13.2.2.1.m1.1.1.1\"></eq><ci id=\"A5.T13.2.2.1.m1.1.1.2.cmml\" xref=\"A5.T13.2.2.1.m1.1.1.2\">𝑝</ci><cn type=\"float\" id=\"A5.T13.2.2.1.m1.1.1.3.cmml\" xref=\"A5.T13.2.2.1.m1.1.1.3\">0.3</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T13.2.2.1.m1.1c\">p=0.3</annotation></semantics></math><span id=\"A5.T13.2.2.1.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span>\n</th>\n<td id=\"A5.T13.2.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"A5.T13.2.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">84.50</span></td>\n<td id=\"A5.T13.2.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"A5.T13.2.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">79.50</span></td>\n<td id=\"A5.T13.2.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"A5.T13.2.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">96.00</span></td>\n<td id=\"A5.T13.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"A5.T13.2.2.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">75.70</span></td>\n<td id=\"A5.T13.2.2.6\" class=\"ltx_td ltx_align_center\"><span id=\"A5.T13.2.2.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">83.93</span></td>\n</tr>\n<tr id=\"A5.T13.3.3\" class=\"ltx_tr\">\n<th id=\"A5.T13.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span id=\"A5.T13.3.3.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">StableFDG</span><span id=\"A5.T13.3.3.1.2\" class=\"ltx_text\" style=\"font-size:90%;\"> (</span><math id=\"A5.T13.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"p=0.5\" display=\"inline\"><semantics id=\"A5.T13.3.3.1.m1.1a\"><mrow id=\"A5.T13.3.3.1.m1.1.1\" xref=\"A5.T13.3.3.1.m1.1.1.cmml\"><mi mathsize=\"90%\" id=\"A5.T13.3.3.1.m1.1.1.2\" xref=\"A5.T13.3.3.1.m1.1.1.2.cmml\">p</mi><mo mathsize=\"90%\" id=\"A5.T13.3.3.1.m1.1.1.1\" xref=\"A5.T13.3.3.1.m1.1.1.1.cmml\">=</mo><mn mathsize=\"90%\" id=\"A5.T13.3.3.1.m1.1.1.3\" xref=\"A5.T13.3.3.1.m1.1.1.3.cmml\">0.5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A5.T13.3.3.1.m1.1b\"><apply id=\"A5.T13.3.3.1.m1.1.1.cmml\" xref=\"A5.T13.3.3.1.m1.1.1\"><eq id=\"A5.T13.3.3.1.m1.1.1.1.cmml\" xref=\"A5.T13.3.3.1.m1.1.1.1\"></eq><ci id=\"A5.T13.3.3.1.m1.1.1.2.cmml\" xref=\"A5.T13.3.3.1.m1.1.1.2\">𝑝</ci><cn type=\"float\" id=\"A5.T13.3.3.1.m1.1.1.3.cmml\" xref=\"A5.T13.3.3.1.m1.1.1.3\">0.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T13.3.3.1.m1.1c\">p=0.5</annotation></semantics></math><span id=\"A5.T13.3.3.1.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span>\n</th>\n<td id=\"A5.T13.3.3.2\" class=\"ltx_td ltx_align_center\"><span id=\"A5.T13.3.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">83.10</span></td>\n<td id=\"A5.T13.3.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"A5.T13.3.3.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">79.50</span></td>\n<td id=\"A5.T13.3.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"A5.T13.3.3.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">96.40</span></td>\n<td id=\"A5.T13.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"A5.T13.3.3.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">76.00</span></td>\n<td id=\"A5.T13.3.3.6\" class=\"ltx_td ltx_align_center\"><span id=\"A5.T13.3.3.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">83.75</span></td>\n</tr>\n<tr id=\"A5.T13.4.4\" class=\"ltx_tr\">\n<th id=\"A5.T13.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span id=\"A5.T13.4.4.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">StableFDG</span><span id=\"A5.T13.4.4.1.2\" class=\"ltx_text\" style=\"font-size:90%;\"> (</span><math id=\"A5.T13.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"p=0.7\" display=\"inline\"><semantics id=\"A5.T13.4.4.1.m1.1a\"><mrow id=\"A5.T13.4.4.1.m1.1.1\" xref=\"A5.T13.4.4.1.m1.1.1.cmml\"><mi mathsize=\"90%\" id=\"A5.T13.4.4.1.m1.1.1.2\" xref=\"A5.T13.4.4.1.m1.1.1.2.cmml\">p</mi><mo mathsize=\"90%\" id=\"A5.T13.4.4.1.m1.1.1.1\" xref=\"A5.T13.4.4.1.m1.1.1.1.cmml\">=</mo><mn mathsize=\"90%\" id=\"A5.T13.4.4.1.m1.1.1.3\" xref=\"A5.T13.4.4.1.m1.1.1.3.cmml\">0.7</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A5.T13.4.4.1.m1.1b\"><apply id=\"A5.T13.4.4.1.m1.1.1.cmml\" xref=\"A5.T13.4.4.1.m1.1.1\"><eq id=\"A5.T13.4.4.1.m1.1.1.1.cmml\" xref=\"A5.T13.4.4.1.m1.1.1.1\"></eq><ci id=\"A5.T13.4.4.1.m1.1.1.2.cmml\" xref=\"A5.T13.4.4.1.m1.1.1.2\">𝑝</ci><cn type=\"float\" id=\"A5.T13.4.4.1.m1.1.1.3.cmml\" xref=\"A5.T13.4.4.1.m1.1.1.3\">0.7</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T13.4.4.1.m1.1c\">p=0.7</annotation></semantics></math><span id=\"A5.T13.4.4.1.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span>\n</th>\n<td id=\"A5.T13.4.4.2\" class=\"ltx_td ltx_align_center\"><span id=\"A5.T13.4.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">82.40</span></td>\n<td id=\"A5.T13.4.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"A5.T13.4.4.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">78.10</span></td>\n<td id=\"A5.T13.4.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"A5.T13.4.4.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">95.70</span></td>\n<td id=\"A5.T13.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"A5.T13.4.4.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">76.30</span></td>\n<td id=\"A5.T13.4.4.6\" class=\"ltx_td ltx_align_center\"><span id=\"A5.T13.4.4.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">83.13</span></td>\n</tr>\n<tr id=\"A5.T13.5.5\" class=\"ltx_tr\">\n<th id=\"A5.T13.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">\n<span id=\"A5.T13.5.5.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">StableFDG</span><span id=\"A5.T13.5.5.1.2\" class=\"ltx_text\" style=\"font-size:90%;\"> (</span><math id=\"A5.T13.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"p=0.9\" display=\"inline\"><semantics id=\"A5.T13.5.5.1.m1.1a\"><mrow id=\"A5.T13.5.5.1.m1.1.1\" xref=\"A5.T13.5.5.1.m1.1.1.cmml\"><mi mathsize=\"90%\" id=\"A5.T13.5.5.1.m1.1.1.2\" xref=\"A5.T13.5.5.1.m1.1.1.2.cmml\">p</mi><mo mathsize=\"90%\" id=\"A5.T13.5.5.1.m1.1.1.1\" xref=\"A5.T13.5.5.1.m1.1.1.1.cmml\">=</mo><mn mathsize=\"90%\" id=\"A5.T13.5.5.1.m1.1.1.3\" xref=\"A5.T13.5.5.1.m1.1.1.3.cmml\">0.9</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A5.T13.5.5.1.m1.1b\"><apply id=\"A5.T13.5.5.1.m1.1.1.cmml\" xref=\"A5.T13.5.5.1.m1.1.1\"><eq id=\"A5.T13.5.5.1.m1.1.1.1.cmml\" xref=\"A5.T13.5.5.1.m1.1.1.1\"></eq><ci id=\"A5.T13.5.5.1.m1.1.1.2.cmml\" xref=\"A5.T13.5.5.1.m1.1.1.2\">𝑝</ci><cn type=\"float\" id=\"A5.T13.5.5.1.m1.1.1.3.cmml\" xref=\"A5.T13.5.5.1.m1.1.1.3\">0.9</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T13.5.5.1.m1.1c\">p=0.9</annotation></semantics></math><span id=\"A5.T13.5.5.1.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span>\n</th>\n<td id=\"A5.T13.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A5.T13.5.5.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">81.40</span></td>\n<td id=\"A5.T13.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A5.T13.5.5.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">78.80</span></td>\n<td id=\"A5.T13.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A5.T13.5.5.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">95.90</span></td>\n<td id=\"A5.T13.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"A5.T13.5.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">73.60</span></td>\n<td id=\"A5.T13.5.5.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A5.T13.5.5.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">82.70</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We conduct additional ablation studies on the probability value (defined as p𝑝p here) utilized to control the operation of the style sharing/shifting and style exploration modules. Larger p𝑝p means that our scheme is more likely to be activated. In Table 13, we provide results on various probability values in a single-domain data distribution scenario using PACS dataset. From the results, it is confirm that for all p𝑝p values, the proposed StableFDG outperforms existing baselines, demonstrating that our scheme can work well with an arbitrarily chosen probability p𝑝p. In detail, when p𝑝p ranges from 0.3 to 0.7, the high performance is maintained while the performance decreases at both extreme probabilities (p=0.1𝑝0.1p=0.1 and 0.9). Therefore, it is recommended for practitioners to select the p𝑝p in an appropriate range, avoiding extreme cases."
        ]
    },
    "A5.T14": {
        "caption": "Table 14: Ablation experiments on applying style-based learning at different layers (PACS dataset).",
        "table": "<table id=\"A5.T14.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A5.T14.2.1.1\" class=\"ltx_tr\">\n<th id=\"A5.T14.2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"A5.T14.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Methods</span></th>\n<th id=\"A5.T14.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A5.T14.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Art</span></th>\n<th id=\"A5.T14.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A5.T14.2.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">Cartoon</span></th>\n<th id=\"A5.T14.2.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A5.T14.2.1.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">Photo</span></th>\n<th id=\"A5.T14.2.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"A5.T14.2.1.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Sketch</span></th>\n<th id=\"A5.T14.2.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A5.T14.2.1.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">Avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A5.T14.2.2.1\" class=\"ltx_tr\">\n<td id=\"A5.T14.2.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"A5.T14.2.2.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Style exploration at 1st, 2nd, 3rd layers (main manuscript)</span></td>\n<td id=\"A5.T14.2.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A5.T14.2.2.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">84.10</span></td>\n<td id=\"A5.T14.2.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A5.T14.2.2.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">78.57</span></td>\n<td id=\"A5.T14.2.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A5.T14.2.2.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">95.40</span></td>\n<td id=\"A5.T14.2.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"A5.T14.2.2.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">72.73</span></td>\n<td id=\"A5.T14.2.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A5.T14.2.2.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">82.70</span></td>\n</tr>\n<tr id=\"A5.T14.2.3.2\" class=\"ltx_tr\">\n<td id=\"A5.T14.2.3.2.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span id=\"A5.T14.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Style exploration at 1st, 2nd, 3rd, 4th layers</span></td>\n<td id=\"A5.T14.2.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A5.T14.2.3.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">82.99</span></td>\n<td id=\"A5.T14.2.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A5.T14.2.3.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">78.54</span></td>\n<td id=\"A5.T14.2.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A5.T14.2.3.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">94.13</span></td>\n<td id=\"A5.T14.2.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"A5.T14.2.3.2.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">73.35</span></td>\n<td id=\"A5.T14.2.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A5.T14.2.3.2.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">82.25</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "For implementation, style-based learning is applied only in the 1st, 2nd, 3rd blocks among 4 residual blocks in ResNet-18. Note that at the output of the 4th block, label information is dominant rather than style information, which results in degraded performance when style-based schemes are applied. This is confirmed by our new experiments in the table below. It can be seen from the results that if we consider the 4th residual block to apply our style-based learning, the performance gets degraded. This result confirms the intuition that style-based learning should be conducted at the earlier layers where style information is preserved."
            ]
        ]
    },
    "A6.T15": {
        "caption": "Table 15: Effects of similarity metrics using DomainNet dataset in a multi-domain data distribution scenario.",
        "table": "<table id=\"A6.T15.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A6.T15.2.1.1\" class=\"ltx_tr\">\n<th id=\"A6.T15.2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"A6.T15.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Methods</span></th>\n<th id=\"A6.T15.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A6.T15.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">Clipart</span></th>\n<th id=\"A6.T15.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A6.T15.2.1.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">Infograph</span></th>\n<th id=\"A6.T15.2.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A6.T15.2.1.1.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">Painting</span></th>\n<th id=\"A6.T15.2.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A6.T15.2.1.1.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">Quickdraw</span></th>\n<th id=\"A6.T15.2.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A6.T15.2.1.1.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">Real</span></th>\n<th id=\"A6.T15.2.1.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"A6.T15.2.1.1.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">Sketch</span></th>\n<th id=\"A6.T15.2.1.1.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A6.T15.2.1.1.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">Avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A6.T15.2.2.1\" class=\"ltx_tr\">\n<td id=\"A6.T15.2.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"A6.T15.2.2.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">StableFDG (with self-attention alone)</span></td>\n<td id=\"A6.T15.2.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A6.T15.2.2.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">61.77</span></td>\n<td id=\"A6.T15.2.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A6.T15.2.2.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">24.88</span></td>\n<td id=\"A6.T15.2.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A6.T15.2.2.1.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">48.28</span></td>\n<td id=\"A6.T15.2.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A6.T15.2.2.1.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">14.15</span></td>\n<td id=\"A6.T15.2.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A6.T15.2.2.1.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">59.78</span></td>\n<td id=\"A6.T15.2.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"A6.T15.2.2.1.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">52.41</span></td>\n<td id=\"A6.T15.2.2.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A6.T15.2.2.1.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">43.55</span></td>\n</tr>\n<tr id=\"A6.T15.2.3.2\" class=\"ltx_tr\">\n<td id=\"A6.T15.2.3.2.1\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"A6.T15.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">StableFDG (with cross-attention alone)</span></td>\n<td id=\"A6.T15.2.3.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"A6.T15.2.3.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">61.24</span></td>\n<td id=\"A6.T15.2.3.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"A6.T15.2.3.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">24.97</span></td>\n<td id=\"A6.T15.2.3.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"A6.T15.2.3.2.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">50.55</span></td>\n<td id=\"A6.T15.2.3.2.5\" class=\"ltx_td ltx_align_center\"><span id=\"A6.T15.2.3.2.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">14.67</span></td>\n<td id=\"A6.T15.2.3.2.6\" class=\"ltx_td ltx_align_center\"><span id=\"A6.T15.2.3.2.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">61.12</span></td>\n<td id=\"A6.T15.2.3.2.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"A6.T15.2.3.2.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">50.44</span></td>\n<td id=\"A6.T15.2.3.2.8\" class=\"ltx_td ltx_align_center\"><span id=\"A6.T15.2.3.2.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">43.83</span></td>\n</tr>\n<tr id=\"A6.T15.2.4.3\" class=\"ltx_tr\">\n<td id=\"A6.T15.2.4.3.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span id=\"A6.T15.2.4.3.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">StableFDG (with self + cross)</span></td>\n<td id=\"A6.T15.2.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A6.T15.2.4.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">62.58</span></td>\n<td id=\"A6.T15.2.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A6.T15.2.4.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">24.12</span></td>\n<td id=\"A6.T15.2.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A6.T15.2.4.3.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">52.23</span></td>\n<td id=\"A6.T15.2.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A6.T15.2.4.3.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">14.87</span></td>\n<td id=\"A6.T15.2.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A6.T15.2.4.3.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">60.60</span></td>\n<td id=\"A6.T15.2.4.3.7\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"A6.T15.2.4.3.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">52.50</span></td>\n<td id=\"A6.T15.2.4.3.8\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A6.T15.2.4.3.8.1\" class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:70%;\">44.48</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "In our main paper, the similarity metric in equation (6) adopts cross-attention, while the metric in equation (8) combines cross-attention and self-attention. When applying only the cross-attention-based metric in equation (6), we found that the similarity value could become low even when the two samples belong to the same class, in special cases. We handled this issue by adding the self-attention component as in equation (8). Intuitively, by doing this, the attention module is learning to extract the important features across images (via cross-attention), and within the image (via self-attention). Table 15 compares the performance of our StableFDG when using (i) self-attention alone, (ii) cross-attention alone (equation (6)), and (iii) both self and cross attentions at the same time (equation (8)), confirming the advantage of using self-attention and cross-attention together."
        ]
    },
    "A6.T16": {
        "caption": "Table 16: Ablation study on attention-based learning using PACS dataset in a single-domain data distribution scenario. ",
        "table": "<table id=\"A6.T16.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A6.T16.2.1.1\" class=\"ltx_tr\">\n<th id=\"A6.T16.2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"A6.T16.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Methods</span></th>\n<th id=\"A6.T16.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A6.T16.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Art</span></th>\n<th id=\"A6.T16.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A6.T16.2.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">Cartoon</span></th>\n<th id=\"A6.T16.2.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A6.T16.2.1.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">Photo</span></th>\n<th id=\"A6.T16.2.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"A6.T16.2.1.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Sketch</span></th>\n<th id=\"A6.T16.2.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A6.T16.2.1.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">Avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A6.T16.2.2.1\" class=\"ltx_tr\">\n<th id=\"A6.T16.2.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A6.T16.2.2.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Baseline (same model size)</span></th>\n<td id=\"A6.T16.2.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A6.T16.2.2.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">78.88</span></td>\n<td id=\"A6.T16.2.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A6.T16.2.2.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">69.03</span></td>\n<td id=\"A6.T16.2.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A6.T16.2.2.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">91.74</span></td>\n<td id=\"A6.T16.2.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"A6.T16.2.2.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">60.77</span></td>\n<td id=\"A6.T16.2.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A6.T16.2.2.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">75.11</span></td>\n</tr>\n<tr id=\"A6.T16.2.3.2\" class=\"ltx_tr\">\n<th id=\"A6.T16.2.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span id=\"A6.T16.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Attention-based learning</span></th>\n<td id=\"A6.T16.2.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A6.T16.2.3.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">79.54</span></td>\n<td id=\"A6.T16.2.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A6.T16.2.3.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">72.48</span></td>\n<td id=\"A6.T16.2.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A6.T16.2.3.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">92.51</span></td>\n<td id=\"A6.T16.2.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"A6.T16.2.3.2.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">64.71</span></td>\n<td id=\"A6.T16.2.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A6.T16.2.3.2.6.1\" class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">77.31</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Our attention module requires 0.44% of additional model parameters to perform the attention-based learning. For a fair comparison to see the effect of our attention-based learning, we consider a different baseline with the same model size but without attention-based learning. Specifically, the baseline computes the attention score map using only additional convolutional operations and take the weighted average of the feature zisubscript𝑧𝑖z_{i} based on the attention score map. Table 16 shows the results using PACS dataset in a single-domain data distribution scenario. The results demonstrate that our attention-based learning achieves performance improvements on all four domains while playing a key role in capturing essential parts of the features."
        ]
    },
    "A6.T17": {
        "caption": "Table 17: Effect of proposed attention-based feature highlighter in a centralized DG setup using PACS dataset. ",
        "table": "<table id=\"A6.T17.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A6.T17.2.1.1\" class=\"ltx_tr\">\n<th id=\"A6.T17.2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"A6.T17.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Methods</span></th>\n<th id=\"A6.T17.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A6.T17.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Art</span></th>\n<th id=\"A6.T17.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A6.T17.2.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">Cartoon</span></th>\n<th id=\"A6.T17.2.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A6.T17.2.1.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">Photo</span></th>\n<th id=\"A6.T17.2.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"A6.T17.2.1.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Sketch</span></th>\n<th id=\"A6.T17.2.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A6.T17.2.1.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">Avg.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A6.T17.2.2.1\" class=\"ltx_tr\">\n<td id=\"A6.T17.2.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"A6.T17.2.2.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">StableFDG (centralized setup, without attention)</span></td>\n<td id=\"A6.T17.2.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A6.T17.2.2.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">84.15</span></td>\n<td id=\"A6.T17.2.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A6.T17.2.2.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">79.45</span></td>\n<td id=\"A6.T17.2.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A6.T17.2.2.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">96.21</span></td>\n<td id=\"A6.T17.2.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"A6.T17.2.2.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">77.09</span></td>\n<td id=\"A6.T17.2.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A6.T17.2.2.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">84.23</span></td>\n</tr>\n<tr id=\"A6.T17.2.3.2\" class=\"ltx_tr\">\n<td id=\"A6.T17.2.3.2.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span id=\"A6.T17.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">StableFDG (centralized setup, with attention)</span></td>\n<td id=\"A6.T17.2.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A6.T17.2.3.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">85.02</span></td>\n<td id=\"A6.T17.2.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A6.T17.2.3.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">79.65</span></td>\n<td id=\"A6.T17.2.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A6.T17.2.3.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">96.38</span></td>\n<td id=\"A6.T17.2.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"A6.T17.2.3.2.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">78.45</span></td>\n<td id=\"A6.T17.2.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A6.T17.2.3.2.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">84.88</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Now we provide answer to the following question: Instead of the FL setup we focused on, can attention provide benefits in the centralized DG setup? Table 17 shows the results with/without attention module in a centralized setup using PACS dataset. The results show that attention still provides performance improvements in the centralized setup by learning domain-invariant features, although the gain is slightly lower than the gain in the FL setup as shown in Table 3 of the main manuscript. These results indicate that the proposed attention-based learning indeed captures the domain-invariant characteristics of samples, while the scheme provides more benefits in the FL setup where each client is prone to overfitting due to lack of data."
        ]
    }
}