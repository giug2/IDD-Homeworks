{
    "id_table_1": {
        "caption": "Table 1:  Performance comparison on molecule captioning task. () / () denotes a higher / lower value is better. The best performance is marked as bold.",
        "table": "Sx2.T1.1.1",
        "footnotes": [],
        "references": [
            "The overview of InstructBioMol is presented in Figure  1 a. InstructBioMol is a unified multimodal Large Language Model that simultaneously handles natural language, molecules, and proteins. It accepts inputs in natural language or multimodal molecules and proteins, and generates natural language, molecules, or proteins in textual form. To process multimodal data of both molecules and proteins, we develop a Motif-Guided Multimodal Feature Extraction Module (Figure  1 b). This module employs a Transformer Encoder-Decoder structure  [ 21 ] . It encodes 2D graphs and 3D structures for molecules and 1D sequences along with 3D structures for proteins. Using pre-trained lightweight encoders, the multimodal inputs are encoded into corresponding representations, and subsequently fed into a Transformer Encoder. In the Transformer Decoder, we extract the biological knowledge in the motifs using a motif prompt extractor, which serves as guiding information for the fusion of multimodal features. The fused features are then integrated into the language model. For model training, outlined in Figure  1 c, a wide range of data is collected, comprising a continual pretraining dataset and an instruction-tuning dataset. The continual pretraining dataset comprises molecules, proteins and natural language texts derived from scientific literature. The instruction-tuning dataset consists of data pairs between natural language, molecules, and proteins. The training process is divided into two stages. First, continual pretraining is employed to augment domain-specific knowledge in the field of biomolecular scientific research. Then, instruction-tuning is performed to achieve an any-to-any pairwise alignment among natural language, molecules, and proteins. We employ a staged instruction-tuning pipeline, learning from large-scale data (stage-1) to refined data (stage-2) to gradually improve performance. As a result, InstructBioMol aligns natural language, molecules, and proteins in an any-to-any manner, demonstrating competency across a broad spectrum of biomolecular tasks, as shown in Figure  1 d. This includes solving practical challenges, like the discovery of molecule drugs for target proteins, and the design of enzymes for specific substrates, following human intention.",
            "Quantitative results on molecule captioning and molecule generation are in Table  1  and Table  2 , respectively. The performance of molecule-specific models significantly surpasses that of generalist language models, primarily due to the finetuning of the latter on domain-specific instruction datasets. This finetuning facilitates an effective alignment between natural language and chemical molecular knowledge. These findings suggest that general language models lack sufficient expertise in specialized domains, which can be effectively compensated by leveraging instruction-tuning. Notably, the experimental results demonstrate that InstructBioMol performs best across almost all evaluation metrics. Specifically, for the molecule captioning task, InstructBioMol yields an average improvement of 0.9% across all metrics. In description-based molecule generation task, the exact match accuracy (EXACT) of generated molecules increases by 0.7%. Furthermore, an average improvement of 2.0% is observed in molecular fingerprint similarity metrics (MACCS FTS, RDK FTS, and MORGAN FTS). These results indicate that InstructBioMol exhibits higher accuracy and efficacy in both understanding and generating chemical molecular information. We attribute this success to the extensive use of high-quality instruction data, which enables the model to comprehensively align molecules and natural language and achieve superior performance across molecular tasks. Some examples in Supplementary Information Section 4.1 provide a detailed analysis of the results generated by InstructBioMol.",
            "The architecture of InstructBioMol (Figure  1 a) consists of two components: the Motif-Guided Multimodal Feature Extraction Module (Figure  1 b) and the Biomolecular Vocabulary-expanded Language Model. The former is designed to extract multimodal features of biomolecules, while the latter handles a unified processing of textual natural language, molecule and protein data, as well as the extracted multimodal features. Specifically, in the Motif-Guided Multimodal Feature Extraction Module, we employ lightweight frozen pre-trained encoders to extract features from each modality separately, and leverage the biological knowledge embedded in motifs to guide the fusion of these multimodal features. Within the Biomolecular Vocabulary-expanded Language Model, to mitigate potential interference among data from different domains, we expand the vocabulary to accommodate molecules and proteins, and standardize the input format for their multimodal features.",
            "Biomolecules exhibit inherent multimodality, characterized by diverse sequential and structural representations across various domains  [ 44 ,  45 ,  46 ,  12 ] . This complexity cannot be fully captured by any single modality in isolation. In this module (Figure  1 b), we incorporate 2D-graph and 3D-structure for molecules, alongside 1D-sequence and 3D-structure for proteins to leverage the multitude of perspectives available. 2D-graph of molecules highlights the basic skeleton, while 3D-structure provides insights into molecular docking and interaction. For proteins, 1D-sequence delineates the fundamental arrangement of amino acids, and 3D-structure unlocks understanding of functional sites and foldings. We utilize frozen pre-trained encoders to process each modality separately, and then leverage the inherent biological knowledge within motifs to guide multimodal feature fusion, which enhances comprehension and processing of complex biological data.",
            "Firstly, the single-model representations obtained from Eq.  1  and Eq.  2  are transformed and then concatenated:",
            "We collect datasets on a hundred-million-scale, including a continual pretraining dataset (Table  3 ), and an instruction-tuning dataset (Table  4 ). The continual pretraining dataset comprises molecules and proteins in textual format, and natural language texts derived from scientific literature, enabling the model to develop a foundational adapting to biomolecular research. The instruction-tuning dataset contains various alignment pairs: molecule-natural language, protein-natural language, and molecule-protein (Figure  1 d), achieving any-to-any alignment among molecules, proteins and natural language. Additionally, both molecular and protein data in the instruction-tuning dataset are multimodal, incorporating 2-D and 3-D structures of molecules and 3-D structures of proteins.",
            "Despite the collection of a broad range of data, the quality of this data exhibits considerable variability across sources. For example, the ChEBI database offers a broader spectrum of molecular descriptions compared to the natural-language-like structure descriptions provided by IUPAC names in PubChem. Similarly, while data within the SwissProt undergo meticulous manual curation, entries in the TrEMBL do not benefit from such rigorous calibration. The tradeoff between the scale and quality of data poses significant challenges to model performance and generalizability. To address this issue, we adopt a two-stage instruction-tuning strategy designed to exploit the extensive data initially, then progressively direct the focus towards the insights offered by higher-quality datasets. Initially, in stage-1, the model is trained across all the available instructions. This stage leverages the diversity and volume of data to build a foundation on biomolecular alignment. Subsequently, in stage-2, the model undergoes further fine-tuning on a subset of higher-quality data. This approach harnesses both the expansive coverage of lower-quality data and the precision inherent in high-quality data, facilitating an efficient and effective utilization of the dataset. The scale and specific details of the different datasets used in the two stages are presented in Figure  1 c and Table  4 .",
            "We use Pytorch  [ 70 ]  to implement the modal. The model is trained on 8 80G NVIDIA H800 GPUs. Additionally, we adopt the DeepSpeed ZeRO-1  [ 71 ]  and BF16 for computational efficiency. The total number of training steps is 1.5 million. The steps for continual pretraining, stage-1 instruction-tuning and stage-2 instruction-tuning are 600,000, 500,000, and 400,000 respectively. We use the AdamW optimizer with  (  1 ,  2 ) subscript  1 subscript  2 (\\beta_{1},\\beta_{2}) ( italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT )  set to  ( 0.9 , 0.95 ) 0.9 0.95 (0.9,0.95) ( 0.9 , 0.95 ) . We follow a linear learning rate schedule, warming up from 0 to maximum learning rate 1e-5 over the first 2,000 steps, and decaying the final learning rate down to 0. During training, all parameters are trainable except for the modality encoders  f m 2  D superscript subscript f m 2 D f_{m}^{2D} italic_f start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 italic_D end_POSTSUPERSCRIPT ,  f m 3  D superscript subscript f m 3 D f_{m}^{3D} italic_f start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPT ,  f p 1  D superscript subscript f p 1 D f_{p}^{1D} italic_f start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 italic_D end_POSTSUPERSCRIPT ,  f p 3  D superscript subscript f p 3 D f_{p}^{3D} italic_f start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPT  in Equations  1  and  2 , with the total trainable parameters being 6.8B. The ratio of different datasets sampled during training is controlled using hyper-parameters, which are detailed in Supplementary Table 1.",
            "During continual pretraining, we set batch size to 32 and fix sequence length to 512 tokens. To ensure balanced training on molecule, protein, and natural language data, the sampling ratio for these three types of data is fixed at 1:1:1. In instruction-tuning, we use a batch size of 24, with the maximum sequence length set to 512. By setting hyperparameters, we control the sampling ratio of different types of instruction data. The sampling ratios for the datasets used in stage-1 and stage-2 of instruction-tuning are detailed in Table  1 .",
            "For the task of description-based molecule generation, several case examples of the ground truth and generation are presented in Figure  1 . Overall, InstructBioMol demonstrates a relatively accurate analysis of molecular structure, function, origin, etc. In the task of description-based molecule generation, InstructBioMol is capable of generating molecules that are completely consistent with the ground truth in certain cases, such as molecules PubChem-CID-5281294 and PubChem-CID-31284 shown in Figure  2 . This demonstrates the strong molecular design capability of InstructBioMol. However, in some other cases, the molecules generated by InstructBioMol show some discrepancies with the ground truth. Our analysis suggests that this may be due to inadequate handling of certain functional groups. For example, for PubChem-CID-118429016 and PubChem-CID-123953, the model omits certain functional groups (a hydroxyl group and a phosphate group, respectively). For PubChem-CID-179394, the model generates a chemically atypical P(O)(O)(O)O group. Overall, InstructBioMol exhibits a high level of accuracy in molecule generation tasks and shows potential for applications in fields such as drug discovery, and further optimization may be required in practical applications.",
            "In the protein function answering task, InstructBioMol generates results that closely resemble the ground truth for certain cases, such as Q9NRY2 and P73070 in Table  10 . Furthermore, we observe that in some cases, the generated descriptions tend to be more detailed. For example, in the case of Q9Y2G3, the generated description includes detailed information on vesicle formation, lipid signal molecule uptake, and the establishment of the thrombopoietin gradient in platelets. Similarly, for Q9FY89, the generated description provides a more comprehensive explanation of the formation of multivesicular bodies (MVBs), specifically describing the formation mechanism of intraluminal vesicles (ILVs) within MVBs, including the invagination and scission of the endosomal membrane. It further elaborates on the function of MVBs, such as transporting their contents to lysosomes for the degradation of membrane proteins, receptors, lysosomal enzymes, and lipids. For P0CP67, the generated description offers more detailed and in-depth functional information, identifying the specific targets of the proteins action (e.g., components of AP-1, c-Jun, and ATF2) and potential biological processes involved (e.g., regulation of circadian clock). Although InstructBioMol may provide researchers with deeper insights, further experimental validation is necessary to confirm these findings."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Performance comparison on description-based molecule generation task. () / () denotes a higher / lower value is better. The best performance is marked as bold.",
        "table": "Sx2.T2.1.1",
        "footnotes": [],
        "references": [
            "Quantitative results on molecule captioning and molecule generation are in Table  1  and Table  2 , respectively. The performance of molecule-specific models significantly surpasses that of generalist language models, primarily due to the finetuning of the latter on domain-specific instruction datasets. This finetuning facilitates an effective alignment between natural language and chemical molecular knowledge. These findings suggest that general language models lack sufficient expertise in specialized domains, which can be effectively compensated by leveraging instruction-tuning. Notably, the experimental results demonstrate that InstructBioMol performs best across almost all evaluation metrics. Specifically, for the molecule captioning task, InstructBioMol yields an average improvement of 0.9% across all metrics. In description-based molecule generation task, the exact match accuracy (EXACT) of generated molecules increases by 0.7%. Furthermore, an average improvement of 2.0% is observed in molecular fingerprint similarity metrics (MACCS FTS, RDK FTS, and MORGAN FTS). These results indicate that InstructBioMol exhibits higher accuracy and efficacy in both understanding and generating chemical molecular information. We attribute this success to the extensive use of high-quality instruction data, which enables the model to comprehensively align molecules and natural language and achieve superior performance across molecular tasks. Some examples in Supplementary Information Section 4.1 provide a detailed analysis of the results generated by InstructBioMol.",
            "Quantitive results on answering protein properties and description-based protein generation are in Figure  2 a and Figure  2 b, respectively. Based on the experimental results, we have reached the following conclusions: Firstly, InstructBioMol demonstrates the best performance in both tasks. In tasks related to answering questions about protein properties, InstructBioMol outperforms previous state-of-the-art (SOTA) methods by 13.1% on average. For protein generation tasks, InstructBioMol achieves 0.9%, 5.7%, and 0.510 improvements in identity, alignment, and BLOSUM substitution, respectively, compared to previous SOTAs, and comparable validity of the generated proteins. Secondly, using domain-specific instruction alignment is effective. In tasks related to answering protein property questions, InstructBioMol shows an average improvement of 80.3% compared to GPT-3.5 (zero-shot). In protein generation tasks, InstructBioMol achieved a 17.5% increase in identity and a 28.7% increase in alignment compared to GPT-3.5 (zero-shot). These results clearly demonstrate that models aligned with domain-specific instructions exhibit significantly enhanced capabilities in handling tasks within specific domains compared to general models. This underscores the importance of customized models in specialized domains, particularly in highly specialized fields like protein engineering, where the integration of domain-specific knowledge and instructions can greatly enhance the models practicality and accuracy. In Supplementary Information Section 4.2, we provide a detailed analysis of InstructBioMols outstanding performance on protein function answering tasks through some examples. Additionally, example cases on description-based protein generation task demonstrate that InstructBioMol can design de-novo proteins with high structural similarity, closely aligning with the structures of ground truth.",
            "Firstly, the single-model representations obtained from Eq.  1  and Eq.  2  are transformed and then concatenated:",
            "We use Pytorch  [ 70 ]  to implement the modal. The model is trained on 8 80G NVIDIA H800 GPUs. Additionally, we adopt the DeepSpeed ZeRO-1  [ 71 ]  and BF16 for computational efficiency. The total number of training steps is 1.5 million. The steps for continual pretraining, stage-1 instruction-tuning and stage-2 instruction-tuning are 600,000, 500,000, and 400,000 respectively. We use the AdamW optimizer with  (  1 ,  2 ) subscript  1 subscript  2 (\\beta_{1},\\beta_{2}) ( italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT )  set to  ( 0.9 , 0.95 ) 0.9 0.95 (0.9,0.95) ( 0.9 , 0.95 ) . We follow a linear learning rate schedule, warming up from 0 to maximum learning rate 1e-5 over the first 2,000 steps, and decaying the final learning rate down to 0. During training, all parameters are trainable except for the modality encoders  f m 2  D superscript subscript f m 2 D f_{m}^{2D} italic_f start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 italic_D end_POSTSUPERSCRIPT ,  f m 3  D superscript subscript f m 3 D f_{m}^{3D} italic_f start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPT ,  f p 1  D superscript subscript f p 1 D f_{p}^{1D} italic_f start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 italic_D end_POSTSUPERSCRIPT ,  f p 3  D superscript subscript f p 3 D f_{p}^{3D} italic_f start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 3 italic_D end_POSTSUPERSCRIPT  in Equations  1  and  2 , with the total trainable parameters being 6.8B. The ratio of different datasets sampled during training is controlled using hyper-parameters, which are detailed in Supplementary Table 1.",
            "For inference on downstream tasks, we load the model using the bfloat16 data format. The specific inference hyperparameters for each task are detailed in Table  2 .",
            "For the task of description-based molecule generation, several case examples of the ground truth and generation are presented in Figure  1 . Overall, InstructBioMol demonstrates a relatively accurate analysis of molecular structure, function, origin, etc. In the task of description-based molecule generation, InstructBioMol is capable of generating molecules that are completely consistent with the ground truth in certain cases, such as molecules PubChem-CID-5281294 and PubChem-CID-31284 shown in Figure  2 . This demonstrates the strong molecular design capability of InstructBioMol. However, in some other cases, the molecules generated by InstructBioMol show some discrepancies with the ground truth. Our analysis suggests that this may be due to inadequate handling of certain functional groups. For example, for PubChem-CID-118429016 and PubChem-CID-123953, the model omits certain functional groups (a hydroxyl group and a phosphate group, respectively). For PubChem-CID-179394, the model generates a chemically atypical P(O)(O)(O)O group. Overall, InstructBioMol exhibits a high level of accuracy in molecule generation tasks and shows potential for applications in fields such as drug discovery, and further optimization may be required in practical applications."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Statistics of continual pretraining dataset.",
        "table": "Sx4.E6",
        "footnotes": [],
        "references": [
            "Figure  3 a presents the experimental results. where InstructBioMol demonstrates superior performance across three key dimensions: binding affinity, general properties, and overall assessment. Specifically, it improves High Affinity and Success Rate by 25.9% and 21.9%, respectively, compared to previous state-of-the-art (SOTA) methods. This highlights InstructBioMols enhanced capability in generating drug-like molecules with high affinity for target proteins and favorable intrinsic properties. Additionally, it achieves an outstanding generation validity of 99.9%. Figure  3 b illustrates the average Vina Scores for top-1, top-5, top-10, and all generated molecules. It can be observed that InstructBioMol consistently outperforms other methods under these settings, and is the only approach where the average scores for all generated molecules surpass the reference values. This suggests that the quality of the molecules generated by InstructBioMol is comparable to the ground truth in the dataset. Moreover, InstructBioMol proves to be the most effective method for designing molecules with the best Vina Scores for most target proteins. (As shown in Figure 4a in Supplementary Information, InstructBioMol achieves the best performance on 35% of the targets.) Figure  3 c shows the distribution of Vina Scores for all generated molecules. The distribution reveals that molecules generated by InstructBioMol have a lower mean and reduced variance, further confirming that the overall quality of these molecules is superior to that of other methods.",
            "The performance of the generated protein enzymes across various evaluation metrics is presented in Figure  3 d. InstructBioMol demonstrates the best performance in terms of similarity to ground truth, interaction capability with substrates, and exhibits superior generation validity. Specifically, InstructBioMol achieves improvements of 13.3 in ESP Score and 0.7 in Vina Score, indicating a stronger potential for substrate binding compared to baseline methods. Notably, InstructBioMol attains an ESP Score of 70.4, making it the only method to surpass the enzyme-substrate interaction threshold of 60.0 recommended by the ESP developer. This demonstrates that enzymes designed by InstructBioMol can bind their corresponding substrates with high affinity. Figure 4b in Supplementary Information further analyzes the top-1 ESP Score of the proteins generated for each substrate, revealing that InstructBioMol achieves the best performance on 66% of the substrates. Additionally, Supplementary Figure 4c presents the top-1 Vina Score on each substrate, with InstructBioMol attaining the best performance on 89% of the substrates. These findings suggest that InstructBioMol holds significant potential in generating highly efficient and specific protein enzymes, offering more effective solutions for fields such as biocatalysis.",
            "where  H m  R 2  | V |  d subscript H m superscript R 2 V d \\mathbf{H}_{m}\\in\\mathbb{R}^{2|V|\\times d} bold_H start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT 2 | italic_V |  italic_d end_POSTSUPERSCRIPT  and  H p  R 2  N  d subscript H p superscript R 2 N d \\mathbf{H}_{p}\\in\\mathbb{R}^{2N\\times d} bold_H start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT 2 italic_N  italic_d end_POSTSUPERSCRIPT  are used as the inputs of the Transformer Encoder for molecule and protein, respectively, and   direct-sum \\oplus   denotes the concatenation operation. We utilize the motif prompt obtained from Eq.  3  as the initial input to the Transformer Decoder. By directing the focus of the Transformer-Decoder toward these motifs, this approach endeavors to anchor the multimodal feature extraction in biologically significant referents. This ensures that the resultant fused features are not only data-derived but also deeply rooted in the biological realities of molecular and protein functionalities. Additionally, the input to the Transformer Decoder includes a sequence of learnable queries. Formally, the joint multimodal feature extraction is defined as:",
            "To simplify, subscripts are omitted since molecules and proteins undergo the same processing. Here,  P P \\mathbf{P} bold_P  and  H H \\mathbf{H} bold_H  are derived from Eq.  3  and Eq.  4  respectively.  Q Q \\mathbf{Q} bold_Q  denotes a sequence of learnable queries as  Q = [ q 1 , q 2 , ... , q N q ]  R N q  d Q subscript q 1 subscript q 2 ... subscript q subscript N q superscript R subscript N q d \\mathbf{Q}=[\\mathbf{q}_{1},\\mathbf{q}_{2},...,\\mathbf{q}_{N_{q}}]\\in\\mathbb{R}% ^{N_{q}\\times d} bold_Q = [ bold_q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , bold_q start_POSTSUBSCRIPT italic_N start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT end_POSTSUBSCRIPT ]  blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT  italic_d end_POSTSUPERSCRIPT , and  Z  R ( 1 + N q )  d Z superscript R 1 subscript N q d \\mathbf{Z}\\in\\mathbb{R}^{(1+N_{q})\\times d} bold_Z  blackboard_R start_POSTSUPERSCRIPT ( 1 + italic_N start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT )  italic_d end_POSTSUPERSCRIPT  is the extracted multimodal features.",
            "We collect datasets on a hundred-million-scale, including a continual pretraining dataset (Table  3 ), and an instruction-tuning dataset (Table  4 ). The continual pretraining dataset comprises molecules and proteins in textual format, and natural language texts derived from scientific literature, enabling the model to develop a foundational adapting to biomolecular research. The instruction-tuning dataset contains various alignment pairs: molecule-natural language, protein-natural language, and molecule-protein (Figure  1 d), achieving any-to-any alignment among molecules, proteins and natural language. Additionally, both molecular and protein data in the instruction-tuning dataset are multimodal, incorporating 2-D and 3-D structures of molecules and 3-D structures of proteins.",
            "To validate the effectiveness of InstructBioMol, we conduct experiments comparing it with various baseline models. In molecule captioning and description-based molecule generation tasks, the selected baseline models include MolT5  [ 14 ] , BioT5  [ 16 ] , BioT5+  [ 18 ] , MolReGPT  [ 31 ] , InstructMol  [ 33 ]  and ChemDFM  [ 32 ] . For comparisons on other tasks, baselines are categorized into two groups. The first group comprises pre-trained models, including Mol-Instructions  [ 17 ] , InstructProtein  [ 15 ] , ProtT3  [ 34 ] , BioMedGPT  [ 19 ] , ProteinDT  [ 35 ] , DrugGPT  [ 42 ]  and RFdiffusionAA  [ 5 ] . These models are all downloaded from their official repositories and evaluated on the test set. The second group consists of baseline models constructed by us using the general-purpose Large Language Model GPT-3.5. It includes three variants: zero-shot, 5-shot random, and 5-shot similarity. In the zero-shot setting, we directly pose task-specific questions to the GPT-3.5 model. In the 5-shot random setting, five examples from the training set are randomly selected as in-context demonstrations for each test entry. In the 5-shot similarity setting, the in-context learning paradigm is also adopted, but the demonstrations are required to be the five most similar examples from the training set relative to the query. The method for computing similarity depends on the data type of the input query: when query is in natural language, TF-IDF with cosine similarity is used as the text similarity measure; for protein sequence queries, MMseq2  [ 76 ]  is employed to calculate protein similarity; and for molecule queries, molecular fingerprint similarity  [ 29 ,  72 ]  is used. The specific input format for the employed GPT baseline is detailed in Table  3 .",
            "For the task of text-based protein generation, we present two examples in Figure  3 . For the ground truth proteins, we utilize the protein structures predicted by AlphaFold Protein Structure Database  [ 69 ] . For the generated protein sequences, we predict their structures using ColabFold  [ 77 ] . Besides sequence similarity metrics Identity, Alignment, and BLOSUM Substitution, we also compare structural similarity metrics: TM-Score  [ 78 ]  and LDDT  [ 79 ] . The results demonstrate that InstructBioMol is capable of de-novo design of proteins, with the designed proteins exhibiting a high degree of structural similarity to ground truth. This suggests that InstructBioMol holds significant potential in designing proteins tailored to specific functional descriptions, acting as an effective copilot to assist researchers in protein design."
        ]
    },
    "id_table_4": {
        "caption": "Table 1:  Sampling ratios of different types of instruction data during instruction-tuning. Note that in practice, the sampling ratios are scaled proportionally to ensure that the sum of the ratios for all data equals 1.",
        "table": "Sx4.T3.1",
        "footnotes": [],
        "references": [
            "We first analyze the impact of different training strategies on model performance. Figure  4 a presents a comparison of InstructBioMol with two of its variants: one variant retains continual pretraining and stage-2 instruction-tuning but removes stage-1 instruction-tuning (denoted as \"w/ continual-pretraining, w/o stage-1 instruction-tuning\"), while the other variant only retains stage-2 instruction-tuning but removes both continual pretraining and stage-1 instruction-tuning (denoted as \"w/o continual-pretraining, w/o stage-1 instruction-tuning\"). From these comparisons, we derive the following conclusions:",
            "Next, we explore the impact of incorporating multimodal data on model performance. As shown in Figure  4 b, we compare InstructBioMol and several of its variants on molecule captioning and protein function answering tasks. These variants include the removal of the 2D encoder (w/o 2D encoder) and 3D encoder (w/o 3D encoder) for molecular data, as well as the removal of the 1D encoder (w/o 1D encoder) and 3D encoder (w/o 3D encoder) for protein data. Additionally, we consider the removal of motif prompts (w/o motif) and the entire Motif-Guided Multimodal Feature Extraction Module (w/o multimodal). For comparison, the InstructBioMol variant used here excludes both the continual-pretraining stage and stage-1 instruction-tuning, serving as a basis for the ablation study on multimodal inputs.",
            "To simplify, subscripts are omitted since molecules and proteins undergo the same processing. Here,  P P \\mathbf{P} bold_P  and  H H \\mathbf{H} bold_H  are derived from Eq.  3  and Eq.  4  respectively.  Q Q \\mathbf{Q} bold_Q  denotes a sequence of learnable queries as  Q = [ q 1 , q 2 , ... , q N q ]  R N q  d Q subscript q 1 subscript q 2 ... subscript q subscript N q superscript R subscript N q d \\mathbf{Q}=[\\mathbf{q}_{1},\\mathbf{q}_{2},...,\\mathbf{q}_{N_{q}}]\\in\\mathbb{R}% ^{N_{q}\\times d} bold_Q = [ bold_q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , bold_q start_POSTSUBSCRIPT italic_N start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT end_POSTSUBSCRIPT ]  blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT  italic_d end_POSTSUPERSCRIPT , and  Z  R ( 1 + N q )  d Z superscript R 1 subscript N q d \\mathbf{Z}\\in\\mathbb{R}^{(1+N_{q})\\times d} bold_Z  blackboard_R start_POSTSUPERSCRIPT ( 1 + italic_N start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT )  italic_d end_POSTSUPERSCRIPT  is the extracted multimodal features.",
            "x m  u  l  t  i  m  o  d  a  l  _  m  o  l subscript x m u l t i m o d a l _ m o l x_{multimodal\\_mol} italic_x start_POSTSUBSCRIPT italic_m italic_u italic_l italic_t italic_i italic_m italic_o italic_d italic_a italic_l _ italic_m italic_o italic_l end_POSTSUBSCRIPT  and  x m  u  l  t  i  m  o  d  a  l  _  p  r  o  t subscript x m u l t i m o d a l _ p r o t x_{multimodal\\_prot} italic_x start_POSTSUBSCRIPT italic_m italic_u italic_l italic_t italic_i italic_m italic_o italic_d italic_a italic_l _ italic_p italic_r italic_o italic_t end_POSTSUBSCRIPT  denote molecules and proteins, respectively. The inputs also encompass instructions  x i  n  s  u  c  t  i  o  n subscript x i n s u c t i o n x_{insuction} italic_x start_POSTSUBSCRIPT italic_i italic_n italic_s italic_u italic_c italic_t italic_i italic_o italic_n end_POSTSUBSCRIPT  in natural language, e.g., \" What is the function of this protein \", as well as description text  x t  e  x  t subscript x t e x t x_{text} italic_x start_POSTSUBSCRIPT italic_t italic_e italic_x italic_t end_POSTSUBSCRIPT , e.g., \" The molecule is a member of benzenes, a sulfone and a member of triazoles. \". The composition of the inputs adapts based on the specific task. For instance, in generating molecular descriptions, the inputs include  x i  n  s  t  r  u  c  t  i  o  n subscript x i n s t r u c t i o n x_{instruction} italic_x start_POSTSUBSCRIPT italic_i italic_n italic_s italic_t italic_r italic_u italic_c italic_t italic_i italic_o italic_n end_POSTSUBSCRIPT  and  x m  u  l  t  i  m  o  d  a  l subscript x m u l t i m o d a l x_{multimodal} italic_x start_POSTSUBSCRIPT italic_m italic_u italic_l italic_t italic_i italic_m italic_o italic_d italic_a italic_l end_POSTSUBSCRIPT , whereas in generating molecules from descriptions, the inputs consist of  x i  n  s  t  r  u  c  t  i  o  n subscript x i n s t r u c t i o n x_{instruction} italic_x start_POSTSUBSCRIPT italic_i italic_n italic_s italic_t italic_r italic_u italic_c italic_t italic_i italic_o italic_n end_POSTSUBSCRIPT  and  x t  e  x  t subscript x t e x t x_{text} italic_x start_POSTSUBSCRIPT italic_t italic_e italic_x italic_t end_POSTSUBSCRIPT . The form of input corresponding to each task is detailed in Table  4 .",
            "We collect datasets on a hundred-million-scale, including a continual pretraining dataset (Table  3 ), and an instruction-tuning dataset (Table  4 ). The continual pretraining dataset comprises molecules and proteins in textual format, and natural language texts derived from scientific literature, enabling the model to develop a foundational adapting to biomolecular research. The instruction-tuning dataset contains various alignment pairs: molecule-natural language, protein-natural language, and molecule-protein (Figure  1 d), achieving any-to-any alignment among molecules, proteins and natural language. Additionally, both molecular and protein data in the instruction-tuning dataset are multimodal, incorporating 2-D and 3-D structures of molecules and 3-D structures of proteins.",
            "Despite the collection of a broad range of data, the quality of this data exhibits considerable variability across sources. For example, the ChEBI database offers a broader spectrum of molecular descriptions compared to the natural-language-like structure descriptions provided by IUPAC names in PubChem. Similarly, while data within the SwissProt undergo meticulous manual curation, entries in the TrEMBL do not benefit from such rigorous calibration. The tradeoff between the scale and quality of data poses significant challenges to model performance and generalizability. To address this issue, we adopt a two-stage instruction-tuning strategy designed to exploit the extensive data initially, then progressively direct the focus towards the insights offered by higher-quality datasets. Initially, in stage-1, the model is trained across all the available instructions. This stage leverages the diversity and volume of data to build a foundation on biomolecular alignment. Subsequently, in stage-2, the model undergoes further fine-tuning on a subset of higher-quality data. This approach harnesses both the expansive coverage of lower-quality data and the precision inherent in high-quality data, facilitating an efficient and effective utilization of the dataset. The scale and specific details of the different datasets used in the two stages are presented in Figure  1 c and Table  4 .",
            "We provide the example entries of continual pretraining dataset in Table  4 , and the example entries of instruction-tuning dataset in Table  5 , Table  6  and Table  7 ."
        ]
    },
    "id_table_5": {
        "caption": "Table 2:  Inference hyperparameters for downstream tasks.",
        "table": "Sx4.T4.16",
        "footnotes": [],
        "references": [
            "By expanding the vocabulary and incorporating diverse multimodal features, we integrate molecules or proteins into textual formats, thereby augmenting the language models capacity to interpret biomolecules. Specifically, we concatenate these multimodal features  [ z 1 , z 2 , ... , z ( N q + 1 ) ] subscript z 1 subscript z 2 ... subscript z subscript N q 1 [\\mathbf{z}_{1},\\mathbf{z}_{2},...,\\mathbf{z}_{(N_{q}+1)}] [ bold_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , bold_z start_POSTSUBSCRIPT ( italic_N start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT + 1 ) end_POSTSUBSCRIPT ]  obtained from Eq.  5  with sequence-modality input, and label them with special tokens:",
            "We provide the example entries of continual pretraining dataset in Table  4 , and the example entries of instruction-tuning dataset in Table  5 , Table  6  and Table  7 ."
        ]
    },
    "id_table_6": {
        "caption": "Table 3:  In-context learning examples of the GPT baseline across different tasks, Where  XX  represents data-specific natural language descriptions, protein sequences, or molecular sequences. In the few-shot setting, the input to the GPT model consists of a template, in-context demonstrations, and a question; in the zero-shot setting, the input consists of a template and a question.",
        "table": "S4.T1.16",
        "footnotes": [],
        "references": [
            "We start with a pretrained language model and continue pretraining it on the continual pretraining dataset in a self-supervised causal language modeling objective  [ 55 ] . Subsequently, we employ instruction-tuning, to establish an any-to-any alignment among natural language, molecules, and proteins. This involves aligning specific instructions and inputs with appropriate responses, represented as  ( x i  n  s  t  r  u  c  t  i  o  n , x i  n  p  u  t )  y  subscript x i n s t r u c t i o n subscript x i n p u t y (x_{instruction},x_{input})\\rightarrow y ( italic_x start_POSTSUBSCRIPT italic_i italic_n italic_s italic_t italic_r italic_u italic_c italic_t italic_i italic_o italic_n end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_i italic_n italic_p italic_u italic_t end_POSTSUBSCRIPT )  italic_y , where  x i  n  p  u  t subscript x i n p u t x_{input} italic_x start_POSTSUBSCRIPT italic_i italic_n italic_p italic_u italic_t end_POSTSUBSCRIPT  may include multimodal molecules  x m  u  l  t  i  m  o  d  a  l  _  m  o  l subscript x m u l t i m o d a l _ m o l x_{multimodal\\_mol} italic_x start_POSTSUBSCRIPT italic_m italic_u italic_l italic_t italic_i italic_m italic_o italic_d italic_a italic_l _ italic_m italic_o italic_l end_POSTSUBSCRIPT , proteins  x m  u  l  t  i  m  o  d  a  l  _  p  r  o  t subscript x m u l t i m o d a l _ p r o t x_{multimodal\\_prot} italic_x start_POSTSUBSCRIPT italic_m italic_u italic_l italic_t italic_i italic_m italic_o italic_d italic_a italic_l _ italic_p italic_r italic_o italic_t end_POSTSUBSCRIPT  defined in Eq.  6 , or natural language in textual format  x t  e  x  t subscript x t e x t x_{text} italic_x start_POSTSUBSCRIPT italic_t italic_e italic_x italic_t end_POSTSUBSCRIPT , and  y y y italic_y  denotes corresponding responses such as natural language  y t  e  x  t subscript y t e x t y_{text} italic_y start_POSTSUBSCRIPT italic_t italic_e italic_x italic_t end_POSTSUBSCRIPT , molecular sequences  y m  o  l subscript y m o l y_{mol} italic_y start_POSTSUBSCRIPT italic_m italic_o italic_l end_POSTSUBSCRIPT , or protein sequences  y p  r  o  t  e  i  n subscript y p r o t e i n y_{protein} italic_y start_POSTSUBSCRIPT italic_p italic_r italic_o italic_t italic_e italic_i italic_n end_POSTSUBSCRIPT . To achieve a thorough alignment, we introduce a bidirectional alignment task for each pairwise alignment among natural language, molecules and proteins. For example, for molecule-natural language pairs, one task generates textual descriptions from molecular data:  ( x i  n  s  t  r  u  c  t  i  o  n , x m  u  l  t  i  m  o  d  a  l  _  m  o  l )  y t  e  x  t  subscript x i n s t r u c t i o n subscript x m u l t i m o d a l _ m o l subscript y t e x t (x_{instruction},x_{multimodal\\_mol})\\rightarrow y_{text} ( italic_x start_POSTSUBSCRIPT italic_i italic_n italic_s italic_t italic_r italic_u italic_c italic_t italic_i italic_o italic_n end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_m italic_u italic_l italic_t italic_i italic_m italic_o italic_d italic_a italic_l _ italic_m italic_o italic_l end_POSTSUBSCRIPT )  italic_y start_POSTSUBSCRIPT italic_t italic_e italic_x italic_t end_POSTSUBSCRIPT , and another task generates molecules from descriptions:  ( x i  n  s  t  r  u  c  t  i  o  n , x t  e  x  t )  y m  o  l  subscript x i n s t r u c t i o n subscript x t e x t subscript y m o l (x_{instruction},x_{text})\\rightarrow y_{mol} ( italic_x start_POSTSUBSCRIPT italic_i italic_n italic_s italic_t italic_r italic_u italic_c italic_t italic_i italic_o italic_n end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_t italic_e italic_x italic_t end_POSTSUBSCRIPT )  italic_y start_POSTSUBSCRIPT italic_m italic_o italic_l end_POSTSUBSCRIPT . The instruction-tuning is optimized under a causal language modeling objective:",
            "We provide the example entries of continual pretraining dataset in Table  4 , and the example entries of instruction-tuning dataset in Table  5 , Table  6  and Table  7 ."
        ]
    },
    "id_table_7": {
        "caption": "Table 4:  Examples of the continual pertaining data.",
        "table": "S4.T2.6",
        "footnotes": [],
        "references": [
            "We provide the example entries of continual pretraining dataset in Table  4 , and the example entries of instruction-tuning dataset in Table  5 , Table  6  and Table  7 ."
        ]
    },
    "id_table_8": {
        "caption": "Table 5:  Examples of the instruction-tuning data on molecule-natural language alignment.",
        "table": "S4.T3.3.1",
        "footnotes": [],
        "references": [
            "To evaluate the models performance, we divide the dataset used for the stage-2 instruction tuning into training and test sets. For the dataset aligning molecules with natural language, we adopt the data split from ref.  14 . For the dataset aligning proteins with natural language, we use the training data defined in ref.  19  and randomly select 3000 samples from the test set as our evaluation data. Statistics of the above two datasets are in Table  8 . For the dataset aligning molecules with proteins, we account for the specific nature of the tasks. In the task of generating drug-like molecules for proteins, a single protein typically corresponds to multiple molecules. Conversely, in the task of generating enzymes for substrate molecules, a single substrate often corresponds to multiple proteins. Accordingly, we split the dataset as follows: for the former task, we select 100 target proteins and their corresponding molecules as the test set. For the latter task, we select 100 target substrates and their corresponding proteins as the test set. The specific sizes of each dataset split are detailed in Table  9 ."
        ]
    },
    "id_table_9": {
        "caption": "Table 6:  Examples of the instruction-tuning data on protein-natural language alignment.",
        "table": "S4.T4.3.1",
        "footnotes": [],
        "references": [
            "To evaluate the models performance, we divide the dataset used for the stage-2 instruction tuning into training and test sets. For the dataset aligning molecules with natural language, we adopt the data split from ref.  14 . For the dataset aligning proteins with natural language, we use the training data defined in ref.  19  and randomly select 3000 samples from the test set as our evaluation data. Statistics of the above two datasets are in Table  8 . For the dataset aligning molecules with proteins, we account for the specific nature of the tasks. In the task of generating drug-like molecules for proteins, a single protein typically corresponds to multiple molecules. Conversely, in the task of generating enzymes for substrate molecules, a single substrate often corresponds to multiple proteins. Accordingly, we split the dataset as follows: for the former task, we select 100 target proteins and their corresponding molecules as the test set. For the latter task, we select 100 target substrates and their corresponding proteins as the test set. The specific sizes of each dataset split are detailed in Table  9 ."
        ]
    },
    "id_table_10": {
        "caption": "Table 7:  Examples of the instruction-tuning data on molecule-protein alignment.",
        "table": "S4.T5.1.1",
        "footnotes": [],
        "references": [
            "In the protein function answering task, InstructBioMol generates results that closely resemble the ground truth for certain cases, such as Q9NRY2 and P73070 in Table  10 . Furthermore, we observe that in some cases, the generated descriptions tend to be more detailed. For example, in the case of Q9Y2G3, the generated description includes detailed information on vesicle formation, lipid signal molecule uptake, and the establishment of the thrombopoietin gradient in platelets. Similarly, for Q9FY89, the generated description provides a more comprehensive explanation of the formation of multivesicular bodies (MVBs), specifically describing the formation mechanism of intraluminal vesicles (ILVs) within MVBs, including the invagination and scission of the endosomal membrane. It further elaborates on the function of MVBs, such as transporting their contents to lysosomes for the degradation of membrane proteins, receptors, lysosomal enzymes, and lipids. For P0CP67, the generated description offers more detailed and in-depth functional information, identifying the specific targets of the proteins action (e.g., components of AP-1, c-Jun, and ATF2) and potential biological processes involved (e.g., regulation of circadian clock). Although InstructBioMol may provide researchers with deeper insights, further experimental validation is necessary to confirm these findings."
        ]
    },
    "id_table_11": {
        "caption": "Table 8:  Statistics of datasets for molecule-natural language alignment and protein-natural language alignment.",
        "table": "S4.T6.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "Table 9:  Statistics of datasets for molecule-protein alignment. # of Entries and # of Targets denote the number of data entries and the number of targets, respectively.",
        "table": "S4.T7.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "Table 10:  Case analysis for protein function answering task.",
        "table": "S4.T8.1",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "S4.T9.1",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "",
        "table": "S4.T10.3.1",
        "footnotes": [],
        "references": []
    }
}