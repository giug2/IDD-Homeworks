{
    "id_table_1": {
        "caption": "Table 1 :  Comparison with baseline methods.    indicates the evaluation results on downsampled ImageNet-1K dataset. Here, SRe 2 L and our proposed methods adopt the ResNet18 as the training and evaluation model, other methods adopt ConvNet.",
        "table": "S4.T1.46.46",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Following the definition of DD, Wang  et al.   [ 33 ]  in seminal propose a principled meta-learning-based algorithm: in each iteration, a novel model is trained on the synthetic dataset in a nested loop, and the trained model is meta-tested on original data, and the gradient is backpropagated through an unrolled computation graph to update current synthetic data. Although effective, such bi-level optimization results in high memory and time complexity in practice. Specifically, on the one hand, each update of the current synthetic dataset necessitates an iterative meta-train process from a new model, making the time required for each step notably prolonged. On the other hand, caching the whole computation graph for back-propagation demands significant GPU memory. As illustrated in Fig.  1 , for instance, under the setting of a 10-class dataset with 50 images per class, each update requires over 50 GiB GPU memory, and the entire data generation takes hundreds of hours, let alone when condensing the large-scale ImageNet dataset, which has thousands of times optimization pixel space of the small dataset like CIFAR10.",
            "As Eq.  1  shows, the objective involves a bi-level optimization process. Each step of updating the synthetic dataset needs to backpropagate through an unrolled computation graph, making it inefficient for both memory and time. Moreover, it should train a novel model in a nested loop for every iteration, resulting in notable time consumption, particularly when handling complex datasets.",
            "In this section, we start with the fundamental solution to DD, the meta-learning-based method, progressively delving into our proposed method, Teddy, with theoretical analysis. The DD formulation is shown in Eq.  1 . Practically, to enhance the generalization ability of the distilled data on the real data, the inner loop is typically configured with multiple steps,  e.g. ,  T > 10 T 10 T>10 italic_T > 10 . Considering the synthetic dataset is of small size, the model is highly susceptible to overfitting on it as training for  T T T italic_T  epochs. Therefore, it has  l  ( S ;  S ( T ) ) <  l S subscript superscript  T S italic- l(\\mathcal{S};\\theta^{(T)}_{\\mathcal{S}})<\\epsilon italic_l ( caligraphic_S ; italic_ start_POSTSUPERSCRIPT ( italic_T ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_S end_POSTSUBSCRIPT ) < italic_ , where   italic- \\epsilon italic_  is close to zero. Employing the Karush-Kuhn-Tucker (KKT) conditions, the optimization objective in Eq.  1  can be transformed into the following:",
            "In summary, we propose an efficient large-scale dataset distillation method, Teddy, solving the core bi-level optimization problem and further improving the efficiency via Taylor approximation. The framework of Teddy is shown in Algorithm  1 . We first generate model pool  M M \\mathcal{M} caligraphic_M  from single base model   b  a  s  e subscript  b a s e \\theta_{base} italic_ start_POSTSUBSCRIPT italic_b italic_a italic_s italic_e end_POSTSUBSCRIPT . According to the state of the   b  a  s  e subscript  b a s e \\theta_{base} italic_ start_POSTSUBSCRIPT italic_b italic_a italic_s italic_e end_POSTSUBSCRIPT , we choose from two efficient model pool generation strategies, prior and post-generation. Then, we follow our proposed optimization objective, Eq.  7 , to update the synthetic dataset, which only requires the first-order optimization process. Here, considering the weak teacher models may mislead the data generation direction, we generate soft labels via weak teacher ensemble, which provides more richer label information expression.",
            "As demonstrated in Table  1 , our proposed method, Teddy, significantly surpasses all the baselines for both the Tiny-ImageNet and ImageNet-1K datasets with all IPC settings. Furthermore, comparing the results across the two datasets reveals that our method exhibits a more pronounced improvement on complex datasets. In particular, in the case of ImageNet-1K with IPC 10, our evaluation results outperform the previous state-of-the-art methods by a large margin up to 12.8%. Moreover, our method achieves the same performance with the setting of IPC 50 (52.5%) as the previous SOTA method in the case of IPC 100 (52.8%). Additionally, for DD, enhancing the performance of the larger synthetic dataset is particularly challenging. This is because the existing DD methods focus mainly on key dataset information, ignoring long-tail information. With increased IPCs, there is less room to enhance as compressing these details is hard. However, as shown in Table  1 , our method considerably improves the performance for large IPCs (5.7% and 3.7% for IPC 50 and 100). For the post-generation, although we applied pruning to the teacher model, resulting in a reduction of statistic information space in its batch normalization layers, the results in Table  1  indicate that the impact is not substantial. We can still achieve comparable performance.",
            "where   ( 0 )   similar-to superscript  0  \\theta^{(0)}\\sim\\Theta italic_ start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT  roman_ , and  u  = u    g T ( t )    g S ( t )  superscript u  u  norm superscript subscript g T t norm superscript subscript g S t u^{{}^{\\prime}}=\\frac{u}{\\alpha||g_{\\mathcal{T}}^{(t)}||||g_{\\mathcal{S}}^{(t)% }||} italic_u start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT  end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT = divide start_ARG italic_u end_ARG start_ARG italic_ | | italic_g start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT | | | | italic_g start_POSTSUBSCRIPT caligraphic_S end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT | | end_ARG . The first term of the Eq.  11  is the sum of the cosine distance of the gradients for the synthetic and original data, respectively, on the all checkpoints of the student trajectory. In other words, the meta-learning-based optimization objective can be Taylor-approximated as the sum of the multi-step gradient matching. Here, we denote  e ( t ) = g ( t )  g ( t )  superscript e t superscript g t norm superscript g t e^{(t)}=\\frac{g^{(t)}}{||g^{(t)}||} italic_e start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT = divide start_ARG italic_g start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT end_ARG start_ARG | | italic_g start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT | | end_ARG , and for each gradient matching at step  t t t italic_t , we have:",
            "However, the Eq.  13  shows that each update step of  S S \\mathcal{S} caligraphic_S  should compute the gradient of  S S \\mathcal{S} caligraphic_S  and  T T \\mathcal{T} caligraphic_T  on all checkpoints of the student trajectory, which is a large amount of computational costs. Here, we reduce the costs by adopting another approximation strategy, turning the second-order optimization into the first-order one. Following the Proposition 2, for simplicity in explanation, we only consider the last linear layer of the network is updated during the generation process, and the parameter is denoted as  W W W italic_W . The preceding layers are regraded as the feature extractor, denoted as  f  subscript f  f_{\\theta} italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT . For each gradient, we have:",
            "The first term of Eq.  14  is a weighted covariance matrix of the feature space, and the second term is the class-wise mean of the feature space. The gradient matching for  l 2 superscript l 2 l^{2} italic_l start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  distance can be transformed as:",
            "From Eq.  15 , it shows that the upper bound of the gradient matching is the first-order and the second-order statistic information matching in feature space. Here, we need to consider the prior condition such that   ( 0 )   similar-to superscript  0  \\theta^{(0)}\\sim\\Theta italic_ start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT  roman_ . It means that for every   ( 0 ) superscript  0 \\theta^{(0)} italic_ start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT  samples from the distribution    \\Theta roman_ , Eq.  15  reaches the minimum value. In this case, the first term of Eq.  15 , the covariance for the original data and synthetic data, is equivalent for all   ( 0 ) superscript  0 \\theta^{(0)} italic_ start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT  samples, and the class-wise mean. Also, the equality in Eq.  15  holds. As for   ( t ) superscript  t \\theta^{(t)} italic_ start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT , as long as the samples are sufficient, the condition still holds. To be more specific, here we assume  f   ( X t )  R N t  f d subscript f  subscript X t superscript R subscript N t subscript f d f_{\\theta}(X_{t})\\in\\mathbb{R}^{N_{t}\\times f_{d}} italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  italic_f start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_POSTSUPERSCRIPT , and  f   ( X s )  R N s  f d subscript f  subscript X s superscript R subscript N s subscript f d f_{\\theta}(X_{s})\\in\\mathbb{R}^{N_{s}\\times f_{d}} italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_X start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT )  blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  italic_f start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ,  f d subscript f d f_{d} italic_f start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT  is the dimension of the feature.  f   ( X t ) = [ f   ( x 1 t ) T , ... , f   ( x N t t ) T ] T subscript f  subscript X t superscript subscript f  superscript subscript superscript x t 1 T ... subscript f  superscript subscript superscript x t subscript N t T T f_{\\theta}(X_{t})=[f_{\\theta}(x^{t}_{1})^{T},\\dots,f_{\\theta}(x^{t}_{N_{t}})^{% T}]^{T} italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = [ italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT , ... , italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT , and  f   ( x i t )  R f d subscript f  subscript superscript x t i superscript R subscript f d f_{\\theta}(x^{t}_{i})\\in\\mathbb{R}^{f_{d}} italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  blackboard_R start_POSTSUPERSCRIPT italic_f start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_POSTSUPERSCRIPT . Also,  f   ( X s ) = [ f   ( x 1 s ) T , ... , f   ( x N s s ) T ] T subscript f  subscript X s superscript subscript f  superscript subscript superscript x s 1 T ... subscript f  superscript subscript superscript x s subscript N s T T f_{\\theta}(X_{s})=[f_{\\theta}(x^{s}_{1})^{T},\\dots,f_{\\theta}(x^{s}_{N_{s}})^{% T}]^{T} italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_X start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) = [ italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_x start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT , ... , italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_x start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_N start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT , and  f   ( x i s )  R f d subscript f  subscript superscript x s i superscript R subscript f d f_{\\theta}(x^{s}_{i})\\in\\mathbb{R}^{f_{d}} italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_x start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  blackboard_R start_POSTSUPERSCRIPT italic_f start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_POSTSUPERSCRIPT .  f   ( X t ) = [ F 1 t , ... , F f d t ] subscript f  subscript X t subscript superscript F t 1 ... subscript superscript F t subscript f d f_{\\theta}(X_{t})=[F^{t}_{1},\\dots,F^{t}_{f_{d}}] italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = [ italic_F start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_F start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_POSTSUBSCRIPT ] ,  F i t  R N t subscript superscript F t i superscript R subscript N t F^{t}_{i}\\in\\mathbb{R}^{N_{t}} italic_F start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUPERSCRIPT , and  f   ( X s ) = [ F 1 s , ... , F f d s ] subscript f  subscript X s subscript superscript F s 1 ... subscript superscript F s subscript f d f_{\\theta}(X_{s})=[F^{s}_{1},\\dots,F^{s}_{f_{d}}] italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_X start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) = [ italic_F start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_F start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_POSTSUBSCRIPT ] ,  F i s  R N s subscript superscript F s i superscript R subscript N s F^{s}_{i}\\in\\mathbb{R}^{N_{s}} italic_F start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUPERSCRIPT . For the first term of Eq.  15 , we have:",
            "Eq.  16  shows that if the covariance matching holds, then variance matching also holds. To improve the calculation efficiency, here we adopt variance matching instead of covariance matching. As for the second term, it is the class-wise mean matching in feature space. Specifically, we have:",
            "Here,  M c t  R N t  c superscript subscript M c t superscript R subscript N t c \\mathcal{M}_{c}^{t}\\in\\mathbb{R}^{N_{t}\\times c} caligraphic_M start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  italic_c end_POSTSUPERSCRIPT  and  M c s  R N s  c superscript subscript M c s superscript R subscript N s c \\mathcal{M}_{c}^{s}\\in\\mathbb{R}^{N_{s}\\times c} caligraphic_M start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  italic_c end_POSTSUPERSCRIPT , are the matrices to indicate whether the samples belong to the classes. For instance,  M i  j = 1 subscript M i j 1 M_{ij}=1 italic_M start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = 1  means the  i t  h superscript i t h i^{th} italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT  sample is belong to the  j t  h superscript j t h j^{th} italic_j start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT  class, otherwise, it is not. The Eq.  17  can be:",
            "where  C  l i C subscript l i Cl_{i} italic_C italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is the set of samples belong to  i t  h superscript i t h i^{th} italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT  class. When the classes of the original dataset is balanced, or the number of samples in every class of the original dataset is same, the second term of Eq.  15  can be replaced by the global mean. Therefore, the Eq.  13  can be transformed as follows:",
            "where   ( 0 )   similar-to superscript  0  \\theta^{(0)}\\sim\\Theta italic_ start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT  roman_ ,   l subscript  l \\mu_{l} italic_ start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT  and   l 2 subscript superscript  2 l \\sigma^{2}_{l} italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT  refer to the mean and variance of the  l t  h superscript l t h l^{th} italic_l start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT  layer features. Although Eq.  19  has significantly improved efficiency, computing the mean and variance of checkpoints at each inner loop remains a substantial computational overhead, especially when there are numerous inner loops and a relatively large original dataset. Here, we propose substituting performance-comparable weak teachers for student models and reducing the number of rounds required for inner loop. As for the student trajectory starting at  t t t italic_t  and ending at  t + m t m t+m italic_t + italic_m , we have:",
            "In Eq.  10 , we recursively apply the first-order Taylor expansion to the original optimization objective to decouple the bi-level optimization process. This section will further analyze the error caused by Taylor approximation part, and theoretical reasonability.",
            "In Eq.  18 , we assume that the dataset is balanced. This section will further analyze this condition and show the robustness of our proposed method. We conduct the experiments under the setting of Tiny-ImageNet IPC 20. For each class of Tiny-ImageNet, we randomly select 60%  similar-to \\sim  100% and 40%  similar-to \\sim  100% of the original daraset to build the imbalanced dataset. The results are shown in the Table  3 . Even if imbalanced models are used, our method still demonstrates some resistance to this issue compared with the baseline as shown in Table  3 . This can be attributed to more informative statistics from diversified teachers for model-to-data synthesis. Moreover, the performance could be further improved by considering the class-wise statistics."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Evaluation results of cross-architecture generalization under the ImageNet-1K with IPC 10 setting. SRe 2 L and our methods use ResNet18 as the training model.",
        "table": "S4.T2.19.19",
        "footnotes": [
            ""
        ],
        "references": [
            "Therefore, the optimization objective in Eq.  2  can be transferred to:",
            "Insight from Eq.  6 , to further improve the time efficiency, we construct the weak teacher model pool from two practical cases, the prior and post-generation. These strategies are both efficiently generating model pool from a single base model, as shown in Fig.  2(a)  and Fig.  2(b) . Here, as we utilize weak teachers instead of fully-convergence ones, we can just use a single model as the base to generate diverse weak teachers. This strategy does not compromise performance. More specifically, if the base model is:",
            "To verify the rationality of this strategy, we conduct validation experiments. We measure the distance between the student and teacher models at different stages and the accuracy of the distilled data generated from teacher models at different stages. Here, we use KL divergence to measure model distance, capturing the information loss when approximating one probability distribution with another. The results shown in Fig.  2(c)  reveal a strong inverse correlation between model distance and distilled data performance. Closer distances indicate better approximation and superior performance.",
            "The generalization capacity is vital for practically applying dataset distillation. Here, we evaluate the generalization performance of the distilled dataset of the ImageNet-1K IPC 10 setting. In selecting model architecture, we adopt the ResNet50, ResNet101, DenseNet121, MobileNetV2, ShuffleNetV2, and EfficientNetB0. All results are reported in Table  2 .",
            "The results show that our proposed method, Teddy, surpasses all previous state-of-the-art methods by a significant margin. When the evaluation models have larger and deeper architectures, there is a significant and proportional performance improvement, comparing 34.1%, 39.0%, and 40.3% for ResNet18, 50, and 101, with enhancements of 12.8%, 10.6% and 9.4%. For structurally different networks, our method demonstrates strong generalization capabilities. For instance, on DenseNet121, MobileNetV2, ShuffleNetV2, and EfficientNetB0, we achieved 34.3%, 23.4%, 40.0%, and 29.2%, respectively, surpassing the SOTA by 12.8%, 13.2%, 10.9%, and 13.1%. As shown in Table  2 , the evaluation results of the post-generation method indicate a strong generalization capability, even with the architecture heterogeneity introduced by network pruning, surpassing the previous SOTA methods.",
            "We conduct ablation studies on the stage of weak teachers adopted in our method under the setting of ImageNet-1K with IPC 10, IPC 50, and IPC 100. The results are shown in Fig.  2(c) . Here, we generate the model pool with weak teachers in different stages. We cache 9 models for each trajectory segment with step 5. For data generation, we randomly select 3 out of the model pool and use the whole models in the model pool to relabel the synthetic data. From the results, it is evident that the performance of teacher models significantly impacts the distilled data. For smaller IPCs, adopting the weak teacher models of the early stage is necessary.",
            "From Eq.  20 , it shows that the multi-step gradient of the model on the synthetic dataset compared to the single-step gradient on the original dataset. Therefore, we cache the weak teacher for every  m m m italic_m  steps from  T b subscript T b T_{b} italic_T start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT  to  T e subscript T e T_{e} italic_T start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT . Here, we also utilize the running mean and running variance stored in the batch normalization layers of weak teachers to replace the mean and variance of the original dataset in feature space, reducing computational costs. The optimization objective is as follows:"
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Results on imbalanced dataset with each class has 60%-100% and 40%-100% samples.",
        "table": "Pt0.A2.T3.3.3",
        "footnotes": [],
        "references": [
            "where   ( 0 )   similar-to superscript  0  \\theta^{(0)}\\sim\\Theta italic_ start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT  roman_ , and  g T ( t ) =   S ( t ) l  ( T ;  S ( t ) ) superscript subscript g T t subscript  subscript superscript  t S l T subscript superscript  t S g_{\\mathcal{T}}^{(t)}=\\nabla_{\\theta^{(t)}_{\\mathcal{S}}}l(\\mathcal{T};\\theta^% {(t)}_{\\mathcal{S}}) italic_g start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT =  start_POSTSUBSCRIPT italic_ start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_l ( caligraphic_T ; italic_ start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT caligraphic_S end_POSTSUBSCRIPT ) . The first term  l  ( T ;  ( 0 ) ) l T superscript  0 l(\\mathcal{T};\\theta^{(0)}) italic_l ( caligraphic_T ; italic_ start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT )  is the loss of the original dataset on the initial model   ( 0 ) superscript  0 \\theta^{(0)} italic_ start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT , and irrelevant to the optimization problem  w.r.t   S S \\mathcal{S} caligraphic_S . Concluded from Eq.  3 , the meta-learning-based optimization objective can be transformed into the sum of the gradient matching for all steps pathing the student model training trajectory.",
            "For the data generation part, we conduct experiments on the setting of ImageNet-1K with IPC 10, and the setting of the model pool is the same as the main table. For data generation, 1, 2, 3, 4, or 5 models will be randomly ensemble selected. The soft labels are generated by the whole model pool. The results in Fig.  3(a)  reveal that the performance is better with multiple models than with a single model. The accuracy drops in Fig.  3(b)  can be attributed to the trade-off between trying fit numerous models comprehensively or concentrating on fitting fewer ones. Also, our method exhibits robustness in terms of the number of models selected for generation.",
            "As for the relabel part, we conduct experiments on the setting of ImageNet-1K with IPC 50, and the setting of the model pool is the same as the main table. For data generation, 3 models are randomly selected from the model pool. Those generated data will be ensemble relabeled by 1, 2, 4, 6, 8, or whole models of the model pool. The evaluation results are shown in Fig.  3(b) . We observe a significant enhancement in performance with ensemble relabeling, but also robust for the number of models selected for relabeling.",
            "We also perform an ablation study on the size of the model pool, and the results are shown in Fig.  3(c) . The experiments are under the setting of ImageNet-1K IPC 10 and IPC 50. For one experiment setting, we cache the teacher models from the same trajectory segments; the beginning and ending points are the same as the main table but with different step lengths. Here, we randomly select 3 models from the model pool to generate data, and the whole model pool is adopted to generate the soft label. From the results, we observe that the size of the model pool does not significantly impact the performance of the generated synthetic data when the size is larger than 5. In this setting, a smaller size of the model pool may lead to an issue with mode collapse and a subsequent decline in performance.",
            "In this section, we evaluate the efficiency of our proposed method. The experiments are under the setting of ImageNet-1K IPC 10, utilizing 8 RTX 4090 GPUs. Here, the cached teachers are at stages 1 to 41. We report the total run time of model pre-training and data synthesizing, and the peak GPU memory of the data synthesizing process. The evaluation results are shown in Fig.  3(d) . From the results, due to the fact that we only need to train teacher models for a limited number of epochs (which is the major time overhead), the total time required for our method is significantly less than that of SRe 2 L. Noticing that when utilizing a single teacher model, the required GPU memory is the same as SRe 2 L, and the performance of the distilled data is notably superior to SRe 2 L.",
            "Here, we follow the previous work  [ 40 ]  employing the GDumb  [ 23 ]  as the base method. This method stores and combines past and new training data to train a model from scratch. We evaluate our proposed method on Tiny-ImageNet with IPC 50 following the 5-step class-incremental protocol. The results are shown in Fig.  3(e) . Our method significantly outperforms the baseline methods and demonstrates an accuracy trend that differs from random selection, gradually increasing with the addition of categories. This trend is attributed to the soft label strategy, which allows for incorporating more cross-class knowledge.",
            "However, the Eq.  13  shows that each update step of  S S \\mathcal{S} caligraphic_S  should compute the gradient of  S S \\mathcal{S} caligraphic_S  and  T T \\mathcal{T} caligraphic_T  on all checkpoints of the student trajectory, which is a large amount of computational costs. Here, we reduce the costs by adopting another approximation strategy, turning the second-order optimization into the first-order one. Following the Proposition 2, for simplicity in explanation, we only consider the last linear layer of the network is updated during the generation process, and the parameter is denoted as  W W W italic_W . The preceding layers are regraded as the feature extractor, denoted as  f  subscript f  f_{\\theta} italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT . For each gradient, we have:",
            "where  C  l i C subscript l i Cl_{i} italic_C italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is the set of samples belong to  i t  h superscript i t h i^{th} italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT  class. When the classes of the original dataset is balanced, or the number of samples in every class of the original dataset is same, the second term of Eq.  15  can be replaced by the global mean. Therefore, the Eq.  13  can be transformed as follows:",
            "In Eq.  18 , we assume that the dataset is balanced. This section will further analyze this condition and show the robustness of our proposed method. We conduct the experiments under the setting of Tiny-ImageNet IPC 20. For each class of Tiny-ImageNet, we randomly select 60%  similar-to \\sim  100% and 40%  similar-to \\sim  100% of the original daraset to build the imbalanced dataset. The results are shown in the Table  3 . Even if imbalanced models are used, our method still demonstrates some resistance to this issue compared with the baseline as shown in Table  3 . This can be attributed to more informative statistics from diversified teachers for model-to-data synthesis. Moreover, the performance could be further improved by considering the class-wise statistics."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  Results on larger model (left) and other task (right).",
        "table": "Pt0.A2.T4.1.1",
        "footnotes": [],
        "references": [
            "The first term of Eq.  4  is the weighted correlation of the extracted features of the dataset, and the second term is the class-wise mean of the extracted features. Under the condition of   ( 0 )   similar-to superscript  0  \\theta^{(0)}\\sim\\Theta italic_ start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT  roman_ , the constraining covariance is essentially equivalent with variance. Also, when classes in the dataset are balanced, the class-wise mean can be replaced by global mean. For detailed proofs, please refer to the supplementary materials.",
            "In this section, we present the visualization of ImageNet-1K synthetic datasets with IPC 50 setting generated by our method and SRe 2 L  [ 36 ] . Fig.  4  shows that our method generates high quality, more distinctive features and a more comprehensive range of variations data, showing more perspective of the original dataset. In comparison, images generated from SRe 2 L, while realistic, face the problem of mode collapse, especially for large IPCs.",
            "The first term of Eq.  14  is a weighted covariance matrix of the feature space, and the second term is the class-wise mean of the feature space. The gradient matching for  l 2 superscript l 2 l^{2} italic_l start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  distance can be transformed as:",
            "To further demonstrate the effectiveness of our proposed method for larger networks, we conduct the experiments on ResNet50. Here, ResNet50 is utilized as the model pool base architecture, and the downstream network. The results are shown in the Table  4  (left). Our method achieves superior performance for larger models.",
            "Current DD field mainly focuses on classification, with detection and other tasks research still blank. Here, we simply apply our method on the detection task to show the generalizability of our proposed method across different tasks. We conduct the experiments on Pascal VOC, and the architecture of the detector is Faster RCNN. Table  4  (right) shows the preliminary results of our method and baseline adapted to object detection on Pascal VOC, demonstrating the potential of our method in wider applications."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 :  Results on small datasets CIFAR-10 and CIFAR-100. For a fair comparison, we drop the soft label adopted in both SRe 2 L and our proposed method and only use DSA as the data augmentation strategy.",
        "table": "Pt0.A4.T5.38.38",
        "footnotes": [],
        "references": [
            "Considering that we have already improved the efficiency by Eq.  5 , it is still time demanding since we need to re-train the new model for each synthetic data updating. It will become significantly pronounced when handling the large-scale datasets.",
            "From Eq.  15 , it shows that the upper bound of the gradient matching is the first-order and the second-order statistic information matching in feature space. Here, we need to consider the prior condition such that   ( 0 )   similar-to superscript  0  \\theta^{(0)}\\sim\\Theta italic_ start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT  roman_ . It means that for every   ( 0 ) superscript  0 \\theta^{(0)} italic_ start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT  samples from the distribution    \\Theta roman_ , Eq.  15  reaches the minimum value. In this case, the first term of Eq.  15 , the covariance for the original data and synthetic data, is equivalent for all   ( 0 ) superscript  0 \\theta^{(0)} italic_ start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT  samples, and the class-wise mean. Also, the equality in Eq.  15  holds. As for   ( t ) superscript  t \\theta^{(t)} italic_ start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT , as long as the samples are sufficient, the condition still holds. To be more specific, here we assume  f   ( X t )  R N t  f d subscript f  subscript X t superscript R subscript N t subscript f d f_{\\theta}(X_{t})\\in\\mathbb{R}^{N_{t}\\times f_{d}} italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  italic_f start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_POSTSUPERSCRIPT , and  f   ( X s )  R N s  f d subscript f  subscript X s superscript R subscript N s subscript f d f_{\\theta}(X_{s})\\in\\mathbb{R}^{N_{s}\\times f_{d}} italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_X start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT )  blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  italic_f start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ,  f d subscript f d f_{d} italic_f start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT  is the dimension of the feature.  f   ( X t ) = [ f   ( x 1 t ) T , ... , f   ( x N t t ) T ] T subscript f  subscript X t superscript subscript f  superscript subscript superscript x t 1 T ... subscript f  superscript subscript superscript x t subscript N t T T f_{\\theta}(X_{t})=[f_{\\theta}(x^{t}_{1})^{T},\\dots,f_{\\theta}(x^{t}_{N_{t}})^{% T}]^{T} italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = [ italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT , ... , italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT , and  f   ( x i t )  R f d subscript f  subscript superscript x t i superscript R subscript f d f_{\\theta}(x^{t}_{i})\\in\\mathbb{R}^{f_{d}} italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  blackboard_R start_POSTSUPERSCRIPT italic_f start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_POSTSUPERSCRIPT . Also,  f   ( X s ) = [ f   ( x 1 s ) T , ... , f   ( x N s s ) T ] T subscript f  subscript X s superscript subscript f  superscript subscript superscript x s 1 T ... subscript f  superscript subscript superscript x s subscript N s T T f_{\\theta}(X_{s})=[f_{\\theta}(x^{s}_{1})^{T},\\dots,f_{\\theta}(x^{s}_{N_{s}})^{% T}]^{T} italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_X start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) = [ italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_x start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT , ... , italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_x start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_N start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT , and  f   ( x i s )  R f d subscript f  subscript superscript x s i superscript R subscript f d f_{\\theta}(x^{s}_{i})\\in\\mathbb{R}^{f_{d}} italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_x start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  blackboard_R start_POSTSUPERSCRIPT italic_f start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_POSTSUPERSCRIPT .  f   ( X t ) = [ F 1 t , ... , F f d t ] subscript f  subscript X t subscript superscript F t 1 ... subscript superscript F t subscript f d f_{\\theta}(X_{t})=[F^{t}_{1},\\dots,F^{t}_{f_{d}}] italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = [ italic_F start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_F start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_POSTSUBSCRIPT ] ,  F i t  R N t subscript superscript F t i superscript R subscript N t F^{t}_{i}\\in\\mathbb{R}^{N_{t}} italic_F start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUPERSCRIPT , and  f   ( X s ) = [ F 1 s , ... , F f d s ] subscript f  subscript X s subscript superscript F s 1 ... subscript superscript F s subscript f d f_{\\theta}(X_{s})=[F^{s}_{1},\\dots,F^{s}_{f_{d}}] italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_X start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) = [ italic_F start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_F start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_POSTSUBSCRIPT ] ,  F i s  R N s subscript superscript F s i superscript R subscript N s F^{s}_{i}\\in\\mathbb{R}^{N_{s}} italic_F start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUPERSCRIPT . For the first term of Eq.  15 , we have:",
            "where  C  l i C subscript l i Cl_{i} italic_C italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is the set of samples belong to  i t  h superscript i t h i^{th} italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT  class. When the classes of the original dataset is balanced, or the number of samples in every class of the original dataset is same, the second term of Eq.  15  can be replaced by the global mean. Therefore, the Eq.  13  can be transformed as follows:",
            "We also conduct validation experiments under the setting of CIFAR10 IPC 5 with the original DD optimization objective and our Taylor-approximation version. The results are shown in Fig.  5 . We evaluate the difference of average losses, average accuracy per iteration, and peak accuracy during training. The results demonstrate that the bound is tight in practice, and indicate that errors introduced by our approximation are negligible and the overall training dynamics are comparable.",
            "Our proposed method, Teddy, is designed mainly for large-scale datasets as we do several Taylor-approximation strategies to improve the efficiency of the original solution to the DD definition, which may cause information loss. However, it still shows quite competitive performance compared with baselines SRe 2 L and DM on small datasets. Here, we adopt CIFAR-10 and CIFAR-100 for experiments. Specifically, we drop the soft label adopted in both SRe 2 L and our method for fair comparison. Also, we only use the DSA strategy for data augmentation. The evaluation results are shown in Table  5 . It shows that our weak-teacher strategy is a more favorable surrogate of the original ones than fully-converged models (SRe 2 L) and random initialization without training (DM)."
        ]
    },
    "id_table_6": {
        "caption": "Table 6 :  The hyper-parameters for model pool generation part.",
        "table": "Pt0.A4.T6.2.1",
        "footnotes": [],
        "references": [
            "Insight from Eq.  6 , to further improve the time efficiency, we construct the weak teacher model pool from two practical cases, the prior and post-generation. These strategies are both efficiently generating model pool from a single base model, as shown in Fig.  2(a)  and Fig.  2(b) . Here, as we utilize weak teachers instead of fully-convergence ones, we can just use a single model as the base to generate diverse weak teachers. This strategy does not compromise performance. More specifically, if the base model is:",
            "Eq.  16  shows that if the covariance matching holds, then variance matching also holds. To improve the calculation efficiency, here we adopt variance matching instead of covariance matching. As for the second term, it is the class-wise mean matching in feature space. Specifically, we have:",
            "In our experiments, we have two strategies to generate the model pool: prior-generation and post-generation. The base network architecture is ResNet18. For prior-generation, we follow the official torchvision code to train ResNet18 for ImageNet-1K and modified ResNet18 for Tiny-ImageNet. We adopt different stage teacher models for different IPC settings. The size of the model pool is 9 for ImageNet-1K and 8 for Tiny-ImageNet. For more details, please refer to Table  6 .",
            "As for post-generation, we utilize the pre-trained ResNet18 provided by the official torchvision for ImageNet-1K and well-trained modified ResNet18 for Tiny-ImageNet. We use DepGraph to perform structural pruning with the random standard, and finetune the pruned models for very limited epochs,  e.g. , 0-2 epochs, with learning rate from 0.1 to 0.001. It is worth noting that during finetuning, we simulate the accuracy curve of teacher models in trajectory-based model pool. The GFLOPs of the target pruned model is 1.2G, and the number of parameters is 7.72M. The size of model pool is 10 for ImageNet-1K and 9 for Tiny-ImageNet. For more details, please refer to Table  6 .",
            "Here, we provide additional visualizations of the synthetic dataset generated by our method, including different settings on Tiny-ImageNet (Fig.  6 ) and ImageNet-1K (Fig.  7 ). From a visualization perspective, our method showcases a broader range of perspectives and diversity. Here, for instance, in the whistle category, it can be observed that the focal points of the generated model pool by different methods are distinct. Post-generation focuses on the object itself, while prior-generation also pays attention to the surrounding environment,  e.g. , the animal blowing the whistle."
        ]
    },
    "id_table_7": {
        "caption": "Table 7 :  The hyper-parameters for data generation part.",
        "table": "Pt0.A4.T7.2.1",
        "footnotes": [],
        "references": [
            "In summary, we propose an efficient large-scale dataset distillation method, Teddy, solving the core bi-level optimization problem and further improving the efficiency via Taylor approximation. The framework of Teddy is shown in Algorithm  1 . We first generate model pool  M M \\mathcal{M} caligraphic_M  from single base model   b  a  s  e subscript  b a s e \\theta_{base} italic_ start_POSTSUBSCRIPT italic_b italic_a italic_s italic_e end_POSTSUBSCRIPT . According to the state of the   b  a  s  e subscript  b a s e \\theta_{base} italic_ start_POSTSUBSCRIPT italic_b italic_a italic_s italic_e end_POSTSUBSCRIPT , we choose from two efficient model pool generation strategies, prior and post-generation. Then, we follow our proposed optimization objective, Eq.  7 , to update the synthetic dataset, which only requires the first-order optimization process. Here, considering the weak teacher models may mislead the data generation direction, we generate soft labels via weak teacher ensemble, which provides more richer label information expression.",
            "Here,  M c t  R N t  c superscript subscript M c t superscript R subscript N t c \\mathcal{M}_{c}^{t}\\in\\mathbb{R}^{N_{t}\\times c} caligraphic_M start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  italic_c end_POSTSUPERSCRIPT  and  M c s  R N s  c superscript subscript M c s superscript R subscript N s c \\mathcal{M}_{c}^{s}\\in\\mathbb{R}^{N_{s}\\times c} caligraphic_M start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  italic_c end_POSTSUPERSCRIPT , are the matrices to indicate whether the samples belong to the classes. For instance,  M i  j = 1 subscript M i j 1 M_{ij}=1 italic_M start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = 1  means the  i t  h superscript i t h i^{th} italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT  sample is belong to the  j t  h superscript j t h j^{th} italic_j start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT  class, otherwise, it is not. The Eq.  17  can be:",
            "For ImageNet-1K, we randomly select 3 models from the prior-generated model pool or 4 models from the post-generated model pool to generate the  i t  h superscript i t h i^{th} italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT  image for all classes. As for Tiny-ImageNet, we randomly select 3 models from prior-generated model pool or 5 models from the post-generated model pool. The batch size is 100, and the number of synthetic data generation iterations is 6000. For more details, please refer to Table  7 .",
            "Here, we provide additional visualizations of the synthetic dataset generated by our method, including different settings on Tiny-ImageNet (Fig.  6 ) and ImageNet-1K (Fig.  7 ). From a visualization perspective, our method showcases a broader range of perspectives and diversity. Here, for instance, in the whistle category, it can be observed that the focal points of the generated model pool by different methods are distinct. Post-generation focuses on the object itself, while prior-generation also pays attention to the surrounding environment,  e.g. , the animal blowing the whistle."
        ]
    }
}