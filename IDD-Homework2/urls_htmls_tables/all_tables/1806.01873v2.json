{
    "S4.T1": {
        "caption": "Table 1: Comparison of different methods on MemexQA by question type. The first three methods do not use the attention mechanism.",
        "table": "<table id=\"S4.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_rr ltx_border_t\">Method</th>\n<th id=\"S4.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">how many</th>\n<th id=\"S4.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">what</th>\n<th id=\"S4.T1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">when</th>\n<th id=\"S4.T1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">where</th>\n<th id=\"S4.T1.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">who</th>\n<th id=\"S4.T1.1.1.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">overall</th>\n</tr>\n<tr id=\"S4.T1.1.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.2.2.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_rr\"></th>\n<th id=\"S4.T1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">(11.8%)</th>\n<th id=\"S4.T1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">(41.9%)</th>\n<th id=\"S4.T1.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">(16.2%)</th>\n<th id=\"S4.T1.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">(17.2%)</th>\n<th id=\"S4.T1.1.2.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">(12.9%)</th>\n<th id=\"S4.T1.1.2.2.7\" class=\"ltx_td ltx_th ltx_th_column\"></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.1.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr ltx_border_tt\">Logistic Regression</th>\n<td id=\"S4.T1.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">0.645</td>\n<td id=\"S4.T1.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">0.241</td>\n<td id=\"S4.T1.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">0.217</td>\n<td id=\"S4.T1.1.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">0.277</td>\n<td id=\"S4.T1.1.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">0.260</td>\n<td id=\"S4.T1.1.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_tt\">0.295</td>\n</tr>\n<tr id=\"S4.T1.1.4.2\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">Embedding + LSTM</th>\n<td id=\"S4.T1.1.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.771</td>\n<td id=\"S4.T1.1.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">0.564</td>\n<td id=\"S4.T1.1.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0.349</td>\n<td id=\"S4.T1.1.4.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\">0.314</td>\n<td id=\"S4.T1.1.4.2.6\" class=\"ltx_td ltx_align_center ltx_border_r\">0.310</td>\n<td id=\"S4.T1.1.4.2.7\" class=\"ltx_td ltx_align_center\">0.478</td>\n</tr>\n<tr id=\"S4.T1.1.5.3\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">Embedding + LSTM + Concat</th>\n<td id=\"S4.T1.1.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.776</td>\n<td id=\"S4.T1.1.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">0.668</td>\n<td id=\"S4.T1.1.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0.398</td>\n<td id=\"S4.T1.1.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_r\">0.433</td>\n<td id=\"S4.T1.1.5.3.6\" class=\"ltx_td ltx_align_center ltx_border_r\">0.409</td>\n<td id=\"S4.T1.1.5.3.7\" class=\"ltx_td ltx_align_center\">0.563</td>\n</tr>\n<tr id=\"S4.T1.1.6.4\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.6.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr ltx_border_t\">DMN+ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">25</a>]</cite>\n</th>\n<td id=\"S4.T1.1.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.792</td>\n<td id=\"S4.T1.1.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.616</td>\n<td id=\"S4.T1.1.6.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.346</td>\n<td id=\"S4.T1.1.6.4.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.248</td>\n<td id=\"S4.T1.1.6.4.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.224</td>\n<td id=\"S4.T1.1.6.4.7\" class=\"ltx_td ltx_align_center ltx_border_t\">0.480</td>\n</tr>\n<tr id=\"S4.T1.1.7.5\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.7.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">Multimodal Compact Bilinear Pooling <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib5\" title=\"\" class=\"ltx_ref\">5</a>]</cite>\n</th>\n<td id=\"S4.T1.1.7.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.773</td>\n<td id=\"S4.T1.1.7.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\">0.618</td>\n<td id=\"S4.T1.1.7.5.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0.250</td>\n<td id=\"S4.T1.1.7.5.5\" class=\"ltx_td ltx_align_center ltx_border_r\">0.229</td>\n<td id=\"S4.T1.1.7.5.6\" class=\"ltx_td ltx_align_center ltx_border_r\">0.248</td>\n<td id=\"S4.T1.1.7.5.7\" class=\"ltx_td ltx_align_center\">0.462</td>\n</tr>\n<tr id=\"S4.T1.1.8.6\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.8.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">Bi-directional Attention Flow <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib19\" title=\"\" class=\"ltx_ref\">19</a>]</cite>\n</th>\n<td id=\"S4.T1.1.8.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.790</td>\n<td id=\"S4.T1.1.8.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\">0.689</td>\n<td id=\"S4.T1.1.8.6.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0.356</td>\n<td id=\"S4.T1.1.8.6.5\" class=\"ltx_td ltx_align_center ltx_border_r\">0.567</td>\n<td id=\"S4.T1.1.8.6.6\" class=\"ltx_td ltx_align_center ltx_border_r\">0.468</td>\n<td id=\"S4.T1.1.8.6.7\" class=\"ltx_td ltx_align_center\">0.598</td>\n</tr>\n<tr id=\"S4.T1.1.9.7\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.9.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">Soft Attention</th>\n<td id=\"S4.T1.1.9.7.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.1.9.7.2.1\" class=\"ltx_text ltx_font_bold\">0.795</span></td>\n<td id=\"S4.T1.1.9.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\">0.697</td>\n<td id=\"S4.T1.1.9.7.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0.346</td>\n<td id=\"S4.T1.1.9.7.5\" class=\"ltx_td ltx_align_center ltx_border_r\">0.604</td>\n<td id=\"S4.T1.1.9.7.6\" class=\"ltx_td ltx_align_center ltx_border_r\">0.582</td>\n<td id=\"S4.T1.1.9.7.7\" class=\"ltx_td ltx_align_center\">0.621</td>\n</tr>\n<tr id=\"S4.T1.1.10.8\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.10.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">TGIF Temporal Attention <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">7</a>]</cite>\n</th>\n<td id=\"S4.T1.1.10.8.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.761</td>\n<td id=\"S4.T1.1.10.8.3\" class=\"ltx_td ltx_align_center ltx_border_r\">0.700</td>\n<td id=\"S4.T1.1.10.8.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.1.10.8.4.1\" class=\"ltx_text ltx_font_bold\">0.522</span></td>\n<td id=\"S4.T1.1.10.8.5\" class=\"ltx_td ltx_align_center ltx_border_r\">0.582</td>\n<td id=\"S4.T1.1.10.8.6\" class=\"ltx_td ltx_align_center ltx_border_r\">0.477</td>\n<td id=\"S4.T1.1.10.8.7\" class=\"ltx_td ltx_align_center\">0.630</td>\n</tr>\n<tr id=\"S4.T1.1.11.9\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.11.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_rr\">FVTA</th>\n<td id=\"S4.T1.1.11.9.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">0.761</td>\n<td id=\"S4.T1.1.11.9.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T1.1.11.9.3.1\" class=\"ltx_text ltx_font_bold\">0.714</span></td>\n<td id=\"S4.T1.1.11.9.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">0.476</td>\n<td id=\"S4.T1.1.11.9.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T1.1.11.9.5.1\" class=\"ltx_text ltx_font_bold\">0.676</span></td>\n<td id=\"S4.T1.1.11.9.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T1.1.11.9.6.1\" class=\"ltx_text ltx_font_bold\">0.668</span></td>\n<td id=\"S4.T1.1.11.9.7\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S4.T1.1.11.9.7.1\" class=\"ltx_text ltx_font_bold\">0.669</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "TableÂ 1 compares the accuracy on the MemexQA. As we see, the proposed method consistently outperforms the baseline methods and achieves the state-of-the-art accuracy on this dataset. The first 3 methods in the table show the performance of embedding methods without any attentions. Although embedding methods are relatively simple to implement, their performance is much lower than the proposed FVTA model. The experiment results advocate the attention model among images and image sequences. Compare to previous attention models, our FVTA network significantly outperforms other methods, which proves the efficacy of the proposed method.",
            "The MemexQA dataset provides ground truth evidence photos for every question. We can compare the correlation between the photos of the highest attention weights and the ground truth photos to correctly answer a question. An ideal VQA model should not only enjoy a high accuracy in answering a question (TableÂ 1) but also can find images that are highly correlated to the ground-truth evidence photos. TableÂ 2 lists the accuracy to examine whether a model puts focus on the correct photos. FVTA outperforms other attention models on finding the relevant photos for the question. The results show that the proposed attention can capture salient information for answering the question. For qualitative comparison, we select some representative questions and show both the answer and the retrieved top images based on the attention weights in Fig.Â 4.\nAs shown in the first example, the system has to find the correct photo and visually identify the object to answer the question âwhat did the daughter eat while her dad was watching during the trip in June 2010?â. FVTA attention puts a high weight on the correct photo of the girl eating a corn, which leads to correctly answering the question. Whereas for soft attention, the one-dimensional attention network outputs the wrong image and gets the wrong answer.\nThis example shows the advantage of FVTA modeling the correlation at every time step, across visual-text sequences over the traditional dimensional attention."
        ]
    },
    "S5.T2": {
        "caption": "Table 2: The quality comparison of the learned FVTA and classic attention. We compare the image of the highest activation in a leaned attention to the ground truth evidence photos which human used to answer the question. HIT@1 means the rate of the top attended images being found in the ground truth evidence photos. AP is computed on the photo ranked by their attention activation.",
        "table": "<table id=\"S5.T2.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T2.4.5.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.4.5.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_rr ltx_border_t\"></th>\n<th id=\"S5.T2.4.5.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">HIT@1</th>\n<th id=\"S5.T2.4.5.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">HIT@3</th>\n<th id=\"S5.T2.4.5.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">mAP</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr ltx_border_tt\">Soft Attention</th>\n<td id=\"S5.T2.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">1.16%</td>\n<td id=\"S5.T2.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">12.60%</td>\n<td id=\"S5.T2.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\">0.168<math id=\"S5.T2.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S5.T2.1.1.1.m1.1a\"><mo id=\"S5.T2.1.1.1.m1.1.1\" xref=\"S5.T2.1.1.1.m1.1.1.cmml\">Â±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.1.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T2.1.1.1.m1.1.1.cmml\" xref=\"S5.T2.1.1.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.1.1.1.m1.1c\">\\pm</annotation></semantics></math>0.002</td>\n</tr>\n<tr id=\"S5.T2.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.2.2.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">MCB</th>\n<td id=\"S5.T2.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">11.98%</td>\n<td id=\"S5.T2.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\">30.54%</td>\n<td id=\"S5.T2.2.2.1\" class=\"ltx_td ltx_align_center\">0.269<math id=\"S5.T2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S5.T2.2.2.1.m1.1a\"><mo id=\"S5.T2.2.2.1.m1.1.1\" xref=\"S5.T2.2.2.1.m1.1.1.cmml\">Â±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.2.2.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T2.2.2.1.m1.1.1.cmml\" xref=\"S5.T2.2.2.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.2.2.1.m1.1c\">\\pm</annotation></semantics></math>0.005</td>\n</tr>\n<tr id=\"S5.T2.3.3\" class=\"ltx_tr\">\n<th id=\"S5.T2.3.3.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">TGIF Temporal</th>\n<td id=\"S5.T2.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">13.28%</td>\n<td id=\"S5.T2.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\">32.83%</td>\n<td id=\"S5.T2.3.3.1\" class=\"ltx_td ltx_align_center\">0.289<math id=\"S5.T2.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S5.T2.3.3.1.m1.1a\"><mo id=\"S5.T2.3.3.1.m1.1.1\" xref=\"S5.T2.3.3.1.m1.1.1.cmml\">Â±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.3.3.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T2.3.3.1.m1.1.1.cmml\" xref=\"S5.T2.3.3.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.3.3.1.m1.1c\">\\pm</annotation></semantics></math>0.005</td>\n</tr>\n<tr id=\"S5.T2.4.4\" class=\"ltx_tr\">\n<th id=\"S5.T2.4.4.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_rr\">FVTA</th>\n<td id=\"S5.T2.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">\n<span id=\"S5.T2.4.4.3.1\" class=\"ltx_text ltx_font_bold\">15.48</span>%</td>\n<td id=\"S5.T2.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">\n<span id=\"S5.T2.4.4.4.1\" class=\"ltx_text ltx_font_bold\">35.66</span>%</td>\n<td id=\"S5.T2.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T2.4.4.1.1\" class=\"ltx_text ltx_font_bold\">0.312<math id=\"S5.T2.4.4.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S5.T2.4.4.1.1.m1.1a\"><mo id=\"S5.T2.4.4.1.1.m1.1.1\" xref=\"S5.T2.4.4.1.1.m1.1.1.cmml\">Â±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.4.4.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T2.4.4.1.1.m1.1.1.cmml\" xref=\"S5.T2.4.4.1.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.4.4.1.1.m1.1c\">\\pm</annotation></semantics></math>0.005</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "The MemexQA dataset provides ground truth evidence photos for every question. We can compare the correlation between the photos of the highest attention weights and the ground truth photos to correctly answer a question. An ideal VQA model should not only enjoy a high accuracy in answering a question (TableÂ 1) but also can find images that are highly correlated to the ground-truth evidence photos. TableÂ 2 lists the accuracy to examine whether a model puts focus on the correct photos. FVTA outperforms other attention models on finding the relevant photos for the question. The results show that the proposed attention can capture salient information for answering the question. For qualitative comparison, we select some representative questions and show both the answer and the retrieved top images based on the attention weights in Fig.Â 4.\nAs shown in the first example, the system has to find the correct photo and visually identify the object to answer the question âwhat did the daughter eat while her dad was watching during the trip in June 2010?â. FVTA attention puts a high weight on the correct photo of the girl eating a corn, which leads to correctly answering the question. Whereas for soft attention, the one-dimensional attention network outputs the wrong image and gets the wrong answer.\nThis example shows the advantage of FVTA modeling the correlation at every time step, across visual-text sequences over the traditional dimensional attention."
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Ablation studies of the proposed FVTA method on the MemexQA dataset. The last column shows the performance drop.",
        "table": "<table id=\"S5.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T3.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_rr ltx_border_t\">Ablations</th>\n<th id=\"S5.T3.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Accuracy</th>\n<th id=\"S5.T3.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math id=\"S5.T3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\Delta\" display=\"inline\"><semantics id=\"S5.T3.1.1.1.m1.1a\"><mi mathvariant=\"normal\" id=\"S5.T3.1.1.1.m1.1.1\" xref=\"S5.T3.1.1.1.m1.1.1.cmml\">Î</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.1.1.1.m1.1b\"><ci id=\"S5.T3.1.1.1.m1.1.1.cmml\" xref=\"S5.T3.1.1.1.m1.1.1\">Î</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.1.1.1.m1.1c\">\\Delta</annotation></semantics></math></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T3.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr ltx_border_tt\">FVTA w/ Cosine Similarity</th>\n<td id=\"S5.T3.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">0.619</td>\n<td id=\"S5.T3.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">-4.9%</td>\n</tr>\n<tr id=\"S5.T3.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">FVTA w/o Intra-seq</th>\n<td id=\"S5.T3.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.569</td>\n<td id=\"S5.T3.1.3.2.3\" class=\"ltx_td ltx_align_center\">-10.0%</td>\n</tr>\n<tr id=\"S5.T3.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">FVTA w/o Cross-seq</th>\n<td id=\"S5.T3.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.604</td>\n<td id=\"S5.T3.1.4.3.3\" class=\"ltx_td ltx_align_center\">-6.5%</td>\n</tr>\n<tr id=\"S5.T3.1.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">FVTA w/o Question Attention</th>\n<td id=\"S5.T3.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.629</td>\n<td id=\"S5.T3.1.5.4.3\" class=\"ltx_td ltx_align_center\">-4.0%</td>\n</tr>\n<tr id=\"S5.T3.1.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_rr\">FVTA w/o Photos</th>\n<td id=\"S5.T3.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">0.577</td>\n<td id=\"S5.T3.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_b\">-9.1%</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Table 3 shows the performance of FVTA mechanism and its ablations on the MemexQA dataset.\nTo evaluate the FVTA attention mechanism, we first replace our kernel tensor with simple cosine similarity function. Results show that standard cosine similarity is inferior to our similarity function.\nFor ablating intra-sequence dependency, we use the representations from the last timestep of each context document. For ablating cross sequence interaction, we average all attended context representation from different modalities to get the final context vector.\nBoth aspects of correlation of the FVTA attention tensor contribute towards the modelâs performance, while intra-sequence dependency shows more importance in this experiment. We compare the effectiveness of context-aware question attention by removing the question attention and use the last timestep of the LSTM output from the question as the question representation. It shows the question attention provides slight improvement. Finally, we train FVTA without photos to see the contribution of visual information. The result is quite good but it is perhaps not surprising due to the language bias in the questions and answers of the dataset, which is not uncommon in VQA datasetÂ [2] and in Visual7WÂ [31]. This also leaves significant rooms of improvement with visual information."
        ]
    },
    "S5.T4": {
        "caption": "Table 4: Accuracy comparison on the test and the validation set of the MovieQA dataset. The test set performance can only be evaluated on the MovieQA server, and thus not all the studies provide the accuracy on Test set.",
        "table": "<table id=\"S5.T4.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_rr ltx_border_t\">Method</th>\n<th id=\"S5.T4.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Val</th>\n<th id=\"S5.T4.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Test</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T4.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr ltx_border_tt\">SSCB <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">22</a>]</cite>\n</th>\n<td id=\"S5.T4.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">0.219</td>\n<td id=\"S5.T4.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">-</td>\n</tr>\n<tr id=\"S5.T4.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">MemN2N <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">22</a>]</cite>\n</th>\n<td id=\"S5.T4.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.342</td>\n<td id=\"S5.T4.1.3.2.3\" class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr id=\"S5.T4.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">DEMN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">10</a>]</cite>\n</th>\n<td id=\"S5.T4.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">-</td>\n<td id=\"S5.T4.1.4.3.3\" class=\"ltx_td ltx_align_center\">0.300</td>\n</tr>\n<tr id=\"S5.T4.1.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">Soft Attention</th>\n<td id=\"S5.T4.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.321</td>\n<td id=\"S5.T4.1.5.4.3\" class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr id=\"S5.T4.1.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">MCB <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib5\" title=\"\" class=\"ltx_ref\">5</a>]</cite>\n</th>\n<td id=\"S5.T4.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.362</td>\n<td id=\"S5.T4.1.6.5.3\" class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr id=\"S5.T4.1.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">TGIF Temporal <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">7</a>]</cite>\n</th>\n<td id=\"S5.T4.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.371</td>\n<td id=\"S5.T4.1.7.6.3\" class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr id=\"S5.T4.1.8.7\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.8.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">RWMN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib15\" title=\"\" class=\"ltx_ref\">15</a>]</cite>\n</th>\n<td id=\"S5.T4.1.8.7.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.387</td>\n<td id=\"S5.T4.1.8.7.3\" class=\"ltx_td ltx_align_center\">0.363</td>\n</tr>\n<tr id=\"S5.T4.1.9.8\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.9.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_rr\">FVTA</th>\n<td id=\"S5.T4.1.9.8.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S5.T4.1.9.8.2.1\" class=\"ltx_text ltx_font_bold\">0.410</span></td>\n<td id=\"S5.T4.1.9.8.3\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T4.1.9.8.3.1\" class=\"ltx_text ltx_font_bold\">0.373</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Experimental Results\nWe compare FVTA with recent results on MovieQA dataset, including End-to-End Memory Network (MemN2N) [23], Deep Embedded Memory Network (DEMN) [10], and Read-Write Memory Network (RWMN) [15].\nTableÂ 4 shows the detailed comparison of MovieQA results using both videos and subtitles. FVTA model outperforms all baseline methods and achieves comparable performance to the state-of-the-art result 222The best test accuracy on the leaderboard by the time of paper submission (Nov. 2017) is 0.39 (Layered Memory Networks). It is not included in the table as there is no publication to cite. on the MovieQA test server. Notably, RWMN [15] is a very recent work that uses memory net to cache sequential input, with a high capacity and flexibility due to the read and write networks. Our accuracy is 0.410 (vs 0.387 by RWMN) on the validation set and 0.373 (vs 0.363) on the test set. Benefiting from such modeling ability,\nFVTA consistently outperforms the classical attention models including soft attention, MCB [5] and TGIF [7]. The result demonstrates the consistent advantages of FVTA over other attention models in question-answering for multiple sequence data."
        ]
    }
}