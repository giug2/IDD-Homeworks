{
    "id_table_1": {
        "caption": "Table 1.  Objective evaluation of item recommendation. The boldface indicates the best result. Significant improvements over best baseline results are marked with * (t-test,  p < 0.05 ) p\\textless 0.05) italic_p < 0.05 ) .",
        "table": "S5.T1.7.5",
        "footnotes": [],
        "references": [
            "Integrating empathy into recommendation and generation.   We analyze the need to integrate empathy into item recommendation and response generation subtasks, respectively. For item recommendation, existing approaches often assume that all entities mentioned in dialogues reflect user preferences and that all items suggested by recommenders meet user expectations.  This hypothesis disregards subtle cues of user emotions expressed in natural language for modeling user preferences.  As illustrated in Figure  1 , a conventional CRS might infer that the user likes Shakespeare and A Midsummer Nights Dream mentioned by the recommender, while overlooking that the user expresses negative emotions towards them during the dialogue. Thus such systems recommend the wrong item Romeo and Juliet.  For response generation, existing methods are trained on the standard responses from datasets, which tend to be short and lack narratives, often resulting in inconsistencies or a lack of emotional engagement.  As shown in Figure  1  (bottom), the conventional CRSs response only contains the item name, which may diminish user satisfaction when interacting with the system. In contrast, based on capturing and expressing emotions, an empathetic CRS recommends a reasonable item with a persuasive response.",
            "Notation.  Given  t  1 t 1 t-1 italic_t - 1  dialogue turns, the dialogue history  D t  1 subscript D t 1 D_{t-1} italic_D start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT  consists of a sequence of utterances from both recommenders and users, i.e.,  D t  1 = { u k r , u k u } k = 1 t  1 subscript D t 1 subscript superscript superscript subscript u k r superscript subscript u k u t 1 k 1 D_{t-1}=\\{u_{k}^{r},u_{k}^{u}\\}^{t-1}_{k=1} italic_D start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT = { italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT , italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT } start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT , where each utterance  u k  = { w j } j = 1 | u k  | superscript subscript u k subscript superscript subscript w j superscript subscript u k j 1 u_{k}^{*}=\\{w_{j}\\}^{|u_{k}^{*}|}_{j=1} italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = { italic_w start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUPERSCRIPT | italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT | end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT  is composed of a sequence of words.  For simplicity, we concatenate all utterances from  D t  1 subscript D t 1 D_{t-1} italic_D start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT  into a single word sequence  D = { w q } q = 1 n w D subscript superscript subscript w q subscript n w q 1 D=\\{w_{q}\\}^{n_{w}}_{q=1} italic_D = { italic_w start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT } start_POSTSUPERSCRIPT italic_n start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_q = 1 end_POSTSUBSCRIPT , where  n w subscript n w n_{w} italic_n start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT  represents the total number of words in  D t  1 subscript D t 1 D_{t-1} italic_D start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT .  To incorporate knowledge about entities mentioned in the dialogue, we set an external knowledge graph (e.g., DBpedia  (Bizer et al . ,  2009 ) ) as  G = ( E , L ) G E L \\mathcal{G}=(\\mathcal{E},\\mathcal{L}) caligraphic_G = ( caligraphic_E , caligraphic_L ) , consisting of triples  T =  e h , l , e t  T subscript e h l subscript e t \\mathcal{T}=\\left\\langle e_{h},l,e_{t}\\right\\rangle caligraphic_T =  italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_l , italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  , where  e h  E subscript e h E e_{h}\\in\\mathcal{E} italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT  caligraphic_E  and  e t  E subscript e t E e_{t}\\in\\mathcal{E} italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  caligraphic_E  are the head and tail entities,  l  L l L l\\in\\mathcal{L} italic_l  caligraphic_L  reflects the relation between  e h subscript e h e_{h} italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT  and  e t subscript e t e_{t} italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .  E E \\mathcal{E} caligraphic_E  and  L L \\mathcal{L} caligraphic_L  denote the sets of entities and relations. We define  I I I italic_I  as the entire set of items, all of which are included in the entities of  G G \\mathcal{G} caligraphic_G , i.e.,  I  E I E I\\in\\mathcal{E} italic_I  caligraphic_E . Entities in each utterance  u k  superscript subscript u k u_{k}^{*} italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  are identified as  E k  = { e j } j = 1 | E k  | superscript subscript E k subscript superscript subscript e j superscript subscript E k j 1 E_{k}^{*}=\\{e_{j}\\}^{|E_{k}^{*}|}_{j=1} italic_E start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = { italic_e start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUPERSCRIPT | italic_E start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT | end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT . Each item  i j subscript i j i_{j} italic_i start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  within  E k  superscript subscript E k E_{k}^{*} italic_E start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is linked with user feedback  f i j subscript f subscript i j f_{i_{j}} italic_f start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT , indicating whether the user likes it.  Similarly, we combined all entities mentioned in  D t  1 subscript D t 1 D_{t-1} italic_D start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT  into an entity list  E l = { e q } q = 1 n e subscript E l subscript superscript subscript e q subscript n e q 1 E_{l}=\\{e_{q}\\}^{n_{e}}_{q=1} italic_E start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT = { italic_e start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT } start_POSTSUPERSCRIPT italic_n start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_q = 1 end_POSTSUBSCRIPT , where  n e subscript n e n_{e} italic_n start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT  is the count of entities in the dialogue history. Here, we refer to the entities mentioned in the dialogue history as  local entities . Correspondingly, we refer to entities co-occurring with the local entities in the training dataset as  global entities , which will be detailed in Section  4.2.1 .",
            "In this section, we introduce our empathetic data enlargement process (Section  4.1 ) and two key modules of ECR: emotion-aware item recommendation (Section  4.2 ) and emotion-aligned response generation (Section  4.3 ). Figure  2  shows an overview of ECR.",
            "Existing datasets lack explicit supervisory signals for identifying user emotions.  To address the problem, we employ GPT-3.5-turbo  (Zoph et al . ,  2022 )  to initially annotate user emotions in 5,082 utterances from the ReDial dataset.  We limit the number of annotated emotions per utterance to a maximum of two labels. In annotating with GPT-3.5-turbo  (Zoph et al . ,  2022 )  for utterance-level user emotions, we adopted nine emotion labels: like, curious, happy, grateful, negative, neutral, nostalgia, agreement, and surprise.  The negative label, denoting adverse emotions, accounted for 8.0%. See Appendix   A.1  for a more detailed annotation process.  Based on the annotations, we fine-tune a GPT-2 model, which achieves 87.75% in terms of Recall@2 in categorizing emotions.  We applied this model to annotate emotions for each user utterance  u k u superscript subscript u k u u_{k}^{u} italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT  in the ReDial dataset and set a threshold    \\beta italic_  to retain relevant emotion label  f f f italic_f .  For each utterance  u k u superscript subscript u k u u_{k}^{u} italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT , we obtain its utterance-level user emotions  F u k u = { f j } j = 1 | F u k u | subscript F superscript subscript u k u superscript subscript subscript f j j 1 subscript F superscript subscript u k u \\mathcal{F}_{u_{k}^{u}}=\\{f_{j}\\}_{j=1}^{|\\mathcal{F}_{u_{k}^{u}}|} caligraphic_F start_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT end_POSTSUBSCRIPT = { italic_f start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | caligraphic_F start_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT end_POSTSUBSCRIPT | end_POSTSUPERSCRIPT , along with the probabilities associated with each emotional label, denoted as  P u k u = { p j } j = 1 | P u k u | subscript P superscript subscript u k u superscript subscript subscript p j j 1 subscript P superscript subscript u k u \\mathcal{P}_{u_{k}^{u}}=\\{p_{j}\\}_{j=1}^{|\\mathcal{P}_{u_{k}^{u}}|} caligraphic_P start_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT end_POSTSUBSCRIPT = { italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | caligraphic_P start_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT end_POSTSUBSCRIPT | end_POSTSUPERSCRIPT , where  | F u k u | = | P u k u | subscript F superscript subscript u k u subscript P superscript subscript u k u |\\mathcal{F}_{u_{k}^{u}}|=|\\mathcal{P}_{u_{k}^{u}}| | caligraphic_F start_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT end_POSTSUBSCRIPT | = | caligraphic_P start_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT end_POSTSUBSCRIPT | .",
            "We address RQ1 by evaluating the performance of item recommendation; see Table  1 .  KGSF and RevCore, introducing external knowledge in CRSs, have demonstrated superior performance compared to KBRD, underscoring the significance of external knowledge in recommendations.  UCCR also performs well on RT@50 and R@50 by extracting user-centric data from cross-session interactions.  UniCRS, which integrates PLM into CRSs, exhibits the best performance among all baselines on RT@ n n n italic_n  and R@ n n n italic_n .  Regarding the AUC, a metric previously overlooked but essential for evaluating a models full alignment with users needs, we find that all baselines exhibit poor performance, with AUC values approaching  0.5 0.5 0.5 0.5 . This finding highlights the considerable challenge faced by CRSs in distinguishing between items receiving positive and negative feedback.",
            "In this section, we show two more cases extracted from the ReDial dataset. Tables  11  and   10  display the generation performance of all models. These tables revealed that a typical CRS, i.e., UniCRS, limited by the quality of the training data, generates a minimal number of emotional words, such as good, which results in weak emotional intensity and hardly creates an emotional connection with the users. Meanwhile, they struggle to generate movie-related information effectively, potentially degrading the user experience. Additionally, We find that GPT-3.5-turbo-instruct exhibits weaker performance in capturing and utilizing contextual keywords compared to GPT-3.5-turbo.",
            "To assess the effectiveness of our proposed metrics in measuring user satisfaction, we invited three human annotators to evaluate user satisfaction (Sat) with ten levels (0-9). These annotators played the role of users in the dialogue. We then asked them to label their satisfaction with responses from each model. The evaluation results are displayed in Table  12 . We also kept other subjective evaluation metrics in the Tabel for comparison. It can be observed that ECR significantly outperforms all baselines in terms of user satisfaction. Meanwhile, the score distributions across different metrics are similar, which confirms the effectiveness of the evaluation system we designed in reflecting user satisfaction for response generation.",
            "Similarly to section 5.6 of the main paper, we further calculate the Cohen kappa between user satisfaction and the five subjective metrics we designed, with results presented in Table  13 .  We find a low kappa score between emotion intensity and user satisfaction, suggesting that excessive emotional expression may offend users, whereas only appropriate emotional intensity contributes to user satisfaction. Additionally, logic persuasiveness shows a substantial consistency with user satisfaction, while emotional persuasiveness, informativeness, and lifelikeness achieve near-perfect consistency, particularly lifelikeness, which correlates highly with user satisfaction. These findings provide further evidence of the effectiveness of our evaluation system. Meanwhile, they reinforce our hypothesis that adopting emotion-rich and human-like expressions in response enhances user experience and satisfaction.",
            "We also evaluated with GPT-4 as an LLM-based scorer to enhance the robustness of employing LLM-based scorers. We sampled 200 examples for GPT-4 and used text-davinci-003 as the baseline rather than GPT-3.5-turbo-instruct. The subjective evaluation results of response generation, presented in Table   14 , demonstrate that GPT-4s scoring results closely mirror that of GPT-4-turbo. ECR significantly outperforms the baselines."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.  Subjective evaluation of LLM-based scorer (GPT-4-turbo) and human annotators for response generation.  The boldface indicates the best result. Significant improvements over best baseline results are marked with * (t-test,  p < 0.05 ) p\\textless 0.05) italic_p < 0.05 ) .",
        "table": "S5.T2.12.10",
        "footnotes": [],
        "references": [
            "Notation.  Given  t  1 t 1 t-1 italic_t - 1  dialogue turns, the dialogue history  D t  1 subscript D t 1 D_{t-1} italic_D start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT  consists of a sequence of utterances from both recommenders and users, i.e.,  D t  1 = { u k r , u k u } k = 1 t  1 subscript D t 1 subscript superscript superscript subscript u k r superscript subscript u k u t 1 k 1 D_{t-1}=\\{u_{k}^{r},u_{k}^{u}\\}^{t-1}_{k=1} italic_D start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT = { italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT , italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT } start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT , where each utterance  u k  = { w j } j = 1 | u k  | superscript subscript u k subscript superscript subscript w j superscript subscript u k j 1 u_{k}^{*}=\\{w_{j}\\}^{|u_{k}^{*}|}_{j=1} italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = { italic_w start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUPERSCRIPT | italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT | end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT  is composed of a sequence of words.  For simplicity, we concatenate all utterances from  D t  1 subscript D t 1 D_{t-1} italic_D start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT  into a single word sequence  D = { w q } q = 1 n w D subscript superscript subscript w q subscript n w q 1 D=\\{w_{q}\\}^{n_{w}}_{q=1} italic_D = { italic_w start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT } start_POSTSUPERSCRIPT italic_n start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_q = 1 end_POSTSUBSCRIPT , where  n w subscript n w n_{w} italic_n start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT  represents the total number of words in  D t  1 subscript D t 1 D_{t-1} italic_D start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT .  To incorporate knowledge about entities mentioned in the dialogue, we set an external knowledge graph (e.g., DBpedia  (Bizer et al . ,  2009 ) ) as  G = ( E , L ) G E L \\mathcal{G}=(\\mathcal{E},\\mathcal{L}) caligraphic_G = ( caligraphic_E , caligraphic_L ) , consisting of triples  T =  e h , l , e t  T subscript e h l subscript e t \\mathcal{T}=\\left\\langle e_{h},l,e_{t}\\right\\rangle caligraphic_T =  italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_l , italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  , where  e h  E subscript e h E e_{h}\\in\\mathcal{E} italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT  caligraphic_E  and  e t  E subscript e t E e_{t}\\in\\mathcal{E} italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  caligraphic_E  are the head and tail entities,  l  L l L l\\in\\mathcal{L} italic_l  caligraphic_L  reflects the relation between  e h subscript e h e_{h} italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT  and  e t subscript e t e_{t} italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .  E E \\mathcal{E} caligraphic_E  and  L L \\mathcal{L} caligraphic_L  denote the sets of entities and relations. We define  I I I italic_I  as the entire set of items, all of which are included in the entities of  G G \\mathcal{G} caligraphic_G , i.e.,  I  E I E I\\in\\mathcal{E} italic_I  caligraphic_E . Entities in each utterance  u k  superscript subscript u k u_{k}^{*} italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  are identified as  E k  = { e j } j = 1 | E k  | superscript subscript E k subscript superscript subscript e j superscript subscript E k j 1 E_{k}^{*}=\\{e_{j}\\}^{|E_{k}^{*}|}_{j=1} italic_E start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = { italic_e start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUPERSCRIPT | italic_E start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT | end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT . Each item  i j subscript i j i_{j} italic_i start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  within  E k  superscript subscript E k E_{k}^{*} italic_E start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is linked with user feedback  f i j subscript f subscript i j f_{i_{j}} italic_f start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT , indicating whether the user likes it.  Similarly, we combined all entities mentioned in  D t  1 subscript D t 1 D_{t-1} italic_D start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT  into an entity list  E l = { e q } q = 1 n e subscript E l subscript superscript subscript e q subscript n e q 1 E_{l}=\\{e_{q}\\}^{n_{e}}_{q=1} italic_E start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT = { italic_e start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT } start_POSTSUPERSCRIPT italic_n start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_q = 1 end_POSTSUBSCRIPT , where  n e subscript n e n_{e} italic_n start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT  is the count of entities in the dialogue history. Here, we refer to the entities mentioned in the dialogue history as  local entities . Correspondingly, we refer to entities co-occurring with the local entities in the training dataset as  global entities , which will be detailed in Section  4.2.1 .",
            "In this section, we introduce our empathetic data enlargement process (Section  4.1 ) and two key modules of ECR: emotion-aware item recommendation (Section  4.2 ) and emotion-aligned response generation (Section  4.3 ). Figure  2  shows an overview of ECR.",
            "Emotion-aware recommendation prompt.  To comprehensively model user preferences with their emotions, we use the local emotion-aware entity representation matrix  E ~ l  superscript subscript bold-~ E l bold- \\bm{\\tilde{E}_{l}^{\\prime}} overbold_~ start_ARG bold_italic_E end_ARG start_POSTSUBSCRIPT bold_italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_ end_POSTSUPERSCRIPT  and global emotion-aware entity representation matrix  E g  superscript subscript E g bold- \\bm{E_{g}^{\\prime}} bold_italic_E start_POSTSUBSCRIPT bold_italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_ end_POSTSUPERSCRIPT  to update the prompt in Eq.  2 . So we formulate an emotion-aware recommendation prompt as:",
            "The ReDial dataset  (Li et al . ,  2018 )  is a large-scale CRS dataset, carefully curated by human workers  (Gao et al . ,  2021 ) . Consequently, it effectively reflects real-world CR scenarios and fully validates the effectiveness of our method. Considering the significant cost of emotion annotations and evaluations in the generation subtask, we use the ReDial dataset for experiments and plan to extend ECR to other datasets in future work. The ReDial dataset is composed of two-party dialogues between a user and a recommender in the movie domain.  It contains 10,006 conversations consisting of 182,150 utterances related to 51,699 movies.  The user feedback towards items recommended in the dataset includes three categories: like, dislike, and not say.  Previous works  (Wang et al . ,  2022 ; Zhou et al . ,  2022 ; Ma et al . ,  2021 )  simply treat all the recommended items as positive labels. However, according to  Li et al .  ( 2018 ) , the dislike and not say labels are distributed separately at 4.9% and 14%, indicating the previous works introduce a large number of incorrect item labels. In contrast, we distinguish between those items with different user feedback. For emotional responses construction, we filter 34,953 reviews related to 4,092 movies for DialoGPT, and 2,459 reviews related to 1,553 movies for Llama 2-7B-Chat. The filtering process is detailed in Appendix   A.2 .  Following  (Chen et al . ,  2019 ) , we extract entities mentioned in each utterance and review from DBpedia.",
            "To analyze whether ECR is capable of expressing emotions for better user satisfaction, we conduct a comparison of response generation, evaluated by both LLM-based scorer (GPT-4-Turbo) and human annotators, as shown in Table  2 .  We observe that the evaluation results from the LLM-based scorer and human annotators are essentially consistent. In comparison to all baseline models, we discovered that LLMs in the zero-shot setting significantly outperform UniCRS, which is fine-tuned on the entire ReDial dataset. This indicates the subpar quality of the datasets standard responses.",
            "User satisfaction.   To confirm ECRs capability in improving user satisfaction by expressing emotions, we direct the human annotators to rate user satisfaction. Our findings indicate that the proposed emotion-enhanced evaluation metrics effectively reflect user satisfaction, especially lifelikeness, which shows a high correlation with user satisfaction. This evidence confirms that by adopting emotion-rich and human-like expressions, ECR significantly improves user experience and satisfaction. See Appendix   D.2  for more details.",
            "To assess the effectiveness of our proposed metrics in measuring user satisfaction, we invited three human annotators to evaluate user satisfaction (Sat) with ten levels (0-9). These annotators played the role of users in the dialogue. We then asked them to label their satisfaction with responses from each model. The evaluation results are displayed in Table  12 . We also kept other subjective evaluation metrics in the Tabel for comparison. It can be observed that ECR significantly outperforms all baselines in terms of user satisfaction. Meanwhile, the score distributions across different metrics are similar, which confirms the effectiveness of the evaluation system we designed in reflecting user satisfaction for response generation."
        ]
    },
    "id_table_3": {
        "caption": "Table 3.    Results of ablation studies for item recommendation. The boldface indicates the best result. Significant improvements are marked with * (t-test,  p < 0.05 ) p\\textless 0.05) italic_p < 0.05 ) .",
        "table": "S6.T3.7",
        "footnotes": [],
        "references": [
            "In this section, we introduce our empathetic data enlargement process (Section  4.1 ) and two key modules of ECR: emotion-aware item recommendation (Section  4.2 ) and emotion-aligned response generation (Section  4.3 ). Figure  2  shows an overview of ECR.",
            "ECR has a set of components to improve the performance. To verify their effectiveness, we conduct an ablation study and report the results in Table   3 .  We considered three variants:        (i)   ECR [ L ] delimited-[] L \\left[\\text{L}\\right] [ L ]  retains only the local emotion-aware entity representation;      (ii)   ECR [ LS ] delimited-[] LS \\left[\\text{LS}\\right] [ LS ]  includes the local emotion-aware entity representation and the feedback-aware item reweighting strategy; and      (iii)   ECR [ LG ] delimited-[] LG \\left[\\text{LG}\\right] [ LG ]  contains local and global emotion-aware entity representations.",
            "Parameter optimization for emotion-aware item recommendation involves a feedback-aware item reweighting strategy (See section 4.2.2 of the main paper), which assigns a weight scalar to each type of user feedback in order to compute a weighted cross-entropy loss.  To verify the effectiveness of this strategy, we conduct experiments where different weight scalars are assigned to the feedback like as (1.0, 1.5, 2.0), while dislike and not say are assigned as (0.0, 0.5, 1.0). When adjusting the weight scalar for one feedback type, the weight scalars for the others are fixed at 1.0. We report the results for RT@10 and RT@50 in Fig.  3 .  As the weight scalar of like increases, the performance of ECR improves, peaking at the weight of 2.0. This peak suggests that items associated with positive user feedback more accurately reflect user preferences, necessitating an emphasis on these items during model training. Conversely, we observe that the weight scalars for dislike and not say should not be minimized indiscriminately. The optimal performance is found when dislike and not say are assigned with the weight scalars of 1.0 and 0.5, respectively. The main reason is that despite some items receiving negative or unclear feedback from users, they still possess a kind of intrinsic correlation with user preferences, offering valuable supervisory information for the item recommendation.",
            "Similarly to section 5.6 of the main paper, we further calculate the Cohen kappa between user satisfaction and the five subjective metrics we designed, with results presented in Table  13 .  We find a low kappa score between emotion intensity and user satisfaction, suggesting that excessive emotional expression may offend users, whereas only appropriate emotional intensity contributes to user satisfaction. Additionally, logic persuasiveness shows a substantial consistency with user satisfaction, while emotional persuasiveness, informativeness, and lifelikeness achieve near-perfect consistency, particularly lifelikeness, which correlates highly with user satisfaction. These findings provide further evidence of the effectiveness of our evaluation system. Meanwhile, they reinforce our hypothesis that adopting emotion-rich and human-like expressions in response enhances user experience and satisfaction."
        ]
    },
    "id_table_4": {
        "caption": "Table 4.  A sampled case extracted from the ReDial dataset. Text in a  green box  represents expressing emotions.",
        "table": "S6.T4.6",
        "footnotes": [],
        "references": [
            "Notation.  Given  t  1 t 1 t-1 italic_t - 1  dialogue turns, the dialogue history  D t  1 subscript D t 1 D_{t-1} italic_D start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT  consists of a sequence of utterances from both recommenders and users, i.e.,  D t  1 = { u k r , u k u } k = 1 t  1 subscript D t 1 subscript superscript superscript subscript u k r superscript subscript u k u t 1 k 1 D_{t-1}=\\{u_{k}^{r},u_{k}^{u}\\}^{t-1}_{k=1} italic_D start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT = { italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT , italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT } start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT , where each utterance  u k  = { w j } j = 1 | u k  | superscript subscript u k subscript superscript subscript w j superscript subscript u k j 1 u_{k}^{*}=\\{w_{j}\\}^{|u_{k}^{*}|}_{j=1} italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = { italic_w start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUPERSCRIPT | italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT | end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT  is composed of a sequence of words.  For simplicity, we concatenate all utterances from  D t  1 subscript D t 1 D_{t-1} italic_D start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT  into a single word sequence  D = { w q } q = 1 n w D subscript superscript subscript w q subscript n w q 1 D=\\{w_{q}\\}^{n_{w}}_{q=1} italic_D = { italic_w start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT } start_POSTSUPERSCRIPT italic_n start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_q = 1 end_POSTSUBSCRIPT , where  n w subscript n w n_{w} italic_n start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT  represents the total number of words in  D t  1 subscript D t 1 D_{t-1} italic_D start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT .  To incorporate knowledge about entities mentioned in the dialogue, we set an external knowledge graph (e.g., DBpedia  (Bizer et al . ,  2009 ) ) as  G = ( E , L ) G E L \\mathcal{G}=(\\mathcal{E},\\mathcal{L}) caligraphic_G = ( caligraphic_E , caligraphic_L ) , consisting of triples  T =  e h , l , e t  T subscript e h l subscript e t \\mathcal{T}=\\left\\langle e_{h},l,e_{t}\\right\\rangle caligraphic_T =  italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_l , italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  , where  e h  E subscript e h E e_{h}\\in\\mathcal{E} italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT  caligraphic_E  and  e t  E subscript e t E e_{t}\\in\\mathcal{E} italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  caligraphic_E  are the head and tail entities,  l  L l L l\\in\\mathcal{L} italic_l  caligraphic_L  reflects the relation between  e h subscript e h e_{h} italic_e start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT  and  e t subscript e t e_{t} italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .  E E \\mathcal{E} caligraphic_E  and  L L \\mathcal{L} caligraphic_L  denote the sets of entities and relations. We define  I I I italic_I  as the entire set of items, all of which are included in the entities of  G G \\mathcal{G} caligraphic_G , i.e.,  I  E I E I\\in\\mathcal{E} italic_I  caligraphic_E . Entities in each utterance  u k  superscript subscript u k u_{k}^{*} italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  are identified as  E k  = { e j } j = 1 | E k  | superscript subscript E k subscript superscript subscript e j superscript subscript E k j 1 E_{k}^{*}=\\{e_{j}\\}^{|E_{k}^{*}|}_{j=1} italic_E start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = { italic_e start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUPERSCRIPT | italic_E start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT | end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT . Each item  i j subscript i j i_{j} italic_i start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  within  E k  superscript subscript E k E_{k}^{*} italic_E start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is linked with user feedback  f i j subscript f subscript i j f_{i_{j}} italic_f start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT , indicating whether the user likes it.  Similarly, we combined all entities mentioned in  D t  1 subscript D t 1 D_{t-1} italic_D start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT  into an entity list  E l = { e q } q = 1 n e subscript E l subscript superscript subscript e q subscript n e q 1 E_{l}=\\{e_{q}\\}^{n_{e}}_{q=1} italic_E start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT = { italic_e start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT } start_POSTSUPERSCRIPT italic_n start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_q = 1 end_POSTSUBSCRIPT , where  n e subscript n e n_{e} italic_n start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT  is the count of entities in the dialogue history. Here, we refer to the entities mentioned in the dialogue history as  local entities . Correspondingly, we refer to entities co-occurring with the local entities in the training dataset as  global entities , which will be detailed in Section  4.2.1 .",
            "In this section, we introduce our empathetic data enlargement process (Section  4.1 ) and two key modules of ECR: emotion-aware item recommendation (Section  4.2 ) and emotion-aligned response generation (Section  4.3 ). Figure  2  shows an overview of ECR.",
            "where  e i subscript e i \\bm{e_{i}} bold_italic_e start_POSTSUBSCRIPT bold_italic_i end_POSTSUBSCRIPT  denotes the representation of  e i subscript e i e_{i} italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  obtained from the RGCN  (Schlichtkrull et al . ,  2018 ) . Following the Eq.  4 , we calculate the global emotion-aware entity representation  E e j  superscript subscript E subscript e j bold- \\bm{\\mathcal{E}_{e_{j}}^{\\prime}} bold_caligraphic_E start_POSTSUBSCRIPT bold_italic_e start_POSTSUBSCRIPT bold_italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_ end_POSTSUPERSCRIPT  by integrating  E e j subscript E subscript e j \\bm{\\mathcal{E}_{e_{j}}} bold_caligraphic_E start_POSTSUBSCRIPT bold_italic_e start_POSTSUBSCRIPT bold_italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT  and  F e j subscript F subscript e j \\bm{\\mathcal{F}_{e_{j}}} bold_caligraphic_F start_POSTSUBSCRIPT bold_italic_e start_POSTSUBSCRIPT bold_italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT .  Finally, we stack global emotion-aware entity representation for each local entity  e j  E l subscript e j subscript E l e_{j}\\in E_{l} italic_e start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  italic_E start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT   into a matrix, denoted as  E g  = [ E e 1  ; ... ; E e n e  ] superscript subscript E g bold- superscript subscript E subscript e 1 bold- ... superscript subscript E subscript e subscript n e bold- \\bm{E_{g}^{\\prime}}=\\left[\\bm{\\mathcal{E}_{e_{1}}^{\\prime}};\\ldots;\\bm{%  \\mathcal{E}_{e_{n_{e}}}^{\\prime}}\\right] bold_italic_E start_POSTSUBSCRIPT bold_italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_ end_POSTSUPERSCRIPT = [ bold_caligraphic_E start_POSTSUBSCRIPT bold_italic_e start_POSTSUBSCRIPT bold_1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_ end_POSTSUPERSCRIPT ; ... ; bold_caligraphic_E start_POSTSUBSCRIPT bold_italic_e start_POSTSUBSCRIPT bold_italic_n start_POSTSUBSCRIPT bold_italic_e end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_ end_POSTSUPERSCRIPT ] .",
            "In this section, we present an example from the ReDial dataset to intuitively demonstrate how ECR works in generating human-like emotional responses. For more cases please refer to Appendix   C .  Given the dialogue history (Hist) between a recommender and a user, the responses (Response) from ECR, GPT-3.5-turbo and Llama 2-7B-Chat as well as the corresponding evaluation results of LLM-based scorer are presented in Table  4 . We also provide the standard responses from the dataset for comparison.  We observe that the standard response rarely expresses emotions and is uninformative, which is hardly attractive and convincing to the users. LLMs, i.e., GPT-3.5-turbo and Llama 2-7B-Chat, prioritize factual and logical interpretability. In the provided example, they try to amplify the user interest in the recommended movie by echoing the mentioned keyword comedy. Additionally, they often deliver objective movie descriptions, such as the plot, genre, and cast. While this approach could make them informative and logic persuasiveness, it neglects the emotional needs of users, inadequately engaging the user. In contrast, our model simulates the vivid conversational style between human beings, incorporating experiences, subjective viewpoints, and emotions. This strategy constructs a deeper emotional connection with users, enlivens their experience, and leads to increased user satisfaction  (Cominelli et al . ,  2021 ) . Furthermore, ECR maintains high-quality factual information to support its emotional rendering and highlight the strengths of the movie. Therefore, even in logic persuasiveness and informativeness  areas where LLMs traditionally excel  ECR remains highly competitive.",
            "We also evaluated with GPT-4 as an LLM-based scorer to enhance the robustness of employing LLM-based scorers. We sampled 200 examples for GPT-4 and used text-davinci-003 as the baseline rather than GPT-3.5-turbo-instruct. The subjective evaluation results of response generation, presented in Table   14 , demonstrate that GPT-4s scoring results closely mirror that of GPT-4-turbo. ECR significantly outperforms the baselines."
        ]
    },
    "id_table_5": {
        "caption": "Table 5.  Subjective evaluation of LLM-based scorer (GPT-4-turbo) for generalization of response generation.",
        "table": "S6.T5.3.1",
        "footnotes": [],
        "references": [
            "In ECR, we use reviews to supervised fine-tune the emotion-aligned generator, endowing it with the ability to express emotions. This process has resulted in some recommended items being seen within the reviews used for training. To determine whether ECR acquires a general ability to generate high-quality emotion-aligned responses, especially for items not encountered in the reviews for the training process, we categorized the 1,000 examples used for the LLM-based scorer evaluation in Section  5.6  into seen and unseen.  The results are presented in Table  5 . We observe a minimal difference in the generation performance between the seen and unseen categories. This indicates that ECR, when provided with knowledge relevant to the recommended item as a part of the emotion-aligned generation prompt, can generalize to generate persuasive and vivid responses for any item, whether or not it is within the training dataset. We observe ECR[Llama 2-Chat] shows better generalization ability than ECR[DialoGPT]. This is likely due to the superior understanding and representation capabilities of Llama 2-7B-Chat because of its large parameter size. Therefore, Llama 2-7B-Chat inherently provides a certain degree of generalization."
        ]
    },
    "id_table_6": {
        "caption": "Table 6.  Annotation case A of user emotions extraction.",
        "table": "A1.T6.3",
        "footnotes": [],
        "references": [
            "In ECR, we use reviews to supervised fine-tune the emotion-aligned generator, endowing it with the ability to express emotions. This process has resulted in some recommended items being seen within the reviews used for training. To determine whether ECR acquires a general ability to generate high-quality emotion-aligned responses, especially for items not encountered in the reviews for the training process, we categorized the 1,000 examples used for the LLM-based scorer evaluation in Section  5.6  into seen and unseen.  The results are presented in Table  5 . We observe a minimal difference in the generation performance between the seen and unseen categories. This indicates that ECR, when provided with knowledge relevant to the recommended item as a part of the emotion-aligned generation prompt, can generalize to generate persuasive and vivid responses for any item, whether or not it is within the training dataset. We observe ECR[Llama 2-Chat] shows better generalization ability than ECR[DialoGPT]. This is likely due to the superior understanding and representation capabilities of Llama 2-7B-Chat because of its large parameter size. Therefore, Llama 2-7B-Chat inherently provides a certain degree of generalization.",
            "Considering the potential bias in emotion annotations, we conducted a validation study. We randomly sampled 100 annotation cases and invited three human annotators to analyze the emotions. We find the average kappa score within human annotators is 0.83, and between LLM is 0.72. These results demonstrate that LLM annotations are highly consistent with humans and have huge potential for reuse in other domains. We present two annotation cases in Table  6  and   7  to facilitate an intuitive understanding of the relevance between emotions and user preference modeling. Each table displays the user utterance along with its corresponding dialogue history. Additionally, we include the emotion labels and the reasons provided by GPT-3.5-turbos annotation, as well as the standard responses from the recommender in the dataset. In Table  6 , GPT-3.5-turbo recognizes an expression of admiration and happy towards the entity Kristen Wiig. So the recommender suggests the movie Whip It in the standard response, in which Kristin Wigg appeared. In Table  7 , the emotions of frustration and disappointment towards animated films are recognized, prompting the recommender to suggest the live-action family movie Cinderella in the standard response."
        ]
    },
    "id_table_7": {
        "caption": "Table 7.  Annotation case B of user emotions extraction.",
        "table": "A1.T7.3",
        "footnotes": [],
        "references": [
            "Considering the potential bias in emotion annotations, we conducted a validation study. We randomly sampled 100 annotation cases and invited three human annotators to analyze the emotions. We find the average kappa score within human annotators is 0.83, and between LLM is 0.72. These results demonstrate that LLM annotations are highly consistent with humans and have huge potential for reuse in other domains. We present two annotation cases in Table  6  and   7  to facilitate an intuitive understanding of the relevance between emotions and user preference modeling. Each table displays the user utterance along with its corresponding dialogue history. Additionally, we include the emotion labels and the reasons provided by GPT-3.5-turbos annotation, as well as the standard responses from the recommender in the dataset. In Table  6 , GPT-3.5-turbo recognizes an expression of admiration and happy towards the entity Kristen Wiig. So the recommender suggests the movie Whip It in the standard response, in which Kristin Wigg appeared. In Table  7 , the emotions of frustration and disappointment towards animated films are recognized, prompting the recommender to suggest the live-action family movie Cinderella in the standard response."
        ]
    },
    "id_table_8": {
        "caption": "Table 8.  Mapping relationships and the percentages of the nine main emotion types.",
        "table": "A1.T8.3",
        "footnotes": [],
        "references": [
            "During the inference stage for generating emotion-aligned responses  u t e superscript subscript u t e u_{t}^{e} italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT , we followed the same prompt design as in Eq.  8 .  We retrieve knowledge triples  T i k = {  i k , l , e j  } j = 1 | T i k | subscript T subscript i k subscript superscript subscript i k l subscript e j subscript T subscript i k j 1 \\mathcal{T}_{i_{k}}=\\{\\langle i_{k},l,e_{j}\\rangle\\}^{|\\mathcal{T}_{i_{k}}|}_{%  j=1} caligraphic_T start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT = {  italic_i start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_l , italic_e start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  } start_POSTSUPERSCRIPT | caligraphic_T start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT | end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT  from the KG  G G \\mathcal{G} caligraphic_G  using the predicted item  i k subscript i k i_{k} italic_i start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT  as the head entity.  And we collect a list of knowledge entities  E i k = { e j } j = 1 | E i k | subscript E subscript i k subscript superscript subscript e j subscript E subscript i k j 1 E_{i_{k}}=\\{e_{j}\\}^{|E_{i_{k}}|}_{j=1} italic_E start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT = { italic_e start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUPERSCRIPT | italic_E start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT | end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT  that is mentioned at least twice in the reviews corresponding to  i k subscript i k i_{k} italic_i start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT .  Then, we filter  p  n t p subscript n t pn_{t} italic_p italic_n start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  triples from  T i k subscript T subscript i k \\mathcal{T}_{i_{k}} caligraphic_T start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT  and  p  n e p subscript n e pn_{e} italic_p italic_n start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT  entities from  E i k r superscript subscript E subscript i k r E_{i_{k}}^{r} italic_E start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT  as a part of the emotion-aligned generation prompt. We simplify the filtering process by random selection, leaving more complex approaches to be explored in the future.",
            "Finally, GPT-3.5-turbo  (Zoph et al . ,  2022 )  recognized a total of 93 emotion types for utterance-level user emotions. However, we encountered issues of synonymous emotion types and label class imbalance. To ensure consistency in the emotion annotation and the subsequent model training, we manually mapped the emotion types recognized by GPT-3.5-turbo into nine main types. The mapping relationship between the recognized types and the main types, along with their percentages, are detailed in Table  8 ."
        ]
    },
    "id_table_9": {
        "caption": "Table 9.  Performance comparison of emotion-aligned response generation w.r.t. different amount and format of knowledge in the emotion-aligned generation prompt. Text in a  yellow box  represents hallucination; the  green boxes  represent emotions.",
        "table": "A1.T9.9",
        "footnotes": [],
        "references": [
            "To enhance the consistency between the recommended item and generated responses while maintaining the models natural language generation capabilities, we retrieve item-relevant knowledge from KGs as a part of the emotion-aligned generation prompt (See section 4.3.1 of the main paper).  It is preferred to provide more precise and explainable information in responses to increase its informativeness and persuasiveness. However, in our experiments, we found that models often face challenges in handling large amounts and complex knowledge as the prompt.  Thus, exploring the impact of the amount and format of item-relevant knowledge on emotion-aligned generation performance is essential. According to our method setup, we retrieve  p  n t p subscript n t pn_{t} italic_p italic_n start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  triples and  p  n e p subscript n e pn_{e} italic_p italic_n start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT  entities to formulate a part of the emotion-aligned generation prompt. We experimented by varying ( p  n t p subscript n t pn_{t} italic_p italic_n start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , p  n e p subscript n e pn_{e} italic_p italic_n start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ) to (0,0), (2,4), and (4,8) and removing triple-format knowledge. We employ an example to quantitatively analyze the performance changes of ECR[DialoGPT], as detailed in Table  9 ."
        ]
    },
    "id_table_10": {
        "caption": "Table 10.  Sampled case A extracted from ReDial dataset.",
        "table": "A3.T10.3",
        "footnotes": [],
        "references": [
            "In this section, we show two more cases extracted from the ReDial dataset. Tables  11  and   10  display the generation performance of all models. These tables revealed that a typical CRS, i.e., UniCRS, limited by the quality of the training data, generates a minimal number of emotional words, such as good, which results in weak emotional intensity and hardly creates an emotional connection with the users. Meanwhile, they struggle to generate movie-related information effectively, potentially degrading the user experience. Additionally, We find that GPT-3.5-turbo-instruct exhibits weaker performance in capturing and utilizing contextual keywords compared to GPT-3.5-turbo."
        ]
    },
    "id_table_11": {
        "caption": "Table 11.  Sampled case B extracted from ReDial dataset.",
        "table": "A4.T11.3",
        "footnotes": [],
        "references": [
            "In this section, we show two more cases extracted from the ReDial dataset. Tables  11  and   10  display the generation performance of all models. These tables revealed that a typical CRS, i.e., UniCRS, limited by the quality of the training data, generates a minimal number of emotional words, such as good, which results in weak emotional intensity and hardly creates an emotional connection with the users. Meanwhile, they struggle to generate movie-related information effectively, potentially degrading the user experience. Additionally, We find that GPT-3.5-turbo-instruct exhibits weaker performance in capturing and utilizing contextual keywords compared to GPT-3.5-turbo."
        ]
    },
    "id_table_12": {
        "caption": "Table 12.  Subjective evaluation of human annotators for response generation.  Boldface indicates the best result. Significant improvements over best baseline results are marked with * (t-test,  p < 0.05 ) p\\textless 0.05) italic_p < 0.05 ) .",
        "table": "A4.T12.8.6",
        "footnotes": [],
        "references": [
            "To assess the effectiveness of our proposed metrics in measuring user satisfaction, we invited three human annotators to evaluate user satisfaction (Sat) with ten levels (0-9). These annotators played the role of users in the dialogue. We then asked them to label their satisfaction with responses from each model. The evaluation results are displayed in Table  12 . We also kept other subjective evaluation metrics in the Tabel for comparison. It can be observed that ECR significantly outperforms all baselines in terms of user satisfaction. Meanwhile, the score distributions across different metrics are similar, which confirms the effectiveness of the evaluation system we designed in reflecting user satisfaction for response generation."
        ]
    },
    "id_table_13": {
        "caption": "Table 13.  Cohen kappa between user satisfaction and subjective metrics we designed for response generation.",
        "table": "A4.T13.1",
        "footnotes": [],
        "references": [
            "Similarly to section 5.6 of the main paper, we further calculate the Cohen kappa between user satisfaction and the five subjective metrics we designed, with results presented in Table  13 .  We find a low kappa score between emotion intensity and user satisfaction, suggesting that excessive emotional expression may offend users, whereas only appropriate emotional intensity contributes to user satisfaction. Additionally, logic persuasiveness shows a substantial consistency with user satisfaction, while emotional persuasiveness, informativeness, and lifelikeness achieve near-perfect consistency, particularly lifelikeness, which correlates highly with user satisfaction. These findings provide further evidence of the effectiveness of our evaluation system. Meanwhile, they reinforce our hypothesis that adopting emotion-rich and human-like expressions in response enhances user experience and satisfaction."
        ]
    },
    "id_table_14": {
        "caption": "Table 14.  Subjective evaluation of LLM-based scorer (GPT-4) for response generation.  Boldface indicates the best result. Significant improvements over best baseline results are marked with * (t-test,  p < 0.05 ) p\\textless 0.05) italic_p < 0.05 ) .",
        "table": "A4.T14.6.4",
        "footnotes": [],
        "references": [
            "We also evaluated with GPT-4 as an LLM-based scorer to enhance the robustness of employing LLM-based scorers. We sampled 200 examples for GPT-4 and used text-davinci-003 as the baseline rather than GPT-3.5-turbo-instruct. The subjective evaluation results of response generation, presented in Table   14 , demonstrate that GPT-4s scoring results closely mirror that of GPT-4-turbo. ECR significantly outperforms the baselines."
        ]
    },
    "global_footnotes": []
}