{
    "id_table_1": {
        "caption": "Table 1:  Main Hyperparameters and Tuning Values for Grid Search",
        "table": "S6.EGx1",
        "footnotes": [],
        "references": [
            "Online RL.  The primary objective in online reinforcement learning is to interact with an unknown Markov Decision Process (MDP), over a finite or infinite decision-making horizon  H H \\mathcal{H} caligraphic_H , in order to optimize the policy    \\pi italic_  by observing transition trajectories   MDP subscript  MDP \\tau_{\\text{MDP}} italic_ start_POSTSUBSCRIPT MDP end_POSTSUBSCRIPT . These trajectories are defined as   MDP = { ( s 0 , a 0 , r 1 , s 1 ) , ( s 1 , a 1 , r 2 , s 2 ) , ... , ( s H  1 , a H  1 , r H , s H ) } subscript  MDP subscript s 0 subscript a 0 subscript r 1 subscript s 1 subscript s 1 subscript a 1 subscript r 2 subscript s 2 ... subscript s H 1 subscript a H 1 subscript r H subscript s H \\tau_{\\text{MDP}}=\\{(s_{0},a_{0},r_{1},s_{1}),(s_{1},a_{1},r_{2},s_{2}),\\dots,% (s_{\\mathcal{H}-1},a_{\\mathcal{H}-1},r_{\\mathcal{H}},s_{\\mathcal{H}})\\} italic_ start_POSTSUBSCRIPT MDP end_POSTSUBSCRIPT = { ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , ( italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) , ... , ( italic_s start_POSTSUBSCRIPT caligraphic_H - 1 end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT caligraphic_H - 1 end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT caligraphic_H end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT caligraphic_H end_POSTSUBSCRIPT ) } . The transition trajectory is generated through the MDP setup, which consists of a continuous state space  S  R d S superscript R d \\mathcal{S}\\subseteq\\mathbb{R}^{d} caligraphic_S  blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT  in a  d d d italic_d -dimensional space, an action space  A = { a ( 0 ) , a ( 1 ) , ... , a ( k ) } A superscript a 0 superscript a 1 ... superscript a k \\mathcal{A}=\\{a^{(0)},a^{(1)},\\dots,a^{(k)}\\} caligraphic_A = { italic_a start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT , italic_a start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT , ... , italic_a start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT }  of  k k k italic_k  discrete actions, a state transition model with transition probability  P  ( s t + 1  s t , a t ) P conditional subscript s t 1 subscript s t subscript a t P(s_{t+1}\\mid s_{t},a_{t}) italic_P ( italic_s start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT  italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , a reward function  R  ( S , A ) R S A \\mathcal{R}(\\mathcal{S},\\mathcal{A}) caligraphic_R ( caligraphic_S , caligraphic_A ) , and a discount factor    \\gamma italic_  to account for the value of rewards over time. At each time step  t t t italic_t , the system observes the previous and current states  s t , s t + 1  S subscript s t subscript s t 1 S s_{t},s_{t+1}\\in\\mathcal{S} italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT  caligraphic_S , the current action  a t + 1  A subscript a t 1 A a_{t+1}\\in\\mathcal{A} italic_a start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT  caligraphic_A , and the current reward  r t + 1 = R  ( s t + 1 , s t , a t ) subscript r t 1 R subscript s t 1 subscript s t subscript a t r_{t+1}=\\mathcal{R}(s_{t+1},s_{t},a_{t}) italic_r start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = caligraphic_R ( italic_s start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) . The objective of the MDP is to maximize the cumulative rewards, as expressed in Equation  1 , where the optimal policy    superscript  \\pi^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  determines the best action the agent can take to maximize this reward.",
            "The training architecture of DQN is inspired by Model-Based Dyna-Q architecture  Peng et al. ( 2018 ) , and is detailed in Algorithm  1 . This method transitions from the traditional focus on state-action pairs to utilizing real-world trajectories as the basis for both training and planning. The approach involves two primary phases:  Direct Real-World Data Collection  and  Planning using Synthetic Data .",
            "We developed a DQN agent based on the zTT framework  Kim et al. ( 2021 )  to optimize system performance under specific thermal and power constraints. The experiments were conducted on the Jetson TX2, which features six CPU cores and an embedded GPU. We captured performance data, including  f  p  s f p s fps italic_f italic_p italic_s , core frequencies ( f f f italic_f ), power consumption (   \\rho italic_ ), and core temperatures (   \\theta italic_ ). The client collected state tuples  { f  p  s , f ,  ,  } f p s f   \\{fps,f,\\rho,\\theta\\} { italic_f italic_p italic_s , italic_f , italic_ , italic_ } , while the server, hosted in the cloud, assigned actions based on the available frequencies  f f f italic_f . The workflow of the proposed method is illustrated in Figure  1 . The experiment was designed to evaluate the agents ability to dynamically adjust the systems frequency and power settings, ensuring optimal performance while maintaining system stability.",
            "Key hyperparameters for this experiment included an action space of 12 possible actions, a target FPS of 60, and a target temperature of 50 degrees Celsius. The agent was trained using a discount factor of 0.99 and an initial learning rate of 0.05, which was dynamically adjusted during training. We conducted a grid search to tune the hyperparameters, and the selected values are highlighted in Table  1 .",
            "We evaluated the correlation of synthetic features across three approaches and compared them with real data using Pearsons correlation coefficient, as introduced in  Sedgwick ( 2012 ) . This method determines the pairwise relationship between each pair of features, each with  n n n italic_n  samples, as shown in Equation  10 . The resulting correlation coefficient  corr i  j subscript corr i j \\text{corr}_{ij} corr start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT  ranges between 1 and -1, where 1 indicates a positive correlation, -1 indicates a negative correlation, and 0 indicates no correlation between two features."
        ]
    },
    "id_table_2": {
        "caption": "",
        "table": "S4.T1.1",
        "footnotes": [],
        "references": [
            "Multi-Armed Bandit Problem.  The bandit problem is a simplified version of online reinforcement learning, where there is only one state  s t subscript s t s_{t} italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  with a decision horizon of  H = 1 H 1 H=1 italic_H = 1 . The reward for taking action  a t subscript a t a_{t} italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  at time  t t t italic_t  is given by  R  ( a t ) = r t R subscript a t subscript r t R(a_{t})=r_{t} italic_R ( italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = italic_r start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . The resulting trajectory for the bandit problem can be represented   bandit = { ( a 0 , r 0 ) , ( a 1 , r 1 ) , ... , ( a T , r T ) } subscript  bandit subscript a 0 subscript r 0 subscript a 1 subscript r 1 ... subscript a T subscript r T \\tau_{\\text{bandit}}=\\{(a_{0},r_{0}),(a_{1},r_{1}),\\dots,(a_{T},r_{T})\\} italic_ start_POSTSUBSCRIPT bandit end_POSTSUBSCRIPT = { ( italic_a start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) , ( italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , ... , ( italic_a start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) }  as  T T T italic_T  stands for the total time steps. The regret  Reg  in this simplified scenario is defined as the difference between the expected reward of the best possible action and the expected reward obtained by the actions actually taken over time. This is expressed in Equation  2 , where   r  = max a t  A  E  [ R  ( a t ) ] superscript subscript  r subscript subscript a t A E delimited-[] R subscript a t \\mu_{r}^{*}=\\max_{a_{t}\\in\\mathcal{A}}\\mathbb{E}[R(a_{t})] italic_ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = roman_max start_POSTSUBSCRIPT italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  caligraphic_A end_POSTSUBSCRIPT blackboard_E [ italic_R ( italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ]  represents the maximum expected reward based on the optimal action at each time step.",
            "The results in Figure  2  represents the correlation among the features in real-data, distribution-aware flow matching, pure flow matching, and model-based generated synthetic data. The Distribution-Aware method shows a correlation pattern that closely mirrors that of the real data, suggesting that it captures the underlying relationships between features more effectively than the other methods."
        ]
    }
}