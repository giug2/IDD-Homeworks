{
    "id_table_1": {
        "caption": "Table 1 :  Token complexities of primary table prompting approaches without truncation. Note that Read Schema does not aware of any cell content.  N N N italic_N : number of rows,  M M M italic_M : number of columns,  K K K italic_K : number of top retrieval results,  D D D italic_D : number of distinct values in the table. It is generally observed that  K < M  D  N  M K M much-less-than D much-less-than N M K<M\\ll D\\ll NM italic_K < italic_M  italic_D  italic_N italic_M .",
        "table": "S3.T1.16.16.6",
        "footnotes": [],
        "references": [
            "In this work, we introduce, TableRAG, a scalable framework that leverages retrieval-augmented generation (RAG) for LM-based table understanding.  We illustrate the key differences between prior table prompting approaches and the proposed TableRAG in Fig.  1 .",
            "The token complexity of primary table prompting approaches are shown in Table  1 .",
            "See Alg.  1 ."
        ]
    },
    "id_table_2": {
        "caption": "Figure 3 :  Histogram of the proportion of number of distinct values to number of cells in ArcadeQA and BirdQA. The figure indicates that for most tables, the number of distinct values ( D D D italic_D ) are much smaller than the number of cells ( N  M N M NM italic_N italic_M ).",
        "table": "S3.T1.11.11.1.1.3.1",
        "footnotes": [],
        "references": [
            "An illustration of the workflow of our method is shown in Fig.  2 .  The core idea is to incorporate schema retrieval and cell retrieval to obtain necessary information for solving the questions by program-aided LMs.  In practice, we often find that processing the entire table for LMs is unnecessary.  Instead, the critical information usually lies in specific column names, data types, and cell values that directly relate to the question.  For example, consider the question  \"What is the average price for wallets?\"   To address this, a program may simply need to extract rows related to \"wallets\" and then calculate the average from the price column.  Knowing just the relevant column names and how \"wallets\" are represented in the table suffices to write the program.  Our method, TableRAG, leverages the observation and addresses the context length limitations by RAG.",
            "In evaluations across the datasets shown in Table  2 , TableRAG consistently outperformed other methods, achieving the highest accuracies across all LMs on both ArcadeQA and BirdQA.  The ReadTable method underperforms on both in ArcadeQA and BirdQA, indicating it suffers from long context.  Among the three LMs, GPT 3.5 Turbo consistently delivers the best performance, regardless of the table prompting method used.  These results demonstrate the effectiveness of TableRAG in handling large-scale TableQA tasks."
        ]
    },
    "id_table_3": {
        "caption": "Table 2 :  Performance comparison of table prompting approaches on ArcadeQA and BirdQA across LMs.",
        "table": "S3.T1.12.12.2.2.3.1",
        "footnotes": [],
        "references": [
            "Our method integrates schema retrieval and cell retrieval to extract essential information from tables, enabling a program-aided LM agent to solve queries based on the provided information. Schema retrieval allows LMs to identify crucial columns and their data types solely by column names, avoiding the need to encode entire columns.  Cell retrieval enables the identification of keywords for indexing or pinpointing columns that contain necessary but hard-to-find information missed by schema retrieval alone.  To build the database for cell retrieval, TableRAG encoded each cell independently, addressing the issue faced  when encoding entire rows and columns.  Furthermore, TableRAG only encodes distinct and the most frequent categorical values, reducing the encoders token cost (as shown in Fig.  3 ) and operating within a user-specified budget effectively.  Both retrieval processes in TableRAG are enhanced by query expansion  [ 21 ]  with dedicated prompts for schema retrieval and cell retrieval, ensuring thorough and relevant data extraction.",
            "Following schema retrieval, we proceed to extract specific cell values needed to answer the question.  This involves building a database of distinct column-value pairs from  T T T italic_T , denoted as  V =  i  j ( C j , v i  j ) V subscript i j subscript C j subscript v i j V=\\bigcup_{ij}(C_{j},v_{ij}) italic_V =  start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( italic_C start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ) , where  C j subscript C j C_{j} italic_C start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  is the column name of the  j j j italic_j -th column.  In practice, the set of distinct values is often much smaller than the total number of cells, as illustrated in Fig.  3 .  This discrepancy significantly enhances the efficiency of cell retrieval.",
            "To better understand the retrieval quality of various table prompting approaches, in Table  3 , we assessed the recall, precision and f1 score for the prompts fed to LMs for reasoning.  The ground truths are extracted from the program annotations in the ArcadeQA and BirdQA datasets.  In column retrieval, while all methods achieved high recall due to the small number of columns, TableRAG demonstrated superior precision across both datasets, indicating its effectiveness in identifying the most relevant columns concisely.  In contrast, ReadSchema and RowColRetrieval showed lower precision, suggesting that they retrieved more irrelevant columns.  For cell retrieval, TableRAG consistently outperformed other methods on all metrics.  TableRAGs high recall in cell retrieval marks a significant improvement over other table prompting methods, indicating it can retrieve most necessary cells for the subsequent reasoning.  In summary, this analysis underscores TableRAGs efficacy in retrieving essential information in both the column and cell aspects."
        ]
    },
    "id_table_4": {
        "caption": "Table 3 :  Evaluation of retrieval performance. TableRAG shows best retrieval quality on all tasks. R: recall, P: precision.",
        "table": "S3.T1.14.14.4.4.4.1",
        "footnotes": [],
        "references": [
            "It should be noted that cell retrieval is primarily beneficial when indexing by cell values is required.  In other scenarios, simply knowing the schema may suffice. For example, to answer the question \"What is the average price?\", identifying the relevant column name for prices is sufficient because the actual computation of the average can be handled programmatically.  Nevertheless, cell retrieval still improves TableRAG with additional key values from the table, which will be shown in Sec  4.8 ."
        ]
    },
    "id_table_5": {
        "caption": "Table 4 :  Comparison of TableRAG with state-of-the-art methods on WikiTableQA.",
        "table": "S3.T1.14.14.4.4.2.2",
        "footnotes": [],
        "references": [
            "To understand the performance across varying table sizes under similar conditions, we create a set of synthetic data from TabFact with table sizes ranging from  50  50 50 50 50\\times 50 50  50 ,  100  100 100 100 100\\times 100 100  100 ,  500  500 500 500 500\\times 500 500  500 , and  1000  1000 1000 1000 1000\\times 1000 1000  1000 .  The synthetic data allow us to analysis how table prompting methods perform in different scales with the same questions and key table contents.  The results are shown in Fig.  5 .  ReadTable exhibited strong initial accuracy with the original data but failed dramatically as table sizes increased, infeasible for sizes 100 and beyond due to the context length limitations.  Conversely, TableRAG demonstrated the most consistent and scalable performance, decreasing moderately from 83.1 to 68.4 as table size increased to 1000 rows and columns, showcasing its effectiveness in understanding larger tables.  Both ReadSchema and RowColRetrieval showed declines in performance with increasing table size, yet maintained moderate accuracy, highlighting their relative scalability compared to ReadTable but less effectiveness than TableRAG in handling large tables.",
            "To evaluate performance on small-scale TableQA datasets, we compared TableRAG with state-of-the-art approaches that rely on reading entire tables, using the commonly used WikiTableQA  [ 12 ]  benchmark.  As shown in Table  5 , TableRAG surpasses all existing methods, including TaBERT  [ 25 ] , Text-to-SQL  [ 15 ] , Binder  [ 3 ] , and Dater  [ 24 ] .  This highlights TableRAGs effectiveness, even in small-scale settings.  These results confirm that, while TableRAG is designed for large-scale TableQA, its approach is versatile and maintains state-of-the-art performance across different table sizes and complexities.",
            "Fig  5  illustrates the impact of varying the number of top retrieval results ( K K K italic_K ) on the performance and the token cost for the subsequent LM reasoning.  The results demonstrate that that increasing the number  K K K italic_K , while increasing the prompt lengths, does not consistently improve performance.  Though larger  K K K italic_K  allows LMs access to more information, it also results in a longer context which can exacerbate the lost-in-the-middle phenomenon.  In contrast, TableRAG excels by requiring fewer  K K K italic_K  values, thus reducing the context tokens needed and lowering subsequent reasoning costs while still outperforming other methods."
        ]
    },
    "id_table_6": {
        "caption": "Table 5 :  Performance comparison of different retrieval approaches in TableRAG on ArcadeQA and BirdQA.",
        "table": "S3.T1.16.16.6.6.2.2",
        "footnotes": [],
        "references": [
            "Table  6  compares different retrieval approaches within TableRAG.  BM25  [ 16 ] , a well-known statistical retrieval method, excels in efficiency and can process all cells but lacks semantic understanding.  We also compare it with our embedding-based retrieval and a hybrid approach that combines scores from both methods.  The results show that embedding-based retrieval achieves the best performance, outperforming both BM25 and the hybrid method, despite not processing the entire table due to encoding constraints.  This underscores the importance of semantic understanding in retrieval, where embedding-based methods offer better comprehension of table data, significantly enhancing TableRAGs performance.",
            "We analyze the impact of schema retrieval and cell retrieval on performance in Table  6 .  The results demonstrate that schema retrieval consistently enhances reasoning performance across datasets and LMs, with a maximum improvement of  9.4 % percent 9.4 9.4\\% 9.4 % , regardless of whether cell values are considered.  The results indicate that even for tables with small number of columns (average  20.5 20.5 20.5 20.5  columns in ArcadeQA and  11.1 11.1 11.1 11.1  columns in BirdQA), schema retrieval is still helpful to only provide relevant columns for the subsequent reasoning.  Similarly, cell retrieval consistently improves accuracies across all datasets and LMs, with a maximum improvement of  11.5 % percent 11.5 11.5\\% 11.5 % , indicating cell retrieval can effectively extract key information from the table contents."
        ]
    },
    "id_table_7": {
        "caption": "Table 6 :  Ablation study of schema retrieval (Rows 1 vs 3 and 2 vs 4) and cell retrieval (Rows 1 vs 2 and 3 vs 4). The first column indicates whether the LM processed all schemas or only retrieved schemas. The second column indicates whether the LM ignored cell values or processed retrieved column-cell pairs. Schema retrieval leads to an improvement in accuracy of up to 9.4%, while cell retrieval results in an increase of up to 11.5%.",
        "table": "S4.T2.6",
        "footnotes": [],
        "references": [
            "We introduce two new real-world benchmarks derived from Arcade and BIRD-SQL, along with an expanded synthetic dataset from TabFact. These datasets include tables ranging from tens to millions of cells (Table  7 ), enabling comprehensive evaluation of LM capabilities across various table scales.",
            "Existing TableQA benchmarks such as TabFact  [ 2 ]  and WikiTableQA  [ 12 ]  feature small web tables that fall within the context limits of most LMs, while others like Spider dataset  [ 27 ]  are designed purely as text-to-SQL tasks with access only to the schema.  To better assess reasoning capabilities over larger, more realistic tables, we have developed two extensive table QA datasets, ArcadeQA and BirdQA, derived from the Arcade  [ 26 ]  and BIRD-SQL  [ 7 ]  datasets, respectively.  ArcadeQA comprises tables with an average of  79 , 000 79 000 79,000 79 , 000  rows and a maximum of  7 7 7 7  million cells, while BirdQA tables feature an average of  62 , 000 62 000 62,000 62 , 000  rows with a peak at  10 10 10 10  million cells.  Furthermore, we expanded TabFact to include synthetic tables ranging from  100  100 100 100 100\\times 100 100  100  to  1 , 000  1 , 000 1 000 1 000 1,000\\times 1,000 1 , 000  1 , 000 , equivalent to a million of cells, to examine the impact of different table sizes under the same question and key information.  Detailed methodology for dataset generation and the statistics of these datasets are provided in Appendix  C  and summarized in Table  7 .",
            "The results from Fig.  7  demonstrate how different token encoding budgets ( B B B italic_B ) affect the performance of TableRAG and RowColRetrieval.  While a higher budget theoretically allows for more information to be retrieved, the results show that it does not always lead to better performance.  Specifically, RowColRetrieval shows a decline in performance with increased budgets, potentially due to the retrieval of more rows that complicate selecting the correct ones and produce noisier embeddings from longer column sequences.  In contrast, TableRAG maintains consistent performance across various budgets, indicating that its approach of building the corpus by cell frequency effectively captures essential information even with limited budgets.",
            "The effectiveness of our query expansion method is analyzed in Fig.  7 .  The results demonstrate that query expansion consistently enhances TableRAGs performance across various datasets and LMs.",
            "Here we describe the generation of each datasets. The statistics of each dataset are shown in Table  7 ."
        ]
    },
    "id_table_8": {
        "caption": "Table 7 :  Dataset statistics. Values are presented in averages, except for the total counts of tables and questions.",
        "table": "S4.T3.6.1",
        "footnotes": [],
        "references": [
            "It should be noted that cell retrieval is primarily beneficial when indexing by cell values is required.  In other scenarios, simply knowing the schema may suffice. For example, to answer the question \"What is the average price?\", identifying the relevant column name for prices is sufficient because the actual computation of the average can be handled programmatically.  Nevertheless, cell retrieval still improves TableRAG with additional key values from the table, which will be shown in Sec  4.8 .",
            "In the worst case, the number of distinct values could match the total number of cells.  To maintain the feasibility of TableRAG in such cases, we introduce a cell encoding budget  B B B italic_B .  If the number of distinct values exceeds  B B B italic_B , we restrict our encoding to the  B B B italic_B  most frequently occurring pairs, thus improving efficiency when processing large tables.  It is important to note that the encoding budget impacts only the cell retrieval process.  Even if a cell is not included for retrieval, the subsequent solver can still access the cell if its column name is known through schema retrieval or other cells.  For instance, as shown in Fig.  8 , the description column contains free-form text, which likely results in a high number of unique values, many of which may be truncated due to the cell encoding budget.  Nevertheless, as long as the solver recognizes the column, it can still perform operations on that column to extract the required information.",
            "After obtaining the column names and cell values relevant to the question, the LMs can use these information to effectively interact with the table.  TableRAG is compatible with LM agents which can interact with tables programmatically. In this work, we consider ReAct  [ 23 ] , which is a popular approach to extend the capability of LMs and has been used in recent literature to achieve state-of-the-art results in Table QA benchmarks  [ 10 ,  29 ] .  An example of how TableRAG works with ReAct is illustrated in Fig.  8 ."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "S4.T5.fig1.5",
        "footnotes": [],
        "references": []
    },
    "id_table_10": {
        "caption": "",
        "table": "S4.T5.fig2.5",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "",
        "table": "S4.T6.6",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "",
        "table": "A3.T7.4.4",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "",
        "table": "A3.T7.4.4.4.6.1",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "A3.T7.4.4.4.7.1",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "",
        "table": "A3.T7.1.1.1.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_16": {
        "caption": "",
        "table": "A3.T7.2.2.2.2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_17": {
        "caption": "",
        "table": "A3.T7.3.3.3.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_18": {
        "caption": "",
        "table": "A3.T7.4.4.4.4.1",
        "footnotes": [],
        "references": []
    },
    "id_table_19": {
        "caption": "",
        "table": "A3.T7.4.4.4.8.1",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": []
}