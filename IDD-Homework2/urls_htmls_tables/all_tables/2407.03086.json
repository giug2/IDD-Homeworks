{
    "PAPER'S NUMBER OF TABLES": 1,
    "S5.T1": {
        "caption": "Table 1. Accuracy, hypernetwork size and hypernetwork training time for different kğ‘˜k.",
        "table": "",
        "footnotes": "\n\n\n\n\n\nCNN - UniMiB\nNumber of Singular Values (kğ‘˜k)\n\nAll\n25\n50\n100\n200\n\nAccuracyâ†‘â†‘\\uparrow\n40.44 (0.00%)\n38.55 (-1.89%)\n38.76 (-1.68%)\n38.86 (-1.58%)\n38.89 (-1.55%)\n\nHN size (MB)â†“â†“\\downarrow\n455.34 (0.00%)\n5.60 (-98.77%)\n5.68 (-98.75%)\n5.83 (-98.72%)\n6.14 (-98.65%)\n\n# of parameters (M)â†“â†“\\downarrow\n227.37 (0.00%)\n1.40 (-99.39%)\n1.42 (-99.38%)\n1.46 (-99.36%)\n1.54 (-99.32%)\n\n\n\n\nAvg training time\n\nper epoch(ms)â†“â†“\\downarrow\n\n\n\n\n226.56Â±0.10\n\n(1.00Ã—\\times)\n\n\n\n\n91.14Â±0.54\n\n(2.49Ã—\\times)\n\n\n\n\n91.54Â±0.76\n\n(2.48Ã—\\times)\n\n\n\n\n92.24Â±0.71\n\n(2.46Ã—\\times)\n\n\n\n\n92.34Â±0.58\n\n(2.48Ã—\\times)\n\n\n\n\n\n\n\\Description",
        "references": [
            "To examine the impact of kğ‘˜k we take a deeper look into this tradeoff using TableÂ 1. As the results in the table show for the CNN baseline model and the UbiMiB dataset, the model compression process does show a performance degradation in accuracy performance. Nevertheless, we can notice that this loss is minimal even for a very small kğ‘˜k (e.g., 1.89% loss for kğ‘˜k=25). At the same time, the reduction in parameters and the size of the hypernetwork reduces by nearly two orders of magnitude. Naturally, this reduces the computation latency to a minimal level as well."
        ]
    }
}