{
    "PAPER'S NUMBER OF TABLES": 1,
    "S5.T1": {
        "caption": "Table 1. Accuracy, hypernetwork size and hypernetwork training time for different k𝑘k.",
        "table": "",
        "footnotes": "\n\n\n\n\n\nCNN - UniMiB\nNumber of Singular Values (k𝑘k)\n\nAll\n25\n50\n100\n200\n\nAccuracy↑↑\\uparrow\n40.44 (0.00%)\n38.55 (-1.89%)\n38.76 (-1.68%)\n38.86 (-1.58%)\n38.89 (-1.55%)\n\nHN size (MB)↓↓\\downarrow\n455.34 (0.00%)\n5.60 (-98.77%)\n5.68 (-98.75%)\n5.83 (-98.72%)\n6.14 (-98.65%)\n\n# of parameters (M)↓↓\\downarrow\n227.37 (0.00%)\n1.40 (-99.39%)\n1.42 (-99.38%)\n1.46 (-99.36%)\n1.54 (-99.32%)\n\n\n\n\nAvg training time\n\nper epoch(ms)↓↓\\downarrow\n\n\n\n\n226.56±0.10\n\n(1.00×\\times)\n\n\n\n\n91.14±0.54\n\n(2.49×\\times)\n\n\n\n\n91.54±0.76\n\n(2.48×\\times)\n\n\n\n\n92.24±0.71\n\n(2.46×\\times)\n\n\n\n\n92.34±0.58\n\n(2.48×\\times)\n\n\n\n\n\n\n\\Description",
        "references": [
            "To examine the impact of k𝑘k we take a deeper look into this tradeoff using Table 1. As the results in the table show for the CNN baseline model and the UbiMiB dataset, the model compression process does show a performance degradation in accuracy performance. Nevertheless, we can notice that this loss is minimal even for a very small k𝑘k (e.g., 1.89% loss for k𝑘k=25). At the same time, the reduction in parameters and the size of the hypernetwork reduces by nearly two orders of magnitude. Naturally, this reduces the computation latency to a minimal level as well."
        ]
    }
}