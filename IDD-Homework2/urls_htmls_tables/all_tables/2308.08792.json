{
    "PAPER'S NUMBER OF TABLES": 3,
    "S4.T1": {
        "caption": "TABLE I: Performance of different algorithms during the simulation",
        "table": "<table id=\"S4.T2.st1.2\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.st1.2.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.1.1.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_border_tt\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"></td>\n<th id=\"S4.T2.st1.2.1.1.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.1.1.2.1\" class=\"ltx_text\" style=\"color:#000000;\">sum reward</span></th>\n<th id=\"S4.T2.st1.2.1.1.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.1.1.3.1\" class=\"ltx_text\" style=\"color:#000000;\">power reward</span></th>\n<th id=\"S4.T2.st1.2.1.1.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.1.1.4.1\" class=\"ltx_text\" style=\"color:#000000;\">anxiety reward</span></th>\n<th id=\"S4.T2.st1.2.1.1.5\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.1.1.5.1\" class=\"ltx_text\" style=\"color:#000000;\">grid reward</span></th>\n</tr>\n<tr id=\"S4.T2.st1.2.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.2.2.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.2.2.1.1\" class=\"ltx_text\" style=\"color:#000000;\">DDPG-MARL</span></td>\n<td id=\"S4.T2.st1.2.2.2.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.2.2.2.1\" class=\"ltx_text\" style=\"color:#000000;\">-5855</span></td>\n<td id=\"S4.T2.st1.2.2.2.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.2.2.3.1\" class=\"ltx_text\" style=\"color:#000000;\">-211</span></td>\n<td id=\"S4.T2.st1.2.2.2.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.2.2.4.1\" class=\"ltx_text\" style=\"color:#000000;\">-4136</span></td>\n<td id=\"S4.T2.st1.2.2.2.5\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.2.2.5.1\" class=\"ltx_text\" style=\"color:#000000;\">-1508</span></td>\n</tr>\n<tr id=\"S4.T2.st1.2.3.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.3.3.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.3.3.1.1\" class=\"ltx_text\" style=\"color:#000000;\">TD3-MARL</span></td>\n<td id=\"S4.T2.st1.2.3.3.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.3.3.2.1\" class=\"ltx_text\" style=\"color:#000000;\">-4763</span></td>\n<td id=\"S4.T2.st1.2.3.3.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.3.3.3.1\" class=\"ltx_text\" style=\"color:#000000;\">396</span></td>\n<td id=\"S4.T2.st1.2.3.3.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.3.3.4.1\" class=\"ltx_text\" style=\"color:#000000;\">-3698</span></td>\n<td id=\"S4.T2.st1.2.3.3.5\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.3.3.5.1\" class=\"ltx_text\" style=\"color:#000000;\">-1461</span></td>\n</tr>\n<tr id=\"S4.T2.st1.2.4.4\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.4.4.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.4.4.1.1\" class=\"ltx_text\" style=\"color:#000000;\">SAC-MARL</span></td>\n<td id=\"S4.T2.st1.2.4.4.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.4.4.2.1\" class=\"ltx_text\" style=\"color:#000000;\">-4412</span></td>\n<td id=\"S4.T2.st1.2.4.4.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.4.4.3.1\" class=\"ltx_text\" style=\"color:#000000;\">519</span></td>\n<td id=\"S4.T2.st1.2.4.4.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.4.4.4.1\" class=\"ltx_text\" style=\"color:#000000;\">-3790</span></td>\n<td id=\"S4.T2.st1.2.4.4.5\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.4.4.5.1\" class=\"ltx_text\" style=\"color:#000000;\">-1141</span></td>\n</tr>\n<tr id=\"S4.T2.st1.2.5.5\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.5.5.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.5.5.1.1\" class=\"ltx_text\" style=\"color:#000000;\">FedSAC</span></td>\n<td id=\"S4.T2.st1.2.5.5.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.5.5.2.1\" class=\"ltx_text ltx_font_bold\" style=\"color:#000000;\">-3720</span></td>\n<td id=\"S4.T2.st1.2.5.5.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.5.5.3.1\" class=\"ltx_text\" style=\"color:#000000;\">550</span></td>\n<td id=\"S4.T2.st1.2.5.5.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.5.5.4.1\" class=\"ltx_text\" style=\"color:#000000;\">-3227</span></td>\n<td id=\"S4.T2.st1.2.5.5.5\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.5.5.5.1\" class=\"ltx_text\" style=\"color:#000000;\">-1043</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Twin delayed deep deterministic policy gradient (TD3) is an extension of the DDPG. TD3 employs the clipped double-Q learning to prevent overestimation and stabilize learning. The multi-agent version introduces the global environment and considers the grid reward. The parameters setting keeps consistent with FedSAC.",
                "We compare the above algorithms from two dimensions: the\nconvergence and generalization. The convergence of agents based on various algorithms is demonstrated with average sum reward curves during training. The smooth reward curves for the thirty agents are shown in Fig. ",
                "9",
                ". In addition, we choose testing rewards and standard deviation of power change during the simulation as metrics to compare the generalization of thirty EVs. The test reward evaluates the performance of the simulated EV charging control strategies, with a higher value indicating better performance. The meanings of the variance of power grid changes have been elaborated in SectionÂ ",
                "IV-D",
                ". The simulation settings are consistent with those described in the preceding section. The testing rewards and average/standard deviation of power change are listed in TableÂ ",
                "I",
                ". The power changes of different algorithms on RDN are shown in Fig.Â ",
                "8",
                ".",
                "Fig.Â ",
                "9",
                " illustrates the convergence process of thirty EVs. All the algorithms achieve a well-converged state under the same number of training episodes. Our proposed FedSAC algorithm, which applies FL mechanisms, achieves the highest sum reward and is the best-performing algorithm. While the SAC-MARL algorithm also performs well, its limitation of isolated data makes its performance slightly inferior to that of FedSAC. However, the reward curve of FedSAC exhibits more volatility than SAC-MARL, demonstrating that FedSAC is slightly inferior in terms of stability. This is attributed to the non-uniformity and heterogeneity of data in distributed environments. TD3-MARL and DDPG-MADRL, which lack exploration of the environment, are far inferior to the SAC-based algorithms. Due to the optimization of the Q-function by the TD3 algorithm, its performance is still significantly better than that of DDPG-MARL.",
                "TableÂ ",
                "I",
                " compares the generalization of different algorithms, and Fig.Â ",
                "8",
                " compares the charging/discharging impact on RDN. We compare the standard deviation of power change and their decline ratio compared to SAC defined in SectionÂ ",
                "IV-E",
                ". The bold values in the table represent the best test results among all algorithms. As shown in TableÂ ",
                "II(a)",
                ", the average test reward of thirty EV agents under FedSAC is -3720, which surpasses other comparative algorithms. Furthermore, the ",
                "Ïƒ",
                "g",
                "subscript",
                "ğœ",
                "ğ‘”",
                "\\sigma_{g}",
                " of thirty EV agents trained by FedSAC is 8.63 in Table. ",
                "II(b)",
                ", indicating the best performance among all algorithms. Therefore, the charging/discharging load on RDN (i.e., Fig.Â ",
                "8",
                "(c)) controlled by FedSAC is more stable than that of other algorithms. On the other hand, the DDPG-MADRL performs the worst on both average reward and standard deviation of power change, which is consistent with the training analysis. In addition, the average reward and ",
                "Ïƒ",
                "g",
                "subscript",
                "ğœ",
                "ğ‘”",
                "\\sigma_{g}",
                " of SAC-MADRL are -4412 and 9.19, respectively, which are the closest to the optimal values. This phenomenon clarifies that the FL mechanism can improve the generalization of SAC. Since the FedSAC outperforms comparative algorithms in test rewards and standard deviation, FedSAC has the best generalization effect.",
                "The results above clarify that the introduction of the FL mechanism dramatically improves EV agentsâ€™ convergence effect and generalization ability. In summary, the comparisons demonstrate that the proposed FedSAC outperforms existing algorithms in EV charging control."
            ]
        ]
    },
    "S4.T2.st1": {
        "caption": "(a) Cumulative average test rewards of different algorithms",
        "table": "<table id=\"S4.T2.st1.2\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.st1.2.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.1.1.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_border_tt\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"></td>\n<th id=\"S4.T2.st1.2.1.1.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.1.1.2.1\" class=\"ltx_text\" style=\"color:#000000;\">sum reward</span></th>\n<th id=\"S4.T2.st1.2.1.1.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.1.1.3.1\" class=\"ltx_text\" style=\"color:#000000;\">power reward</span></th>\n<th id=\"S4.T2.st1.2.1.1.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.1.1.4.1\" class=\"ltx_text\" style=\"color:#000000;\">anxiety reward</span></th>\n<th id=\"S4.T2.st1.2.1.1.5\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.1.1.5.1\" class=\"ltx_text\" style=\"color:#000000;\">grid reward</span></th>\n</tr>\n<tr id=\"S4.T2.st1.2.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.2.2.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.2.2.1.1\" class=\"ltx_text\" style=\"color:#000000;\">DDPG-MARL</span></td>\n<td id=\"S4.T2.st1.2.2.2.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.2.2.2.1\" class=\"ltx_text\" style=\"color:#000000;\">-5855</span></td>\n<td id=\"S4.T2.st1.2.2.2.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.2.2.3.1\" class=\"ltx_text\" style=\"color:#000000;\">-211</span></td>\n<td id=\"S4.T2.st1.2.2.2.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.2.2.4.1\" class=\"ltx_text\" style=\"color:#000000;\">-4136</span></td>\n<td id=\"S4.T2.st1.2.2.2.5\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.2.2.5.1\" class=\"ltx_text\" style=\"color:#000000;\">-1508</span></td>\n</tr>\n<tr id=\"S4.T2.st1.2.3.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.3.3.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.3.3.1.1\" class=\"ltx_text\" style=\"color:#000000;\">TD3-MARL</span></td>\n<td id=\"S4.T2.st1.2.3.3.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.3.3.2.1\" class=\"ltx_text\" style=\"color:#000000;\">-4763</span></td>\n<td id=\"S4.T2.st1.2.3.3.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.3.3.3.1\" class=\"ltx_text\" style=\"color:#000000;\">396</span></td>\n<td id=\"S4.T2.st1.2.3.3.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.3.3.4.1\" class=\"ltx_text\" style=\"color:#000000;\">-3698</span></td>\n<td id=\"S4.T2.st1.2.3.3.5\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.3.3.5.1\" class=\"ltx_text\" style=\"color:#000000;\">-1461</span></td>\n</tr>\n<tr id=\"S4.T2.st1.2.4.4\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.4.4.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.4.4.1.1\" class=\"ltx_text\" style=\"color:#000000;\">SAC-MARL</span></td>\n<td id=\"S4.T2.st1.2.4.4.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.4.4.2.1\" class=\"ltx_text\" style=\"color:#000000;\">-4412</span></td>\n<td id=\"S4.T2.st1.2.4.4.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.4.4.3.1\" class=\"ltx_text\" style=\"color:#000000;\">519</span></td>\n<td id=\"S4.T2.st1.2.4.4.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.4.4.4.1\" class=\"ltx_text\" style=\"color:#000000;\">-3790</span></td>\n<td id=\"S4.T2.st1.2.4.4.5\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-bottom:2.84544pt;padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.4.4.5.1\" class=\"ltx_text\" style=\"color:#000000;\">-1141</span></td>\n</tr>\n<tr id=\"S4.T2.st1.2.5.5\" class=\"ltx_tr\">\n<td id=\"S4.T2.st1.2.5.5.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.5.5.1.1\" class=\"ltx_text\" style=\"color:#000000;\">FedSAC</span></td>\n<td id=\"S4.T2.st1.2.5.5.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.5.5.2.1\" class=\"ltx_text ltx_font_bold\" style=\"color:#000000;\">-3720</span></td>\n<td id=\"S4.T2.st1.2.5.5.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.5.5.3.1\" class=\"ltx_text\" style=\"color:#000000;\">550</span></td>\n<td id=\"S4.T2.st1.2.5.5.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.5.5.4.1\" class=\"ltx_text\" style=\"color:#000000;\">-3227</span></td>\n<td id=\"S4.T2.st1.2.5.5.5\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.6pt;padding-right:1.6pt;\"><span id=\"S4.T2.st1.2.5.5.5.1\" class=\"ltx_text\" style=\"color:#000000;\">-1043</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Model-free deep reinforcement algorithm (DRL) has demonstrated great potential in sequential decision-making problems without prior knowledge of the environmentâ€™s dynamics [25, 26, 27]. In DRL, the agent makes a decision for each state to obtain the reward sequence and then learns to optimize its policy to maximize expected future rewards. DRL algorithms have been widely adopted in grid management and have achieved excellent performance [28], including in EV charging control.\nIn [29], discriminative features of electricity prices are extracted through a representation network, and Q-learning is developed to obtain the optimal discrete EV charging control strategy.\nReference [30] utilizes the long short-term memory (LSTM) method to extract the trend of electricity prices. The estimated electricity price is applied in a deep deterministic policy gradient (DDPG) algorithm for continuous EV charging control policy. In [31], the traffic conditions are considered to minimize the travel time and charging costs during charging navigation. [32] formulates the EV charging control problem as a constrained MDP and develops a safe DRL method to learn the optimal scheduling strategy.\nIn [33], the authors mathematically model the driverâ€™s anxiety using statistical principles and propose using a soft actor-critic (SAC) to learn the optimal policy.\nHowever, EV charging behaviors in the same system tend to affect each other, where such mutual influence is often ignored in single-agent DRL-based approaches.\nMulti-agent deep reinforcement learning (MADRL) algorithms are more suitable for addressing EV charging control problems than single-agent ones [34]. For example, the work [35] promotes the type of tariff and develops a multi-agent multi-objective reinforcement learning framework to optimize the charging schedule. In [36], a multi-agent SAC-based framework is proposed to learn the environmentâ€™s dynamics, and its performance is verified in a simulated distribution network. In [37], the authors study the transformer constraints in a residential system and approximate the collective behaviors of EVs through a collective-policy model, using multi-agent SAC to learn the charging scheduling strategy.\nOverall, model-free DRL-based methods have proven to be superior in addressing uncertainty in EV charging control.",
            "AlgorithmÂ 1 presents charging control of EViğ‘–i based on SAC. Firstly, EViğ‘–i clears its experience replay buffer Disubscriptğ·ğ‘–D_{i} and initializes network parameters. Afterward, the EViğ‘–i trains until a stable charging strategy is obtained. During the environment step, the EV interacts with local and global environments to collect experience. In the global environment, the RDN calculates the power change gtsubscriptğ‘”ğ‘¡g_{t} at time tğ‘¡t through solving OPF in (9). It sends gtsubscriptğ‘”ğ‘¡g_{t} and real-time electricity price Î¾tsubscriptğœ‰ğ‘¡\\xi_{t} to all subordinate nodes at fixed intervals. The EV receives gtsubscriptğ‘”ğ‘¡g_{t}, and Î¾tsubscriptğœ‰ğ‘¡\\xi_{t} from RDN and then executes grid reward ri,tgsuperscriptsubscriptğ‘Ÿğ‘–ğ‘¡gr_{i,t}^{\\text{g}} through (16). The EViğ‘–i subsequently calculates the power reward ri,tpsuperscriptsubscriptğ‘Ÿğ‘–ğ‘¡pr_{i,t}^{\\text{p}} and anxiety reward ri,tasuperscriptsubscriptğ‘Ÿğ‘–ğ‘¡ar_{i,t}^{\\text{a}} through interaction with the local environment. The sum reward rtâ€‹(si,t,ai,t)subscriptğ‘Ÿğ‘¡subscriptğ‘ ğ‘–ğ‘¡subscriptğ‘ğ‘–ğ‘¡r_{t}\\left(s_{i,t},a_{i,t}\\right) is executed, and the obtained state is stored into Disubscriptğ·ğ‘–D_{i} for the local training. In the gradient step, the EViğ‘–i locally updates the parameters, i.e, (Î¸ik,Ï•i,Î±i,Î¸^ik)superscriptsubscriptğœƒğ‘–ğ‘˜subscriptitalic-Ï•ğ‘–subscriptğ›¼ğ‘–superscriptsubscript^ğœƒğ‘–ğ‘˜(\\theta_{i}^{k},\\phi_{i},\\alpha_{i},\\hat{\\theta}_{i}^{k}), by randomly sampling experience from Disubscriptğ·ğ‘–D_{i}. Here, the Î´Qsubscriptğ›¿ğ‘„\\delta_{Q}, Î´â€‹Ï€ğ›¿ğœ‹\\delta{\\pi} and Î´â€‹Î±ğ›¿ğ›¼\\delta{\\alpha} are the learning rates. Finally, the EV generates its charging control strategy."
        ]
    },
    "S4.T2.st2": {
        "caption": "(b) Standard deviation of power change and their decline ratio",
        "table": "<table id=\"S4.T2.st2.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.st2.1.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.st2.1.2.1.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:1.2pt;padding-right:1.2pt;\"></th>\n<th id=\"S4.T2.st2.1.2.1.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:1.2pt;padding-right:1.2pt;\"><span id=\"S4.T2.st2.1.2.1.2.1\" class=\"ltx_text\" style=\"color:#000000;\">FedSAC</span></th>\n<th id=\"S4.T2.st2.1.2.1.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:1.2pt;padding-right:1.2pt;\"><span id=\"S4.T2.st2.1.2.1.3.1\" class=\"ltx_text\" style=\"color:#000000;\">SAC-MADRL</span></th>\n<th id=\"S4.T2.st2.1.2.1.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:1.2pt;padding-right:1.2pt;\"><span id=\"S4.T2.st2.1.2.1.4.1\" class=\"ltx_text\" style=\"color:#000000;\">TD3-MADRL</span></th>\n<th id=\"S4.T2.st2.1.2.1.5\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:1.2pt;padding-right:1.2pt;\"><span id=\"S4.T2.st2.1.2.1.5.1\" class=\"ltx_text\" style=\"color:#000000;\">DDPG-MADRL</span></th>\n<th id=\"S4.T2.st2.1.2.1.6\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:1.2pt;padding-right:1.2pt;\"><span id=\"S4.T2.st2.1.2.1.6.1\" class=\"ltx_text\" style=\"color:#000000;\">SAC</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.st2.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.st2.1.1.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-bottom:4.55254pt;padding-left:1.2pt;padding-right:1.2pt;\">\n<math id=\"S4.T2.st2.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sigma_{g}\" display=\"inline\"><semantics id=\"S4.T2.st2.1.1.1.m1.1a\"><msub id=\"S4.T2.st2.1.1.1.m1.1.1\" xref=\"S4.T2.st2.1.1.1.m1.1.1.cmml\"><mi mathcolor=\"#000000\" id=\"S4.T2.st2.1.1.1.m1.1.1.2\" xref=\"S4.T2.st2.1.1.1.m1.1.1.2.cmml\">Ïƒ</mi><mi mathcolor=\"#000000\" id=\"S4.T2.st2.1.1.1.m1.1.1.3\" xref=\"S4.T2.st2.1.1.1.m1.1.1.3.cmml\">g</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.st2.1.1.1.m1.1b\"><apply id=\"S4.T2.st2.1.1.1.m1.1.1.cmml\" xref=\"S4.T2.st2.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S4.T2.st2.1.1.1.m1.1.1.1.cmml\" xref=\"S4.T2.st2.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S4.T2.st2.1.1.1.m1.1.1.2.cmml\" xref=\"S4.T2.st2.1.1.1.m1.1.1.2\">ğœ</ci><ci id=\"S4.T2.st2.1.1.1.m1.1.1.3.cmml\" xref=\"S4.T2.st2.1.1.1.m1.1.1.3\">ğ‘”</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.st2.1.1.1.m1.1c\">\\sigma_{g}</annotation></semantics></math><span id=\"S4.T2.st2.1.1.1.1\" class=\"ltx_text\" style=\"color:#000000;\">(1e-2)</span>\n</th>\n<td id=\"S4.T2.st2.1.1.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-bottom:4.55254pt;padding-left:1.2pt;padding-right:1.2pt;\"><span id=\"S4.T2.st2.1.1.2.1\" class=\"ltx_text ltx_font_bold\" style=\"color:#000000;\">8.63</span></td>\n<td id=\"S4.T2.st2.1.1.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-bottom:4.55254pt;padding-left:1.2pt;padding-right:1.2pt;\"><span id=\"S4.T2.st2.1.1.3.1\" class=\"ltx_text\" style=\"color:#000000;\">9.19</span></td>\n<td id=\"S4.T2.st2.1.1.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-bottom:4.55254pt;padding-left:1.2pt;padding-right:1.2pt;\"><span id=\"S4.T2.st2.1.1.4.1\" class=\"ltx_text\" style=\"color:#000000;\">12.29</span></td>\n<td id=\"S4.T2.st2.1.1.5\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-bottom:4.55254pt;padding-left:1.2pt;padding-right:1.2pt;\"><span id=\"S4.T2.st2.1.1.5.1\" class=\"ltx_text\" style=\"color:#000000;\">12.41</span></td>\n<td id=\"S4.T2.st2.1.1.6\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-bottom:4.55254pt;padding-left:1.2pt;padding-right:1.2pt;\"><span id=\"S4.T2.st2.1.1.6.1\" class=\"ltx_text\" style=\"color:#000000;\">51.63</span></td>\n</tr>\n<tr id=\"S4.T2.st2.1.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.st2.1.3.1.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:1.2pt;padding-right:1.2pt;\"><span id=\"S4.T2.st2.1.3.1.1.1\" class=\"ltx_text\" style=\"color:#000000;\">decline</span></th>\n<td id=\"S4.T2.st2.1.3.1.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.2pt;padding-right:1.2pt;\"><span id=\"S4.T2.st2.1.3.1.2.1\" class=\"ltx_text ltx_font_bold\" style=\"color:#000000;\">83.28%</span></td>\n<td id=\"S4.T2.st2.1.3.1.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.2pt;padding-right:1.2pt;\"><span id=\"S4.T2.st2.1.3.1.3.1\" class=\"ltx_text\" style=\"color:#000000;\">82.2%</span></td>\n<td id=\"S4.T2.st2.1.3.1.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.2pt;padding-right:1.2pt;\"><span id=\"S4.T2.st2.1.3.1.4.1\" class=\"ltx_text\" style=\"color:#000000;\">76.2%</span></td>\n<td id=\"S4.T2.st2.1.3.1.5\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.2pt;padding-right:1.2pt;\"><span id=\"S4.T2.st2.1.3.1.5.1\" class=\"ltx_text\" style=\"color:#000000;\">75.96%</span></td>\n<td id=\"S4.T2.st2.1.3.1.6\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.2pt;padding-right:1.2pt;\"><span id=\"S4.T2.st2.1.3.1.6.1\" class=\"ltx_text\" style=\"color:#000000;\">0%</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Model-free deep reinforcement algorithm (DRL) has demonstrated great potential in sequential decision-making problems without prior knowledge of the environmentâ€™s dynamics [25, 26, 27]. In DRL, the agent makes a decision for each state to obtain the reward sequence and then learns to optimize its policy to maximize expected future rewards. DRL algorithms have been widely adopted in grid management and have achieved excellent performance [28], including in EV charging control.\nIn [29], discriminative features of electricity prices are extracted through a representation network, and Q-learning is developed to obtain the optimal discrete EV charging control strategy.\nReference [30] utilizes the long short-term memory (LSTM) method to extract the trend of electricity prices. The estimated electricity price is applied in a deep deterministic policy gradient (DDPG) algorithm for continuous EV charging control policy. In [31], the traffic conditions are considered to minimize the travel time and charging costs during charging navigation. [32] formulates the EV charging control problem as a constrained MDP and develops a safe DRL method to learn the optimal scheduling strategy.\nIn [33], the authors mathematically model the driverâ€™s anxiety using statistical principles and propose using a soft actor-critic (SAC) to learn the optimal policy.\nHowever, EV charging behaviors in the same system tend to affect each other, where such mutual influence is often ignored in single-agent DRL-based approaches.\nMulti-agent deep reinforcement learning (MADRL) algorithms are more suitable for addressing EV charging control problems than single-agent ones [34]. For example, the work [35] promotes the type of tariff and develops a multi-agent multi-objective reinforcement learning framework to optimize the charging schedule. In [36], a multi-agent SAC-based framework is proposed to learn the environmentâ€™s dynamics, and its performance is verified in a simulated distribution network. In [37], the authors study the transformer constraints in a residential system and approximate the collective behaviors of EVs through a collective-policy model, using multi-agent SAC to learn the charging scheduling strategy.\nOverall, model-free DRL-based methods have proven to be superior in addressing uncertainty in EV charging control.",
            "AlgorithmÂ 1 presents charging control of EViğ‘–i based on SAC. Firstly, EViğ‘–i clears its experience replay buffer Disubscriptğ·ğ‘–D_{i} and initializes network parameters. Afterward, the EViğ‘–i trains until a stable charging strategy is obtained. During the environment step, the EV interacts with local and global environments to collect experience. In the global environment, the RDN calculates the power change gtsubscriptğ‘”ğ‘¡g_{t} at time tğ‘¡t through solving OPF in (9). It sends gtsubscriptğ‘”ğ‘¡g_{t} and real-time electricity price Î¾tsubscriptğœ‰ğ‘¡\\xi_{t} to all subordinate nodes at fixed intervals. The EV receives gtsubscriptğ‘”ğ‘¡g_{t}, and Î¾tsubscriptğœ‰ğ‘¡\\xi_{t} from RDN and then executes grid reward ri,tgsuperscriptsubscriptğ‘Ÿğ‘–ğ‘¡gr_{i,t}^{\\text{g}} through (16). The EViğ‘–i subsequently calculates the power reward ri,tpsuperscriptsubscriptğ‘Ÿğ‘–ğ‘¡pr_{i,t}^{\\text{p}} and anxiety reward ri,tasuperscriptsubscriptğ‘Ÿğ‘–ğ‘¡ar_{i,t}^{\\text{a}} through interaction with the local environment. The sum reward rtâ€‹(si,t,ai,t)subscriptğ‘Ÿğ‘¡subscriptğ‘ ğ‘–ğ‘¡subscriptğ‘ğ‘–ğ‘¡r_{t}\\left(s_{i,t},a_{i,t}\\right) is executed, and the obtained state is stored into Disubscriptğ·ğ‘–D_{i} for the local training. In the gradient step, the EViğ‘–i locally updates the parameters, i.e, (Î¸ik,Ï•i,Î±i,Î¸^ik)superscriptsubscriptğœƒğ‘–ğ‘˜subscriptitalic-Ï•ğ‘–subscriptğ›¼ğ‘–superscriptsubscript^ğœƒğ‘–ğ‘˜(\\theta_{i}^{k},\\phi_{i},\\alpha_{i},\\hat{\\theta}_{i}^{k}), by randomly sampling experience from Disubscriptğ·ğ‘–D_{i}. Here, the Î´Qsubscriptğ›¿ğ‘„\\delta_{Q}, Î´â€‹Ï€ğ›¿ğœ‹\\delta{\\pi} and Î´â€‹Î±ğ›¿ğ›¼\\delta{\\alpha} are the learning rates. Finally, the EV generates its charging control strategy."
        ]
    }
}