{
    "id_table_1": {
        "caption": "Table 1 :  Statistics of the RAG benchmark. This benchmark is repurposed from public datasets across 10 domains, containing 4,162 questions. For the domains of Finance, Lifestyle, Recreation, Technology, Science and Novel, the short answers are extended to long-form answers with GPT-4.",
        "table": "S4.T1.4.1",
        "footnotes": [],
        "references": [
            "We prepare each sample in our benchmark dataset in the format of a tuple   q , D , g  t  q D g t \\left<q,D,gt\\right>  italic_q , italic_D , italic_g italic_t   representing query, documents, and ground-truth answer, where query is the input question to a RAG system, documents form the database providing possible context and are processed into chunks with the same number of tokens, and ground-truth answer is a complete and correct answer for the input question. Further information is provided in Sec.  4.1 .",
            "As illustrated in Fig.  1 , a response generated by a RAG system might be a mixture of correct (      ) and incorrect claims (      ), while also missing some in-ground-truth claims (      ). In this sense, evaluating responses at a finer granularity is crucial to comprehensively assess the quality of an answer. For this purpose, we introduce two components: 1) a text-to-claim extractor that decomposes a given text  T T T italic_T  into a set of claims  { c i } subscript c i \\{c_{i}\\} { italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } , and 2) a claim-entailment checker to determine whether a given claim  c c c italic_c  is entailed (  \\in  ) in a reference text  R  e  f R e f Ref italic_R italic_e italic_f  or not (  \\notin  ).",
            "Note that for simplicity we group the two noise sensitivities in Fig.  1 , but later in Sec.  4.3  we can see that generators generally has different sensitivity to relevant and irrelevant noise.",
            "For comprehensive evaluations, we curate a benchmark containing 4,162 queries across 10 domains. This benchmark is repurposed from public datasets of open domain question answering, spanning domains of Wikipedia, AI science, novel, biomedical, finance, lifestyle, recreation, science, technology and writing. We convert the short answers to long-form answers in the datasets to align with the current LLM-based RAG systems. Please refer to Appendix  A  for the details of the benchmark curation process. The statistics of the benchmark are shown in Tab.  1 .",
            "Meta Evaluation Dataset  All baseline metrics are designed with different aspects and functionalities to a certain degree, thus making an exact comparison over metric scores inapplicable. However, we argue that a good metric should reflect the relative human preference over different RAG systems.  In this spirit, we construct the meta evaluation dataset with sampled instances from the generated responses of 8 baseline RAG systems introduced in Sec.  4.1  on our benchmark. Each meta evaluation instance is a pair of responses from two baseline RAG systems given the same query. By considering all combinations over 10 domains and 28 baseline pairs, we end up with 280 instances for pairwise human preference labeling.  For each instance, annotators compare a pair of responses based on correctness, completeness, and overall assessment. For each aspect, annotators measure their preferences as one out of five relative choices, including significantly better, slightly better, tie, slightly worse and significantly worse.  For quality control, each instance is annotated by two annotators, and their overall agreement and correlation are measured. To conclude, we build a meta evaluation dataset with 280 instances, each instance is labeled by two annotators with their preference in terms of correctness, completeness and overall assessment.",
            "In this section, we introduce the benchmark datasets and the curation process for RAG evaluation. This benchmark datasets are derived from existing open-domain question answering (ODQA) datasets, including RobustQA  [ 9 ] , KIWI  [ 51 ] , ClapNQ  [ 34 ] , and NovelQA  [ 47 ] . However, most of the ground truth answers in existing ODQA datasets are short answers, while the answers provided by modern LLM-based RAG systems tend to be long-form answers. Therefore, we repurpose the ODQA datasets by eliminating overly simple questions and converting the short answers into long-form answers to match the capabilities of current RAG systems. The statistics of the benchmark are summarized in Tab.  1 . In the rest of this section, we describe the datasets we use and the curation process for each domain.",
            "The detailed evaluation results for all our benchmark datasets can be found in Tab.  6  to Tab.  15 .",
            "According to our results in Fig.  10 , higher overlap ratios generally lead to improved  context precision . However, this does not necessarily translate to an increase in the total amount of useful information retrieved. This phenomenon can be attributed to the retrieval of more chunks that contain the same segment of useful information. Consequently, we observed that overlap ratio adjustments do not have a significant impact on other performance metrics in a consistent and obvious manner. This suggests that the overlap ratio may not require extensive tuning in practice.",
            "We use Llama3-70B-Instruct for the extractor and checker in RefChecker. To validate the effectiveness of this combination, we test its performance on the RefChecker benchmark. As shown in Tab.  16 , Llama 3 based RefChecker outperforms the best purely open-sourced combinations reported in the RefChecker paper in all the three context settings."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Correlation results with Human Evaluation of Correctness, Completeness, and Overall Assessment. We only show the metric with the best correlation for each baseline framework. Full results can be found in Tab.  5  of Appendix  C .",
        "table": "S4.T2.4.1",
        "footnotes": [],
        "references": [
            "Another line of work focused on assessing end-to-end quality scores of RAG systems. TruLens  [ 6 ]  introduced the concept of RAG Triad, which decompose the quality scores into three aspects: context relevance, groundedness and answer relevance, then predicted the score by prompting LLMs or using NLI models. RAGAS  [ 5 ]  and ARES  [ 35 ]  followed the RAG Triad concept and improved the score prediction approaches on different datasets. CRUD-RAG  [ 25 ]  refered to the CRUD (Create, Read, Update and Delete) actions between users and knowledge bases to develop corresponding datasets and evaluation metrics for RAG systems. We compare the above four evaluation frameworks with  RagChecker  in the meta evaluation of Sec.  4.2 .",
            "Meta Evaluation Process and Results  Based on the meta evaluation dataset, we perform the following evaluation process. Since the human preference labels can be seen as the score difference of a response pair:  h i = H  ( r i 2 )  H  ( r i 1 )  {  2 ,  1 , 0 , 1 , 2 } subscript h i H superscript subscript r i 2 H superscript subscript r i 1 2 1 0 1 2 h_{i}=H(r_{i}^{2})-H(r_{i}^{1})\\in\\{-2,-1,0,1,2\\} italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_H ( italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) - italic_H ( italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT )  { - 2 , - 1 , 0 , 1 , 2 } , with a baseline RAG evaluation model  E E E italic_E , we compute a normalized score difference as  e i = f  ( E  ( r i 2 )  E  ( r i 1 ) )  [  2 , 2 ] subscript e i f E superscript subscript r i 2 E superscript subscript r i 1 2 2 e_{i}=f(E(r_{i}^{2})-E(r_{i}^{1}))\\in[-2,2] italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_f ( italic_E ( italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) - italic_E ( italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) )  [ - 2 , 2 ] , where  f f f italic_f  is a linear normalization function. Our meta evaluation is the correlation between  h i subscript h i h_{i} italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and  e i subscript e i e_{i} italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  overall 280 instances as reported in Tab.  2 , together with the correlation between  h i subscript h i h_{i} italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and  h i  superscript subscript h i  h_{i}^{\\prime} italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  from two annotators as the upper-bound. In addition, we further compute human agreement rate as the proportion of instances satisfying  abs  ( h i  h i  )  1 abs subscript h i superscript subscript h i  1 \\text{abs}(h_{i}-h_{i}^{\\prime})\\leq 1 abs ( italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT )  1 , and the result is 90.95%.",
            "We employ GPT-4 ( gpt-4-turbo-2024-04-09 ) to convert the human annotated short answers to long-form answers in the dataset of RobustQA and NovelQA. For RobustQA, the short answers are spans of the annotated ground truth passages, we take all the annotated short answers and the corresponding passages in the prompt and ask GPT-4 to convert them to one single long-form answer. For NovelQA, we take the human written evidences as the ground truth passage content and the human written short answers for the long-form answer generation. The prompt is shown in Fig.  2 .",
            "The 10 metrics included in the meta evaluation are selected from Trulens  [ 6 ] , RAGAS  [ 5 ] , ARES  [ 35 ]  and CRUD-RAG  [ 25 ]  as explained in Sec.  4.2 . Their descriptions are summarized in Tab.  4 .  As a supplement of Tab.  2 , the full correlation results of meta evaluation is shown in Tab.  5 .  For a detailed comparison between  RagChecker  and the strongest baseline metric, RAGAS Answer Similarity, we plot the prediction score distribution of two metrics in Fig.  4 . From the prediction score distribution and the mean line (dashed line) of the plot, we can observe a stronger correlation of  RagChecker  than RAGAS Answer Similarity."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  The averaged evaluation results for different RAG systems across 10 datasets. The overall performance of the RAG system is quantified using precision (Prec.), recall (Rec.), and F1 scores. The retriever component is evaluated based on claim recall (CR) and context precision (CP), while the generator component is diagnosed through context utilization (CU), relevant noise sensitivity (NS(I)), irrelevant noise sensitivity (NS(II)), hallucination (Hallu.), self-knowledge (SK), and faithfulness (Faith.). Additionally, the average number of response claims for each RAG system is provided.",
        "table": "S4.T3.11.11",
        "footnotes": [],
        "references": [
            "To ensure the reliability of  RagChecker , we annotate a human judgment dataset to assess the correlations between the proposed metrics and human judgments. This meta-evaluation validates the effectiveness of  RagChecker  in capturing the quality and reliability of RAG systems from a human perspective.  We demonstrate the effectiveness of  RagChecker  through comprehensive experiments evaluating 8 state-of-the-art RAG systems on a benchmark repurposed from public datasets across 10 domains. In-depth analysis of the evaluation results reveals that  RagChecker  provides insightful diagnostic signals (Sec.  4.3 ) pointing the directions for improvements of RAG systems (Sec.  4.4 ).",
            "Within the two components of a RAG system, the retriever has been well studied in recent years, thus a line of recent work focused on evaluating essential generator capabilities.  RGB  [ 4 ]  evaluated 4 fundamental abilities required for generators including Noise Robustness, Negative Rejection, Information Integration and Counterfactual Robustness by manually constructed test sets.  RECALL  [ 22 ]  introduced manually edited counterfactual contexts into QA and text generation datasets to evaluate the counterfactual robustness of LLMs.  NoMIRACL  [ 40 ]  evaluated LLMs robustness against first-stage retrieval errors of RAG systems with manually judged relevant and non-relevant datasets.  Wu et al.  [ 49 ]  quantified the tug-of-war between LLMs faithfulness and internal prior by introducing varying levels of perturbations on the provided contexts.  FaaF  [ 15 ]  introduced a fine-grained fact verification formulation to improve previous prompting-based approaches in evaluating factuality of generators.  However, we argue that above generator-only evaluation approaches with manually constructed datasets cannot serve as a general RAG evaluation framework to reveal the entanglement of between generation results and different retrieval behaviors, as shown in the analysis of Sec.  4.3 .",
            "Note that for simplicity we group the two noise sensitivities in Fig.  1 , but later in Sec.  4.3  we can see that generators generally has different sensitivity to relevant and irrelevant noise.",
            "We present the averaged evaluation results for 8 RAG systems across 10 diverse domain datasets in Tab.  3 . Additional results for all datasets are provided in Appendix  E . The RAG system that exhibited the best performance in our experiments is E5-Mistral_GPT-4, owing to the strong retrieval capability of E5-Mistral coupled with the adept comprehension abilities of GPT-4. Next, we provide a list of insights induced from Tab.  3 , along with their interpretation and possible directions for improvements.",
            "Guided by observations in Sec.  4.3 , we modify settings commonly tuned in RAG systems that may lead to improvements, diagnose their working mechanisms with  RagChecker  metrics, and provide suggestions for improvements on certain aspects. We experiment with different numbers of chunks, chunk sizes, chunk overlap ratios, and generation prompts. We highlight our main findings and suggestions as below, please refer to Appendix  F  for detailed analysis and results.",
            "In accordance with the definitions provided in Section  3.3 , we compute each metric using the following formulations:",
            "Annotators are required to choose their preference from five options: significantly better, slightly better, tie, slightly worse, or significantly worse. The annotation is based on three metrics: correctness, completeness, and overall assessment. The annotation interface with instructions are shown in Fig.  3",
            "For the two generators, GPT-4 generally showes improvements in metrics related to faithfulness ( hallucination ,  self-knowledge ,  faithfulness ), whereas Llama3 does not exhibit the same behavior. This aligns with our previous observation (Sec.  4.3 ) that Llama3 already performs well on  faithfulness , while GPT-4 tends to rely on self-knowledge without explicit requirements. Consequently, there is a steady improvement in overall  F1  for GPT-4 when switched to the optimized prompt, while the difference for Llama3 is negligible."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  Summary of the metrics included in the meta evaluation.",
        "table": "A3.T4.4.1",
        "footnotes": [],
        "references": [
            "To ensure the reliability of  RagChecker , we annotate a human judgment dataset to assess the correlations between the proposed metrics and human judgments. This meta-evaluation validates the effectiveness of  RagChecker  in capturing the quality and reliability of RAG systems from a human perspective.  We demonstrate the effectiveness of  RagChecker  through comprehensive experiments evaluating 8 state-of-the-art RAG systems on a benchmark repurposed from public datasets across 10 domains. In-depth analysis of the evaluation results reveals that  RagChecker  provides insightful diagnostic signals (Sec.  4.3 ) pointing the directions for improvements of RAG systems (Sec.  4.4 ).",
            "Within the two components of a RAG system, the retriever has been well studied in recent years, thus a line of recent work focused on evaluating essential generator capabilities.  RGB  [ 4 ]  evaluated 4 fundamental abilities required for generators including Noise Robustness, Negative Rejection, Information Integration and Counterfactual Robustness by manually constructed test sets.  RECALL  [ 22 ]  introduced manually edited counterfactual contexts into QA and text generation datasets to evaluate the counterfactual robustness of LLMs.  NoMIRACL  [ 40 ]  evaluated LLMs robustness against first-stage retrieval errors of RAG systems with manually judged relevant and non-relevant datasets.  Wu et al.  [ 49 ]  quantified the tug-of-war between LLMs faithfulness and internal prior by introducing varying levels of perturbations on the provided contexts.  FaaF  [ 15 ]  introduced a fine-grained fact verification formulation to improve previous prompting-based approaches in evaluating factuality of generators.  However, we argue that above generator-only evaluation approaches with manually constructed datasets cannot serve as a general RAG evaluation framework to reveal the entanglement of between generation results and different retrieval behaviors, as shown in the analysis of Sec.  4.3 .",
            "Another line of work focused on assessing end-to-end quality scores of RAG systems. TruLens  [ 6 ]  introduced the concept of RAG Triad, which decompose the quality scores into three aspects: context relevance, groundedness and answer relevance, then predicted the score by prompting LLMs or using NLI models. RAGAS  [ 5 ]  and ARES  [ 35 ]  followed the RAG Triad concept and improved the score prediction approaches on different datasets. CRUD-RAG  [ 25 ]  refered to the CRUD (Create, Read, Update and Delete) actions between users and knowledge bases to develop corresponding datasets and evaluation metrics for RAG systems. We compare the above four evaluation frameworks with  RagChecker  in the meta evaluation of Sec.  4.2 .",
            "We prepare each sample in our benchmark dataset in the format of a tuple   q , D , g  t  q D g t \\left<q,D,gt\\right>  italic_q , italic_D , italic_g italic_t   representing query, documents, and ground-truth answer, where query is the input question to a RAG system, documents form the database providing possible context and are processed into chunks with the same number of tokens, and ground-truth answer is a complete and correct answer for the input question. Further information is provided in Sec.  4.1 .",
            "Note that for simplicity we group the two noise sensitivities in Fig.  1 , but later in Sec.  4.3  we can see that generators generally has different sensitivity to relevant and irrelevant noise.",
            "Baseline RAG Evaluation Frameworks  We include a total of 10 metrics from Trulens  [ 6 ] , RAGAS  [ 5 ] , ARES  [ 35 ]  and CRUD-RAG  [ 25 ]  in the meta evaluation, as they are capable to evaluate end-to-end performance with long answers. Metrics selected for comparison along with their descriptions are summarized in Tab.  4  of Appendix  C .  To ensure a fair comparison, we use Llama3-70B-Instruct as the LLM backbone when applicable. Since models in the Llama3 family dont provide an embedding model, baseline metrics requiring embedding capability still use their corresponding default LLM backbones.  In addition to the 10 metrics detailed in the table, we also incorporate BLEU  [ 31 ] , ROUGE-L  [ 20 ] , and BERTScore  [ 56 ]  to assess the correlation between the generated responses and the ground truth answers.",
            "Meta Evaluation Dataset  All baseline metrics are designed with different aspects and functionalities to a certain degree, thus making an exact comparison over metric scores inapplicable. However, we argue that a good metric should reflect the relative human preference over different RAG systems.  In this spirit, we construct the meta evaluation dataset with sampled instances from the generated responses of 8 baseline RAG systems introduced in Sec.  4.1  on our benchmark. Each meta evaluation instance is a pair of responses from two baseline RAG systems given the same query. By considering all combinations over 10 domains and 28 baseline pairs, we end up with 280 instances for pairwise human preference labeling.  For each instance, annotators compare a pair of responses based on correctness, completeness, and overall assessment. For each aspect, annotators measure their preferences as one out of five relative choices, including significantly better, slightly better, tie, slightly worse and significantly worse.  For quality control, each instance is annotated by two annotators, and their overall agreement and correlation are measured. To conclude, we build a meta evaluation dataset with 280 instances, each instance is labeled by two annotators with their preference in terms of correctness, completeness and overall assessment.",
            "From the table, we can observe that  RagChecker  has the strongest correlation with human preference in terms of three aspects. Among other baseline metrics, Answer Similarity of RAGAS, which is based on the stronger backbone model text-embedding-ada-002  [ 28 ] , shows the best performance. We also provide a detailed comparison between  RagChecker  and this strongest baseline in Fig.  4  of Appendix  C . As an upper bound, the human correlations at the bottom show that there is still a clear gap between model predictions and human annotators.",
            "Guided by observations in Sec.  4.3 , we modify settings commonly tuned in RAG systems that may lead to improvements, diagnose their working mechanisms with  RagChecker  metrics, and provide suggestions for improvements on certain aspects. We experiment with different numbers of chunks, chunk sizes, chunk overlap ratios, and generation prompts. We highlight our main findings and suggestions as below, please refer to Appendix  F  for detailed analysis and results.",
            "The 10 metrics included in the meta evaluation are selected from Trulens  [ 6 ] , RAGAS  [ 5 ] , ARES  [ 35 ]  and CRUD-RAG  [ 25 ]  as explained in Sec.  4.2 . Their descriptions are summarized in Tab.  4 .  As a supplement of Tab.  2 , the full correlation results of meta evaluation is shown in Tab.  5 .  For a detailed comparison between  RagChecker  and the strongest baseline metric, RAGAS Answer Similarity, we plot the prediction score distribution of two metrics in Fig.  4 . From the prediction score distribution and the mean line (dashed line) of the plot, we can observe a stronger correlation of  RagChecker  than RAGAS Answer Similarity.",
            "For the two generators, GPT-4 generally showes improvements in metrics related to faithfulness ( hallucination ,  self-knowledge ,  faithfulness ), whereas Llama3 does not exhibit the same behavior. This aligns with our previous observation (Sec.  4.3 ) that Llama3 already performs well on  faithfulness , while GPT-4 tends to rely on self-knowledge without explicit requirements. Consequently, there is a steady improvement in overall  F1  for GPT-4 when switched to the optimized prompt, while the difference for Llama3 is negligible."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 :  Full Correlation results with Human Evaluation of Correctness, Completeness, and Overall Assessment",
        "table": "A3.T5.4.1",
        "footnotes": [],
        "references": [
            "The 10 metrics included in the meta evaluation are selected from Trulens  [ 6 ] , RAGAS  [ 5 ] , ARES  [ 35 ]  and CRUD-RAG  [ 25 ]  as explained in Sec.  4.2 . Their descriptions are summarized in Tab.  4 .  As a supplement of Tab.  2 , the full correlation results of meta evaluation is shown in Tab.  5 .  For a detailed comparison between  RagChecker  and the strongest baseline metric, RAGAS Answer Similarity, we plot the prediction score distribution of two metrics in Fig.  4 . From the prediction score distribution and the mean line (dashed line) of the plot, we can observe a stronger correlation of  RagChecker  than RAGAS Answer Similarity.",
            "We adopt OpenSearch 4 4 4 https://opensearch.org/  as the tool to implement the inverted index for BM25 and the approximate KNN search for dense retrieval. We use a g5.48xlarge instance with 8 NVIDIA A10G GPUs on AWS for inference of open-source models. We split documents in the corpus to chunks of 300 tokens with an overlap ratio of 0.2 by default. We use the tokenizer of E5-Mistral for both retrievers to control the chunking. For each query, top-20 chunks ranked by retrievers are used as context for LLM generation. The default prompt for all generators is shown in Fig.  5 . We set the generation temperature to 0.0 (deterministic) and the maximum generation length to 2,048 tokens when calling proprietary LLMs.",
            "The detailed evaluation results for all our benchmark datasets can be found in Tab.  6  to Tab.  15 ."
        ]
    },
    "id_table_6": {
        "caption": "Table 6 :  Evaluation results for different RAG systems on ClapNQ dataset",
        "table": "A9.T6.11.11",
        "footnotes": [],
        "references": [
            "The detailed evaluation results for all our benchmark datasets can be found in Tab.  6  to Tab.  15 .",
            "Top-k selection and chunk size both balance the amount of noise and useful information presented to the generator, but in different manners. Corresponding results are demonstrated in Fig.  6  and Fig.  7 . Increasing  k k k italic_k  adds more context that could be less relevant, while increasing chunk size provides more surrounding context of relevant facts. Thus  context precision  decreases with larger  k k k italic_k  but increases with larger chunk sizes. Despite this, they both lead to better  claim recall  in Retrieval.",
            "We use Llama3-70B-Instruct for the extractor and checker in RefChecker. To validate the effectiveness of this combination, we test its performance on the RefChecker benchmark. As shown in Tab.  16 , Llama 3 based RefChecker outperforms the best purely open-sourced combinations reported in the RefChecker paper in all the three context settings."
        ]
    },
    "id_table_7": {
        "caption": "Table 7 :  Evaluation results for different RAG systems on NovelQA dataset",
        "table": "A9.T7.11.11",
        "footnotes": [],
        "references": [
            "Top-k selection and chunk size both balance the amount of noise and useful information presented to the generator, but in different manners. Corresponding results are demonstrated in Fig.  6  and Fig.  7 . Increasing  k k k italic_k  adds more context that could be less relevant, while increasing chunk size provides more surrounding context of relevant facts. Thus  context precision  decreases with larger  k k k italic_k  but increases with larger chunk sizes. Despite this, they both lead to better  claim recall  in Retrieval."
        ]
    },
    "id_table_8": {
        "caption": "Table 8 :  Evaluation results for different RAG systems on RobustQA - Writing dataset",
        "table": "A9.T8.11.11",
        "footnotes": [],
        "references": [
            "As shown in Fig.  8 , we observed a general improvement in  context utilization . However, as a counterpart to  context utilization ,  noise sensitivity  generally worsened. It demonstrates the difficulty of meeting all prompt requirements when there are subtle tension between them."
        ]
    },
    "id_table_9": {
        "caption": "Table 9 :  Evaluation results for different RAG systems on RobustQA - BioASQ dataset",
        "table": "A9.T9.11.11",
        "footnotes": [],
        "references": [
            "To validate the effect of the generation prompt, we added more detailed requirements to guide the generation for better  faithfulness ,  context utilization , and lower  noise sensitivity . The optimized prompt is shown in Fig.  9 ."
        ]
    },
    "id_table_10": {
        "caption": "Table 10 :  Evaluation results for different RAG systems on RobustQA - Finance dataset",
        "table": "A9.T10.11.11",
        "footnotes": [],
        "references": [
            "According to our results in Fig.  10 , higher overlap ratios generally lead to improved  context precision . However, this does not necessarily translate to an increase in the total amount of useful information retrieved. This phenomenon can be attributed to the retrieval of more chunks that contain the same segment of useful information. Consequently, we observed that overlap ratio adjustments do not have a significant impact on other performance metrics in a consistent and obvious manner. This suggests that the overlap ratio may not require extensive tuning in practice."
        ]
    },
    "id_table_11": {
        "caption": "Table 11 :  Evaluation results for different RAG systems on RobustQA - Lifestyle dataset",
        "table": "A9.T11.11.11",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "Table 12 :  Evaluation results for different RAG systems on RobustQA - Recreation dataset",
        "table": "A9.T12.11.11",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "Table 13 :  Evaluation results for different RAG systems on RobustQA - Science dataset",
        "table": "A9.T13.11.11",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "Table 14 :  Evaluation results for different RAG systems on RobustQA - Technology dataset",
        "table": "A9.T14.11.11",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "Table 15 :  Evaluation results for different RAG systems on KIWI dataset",
        "table": "A9.T15.11.11",
        "footnotes": [],
        "references": [
            "The detailed evaluation results for all our benchmark datasets can be found in Tab.  6  to Tab.  15 ."
        ]
    },
    "id_table_16": {
        "caption": "Table 16 :  Performance of RefChecker on the RefChecker benchmark using Llama 3 70B Instruct as both the extractor and checker. We compare the results with the best performed purely open-sourced combinations reported in the RefChecker paper.",
        "table": "A9.T16.4",
        "footnotes": [],
        "references": [
            "We use Llama3-70B-Instruct for the extractor and checker in RefChecker. To validate the effectiveness of this combination, we test its performance on the RefChecker benchmark. As shown in Tab.  16 , Llama 3 based RefChecker outperforms the best purely open-sourced combinations reported in the RefChecker paper in all the three context settings."
        ]
    },
    "global_footnotes": [
        "This work has been open sourced at",
        "We omit chunk size of 600 for E5-Mistral_Llama3-70B due to the limited 8K context window of Llama3."
    ]
}