{
    "S6.T1": {
        "caption": "Table 1:  Results of fine-tuning models with varying percentages of token replacement",
        "table": "<table id=\"S6.T1.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S6.T1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S6.T1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">%</span></td>\n<td id=\"S6.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S6.T1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Data Size</span></td>\n<td id=\"S6.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S6.T1.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Avg Loss</span></td>\n<td id=\"S6.T1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S6.T1.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">Training Time in Hrs</span></td>\n<td id=\"S6.T1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S6.T1.1.1.1.5.1\" class=\"ltx_text ltx_font_bold\">Prplx</span></td>\n</tr>\n<tr id=\"S6.T1.1.2.2\" class=\"ltx_tr\">\n<td id=\"S6.T1.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">0%</td>\n<td id=\"S6.T1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">52,640</td>\n<td id=\"S6.T1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.5436</td>\n<td id=\"S6.T1.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">9.75</td>\n<td id=\"S6.T1.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4.1206</td>\n</tr>\n<tr id=\"S6.T1.1.3.3\" class=\"ltx_tr\">\n<td id=\"S6.T1.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">10%</td>\n<td id=\"S6.T1.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">105,280</td>\n<td id=\"S6.T1.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.5306</td>\n<td id=\"S6.T1.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">18.2</td>\n<td id=\"S6.T1.1.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.6771</td>\n</tr>\n<tr id=\"S6.T1.1.4.4\" class=\"ltx_tr\">\n<td id=\"S6.T1.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">15%</td>\n<td id=\"S6.T1.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">105,280</td>\n<td id=\"S6.T1.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.6059</td>\n<td id=\"S6.T1.1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">18</td>\n<td id=\"S6.T1.1.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.7867</td>\n</tr>\n<tr id=\"S6.T1.1.5.5\" class=\"ltx_tr\">\n<td id=\"S6.T1.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">20%</td>\n<td id=\"S6.T1.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">105,280</td>\n<td id=\"S6.T1.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.6646</td>\n<td id=\"S6.T1.1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">18.2</td>\n<td id=\"S6.T1.1.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.8593</td>\n</tr>\n<tr id=\"S6.T1.1.6.6\" class=\"ltx_tr\">\n<td id=\"S6.T1.1.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">25%</td>\n<td id=\"S6.T1.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">105,280</td>\n<td id=\"S6.T1.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.7156</td>\n<td id=\"S6.T1.1.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">18.8</td>\n<td id=\"S6.T1.1.6.6.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.9142</td>\n</tr>\n<tr id=\"S6.T1.1.7.7\" class=\"ltx_tr\">\n<td id=\"S6.T1.1.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">30%</td>\n<td id=\"S6.T1.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">105,280</td>\n<td id=\"S6.T1.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1.7610</td>\n<td id=\"S6.T1.1.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">17.8</td>\n<td id=\"S6.T1.1.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">3.9574</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "For DialoGPT, we fine-tuned a baseline model and five augmented models with varying percentages of token replacement. The baseline model was fine-tuned with all 53K untouched conversations from the corpus and zero synthetic data for approximately 10 hours. The augmented models were fine-tuned with a merged corpus of original data (53K) and synthetic data (53K) for approximately 18 hours each. The varying percentages of RoBERTa-generated tokens present in the synthetic dataset constitutes the only independent variable for the augmented models. We maintain identical training configurations for all the experiments. The specific finetuning scores are shown in Table 1."
        ]
    },
    "S6.T2": {
        "caption": "Table 2:  Results of fine-tuning models in varying training data sizes",
        "table": "<table id=\"S6.T2.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T2.1.1.1\" class=\"ltx_tr\">\n<td id=\"S6.T2.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Method</td>\n<td id=\"S6.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Data Size</td>\n<td id=\"S6.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Avg Loss</td>\n<td id=\"S6.T2.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Training Time in Hrs</td>\n<td id=\"S6.T2.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Prplx</td>\n</tr>\n<tr id=\"S6.T2.1.2.2\" class=\"ltx_tr\">\n<td id=\"S6.T2.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">1K Base</td>\n<td id=\"S6.T2.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1,000</td>\n<td id=\"S6.T2.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.4788</td>\n<td id=\"S6.T2.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.17</td>\n<td id=\"S6.T2.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">9.5821</td>\n</tr>\n<tr id=\"S6.T2.1.3.3\" class=\"ltx_tr\">\n<td id=\"S6.T2.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">1K Aug</td>\n<td id=\"S6.T2.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2,000</td>\n<td id=\"S6.T2.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.3005</td>\n<td id=\"S6.T2.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.35</td>\n<td id=\"S6.T2.1.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">8.4791</td>\n</tr>\n<tr id=\"S6.T2.1.4.4\" class=\"ltx_tr\">\n<td id=\"S6.T2.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\">10K Base</td>\n<td id=\"S6.T2.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">10,000</td>\n<td id=\"S6.T2.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">1.8721</td>\n<td id=\"S6.T2.1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">1.6</td>\n<td id=\"S6.T2.1.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">6.1937</td>\n</tr>\n<tr id=\"S6.T2.1.5.5\" class=\"ltx_tr\">\n<td id=\"S6.T2.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">10K Aug</td>\n<td id=\"S6.T2.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">20,000</td>\n<td id=\"S6.T2.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.8311</td>\n<td id=\"S6.T2.1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.4</td>\n<td id=\"S6.T2.1.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">5.755</td>\n</tr>\n<tr id=\"S6.T2.1.6.6\" class=\"ltx_tr\">\n<td id=\"S6.T2.1.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\">25K Base</td>\n<td id=\"S6.T2.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">25,000</td>\n<td id=\"S6.T2.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">1.7009</td>\n<td id=\"S6.T2.1.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">4.3</td>\n<td id=\"S6.T2.1.6.6.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">4.9905</td>\n</tr>\n<tr id=\"S6.T2.1.7.7\" class=\"ltx_tr\">\n<td id=\"S6.T2.1.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">25K Aug</td>\n<td id=\"S6.T2.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">50,000</td>\n<td id=\"S6.T2.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1.6780</td>\n<td id=\"S6.T2.1.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">9.16</td>\n<td id=\"S6.T2.1.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">4.5684</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "As part our ablations below, we fine-tuned another six models with decreased training data sizes following the same training configurations. Finetuning results are displayed in Table 2."
        ]
    },
    "S8.T3": {
        "caption": "Table 3: Perplexity, F1, and Function Word and Content Word count for the models with different token replacement percentages",
        "table": "<table id=\"S8.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S8.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S8.T3.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S8.T3.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">%</span></th>\n<th id=\"S8.T3.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S8.T3.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Prplx</span></th>\n<th id=\"S8.T3.1.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S8.T3.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">F1</span></th>\n<th id=\"S8.T3.1.1.1.4\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S8.T3.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">Func.</span></th>\n<th id=\"S8.T3.1.1.1.5\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S8.T3.1.1.1.5.1\" class=\"ltx_text ltx_font_bold\">Cont.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S8.T3.1.2.1\" class=\"ltx_tr\">\n<th id=\"S8.T3.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">0%</th>\n<td id=\"S8.T3.1.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_t\">4.1206</td>\n<td id=\"S8.T3.1.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.3379</td>\n<td id=\"S8.T3.1.2.1.4\" class=\"ltx_td ltx_align_left ltx_border_t\">82,956</td>\n<td id=\"S8.T3.1.2.1.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">100,940</td>\n</tr>\n<tr id=\"S8.T3.1.3.2\" class=\"ltx_tr\">\n<th id=\"S8.T3.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">10%</th>\n<td id=\"S8.T3.1.3.2.2\" class=\"ltx_td ltx_align_left ltx_border_t\">3.6771</td>\n<td id=\"S8.T3.1.3.2.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.3794</td>\n<td id=\"S8.T3.1.3.2.4\" class=\"ltx_td ltx_align_left ltx_border_t\">88,944</td>\n<td id=\"S8.T3.1.3.2.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">112,776</td>\n</tr>\n<tr id=\"S8.T3.1.4.3\" class=\"ltx_tr\">\n<th id=\"S8.T3.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">15%</th>\n<td id=\"S8.T3.1.4.3.2\" class=\"ltx_td ltx_align_left ltx_border_t\">3.7867</td>\n<td id=\"S8.T3.1.4.3.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.3762</td>\n<td id=\"S8.T3.1.4.3.4\" class=\"ltx_td ltx_align_left ltx_border_t\">88,175</td>\n<td id=\"S8.T3.1.4.3.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">116,501</td>\n</tr>\n<tr id=\"S8.T3.1.5.4\" class=\"ltx_tr\">\n<th id=\"S8.T3.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">20%</th>\n<td id=\"S8.T3.1.5.4.2\" class=\"ltx_td ltx_align_left ltx_border_t\">3.8593</td>\n<td id=\"S8.T3.1.5.4.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.3740</td>\n<td id=\"S8.T3.1.5.4.4\" class=\"ltx_td ltx_align_left ltx_border_t\">87,910</td>\n<td id=\"S8.T3.1.5.4.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">114,018</td>\n</tr>\n<tr id=\"S8.T3.1.6.5\" class=\"ltx_tr\">\n<th id=\"S8.T3.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">25%</th>\n<td id=\"S8.T3.1.6.5.2\" class=\"ltx_td ltx_align_left ltx_border_t\">3.9142</td>\n<td id=\"S8.T3.1.6.5.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.3719</td>\n<td id=\"S8.T3.1.6.5.4\" class=\"ltx_td ltx_align_left ltx_border_t\">88,467</td>\n<td id=\"S8.T3.1.6.5.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">113,473</td>\n</tr>\n<tr id=\"S8.T3.1.7.6\" class=\"ltx_tr\">\n<th id=\"S8.T3.1.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">30%</th>\n<td id=\"S8.T3.1.7.6.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\">3.9574</td>\n<td id=\"S8.T3.1.7.6.3\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\">0.3704</td>\n<td id=\"S8.T3.1.7.6.4\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\">86,856</td>\n<td id=\"S8.T3.1.7.6.5\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">110,135</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "An ablation was performed to determine the effect of varying token replacement percentages on the final performance of the model. The evaluation scores can be seen in Table 3",
            "As shown in Table 3, replacing 10% of the original tokens exhibits the highest improvement in model performance both in Perplexity and BERTScore as compared to the baseline model. We observe a degradation in both metrics after the 10% token replacement when increasing the replacement percentage further. We theorized that the presence of more synthetic tokens introduces less contextually-similar responses regardless if the synthetic tokens are semantically valid. We also hypothesize that there is a higher possibility of semantically-invalid tokens present in higher replacement percentages as the masking and filling process occurs independently as mentioned in Section 5."
        ]
    },
    "S8.T4": {
        "caption": "Table 4: Perpelixty, F1, and Function Word and Content Word count for varying data sizes",
        "table": "<table id=\"S8.T4.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S8.T4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S8.T4.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S8.T4.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Setup</span></th>\n<th id=\"S8.T4.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S8.T4.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Prplx</span></th>\n<th id=\"S8.T4.1.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S8.T4.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">F1</span></th>\n<th id=\"S8.T4.1.1.1.4\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S8.T4.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">Func.</span></th>\n<th id=\"S8.T4.1.1.1.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span id=\"S8.T4.1.1.1.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S8.T4.1.1.1.5.1.1\" class=\"ltx_p\" style=\"width:31.3pt;\"><span id=\"S8.T4.1.1.1.5.1.1.1\" class=\"ltx_text ltx_font_bold\">Cont.</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S8.T4.1.2.1\" class=\"ltx_tr\">\n<td id=\"S8.T4.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">1K</td>\n<td id=\"S8.T4.1.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_t\">9.5821</td>\n<td id=\"S8.T4.1.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.3387</td>\n<td id=\"S8.T4.1.2.1.4\" class=\"ltx_td ltx_align_left ltx_border_t\">57,327</td>\n<td id=\"S8.T4.1.2.1.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S8.T4.1.2.1.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S8.T4.1.2.1.5.1.1\" class=\"ltx_p\" style=\"width:31.3pt;\">58,872</span>\n</span>\n</td>\n</tr>\n<tr id=\"S8.T4.1.3.2\" class=\"ltx_tr\">\n<td id=\"S8.T4.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\">1K+Aug</td>\n<td id=\"S8.T4.1.3.2.2\" class=\"ltx_td ltx_align_left\">8.4791</td>\n<td id=\"S8.T4.1.3.2.3\" class=\"ltx_td ltx_align_left\">0.3441</td>\n<td id=\"S8.T4.1.3.2.4\" class=\"ltx_td ltx_align_left\">60,774</td>\n<td id=\"S8.T4.1.3.2.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span id=\"S8.T4.1.3.2.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S8.T4.1.3.2.5.1.1\" class=\"ltx_p\" style=\"width:31.3pt;\">64,368</span>\n</span>\n</td>\n</tr>\n<tr id=\"S8.T4.1.4.3\" class=\"ltx_tr\">\n<td id=\"S8.T4.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">10K</td>\n<td id=\"S8.T4.1.4.3.2\" class=\"ltx_td ltx_align_left ltx_border_t\">6.1937</td>\n<td id=\"S8.T4.1.4.3.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.3520</td>\n<td id=\"S8.T4.1.4.3.4\" class=\"ltx_td ltx_align_left ltx_border_t\">66,371</td>\n<td id=\"S8.T4.1.4.3.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S8.T4.1.4.3.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S8.T4.1.4.3.5.1.1\" class=\"ltx_p\" style=\"width:31.3pt;\">71,706</span>\n</span>\n</td>\n</tr>\n<tr id=\"S8.T4.1.5.4\" class=\"ltx_tr\">\n<td id=\"S8.T4.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\">10K+Aug</td>\n<td id=\"S8.T4.1.5.4.2\" class=\"ltx_td ltx_align_left\">5.755</td>\n<td id=\"S8.T4.1.5.4.3\" class=\"ltx_td ltx_align_left\">0.3539</td>\n<td id=\"S8.T4.1.5.4.4\" class=\"ltx_td ltx_align_left\">70,078</td>\n<td id=\"S8.T4.1.5.4.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span id=\"S8.T4.1.5.4.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S8.T4.1.5.4.5.1.1\" class=\"ltx_p\" style=\"width:31.3pt;\">80,662</span>\n</span>\n</td>\n</tr>\n<tr id=\"S8.T4.1.6.5\" class=\"ltx_tr\">\n<td id=\"S8.T4.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">25K</td>\n<td id=\"S8.T4.1.6.5.2\" class=\"ltx_td ltx_align_left ltx_border_t\">4.1206</td>\n<td id=\"S8.T4.1.6.5.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.3584</td>\n<td id=\"S8.T4.1.6.5.4\" class=\"ltx_td ltx_align_left ltx_border_t\">76,665</td>\n<td id=\"S8.T4.1.6.5.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S8.T4.1.6.5.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S8.T4.1.6.5.5.1.1\" class=\"ltx_p\" style=\"width:31.3pt;\">87,873</span>\n</span>\n</td>\n</tr>\n<tr id=\"S8.T4.1.7.6\" class=\"ltx_tr\">\n<td id=\"S8.T4.1.7.6.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r\">25K+Aug</td>\n<td id=\"S8.T4.1.7.6.2\" class=\"ltx_td ltx_align_left ltx_border_b\">3.6771</td>\n<td id=\"S8.T4.1.7.6.3\" class=\"ltx_td ltx_align_left ltx_border_b\">0.3617</td>\n<td id=\"S8.T4.1.7.6.4\" class=\"ltx_td ltx_align_left ltx_border_b\">82,240</td>\n<td id=\"S8.T4.1.7.6.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r\">\n<span id=\"S8.T4.1.7.6.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S8.T4.1.7.6.5.1.1\" class=\"ltx_p\" style=\"width:31.3pt;\">104,211</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "The models also showed relative minor improvements in BERTScore as seen in Table 4. The model with a base training size of 1K samples improved in its BERTScore as well by 1.59% or a 0.0054 increase in F1 after being trained with synthetic data. The full training size improved the most, with a 3.26% increase in F1, while the models trained with 10K and 25K base training sizes improved relatively insignificant.",
            "We theorized that the smaller training samples tend to be more sensitive when using a model such as DialoGPT medium which requires more data on average than 1K samples. We speculate that the dip in scores for 10K and 25K base training sizes and the sudden increase in scores again for the full 53K training size could be attributed to the normal expected training sizes of the model. Consistent with the previous ablation, content word and function word usage also increased with all augmented models as seen in Table 4."
        ]
    }
}