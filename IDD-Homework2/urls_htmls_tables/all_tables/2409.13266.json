{
    "id_table_1": {
        "caption": "Table 1:  Statistics for the datasets and the top-1K synthetics samples generated by  silk  for each domain.",
        "table": "S3.T1.1.1",
        "footnotes": [],
        "references": [
            "An effective strategy for addressing this challenge involves low-resource fine-tuning  Wu et al. ( 2022 ); Meng et al. ( 2023 ) , wherein a pre-trained model is exposed to a limited amount of in-domain data with annotated keyphrases. Nevertheless, annotating even a limited number of documents can be prohibitively expensive, and often impractical due to the necessity for expert annotators  Chau et al. ( 2020 ) . Finding a way to collect such data in an unsupervised fashion would open up possibilities for effortlessly adapting models to new domains. Here, we propose  silk , a method to do so that relies on extracting  sil ver-standard  k eyphrases from citation contexts to generate synthetic labeled data for domain adaptation (see Figure  1 ).",
            "We conduct experiments on few-shot fine-tuning a pre-trained model for keyphrase generation and report significant improvements in in-domain performance using synthetic samples generated by  silk . Additionally, we undertake further experiments to validate the quality of the synthetic samples through both empirical ( 5.1 ) and human ( 5.3 ) evaluations, and we examine whether our adapted models experience catastrophic forgetting of the initial domain ( 5.2 )  or exhibit bias towards keyphrases from highly cited papers ( 5.4 ) .",
            "The final step involves ordering the cited documents based on how confident our method is in its silver-standard keyphrases, and selecting the top- N N N italic_N  ranked documents as synthetic labeled data. Here, we determine the confidence of our method by averaging the scores of its silver-standard keyphrases, as computed in Equation  1 . We remind that our objective is to  generate small, high quality in-domain data for fine-tuning keyphrase generation models , which advocates for a conservative approach.",
            "To the best of our knowledge, there is no dataset of scientific papers available for the  paleo  domain. Thus, we collected 12 353 open- or free-access papers in PDF format from a wide range of journals in Paleontology. 6 6 6 See Table  13  in Appendix  A  for the detailed sources.  We use GROBID 7 7 7 https://github.com/kermitt2/grobid  for extracting the full-text from PDF papers, detecting inline citations and parsing bibliography, as it was shown to outperform other freely available tools  Meuschke et al. ( 2023 ); Rohatgi et al. ( 2023 ) . From the XML output of GROBID, we extracted 53 133 citation contexts from the introductory parts of the papers (i.e.  Introduction ,  Materials and Methods  and  Geological Settings ). With such a small collection, applying our method yields too few synthetic samples. To generate sufficient data for fine-tuning keyphrase generation models, we adjusted the threshold for candidate relevance (i.e.   r = 0.75  0.60 subscript  r 0.75  0.60 \\lambda_{r}=0.75\\rightarrow 0.60 italic_ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT = 0.75  0.60 ) and queried the Semantic Scholar API 8 8 8 https://www.semanticscholar.org/  to include cited papers not present in our collection. These modifications resulted in our method generating a confidence-ordered list of 2 806 synthetic samples.",
            "Table  1  summarizes the statistics of the datasets for each domain we apply our method on. There is a noticeable diversity in characteristics across the datasets, with  nlp  showing the highest citation rate per document and  paleo  the lowest. We suspect there are two reasons for this. First, papers within the  nlp  domain seem to garner higher average citations compared to papers in the other two domains. Second, papers from  paleo  tend to cite works from both related domains (e.g. Biology, Geology) and sources outside our collection of gathered papers. Conversely, the average number of candidate keyphrases per document those found in the title, abstract, or citation contexts remains stable across the domains (  \\approx  80 candidates).",
            "Table  4  presents the results of the keyphrase generation models and our domain adaptation method on each domain. 14 14 14 See Table  14  in Appendix  A  for present/absent results.  We observe that  silk  brings consistent and significant improvements over BART-FT on the three domains. The best overall performance is achieved by fine-tuning the model with the top-1K most confident synthetic samples, however gains are observed with just the top-500 samples. Self-learning for domain adaptation yields only marginal gains at best and often degrades performance. This suggests that the initial performance of BART-FT on these domains is not sufficient to generate high-quality pseudo-labels. A closer look at the numbers shows that BART-FT performs comparably on  nlp  as it does on KP20k (see Table  3 ), but it gives substantially lower scores on  paleo  and  astro . This empirically confirms the growing distance between KP20k and these three domains, correlating model performance with the distance from the initial domain.",
            "The purpose of  silk  is to generate small, high quality in-domain data for fine-tuning keyphrase generation models. Accordingly, synthetic samples are ordered by confidence (described in  2 ) and only the top- N N N italic_N  ranked samples are employed for adapting models. To validate the quality of our ranking, and consequently the effectiveness of our keyphrase candidate scoring function (see Equation  1 ), we compare the performance of BART-FT when we continue the fine-tuning with the top-1K, bottom-1K and a random selection of 1K samples. Results are presented in Table  5 . We note that, uniformly across the three domains, the random and top-1K ordering schemes lead to improvements, with top-1K yielding the best results. In contrast, using the least confident samples (bottom-1K) systematically degrades the performance. Insights from these results are twofold: 1) our confidence ranking proves to be beneficial for selecting high-quality synthetic samples, and 2) even samples beyond the top-1K are qualitative enough for domain adaptation."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Statistics for the test data we collected for each domain in comparison with the commonly used test sets for keyphrase generation.",
        "table": "S3.T2.1.1",
        "footnotes": [],
        "references": [
            "We conduct experiments on few-shot fine-tuning a pre-trained model for keyphrase generation and report significant improvements in in-domain performance using synthetic samples generated by  silk . Additionally, we undertake further experiments to validate the quality of the synthetic samples through both empirical ( 5.1 ) and human ( 5.3 ) evaluations, and we examine whether our adapted models experience catastrophic forgetting of the initial domain ( 5.2 )  or exhibit bias towards keyphrases from highly cited papers ( 5.4 ) .",
            "We use the widely adopted KP20k dataset  Meng et al. ( 2017 )  as a starting point for pre-training keyphrase generation models. This dataset contains   514  K absent 514 K \\approx 514\\mathrm{K}  514 roman_K  scientific documents (titles and abstracts) paired with author-assigned keyphrases in the broader domain of computer science. We investigate the effectiveness of our domain adaptation method across three distinct scientific domains: Natural Language Processing ( nlp ), Astrophysics ( astro ), and Paleontology ( paleo ). These domains differ with increasing distances from the initial KP20k dataset, with  nlp  being the closest and  paleo  standing as the furthest. This section gives details about the data we use for each domain, presents the statistics of the resulting synthetic in-domain data we generate, and describes how we collect 3 3 3 Detailed information on the sources can be found in  A.4 .  annotated test data to validate the usefulness of our method for domain adaptation. We set our method parameters (step 2 in  2 ) based on their observed values in the validation split of KP20k, specifically,   x = 0.85 subscript  x 0.85 \\lambda_{x}=0.85 italic_ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT = 0.85  and   r = 0.75 subscript  r 0.75 \\lambda_{r}=0.75 italic_ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT = 0.75 .",
            "Upon examining the synthetic fine-tuning data generated by our method (restricted to the top-1K), we observe that  nlp  documents are nearly half the length of those in the  paleo  domain, while  astro  documents fall in-between. These differences in length directly impact the ratio of absent keyphrases 9 9 9 We follow the definition of  Boudin and Gallina ( 2021 )  and consider keyphrases that do not match contiguous sequences of (stemmed) words in the source document as absent. , decreasing from 24% to below 10%. These numbers further decrease when computed beyond the top-1K, as the number of citation contexts declines and, consequently, as the pool of absent keyphrase candidates reduces. Constraints we introduced for selecting the optimal subset of phrases allow for an average of about 4 silver keyphrases per document, predominantly unigrams and bigrams, which is in line with both past observations and the test data we compiled (see Table  2 ).",
            "To analyze the disparities between the domains we selected, and also how they depart from KP20k (initial domain) and from other existing test datasets for keyphrase generation, we compare the main statistics of their test splits in Table  2 . Here, we include three additional datasets, Inspec  Hulth ( 2003 ) , NUS  Nguyen and Kan ( 2007 )  and SemEval-2010  Kim et al. ( 2010 ) , that are composed of scientific abstracts in the computer science domain. Together with KP20k, these are likely the most commonly-used datasets for evaluating keyphrase generation models. Overall, we observe many similarities between KP20k and the test data we collected for each domain, whether in terms of the number of gold keyphrases (  \\approx  5 per document), their average length (  \\approx  2 tokens) or the ratio of absent keyphrases (  \\approx  40%). This suggests a uniform trend in author-assigned keyphrases across scientific domains, which should facilitate generalization for keyphrase generation models. It should be noted that higher number of gold keyphrases in NUS, SemEval-2010 and Inspec stems from their distinct annotation processes, with the former two combining author- and reader-assigned keyphrases and the latter relying on professional indexers. Comparing the sizes of our domain-specific test data with those of the test splits in existing datasets shows that they are on a similar scale.",
            "Lastly, we examine the differences between the domains from a semantic perspective. Figure  2  shows a t-SNE visualization  van der Maaten and Hinton ( 2008 )  of the gold keyphrases in the test data that we collected for each domain and those of the KP20k test split. We clearly discern the different domains within the vector space, roughly dividing it into four clusters. The most notable overlap occurs between  nlp  and KP20k (computer science), whereas  astro  and  paleo  exhibit clear separation. These visual insights support our initial assumptions regarding the growing differences of our selected domains from KP20k, with  nlp  being the closest and  paleo  standing as the furthest.",
            "We use BART  Lewis et al. ( 2020 )  as our initial pre-trained language model and perform fine-tuning on the KP20k training set for 15 epochs, following  Wu et al. ( 2023 ) . BART was shown to yield state-of-the-art performance in keyphrase generation  Zhao et al. ( 2022 ); Wu et al. ( 2022 ); Meng et al. ( 2023 ) , surpassing other pre-trained language models, such as T5  Wu et al. ( 2023 ) . Following previous work, we fine-tune BART in a  One2Many  setting  (Yuan et al.,  2020 ) , that is, given a source text as input, the task is to generate keyphrases as a single sequence of delimiter-separated phrases. During fine-tuning, gold keyphrases are arranged in the present-absent order which was found to give the best results  Meng et al. ( 2021 ) . At test time, we use either greedy decoding and let the model generate the most probable keyphrases, or beam search (K=20) and assemble the top- k k k italic_k  keyphrases from all the beams as the model output. Implementation details and training times are provided in Appendix  A.2 .",
            "We use the test split of KP20k for evaluating the initial performance of the models, and our manually collected test sets to assess their in-domain performance. Detailed statistics for these datasets are presented in Table  2 . Following common practice, we evaluate the performance of the models in terms of  F 1 subscript F 1 F_{1} italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  score using exact match between gold and predicted keyphrases. Stemming (Porter stemmer) is applied to reduce the number of mismatches and duplicates are removed. We compute the scores both at the top- k k k italic_k  predicted keyphrases with  k  5 , 10 k 5 10 k\\in{5,10} italic_k  5 , 10 , and at the number  M M M italic_M  of keyphrases predicted by the models as proposed in  Yuan et al. ( 2020 ) . For  F 1  @  k subscript F 1 @ k F_{1}@k italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT @ italic_k  scores, if the number of predicted keyphrases is below  k k k italic_k , we append incorrect predictions until it reaches exactly  k k k italic_k  keyphrases. We also report scores for present and absent keyphrases separately to get more insights about the extractive and generative capabilities of the models. We compute the Students paired t-test to assess the statistical significance of our results at  p < 0.05 p 0.05 p<0.05 italic_p < 0.05 .",
            "The purpose of  silk  is to generate small, high quality in-domain data for fine-tuning keyphrase generation models. Accordingly, synthetic samples are ordered by confidence (described in  2 ) and only the top- N N N italic_N  ranked samples are employed for adapting models. To validate the quality of our ranking, and consequently the effectiveness of our keyphrase candidate scoring function (see Equation  1 ), we compare the performance of BART-FT when we continue the fine-tuning with the top-1K, bottom-1K and a random selection of 1K samples. Results are presented in Table  5 . We note that, uniformly across the three domains, the random and top-1K ordering schemes lead to improvements, with top-1K yielding the best results. In contrast, using the least confident samples (bottom-1K) systematically degrades the performance. Insights from these results are twofold: 1) our confidence ranking proves to be beneficial for selecting high-quality synthetic samples, and 2) even samples beyond the top-1K are qualitative enough for domain adaptation."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Performance comparison of our fine-tuned BART model and baseline models on the KP20k test set, with    \\dagger   indicating statistical significance. Scores for present and absent keyphrases separately are reported.",
        "table": "S4.T3.11.11",
        "footnotes": [],
        "references": [
            "We conduct experiments on few-shot fine-tuning a pre-trained model for keyphrase generation and report significant improvements in in-domain performance using synthetic samples generated by  silk . Additionally, we undertake further experiments to validate the quality of the synthetic samples through both empirical ( 5.1 ) and human ( 5.3 ) evaluations, and we examine whether our adapted models experience catastrophic forgetting of the initial domain ( 5.2 )  or exhibit bias towards keyphrases from highly cited papers ( 5.4 ) .",
            "To the best of our knowledge, there is no dataset of scientific papers available for the  paleo  domain. Thus, we collected 12 353 open- or free-access papers in PDF format from a wide range of journals in Paleontology. 6 6 6 See Table  13  in Appendix  A  for the detailed sources.  We use GROBID 7 7 7 https://github.com/kermitt2/grobid  for extracting the full-text from PDF papers, detecting inline citations and parsing bibliography, as it was shown to outperform other freely available tools  Meuschke et al. ( 2023 ); Rohatgi et al. ( 2023 ) . From the XML output of GROBID, we extracted 53 133 citation contexts from the introductory parts of the papers (i.e.  Introduction ,  Materials and Methods  and  Geological Settings ). With such a small collection, applying our method yields too few synthetic samples. To generate sufficient data for fine-tuning keyphrase generation models, we adjusted the threshold for candidate relevance (i.e.   r = 0.75  0.60 subscript  r 0.75  0.60 \\lambda_{r}=0.75\\rightarrow 0.60 italic_ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT = 0.75  0.60 ) and queried the Semantic Scholar API 8 8 8 https://www.semanticscholar.org/  to include cited papers not present in our collection. These modifications resulted in our method generating a confidence-ordered list of 2 806 synthetic samples.",
            "Table  3  presents the results of our fine-tuned BART model (hereafter denoted as BART-FT) and the baselines on the test split of KP20k. It should be noted that MultiPartiteRank and Yake cannot be assessed using  F 1  @  M subscript F 1 @ M F_{1}@M italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT @ italic_M  as they require setting a top- k k k italic_k  parameter, and that One2Set cannot be assessed using  F 1  @  10 subscript F 1 @ 10 F_{1}@10 italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT @ 10  since it only outputs the most probable keyphrases (  \\approx  7 per document). Overall, BART-FT demonstrates superior performance, significantly outperforming the baselines for both all and only the present keyphrases. We observe that One2Set achieves the best scores for the absent keyphrases, confirming previous findings  Wu et al. ( 2023 ) . In light of these results, we argue that BART-FT is a strong model for keyphrase generation, providing a solid basis for the application of our domain adaptation method.",
            "Table  4  presents the results of the keyphrase generation models and our domain adaptation method on each domain. 14 14 14 See Table  14  in Appendix  A  for present/absent results.  We observe that  silk  brings consistent and significant improvements over BART-FT on the three domains. The best overall performance is achieved by fine-tuning the model with the top-1K most confident synthetic samples, however gains are observed with just the top-500 samples. Self-learning for domain adaptation yields only marginal gains at best and often degrades performance. This suggests that the initial performance of BART-FT on these domains is not sufficient to generate high-quality pseudo-labels. A closer look at the numbers shows that BART-FT performs comparably on  nlp  as it does on KP20k (see Table  3 ), but it gives substantially lower scores on  paleo  and  astro . This empirically confirms the growing distance between KP20k and these three domains, correlating model performance with the distance from the initial domain.",
            "We further examine the quality of the synthetic samples produced with  silk  by conducting a manual evaluation of the top-100 samples of the  nlp  domain. 15 15 15 Annotation guidelines and examples can be found in  A.3 .  Annotators were instructed to assess the relevance of silver-standard keyphrases using a 3-point scale:  not relevant ,  partially relevant  and  relevant . Additionally, we requested annotators to assess the well-formedness of the keyphrases with a binary rating. To quantify the qualitative difference between  silk  keyphrases and automatically generated ones, we perform a second round of human evaluation for BART-FT utilizing the same top-100 samples. Table  7  presents the results of our qualitative analysis. First, we note that nearly all  silk  keyphrases are well-formed, with any exceptions attributable to tagging errors (e.g.  inter alia ). More importantly, we observe that 80% of  silk  keyphrases are relevant, demonstrating the effectiveness of our method. In contrast, only 54.5% of the keyphrases generated by BART-FT are deemed relevant, which explains why the self-learning approach to domain adaptation falls short. We also note that BART-FT tends to generate more keyphrases (  \\approx  5.5 per doc.), many of which are broader terms that are often irrelevant for the NLP domain (e.g.  natural language processing ,  statistics  or  machine learning )."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Performance of keyphrase generation models on the  nlp ,  astro  and  paleo  domains for all keyphrases (i.e. present and absent combined). Values in  bold  indicate best scores and    \\dagger   indicates significance over BART-FT.",
        "table": "S5.T4.16",
        "footnotes": [],
        "references": [
            "We conduct experiments on few-shot fine-tuning a pre-trained model for keyphrase generation and report significant improvements in in-domain performance using synthetic samples generated by  silk . Additionally, we undertake further experiments to validate the quality of the synthetic samples through both empirical ( 5.1 ) and human ( 5.3 ) evaluations, and we examine whether our adapted models experience catastrophic forgetting of the initial domain ( 5.2 )  or exhibit bias towards keyphrases from highly cited papers ( 5.4 ) .",
            "We use the widely adopted KP20k dataset  Meng et al. ( 2017 )  as a starting point for pre-training keyphrase generation models. This dataset contains   514  K absent 514 K \\approx 514\\mathrm{K}  514 roman_K  scientific documents (titles and abstracts) paired with author-assigned keyphrases in the broader domain of computer science. We investigate the effectiveness of our domain adaptation method across three distinct scientific domains: Natural Language Processing ( nlp ), Astrophysics ( astro ), and Paleontology ( paleo ). These domains differ with increasing distances from the initial KP20k dataset, with  nlp  being the closest and  paleo  standing as the furthest. This section gives details about the data we use for each domain, presents the statistics of the resulting synthetic in-domain data we generate, and describes how we collect 3 3 3 Detailed information on the sources can be found in  A.4 .  annotated test data to validate the usefulness of our method for domain adaptation. We set our method parameters (step 2 in  2 ) based on their observed values in the validation split of KP20k, specifically,   x = 0.85 subscript  x 0.85 \\lambda_{x}=0.85 italic_ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT = 0.85  and   r = 0.75 subscript  r 0.75 \\lambda_{r}=0.75 italic_ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT = 0.75 .",
            "Table  4  presents the results of the keyphrase generation models and our domain adaptation method on each domain. 14 14 14 See Table  14  in Appendix  A  for present/absent results.  We observe that  silk  brings consistent and significant improvements over BART-FT on the three domains. The best overall performance is achieved by fine-tuning the model with the top-1K most confident synthetic samples, however gains are observed with just the top-500 samples. Self-learning for domain adaptation yields only marginal gains at best and often degrades performance. This suggests that the initial performance of BART-FT on these domains is not sufficient to generate high-quality pseudo-labels. A closer look at the numbers shows that BART-FT performs comparably on  nlp  as it does on KP20k (see Table  3 ), but it gives substantially lower scores on  paleo  and  astro . This empirically confirms the growing distance between KP20k and these three domains, correlating model performance with the distance from the initial domain."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Performance of BART-FT fine-tuned on the top-1K, bottom-1K and random-1K (averaged over 5 runs with different seed values) samples.    \\dagger   indicates significance over BART-FT.",
        "table": "S5.T5.9.9",
        "footnotes": [],
        "references": [
            "We conduct experiments on few-shot fine-tuning a pre-trained model for keyphrase generation and report significant improvements in in-domain performance using synthetic samples generated by  silk . Additionally, we undertake further experiments to validate the quality of the synthetic samples through both empirical ( 5.1 ) and human ( 5.3 ) evaluations, and we examine whether our adapted models experience catastrophic forgetting of the initial domain ( 5.2 )  or exhibit bias towards keyphrases from highly cited papers ( 5.4 ) .",
            "The purpose of  silk  is to generate small, high quality in-domain data for fine-tuning keyphrase generation models. Accordingly, synthetic samples are ordered by confidence (described in  2 ) and only the top- N N N italic_N  ranked samples are employed for adapting models. To validate the quality of our ranking, and consequently the effectiveness of our keyphrase candidate scoring function (see Equation  1 ), we compare the performance of BART-FT when we continue the fine-tuning with the top-1K, bottom-1K and a random selection of 1K samples. Results are presented in Table  5 . We note that, uniformly across the three domains, the random and top-1K ordering schemes lead to improvements, with top-1K yielding the best results. In contrast, using the least confident samples (bottom-1K) systematically degrades the performance. Insights from these results are twofold: 1) our confidence ranking proves to be beneficial for selecting high-quality synthetic samples, and 2) even samples beyond the top-1K are qualitative enough for domain adaptation."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Performance comparison of BART-FT and its adaptions ( silk  1K) on the KP20k test set.",
        "table": "S5.T6.3.3",
        "footnotes": [],
        "references": [
            "Although continued training is effective for domain adaptation, it has been found to adversely affect performance in the initial domain for language generation tasks  Li et al. ( 2022 ) . Here, we investigate whether this phenomenon, referred to in the literature as catastrophic forgetting  French ( 1999 ) , also manifests in our adapted models. Table  6  presents the results of our domain adapted BART-FT models (using 1K synthetic samples) on the KP20k test set. Overall, we observe no drop in performance for our adapted models. Rather surprisingly, we notice small improvements in  F 1  @  k subscript F 1 @ k F_{1}@k italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT @ italic_k  scores over the initial BART-FT model. Upon closer examination, these gains derive from improved extractive capabilities, while the scores for absent keyphrases consistently degrade. We hypothesise that the domain adaption process makes the model lose generative ability and reinforces its extractive capability which translates more effectively across domains."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Human evaluation results (%) in terms of well-formedness and relevance of the top-100  nlp  samples generated by  silk  and re-annotated using BART-FT.",
        "table": "S5.T7.1.1",
        "footnotes": [],
        "references": [
            "We further examine the quality of the synthetic samples produced with  silk  by conducting a manual evaluation of the top-100 samples of the  nlp  domain. 15 15 15 Annotation guidelines and examples can be found in  A.3 .  Annotators were instructed to assess the relevance of silver-standard keyphrases using a 3-point scale:  not relevant ,  partially relevant  and  relevant . Additionally, we requested annotators to assess the well-formedness of the keyphrases with a binary rating. To quantify the qualitative difference between  silk  keyphrases and automatically generated ones, we perform a second round of human evaluation for BART-FT utilizing the same top-100 samples. Table  7  presents the results of our qualitative analysis. First, we note that nearly all  silk  keyphrases are well-formed, with any exceptions attributable to tagging errors (e.g.  inter alia ). More importantly, we observe that 80% of  silk  keyphrases are relevant, demonstrating the effectiveness of our method. In contrast, only 54.5% of the keyphrases generated by BART-FT are deemed relevant, which explains why the self-learning approach to domain adaptation falls short. We also note that BART-FT tends to generate more keyphrases (  \\approx  5.5 per doc.), many of which are broader terms that are often irrelevant for the NLP domain (e.g.  natural language processing ,  statistics  or  machine learning )."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Difference in the number of generated keyphrases found in  silk  samples between BART-FT and its adaptations; a negative number means the adapted model generates fewer keyphrases from highly cited papers.",
        "table": "S5.T8.1",
        "footnotes": [],
        "references": [
            "Since our method leverages citation contexts, it produces synthetic samples that are inherently biased towards highly cited papers and their corresponding keyphrases. To investigate whether this bias is present in the adapted BART-FT models, we measure how frequently they generate keyphrases found in the synthetic samples and compare this number to that of our initial model. Results are presented in Table  8 . We observe only minor differences in the number of generated keyphrases from the synthetic samples, suggesting no apparent bias. Conversely, we notice that the adapted models produce fewer of these keyphrases, as evidenced by the negative scores. We attribute this to the few-shot fine-tuning, which may not sufficiently affect the model weights to propagate bias, and reinforces the extractive capabilities of the models, thereby making them less sensitive to bias."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Examples of document (title and abstract) from the  nlp  domain with silver-standard keyphrases generated by  silk  and automatically generated keyphrases from BART-FT.",
        "table": "A1.T9.1.1",
        "footnotes": [],
        "references": [
            "An example of output for  silk  and BART-FT is shown is Table  9 ."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:  Detailed information on the sources of the test documents for the  nlp  domain.  \\faHandPaper [regular] indicates that we manually selected the documents to filter out out-of-domain ones.",
        "table": "A1.T10.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "Table 11:  Detailed information on the sources of the test documents for the  astro  domain.",
        "table": "A1.T11.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "Table 12:  Detailed information on the sources of the test documents for the  paleo  domain.",
        "table": "A1.T12.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "Table 13:  Detailed information on the sources of the scientific papers collected for the Paleontology corpus.",
        "table": "A1.T13.1.1",
        "footnotes": [],
        "references": [
            "To the best of our knowledge, there is no dataset of scientific papers available for the  paleo  domain. Thus, we collected 12 353 open- or free-access papers in PDF format from a wide range of journals in Paleontology. 6 6 6 See Table  13  in Appendix  A  for the detailed sources.  We use GROBID 7 7 7 https://github.com/kermitt2/grobid  for extracting the full-text from PDF papers, detecting inline citations and parsing bibliography, as it was shown to outperform other freely available tools  Meuschke et al. ( 2023 ); Rohatgi et al. ( 2023 ) . From the XML output of GROBID, we extracted 53 133 citation contexts from the introductory parts of the papers (i.e.  Introduction ,  Materials and Methods  and  Geological Settings ). With such a small collection, applying our method yields too few synthetic samples. To generate sufficient data for fine-tuning keyphrase generation models, we adjusted the threshold for candidate relevance (i.e.   r = 0.75  0.60 subscript  r 0.75  0.60 \\lambda_{r}=0.75\\rightarrow 0.60 italic_ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT = 0.75  0.60 ) and queried the Semantic Scholar API 8 8 8 https://www.semanticscholar.org/  to include cited papers not present in our collection. These modifications resulted in our method generating a confidence-ordered list of 2 806 synthetic samples."
        ]
    },
    "id_table_14": {
        "caption": "Table 14:  Performance of keyphrase generation models on the  nlp ,  astro  and  paleo  domains for present and absent keyphrases separately. Values in  bold  indicate best scores and    \\dagger   indicates significance over BART-FT.",
        "table": "A1.T14.16.16",
        "footnotes": [],
        "references": [
            "Table  4  presents the results of the keyphrase generation models and our domain adaptation method on each domain. 14 14 14 See Table  14  in Appendix  A  for present/absent results.  We observe that  silk  brings consistent and significant improvements over BART-FT on the three domains. The best overall performance is achieved by fine-tuning the model with the top-1K most confident synthetic samples, however gains are observed with just the top-500 samples. Self-learning for domain adaptation yields only marginal gains at best and often degrades performance. This suggests that the initial performance of BART-FT on these domains is not sufficient to generate high-quality pseudo-labels. A closer look at the numbers shows that BART-FT performs comparably on  nlp  as it does on KP20k (see Table  3 ), but it gives substantially lower scores on  paleo  and  astro . This empirically confirms the growing distance between KP20k and these three domains, correlating model performance with the distance from the initial domain."
        ]
    }
}