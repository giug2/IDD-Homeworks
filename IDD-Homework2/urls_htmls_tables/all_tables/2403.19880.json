{
    "S3.T3": {
        "caption": "Table 3: (a) Sample segmentation results of different configurations. Text+segmentation model exhibits finer boundaries (high-fidelity details) for different regions with fewer false positive predictions. (b) Downstream echo classification of ED/ES results (performed with Real+100% approach) using Accuracy (ACC), Precision (PR), Recall (RC), and F1 metrics. Blue and red indicates the best and the second-best results.(a) Segmentation Results(b) Classification",
        "table": "<table id=\"S3.T3.st1.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T3.st1.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T3.st1.1.1.1\" class=\"ltx_td ltx_align_center\"><img src=\"/html/2403.19880/assets/x3.png\" id=\"S3.T3.st1.1.1.1.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"461\" height=\"289\" alt=\"[Uncaptioned image]\"></td>\n</tr>\n</tbody>\n</table>\n<table id=\"S3.T3.st2.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T3.st2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T3.st2.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr ltx_border_tt\" rowspan=\"2\"><span id=\"S3.T3.st2.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Methods &amp; dataset</span></th>\n<td id=\"S3.T3.st2.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\">\n<span id=\"S3.T3.st2.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Metrics</span> <math id=\"S3.T3.st2.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S3.T3.st2.1.1.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S3.T3.st2.1.1.1.1.m1.1.1\" xref=\"S3.T3.st2.1.1.1.1.m1.1.1.cmml\">&#8593;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.T3.st2.1.1.1.1.m1.1b\"><ci id=\"S3.T3.st2.1.1.1.1.m1.1.1.cmml\" xref=\"S3.T3.st2.1.1.1.1.m1.1.1\">&#8593;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T3.st2.1.1.1.1.m1.1c\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr id=\"S3.T3.st2.1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S3.T3.st2.1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T3.st2.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">ACC</span></td>\n<td id=\"S3.T3.st2.1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T3.st2.1.1.2.1.2.1\" class=\"ltx_text ltx_font_bold\">PR</span></td>\n<td id=\"S3.T3.st2.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T3.st2.1.1.2.1.3.1\" class=\"ltx_text ltx_font_bold\">RC</span></td>\n<td id=\"S3.T3.st2.1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T3.st2.1.1.2.1.4.1\" class=\"ltx_text ltx_font_bold\">F1</span></td>\n</tr>\n<tr id=\"S3.T3.st2.1.1.3.2\" class=\"ltx_tr\">\n<th id=\"S3.T3.st2.1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr ltx_border_t\">ResNet18 (Real data only)</th>\n<td id=\"S3.T3.st2.1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.8400</td>\n<td id=\"S3.T3.st2.1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.8400</td>\n<td id=\"S3.T3.st2.1.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.8400</td>\n<td id=\"S3.T3.st2.1.1.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.8399</td>\n</tr>\n<tr id=\"S3.T3.st2.1.1.4.3\" class=\"ltx_tr\">\n<th id=\"S3.T3.st2.1.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">ResNet18 (Unconditional)</th>\n<td id=\"S3.T3.st2.1.1.4.3.2\" class=\"ltx_td ltx_align_center\">0.8350</td>\n<td id=\"S3.T3.st2.1.1.4.3.3\" class=\"ltx_td ltx_align_center\">0.8366</td>\n<td id=\"S3.T3.st2.1.1.4.3.4\" class=\"ltx_td ltx_align_center\">0.8350</td>\n<td id=\"S3.T3.st2.1.1.4.3.5\" class=\"ltx_td ltx_align_center\">0.8347</td>\n</tr>\n<tr id=\"S3.T3.st2.1.1.5.4\" class=\"ltx_tr\">\n<th id=\"S3.T3.st2.1.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">ResNet18 (Text-Conditioned)</th>\n<td id=\"S3.T3.st2.1.1.5.4.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T3.st2.1.1.5.4.2.1\" class=\"ltx_text\" style=\"color:#0000FF;\">0.8700</span></td>\n<td id=\"S3.T3.st2.1.1.5.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T3.st2.1.1.5.4.3.1\" class=\"ltx_text\" style=\"color:#0000FF;\">0.8773</span></td>\n<td id=\"S3.T3.st2.1.1.5.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T3.st2.1.1.5.4.4.1\" class=\"ltx_text\" style=\"color:#0000FF;\">0.8700</span></td>\n<td id=\"S3.T3.st2.1.1.5.4.5\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T3.st2.1.1.5.4.5.1\" class=\"ltx_text\" style=\"color:#0000FF;\">0.8693</span></td>\n</tr>\n<tr id=\"S3.T3.st2.1.1.6.5\" class=\"ltx_tr\">\n<th id=\"S3.T3.st2.1.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">ResNet18 (SDM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">23</a>]</cite>)</th>\n<td id=\"S3.T3.st2.1.1.6.5.2\" class=\"ltx_td ltx_align_center\">0.8300</td>\n<td id=\"S3.T3.st2.1.1.6.5.3\" class=\"ltx_td ltx_align_center\">0.8311</td>\n<td id=\"S3.T3.st2.1.1.6.5.4\" class=\"ltx_td ltx_align_center\">0.8300</td>\n<td id=\"S3.T3.st2.1.1.6.5.5\" class=\"ltx_td ltx_align_center\">0.8298</td>\n</tr>\n<tr id=\"S3.T3.st2.1.1.7.6\" class=\"ltx_tr\">\n<th id=\"S3.T3.st2.1.1.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">ResNet18 (Text+Segmentation)</th>\n<td id=\"S3.T3.st2.1.1.7.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T3.st2.1.1.7.6.2.1\" class=\"ltx_text\" style=\"color:#FF0000;\">0.8650</span></td>\n<td id=\"S3.T3.st2.1.1.7.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T3.st2.1.1.7.6.3.1\" class=\"ltx_text\" style=\"color:#FF0000;\">0.8659</span></td>\n<td id=\"S3.T3.st2.1.1.7.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T3.st2.1.1.7.6.4.1\" class=\"ltx_text\" style=\"color:#FF0000;\">0.8650</span></td>\n<td id=\"S3.T3.st2.1.1.7.6.5\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T3.st2.1.1.7.6.5.1\" class=\"ltx_text\" style=\"color:#FF0000;\">0.8640</span></td>\n</tr>\n<tr id=\"S3.T3.st2.1.1.8.7\" class=\"ltx_tr\">\n<th id=\"S3.T3.st2.1.1.8.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr ltx_border_tt\">VGG16 (Real data only)</th>\n<td id=\"S3.T3.st2.1.1.8.7.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">0.7450</td>\n<td id=\"S3.T3.st2.1.1.8.7.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">0.7471</td>\n<td id=\"S3.T3.st2.1.1.8.7.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">0.7450</td>\n<td id=\"S3.T3.st2.1.1.8.7.5\" class=\"ltx_td ltx_align_center ltx_border_tt\">0.7445</td>\n</tr>\n<tr id=\"S3.T3.st2.1.1.9.8\" class=\"ltx_tr\">\n<th id=\"S3.T3.st2.1.1.9.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">VGG16 (Uniconditional)</th>\n<td id=\"S3.T3.st2.1.1.9.8.2\" class=\"ltx_td ltx_align_center\">0.7500</td>\n<td id=\"S3.T3.st2.1.1.9.8.3\" class=\"ltx_td ltx_align_center\">0.7549</td>\n<td id=\"S3.T3.st2.1.1.9.8.4\" class=\"ltx_td ltx_align_center\">7500</td>\n<td id=\"S3.T3.st2.1.1.9.8.5\" class=\"ltx_td ltx_align_center\">0.7487</td>\n</tr>\n<tr id=\"S3.T3.st2.1.1.10.9\" class=\"ltx_tr\">\n<th id=\"S3.T3.st2.1.1.10.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">VGG16 (Text-Conditioned)</th>\n<td id=\"S3.T3.st2.1.1.10.9.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T3.st2.1.1.10.9.2.1\" class=\"ltx_text\" style=\"color:#0000FF;\">0.7850</span></td>\n<td id=\"S3.T3.st2.1.1.10.9.3\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T3.st2.1.1.10.9.3.1\" class=\"ltx_text\" style=\"color:#0000FF;\">0.7850</span></td>\n<td id=\"S3.T3.st2.1.1.10.9.4\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T3.st2.1.1.10.9.4.1\" class=\"ltx_text\" style=\"color:#0000FF;\">0.7850</span></td>\n<td id=\"S3.T3.st2.1.1.10.9.5\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T3.st2.1.1.10.9.5.1\" class=\"ltx_text\" style=\"color:#0000FF;\">0.7849</span></td>\n</tr>\n<tr id=\"S3.T3.st2.1.1.11.10\" class=\"ltx_tr\">\n<th id=\"S3.T3.st2.1.1.11.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">VGG16 (SDM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">23</a>]</cite>)</th>\n<td id=\"S3.T3.st2.1.1.11.10.2\" class=\"ltx_td ltx_align_center\">0.7600</td>\n<td id=\"S3.T3.st2.1.1.11.10.3\" class=\"ltx_td ltx_align_center\">0.7687</td>\n<td id=\"S3.T3.st2.1.1.11.10.4\" class=\"ltx_td ltx_align_center\">0.7600</td>\n<td id=\"S3.T3.st2.1.1.11.10.5\" class=\"ltx_td ltx_align_center\">0.7580</td>\n</tr>\n<tr id=\"S3.T3.st2.1.1.12.11\" class=\"ltx_tr\">\n<th id=\"S3.T3.st2.1.1.12.11.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_rr\">VGG16 (Text+Segmentation)</th>\n<td id=\"S3.T3.st2.1.1.12.11.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T3.st2.1.1.12.11.2.1\" class=\"ltx_text\" style=\"color:#FF0000;\">0.7750</span></td>\n<td id=\"S3.T3.st2.1.1.12.11.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T3.st2.1.1.12.11.3.1\" class=\"ltx_text\" style=\"color:#FF0000;\">0.7763</span></td>\n<td id=\"S3.T3.st2.1.1.12.11.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T3.st2.1.1.12.11.4.1\" class=\"ltx_text\" style=\"color:#FF0000;\">0.7750</span></td>\n<td id=\"S3.T3.st2.1.1.12.11.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T3.st2.1.1.12.11.5.1\" class=\"ltx_text\" style=\"color:#FF0000;\">0.7747</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Training and Inference Settings. Our proposed technique was developed using PyTorch and Diffusers libraries and executed on four NVIDIA V100 16GB GPUs. A batch size of 1 (per device) and an Adam optimizer with a learning rate of 5\u200be\u221265e65\\mathrm{e}{-6} were utilized during the training process, which was carried out for 120000 iterations. We employed a DDPM scheduler with 1000 denoising steps, and for quicker inference, a UniPC multi-step scheduler [28] with 50 steps.\nDataset and Evaluation Metrics. For our study, we used the CAMUS echocardiography dataset\u00a0[13], featuring 2D apical views of both two-chamber (2CH) and four-chamber (4CH) perspectives from 500 patients across end-diastole (ED) and end-systole (ES) phases. The initial dataset randomly selected images from 50 patients for the test split, without segmentation labels, while the remaining 450 patients were assigned to the training split. Following the baseline work\u00a0[23], we allocated the first 50 patients for validation and utilized the remaining 400 patients for training, leading to a total of 1600/200 images for train/validation respectively. We used Fr\u00e9chet Inception Distance (FID)\u00a0[8] and Kernel Inception Distance (KID)\u00a0[2] to evaluate the quality of image synthesis. FID measures the distance between feature vectors of real and generated images, while KID quantifies the similarity between probability distributions of real and generated images\u2019 features. For the downstream tasks, namely segmentation, and classification, we opt for well-established metrics as shown in Table\u00a02 and 3(b).",
            "We investigated multiple data augmentation methods, including only real images (Real), a combination of 1600 real with 800 synthetic images (Real+50%), 1600 real with 1600 synthetic images (Real+100%), and 1600 real with 3200 synthetic images (Real+200%). These synthetic images were created using our text+segmentation model. For validation, we used 50 held-out patients, following [23] (It is noteworthy to mention that [23] developed their segmentation model using 8000 synthetic images.).\nTable\u00a02 and 3(b) present the comparative effectiveness of our generated data in diverse scenarios. Specifically, Table\u00a02 shows that our synthetic data distinctly impact the performance of different models, resulting in additional performance gains when utilizing data generated with comprehensive guidance compared to other competitors, as evidenced by various evaluation metrics. This also aligns with the visual comparison of echo segmentation in 3(a) which indicates that our text+segmentation guided model can produce echo images with better overall realism and a tendency to adhere to anatomical input constraints. Lastly, the classification results shown in 3(b) highlight the value of synthetic data, demonstrating that image-level details (text prompts) outperform those from full guidance (text+segmentation)."
        ]
    },
    "S3.T3.st1": {
        "caption": "(a) Segmentation Results",
        "table": "<table id=\"S3.T3.st1.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T3.st1.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T3.st1.1.1.1\" class=\"ltx_td ltx_align_center\"><img src=\"/html/2403.19880/assets/x3.png\" id=\"S3.T3.st1.1.1.1.g1\" class=\"ltx_graphics ltx_img_landscape\" width=\"461\" height=\"289\" alt=\"[Uncaptioned image]\"></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "We investigated multiple data augmentation methods, including only real images (Real), a combination of 1600 real with 800 synthetic images (Real+50%), 1600 real with 1600 synthetic images (Real+100%), and 1600 real with 3200 synthetic images (Real+200%). These synthetic images were created using our text+segmentation model. For validation, we used 50 held-out patients, following [23] (It is noteworthy to mention that [23] developed their segmentation model using 8000 synthetic images.).\nTable\u00a02 and 3(b) present the comparative effectiveness of our generated data in diverse scenarios. Specifically, Table\u00a02 shows that our synthetic data distinctly impact the performance of different models, resulting in additional performance gains when utilizing data generated with comprehensive guidance compared to other competitors, as evidenced by various evaluation metrics. This also aligns with the visual comparison of echo segmentation in 3(a) which indicates that our text+segmentation guided model can produce echo images with better overall realism and a tendency to adhere to anatomical input constraints. Lastly, the classification results shown in 3(b) highlight the value of synthetic data, demonstrating that image-level details (text prompts) outperform those from full guidance (text+segmentation)."
        ]
    },
    "S3.T3.st2": {
        "caption": "(b) Classification",
        "table": "<table id=\"S3.T3.st2.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T3.st2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T3.st2.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr ltx_border_tt\" rowspan=\"2\"><span id=\"S3.T3.st2.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Methods &amp; dataset</span></th>\n<td id=\"S3.T3.st2.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\">\n<span id=\"S3.T3.st2.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Metrics</span> <math id=\"S3.T3.st2.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S3.T3.st2.1.1.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S3.T3.st2.1.1.1.1.m1.1.1\" xref=\"S3.T3.st2.1.1.1.1.m1.1.1.cmml\">&#8593;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.T3.st2.1.1.1.1.m1.1b\"><ci id=\"S3.T3.st2.1.1.1.1.m1.1.1.cmml\" xref=\"S3.T3.st2.1.1.1.1.m1.1.1\">&#8593;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T3.st2.1.1.1.1.m1.1c\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr id=\"S3.T3.st2.1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S3.T3.st2.1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T3.st2.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">ACC</span></td>\n<td id=\"S3.T3.st2.1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T3.st2.1.1.2.1.2.1\" class=\"ltx_text ltx_font_bold\">PR</span></td>\n<td id=\"S3.T3.st2.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T3.st2.1.1.2.1.3.1\" class=\"ltx_text ltx_font_bold\">RC</span></td>\n<td id=\"S3.T3.st2.1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T3.st2.1.1.2.1.4.1\" class=\"ltx_text ltx_font_bold\">F1</span></td>\n</tr>\n<tr id=\"S3.T3.st2.1.1.3.2\" class=\"ltx_tr\">\n<th id=\"S3.T3.st2.1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr ltx_border_t\">ResNet18 (Real data only)</th>\n<td id=\"S3.T3.st2.1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.8400</td>\n<td id=\"S3.T3.st2.1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.8400</td>\n<td id=\"S3.T3.st2.1.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.8400</td>\n<td id=\"S3.T3.st2.1.1.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.8399</td>\n</tr>\n<tr id=\"S3.T3.st2.1.1.4.3\" class=\"ltx_tr\">\n<th id=\"S3.T3.st2.1.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">ResNet18 (Unconditional)</th>\n<td id=\"S3.T3.st2.1.1.4.3.2\" class=\"ltx_td ltx_align_center\">0.8350</td>\n<td id=\"S3.T3.st2.1.1.4.3.3\" class=\"ltx_td ltx_align_center\">0.8366</td>\n<td id=\"S3.T3.st2.1.1.4.3.4\" class=\"ltx_td ltx_align_center\">0.8350</td>\n<td id=\"S3.T3.st2.1.1.4.3.5\" class=\"ltx_td ltx_align_center\">0.8347</td>\n</tr>\n<tr id=\"S3.T3.st2.1.1.5.4\" class=\"ltx_tr\">\n<th id=\"S3.T3.st2.1.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">ResNet18 (Text-Conditioned)</th>\n<td id=\"S3.T3.st2.1.1.5.4.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T3.st2.1.1.5.4.2.1\" class=\"ltx_text\" style=\"color:#0000FF;\">0.8700</span></td>\n<td id=\"S3.T3.st2.1.1.5.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T3.st2.1.1.5.4.3.1\" class=\"ltx_text\" style=\"color:#0000FF;\">0.8773</span></td>\n<td id=\"S3.T3.st2.1.1.5.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T3.st2.1.1.5.4.4.1\" class=\"ltx_text\" style=\"color:#0000FF;\">0.8700</span></td>\n<td id=\"S3.T3.st2.1.1.5.4.5\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T3.st2.1.1.5.4.5.1\" class=\"ltx_text\" style=\"color:#0000FF;\">0.8693</span></td>\n</tr>\n<tr id=\"S3.T3.st2.1.1.6.5\" class=\"ltx_tr\">\n<th id=\"S3.T3.st2.1.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">ResNet18 (SDM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">23</a>]</cite>)</th>\n<td id=\"S3.T3.st2.1.1.6.5.2\" class=\"ltx_td ltx_align_center\">0.8300</td>\n<td id=\"S3.T3.st2.1.1.6.5.3\" class=\"ltx_td ltx_align_center\">0.8311</td>\n<td id=\"S3.T3.st2.1.1.6.5.4\" class=\"ltx_td ltx_align_center\">0.8300</td>\n<td id=\"S3.T3.st2.1.1.6.5.5\" class=\"ltx_td ltx_align_center\">0.8298</td>\n</tr>\n<tr id=\"S3.T3.st2.1.1.7.6\" class=\"ltx_tr\">\n<th id=\"S3.T3.st2.1.1.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">ResNet18 (Text+Segmentation)</th>\n<td id=\"S3.T3.st2.1.1.7.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T3.st2.1.1.7.6.2.1\" class=\"ltx_text\" style=\"color:#FF0000;\">0.8650</span></td>\n<td id=\"S3.T3.st2.1.1.7.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T3.st2.1.1.7.6.3.1\" class=\"ltx_text\" style=\"color:#FF0000;\">0.8659</span></td>\n<td id=\"S3.T3.st2.1.1.7.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T3.st2.1.1.7.6.4.1\" class=\"ltx_text\" style=\"color:#FF0000;\">0.8650</span></td>\n<td id=\"S3.T3.st2.1.1.7.6.5\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T3.st2.1.1.7.6.5.1\" class=\"ltx_text\" style=\"color:#FF0000;\">0.8640</span></td>\n</tr>\n<tr id=\"S3.T3.st2.1.1.8.7\" class=\"ltx_tr\">\n<th id=\"S3.T3.st2.1.1.8.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr ltx_border_tt\">VGG16 (Real data only)</th>\n<td id=\"S3.T3.st2.1.1.8.7.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">0.7450</td>\n<td id=\"S3.T3.st2.1.1.8.7.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">0.7471</td>\n<td id=\"S3.T3.st2.1.1.8.7.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">0.7450</td>\n<td id=\"S3.T3.st2.1.1.8.7.5\" class=\"ltx_td ltx_align_center ltx_border_tt\">0.7445</td>\n</tr>\n<tr id=\"S3.T3.st2.1.1.9.8\" class=\"ltx_tr\">\n<th id=\"S3.T3.st2.1.1.9.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">VGG16 (Uniconditional)</th>\n<td id=\"S3.T3.st2.1.1.9.8.2\" class=\"ltx_td ltx_align_center\">0.7500</td>\n<td id=\"S3.T3.st2.1.1.9.8.3\" class=\"ltx_td ltx_align_center\">0.7549</td>\n<td id=\"S3.T3.st2.1.1.9.8.4\" class=\"ltx_td ltx_align_center\">7500</td>\n<td id=\"S3.T3.st2.1.1.9.8.5\" class=\"ltx_td ltx_align_center\">0.7487</td>\n</tr>\n<tr id=\"S3.T3.st2.1.1.10.9\" class=\"ltx_tr\">\n<th id=\"S3.T3.st2.1.1.10.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">VGG16 (Text-Conditioned)</th>\n<td id=\"S3.T3.st2.1.1.10.9.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T3.st2.1.1.10.9.2.1\" class=\"ltx_text\" style=\"color:#0000FF;\">0.7850</span></td>\n<td id=\"S3.T3.st2.1.1.10.9.3\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T3.st2.1.1.10.9.3.1\" class=\"ltx_text\" style=\"color:#0000FF;\">0.7850</span></td>\n<td id=\"S3.T3.st2.1.1.10.9.4\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T3.st2.1.1.10.9.4.1\" class=\"ltx_text\" style=\"color:#0000FF;\">0.7850</span></td>\n<td id=\"S3.T3.st2.1.1.10.9.5\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T3.st2.1.1.10.9.5.1\" class=\"ltx_text\" style=\"color:#0000FF;\">0.7849</span></td>\n</tr>\n<tr id=\"S3.T3.st2.1.1.11.10\" class=\"ltx_tr\">\n<th id=\"S3.T3.st2.1.1.11.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">VGG16 (SDM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">23</a>]</cite>)</th>\n<td id=\"S3.T3.st2.1.1.11.10.2\" class=\"ltx_td ltx_align_center\">0.7600</td>\n<td id=\"S3.T3.st2.1.1.11.10.3\" class=\"ltx_td ltx_align_center\">0.7687</td>\n<td id=\"S3.T3.st2.1.1.11.10.4\" class=\"ltx_td ltx_align_center\">0.7600</td>\n<td id=\"S3.T3.st2.1.1.11.10.5\" class=\"ltx_td ltx_align_center\">0.7580</td>\n</tr>\n<tr id=\"S3.T3.st2.1.1.12.11\" class=\"ltx_tr\">\n<th id=\"S3.T3.st2.1.1.12.11.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_rr\">VGG16 (Text+Segmentation)</th>\n<td id=\"S3.T3.st2.1.1.12.11.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T3.st2.1.1.12.11.2.1\" class=\"ltx_text\" style=\"color:#FF0000;\">0.7750</span></td>\n<td id=\"S3.T3.st2.1.1.12.11.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T3.st2.1.1.12.11.3.1\" class=\"ltx_text\" style=\"color:#FF0000;\">0.7763</span></td>\n<td id=\"S3.T3.st2.1.1.12.11.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T3.st2.1.1.12.11.4.1\" class=\"ltx_text\" style=\"color:#FF0000;\">0.7750</span></td>\n<td id=\"S3.T3.st2.1.1.12.11.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T3.st2.1.1.12.11.5.1\" class=\"ltx_text\" style=\"color:#FF0000;\">0.7747</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Training and Inference Settings. Our proposed technique was developed using PyTorch and Diffusers libraries and executed on four NVIDIA V100 16GB GPUs. A batch size of 1 (per device) and an Adam optimizer with a learning rate of 5\u200be\u221265e65\\mathrm{e}{-6} were utilized during the training process, which was carried out for 120000 iterations. We employed a DDPM scheduler with 1000 denoising steps, and for quicker inference, a UniPC multi-step scheduler [28] with 50 steps.\nDataset and Evaluation Metrics. For our study, we used the CAMUS echocardiography dataset\u00a0[13], featuring 2D apical views of both two-chamber (2CH) and four-chamber (4CH) perspectives from 500 patients across end-diastole (ED) and end-systole (ES) phases. The initial dataset randomly selected images from 50 patients for the test split, without segmentation labels, while the remaining 450 patients were assigned to the training split. Following the baseline work\u00a0[23], we allocated the first 50 patients for validation and utilized the remaining 400 patients for training, leading to a total of 1600/200 images for train/validation respectively. We used Fr\u00e9chet Inception Distance (FID)\u00a0[8] and Kernel Inception Distance (KID)\u00a0[2] to evaluate the quality of image synthesis. FID measures the distance between feature vectors of real and generated images, while KID quantifies the similarity between probability distributions of real and generated images\u2019 features. For the downstream tasks, namely segmentation, and classification, we opt for well-established metrics as shown in Table\u00a02 and 3(b).",
            "We investigated multiple data augmentation methods, including only real images (Real), a combination of 1600 real with 800 synthetic images (Real+50%), 1600 real with 1600 synthetic images (Real+100%), and 1600 real with 3200 synthetic images (Real+200%). These synthetic images were created using our text+segmentation model. For validation, we used 50 held-out patients, following [23] (It is noteworthy to mention that [23] developed their segmentation model using 8000 synthetic images.).\nTable\u00a02 and 3(b) present the comparative effectiveness of our generated data in diverse scenarios. Specifically, Table\u00a02 shows that our synthetic data distinctly impact the performance of different models, resulting in additional performance gains when utilizing data generated with comprehensive guidance compared to other competitors, as evidenced by various evaluation metrics. This also aligns with the visual comparison of echo segmentation in 3(a) which indicates that our text+segmentation guided model can produce echo images with better overall realism and a tendency to adhere to anatomical input constraints. Lastly, the classification results shown in 3(b) highlight the value of synthetic data, demonstrating that image-level details (text prompts) outperform those from full guidance (text+segmentation)."
        ]
    }
}