{
    "id_table_1": {
        "caption": "Table 1 :  Comparing on varied ID (PASCAL VOC  [ 16 ] , BDD-100K  [ 65 ] ) and OOD (MS-COCO  [ 36 ] , OpenImages  [ 32 ] ) datasets, our method significantly outperforms other methods on different metrics and achieves SOTA performance on OOD object detection. Our method is validated on two different existing object detectors, Faster R-CNN  [ 48 ]  and VOS  [ 15 ]  (denoted as  Ours +Faster-R-CNN +Faster-R-CNN {}_{\\text{+Faster-R-CNN}} start_FLOATSUBSCRIPT +Faster-R-CNN end_FLOATSUBSCRIPT  and  Ours +VOS +VOS {}_{\\text{+VOS}} start_FLOATSUBSCRIPT +VOS end_FLOATSUBSCRIPT  respectively). (Top results are shown in  bold .)",
        "table": "S4.T1.14.10",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "We introduce SyncOOD, an automatic data curation process that leverages foundation models as tools to harvest meaningful data from text-to-image generation models for OOD object detection (see Figure  1 ). The process is based on two key observations: 1) Hard OOD samples that are close to the ID data contribute more to learning a better OOD detector, and 2) Context may become a distracting cue for OOD object detection tasks, leading to bias towards contexts. With these observations in mind, the outlier synthesis process is formulated as box-conditioned image in-painting, and driven by Stable Diffusion  [ 49 ]  for high-quality controllable editing. The concepts to replace with are imagined by a Large Language Model (LLM)  [ 1 ]  with the aim of semantic novelty, 1 1 1 Concepts overlapping with the test data are removed to avoid information leakage.  and the associated bounding box is further refined with SAM  [ 30 ] . Automated by foundation models, this data collection pipeline requires minimal human labor, while producing high-quality OOD data.",
            "As illustrated in  Fig.   1 , our outlier synthesis pipeline consists of (1) synthesizing a set of effective photo-realistic scene-level OOD images  x edit superscript x edit \\textbf{x}^{\\text{edit}} x start_POSTSUPERSCRIPT edit end_POSTSUPERSCRIPT , denoted as  D edit = { ( x edit , b edit ) } subscript D edit superscript x edit superscript b edit \\mathcal{D}_{\\text{edit}}=\\left\\{(\\textbf{x}^{\\text{edit}},\\textbf{b}^{\\text{% edit}})\\right\\} caligraphic_D start_POSTSUBSCRIPT edit end_POSTSUBSCRIPT = { ( x start_POSTSUPERSCRIPT edit end_POSTSUPERSCRIPT , b start_POSTSUPERSCRIPT edit end_POSTSUPERSCRIPT ) } , which contains novel objects and corresponding annotation boxes  b edit superscript b edit \\textbf{b}^{\\text{edit}} b start_POSTSUPERSCRIPT edit end_POSTSUPERSCRIPT  based on region-level editing from  D id subscript D id \\mathcal{D}_{\\text{id}} caligraphic_D start_POSTSUBSCRIPT id end_POSTSUBSCRIPT  in a fully automated, labor-free way; and (2) select and use the efficient synthetic data to provide pseudo-OOD supervisions for training OOD object detector together with the ID samples in the training set. Further design of the pipeline requires answering the following questions: (1) how to distill the open-set knowledge embedded in foundation models to scene-level OOD data and (2) how to utilize the synthesized data to regularize the decision boundary and facilitate OOD object detection. We discuss them accordingly in  Sec.   3.1  and  Sec.   3.2 .",
            "where    \\gamma italic_  denotes a threshold value on IoU. It ensures a high enough recall rate to rule out the instability of Stable Diffusion and SAM and uncontrollable localization of the edited objects. Thus we obtain the synthetic outlier data  D edit subscript D edit \\mathcal{D}_{\\text{edit}} caligraphic_D start_POSTSUBSCRIPT edit end_POSTSUBSCRIPT  as illustrated in  Fig.   1 .",
            "We evaluate the performance of the proposed method on different challenging benchmarks and obtain notable results (see  Tab.   1 ). As the first work to introduce synthetic scene-level natural images as OOD samples, we incorporate our data-centric method to two off-the-shelf object detectors  [ 48 ,  15 ] , achieving new state-of-the-art performance in OOD object detection.",
            "Compared with previous methods, we present comprehensive and substantial performance improvements on FPR95. The encouraging outcomes clearly show that our synthetic data offers superior OOD supervision and are well-suited for forming a precise decision boundary between ID and OOD samples as illustrated in  Fig.   1 , which significantly reduces the interference caused by contextual information when optimizing the decision boundary.",
            "As mentioned in  Sec.   3.1 , we propose to utilize SAM-based refiner to correct the bounding boxes of novel objects to obtain higher-quality instance-level OOD features. Therefore, we comparatively remove the proposed refiner and directly used the corresponding editing masks as bounding boxes to extract OOD features for training. Taking PASCAL-VOC as the ID dataset, after removing the refiner, we obtain  39.55/13.72  of  FPR95  and  85.94/95.37  of  AUROC  on MS-COCO/OpenImages datasets, which is better than previous methods but worse than the results (Faster R-CNN +  Ours  in  Tab.   1 ) when using the refiner. This proves that OOD supervision signals contained in the synthetic data are already extracted and achieve good results under the localization of the fuzzy boxes, but more precise boxes mean higher quality features. More demos and analyses of the SAM-based refiner are shown in the supplementary material.",
            "We are the first to explore how to edit scene-level images to include novel categories, which contain annotation boxes and ensure context consistency, facilitating OOD object detection. The achieved state-of-the-art performance ( Tab.   1 ) benefits from the optimization of decision boundaries driven by high-quality OOD features. Our exploration demonstrates that  annotation boxes  and  context consistency  are particularly important for synthesizing high-quality OOD instances. On the one hand, high-quality annotation boxes provide the possibility to extract high-quality instance-level features from scene-level images, while unrefined boxes ( Sec.   4.2.3 ) or discarded boxes ( Tab.   5 ) will have a negative impact on performance. On the other hand, context consistency ensures the most effective OOD features are not interfered by different contexts and selected for utilizing ( Fig.   4 ).",
            "We are the first to build  an automatic, transparent, and low-cost pipeline  ( Sec.   3.1 ) for synthesizing scene-level images containing novel objects with annotation boxes and context consistency. It benefits object detectors robustness and reliability to unseen data and sets up clear state-of-the-art on multiple OOD object detection benchmarks. Specifically, we organically combine and cleverly use different foundation models  [ 1 ,  49 ,  30 ]  ( Sec.   3.1 ) to distill open-world knowledge and inpaint the existing scenes for simulating real OOD scenarios. In addition, our design  takes into account the instability of the current foundation models  and can release better potential performance in the future development of foundation models.",
            "We are the first to  explicitly decouple OOD data synthesis and selection . On the one hand, we ensure the separability of the synthetic objects in semantic concepts through open-world knowledge provided by LLMs ( Sec.   3.1 ). On the other hand, we ensure the similarity of ID and OOD objects in visual patterns ( Sec.   3.1 ), thereby optimizing the precise decision boundary. This line of thinking has the potential to facilitate more open-world solutions."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Ablation on the number of our synthetic data in training. Taking PASCAL-VOC as the ID dataset, we perform seven groups of random sampling with different numbers in the synthetic dataset to extract features as OOD samples, evaluate and report the performance on the MS-COCO/OpenImages datasets.",
        "table": "S4.T2.2.2",
        "footnotes": [],
        "references": [
            "As illustrated in  Fig.   1 , our outlier synthesis pipeline consists of (1) synthesizing a set of effective photo-realistic scene-level OOD images  x edit superscript x edit \\textbf{x}^{\\text{edit}} x start_POSTSUPERSCRIPT edit end_POSTSUPERSCRIPT , denoted as  D edit = { ( x edit , b edit ) } subscript D edit superscript x edit superscript b edit \\mathcal{D}_{\\text{edit}}=\\left\\{(\\textbf{x}^{\\text{edit}},\\textbf{b}^{\\text{% edit}})\\right\\} caligraphic_D start_POSTSUBSCRIPT edit end_POSTSUBSCRIPT = { ( x start_POSTSUPERSCRIPT edit end_POSTSUPERSCRIPT , b start_POSTSUPERSCRIPT edit end_POSTSUPERSCRIPT ) } , which contains novel objects and corresponding annotation boxes  b edit superscript b edit \\textbf{b}^{\\text{edit}} b start_POSTSUPERSCRIPT edit end_POSTSUPERSCRIPT  based on region-level editing from  D id subscript D id \\mathcal{D}_{\\text{id}} caligraphic_D start_POSTSUBSCRIPT id end_POSTSUBSCRIPT  in a fully automated, labor-free way; and (2) select and use the efficient synthetic data to provide pseudo-OOD supervisions for training OOD object detector together with the ID samples in the training set. Further design of the pipeline requires answering the following questions: (1) how to distill the open-set knowledge embedded in foundation models to scene-level OOD data and (2) how to utilize the synthesized data to regularize the decision boundary and facilitate OOD object detection. We discuss them accordingly in  Sec.   3.1  and  Sec.   3.2 .",
            "As shown in  Fig.   2 (a), based on the ID labels  Y id subscript Y id \\mathcal{Y}_{\\text{id}} caligraphic_Y start_POSTSUBSCRIPT id end_POSTSUBSCRIPT  in training set  D id subscript D id \\mathcal{D}_{\\text{id}} caligraphic_D start_POSTSUBSCRIPT id end_POSTSUBSCRIPT , we consider that novel concepts that are different from ID categories can be potential candidates for generating OOD objects. The next is to discover novel concepts that offer hard OOD samples sharing high visual similarity with ID samples and being contextually compatible with the scene context for object detection. Rather than relying on human labor to investigate all potential candidates, we leverage the vast knowledge and reasoning capabilities of LLM, GPT-4  [ 1 ]  to check the visual similarity and contextual compatibility. This allows us to associate ID objects and promote the conceptualization of possible novel objects to replace existing ID objects through the use of a prompt with in-context examples  [ 4 ]  as:",
            "With the discovered novel concepts  Y novel = { y 1 novel , y 2 novel , ... , y K novel } subscript Y novel subscript superscript y novel 1 subscript superscript y novel 2 ... subscript superscript y novel K \\mathcal{Y}_{\\text{novel}}=\\{\\textbf{y}^{\\text{novel}}_{1},\\textbf{y}^{\\text{% novel}}_{2},...,\\textbf{y}^{\\text{novel}}_{K}\\} caligraphic_Y start_POSTSUBSCRIPT novel end_POSTSUBSCRIPT = { y start_POSTSUPERSCRIPT novel end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , y start_POSTSUPERSCRIPT novel end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , y start_POSTSUPERSCRIPT novel end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT } , the next step is to use them as prompts for the text-to-image generation model to generate an image. To generate a new image with novel concepts  y j  y i novel subscript y j subscript superscript y novel i y_{j}\\in\\textbf{y}^{\\text{novel}}_{i} italic_y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  y start_POSTSUPERSCRIPT novel end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , we choose to replace existing ID objects in existing images with label  y i id superscript subscript y i id y_{i}^{\\text{id}} italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT id end_POSTSUPERSCRIPT  instead of finding new locations or generating one image from scratch. By doing so, we can ensure context compatibility and eliminate distractions from the scene context as it is preserved. As illustrated in  Fig.   2  (b), we use Stable-Diffusion-Inpainting  [ 49 ] , denoted as  SDI  (  ) SDI  \\text{SDI}(\\cdot) SDI (  ) , to perform region-level editing on ID images. The ID object is denoted as  x id superscript x id \\textbf{x}^{\\text{id}} x start_POSTSUPERSCRIPT id end_POSTSUPERSCRIPT , with its corresponding annotation box  b id superscript b id \\textbf{b}^{\\text{id}} b start_POSTSUPERSCRIPT id end_POSTSUPERSCRIPT  serving as the editing mask, and the associated imagined novel concept  y novel superscript y novel \\textbf{y}^{\\text{novel}} y start_POSTSUPERSCRIPT novel end_POSTSUPERSCRIPT  are provided as inputs to the  SDI , which is one of the most successful models for conditional image generation and editing. Thus, an edited image  x edit superscript x edit \\textbf{x}^{\\text{edit}} x start_POSTSUPERSCRIPT edit end_POSTSUPERSCRIPT  containing a novel object is obtained as:",
            "Due to the randomness in diffusion models, the attributes of edited objects, such as their quality, volume, and localization, may not match the original object box. To address this issue, as depicted in  Fig.   2  (c), we design an efficient and effective refiner based on SAM  [ 30 ]  to obtain refined accurate bounding boxes on novel objects. First, for an edited image  x edit superscript x edit \\textbf{x}^{\\text{edit}} x start_POSTSUPERSCRIPT edit end_POSTSUPERSCRIPT  with the editing mask  b id superscript b id \\textbf{b}^{\\text{id}} b start_POSTSUPERSCRIPT id end_POSTSUPERSCRIPT , we use a padding area extended from  b id superscript b id \\textbf{b}^{\\text{id}} b start_POSTSUPERSCRIPT id end_POSTSUPERSCRIPT  as the prompt and employ SAM to output the instance mask with highest confidence  m SAM superscript m SAM \\textbf{m}^{\\text{SAM}} m start_POSTSUPERSCRIPT SAM end_POSTSUPERSCRIPT  for the novel object in the area:",
            "Bridged by our synthetic data, foundation models extensive knowledge and powerful logic about novel concepts are effectively injected into our model through novel concept imagining and region-level editing. Furthermore, powered by the similarity-based filter, our synthetic data proves to be highly effective. Compared with SAFE  [ 59 ]  which uses a similar framework, we only use around 25% (on PASCAL-VOC) and 20% (on BDD-100K) of auxiliary data to achieve a significant performance improvement. Further analysis is presented in  Sec.   4.2 .",
            "We conduct an extensive ablation study on the quantity of synthetic data utilized, as illustrated in  Tab.   2 . We employ seven sets of synthetic data with varying quantities as OOD samples, using PASCAL-VOC as the ID dataset. Features are extracted and the OOD detector is trained based on the same Faster R-CNN checkpoint for each set. It is noteworthy that as the number of samples decreases from 14k to 2k, the performance does not deteriorate but rather maintains stable and superior results. This highlights our methods data efficiency (SAFE used 16k samples). Combined with our feature similarity-based filtering strategy as in  Sec.   3.2 , a small number of high-quality OOD samples with  visual similarity  directly promotes the optimization of precise decision boundaries to achieve stable improvements.",
            "We are the first to explore how to edit scene-level images to include novel categories, which contain annotation boxes and ensure context consistency, facilitating OOD object detection. The achieved state-of-the-art performance ( Tab.   1 ) benefits from the optimization of decision boundaries driven by high-quality OOD features. Our exploration demonstrates that  annotation boxes  and  context consistency  are particularly important for synthesizing high-quality OOD instances. On the one hand, high-quality annotation boxes provide the possibility to extract high-quality instance-level features from scene-level images, while unrefined boxes ( Sec.   4.2.3 ) or discarded boxes ( Tab.   5 ) will have a negative impact on performance. On the other hand, context consistency ensures the most effective OOD features are not interfered by different contexts and selected for utilizing ( Fig.   4 )."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  We study the impact of associating varying numbers of imagined novel objects with each ID object. Taking PASCAL-VOC as the ID dataset, we report the performance on MS-COCO/OpenImages datasets.",
        "table": "S4.T3.2.2",
        "footnotes": [],
        "references": [
            "As illustrated in  Fig.   1 , our outlier synthesis pipeline consists of (1) synthesizing a set of effective photo-realistic scene-level OOD images  x edit superscript x edit \\textbf{x}^{\\text{edit}} x start_POSTSUPERSCRIPT edit end_POSTSUPERSCRIPT , denoted as  D edit = { ( x edit , b edit ) } subscript D edit superscript x edit superscript b edit \\mathcal{D}_{\\text{edit}}=\\left\\{(\\textbf{x}^{\\text{edit}},\\textbf{b}^{\\text{% edit}})\\right\\} caligraphic_D start_POSTSUBSCRIPT edit end_POSTSUBSCRIPT = { ( x start_POSTSUPERSCRIPT edit end_POSTSUPERSCRIPT , b start_POSTSUPERSCRIPT edit end_POSTSUPERSCRIPT ) } , which contains novel objects and corresponding annotation boxes  b edit superscript b edit \\textbf{b}^{\\text{edit}} b start_POSTSUPERSCRIPT edit end_POSTSUPERSCRIPT  based on region-level editing from  D id subscript D id \\mathcal{D}_{\\text{id}} caligraphic_D start_POSTSUBSCRIPT id end_POSTSUBSCRIPT  in a fully automated, labor-free way; and (2) select and use the efficient synthetic data to provide pseudo-OOD supervisions for training OOD object detector together with the ID samples in the training set. Further design of the pipeline requires answering the following questions: (1) how to distill the open-set knowledge embedded in foundation models to scene-level OOD data and (2) how to utilize the synthesized data to regularize the decision boundary and facilitate OOD object detection. We discuss them accordingly in  Sec.   3.1  and  Sec.   3.2 .",
            "We conduct an extensive ablation study on the quantity of synthetic data utilized, as illustrated in  Tab.   2 . We employ seven sets of synthetic data with varying quantities as OOD samples, using PASCAL-VOC as the ID dataset. Features are extracted and the OOD detector is trained based on the same Faster R-CNN checkpoint for each set. It is noteworthy that as the number of samples decreases from 14k to 2k, the performance does not deteriorate but rather maintains stable and superior results. This highlights our methods data efficiency (SAFE used 16k samples). Combined with our feature similarity-based filtering strategy as in  Sec.   3.2 , a small number of high-quality OOD samples with  visual similarity  directly promotes the optimization of precise decision boundaries to achieve stable improvements.",
            "We employ in-context learning to guide LLM in associating new objects for driving image editing. For each ID obejct, the LLM connects a steady stream of novel objects. We further explore the impact of the number of corresponding novel objects for each ID object on data performance. We randomly sample different numbers of novel objects from LLMs responses for each ID object. As shown in  Tab.   3 , the performance remains stable despite changes in the number of concepts, further highlighting the stability and robustness of our synthetic data.",
            "As mentioned in  Sec.   3.1 , we propose to utilize SAM-based refiner to correct the bounding boxes of novel objects to obtain higher-quality instance-level OOD features. Therefore, we comparatively remove the proposed refiner and directly used the corresponding editing masks as bounding boxes to extract OOD features for training. Taking PASCAL-VOC as the ID dataset, after removing the refiner, we obtain  39.55/13.72  of  FPR95  and  85.94/95.37  of  AUROC  on MS-COCO/OpenImages datasets, which is better than previous methods but worse than the results (Faster R-CNN +  Ours  in  Tab.   1 ) when using the refiner. This proves that OOD supervision signals contained in the synthetic data are already extracted and achieve good results under the localization of the fuzzy boxes, but more precise boxes mean higher quality features. More demos and analyses of the SAM-based refiner are shown in the supplementary material.",
            "To provide more insights on the choices of filtering thresholds, we display some cases in different intervals of feature similarity in  Fig.   3 . We show the synthetic images, the corresponding initial images, and the difference between feature maps (denoted as Diff-map), respectively. The Diff-maps prove that the edited area is sensitively attended to by the model. But for images with extremely high similarity ( > 0.9 absent 0.9 >0.9 > 0.9 ), they always contain some editing failures and blurs. As illustrated on the top of the first column in  Fig.   3 , it is not intuitively evident what the  ship  has been edited into (the target object is a  raft ). Besides, as the similarity upperbound decreases, we progressively obtain more realistic and reasonable images. But note that when the similarity is excessively low, as seen in the last column of  Fig.   3 , the objects are edited into the corresponding text or an unnatural object, leading to image distortion. This strongly supports the idea that the quality and usability of edited images are closely connected to visual similarity. More cases and analyses are presented in the supplementary material.",
            "We are the first to explore how to edit scene-level images to include novel categories, which contain annotation boxes and ensure context consistency, facilitating OOD object detection. The achieved state-of-the-art performance ( Tab.   1 ) benefits from the optimization of decision boundaries driven by high-quality OOD features. Our exploration demonstrates that  annotation boxes  and  context consistency  are particularly important for synthesizing high-quality OOD instances. On the one hand, high-quality annotation boxes provide the possibility to extract high-quality instance-level features from scene-level images, while unrefined boxes ( Sec.   4.2.3 ) or discarded boxes ( Tab.   5 ) will have a negative impact on performance. On the other hand, context consistency ensures the most effective OOD features are not interfered by different contexts and selected for utilizing ( Fig.   4 ).",
            "We are the first to build  an automatic, transparent, and low-cost pipeline  ( Sec.   3.1 ) for synthesizing scene-level images containing novel objects with annotation boxes and context consistency. It benefits object detectors robustness and reliability to unseen data and sets up clear state-of-the-art on multiple OOD object detection benchmarks. Specifically, we organically combine and cleverly use different foundation models  [ 1 ,  49 ,  30 ]  ( Sec.   3.1 ) to distill open-world knowledge and inpaint the existing scenes for simulating real OOD scenarios. In addition, our design  takes into account the instability of the current foundation models  and can release better potential performance in the future development of foundation models.",
            "We are the first to  explicitly decouple OOD data synthesis and selection . On the one hand, we ensure the separability of the synthetic objects in semantic concepts through open-world knowledge provided by LLMs ( Sec.   3.1 ). On the other hand, we ensure the similarity of ID and OOD objects in visual patterns ( Sec.   3.1 ), thereby optimizing the precise decision boundary. This line of thinking has the potential to facilitate more open-world solutions."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  We randomly sample the same numbers of OOD features as the main experiment instead of using the feature filter (denoted as w/o filter), and evaluate on multiple datasets. The obtained results demonstrate the effectiveness of the proposed data filter.",
        "table": "S4.T4.8.8",
        "footnotes": [],
        "references": [
            "Bridged by our synthetic data, foundation models extensive knowledge and powerful logic about novel concepts are effectively injected into our model through novel concept imagining and region-level editing. Furthermore, powered by the similarity-based filter, our synthetic data proves to be highly effective. Compared with SAFE  [ 59 ]  which uses a similar framework, we only use around 25% (on PASCAL-VOC) and 20% (on BDD-100K) of auxiliary data to achieve a significant performance improvement. Further analysis is presented in  Sec.   4.2 .",
            "The filter is designed to incorporate the most useful data into training, and avoid unnecessary noise. The design is reflected in two aspects: on one hand, the outlier object should process similar visual patterns to the original object, thus being confusing and can facilitate learning; on the other hand, over-high similarity may indicate failures of the editing process ( e.g . , when the concept is not an object). These considerations are applied as thresholding on pairwise cosine similarity between object features, as in  Eq.   5 . As shown in  Tab.   4 , this filter brings a notable improvement across benchmarks.",
            "Given the importance of scene-level synthesis as discussed in the previous paragraph, we study the factors that make scene-level editing indispensable, and find context consistency to be a crucial one. Besides calculating the similarity between ID/OOD object pairs before/after box-conditioned editing, we also try further editing parts of the background of the already edited images, and also calculate its object similarity with the initial object. As shown in  Fig.   4 , even small editing on parts of the background (out of the object boxes) can make foreground objects look notably different as perceived by the detector. This highlights the importance of keeping the context unchanged when synthesizing outlier samples, in that if the context is changed, the model easily identifies the object as OOD and cannot break the context bias.",
            "We are the first to explore how to edit scene-level images to include novel categories, which contain annotation boxes and ensure context consistency, facilitating OOD object detection. The achieved state-of-the-art performance ( Tab.   1 ) benefits from the optimization of decision boundaries driven by high-quality OOD features. Our exploration demonstrates that  annotation boxes  and  context consistency  are particularly important for synthesizing high-quality OOD instances. On the one hand, high-quality annotation boxes provide the possibility to extract high-quality instance-level features from scene-level images, while unrefined boxes ( Sec.   4.2.3 ) or discarded boxes ( Tab.   5 ) will have a negative impact on performance. On the other hand, context consistency ensures the most effective OOD features are not interfered by different contexts and selected for utilizing ( Fig.   4 )."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 :  Comparing varied images as OOD samples for training, we first show some synthetic object-centric images generated by Stable Diffusion (left side). Then with PASCAL-VOC as ID dataset, we report the results obtained by using synthetic object-centric images (denoted as object-centric images) and scene-level images with novel objects but without bounding boxes (denoted as scene-level w/o boxes) as OOD samples to participate in training.",
        "table": "S4.T5.5.5",
        "footnotes": [],
        "references": [
            "The filter is designed to incorporate the most useful data into training, and avoid unnecessary noise. The design is reflected in two aspects: on one hand, the outlier object should process similar visual patterns to the original object, thus being confusing and can facilitate learning; on the other hand, over-high similarity may indicate failures of the editing process ( e.g . , when the concept is not an object). These considerations are applied as thresholding on pairwise cosine similarity between object features, as in  Eq.   5 . As shown in  Tab.   4 , this filter brings a notable improvement across benchmarks.",
            "Through regional-level editing, we replace the ID object with a novel object with a bounding box and ensure consistent context information. However, some simpler methods based on foundation models also achieve the acquisition and use of OOD data. For example, Dream-OOD  [ 13 ]  uses well-trained text-conditional space and diffusion model to synthesize realistic object-centric data for promoting OOD image classification. Similarly, keeping other settings unchanged, we use our novel concepts to drive Stable-Diffusion instead of Stable-Diffusion-Inpainting  [ 49 ]  to synthesize novel images, which are also processed and filtered by our proposed refiner and filter (some synthesized images are shown in  Tab.   5 ), thereby participating in the training as OOD supervision. However, as shown in  Tab.   5  (object-centric images), the synthetic novel object-centric images do not aid in training and result in poor performance, even though they possess high visual quality. This clearly validates our decision to edit scene-level images rather than composing new ones, and highlights the significance of maintaining contextual consistency.",
            "Additionally, we examine the possibility of using the edited scene-level image as a whole (ignoring the boxes) as OOD samples in the training process. The results, as depicted in  Tab.   5  (scene-level w/o boxes), are significantly inferior compared to our methods performance. This demonstrates that controllable bounding boxes are indispensable in this task.",
            "We are the first to explore how to edit scene-level images to include novel categories, which contain annotation boxes and ensure context consistency, facilitating OOD object detection. The achieved state-of-the-art performance ( Tab.   1 ) benefits from the optimization of decision boundaries driven by high-quality OOD features. Our exploration demonstrates that  annotation boxes  and  context consistency  are particularly important for synthesizing high-quality OOD instances. On the one hand, high-quality annotation boxes provide the possibility to extract high-quality instance-level features from scene-level images, while unrefined boxes ( Sec.   4.2.3 ) or discarded boxes ( Tab.   5 ) will have a negative impact on performance. On the other hand, context consistency ensures the most effective OOD features are not interfered by different contexts and selected for utilizing ( Fig.   4 )."
        ]
    }
}