{
    "id_table_1": {
        "caption": "",
        "table": "S4.E7",
        "footnotes": [],
        "references": [
            "Numerous methodologies for synthetic data generation have been advanced  (Patel et al.,  2024 ; Mller et al.,  2023 ; Park et al.,  2024 ) , yet the most prevalent and efficacious approach within the community involves generating synthetic data through sampling from a proficiently trained generative model, often another LLM tailored for specific domain tasks. To delineate this process more precisely,  Long et al. ( 2024 )  describe the generation of synthetic data as follows: a well-trained generative model  M M M italic_M  is utilized, and synthetic data  S gen subscript S gen S_{\\text{gen}} italic_S start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT  is produced by sampling from  M M M italic_M , conditioned on a set of prompts  p p p italic_p , just as illustrated in the lower part of Figure  1  (a).",
            "For simplicity, we note that  S anchor  D similar-to subscript S anchor D S_{\\text{anchor}}\\sim\\mathcal{D} italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT  caligraphic_D ,  p  D p similar-to p subscript D p p\\sim\\mathcal{D}_{p} italic_p  caligraphic_D start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ,  M ( p )  D M (  | p ) M(p)\\sim\\mathcal{D}_{M}(\\cdot|p) italic_M ( italic_p )  caligraphic_D start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT (  | italic_p ) , and  S gen  D gen similar-to subscript S gen subscript D gen S_{\\text{gen}}\\sim\\mathcal{D}_{\\text{gen}} italic_S start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT  caligraphic_D start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT , and comprehensive details about the relationships between the distributions are listed in Appendix  C . The overall synthetic data generation process in our modeling is depicted in Figure  1  (a). This illustration enhances our understanding of the connection between the generation process and distributions.",
            "The lower part of Figure  1  (a) details the specific stages of data generation. Initially, the anchor data  S anchor subscript S anchor S_{\\text{anchor}} italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT  undergoes a transformation via the function   T subscript italic- T \\phi_{\\mathcal{T}} italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT  to constitute the prompt  p p p italic_p , which in turn is used by the LLM  M M M italic_M  to generate the synthetic data  S gen subscript S gen S_{\\text{gen}} italic_S start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT , incorporating noise   italic- \\epsilon italic_ . The upper portion of Figure  1  (a) delineates the corresponding process of distribution shift.  S anchor subscript S anchor S_{\\text{anchor}} italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT  is derived from distribution  D D \\mathcal{D} caligraphic_D , and the prompt  p p p italic_p  emerges from distribution  D p subscript D p \\mathcal{D}_{p} caligraphic_D start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT  conditioned on   T subscript italic- T \\phi_{\\mathcal{T}} italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT . The LLM  M M M italic_M  produces the output  M  ( p ) M p M(p) italic_M ( italic_p )  from the conditional distribution  D M (  | p ) \\mathcal{D}_{M}(\\cdot|p) caligraphic_D start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT (  | italic_p ) , and the final synthetic data  S gen subscript S gen S_{\\text{gen}} italic_S start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT  is sampled from  D gen subscript D gen \\mathcal{D}_{\\text{gen}} caligraphic_D start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT , representing a convolution of  D M subscript D M \\mathcal{D}_{M} caligraphic_D start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT  and  D  subscript D italic- \\mathcal{D}_{\\epsilon} caligraphic_D start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  also conditioned on  p p p italic_p .",
            "Given that  D p subscript D p \\mathcal{D}_{p} caligraphic_D start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT  relates solely to   T subscript italic- T \\phi_{\\mathcal{T}} italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT  and  D D \\mathcal{D} caligraphic_D  (or  S anchor subscript S anchor S_{\\text{anchor}} italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT ), and  D gen subscript D gen \\mathcal{D}_{\\text{gen}} caligraphic_D start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT  related only to  M M M italic_M  and  p p p italic_p , the transition from  S anchor subscript S anchor S_{\\text{anchor}} italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT  to  p p p italic_p  to  S gen subscript S gen S_{\\text{gen}} italic_S start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT , (i.e.  S anchor  p  S gen  subscript S anchor p  subscript S gen S_{\\text{anchor}}\\rightarrow p\\rightarrow S_{\\text{gen}} italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT  italic_p  italic_S start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT ), constitutes a Markov chain. Figure  1  (b) provides a comprehensive view of the distributions and the nature of the distribution shift discussed. Specifically,  D D \\mathcal{D} caligraphic_D  is denoted as the orange circle, and  D M subscript D M \\mathcal{D}_{M} caligraphic_D start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT  is denoted as the blue circle. After  M M M italic_M  being prompted on  p p p italic_p , the conditioned distribution  D M (  | p ) \\mathcal{D}_{M}(\\cdot|p) caligraphic_D start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT (  | italic_p )  is denoted as all blue areas, and the final  D gen subscript D gen \\mathcal{D}_{\\text{gen}} caligraphic_D start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT  is represented as the deep blue area after the compression on   italic- \\epsilon italic_ . This illustration aids in understanding that  the generation process essentially compresses the output distribution of  M M M italic_M ,  D M subscript D M \\mathcal{D}_{M} caligraphic_D start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT , towards the post-training target distribution  D D \\mathcal{D} caligraphic_D , based on the conditions imposed by the prompt  p p p italic_p  and noise   italic- \\epsilon italic_ .",
            "The proof is referred to the Appendix  D . The divergences can be defined as the task divergence ( D TV  ( D , D M ) subscript D TV D subscript D M D_{\\text{TV}}(\\mathcal{D},\\mathcal{D}_{M}) italic_D start_POSTSUBSCRIPT TV end_POSTSUBSCRIPT ( caligraphic_D , caligraphic_D start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT ) ) and the generation divergence ( D TV  ( D M , D gen ) subscript D TV subscript D M subscript D gen D_{\\text{TV}}(\\mathcal{D}_{M},\\mathcal{D}_{\\text{gen}}) italic_D start_POSTSUBSCRIPT TV end_POSTSUBSCRIPT ( caligraphic_D start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT ) ), which is denoted in Figure  1  (b). The task divergence is determined by the ability of the LLM  M M M italic_M  and the relevance with the task  T T \\mathcal{T} caligraphic_T . The generation divergence is determined by the generation process including the prompt engineering and the data curation. In the training practice, the two divergences are controlled by either the strong ability of  M M M italic_M  or the strict prompt engineering, this partially explains why synthetic data is effective.",
            "In preceding sections, we established a comprehensive modeling for synthetic data generation and elucidated the connection between this process and the generalization error as delineated in Lemma  3.1 . This section delves deeper into the implications of the synthetic data generation process on the generalization capabilities.",
            "These factors collectively influence the generalization performance, indicating that a better generalization ability can be achieved by enhancing the information gain, reducing the compression bottleneck, minimizing the entropy, and balancing the efficiency. Finally, by integrating the insights from Lemma  3.1 , the overall upper bound of the expected generalization error in the LLM post-training with synthetic data can be derived as a comprehensive boundary in Theorem  4.7 .",
            "A larger upper bound for the GGMI signifies greater potential generalization benefits when utilizing synthetic data. To elucidate the impact of synthetic data on model generalization, we isolate the influence of  W  superscript W  W^{{}^{\\prime}} italic_W start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT  end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT  and establish that the GGMI can be effectively bounded. This relationship is formalized in Theorem  4.10 , which demonstrates that GGMI is constrained by    I  I \\Delta I roman_ italic_I the measure of information gainand various entropy components solely associated with  W W W italic_W .",
            "The proof is referred to Appendix  F . Consequently, we proceed to conduct a thorough analysis of each component specified in Theorem  4.10 .",
            "As emphasized in  (Long et al.,  2024 ) , the generation of synthetic data typically focuses on two primary objectives: faithfulness and diversity. These objectives are associated with  H  ( S anchor | W ) H conditional subscript S anchor W H(S_{\\text{anchor}}|W) italic_H ( italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT | italic_W )  and    I  I \\Delta I roman_ italic_I , respectively. Specifically,  H  ( S anchor | W ) H conditional subscript S anchor W H(S_{\\text{anchor}}|W) italic_H ( italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT | italic_W ) , which quantifies the relevance between  W W W italic_W  and  S anchor subscript S anchor S_{\\text{anchor}} italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT , as presented in Theorem  4.10 , encourages the model to align with the information derived from the original real data. This alignment is crucial for achieving the objective of faithfulness. In addition,    I  I \\Delta I roman_ italic_I  serves as a measurement of the additional information introduced by the generative model  M M M italic_M . Given that  M M M italic_M  is typically pre-trained on a more extensive dataset,    I  I \\Delta I roman_ italic_I  in Theorem  4.10  promotes the objective of diversity by facilitating greater information gain from  M M M italic_M .",
            "To control the variables outlined in Theorem  4.10 , we adjust the number of components in the GMM  M M M italic_M  and the ground-truth GMM  G G G italic_G . The result is illustrated in Figure  4 . Generally, increasing  J J J italic_J  facilitates the scaling of    I  I \\Delta I roman_ italic_I , resulting in a larger upper bound for GGMI. In contrast, larger  K K K italic_K  amplifies the influence of anchor data within the post-training target distribution, thereby increasing the  H  ( S anchor | W ) H conditional subscript S anchor W H(S_{\\text{anchor}}|W) italic_H ( italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT | italic_W )  term and tightening the upper bound of GGMI. Additionally, while an increase in  L L L italic_L  enhances  H  ( S gen | W ) H conditional subscript S gen W H(S_{\\text{gen}}|W) italic_H ( italic_S start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT | italic_W )  due to the introduction of greater diversity, it concurrently leads to a reduction in    H  H \\Delta H roman_ italic_H . As a result, we observe a trade-off manifested as a decrease in the KL Gap in our simulation outcomes.",
            "We summarize the notations used in subsection  3.1  and provide their specific definitions.",
            "Together with Eq. ( 11 ), Eq. ( 12 ), and Eq. ( 13 ), we have:",
            "Building upon equation ( 16 ), we can derive the following equations:",
            "Lemma  E.1  indicates that the entropy of  e p subscript e p e_{p} italic_e start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT  is upper bounded by the mutual information between the synthetic factor  e p subscript e p e_{p} italic_e start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT  and the model output  M  ( p ) M p M(p) italic_M ( italic_p )  by a factor of    \\lambda italic_ . In other words, the efficiency of the model utilizing the prompt is reflected in the value of    \\lambda italic_ , which quantifies the extent to which the model can leverage the information contained in the prompt. For example, a larger    \\lambda italic_  indicates a smaller  I  ( e p , M  ( p ) ) I subscript e p M p I(e_{p},M(p)) italic_I ( italic_e start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_M ( italic_p ) ) , which implies that the  M  ( p ) M p M(p) italic_M ( italic_p )  contains more information from the prompt  p p p italic_p .",
            "Building upon Lemma  E.1 , we can further derive the deduction following equation ( 21 ):",
            "Together with equations ( 17 ) and ( 26 ), we have:",
            "By the definition of GGMI, and with equation ( 31 ), the following result can be deduced:"
        ]
    },
    "id_table_2": {
        "caption": "",
        "table": "A6.EGx1",
        "footnotes": [],
        "references": [
            "The remainder of this paper is structured as follows. In Section  2 , we provide a comprehensive review of literature pertinent to our research. In Section  3 , we first delineate the symbols and foundational concepts critical to our analysis, then introduce the modeling for synthetic data generation and bridge its connection with generalization capability of post-trained models. Section  4  introduces our novel reverse-bottleneck framework, designed to assess the effects of synthetic data on post-training stages of LLMs, and to establish generalization error upper bounds. The paper concludes with Section  5 , summarizing our findings and discussing potential avenues for future research.",
            "To provide a clearer visualization, we simulate the distribution relationships using GMMs, the result is depicted in Figure  2 . The anchor data is represented by blue dots, and their distribution is illustrated by blue ellipses. In contrast, synthetic data is represented by orange. The distributions of  S gen subscript S gen S_{\\text{gen}} italic_S start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT  are visualized as an effort to encompass the distributions of  S anchor subscript S anchor S_{\\text{anchor}} italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT . However, since  S gen subscript S gen S_{\\text{gen}} italic_S start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT  is derived from the model  M M M italic_M , which incorporates more complex distribution components,  the distribution of  S gen subscript S gen S_{\\text{gen}} italic_S start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT  not only attempts to mirror  S anchor subscript S anchor S_{\\text{anchor}} italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT  but also extends beyond, covering broader areas.",
            "Subsection  3.2  offers an exhaustive examination of the synthetic data generation process, which is pivotal for elucidating the generalization error associated with the under-aligned LLM    \\pi italic_  when applied to synthetic data  S gen subscript S gen S_{\\text{gen}} italic_S start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT . This subsection endeavors to correlate the generalization error of    \\pi italic_  on the synthetic data  S gen subscript S gen S_{\\text{gen}} italic_S start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT  with the synthetic data generation process as previously delineated.",
            "Together with Eq. ( 11 ), Eq. ( 12 ), and Eq. ( 13 ), we have:",
            "Building upon Lemma  E.1 , we can further derive the deduction following equation ( 21 ):",
            "The proof of Lemma  E.2  is listed in equation ( 25 ):",
            "Building upon Lemma  E.2 , we can further deduce the following inequality following equation ( 23 ):",
            "Together with equations ( 17 ) and ( 26 ), we have:",
            "Subsequently, together with equations ( 30 ) and ( 32 ), we can further deduce that:"
        ]
    },
    "id_table_3": {
        "caption": "",
        "table": "A4.E11",
        "footnotes": [],
        "references": [
            "The remainder of this paper is structured as follows. In Section  2 , we provide a comprehensive review of literature pertinent to our research. In Section  3 , we first delineate the symbols and foundational concepts critical to our analysis, then introduce the modeling for synthetic data generation and bridge its connection with generalization capability of post-trained models. Section  4  introduces our novel reverse-bottleneck framework, designed to assess the effects of synthetic data on post-training stages of LLMs, and to establish generalization error upper bounds. The paper concludes with Section  5 , summarizing our findings and discussing potential avenues for future research.",
            "Subsection  3.2  offers an exhaustive examination of the synthetic data generation process, which is pivotal for elucidating the generalization error associated with the under-aligned LLM    \\pi italic_  when applied to synthetic data  S gen subscript S gen S_{\\text{gen}} italic_S start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT . This subsection endeavors to correlate the generalization error of    \\pi italic_  on the synthetic data  S gen subscript S gen S_{\\text{gen}} italic_S start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT  with the synthetic data generation process as previously delineated.",
            "In preceding sections, we established a comprehensive modeling for synthetic data generation and elucidated the connection between this process and the generalization error as delineated in Lemma  3.1 . This section delves deeper into the implications of the synthetic data generation process on the generalization capabilities.",
            "The former Markov chain, as depicted in the left part of Figure  3 , parallels a classical machine learning (ML) process, in which the input  X X X italic_X  is transformed into a latent representation  Z Z Z italic_Z  via an encoder, and then  Z Z Z italic_Z  is further decoded into the output  Y Y Y italic_Y  through a decoder. Similarly, in the synthetic data generation process, the input  S anchor subscript S anchor S_{\\text{anchor}} italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT  is converted to  p p p italic_p  (which is often assumed as latent in practical applications) via   T subscript italic- T \\phi_{\\mathcal{T}} italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT , and subsequently  p p p italic_p  is transformed into  S gen subscript S gen S_{\\text{gen}} italic_S start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT  by  M M M italic_M . However, the presence of the latter Markov chain introduces a crucial distinction between the two processes from an information flow perspective due to the prior knowledge embedded by  M M M italic_M . As illustrated in the right part of Figure  3 , unlike classic ML process,  the synthetic data generation process leverages  M M M italic_M  to facilitate information gains, thereby enriching the informational content of  S gen subscript S gen S_{\\text{gen}} italic_S start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT .",
            "These factors collectively influence the generalization performance, indicating that a better generalization ability can be achieved by enhancing the information gain, reducing the compression bottleneck, minimizing the entropy, and balancing the efficiency. Finally, by integrating the insights from Lemma  3.1 , the overall upper bound of the expected generalization error in the LLM post-training with synthetic data can be derived as a comprehensive boundary in Theorem  4.7 .",
            "We summarize the notations used in subsection  3.1  and provide their specific definitions.",
            "Together with Eq. ( 11 ), Eq. ( 12 ), and Eq. ( 13 ), we have:",
            "Building upon Lemma  E.2 , we can further deduce the following inequality following equation ( 23 ):",
            "By the definition of GGMI, and with equation ( 31 ), the following result can be deduced:",
            "Subsequently, together with equations ( 30 ) and ( 32 ), we can further deduce that:"
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "A4.E12",
        "footnotes": [],
        "references": [
            "The remainder of this paper is structured as follows. In Section  2 , we provide a comprehensive review of literature pertinent to our research. In Section  3 , we first delineate the symbols and foundational concepts critical to our analysis, then introduce the modeling for synthetic data generation and bridge its connection with generalization capability of post-trained models. Section  4  introduces our novel reverse-bottleneck framework, designed to assess the effects of synthetic data on post-training stages of LLMs, and to establish generalization error upper bounds. The paper concludes with Section  5 , summarizing our findings and discussing potential avenues for future research.",
            "In this subsection, we endeavor to derive the upper bounds of the generalization error from an information-flow perspective, employing the concepts previously defined. We initiate our analysis with a classical information upper bound applicable to deep neural networks, as elaborated in Lemma  4.4   (Zhang et al.,  2018 ) .",
            "Lemma  4.4  establishes a connection between the expected generalization error and the mutual information between training data  S S S italic_S  and learned model parameters  W W W italic_W . Despite network depth  L L L italic_L  and instance volume  n n n italic_n , the principal constraint is imposed by the mutual information term.",
            "Accordingly, in scenarios where post-training is with synthetic data, the generalization error is inherently constrained by the mutual information between the synthetic data  S gen subscript S gen S_{\\text{gen}} italic_S start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT  and LLM parameters after training, denoted as  I  ( S gen , W ) I subscript S gen W I(S_{\\text{gen}},W) italic_I ( italic_S start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT , italic_W ) . Characterizing this term presents a significant challenge due to the difficulty in measuring mutual information accurately. To address this, we introduce an analytical upper bound for  I  ( S gen , W ) I subscript S gen W I(S_{\\text{gen}},W) italic_I ( italic_S start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT , italic_W )  in Lemma  4.5  to facilitate a more comprehensive understanding of the dynamics influencing model performance in post-training.",
            "where    , p subscript  italic- p \\delta_{\\epsilon,p} italic_ start_POSTSUBSCRIPT italic_ , italic_p end_POSTSUBSCRIPT  indicates the efficiency during the data curation and model prompting process, which is detailed in the proof in Appendix  E . Together with Lemma  4.4 , we can further derive an upper bound for a training procedure with relation to the synthetic data defined above in Lemma  4.6 .",
            "Lemma  4.6  delineates a quantifiable upper bound for the expected generalization error in relation to synthetic data. Beyond basic configuration parameters such as network depth  L L L italic_L  and data size  n n n italic_n , this upper bound is determined by four key factors outlined in the corresponding remarks.",
            "These factors collectively influence the generalization performance, indicating that a better generalization ability can be achieved by enhancing the information gain, reducing the compression bottleneck, minimizing the entropy, and balancing the efficiency. Finally, by integrating the insights from Lemma  3.1 , the overall upper bound of the expected generalization error in the LLM post-training with synthetic data can be derived as a comprehensive boundary in Theorem  4.7 .",
            "(Synthetic data post-training upper bound.) For the same condition as lemma  4.6  and a synthetic data generation process described above, the generalization error of the model    \\pi italic_  post-trained on the synthetic data can be bounded as:",
            "Theorem  4.7  establishes a general upper bound for the generalization error of LLMs post-trained with synthetic data. In this section, our objective is to analyze the generalization gains achieved by using synthetic data compared to scenarios devoid of synthetic data.",
            "We commence our analysis with the anchor data  S anchor subscript S anchor S_{\\text{anchor}} italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT . Analogous to the definition of  Err  (  S gen ) Err superscript  subscript S gen \\operatorname{Err}(\\pi^{S_{\\text{gen}}}) roman_Err ( italic_ start_POSTSUPERSCRIPT italic_S start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) , the generalization error of an LLM that has been post-trained on  S anchor subscript S anchor S_{\\text{anchor}} italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT  is defined as  Err  (  S anchor ) = | R D  (  S anchor )  R ^ S anchor  (  S anchor ) | Err superscript  subscript S anchor subscript R D superscript  subscript S anchor subscript ^ R subscript S anchor superscript  subscript S anchor \\operatorname{Err}(\\pi^{S_{\\text{anchor}}})=\\left|R_{\\mathcal{D}}(\\pi^{S_{% \\text{anchor}}})-\\widehat{R}_{S_{\\text{anchor}}}(\\pi^{S_{\\text{anchor}}})\\right| roman_Err ( italic_ start_POSTSUPERSCRIPT italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) = | italic_R start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ( italic_ start_POSTSUPERSCRIPT italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) - over^ start_ARG italic_R end_ARG start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_ start_POSTSUPERSCRIPT italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) | . It is logically sound to assume that  S anchor subscript S anchor S_{\\text{anchor}} italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT  is sampled from the distribution  D D D italic_D . Building upon Lemma  4.4  and assume that  S anchor subscript S anchor S_{\\text{anchor}} italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT  comprises  m m m italic_m  instances, we can derive the subsequent result in Lemma  4.8 .",
            "(Anchor data post-training upper bound.) For the same condition as lemma  4.6 , the generalization error of the model    \\pi italic_  post-trained on the anchor data can be bounded as:",
            "Given that  m << n much-less-than m n m<<n italic_m < < italic_n  typically applies in real-world scenarios, Lemma  4.8  often represents a less stringent upper bound compared to Lemma  4.4 , this results in potentially poorer generalization when relying solely on  S anchor subscript S anchor S_{\\text{anchor}} italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT  rather than utilizing synthetic data.",
            "But a pertinent question arises:  do other aspects of synthetic data generation, beyond the influence of data size, also contribute to improvements in generalization performance?  Our focus is on examining how various elements within the synthetic data process impact generalization during post-training. It is inappropriate, however, to directly compare other components across these two bounds due to variations in loss and training data specifics, which affect the parameters    \\eta italic_  and  W W W italic_W  differently, where    \\eta italic_  represents a measure of information compression and is challenging to quantify accurately  (Zhang et al.,  2018 ) . Thus, our analysis primarily centers on the mutual information terms  I  ( S anchor , W  ) I subscript S anchor superscript W  I(S_{\\text{anchor}},W^{{}^{\\prime}}) italic_I ( italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT , italic_W start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT  end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT )  and  I  ( S gen , W ) I subscript S gen W I(S_{\\text{gen}},W) italic_I ( italic_S start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT , italic_W ) . To systematically evaluate the generalization capabilities conferred by synthetic data in relation to these mutual information metrics, we introduce a definition for generalization gain measurement as Definition  4.9 .",
            "A larger upper bound for the GGMI signifies greater potential generalization benefits when utilizing synthetic data. To elucidate the impact of synthetic data on model generalization, we isolate the influence of  W  superscript W  W^{{}^{\\prime}} italic_W start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT  end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT  and establish that the GGMI can be effectively bounded. This relationship is formalized in Theorem  4.10 , which demonstrates that GGMI is constrained by    I  I \\Delta I roman_ italic_I the measure of information gainand various entropy components solely associated with  W W W italic_W .",
            "The proof is referred to Appendix  F . Consequently, we proceed to conduct a thorough analysis of each component specified in Theorem  4.10 .",
            "As emphasized in  (Long et al.,  2024 ) , the generation of synthetic data typically focuses on two primary objectives: faithfulness and diversity. These objectives are associated with  H  ( S anchor | W ) H conditional subscript S anchor W H(S_{\\text{anchor}}|W) italic_H ( italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT | italic_W )  and    I  I \\Delta I roman_ italic_I , respectively. Specifically,  H  ( S anchor | W ) H conditional subscript S anchor W H(S_{\\text{anchor}}|W) italic_H ( italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT | italic_W ) , which quantifies the relevance between  W W W italic_W  and  S anchor subscript S anchor S_{\\text{anchor}} italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT , as presented in Theorem  4.10 , encourages the model to align with the information derived from the original real data. This alignment is crucial for achieving the objective of faithfulness. In addition,    I  I \\Delta I roman_ italic_I  serves as a measurement of the additional information introduced by the generative model  M M M italic_M . Given that  M M M italic_M  is typically pre-trained on a more extensive dataset,    I  I \\Delta I roman_ italic_I  in Theorem  4.10  promotes the objective of diversity by facilitating greater information gain from  M M M italic_M .",
            "To control the variables outlined in Theorem  4.10 , we adjust the number of components in the GMM  M M M italic_M  and the ground-truth GMM  G G G italic_G . The result is illustrated in Figure  4 . Generally, increasing  J J J italic_J  facilitates the scaling of    I  I \\Delta I roman_ italic_I , resulting in a larger upper bound for GGMI. In contrast, larger  K K K italic_K  amplifies the influence of anchor data within the post-training target distribution, thereby increasing the  H  ( S anchor | W ) H conditional subscript S anchor W H(S_{\\text{anchor}}|W) italic_H ( italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT | italic_W )  term and tightening the upper bound of GGMI. Additionally, while an increase in  L L L italic_L  enhances  H  ( S gen | W ) H conditional subscript S gen W H(S_{\\text{gen}}|W) italic_H ( italic_S start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT | italic_W )  due to the introduction of greater diversity, it concurrently leads to a reduction in    H  H \\Delta H roman_ italic_H . As a result, we observe a trade-off manifested as a decrease in the KL Gap in our simulation outcomes."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "A4.E13",
        "footnotes": [],
        "references": [
            "The remainder of this paper is structured as follows. In Section  2 , we provide a comprehensive review of literature pertinent to our research. In Section  3 , we first delineate the symbols and foundational concepts critical to our analysis, then introduce the modeling for synthetic data generation and bridge its connection with generalization capability of post-trained models. Section  4  introduces our novel reverse-bottleneck framework, designed to assess the effects of synthetic data on post-training stages of LLMs, and to establish generalization error upper bounds. The paper concludes with Section  5 , summarizing our findings and discussing potential avenues for future research.",
            "Accordingly, in scenarios where post-training is with synthetic data, the generalization error is inherently constrained by the mutual information between the synthetic data  S gen subscript S gen S_{\\text{gen}} italic_S start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT  and LLM parameters after training, denoted as  I  ( S gen , W ) I subscript S gen W I(S_{\\text{gen}},W) italic_I ( italic_S start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT , italic_W ) . Characterizing this term presents a significant challenge due to the difficulty in measuring mutual information accurately. To address this, we introduce an analytical upper bound for  I  ( S gen , W ) I subscript S gen W I(S_{\\text{gen}},W) italic_I ( italic_S start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT , italic_W )  in Lemma  4.5  to facilitate a more comprehensive understanding of the dynamics influencing model performance in post-training.",
            "We utilize Gaussian Mixture Models (GMMs) to simulate the data generation process, as illustrated in Figure  5 . Overall, we use a gt-GMM to simulate the ground-truth, or the post-training target distribution, and a GMM  M M M italic_M  to simulate the generative model applied in the data generation process.",
            "Their are three parts of components in the GMMs: the anchor sample part with  K K K italic_K  components, the unsampled part with  J J J italic_J  components, and task-irrelevant part with  L L L italic_L  components. It is assumed that the post-training target distribution is a combination of the anchor sample part and the unsampled part, thus the gt-GMM contains  K + J K J K+J italic_K + italic_J  components from the anchor sample part and the unsampled part, which is denoted as blue in Figure  5 . However, the anchor data is only sampled from the anchor sample part. This is a reasonable assumption for the real-world scenario, since the anchor data is sparse and hard to cover the whole post-training task distribution.",
            "Additionally, the generative model  M M M italic_M  is assumed to be a GMM with  K + J + L K J L K+J+L italic_K + italic_J + italic_L  components. Except for the post-training target distribution,  M M M italic_M  also contains a task-irrelevant part, which is denoted as orange in Figure  5 . This is due to the fact that the generative model is always pre-trained on a larger scale of data, and may not be perfectly aligned with the post-training target distribution, and may introduce task-irrelevant components in the synthetic data generation process.",
            "The proof of Lemma  E.2  is listed in equation ( 25 ):"
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "A4.E14",
        "footnotes": [],
        "references": [
            "where    , p subscript  italic- p \\delta_{\\epsilon,p} italic_ start_POSTSUBSCRIPT italic_ , italic_p end_POSTSUBSCRIPT  indicates the efficiency during the data curation and model prompting process, which is detailed in the proof in Appendix  E . Together with Lemma  4.4 , we can further derive an upper bound for a training procedure with relation to the synthetic data defined above in Lemma  4.6 .",
            "Lemma  4.6  delineates a quantifiable upper bound for the expected generalization error in relation to synthetic data. Beyond basic configuration parameters such as network depth  L L L italic_L  and data size  n n n italic_n , this upper bound is determined by four key factors outlined in the corresponding remarks.",
            "(Synthetic data post-training upper bound.) For the same condition as lemma  4.6  and a synthetic data generation process described above, the generalization error of the model    \\pi italic_  post-trained on the synthetic data can be bounded as:",
            "(Anchor data post-training upper bound.) For the same condition as lemma  4.6 , the generalization error of the model    \\pi italic_  post-trained on the anchor data can be bounded as:",
            "Building upon equation ( 16 ), we can derive the following equations:",
            "Together with equations ( 17 ) and ( 26 ), we have:"
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "A5.E17",
        "footnotes": [],
        "references": [
            "These factors collectively influence the generalization performance, indicating that a better generalization ability can be achieved by enhancing the information gain, reducing the compression bottleneck, minimizing the entropy, and balancing the efficiency. Finally, by integrating the insights from Lemma  3.1 , the overall upper bound of the expected generalization error in the LLM post-training with synthetic data can be derived as a comprehensive boundary in Theorem  4.7 .",
            "Theorem  4.7  establishes a general upper bound for the generalization error of LLMs post-trained with synthetic data. In this section, our objective is to analyze the generalization gains achieved by using synthetic data compared to scenarios devoid of synthetic data.",
            "Together with equations ( 17 ) and ( 26 ), we have:"
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "A5.E19",
        "footnotes": [],
        "references": [
            "We commence our analysis with the anchor data  S anchor subscript S anchor S_{\\text{anchor}} italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT . Analogous to the definition of  Err  (  S gen ) Err superscript  subscript S gen \\operatorname{Err}(\\pi^{S_{\\text{gen}}}) roman_Err ( italic_ start_POSTSUPERSCRIPT italic_S start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) , the generalization error of an LLM that has been post-trained on  S anchor subscript S anchor S_{\\text{anchor}} italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT  is defined as  Err  (  S anchor ) = | R D  (  S anchor )  R ^ S anchor  (  S anchor ) | Err superscript  subscript S anchor subscript R D superscript  subscript S anchor subscript ^ R subscript S anchor superscript  subscript S anchor \\operatorname{Err}(\\pi^{S_{\\text{anchor}}})=\\left|R_{\\mathcal{D}}(\\pi^{S_{% \\text{anchor}}})-\\widehat{R}_{S_{\\text{anchor}}}(\\pi^{S_{\\text{anchor}}})\\right| roman_Err ( italic_ start_POSTSUPERSCRIPT italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) = | italic_R start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT ( italic_ start_POSTSUPERSCRIPT italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) - over^ start_ARG italic_R end_ARG start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_ start_POSTSUPERSCRIPT italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) | . It is logically sound to assume that  S anchor subscript S anchor S_{\\text{anchor}} italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT  is sampled from the distribution  D D D italic_D . Building upon Lemma  4.4  and assume that  S anchor subscript S anchor S_{\\text{anchor}} italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT  comprises  m m m italic_m  instances, we can derive the subsequent result in Lemma  4.8 .",
            "Given that  m << n much-less-than m n m<<n italic_m < < italic_n  typically applies in real-world scenarios, Lemma  4.8  often represents a less stringent upper bound compared to Lemma  4.4 , this results in potentially poorer generalization when relying solely on  S anchor subscript S anchor S_{\\text{anchor}} italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT  rather than utilizing synthetic data."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "A5.E20",
        "footnotes": [],
        "references": [
            "But a pertinent question arises:  do other aspects of synthetic data generation, beyond the influence of data size, also contribute to improvements in generalization performance?  Our focus is on examining how various elements within the synthetic data process impact generalization during post-training. It is inappropriate, however, to directly compare other components across these two bounds due to variations in loss and training data specifics, which affect the parameters    \\eta italic_  and  W W W italic_W  differently, where    \\eta italic_  represents a measure of information compression and is challenging to quantify accurately  (Zhang et al.,  2018 ) . Thus, our analysis primarily centers on the mutual information terms  I  ( S anchor , W  ) I subscript S anchor superscript W  I(S_{\\text{anchor}},W^{{}^{\\prime}}) italic_I ( italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT , italic_W start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT  end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT )  and  I  ( S gen , W ) I subscript S gen W I(S_{\\text{gen}},W) italic_I ( italic_S start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT , italic_W ) . To systematically evaluate the generalization capabilities conferred by synthetic data in relation to these mutual information metrics, we introduce a definition for generalization gain measurement as Definition  4.9 ."
        ]
    },
    "id_table_10": {
        "caption": "",
        "table": "A5.E21",
        "footnotes": [],
        "references": [
            "A larger upper bound for the GGMI signifies greater potential generalization benefits when utilizing synthetic data. To elucidate the impact of synthetic data on model generalization, we isolate the influence of  W  superscript W  W^{{}^{\\prime}} italic_W start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT  end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT  and establish that the GGMI can be effectively bounded. This relationship is formalized in Theorem  4.10 , which demonstrates that GGMI is constrained by    I  I \\Delta I roman_ italic_I the measure of information gainand various entropy components solely associated with  W W W italic_W .",
            "The proof is referred to Appendix  F . Consequently, we proceed to conduct a thorough analysis of each component specified in Theorem  4.10 .",
            "As emphasized in  (Long et al.,  2024 ) , the generation of synthetic data typically focuses on two primary objectives: faithfulness and diversity. These objectives are associated with  H  ( S anchor | W ) H conditional subscript S anchor W H(S_{\\text{anchor}}|W) italic_H ( italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT | italic_W )  and    I  I \\Delta I roman_ italic_I , respectively. Specifically,  H  ( S anchor | W ) H conditional subscript S anchor W H(S_{\\text{anchor}}|W) italic_H ( italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT | italic_W ) , which quantifies the relevance between  W W W italic_W  and  S anchor subscript S anchor S_{\\text{anchor}} italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT , as presented in Theorem  4.10 , encourages the model to align with the information derived from the original real data. This alignment is crucial for achieving the objective of faithfulness. In addition,    I  I \\Delta I roman_ italic_I  serves as a measurement of the additional information introduced by the generative model  M M M italic_M . Given that  M M M italic_M  is typically pre-trained on a more extensive dataset,    I  I \\Delta I roman_ italic_I  in Theorem  4.10  promotes the objective of diversity by facilitating greater information gain from  M M M italic_M .",
            "To control the variables outlined in Theorem  4.10 , we adjust the number of components in the GMM  M M M italic_M  and the ground-truth GMM  G G G italic_G . The result is illustrated in Figure  4 . Generally, increasing  J J J italic_J  facilitates the scaling of    I  I \\Delta I roman_ italic_I , resulting in a larger upper bound for GGMI. In contrast, larger  K K K italic_K  amplifies the influence of anchor data within the post-training target distribution, thereby increasing the  H  ( S anchor | W ) H conditional subscript S anchor W H(S_{\\text{anchor}}|W) italic_H ( italic_S start_POSTSUBSCRIPT anchor end_POSTSUBSCRIPT | italic_W )  term and tightening the upper bound of GGMI. Additionally, while an increase in  L L L italic_L  enhances  H  ( S gen | W ) H conditional subscript S gen W H(S_{\\text{gen}}|W) italic_H ( italic_S start_POSTSUBSCRIPT gen end_POSTSUBSCRIPT | italic_W )  due to the introduction of greater diversity, it concurrently leads to a reduction in    H  H \\Delta H roman_ italic_H . As a result, we observe a trade-off manifested as a decrease in the KL Gap in our simulation outcomes."
        ]
    },
    "id_table_11": {
        "caption": "",
        "table": "A5.E23",
        "footnotes": [],
        "references": [
            "Together with Eq. ( 11 ), Eq. ( 12 ), and Eq. ( 13 ), we have:"
        ]
    },
    "id_table_12": {
        "caption": "",
        "table": "A5.E25",
        "footnotes": [],
        "references": [
            "Together with Eq. ( 11 ), Eq. ( 12 ), and Eq. ( 13 ), we have:"
        ]
    },
    "id_table_13": {
        "caption": "",
        "table": "A5.E26",
        "footnotes": [],
        "references": [
            "Together with Eq. ( 11 ), Eq. ( 12 ), and Eq. ( 13 ), we have:"
        ]
    },
    "id_table_14": {
        "caption": "",
        "table": "A5.E27",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "",
        "table": "A6.E30",
        "footnotes": [],
        "references": []
    },
    "id_table_16": {
        "caption": "",
        "table": "A6.E31",
        "footnotes": [],
        "references": [
            "Building upon equation ( 16 ), we can derive the following equations:"
        ]
    },
    "id_table_17": {
        "caption": "",
        "table": "A6.E32",
        "footnotes": [],
        "references": [
            "Together with equations ( 17 ) and ( 26 ), we have:"
        ]
    },
    "id_table_18": {
        "caption": "",
        "table": "A6.E33",
        "footnotes": [],
        "references": []
    },
    "id_table_19": {
        "caption": "",
        "table": "A6.E34",
        "footnotes": [],
        "references": []
    }
}