{
    "A1.T4": {
        "caption": "Table 4: Additional results on standard benchmarks.",
        "table": "<table id=\"A1.T4.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A1.T4.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"A1.T4.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_rr ltx_border_tt\"><span id=\"A1.T4.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Methods</span></th>\n<th id=\"A1.T4.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A1.T4.1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">BBH</span></th>\n<th id=\"A1.T4.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A1.T4.1.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">MMLU</span></th>\n<th id=\"A1.T4.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"A1.T4.1.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">Average</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T4.1.1.2.1\" class=\"ltx_tr\">\n<th id=\"A1.T4.1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_t\">LLaMA-7B</th>\n<td id=\"A1.T4.1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">30.93</td>\n<td id=\"A1.T4.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">35.17</td>\n<td id=\"A1.T4.1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">33.05</td>\n</tr>\n<tr id=\"A1.T4.1.1.3.2\" class=\"ltx_tr\">\n<th id=\"A1.T4.1.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr\">Alpagasus</th>\n<td id=\"A1.T4.1.1.3.2.2\" class=\"ltx_td ltx_align_center\">31.55</td>\n<td id=\"A1.T4.1.1.3.2.3\" class=\"ltx_td ltx_align_center\">36.46</td>\n<td id=\"A1.T4.1.1.3.2.4\" class=\"ltx_td ltx_align_center\">34.01</td>\n</tr>\n<tr id=\"A1.T4.1.1.4.3\" class=\"ltx_tr\">\n<th id=\"A1.T4.1.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr\">WizardLM+</th>\n<td id=\"A1.T4.1.1.4.3.2\" class=\"ltx_td ltx_align_center\">31.72</td>\n<td id=\"A1.T4.1.1.4.3.3\" class=\"ltx_td ltx_align_center\">37.89</td>\n<td id=\"A1.T4.1.1.4.3.4\" class=\"ltx_td ltx_align_center\">34.81</td>\n</tr>\n<tr id=\"A1.T4.1.1.5.4\" class=\"ltx_tr\">\n<th id=\"A1.T4.1.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_rr\">CodecLM (ours)</th>\n<td id=\"A1.T4.1.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A1.T4.1.1.5.4.2.1\" class=\"ltx_text ltx_font_bold\">32.60</span></td>\n<td id=\"A1.T4.1.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A1.T4.1.1.5.4.3.1\" class=\"ltx_text ltx_font_bold\">42.67</span></td>\n<td id=\"A1.T4.1.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A1.T4.1.1.5.4.4.1\" class=\"ltx_text ltx_font_bold\">37.64</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "To complement the performance result using LLM-based automatic evaluator, we also evaluate LLMs tuned with the top methods presented in Section\u00a05.4 on standard NLP benchmarks, MMLU\u00a0(Hendrycks et\u00a0al., 2020) and BBH\u00a0(Suzgun et\u00a0al., 2022). We follow the same settings introduced in\u00a0(Wang et\u00a0al., 2023) without demonstrations or CoT\u00a0(Wei et\u00a0al., 2022) prompt for evaluating the target models based on LLaMA-7B. For our method, we follow the same setting as in Evol-Instruction benchmark evaluation. We present the evaluation results in Table\u00a04 and use the performance of vanilla LLaMA-7B as a reference. We observe the same performance ranking of all methods as that in Table\u00a01 where we use LLM-based automatic evaluator. The consistency between two different evaluation approaches indicates the reliability of LLM-based evaluator in terms of demonstrating relative performance of competing methods."
        ]
    },
    "A1.T5": {
        "caption": "Table 5: Detailed comparison results with LLaMA-based models on Evol-Instruct benchmark. Each method trains a target model based on LLaMA-7B or -13B, and compares against the strong model, Gemini-Pro. Capacity Recovery Ratio (%), CRR=wins+tiestotal comparisonsCRRwinstiestotal comparisons\\texttt{CRR}=\\frac{\\texttt{wins}+\\texttt{ties}}{\\texttt{total comparisons}}.\n",
        "table": "<table id=\"A1.T5.3.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T5.3.1.1.1\" class=\"ltx_tr\">\n<th id=\"A1.T5.3.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr ltx_border_tt\" rowspan=\"2\"><span id=\"A1.T5.3.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Methods</span></th>\n<td id=\"A1.T5.3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_tt\" colspan=\"4\"><span id=\"A1.T5.3.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">LLaMA-7B vs. Gemini-Pro</span></td>\n<td id=\"A1.T5.3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span id=\"A1.T5.3.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">LLaMA-13B vs. Gemini-Pro</span></td>\n</tr>\n<tr id=\"A1.T5.3.1.2.2\" class=\"ltx_tr\">\n<td id=\"A1.T5.3.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T5.3.1.2.2.1.1\" class=\"ltx_text ltx_font_bold\">Wins</span></td>\n<td id=\"A1.T5.3.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T5.3.1.2.2.2.1\" class=\"ltx_text ltx_font_bold\">Ties</span></td>\n<td id=\"A1.T5.3.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T5.3.1.2.2.3.1\" class=\"ltx_text ltx_font_bold\">Losses</span></td>\n<td id=\"A1.T5.3.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\"><span id=\"A1.T5.3.1.2.2.4.1\" class=\"ltx_text ltx_font_bold\">CRR</span></td>\n<td id=\"A1.T5.3.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T5.3.1.2.2.5.1\" class=\"ltx_text ltx_font_bold\">Wins</span></td>\n<td id=\"A1.T5.3.1.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T5.3.1.2.2.6.1\" class=\"ltx_text ltx_font_bold\">Ties</span></td>\n<td id=\"A1.T5.3.1.2.2.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T5.3.1.2.2.7.1\" class=\"ltx_text ltx_font_bold\">Losses</span></td>\n<td id=\"A1.T5.3.1.2.2.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A1.T5.3.1.2.2.8.1\" class=\"ltx_text ltx_font_bold\">CRR</span></td>\n</tr>\n<tr id=\"A1.T5.3.1.3.3\" class=\"ltx_tr\">\n<th id=\"A1.T5.3.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr ltx_border_t\">Self-Instruct</th>\n<td id=\"A1.T5.3.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">17</td>\n<td id=\"A1.T5.3.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">140</td>\n<td id=\"A1.T5.3.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">61</td>\n<td id=\"A1.T5.3.1.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">72.02</td>\n<td id=\"A1.T5.3.1.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\">29</td>\n<td id=\"A1.T5.3.1.3.3.7\" class=\"ltx_td ltx_align_center ltx_border_t\">136</td>\n<td id=\"A1.T5.3.1.3.3.8\" class=\"ltx_td ltx_align_center ltx_border_t\">53</td>\n<td id=\"A1.T5.3.1.3.3.9\" class=\"ltx_td ltx_align_center ltx_border_t\">75.69</td>\n</tr>\n<tr id=\"A1.T5.3.1.4.4\" class=\"ltx_tr\">\n<th id=\"A1.T5.3.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">Alpagasus</th>\n<td id=\"A1.T5.3.1.4.4.2\" class=\"ltx_td ltx_align_center\">17</td>\n<td id=\"A1.T5.3.1.4.4.3\" class=\"ltx_td ltx_align_center\">147</td>\n<td id=\"A1.T5.3.1.4.4.4\" class=\"ltx_td ltx_align_center\">54</td>\n<td id=\"A1.T5.3.1.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_rr\">75.23</td>\n<td id=\"A1.T5.3.1.4.4.6\" class=\"ltx_td ltx_align_center\">26</td>\n<td id=\"A1.T5.3.1.4.4.7\" class=\"ltx_td ltx_align_center\">148</td>\n<td id=\"A1.T5.3.1.4.4.8\" class=\"ltx_td ltx_align_center\">44</td>\n<td id=\"A1.T5.3.1.4.4.9\" class=\"ltx_td ltx_align_center\">79.82</td>\n</tr>\n<tr id=\"A1.T5.3.1.5.5\" class=\"ltx_tr\">\n<th id=\"A1.T5.3.1.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">Tree-Instruct</th>\n<td id=\"A1.T5.3.1.5.5.2\" class=\"ltx_td ltx_align_center\">23</td>\n<td id=\"A1.T5.3.1.5.5.3\" class=\"ltx_td ltx_align_center\">141</td>\n<td id=\"A1.T5.3.1.5.5.4\" class=\"ltx_td ltx_align_center\">54</td>\n<td id=\"A1.T5.3.1.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_rr\">75.23</td>\n<td id=\"A1.T5.3.1.5.5.6\" class=\"ltx_td ltx_align_center\">26</td>\n<td id=\"A1.T5.3.1.5.5.7\" class=\"ltx_td ltx_align_center\">154</td>\n<td id=\"A1.T5.3.1.5.5.8\" class=\"ltx_td ltx_align_center\">38</td>\n<td id=\"A1.T5.3.1.5.5.9\" class=\"ltx_td ltx_align_center\">82.57</td>\n</tr>\n<tr id=\"A1.T5.3.1.6.6\" class=\"ltx_tr\">\n<th id=\"A1.T5.3.1.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">WizardLM</th>\n<td id=\"A1.T5.3.1.6.6.2\" class=\"ltx_td ltx_align_center\">19</td>\n<td id=\"A1.T5.3.1.6.6.3\" class=\"ltx_td ltx_align_center\">143</td>\n<td id=\"A1.T5.3.1.6.6.4\" class=\"ltx_td ltx_align_center\">56</td>\n<td id=\"A1.T5.3.1.6.6.5\" class=\"ltx_td ltx_align_center ltx_border_rr\">74.31</td>\n<td id=\"A1.T5.3.1.6.6.6\" class=\"ltx_td ltx_align_center\">30</td>\n<td id=\"A1.T5.3.1.6.6.7\" class=\"ltx_td ltx_align_center\">149</td>\n<td id=\"A1.T5.3.1.6.6.8\" class=\"ltx_td ltx_align_center\">39</td>\n<td id=\"A1.T5.3.1.6.6.9\" class=\"ltx_td ltx_align_center\">82.11</td>\n</tr>\n<tr id=\"A1.T5.3.1.7.7\" class=\"ltx_tr\">\n<th id=\"A1.T5.3.1.7.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">WizardLM+</th>\n<td id=\"A1.T5.3.1.7.7.2\" class=\"ltx_td ltx_align_center\">19</td>\n<td id=\"A1.T5.3.1.7.7.3\" class=\"ltx_td ltx_align_center\">146</td>\n<td id=\"A1.T5.3.1.7.7.4\" class=\"ltx_td ltx_align_center\">53</td>\n<td id=\"A1.T5.3.1.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_rr\">75.69</td>\n<td id=\"A1.T5.3.1.7.7.6\" class=\"ltx_td ltx_align_center\">31</td>\n<td id=\"A1.T5.3.1.7.7.7\" class=\"ltx_td ltx_align_center\">153</td>\n<td id=\"A1.T5.3.1.7.7.8\" class=\"ltx_td ltx_align_center\">34</td>\n<td id=\"A1.T5.3.1.7.7.9\" class=\"ltx_td ltx_align_center\">84.40</td>\n</tr>\n<tr id=\"A1.T5.3.1.8.8\" class=\"ltx_tr\">\n<th id=\"A1.T5.3.1.8.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_rr\">CodecLM (ours)</th>\n<td id=\"A1.T5.3.1.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A1.T5.3.1.8.8.2.1\" class=\"ltx_text ltx_font_bold\">29</span></td>\n<td id=\"A1.T5.3.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A1.T5.3.1.8.8.3.1\" class=\"ltx_text ltx_font_bold\">145</span></td>\n<td id=\"A1.T5.3.1.8.8.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A1.T5.3.1.8.8.4.1\" class=\"ltx_text ltx_font_bold\">44</span></td>\n<td id=\"A1.T5.3.1.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_rr\"><span id=\"A1.T5.3.1.8.8.5.1\" class=\"ltx_text ltx_font_bold\">79.82</span></td>\n<td id=\"A1.T5.3.1.8.8.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A1.T5.3.1.8.8.6.1\" class=\"ltx_text ltx_font_bold\">35</span></td>\n<td id=\"A1.T5.3.1.8.8.7\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A1.T5.3.1.8.8.7.1\" class=\"ltx_text ltx_font_bold\">154</span></td>\n<td id=\"A1.T5.3.1.8.8.8\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A1.T5.3.1.8.8.8.1\" class=\"ltx_text ltx_font_bold\">29</span></td>\n<td id=\"A1.T5.3.1.8.8.9\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A1.T5.3.1.8.8.9.1\" class=\"ltx_text ltx_font_bold\">86.70</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "We show the details of pairwise comparison on Evol-Instruct benchmark with LLaMA-based models, as a demonstration of how CRR faithfully reflects the capability of the target LLMs trained by different methods. In Table\u00a05, we observe that number of ties dominates the results and the number of wins are scarce. We attribute it to the fact that the target model is essentially distilling knowledge from the strong model. As a result, most of the time, the instruction-tuned target model is only able to respond as good as the strong model, through the lens of the LLM-based evaluator."
        ]
    },
    "A1.T6": {
        "caption": "Table 6: Performance gap to Self-Instruct in terms of CRR on Evol-Instruct, evaluated by ChatGPT and GPT4, respectively. Each method trains a target model based on LLaMA-7B or -13B, and compares against the strong model, Gemini-Pro. We observe two LLM-based automatic evaluators yields consistent results.\n",
        "table": "<table id=\"A1.T6.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A1.T6.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"A1.T6.1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_rr ltx_border_tt\" rowspan=\"2\"><span id=\"A1.T6.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Methods</span></th>\n<th id=\"A1.T6.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_tt\" colspan=\"2\"><span id=\"A1.T6.1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">LLaMA-7B vs. Gemini-Pro</span></th>\n<th id=\"A1.T6.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span id=\"A1.T6.1.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">LLaMA-13B vs. Gemini-Pro</span></th>\n</tr>\n<tr id=\"A1.T6.1.1.2.2\" class=\"ltx_tr\">\n<th id=\"A1.T6.1.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"A1.T6.1.1.2.2.1.1\" class=\"ltx_text ltx_font_bold\">ChatGPT</span></th>\n<th id=\"A1.T6.1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t\"><span id=\"A1.T6.1.1.2.2.2.1\" class=\"ltx_text ltx_font_bold\">GPT4</span></th>\n<th id=\"A1.T6.1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"A1.T6.1.1.2.2.3.1\" class=\"ltx_text ltx_font_bold\">ChatGPT</span></th>\n<th id=\"A1.T6.1.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"A1.T6.1.1.2.2.4.1\" class=\"ltx_text ltx_font_bold\">GPT4</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T6.1.1.3.1\" class=\"ltx_tr\">\n<th id=\"A1.T6.1.1.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr ltx_border_t\">Self-Instruct</th>\n<td id=\"A1.T6.1.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.00</td>\n<td id=\"A1.T6.1.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">0.00</td>\n<td id=\"A1.T6.1.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.00</td>\n<td id=\"A1.T6.1.1.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.00</td>\n</tr>\n<tr id=\"A1.T6.1.1.4.2\" class=\"ltx_tr\">\n<th id=\"A1.T6.1.1.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">Alpagasus</th>\n<td id=\"A1.T6.1.1.4.2.2\" class=\"ltx_td ltx_align_center\">+3.21</td>\n<td id=\"A1.T6.1.1.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_rr\">+1.38</td>\n<td id=\"A1.T6.1.1.4.2.4\" class=\"ltx_td ltx_align_center\">+4.13</td>\n<td id=\"A1.T6.1.1.4.2.5\" class=\"ltx_td ltx_align_center\">+1.83</td>\n</tr>\n<tr id=\"A1.T6.1.1.5.3\" class=\"ltx_tr\">\n<th id=\"A1.T6.1.1.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">Tree-Instruct</th>\n<td id=\"A1.T6.1.1.5.3.2\" class=\"ltx_td ltx_align_center\">+3.21</td>\n<td id=\"A1.T6.1.1.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_rr\">+2.29</td>\n<td id=\"A1.T6.1.1.5.3.4\" class=\"ltx_td ltx_align_center\">+6.88</td>\n<td id=\"A1.T6.1.1.5.3.5\" class=\"ltx_td ltx_align_center\">+4.59</td>\n</tr>\n<tr id=\"A1.T6.1.1.6.4\" class=\"ltx_tr\">\n<th id=\"A1.T6.1.1.6.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">WizardLM</th>\n<td id=\"A1.T6.1.1.6.4.2\" class=\"ltx_td ltx_align_center\">+2.29</td>\n<td id=\"A1.T6.1.1.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_rr\">+0.46</td>\n<td id=\"A1.T6.1.1.6.4.4\" class=\"ltx_td ltx_align_center\">+6.42</td>\n<td id=\"A1.T6.1.1.6.4.5\" class=\"ltx_td ltx_align_center\">+3.21</td>\n</tr>\n<tr id=\"A1.T6.1.1.7.5\" class=\"ltx_tr\">\n<th id=\"A1.T6.1.1.7.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">WizardLM+</th>\n<td id=\"A1.T6.1.1.7.5.2\" class=\"ltx_td ltx_align_center\">+3.67</td>\n<td id=\"A1.T6.1.1.7.5.3\" class=\"ltx_td ltx_align_center ltx_border_rr\">+2.29</td>\n<td id=\"A1.T6.1.1.7.5.4\" class=\"ltx_td ltx_align_center\">+8.72</td>\n<td id=\"A1.T6.1.1.7.5.5\" class=\"ltx_td ltx_align_center\">+5.50</td>\n</tr>\n<tr id=\"A1.T6.1.1.8.6\" class=\"ltx_tr\">\n<th id=\"A1.T6.1.1.8.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_rr\">CodecLM (ours)</th>\n<td id=\"A1.T6.1.1.8.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A1.T6.1.1.8.6.2.1\" class=\"ltx_text ltx_font_bold\">+7.80</span></td>\n<td id=\"A1.T6.1.1.8.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_rr\"><span id=\"A1.T6.1.1.8.6.3.1\" class=\"ltx_text ltx_font_bold\">+8.26</span></td>\n<td id=\"A1.T6.1.1.8.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A1.T6.1.1.8.6.4.1\" class=\"ltx_text ltx_font_bold\">+11.01</span></td>\n<td id=\"A1.T6.1.1.8.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A1.T6.1.1.8.6.5.1\" class=\"ltx_text ltx_font_bold\">+8.72</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "In the main paper, we use ChatGPT as the LLM judge for final evaluation, for its efficiency, price and accessibility for the community to reproduce our results. As pointed out in\u00a0(Chiang et\u00a0al., 2023), LLMs evaluators, although largely consistent with human preferences, may have their own biases. Therefore, to make sure our experimental results are solid, we also use GPT-4 as the judge and compare against the performance gap in CRR between different baselines and the Self-Instruct method. The comparison results in Table\u00a06 demonstrates the agreement of two LLM-based judges and confirms the superior performance of CodecLM against comparing methods."
        ]
    },
    "S5.T1": {
        "caption": "Table 1: Results with LLaMA-based target models on four open-domain instruction following benchmarks. Each method trains a target model based on LLaMA-7B or -13B, and compares against the strong model, Gemini-Pro. The reported metric Capacity Recovery Ratio (%), CRR=wins+tiestotal comparisonsCRRwinstiestotal comparisons\\texttt{CRR}=\\frac{\\texttt{wins}+\\texttt{ties}}{\\texttt{total comparisons}}. Larger CRR means better performance.\n",
        "table": "<table id=\"S5.T1.3.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T1.3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.3.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_rr ltx_border_tt\" rowspan=\"2\"><span id=\"S5.T1.3.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Methods</span></th>\n<th id=\"S5.T1.3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_tt\" colspan=\"4\"><span id=\"S5.T1.3.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">LLaMA-7B vs. Gemini-Pro</span></th>\n<th id=\"S5.T1.3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\"><span id=\"S5.T1.3.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">LLaMA-13B vs. Gemini-Pro</span></th>\n</tr>\n<tr id=\"S5.T1.3.1.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T1.3.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T1.3.1.2.2.1.1\" class=\"ltx_text ltx_font_bold\">Evol-Ins.</span></th>\n<th id=\"S5.T1.3.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T1.3.1.2.2.2.1\" class=\"ltx_text ltx_font_bold\">Vicuna</span></th>\n<th id=\"S5.T1.3.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T1.3.1.2.2.3.1\" class=\"ltx_text ltx_font_bold\">Koala</span></th>\n<th id=\"S5.T1.3.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t\"><span id=\"S5.T1.3.1.2.2.4.1\" class=\"ltx_text ltx_font_bold\">Self-Ins.</span></th>\n<th id=\"S5.T1.3.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T1.3.1.2.2.5.1\" class=\"ltx_text ltx_font_bold\">Evol-Ins.</span></th>\n<th id=\"S5.T1.3.1.2.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T1.3.1.2.2.6.1\" class=\"ltx_text ltx_font_bold\">Vicuna</span></th>\n<th id=\"S5.T1.3.1.2.2.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T1.3.1.2.2.7.1\" class=\"ltx_text ltx_font_bold\">Koala</span></th>\n<th id=\"S5.T1.3.1.2.2.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T1.3.1.2.2.8.1\" class=\"ltx_text ltx_font_bold\">Self-Ins.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.3.1.3.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.3.1.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr ltx_border_t\">Self-Instruct</th>\n<td id=\"S5.T1.3.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">72.02</td>\n<td id=\"S5.T1.3.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">81.25</td>\n<td id=\"S5.T1.3.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">67.78</td>\n<td id=\"S5.T1.3.1.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">65.87</td>\n<td id=\"S5.T1.3.1.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">75.69</td>\n<td id=\"S5.T1.3.1.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">86.25</td>\n<td id=\"S5.T1.3.1.3.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\">77.22</td>\n<td id=\"S5.T1.3.1.3.1.9\" class=\"ltx_td ltx_align_center ltx_border_t\">69.05</td>\n</tr>\n<tr id=\"S5.T1.3.1.4.2\" class=\"ltx_tr\">\n<th id=\"S5.T1.3.1.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">Alpagasus</th>\n<td id=\"S5.T1.3.1.4.2.2\" class=\"ltx_td ltx_align_center\">75.23 <span id=\"S5.T1.3.1.4.2.2.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+3.2)</span>\n</td>\n<td id=\"S5.T1.3.1.4.2.3\" class=\"ltx_td ltx_align_center\">81.25 <span id=\"S5.T1.3.1.4.2.3.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+0.0)</span>\n</td>\n<td id=\"S5.T1.3.1.4.2.4\" class=\"ltx_td ltx_align_center\">71.11 <span id=\"S5.T1.3.1.4.2.4.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+3.3)</span>\n</td>\n<td id=\"S5.T1.3.1.4.2.5\" class=\"ltx_td ltx_align_center ltx_border_rr\">70.24 <span id=\"S5.T1.3.1.4.2.5.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+4.4)</span>\n</td>\n<td id=\"S5.T1.3.1.4.2.6\" class=\"ltx_td ltx_align_center\">79.82 <span id=\"S5.T1.3.1.4.2.6.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+4.1)</span>\n</td>\n<td id=\"S5.T1.3.1.4.2.7\" class=\"ltx_td ltx_align_center\">87.50 <span id=\"S5.T1.3.1.4.2.7.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+1.3)</span>\n</td>\n<td id=\"S5.T1.3.1.4.2.8\" class=\"ltx_td ltx_align_center\">77.78 <span id=\"S5.T1.3.1.4.2.8.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+0.6)</span>\n</td>\n<td id=\"S5.T1.3.1.4.2.9\" class=\"ltx_td ltx_align_center\">71.03 <span id=\"S5.T1.3.1.4.2.9.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+2.0)</span>\n</td>\n</tr>\n<tr id=\"S5.T1.3.1.5.3\" class=\"ltx_tr\">\n<th id=\"S5.T1.3.1.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">Tree-Instruct</th>\n<td id=\"S5.T1.3.1.5.3.2\" class=\"ltx_td ltx_align_center\">75.23 <span id=\"S5.T1.3.1.5.3.2.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+3.2)</span>\n</td>\n<td id=\"S5.T1.3.1.5.3.3\" class=\"ltx_td ltx_align_center\">81.25 <span id=\"S5.T1.3.1.5.3.3.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+0.0)</span>\n</td>\n<td id=\"S5.T1.3.1.5.3.4\" class=\"ltx_td ltx_align_center\">72.78 <span id=\"S5.T1.3.1.5.3.4.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+5.0)</span>\n</td>\n<td id=\"S5.T1.3.1.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_rr\">68.65 <span id=\"S5.T1.3.1.5.3.5.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+2.8)</span>\n</td>\n<td id=\"S5.T1.3.1.5.3.6\" class=\"ltx_td ltx_align_center\">82.57 <span id=\"S5.T1.3.1.5.3.6.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+6.9)</span>\n</td>\n<td id=\"S5.T1.3.1.5.3.7\" class=\"ltx_td ltx_align_center\">87.50 <span id=\"S5.T1.3.1.5.3.7.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+1.3)</span>\n</td>\n<td id=\"S5.T1.3.1.5.3.8\" class=\"ltx_td ltx_align_center\">80.56 <span id=\"S5.T1.3.1.5.3.8.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+3.3)</span>\n</td>\n<td id=\"S5.T1.3.1.5.3.9\" class=\"ltx_td ltx_align_center\">79.37 <span id=\"S5.T1.3.1.5.3.9.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+10.3)</span>\n</td>\n</tr>\n<tr id=\"S5.T1.3.1.6.4\" class=\"ltx_tr\">\n<th id=\"S5.T1.3.1.6.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">WizardLM</th>\n<td id=\"S5.T1.3.1.6.4.2\" class=\"ltx_td ltx_align_center\">74.31 <span id=\"S5.T1.3.1.6.4.2.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+2.3)</span>\n</td>\n<td id=\"S5.T1.3.1.6.4.3\" class=\"ltx_td ltx_align_center\">76.25 <span id=\"S5.T1.3.1.6.4.3.1\" class=\"ltx_text\" style=\"color:#FF0000;\">(-5.0)</span>\n</td>\n<td id=\"S5.T1.3.1.6.4.4\" class=\"ltx_td ltx_align_center\">65.56 <span id=\"S5.T1.3.1.6.4.4.1\" class=\"ltx_text\" style=\"color:#FF0000;\">(-2.2)</span>\n</td>\n<td id=\"S5.T1.3.1.6.4.5\" class=\"ltx_td ltx_align_center ltx_border_rr\">71.43 <span id=\"S5.T1.3.1.6.4.5.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+5.6)</span>\n</td>\n<td id=\"S5.T1.3.1.6.4.6\" class=\"ltx_td ltx_align_center\">82.11 <span id=\"S5.T1.3.1.6.4.6.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+6.4)</span>\n</td>\n<td id=\"S5.T1.3.1.6.4.7\" class=\"ltx_td ltx_align_center\">86.25 <span id=\"S5.T1.3.1.6.4.7.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+0.0)</span>\n</td>\n<td id=\"S5.T1.3.1.6.4.8\" class=\"ltx_td ltx_align_center\">78.89 <span id=\"S5.T1.3.1.6.4.8.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+1.7)</span>\n</td>\n<td id=\"S5.T1.3.1.6.4.9\" class=\"ltx_td ltx_align_center\">76.19 <span id=\"S5.T1.3.1.6.4.9.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+7.1)</span>\n</td>\n</tr>\n<tr id=\"S5.T1.3.1.7.5\" class=\"ltx_tr\">\n<th id=\"S5.T1.3.1.7.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\">WizardLM+</th>\n<td id=\"S5.T1.3.1.7.5.2\" class=\"ltx_td ltx_align_center\">75.69 <span id=\"S5.T1.3.1.7.5.2.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+3.7)</span>\n</td>\n<td id=\"S5.T1.3.1.7.5.3\" class=\"ltx_td ltx_align_center\">83.75 <span id=\"S5.T1.3.1.7.5.3.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+2.5)</span>\n</td>\n<td id=\"S5.T1.3.1.7.5.4\" class=\"ltx_td ltx_align_center\">68.33 <span id=\"S5.T1.3.1.7.5.4.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+0.6)</span>\n</td>\n<td id=\"S5.T1.3.1.7.5.5\" class=\"ltx_td ltx_align_center ltx_border_rr\">72.22 <span id=\"S5.T1.3.1.7.5.5.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+6.4)</span>\n</td>\n<td id=\"S5.T1.3.1.7.5.6\" class=\"ltx_td ltx_align_center\">84.40 <span id=\"S5.T1.3.1.7.5.6.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+8.7)</span>\n</td>\n<td id=\"S5.T1.3.1.7.5.7\" class=\"ltx_td ltx_align_center\">88.75 <span id=\"S5.T1.3.1.7.5.7.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+2.5)</span>\n</td>\n<td id=\"S5.T1.3.1.7.5.8\" class=\"ltx_td ltx_align_center\">81.11 <span id=\"S5.T1.3.1.7.5.8.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+3.9)</span>\n</td>\n<td id=\"S5.T1.3.1.7.5.9\" class=\"ltx_td ltx_align_center\">79.76 <span id=\"S5.T1.3.1.7.5.9.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+10.7)</span>\n</td>\n</tr>\n<tr id=\"S5.T1.3.1.8.6\" class=\"ltx_tr\">\n<th id=\"S5.T1.3.1.8.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_rr\">CodecLM (ours)</th>\n<td id=\"S5.T1.3.1.8.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T1.3.1.8.6.2.1\" class=\"ltx_text ltx_font_bold\">79.82 <span id=\"S5.T1.3.1.8.6.2.1.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+7.8)</span></span></td>\n<td id=\"S5.T1.3.1.8.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T1.3.1.8.6.3.1\" class=\"ltx_text ltx_font_bold\">88.75 <span id=\"S5.T1.3.1.8.6.3.1.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+7.5)</span></span></td>\n<td id=\"S5.T1.3.1.8.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T1.3.1.8.6.4.1\" class=\"ltx_text ltx_font_bold\">74.44 <span id=\"S5.T1.3.1.8.6.4.1.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+6.7)</span></span></td>\n<td id=\"S5.T1.3.1.8.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_rr\"><span id=\"S5.T1.3.1.8.6.5.1\" class=\"ltx_text ltx_font_bold\">78.17 <span id=\"S5.T1.3.1.8.6.5.1.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+12.3)</span></span></td>\n<td id=\"S5.T1.3.1.8.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T1.3.1.8.6.6.1\" class=\"ltx_text ltx_font_bold\">86.70 <span id=\"S5.T1.3.1.8.6.6.1.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+11.0)</span></span></td>\n<td id=\"S5.T1.3.1.8.6.7\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T1.3.1.8.6.7.1\" class=\"ltx_text ltx_font_bold\">90.00 <span id=\"S5.T1.3.1.8.6.7.1.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+3.8)</span></span></td>\n<td id=\"S5.T1.3.1.8.6.8\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T1.3.1.8.6.8.1\" class=\"ltx_text ltx_font_bold\">82.22 <span id=\"S5.T1.3.1.8.6.8.1.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+5.0)</span></span></td>\n<td id=\"S5.T1.3.1.8.6.9\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T1.3.1.8.6.9.1\" class=\"ltx_text ltx_font_bold\">83.33 <span id=\"S5.T1.3.1.8.6.9.1.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+14.3)</span></span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Results with LLaMA-based Target LLMs. Table\u00a01 summarizes the performance of CodecLM and the comparing baselines with 2000 synthetic data for instruction tuning. All methods are trained on LLaMA-7B or -13B as the target LLM and compared against Gemini-Pro, the strong LLM that generates the data. CodecLM outperforms comparing methods consistently on all benchmarks, with two target LLMs of different sizes. The consistently superior performance of CodecLM highlights its generalizability to different downstream instruction distributions and target LLMs. Both Tree-Instruct and variants of WizardLM focus on the importance of instruction complexity, however, their performances are not always better than Alpagasus with simple instructions, especially with larger target LLM. This observation indicates that the effectiveness of data cannot be solely determined by instruction complexity, and validates the motivation of our design of Self-Rubrics and Contrastive Filtering. Moreover, the win of WizardLM+ over WizardLM confirms the efficacy of instruction distribution matching via instruction metadata. When shifting the target LLM from LLaMA-7B to -13B, all methods get a significant performance boost, which accords with prior discoveries on scaling model size\u00a0(Wei et\u00a0al., 2021).",
            "Effectiveness of Core Designs.\nWe show component-wise contributions in our framework in Table\u00a03. The 1st row has the result from Self-Instruct as a baseline; In the 2nd row, we only align the LLM with basic instructions from instruction metadata; We gradually add Self-Rubrics and Contrastive Filtering in the 3rd and 4th rows, respectively. We clearly observe that every component contributes to the final performance. Interesting, the performance of using basic instructions from metadata is even on par with that of WizardLM+ in Table\u00a01. This observation indicates that human-crafted strategies for complicating instructions may not fit different types of instructions. On the contrary, Self-Rubrics adaptively generates instruction improving actions based on different metadata, resulting in better tailored instructions for the target LLM. Further improvements from Contrastive Filtering demonstrate that selected data are indeed more effective for alignment.",
            "To complement the performance result using LLM-based automatic evaluator, we also evaluate LLMs tuned with the top methods presented in Section\u00a05.4 on standard NLP benchmarks, MMLU\u00a0(Hendrycks et\u00a0al., 2020) and BBH\u00a0(Suzgun et\u00a0al., 2022). We follow the same settings introduced in\u00a0(Wang et\u00a0al., 2023) without demonstrations or CoT\u00a0(Wei et\u00a0al., 2022) prompt for evaluating the target models based on LLaMA-7B. For our method, we follow the same setting as in Evol-Instruction benchmark evaluation. We present the evaluation results in Table\u00a04 and use the performance of vanilla LLaMA-7B as a reference. We observe the same performance ranking of all methods as that in Table\u00a01 where we use LLM-based automatic evaluator. The consistency between two different evaluation approaches indicates the reliability of LLM-based evaluator in terms of demonstrating relative performance of competing methods."
        ]
    },
    "S5.T2": {
        "caption": "Table 2: CRR Results on PaLM-based models. Each method trains a target model based on text-bison, and compares against the strong model, text-unicorn.\n",
        "table": "<table id=\"S5.T2.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T2.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_rr ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\" rowspan=\"2\"><span id=\"S5.T2.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Methods</span></th>\n<th id=\"S5.T2.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\" colspan=\"4\"><span id=\"S5.T2.1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">text-bison vs. text-unicorn</span></th>\n</tr>\n<tr id=\"S5.T2.1.1.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span id=\"S5.T2.1.1.2.2.1.1\" class=\"ltx_text ltx_font_bold\">Evol-Ins.</span></th>\n<th id=\"S5.T2.1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span id=\"S5.T2.1.1.2.2.2.1\" class=\"ltx_text ltx_font_bold\">Vicuna</span></th>\n<th id=\"S5.T2.1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span id=\"S5.T2.1.1.2.2.3.1\" class=\"ltx_text ltx_font_bold\">Self-Ins.</span></th>\n<th id=\"S5.T2.1.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span id=\"S5.T2.1.1.2.2.4.1\" class=\"ltx_text ltx_font_bold\">Koala</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.1.1.3.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.1.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">text-bison</th>\n<td id=\"S5.T2.1.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">87.16</td>\n<td id=\"S5.T2.1.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">81.25</td>\n<td id=\"S5.T2.1.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span id=\"S5.T2.1.1.3.1.4.1\" class=\"ltx_text ltx_font_bold\">74.21</span></td>\n<td id=\"S5.T2.1.1.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">77.47</td>\n</tr>\n<tr id=\"S5.T2.1.1.4.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.1.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">Alpagasus</th>\n<td id=\"S5.T2.1.1.4.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">82.11<span id=\"S5.T2.1.1.4.2.2.1\" class=\"ltx_text\" style=\"color:#FF0000;\">(-5.1)</span>\n</td>\n<td id=\"S5.T2.1.1.4.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">81.25 <span id=\"S5.T2.1.1.4.2.3.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+0.0)</span>\n</td>\n<td id=\"S5.T2.1.1.4.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">67.86 <span id=\"S5.T2.1.1.4.2.4.1\" class=\"ltx_text\" style=\"color:#FF0000;\">(-6.4)</span>\n</td>\n<td id=\"S5.T2.1.1.4.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">73.33 <span id=\"S5.T2.1.1.4.2.5.1\" class=\"ltx_text\" style=\"color:#FF0000;\">(-4.1)</span>\n</td>\n</tr>\n<tr id=\"S5.T2.1.1.5.3\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.1.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">WizardLM+</th>\n<td id=\"S5.T2.1.1.5.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">84.40 <span id=\"S5.T2.1.1.5.3.2.1\" class=\"ltx_text\" style=\"color:#FF0000;\">(-2.8)</span>\n</td>\n<td id=\"S5.T2.1.1.5.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">78.75 <span id=\"S5.T2.1.1.5.3.3.1\" class=\"ltx_text\" style=\"color:#FF0000;\">(-2.5)</span>\n</td>\n<td id=\"S5.T2.1.1.5.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">69.44 <span id=\"S5.T2.1.1.5.3.4.1\" class=\"ltx_text\" style=\"color:#FF0000;\">(-4.8)</span>\n</td>\n<td id=\"S5.T2.1.1.5.3.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">73.89 <span id=\"S5.T2.1.1.5.3.5.1\" class=\"ltx_text\" style=\"color:#FF0000;\">(-3.6)</span>\n</td>\n</tr>\n<tr id=\"S5.T2.1.1.6.4\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.1.6.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_rr\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">CodecLM (ours)</th>\n<td id=\"S5.T2.1.1.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span id=\"S5.T2.1.1.6.4.2.1\" class=\"ltx_text ltx_font_bold\">88.53 <span id=\"S5.T2.1.1.6.4.2.1.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+1.4)</span></span></td>\n<td id=\"S5.T2.1.1.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span id=\"S5.T2.1.1.6.4.3.1\" class=\"ltx_text ltx_font_bold\">86.25 <span id=\"S5.T2.1.1.6.4.3.1.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+5.0)</span></span></td>\n<td id=\"S5.T2.1.1.6.4.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\">72.22 <span id=\"S5.T2.1.1.6.4.4.1\" class=\"ltx_text\" style=\"color:#FF0000;\">(-2.0)</span>\n</td>\n<td id=\"S5.T2.1.1.6.4.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span id=\"S5.T2.1.1.6.4.5.1\" class=\"ltx_text ltx_font_bold\">80.56 <span id=\"S5.T2.1.1.6.4.5.1.1\" class=\"ltx_text\" style=\"color:#006B3D;\">(+3.1)</span></span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Results with PaLM-based Models. Table\u00a02 summarizes the results of CodecLM and the best performing baselines in LLaMA-based experiments. We generate 1000 synthetic data due to computation budget. Since text-bison is a proprietary model that has been aligned with various techniques including instruction tuning, we also include it as a baseline approach. Interestingly, text-bison obtains strong performance across different benchmarks. Both Alpagasus and WizardLM+ underperform text-bison, suggesting it is non-trivial to improve upon a well-tuned LLM continually. CodecLM, on the contrary, outperforms text-bison in most cases, thanks to our core designs that adaptively tailor high quality data pairs to improve the target LLM."
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Ablation study of CodecLM\u2019s core designs. All components contribute to the final performance.",
        "table": "<table id=\"S5.T3.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T3.1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T3.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Metadata</span></td>\n<td id=\"S5.T3.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T3.1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Self-Rubrics</span></td>\n<td id=\"S5.T3.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_tt\"><span id=\"S5.T3.1.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Contrastive Filtering</span></td>\n<td id=\"S5.T3.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T3.1.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">CRR</span></td>\n</tr>\n<tr id=\"S5.T3.1.1.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\">&#10007;</td>\n<td id=\"S5.T3.1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">&#10007;</td>\n<td id=\"S5.T3.1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">&#10007;</td>\n<td id=\"S5.T3.1.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">72.02</td>\n</tr>\n<tr id=\"S5.T3.1.1.3.3\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.3.3.1\" class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td id=\"S5.T3.1.1.3.3.2\" class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td id=\"S5.T3.1.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_rr\">&#10007;</td>\n<td id=\"S5.T3.1.1.3.3.4\" class=\"ltx_td ltx_align_center\">75.23</td>\n</tr>\n<tr id=\"S5.T3.1.1.4.4\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.4.4.1\" class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td id=\"S5.T3.1.1.4.4.2\" class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td id=\"S5.T3.1.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_rr\">&#10007;</td>\n<td id=\"S5.T3.1.1.4.4.4\" class=\"ltx_td ltx_align_center\">77.52</td>\n</tr>\n<tr id=\"S5.T3.1.1.5.5\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">&#10003;</td>\n<td id=\"S5.T3.1.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">&#10003;</td>\n<td id=\"S5.T3.1.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_rr\">&#10003;</td>\n<td id=\"S5.T3.1.1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">79.82</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Effectiveness of Core Designs.\nWe show component-wise contributions in our framework in Table\u00a03. The 1st row has the result from Self-Instruct as a baseline; In the 2nd row, we only align the LLM with basic instructions from instruction metadata; We gradually add Self-Rubrics and Contrastive Filtering in the 3rd and 4th rows, respectively. We clearly observe that every component contributes to the final performance. Interesting, the performance of using basic instructions from metadata is even on par with that of WizardLM+ in Table\u00a01. This observation indicates that human-crafted strategies for complicating instructions may not fit different types of instructions. On the contrary, Self-Rubrics adaptively generates instruction improving actions based on different metadata, resulting in better tailored instructions for the target LLM. Further improvements from Contrastive Filtering demonstrate that selected data are indeed more effective for alignment."
        ]
    }
}