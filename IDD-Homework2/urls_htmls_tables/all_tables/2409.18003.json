{
    "id_table_1": {
        "caption": "Table 1 :    Table comparing the performance of the  Llama-3.1-Instruct-8B  and  Mistral-Instruct-7B  models across 200 prompts, evaluating both baseline and SAR scenarios using different metrics.",
        "table": "S4.T1.8.8",
        "footnotes": [],
        "references": [
            "Typically, a naive RAG system comprises of three main phases  information Retrieval, prompt Augmentation, and Generation  [ 30 ] .  We modify the conventional naive RAG system by incorporating a sustainability metric (S-Fairness indicator) based on the popularity and the monthly seasonal demand of the city to re-rank the retrieved context during the prompt augmentation phase  [ 9 ] .  This enhancement, termed Sustainability Augmented Reranking (SAR), ensures that the generated results prioritize sustainability considerations.  While  Figure 1  visually represents our systems workflow, the subsequent sections offer a detailed explanation of this process.",
            "In this section, we evaluate the effectiveness of our proposed approach by comparing it against a baseline method, i.e., without the SAR enhancement.  We utilize two popular open-source instruction-tuned LLMs   Llama-3.1-Instruct-8B   [ 18 ]  and  Mistral-Instruct-7B   [ 28 ]  to conduct this comparison, aiming to assess the impact of incorporating SAR on the quality of the generated recommendations.    Section 4.1  describes our experimental setup, including the use of other LLMs to generate synthetic test cases for evaluation. In   Section 4.2 , we discuss the different metrics used for evaluation and present the corresponding results.",
            "Since RAG systems often consist of multiple independent components, evaluating the system can be challenging, as different phases need to be evaluated.  For the scope of this paper, we only consider the final LLM-generated response for both baseline and SAR for evaluation.  Our metrics are inspired by the RAGAS framework proposed by  Es et al., [ 19 ] .  The research questions we aim to address through our evaluation, along with their updated results after further optimization (as presented in  Table 1 ), are discussed in the following subsections.",
            "We employ two LLMs to judge Answer Relevance   GPT-4o-mini   [ 3 ]  and  Claude-3-5-sonnet   [ 5 ]  by instructing them to grade the answer on a scale of 0 - 10, where 0 means that the answer is completely irrelevant and 10 implies that the answer is relevant and completely answers the question 7 7 7 https://huggingface.co/learn/cookbook/en/llm_judge . We compute the mean and standard deviation (indicated by the   plus-or-minus \\pm  values ) for both  Llama-3.1-Instruct-8B  and  Mistral-Instruct-7B  for the baseline and SAR, as shown in  Table 1 .",
            "As discussed in  Section 3.3 , the main goal of SAR is to help models consider the societal impact of recommended cities during the reranking process.  For our evaluation, we focus on the S-Fairness ranks of the retrieved cities, where a better rank indicates a lower S-Fairness indicator value.  To extract the list of recommended cities, we tokenize the generated response and then compare it with the list of retrieved cities from the context.  We measure the  accuracy  of our methodology by measuring how often each model selects the city with the lowest sustainability score as its top choice.  Our results, as described under \"Sustainability\" in  Table 1 , show that  Llama-3.1-Instruct-8B  recommends the most sustainable city as a top candidate 10.5% of the time, compared to  Mistral-Instruct-7B  at 7.5%.",
            "Our results, listed under \"Faithfulness\" in  Table 1  show that  Llama-3.1-Instruct-8B  performs exceptionally well, with no OC responses in the baseline model, outperforming  Mistral-Instruct-7B , which hallucinates 14% of the time. Introducing SAR reduces hallucinations in  Mistral-Instruct-7B  to 9.5%, further supporting its effectiveness. With  Llama-3.1-Instruct-8B , neither the baseline nor the SAR-enabled models recommend any OC responses."
        ]
    },
    "global_footnotes": [
        "https://www.wikivoyage.org/",
        "https://tripadvisor-content-api.readme.io/reference/overview",
        "https://lancedb.github.io/lancedb/",
        "https://huggingface.co/learn/cookbook/en/llm_judge"
    ]
}