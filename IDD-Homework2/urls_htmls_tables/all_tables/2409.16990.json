{
    "id_table_1": {
        "caption": "Table 1 :  Out-of-domain  single image 3D face generation results on FFHQ.",
        "table": "S4.T1.5",
        "footnotes": [
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "To mitigate this data scarcity challenge, the latest attempt for single-image 3D face generation leverages the human geometric priors by incorporating ground-truth mesh in multi-view synthesis  [ 7 ] . A promising finding from this work is that properly blending image appearance and meshs geometric knowledge enables the model to work across different views at good quality. However, we find that their method suffers from several limitations that significantly hampers its generalisation to unconstrained face images shown in Figure  1 : (i)  Over reliance on the ground-truth mesh , which is often unavailable in practice; (ii)  Overfitting to the training domain  due to the stringent need with training data, thus limited data availability, so that the model just cannot generalise to different unseen styles.",
            "Joint conditioning of appearance and geometry  The key in our context is how to effectively condition the multi-view diffusion process with both the appearance of the single image  y y y italic_y  and the geometry of the estimated mesh  M M \\mathbb{M} blackboard_M  (Sec.  3.1 ).",
            "Out-of-domain evaluation:   From the quantitative results in Table   1 , we observe that:  (1)  Interestingly, generic object diffusion models (Zero-1-to-3  [ 29 ] , SyncDreamer  [ 30 ] ) and earlier GAN models (PanoHead  [ 2 ] ) even outperforms the latest face focused diffusion model (Morphable Diffusion  [ 7 ] ) on the three out of four metrics. This is an  opposite phenomenon  under this more challenging setting as compared to what was discovered in  [ 7 ] , suggesting that the evaluation setting  matters , fundamentally. This also implies that the way of imposing human geometry as  [ 7 ]  pays the generalisation cost implicitly in exchange of better performance for the limited in-domain setting.  (2)  Overall our Gen3D-Face is the best performer, except being secondary to Morphable Diffusion  [ 7 ]  on the output-to-output ID consistency metric. We note that looking at this metric  alone  however is not comprehensive and even misleading since it overlooked the divergence of generated images from the input (e.g. being consistent multi-view images of a totally difference identity). Instead, we should jointly consider both input-to-output and output-to-output ID consistency. Fusing the two metrics could make the comparison easier but hard to make sense."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  In-domain  single image 3D face generation result on Facescape.",
        "table": "S4.T2.5",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Given a single face image  y y y italic_y  as input, we aim to generate a 3D face avatar for this person. To that end, we propose a new latent diffusion approach,  Gen3D-Face , with the architecture depicted in Figure  2 . It generates multi-view consistent images from the single image, which can then be fed into existing neural surface construction methods (e.g.  [ 57 ] ). For the former, we adopt the off-the-shelf Stable Diffusion  [ 46 ]  as the backbone where the diffusion and denoising take place in a latent feature embedding space (e.g. a pretrained VAE  [ 43 ] ). For self-containing, we first brief 2D diffusion and 3D diffusion next.",
            "As seen from Eq ( 2 ), previous methods  [ 7 ,  30 ]  often denoise a single view each time individually with condition on the previous steps output of all the views. This design requires  N N N italic_N  denoising times each for one view, which we consider is inferior in maintaining the view consistency. To overcome this issue, we propose a  multi-view joint generation  algorithm that instead denoises all the views concurrently at one time so that multi-view information interaction can be imposed and exploited. Specifically, instead of feeding one view  x t ( n ) subscript superscript x n t \\mathbf{x}^{(n)}_{t} bold_x start_POSTSUPERSCRIPT ( italic_n ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  as the decoders query at a time, we input all the views  x t ( 1 : N ) subscript superscript x : 1 N t \\mathbf{x}^{(1:N)}_{t} bold_x start_POSTSUPERSCRIPT ( 1 : italic_N ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  together. This difference enables us to additionally perform the 3D self attention operation  [ 48 ,  3 ,  68 ]  among all the novel views  x t ( 1 : N ) subscript superscript x : 1 N t \\mathbf{x}^{(1:N)}_{t} bold_x start_POSTSUPERSCRIPT ( 1 : italic_N ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and the input  y y y italic_y  for information exchange and enhancing view consistency.",
            "In-domain evaluation:   While this work stresses the importance of out-of-domain generalisation, we still evaluate the conventional in-domain setting. From Table   2  we observe that our method performs on par with the previous art model Morphable Diffusion  [ 7 ] . This suggests that our model does not sacrifice the training domain performance while enhancing the model generalisation. The qualitative evaluations in Figure  7  to display our method keep good identity consistency."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Ablation on the effect of synthetic and real training data.",
        "table": "S4.T3.4",
        "footnotes": [],
        "references": [
            "Multi-view face synthesis  We adopt the Panohead  [ 2 ]  to generate additional training images. We generated 25,000 virtual human identities, each represented by 48 images, with azimuth ranging from -180 to 180 degrees, and elevation angle from -40 to 40 degrees (see Figure  3 ).",
            "Joint conditioning of appearance and geometry  The key in our context is how to effectively condition the multi-view diffusion process with both the appearance of the single image  y y y italic_y  and the geometry of the estimated mesh  M M \\mathbb{M} blackboard_M  (Sec.  3.1 ).",
            "Training data  We evaluate the effect of synthetic and real training data. As shown in Table  3 , we find that (1) both real and synthetic data contribute positively, and real data is more useful despite smaller size; (2) Using both could significantly boost the performance, validating our motivation of training data expansion by synthesis; (3) The significant decrease for only use synthesis data is because training and evaluating camera view not same.",
            "Data pruning  We show examples of Janus problem and Identity inconsistency in Figure  4 , which are filtered out using our pruning process, and the effect of pruning shows in Table  3 ."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  Ablation on the effect of multi-view joint generation (MVJG) on the whole dataset.",
        "table": "S4.T4.4",
        "footnotes": [],
        "references": [
            "Data pruning  We show examples of Janus problem and Identity inconsistency in Figure  4 , which are filtered out using our pruning process, and the effect of pruning shows in Table  3 .",
            "Multi-view joint generation  We evaluate the effect of our multi-view joint generation. From Table  4  we find that this design helps improve all the metrics, suggesting a good contribution."
        ]
    }
}