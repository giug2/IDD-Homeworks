{
    "id_table_1": {
        "caption": "TABLE I :  Generation accuracy when aligning to expert features computed on the original noise-free training samples vs. expert features computed on noise-added training inputs.",
        "table": "S4.T1.4",
        "footnotes": [],
        "references": [
            "We propose feature-aligned diffusion to improve model generations, by aligning intermediate features of the diffusion model with the output features of an expert during fine-tuning. In this context, the expert model refers to a classification model, often used to evaluate synthetic generations  [ 7 ,  9 ] . Our approach is shown in Figure  1 .",
            "Typical diffusion model training involves adding noise according to a pre-specified schedule to each training sample passed into the diffusion model. Here, the loss function (from Section  III ) compares the model predicted noise to the true noise added to the sample. Feature alignment incorporates one additional step into the typical flow: we additionally pass the  noisy  training sample to the expert model, to compute the corresponding output features. Intermediate features of the diffusion model are then extracted and aligned with the expert features. In our case, the intermediate features are obtained from the output of the downsampling block of the diffusion U-Net (shown in Figure  1 ). In order to align the intermediate diffusion model features to the output features of the expert, we introduce the following loss function that maximizes the cosine similarity between the two. Computing cosine similarity in this manner requires the expert feature dimensions  E e subscript E e E_{e} italic_E start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT  to match the intermediate diffusion feature dimensions  E d subscript E d E_{d} italic_E start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT . Hence, we also add an additional trainable projection  W p subscript W p W_{p} italic_W start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT :"
        ]
    }
}