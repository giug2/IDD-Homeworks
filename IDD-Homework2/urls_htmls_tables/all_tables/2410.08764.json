{
    "id_table_1": {
        "caption": "Table 1:  Data Set Statistics",
        "table": "S3.T1.1",
        "footnotes": [],
        "references": [
            "Our methodology investigates diverse approaches to classify responses based on their foundation in the provided source material (cf. Fig.  1 ).  We utilize:",
            "These adapted responses, which we consider partially ungrounded 2 2 2 Only some sentences ended up with slight modifications, while most were kept as the original sentences. , complement our final dataset.  The inclusion of both grounded and ungrounded responses allows for a more comprehensive evaluation of response quality and adherence to provided context.  An example of this subtle deviation from the source material in the generated response was depicted in the leading example in Fig.  1 .",
            "The resulting counts for each split are presented in Table  1 .  It is noteworthy that the number of responses is not exactly twice the number of queries.  This discrepancy arises from our dataset creation process, where we retained multiple significant variations of generated responses for certain queries to enhance the diversity and coverage of our dataset.",
            "To establish ground truth for the grounded responses, we employed a semantic similarity measure (as described in Section  5.1 ).  For each sentence in the grounded response, we identified the most semantically similar sentence from the context and assigned it the corresponding cosine similarity score.  These scores typically ranged from  0.8 0.8 0.8 0.8  to  0.99 0.99 0.99 0.99 , indicating high levels of semantic alignment.",
            "We aggregated predictions where both LLMs agreed.  The distribution of unique error types is visualized in Fig.  3 , with per-response occurrences in Fig.  4  (App.  A.1 ).   Factual Inaccuracies  were most common, followed by  Reasoning Errors .  All initially defined error types were represented, validating our classification scheme."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  DEV set metrics for DeBERTa models",
        "table": "S5.T2.1",
        "footnotes": [],
        "references": [
            "The outcomes of our fine-tuning efforts (after hyper-parameter optimization), are comprehensively presented (macro averaged) in Tab.  2 .",
            "Following the fine-tuning stage, we integrated this grounding classification (GC) model into our benchmark, employing a methodology analogous to that used for the NLI approaches described in Section  5.2 .",
            "The multi-stage prompt chaining approach,  DeepEval Claims Verify , achieved top classification metrics, but with high latency ( 26.1 26.1 26.1 26.1  seconds per request).  In contrast,  direct prompting  with  GPT-4o  achieved the second-highest scores with significantly lower latency ( 2.2 2.2 2.2 2.2  seconds), as illustrated in Fig.  2 .",
            "Through examination of error spans in the training set, we identified six distinct error types.  The models were instructed to select from our predefined error types (Tab.  5 , App.  A.2 )."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Performance comparison of different models on Development and Test sets",
        "table": "S6.T3.1.1",
        "footnotes": [],
        "references": [
            "Our benchmark evaluation of groundedness classification approaches revealed insightful performance trade-offs, as shown in Tab.  3 .  The metrics include classification precision, recall, F1-score, and accuracy, providing a comprehensive view of each methods applicability.",
            "We aggregated predictions where both LLMs agreed.  The distribution of unique error types is visualized in Fig.  3 , with per-response occurrences in Fig.  4  (App.  A.1 ).   Factual Inaccuracies  were most common, followed by  Reasoning Errors .  All initially defined error types were represented, validating our classification scheme."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Development set misclassification of the best performing model by error types.",
        "table": "S6.T4.1.1",
        "footnotes": [],
        "references": [
            "We aggregated predictions where both LLMs agreed.  The distribution of unique error types is visualized in Fig.  3 , with per-response occurrences in Fig.  4  (App.  A.1 ).   Factual Inaccuracies  were most common, followed by  Reasoning Errors .  All initially defined error types were represented, validating our classification scheme.",
            "We conducted a misclassification analysis on our best-performing model,  DeepEval Claims Verify , to gain deeper insights into its performance across different error types.  As summarized in Tab.  4 ,  Terminological Errors  showed the highest misclassification rate ( 67 % percent 67 67\\% 67 % ), despite their low frequency, followed by  Factual Inaccuracies  ( 20 % percent 20 20\\% 20 % ) and  Procedural Errors  ( 20 % percent 20 20\\% 20 % ).  These findings reveal the varying challenges posed by different error categories and highlight areas for potential improvement in groundedness classification models, particularly in handling less common but difficult-to-classify error types."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Response error types with a description and examples",
        "table": "A1.T5.1",
        "footnotes": [],
        "references": [
            "To establish ground truth for the grounded responses, we employed a semantic similarity measure (as described in Section  5.1 ).  For each sentence in the grounded response, we identified the most semantically similar sentence from the context and assigned it the corresponding cosine similarity score.  These scores typically ranged from  0.8 0.8 0.8 0.8  to  0.99 0.99 0.99 0.99 , indicating high levels of semantic alignment.",
            "Following the fine-tuning stage, we integrated this grounding classification (GC) model into our benchmark, employing a methodology analogous to that used for the NLI approaches described in Section  5.2 .",
            "Through examination of error spans in the training set, we identified six distinct error types.  The models were instructed to select from our predefined error types (Tab.  5 , App.  A.2 ).",
            "See the table  5  for our six response error types with their descriptions and examples."
        ]
    },
    "global_footnotes": [
        "Only some sentences ended up with slight modifications, while most were kept as the original sentences."
    ]
}