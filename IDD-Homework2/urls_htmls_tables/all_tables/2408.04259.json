{
    "id_table_1": {
        "caption": "Table 1:  Amount of synthesized training data for different datasets.",
        "table": "S3.T1.1",
        "footnotes": [],
        "references": [
            "In this section, we conducted an empirical study to assess how well an LLM-based generator performs with different levels of retrieved information.  We test on three settings: direct prompt (no retrieved chunks), oracle chunks (oracle chunks as the context), and mixed chunks (both oracle and irrelevant chunks as the context) on three datasets,  i.e. , HotpotQA  Yang et al. ( 2018b ) , 2Wiki-multihop (2WikiMQA)  Ho et al. ( 2020a )  and MuSiQue  Trivedi et al. ( 2022b ) .  The generator model includes GPT-3.5  OpenAI ( 2022 ) , GPT-4  OpenAI ( 2023 )  with 1106-preview version, and Llama-3-8B 1 1 1 https://llama.meta.com/llama3/   Touvron et al. ( 2023a ) .  We evaluate the model answer with accuracy metric by GPT-3.5, the prompt can be found in Appendix  B.1 .  As illustrated in Figure   1 , retrieval proves beneficial, with both oracle and mixture settings outperforming the direct answering approach. Nonetheless, the presence of irrelevant chunks continues to challenge the LLM generator, underscoring the need for more precise information retrieval.",
            "It is a common practice to use LLMs for query decomposition when facing complex multi-hop questions  Gao et al. ( 2023 ) . We conduct another empirical study to check how query decomposition approaches impact the retrieval stage. As shown in Figure  2 , the number of oracle chunks retrieved by one-time decomposition (LLM Decompose, detailed in Table  11 ) outperforms the Direct retrieval for the original query.  At a similar number of chunks, iterative decomposition (EfficientRAG Decompose) achieves higher recall.  When retrieving approximately 20 chunks, the Recall achieved by EfficientRAG Decompose has comparable performance with the LLM Decompose when retrieving around 200 chunks, thus demonstrating the efficiency of EfficientRAG Decompose. All retrievers used the contriever-msmarco  Izacard et al. ( 2022 )  setup, with chunk retrievals configured as 1/3/5/10/20/30/50/100, and the LLM endpoint is gpt35-turbo-1106.",
            "We utilize LLM to synthesize training data for the Labeler and Filter.  The process consists of multi-hop question decomposition, token labeling, next-hop question filtering, and negative sampling.  Synthetic data is detailed in Table  1 .",
            "We conduct evaluations of our EfficientRAG and multiple baselines on three multi-hop question-answering datasets same as  2.1 .  We select the following models as our baselines.  First is direct answering without retrieval, including LMs with proprietary data.  We include direct prompting and Chain-of-Thought prompting  Touvron et al. ( 2023a )  and question decomposition prompting in this setting.  Secondly, we include baselines with naive RAG with top-10 retrieve chunks as its knowledge.  Third, we include advanced iterative RAG methods like Iter-RetGen  Shao et al. ( 2023 )  and SelfAsk  Press et al. ( 2023 ) .  The implementation prompts are in Appendix   B.3 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Results on retrieval performance. Baselines are implemented from the source code. Bold and underlined fonts denote the best and second-best results respectively. EfficientRAG demonstrates comparable recall while retrieving the fewest number of chunks.",
        "table": "S3.T2.1",
        "footnotes": [],
        "references": [
            "It is a common practice to use LLMs for query decomposition when facing complex multi-hop questions  Gao et al. ( 2023 ) . We conduct another empirical study to check how query decomposition approaches impact the retrieval stage. As shown in Figure  2 , the number of oracle chunks retrieved by one-time decomposition (LLM Decompose, detailed in Table  11 ) outperforms the Direct retrieval for the original query.  At a similar number of chunks, iterative decomposition (EfficientRAG Decompose) achieves higher recall.  When retrieving approximately 20 chunks, the Recall achieved by EfficientRAG Decompose has comparable performance with the LLM Decompose when retrieving around 200 chunks, thus demonstrating the efficiency of EfficientRAG Decompose. All retrievers used the contriever-msmarco  Izacard et al. ( 2022 )  setup, with chunk retrievals configured as 1/3/5/10/20/30/50/100, and the LLM endpoint is gpt35-turbo-1106.",
            "We conduct evaluations of our EfficientRAG and multiple baselines on three multi-hop question-answering datasets same as  2.1 .  We select the following models as our baselines.  First is direct answering without retrieval, including LMs with proprietary data.  We include direct prompting and Chain-of-Thought prompting  Touvron et al. ( 2023a )  and question decomposition prompting in this setting.  Secondly, we include baselines with naive RAG with top-10 retrieve chunks as its knowledge.  Third, we include advanced iterative RAG methods like Iter-RetGen  Shao et al. ( 2023 )  and SelfAsk  Press et al. ( 2023 ) .  The implementation prompts are in Appendix   B.3 .",
            "We constructed the training data following Section  3.2  with Llama-3-70B-Instruct (Prompts are detailed in Appendix   B.2 ).  We trained our model on  4  4\\times 4   Nvidia A100 GPUs for about 10 GPU-hours separately, with AdamW  Loshchilov and Hutter ( 2019 )  optimizer and a learning rate of 5e-6.",
            "The models performance was assessed using the Recall@K metric across three distinct datasets. As presented in Table   2 , EfficientRAG achieves notably high recall scores on HotpotQA and 2WikiMQA datasets, with recall values of 81.84 and 84.08, respectively. These results are impressive considering the minimal number of chunks retrieved 6.41 for HotpotQA and 3.69 for 2WikiMQA. However, the performance of EfficientRAG on the MuSiQue dataset was less satisfactory. This suboptimal result may be attributed to the smaller number of chunks retrieved and the increased complexity of the dataset.",
            "All prompts can be found in this section, and are given in the order of Direct, CoT, Decompose, Direct-R, Iter-RetGen, and Self-ask, as shown in Tables  9  to  22 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Results on end-to-end question answering performance across three datasets. The highest accuracy (Acc) values are highlighted in bold, while the second-highest are underlined. EfficientRAG exhibits promising high accuracy, comparable to that of the LLM-based baselines.",
        "table": "S3.T3.1",
        "footnotes": [],
        "references": [
            "EfficientRAG  consists of two lightweight components: the Labeler & Tagger and the Filter.  These components share the same model structure, with the Labeler & Tagger 2 2 2 We use the term Labeler as the representation.  producing outputs from separate heads within the same model and the filters output comes from another model.  Both the Labeler and the Filter function as token-level classifiers, classifying tokens as either true or false.  Figure  3  shows that how EfficientRAG  fits into traditional RAG systems.  Given a query, the retriever obtains relevant chunks from the database.  Then the labeler module annotates a sequence of tokens in this document representing the useful information that could (partially) answer the query.  The tagger module then tags the chunk, indicating whether the retrieved chunk is helpful or irrelevant. If the tag indicates there needs more information to answer the query,  i.e. , tagged as <Continue>, we will add this chunk to a candidate pool, which will be fed to the LLM-based generator to have the final answer.  Otherwise, if the document is labeled useless or irrelevant, we stop searching for the successor branches from this query.  The filter module takes both the labeled tokens and the current query to construct a new query for the next round of retrieval.  It is done by replacing the unknown part of the query with labeled tokens (useful information).",
            "We conduct evaluations of our EfficientRAG and multiple baselines on three multi-hop question-answering datasets same as  2.1 .  We select the following models as our baselines.  First is direct answering without retrieval, including LMs with proprietary data.  We include direct prompting and Chain-of-Thought prompting  Touvron et al. ( 2023a )  and question decomposition prompting in this setting.  Secondly, we include baselines with naive RAG with top-10 retrieve chunks as its knowledge.  Third, we include advanced iterative RAG methods like Iter-RetGen  Shao et al. ( 2023 )  and SelfAsk  Press et al. ( 2023 ) .  The implementation prompts are in Appendix   B.3 .",
            "We constructed the training data following Section  3.2  with Llama-3-70B-Instruct (Prompts are detailed in Appendix   B.2 ).  We trained our model on  4  4\\times 4   Nvidia A100 GPUs for about 10 GPU-hours separately, with AdamW  Loshchilov and Hutter ( 2019 )  optimizer and a learning rate of 5e-6.",
            "We further evaluate the QnA performance on the three datasets.  As is illustrated in Table   3 , our EfficientRAG framework achieves the second-highest accuracy on both HotpotQA and 2WikiMQA, and it also performs well on MuSiQue even with low recall."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Efficiency evaluation on different RAG paradigms. EfficientRAG exhibits a speed equivalent to direct retrieval methods and is three times faster than LLM-based baselines while maintaining a similar number of iterations.",
        "table": "S5.T4.1.1",
        "footnotes": [],
        "references": [
            "We randomly selected 200 samples from the MusiQue dataset for empirical research and calculated four indicators: LLM calls, iterations, latency, and GPU utilization. As shown in table  4  our method requires fewer iterations and achieves a 60%-80% improvement in time efficiency compared to other iterative methods while maintaining similar GPU utilization."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  End-to-end QA performance on the 2WikiMQA dataset using GPT-3.5-turbo-1106 generator. EfficientRAG achieves state-of-the-art accuracy.",
        "table": "S5.T5.1",
        "footnotes": [],
        "references": [
            "EfficientRAG can benefit from more powerful generators.  As is shown in Table  5 , the use of GPT-3.5 as a generator enhances the end-to-end performance of both the baselines and our method.  Notably, EfficientRAG continues to deliver exceptional results."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Out-of-domain experiments on 2WikiMQA and HotpotQA dataset. EfficientRAG demonstrates remarkable transferability across diverse datasets.",
        "table": "S5.T6.1",
        "footnotes": [],
        "references": [
            "EfficientRAG has the potential to adapt to different scenarios without further downstream training.  We propose an out-of-domain experiment across HotpotQA and 2WikiMQA datasets, where we train the model on one dataset and test it on the other.  Table  6  shows that our model adapts well to different datasets, and even surpasses the model trained on the original data in some cases.  It shows that EfficientRAG does not rely on domain-specific knowledge, exhibiting a certain degree of transferability."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Detailed prompts for GPT-3.5 evaluation.",
        "table": "A2.T7.1",
        "footnotes": [],
        "references": []
    },
    "id_table_8": {
        "caption": "Table 8:  Detailed prompts for training data construction with Llama-3 70B",
        "table": "A2.T8.1",
        "footnotes": [],
        "references": []
    },
    "id_table_9": {
        "caption": "Table 9:  Detailed prompts for Direct Question Answering with Llama-3 8B",
        "table": "A2.T9.1",
        "footnotes": [],
        "references": [
            "All prompts can be found in this section, and are given in the order of Direct, CoT, Decompose, Direct-R, Iter-RetGen, and Self-ask, as shown in Tables  9  to  22 ."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:  Detailed prompts for Chain-of-Thought Question Answering with Llama-3 8B On hotpotQA",
        "table": "A2.T10.1",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "Table 11:  Detailed prompts for multi-hop question decomposition, applicable to all datasets.",
        "table": "A2.T11.1",
        "footnotes": [],
        "references": [
            "It is a common practice to use LLMs for query decomposition when facing complex multi-hop questions  Gao et al. ( 2023 ) . We conduct another empirical study to check how query decomposition approaches impact the retrieval stage. As shown in Figure  2 , the number of oracle chunks retrieved by one-time decomposition (LLM Decompose, detailed in Table  11 ) outperforms the Direct retrieval for the original query.  At a similar number of chunks, iterative decomposition (EfficientRAG Decompose) achieves higher recall.  When retrieving approximately 20 chunks, the Recall achieved by EfficientRAG Decompose has comparable performance with the LLM Decompose when retrieving around 200 chunks, thus demonstrating the efficiency of EfficientRAG Decompose. All retrievers used the contriever-msmarco  Izacard et al. ( 2022 )  setup, with chunk retrievals configured as 1/3/5/10/20/30/50/100, and the LLM endpoint is gpt35-turbo-1106."
        ]
    },
    "id_table_12": {
        "caption": "Table 12:  Detailed prompts for Chain-of-Thought Question Answering with Llama-3 8B on MuSiQue",
        "table": "A2.T12.1",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "Table 13:  Detailed prompts for Chain-of-Thought Question Answering with Llama-3 8B on 2Wiki-MultihopQA",
        "table": "A2.T13.1",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "Table 14:  Detailed prompt for retrieval on HotpotQA",
        "table": "A2.T14.1",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "Table 15:  Detailed prompt for retrieval on MuSiQue",
        "table": "A2.T15.1",
        "footnotes": [],
        "references": []
    },
    "id_table_16": {
        "caption": "Table 16:  Detailed prompt for retrieval on 2Wiki-MultihopQA",
        "table": "A2.T16.1",
        "footnotes": [],
        "references": []
    },
    "id_table_17": {
        "caption": "Table 17:  Detailed prompt for Iter-RetGen on HotpotQA",
        "table": "A2.T17.1",
        "footnotes": [],
        "references": []
    },
    "id_table_18": {
        "caption": "Table 18:  Detailed prompt for Iter-RetGen on MuSiQue",
        "table": "A2.T18.1",
        "footnotes": [],
        "references": []
    },
    "id_table_19": {
        "caption": "Table 19:  Detailed prompt for Iter-RetGen on 2Wiki-MultihopQA",
        "table": "A2.T19.1",
        "footnotes": [],
        "references": []
    },
    "id_table_20": {
        "caption": "Table 20:  Detailed prompt for self-ask on HotpotQA",
        "table": "A2.T20.1",
        "footnotes": [],
        "references": []
    },
    "id_table_21": {
        "caption": "Table 21:  Detailed prompt for self-ask on MuSiQue",
        "table": "A2.T21.1",
        "footnotes": [],
        "references": []
    },
    "id_table_22": {
        "caption": "Table 22:  Detailed prompt for self-ask on 2Wiki-MultihopQA",
        "table": "A2.T22.1",
        "footnotes": [],
        "references": [
            "All prompts can be found in this section, and are given in the order of Direct, CoT, Decompose, Direct-R, Iter-RetGen, and Self-ask, as shown in Tables  9  to  22 ."
        ]
    },
    "global_footnotes": [
        "Equal Contribution",
        "Equal Contribution",
        "Corresponding author",
        "Work is done during an internship at Microsoft",
        "https://llama.meta.com/llama3/",
        "We use the term Labeler as the representation.",
        "https://spacy.io/"
    ]
}