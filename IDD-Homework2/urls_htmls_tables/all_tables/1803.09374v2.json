{
    "S5.T1": {
        "caption": "TABLE I: An ablation study on Nonlinearity Ensembling, Feature Gating, and Polarity Swap.",
        "table": "<table id=\"S5.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.1.1.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row\"><span id=\"S5.T1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th id=\"S5.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span id=\"S5.T1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">VQA 1.0 val</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.2.1.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t\">MUTANÂ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">8</a>]</cite>\n</th>\n<td id=\"S5.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">61.54</td>\n</tr>\n<tr id=\"S5.T1.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.3.2.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t\">Nonlinearity Ensembling (NE)</th>\n<td id=\"S5.T1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">61.66</td>\n</tr>\n<tr id=\"S5.T1.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.4.3.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">Feature Gating (FG)</th>\n<td id=\"S5.T1.1.4.3.2\" class=\"ltx_td ltx_align_center\">61.72</td>\n</tr>\n<tr id=\"S5.T1.1.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.5.4.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">NE + Polarity Swap (PS)</th>\n<td id=\"S5.T1.1.5.4.2\" class=\"ltx_td ltx_align_center\">61.77</td>\n</tr>\n<tr id=\"S5.T1.1.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.6.5.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row\"><span id=\"S5.T1.1.6.5.1.1\" class=\"ltx_text ltx_font_bold\">NE + FG</span></th>\n<td id=\"S5.T1.1.6.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T1.1.6.5.2.1\" class=\"ltx_text ltx_font_bold\">61.86</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "FigureÂ 3 demonstrates an example instantiated fusion\noperator that makes use of the Feature Gating idea, and implements the\nFeature Gating experiment described above. In FigureÂ 3,\nthe number of branches is set toÂ R=3ğ‘…3R=3 for clarity, while in the experiments\nof TableÂ I, the models haveÂ R=5ğ‘…5R=5 branches.",
            "In TableÂ I, we compare performance improvements between\ndifferent instantiations of the generalized fusion operator described in\nEquationÂ 10 and\nAlgorithmÂ 1, which are discussed in\nSectionÂ IV. We use the VQAÂ 1.0 validation set to compare the\neffect on the performance of fusion operators of adding Nonlinearity\nEnsembling, as well as Feature Gating and Polarity Swap, independently. We\nchose to first investigate these static components of the fusion operator\ndesign before fixing them while investigating possible post-fusion neural\nnetworks. Since the neural network is a learned component of the architecture,\nit has to adapt to the static components during training, and hence the optimal\nhyperparameters of the neural network may vary depending on the choice of\nstatic components in the model.",
            "Building on the best model based on the ablation study of\nTableÂ I, we use the more challenging VQAÂ 2.0 validation\nset to evaluate different post-fusion neural network architectures. We find\nthat a post-fusion neural network with six layers andÂ 128128128 hidden units\nper layer outperforms the NE + FG model by a margin ofÂ 0.530.530.53 percentage\npoints, improving the VQAÂ 2.0 validation OpenEnded accuracy (as defined\ninÂ [15]) fromÂ 60.57%percent60.5760.57\\% toÂ 61.1%percent61.161.1\\%. We find that with the\nlow number ofÂ 128128128 hidden units, dropout is detrimental to the accuracy,\nand the best model does not use dropout."
        ]
    },
    "S5.T2": {
        "caption": "TABLE II: A comparison with the state of the art of our best single model on the VQAÂ 2.0 test-dev and test-std sets.",
        "table": "<table id=\"S5.T2.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T2.2.3.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.2.3.1.1\" class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th id=\"S5.T2.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" colspan=\"4\"><span id=\"S5.T2.2.3.1.2.1\" class=\"ltx_text ltx_font_bold\">VQAÂ 2.0 test-dev</span></th>\n<th id=\"S5.T2.2.3.1.3\" class=\"ltx_td ltx_th ltx_th_column\"></th>\n<th id=\"S5.T2.2.3.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" colspan=\"4\"><span id=\"S5.T2.2.3.1.4.1\" class=\"ltx_text ltx_font_bold\">VQAÂ 2.0 test-std</span></th>\n</tr>\n<tr id=\"S5.T2.2.4.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.2.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\"><span id=\"S5.T2.2.4.2.1.1\" class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th id=\"S5.T2.2.4.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">All</th>\n<th id=\"S5.T2.2.4.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Y/N</th>\n<th id=\"S5.T2.2.4.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Number</th>\n<th id=\"S5.T2.2.4.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Other</th>\n<th id=\"S5.T2.2.4.2.6\" class=\"ltx_td ltx_th ltx_th_column\"></th>\n<th id=\"S5.T2.2.4.2.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">All</th>\n<th id=\"S5.T2.2.4.2.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Y/N</th>\n<th id=\"S5.T2.2.4.2.9\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Number</th>\n<th id=\"S5.T2.2.4.2.10\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Other</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.2.5.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.2.5.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">MCBÂ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">6</a>]</cite> as reported inÂ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib15\" title=\"\" class=\"ltx_ref\">15</a>]</cite>\n</th>\n<td id=\"S5.T2.2.5.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">61.96</td>\n<td id=\"S5.T2.2.5.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">78.41</td>\n<td id=\"S5.T2.2.5.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">38.81</td>\n<td id=\"S5.T2.2.5.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">53.23</td>\n<td id=\"S5.T2.2.5.1.6\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S5.T2.2.5.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">62.27</td>\n<td id=\"S5.T2.2.5.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\">78.82</td>\n<td id=\"S5.T2.2.5.1.9\" class=\"ltx_td ltx_align_center ltx_border_t\">38.28</td>\n<td id=\"S5.T2.2.5.1.10\" class=\"ltx_td ltx_align_center ltx_border_t\">53.36</td>\n</tr>\n<tr id=\"S5.T2.2.6.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.2.6.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MUTANÂ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">8</a>]</cite> as trained and evaluated by us</th>\n<td id=\"S5.T2.2.6.2.2\" class=\"ltx_td ltx_align_center\">63.13</td>\n<td id=\"S5.T2.2.6.2.3\" class=\"ltx_td ltx_align_center\">80.7</td>\n<td id=\"S5.T2.2.6.2.4\" class=\"ltx_td ltx_align_center\">39.4</td>\n<td id=\"S5.T2.2.6.2.5\" class=\"ltx_td ltx_align_center\">53.55</td>\n<td id=\"S5.T2.2.6.2.6\" class=\"ltx_td\"></td>\n<td id=\"S5.T2.2.6.2.7\" class=\"ltx_td ltx_align_center\">â€”</td>\n<td id=\"S5.T2.2.6.2.8\" class=\"ltx_td ltx_align_center\">â€”</td>\n<td id=\"S5.T2.2.6.2.9\" class=\"ltx_td ltx_align_center\">â€”</td>\n<td id=\"S5.T2.2.6.2.10\" class=\"ltx_td ltx_align_center\">â€”</td>\n</tr>\n<tr id=\"S5.T2.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ResNet featuresÂ <math id=\"S5.T2.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"7\\times 7\" display=\"inline\"><semantics id=\"S5.T2.1.1.1.m1.1a\"><mrow id=\"S5.T2.1.1.1.m1.1.1\" xref=\"S5.T2.1.1.1.m1.1.1.cmml\"><mn id=\"S5.T2.1.1.1.m1.1.1.2\" xref=\"S5.T2.1.1.1.m1.1.1.2.cmml\">7</mn><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S5.T2.1.1.1.m1.1.1.1\" xref=\"S5.T2.1.1.1.m1.1.1.1.cmml\">Ã—</mo><mn id=\"S5.T2.1.1.1.m1.1.1.3\" xref=\"S5.T2.1.1.1.m1.1.1.3.cmml\">7</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.1.1.1.m1.1b\"><apply id=\"S5.T2.1.1.1.m1.1.1.cmml\" xref=\"S5.T2.1.1.1.m1.1.1\"><times id=\"S5.T2.1.1.1.m1.1.1.1.cmml\" xref=\"S5.T2.1.1.1.m1.1.1.1\"></times><cn type=\"integer\" id=\"S5.T2.1.1.1.m1.1.1.2.cmml\" xref=\"S5.T2.1.1.1.m1.1.1.2\">7</cn><cn type=\"integer\" id=\"S5.T2.1.1.1.m1.1.1.3.cmml\" xref=\"S5.T2.1.1.1.m1.1.1.3\">7</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.1.1.1.m1.1c\">7\\times 7</annotation></semantics></math> (single)Â <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">22</a>]</cite>\n</th>\n<td id=\"S5.T2.1.1.2\" class=\"ltx_td ltx_align_center\">62.07</td>\n<td id=\"S5.T2.1.1.3\" class=\"ltx_td ltx_align_center\">79.20</td>\n<td id=\"S5.T2.1.1.4\" class=\"ltx_td ltx_align_center\">39.46</td>\n<td id=\"S5.T2.1.1.5\" class=\"ltx_td ltx_align_center\">52.62</td>\n<td id=\"S5.T2.1.1.6\" class=\"ltx_td\"></td>\n<td id=\"S5.T2.1.1.7\" class=\"ltx_td ltx_align_center\">62.27</td>\n<td id=\"S5.T2.1.1.8\" class=\"ltx_td ltx_align_center\">79.32</td>\n<td id=\"S5.T2.1.1.9\" class=\"ltx_td ltx_align_center\">39.77</td>\n<td id=\"S5.T2.1.1.10\" class=\"ltx_td ltx_align_center\">52.59</td>\n</tr>\n<tr id=\"S5.T2.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Bottom-up attention image features, adaptiveÂ <math id=\"S5.T2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"K\" display=\"inline\"><semantics id=\"S5.T2.2.2.1.m1.1a\"><mi id=\"S5.T2.2.2.1.m1.1.1\" xref=\"S5.T2.2.2.1.m1.1.1.cmml\">K</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.2.2.1.m1.1b\"><ci id=\"S5.T2.2.2.1.m1.1.1.cmml\" xref=\"S5.T2.2.2.1.m1.1.1\">ğ¾</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.2.2.1.m1.1c\">K</annotation></semantics></math> (single)Â <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">22</a>]</cite>\n</th>\n<td id=\"S5.T2.2.2.2\" class=\"ltx_td ltx_align_center\">65.32</td>\n<td id=\"S5.T2.2.2.3\" class=\"ltx_td ltx_align_center\">81.82</td>\n<td id=\"S5.T2.2.2.4\" class=\"ltx_td ltx_align_center\">44.21</td>\n<td id=\"S5.T2.2.2.5\" class=\"ltx_td ltx_align_center\">56.05</td>\n<td id=\"S5.T2.2.2.6\" class=\"ltx_td\"></td>\n<td id=\"S5.T2.2.2.7\" class=\"ltx_td ltx_align_center\">65.67</td>\n<td id=\"S5.T2.2.2.8\" class=\"ltx_td ltx_align_center\">82.20</td>\n<td id=\"S5.T2.2.2.9\" class=\"ltx_td ltx_align_center\">43.90</td>\n<td id=\"S5.T2.2.2.10\" class=\"ltx_td ltx_align_center\">56.26</td>\n</tr>\n<tr id=\"S5.T2.2.7.3\" class=\"ltx_tr\">\n<th id=\"S5.T2.2.7.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t\">Ours (single)</th>\n<td id=\"S5.T2.2.7.3.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">64.22</td>\n<td id=\"S5.T2.2.7.3.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">81.19</td>\n<td id=\"S5.T2.2.7.3.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">40.95</td>\n<td id=\"S5.T2.2.7.3.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">55.05</td>\n<td id=\"S5.T2.2.7.3.6\" class=\"ltx_td ltx_border_b ltx_border_t\"></td>\n<td id=\"S5.T2.2.7.3.7\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">64.64</td>\n<td id=\"S5.T2.2.7.3.8\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">81.62</td>\n<td id=\"S5.T2.2.7.3.9\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">41.19</td>\n<td id=\"S5.T2.2.7.3.10\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">55.22</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "In TableÂ II, we evaluate our best model on the VQAÂ 2.0\ntest-dev and test-std sets, and compare to previous state of the art models\nupon which our work is based, as well as to the best models\nofÂ [22], the winners of the 2017 VQA challenge. We\nfind that our best model achieves an absolute percentage point improvement\nofÂ 1.1%percent1.11.1\\% over the strong baseline ofÂ [8]. We note that the\nimprovement in OpenEnded accuracy of our model on the test-dev set is\nsignificantly larger in magnitude when compared to the improvement\nofÂ [8] overÂ [6]. We attribute\nour relatively large improvement in accuracy to the introduction of\nnonlinearities in the fusion operator. The nonlinearities both allow the fusion\noperator to ensemble nonlinear functions of the input features (via\nNonlinearity Ensembling), and to model nonlinear relationships between the\nbilinear features extracted by the Hadamard product (via the post-fusion neural\nnetworks).",
            "When comparing our modelâ€™s performance to that of the models\nofÂ [22] in TableÂ II, we note that the\nbest performing models ofÂ [22] gain a significant performance\nboost from using superior image features for the VQA task. In\nparticular,Â [22] use bottom-up\nattentionÂ [23], which makes use of a Faster\nR-CNNÂ [24] pipeline, to obtain features from object proposal\nregions of an image. Bottom-up attention features improve the models\nofÂ [22] byÂ â‰ˆ3%absentpercent3\\approx 3\\% absolute percentage points on average,\nand since our contribution focuses solely on improving the fusion operator, our\nmodel should gain similar improvements from using bottom-up attention features.\nThe multimodal fusion operator used inÂ [22] is a Hadamard\nproduct, as in MLB."
        ]
    }
}