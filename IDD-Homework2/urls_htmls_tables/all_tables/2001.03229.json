{
    "PAPER'S NUMBER OF TABLES": 1,
    "S6.T1": {
        "caption": "TABLE I: Statistics of Datasets",
        "table": "<table id=\"S6.T1.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T1.4.1.1\" class=\"ltx_tr\">\n<th id=\"S6.T1.4.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\" rowspan=\"2\"><span id=\"S6.T1.4.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Dataset</span></th>\n<th id=\"S6.T1.4.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column\" rowspan=\"2\"><span id=\"S6.T1.4.1.1.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Nodes</span></th>\n<th id=\"S6.T1.4.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column\"><span id=\"S6.T1.4.1.1.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sample per Node</span></th>\n<td id=\"S6.T1.4.1.1.4\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S6.T1.4.2.2\" class=\"ltx_tr\">\n<th id=\"S6.T1.4.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S6.T1.4.2.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">mean</span></th>\n<th id=\"S6.T1.4.2.2.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S6.T1.4.2.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">stdev</span></th>\n</tr>\n<tr id=\"S6.T1.4.3.3\" class=\"ltx_tr\">\n<th id=\"S6.T1.4.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span id=\"S6.T1.4.3.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Synthetic</span></th>\n<td id=\"S6.T1.4.3.3.2\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S6.T1.4.3.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">50</span></td>\n<td id=\"S6.T1.4.3.3.3\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S6.T1.4.3.3.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">17</span></td>\n<td id=\"S6.T1.4.3.3.4\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S6.T1.4.3.3.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">5</span></td>\n</tr>\n<tr id=\"S6.T1.4.4.4\" class=\"ltx_tr\">\n<th id=\"S6.T1.4.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S6.T1.4.4.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">MNIST</span></th>\n<td id=\"S6.T1.4.4.4.2\" class=\"ltx_td ltx_align_left\"><span id=\"S6.T1.4.4.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">100</span></td>\n<td id=\"S6.T1.4.4.4.3\" class=\"ltx_td ltx_align_left\"><span id=\"S6.T1.4.4.4.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">34</span></td>\n<td id=\"S6.T1.4.4.4.4\" class=\"ltx_td ltx_align_left\"><span id=\"S6.T1.4.4.4.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">5</span></td>\n</tr>\n<tr id=\"S6.T1.4.5.5\" class=\"ltx_tr\">\n<th id=\"S6.T1.4.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S6.T1.4.5.5.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Sent140</span></th>\n<td id=\"S6.T1.4.5.5.2\" class=\"ltx_td ltx_align_left\"><span id=\"S6.T1.4.5.5.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">706</span></td>\n<td id=\"S6.T1.4.5.5.3\" class=\"ltx_td ltx_align_left\"><span id=\"S6.T1.4.5.5.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">42</span></td>\n<td id=\"S6.T1.4.5.5.4\" class=\"ltx_td ltx_align_left\"><span id=\"S6.T1.4.5.5.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">35</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Synthetic data.",
                " To evaluate the impact of node similarity on the performance of convergence and fast adaptation, we follow a similar setup in ",
                "[",
                "14",
                "]",
                " to generate synthetic data. Specifically, the synthetic sample ",
                "(",
                "𝐱",
                "i",
                "j",
                ",",
                "𝐲",
                "i",
                "j",
                ")",
                "superscript",
                "subscript",
                "𝐱",
                "𝑖",
                "𝑗",
                "superscript",
                "subscript",
                "𝐲",
                "𝑖",
                "𝑗",
                "(\\mathbf{x}_{i}^{j},\\mathbf{y}_{i}^{j})",
                " for each node ",
                "i",
                "𝑖",
                "i",
                " is generated from the model ",
                "𝐲",
                "=",
                "a",
                "​",
                "r",
                "​",
                "g",
                "​",
                "m",
                "​",
                "a",
                "​",
                "x",
                "​",
                "(",
                "s",
                "​",
                "o",
                "​",
                "f",
                "​",
                "t",
                "​",
                "m",
                "​",
                "a",
                "​",
                "x",
                "​",
                "(",
                "𝐖𝐱",
                "+",
                "𝐛",
                ")",
                ")",
                "𝐲",
                "𝑎",
                "𝑟",
                "𝑔",
                "𝑚",
                "𝑎",
                "𝑥",
                "𝑠",
                "𝑜",
                "𝑓",
                "𝑡",
                "𝑚",
                "𝑎",
                "𝑥",
                "𝐖𝐱",
                "𝐛",
                "\\mathbf{y}=argmax(softmax(\\mathbf{Wx+b}))",
                " where ",
                "𝐱",
                "∈",
                "ℝ",
                "60",
                "𝐱",
                "superscript",
                "ℝ",
                "60",
                "\\mathbf{x}\\in\\mathbb{R}^{60}",
                ", ",
                "𝐖",
                "∈",
                "ℝ",
                "10",
                "×",
                "60",
                "𝐖",
                "superscript",
                "ℝ",
                "10",
                "60",
                "\\mathbf{W}\\in\\mathbb{R}^{10\\times 60}",
                " and ",
                "𝐛",
                "∈",
                "ℝ",
                "10",
                "𝐛",
                "superscript",
                "ℝ",
                "10",
                "\\mathbf{b}\\in\\mathbb{R}^{10}",
                ". Moreover, ",
                "𝐖",
                "i",
                "∼",
                "𝒩",
                "​",
                "(",
                "𝐮",
                "i",
                ",",
                "𝟏",
                ")",
                "similar-to",
                "subscript",
                "𝐖",
                "𝑖",
                "𝒩",
                "subscript",
                "𝐮",
                "𝑖",
                "1",
                "\\mathbf{W}_{i}\\sim\\mathcal{N}(\\mathbf{u}_{i},\\mathbf{1})",
                ", ",
                "𝐛",
                "i",
                "∼",
                "𝒩",
                "​",
                "(",
                "𝐮",
                "i",
                ",",
                "𝟏",
                ")",
                "similar-to",
                "subscript",
                "𝐛",
                "𝑖",
                "𝒩",
                "subscript",
                "𝐮",
                "𝑖",
                "1",
                "\\mathbf{b}_{i}\\sim\\mathcal{N}(\\mathbf{u}_{i},\\mathbf{1})",
                ", ",
                "𝐮",
                "i",
                "∼",
                "𝒩",
                "​",
                "(",
                "0",
                ",",
                "α",
                "~",
                ")",
                "similar-to",
                "subscript",
                "𝐮",
                "𝑖",
                "𝒩",
                "0",
                "~",
                "𝛼",
                "\\mathbf{u}_{i}\\sim\\mathcal{N}(0,\\tilde{\\alpha})",
                "; ",
                "𝐱",
                "i",
                "j",
                "∼",
                "𝒩",
                "​",
                "(",
                "𝐯",
                "i",
                ",",
                "Σ",
                ")",
                "similar-to",
                "superscript",
                "subscript",
                "𝐱",
                "𝑖",
                "𝑗",
                "𝒩",
                "subscript",
                "𝐯",
                "𝑖",
                "Σ",
                "\\mathbf{x}_{i}^{j}\\sim\\mathcal{N}(\\mathbf{v}_{i},\\Sigma)",
                " where the covariance matrix ",
                "Σ",
                "Σ",
                "\\Sigma",
                " is diagonal with ",
                "Σ",
                "k",
                ",",
                "k",
                "=",
                "k",
                "−",
                "1.2",
                "subscript",
                "Σ",
                "𝑘",
                "𝑘",
                "superscript",
                "𝑘",
                "1.2",
                "\\Sigma_{k,k}=k^{-1.2}",
                " and ",
                "𝐯",
                "i",
                "∼",
                "𝒩",
                "​",
                "(",
                "B",
                "i",
                ",",
                "1",
                ")",
                "similar-to",
                "subscript",
                "𝐯",
                "𝑖",
                "𝒩",
                "subscript",
                "𝐵",
                "𝑖",
                "1",
                "\\mathbf{v}_{i}\\sim\\mathcal{N}(B_{i},1)",
                ", ",
                "B",
                "i",
                "∼",
                "N",
                "​",
                "(",
                "0",
                ",",
                "β",
                "~",
                ")",
                "similar-to",
                "subscript",
                "𝐵",
                "𝑖",
                "𝑁",
                "0",
                "~",
                "𝛽",
                "B_{i}\\sim N(0,\\tilde{\\beta})",
                ". Intuitively, ",
                "α",
                "~",
                "~",
                "𝛼",
                "\\tilde{\\alpha}",
                " and ",
                "β",
                "~",
                "~",
                "𝛽",
                "\\tilde{\\beta}",
                " control the local model similarity across all nodes, which can be changed to generate heterogeneous local datasets named Synthetic(",
                "α",
                "~",
                "~",
                "𝛼",
                "\\tilde{\\alpha}",
                ", ",
                "β",
                "~",
                "~",
                "𝛽",
                "\\tilde{\\beta}",
                "). For all synthetic datasets, we consider 50 nodes in total and the number of samples on each node follows a power law. The objective is to learn the model parameters ",
                "𝐖",
                "𝐖",
                "\\mathbf{W}",
                " and ",
                "𝐛",
                "𝐛",
                "\\mathbf{b}",
                " with the cross-entropy loss function.",
                "Real data.",
                " We also explore two real datasets, MNIST ",
                "[",
                "22",
                "]",
                ", and Sentiment140 (Sent140) ",
                "[",
                "23",
                "]",
                " used for text sentiment analysis on tweets. For MNIST, we sample part of data and distribute the data among 100 nodes such that every node has samples of only two digits and the number of samples per device follows a power law. We study a convex classification problem with MNIST using multinomial logistic regression. Next, we consider a more complicated classification problem on Sent140 by taking each twitter account as a node, where the model takes a sequence of 25 characters as input, embeds each of the character into a 300 dimensional space by looking up the pretrained 300D GloVe embedding ",
                "[",
                "24",
                "]",
                ", and outputs one character per training sample through a network with 3 hidden layers with sizes 256, 128, 64, each including batch normalization and ReLU nonlinearities, followed by a linear layer and softmax. The loss function is the cross-entropy error between the predicted and true class for all models.",
                "Implementation.",
                " For each node, we divide the local dataset as a training set and a testing set. We select 80% nodes as the source nodes and evaluate the fast adaptation performance on the rest. When training with FedML, we vary the size of the training set, i.e., ",
                "K",
                "𝐾",
                "K",
                ", for the one-step gradient update, whereas the entire dataset is used for training in Fedavg. During testing, the trained model is first updated with the training set of testing nodes, and then evaluated on their testing sets. For FedML, we set both the learning rate ",
                "α",
                "𝛼",
                "\\alpha",
                " and meta learning rate ",
                "β",
                "𝛽",
                "\\beta",
                " as 0.01 for synthetic data and MNIST, while ",
                "α",
                "=",
                "0.01",
                "𝛼",
                "0.01",
                "\\alpha=0.01",
                " and ",
                "β",
                "=",
                "0.3",
                "𝛽",
                "0.3",
                "\\beta=0.3",
                " for Sent140. Fedavg has the same learning rate with ",
                "β",
                "𝛽",
                "\\beta",
                "."
            ]
        ]
    }
}