{
    "PAPER'S NUMBER OF TABLES": 7,
    "S5.T1": {
        "caption": "Table 1: Accuracy and training times of the extraction feature methods on 300 videos of the RWF-2000 dataset.",
        "table": "<table id=\"S5.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Classifier</th>\n<th id=\"S5.T1.1.1.1.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">Accuracy</th>\n<th id=\"S5.T1.1.1.1.3\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">Training time</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">C3D + SVM(fc7)</th>\n<td id=\"S5.T1.1.2.1.2\" class=\"ltx_td ltx_align_right ltx_border_t\">99.8%</td>\n<td id=\"S5.T1.1.2.1.3\" class=\"ltx_td ltx_align_right ltx_border_t\">23s</td>\n</tr>\n<tr id=\"S5.T1.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">C3D + Decision Trees(fc7)</th>\n<td id=\"S5.T1.1.3.2.2\" class=\"ltx_td ltx_align_right\">83.9%</td>\n<td id=\"S5.T1.1.3.2.3\" class=\"ltx_td ltx_align_right\">10s</td>\n</tr>\n<tr id=\"S5.T1.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">C3D + Random Forest (fc7)</th>\n<td id=\"S5.T1.1.4.3.2\" class=\"ltx_td ltx_align_right\">99.5%</td>\n<td id=\"S5.T1.1.4.3.3\" class=\"ltx_td ltx_align_right\">10s</td>\n</tr>\n<tr id=\"S5.T1.1.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">C3D + Decision Trees(fc6)</th>\n<td id=\"S5.T1.1.5.4.2\" class=\"ltx_td ltx_align_right\">85.9%</td>\n<td id=\"S5.T1.1.5.4.3\" class=\"ltx_td ltx_align_right\">10s</td>\n</tr>\n<tr id=\"S5.T1.1.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">C3D + Random Forest(fc6)</th>\n<td id=\"S5.T1.1.6.5.2\" class=\"ltx_td ltx_align_right ltx_border_bb\">99.5%</td>\n<td id=\"S5.T1.1.6.5.3\" class=\"ltx_td ltx_align_right ltx_border_bb\">10s</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We tried many different architectures, from starting the transfer learning one layer earlier, to adding multiple training layers, each layer containing from 256 to 4096 neurons. The goal being to get a better training time or ROC curve than the model used in the previous paper, therefore, the time to beat was 12min30s and the AUC was 0,987.\nThrough trial and error, we obtained results ranging from 8 minutes of training time and an AUC of 0.941 to 17 minutes and and AUC of 0.991.\nWe noticed that the more neurons we used on the before last layer, the faster the network converged, but at the same time, the worse the accuracy became.\nHowever, these results are not what we need since they are slower and less accurate than what a random forest classifier is able to produce (Table 1). The aim being to get the most efficient training possible due to material constraints in a federated context, we want to reduce training time as much as possible, which leads us to the next section.",
            "Table 1 presents the accuracy and training times of various classifiers using the C3D model and different feature extraction methods on 300 videos of the RWF-2000 dataset Cheng et al. (2021). The feature extraction process takes approximately five minutes, which needs to be added to the training time to determine the overall time required for generating a classifier."
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Number of videos per dataset and their approximate preprocessing time.",
        "table": "<table id=\"S5.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T2.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Dataset</th>\n<th id=\"S5.T2.1.1.3\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"># videos</th>\n<th id=\"S5.T2.1.1.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<math id=\"S5.T2.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\approx\" display=\"inline\"><semantics id=\"S5.T2.1.1.1.m1.1a\"><mo id=\"S5.T2.1.1.1.m1.1.1\" xref=\"S5.T2.1.1.1.m1.1.1.cmml\">‚âà</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.1.1.1.m1.1b\"><approx id=\"S5.T2.1.1.1.m1.1.1.cmml\" xref=\"S5.T2.1.1.1.m1.1.1\"></approx></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.1.1.1.m1.1c\">\\approx</annotation></semantics></math> preprocessing time</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">AIRTLab</th>\n<td id=\"S5.T2.1.2.1.2\" class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">350</td>\n<td id=\"S5.T2.1.2.1.3\" class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3m30s</td>\n</tr>\n<tr id=\"S5.T2.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Crowd Violence</th>\n<td id=\"S5.T2.1.3.2.2\" class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">246</td>\n<td id=\"S5.T2.1.3.2.3\" class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">36s</td>\n</tr>\n<tr id=\"S5.T2.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Hockey Fights</th>\n<td id=\"S5.T2.1.4.3.2\" class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1000</td>\n<td id=\"S5.T2.1.4.3.3\" class=\"ltx_td ltx_align_right\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1m08s</td>\n</tr>\n<tr id=\"S5.T2.1.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">RWF-2000 (400 videos)</th>\n<td id=\"S5.T2.1.5.4.2\" class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">400</td>\n<td id=\"S5.T2.1.5.4.3\" class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2m36s</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Although using decision trees instead of a SVM can save some time in training, the amount of time needed to extract features makes it not worthwhile to pursue this avenue in our context Sernani et al. (2021). The preprocessing times shown in table 2 are influenced by several factors, such as the frame rate, resolution, and length of the videos used in the datasets. For instance, the Hockey Fights dataset takes less time to process because each video lasts only one second, is filmed at around 30fps, and has a resolution of 360p. On the other hand, the AIRTlab dataset has longer videos, with a duration of five seconds, filmed at 30fps and a resolution of 1080p."
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Results of the model proposed by Sernani et al. (2021) on different datasets.",
        "table": "<table id=\"S5.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">C3DFC w/ Early Stopping</th>\n<th id=\"S5.T3.1.1.1.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">Accuracy</th>\n<th id=\"S5.T3.1.1.1.3\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">ROC AUC</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T3.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">AIRTLab</th>\n<td id=\"S5.T3.1.2.1.2\" class=\"ltx_td ltx_align_right ltx_border_t\">95.6%</td>\n<td id=\"S5.T3.1.2.1.3\" class=\"ltx_td ltx_align_right ltx_border_t\">0.9894</td>\n</tr>\n<tr id=\"S5.T3.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Crowd Violence</th>\n<td id=\"S5.T3.1.3.2.2\" class=\"ltx_td ltx_align_right\">99.0%</td>\n<td id=\"S5.T3.1.3.2.3\" class=\"ltx_td ltx_align_right\">0.9994</td>\n</tr>\n<tr id=\"S5.T3.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Hockey Fights</th>\n<td id=\"S5.T3.1.4.3.2\" class=\"ltx_td ltx_align_right\">96.6%</td>\n<td id=\"S5.T3.1.4.3.3\" class=\"ltx_td ltx_align_right\">0.9931</td>\n</tr>\n<tr id=\"S5.T3.1.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">RWF-2000 (400 videos)</th>\n<td id=\"S5.T3.1.5.4.2\" class=\"ltx_td ltx_align_right ltx_border_bb\">94.7%</td>\n<td id=\"S5.T3.1.5.4.3\" class=\"ltx_td ltx_align_right ltx_border_bb\">0.9922</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "The results presented in Table 3 can be used as a baseline for comparison with the subsequent models. It is important to note that the model used is the one presented in section 3.2, with 512 neurons on its penultimate dense layer."
        ]
    },
    "S5.T4": {
        "caption": "Table 4: Results of our model trained for 2 epochs on four different datasets using super-convergence and a maximum learning rate of 50‚Äãe‚àí250superscriptùëí250e^{-2}. The table shows the training time in minutes, the accuracy (ACC), and the receiver operating characteristic area under the curve (ROC AUC) achieved on each dataset. The model has 1024 neurons in the penultimate layer.",
        "table": "<table id=\"S5.T4.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T4.3.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.3.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">C3DFC w/ One-Cycle</th>\n<th id=\"S5.T4.3.1.1.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Training</th>\n<th id=\"S5.T4.3.1.1.3\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">ACC</th>\n<th id=\"S5.T4.3.1.1.4\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">ROC AUC</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T4.3.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.3.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">AIRTLab</th>\n<td id=\"S5.T4.3.2.1.2\" class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">3m11s</td>\n<td id=\"S5.T4.3.2.1.3\" class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">91.0%</td>\n<td id=\"S5.T4.3.2.1.4\" class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.9972</td>\n</tr>\n<tr id=\"S5.T4.3.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T4.3.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Crowd Violence</th>\n<td id=\"S5.T4.3.3.2.2\" class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">59s</td>\n<td id=\"S5.T4.3.3.2.3\" class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">98.4%</td>\n<td id=\"S5.T4.3.3.2.4\" class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.9994</td>\n</tr>\n<tr id=\"S5.T4.3.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T4.3.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Hockey Fights</th>\n<td id=\"S5.T4.3.4.3.2\" class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">1m35s</td>\n<td id=\"S5.T4.3.4.3.3\" class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">94.8%</td>\n<td id=\"S5.T4.3.4.3.4\" class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.9828</td>\n</tr>\n<tr id=\"S5.T4.3.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T4.3.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">RWF-2000 (400 videos)</th>\n<td id=\"S5.T4.3.5.4.2\" class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">3m03s</td>\n<td id=\"S5.T4.3.5.4.3\" class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">94.9%</td>\n<td id=\"S5.T4.3.5.4.4\" class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.9875</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "The tables ",
                "4",
                " and ",
                "5",
                " display the results of training our model on four different databases, with the penultimate layer utilizing 1024 neurons.",
                "The table ",
                "6",
                " shows us the accuracy of the multi-channel models we have experimented with during this research on the complete RWF-2000 dataset ",
                "Cheng ",
                "et al.",
                " (",
                "2021",
                ")",
                ".",
                "Our proposed model, referred to as ",
                "Diff-Gated",
                ", achieves higher accuracy while reducing computation in comparison to the original Flow-Gated architecture ",
                "Cheng ",
                "et al.",
                " (",
                "2021",
                ")",
                ". Table ",
                "7",
                " presents the validation accuracy of our federated model for each round of training on 400 videos from the RWF-2000 dataset. Round 0 accuracy corresponds to the validation accuracy of the model before the first round of training."
            ]
        ]
    },
    "S5.T5": {
        "caption": "Table 5: Results of our model using super-convergence and 1 epoch of training and a maximum learning rate of 60‚Äãe‚àí260superscriptùëí260e^{-2}.",
        "table": "<table id=\"S5.T5.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T5.3.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T5.3.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">C3DFC w/ One-Cycle</th>\n<th id=\"S5.T5.3.1.1.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Training</th>\n<th id=\"S5.T5.3.1.1.3\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Acc</th>\n<th id=\"S5.T5.3.1.1.4\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">ROC AUC</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T5.3.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T5.3.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">AIRTLab</th>\n<td id=\"S5.T5.3.2.1.2\" class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">1m45</td>\n<td id=\"S5.T5.3.2.1.3\" class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">89.8%</td>\n<td id=\"S5.T5.3.2.1.4\" class=\"ltx_td ltx_align_right ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.9437</td>\n</tr>\n<tr id=\"S5.T5.3.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T5.3.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Crowd Violence</th>\n<td id=\"S5.T5.3.3.2.2\" class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">32s</td>\n<td id=\"S5.T5.3.3.2.3\" class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">97.2%</td>\n<td id=\"S5.T5.3.3.2.4\" class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.9627</td>\n</tr>\n<tr id=\"S5.T5.3.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T5.3.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Hockey Fights</th>\n<td id=\"S5.T5.3.4.3.2\" class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">59s</td>\n<td id=\"S5.T5.3.4.3.3\" class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">94.8%</td>\n<td id=\"S5.T5.3.4.3.4\" class=\"ltx_td ltx_align_right\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.9755</td>\n</tr>\n<tr id=\"S5.T5.3.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T5.3.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">RWF-2000 (400 videos)</th>\n<td id=\"S5.T5.3.5.4.2\" class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">1m52s</td>\n<td id=\"S5.T5.3.5.4.3\" class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">90.4%</td>\n<td id=\"S5.T5.3.5.4.4\" class=\"ltx_td ltx_align_right ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0.9669</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "The tables ",
                "4",
                " and ",
                "5",
                " display the results of training our model on four different databases, with the penultimate layer utilizing 1024 neurons.",
                "The table ",
                "6",
                " shows us the accuracy of the multi-channel models we have experimented with during this research on the complete RWF-2000 dataset ",
                "Cheng ",
                "et al.",
                " (",
                "2021",
                ")",
                ".",
                "Our proposed model, referred to as ",
                "Diff-Gated",
                ", achieves higher accuracy while reducing computation in comparison to the original Flow-Gated architecture ",
                "Cheng ",
                "et al.",
                " (",
                "2021",
                ")",
                ". Table ",
                "7",
                " presents the validation accuracy of our federated model for each round of training on 400 videos from the RWF-2000 dataset. Round 0 accuracy corresponds to the validation accuracy of the model before the first round of training."
            ]
        ]
    },
    "S5.T6": {
        "caption": "Table 6: Accuracy, training time, and processing time of our multi-channel input models on the RWF-2000 dataset.",
        "table": "<table id=\"S5.T6.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T6.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T6.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Context</th>\n<th id=\"S5.T6.1.1.1.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">Flow-gated</th>\n<th id=\"S5.T6.1.1.1.3\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">Diff-Gated</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T6.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T6.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Accuracy</th>\n<td id=\"S5.T6.1.2.1.2\" class=\"ltx_td ltx_align_right ltx_border_t\">87.25%</td>\n<td id=\"S5.T6.1.2.1.3\" class=\"ltx_td ltx_align_right ltx_border_t\">89.75%</td>\n</tr>\n<tr id=\"S5.T6.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T6.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Training time</th>\n<td id=\"S5.T6.1.3.2.2\" class=\"ltx_td ltx_align_right\">5h30</td>\n<td id=\"S5.T6.1.3.2.3\" class=\"ltx_td ltx_align_right\">5h00</td>\n</tr>\n<tr id=\"S5.T6.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T6.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Processing time for a video</th>\n<td id=\"S5.T6.1.4.3.2\" class=\"ltx_td ltx_align_right ltx_border_bb\">9s</td>\n<td id=\"S5.T6.1.4.3.3\" class=\"ltx_td ltx_align_right ltx_border_bb\">0.065s</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We retained the original Flow-Gated model architecture and modified the optical flow channel to use frame differences. We also added more dropout to the fully connected layer to prevent overfitting. This modified architecture, which we named ‚ÄùDiff-Gated‚Äù, is lightweight (272,546 parameters) and achieves better results than the original Flow-Gated network, as shown in Table 6. These changes have reduced preprocessing and training time while improving accuracy.",
            "The table 6 shows us the accuracy of the multi-channel models we have experimented with during this research on the complete RWF-2000 dataset Cheng et al. (2021)."
        ]
    },
    "S5.T7": {
        "caption": "Table 7: Accuracy table of our federated model for each round",
        "table": "<table id=\"S5.T7.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T7.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T7.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Round</th>\n<th id=\"S5.T7.1.1.1.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0</th>\n<th id=\"S5.T7.1.1.1.3\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1</th>\n<th id=\"S5.T7.1.1.1.4\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2</th>\n<th id=\"S5.T7.1.1.1.5\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3</th>\n<th id=\"S5.T7.1.1.1.6\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T7.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T7.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Accuracy</th>\n<td id=\"S5.T7.1.2.1.2\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">50.40%</td>\n<td id=\"S5.T7.1.2.1.3\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">94.84%</td>\n<td id=\"S5.T7.1.2.1.4\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">97.72%</td>\n<td id=\"S5.T7.1.2.1.5\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">98.80%</td>\n<td id=\"S5.T7.1.2.1.6\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">99.60%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Our proposed model, referred to as Diff-Gated, achieves higher accuracy while reducing computation in comparison to the original Flow-Gated architecture Cheng et al. (2021). Table 7 presents the validation accuracy of our federated model for each round of training on 400 videos from the RWF-2000 dataset. Round 0 accuracy corresponds to the validation accuracy of the model before the first round of training."
        ]
    }
}