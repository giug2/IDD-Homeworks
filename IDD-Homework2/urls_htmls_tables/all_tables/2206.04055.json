{
    "PAPER'S NUMBER OF TABLES": 2,
    "S6.T1": {
        "caption": "Table 1: \nImage pair and corresponding tags in the ROGS attack.\n\n",
        "table": "<table id=\"S6.T1.8\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S6.T1.8.9\" class=\"ltx_tr\">\n<td id=\"S6.T1.8.9.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:60.7pt;\">\n<span id=\"S6.T1.8.9.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S6.T1.8.9.1.1.1\" class=\"ltx_p\">Image</span>\n</span>\n</td>\n<td id=\"S6.T1.8.9.2\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:136.6pt;\">\n<span id=\"S6.T1.8.9.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S6.T1.8.9.2.1.1\" class=\"ltx_p\">Tags</span>\n</span>\n</td>\n<td id=\"S6.T1.8.9.3\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:60.7pt;\">\n<span id=\"S6.T1.8.9.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S6.T1.8.9.3.1.1\" class=\"ltx_p\">Image</span>\n</span>\n</td>\n<td id=\"S6.T1.8.9.4\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_tt\" style=\"width:136.6pt;\">\n<span id=\"S6.T1.8.9.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S6.T1.8.9.4.1.1\" class=\"ltx_p\">Tags</span>\n</span>\n</td>\n</tr>\n<tr id=\"S6.T1.2.2\" class=\"ltx_tr\">\n<td id=\"S6.T1.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:60.7pt;\">\n<span id=\"S6.T1.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\"><img src=\"/html/2206.04055/assets/figs/semantic/Bernese_mountain_dog/ori.png\" id=\"S6.T1.1.1.1.1.g1\" class=\"ltx_graphics ltx_img_square\" width=\"77\" height=\"77\" alt=\"[Uncaptioned image]\">\n</span>\n</td>\n<td id=\"S6.T1.2.2.3\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:136.6pt;\">\n<span id=\"S6.T1.2.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S6.T1.2.2.3.1.1\" class=\"ltx_p\"><span id=\"S6.T1.2.2.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Black, Dog, White, Brown, Green, Snout, Grass, Tints and shades, Turquoise (Color), Habitat, Beige, Grey, Carnivore, Bernese mountain dog, Light, Limb, …</span></span>\n</span>\n</td>\n<td id=\"S6.T1.2.2.2\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:60.7pt;\">\n<span id=\"S6.T1.2.2.2.1\" class=\"ltx_inline-block ltx_align_top\"><img src=\"/html/2206.04055/assets/figs/semantic/mailbox/ori.png\" id=\"S6.T1.2.2.2.1.g1\" class=\"ltx_graphics ltx_img_square\" width=\"77\" height=\"77\" alt=\"[Uncaptioned image]\">\n</span>\n</td>\n<td id=\"S6.T1.2.2.4\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:136.6pt;\">\n<span id=\"S6.T1.2.2.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S6.T1.2.2.4.1.1\" class=\"ltx_p\"><span id=\"S6.T1.2.2.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Post box, Mailbox, Red, Daytime, Maroon, Infrastructure, Public space, White, Snow, Carmine, Line, Amber (Color), Morning, Circle, Material, …</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S6.T1.4.4\" class=\"ltx_tr\">\n<td id=\"S6.T1.3.3.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:60.7pt;\">\n<span id=\"S6.T1.3.3.1.1\" class=\"ltx_inline-block ltx_align_top\"><img src=\"/html/2206.04055/assets/figs/semantic/Bernese_mountain_dog/798.png\" id=\"S6.T1.3.3.1.1.g1\" class=\"ltx_graphics ltx_img_square\" width=\"77\" height=\"77\" alt=\"[Uncaptioned image]\">\n</span>\n</td>\n<td id=\"S6.T1.4.4.3\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:136.6pt;\">\n<span id=\"S6.T1.4.4.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S6.T1.4.4.3.1.1\" class=\"ltx_p\"><span id=\"S6.T1.4.4.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Dog, Snout, Black, Brown, White, Bernese mountain dog, Green, Tints and shades, Habitat, Grass, Carnivore, Shoe, Nature, Turquoise (Color), …</span></span>\n</span>\n</td>\n<td id=\"S6.T1.4.4.2\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:60.7pt;\">\n<span id=\"S6.T1.4.4.2.1\" class=\"ltx_inline-block ltx_align_top\"><img src=\"/html/2206.04055/assets/figs/semantic/mailbox/595.png\" id=\"S6.T1.4.4.2.1.g1\" class=\"ltx_graphics ltx_img_square\" width=\"77\" height=\"77\" alt=\"[Uncaptioned image]\">\n</span>\n</td>\n<td id=\"S6.T1.4.4.4\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:136.6pt;\">\n<span id=\"S6.T1.4.4.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S6.T1.4.4.4.1.1\" class=\"ltx_p\"><span id=\"S6.T1.4.4.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Post box, Red, Mailbox, Snow, Daytime, White, Maroon, Carmine, Public space, Composite material, Structure, Line, Product, Winter, …</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S6.T1.6.6\" class=\"ltx_tr\">\n<td id=\"S6.T1.5.5.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:60.7pt;\">\n<span id=\"S6.T1.5.5.1.1\" class=\"ltx_inline-block ltx_align_top\"><img src=\"/html/2206.04055/assets/figs/semantic/robin/ori.png\" id=\"S6.T1.5.5.1.1.g1\" class=\"ltx_graphics ltx_img_square\" width=\"77\" height=\"77\" alt=\"[Uncaptioned image]\">\n</span>\n</td>\n<td id=\"S6.T1.6.6.3\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:136.6pt;\">\n<span id=\"S6.T1.6.6.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S6.T1.6.6.3.1.1\" class=\"ltx_p\"><span id=\"S6.T1.6.6.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Bird, Brown, Beak, Orange (Color), Feather, Habitat, Daytime, Amber (Color), American robin, Morning, Branch, Twig, Ivory (Color), …</span></span>\n</span>\n</td>\n<td id=\"S6.T1.6.6.2\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:60.7pt;\">\n<span id=\"S6.T1.6.6.2.1\" class=\"ltx_inline-block ltx_align_top\"><img src=\"/html/2206.04055/assets/figs/semantic/sulphur_butterfly/ori.png\" id=\"S6.T1.6.6.2.1.g1\" class=\"ltx_graphics ltx_img_square\" width=\"77\" height=\"77\" alt=\"[Uncaptioned image]\">\n</span>\n</td>\n<td id=\"S6.T1.6.6.4\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:136.6pt;\">\n<span id=\"S6.T1.6.6.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S6.T1.6.6.4.1.1\" class=\"ltx_p\"><span id=\"S6.T1.6.6.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Pollinator, Moths and butterflies, Flowering plant, Flower, Arthropod, Insect, Yellow, Butterfly, Magenta, Green, Purple coneflower, Spring (Season), …</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S6.T1.8.8\" class=\"ltx_tr\">\n<td id=\"S6.T1.7.7.1\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t\" style=\"width:60.7pt;\">\n<span id=\"S6.T1.7.7.1.1\" class=\"ltx_inline-block ltx_align_top\"><img src=\"/html/2206.04055/assets/figs/semantic/robin/924.png\" id=\"S6.T1.7.7.1.1.g1\" class=\"ltx_graphics ltx_img_square\" width=\"77\" height=\"77\" alt=\"[Uncaptioned image]\">\n</span>\n</td>\n<td id=\"S6.T1.8.8.3\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t\" style=\"width:136.6pt;\">\n<span id=\"S6.T1.8.8.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S6.T1.8.8.3.1.1\" class=\"ltx_p\"><span id=\"S6.T1.8.8.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Bird, Beak, Brown, Feather, Habitat, Orange (Color), Amber (Color), Daytime, Ivory (Color), Ecoregion, Blond, Orange (Color), Peach (Color), …</span></span>\n</span>\n</td>\n<td id=\"S6.T1.8.8.2\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t\" style=\"width:60.7pt;\">\n<span id=\"S6.T1.8.8.2.1\" class=\"ltx_inline-block ltx_align_top\"><img src=\"/html/2206.04055/assets/figs/semantic/sulphur_butterfly/983.png\" id=\"S6.T1.8.8.2.1.g1\" class=\"ltx_graphics ltx_img_square\" width=\"77\" height=\"77\" alt=\"[Uncaptioned image]\">\n</span>\n</td>\n<td id=\"S6.T1.8.8.4\" class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t\" style=\"width:136.6pt;\">\n<span id=\"S6.T1.8.8.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S6.T1.8.8.4.1.1\" class=\"ltx_p\"><span id=\"S6.T1.8.8.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Pollinator, Arthropod, Moths and butterflies, Butterfly, Insect, Invertebrate, Yellow, Wing, Flowering plant, Green, Ecosystem, Computer wallpaper, …</span></span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "The evaluation results of the same set of images in Figure 10 are given in Table 1 and Table 6.\nTable 1 lists the pairs of the original image and the reconstructed one, along with the detected tags excluding some generic tags such as “Photograph”, “World”, and “Color”.\nTable 6 gives the reconstruction quality scores.\nIt is observed that the tags of the dominating objects can be detected, including “Bernese mountain dog”, “Post box”, “Bird”, and “Butterfly”.\nThe background information is also included in the tags, such as “Grass”, “Snow”, “Twig”, and “Purple coneflower”.\nThe average PSNR, SSIM, and LPIPS scores compared to an image randomly selected from the same class are around 7.77.77.7 dB, 0.070.070.07, 0.630.630.63, respectively.\nPSNR values of ROGS results are around 141414 dB, which are worse than the ROG reconstruction and better than the random cases.\nIn addition, ROGS has slightly worse LPIPS values and worse SSIM compared to ROG.\nIn other words, ROGS does not directly improve the image similarity at the pixel level as compared to ROG, but consistently outperforms ROG in terms of Jaccard similarity scores, which is more oriented toward recognition at the semantic level.\nWe now compare the difference between the SSIM value and the Jaccard similarity.\nFor the first pair of Bernese mountain dogs, SSIM and Jaccard values are above 0.60.60.6 and tend to agree with each other.\nFor the second pair of post boxes, the SSIM gives a much higher value, 0.8320.8320.832, compared to Jaccard similarity, 0.60.60.6.\nA large discrepancy between SSIM and Jaccard can be found in the third and fourth pairs.\nFor the third pair of robins, the SSIM is 0.2850.2850.285, which is relatively low.\nIn the meantime, the semantics have been successfully reconstructed, which is also reflected in a Jaccard similarity of 0.60.60.6.\nA similar pattern can be observed in the fourth pair.\nThe reconstruction has revealed important information about butterflies resting on the flower.\nSuch a privacy loss may be ignored when merely checking the SSIM value of 0.3340.3340.334.",
            "Limitations.  \nWe would like to clarify that the aforementioned Jaccard similarity is not intended to replace the current metrics for image privacy loss measurement, but rather to serve as a supplement or as a motivation to help us rethink the privacy leakage problem.\nIn the meantime, there still exist some limitations in the reconstruction and evaluation at the semantic level.\nWe give some negative reconstruction examples in Figure 11.\nWhen the raw images contain a complicated scenario, such as including multiple dominating objects, the attack algorithm may fail to converge.\nThis can be observed in the first row of Figure 11.\nAnother example is when the raw image involves human faces, the reconstruction may not be able to reveal the person’s identity.\nThis may not be considered as a severe privacy leakage in some applications, such as facial data analysis.\nFurthermore, not all of the semantics can be included in the tags, such as the action, orientation, and body size.\nIn addition, the multi-label classification neural network may not always give the correct tags.\nFor example, a tag “Shoe” is detected in the second row of Table 1, which can be considered as a misclassification.\nThe importance of different tags and their correlations may also need to be considered to improve the similarity score design.\nA more comprehensive study is out of the scope of this work.\nHowever, we hope these results can inspire researchers to revisit the privacy leakage issues in federated learning."
        ]
    },
    "S6.12": {
        "caption": "Table 2: \nComparisons between ROG and ROGS with different metrics.\nROGS improves the semantic similarity score.\n\n",
        "table": "<table id=\"S6.2.2\" class=\"ltx_tabular ltx_figure_panel ltx_align_middle\">\n<tr id=\"S6.2.2.3\" class=\"ltx_tr\">\n<td id=\"S6.2.2.3.1\" class=\"ltx_td ltx_align_left ltx_border_tt\">Image</td>\n<td id=\"S6.2.2.3.2\" class=\"ltx_td ltx_align_left ltx_border_tt\">Attack</td>\n<td id=\"S6.2.2.3.3\" class=\"ltx_td ltx_align_left ltx_border_tt\">PSNR</td>\n<td id=\"S6.2.2.3.4\" class=\"ltx_td ltx_align_left ltx_border_tt\">SSIM</td>\n<td id=\"S6.2.2.3.5\" class=\"ltx_td ltx_align_left ltx_border_tt\">LPIPS</td>\n<td id=\"S6.2.2.3.6\" class=\"ltx_td ltx_align_left ltx_border_tt\">Jaccard</td>\n</tr>\n<tr id=\"S6.2.2.4\" class=\"ltx_tr\">\n<td id=\"S6.2.2.4.1\" class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\"><span id=\"S6.2.2.4.1.1\" class=\"ltx_text\">Dog</span></td>\n<td id=\"S6.2.2.4.2\" class=\"ltx_td ltx_align_left ltx_border_t\">ROG</td>\n<td id=\"S6.2.2.4.3\" class=\"ltx_td ltx_align_left ltx_border_t\">22.1 dB</td>\n<td id=\"S6.2.2.4.4\" class=\"ltx_td ltx_align_left ltx_border_t\">0.897</td>\n<td id=\"S6.2.2.4.5\" class=\"ltx_td ltx_align_left ltx_border_t\">0.332</td>\n<td id=\"S6.2.2.4.6\" class=\"ltx_td ltx_align_left ltx_border_t\">0.400</td>\n</tr>\n<tr id=\"S6.2.2.5\" class=\"ltx_tr\">\n<td id=\"S6.2.2.5.1\" class=\"ltx_td ltx_align_left\">ROGS</td>\n<td id=\"S6.2.2.5.2\" class=\"ltx_td ltx_align_left\">14.9 dB</td>\n<td id=\"S6.2.2.5.3\" class=\"ltx_td ltx_align_left\">0.611</td>\n<td id=\"S6.2.2.5.4\" class=\"ltx_td ltx_align_left\">0.348</td>\n<td id=\"S6.2.2.5.5\" class=\"ltx_td ltx_align_left\">0.632</td>\n</tr>\n<tr id=\"S6.2.2.6\" class=\"ltx_tr\">\n<td id=\"S6.2.2.6.1\" class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\"><span id=\"S6.2.2.6.1.1\" class=\"ltx_text\">Mailbox</span></td>\n<td id=\"S6.2.2.6.2\" class=\"ltx_td ltx_align_left ltx_border_t\">ROG</td>\n<td id=\"S6.2.2.6.3\" class=\"ltx_td ltx_align_left ltx_border_t\">21.3 dB</td>\n<td id=\"S6.2.2.6.4\" class=\"ltx_td ltx_align_left ltx_border_t\">0.946</td>\n<td id=\"S6.2.2.6.5\" class=\"ltx_td ltx_align_left ltx_border_t\">0.206</td>\n<td id=\"S6.2.2.6.6\" class=\"ltx_td ltx_align_left ltx_border_t\">0.433</td>\n</tr>\n<tr id=\"S6.2.2.7\" class=\"ltx_tr\">\n<td id=\"S6.2.2.7.1\" class=\"ltx_td ltx_align_left\">ROGS</td>\n<td id=\"S6.2.2.7.2\" class=\"ltx_td ltx_align_left\">14.5 dB</td>\n<td id=\"S6.2.2.7.3\" class=\"ltx_td ltx_align_left\">0.832</td>\n<td id=\"S6.2.2.7.4\" class=\"ltx_td ltx_align_left\">0.237</td>\n<td id=\"S6.2.2.7.5\" class=\"ltx_td ltx_align_left\">0.600</td>\n</tr>\n<tr id=\"S6.2.2.8\" class=\"ltx_tr\">\n<td id=\"S6.2.2.8.1\" class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\"><span id=\"S6.2.2.8.1.1\" class=\"ltx_text\">Robin</span></td>\n<td id=\"S6.2.2.8.2\" class=\"ltx_td ltx_align_left ltx_border_t\">ROG</td>\n<td id=\"S6.2.2.8.3\" class=\"ltx_td ltx_align_left ltx_border_t\">20.8 dB</td>\n<td id=\"S6.2.2.8.4\" class=\"ltx_td ltx_align_left ltx_border_t\">0.836</td>\n<td id=\"S6.2.2.8.5\" class=\"ltx_td ltx_align_left ltx_border_t\">0.299</td>\n<td id=\"S6.2.2.8.6\" class=\"ltx_td ltx_align_left ltx_border_t\">0.250</td>\n</tr>\n<tr id=\"S6.2.2.9\" class=\"ltx_tr\">\n<td id=\"S6.2.2.9.1\" class=\"ltx_td ltx_align_left\">ROGS</td>\n<td id=\"S6.2.2.9.2\" class=\"ltx_td ltx_align_left\">15.3 dB</td>\n<td id=\"S6.2.2.9.3\" class=\"ltx_td ltx_align_left\">0.285</td>\n<td id=\"S6.2.2.9.4\" class=\"ltx_td ltx_align_left\">0.350</td>\n<td id=\"S6.2.2.9.5\" class=\"ltx_td ltx_align_left\">0.600</td>\n</tr>\n<tr id=\"S6.2.2.10\" class=\"ltx_tr\">\n<td id=\"S6.2.2.10.1\" class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\"><span id=\"S6.2.2.10.1.1\" class=\"ltx_text\">Butterfly</span></td>\n<td id=\"S6.2.2.10.2\" class=\"ltx_td ltx_align_left ltx_border_t\">ROG</td>\n<td id=\"S6.2.2.10.3\" class=\"ltx_td ltx_align_left ltx_border_t\">21.3 dB</td>\n<td id=\"S6.2.2.10.4\" class=\"ltx_td ltx_align_left ltx_border_t\">0.900</td>\n<td id=\"S6.2.2.10.5\" class=\"ltx_td ltx_align_left ltx_border_t\">0.287</td>\n<td id=\"S6.2.2.10.6\" class=\"ltx_td ltx_align_left ltx_border_t\">0.312</td>\n</tr>\n<tr id=\"S6.2.2.11\" class=\"ltx_tr\">\n<td id=\"S6.2.2.11.1\" class=\"ltx_td ltx_align_left\">ROGS</td>\n<td id=\"S6.2.2.11.2\" class=\"ltx_td ltx_align_left\">13.9 dB</td>\n<td id=\"S6.2.2.11.3\" class=\"ltx_td ltx_align_left\">0.334</td>\n<td id=\"S6.2.2.11.4\" class=\"ltx_td ltx_align_left\">0.458</td>\n<td id=\"S6.2.2.11.5\" class=\"ltx_td ltx_align_left\">0.579</td>\n</tr>\n<tr id=\"S6.1.1.1\" class=\"ltx_tr\">\n<td id=\"S6.1.1.1.1\" class=\"ltx_td ltx_align_left\" colspan=\"2\">\n<span id=\"S6.1.1.1.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span>     Random Noise<sup id=\"S6.1.1.1.1.2\" class=\"ltx_sup\">1</sup>\n</td>\n<td id=\"S6.1.1.1.2\" class=\"ltx_td ltx_align_left\">6.5 dB</td>\n<td id=\"S6.1.1.1.3\" class=\"ltx_td ltx_align_left\">0.006</td>\n<td id=\"S6.1.1.1.4\" class=\"ltx_td ltx_align_left\">1.409</td>\n<td id=\"S6.1.1.1.5\" class=\"ltx_td ltx_align_left\">0.030</td>\n</tr>\n<tr id=\"S6.2.2.2\" class=\"ltx_tr\">\n<td id=\"S6.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_bb\" colspan=\"2\">Same Class Image<sup id=\"S6.2.2.2.1.1\" class=\"ltx_sup\">2</sup>\n</td>\n<td id=\"S6.2.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_bb\">7.7 dB</td>\n<td id=\"S6.2.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_bb\">0.070</td>\n<td id=\"S6.2.2.2.4\" class=\"ltx_td ltx_align_left ltx_border_bb\">0.631</td>\n<td id=\"S6.2.2.2.5\" class=\"ltx_td ltx_align_left ltx_border_bb\">0.248</td>\n</tr>\n</table>\n\n",
        "footnotes": "1 Average similarity between ROGS and random Gaussian noise over five repetitions.2 Average similarity between ROGS and images from the same ImageNet class over five repetitions.The evaluation results of the same set of images in Figure 10 are given in Table 1 and Table 6.\nTable 1 lists the pairs of the original image and the reconstructed one, along with the detected tags excluding some generic tags such as “Photograph”, “World”, and “Color”.\nTable 6 gives the reconstruction quality scores.\nIt is observed that the tags of the dominating objects can be detected, including “Bernese mountain dog”, “Post box”, “Bird”, and “Butterfly”.\nThe background information is also included in the tags, such as “Grass”, “Snow”, “Twig”, and “Purple coneflower”.\nThe average PSNR, SSIM, and LPIPS scores compared to an image randomly selected from the same class are around 7.77.77.7 dB, 0.070.070.07, 0.630.630.63, respectively.\nPSNR values of ROGS results are around 141414 dB, which are worse than the ROG reconstruction and better than the random cases.\nIn addition, ROGS has slightly worse LPIPS values and worse SSIM compared to ROG.\nIn other words, ROGS does not directly improve the image similarity at the pixel level as compared to ROG, but consistently outperforms ROG in terms of Jaccard similarity scores, which is more oriented toward recognition at the semantic level.\nWe now compare the difference between the SSIM value and the Jaccard similarity.\nFor the first pair of Bernese mountain dogs, SSIM and Jaccard values are above 0.60.60.6 and tend to agree with each other.\nFor the second pair of post boxes, the SSIM gives a much higher value, 0.8320.8320.832, compared to Jaccard similarity, 0.60.60.6.\nA large discrepancy between SSIM and Jaccard can be found in the third and fourth pairs.\nFor the third pair of robins, the SSIM is 0.2850.2850.285, which is relatively low.\nIn the meantime, the semantics have been successfully reconstructed, which is also reflected in a Jaccard similarity of 0.60.60.6.\nA similar pattern can be observed in the fourth pair.\nThe reconstruction has revealed important information about butterflies resting on the flower.\nSuch a privacy loss may be ignored when merely checking the SSIM value of 0.3340.3340.334.Limitations.  \nWe would like to clarify that the aforementioned Jaccard similarity is not intended to replace the current metrics for image privacy loss measurement, but rather to serve as a supplement or as a motivation to help us rethink the privacy leakage problem.\nIn the meantime, there still exist some limitations in the reconstruction and evaluation at the semantic level.\nWe give some negative reconstruction examples in Figure 11.\nWhen the raw images contain a complicated scenario, such as including multiple dominating objects, the attack algorithm may fail to converge.\nThis can be observed in the first row of Figure 11.\nAnother example is when the raw image involves human faces, the reconstruction may not be able to reveal the person’s identity.\nThis may not be considered as a severe privacy leakage in some applications, such as facial data analysis.\nFurthermore, not all of the semantics can be included in the tags, such as the action, orientation, and body size.\nIn addition, the multi-label classification neural network may not always give the correct tags.\nFor example, a tag “Shoe” is detected in the second row of Table 1, which can be considered as a misclassification.\nThe importance of different tags and their correlations may also need to be considered to improve the similarity score design.\nA more comprehensive study is out of the scope of this work.\nHowever, we hope these results can inspire researchers to revisit the privacy leakage issues in federated learning.The concept of federated learning was proposed to preserve sensitive client data for multi-party machine learning.\nIn this work, we have shown that it is possible to reconstruct client data with high quality from noisy gradients in a more realistic federated learning setting compared to existing attacks.\nContrary to prior empirical work, we have demonstrated that gradient compression and perturbation cannot be treated as effective privacy protection strategies.\nOur attack scheme has also successfully reconstructed private information under existing defenses.\nBased on these observations, we conjecture that for federated learning algorithms that do not provide privacy guarantees for data attributes, an adversary can always find a certain attack scheme to disclose the privacy in the raw data, as long as the mutual information between gradient and raw data is not close to zero.In the future, it will be interesting to investigate the trade-off between utility and privacy for hybrid defenses.\nAs it has been observed in [26], a combination of multiple defenses can provide better privacy protection.\nBesides, differentially private training with secure aggregation [2, 33] may also have potential to defend against the proposed ROG attack.\nIn parallel to the existing differential privacy paradigm, a more general formulation of the privacy concept will be worth exploring.\nWe also believe that the study of privacy leakage beyond the image classification task will provide more insights to the research community.We provide more details of the postprocessing neural networks as follows.\nWe adopt the U-Net architecture [53] as the GAN generator, which is composed of an encoder that downsamples the images and a decoder that upsamples the feature maps back to the original size.\nThis architecture has been widely adopted in image-to-image translation [28] and image inpainting [46].\nIn the generator, we take the form of convolution-InstanceNorm-ReLu for the backbone module, and downsample the input image four times.\nWe add skip connections between the encoder and the decoder to circumvent severe information loss caused by the downsampling.\nThe discriminator architecture is based on PatchGAN [46].To synthesize the dataset, we randomly apply one of the downsampling approaches, including nearest neighbor, bilinear scaling, and bicubic scaling, and add Gaussian noise to the training examples in the ImageNet dataset.\nIn particular, we randomly choose the order of downsampling, noise injection, and a Gaussian blur kernel to degrade the original images.\nWe use a linear combination of the ℓ1subscriptℓ1\\ell_{1} loss and the adversarial loss as the cost function.\nThe Adam optimizer is adopted for the GAN training, and the learning rate is set to 1×10−51superscript1051\\times 10^{-5} for both the generator and the discriminator.\nFor ROGS, we set λ1=λ2=1subscript𝜆1subscript𝜆21\\lambda_{1}=\\lambda_{2}=1 and optimize for 100010001000 iterations.\nThe pretrained models and implementation of the attack are available at https://anonymous.4open.science/r/rog-DC5E.We review the image quality metrics used in this work, including PSNR, SSIM, and LPIPS.\nGiven two images 𝐗,𝐘∈ℝh×w𝐗𝐘superscriptℝℎ𝑤\\mathbf{X},\\mathbf{Y}\\in\\mathbb{R}^{h\\times w}, the mean squared error (MSE) is defined as:where Xi,jsubscript𝑋𝑖𝑗X_{i,j} denotes the (i,j)𝑖𝑗(i,j)th entry of the matrix 𝐗𝐗\\mathbf{X}.\nThe PSNR in dB is given by:where MAXIsubscriptMAX𝐼\\mathrm{MAX}_{I} is the maximum possible pixel value of the image.\nBy definition, PSNR is a pixel-wise similarity metric.\nSSIM better approximates the perception model.\nGiven two images 𝐗,𝐘𝐗𝐘\\mathbf{X},\\mathbf{Y}, suppose μXsubscript𝜇𝑋\\mu_{X} and μYsubscript𝜇𝑌\\mu_{Y} are the average of 𝐗𝐗\\mathbf{X} and 𝐘𝐘\\mathbf{Y}, σX2superscriptsubscript𝜎𝑋2\\sigma_{X}^{2} and σY2superscriptsubscript𝜎𝑌2\\sigma_{Y}^{2} are the variance of the two images, and σX​Ysubscript𝜎𝑋𝑌\\sigma_{XY} is the covariance.\nThe SSIM is given by:where c1subscript𝑐1c_{1} and c2subscript𝑐2c_{2} are two variables to stabilize the division.\nBy design, SSIM∈[0,1]SSIM01\\operatorname{SSIM}\\in[0,1], and a higher value indicates a higher similarity.Compared to the traditional image quality metrics, LPIPS was proposed based on the activations of convolutional neural networks.\nConsider the l𝑙lth layer of a neural network with unit-normalized feature 𝑨l∈ℝHl×Wl×Clsuperscript𝑨𝑙superscriptℝsubscript𝐻𝑙subscript𝑊𝑙subscript𝐶𝑙\\boldsymbol{A}^{l}\\in\\mathbb{R}^{H_{l}\\times W_{l}\\times C_{l}}, where Hlsubscript𝐻𝑙H_{l}, Wlsubscript𝑊𝑙W_{l}, and Clsubscript𝐶𝑙C_{l} denote its height, width, and channels, respectively.\nWith predefined coefficients 𝐜l∈ℝClsubscript𝐜𝑙superscriptℝsubscript𝐶𝑙\\mathbf{c}_{l}\\in\\mathbb{R}^{C_{l}}, the LPIPS value can be calculated aswhere 𝑨h,wlsubscriptsuperscript𝑨𝑙ℎ𝑤\\boldsymbol{A}^{l}_{h,w} and 𝑩h,wlsubscriptsuperscript𝑩𝑙ℎ𝑤\\boldsymbol{B}^{l}_{h,w} denote the feature maps of two images.We provide more reconstruction results on the whole batch in addition to those given in Section 4 and Figure 3.\nThe reconstruction of a batch size of 161616 under a 333-bit QSGD quantizer is shown in Figure 12.\nWe compare the best and worst reconstruction results in Figure 13 when different batch sizes are used.\nThe best and worst results are selected based on LPIPS values.\nIn Figure 14, we show the best 161616 and worst 161616 reconstructed images when the batch size is set to 128128128.\nThe best reconstructed images convey meaningful pictorial information, whereas the worst reconstructed images are almost unrecognizable.We discuss some additional simulation results in this section.\nFirst, we demonstrate the full batch reconstruction with InvertGrad and DLG in Figure 15.\nFor InvertGrad, we use the Adam optimizer and set the number of iterations to 242424k.\nFor DLG, we use L-BFGS optimizer and set the number of iterations to 300300300.\nWe use the original implementations provided by [73, 17].Neural Network Architectures  \nIn this experiment, we study the impact of different neural network architectures on the attack scheme.\nWe choose the LeNet, VGG-7, and ResNet-18 and demonstrate the reconstruction results in Figure 16.\nLeNet is a five-layer convolutional network with 5×5555\\times 5 kernels, and VGG-7 is a seven-layer convolutional network with 3×3333\\times 3 kernels.\nResNet-18 is an eighteen-layer residual network with skip connections.\nIntuitively, the increased number of nodes in a neural network appears to give the adversary more advantages, as the number of known conditions will also increase.\nOn the other hand, more sophisticated neural network architecture will have an increased nonlinearity, thus impeding a successful reconstruction.\nIn Figure 16, we can observe that all reconstructed images under different neural network architectures are visually similar.\nOur observation is consistent with the prior study that changing the neural network architecture may not directly affect the privacy protection level.\nA more extensive study concerning neural network architectures is warranted in future work.",
        "references": [
            [
                "In this section, we present a variant of the ROG attack by using a different postprocessing module.\nWe propose to reconstruct a good-quality image at a ",
                "s",
                "emantic level (ROGS).\nInstead of focusing on the pixel-level error between the original image and the reconstructed counterpart, ROGS leverages a pretrained generative model ",
                "G",
                "𝐺",
                "G",
                ", such as BigGAN ",
                "[",
                "11",
                "]",
                ", to synthesize realistic images while maintaining the semantics of the original image.\nThe generative model ",
                "G",
                "𝐺",
                "G",
                " maps a latent vector ",
                "𝐳",
                "𝐳",
                "\\mathbf{z}",
                " to the image space, and the output has a similar distribution to the real images.\nWe search the latent space of the generative model ",
                "G",
                "𝐺",
                "G",
                " and restrict the similarity between the generated image and the original ROG lower-quality reconstruction.\nThe goal of ROGS attack is to reconstruct an image sharing the same knowledge and semantics as in the original private image.\nImage reconstruction via the ROGS attack differs from the notion of property inference attack ",
                "[",
                "44",
                "]",
                " that target sensitive information.\nSensitive information is generally difficult to define and corresponding privacy relies on personal preference ",
                "[",
                "48",
                "]",
                ".\nCompared to property inference, such as recovering gender, age, or race, ROGS may be viewed as a more general attack that diversifies the concept of compromising privacy.\nIt has the capability to reveal privacy even if some sensitive information is not present in the corpus.",
                "From the technical perspective, we let ROGS invert an input to the GAN latent space, which is also known as the GAN inversion task in the literature ",
                "[",
                "30",
                ", ",
                "64",
                "]",
                ".\nThe optimization problem can be formulated as",
                "where ",
                "ℒ",
                "ℒ",
                "\\mathcal{L}",
                " is a loss function quantifying the distance between two images, ",
                "𝐳",
                "𝐳",
                "\\mathbf{z}",
                " denotes a latent vector, and ",
                "𝐗",
                "^",
                "^",
                "𝐗",
                "\\hat{\\mathbf{X}}",
                " is the original ROG output.\nThe ROGS attack will take the latent vector ",
                "𝐳",
                "m",
                ",",
                "i",
                "⋆",
                "superscript",
                "subscript",
                "𝐳",
                "𝑚",
                "𝑖",
                "⋆",
                "\\mathbf{z}_{m,i}^{\\star}",
                " as the input and output ",
                "G",
                "​",
                "(",
                "𝐳",
                "m",
                ",",
                "i",
                "⋆",
                ")",
                "𝐺",
                "superscript",
                "subscript",
                "𝐳",
                "𝑚",
                "𝑖",
                "⋆",
                "G(\\mathbf{z}_{m,i}^{\\star})",
                " as the attack result.",
                "In our implementation, we use a combination of the ",
                "ℓ",
                "2",
                "subscript",
                "ℓ",
                "2",
                "\\ell_{2}",
                " distance and the LPIPS metric, namely,",
                "where ",
                "λ",
                "1",
                "subscript",
                "𝜆",
                "1",
                "\\lambda_{1}",
                " and ",
                "λ",
                "2",
                "subscript",
                "𝜆",
                "2",
                "\\lambda_{2}",
                " are the coefficients balancing the two terms.\nThe loss function encourages the reconstructed image to preserve the structure with the ",
                "ℓ",
                "2",
                "subscript",
                "ℓ",
                "2",
                "\\ell_{2}",
                " distance and the perceptual styles of the original image with the ",
                "LPIPS",
                "LPIPS",
                "\\mathrm{LPIPS}",
                " score.\nWe choose the BigGAN model ",
                "[",
                "11",
                "]",
                " as the generative model ",
                "G",
                "𝐺",
                "G",
                ", which is a class conditional GAN model trained on the ImageNet dataset.\nIn particular, given a latent vector ",
                "𝐳",
                "m",
                ",",
                "i",
                "subscript",
                "𝐳",
                "𝑚",
                "𝑖",
                "\\mathbf{z}_{m,i}",
                " and a class label ",
                "y",
                "m",
                ",",
                "i",
                "subscript",
                "𝑦",
                "𝑚",
                "𝑖",
                "y_{m,i}",
                ", the output of the model is constrained to be within the specific class.\nWe start from a random initialization in the latent space and use the Adam optimizer to solve (",
                "19",
                ").",
                "We use lower-quality images obtained in the ROG attack and show the attack results of ROGS in Figure ",
                "10",
                ".\nIt can be observed that the optimization process begins with a random initialization from the same image class, and gradually changes the foreground and background during the latent space search.\nIn the first row, the raw image shows that a Bernese mountain dog is standing on the grass, facing the camera in the dark.\nThe initialization chooses a dog of the same breed running on the grass in the daytime.\nDuring the optimization, the dog’s posture and orientation are changed smoothly to match the lower-quality input.\nMeanwhile, the background is also gradually changed to the grass in the dark night.\nAlthough the reconstruction is not pixel-wise accurate, privacy leakage can still happen when rich semantics are revealed.\nLikewise, the details of the mailbox on the second row, including the shape, color, and layout, are adapted to match the counterpart in the input.\nThe snow and the road in the background are reconstructed after the optimization.\nIn the third row, the robin’s size, coat color, and posture have been changed to a similar style as the original image after the latent space search, as well as the seasonal information.\nSimilar changes can be observed for the sulphur butterfly picture on the fourth row, where the orientation and size of the butterfly are adjusted during the optimization.",
                "We now discuss how to quantify privacy leakage at the semantic level.\nWe start by raising two concerns on current reconstruction evaluation metrics adopted in the literature based on the reconstruction results in Sections ",
                "4",
                "–",
                "5",
                ".\nFirst, we have observed that the attack success rate suggested by Wei et al. ",
                "[",
                "61",
                "]",
                " may not be a good indicator in some scenarios.\nFor the reconstruction under existing defense schemes, privacy leakage should not be treated as a binary quantity.\nMerely using a threshold may not be a good indicator for a successful attack.\nThe structural information or semantics can be revealed in the reconstructed image, although the results do not perfectly match the raw images.\nThere exists ambiguity in telling whether the attack is successful or not, let alone using an algorithm to automatically calculate the attack success rate.\nSecond, the pixel-level based metrics, including MSE, PSNR, and SSIM, may fail to capture privacy leakage in some scenarios.\nFor example, for the attack against signSGD in Figure ",
                "7",
                ", the average SSIM is ",
                "0.37",
                "0.37",
                "0.37",
                ", which may not be able to fully capture the perceptual similarity due to the color jitter effect.\nA low PSNR value or a high MSE may give misleading conclusions on privacy leakage.",
                "To measure privacy leakage at a semantic level, we use a state-of-the-art multi-label classification network ",
                "[",
                "7",
                "]",
                " to tag the images.\nSpecifically, the classification network has been trained on OpenImage (V6) ",
                "[",
                "38",
                "]",
                ", which contains ",
                "9",
                ",",
                "600",
                "9",
                "600",
                "9,\\!600",
                " classes, including the object category, color, season, etc.\nGiven two images ",
                "𝐗",
                "1",
                "subscript",
                "𝐗",
                "1",
                "\\mathbf{X}_{1}",
                " and ",
                "𝐗",
                "2",
                "subscript",
                "𝐗",
                "2",
                "\\mathbf{X}_{2}",
                ", suppose their detected tags are denoted by set ",
                "A",
                "𝐴",
                "A",
                " and ",
                "B",
                "𝐵",
                "B",
                ", respectively.\nThe Jaccard similarity ",
                "[",
                "29",
                "]",
                " between ",
                "A",
                "𝐴",
                "A",
                " and ",
                "B",
                "𝐵",
                "B",
                " is given as follows:",
                "where ",
                "|",
                "⋅",
                "|",
                "|\\cdot|",
                " denotes the cardinality of the set.\nThe Jaccard similarity ranges from ",
                "0",
                "0",
                " to ",
                "1",
                "1",
                "1",
                " by design."
            ]
        ]
    }
}