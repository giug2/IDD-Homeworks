{
    "PAPER'S NUMBER OF TABLES": 15,
    "S4.T1": {
        "caption": "Table 1: Experiments on two heterogeneity types, four datasets, two heterogeneity levels, and six baselines. Test accuracy (%) averaged over three trials is reported. FedGC consistently and significantly brings performance gain over baselines across diverse settings.",
        "table": "<table id=\"S4.T1.1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.5pt;padding-right:4.5pt;\" rowspan=\"3\"><span id=\"S4.T1.1.1.2.1.1.1\" class=\"ltx_text\">Baseline</span></th>\n<th id=\"S4.T1.1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">H-Type</th>\n<th id=\"S4.T1.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-left:4.5pt;padding-right:4.5pt;\" colspan=\"4\">Label Level</th>\n<th id=\"S4.T1.1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-left:4.5pt;padding-right:4.5pt;\" colspan=\"4\">Feature Level</th>\n<th id=\"S4.T1.1.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.2.1.5.1\" class=\"ltx_text ltx_font_bold\">Avg.</span></th>\n</tr>\n<tr id=\"S4.T1.1.1.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">Dataset</th>\n<th id=\"S4.T1.1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.5pt;padding-right:4.5pt;\" colspan=\"2\">CIFAR-10</th>\n<th id=\"S4.T1.1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\" colspan=\"2\">EuroSAT</th>\n<th id=\"S4.T1.1.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.5pt;padding-right:4.5pt;\" colspan=\"2\">PACS</th>\n<th id=\"S4.T1.1.1.3.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\" colspan=\"2\">VLCS</th>\n<th id=\"S4.T1.1.1.3.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.3.2.6.1\" class=\"ltx_text ltx_font_bold\">Acc.</span></th>\n</tr>\n<tr id=\"S4.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">H-Level</th>\n<th id=\"S4.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">High</th>\n<th id=\"S4.T1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">Low</th>\n<th id=\"S4.T1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">High</th>\n<th id=\"S4.T1.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">Low</th>\n<th id=\"S4.T1.1.1.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">High</th>\n<th id=\"S4.T1.1.1.1.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">Low</th>\n<th id=\"S4.T1.1.1.1.9\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">High</th>\n<th id=\"S4.T1.1.1.1.10\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">Low</th>\n<th id=\"S4.T1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><math id=\"S4.T1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\bm{\\Delta}\" display=\"inline\"><semantics id=\"S4.T1.1.1.1.1.m1.1a\"><mi id=\"S4.T1.1.1.1.1.m1.1.1\" xref=\"S4.T1.1.1.1.1.m1.1.1.cmml\">ğš«</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.1.1.1.1.m1.1b\"><ci id=\"S4.T1.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T1.1.1.1.1.m1.1.1\">ğš«</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.1.1.1.1.m1.1c\">\\bm{\\Delta}</annotation></semantics></math></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.1.1.4.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.1.4.1.1\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"></td>\n<td id=\"S4.T1.1.1.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">Vanilla</td>\n<td id=\"S4.T1.1.1.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">61.25</td>\n<td id=\"S4.T1.1.1.4.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">75.88</td>\n<td id=\"S4.T1.1.1.4.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">53.82</td>\n<td id=\"S4.T1.1.1.4.1.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">75.59</td>\n<td id=\"S4.T1.1.1.4.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">27.16</td>\n<td id=\"S4.T1.1.1.4.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">36.47</td>\n<td id=\"S4.T1.1.1.4.1.9\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">43.69</td>\n<td id=\"S4.T1.1.1.4.1.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">47.95</td>\n<td id=\"S4.T1.1.1.4.1.11\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\" rowspan=\"2\"><span id=\"S4.T1.1.1.4.1.11.1\" class=\"ltx_text ltx_font_bold\">+12.26</span></td>\n</tr>\n<tr id=\"S4.T1.1.1.5.2\" class=\"ltx_tr\" style=\"background-color:#ECECEC;\">\n<td id=\"S4.T1.1.1.5.2.1\" class=\"ltx_td ltx_align_center\" style=\"background-color:#FFFFFF;padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.5.2.1.1\" class=\"ltx_text\" style=\"background-color:#FFFFFF;\">FedAvg</span></td>\n<td id=\"S4.T1.1.1.5.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.5.2.2.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">+ FedGC</span></td>\n<td id=\"S4.T1.1.1.5.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.5.2.3.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">74.50</span></td>\n<td id=\"S4.T1.1.1.5.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.5.2.4.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">79.73</span></td>\n<td id=\"S4.T1.1.1.5.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.5.2.5.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">74.83</span></td>\n<td id=\"S4.T1.1.1.5.2.6\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.5.2.6.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">84.46</span></td>\n<td id=\"S4.T1.1.1.5.2.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.5.2.7.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">54.43</span></td>\n<td id=\"S4.T1.1.1.5.2.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.5.2.8.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">53.93</span></td>\n<td id=\"S4.T1.1.1.5.2.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.5.2.9.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">46.49</span></td>\n<td id=\"S4.T1.1.1.5.2.10\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.5.2.10.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">50.50</span></td>\n</tr>\n<tr id=\"S4.T1.1.1.6.3\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.1.6.3.1\" class=\"ltx_td\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"></td>\n<td id=\"S4.T1.1.1.6.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">Vanilla</td>\n<td id=\"S4.T1.1.1.6.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">60.83</td>\n<td id=\"S4.T1.1.1.6.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">74.40</td>\n<td id=\"S4.T1.1.1.6.3.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">50.91</td>\n<td id=\"S4.T1.1.1.6.3.6\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">72.80</td>\n<td id=\"S4.T1.1.1.6.3.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">28.96</td>\n<td id=\"S4.T1.1.1.6.3.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">34.52</td>\n<td id=\"S4.T1.1.1.6.3.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">46.64</td>\n<td id=\"S4.T1.1.1.6.3.10\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">45.74</td>\n<td id=\"S4.T1.1.1.6.3.11\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\" rowspan=\"2\"><span id=\"S4.T1.1.1.6.3.11.1\" class=\"ltx_text ltx_font_bold\">+13.04</span></td>\n</tr>\n<tr id=\"S4.T1.1.1.7.4\" class=\"ltx_tr\" style=\"background-color:#ECECEC;\">\n<td id=\"S4.T1.1.1.7.4.1\" class=\"ltx_td ltx_align_center\" style=\"background-color:#FFFFFF;padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.7.4.1.1\" class=\"ltx_text\" style=\"background-color:#FFFFFF;\">FedAvgM</span></td>\n<td id=\"S4.T1.1.1.7.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.7.4.2.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">+ FedGC</span></td>\n<td id=\"S4.T1.1.1.7.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.7.4.3.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">73.84</span></td>\n<td id=\"S4.T1.1.1.7.4.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.7.4.4.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">78.90</span></td>\n<td id=\"S4.T1.1.1.7.4.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.7.4.5.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">73.48</span></td>\n<td id=\"S4.T1.1.1.7.4.6\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.7.4.6.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">84.87</span></td>\n<td id=\"S4.T1.1.1.7.4.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.7.4.7.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">53.23</span></td>\n<td id=\"S4.T1.1.1.7.4.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.7.4.8.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">55.73</span></td>\n<td id=\"S4.T1.1.1.7.4.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.7.4.9.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">48.45</span></td>\n<td id=\"S4.T1.1.1.7.4.10\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.7.4.10.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">50.65</span></td>\n</tr>\n<tr id=\"S4.T1.1.1.8.5\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.1.8.5.1\" class=\"ltx_td\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"></td>\n<td id=\"S4.T1.1.1.8.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">Vanilla</td>\n<td id=\"S4.T1.1.1.8.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">64.02</td>\n<td id=\"S4.T1.1.1.8.5.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">75.62</td>\n<td id=\"S4.T1.1.1.8.5.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">59.61</td>\n<td id=\"S4.T1.1.1.8.5.6\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">73.20</td>\n<td id=\"S4.T1.1.1.8.5.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">27.71</td>\n<td id=\"S4.T1.1.1.8.5.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">39.52</td>\n<td id=\"S4.T1.1.1.8.5.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">38.83</td>\n<td id=\"S4.T1.1.1.8.5.10\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">48.50</td>\n<td id=\"S4.T1.1.1.8.5.11\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\" rowspan=\"2\"><span id=\"S4.T1.1.1.8.5.11.1\" class=\"ltx_text ltx_font_bold\">+11.49</span></td>\n</tr>\n<tr id=\"S4.T1.1.1.9.6\" class=\"ltx_tr\" style=\"background-color:#ECECEC;\">\n<td id=\"S4.T1.1.1.9.6.1\" class=\"ltx_td ltx_align_center\" style=\"background-color:#FFFFFF;padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.9.6.1.1\" class=\"ltx_text\" style=\"background-color:#FFFFFF;\">FedProx</span></td>\n<td id=\"S4.T1.1.1.9.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.9.6.2.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">+ FedGC</span></td>\n<td id=\"S4.T1.1.1.9.6.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.9.6.3.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">74.36</span></td>\n<td id=\"S4.T1.1.1.9.6.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.9.6.4.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">79.25</span></td>\n<td id=\"S4.T1.1.1.9.6.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.9.6.5.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">73.04</span></td>\n<td id=\"S4.T1.1.1.9.6.6\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.9.6.6.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">84.76</span></td>\n<td id=\"S4.T1.1.1.9.6.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.9.6.7.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">54.28</span></td>\n<td id=\"S4.T1.1.1.9.6.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.9.6.8.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">55.83</span></td>\n<td id=\"S4.T1.1.1.9.6.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.9.6.9.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">45.69</span></td>\n<td id=\"S4.T1.1.1.9.6.10\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.9.6.10.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">51.70</span></td>\n</tr>\n<tr id=\"S4.T1.1.1.10.7\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.1.10.7.1\" class=\"ltx_td\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"></td>\n<td id=\"S4.T1.1.1.10.7.2\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">Vanilla</td>\n<td id=\"S4.T1.1.1.10.7.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">63.98</td>\n<td id=\"S4.T1.1.1.10.7.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">78.79</td>\n<td id=\"S4.T1.1.1.10.7.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">52.72</td>\n<td id=\"S4.T1.1.1.10.7.6\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">76.80</td>\n<td id=\"S4.T1.1.1.10.7.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">29.72</td>\n<td id=\"S4.T1.1.1.10.7.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">37.52</td>\n<td id=\"S4.T1.1.1.10.7.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">43.64</td>\n<td id=\"S4.T1.1.1.10.7.10\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">40.83</td>\n<td id=\"S4.T1.1.1.10.7.11\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\" rowspan=\"2\"><span id=\"S4.T1.1.1.10.7.11.1\" class=\"ltx_text ltx_font_bold\">+12.57</span></td>\n</tr>\n<tr id=\"S4.T1.1.1.11.8\" class=\"ltx_tr\" style=\"background-color:#ECECEC;\">\n<td id=\"S4.T1.1.1.11.8.1\" class=\"ltx_td ltx_align_center\" style=\"background-color:#FFFFFF;padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.11.8.1.1\" class=\"ltx_text\" style=\"background-color:#FFFFFF;\">SCAFFOLD</span></td>\n<td id=\"S4.T1.1.1.11.8.2\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.11.8.2.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">+ FedGC</span></td>\n<td id=\"S4.T1.1.1.11.8.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.11.8.3.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">73.96</span></td>\n<td id=\"S4.T1.1.1.11.8.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.11.8.4.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">80.29</span></td>\n<td id=\"S4.T1.1.1.11.8.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.11.8.5.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">69.48</span></td>\n<td id=\"S4.T1.1.1.11.8.6\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.11.8.6.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">81.04</span></td>\n<td id=\"S4.T1.1.1.11.8.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.11.8.7.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">59.73</span></td>\n<td id=\"S4.T1.1.1.11.8.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.11.8.8.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">60.63</span></td>\n<td id=\"S4.T1.1.1.11.8.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.11.8.9.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">47.65</span></td>\n<td id=\"S4.T1.1.1.11.8.10\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.11.8.10.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">51.75</span></td>\n</tr>\n<tr id=\"S4.T1.1.1.12.9\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.1.12.9.1\" class=\"ltx_td\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"></td>\n<td id=\"S4.T1.1.1.12.9.2\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">Vanilla</td>\n<td id=\"S4.T1.1.1.12.9.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">63.40</td>\n<td id=\"S4.T1.1.1.12.9.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">75.43</td>\n<td id=\"S4.T1.1.1.12.9.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">52.67</td>\n<td id=\"S4.T1.1.1.12.9.6\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">70.02</td>\n<td id=\"S4.T1.1.1.12.9.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">27.91</td>\n<td id=\"S4.T1.1.1.12.9.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">36.52</td>\n<td id=\"S4.T1.1.1.12.9.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">45.89</td>\n<td id=\"S4.T1.1.1.12.9.10\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">48.30</td>\n<td id=\"S4.T1.1.1.12.9.11\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\" rowspan=\"2\"><span id=\"S4.T1.1.1.12.9.11.1\" class=\"ltx_text ltx_font_bold\">+12.47</span></td>\n</tr>\n<tr id=\"S4.T1.1.1.13.10\" class=\"ltx_tr\" style=\"background-color:#ECECEC;\">\n<td id=\"S4.T1.1.1.13.10.1\" class=\"ltx_td ltx_align_center\" style=\"background-color:#FFFFFF;padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.13.10.1.1\" class=\"ltx_text\" style=\"background-color:#FFFFFF;\">MOON</span></td>\n<td id=\"S4.T1.1.1.13.10.2\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.13.10.2.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">+ FedGC</span></td>\n<td id=\"S4.T1.1.1.13.10.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.13.10.3.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">74.02</span></td>\n<td id=\"S4.T1.1.1.13.10.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.13.10.4.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">79.82</span></td>\n<td id=\"S4.T1.1.1.13.10.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.13.10.5.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">73.69</span></td>\n<td id=\"S4.T1.1.1.13.10.6\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.13.10.6.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">86.06</span></td>\n<td id=\"S4.T1.1.1.13.10.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.13.10.7.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">53.81</span></td>\n<td id=\"S4.T1.1.1.13.10.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.13.10.8.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">55.08</span></td>\n<td id=\"S4.T1.1.1.13.10.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.13.10.9.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">48.05</span></td>\n<td id=\"S4.T1.1.1.13.10.10\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.13.10.10.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">49.35</span></td>\n</tr>\n<tr id=\"S4.T1.1.1.14.11\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.1.14.11.1\" class=\"ltx_td\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"></td>\n<td id=\"S4.T1.1.1.14.11.2\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">Vanilla</td>\n<td id=\"S4.T1.1.1.14.11.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">64.14</td>\n<td id=\"S4.T1.1.1.14.11.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">76.19</td>\n<td id=\"S4.T1.1.1.14.11.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">63.74</td>\n<td id=\"S4.T1.1.1.14.11.6\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">69.57</td>\n<td id=\"S4.T1.1.1.14.11.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">27.51</td>\n<td id=\"S4.T1.1.1.14.11.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">29.07</td>\n<td id=\"S4.T1.1.1.14.11.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">37.02</td>\n<td id=\"S4.T1.1.1.14.11.10\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">47.70</td>\n<td id=\"S4.T1.1.1.14.11.11\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.5pt;padding-right:4.5pt;\" rowspan=\"2\"><span id=\"S4.T1.1.1.14.11.11.1\" class=\"ltx_text ltx_font_bold\">+9.62</span></td>\n</tr>\n<tr id=\"S4.T1.1.1.15.12\" class=\"ltx_tr\" style=\"background-color:#ECECEC;\">\n<td id=\"S4.T1.1.1.15.12.1\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"background-color:#FFFFFF;padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.15.12.1.1\" class=\"ltx_text\" style=\"background-color:#FFFFFF;\">FedDecorr</span></td>\n<td id=\"S4.T1.1.1.15.12.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.15.12.2.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">+ FedGC</span></td>\n<td id=\"S4.T1.1.1.15.12.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.15.12.3.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">73.94</span></td>\n<td id=\"S4.T1.1.1.15.12.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.15.12.4.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">78.16</span></td>\n<td id=\"S4.T1.1.1.15.12.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.15.12.5.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">69.93</span></td>\n<td id=\"S4.T1.1.1.15.12.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.15.12.6.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">81.30</span></td>\n<td id=\"S4.T1.1.1.15.12.7\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.15.12.7.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">48.77</span></td>\n<td id=\"S4.T1.1.1.15.12.8\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.15.12.8.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">47.42</span></td>\n<td id=\"S4.T1.1.1.15.12.9\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.15.12.9.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">43.39</span></td>\n<td id=\"S4.T1.1.1.15.12.10\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"S4.T1.1.1.15.12.10.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">49.00</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "FedGC significantly improves the FL performance under data heterogeneity. In TableÂ 1, we show experimental results on two heterogeneity types (label-level and feature-level heterogeneity), two datasets for each type (CIFAR-10, EuroSAT, PACS, and VLCS), and two heterogeneity levels for each dataset.\nFrom the table, we see that (1) incorporating baseline in our FedGC framework can consistently and significantly improve the performance of baseline across diverse settings.\n(2) FedGC is extremely helpful when the heterogeneity level is relatively high, convincingly supporting our idea of introducing generative data to mitigate the effects of data heterogeneity.\nSpecifically, based on FedAvg, FedGC brings 21.01 absolute accuracy improvement under a high heterogeneity level on EuroSAT and 12.26 absolute accuracy improvement on average.",
            "FedGC is compatible with existing FL methods. From TableÂ 1, we see that FedGC consistently and significantly brings performance gain across 6 different baselines, including FedAvg, FedAvgM, FedProx, SCAFFOLD, MOON, and FedDecorr.\nFor example, FedGC averagely brings 12.68 absolute accuracy improvement to SCAFFOLDÂ (Karimireddy etÂ al., 2020).\nThis demonstrates the compatibility and universality of our proposed FedGC framework.",
            "FedGC achieves better performance and privacy preservation at the same time. In FigureÂ 2, we show the performance and privacy preservation trade-off comparisons before and after using FedGC.\nTo measure privacy preservation, we use a simple membership inference attack method based on loss evaluationÂ (Yu etÂ al., 2021; Sablayrolles etÂ al., 2019) to evaluate attack accuracy, see details in SectionÂ A.3.\nLower attack accuracy indicates better privacy preservation.\nFrom the figure, we have an interesting finding that our FedGC framework can not only improve the performance under data heterogeneity, but also enhance the privacy preservation.\nWe also show in TableÂ 10 that FedGC achieves significantly lower attack accuracy when similar task accuracy is achieved.\nThis is surprising yet reasonable since FedGC requires the model to learn from both the private data and the diverse generative data, meaning that the generative data can dilute the concentration of real, sensitive data.",
            "We also compare the attack accuracy at the point when FedAvg and FedGC achieve similar task accuracy in TableÂ 10.\nFrom the table, we see a much more significant reduction in privacy leakage (i.e., much lower attack accuracy).\nThis is reasonable as FedGC can accelerate the convergence speed, which means FedGC requires fewer steps of optimization on the sensitive private data to achieve the same.",
            "Our proposed FedGC framework is also applicable in cases where not every client has the capability to generate data. Here, we experiment on CIFAR-10 under two different heterogeneity levels. In TableÂ 11, we compare vanilla baseline with no generative data, FedGC where all clients can generate data, and FedGC where only half of the clients can generate data.",
            "Here, we conduct experiments of three baselines including FedAvg, FedProx, and SCAFFOLD on CIFAR-10 with Dirichlet distribution parameter Î²ğ›½\\beta = 0.1. Specifically, we set the communication round to 200, local iteration number to 100, and try different client number and participation rate. As illustrated in Table 12, we can observe that FedGC still significantly outperforms the baseline with no generated data under each circumstance.",
            "Here, we perform experiments on EuroSAT dataset with two heterogeneity levels in TableÂ 13.\nVanilla denotes FedAvg itself, No F denotes FedGC without filtering, F@50 denotes filtering from round 50, F@50-C denotes category-wise filtering.\nFrom the table, we see that (1) under a high heterogeneity level, F@75 contributes to higher performance than No F, even with only 90% of data at final rounds.\n(2) Category-wise filtering generally performs better than unified filtering, indicating its effectiveness.\n(3) Nevertheless, such filtering technique can not always ensure performance improvement, calling for more future work.\nThe performance drop could result from reduced number of data samples and ineffective filtering."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Increasing number of generated samples makes FedAvgÂ (McMahan etÂ al., 2017) prevail.",
        "table": "<table id=\"S4.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">No. Gen.</th>\n<th id=\"S4.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">0</th>\n<th id=\"S4.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">100</th>\n<th id=\"S4.T2.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">200</th>\n<th id=\"S4.T2.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">500</th>\n<th id=\"S4.T2.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">1000</th>\n<th id=\"S4.T2.1.1.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">2000</th>\n<th id=\"S4.T2.1.1.1.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">5000</th>\n<th id=\"S4.T2.1.1.1.9\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">10000</th>\n<th id=\"S4.T2.1.1.1.10\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">20000</th>\n<th id=\"S4.T2.1.1.1.11\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">50000</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.1.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">FedAvg</th>\n<td id=\"S4.T2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">61.25</td>\n<td id=\"S4.T2.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">63.67</td>\n<td id=\"S4.T2.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">66.21</td>\n<td id=\"S4.T2.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">67.13</td>\n<td id=\"S4.T2.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">66.98</td>\n<td id=\"S4.T2.1.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">66.28</td>\n<td id=\"S4.T2.1.2.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">71.65</td>\n<td id=\"S4.T2.1.2.1.9\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"background-color:#EBEBFF;padding-left:5.0pt;padding-right:5.0pt;\"><span id=\"S4.T2.1.2.1.9.1\" class=\"ltx_text ltx_font_bold\" style=\"background-color:#EBEBFF;\">74.50</span></td>\n<td id=\"S4.T2.1.2.1.10\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"background-color:#EBEBFF;padding-left:5.0pt;padding-right:5.0pt;\"><span id=\"S4.T2.1.2.1.10.1\" class=\"ltx_text ltx_font_bold\" style=\"background-color:#EBEBFF;\">76.93</span></td>\n<td id=\"S4.T2.1.2.1.11\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"background-color:#EBEBFF;padding-left:5.0pt;padding-right:5.0pt;\"><span id=\"S4.T2.1.2.1.11.1\" class=\"ltx_text ltx_font_bold\" style=\"background-color:#EBEBFF;\">76.39</span></td>\n</tr>\n<tr id=\"S4.T2.1.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">FedProx</th>\n<td id=\"S4.T2.1.3.2.2\" class=\"ltx_td ltx_align_center\" style=\"background-color:#EBEBFF;padding-left:5.0pt;padding-right:5.0pt;\"><span id=\"S4.T2.1.3.2.2.1\" class=\"ltx_text ltx_font_bold\" style=\"background-color:#EBEBFF;\">64.02</span></td>\n<td id=\"S4.T2.1.3.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">66.47</td>\n<td id=\"S4.T2.1.3.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">67.40</td>\n<td id=\"S4.T2.1.3.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">67.05</td>\n<td id=\"S4.T2.1.3.2.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">68.55</td>\n<td id=\"S4.T2.1.3.2.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">69.19</td>\n<td id=\"S4.T2.1.3.2.8\" class=\"ltx_td ltx_align_center\" style=\"background-color:#EBEBFF;padding-left:5.0pt;padding-right:5.0pt;\"><span id=\"S4.T2.1.3.2.8.1\" class=\"ltx_text ltx_font_bold\" style=\"background-color:#EBEBFF;\">72.10</span></td>\n<td id=\"S4.T2.1.3.2.9\" class=\"ltx_td ltx_align_center\" style=\"background-color:#EBEBFF;padding-left:5.0pt;padding-right:5.0pt;\"><span id=\"S4.T2.1.3.2.9.1\" class=\"ltx_text ltx_font_bold\" style=\"background-color:#EBEBFF;\">74.36</span></td>\n<td id=\"S4.T2.1.3.2.10\" class=\"ltx_td ltx_align_center\" style=\"background-color:#EBEBFF;padding-left:5.0pt;padding-right:5.0pt;\"><span id=\"S4.T2.1.3.2.10.1\" class=\"ltx_text ltx_font_bold\" style=\"background-color:#EBEBFF;\">76.81</span></td>\n<td id=\"S4.T2.1.3.2.11\" class=\"ltx_td ltx_align_center\" style=\"background-color:#EBEBFF;padding-left:5.0pt;padding-right:5.0pt;\"><span id=\"S4.T2.1.3.2.11.1\" class=\"ltx_text ltx_font_bold\" style=\"background-color:#EBEBFF;\">76.73</span></td>\n</tr>\n<tr id=\"S4.T2.1.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">SCAFFOLD</th>\n<td id=\"S4.T2.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"background-color:#EBEBFF;padding-left:5.0pt;padding-right:5.0pt;\"><span id=\"S4.T2.1.4.3.2.1\" class=\"ltx_text ltx_font_bold\" style=\"background-color:#EBEBFF;\">63.98</span></td>\n<td id=\"S4.T2.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"background-color:#EBEBFF;padding-left:5.0pt;padding-right:5.0pt;\"><span id=\"S4.T2.1.4.3.3.1\" class=\"ltx_text ltx_font_bold\" style=\"background-color:#EBEBFF;\">69.05</span></td>\n<td id=\"S4.T2.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"background-color:#EBEBFF;padding-left:5.0pt;padding-right:5.0pt;\"><span id=\"S4.T2.1.4.3.4.1\" class=\"ltx_text ltx_font_bold\" style=\"background-color:#EBEBFF;\">71.33</span></td>\n<td id=\"S4.T2.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"background-color:#EBEBFF;padding-left:5.0pt;padding-right:5.0pt;\"><span id=\"S4.T2.1.4.3.5.1\" class=\"ltx_text ltx_font_bold\" style=\"background-color:#EBEBFF;\">71.55</span></td>\n<td id=\"S4.T2.1.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"background-color:#EBEBFF;padding-left:5.0pt;padding-right:5.0pt;\"><span id=\"S4.T2.1.4.3.6.1\" class=\"ltx_text ltx_font_bold\" style=\"background-color:#EBEBFF;\">71.33</span></td>\n<td id=\"S4.T2.1.4.3.7\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"background-color:#EBEBFF;padding-left:5.0pt;padding-right:5.0pt;\"><span id=\"S4.T2.1.4.3.7.1\" class=\"ltx_text ltx_font_bold\" style=\"background-color:#EBEBFF;\">70.04</span></td>\n<td id=\"S4.T2.1.4.3.8\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">70.34</td>\n<td id=\"S4.T2.1.4.3.9\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">73.96</td>\n<td id=\"S4.T2.1.4.3.10\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">74.88</td>\n<td id=\"S4.T2.1.4.3.11\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">73.98</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Generating more data could make FedAvg prevail. In TableÂ 2, we explore the effects of number of generated samples on FLâ€™s performance. 0 denotes vanilla FL baseline. Experiments are conducted on CIFAR-10 (Î²=0.05ğ›½0.05\\beta=0.05). From the table, we have an interesting finding: (1) when the number of generated samples is relatively small (0âˆ¼similar-to\\sim2000), FedGC can enlarge the gap between standard FedAvg and the method (SCAFFOLD) that is specifically designed for addressing data heterogeneity; (2) however, as the number continues to grow, the situation is reversed that the basic FL method FedAvg prevails. This finding suggests that apart from carefully designing FL algorithm, it is also a promising direction to explore the greater potential from the perspective of generative data."
        ]
    },
    "S4.SS3.2": {
        "caption": "",
        "table": "<table id=\"S4.SS3.1.fig1.1\" class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.SS3.1.fig1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.SS3.1.fig1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.3pt;padding-right:3.3pt;\">Baseline</th>\n<th id=\"S4.SS3.1.fig1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.3pt;padding-right:3.3pt;\">No-GC</th>\n<th id=\"S4.SS3.1.fig1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.3pt;padding-right:3.3pt;\">Single</th>\n<th id=\"S4.SS3.1.fig1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"background-color:#ECECEC;padding-left:3.3pt;padding-right:3.3pt;\"><span id=\"S4.SS3.1.fig1.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\" style=\"background-color:#ECECEC;\">Multiple</span></th>\n<th id=\"S4.SS3.1.fig1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.3pt;padding-right:3.3pt;\">LLM</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.SS3.1.fig1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S4.SS3.1.fig1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.3pt;padding-right:3.3pt;\">FedAvg</td>\n<td id=\"S4.SS3.1.fig1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.3pt;padding-right:3.3pt;\">27.06</td>\n<td id=\"S4.SS3.1.fig1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.3pt;padding-right:3.3pt;\">50.53</td>\n<td id=\"S4.SS3.1.fig1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"background-color:#ECECEC;padding-left:3.3pt;padding-right:3.3pt;\"><span id=\"S4.SS3.1.fig1.1.2.1.4.1\" class=\"ltx_text ltx_font_bold\" style=\"background-color:#ECECEC;\">54.08</span></td>\n<td id=\"S4.SS3.1.fig1.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.3pt;padding-right:3.3pt;\">41.32</td>\n</tr>\n<tr id=\"S4.SS3.1.fig1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S4.SS3.1.fig1.1.3.2.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.3pt;padding-right:3.3pt;\">FedProx</td>\n<td id=\"S4.SS3.1.fig1.1.3.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.3pt;padding-right:3.3pt;\">29.12</td>\n<td id=\"S4.SS3.1.fig1.1.3.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.3pt;padding-right:3.3pt;\">50.48</td>\n<td id=\"S4.SS3.1.fig1.1.3.2.4\" class=\"ltx_td ltx_align_center\" style=\"background-color:#ECECEC;padding-left:3.3pt;padding-right:3.3pt;\"><span id=\"S4.SS3.1.fig1.1.3.2.4.1\" class=\"ltx_text ltx_font_bold\" style=\"background-color:#ECECEC;\">53.03</span></td>\n<td id=\"S4.SS3.1.fig1.1.3.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.3pt;padding-right:3.3pt;\">40.82</td>\n</tr>\n<tr id=\"S4.SS3.1.fig1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S4.SS3.1.fig1.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.3pt;padding-right:3.3pt;\">SCAFFOLD</td>\n<td id=\"S4.SS3.1.fig1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.3pt;padding-right:3.3pt;\">28.56</td>\n<td id=\"S4.SS3.1.fig1.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.3pt;padding-right:3.3pt;\">54.13</td>\n<td id=\"S4.SS3.1.fig1.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"background-color:#ECECEC;padding-left:3.3pt;padding-right:3.3pt;\"><span id=\"S4.SS3.1.fig1.1.4.3.4.1\" class=\"ltx_text ltx_font_bold\" style=\"background-color:#ECECEC;\">58.53</span></td>\n<td id=\"S4.SS3.1.fig1.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.3pt;padding-right:3.3pt;\">45.87</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Federated learning (FL) enables leveraging distributed private data for model training in a privacy-preserving way.\nHowever, data heterogeneity significantly limits the performance of current FL methods.\nIn this paper, we propose a novel FL framework termed FedGC, designed to mitigate data heterogeneity issues by diversifying private data with generative content.\nFedGC is a simple-to-implement framework as it only introduces a one-shot step of data generation.\nIn data generation, we summarize three crucial and worth-exploring aspects (budget allocation, prompt design, and generation guidance) and propose three solution candidates for each aspect.\nSpecifically, to achieve a better trade-off between data diversity and fidelity for generation guidance, we propose to generate data based on the guidance of prompts and real data simultaneously.\nThe generated data is then merged with private data to facilitate local model training.\nSuch generative data increases the diversity of private data to prevent each client from fitting the potentially biased private data, alleviating the issue of data heterogeneity.\nWe conduct a systematic empirical study on FedGC, covering diverse baselines, datasets, scenarios, and modalities.\nInteresting findings include (1) FedGC consistently and significantly enhances the performance of FL methods, even when notable disparities exist between generative and private data;\n(2) FedGC achieves both better performance and privacy-preservation.\nWe wish this work can inspire future works to further explore the potential of enhancing FL with generative content.",
            "For image task, we consider three diversity levels.\n(1) Single prompt, where we use â€œa photo of {class}â€Â (Radford etÂ al., 2021).\n(2) Multiple prompts, where we consider diverse formats such as â€œ{class}â€.\n(3) LLM-based diversified prompts, where we instruct an LLM such as ChatGPT to diversify the prompts.\nWhile for text generation, we only design one prompt since the ChatGPTÂ (OpenAI, 2023) is sufficient to generate diverse content if we instruct it to be diverse; see TableÂ 8.",
            "(Real-Data Guidance) To alleviate this issue, we propose a new real-data-guided generation approach, which conditions data generation on both real data and prompts.\nFor image task, unlike the original text-guided generation that starts from a random Gaussian noise at latent space ğ’›T1subscriptsuperscriptğ’›1ğ‘‡{\\bm{z}}^{1}_{T}Â (Rombach etÂ al., 2022), we propose to inject information of real data into the starting noise.\nSpecifically, we first use the auto-encoder to encode the real image ğ’™ğ’™{\\bm{x}} to latent representation ğ’›ğ’›{\\bm{z}}, then add some Gaussian variation to obtain a new ğ’›T2subscriptsuperscriptğ’›2ğ‘‡{\\bm{z}}^{2}_{T}, which substitutes ğ’›T1subscriptsuperscriptğ’›1ğ‘‡{\\bm{z}}^{1}_{T} as the starting point; see illustration in Â 7.\nThis enriched latent representation, infused with real data insights, enables the generative model to produce outputs closely resembling real data, optimizing the trade-off between diversity and fidelity.\nFor text task, see illustration in TableÂ 8 using ChatGPT.",
            "We set the number of communication rounds as 100. TableÂ 9 lists client number for each dataset.",
            "FedGC significantly improves the FL performance under data heterogeneity. In TableÂ 1, we show experimental results on two heterogeneity types (label-level and feature-level heterogeneity), two datasets for each type (CIFAR-10, EuroSAT, PACS, and VLCS), and two heterogeneity levels for each dataset.\nFrom the table, we see that (1) incorporating baseline in our FedGC framework can consistently and significantly improve the performance of baseline across diverse settings.\n(2) FedGC is extremely helpful when the heterogeneity level is relatively high, convincingly supporting our idea of introducing generative data to mitigate the effects of data heterogeneity.\nSpecifically, based on FedAvg, FedGC brings 21.01 absolute accuracy improvement under a high heterogeneity level on EuroSAT and 12.26 absolute accuracy improvement on average.",
            "FedGC is compatible with existing FL methods. From TableÂ 1, we see that FedGC consistently and significantly brings performance gain across 6 different baselines, including FedAvg, FedAvgM, FedProx, SCAFFOLD, MOON, and FedDecorr.\nFor example, FedGC averagely brings 12.68 absolute accuracy improvement to SCAFFOLDÂ (Karimireddy etÂ al., 2020).\nThis demonstrates the compatibility and universality of our proposed FedGC framework.",
            "FedGC achieves better performance and privacy preservation at the same time. In FigureÂ 2, we show the performance and privacy preservation trade-off comparisons before and after using FedGC.\nTo measure privacy preservation, we use a simple membership inference attack method based on loss evaluationÂ (Yu etÂ al., 2021; Sablayrolles etÂ al., 2019) to evaluate attack accuracy, see details in SectionÂ A.3.\nLower attack accuracy indicates better privacy preservation.\nFrom the figure, we have an interesting finding that our FedGC framework can not only improve the performance under data heterogeneity, but also enhance the privacy preservation.\nWe also show in TableÂ 10 that FedGC achieves significantly lower attack accuracy when similar task accuracy is achieved.\nThis is surprising yet reasonable since FedGC requires the model to learn from both the private data and the diverse generative data, meaning that the generative data can dilute the concentration of real, sensitive data.",
            "Generating more data could make FedAvg prevail. In TableÂ 2, we explore the effects of number of generated samples on FLâ€™s performance. 0 denotes vanilla FL baseline. Experiments are conducted on CIFAR-10 (Î²=0.05ğ›½0.05\\beta=0.05). From the table, we have an interesting finding: (1) when the number of generated samples is relatively small (0âˆ¼similar-to\\sim2000), FedGC can enlarge the gap between standard FedAvg and the method (SCAFFOLD) that is specifically designed for addressing data heterogeneity; (2) however, as the number continues to grow, the situation is reversed that the basic FL method FedAvg prevails. This finding suggests that apart from carefully designing FL algorithm, it is also a promising direction to explore the greater potential from the perspective of generative data.",
            "Equal allocation is a preferred allocation strategy for its effectiveness and simplicity. In TableÂ 5, we compare different budget allocation strategies on CIFAR-10, including equal allocation, inverse allocation, and water-filling-based allocation. Experiments show that equal allocation contributes to better performance for both FedAvg and FedProx, and comparable performance compared with water-filling-based allocation for SCAFFOLD. Considering effectiveness and simplicity, we conclude that equal allocation is a preferred allocation strategy.",
            "Multiple prompts lead to better performance, while LLM-based diversification might be unnecessary.\nIn TableÂ 3, we explore different prompt designs on PACS dataset. PACS contains significant label-level and feature-level variations, making it an apt choice for this exploration.\nWe compare baseline without FedGC, FedGC with single, multiple, and LLM-based prompts (see prompt generation in TableÂ 7).\nFrom the table, (1) we see that FedGC incorporated with all the prompt designs improves the performance of baselines (see improvement over the No-GC column).\n(2) We see that multiple prompts consistently and significantly perform better, while LLM-based prompts perform ordinarily.\nThis may result from the fact that the scene descriptions from the LLM are usually complicated, causing multifaceted patterns in one sample, thereby complicating model training.\nOverall, we prefer the design of multiple prompts for its effectiveness, diversity, and simplicity.",
            "Mixed guidance contributes to higher performance for rare tasks.\nIn TableÂ 4, we compare different generation guidance designs on a medical dataset HAM10000Â (Tschandl etÂ al., 2018).\nThe reason for choosing this dataset is that the diffusion modelÂ (Rombach etÂ al., 2022) fails to correctly understand medical promptsÂ (Kazerouni etÂ al., 2022), which helps support our claim more convincingly. We consider three designs, including text-guided generation (T2I), our proposed data generation with guidance of text and real data (IT2I), and the mixed usage of T2I and IT2I. These experiments convey three interesting findings: (1) even though the diffusion model fails to generate data that visually agrees with real data, the generated data still contributes to enhancing the performance of FL (see improvement from Pri. to T2I). (2) IT2I itself fails to bring performance gain, which may result from the limited diversity and incapability to generate for missing classes. (3) Mixing these two strategies contributes to consistently and significantly better performance.",
            "Mixed training is the most effective training strategy. In TableÂ 6, we compare different training strategies on CIFAR-10, including training only on the private dataset (Pri.), training only on the generative dataset (Gen.), sequential training with private dataset first (P2G), sequential training with generative dataset first (G2P), and mixed training. Experiments show that 1) generative data itself fails to ensure training, indicating that there is a gap between generative data and real private data. 2) However, when using generative data together with real private data, we see consistent performance gain compared to training on private data. This indicates that despite the incapability of fully representing real data, the generative data still contributes to improving training by increasing diversity. 3) Mixed training consistently and significantly achieves better performance.",
            "For the prompts conditioned on the latent diffusion model, we show the LLM-based prompts for generating images in TableÂ 7.\nIn detail, we instruct ChatGPT through System Prompt and User Prompt, to help us create text samples containing the corresponding class name for image generation. Utilizing ChatGPTâ€™s rich imagination of scenarios and the diversity of text styles, we can achieve a diversity of prompts. Therefore, it helps Stable-diffusion to generate diverse and more realistic pictures.",
            "We show the real-data-guidance for text generation using ChatGPT in TableÂ 8.\nCompared to prompts containing class num, here we instruct ChatGPT to imitate the theme and content of the corresponding text and directly expand the amount of text data.\nIn our illustrative examples shown in TableÂ 8, we simulate real-world data scenarios by incorporating four actual instances and generating an additional set of four synthetic instances. In this experimental setup, we task ChatGPT with the generation of data that exhibits diverse patterns akin to those found in authentic real data. Furthermore, we guide ChatGPT to produce two distinct samples for each distinct label category, fostering a balanced and representative dataset.",
            "We list the number of clients for each dataset in TableÂ 9.",
            "We also compare the attack accuracy at the point when FedAvg and FedGC achieve similar task accuracy in TableÂ 10.\nFrom the table, we see a much more significant reduction in privacy leakage (i.e., much lower attack accuracy).\nThis is reasonable as FedGC can accelerate the convergence speed, which means FedGC requires fewer steps of optimization on the sensitive private data to achieve the same.",
            "Our proposed FedGC framework is also applicable in cases where not every client has the capability to generate data. Here, we experiment on CIFAR-10 under two different heterogeneity levels. In TableÂ 11, we compare vanilla baseline with no generative data, FedGC where all clients can generate data, and FedGC where only half of the clients can generate data.",
            "From the table, we see that\n(1) our proposed FedGC can consistently and significantly achieve the best performance despite the amount of generation-capable clients.\n(2) Surprisingly, we find that under low heterogeneity level, when applied to SCAFFOLDÂ (Karimireddy etÂ al., 2020), FedGC with few generation-capable clients even performs better.\nThis interesting finding demonstrates that our framework may be further improved by more fine-grained designs regarding who is responsible for data generation and the volume of data to be generated.",
            "Here, we conduct experiments of three baselines including FedAvg, FedProx, and SCAFFOLD on CIFAR-10 with Dirichlet distribution parameter Î²ğ›½\\beta = 0.1. Specifically, we set the communication round to 200, local iteration number to 100, and try different client number and participation rate. As illustrated in Table 12, we can observe that FedGC still significantly outperforms the baseline with no generated data under each circumstance.",
            "Here, we perform experiments on EuroSAT dataset with two heterogeneity levels in TableÂ 13.\nVanilla denotes FedAvg itself, No F denotes FedGC without filtering, F@50 denotes filtering from round 50, F@50-C denotes category-wise filtering.\nFrom the table, we see that (1) under a high heterogeneity level, F@75 contributes to higher performance than No F, even with only 90% of data at final rounds.\n(2) Category-wise filtering generally performs better than unified filtering, indicating its effectiveness.\n(3) Nevertheless, such filtering technique can not always ensure performance improvement, calling for more future work.\nThe performance drop could result from reduced number of data samples and ineffective filtering."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Different prompt designs of FedGC applied on baselines. The design of multiple prompt formats is preferred for its effectiveness, diversity, and simplicity.",
        "table": "",
        "footnotes": "",
        "references": [
            "Multiple prompts lead to better performance, while LLM-based diversification might be unnecessary.\nIn TableÂ 3, we explore different prompt designs on PACS dataset. PACS contains significant label-level and feature-level variations, making it an apt choice for this exploration.\nWe compare baseline without FedGC, FedGC with single, multiple, and LLM-based prompts (see prompt generation in TableÂ 7).\nFrom the table, (1) we see that FedGC incorporated with all the prompt designs improves the performance of baselines (see improvement over the No-GC column).\n(2) We see that multiple prompts consistently and significantly perform better, while LLM-based prompts perform ordinarily.\nThis may result from the fact that the scene descriptions from the LLM are usually complicated, causing multifaceted patterns in one sample, thereby complicating model training.\nOverall, we prefer the design of multiple prompts for its effectiveness, diversity, and simplicity."
        ]
    },
    "S4.T4": {
        "caption": "Table 4: Different generation guidance designs of FedGC applied on baselines. The mixed guidance that combines text2img and img&text2img is the most effective strategy.",
        "table": "",
        "footnotes": "",
        "references": [
            "Mixed guidance contributes to higher performance for rare tasks.\nIn TableÂ 4, we compare different generation guidance designs on a medical dataset HAM10000Â (Tschandl etÂ al., 2018).\nThe reason for choosing this dataset is that the diffusion modelÂ (Rombach etÂ al., 2022) fails to correctly understand medical promptsÂ (Kazerouni etÂ al., 2022), which helps support our claim more convincingly. We consider three designs, including text-guided generation (T2I), our proposed data generation with guidance of text and real data (IT2I), and the mixed usage of T2I and IT2I. These experiments convey three interesting findings: (1) even though the diffusion model fails to generate data that visually agrees with real data, the generated data still contributes to enhancing the performance of FL (see improvement from Pri. to T2I). (2) IT2I itself fails to bring performance gain, which may result from the limited diversity and incapability to generate for missing classes. (3) Mixing these two strategies contributes to consistently and significantly better performance."
        ]
    },
    "S4.SS3.4": {
        "caption": "",
        "table": "<table id=\"S4.SS3.3.fig1.1\" class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.SS3.3.fig1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.SS3.3.fig1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Baseline</th>\n<th id=\"S4.SS3.3.fig1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"background-color:#ECECEC;padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S4.SS3.3.fig1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\" style=\"background-color:#ECECEC;\">Equal</span></th>\n<th id=\"S4.SS3.3.fig1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Inverse</th>\n<th id=\"S4.SS3.3.fig1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Water</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.SS3.3.fig1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S4.SS3.3.fig1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">FedAvg</td>\n<td id=\"S4.SS3.3.fig1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"background-color:#ECECEC;padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S4.SS3.3.fig1.1.2.1.2.1\" class=\"ltx_text ltx_font_bold\" style=\"background-color:#ECECEC;\">74.50</span></td>\n<td id=\"S4.SS3.3.fig1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">68.10</td>\n<td id=\"S4.SS3.3.fig1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">71.26</td>\n</tr>\n<tr id=\"S4.SS3.3.fig1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S4.SS3.3.fig1.1.3.2.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">FedProx</td>\n<td id=\"S4.SS3.3.fig1.1.3.2.2\" class=\"ltx_td ltx_align_center\" style=\"background-color:#ECECEC;padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S4.SS3.3.fig1.1.3.2.2.1\" class=\"ltx_text ltx_font_bold\" style=\"background-color:#ECECEC;\">74.36</span></td>\n<td id=\"S4.SS3.3.fig1.1.3.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">68.51</td>\n<td id=\"S4.SS3.3.fig1.1.3.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">72.23</td>\n</tr>\n<tr id=\"S4.SS3.3.fig1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S4.SS3.3.fig1.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">SCAFFOLD</td>\n<td id=\"S4.SS3.3.fig1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"background-color:#ECECEC;padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S4.SS3.3.fig1.1.4.3.2.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">73.96</span></td>\n<td id=\"S4.SS3.3.fig1.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">73.94</td>\n<td id=\"S4.SS3.3.fig1.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S4.SS3.3.fig1.1.4.3.4.1\" class=\"ltx_text ltx_font_bold\">74.43</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Federated learning (FL) enables leveraging distributed private data for model training in a privacy-preserving way.\nHowever, data heterogeneity significantly limits the performance of current FL methods.\nIn this paper, we propose a novel FL framework termed FedGC, designed to mitigate data heterogeneity issues by diversifying private data with generative content.\nFedGC is a simple-to-implement framework as it only introduces a one-shot step of data generation.\nIn data generation, we summarize three crucial and worth-exploring aspects (budget allocation, prompt design, and generation guidance) and propose three solution candidates for each aspect.\nSpecifically, to achieve a better trade-off between data diversity and fidelity for generation guidance, we propose to generate data based on the guidance of prompts and real data simultaneously.\nThe generated data is then merged with private data to facilitate local model training.\nSuch generative data increases the diversity of private data to prevent each client from fitting the potentially biased private data, alleviating the issue of data heterogeneity.\nWe conduct a systematic empirical study on FedGC, covering diverse baselines, datasets, scenarios, and modalities.\nInteresting findings include (1) FedGC consistently and significantly enhances the performance of FL methods, even when notable disparities exist between generative and private data;\n(2) FedGC achieves both better performance and privacy-preservation.\nWe wish this work can inspire future works to further explore the potential of enhancing FL with generative content.",
            "For image task, we consider three diversity levels.\n(1) Single prompt, where we use â€œa photo of {class}â€Â (Radford etÂ al., 2021).\n(2) Multiple prompts, where we consider diverse formats such as â€œ{class}â€.\n(3) LLM-based diversified prompts, where we instruct an LLM such as ChatGPT to diversify the prompts.\nWhile for text generation, we only design one prompt since the ChatGPTÂ (OpenAI, 2023) is sufficient to generate diverse content if we instruct it to be diverse; see TableÂ 8.",
            "(Real-Data Guidance) To alleviate this issue, we propose a new real-data-guided generation approach, which conditions data generation on both real data and prompts.\nFor image task, unlike the original text-guided generation that starts from a random Gaussian noise at latent space ğ’›T1subscriptsuperscriptğ’›1ğ‘‡{\\bm{z}}^{1}_{T}Â (Rombach etÂ al., 2022), we propose to inject information of real data into the starting noise.\nSpecifically, we first use the auto-encoder to encode the real image ğ’™ğ’™{\\bm{x}} to latent representation ğ’›ğ’›{\\bm{z}}, then add some Gaussian variation to obtain a new ğ’›T2subscriptsuperscriptğ’›2ğ‘‡{\\bm{z}}^{2}_{T}, which substitutes ğ’›T1subscriptsuperscriptğ’›1ğ‘‡{\\bm{z}}^{1}_{T} as the starting point; see illustration in Â 7.\nThis enriched latent representation, infused with real data insights, enables the generative model to produce outputs closely resembling real data, optimizing the trade-off between diversity and fidelity.\nFor text task, see illustration in TableÂ 8 using ChatGPT.",
            "We set the number of communication rounds as 100. TableÂ 9 lists client number for each dataset.",
            "FedGC significantly improves the FL performance under data heterogeneity. In TableÂ 1, we show experimental results on two heterogeneity types (label-level and feature-level heterogeneity), two datasets for each type (CIFAR-10, EuroSAT, PACS, and VLCS), and two heterogeneity levels for each dataset.\nFrom the table, we see that (1) incorporating baseline in our FedGC framework can consistently and significantly improve the performance of baseline across diverse settings.\n(2) FedGC is extremely helpful when the heterogeneity level is relatively high, convincingly supporting our idea of introducing generative data to mitigate the effects of data heterogeneity.\nSpecifically, based on FedAvg, FedGC brings 21.01 absolute accuracy improvement under a high heterogeneity level on EuroSAT and 12.26 absolute accuracy improvement on average.",
            "FedGC is compatible with existing FL methods. From TableÂ 1, we see that FedGC consistently and significantly brings performance gain across 6 different baselines, including FedAvg, FedAvgM, FedProx, SCAFFOLD, MOON, and FedDecorr.\nFor example, FedGC averagely brings 12.68 absolute accuracy improvement to SCAFFOLDÂ (Karimireddy etÂ al., 2020).\nThis demonstrates the compatibility and universality of our proposed FedGC framework.",
            "FedGC achieves better performance and privacy preservation at the same time. In FigureÂ 2, we show the performance and privacy preservation trade-off comparisons before and after using FedGC.\nTo measure privacy preservation, we use a simple membership inference attack method based on loss evaluationÂ (Yu etÂ al., 2021; Sablayrolles etÂ al., 2019) to evaluate attack accuracy, see details in SectionÂ A.3.\nLower attack accuracy indicates better privacy preservation.\nFrom the figure, we have an interesting finding that our FedGC framework can not only improve the performance under data heterogeneity, but also enhance the privacy preservation.\nWe also show in TableÂ 10 that FedGC achieves significantly lower attack accuracy when similar task accuracy is achieved.\nThis is surprising yet reasonable since FedGC requires the model to learn from both the private data and the diverse generative data, meaning that the generative data can dilute the concentration of real, sensitive data.",
            "Generating more data could make FedAvg prevail. In TableÂ 2, we explore the effects of number of generated samples on FLâ€™s performance. 0 denotes vanilla FL baseline. Experiments are conducted on CIFAR-10 (Î²=0.05ğ›½0.05\\beta=0.05). From the table, we have an interesting finding: (1) when the number of generated samples is relatively small (0âˆ¼similar-to\\sim2000), FedGC can enlarge the gap between standard FedAvg and the method (SCAFFOLD) that is specifically designed for addressing data heterogeneity; (2) however, as the number continues to grow, the situation is reversed that the basic FL method FedAvg prevails. This finding suggests that apart from carefully designing FL algorithm, it is also a promising direction to explore the greater potential from the perspective of generative data.",
            "Equal allocation is a preferred allocation strategy for its effectiveness and simplicity. In TableÂ 5, we compare different budget allocation strategies on CIFAR-10, including equal allocation, inverse allocation, and water-filling-based allocation. Experiments show that equal allocation contributes to better performance for both FedAvg and FedProx, and comparable performance compared with water-filling-based allocation for SCAFFOLD. Considering effectiveness and simplicity, we conclude that equal allocation is a preferred allocation strategy.",
            "Multiple prompts lead to better performance, while LLM-based diversification might be unnecessary.\nIn TableÂ 3, we explore different prompt designs on PACS dataset. PACS contains significant label-level and feature-level variations, making it an apt choice for this exploration.\nWe compare baseline without FedGC, FedGC with single, multiple, and LLM-based prompts (see prompt generation in TableÂ 7).\nFrom the table, (1) we see that FedGC incorporated with all the prompt designs improves the performance of baselines (see improvement over the No-GC column).\n(2) We see that multiple prompts consistently and significantly perform better, while LLM-based prompts perform ordinarily.\nThis may result from the fact that the scene descriptions from the LLM are usually complicated, causing multifaceted patterns in one sample, thereby complicating model training.\nOverall, we prefer the design of multiple prompts for its effectiveness, diversity, and simplicity.",
            "Mixed guidance contributes to higher performance for rare tasks.\nIn TableÂ 4, we compare different generation guidance designs on a medical dataset HAM10000Â (Tschandl etÂ al., 2018).\nThe reason for choosing this dataset is that the diffusion modelÂ (Rombach etÂ al., 2022) fails to correctly understand medical promptsÂ (Kazerouni etÂ al., 2022), which helps support our claim more convincingly. We consider three designs, including text-guided generation (T2I), our proposed data generation with guidance of text and real data (IT2I), and the mixed usage of T2I and IT2I. These experiments convey three interesting findings: (1) even though the diffusion model fails to generate data that visually agrees with real data, the generated data still contributes to enhancing the performance of FL (see improvement from Pri. to T2I). (2) IT2I itself fails to bring performance gain, which may result from the limited diversity and incapability to generate for missing classes. (3) Mixing these two strategies contributes to consistently and significantly better performance.",
            "Mixed training is the most effective training strategy. In TableÂ 6, we compare different training strategies on CIFAR-10, including training only on the private dataset (Pri.), training only on the generative dataset (Gen.), sequential training with private dataset first (P2G), sequential training with generative dataset first (G2P), and mixed training. Experiments show that 1) generative data itself fails to ensure training, indicating that there is a gap between generative data and real private data. 2) However, when using generative data together with real private data, we see consistent performance gain compared to training on private data. This indicates that despite the incapability of fully representing real data, the generative data still contributes to improving training by increasing diversity. 3) Mixed training consistently and significantly achieves better performance.",
            "For the prompts conditioned on the latent diffusion model, we show the LLM-based prompts for generating images in TableÂ 7.\nIn detail, we instruct ChatGPT through System Prompt and User Prompt, to help us create text samples containing the corresponding class name for image generation. Utilizing ChatGPTâ€™s rich imagination of scenarios and the diversity of text styles, we can achieve a diversity of prompts. Therefore, it helps Stable-diffusion to generate diverse and more realistic pictures.",
            "We show the real-data-guidance for text generation using ChatGPT in TableÂ 8.\nCompared to prompts containing class num, here we instruct ChatGPT to imitate the theme and content of the corresponding text and directly expand the amount of text data.\nIn our illustrative examples shown in TableÂ 8, we simulate real-world data scenarios by incorporating four actual instances and generating an additional set of four synthetic instances. In this experimental setup, we task ChatGPT with the generation of data that exhibits diverse patterns akin to those found in authentic real data. Furthermore, we guide ChatGPT to produce two distinct samples for each distinct label category, fostering a balanced and representative dataset.",
            "We list the number of clients for each dataset in TableÂ 9.",
            "We also compare the attack accuracy at the point when FedAvg and FedGC achieve similar task accuracy in TableÂ 10.\nFrom the table, we see a much more significant reduction in privacy leakage (i.e., much lower attack accuracy).\nThis is reasonable as FedGC can accelerate the convergence speed, which means FedGC requires fewer steps of optimization on the sensitive private data to achieve the same.",
            "Our proposed FedGC framework is also applicable in cases where not every client has the capability to generate data. Here, we experiment on CIFAR-10 under two different heterogeneity levels. In TableÂ 11, we compare vanilla baseline with no generative data, FedGC where all clients can generate data, and FedGC where only half of the clients can generate data.",
            "From the table, we see that\n(1) our proposed FedGC can consistently and significantly achieve the best performance despite the amount of generation-capable clients.\n(2) Surprisingly, we find that under low heterogeneity level, when applied to SCAFFOLDÂ (Karimireddy etÂ al., 2020), FedGC with few generation-capable clients even performs better.\nThis interesting finding demonstrates that our framework may be further improved by more fine-grained designs regarding who is responsible for data generation and the volume of data to be generated.",
            "Here, we conduct experiments of three baselines including FedAvg, FedProx, and SCAFFOLD on CIFAR-10 with Dirichlet distribution parameter Î²ğ›½\\beta = 0.1. Specifically, we set the communication round to 200, local iteration number to 100, and try different client number and participation rate. As illustrated in Table 12, we can observe that FedGC still significantly outperforms the baseline with no generated data under each circumstance.",
            "Here, we perform experiments on EuroSAT dataset with two heterogeneity levels in TableÂ 13.\nVanilla denotes FedAvg itself, No F denotes FedGC without filtering, F@50 denotes filtering from round 50, F@50-C denotes category-wise filtering.\nFrom the table, we see that (1) under a high heterogeneity level, F@75 contributes to higher performance than No F, even with only 90% of data at final rounds.\n(2) Category-wise filtering generally performs better than unified filtering, indicating its effectiveness.\n(3) Nevertheless, such filtering technique can not always ensure performance improvement, calling for more future work.\nThe performance drop could result from reduced number of data samples and ineffective filtering."
        ]
    },
    "S4.T5": {
        "caption": "Table 5: Different budget allocation strategies of FedGC applied on baselines. Equal allocation is preferred for its effectiveness and simplicity.",
        "table": "",
        "footnotes": "",
        "references": [
            "Equal allocation is a preferred allocation strategy for its effectiveness and simplicity. In TableÂ 5, we compare different budget allocation strategies on CIFAR-10, including equal allocation, inverse allocation, and water-filling-based allocation. Experiments show that equal allocation contributes to better performance for both FedAvg and FedProx, and comparable performance compared with water-filling-based allocation for SCAFFOLD. Considering effectiveness and simplicity, we conclude that equal allocation is a preferred allocation strategy."
        ]
    },
    "S4.T6": {
        "caption": "Table 6: Different training strategies of FedGC applied on baselines. Generated data can only exhibit its efficacy when used in conjunction with real data. Mixed training is the most effective.",
        "table": "",
        "footnotes": "",
        "references": [
            "Mixed training is the most effective training strategy. In TableÂ 6, we compare different training strategies on CIFAR-10, including training only on the private dataset (Pri.), training only on the generative dataset (Gen.), sequential training with private dataset first (P2G), sequential training with generative dataset first (G2P), and mixed training. Experiments show that 1) generative data itself fails to ensure training, indicating that there is a gap between generative data and real private data. 2) However, when using generative data together with real private data, we see consistent performance gain compared to training on private data. This indicates that despite the incapability of fully representing real data, the generative data still contributes to improving training by increasing diversity. 3) Mixed training consistently and significantly achieves better performance."
        ]
    },
    "A1.T7": {
        "caption": "Table 7: Obtaining LLM-based prompts for generating images using diffusion models. Instructions for generating scene descriptions (i.e., prompts for diffusion models) given a class name using ChatGPT. Here, we provide an example on the dog category of PACS dataset.",
        "table": "<svg id=\"A1.T7.pic1\" class=\"ltx_picture\" height=\"239.11\" overflow=\"visible\" version=\"1.1\" width=\"600\"><g transform=\"translate(0,239.11) matrix(1 0 0 -1 0 0)\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\"><g fill=\"#404040\" fill-opacity=\"1.0\"><path d=\"M 0 5.91 L 0 233.21 C 0 236.47 2.64 239.11 5.91 239.11 L 594.09 239.11 C 597.36 239.11 600 236.47 600 233.21 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z\" style=\"stroke:none\"></path></g><g fill=\"#FAFAFF\" fill-opacity=\"1.0\"><path d=\"M 1.97 5.91 L 1.97 233.21 C 1.97 235.38 3.73 237.14 5.91 237.14 L 594.09 237.14 C 596.27 237.14 598.03 235.38 598.03 233.21 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z\" style=\"stroke:none\"></path></g><g fill-opacity=\"1.0\" transform=\"matrix(1.0 0.0 0.0 1.0 21.65 13.78)\"><foreignobject width=\"556.69\" height=\"211.55\" transform=\"matrix(1 0 0 -1 0 16.6)\" overflow=\"visible\" color=\"#000000\">\n<span id=\"A1.T7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1\" class=\"ltx_inline-block ltx_minipage ltx_align_bottom\" style=\"width:402.3pt;\">\n<span id=\"A1.T7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2\" class=\"ltx_p\"><span id=\"A1.T7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">System Prompt:\n<br class=\"ltx_break\"></span>You are an AI assistant that helps people find information.</span>\n<span id=\"A1.T7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3\" class=\"ltx_p\"><span id=\"A1.T7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">User Prompt:\n<br class=\"ltx_break\"></span>Please help me come up with scene descriptions that contain a dog while not containing an elephant, giraffe, guitar, horse, house, person.</span>\n<span id=\"A1.T7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.4\" class=\"ltx_p\">For example:</span>\n<span id=\"A1.T7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1\" class=\"ltx_p\"><math id=\"A1.T7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\left[\\text{``A dog is running on the grass'', ``A dog is sleeping on the floor''}\\right]\" display=\"inline\"><semantics id=\"A1.T7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a\"><mrow id=\"A1.T7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.2.2\" xref=\"A1.T7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.2.1.cmml\"><mo id=\"A1.T7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.2.2.1\" xref=\"A1.T7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.2.1.1.cmml\">[</mo><mtext id=\"A1.T7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1\" xref=\"A1.T7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1a.cmml\">â€œA dog is running on the grassâ€, â€œA dog is sleeping on the floorâ€</mtext><mo id=\"A1.T7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.2.2.2\" xref=\"A1.T7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.2.1.1.cmml\">]</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.T7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b\"><apply id=\"A1.T7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.2.1.cmml\" xref=\"A1.T7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.2.2\"><csymbol cd=\"latexml\" id=\"A1.T7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.2.1.1.cmml\" xref=\"A1.T7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.2.2.1\">delimited-[]</csymbol><ci id=\"A1.T7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1a.cmml\" xref=\"A1.T7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1\"><mtext id=\"A1.T7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml\" xref=\"A1.T7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1\">â€œA dog is running on the grassâ€, â€œA dog is sleeping on the floorâ€</mtext></ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c\">\\left[\\text{``A dog is running on the grass'', ``A dog is sleeping on the floor''}\\right]</annotation></semantics></math></span>\n<span id=\"A1.T7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.5\" class=\"ltx_p\">Please generate 10 samples in the format of a list.</span>\n<span id=\"A1.T7.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.6\" class=\"ltx_p\">Remember: each description should be within 10 words.</span>\n</span></foreignobject></g></g></svg>\n\n",
        "footnotes": "",
        "references": [
            "Multiple prompts lead to better performance, while LLM-based diversification might be unnecessary.\nIn TableÂ 3, we explore different prompt designs on PACS dataset. PACS contains significant label-level and feature-level variations, making it an apt choice for this exploration.\nWe compare baseline without FedGC, FedGC with single, multiple, and LLM-based prompts (see prompt generation in TableÂ 7).\nFrom the table, (1) we see that FedGC incorporated with all the prompt designs improves the performance of baselines (see improvement over the No-GC column).\n(2) We see that multiple prompts consistently and significantly perform better, while LLM-based prompts perform ordinarily.\nThis may result from the fact that the scene descriptions from the LLM are usually complicated, causing multifaceted patterns in one sample, thereby complicating model training.\nOverall, we prefer the design of multiple prompts for its effectiveness, diversity, and simplicity.",
            "For the prompts conditioned on the latent diffusion model, we show the LLM-based prompts for generating images in TableÂ 7.\nIn detail, we instruct ChatGPT through System Prompt and User Prompt, to help us create text samples containing the corresponding class name for image generation. Utilizing ChatGPTâ€™s rich imagination of scenarios and the diversity of text styles, we can achieve a diversity of prompts. Therefore, it helps Stable-diffusion to generate diverse and more realistic pictures."
        ]
    },
    "A1.T8": {
        "caption": "Table 8: Real-data-guidance for text generation using ChatGPT. Real data is modeled in the examples, where we provide four real examples and generate four new examples. We instruct the ChatGPT to generate diverse data that has a similar pattern to real data. We also instruct the ChatGPT to generate two samples for each label.",
        "table": "<svg id=\"A1.T8.pic1\" class=\"ltx_picture\" height=\"340.28\" overflow=\"visible\" version=\"1.1\" width=\"600\"><g transform=\"translate(0,340.28) matrix(1 0 0 -1 0 0)\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\"><g fill=\"#404040\" fill-opacity=\"1.0\"><path d=\"M 0 5.91 L 0 334.37 C 0 337.63 2.64 340.28 5.91 340.28 L 594.09 340.28 C 597.36 340.28 600 337.63 600 334.37 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z\" style=\"stroke:none\"></path></g><g fill=\"#FAFAFF\" fill-opacity=\"1.0\"><path d=\"M 1.97 5.91 L 1.97 334.37 C 1.97 336.54 3.73 338.31 5.91 338.31 L 594.09 338.31 C 596.27 338.31 598.03 336.54 598.03 334.37 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z\" style=\"stroke:none\"></path></g><g fill-opacity=\"1.0\" transform=\"matrix(1.0 0.0 0.0 1.0 21.65 13.78)\"><foreignobject width=\"556.69\" height=\"312.72\" transform=\"matrix(1 0 0 -1 0 16.6)\" overflow=\"visible\" color=\"#000000\">\n<span id=\"A1.T8.pic1.1.1.1.1.1\" class=\"ltx_inline-block ltx_minipage ltx_align_bottom\" style=\"width:402.3pt;\">\n<span id=\"A1.T8.pic1.1.1.1.1.1.1\" class=\"ltx_p\"><span id=\"A1.T8.pic1.1.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">System Prompt:\n<br class=\"ltx_break\"></span>Assistant is an intelligent chatbot designed to help users generate similar data. Users will provide a few real samples and the Assistant will generate data that follows the pattern of real samples. This is a binary dataset on sentiment analysis, where 0 denotes negative and 1 denotes positive.</span>\n<span id=\"A1.T8.pic1.1.1.1.1.1.2\" class=\"ltx_p\">Instructions:</span>\n<span id=\"A1.T8.pic1.1.1.1.1.1.3\" class=\"ltx_p\">1. Generate two samples with label 0 and two samples with label 1, try to make the content diverse</span>\n<span id=\"A1.T8.pic1.1.1.1.1.1.4\" class=\"ltx_p\">2. Should have a similar pattern of usersâ€™ data.</span>\n<span id=\"A1.T8.pic1.1.1.1.1.1.5\" class=\"ltx_p\"><span id=\"A1.T8.pic1.1.1.1.1.1.5.1\" class=\"ltx_text ltx_font_bold\">User Prompt:\n<br class=\"ltx_break\"></span>**Data: {example_input_1}, Label: {example_label_1}**</span>\n<span id=\"A1.T8.pic1.1.1.1.1.1.6\" class=\"ltx_p\">**Data: {example_input_2}, Label: {example_label_2}**</span>\n<span id=\"A1.T8.pic1.1.1.1.1.1.7\" class=\"ltx_p\">**Data: {example_input_3}, Label: {example_label_3}**</span>\n<span id=\"A1.T8.pic1.1.1.1.1.1.8\" class=\"ltx_p\">**Data: {example_input_4}, Label: {example_label_4}**</span>\n<span id=\"A1.T8.pic1.1.1.1.1.1.9\" class=\"ltx_p\">Generate two samples with label 0 and two samples with label 1.</span>\n<span id=\"A1.T8.pic1.1.1.1.1.1.10\" class=\"ltx_p\">In the format of Data: {}, Label: {}. Each sample should start with ** and end with **.</span>\n</span></foreignobject></g></g></svg>\n\n",
        "footnotes": "",
        "references": [
            "For image task, we consider three diversity levels.\n(1) Single prompt, where we use â€œa photo of {class}â€Â (Radford etÂ al., 2021).\n(2) Multiple prompts, where we consider diverse formats such as â€œ{class}â€.\n(3) LLM-based diversified prompts, where we instruct an LLM such as ChatGPT to diversify the prompts.\nWhile for text generation, we only design one prompt since the ChatGPTÂ (OpenAI, 2023) is sufficient to generate diverse content if we instruct it to be diverse; see TableÂ 8.",
            "(Real-Data Guidance) To alleviate this issue, we propose a new real-data-guided generation approach, which conditions data generation on both real data and prompts.\nFor image task, unlike the original text-guided generation that starts from a random Gaussian noise at latent space ğ’›T1subscriptsuperscriptğ’›1ğ‘‡{\\bm{z}}^{1}_{T}Â (Rombach etÂ al., 2022), we propose to inject information of real data into the starting noise.\nSpecifically, we first use the auto-encoder to encode the real image ğ’™ğ’™{\\bm{x}} to latent representation ğ’›ğ’›{\\bm{z}}, then add some Gaussian variation to obtain a new ğ’›T2subscriptsuperscriptğ’›2ğ‘‡{\\bm{z}}^{2}_{T}, which substitutes ğ’›T1subscriptsuperscriptğ’›1ğ‘‡{\\bm{z}}^{1}_{T} as the starting point; see illustration in Â 7.\nThis enriched latent representation, infused with real data insights, enables the generative model to produce outputs closely resembling real data, optimizing the trade-off between diversity and fidelity.\nFor text task, see illustration in TableÂ 8 using ChatGPT.",
            "We show the real-data-guidance for text generation using ChatGPT in TableÂ 8.\nCompared to prompts containing class num, here we instruct ChatGPT to imitate the theme and content of the corresponding text and directly expand the amount of text data.\nIn our illustrative examples shown in TableÂ 8, we simulate real-world data scenarios by incorporating four actual instances and generating an additional set of four synthetic instances. In this experimental setup, we task ChatGPT with the generation of data that exhibits diverse patterns akin to those found in authentic real data. Furthermore, we guide ChatGPT to produce two distinct samples for each distinct label category, fostering a balanced and representative dataset."
        ]
    },
    "A1.T9": {
        "caption": "Table 9: Number of clients for each dataset.",
        "table": "<table id=\"A1.T9.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A1.T9.1.1.1\" class=\"ltx_tr\">\n<th id=\"A1.T9.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Dataset</th>\n<th id=\"A1.T9.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">CIFAR-10</th>\n<th id=\"A1.T9.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">EuroSAT</th>\n<th id=\"A1.T9.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">PACS</th>\n<th id=\"A1.T9.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">VLCS</th>\n<th id=\"A1.T9.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">HAM10000</th>\n<th id=\"A1.T9.1.1.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Sentiment</th>\n<th id=\"A1.T9.1.1.1.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Yahoo!</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T9.1.2.1\" class=\"ltx_tr\">\n<th id=\"A1.T9.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Client Number</th>\n<td id=\"A1.T9.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">10</td>\n<td id=\"A1.T9.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">10</td>\n<td id=\"A1.T9.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">20</td>\n<td id=\"A1.T9.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">20</td>\n<td id=\"A1.T9.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">10</td>\n<td id=\"A1.T9.1.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">1000</td>\n<td id=\"A1.T9.1.2.1.8\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">100</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We set the number of communication rounds as 100. TableÂ 9 lists client number for each dataset.",
            "We list the number of clients for each dataset in TableÂ 9."
        ]
    },
    "A1.T10": {
        "caption": "Table 10: Membership inference attack accuracy comparisons when FedAvg and FedGC achieve similar task accuracy. We consider two scenarios where the total number of clientsâ€™ real samples is 50k and 10k, respectively. We also explore the effects of using different number of generated samples. FedGC can reduce privacy leakage to a very low level (since random guess is 50%) while maintaining task accuracy at the same time. ",
        "table": "<table id=\"A1.T10.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A1.T10.1.1.1\" class=\"ltx_tr\">\n<th id=\"A1.T10.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" colspan=\"2\">Number of Real Samples</th>\n<th id=\"A1.T10.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" colspan=\"2\">50k</th>\n<th id=\"A1.T10.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" colspan=\"2\">10k</th>\n</tr>\n<tr id=\"A1.T10.1.2.2\" class=\"ltx_tr\">\n<th id=\"A1.T10.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r\" colspan=\"2\">Accuracy</th>\n<th id=\"A1.T10.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row\">Task</th>\n<th id=\"A1.T10.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">Attack</th>\n<th id=\"A1.T10.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row\">Task</th>\n<th id=\"A1.T10.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Attack</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T10.1.3.1\" class=\"ltx_tr\">\n<th id=\"A1.T10.1.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t\" rowspan=\"6\"><span id=\"A1.T10.1.3.1.1.1\" class=\"ltx_text\">No. of Generated Samples</span></th>\n<td id=\"A1.T10.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0</td>\n<th id=\"A1.T10.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">59.71</th>\n<td id=\"A1.T10.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">60.55</td>\n<th id=\"A1.T10.1.3.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">35.48</th>\n<td id=\"A1.T10.1.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">77.55</td>\n</tr>\n<tr id=\"A1.T10.1.4.2\" class=\"ltx_tr\">\n<td id=\"A1.T10.1.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_r\">10k</td>\n<th id=\"A1.T10.1.4.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">61.65</th>\n<td id=\"A1.T10.1.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">52.05</td>\n<th id=\"A1.T10.1.4.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">35.97</th>\n<td id=\"A1.T10.1.4.2.5\" class=\"ltx_td ltx_align_center\">52.80</td>\n</tr>\n<tr id=\"A1.T10.1.5.3\" class=\"ltx_tr\">\n<td id=\"A1.T10.1.5.3.1\" class=\"ltx_td ltx_align_center ltx_border_r\">20k</td>\n<th id=\"A1.T10.1.5.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">62.49</th>\n<td id=\"A1.T10.1.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">51.20</td>\n<th id=\"A1.T10.1.5.3.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">39.18</th>\n<td id=\"A1.T10.1.5.3.5\" class=\"ltx_td ltx_align_center\">52.85</td>\n</tr>\n<tr id=\"A1.T10.1.6.4\" class=\"ltx_tr\">\n<td id=\"A1.T10.1.6.4.1\" class=\"ltx_td ltx_align_center ltx_border_r\">30k</td>\n<th id=\"A1.T10.1.6.4.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">61.82</th>\n<td id=\"A1.T10.1.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">51.95</td>\n<th id=\"A1.T10.1.6.4.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">39.40</th>\n<td id=\"A1.T10.1.6.4.5\" class=\"ltx_td ltx_align_center\">52.50</td>\n</tr>\n<tr id=\"A1.T10.1.7.5\" class=\"ltx_tr\">\n<td id=\"A1.T10.1.7.5.1\" class=\"ltx_td ltx_align_center ltx_border_r\">40k</td>\n<th id=\"A1.T10.1.7.5.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">60.38</th>\n<td id=\"A1.T10.1.7.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\">51.20</td>\n<th id=\"A1.T10.1.7.5.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">37.17</th>\n<td id=\"A1.T10.1.7.5.5\" class=\"ltx_td ltx_align_center\">52.75</td>\n</tr>\n<tr id=\"A1.T10.1.8.6\" class=\"ltx_tr\">\n<td id=\"A1.T10.1.8.6.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">50k</td>\n<th id=\"A1.T10.1.8.6.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\">62.49</th>\n<td id=\"A1.T10.1.8.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">51.60</td>\n<th id=\"A1.T10.1.8.6.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\">38.68</th>\n<td id=\"A1.T10.1.8.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">52.35</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "FedGC achieves better performance and privacy preservation at the same time. In FigureÂ 2, we show the performance and privacy preservation trade-off comparisons before and after using FedGC.\nTo measure privacy preservation, we use a simple membership inference attack method based on loss evaluationÂ (Yu etÂ al., 2021; Sablayrolles etÂ al., 2019) to evaluate attack accuracy, see details in SectionÂ A.3.\nLower attack accuracy indicates better privacy preservation.\nFrom the figure, we have an interesting finding that our FedGC framework can not only improve the performance under data heterogeneity, but also enhance the privacy preservation.\nWe also show in TableÂ 10 that FedGC achieves significantly lower attack accuracy when similar task accuracy is achieved.\nThis is surprising yet reasonable since FedGC requires the model to learn from both the private data and the diverse generative data, meaning that the generative data can dilute the concentration of real, sensitive data.",
            "We also compare the attack accuracy at the point when FedAvg and FedGC achieve similar task accuracy in TableÂ 10.\nFrom the table, we see a much more significant reduction in privacy leakage (i.e., much lower attack accuracy).\nThis is reasonable as FedGC can accelerate the convergence speed, which means FedGC requires fewer steps of optimization on the sensitive private data to achieve the same."
        ]
    },
    "A1.T11": {
        "caption": "Table 11: Experiments of a scene in which partial clients are capable of generation. 1k/50% indicates only half of the clients are capable of generation. However, FedGC still significantly outperforms the baseline with no generative data.",
        "table": "<table id=\"A1.T11.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A1.T11.1.1.1\" class=\"ltx_tr\">\n<th id=\"A1.T11.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\">H-Level</th>\n<th id=\"A1.T11.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"3\">High</th>\n<th id=\"A1.T11.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\">Low</th>\n</tr>\n<tr id=\"A1.T11.1.2.2\" class=\"ltx_tr\">\n<th id=\"A1.T11.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r\">Generation</th>\n<th id=\"A1.T11.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">No</th>\n<th id=\"A1.T11.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">1k/100%</th>\n<th id=\"A1.T11.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">1k/50%</th>\n<th id=\"A1.T11.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">No</th>\n<th id=\"A1.T11.1.2.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">1k/100%</th>\n<th id=\"A1.T11.1.2.2.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">1k/50%</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T11.1.3.1\" class=\"ltx_tr\">\n<th id=\"A1.T11.1.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">FedAvg</th>\n<td id=\"A1.T11.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">60.77</td>\n<td id=\"A1.T11.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">73.99</td>\n<td id=\"A1.T11.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">71.53</td>\n<td id=\"A1.T11.1.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">71.57</td>\n<td id=\"A1.T11.1.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">79.73</td>\n<td id=\"A1.T11.1.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">77.45</td>\n</tr>\n<tr id=\"A1.T11.1.4.2\" class=\"ltx_tr\">\n<th id=\"A1.T11.1.4.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">FedProx</th>\n<td id=\"A1.T11.1.4.2.2\" class=\"ltx_td ltx_align_center\">63.62</td>\n<td id=\"A1.T11.1.4.2.3\" class=\"ltx_td ltx_align_center\">73.69</td>\n<td id=\"A1.T11.1.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\">72.65</td>\n<td id=\"A1.T11.1.4.2.5\" class=\"ltx_td ltx_align_center\">75.76</td>\n<td id=\"A1.T11.1.4.2.6\" class=\"ltx_td ltx_align_center\">79.25</td>\n<td id=\"A1.T11.1.4.2.7\" class=\"ltx_td ltx_align_center\">79.23</td>\n</tr>\n<tr id=\"A1.T11.1.5.3\" class=\"ltx_tr\">\n<th id=\"A1.T11.1.5.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">SCAFFOLD</th>\n<td id=\"A1.T11.1.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">65.00</td>\n<td id=\"A1.T11.1.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">75.75</td>\n<td id=\"A1.T11.1.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">73.28</td>\n<td id=\"A1.T11.1.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">78.74</td>\n<td id=\"A1.T11.1.5.3.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">80.29</td>\n<td id=\"A1.T11.1.5.3.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">81.27</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Our proposed FedGC framework is also applicable in cases where not every client has the capability to generate data. Here, we experiment on CIFAR-10 under two different heterogeneity levels. In TableÂ 11, we compare vanilla baseline with no generative data, FedGC where all clients can generate data, and FedGC where only half of the clients can generate data."
        ]
    },
    "A1.T12": {
        "caption": "Table 12: Experiments of a scene in which only partial clients participate in training each round. We conduct experiments on three different total client numbers and several different participation rates. For example, client 200 and participation rate 5% means randomly selecting 10 clients to participate in training each round. In each case, FedGC still significantly outperforms the baseline with no generative data. ",
        "table": "<table id=\"A1.T12.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A1.T12.1.1.1\" class=\"ltx_tr\">\n<th id=\"A1.T12.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.5pt;padding-right:4.5pt;\" rowspan=\"2\"><span id=\"A1.T12.1.1.1.1.1\" class=\"ltx_text\">Baseline</span></th>\n<th id=\"A1.T12.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">Client</th>\n<th id=\"A1.T12.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-left:4.5pt;padding-right:4.5pt;\" colspan=\"3\">200</th>\n<th id=\"A1.T12.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-left:4.5pt;padding-right:4.5pt;\" colspan=\"2\">100</th>\n<th id=\"A1.T12.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.5pt;padding-right:4.5pt;\" colspan=\"2\">50</th>\n</tr>\n<tr id=\"A1.T12.1.2.2\" class=\"ltx_tr\">\n<th id=\"A1.T12.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">Participation</th>\n<th id=\"A1.T12.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">5%</th>\n<th id=\"A1.T12.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">10%</th>\n<th id=\"A1.T12.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">20%</th>\n<th id=\"A1.T12.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">10%</th>\n<th id=\"A1.T12.1.2.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">20%</th>\n<th id=\"A1.T12.1.2.2.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">10%</th>\n<th id=\"A1.T12.1.2.2.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">20%</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T12.1.3.1\" class=\"ltx_tr\">\n<td id=\"A1.T12.1.3.1.1\" class=\"ltx_td ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"></td>\n<td id=\"A1.T12.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">Vanilla</td>\n<td id=\"A1.T12.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">53.62</td>\n<td id=\"A1.T12.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">60.00</td>\n<td id=\"A1.T12.1.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">65.76</td>\n<td id=\"A1.T12.1.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">56.53</td>\n<td id=\"A1.T12.1.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">57.69</td>\n<td id=\"A1.T12.1.3.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">55.90</td>\n<td id=\"A1.T12.1.3.1.9\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">63.33</td>\n</tr>\n<tr id=\"A1.T12.1.4.2\" class=\"ltx_tr\" style=\"background-color:#ECECEC;\">\n<td id=\"A1.T12.1.4.2.1\" class=\"ltx_td ltx_align_center\" style=\"background-color:#FFFFFF;padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"A1.T12.1.4.2.1.1\" class=\"ltx_text\" style=\"background-color:#FFFFFF;\">FedAvg</span></td>\n<td id=\"A1.T12.1.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"A1.T12.1.4.2.2.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">+ FedGC</span></td>\n<td id=\"A1.T12.1.4.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"A1.T12.1.4.2.3.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">68.93</span></td>\n<td id=\"A1.T12.1.4.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"A1.T12.1.4.2.4.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">74.06</span></td>\n<td id=\"A1.T12.1.4.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"A1.T12.1.4.2.5.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">75.74</span></td>\n<td id=\"A1.T12.1.4.2.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"A1.T12.1.4.2.6.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">74.16</span></td>\n<td id=\"A1.T12.1.4.2.7\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"A1.T12.1.4.2.7.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">74.26</span></td>\n<td id=\"A1.T12.1.4.2.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"A1.T12.1.4.2.8.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">75.34</span></td>\n<td id=\"A1.T12.1.4.2.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"A1.T12.1.4.2.9.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">77.20</span></td>\n</tr>\n<tr id=\"A1.T12.1.5.3\" class=\"ltx_tr\">\n<td id=\"A1.T12.1.5.3.1\" class=\"ltx_td\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"></td>\n<td id=\"A1.T12.1.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">Vanilla</td>\n<td id=\"A1.T12.1.5.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">53.93</td>\n<td id=\"A1.T12.1.5.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">59.95</td>\n<td id=\"A1.T12.1.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">64.53</td>\n<td id=\"A1.T12.1.5.3.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">56.74</td>\n<td id=\"A1.T12.1.5.3.7\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">59.54</td>\n<td id=\"A1.T12.1.5.3.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">56.36</td>\n<td id=\"A1.T12.1.5.3.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">65.66</td>\n</tr>\n<tr id=\"A1.T12.1.6.4\" class=\"ltx_tr\" style=\"background-color:#ECECEC;\">\n<td id=\"A1.T12.1.6.4.1\" class=\"ltx_td ltx_align_center\" style=\"background-color:#FFFFFF;padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"A1.T12.1.6.4.1.1\" class=\"ltx_text\" style=\"background-color:#FFFFFF;\">FedProx</span></td>\n<td id=\"A1.T12.1.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"A1.T12.1.6.4.2.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">+ FedGC</span></td>\n<td id=\"A1.T12.1.6.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"A1.T12.1.6.4.3.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">70.23</span></td>\n<td id=\"A1.T12.1.6.4.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"A1.T12.1.6.4.4.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">73.79</span></td>\n<td id=\"A1.T12.1.6.4.5\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"A1.T12.1.6.4.5.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">75.07</span></td>\n<td id=\"A1.T12.1.6.4.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"A1.T12.1.6.4.6.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">74.39</span></td>\n<td id=\"A1.T12.1.6.4.7\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"A1.T12.1.6.4.7.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">74.05</span></td>\n<td id=\"A1.T12.1.6.4.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"A1.T12.1.6.4.8.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">75.47</span></td>\n<td id=\"A1.T12.1.6.4.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"A1.T12.1.6.4.9.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">77.47</span></td>\n</tr>\n<tr id=\"A1.T12.1.7.5\" class=\"ltx_tr\">\n<td id=\"A1.T12.1.7.5.1\" class=\"ltx_td\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"></td>\n<td id=\"A1.T12.1.7.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">Vanilla</td>\n<td id=\"A1.T12.1.7.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">60.41</td>\n<td id=\"A1.T12.1.7.5.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">68.02</td>\n<td id=\"A1.T12.1.7.5.5\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">70.15</td>\n<td id=\"A1.T12.1.7.5.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">65.03</td>\n<td id=\"A1.T12.1.7.5.7\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">68.12</td>\n<td id=\"A1.T12.1.7.5.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">65.73</td>\n<td id=\"A1.T12.1.7.5.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">72.42</td>\n</tr>\n<tr id=\"A1.T12.1.8.6\" class=\"ltx_tr\" style=\"background-color:#ECECEC;\">\n<td id=\"A1.T12.1.8.6.1\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"background-color:#FFFFFF;padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"A1.T12.1.8.6.1.1\" class=\"ltx_text\" style=\"background-color:#FFFFFF;\">SCAFFOLD</span></td>\n<td id=\"A1.T12.1.8.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"A1.T12.1.8.6.2.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">+ FedGC</span></td>\n<td id=\"A1.T12.1.8.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"A1.T12.1.8.6.3.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">71.65</span></td>\n<td id=\"A1.T12.1.8.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"A1.T12.1.8.6.4.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">74.83</span></td>\n<td id=\"A1.T12.1.8.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"A1.T12.1.8.6.5.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">77.54</span></td>\n<td id=\"A1.T12.1.8.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"A1.T12.1.8.6.6.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">74.38</span></td>\n<td id=\"A1.T12.1.8.6.7\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"A1.T12.1.8.6.7.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">76.26</span></td>\n<td id=\"A1.T12.1.8.6.8\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"A1.T12.1.8.6.8.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">72.74</span></td>\n<td id=\"A1.T12.1.8.6.9\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span id=\"A1.T12.1.8.6.9.1\" class=\"ltx_text\" style=\"background-color:#ECECEC;\">77.56</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Here, we conduct experiments of three baselines including FedAvg, FedProx, and SCAFFOLD on CIFAR-10 with Dirichlet distribution parameter Î²ğ›½\\beta = 0.1. Specifically, we set the communication round to 200, local iteration number to 100, and try different client number and participation rate. As illustrated in Table 12, we can observe that FedGC still significantly outperforms the baseline with no generated data under each circumstance."
        ]
    },
    "A1.T13": {
        "caption": "Table 13: Experiments of global-model-based data filtering. We conduct our initial attempt on EuroSAT dataset with two heterogeneity types. F@50 means start filtering after 50 communication rounds and C means filtering by each class.",
        "table": "<table id=\"A1.T13.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A1.T13.1.1.1\" class=\"ltx_tr\">\n<th id=\"A1.T13.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\">Heterogeneity Level</th>\n<th id=\"A1.T13.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Vanilla</th>\n<th id=\"A1.T13.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">No F</th>\n<th id=\"A1.T13.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">F@50</th>\n<th id=\"A1.T13.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">F@75</th>\n<th id=\"A1.T13.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">F@50-C</th>\n<th id=\"A1.T13.1.1.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">F@75-C</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T13.1.2.1\" class=\"ltx_tr\">\n<th id=\"A1.T13.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">High</th>\n<td id=\"A1.T13.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">53.82</td>\n<td id=\"A1.T13.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">74.83</td>\n<td id=\"A1.T13.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">72.96</td>\n<td id=\"A1.T13.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">74.93</td>\n<td id=\"A1.T13.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">73.50</td>\n<td id=\"A1.T13.1.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">74.20</td>\n</tr>\n<tr id=\"A1.T13.1.3.2\" class=\"ltx_tr\">\n<th id=\"A1.T13.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">Low</th>\n<td id=\"A1.T13.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">75.59</td>\n<td id=\"A1.T13.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">84.46</td>\n<td id=\"A1.T13.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">83.82</td>\n<td id=\"A1.T13.1.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">83.83</td>\n<td id=\"A1.T13.1.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">84.19</td>\n<td id=\"A1.T13.1.3.2.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">83.83</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Here, we perform experiments on EuroSAT dataset with two heterogeneity levels in TableÂ 13.\nVanilla denotes FedAvg itself, No F denotes FedGC without filtering, F@50 denotes filtering from round 50, F@50-C denotes category-wise filtering.\nFrom the table, we see that (1) under a high heterogeneity level, F@75 contributes to higher performance than No F, even with only 90% of data at final rounds.\n(2) Category-wise filtering generally performs better than unified filtering, indicating its effectiveness.\n(3) Nevertheless, such filtering technique can not always ensure performance improvement, calling for more future work.\nThe performance drop could result from reduced number of data samples and ineffective filtering."
        ]
    }
}