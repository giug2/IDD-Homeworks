{
    "id_table_1": {
        "caption": "Table 1 :  Comparison of pre-training conditions and backbones of foundation models.",
        "table": "S4.T1.1",
        "footnotes": [
            "",
            "",
            "",
            ""
        ],
        "references": [
            "UnivFD  [ 51 ]  is the first method to employ this approach. In UnivFD, the parameters of the feature extractor   : R d  R n : italic-  superscript R d superscript R n \\phi:\\mathbb{R}^{d}\\to\\mathbb{R}^{n} italic_ : blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT  are frozen, where  n n n italic_n  is the embedding space dimension. The parameters of the detector, denoted as    \\boldsymbol{\\theta} bold_italic_ , are trained using binary cross-entropy (BCE) loss, as shown in Equation ( 1 ):",
            "In all experiments, we use UnivFD as the framework for SID. Different pre-trained foundation models are adopted as the backbone of UnivFD, and we analyze their impact. We use ViT-B a variant of ViT, as the architecture for the backbone. For pre-training the backbone, we employ CLIP and DINOv2 with real images, and StableRep and SynCLR with synthetic data. A comparison of the pre-training conditions is shown in Table  1 .",
            "Fig.  1  shows a visual analysis of the embedding spaces of backbones pre-trained using different methods. Using the feature vectors from each model, we plotted four feature banks consisting of the same real and fake images obtained from ProGAN, and color-coded the resulting UMAP  [ 43 ]  plots with binary (real/fake) labels. All backbones exhibit a certain level of performance in separating real (blue) and fake (yellow) features, but the embedding space of SynCLR demonstrates the best separation performance.",
            "We also provide visualizations to confirm the differences in detection performance across different types of generative models. Fig.  2  shows a visualization of the embedding spaces using UMAP, similar to Fig.  1 , but the fake data includes images generated by various generative models. The GAN category includes images generated by ProGAN, CycleGAN, BigGAN, StarGAN, and StyleGAN2, while the DM category includes images generated by Guided, LDM, and Glide. The embedding space of SynCLR separates GAN and real images better compared to that of CLIP. On the other hand, the embedding space of SynCLR does not sufficiently separate DM and real images. This visualization result is consistent with the numerical evaluation results presented in the previous section.",
            "In the previous section, qualitative analysis confirmed that synthetic data-driven representations capture different features compared to those learned using only real images. Based on this analysis, we conduct a simple ensemble learning experiment to verify whether combining backbones having synthetic data-driven representations with those having different representations can improve the performance of synthetic image detectors. For the ensemble method, we adopt feature fusion  [ 25 ,  78 ,  71 ,  37 ] , where the features are combined just before the fully connected layer, and the parameters of the fully connected layer are then trained using the combined features. Apart from adopting feature fusion, the training process is the same as described in Section  4.1 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Average precision (AP) for all backbone pre-training methods (rows) in detecting fake images from different generative models (columns). We note that the variant within Ours which uses CLIP as the backbone becomes identical configuration to the original UnivFD.",
        "table": "S4.T2.4.1",
        "footnotes": [
            "",
            "",
            ""
        ],
        "references": [
            "Table  2  and Table  3  show AP and classification accuracy, respectively, of all backbone pre-training methods (rows) in detecting fake images from different generative models (columns). For classification accuracy, the numbers shown are averaged over the real and fake classes for each generative model.",
            "We also provide visualizations to confirm the differences in detection performance across different types of generative models. Fig.  2  shows a visualization of the embedding spaces using UMAP, similar to Fig.  1 , but the fake data includes images generated by various generative models. The GAN category includes images generated by ProGAN, CycleGAN, BigGAN, StarGAN, and StyleGAN2, while the DM category includes images generated by Guided, LDM, and Glide. The embedding space of SynCLR separates GAN and real images better compared to that of CLIP. On the other hand, the embedding space of SynCLR does not sufficiently separate DM and real images. This visualization result is consistent with the numerical evaluation results presented in the previous section."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Classification accuracy for all backbone pre-training methods (rows) averaged over real and fake classes for each generative model (columns). We note that the variant within Ours which uses CLIP as the backbone becomes identical configuration to the original UnivFD.",
        "table": "S4.T3.4.1",
        "footnotes": [
            "",
            "",
            ""
        ],
        "references": [
            "Table  2  and Table  3  show AP and classification accuracy, respectively, of all backbone pre-training methods (rows) in detecting fake images from different generative models (columns). For classification accuracy, the numbers shown are averaged over the real and fake classes for each generative model.",
            "We use attention maps to visualize which parts of the images the backbones with synthetic data-driven representations are focusing on. Fig.  3  shows the results for CLIP and SynCLR for real and fake images. The attention maps are visualized for the initial layer, middle layer, and final layer, and the maps are averaged across all heads. The synthetic images used as sample inputs were generated by ProGAN, CycleGAN, BigGAN, and StyleGAN2 for GANs, and by Guided, LDM, and Glide for DMs. Compared to CLIP, SynCLRs shallow layer maps show a broad attention spread across the entire image. As the layers deepen, there is a tendency for attention to focus more on the main elements. Additionally, in SynCLRs maps, there are almost no artifacts caused by high-norm tokens  [ 18 ]  that are observed in CLIPs maps, despite using the same ViT architecture. These observations qualitatively suggest that the synthetic data-driven representations acquired by SynCLR are highly different from those learned by CLIP."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  Average precision (AP) for all combinations of backbone pre-training methods (rows) in detecting fake images from different generative models (columns). CLIP is the 32nd epoch of OpenCLIP, and  CLIP  superscript CLIP \\text{CLIP}^{*} CLIP start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  represents the weights of the 31st epoch. The combination of CLIP and  CLIP  superscript CLIP \\text{CLIP}^{*} CLIP start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  in the first row is the baseline.",
        "table": "S4.T4.5.1",
        "footnotes": [],
        "references": [
            "In the previous section, qualitative analysis confirmed that synthetic data-driven representations capture different features compared to those learned using only real images. Based on this analysis, we conduct a simple ensemble learning experiment to verify whether combining backbones having synthetic data-driven representations with those having different representations can improve the performance of synthetic image detectors. For the ensemble method, we adopt feature fusion  [ 25 ,  78 ,  71 ,  37 ] , where the features are combined just before the fully connected layer, and the parameters of the fully connected layer are then trained using the combined features. Apart from adopting feature fusion, the training process is the same as described in Section  4.1 .",
            "Tables  4  and  5  present the AP and classification accuracy, respectively, for all ensemble combinations (rows) in detecting synthetic images from different generative models (columns). For classification accuracy, the numbers shown are averaged over the real and fake classes for each generative model. Additionally, as a baseline for comparison, we use an ensemble of OpenCLIP weights from the 31st and 32nd epochs."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 :  Classification accuracy for all combinations of backbone pre-training methods (rows) in detecting fake images from different generative models (columns). CLIP is the 32nd epoch of OpenCLIP, and  CLIP  superscript CLIP \\text{CLIP}^{*} CLIP start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  represents the weights of the 31st epoch. The combination of CLIP and  CLIP  superscript CLIP \\text{CLIP}^{*} CLIP start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  in the first row is the baseline.",
        "table": "S4.T5.5.1",
        "footnotes": [],
        "references": [
            "Tables  4  and  5  present the AP and classification accuracy, respectively, for all ensemble combinations (rows) in detecting synthetic images from different generative models (columns). For classification accuracy, the numbers shown are averaged over the real and fake classes for each generative model. Additionally, as a baseline for comparison, we use an ensemble of OpenCLIP weights from the 31st and 32nd epochs."
        ]
    }
}