{
    "id_table_1": {
        "caption": "Table 1.  Total number and average length (in terms of word count) for the KB articles and various Q/A pairs for the RAG implementation",
        "table": "S3.T1.1.1",
        "footnotes": [],
        "references": [
            "LLMs have recently been widely adopted across various industries, particularly in contact centers, to enhance chatbot development and agent-facing automation  (Wortmann et al . ,  2020 ; Chiang et al . ,  2024 ; Freire et al . ,  2024 ) . A prime example is the Response Prediction System (RPS), an agent-assist solution that generates contextually relevant responses, enabling agents to efficiently address customer queries with a single click. This boosts productivity, improves customer experience, and streamlines communication processes. In industry settings, the focus is on generating accurate, contextually appropriate responses with minimal latency. Therefore, RAG-based responses, grounded in company policies, deliver swift and accurate resolutions to customer issues. Figure  1  demonstrates a possible example of RPS in real settings, where the agent can directly utilize the generated response with a single click.",
            "To create a robust test set (Table  1  for details), we utilize LLM to generate both relevant question-answer pairs from the companys KB articles (refer to Section A in the Appendix for prompts). Additionally, we supplement these relevant pairs with samples from previous queries & responses in the contact center with out-of-domain questions by sampling from open-source datasets such as MS-MARCO  (Bajaj et al . ,  2018 ) .",
            "Table  9  shows the accuracy, hallucination, and missing rate for the open sources datasets (through automated evaluation as described in Subsection  4.2.1 . As observed in the Retrieval evaluation section, we notice a similar relationship in the accuracy and hallucination as document size increases. From Table  7 , we observe that the token length of the TriviaQA documents is much larger than MS-MARCO and SQuAD. Similarlt, we observed lower accuracy and higher hallucination rates with TriviaQA when compared to MS-MARCO and SQuAD.  Table  10  and  11  show performance of Chain of Thought Prompting and Chain of Verification performance on open-source datasets.  Accuracy and hallucination rate improvement vary based on the open source dataset."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.  Performance of different embedding technique in the company data for ScaNN. Performance is shown as the (%) of improvement wrt the lowest performing embedding (USE in this scenario)",
        "table": "S3.T1.1.1.3.3.1.1",
        "footnotes": [],
        "references": [
            "However, implementing RAG for industry-specific use cases to assist human agents in generating valid responses involves several architectural decisions that can affect performance and viability. The retrieval style can be integrated into both encoder-decoder  (Yu,  2022 ; Izacard et al . ,  2022 ) ) and decoder-only models  (Khandelwal et al . ,  2020 ; Borgeaud et al . ,  2022 ; Shi et al . ,  2022 ; Rubin et al . ,  2022 ) , with various embedding and prompting techniques influencing the final LLM output. In contact centers, where the risk of hallucinations is high and can critically impact business performance, ReAct (Reason+Act)  (Yao et al . ,  2023 )  prompts can help mitigate issues. Therefore, our research focuses on developing an optimal RAG based knowledge-grounded RPS for a major retail companys contact center. To ensure response accuracy, we also conduct thorough evaluations with human evaluators and automated measures, comparing RAG-based responses to human ground truth and the existing BERT-based system (Figure  2  shows an overview of traditional customer care scenario with existing and proposed system).  In short, we answer the following research questions.",
            "We assessed retriever efficiency using the Recall at K ( R  @  k R @ k R@k italic_R @ italic_k ) metric, where  K K K italic_K  represents the top 1, 3, 5, or 10 documents retrieved, measuring how well the retriever retrieves relevant documents.  The Vertex AI - textembedding-gecko@001 (768) embedding, paired with ScaNN retrieval, yielded the best outcomes. Overall, ScaNN generally outperformed KNN HNSW in most cases due to its efficient handling of large-scale datasets and superior retrieval accuracy through quantization and re-ranking techniques  (Chang et al . ,  2024 ) , so we include only the ScaNN results in Table  2 . Similarly, Vertex AI embeddings surpassed Sentence BERT and USE due to its superior ability to capture complex semantic relationships tailored for large-scale industry applications.",
            "Table  9  shows the accuracy, hallucination, and missing rate for the open sources datasets (through automated evaluation as described in Subsection  4.2.1 . As observed in the Retrieval evaluation section, we notice a similar relationship in the accuracy and hallucination as document size increases. From Table  7 , we observe that the token length of the TriviaQA documents is much larger than MS-MARCO and SQuAD. Similarlt, we observed lower accuracy and higher hallucination rates with TriviaQA when compared to MS-MARCO and SQuAD.  Table  10  and  11  show performance of Chain of Thought Prompting and Chain of Verification performance on open-source datasets.  Accuracy and hallucination rate improvement vary based on the open source dataset."
        ]
    },
    "id_table_3": {
        "caption": "Table 3.  Comparision between RAG based responses and existing BERT-based ones for automated evaluations. Values indicate the difference in percentage (%) as average of all samples",
        "table": "S3.T1.1.1.4.4.1.1",
        "footnotes": [],
        "references": [
            "An ideal golden dataset for evaluating RAG architecture (Figure  3 ) should include:",
            "We utilize the following evaluation techniques, with Table  3  illustrating our RAG LLM-based techniques performance against the current BERT-based system."
        ]
    },
    "id_table_4": {
        "caption": "Table 4.    Human evaluation comparison (% diff.) between RAG and existing BERT-based ones.",
        "table": "S3.T1.1.1.5.5.1.1",
        "footnotes": [],
        "references": [
            "For out-of-domain or trivial customer queries like Hello or Bye, document retrieval is unnecessary, as shown by 98.59% of retrieved articles having a cosine similarity score below 0.7. In contrast, 88.96% of articles retrieved for relevant company data questions scored above 0.7 (Figure  4 ). This suggests that setting the retrieval threshold at 0.7 effectively determines when retrieval is needed, thereby enhancing response generation efficiency.",
            "The results, as detailed in the Table  4 , Responses generated by the RAG model demonstrated a 45% improvement in factual accuracy and a 27% decrease in the rate of hallucinations compared to the existing model. Moreover, the human evaluators favored responses from the RAG model over the current production model 75% of the times.",
            "Table  9  shows the accuracy, hallucination, and missing rate for the open sources datasets (through automated evaluation as described in Subsection  4.2.1 . As observed in the Retrieval evaluation section, we notice a similar relationship in the accuracy and hallucination as document size increases. From Table  7 , we observe that the token length of the TriviaQA documents is much larger than MS-MARCO and SQuAD. Similarlt, we observed lower accuracy and higher hallucination rates with TriviaQA when compared to MS-MARCO and SQuAD.  Table  10  and  11  show performance of Chain of Thought Prompting and Chain of Verification performance on open-source datasets.  Accuracy and hallucination rate improvement vary based on the open source dataset."
        ]
    },
    "id_table_5": {
        "caption": "Table 5.    Comparison (% diff.) of ReAct RAG (with different values of k), CoVe and CoTP performance with respect to baseline on company data.",
        "table": "S4.T2.1.1",
        "footnotes": [],
        "references": [
            "To answer our third RQ, we utilized ReAct Tools to determine when to activate the information retrieval component within the RAG framework, while maintaining the same retrieval, embeddings, and generation strategies. We evaluated two scenarios: RAG with ReAct and RAG without ReAct, with K = 3. As shown in Table  5 . While ReAct improved the accuracy by 7% and reduced hallucination by 13.5%, it resulted in slower performance  6 , making it inconvenient in real-time conversation.",
            "We evaluated Chain of Verification (CoVe)  (Dhuliawala et al . ,  2023 )  and Chain of Thought Prompting (CoTP)  (Wei et al . ,  2023 )  to improve factual accuracy and reduce hallucinations. However, both techniques are time-consuming, requiring multiple LLM calls per query, and did not show significant improvements for the Company data. CoVe was 43% less accurate and CoTP was 3% less accurate (see Table  5 ). Therefore, we decided against using these prompting techniques."
        ]
    },
    "id_table_6": {
        "caption": "Table 6.    Latency Comparison (seconds) between ReAct RAG and non-ReAct RAG based on 10000 queries",
        "table": "S4.T3.1",
        "footnotes": [],
        "references": [
            "To answer our third RQ, we utilized ReAct Tools to determine when to activate the information retrieval component within the RAG framework, while maintaining the same retrieval, embeddings, and generation strategies. We evaluated two scenarios: RAG with ReAct and RAG without ReAct, with K = 3. As shown in Table  5 . While ReAct improved the accuracy by 7% and reduced hallucination by 13.5%, it resulted in slower performance  6 , making it inconvenient in real-time conversation."
        ]
    },
    "id_table_7": {
        "caption": "Table 7.    Open source dataset random sample statistics",
        "table": "S4.T4.1",
        "footnotes": [],
        "references": [
            "We consider three popular open source question answering datasets focusing on a varitey of topics. A random subset of  MS MARCO ( Bajaj et al . ,  2018 ) ,  SQuAD   (Rajpurkar et al . ,  2016 ) ,  TriviaQA   (Joshi et al . ,  2017 )  were considered for the evaluation. Table  7  shows the brief overview of the considered datasets.",
            "Table  9  shows the accuracy, hallucination, and missing rate for the open sources datasets (through automated evaluation as described in Subsection  4.2.1 . As observed in the Retrieval evaluation section, we notice a similar relationship in the accuracy and hallucination as document size increases. From Table  7 , we observe that the token length of the TriviaQA documents is much larger than MS-MARCO and SQuAD. Similarlt, we observed lower accuracy and higher hallucination rates with TriviaQA when compared to MS-MARCO and SQuAD.  Table  10  and  11  show performance of Chain of Thought Prompting and Chain of Verification performance on open-source datasets.  Accuracy and hallucination rate improvement vary based on the open source dataset."
        ]
    },
    "id_table_8": {
        "caption": "Table 8.    Recall@K for retrieval and embedding strategies for different data sets.",
        "table": "S4.T5.1",
        "footnotes": [],
        "references": [
            "We observed a specific trend in Recall values in lower k values (1, 3) versus higher k values (5, 10) for SQuAD and TRIVIA. For SQuAD, Vertex AI textembedding-gecko@001 (768) embedding with ScaNN retrieval performed the best at lower k but at higher k, SBERT-all-mpnet-base-v2 (768) with ScaNN performed better. For TRIVIA, SBERTall-mpnet-base-v2 (768) embedding with HNSW KNN retrieval performed the best at lower k but at higher k, SBERT-all-mpnet-base-v2 (768) with ScaNN performed better. For MSMARCO, Vertex AI -textembedding-gecko@001 (768) embedding with ScaNN retrieval combination was a clear winner. Refer Table  8  for more details."
        ]
    },
    "id_table_9": {
        "caption": "Table 9.    Generation quality metrics (%) using text-bison@001 and ScaNN",
        "table": "S4.T6.1",
        "footnotes": [],
        "references": [
            "Table  9  shows the accuracy, hallucination, and missing rate for the open sources datasets (through automated evaluation as described in Subsection  4.2.1 . As observed in the Retrieval evaluation section, we notice a similar relationship in the accuracy and hallucination as document size increases. From Table  7 , we observe that the token length of the TriviaQA documents is much larger than MS-MARCO and SQuAD. Similarlt, we observed lower accuracy and higher hallucination rates with TriviaQA when compared to MS-MARCO and SQuAD.  Table  10  and  11  show performance of Chain of Thought Prompting and Chain of Verification performance on open-source datasets.  Accuracy and hallucination rate improvement vary based on the open source dataset."
        ]
    },
    "id_table_10": {
        "caption": "Table 10.    Baseline vs CoTP evaluation metrics (%) without retrieval (question-doc. pair used as it is)",
        "table": "A1.T7.1.1",
        "footnotes": [],
        "references": [
            "Table  9  shows the accuracy, hallucination, and missing rate for the open sources datasets (through automated evaluation as described in Subsection  4.2.1 . As observed in the Retrieval evaluation section, we notice a similar relationship in the accuracy and hallucination as document size increases. From Table  7 , we observe that the token length of the TriviaQA documents is much larger than MS-MARCO and SQuAD. Similarlt, we observed lower accuracy and higher hallucination rates with TriviaQA when compared to MS-MARCO and SQuAD.  Table  10  and  11  show performance of Chain of Thought Prompting and Chain of Verification performance on open-source datasets.  Accuracy and hallucination rate improvement vary based on the open source dataset."
        ]
    },
    "id_table_11": {
        "caption": "Table 11.    Baseline vs CoVe evaluation metrics (%) without retrieval (question-doc. pair used as it is)",
        "table": "A1.T8.1.1",
        "footnotes": [],
        "references": [
            "Table  9  shows the accuracy, hallucination, and missing rate for the open sources datasets (through automated evaluation as described in Subsection  4.2.1 . As observed in the Retrieval evaluation section, we notice a similar relationship in the accuracy and hallucination as document size increases. From Table  7 , we observe that the token length of the TriviaQA documents is much larger than MS-MARCO and SQuAD. Similarlt, we observed lower accuracy and higher hallucination rates with TriviaQA when compared to MS-MARCO and SQuAD.  Table  10  and  11  show performance of Chain of Thought Prompting and Chain of Verification performance on open-source datasets.  Accuracy and hallucination rate improvement vary based on the open source dataset."
        ]
    },
    "id_table_12": {
        "caption": "",
        "table": "A1.T9.1",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "",
        "table": "A1.T10.1",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "A1.T11.1",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": []
}