{
    "id_table_1": {
        "caption": "Table 1:  Critic training data (QU-MTC) statistics. Q= and U= denotes number of instances sampled from QReCC and UltraChat, respectively.",
        "table": "S3.T1.1.1",
        "footnotes": [],
        "references": [
            "Thus, understanding  if retrieval is necessary  for high-quality response generation is an important research question, especially in the context of conversational QA. In multi-turn question answering, comprehending users contextual intent and generating responses pose significant challenges due to complexities introduced by the extended context window containing previous user interactions  Aliannejadi et al. ( 2020 ); Mao et al. ( 2023b ); Wu et al. ( 2024 ) .  The system, when deciding whether to retrieve or estimating the usefulness of its own response, must process a longer context of the conversation history, understand the user intent of current turn, ensure prevention of information repetition, maintain user engagement, etc. An example is provided in Figure  1 , which shows that the decision to retrieve or not might depend on the conversational context rather than last turn only.",
            "In a multi-turn context, evaluating the relevance of a retrieved passage involves considering both the current question and the previous conversation history, unlike a single-turn question. Furthermore, retrieved passages from previous turns can also be provided in the conversational context or a user may simply provide a passage and ask questions based on that. When the cost of retrieval is high, a model should  not  decide to retrieve if the question in the current turn can be answered from passages retrieved in previous turns (as shown in Figure  1 ). In that case, the model should not only comprehend the conversational context but also the previously retrieved passages to be able to determine the necessity of retrieval. Utilizing already retrieved passages (given they are relevant) not only mitigates the harmful effects of noisy retrieval  Oh and Thorne ( 2023 ) , but also saves costs by reducing the number of context tokens  Kulkarni et al. ( 2024 ) .",
            "The task of the critic model is to output the special reflection tokens given a conversational context (as compared to a single question like in the original  SELF-RAG  framework). We employ the five critic tasks introduced by  Asai et al. ( 2023 ) . However, we redesign the framework to include a conversational history instead of a single-turn question. The important distinction of our approach is that it teaches the critic model to judge whether retrieval is needed or not, and relevance of retrieved documents based on the entire conversation history. The critic tasks and special tokens are shown in Table  1 .",
            "The training data for our critic and generator models are sampled from QReCC and UltraChat. We employ GPT-4 to collect the labels for training data for critic model. The prompt for collecting GPT-4 labels are provided in the Appendix  A . We denote this training data as QReCC-UltraChat Multi-turn Critic Data (QU-MTC), details of which are provided in Table  1 .  Furthermore, we also create a single-turn variant (QU-STC) by flattening the conversation history to a single-turn using a T5 based query rewriter T5QR  Lin et al. ( 2020 ) . Thus QU-MTC and QU-STC come from the same data distribution, with the difference being the representation of the conversation history. Lastly, we also use the  original  single-turn critic training dataset, STC, released by  Asai et al. ( 2023 ) .",
            "We first evaluate critic performance on the self-reflection tasks by calculating the accuracy of the predicted tokens described in Table  1 . We use held out test split of STC and QU-MTC for evaluating critic. To measure the quality of the responses of the generator models, we employ both automatic metrics and human annotations. We collect GPT-4 and human evaluation scores that rate the responses generated by the models on a scale of 1-5 that includes different dimensions such coherence, understandability, and overall quality. The prompt for GPT-4 evaluation is provided in Appendix  D . We employ BERTScore  Zhang et al. ( 2019 )  to measure similarity of generated response with ground truth response. To measure coherence of response given the conversation history and grounding of response given retrieved documents, we employ UniEval  Zhong et al. ( 2022 ) .",
            "Prompts to collect critic training data for the different tasks using GPT-4 are outlined in Table  8  ( Retrieval ), Table  9  ( 3-way Retrieval ), Table  10  ( Relevance ), Table  11  ( Groundedness ), Table  12  ( Utility ), We modify the prompts of  Asai et al. ( 2023 )  to be compatible with multi-turn dialogues (as compared to single-turn QA). Furthermore, we collect conversation summarization training data for our generator model using GPT-4 prompt detailed in Table  13 .",
            "While QReCC is traditionally used as the primary benchmark for evaluating conversational query rewriting models or response generation, we note a few differences with UltraChat and MT-Eval. Firstly, as mentioned earlier, responses to all conversation turns are grounded in passage from Wikipedia/Common Crawl. Hence, ideally retrieval should be needed at every turn, and hence RAG should lead to the best response. Whereas in the other two datasets there are examples where a passage is already included in a conversation turn or ones where the response to a turn should be based on response generated in previous turns or conversations where users ask creative questions (examples provided in Table  14 ). Hence, in these two datasets the decision to call retrieval is more important where a model should understand from the context whether retrieval is needed or not. Secondly, questions and gold responses in QReCC are an order of magnitude shorter than UltraChat as shown in Table  14 ).",
            "We employ GPT-4 to evaluate the responses generated from the different SELF-RAG models. We set temperature to be  1.0 1.0 1.0 1.0  and maximum number of generated tokens ( max_tokens ) to be  512 512 512 512 . The prompt for GPT-4 evaluation is detailed in Table  15 .",
            "We conduct human evaluations using Amazon Mechanical Turk to determine the quality of the generated answers based on coherence, engagingness and understandability. We give annotators fair compensation. We also use a bonus incentive structure. Every worker who passes the automatic quality checks receives a bonus at the end. In addition, we only consider workers from a country whose main language is English, who has completed 100 or more HITs so far with an acceptance rate of 95% or higher. Figure  5  shows the template for evaluating the generated response with respect to the question and conversational history. In Table  16 , we provide the scores of generated response along the dimensions of coherence, engagingness and understandability as rate by the human annotators.",
            "In Table  17 , we provide an example where  SELF - multi - RAG  comprehends longer conversational contexts better than its single-turn counterpart."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Critic performance on self-reflection tasks on the test splits of each dataset. We report classification accuracy of predicting the correct special tokens for each task as outlined in Table  1 . For the  Retrieval  task, we evaluate critic accuracy, either with passages in the conversation history ( Ret.  w P.) or without ( Ret.  w/o P.).",
        "table": "S4.T2.8",
        "footnotes": [],
        "references": [
            "Furthermore, when retrieval is expensive or noisy, it is beneficial to utilize already retrieved documents in the conversation, given they are relevant and contain the necessary answers. Additionally, detrimental context as a result of noisy retrieval can degrade response generation quality  Shi et al. ( 2023 ); Oh and Thorne ( 2023 ) . Lastly, when conversation history is longer, traditional conversational query rewriting methods  Anantha et al. ( 2020 ); Ishii et al. ( 2022 ); Ye et al. ( 2023 )  that typically emphasize co-reference resolution, might be insufficient to contain all the information required for an effective retrieval  Bai et al. ( 2024 ) . As shown in Figure  2 , both the gold and a T5-based query rewriting miss potentially important signals (e.g., hip injury) for retrieving correct passages. In that case, representing the conversation history in a summarized form can lead to more effective retrieval.",
            "Representing conversational context using a single question is difficult  Anantha et al. ( 2020 )  and might result in loss of information while retrieving relevant passages (as shown in Figure  2 ). This is especially true in case of long conversations. Traditional conversation query rewriting methods  Ye et al. ( 2023 ); Mo et al. ( 2023 ); Lin et al. ( 2020 )  are trained to select important parts of the conversation but, typically, in a single question format. Hence, we hypothesize that summarising a conversational context can potentially include more relevant signals when retrieving passages without adding noise. This can be beneficial for both sparse and bi-encoder based dense retrieval.",
            "Table  2  shows that the critic model trained on both single and multi-turn data Critic sm  has overall the best accuracy (generating  correct  reflection tokens) on the critic tasks (based on GPT 4 labels), even improving in the single-turn setting. This suggests that Critic sm  is better at handling longer context of conversations while judging whether retrieval is needed or not, judging relevance of retrieved passages, and utility of an answer.",
            "Returning to Table  2 , we also explore the value of previously retrieved passages into the context history of the input for the critic model in the  Retrieval  critic task. We evaluate critic accuracy, either with previous retrieved passages in the conversation history ( Ret.  w P.) or without ( Ret.  w/o P.). For QReCC, we include ground truth passages of previous turn in the context. For UltraChat, we sample instances where passages are present as part of a question in a conversation. As shown in Table  2 , Critic sm  performs the best in  Ret.  w P., where the model has to judge whether retrieval is needed or not given both the conversation history and passages retrieved in previous turn. This indicates superior ability of Critic sm  to comprehend not only the conversation history but also previously retrieved passages to deem the necessity of retrieval in a multi-turn setting. Overall, the critic models have lower performance in this task compared to the  Retrieval without passages  ( Ret.  w/o P.) indicating the increased difficulty of the task.",
            "Prompts to collect critic training data for the different tasks using GPT-4 are outlined in Table  8  ( Retrieval ), Table  9  ( 3-way Retrieval ), Table  10  ( Relevance ), Table  11  ( Groundedness ), Table  12  ( Utility ), We modify the prompts of  Asai et al. ( 2023 )  to be compatible with multi-turn dialogues (as compared to single-turn QA). Furthermore, we collect conversation summarization training data for our generator model using GPT-4 prompt detailed in Table  13 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Configurations explored as training data for our approaches; ST = single-turn; MT = multi-turn; C = Critic, G = Generator. CSD refers to the conversation summarization dataset.",
        "table": "S4.T3.9",
        "footnotes": [],
        "references": [
            "SELF - multi - RAG  is thus capable of comprehending longer conversational contexts (for the various critic tasks) than  SELF-RAG  and also summarizing the conversational context that when used as a query can improve retrieval effectiveness. The framework is depicted  in Figure  3 .",
            "We train our own critic models from scratch using  mistralai/Mistral-7B-Instruct-v0.2  as initial checkpoint. The different versions of critic models are outlined in Table  3 . Critic o  is equivalent to the original critic model trained by  Asai et al. ( 2023 )  that is sampled from a number of single-turn benchmarks. However, to understand the impact of training in the conversational setting, Critic sm  must be compared with Critic s  as they are trained on the same data distribution.",
            "We also train our generator model from the  mistralai/Mistral-7B-Instruct-v0.2  checkpoint. The different versions of generator models, that we use to compare the performance of  SELF - multi - RAG  with, are also outlined in Table  3 .  Henceforth, we use  SELF - multi - RAG  to refer to the final model that is trained end-to-end on single-turn, multi-turn conversation data and also trained to summarise conversational context and compare its performance against the other generator models.",
            "Prompts to collect critic training data for the different tasks using GPT-4 are outlined in Table  8  ( Retrieval ), Table  9  ( 3-way Retrieval ), Table  10  ( Relevance ), Table  11  ( Groundedness ), Table  12  ( Utility ), We modify the prompts of  Asai et al. ( 2023 )  to be compatible with multi-turn dialogues (as compared to single-turn QA). Furthermore, we collect conversation summarization training data for our generator model using GPT-4 prompt detailed in Table  13 ."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Performance of response generation models on the three dataset. QR = rewritten conversation history as context; FC = full conversation history as context.    indicates adaptive retrieval;  -  indicates no retrieval.    We measure groundness with retrieved documents only for the cases when the model decides to call retrieval.",
        "table": "S4.T4.16",
        "footnotes": [],
        "references": [
            "We conduct extensive experiments to observe that response quality of  SELF - multi - RAG  significantly outperforms  SELF-RAG  with an average improvement of   similar-to \\sim  13% for conversational datasets, measured by human annotations in Table  4 . Moreover,  SELF - multi - RAG  summarization capabilities improves the retrieval effectiveness by 13.5% on average (R@5), compared to query rewriting baselines (Table  5 ).",
            "As shown in Table  4 ,  SELF - multi - RAG , trained on data created by Critic sm , leads to improvement on all conversational benchmarks according to the evaluation metrics. Comparing its performance with  SELF-RAG s , that has been trained on data from the same dataset but using single-turn contexts,  SELF - multi - RAG  performs better in comprehending the conversational context. The decision of whether retrieval is required is more accurate for  SELF - multi - RAG  as compared to the baselines. As evidence, we see Critic sm  has higher accuracy on retrieval tasks (both with and without passages included in the conversation history) than Critic s . Figure  4  shows that  SELF - multi - RAG  decides to call retrieval   100 similar-to absent 100 \\sim 100  100 % of time for QReCC, however not so for UltraChat. This is the expected behaviour as conversations in QReCC are mostly knowledge grounded, whereas in UltraChat there are more instructional conversations that not always require retrieved knowledge. This suggests that the decision to call retrieval or not is indeed important for conversational QA and adapting the model to better handle conversational context is beneficial. Figure  4  further shows that  SELF - multi - RAG  generated responses are better at all turns (upto 6) of the conversations further providing evidence to its ability to understand long conversational context. Some of the cases where  SELF - multi - RAG  called retrieval and could not provide a satisfactory answer is typically the cases where it could not find relevant answers within the retrieved documents. Groundedness of the generated response to the conversational context and retrieved passages, as measured using UniEval, is also higher for  SELF - multi - RAG  as compared to its single-turn counterparts. Lastly, we see  SELF - multi - RAG  perform the best on MT-Eval, indicating strong performance on held-out conversational benchmarks.",
            "While QReCC is traditionally used as the primary benchmark for evaluating conversational query rewriting models or response generation, we note a few differences with UltraChat and MT-Eval. Firstly, as mentioned earlier, responses to all conversation turns are grounded in passage from Wikipedia/Common Crawl. Hence, ideally retrieval should be needed at every turn, and hence RAG should lead to the best response. Whereas in the other two datasets there are examples where a passage is already included in a conversation turn or ones where the response to a turn should be based on response generated in previous turns or conversations where users ask creative questions (examples provided in Table  14 ). Hence, in these two datasets the decision to call retrieval is more important where a model should understand from the context whether retrieval is needed or not. Secondly, questions and gold responses in QReCC are an order of magnitude shorter than UltraChat as shown in Table  14 )."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Retrieval effectiveness of different conversational context representation. GPT-4 summaries are the ground truth summaries that we collect for training  SELF - multi - RAG  for the summarization task.",
        "table": "S5.T5.1",
        "footnotes": [],
        "references": [
            "We conduct extensive experiments to observe that response quality of  SELF - multi - RAG  significantly outperforms  SELF-RAG  with an average improvement of   similar-to \\sim  13% for conversational datasets, measured by human annotations in Table  4 . Moreover,  SELF - multi - RAG  summarization capabilities improves the retrieval effectiveness by 13.5% on average (R@5), compared to query rewriting baselines (Table  5 ).",
            "Since QReCC provides ground truth labels of relevant passages (to a conversational context), we use it to evaluate retrieval effectiveness of different representations of the conversational context. Table  5  shows that summaries generated by our approach perform better than rewrites in the form of single-questions (using T5QR) in case of both sparse and dense retrievals. This is in line with research that show expanding queries and documents help in improving retrieval effectiveness  Ayoub et al. ( 2024 ); Mackie et al. ( 2023 ); Nogueira et al. ( 2019 ) .",
            "We employ GPT-4 to evaluate the responses generated from the different SELF-RAG models. We set temperature to be  1.0 1.0 1.0 1.0  and maximum number of generated tokens ( max_tokens ) to be  512 512 512 512 . The prompt for GPT-4 evaluation is detailed in Table  15 .",
            "We conduct human evaluations using Amazon Mechanical Turk to determine the quality of the generated answers based on coherence, engagingness and understandability. We give annotators fair compensation. We also use a bonus incentive structure. Every worker who passes the automatic quality checks receives a bonus at the end. In addition, we only consider workers from a country whose main language is English, who has completed 100 or more HITs so far with an acceptance rate of 95% or higher. Figure  5  shows the template for evaluating the generated response with respect to the question and conversational history. In Table  16 , we provide the scores of generated response along the dimensions of coherence, engagingness and understandability as rate by the human annotators."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Response generation quality of  SELF - multi - RAG  with different conversation history representations as query to retrieve relevant passages.",
        "table": "S5.T6.1",
        "footnotes": [],
        "references": [
            "To better understand observed superior performance of  SELF - multi - RAG  as compared to its other variants, we perform ablations to narrow down the causes of gain. Table  6  reports the performance of different conversation history representations as query to retrieve relevant passages. Note that when we use the  SELF - multi - RAG  generated summary as a query to the retrieval model, the response generation is the best for both datasets. Other forms of conversation context as query representations (e.g., T5QR) have lower performance. Overall,  SELF - multi - RAG  improves in two directions, (i) it generates summaries as query with better retrieval effectiveness, and (ii) enhances response generation quality taking into account more suitable retrieval knowledge and conversational context.",
            "We conduct human evaluations using Amazon Mechanical Turk to determine the quality of the generated answers based on coherence, engagingness and understandability. We give annotators fair compensation. We also use a bonus incentive structure. Every worker who passes the automatic quality checks receives a bonus at the end. In addition, we only consider workers from a country whose main language is English, who has completed 100 or more HITs so far with an acceptance rate of 95% or higher. Figure  5  shows the template for evaluating the generated response with respect to the question and conversational history. In Table  16 , we provide the scores of generated response along the dimensions of coherence, engagingness and understandability as rate by the human annotators."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Comparison of response generation performance with and without passages included in the context. Ret? = % of instances retrieval was used.",
        "table": "S6.T7.6.6",
        "footnotes": [],
        "references": [
            "Table  7  compares response generation performance with and without retrieved passages from previous turns are included in the input together with the conversation history. We observe that  SELF - multi - RAG  not only (correctly) decides to call retrieval less number of times, as compared to its single-turn baseline, but is also better at generating responses when the conversation context is composed of both the dialogue and previously retrieved passages indicating its ability to comprehend more complex contexts. Moreover, either configuration ( Ret.  w P.,  Ret.  w/o P.) can be chosen based on the desired balance between efficiency and accuracy;  Ret.  w/o P. is useful when performance is prioritized over efficiency, whereas  Ret.  w/o P. is suitable when efficiency is crucial and a slight reduction in performance is acceptable. We present examples when passages are included in the context in the Appendix  F .",
            "In Table  17 , we provide an example where  SELF - multi - RAG  comprehends longer conversational contexts better than its single-turn counterpart."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  GPT-4 prompt for collecting training data for Retrieval critic task.",
        "table": "A1.T8.1",
        "footnotes": [],
        "references": [
            "Prompts to collect critic training data for the different tasks using GPT-4 are outlined in Table  8  ( Retrieval ), Table  9  ( 3-way Retrieval ), Table  10  ( Relevance ), Table  11  ( Groundedness ), Table  12  ( Utility ), We modify the prompts of  Asai et al. ( 2023 )  to be compatible with multi-turn dialogues (as compared to single-turn QA). Furthermore, we collect conversation summarization training data for our generator model using GPT-4 prompt detailed in Table  13 ."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  GPT-4 prompt for collecting training data for Multi-Retrieval critic task.",
        "table": "A1.T9.1",
        "footnotes": [],
        "references": [
            "Prompts to collect critic training data for the different tasks using GPT-4 are outlined in Table  8  ( Retrieval ), Table  9  ( 3-way Retrieval ), Table  10  ( Relevance ), Table  11  ( Groundedness ), Table  12  ( Utility ), We modify the prompts of  Asai et al. ( 2023 )  to be compatible with multi-turn dialogues (as compared to single-turn QA). Furthermore, we collect conversation summarization training data for our generator model using GPT-4 prompt detailed in Table  13 ."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:  GPT-4 prompt for collecting training data for Relevance critic task.",
        "table": "A1.T10.1",
        "footnotes": [],
        "references": [
            "Prompts to collect critic training data for the different tasks using GPT-4 are outlined in Table  8  ( Retrieval ), Table  9  ( 3-way Retrieval ), Table  10  ( Relevance ), Table  11  ( Groundedness ), Table  12  ( Utility ), We modify the prompts of  Asai et al. ( 2023 )  to be compatible with multi-turn dialogues (as compared to single-turn QA). Furthermore, we collect conversation summarization training data for our generator model using GPT-4 prompt detailed in Table  13 ."
        ]
    },
    "id_table_11": {
        "caption": "Table 11:  GPT-4 prompt for collecting training data for Groundedness critic task.",
        "table": "A1.T11.1",
        "footnotes": [],
        "references": [
            "Prompts to collect critic training data for the different tasks using GPT-4 are outlined in Table  8  ( Retrieval ), Table  9  ( 3-way Retrieval ), Table  10  ( Relevance ), Table  11  ( Groundedness ), Table  12  ( Utility ), We modify the prompts of  Asai et al. ( 2023 )  to be compatible with multi-turn dialogues (as compared to single-turn QA). Furthermore, we collect conversation summarization training data for our generator model using GPT-4 prompt detailed in Table  13 ."
        ]
    },
    "id_table_12": {
        "caption": "Table 12:  GPT-4 prompt for collecting training data for Utility critic task.",
        "table": "A1.T12.1",
        "footnotes": [],
        "references": [
            "Prompts to collect critic training data for the different tasks using GPT-4 are outlined in Table  8  ( Retrieval ), Table  9  ( 3-way Retrieval ), Table  10  ( Relevance ), Table  11  ( Groundedness ), Table  12  ( Utility ), We modify the prompts of  Asai et al. ( 2023 )  to be compatible with multi-turn dialogues (as compared to single-turn QA). Furthermore, we collect conversation summarization training data for our generator model using GPT-4 prompt detailed in Table  13 ."
        ]
    },
    "id_table_13": {
        "caption": "Table 13:  GPT-4 prompt for collecting generator training data for conversation summarization.",
        "table": "A1.T13.1",
        "footnotes": [],
        "references": [
            "Prompts to collect critic training data for the different tasks using GPT-4 are outlined in Table  8  ( Retrieval ), Table  9  ( 3-way Retrieval ), Table  10  ( Relevance ), Table  11  ( Groundedness ), Table  12  ( Utility ), We modify the prompts of  Asai et al. ( 2023 )  to be compatible with multi-turn dialogues (as compared to single-turn QA). Furthermore, we collect conversation summarization training data for our generator model using GPT-4 prompt detailed in Table  13 ."
        ]
    },
    "id_table_14": {
        "caption": "Table 14:  Statistics and examples for the three datasets considered in this study.",
        "table": "A2.T14.1.1",
        "footnotes": [],
        "references": [
            "While QReCC is traditionally used as the primary benchmark for evaluating conversational query rewriting models or response generation, we note a few differences with UltraChat and MT-Eval. Firstly, as mentioned earlier, responses to all conversation turns are grounded in passage from Wikipedia/Common Crawl. Hence, ideally retrieval should be needed at every turn, and hence RAG should lead to the best response. Whereas in the other two datasets there are examples where a passage is already included in a conversation turn or ones where the response to a turn should be based on response generated in previous turns or conversations where users ask creative questions (examples provided in Table  14 ). Hence, in these two datasets the decision to call retrieval is more important where a model should understand from the context whether retrieval is needed or not. Secondly, questions and gold responses in QReCC are an order of magnitude shorter than UltraChat as shown in Table  14 )."
        ]
    },
    "id_table_15": {
        "caption": "Table 15:  Prompt for GPT-4 evaluation of generate responses.",
        "table": "A4.T15.1",
        "footnotes": [],
        "references": [
            "We employ GPT-4 to evaluate the responses generated from the different SELF-RAG models. We set temperature to be  1.0 1.0 1.0 1.0  and maximum number of generated tokens ( max_tokens ) to be  512 512 512 512 . The prompt for GPT-4 evaluation is detailed in Table  15 ."
        ]
    },
    "id_table_16": {
        "caption": "Table 16:  Detailed result of human annotation scores along different dimensions of generated responses by different models.",
        "table": "A5.T16.15",
        "footnotes": [],
        "references": [
            "We conduct human evaluations using Amazon Mechanical Turk to determine the quality of the generated answers based on coherence, engagingness and understandability. We give annotators fair compensation. We also use a bonus incentive structure. Every worker who passes the automatic quality checks receives a bonus at the end. In addition, we only consider workers from a country whose main language is English, who has completed 100 or more HITs so far with an acceptance rate of 95% or higher. Figure  5  shows the template for evaluating the generated response with respect to the question and conversational history. In Table  16 , we provide the scores of generated response along the dimensions of coherence, engagingness and understandability as rate by the human annotators."
        ]
    },
    "id_table_17": {
        "caption": "Table 17:  Examples of  SELF - multi - RAG  improvements in conversational QA over  SELF-RAG s .",
        "table": "A6.T17.1",
        "footnotes": [],
        "references": [
            "In Table  17 , we provide an example where  SELF - multi - RAG  comprehends longer conversational contexts better than its single-turn counterpart."
        ]
    },
    "global_footnotes": [
        "The corpus is released as part of the QReCC dataset.",
        "We use default values defined by",
        ".",
        "The prompt for GPT-4 summary collection is provided in Appendix",
        "."
    ]
}