{
    "PAPER'S NUMBER OF TABLES": 14,
    "S7.T1": {
        "caption": "TABLE I: Extractability with Random Initializations. Impact of random initialization functions on the extraction-precision (P) and extraction-recall (R) of individual training data points from the model gradients. The displayed numbers refer to a mini-batch of 100 data points and 1000 neurons for extraction in the first model layer (FC-NN architecture from Table X). Results are averaged over 10 runs with different random initializations.",
        "table": "<table id=\"S7.T1.5\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S7.T1.5.6\" class=\"ltx_tr\">\n<td id=\"S7.T1.5.6.1\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"S7.T1.5.6.2\" class=\"ltx_td ltx_align_left ltx_border_tt\" colspan=\"2\"><span id=\"S7.T1.5.6.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">MNIST</span></td>\n<td id=\"S7.T1.5.6.3\" class=\"ltx_td ltx_align_left ltx_border_tt\" colspan=\"2\"><span id=\"S7.T1.5.6.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">CIFAR10</span></td>\n<td id=\"S7.T1.5.6.4\" class=\"ltx_td ltx_align_left ltx_border_tt\" colspan=\"2\"><span id=\"S7.T1.5.6.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">ImageNet</span></td>\n</tr>\n<tr id=\"S7.T1.5.7\" class=\"ltx_tr\">\n<td id=\"S7.T1.5.7.1\" class=\"ltx_td ltx_align_left\"><span id=\"S7.T1.5.7.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Weights Initializer</span></td>\n<td id=\"S7.T1.5.7.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.5.7.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">P</span></td>\n<td id=\"S7.T1.5.7.3\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.5.7.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">R</span></td>\n<td id=\"S7.T1.5.7.4\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.5.7.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">P</span></td>\n<td id=\"S7.T1.5.7.5\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.5.7.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">R</span></td>\n<td id=\"S7.T1.5.7.6\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.5.7.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">P</span></td>\n<td id=\"S7.T1.5.7.7\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.5.7.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">R</span></td>\n</tr>\n<tr id=\"S7.T1.5.8\" class=\"ltx_tr\">\n<td id=\"S7.T1.5.8.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S7.T1.5.8.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Xavier Normal</span></td>\n<td id=\"S7.T1.5.8.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T1.5.8.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.004</span></td>\n<td id=\"S7.T1.5.8.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T1.5.8.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">.037</span></td>\n<td id=\"S7.T1.5.8.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T1.5.8.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.048</span></td>\n<td id=\"S7.T1.5.8.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T1.5.8.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">.203</span></td>\n<td id=\"S7.T1.5.8.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T1.5.8.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">.046</span></td>\n<td id=\"S7.T1.5.8.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T1.5.8.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">.213</span></td>\n</tr>\n<tr id=\"S7.T1.5.9\" class=\"ltx_tr\">\n<td id=\"S7.T1.5.9.1\" class=\"ltx_td ltx_align_left\"><span id=\"S7.T1.5.9.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Xavier Uniform</span></td>\n<td id=\"S7.T1.5.9.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.5.9.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.005</span></td>\n<td id=\"S7.T1.5.9.3\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.5.9.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">.048</span></td>\n<td id=\"S7.T1.5.9.4\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.5.9.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.053</span></td>\n<td id=\"S7.T1.5.9.5\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.5.9.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">.229</span></td>\n<td id=\"S7.T1.5.9.6\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.5.9.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">.040</span></td>\n<td id=\"S7.T1.5.9.7\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.5.9.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">.201</span></td>\n</tr>\n<tr id=\"S7.T1.1.1\" class=\"ltx_tr\">\n<td id=\"S7.T1.1.1.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S7.T1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Gaussian (</span><math id=\"S7.T1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sigma\" display=\"inline\"><semantics id=\"S7.T1.1.1.1.m1.1a\"><mi mathsize=\"70%\" id=\"S7.T1.1.1.1.m1.1.1\" xref=\"S7.T1.1.1.1.m1.1.1.cmml\">σ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S7.T1.1.1.1.m1.1b\"><ci id=\"S7.T1.1.1.1.m1.1.1.cmml\" xref=\"S7.T1.1.1.1.m1.1.1\">𝜎</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T1.1.1.1.m1.1c\">\\sigma</annotation></semantics></math><span id=\"S7.T1.1.1.1.2\" class=\"ltx_text\" style=\"font-size:70%;\">=0.01)</span>\n</td>\n<td id=\"S7.T1.1.1.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.005</span></td>\n<td id=\"S7.T1.1.1.3\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">.048</span></td>\n<td id=\"S7.T1.1.1.4\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.1.1.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.051</span></td>\n<td id=\"S7.T1.1.1.5\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.1.1.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">.226</span></td>\n<td id=\"S7.T1.1.1.6\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.1.1.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">.041</span></td>\n<td id=\"S7.T1.1.1.7\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.1.1.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">.203</span></td>\n</tr>\n<tr id=\"S7.T1.2.2\" class=\"ltx_tr\">\n<td id=\"S7.T1.2.2.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S7.T1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Gaussian (</span><math id=\"S7.T1.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sigma\" display=\"inline\"><semantics id=\"S7.T1.2.2.1.m1.1a\"><mi mathsize=\"70%\" id=\"S7.T1.2.2.1.m1.1.1\" xref=\"S7.T1.2.2.1.m1.1.1.cmml\">σ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S7.T1.2.2.1.m1.1b\"><ci id=\"S7.T1.2.2.1.m1.1.1.cmml\" xref=\"S7.T1.2.2.1.m1.1.1\">𝜎</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T1.2.2.1.m1.1c\">\\sigma</annotation></semantics></math><span id=\"S7.T1.2.2.1.2\" class=\"ltx_text\" style=\"font-size:70%;\">=0.1)</span>\n</td>\n<td id=\"S7.T1.2.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.2.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.005</span></td>\n<td id=\"S7.T1.2.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.2.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">.049</span></td>\n<td id=\"S7.T1.2.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.2.2.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.053</span></td>\n<td id=\"S7.T1.2.2.5\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.2.2.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">.238</span></td>\n<td id=\"S7.T1.2.2.6\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.2.2.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">.043</span></td>\n<td id=\"S7.T1.2.2.7\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.2.2.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">.209</span></td>\n</tr>\n<tr id=\"S7.T1.3.3\" class=\"ltx_tr\">\n<td id=\"S7.T1.3.3.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S7.T1.3.3.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Gaussian (</span><math id=\"S7.T1.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sigma\" display=\"inline\"><semantics id=\"S7.T1.3.3.1.m1.1a\"><mi mathsize=\"70%\" id=\"S7.T1.3.3.1.m1.1.1\" xref=\"S7.T1.3.3.1.m1.1.1.cmml\">σ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S7.T1.3.3.1.m1.1b\"><ci id=\"S7.T1.3.3.1.m1.1.1.cmml\" xref=\"S7.T1.3.3.1.m1.1.1\">𝜎</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T1.3.3.1.m1.1c\">\\sigma</annotation></semantics></math><span id=\"S7.T1.3.3.1.2\" class=\"ltx_text\" style=\"font-size:70%;\">=0.5)</span>\n</td>\n<td id=\"S7.T1.3.3.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.3.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.006</span></td>\n<td id=\"S7.T1.3.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.3.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">.050</span></td>\n<td id=\"S7.T1.3.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.3.3.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.058</span></td>\n<td id=\"S7.T1.3.3.5\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.3.3.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">.255</span></td>\n<td id=\"S7.T1.3.3.6\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.3.3.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">.044</span></td>\n<td id=\"S7.T1.3.3.7\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.3.3.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.218</span></td>\n</tr>\n<tr id=\"S7.T1.4.4\" class=\"ltx_tr\">\n<td id=\"S7.T1.4.4.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S7.T1.4.4.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Gaussian (</span><math id=\"S7.T1.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sigma\" display=\"inline\"><semantics id=\"S7.T1.4.4.1.m1.1a\"><mi mathsize=\"70%\" id=\"S7.T1.4.4.1.m1.1.1\" xref=\"S7.T1.4.4.1.m1.1.1.cmml\">σ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S7.T1.4.4.1.m1.1b\"><ci id=\"S7.T1.4.4.1.m1.1.1.cmml\" xref=\"S7.T1.4.4.1.m1.1.1\">𝜎</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T1.4.4.1.m1.1c\">\\sigma</annotation></semantics></math><span id=\"S7.T1.4.4.1.2\" class=\"ltx_text\" style=\"font-size:70%;\">=1)</span>\n</td>\n<td id=\"S7.T1.4.4.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.4.4.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.006</span></td>\n<td id=\"S7.T1.4.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.4.4.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">.059</span></td>\n<td id=\"S7.T1.4.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.4.4.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.058</span></td>\n<td id=\"S7.T1.4.4.5\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.4.4.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">.256</span></td>\n<td id=\"S7.T1.4.4.6\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.4.4.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">.045</span></td>\n<td id=\"S7.T1.4.4.7\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T1.4.4.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.218</span></td>\n</tr>\n<tr id=\"S7.T1.5.5\" class=\"ltx_tr\">\n<td id=\"S7.T1.5.5.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">\n<span id=\"S7.T1.5.5.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Gaussian (</span><math id=\"S7.T1.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sigma\" display=\"inline\"><semantics id=\"S7.T1.5.5.1.m1.1a\"><mi mathsize=\"70%\" id=\"S7.T1.5.5.1.m1.1.1\" xref=\"S7.T1.5.5.1.m1.1.1.cmml\">σ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S7.T1.5.5.1.m1.1b\"><ci id=\"S7.T1.5.5.1.m1.1.1.cmml\" xref=\"S7.T1.5.5.1.m1.1.1\">𝜎</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T1.5.5.1.m1.1c\">\\sigma</annotation></semantics></math><span id=\"S7.T1.5.5.1.2\" class=\"ltx_text\" style=\"font-size:70%;\">=2)</span>\n</td>\n<td id=\"S7.T1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T1.5.5.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.007</span></td>\n<td id=\"S7.T1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T1.5.5.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.061</span></td>\n<td id=\"S7.T1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T1.5.5.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.058</span></td>\n<td id=\"S7.T1.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T1.5.5.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.259</span></td>\n<td id=\"S7.T1.5.5.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T1.5.5.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">.047</span></td>\n<td id=\"S7.T1.5.5.7\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T1.5.5.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">.217</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Attacker without Auxiliary Data.\nWe experiment with an attacker who does not have access to a small mini-batch of data from the users’ distribution to tune the scaling factor s𝑠s of our trap weights.\nIn this setup, the only knowledge an attacker holds is about the dimensionality of the users’ data which it needs to instantiate an adequate model architecture.\nWe evaluate three attacks in this setup.\n1) Exploiting passive data leakage and composing a tuning dataset: the attacker randomly initializes the model in a first round of the protocol.\nOur results in Table I show that also randomly initialized models’ gradients leak significant fractions of the users’ data (MNIST 6.1%, CIFAR10 25.9%, and ImageNet 21.7%).\nBy plotting the user’s gradients and eyeballing which data points resemble natural images, the attacker can build a tuning set for s𝑠s.\nSince we only require a maximum of 100 data points to find the optimal values for s𝑠s per dataset in Table IV, the attacker only has to inspect the gradients of 17, 4, and 5 users for MNIST, CIFAR10, and ImageNet, respectively in the first round of the protocol.\nOn the selected data, they can tune s𝑠s and use it in every subsequent iteration.\nWe performed tuning on 100 data points obtained through passive extraction and obtained the same s𝑠s as through tuning on a random mini-batch of data (0.7, 0.95, and 0.99 for MNIST, CIFAR10, and ImageNet, respectively).\n2) Exploiting raw passive data leakage: Since manually, selecting suitable data points is time-consuming, we propose an alternative approach where the attacker uses all extracted data points with are in a valid range for input pixels ([0,1]) from the passive extraction on non-adversarially initialized model weights in the first round of the protocol.\nThese data points are not necessarily individually extracted user data points as we show in Figure 9 in Appendix D.2.\nBut the attacker can still consider them as a tuning dataset for s𝑠s and evaluate the extraction-recall on this dataset when initializing the shared model with different trap weights to tune s𝑠s.\nOur results in Table XIII show that for CIFAR10 and ImageNet, the best s𝑠s found on these passively reconstructed data points are equal to the best s𝑠s obtained directly by tuning on one mini-batch of the original data.\nFor MNIST, the s𝑠s on the extracted gradients differs slightly from the original best s𝑠s (0.75 vs. 0.7).\nWe suspect these changes to result from MNIST data being much sparser (many more zero features) than the extracted gradients in Figure 9(a).\n3) Using a surrogate dataset of same data dimensions: Lastly, the attacker can tune s𝑠s on a surrogate dataset of the same dimension (but potentially different distribution) than the users’ data.\nWe compare extraction-recall of an adversarial weight initialization with s𝑠s found on a surrogate dataset and the optimal s∗superscript𝑠s^{*} found on the actual dataset for Fashion MNIST, SVHN, CIFAR100, and Open Images\n [30] in Table VI.\nOur results highlight that extraction with the surrogate s𝑠s obtained through tuning on MNIST, CIFAR10, and ImageNet, already yields a significantly higher success than passive extraction on non-manipulated weights.\nFurthermore, the closer the surrogate dataset’s distribution is to the users’ dataset, the closer s𝑠s and s∗superscript𝑠s^{*}.\nEspecially for CIFAR10 and CIFAR100, and ImageNet and Open Images, we find that s=s∗𝑠superscript𝑠s=s^{*} which leads to highest extraction success."
        ]
    },
    "S7.T2": {
        "caption": "TABLE II: Data Extraction on IMDb Dataset. The extraction success depends on the size B of the mini-batches for passive attack and active attack with adversarial initialization. The results depict the percentage of active neurons (A), extraction-precision (P), and extraction-recall (R). All numbers are averaged over 10 runs with different random and adversarial initialization of the model from Table X, respectively.",
        "table": "<table id=\"S7.T2.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S7.T2.2.1\" class=\"ltx_tr\">\n<td id=\"S7.T2.2.1.1\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"S7.T2.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span id=\"S7.T2.2.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">Passive Attack</span></td>\n<td id=\"S7.T2.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span id=\"S7.T2.2.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">Our Active Attack</span></td>\n</tr>\n<tr id=\"S7.T2.2.2\" class=\"ltx_tr\">\n<td id=\"S7.T2.2.2.1\" class=\"ltx_td ltx_align_left\"><span id=\"S7.T2.2.2.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">B</span></td>\n<td id=\"S7.T2.2.2.2\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T2.2.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">A</span></td>\n<td id=\"S7.T2.2.2.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T2.2.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">P</span></td>\n<td id=\"S7.T2.2.2.4\" class=\"ltx_td ltx_align_right ltx_border_r\"><span id=\"S7.T2.2.2.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">R</span></td>\n<td id=\"S7.T2.2.2.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T2.2.2.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">A</span></td>\n<td id=\"S7.T2.2.2.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T2.2.2.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">P</span></td>\n<td id=\"S7.T2.2.2.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T2.2.2.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">R</span></td>\n</tr>\n<tr id=\"S7.T2.2.3\" class=\"ltx_tr\">\n<td id=\"S7.T2.2.3.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S7.T2.2.3.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">20</span></td>\n<td id=\"S7.T2.2.3.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S7.T2.2.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.842</span></td>\n<td id=\"S7.T2.2.3.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S7.T2.2.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">.072</span></td>\n<td id=\"S7.T2.2.3.4\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\"><span id=\"S7.T2.2.3.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.900</span></td>\n<td id=\"S7.T2.2.3.5\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S7.T2.2.3.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">.519</span></td>\n<td id=\"S7.T2.2.3.6\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S7.T2.2.3.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">.610</span></td>\n<td id=\"S7.T2.2.3.7\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S7.T2.2.3.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">1.000</span></td>\n</tr>\n<tr id=\"S7.T2.2.4\" class=\"ltx_tr\">\n<td id=\"S7.T2.2.4.1\" class=\"ltx_td ltx_align_left\"><span id=\"S7.T2.2.4.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">50</span></td>\n<td id=\"S7.T2.2.4.2\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T2.2.4.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.885</span></td>\n<td id=\"S7.T2.2.4.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T2.2.4.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">.050</span></td>\n<td id=\"S7.T2.2.4.4\" class=\"ltx_td ltx_align_right ltx_border_r\"><span id=\"S7.T2.2.4.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.552</span></td>\n<td id=\"S7.T2.2.4.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T2.2.4.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">.776</span></td>\n<td id=\"S7.T2.2.4.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T2.2.4.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">.376</span></td>\n<td id=\"S7.T2.2.4.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T2.2.4.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.962</span></td>\n</tr>\n<tr id=\"S7.T2.2.5\" class=\"ltx_tr\">\n<td id=\"S7.T2.2.5.1\" class=\"ltx_td ltx_align_left\"><span id=\"S7.T2.2.5.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">100</span></td>\n<td id=\"S7.T2.2.5.2\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T2.2.5.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.909</span></td>\n<td id=\"S7.T2.2.5.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T2.2.5.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">.036</span></td>\n<td id=\"S7.T2.2.5.4\" class=\"ltx_td ltx_align_right ltx_border_r\"><span id=\"S7.T2.2.5.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.254</span></td>\n<td id=\"S7.T2.2.5.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T2.2.5.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">.910</span></td>\n<td id=\"S7.T2.2.5.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T2.2.5.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">.192</span></td>\n<td id=\"S7.T2.2.5.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T2.2.5.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.654</span></td>\n</tr>\n<tr id=\"S7.T2.2.6\" class=\"ltx_tr\">\n<td id=\"S7.T2.2.6.1\" class=\"ltx_td ltx_align_left ltx_border_bb\"><span id=\"S7.T2.2.6.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">200</span></td>\n<td id=\"S7.T2.2.6.2\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S7.T2.2.6.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.927</span></td>\n<td id=\"S7.T2.2.6.3\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S7.T2.2.6.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">.030</span></td>\n<td id=\"S7.T2.2.6.4\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\"><span id=\"S7.T2.2.6.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.128</span></td>\n<td id=\"S7.T2.2.6.5\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S7.T2.2.6.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">.978</span></td>\n<td id=\"S7.T2.2.6.6\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S7.T2.2.6.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">.070</span></td>\n<td id=\"S7.T2.2.6.7\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S7.T2.2.6.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.255</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Impact of Mini-Batch Sizes. We also set out to investigate the impact of the mini-batch size B𝐵B and the number of weight rows that we can use for extraction.\nTable V depicts the resulting metrics.\nThe metrics show that the smaller the mini-batch sizes are, and the more weight rows there are for extraction, the more individual training data points can be individually reconstructed.\nFor 300030003000 weight rows, even up to 505050% of the individual training data points for mini-batch sizes as large as 200 in the MNIST dataset can be perfectly extracted.\nSmall mini-batches of 202020 training data points are entirely extractable without any loss in this setting.\nAlso for the IMDB dataset, smaller batch-sizes for the same number of neurons yield much higher extraction-recall, and embeddings of data from small mini-batches of 202020 training data points are perfectly extractable, see Table II.\nThis suggests that in practice, the success of the extraction attack can be significantly increased by the central party demanding smaller mini-batch sizes from the users or initializing larger models."
        ]
    },
    "S7.T3": {
        "caption": "TABLE III: Data Extractability from Converging Models. Results depict the success of passive data extraction based on the training stage of the corresponding models. We show the percentage of active neurons (A), extraction-precision (P), and extraction-recall (R) for extraction with a mini-batch size of 100 data points from the first layer of the fully-connected network from Table IX. All numbers are averaged over 10 runs with different random initializations.",
        "table": "<table id=\"S7.T3.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S7.T3.2.3\" class=\"ltx_tr\">\n<td id=\"S7.T3.2.3.1\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"S7.T3.2.3.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span id=\"S7.T3.2.3.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">MNIST</span></td>\n<td id=\"S7.T3.2.3.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span id=\"S7.T3.2.3.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">CIFAR10</span></td>\n</tr>\n<tr id=\"S7.T3.2.2\" class=\"ltx_tr\">\n<td id=\"S7.T3.2.2.3\" class=\"ltx_td ltx_align_left\"><span id=\"S7.T3.2.2.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Epoch</span></td>\n<td id=\"S7.T3.1.1.1\" class=\"ltx_td ltx_align_right\">\n<span id=\"S7.T3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Loss </span><math id=\"S7.T3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\mathcal{L}\" display=\"inline\"><semantics id=\"S7.T3.1.1.1.m1.1a\"><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"70%\" id=\"S7.T3.1.1.1.m1.1.1\" xref=\"S7.T3.1.1.1.m1.1.1.cmml\">ℒ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S7.T3.1.1.1.m1.1b\"><ci id=\"S7.T3.1.1.1.m1.1.1.cmml\" xref=\"S7.T3.1.1.1.m1.1.1\">ℒ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T3.1.1.1.m1.1c\">\\mathcal{L}</annotation></semantics></math>\n</td>\n<td id=\"S7.T3.2.2.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.2.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">A</span></td>\n<td id=\"S7.T3.2.2.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.2.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">P</span></td>\n<td id=\"S7.T3.2.2.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.2.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">R</span></td>\n<td id=\"S7.T3.2.2.2\" class=\"ltx_td ltx_align_right\">\n<span id=\"S7.T3.2.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">Loss </span><math id=\"S7.T3.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\mathcal{L}\" display=\"inline\"><semantics id=\"S7.T3.2.2.2.m1.1a\"><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"70%\" id=\"S7.T3.2.2.2.m1.1.1\" xref=\"S7.T3.2.2.2.m1.1.1.cmml\">ℒ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S7.T3.2.2.2.m1.1b\"><ci id=\"S7.T3.2.2.2.m1.1.1.cmml\" xref=\"S7.T3.2.2.2.m1.1.1\">ℒ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T3.2.2.2.m1.1c\">\\mathcal{L}</annotation></semantics></math>\n</td>\n<td id=\"S7.T3.2.2.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.2.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">A</span></td>\n<td id=\"S7.T3.2.2.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.2.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">P</span></td>\n<td id=\"S7.T3.2.2.9\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.2.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">R</span></td>\n</tr>\n<tr id=\"S7.T3.2.4\" class=\"ltx_tr\">\n<td id=\"S7.T3.2.4.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S7.T3.2.4.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">0</span></td>\n<td id=\"S7.T3.2.4.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S7.T3.2.4.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.526</span></td>\n<td id=\"S7.T3.2.4.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S7.T3.2.4.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">.998</span></td>\n<td id=\"S7.T3.2.4.4\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S7.T3.2.4.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.005</span></td>\n<td id=\"S7.T3.2.4.5\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S7.T3.2.4.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">.050</span></td>\n<td id=\"S7.T3.2.4.6\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S7.T3.2.4.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">1.857</span></td>\n<td id=\"S7.T3.2.4.7\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S7.T3.2.4.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">.907</span></td>\n<td id=\"S7.T3.2.4.8\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S7.T3.2.4.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">.053</span></td>\n<td id=\"S7.T3.2.4.9\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S7.T3.2.4.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">.232</span></td>\n</tr>\n<tr id=\"S7.T3.2.5\" class=\"ltx_tr\">\n<td id=\"S7.T3.2.5.1\" class=\"ltx_td ltx_align_left\"><span id=\"S7.T3.2.5.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">5</span></td>\n<td id=\"S7.T3.2.5.2\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.5.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.067</span></td>\n<td id=\"S7.T3.2.5.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.5.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">.997</span></td>\n<td id=\"S7.T3.2.5.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.5.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.044</span></td>\n<td id=\"S7.T3.2.5.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.5.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">.137</span></td>\n<td id=\"S7.T3.2.5.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.5.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">1.352</span></td>\n<td id=\"S7.T3.2.5.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.5.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">.900</span></td>\n<td id=\"S7.T3.2.5.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.5.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">.044</span></td>\n<td id=\"S7.T3.2.5.9\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.5.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">.195</span></td>\n</tr>\n<tr id=\"S7.T3.2.6\" class=\"ltx_tr\">\n<td id=\"S7.T3.2.6.1\" class=\"ltx_td ltx_align_left\"><span id=\"S7.T3.2.6.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">10</span></td>\n<td id=\"S7.T3.2.6.2\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.6.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.021</span></td>\n<td id=\"S7.T3.2.6.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.6.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">.997</span></td>\n<td id=\"S7.T3.2.6.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.6.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.116</span></td>\n<td id=\"S7.T3.2.6.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.6.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">.154</span></td>\n<td id=\"S7.T3.2.6.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.6.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">1.088</span></td>\n<td id=\"S7.T3.2.6.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.6.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">.913</span></td>\n<td id=\"S7.T3.2.6.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.6.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">.041</span></td>\n<td id=\"S7.T3.2.6.9\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.6.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">.196</span></td>\n</tr>\n<tr id=\"S7.T3.2.7\" class=\"ltx_tr\">\n<td id=\"S7.T3.2.7.1\" class=\"ltx_td ltx_align_left\"><span id=\"S7.T3.2.7.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">15</span></td>\n<td id=\"S7.T3.2.7.2\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.7.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.006</span></td>\n<td id=\"S7.T3.2.7.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.7.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">.997</span></td>\n<td id=\"S7.T3.2.7.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.7.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.131</span></td>\n<td id=\"S7.T3.2.7.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.7.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">.165</span></td>\n<td id=\"S7.T3.2.7.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.7.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">.768</span></td>\n<td id=\"S7.T3.2.7.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.7.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">.923</span></td>\n<td id=\"S7.T3.2.7.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.7.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">.043</span></td>\n<td id=\"S7.T3.2.7.9\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.7.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">.206</span></td>\n</tr>\n<tr id=\"S7.T3.2.8\" class=\"ltx_tr\">\n<td id=\"S7.T3.2.8.1\" class=\"ltx_td ltx_align_left\"><span id=\"S7.T3.2.8.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">20</span></td>\n<td id=\"S7.T3.2.8.2\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.8.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.002</span></td>\n<td id=\"S7.T3.2.8.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.8.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">.997</span></td>\n<td id=\"S7.T3.2.8.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.8.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.136</span></td>\n<td id=\"S7.T3.2.8.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.8.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">.167</span></td>\n<td id=\"S7.T3.2.8.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.8.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">.472</span></td>\n<td id=\"S7.T3.2.8.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.8.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">.931</span></td>\n<td id=\"S7.T3.2.8.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.8.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">.050</span></td>\n<td id=\"S7.T3.2.8.9\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.8.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">.232</span></td>\n</tr>\n<tr id=\"S7.T3.2.9\" class=\"ltx_tr\">\n<td id=\"S7.T3.2.9.1\" class=\"ltx_td ltx_align_left\"><span id=\"S7.T3.2.9.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">25</span></td>\n<td id=\"S7.T3.2.9.2\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.9.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.001</span></td>\n<td id=\"S7.T3.2.9.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.9.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">.997</span></td>\n<td id=\"S7.T3.2.9.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.9.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.140</span></td>\n<td id=\"S7.T3.2.9.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.9.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.169</span></td>\n<td id=\"S7.T3.2.9.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.9.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">.282</span></td>\n<td id=\"S7.T3.2.9.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.9.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">.935</span></td>\n<td id=\"S7.T3.2.9.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.9.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">.058</span></td>\n<td id=\"S7.T3.2.9.9\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T3.2.9.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">.241</span></td>\n</tr>\n<tr id=\"S7.T3.2.10\" class=\"ltx_tr\">\n<td id=\"S7.T3.2.10.1\" class=\"ltx_td ltx_align_left ltx_border_bb\"><span id=\"S7.T3.2.10.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">30</span></td>\n<td id=\"S7.T3.2.10.2\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S7.T3.2.10.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.001</span></td>\n<td id=\"S7.T3.2.10.3\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S7.T3.2.10.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">.997</span></td>\n<td id=\"S7.T3.2.10.4\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S7.T3.2.10.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.142</span></td>\n<td id=\"S7.T3.2.10.5\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S7.T3.2.10.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">.168</span></td>\n<td id=\"S7.T3.2.10.6\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S7.T3.2.10.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">.200</span></td>\n<td id=\"S7.T3.2.10.7\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S7.T3.2.10.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">.936</span></td>\n<td id=\"S7.T3.2.10.8\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S7.T3.2.10.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">.062</span></td>\n<td id=\"S7.T3.2.10.9\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S7.T3.2.10.9.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.267</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Recall from ",
                "Section",
                " ",
                "5",
                " that extraction of training data from gradients is possible even when model weights are initialized randomly.\nWe evaluate this passive attack to obtain a baseline for our adversarial weight initialization strategies.\nTo evaluate the passive attack, we measure the extraction success of individual training data points from the gradients of randomly initialized models.",
                "Table",
                " ",
                "I",
                " reports the ",
                "extraction-precision",
                " and ",
                "extraction-recall",
                " of training data point extraction from the gradients of randomly initialized models. These gradients are computed over a mini-batch of 100 data points for 1000 neurons (",
                "i.e",
                ".",
                " 1000 weight rows’ gradients for extraction) in the first fully-connected layer. We later study the impact of these two parameters on the success of reconstruction attacks.\nEven if this attack is passive, and the central party has not modified any of the weights adversarially, training data extraction is often successful: for the MNIST dataset, around ",
                "6",
                "6",
                "6",
                "% of individual training data points can be directly extracted from the model gradients, whereas for CIFAR10 and ImageNet, roughly ",
                "26",
                "26",
                "26",
                "% and ",
                "22",
                "22",
                "22",
                "% of the training data points can be perfectly extracted.\nThe passive attack for extracting embeddings from the IMDB dataset yields roughly ",
                "25",
                "25",
                "25",
                "% ",
                "extraction-recall",
                " for 1000 neurons and mini-batches of 100 data points, see ",
                "Table",
                " ",
                "II",
                ".",
                "These results also suggest that setting higher spread, in form of standard deviations to random weight distributions alone can already significantly increase the ",
                "extraction-recall",
                " of individual data points from the model gradients, see ",
                "Table",
                " ",
                "I",
                ".\nThis is, most likely, due to the larger span within the weight values.",
                "Additionally, we also set out to investigate how as training progresses, and the model’s weights converge, the extraction’ success evolves.\nWe initialized the FC-NN from ",
                "Table",
                " ",
                "IX",
                " with a Xavier Uniform distribution and trained the model on MNIST and CIFAR10 for 30 epochs.\n",
                "Table",
                " ",
                "III",
                " depicts the results.\nWe observe that the ",
                "extraction-recall",
                " increases slightly over the training epochs.\nAnalyzing the distribution of the model weights in ",
                "Figure",
                " ",
                "3",
                " shows that over training, the uniformly initialized weight values resemble more a normal distribution and obtain a wider spread, which might be the reason for the increased extraction success."
            ]
        ]
    },
    "S7.T4": {
        "caption": "TABLE IV: Impact of Hyperparameter 𝐬𝐬\\mathbf{s}. Success of our adversarial weight initialization dependent on the hyperparameter s𝑠s, which downscales the positive weights. The results depict the percentage of active neurons (A), extraction-precision (P), and extraction-recall (R) with a mini-batch size of 100 data points from the first fully-connected layer of the respective architectures from Table IX. All numbers are averaged over 10 runs with different adversarial initializations.",
        "table": "<table id=\"S7.T4.13\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S7.T4.13.14\" class=\"ltx_tr\">\n<td id=\"S7.T4.13.14.1\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"S7.T4.13.14.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span id=\"S7.T4.13.14.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">MNIST</span></td>\n<td id=\"S7.T4.13.14.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span id=\"S7.T4.13.14.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">CIFAR10</span></td>\n<td id=\"S7.T4.13.14.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span id=\"S7.T4.13.14.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">ImageNet</span></td>\n</tr>\n<tr id=\"S7.T4.13.15\" class=\"ltx_tr\">\n<td id=\"S7.T4.13.15.1\" class=\"ltx_td ltx_align_left\"><span id=\"S7.T4.13.15.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:50%;\">s</span></td>\n<td id=\"S7.T4.13.15.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.13.15.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">A</span></td>\n<td id=\"S7.T4.13.15.3\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.13.15.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">P</span></td>\n<td id=\"S7.T4.13.15.4\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.13.15.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">R</span></td>\n<td id=\"S7.T4.13.15.5\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.13.15.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">A</span></td>\n<td id=\"S7.T4.13.15.6\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.13.15.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">P</span></td>\n<td id=\"S7.T4.13.15.7\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.13.15.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">R</span></td>\n<td id=\"S7.T4.13.15.8\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.13.15.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">A</span></td>\n<td id=\"S7.T4.13.15.9\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.13.15.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">P</span></td>\n<td id=\"S7.T4.13.15.10\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.13.15.10.1\" class=\"ltx_text\" style=\"font-size:50%;\">R</span></td>\n</tr>\n<tr id=\"S7.T4.1.1\" class=\"ltx_tr\">\n<td id=\"S7.T4.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><math id=\"S7.T4.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\".400\" display=\"inline\"><semantics id=\"S7.T4.1.1.1.m1.1a\"><mn mathsize=\"50%\" id=\"S7.T4.1.1.1.m1.1.1\" xref=\"S7.T4.1.1.1.m1.1.1.cmml\">.400</mn><annotation-xml encoding=\"MathML-Content\" id=\"S7.T4.1.1.1.m1.1b\"><cn type=\"float\" id=\"S7.T4.1.1.1.m1.1.1.cmml\" xref=\"S7.T4.1.1.1.m1.1.1\">.400</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T4.1.1.1.m1.1c\">.400</annotation></semantics></math></td>\n<td id=\"S7.T4.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T4.1.1.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">.022</span></td>\n<td id=\"S7.T4.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T4.1.1.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">.803</span></td>\n<td id=\"S7.T4.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T4.1.1.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">.114</span></td>\n<td id=\"S7.T4.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T4.1.1.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T4.1.1.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.1.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T4.1.1.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.1.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T4.1.1.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">.0.</span></td>\n<td id=\"S7.T4.1.1.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T4.1.1.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.1.1.10\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T4.1.1.10.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n</tr>\n<tr id=\"S7.T4.2.2\" class=\"ltx_tr\">\n<td id=\"S7.T4.2.2.1\" class=\"ltx_td ltx_align_left\"><math id=\"S7.T4.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\".500\" display=\"inline\"><semantics id=\"S7.T4.2.2.1.m1.1a\"><mn mathsize=\"50%\" id=\"S7.T4.2.2.1.m1.1.1\" xref=\"S7.T4.2.2.1.m1.1.1.cmml\">.500</mn><annotation-xml encoding=\"MathML-Content\" id=\"S7.T4.2.2.1.m1.1b\"><cn type=\"float\" id=\"S7.T4.2.2.1.m1.1.1.cmml\" xref=\"S7.T4.2.2.1.m1.1.1\">.500</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T4.2.2.1.m1.1c\">.500</annotation></semantics></math></td>\n<td id=\"S7.T4.2.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.2.2.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">.149</span></td>\n<td id=\"S7.T4.2.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.2.2.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">.636</span></td>\n<td id=\"S7.T4.2.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.2.2.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">.354</span></td>\n<td id=\"S7.T4.2.2.5\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.2.2.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.2.2.6\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.2.2.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.2.2.7\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.2.2.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.2.2.8\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.2.2.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.2.2.9\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.2.2.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.2.2.10\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.2.2.10.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n</tr>\n<tr id=\"S7.T4.3.3\" class=\"ltx_tr\">\n<td id=\"S7.T4.3.3.1\" class=\"ltx_td ltx_align_left\"><math id=\"S7.T4.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\".600\" display=\"inline\"><semantics id=\"S7.T4.3.3.1.m1.1a\"><mn mathsize=\"50%\" id=\"S7.T4.3.3.1.m1.1.1\" xref=\"S7.T4.3.3.1.m1.1.1.cmml\">.600</mn><annotation-xml encoding=\"MathML-Content\" id=\"S7.T4.3.3.1.m1.1b\"><cn type=\"float\" id=\"S7.T4.3.3.1.m1.1.1.cmml\" xref=\"S7.T4.3.3.1.m1.1.1\">.600</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T4.3.3.1.m1.1c\">.600</annotation></semantics></math></td>\n<td id=\"S7.T4.3.3.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.3.3.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">.462</span></td>\n<td id=\"S7.T4.3.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.3.3.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">.408</span></td>\n<td id=\"S7.T4.3.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.3.3.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">.526</span></td>\n<td id=\"S7.T4.3.3.5\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.3.3.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.3.3.6\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.3.3.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.3.3.7\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.3.3.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.3.3.8\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.3.3.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.3.3.9\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.3.3.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.3.3.10\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.3.3.10.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n</tr>\n<tr id=\"S7.T4.4.4\" class=\"ltx_tr\">\n<td id=\"S7.T4.4.4.1\" class=\"ltx_td ltx_align_left\"><math id=\"S7.T4.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\".700\" display=\"inline\"><semantics id=\"S7.T4.4.4.1.m1.1a\"><mn mathsize=\"50%\" id=\"S7.T4.4.4.1.m1.1.1\" xref=\"S7.T4.4.4.1.m1.1.1.cmml\">.700</mn><annotation-xml encoding=\"MathML-Content\" id=\"S7.T4.4.4.1.m1.1b\"><cn type=\"float\" id=\"S7.T4.4.4.1.m1.1.1.cmml\" xref=\"S7.T4.4.4.1.m1.1.1\">.700</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T4.4.4.1.m1.1c\">.700</annotation></semantics></math></td>\n<td id=\"S7.T4.4.4.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.4.4.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">.796</span></td>\n<td id=\"S7.T4.4.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.4.4.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">.203</span></td>\n<td id=\"S7.T4.4.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.4.4.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:50%;\">.540</span></td>\n<td id=\"S7.T4.4.4.5\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.4.4.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.4.4.6\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.4.4.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.4.4.7\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.4.4.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.4.4.8\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.4.4.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.4.4.9\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.4.4.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.4.4.10\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.4.4.10.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n</tr>\n<tr id=\"S7.T4.5.5\" class=\"ltx_tr\">\n<td id=\"S7.T4.5.5.1\" class=\"ltx_td ltx_align_left\"><math id=\"S7.T4.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\".800\" display=\"inline\"><semantics id=\"S7.T4.5.5.1.m1.1a\"><mn mathsize=\"50%\" id=\"S7.T4.5.5.1.m1.1.1\" xref=\"S7.T4.5.5.1.m1.1.1.cmml\">.800</mn><annotation-xml encoding=\"MathML-Content\" id=\"S7.T4.5.5.1.m1.1b\"><cn type=\"float\" id=\"S7.T4.5.5.1.m1.1.1.cmml\" xref=\"S7.T4.5.5.1.m1.1.1\">.800</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T4.5.5.1.m1.1c\">.800</annotation></semantics></math></td>\n<td id=\"S7.T4.5.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.5.5.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">.959</span></td>\n<td id=\"S7.T4.5.5.3\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.5.5.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">.062</span></td>\n<td id=\"S7.T4.5.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.5.5.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">.334</span></td>\n<td id=\"S7.T4.5.5.5\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.5.5.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.5.5.6\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.5.5.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.5.5.7\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.5.5.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.5.5.8\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.5.5.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.5.5.9\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.5.5.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.5.5.10\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.5.5.10.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n</tr>\n<tr id=\"S7.T4.6.6\" class=\"ltx_tr\">\n<td id=\"S7.T4.6.6.1\" class=\"ltx_td ltx_align_left\"><math id=\"S7.T4.6.6.1.m1.1\" class=\"ltx_Math\" alttext=\".900\" display=\"inline\"><semantics id=\"S7.T4.6.6.1.m1.1a\"><mn mathsize=\"50%\" id=\"S7.T4.6.6.1.m1.1.1\" xref=\"S7.T4.6.6.1.m1.1.1.cmml\">.900</mn><annotation-xml encoding=\"MathML-Content\" id=\"S7.T4.6.6.1.m1.1b\"><cn type=\"float\" id=\"S7.T4.6.6.1.m1.1.1.cmml\" xref=\"S7.T4.6.6.1.m1.1.1\">.900</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T4.6.6.1.m1.1c\">.900</annotation></semantics></math></td>\n<td id=\"S7.T4.6.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.6.6.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">.996</span></td>\n<td id=\"S7.T4.6.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.6.6.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">.010</span></td>\n<td id=\"S7.T4.6.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.6.6.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">.089</span></td>\n<td id=\"S7.T4.6.6.5\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.6.6.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">.034</span></td>\n<td id=\"S7.T4.6.6.6\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.6.6.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">.946</span></td>\n<td id=\"S7.T4.6.6.7\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.6.6.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">.077</span></td>\n<td id=\"S7.T4.6.6.8\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.6.6.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.6.6.9\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.6.6.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.6.6.10\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.6.6.10.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n</tr>\n<tr id=\"S7.T4.7.7\" class=\"ltx_tr\">\n<td id=\"S7.T4.7.7.1\" class=\"ltx_td ltx_align_left\"><math id=\"S7.T4.7.7.1.m1.1\" class=\"ltx_Math\" alttext=\".950\" display=\"inline\"><semantics id=\"S7.T4.7.7.1.m1.1a\"><mn mathsize=\"50%\" id=\"S7.T4.7.7.1.m1.1.1\" xref=\"S7.T4.7.7.1.m1.1.1.cmml\">.950</mn><annotation-xml encoding=\"MathML-Content\" id=\"S7.T4.7.7.1.m1.1b\"><cn type=\"float\" id=\"S7.T4.7.7.1.m1.1.1.cmml\" xref=\"S7.T4.7.7.1.m1.1.1\">.950</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T4.7.7.1.m1.1c\">.950</annotation></semantics></math></td>\n<td id=\"S7.T4.7.7.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.7.7.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">.999</span></td>\n<td id=\"S7.T4.7.7.3\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.7.7.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">.003</span></td>\n<td id=\"S7.T4.7.7.4\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.7.7.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">.029</span></td>\n<td id=\"S7.T4.7.7.5\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.7.7.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">.729</span></td>\n<td id=\"S7.T4.7.7.6\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.7.7.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">.412</span></td>\n<td id=\"S7.T4.7.7.7\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.7.7.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:50%;\">.540</span></td>\n<td id=\"S7.T4.7.7.8\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.7.7.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.7.7.9\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.7.7.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.7.7.10\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.7.7.10.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n</tr>\n<tr id=\"S7.T4.8.8\" class=\"ltx_tr\">\n<td id=\"S7.T4.8.8.1\" class=\"ltx_td ltx_align_left\"><math id=\"S7.T4.8.8.1.m1.1\" class=\"ltx_Math\" alttext=\".960\" display=\"inline\"><semantics id=\"S7.T4.8.8.1.m1.1a\"><mn mathsize=\"50%\" id=\"S7.T4.8.8.1.m1.1.1\" xref=\"S7.T4.8.8.1.m1.1.1.cmml\">.960</mn><annotation-xml encoding=\"MathML-Content\" id=\"S7.T4.8.8.1.m1.1b\"><cn type=\"float\" id=\"S7.T4.8.8.1.m1.1.1.cmml\" xref=\"S7.T4.8.8.1.m1.1.1\">.960</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T4.8.8.1.m1.1c\">.960</annotation></semantics></math></td>\n<td id=\"S7.T4.8.8.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.8.8.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">.999</span></td>\n<td id=\"S7.T4.8.8.3\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.8.8.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">.003</span></td>\n<td id=\"S7.T4.8.8.4\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.8.8.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">.027</span></td>\n<td id=\"S7.T4.8.8.5\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.8.8.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">.925</span></td>\n<td id=\"S7.T4.8.8.6\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.8.8.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">.175</span></td>\n<td id=\"S7.T4.8.8.7\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.8.8.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">.522</span></td>\n<td id=\"S7.T4.8.8.8\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.8.8.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.8.8.9\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.8.8.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.8.8.10\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.8.8.10.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n</tr>\n<tr id=\"S7.T4.9.9\" class=\"ltx_tr\">\n<td id=\"S7.T4.9.9.1\" class=\"ltx_td ltx_align_left\"><math id=\"S7.T4.9.9.1.m1.1\" class=\"ltx_Math\" alttext=\".970\" display=\"inline\"><semantics id=\"S7.T4.9.9.1.m1.1a\"><mn mathsize=\"50%\" id=\"S7.T4.9.9.1.m1.1.1\" xref=\"S7.T4.9.9.1.m1.1.1.cmml\">.970</mn><annotation-xml encoding=\"MathML-Content\" id=\"S7.T4.9.9.1.m1.1b\"><cn type=\"float\" id=\"S7.T4.9.9.1.m1.1.1.cmml\" xref=\"S7.T4.9.9.1.m1.1.1\">.970</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T4.9.9.1.m1.1c\">.970</annotation></semantics></math></td>\n<td id=\"S7.T4.9.9.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.9.9.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">1.</span></td>\n<td id=\"S7.T4.9.9.3\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.9.9.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">.002</span></td>\n<td id=\"S7.T4.9.9.4\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.9.9.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">.020</span></td>\n<td id=\"S7.T4.9.9.5\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.9.9.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">.993</span></td>\n<td id=\"S7.T4.9.9.6\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.9.9.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">.025</span></td>\n<td id=\"S7.T4.9.9.7\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.9.9.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">.198</span></td>\n<td id=\"S7.T4.9.9.8\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.9.9.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">.002</span></td>\n<td id=\"S7.T4.9.9.9\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.9.9.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">.900</span></td>\n<td id=\"S7.T4.9.9.10\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.9.9.10.1\" class=\"ltx_text\" style=\"font-size:50%;\">.013</span></td>\n</tr>\n<tr id=\"S7.T4.10.10\" class=\"ltx_tr\">\n<td id=\"S7.T4.10.10.1\" class=\"ltx_td ltx_align_left\"><math id=\"S7.T4.10.10.1.m1.1\" class=\"ltx_Math\" alttext=\".980\" display=\"inline\"><semantics id=\"S7.T4.10.10.1.m1.1a\"><mn mathsize=\"50%\" id=\"S7.T4.10.10.1.m1.1.1\" xref=\"S7.T4.10.10.1.m1.1.1.cmml\">.980</mn><annotation-xml encoding=\"MathML-Content\" id=\"S7.T4.10.10.1.m1.1b\"><cn type=\"float\" id=\"S7.T4.10.10.1.m1.1.1.cmml\" xref=\"S7.T4.10.10.1.m1.1.1\">.980</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T4.10.10.1.m1.1c\">.980</annotation></semantics></math></td>\n<td id=\"S7.T4.10.10.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.10.10.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">1.</span></td>\n<td id=\"S7.T4.10.10.3\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.10.10.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">.002</span></td>\n<td id=\"S7.T4.10.10.4\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.10.10.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">.021</span></td>\n<td id=\"S7.T4.10.10.5\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.10.10.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">1.</span></td>\n<td id=\"S7.T4.10.10.6\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.10.10.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">.001</span></td>\n<td id=\"S7.T4.10.10.7\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.10.10.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">.008</span></td>\n<td id=\"S7.T4.10.10.8\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.10.10.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">.043</span></td>\n<td id=\"S7.T4.10.10.9\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.10.10.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">.986</span></td>\n<td id=\"S7.T4.10.10.10\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.10.10.10.1\" class=\"ltx_text\" style=\"font-size:50%;\">.049</span></td>\n</tr>\n<tr id=\"S7.T4.11.11\" class=\"ltx_tr\">\n<td id=\"S7.T4.11.11.1\" class=\"ltx_td ltx_align_left\"><math id=\"S7.T4.11.11.1.m1.1\" class=\"ltx_Math\" alttext=\".990\" display=\"inline\"><semantics id=\"S7.T4.11.11.1.m1.1a\"><mn mathsize=\"50%\" id=\"S7.T4.11.11.1.m1.1.1\" xref=\"S7.T4.11.11.1.m1.1.1.cmml\">.990</mn><annotation-xml encoding=\"MathML-Content\" id=\"S7.T4.11.11.1.m1.1b\"><cn type=\"float\" id=\"S7.T4.11.11.1.m1.1.1.cmml\" xref=\"S7.T4.11.11.1.m1.1.1\">.990</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T4.11.11.1.m1.1c\">.990</annotation></semantics></math></td>\n<td id=\"S7.T4.11.11.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.11.11.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">1.</span></td>\n<td id=\"S7.T4.11.11.3\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.11.11.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">.002</span></td>\n<td id=\"S7.T4.11.11.4\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.11.11.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">.020</span></td>\n<td id=\"S7.T4.11.11.5\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.11.11.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">1.</span></td>\n<td id=\"S7.T4.11.11.6\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.11.11.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.11.11.7\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.11.11.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.11.11.8\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.11.11.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">.655</span></td>\n<td id=\"S7.T4.11.11.9\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.11.11.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">.514</span></td>\n<td id=\"S7.T4.11.11.10\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.11.11.10.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:50%;\">.457</span></td>\n</tr>\n<tr id=\"S7.T4.12.12\" class=\"ltx_tr\">\n<td id=\"S7.T4.12.12.1\" class=\"ltx_td ltx_align_left\"><math id=\"S7.T4.12.12.1.m1.1\" class=\"ltx_Math\" alttext=\".995\" display=\"inline\"><semantics id=\"S7.T4.12.12.1.m1.1a\"><mn mathsize=\"50%\" id=\"S7.T4.12.12.1.m1.1.1\" xref=\"S7.T4.12.12.1.m1.1.1.cmml\">.995</mn><annotation-xml encoding=\"MathML-Content\" id=\"S7.T4.12.12.1.m1.1b\"><cn type=\"float\" id=\"S7.T4.12.12.1.m1.1.1.cmml\" xref=\"S7.T4.12.12.1.m1.1.1\">.995</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T4.12.12.1.m1.1c\">.995</annotation></semantics></math></td>\n<td id=\"S7.T4.12.12.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.12.12.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">1.</span></td>\n<td id=\"S7.T4.12.12.3\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.12.12.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">.002</span></td>\n<td id=\"S7.T4.12.12.4\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.12.12.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">.018</span></td>\n<td id=\"S7.T4.12.12.5\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.12.12.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">1.</span></td>\n<td id=\"S7.T4.12.12.6\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.12.12.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.12.12.7\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.12.12.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.12.12.8\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.12.12.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">.999</span></td>\n<td id=\"S7.T4.12.12.9\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.12.12.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">.007</span></td>\n<td id=\"S7.T4.12.12.10\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T4.12.12.10.1\" class=\"ltx_text\" style=\"font-size:50%;\">.055</span></td>\n</tr>\n<tr id=\"S7.T4.13.13\" class=\"ltx_tr\">\n<td id=\"S7.T4.13.13.1\" class=\"ltx_td ltx_align_left ltx_border_bb\"><math id=\"S7.T4.13.13.1.m1.1\" class=\"ltx_Math\" alttext=\".999\" display=\"inline\"><semantics id=\"S7.T4.13.13.1.m1.1a\"><mn mathsize=\"50%\" id=\"S7.T4.13.13.1.m1.1.1\" xref=\"S7.T4.13.13.1.m1.1.1.cmml\">.999</mn><annotation-xml encoding=\"MathML-Content\" id=\"S7.T4.13.13.1.m1.1b\"><cn type=\"float\" id=\"S7.T4.13.13.1.m1.1.1.cmml\" xref=\"S7.T4.13.13.1.m1.1.1\">.999</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T4.13.13.1.m1.1c\">.999</annotation></semantics></math></td>\n<td id=\"S7.T4.13.13.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T4.13.13.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">1.</span></td>\n<td id=\"S7.T4.13.13.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T4.13.13.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">.002</span></td>\n<td id=\"S7.T4.13.13.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T4.13.13.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">.017</span></td>\n<td id=\"S7.T4.13.13.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T4.13.13.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">1.</span></td>\n<td id=\"S7.T4.13.13.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T4.13.13.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.13.13.7\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T4.13.13.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.13.13.8\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T4.13.13.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">1.</span></td>\n<td id=\"S7.T4.13.13.9\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T4.13.13.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n<td id=\"S7.T4.13.13.10\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T4.13.13.10.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Attacker without Auxiliary Data.\nWe experiment with an attacker who does not have access to a small mini-batch of data from the users’ distribution to tune the scaling factor s𝑠s of our trap weights.\nIn this setup, the only knowledge an attacker holds is about the dimensionality of the users’ data which it needs to instantiate an adequate model architecture.\nWe evaluate three attacks in this setup.\n1) Exploiting passive data leakage and composing a tuning dataset: the attacker randomly initializes the model in a first round of the protocol.\nOur results in Table I show that also randomly initialized models’ gradients leak significant fractions of the users’ data (MNIST 6.1%, CIFAR10 25.9%, and ImageNet 21.7%).\nBy plotting the user’s gradients and eyeballing which data points resemble natural images, the attacker can build a tuning set for s𝑠s.\nSince we only require a maximum of 100 data points to find the optimal values for s𝑠s per dataset in Table IV, the attacker only has to inspect the gradients of 17, 4, and 5 users for MNIST, CIFAR10, and ImageNet, respectively in the first round of the protocol.\nOn the selected data, they can tune s𝑠s and use it in every subsequent iteration.\nWe performed tuning on 100 data points obtained through passive extraction and obtained the same s𝑠s as through tuning on a random mini-batch of data (0.7, 0.95, and 0.99 for MNIST, CIFAR10, and ImageNet, respectively).\n2) Exploiting raw passive data leakage: Since manually, selecting suitable data points is time-consuming, we propose an alternative approach where the attacker uses all extracted data points with are in a valid range for input pixels ([0,1]) from the passive extraction on non-adversarially initialized model weights in the first round of the protocol.\nThese data points are not necessarily individually extracted user data points as we show in Figure 9 in Appendix D.2.\nBut the attacker can still consider them as a tuning dataset for s𝑠s and evaluate the extraction-recall on this dataset when initializing the shared model with different trap weights to tune s𝑠s.\nOur results in Table XIII show that for CIFAR10 and ImageNet, the best s𝑠s found on these passively reconstructed data points are equal to the best s𝑠s obtained directly by tuning on one mini-batch of the original data.\nFor MNIST, the s𝑠s on the extracted gradients differs slightly from the original best s𝑠s (0.75 vs. 0.7).\nWe suspect these changes to result from MNIST data being much sparser (many more zero features) than the extracted gradients in Figure 9(a).\n3) Using a surrogate dataset of same data dimensions: Lastly, the attacker can tune s𝑠s on a surrogate dataset of the same dimension (but potentially different distribution) than the users’ data.\nWe compare extraction-recall of an adversarial weight initialization with s𝑠s found on a surrogate dataset and the optimal s∗superscript𝑠s^{*} found on the actual dataset for Fashion MNIST, SVHN, CIFAR100, and Open Images\n [30] in Table VI.\nOur results highlight that extraction with the surrogate s𝑠s obtained through tuning on MNIST, CIFAR10, and ImageNet, already yields a significantly higher success than passive extraction on non-manipulated weights.\nFurthermore, the closer the surrogate dataset’s distribution is to the users’ dataset, the closer s𝑠s and s∗superscript𝑠s^{*}.\nEspecially for CIFAR10 and CIFAR100, and ImageNet and Open Images, we find that s=s∗𝑠superscript𝑠s=s^{*} which leads to highest extraction success."
        ]
    },
    "S7.T5": {
        "caption": "TABLE V: Effect of Mini-Batch Size and Number of Neurons on Data Extraction. Success of our adversarial weight initialization is dependent on the mini-batch size B and the number of neurons N that corresponds to the number of weights rows. The results depict the percentage of active neurons (A), extraction-precision (P), and extraction-recall (R). All numbers are averaged over 10 runs with different adversarial initializations.",
        "table": "<table id=\"S7.T5.2\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S7.T5.2.1\" class=\"ltx_tr\">\n<td id=\"S7.T5.2.1.1\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"S7.T5.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_tt\" colspan=\"3\"><span id=\"S7.T5.2.1.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:50%;\">MNIST</span></td>\n<td id=\"S7.T5.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_tt\" colspan=\"3\"><span id=\"S7.T5.2.1.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:50%;\">CIFAR10</span></td>\n<td id=\"S7.T5.2.1.4\" class=\"ltx_td ltx_align_left ltx_border_tt\" colspan=\"3\"><span id=\"S7.T5.2.1.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:50%;\">ImageNet</span></td>\n</tr>\n<tr id=\"S7.T5.2.2\" class=\"ltx_tr\">\n<td id=\"S7.T5.2.2.1\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T5.2.2.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:50%;\">(B, N)</span></td>\n<td id=\"S7.T5.2.2.2\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.2.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">A</span></td>\n<td id=\"S7.T5.2.2.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.2.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">P</span></td>\n<td id=\"S7.T5.2.2.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.2.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">R</span></td>\n<td id=\"S7.T5.2.2.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.2.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">A</span></td>\n<td id=\"S7.T5.2.2.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.2.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">P</span></td>\n<td id=\"S7.T5.2.2.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.2.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">R</span></td>\n<td id=\"S7.T5.2.2.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.2.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">A</span></td>\n<td id=\"S7.T5.2.2.9\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.2.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">P</span></td>\n<td id=\"S7.T5.2.2.10\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.2.10.1\" class=\"ltx_text\" style=\"font-size:50%;\">R</span></td>\n</tr>\n<tr id=\"S7.T5.2.3\" class=\"ltx_tr\">\n<td id=\"S7.T5.2.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T5.2.3.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">(200, 20)</span></td>\n<td id=\"S7.T5.2.3.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S7.T5.2.3.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">.522</span></td>\n<td id=\"S7.T5.2.3.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S7.T5.2.3.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">.436</span></td>\n<td id=\"S7.T5.2.3.4\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S7.T5.2.3.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:50%;\">.720</span></td>\n<td id=\"S7.T5.2.3.5\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S7.T5.2.3.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">.454</span></td>\n<td id=\"S7.T5.2.3.6\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S7.T5.2.3.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">.670</span></td>\n<td id=\"S7.T5.2.3.7\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S7.T5.2.3.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:50%;\">.695</span></td>\n<td id=\"S7.T5.2.3.8\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S7.T5.2.3.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">.090</span></td>\n<td id=\"S7.T5.2.3.9\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S7.T5.2.3.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">.948</span></td>\n<td id=\"S7.T5.2.3.10\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S7.T5.2.3.10.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:50%;\">.355</span></td>\n</tr>\n<tr id=\"S7.T5.2.4\" class=\"ltx_tr\">\n<td id=\"S7.T5.2.4.1\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T5.2.4.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">(200,50)</span></td>\n<td id=\"S7.T5.2.4.2\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.4.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">.690</span></td>\n<td id=\"S7.T5.2.4.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.4.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">.302</span></td>\n<td id=\"S7.T5.2.4.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.4.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">.428</span></td>\n<td id=\"S7.T5.2.4.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.4.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">.662</span></td>\n<td id=\"S7.T5.2.4.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.4.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">.494</span></td>\n<td id=\"S7.T5.2.4.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.4.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">.452</span></td>\n<td id=\"S7.T5.2.4.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.4.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">.381</span></td>\n<td id=\"S7.T5.2.4.9\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.4.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">.763</span></td>\n<td id=\"S7.T5.2.4.10\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.4.10.1\" class=\"ltx_text\" style=\"font-size:50%;\">.304</span></td>\n</tr>\n<tr id=\"S7.T5.2.5\" class=\"ltx_tr\">\n<td id=\"S7.T5.2.5.1\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T5.2.5.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">(200,100)</span></td>\n<td id=\"S7.T5.2.5.2\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.5.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">.782</span></td>\n<td id=\"S7.T5.2.5.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.5.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">.196</span></td>\n<td id=\"S7.T5.2.5.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.5.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">.218</span></td>\n<td id=\"S7.T5.2.5.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.5.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">.846</span></td>\n<td id=\"S7.T5.2.5.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.5.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">.280</span></td>\n<td id=\"S7.T5.2.5.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.5.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">.269</span></td>\n<td id=\"S7.T5.2.5.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.5.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">.653</span></td>\n<td id=\"S7.T5.2.5.9\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.5.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">.500</span></td>\n<td id=\"S7.T5.2.5.10\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.5.10.1\" class=\"ltx_text\" style=\"font-size:50%;\">.240</span></td>\n</tr>\n<tr id=\"S7.T5.2.6\" class=\"ltx_tr\">\n<td id=\"S7.T5.2.6.1\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T5.2.6.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">(200,200)</span></td>\n<td id=\"S7.T5.2.6.2\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.6.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">.859</span></td>\n<td id=\"S7.T5.2.6.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.6.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">.121</span></td>\n<td id=\"S7.T5.2.6.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.6.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">.086</span></td>\n<td id=\"S7.T5.2.6.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.6.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">.954</span></td>\n<td id=\"S7.T5.2.6.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.6.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">.124</span></td>\n<td id=\"S7.T5.2.6.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.6.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">.096</span></td>\n<td id=\"S7.T5.2.6.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.6.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">.886</span></td>\n<td id=\"S7.T5.2.6.9\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.6.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">.233</span></td>\n<td id=\"S7.T5.2.6.10\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.6.10.1\" class=\"ltx_text\" style=\"font-size:50%;\">.113</span></td>\n</tr>\n<tr id=\"S7.T5.2.7\" class=\"ltx_tr\">\n<td id=\"S7.T5.2.7.1\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T5.2.7.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">(500,20)</span></td>\n<td id=\"S7.T5.2.7.2\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.7.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">.535</span></td>\n<td id=\"S7.T5.2.7.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.7.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">.451</span></td>\n<td id=\"S7.T5.2.7.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.7.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:50%;\">.915</span></td>\n<td id=\"S7.T5.2.7.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.7.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">.452</span></td>\n<td id=\"S7.T5.2.7.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.7.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">.689</span></td>\n<td id=\"S7.T5.2.7.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.7.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:50%;\">.870</span></td>\n<td id=\"S7.T5.2.7.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.7.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">.096</span></td>\n<td id=\"S7.T5.2.7.9\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.7.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">.939</span></td>\n<td id=\"S7.T5.2.7.10\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.7.10.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:50%;\">.490</span></td>\n</tr>\n<tr id=\"S7.T5.2.8\" class=\"ltx_tr\">\n<td id=\"S7.T5.2.8.1\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T5.2.8.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">(500,50)</span></td>\n<td id=\"S7.T5.2.8.2\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.8.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">.697</span></td>\n<td id=\"S7.T5.2.8.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.8.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">.301</span></td>\n<td id=\"S7.T5.2.8.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.8.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">.624</span></td>\n<td id=\"S7.T5.2.8.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.8.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">.653</span></td>\n<td id=\"S7.T5.2.8.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.8.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">.505</span></td>\n<td id=\"S7.T5.2.8.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.8.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">.614</span></td>\n<td id=\"S7.T5.2.8.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.8.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">.387</span></td>\n<td id=\"S7.T5.2.8.9\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.8.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">.767</span></td>\n<td id=\"S7.T5.2.8.10\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.8.10.1\" class=\"ltx_text\" style=\"font-size:50%;\">.426</span></td>\n</tr>\n<tr id=\"S7.T5.2.9\" class=\"ltx_tr\">\n<td id=\"S7.T5.2.9.1\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T5.2.9.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">(500,100)</span></td>\n<td id=\"S7.T5.2.9.2\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.9.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">.792</span></td>\n<td id=\"S7.T5.2.9.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.9.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">.205</span></td>\n<td id=\"S7.T5.2.9.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.9.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">.397</span></td>\n<td id=\"S7.T5.2.9.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.9.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">.845</span></td>\n<td id=\"S7.T5.2.9.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.9.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">.290</span></td>\n<td id=\"S7.T5.2.9.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.9.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">.422</span></td>\n<td id=\"S7.T5.2.9.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.9.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">.646</span></td>\n<td id=\"S7.T5.2.9.9\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.9.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">.508</span></td>\n<td id=\"S7.T5.2.9.10\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.9.10.1\" class=\"ltx_text\" style=\"font-size:50%;\">.358</span></td>\n</tr>\n<tr id=\"S7.T5.2.10\" class=\"ltx_tr\">\n<td id=\"S7.T5.2.10.1\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T5.2.10.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">(500,200)</span></td>\n<td id=\"S7.T5.2.10.2\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.10.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">.871</span></td>\n<td id=\"S7.T5.2.10.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.10.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">.129</span></td>\n<td id=\"S7.T5.2.10.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.10.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">.185</span></td>\n<td id=\"S7.T5.2.10.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.10.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">.950</span></td>\n<td id=\"S7.T5.2.10.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.10.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">.119</span></td>\n<td id=\"S7.T5.2.10.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.10.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">.177</span></td>\n<td id=\"S7.T5.2.10.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.10.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">.892</span></td>\n<td id=\"S7.T5.2.10.9\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.10.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">.240</span></td>\n<td id=\"S7.T5.2.10.10\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.10.10.1\" class=\"ltx_text\" style=\"font-size:50%;\">.199</span></td>\n</tr>\n<tr id=\"S7.T5.2.11\" class=\"ltx_tr\">\n<td id=\"S7.T5.2.11.1\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T5.2.11.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">(1000,20)</span></td>\n<td id=\"S7.T5.2.11.2\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.11.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">.539</span></td>\n<td id=\"S7.T5.2.11.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.11.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">.444</span></td>\n<td id=\"S7.T5.2.11.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.11.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:50%;\">.950</span></td>\n<td id=\"S7.T5.2.11.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.11.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">.441</span></td>\n<td id=\"S7.T5.2.11.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.11.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">.703</span></td>\n<td id=\"S7.T5.2.11.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.11.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:50%;\">.915</span></td>\n<td id=\"S7.T5.2.11.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.11.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">.102</span></td>\n<td id=\"S7.T5.2.11.9\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.11.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">.942</span></td>\n<td id=\"S7.T5.2.11.10\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.11.10.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:50%;\">.595</span></td>\n</tr>\n<tr id=\"S7.T5.2.12\" class=\"ltx_tr\">\n<td id=\"S7.T5.2.12.1\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T5.2.12.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">(1000,50)</span></td>\n<td id=\"S7.T5.2.12.2\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.12.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">.705</span></td>\n<td id=\"S7.T5.2.12.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.12.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">.300</span></td>\n<td id=\"S7.T5.2.12.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.12.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">.760</span></td>\n<td id=\"S7.T5.2.12.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.12.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">.648</span></td>\n<td id=\"S7.T5.2.12.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.12.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">.504</span></td>\n<td id=\"S7.T5.2.12.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.12.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">.724</span></td>\n<td id=\"S7.T5.2.12.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.12.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">.388</span></td>\n<td id=\"S7.T5.2.12.9\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.12.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">.770</span></td>\n<td id=\"S7.T5.2.12.10\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.12.10.1\" class=\"ltx_text\" style=\"font-size:50%;\">.516</span></td>\n</tr>\n<tr id=\"S7.T5.2.13\" class=\"ltx_tr\">\n<td id=\"S7.T5.2.13.1\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T5.2.13.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">(1000,100)</span></td>\n<td id=\"S7.T5.2.13.2\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.13.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">.796</span></td>\n<td id=\"S7.T5.2.13.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.13.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">.203</span></td>\n<td id=\"S7.T5.2.13.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.13.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">.540</span></td>\n<td id=\"S7.T5.2.13.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.13.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">.844</span></td>\n<td id=\"S7.T5.2.13.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.13.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">.297</span></td>\n<td id=\"S7.T5.2.13.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.13.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">.556</span></td>\n<td id=\"S7.T5.2.13.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.13.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">.655</span></td>\n<td id=\"S7.T5.2.13.9\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.13.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">.514</span></td>\n<td id=\"S7.T5.2.13.10\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.13.10.1\" class=\"ltx_text\" style=\"font-size:50%;\">.457</span></td>\n</tr>\n<tr id=\"S7.T5.2.14\" class=\"ltx_tr\">\n<td id=\"S7.T5.2.14.1\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T5.2.14.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">(1000,200)</span></td>\n<td id=\"S7.T5.2.14.2\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.14.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">.871</span></td>\n<td id=\"S7.T5.2.14.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.14.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">.124</span></td>\n<td id=\"S7.T5.2.14.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.14.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">.293</span></td>\n<td id=\"S7.T5.2.14.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.14.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">.951</span></td>\n<td id=\"S7.T5.2.14.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.14.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">.120</span></td>\n<td id=\"S7.T5.2.14.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.14.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">.256</span></td>\n<td id=\"S7.T5.2.14.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.14.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">.892</span></td>\n<td id=\"S7.T5.2.14.9\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.14.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">.238</span></td>\n<td id=\"S7.T5.2.14.10\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.14.10.1\" class=\"ltx_text\" style=\"font-size:50%;\">.288</span></td>\n</tr>\n<tr id=\"S7.T5.2.15\" class=\"ltx_tr\">\n<td id=\"S7.T5.2.15.1\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T5.2.15.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">(3000,20)</span></td>\n<td id=\"S7.T5.2.15.2\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.15.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">.541</span></td>\n<td id=\"S7.T5.2.15.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.15.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">.442</span></td>\n<td id=\"S7.T5.2.15.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.15.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:50%;\">1.</span></td>\n<td id=\"S7.T5.2.15.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.15.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">.441</span></td>\n<td id=\"S7.T5.2.15.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.15.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">.696</span></td>\n<td id=\"S7.T5.2.15.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.15.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:50%;\">.945</span></td>\n<td id=\"S7.T5.2.15.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.15.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">.101</span></td>\n<td id=\"S7.T5.2.15.9\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.15.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">.934</span></td>\n<td id=\"S7.T5.2.15.10\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.15.10.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:50%;\">.640</span></td>\n</tr>\n<tr id=\"S7.T5.2.16\" class=\"ltx_tr\">\n<td id=\"S7.T5.2.16.1\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T5.2.16.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">(3000,50)</span></td>\n<td id=\"S7.T5.2.16.2\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.16.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">.704</span></td>\n<td id=\"S7.T5.2.16.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.16.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">.299</span></td>\n<td id=\"S7.T5.2.16.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.16.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">.888</span></td>\n<td id=\"S7.T5.2.16.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.16.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">.646</span></td>\n<td id=\"S7.T5.2.16.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.16.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">.503</span></td>\n<td id=\"S7.T5.2.16.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.16.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">.812</span></td>\n<td id=\"S7.T5.2.16.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.16.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">.386</span></td>\n<td id=\"S7.T5.2.16.9\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.16.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">.764</span></td>\n<td id=\"S7.T5.2.16.10\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.16.10.1\" class=\"ltx_text\" style=\"font-size:50%;\">.586</span></td>\n</tr>\n<tr id=\"S7.T5.2.17\" class=\"ltx_tr\">\n<td id=\"S7.T5.2.17.1\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T5.2.17.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">(3000,100)</span></td>\n<td id=\"S7.T5.2.17.2\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.17.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">.797</span></td>\n<td id=\"S7.T5.2.17.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.17.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">.203</span></td>\n<td id=\"S7.T5.2.17.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.17.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">.746</span></td>\n<td id=\"S7.T5.2.17.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.17.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">.840</span></td>\n<td id=\"S7.T5.2.17.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.17.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">.286</span></td>\n<td id=\"S7.T5.2.17.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.17.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">.711</span></td>\n<td id=\"S7.T5.2.17.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.17.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">.649</span></td>\n<td id=\"S7.T5.2.17.9\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.17.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">.518</span></td>\n<td id=\"S7.T5.2.17.10\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T5.2.17.10.1\" class=\"ltx_text\" style=\"font-size:50%;\">.579</span></td>\n</tr>\n<tr id=\"S7.T5.2.18\" class=\"ltx_tr\">\n<td id=\"S7.T5.2.18.1\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T5.2.18.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">(3000,200)</span></td>\n<td id=\"S7.T5.2.18.2\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S7.T5.2.18.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">.873</span></td>\n<td id=\"S7.T5.2.18.3\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S7.T5.2.18.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">.129</span></td>\n<td id=\"S7.T5.2.18.4\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S7.T5.2.18.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">.504</span></td>\n<td id=\"S7.T5.2.18.5\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S7.T5.2.18.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">.951</span></td>\n<td id=\"S7.T5.2.18.6\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S7.T5.2.18.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">.122</span></td>\n<td id=\"S7.T5.2.18.7\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S7.T5.2.18.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">.414</span></td>\n<td id=\"S7.T5.2.18.8\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S7.T5.2.18.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">.889</span></td>\n<td id=\"S7.T5.2.18.9\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S7.T5.2.18.9.1\" class=\"ltx_text\" style=\"font-size:50%;\">.243</span></td>\n<td id=\"S7.T5.2.18.10\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S7.T5.2.18.10.1\" class=\"ltx_text\" style=\"font-size:50%;\">.404</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Impact of Mini-Batch Sizes. We also set out to investigate the impact of the mini-batch size B𝐵B and the number of weight rows that we can use for extraction.\nTable V depicts the resulting metrics.\nThe metrics show that the smaller the mini-batch sizes are, and the more weight rows there are for extraction, the more individual training data points can be individually reconstructed.\nFor 300030003000 weight rows, even up to 505050% of the individual training data points for mini-batch sizes as large as 200 in the MNIST dataset can be perfectly extracted.\nSmall mini-batches of 202020 training data points are entirely extractable without any loss in this setting.\nAlso for the IMDB dataset, smaller batch-sizes for the same number of neurons yield much higher extraction-recall, and embeddings of data from small mini-batches of 202020 training data points are perfectly extractable, see Table II.\nThis suggests that in practice, the success of the extraction attack can be significantly increased by the central party demanding smaller mini-batch sizes from the users or initializing larger models."
        ]
    },
    "S7.T6": {
        "caption": "TABLE VI: Surrogate Data for Tuning s𝑠s. We report extraction-recall for passive extraction and extraction under adversarial weight initializations.\nFor the latter, we compare the extraction under an s𝑠s found on a surrogate dataset, and the optimal s∗superscript𝑠s^{*} found through tuning on 100 data points from the given datasets.\nAs surrogate datasets, we use MNIST for Fashion MNIST, CIFAR10 for SVHN and CIFAR100, and ImageNet for Open Images.\nResults are averaged over 5 runs.",
        "table": "<table id=\"S7.T6.4\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S7.T6.4.4\" class=\"ltx_tr\">\n<td id=\"S7.T6.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S7.T6.4.4.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Dataset</span></td>\n<td id=\"S7.T6.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S7.T6.4.4.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">Passive R</span></td>\n<td id=\"S7.T6.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\"><math id=\"S7.T6.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"s\" display=\"inline\"><semantics id=\"S7.T6.1.1.1.m1.1a\"><mi mathsize=\"70%\" id=\"S7.T6.1.1.1.m1.1.1\" xref=\"S7.T6.1.1.1.m1.1.1.cmml\">s</mi><annotation-xml encoding=\"MathML-Content\" id=\"S7.T6.1.1.1.m1.1b\"><ci id=\"S7.T6.1.1.1.m1.1.1.cmml\" xref=\"S7.T6.1.1.1.m1.1.1\">𝑠</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T6.1.1.1.m1.1c\">s</annotation></semantics></math></td>\n<td id=\"S7.T6.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S7.T6.2.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">R with </span><math id=\"S7.T6.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"s\" display=\"inline\"><semantics id=\"S7.T6.2.2.2.m1.1a\"><mi mathsize=\"70%\" id=\"S7.T6.2.2.2.m1.1.1\" xref=\"S7.T6.2.2.2.m1.1.1.cmml\">s</mi><annotation-xml encoding=\"MathML-Content\" id=\"S7.T6.2.2.2.m1.1b\"><ci id=\"S7.T6.2.2.2.m1.1.1.cmml\" xref=\"S7.T6.2.2.2.m1.1.1\">𝑠</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T6.2.2.2.m1.1c\">s</annotation></semantics></math>\n</td>\n<td id=\"S7.T6.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><math id=\"S7.T6.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"s^{*}\" display=\"inline\"><semantics id=\"S7.T6.3.3.3.m1.1a\"><msup id=\"S7.T6.3.3.3.m1.1.1\" xref=\"S7.T6.3.3.3.m1.1.1.cmml\"><mi mathsize=\"70%\" id=\"S7.T6.3.3.3.m1.1.1.2\" xref=\"S7.T6.3.3.3.m1.1.1.2.cmml\">s</mi><mo mathsize=\"70%\" id=\"S7.T6.3.3.3.m1.1.1.3\" xref=\"S7.T6.3.3.3.m1.1.1.3.cmml\">∗</mo></msup><annotation-xml encoding=\"MathML-Content\" id=\"S7.T6.3.3.3.m1.1b\"><apply id=\"S7.T6.3.3.3.m1.1.1.cmml\" xref=\"S7.T6.3.3.3.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S7.T6.3.3.3.m1.1.1.1.cmml\" xref=\"S7.T6.3.3.3.m1.1.1\">superscript</csymbol><ci id=\"S7.T6.3.3.3.m1.1.1.2.cmml\" xref=\"S7.T6.3.3.3.m1.1.1.2\">𝑠</ci><times id=\"S7.T6.3.3.3.m1.1.1.3.cmml\" xref=\"S7.T6.3.3.3.m1.1.1.3\"></times></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T6.3.3.3.m1.1c\">s^{*}</annotation></semantics></math></td>\n<td id=\"S7.T6.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S7.T6.4.4.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">R with </span><math id=\"S7.T6.4.4.4.m1.1\" class=\"ltx_Math\" alttext=\"s^{*}\" display=\"inline\"><semantics id=\"S7.T6.4.4.4.m1.1a\"><msup id=\"S7.T6.4.4.4.m1.1.1\" xref=\"S7.T6.4.4.4.m1.1.1.cmml\"><mi mathsize=\"70%\" id=\"S7.T6.4.4.4.m1.1.1.2\" xref=\"S7.T6.4.4.4.m1.1.1.2.cmml\">s</mi><mo mathsize=\"70%\" id=\"S7.T6.4.4.4.m1.1.1.3\" xref=\"S7.T6.4.4.4.m1.1.1.3.cmml\">∗</mo></msup><annotation-xml encoding=\"MathML-Content\" id=\"S7.T6.4.4.4.m1.1b\"><apply id=\"S7.T6.4.4.4.m1.1.1.cmml\" xref=\"S7.T6.4.4.4.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S7.T6.4.4.4.m1.1.1.1.cmml\" xref=\"S7.T6.4.4.4.m1.1.1\">superscript</csymbol><ci id=\"S7.T6.4.4.4.m1.1.1.2.cmml\" xref=\"S7.T6.4.4.4.m1.1.1.2\">𝑠</ci><times id=\"S7.T6.4.4.4.m1.1.1.3.cmml\" xref=\"S7.T6.4.4.4.m1.1.1.3\"></times></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T6.4.4.4.m1.1c\">s^{*}</annotation></semantics></math>\n</td>\n</tr>\n<tr id=\"S7.T6.4.5\" class=\"ltx_tr\">\n<td id=\"S7.T6.4.5.1\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T6.4.5.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Fashion MNIST</span></td>\n<td id=\"S7.T6.4.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T6.4.5.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.09</span></td>\n<td id=\"S7.T6.4.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T6.4.5.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.7</span></td>\n<td id=\"S7.T6.4.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T6.4.5.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.22</span></td>\n<td id=\"S7.T6.4.5.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T6.4.5.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.77</span></td>\n<td id=\"S7.T6.4.5.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T6.4.5.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.31</span></td>\n</tr>\n<tr id=\"S7.T6.4.6\" class=\"ltx_tr\">\n<td id=\"S7.T6.4.6.1\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T6.4.6.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">SVHN</span></td>\n<td id=\"S7.T6.4.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T6.4.6.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.22</span></td>\n<td id=\"S7.T6.4.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T6.4.6.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.95</span></td>\n<td id=\"S7.T6.4.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T6.4.6.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.26</span></td>\n<td id=\"S7.T6.4.6.5\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T6.4.6.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.97</span></td>\n<td id=\"S7.T6.4.6.6\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T6.4.6.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.40</span></td>\n</tr>\n<tr id=\"S7.T6.4.7\" class=\"ltx_tr\">\n<td id=\"S7.T6.4.7.1\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T6.4.7.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">CIFAR100</span></td>\n<td id=\"S7.T6.4.7.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T6.4.7.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.25</span></td>\n<td id=\"S7.T6.4.7.3\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T6.4.7.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.95</span></td>\n<td id=\"S7.T6.4.7.4\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T6.4.7.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.42</span></td>\n<td id=\"S7.T6.4.7.5\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T6.4.7.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.95</span></td>\n<td id=\"S7.T6.4.7.6\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T6.4.7.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.42</span></td>\n</tr>\n<tr id=\"S7.T6.4.8\" class=\"ltx_tr\">\n<td id=\"S7.T6.4.8.1\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T6.4.8.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Open Images</span></td>\n<td id=\"S7.T6.4.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T6.4.8.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.21</span></td>\n<td id=\"S7.T6.4.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T6.4.8.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.99</span></td>\n<td id=\"S7.T6.4.8.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T6.4.8.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.44</span></td>\n<td id=\"S7.T6.4.8.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T6.4.8.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.99</span></td>\n<td id=\"S7.T6.4.8.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T6.4.8.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.44</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Attacker without Auxiliary Data.\nWe experiment with an attacker who does not have access to a small mini-batch of data from the users’ distribution to tune the scaling factor s𝑠s of our trap weights.\nIn this setup, the only knowledge an attacker holds is about the dimensionality of the users’ data which it needs to instantiate an adequate model architecture.\nWe evaluate three attacks in this setup.\n1) Exploiting passive data leakage and composing a tuning dataset: the attacker randomly initializes the model in a first round of the protocol.\nOur results in Table I show that also randomly initialized models’ gradients leak significant fractions of the users’ data (MNIST 6.1%, CIFAR10 25.9%, and ImageNet 21.7%).\nBy plotting the user’s gradients and eyeballing which data points resemble natural images, the attacker can build a tuning set for s𝑠s.\nSince we only require a maximum of 100 data points to find the optimal values for s𝑠s per dataset in Table IV, the attacker only has to inspect the gradients of 17, 4, and 5 users for MNIST, CIFAR10, and ImageNet, respectively in the first round of the protocol.\nOn the selected data, they can tune s𝑠s and use it in every subsequent iteration.\nWe performed tuning on 100 data points obtained through passive extraction and obtained the same s𝑠s as through tuning on a random mini-batch of data (0.7, 0.95, and 0.99 for MNIST, CIFAR10, and ImageNet, respectively).\n2) Exploiting raw passive data leakage: Since manually, selecting suitable data points is time-consuming, we propose an alternative approach where the attacker uses all extracted data points with are in a valid range for input pixels ([0,1]) from the passive extraction on non-adversarially initialized model weights in the first round of the protocol.\nThese data points are not necessarily individually extracted user data points as we show in Figure 9 in Appendix D.2.\nBut the attacker can still consider them as a tuning dataset for s𝑠s and evaluate the extraction-recall on this dataset when initializing the shared model with different trap weights to tune s𝑠s.\nOur results in Table XIII show that for CIFAR10 and ImageNet, the best s𝑠s found on these passively reconstructed data points are equal to the best s𝑠s obtained directly by tuning on one mini-batch of the original data.\nFor MNIST, the s𝑠s on the extracted gradients differs slightly from the original best s𝑠s (0.75 vs. 0.7).\nWe suspect these changes to result from MNIST data being much sparser (many more zero features) than the extracted gradients in Figure 9(a).\n3) Using a surrogate dataset of same data dimensions: Lastly, the attacker can tune s𝑠s on a surrogate dataset of the same dimension (but potentially different distribution) than the users’ data.\nWe compare extraction-recall of an adversarial weight initialization with s𝑠s found on a surrogate dataset and the optimal s∗superscript𝑠s^{*} found on the actual dataset for Fashion MNIST, SVHN, CIFAR100, and Open Images\n [30] in Table VI.\nOur results highlight that extraction with the surrogate s𝑠s obtained through tuning on MNIST, CIFAR10, and ImageNet, already yields a significantly higher success than passive extraction on non-manipulated weights.\nFurthermore, the closer the surrogate dataset’s distribution is to the users’ dataset, the closer s𝑠s and s∗superscript𝑠s^{*}.\nEspecially for CIFAR10 and CIFAR100, and ImageNet and Open Images, we find that s=s∗𝑠superscript𝑠s=s^{*} which leads to highest extraction success."
        ]
    },
    "S7.T7": {
        "caption": "TABLE VII: Local Accuracy Improvement and Extraction Success with FedAvg.\nWe present results for FedAvg where each user holds five mini-batches of size B and computes 1,2,3,4, or 5 local epochs of training with the FC-NN from Table IX.\nThe ΔaccsubscriptΔacc\\Delta_{\\text{acc}} indicates the accuracy improvement on the user’s local data w.r.t. the received shared model (initially around 10% accuracy).\nThe extraction-precision (P) and extraction-recall (R) for every B stay at the same high level even after multiple local epochs of training.",
        "table": "<table id=\"S7.T7.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S7.T7.2.3\" class=\"ltx_tr\">\n<td id=\"S7.T7.2.3.1\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S7.T7.2.3.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">local</span></td>\n<td id=\"S7.T7.2.3.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S7.T7.2.3.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">local</span></td>\n<td id=\"S7.T7.2.3.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span id=\"S7.T7.2.3.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">MNIST</span></td>\n<td id=\"S7.T7.2.3.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span id=\"S7.T7.2.3.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">CIFAR10</span></td>\n</tr>\n<tr id=\"S7.T7.2.2\" class=\"ltx_tr\">\n<td id=\"S7.T7.2.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T7.2.2.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">epochs</span></td>\n<td id=\"S7.T7.2.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T7.2.2.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">B</span></td>\n<td id=\"S7.T7.1.1.1\" class=\"ltx_td ltx_align_right\">\n<span id=\"S7.T7.1.1.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">↑</span><math id=\"S7.T7.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\Delta_{\\text{acc}}\" display=\"inline\"><semantics id=\"S7.T7.1.1.1.m1.1a\"><msub id=\"S7.T7.1.1.1.m1.1.1\" xref=\"S7.T7.1.1.1.m1.1.1.cmml\"><mi mathsize=\"50%\" mathvariant=\"normal\" id=\"S7.T7.1.1.1.m1.1.1.2\" xref=\"S7.T7.1.1.1.m1.1.1.2.cmml\">Δ</mi><mtext mathsize=\"50%\" id=\"S7.T7.1.1.1.m1.1.1.3\" xref=\"S7.T7.1.1.1.m1.1.1.3a.cmml\">acc</mtext></msub><annotation-xml encoding=\"MathML-Content\" id=\"S7.T7.1.1.1.m1.1b\"><apply id=\"S7.T7.1.1.1.m1.1.1.cmml\" xref=\"S7.T7.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S7.T7.1.1.1.m1.1.1.1.cmml\" xref=\"S7.T7.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S7.T7.1.1.1.m1.1.1.2.cmml\" xref=\"S7.T7.1.1.1.m1.1.1.2\">Δ</ci><ci id=\"S7.T7.1.1.1.m1.1.1.3a.cmml\" xref=\"S7.T7.1.1.1.m1.1.1.3\"><mtext mathsize=\"35%\" id=\"S7.T7.1.1.1.m1.1.1.3.cmml\" xref=\"S7.T7.1.1.1.m1.1.1.3\">acc</mtext></ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T7.1.1.1.m1.1c\">\\Delta_{\\text{acc}}</annotation></semantics></math>\n</td>\n<td id=\"S7.T7.2.2.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.2.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">P</span></td>\n<td id=\"S7.T7.2.2.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.2.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">R</span></td>\n<td id=\"S7.T7.2.2.2\" class=\"ltx_td ltx_align_right\">\n<span id=\"S7.T7.2.2.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">↑</span><math id=\"S7.T7.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\Delta_{\\text{acc}}\" display=\"inline\"><semantics id=\"S7.T7.2.2.2.m1.1a\"><msub id=\"S7.T7.2.2.2.m1.1.1\" xref=\"S7.T7.2.2.2.m1.1.1.cmml\"><mi mathsize=\"50%\" mathvariant=\"normal\" id=\"S7.T7.2.2.2.m1.1.1.2\" xref=\"S7.T7.2.2.2.m1.1.1.2.cmml\">Δ</mi><mtext mathsize=\"50%\" id=\"S7.T7.2.2.2.m1.1.1.3\" xref=\"S7.T7.2.2.2.m1.1.1.3a.cmml\">acc</mtext></msub><annotation-xml encoding=\"MathML-Content\" id=\"S7.T7.2.2.2.m1.1b\"><apply id=\"S7.T7.2.2.2.m1.1.1.cmml\" xref=\"S7.T7.2.2.2.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S7.T7.2.2.2.m1.1.1.1.cmml\" xref=\"S7.T7.2.2.2.m1.1.1\">subscript</csymbol><ci id=\"S7.T7.2.2.2.m1.1.1.2.cmml\" xref=\"S7.T7.2.2.2.m1.1.1.2\">Δ</ci><ci id=\"S7.T7.2.2.2.m1.1.1.3a.cmml\" xref=\"S7.T7.2.2.2.m1.1.1.3\"><mtext mathsize=\"35%\" id=\"S7.T7.2.2.2.m1.1.1.3.cmml\" xref=\"S7.T7.2.2.2.m1.1.1.3\">acc</mtext></ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T7.2.2.2.m1.1c\">\\Delta_{\\text{acc}}</annotation></semantics></math>\n</td>\n<td id=\"S7.T7.2.2.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.2.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">P</span></td>\n<td id=\"S7.T7.2.2.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.2.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">R</span></td>\n</tr>\n<tr id=\"S7.T7.2.4\" class=\"ltx_tr\">\n<td id=\"S7.T7.2.4.1\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T7.2.4.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">1</span></td>\n<td id=\"S7.T7.2.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T7.2.4.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">10</span></td>\n<td id=\"S7.T7.2.4.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S7.T7.2.4.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.296</span></td>\n<td id=\"S7.T7.2.4.4\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S7.T7.2.4.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.238</span></td>\n<td id=\"S7.T7.2.4.5\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S7.T7.2.4.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.704</span></td>\n<td id=\"S7.T7.2.4.6\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S7.T7.2.4.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.264</span></td>\n<td id=\"S7.T7.2.4.7\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S7.T7.2.4.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.279</span></td>\n<td id=\"S7.T7.2.4.8\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S7.T7.2.4.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.584</span></td>\n</tr>\n<tr id=\"S7.T7.2.5\" class=\"ltx_tr\">\n<td id=\"S7.T7.2.5.1\" class=\"ltx_td\"></td>\n<td id=\"S7.T7.2.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T7.2.5.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">20</span></td>\n<td id=\"S7.T7.2.5.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.5.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.220</span></td>\n<td id=\"S7.T7.2.5.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.5.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.175</span></td>\n<td id=\"S7.T7.2.5.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.5.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.496</span></td>\n<td id=\"S7.T7.2.5.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.5.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.238</span></td>\n<td id=\"S7.T7.2.5.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.5.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.277</span></td>\n<td id=\"S7.T7.2.5.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.5.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.466</span></td>\n</tr>\n<tr id=\"S7.T7.2.6\" class=\"ltx_tr\">\n<td id=\"S7.T7.2.6.1\" class=\"ltx_td\"></td>\n<td id=\"S7.T7.2.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T7.2.6.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">40</span></td>\n<td id=\"S7.T7.2.6.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.6.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.289</span></td>\n<td id=\"S7.T7.2.6.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.6.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.109</span></td>\n<td id=\"S7.T7.2.6.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.6.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.280</span></td>\n<td id=\"S7.T7.2.6.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.6.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.188</span></td>\n<td id=\"S7.T7.2.6.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.6.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.127</span></td>\n<td id=\"S7.T7.2.6.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.6.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.213</span></td>\n</tr>\n<tr id=\"S7.T7.2.7\" class=\"ltx_tr\">\n<td id=\"S7.T7.2.7.1\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T7.2.7.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">2</span></td>\n<td id=\"S7.T7.2.7.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T7.2.7.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">10</span></td>\n<td id=\"S7.T7.2.7.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.7.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.384</span></td>\n<td id=\"S7.T7.2.7.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.7.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.252</span></td>\n<td id=\"S7.T7.2.7.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.7.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.704</span></td>\n<td id=\"S7.T7.2.7.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.7.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.316</span></td>\n<td id=\"S7.T7.2.7.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.7.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.311</span></td>\n<td id=\"S7.T7.2.7.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.7.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.560</span></td>\n</tr>\n<tr id=\"S7.T7.2.8\" class=\"ltx_tr\">\n<td id=\"S7.T7.2.8.1\" class=\"ltx_td\"></td>\n<td id=\"S7.T7.2.8.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T7.2.8.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">20</span></td>\n<td id=\"S7.T7.2.8.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.8.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.772</span></td>\n<td id=\"S7.T7.2.8.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.8.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.184</span></td>\n<td id=\"S7.T7.2.8.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.8.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.478</span></td>\n<td id=\"S7.T7.2.8.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.8.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.296</span></td>\n<td id=\"S7.T7.2.8.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.8.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.295</span></td>\n<td id=\"S7.T7.2.8.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.8.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.476</span></td>\n</tr>\n<tr id=\"S7.T7.2.9\" class=\"ltx_tr\">\n<td id=\"S7.T7.2.9.1\" class=\"ltx_td\"></td>\n<td id=\"S7.T7.2.9.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T7.2.9.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">40</span></td>\n<td id=\"S7.T7.2.9.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.9.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.671</span></td>\n<td id=\"S7.T7.2.9.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.9.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.111</span></td>\n<td id=\"S7.T7.2.9.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.9.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.264</span></td>\n<td id=\"S7.T7.2.9.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.9.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.282</span></td>\n<td id=\"S7.T7.2.9.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.9.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.126</span></td>\n<td id=\"S7.T7.2.9.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.9.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.247</span></td>\n</tr>\n<tr id=\"S7.T7.2.10\" class=\"ltx_tr\">\n<td id=\"S7.T7.2.10.1\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T7.2.10.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">3</span></td>\n<td id=\"S7.T7.2.10.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T7.2.10.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">10</span></td>\n<td id=\"S7.T7.2.10.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.10.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.604</span></td>\n<td id=\"S7.T7.2.10.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.10.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.241</span></td>\n<td id=\"S7.T7.2.10.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.10.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.712</span></td>\n<td id=\"S7.T7.2.10.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.10.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.372</span></td>\n<td id=\"S7.T7.2.10.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.10.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.329</span></td>\n<td id=\"S7.T7.2.10.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.10.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.596</span></td>\n</tr>\n<tr id=\"S7.T7.2.11\" class=\"ltx_tr\">\n<td id=\"S7.T7.2.11.1\" class=\"ltx_td\"></td>\n<td id=\"S7.T7.2.11.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T7.2.11.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">20</span></td>\n<td id=\"S7.T7.2.11.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.11.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.790</span></td>\n<td id=\"S7.T7.2.11.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.11.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.189</span></td>\n<td id=\"S7.T7.2.11.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.11.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.544</span></td>\n<td id=\"S7.T7.2.11.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.11.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.420</span></td>\n<td id=\"S7.T7.2.11.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.11.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.277</span></td>\n<td id=\"S7.T7.2.11.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.11.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.496</span></td>\n</tr>\n<tr id=\"S7.T7.2.12\" class=\"ltx_tr\">\n<td id=\"S7.T7.2.12.1\" class=\"ltx_td\"></td>\n<td id=\"S7.T7.2.12.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T7.2.12.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">40</span></td>\n<td id=\"S7.T7.2.12.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.12.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.823</span></td>\n<td id=\"S7.T7.2.12.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.12.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.111</span></td>\n<td id=\"S7.T7.2.12.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.12.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.283</span></td>\n<td id=\"S7.T7.2.12.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.12.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.324</span></td>\n<td id=\"S7.T7.2.12.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.12.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.138</span></td>\n<td id=\"S7.T7.2.12.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.12.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.257</span></td>\n</tr>\n<tr id=\"S7.T7.2.13\" class=\"ltx_tr\">\n<td id=\"S7.T7.2.13.1\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T7.2.13.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">4</span></td>\n<td id=\"S7.T7.2.13.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T7.2.13.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">10</span></td>\n<td id=\"S7.T7.2.13.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.13.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.644</span></td>\n<td id=\"S7.T7.2.13.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.13.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.251</span></td>\n<td id=\"S7.T7.2.13.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.13.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.692</span></td>\n<td id=\"S7.T7.2.13.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.13.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.396</span></td>\n<td id=\"S7.T7.2.13.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.13.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.332</span></td>\n<td id=\"S7.T7.2.13.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.13.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.632</span></td>\n</tr>\n<tr id=\"S7.T7.2.14\" class=\"ltx_tr\">\n<td id=\"S7.T7.2.14.1\" class=\"ltx_td\"></td>\n<td id=\"S7.T7.2.14.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T7.2.14.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">20</span></td>\n<td id=\"S7.T7.2.14.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.14.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.848</span></td>\n<td id=\"S7.T7.2.14.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.14.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.178</span></td>\n<td id=\"S7.T7.2.14.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.14.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.494</span></td>\n<td id=\"S7.T7.2.14.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.14.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.440</span></td>\n<td id=\"S7.T7.2.14.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.14.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.288</span></td>\n<td id=\"S7.T7.2.14.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.14.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.478</span></td>\n</tr>\n<tr id=\"S7.T7.2.15\" class=\"ltx_tr\">\n<td id=\"S7.T7.2.15.1\" class=\"ltx_td\"></td>\n<td id=\"S7.T7.2.15.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T7.2.15.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">40</span></td>\n<td id=\"S7.T7.2.15.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.15.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.825</span></td>\n<td id=\"S7.T7.2.15.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.15.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.113</span></td>\n<td id=\"S7.T7.2.15.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.15.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.273</span></td>\n<td id=\"S7.T7.2.15.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.15.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.415</span></td>\n<td id=\"S7.T7.2.15.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.15.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.138</span></td>\n<td id=\"S7.T7.2.15.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.15.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.256</span></td>\n</tr>\n<tr id=\"S7.T7.2.16\" class=\"ltx_tr\">\n<td id=\"S7.T7.2.16.1\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T7.2.16.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">5</span></td>\n<td id=\"S7.T7.2.16.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T7.2.16.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">10</span></td>\n<td id=\"S7.T7.2.16.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.16.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.604</span></td>\n<td id=\"S7.T7.2.16.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.16.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.270</span></td>\n<td id=\"S7.T7.2.16.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.16.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.732</span></td>\n<td id=\"S7.T7.2.16.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.16.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.412</span></td>\n<td id=\"S7.T7.2.16.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.16.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.354</span></td>\n<td id=\"S7.T7.2.16.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.16.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.620</span></td>\n</tr>\n<tr id=\"S7.T7.2.17\" class=\"ltx_tr\">\n<td id=\"S7.T7.2.17.1\" class=\"ltx_td\"></td>\n<td id=\"S7.T7.2.17.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T7.2.17.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">20</span></td>\n<td id=\"S7.T7.2.17.3\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.17.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.870</span></td>\n<td id=\"S7.T7.2.17.4\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.17.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.178</span></td>\n<td id=\"S7.T7.2.17.5\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.17.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.494</span></td>\n<td id=\"S7.T7.2.17.6\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.17.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.492</span></td>\n<td id=\"S7.T7.2.17.7\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.17.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.289</span></td>\n<td id=\"S7.T7.2.17.8\" class=\"ltx_td ltx_align_right\"><span id=\"S7.T7.2.17.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.502</span></td>\n</tr>\n<tr id=\"S7.T7.2.18\" class=\"ltx_tr\">\n<td id=\"S7.T7.2.18.1\" class=\"ltx_td ltx_border_bb\"></td>\n<td id=\"S7.T7.2.18.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T7.2.18.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">40</span></td>\n<td id=\"S7.T7.2.18.3\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S7.T7.2.18.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.873</span></td>\n<td id=\"S7.T7.2.18.4\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S7.T7.2.18.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.119</span></td>\n<td id=\"S7.T7.2.18.5\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S7.T7.2.18.5.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.283</span></td>\n<td id=\"S7.T7.2.18.6\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S7.T7.2.18.6.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.461</span></td>\n<td id=\"S7.T7.2.18.7\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S7.T7.2.18.7.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.139</span></td>\n<td id=\"S7.T7.2.18.8\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S7.T7.2.18.8.1\" class=\"ltx_text\" style=\"font-size:50%;\">0.277</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "We now turn to our active attack, which implements our trap weights to amplify the vulnerability exploited by passive attacks.\nThis amplification is controlled by the scaling factor ",
                "s",
                "𝑠",
                "s",
                " in the trap weights. We first evaluate the impact of this scaling factor on the reconstruction quality of individual training data points over a mini-batch of 100 data points and 1000 neurons.",
                "Table",
                " ",
                "IV",
                " depicts the results, averaged over ten different random adversarial initializations.\nWe can see that the best scaling factor for MNIST, when it comes to the ",
                "extraction-recall",
                ", is ",
                "s",
                "=",
                "0.7",
                "𝑠",
                "0.7",
                "s=0.7",
                ".\nWith this scaling factor, we are able to extract on average ",
                "54.0",
                "54.0",
                "54.0",
                "% of the individual training data points which were involved in the users’ gradient computations.\nThis is an improvement by around factor nine to the passive attack.\nFor CIFAR10 and Imagenet, the best scaling factors concerning ",
                "extraction-recall",
                " are ",
                "s",
                "=",
                "0.95",
                "𝑠",
                "0.95",
                "s=0.95",
                ", and ",
                "s",
                "=",
                "0.99",
                "𝑠",
                "0.99",
                "s=0.99",
                ", which allow for a perfect reconstruction of ",
                "54.0",
                "54.0",
                "54.0",
                "%, and ",
                "45.7",
                "45.7",
                "45.7",
                "% of the individual training data points, respectively, for 1000 neurons and a mini-batch size of ",
                "100",
                "100",
                "100",
                " data points.\nThereby, the active attack is more than twice as successful as the passive attack for extracting individual training data points in these datasets.\n",
                "Figure",
                " ",
                "11",
                ", ",
                "Figure",
                " ",
                "12",
                ", and ",
                "Figure",
                " ",
                "13",
                " in the Appendix ",
                "D",
                " show the visual reconstruction results of the best run for the MNIST, CIFAR10, and ImageNet dataset, respectively.\nFor CIFAR10, we additionally present extraction success when all local data stems from the same single class.",
                "Similar improvements of performance could be achieved for the IMDB dataset.\nThe best extraction was achieved also with ",
                "s",
                "=",
                "0.99",
                "𝑠",
                "0.99",
                "s=0.99",
                ", for which, with 1000 neurons and mini-batches of 100 data points, we obtained an ",
                "extraction-recall",
                " of ",
                "65.4",
                "65.4",
                "65.4",
                "%, which is around 2.5 time as high as the passive attack, see ",
                "Table",
                " ",
                "II",
                ".",
                "In ",
                "Figure",
                " ",
                "4",
                ", we show the influence of the scaling factor ",
                "s",
                "𝑠",
                "s",
                ", our method’s hyperparameter, on the distribution of our trap weights.\nThe case ",
                "s",
                "=",
                "1",
                "𝑠",
                "1",
                "s=1",
                " corresponds to the baseline where positive components in the trap weights are not scaled down.\nThe figure shows that the more ",
                "s",
                "𝑠",
                "s",
                " deviates from ",
                "1",
                "1",
                "1",
                ", the larger the difference between a random distribution and our trap weights.\nFor CIFAR10 and ImageNet (",
                "s",
                "=",
                "0.95",
                "𝑠",
                "0.95",
                "s=0.95",
                ", and ",
                "s",
                "=",
                "0.99",
                "𝑠",
                "0.99",
                "s=0.99",
                "), our trap weights’s distribution is very close to the the random distribution, making our trap weights more stealthy.\nThe best scaling factor for MNIST, ",
                "s",
                "=",
                "0.7",
                "𝑠",
                "0.7",
                "s=0.7",
                ", is significant smaller than for ImageNet and CIFAR10 due to the sparsity in the data (the background in MNIST images consists of zero pixels).\nOur experiments indicate that with decreasing sparsity and increasing data dimensionality, ",
                "s",
                "𝑠",
                "s",
                " approaches 1.\nEspecially the last observation makes sense since scaling more positive components with a factor closer to 1 is in effect of the weighted sum equivalent to scaling fewer positive components with a factor much smaller than 1.\nThereby, our trap weights increase in stealthiness with increasing complexity of the data to be extracted.",
                "As hypothesized above, from ",
                "Table",
                " ",
                "IV",
                ", we furthermore confirm that the ",
                "extraction-recall",
                " of our attack is related to the percentage of ",
                "active neurons",
                ":\nWhen very few neurons are activated, it is not possible to extract large numbers of individual data points due to the lack of gradients to extract them from.\nHowever, when the percentage of ",
                "active neurons",
                " is high, the ",
                "extraction-recall",
                " also becomes very small, which is due to the fact that each neuron gets activated by several input data points, and thereby, individual extraction is impossible.",
                "Attacker without Auxiliary Data.",
                "\nWe experiment with an attacker who does not have access to a small mini-batch of data from the users’ distribution to tune the scaling factor ",
                "s",
                "𝑠",
                "s",
                " of our trap weights.\nIn this setup, the only knowledge an attacker holds is about the dimensionality of the users’ data which it needs to instantiate an adequate model architecture.\nWe evaluate three attacks in this setup.\n",
                "1) Exploiting passive data leakage and composing a tuning dataset:",
                " the attacker randomly initializes the model in a first round of the protocol.\nOur results in ",
                "Table",
                " ",
                "I",
                " show that also randomly initialized models’ gradients leak significant fractions of the users’ data (MNIST 6.1%, CIFAR10 25.9%, and ImageNet 21.7%).\nBy plotting the user’s gradients and eyeballing which data points resemble natural images, the attacker can build a tuning set for ",
                "s",
                "𝑠",
                "s",
                ".\nSince we only require a maximum of 100 data points to find the optimal values for ",
                "s",
                "𝑠",
                "s",
                " per dataset in ",
                "Table",
                " ",
                "IV",
                ", the attacker only has to inspect the gradients of 17, 4, and 5 users for MNIST, CIFAR10, and ImageNet, respectively in the first round of the protocol.\nOn the selected data, they can tune ",
                "s",
                "𝑠",
                "s",
                " and use it in every subsequent iteration.\nWe performed tuning on 100 data points obtained through passive extraction and obtained the same ",
                "s",
                "𝑠",
                "s",
                " as through tuning on a random mini-batch of data (0.7, 0.95, and 0.99 for MNIST, CIFAR10, and ImageNet, respectively).\n",
                "2) Exploiting raw passive data leakage:",
                " Since manually, selecting suitable data points is time-consuming, we propose an alternative approach where the attacker uses all extracted data points with are in a valid range for input pixels ([0,1]) from the passive extraction on non-adversarially initialized model weights in the first round of the protocol.\nThese data points are not necessarily individually extracted user data points as we show in ",
                "Figure",
                " ",
                "9",
                " in Appendix ",
                "D.2",
                ".\nBut the attacker can still consider them as a tuning dataset for ",
                "s",
                "𝑠",
                "s",
                " and evaluate the ",
                "extraction-recall",
                " on this dataset when initializing the shared model with different trap weights to tune ",
                "s",
                "𝑠",
                "s",
                ".\nOur results in ",
                "Table",
                " ",
                "XIII",
                " show that for CIFAR10 and ImageNet, the best ",
                "s",
                "𝑠",
                "s",
                " found on these passively reconstructed data points are equal to the best ",
                "s",
                "𝑠",
                "s",
                " obtained directly by tuning on one mini-batch of the original data.\nFor MNIST, the ",
                "s",
                "𝑠",
                "s",
                " on the extracted gradients differs slightly from the original best ",
                "s",
                "𝑠",
                "s",
                " (0.75 vs. 0.7).\nWe suspect these changes to result from MNIST data being much sparser (many more zero features) than the extracted gradients in ",
                "Figure",
                " ",
                "9(a)",
                ".\n",
                "3) Using a surrogate dataset of same data dimensions:",
                " Lastly, the attacker can tune ",
                "s",
                "𝑠",
                "s",
                " on a surrogate dataset of the same dimension (but potentially different distribution) than the users’ data.\nWe compare ",
                "extraction-recall",
                " of an adversarial weight initialization with ",
                "s",
                "𝑠",
                "s",
                " found on a surrogate dataset and the optimal ",
                "s",
                "∗",
                "superscript",
                "𝑠",
                "s^{*}",
                " found on the actual dataset for Fashion MNIST, SVHN, CIFAR100, and Open Images\n ",
                "[",
                "30",
                "]",
                " in ",
                "Table",
                " ",
                "VI",
                ".\nOur results highlight that extraction with the surrogate ",
                "s",
                "𝑠",
                "s",
                " obtained through tuning on MNIST, CIFAR10, and ImageNet, already yields a significantly higher success than passive extraction on non-manipulated weights.\nFurthermore, the closer the surrogate dataset’s distribution is to the users’ dataset, the closer ",
                "s",
                "𝑠",
                "s",
                " and ",
                "s",
                "∗",
                "superscript",
                "𝑠",
                "s^{*}",
                ".\nEspecially for CIFAR10 and CIFAR100, and ImageNet and Open Images, we find that ",
                "s",
                "=",
                "s",
                "∗",
                "𝑠",
                "superscript",
                "𝑠",
                "s=s^{*}",
                " which leads to highest extraction success.",
                "Impact of Data Labels.",
                " Additionally, we investigated whether this high reconstruction success could also be achieved =in a non-IID setting when users hold local mini-batches of data that belongs to one single class, different from the other users.\nThis is a particularly challenging setting for prior work on optimization-based attacks that end up reconstructing average points rather than individual points exactly.\nInstead, ",
                "Figure",
                " ",
                "12(b)",
                " and ",
                "Table",
                " ",
                "XII",
                " in Appendix ",
                "D.2",
                " show on CIFAR10, how our method remains able to perfectly extract individual data points from the gradients even when all points stem from the same class.",
                "Impact of Mini-Batch Sizes.",
                " We also set out to investigate the impact of the mini-batch size ",
                "B",
                "𝐵",
                "B",
                " and the number of weight rows that we can use for extraction.\n",
                "Table",
                " ",
                "V",
                " depicts the resulting metrics.\nThe metrics show that the smaller the mini-batch sizes are, and the more weight rows there are for extraction, the more individual training data points can be individually reconstructed.\nFor ",
                "3000",
                "3000",
                "3000",
                " weight rows, even up to ",
                "50",
                "50",
                "50",
                "% of the individual training data points for mini-batch sizes as large as 200 in the MNIST dataset can be perfectly extracted.\nSmall mini-batches of ",
                "20",
                "20",
                "20",
                " training data points are entirely extractable without any loss in this setting.\nAlso for the IMDB dataset, smaller batch-sizes for the same number of neurons yield much higher ",
                "extraction-recall",
                ", and embeddings of data from small mini-batches of ",
                "20",
                "20",
                "20",
                " training data points are perfectly extractable, see ",
                "Table",
                " ",
                "II",
                ".\nThis suggests that in practice, the success of the extraction attack can be significantly increased by the central party demanding smaller mini-batch sizes from the users or initializing larger models.",
                "Impact of Lossy Layers.",
                "\nFor perfect extraction of data points in CNN architectures, our attack requires the input to the first fully-connected layer to have at least as many parameters as the original input data point.\nCNN architectures can contain pooling layers to reduce input size.\nIn Appendix ",
                "D.3",
                ", we evaluate the impact of pooling on the fidelity of the extracted data.\nOur evaluation shows that pooling results in some form of compression of the user’s input data, see for example ",
                "Figure",
                " ",
                "16(a)",
                ".\nIn Appendix ",
                "B.2",
                ", we show how the central party can implement an alternative to pooling for size-reduction in CNNs based on convolutional layers which still allows for prefect extracability, as long as there are enough model parameters.\nWe also evaluate the effect of dropout on the fidelity of the extracted data.\n",
                "Figure",
                " ",
                "15",
                " and ",
                "17",
                " visualize the effect of pure dropout, while ",
                "Figure",
                " ",
                "16",
                " and ",
                "18",
                " visualize the joint effect of dropout and pooling.\nTo increase fidelity of extraction under lossy layers, an attacker can apply post-processing, such as de-compression.",
                "VGG and ResNet.",
                "\nIn addition to our custom FC-NN and CNN architecture from ",
                "Table",
                " ",
                "IX",
                ", we experimented with a VGG7 and a ResNet20 ",
                "[",
                "44",
                "]",
                " architecture.\nFor VGG7, we initialize all convolutional layers as illustrated in ",
                "Figure",
                " ",
                "6",
                " in Appendix ",
                "B.1",
                ", and the fully-connected layer directly after the convolutional layers with our trap weights.\nFor the ResNet20, we only initialize the first convolutional layer according to ",
                "Figure",
                " ",
                "6",
                ".\nThanks to the skip connections, the remaining convolutional layers can remain unchanged, apart from the convolutional filters whose output is added to the output of the skip connections.\nThese need to be set to zero, such that the input data can be propagated unaltered over the skip-connections to the fully-connected layer that we initialize with our trap weights.\nWe set the last pooling layer before this fully-connected layer in ResNet20 to implement average pooling.\nOur extraction results for ImageNet are depicted in ",
                "Figure",
                " ",
                "5",
                ".\nThe compression of extracted data in comparison to the original data results from the pooling layers in both architectures that reduce input dimensions.",
                "Impact of Local Mini-Batch-Averaging.",
                "\nAdditionally, we looked into the effect of averaging over the gradients of multiple mini-batches, ",
                "e.g",
                ".",
                " the average of gradients received from multiple users.\nThe results in ",
                "Table",
                " ",
                "XI",
                " in Appendix ",
                "D.2",
                " show that through averaging, the attack success is significantly reduced.\nAlready when averaging over ",
                "20",
                "20",
                "20",
                " mini-batches of size ",
                "B",
                "=",
                "100",
                "𝐵",
                "100",
                "B=100",
                " in the MNIST dataset, the average ",
                "extraction-recall",
                " drops from ",
                "54.0",
                "54.0",
                "54.0",
                "% to ",
                "2",
                "2",
                "2",
                "% because multiple data points overlay in the gradients.\nThis highlights that the central party needs to perform the extraction before the averaging operation.\nThe following section shows that this simple change to the protocol is easily implemented by an actively dishonest central party, even for standard FL libraries.",
                "FedAvg.",
                "\nTo validate our theoretical insights from ",
                "Section",
                " ",
                "5.3",
                " which highlights that even under FedAvg, perfect extraction of individual data points is possible, we run FedAvg experiments in which users hold five mini-batches of {10, 20, or 40} different data points (yielding a total of 50, 100, and 200 local data points per user), and perform {1, 2, 3, 4, or 5} local training epochs.\nIn ",
                "Table",
                " ",
                "VII",
                ", we depict results from the FC-NN architecture from ",
                "Table",
                " ",
                "IX",
                " on MNIST and CIFAR10.\nOur results highlight that the while accuracy on the users’ data significantly increases through the local training, the extraction success stays constant over multiple local epochs.\nWe even observe a slight increase in ",
                "extraction-recall",
                ".\nFor example, we can extract ",
                "58.4",
                "58.4",
                "58.4",
                "% of data points from users who hold five mini-batches with ten data points each after one epoch, while this number increases to ",
                "62",
                "62",
                "62",
                "% after five local epochs of training.\nThis finding is congruent with our finding in ",
                "Table",
                " ",
                "III",
                ", where we show that extraction success increases with convergence.\nFor CNNs, the extraction success degrades over multiple local epochs of training.\nThis is due to the convolutional filters that, after a local update, do not have the zero-elements anymore which prevent features in the forward-pass from overlapping.\nFor our CNN architecture from ",
                "Table",
                " ",
                "IX",
                ", we report ",
                "ℓ",
                "2",
                "subscript",
                "ℓ",
                "2",
                "\\ell_{2}",
                "-distances between original and extracted data of ",
                "[",
                "3.81",
                "​",
                "e",
                "−",
                "5",
                ",",
                "5.06",
                "​",
                "e",
                "−",
                "5",
                ",",
                "0.07",
                ",",
                "0.27",
                ",",
                "0.92",
                "]",
                "3.81",
                "e",
                "5",
                "5.06",
                "e",
                "5",
                "0.07",
                "0.27",
                "0.92",
                "[3.81\\mathrm{e}{-5},5.06\\mathrm{e}{-5},0.07,0.27,0.92]",
                " and ",
                "[",
                "1.4",
                "​",
                "e",
                "−",
                "3",
                ",",
                "138.94",
                ",",
                "199.69",
                ",",
                "264.45",
                ",",
                "269.32",
                "]",
                "1.4",
                "e",
                "3",
                "138.94",
                "199.69",
                "264.45",
                "269.32",
                "[1.4\\mathrm{e}{-3},138.94,199.69,264.45,269.32]",
                " for CIFAR10 and ImageNet after 1,2,3,4, and 5 epochs, respectively.",
                "TensorFlow Federated.",
                "\n\nWe experimented with TensorFlow Federated ",
                "[",
                "2",
                "]",
                "—a standard open source library for FL deployments.\nIn Appendix ",
                "D.4",
                ", we show that a dishonest central party only requires minimal code changes to implement our trap weights."
            ]
        ]
    },
    "A1.T8": {
        "caption": "TABLE VIII: Comparison of data reconstruction attacks. KEY– U: User, S: Server, C: Class, ID: Individual Data Points, B: Mini-batch size, Opt.: Optimization, Train.: Training, Rep.: Representative.",
        "table": "<table id=\"A1.T8.3\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"A1.T8.3.4\" class=\"ltx_tr\">\n<td id=\"A1.T8.3.4.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\" rowspan=\"2\"><span id=\"A1.T8.3.4.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Method</span></td>\n<td id=\"A1.T8.3.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\" colspan=\"2\"><span id=\"A1.T8.3.4.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Attacker</span></td>\n<td id=\"A1.T8.3.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\" colspan=\"3\"><span id=\"A1.T8.3.4.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Rep.</span></td>\n<td id=\"A1.T8.3.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.3.4.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Label-</span></td>\n<td id=\"A1.T8.3.4.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\" colspan=\"2\"><span id=\"A1.T8.3.4.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">B</span></td>\n<td id=\"A1.T8.3.4.6\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.3.4.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Opt.-/</span></td>\n</tr>\n<tr id=\"A1.T8.1.1\" class=\"ltx_tr\">\n<td id=\"A1.T8.1.1.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.1.1.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">U</span></td>\n<td id=\"A1.T8.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.1.1.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">S</span></td>\n<td id=\"A1.T8.1.1.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.1.1.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">C</span></td>\n<td id=\"A1.T8.1.1.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.1.1.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">U</span></td>\n<td id=\"A1.T8.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.1.1.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">ID</span></td>\n<td id=\"A1.T8.1.1.7\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.1.1.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Free</span></td>\n<td id=\"A1.T8.1.1.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.1.1.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">1</span></td>\n<td id=\"A1.T8.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"A1.T8.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\geq 1\" display=\"inline\"><semantics id=\"A1.T8.1.1.1.m1.1a\"><mrow id=\"A1.T8.1.1.1.m1.1.1\" xref=\"A1.T8.1.1.1.m1.1.1.cmml\"><mi id=\"A1.T8.1.1.1.m1.1.1.2\" xref=\"A1.T8.1.1.1.m1.1.1.2.cmml\"></mi><mo mathsize=\"70%\" id=\"A1.T8.1.1.1.m1.1.1.1\" xref=\"A1.T8.1.1.1.m1.1.1.1.cmml\">≥</mo><mn mathsize=\"70%\" id=\"A1.T8.1.1.1.m1.1.1.3\" xref=\"A1.T8.1.1.1.m1.1.1.3.cmml\">1</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A1.T8.1.1.1.m1.1b\"><apply id=\"A1.T8.1.1.1.m1.1.1.cmml\" xref=\"A1.T8.1.1.1.m1.1.1\"><geq id=\"A1.T8.1.1.1.m1.1.1.1.cmml\" xref=\"A1.T8.1.1.1.m1.1.1.1\"></geq><csymbol cd=\"latexml\" id=\"A1.T8.1.1.1.m1.1.1.2.cmml\" xref=\"A1.T8.1.1.1.m1.1.1.2\">absent</csymbol><cn type=\"integer\" id=\"A1.T8.1.1.1.m1.1.1.3.cmml\" xref=\"A1.T8.1.1.1.m1.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T8.1.1.1.m1.1c\">\\geq 1</annotation></semantics></math></td>\n<td id=\"A1.T8.1.1.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.1.1.9.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Train.-Free</span></td>\n</tr>\n<tr id=\"A1.T8.3.3\" class=\"ltx_tr\">\n<td id=\"A1.T8.3.3.3\" class=\"ltx_td ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.3.3.4\" class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.3.3.5\" class=\"ltx_td ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\" colspan=\"3\"><math id=\"A1.T8.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\xrightarrow{stronger}\" display=\"inline\"><semantics id=\"A1.T8.2.2.1.m1.1a\"><mover accent=\"true\" id=\"A1.T8.2.2.1.m1.1.1\" xref=\"A1.T8.2.2.1.m1.1.1.cmml\"><mo mathsize=\"50%\" stretchy=\"false\" id=\"A1.T8.2.2.1.m1.1.1.2\" xref=\"A1.T8.2.2.1.m1.1.1.2.cmml\">→</mo><mrow id=\"A1.T8.2.2.1.m1.1.1.1\" xref=\"A1.T8.2.2.1.m1.1.1.1.cmml\"><mi mathsize=\"50%\" id=\"A1.T8.2.2.1.m1.1.1.1.2\" xref=\"A1.T8.2.2.1.m1.1.1.1.2.cmml\">s</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A1.T8.2.2.1.m1.1.1.1.1\" xref=\"A1.T8.2.2.1.m1.1.1.1.1.cmml\">​</mo><mi mathsize=\"50%\" id=\"A1.T8.2.2.1.m1.1.1.1.3\" xref=\"A1.T8.2.2.1.m1.1.1.1.3.cmml\">t</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A1.T8.2.2.1.m1.1.1.1.1a\" xref=\"A1.T8.2.2.1.m1.1.1.1.1.cmml\">​</mo><mi mathsize=\"50%\" id=\"A1.T8.2.2.1.m1.1.1.1.4\" xref=\"A1.T8.2.2.1.m1.1.1.1.4.cmml\">r</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A1.T8.2.2.1.m1.1.1.1.1b\" xref=\"A1.T8.2.2.1.m1.1.1.1.1.cmml\">​</mo><mi mathsize=\"50%\" id=\"A1.T8.2.2.1.m1.1.1.1.5\" xref=\"A1.T8.2.2.1.m1.1.1.1.5.cmml\">o</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A1.T8.2.2.1.m1.1.1.1.1c\" xref=\"A1.T8.2.2.1.m1.1.1.1.1.cmml\">​</mo><mi mathsize=\"50%\" id=\"A1.T8.2.2.1.m1.1.1.1.6\" xref=\"A1.T8.2.2.1.m1.1.1.1.6.cmml\">n</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A1.T8.2.2.1.m1.1.1.1.1d\" xref=\"A1.T8.2.2.1.m1.1.1.1.1.cmml\">​</mo><mi mathsize=\"50%\" id=\"A1.T8.2.2.1.m1.1.1.1.7\" xref=\"A1.T8.2.2.1.m1.1.1.1.7.cmml\">g</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A1.T8.2.2.1.m1.1.1.1.1e\" xref=\"A1.T8.2.2.1.m1.1.1.1.1.cmml\">​</mo><mi mathsize=\"50%\" id=\"A1.T8.2.2.1.m1.1.1.1.8\" xref=\"A1.T8.2.2.1.m1.1.1.1.8.cmml\">e</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A1.T8.2.2.1.m1.1.1.1.1f\" xref=\"A1.T8.2.2.1.m1.1.1.1.1.cmml\">​</mo><mi mathsize=\"50%\" id=\"A1.T8.2.2.1.m1.1.1.1.9\" xref=\"A1.T8.2.2.1.m1.1.1.1.9.cmml\">r</mi></mrow></mover><annotation-xml encoding=\"MathML-Content\" id=\"A1.T8.2.2.1.m1.1b\"><apply id=\"A1.T8.2.2.1.m1.1.1.cmml\" xref=\"A1.T8.2.2.1.m1.1.1\"><apply id=\"A1.T8.2.2.1.m1.1.1.1.cmml\" xref=\"A1.T8.2.2.1.m1.1.1.1\"><times id=\"A1.T8.2.2.1.m1.1.1.1.1.cmml\" xref=\"A1.T8.2.2.1.m1.1.1.1.1\"></times><ci id=\"A1.T8.2.2.1.m1.1.1.1.2.cmml\" xref=\"A1.T8.2.2.1.m1.1.1.1.2\">𝑠</ci><ci id=\"A1.T8.2.2.1.m1.1.1.1.3.cmml\" xref=\"A1.T8.2.2.1.m1.1.1.1.3\">𝑡</ci><ci id=\"A1.T8.2.2.1.m1.1.1.1.4.cmml\" xref=\"A1.T8.2.2.1.m1.1.1.1.4\">𝑟</ci><ci id=\"A1.T8.2.2.1.m1.1.1.1.5.cmml\" xref=\"A1.T8.2.2.1.m1.1.1.1.5\">𝑜</ci><ci id=\"A1.T8.2.2.1.m1.1.1.1.6.cmml\" xref=\"A1.T8.2.2.1.m1.1.1.1.6\">𝑛</ci><ci id=\"A1.T8.2.2.1.m1.1.1.1.7.cmml\" xref=\"A1.T8.2.2.1.m1.1.1.1.7\">𝑔</ci><ci id=\"A1.T8.2.2.1.m1.1.1.1.8.cmml\" xref=\"A1.T8.2.2.1.m1.1.1.1.8\">𝑒</ci><ci id=\"A1.T8.2.2.1.m1.1.1.1.9.cmml\" xref=\"A1.T8.2.2.1.m1.1.1.1.9\">𝑟</ci></apply><ci id=\"A1.T8.2.2.1.m1.1.1.2.cmml\" xref=\"A1.T8.2.2.1.m1.1.1.2\">→</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T8.2.2.1.m1.1c\">\\xrightarrow{stronger}</annotation></semantics></math></td>\n<td id=\"A1.T8.3.3.6\" class=\"ltx_td ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\" colspan=\"2\"><math id=\"A1.T8.3.3.2.m1.1\" class=\"ltx_Math\" alttext=\"\\xrightarrow{stronger}\" display=\"inline\"><semantics id=\"A1.T8.3.3.2.m1.1a\"><mover accent=\"true\" id=\"A1.T8.3.3.2.m1.1.1\" xref=\"A1.T8.3.3.2.m1.1.1.cmml\"><mo mathsize=\"50%\" stretchy=\"false\" id=\"A1.T8.3.3.2.m1.1.1.2\" xref=\"A1.T8.3.3.2.m1.1.1.2.cmml\">→</mo><mrow id=\"A1.T8.3.3.2.m1.1.1.1\" xref=\"A1.T8.3.3.2.m1.1.1.1.cmml\"><mi mathsize=\"50%\" id=\"A1.T8.3.3.2.m1.1.1.1.2\" xref=\"A1.T8.3.3.2.m1.1.1.1.2.cmml\">s</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A1.T8.3.3.2.m1.1.1.1.1\" xref=\"A1.T8.3.3.2.m1.1.1.1.1.cmml\">​</mo><mi mathsize=\"50%\" id=\"A1.T8.3.3.2.m1.1.1.1.3\" xref=\"A1.T8.3.3.2.m1.1.1.1.3.cmml\">t</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A1.T8.3.3.2.m1.1.1.1.1a\" xref=\"A1.T8.3.3.2.m1.1.1.1.1.cmml\">​</mo><mi mathsize=\"50%\" id=\"A1.T8.3.3.2.m1.1.1.1.4\" xref=\"A1.T8.3.3.2.m1.1.1.1.4.cmml\">r</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A1.T8.3.3.2.m1.1.1.1.1b\" xref=\"A1.T8.3.3.2.m1.1.1.1.1.cmml\">​</mo><mi mathsize=\"50%\" id=\"A1.T8.3.3.2.m1.1.1.1.5\" xref=\"A1.T8.3.3.2.m1.1.1.1.5.cmml\">o</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A1.T8.3.3.2.m1.1.1.1.1c\" xref=\"A1.T8.3.3.2.m1.1.1.1.1.cmml\">​</mo><mi mathsize=\"50%\" id=\"A1.T8.3.3.2.m1.1.1.1.6\" xref=\"A1.T8.3.3.2.m1.1.1.1.6.cmml\">n</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A1.T8.3.3.2.m1.1.1.1.1d\" xref=\"A1.T8.3.3.2.m1.1.1.1.1.cmml\">​</mo><mi mathsize=\"50%\" id=\"A1.T8.3.3.2.m1.1.1.1.7\" xref=\"A1.T8.3.3.2.m1.1.1.1.7.cmml\">g</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A1.T8.3.3.2.m1.1.1.1.1e\" xref=\"A1.T8.3.3.2.m1.1.1.1.1.cmml\">​</mo><mi mathsize=\"50%\" id=\"A1.T8.3.3.2.m1.1.1.1.8\" xref=\"A1.T8.3.3.2.m1.1.1.1.8.cmml\">e</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"A1.T8.3.3.2.m1.1.1.1.1f\" xref=\"A1.T8.3.3.2.m1.1.1.1.1.cmml\">​</mo><mi mathsize=\"50%\" id=\"A1.T8.3.3.2.m1.1.1.1.9\" xref=\"A1.T8.3.3.2.m1.1.1.1.9.cmml\">r</mi></mrow></mover><annotation-xml encoding=\"MathML-Content\" id=\"A1.T8.3.3.2.m1.1b\"><apply id=\"A1.T8.3.3.2.m1.1.1.cmml\" xref=\"A1.T8.3.3.2.m1.1.1\"><apply id=\"A1.T8.3.3.2.m1.1.1.1.cmml\" xref=\"A1.T8.3.3.2.m1.1.1.1\"><times id=\"A1.T8.3.3.2.m1.1.1.1.1.cmml\" xref=\"A1.T8.3.3.2.m1.1.1.1.1\"></times><ci id=\"A1.T8.3.3.2.m1.1.1.1.2.cmml\" xref=\"A1.T8.3.3.2.m1.1.1.1.2\">𝑠</ci><ci id=\"A1.T8.3.3.2.m1.1.1.1.3.cmml\" xref=\"A1.T8.3.3.2.m1.1.1.1.3\">𝑡</ci><ci id=\"A1.T8.3.3.2.m1.1.1.1.4.cmml\" xref=\"A1.T8.3.3.2.m1.1.1.1.4\">𝑟</ci><ci id=\"A1.T8.3.3.2.m1.1.1.1.5.cmml\" xref=\"A1.T8.3.3.2.m1.1.1.1.5\">𝑜</ci><ci id=\"A1.T8.3.3.2.m1.1.1.1.6.cmml\" xref=\"A1.T8.3.3.2.m1.1.1.1.6\">𝑛</ci><ci id=\"A1.T8.3.3.2.m1.1.1.1.7.cmml\" xref=\"A1.T8.3.3.2.m1.1.1.1.7\">𝑔</ci><ci id=\"A1.T8.3.3.2.m1.1.1.1.8.cmml\" xref=\"A1.T8.3.3.2.m1.1.1.1.8\">𝑒</ci><ci id=\"A1.T8.3.3.2.m1.1.1.1.9.cmml\" xref=\"A1.T8.3.3.2.m1.1.1.1.9\">𝑟</ci></apply><ci id=\"A1.T8.3.3.2.m1.1.1.2.cmml\" xref=\"A1.T8.3.3.2.m1.1.1.2\">→</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T8.3.3.2.m1.1c\">\\xrightarrow{stronger}</annotation></semantics></math></td>\n<td id=\"A1.T8.3.3.7\" class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n</tr>\n<tr id=\"A1.T8.3.5\" class=\"ltx_tr\">\n<td id=\"A1.T8.3.5.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span id=\"A1.T8.3.5.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">DMU-GAN </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A1.T8.3.5.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">23</a><span id=\"A1.T8.3.5.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"A1.T8.3.5.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.3.5.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">✓</span></td>\n<td id=\"A1.T8.3.5.3\" class=\"ltx_td ltx_border_r ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.3.5.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.3.5.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">✓</span></td>\n<td id=\"A1.T8.3.5.5\" class=\"ltx_td ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.3.5.6\" class=\"ltx_td ltx_border_r ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.3.5.7\" class=\"ltx_td ltx_border_r ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.3.5.8\" class=\"ltx_td ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.3.5.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.3.5.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">✓</span></td>\n<td id=\"A1.T8.3.5.10\" class=\"ltx_td ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n</tr>\n<tr id=\"A1.T8.3.6\" class=\"ltx_tr\">\n<td id=\"A1.T8.3.6.1\" class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span id=\"A1.T8.3.6.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">mGAN-AI </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A1.T8.3.6.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib50\" title=\"\" class=\"ltx_ref\">50</a><span id=\"A1.T8.3.6.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"A1.T8.3.6.2\" class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.3.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.3.6.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">✓</span></td>\n<td id=\"A1.T8.3.6.4\" class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.3.6.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.3.6.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">✓</span></td>\n<td id=\"A1.T8.3.6.6\" class=\"ltx_td ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.3.6.7\" class=\"ltx_td ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.3.6.8\" class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.3.6.9\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.3.6.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">✓</span></td>\n<td id=\"A1.T8.3.6.10\" class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n</tr>\n<tr id=\"A1.T8.3.7\" class=\"ltx_tr\">\n<td id=\"A1.T8.3.7.1\" class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span id=\"A1.T8.3.7.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">DLG </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A1.T8.3.7.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib56\" title=\"\" class=\"ltx_ref\">56</a><span id=\"A1.T8.3.7.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"A1.T8.3.7.2\" class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.3.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.3.7.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">✓</span></td>\n<td id=\"A1.T8.3.7.4\" class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.3.7.5\" class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.3.7.6\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.3.7.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">✓</span></td>\n<td id=\"A1.T8.3.7.7\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.3.7.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">✓</span></td>\n<td id=\"A1.T8.3.7.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.3.7.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">✓</span></td>\n<td id=\"A1.T8.3.7.9\" class=\"ltx_td ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.3.7.10\" class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n</tr>\n<tr id=\"A1.T8.3.8\" class=\"ltx_tr\">\n<td id=\"A1.T8.3.8.1\" class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span id=\"A1.T8.3.8.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">iDLG </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A1.T8.3.8.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib55\" title=\"\" class=\"ltx_ref\">55</a><span id=\"A1.T8.3.8.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"A1.T8.3.8.2\" class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.3.8.3\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.3.8.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">✓</span></td>\n<td id=\"A1.T8.3.8.4\" class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.3.8.5\" class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.3.8.6\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.3.8.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">✓</span></td>\n<td id=\"A1.T8.3.8.7\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.3.8.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">✓</span></td>\n<td id=\"A1.T8.3.8.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.3.8.8.1\" class=\"ltx_text\" style=\"font-size:70%;\">✓</span></td>\n<td id=\"A1.T8.3.8.9\" class=\"ltx_td ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.3.8.10\" class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n</tr>\n<tr id=\"A1.T8.3.9\" class=\"ltx_tr\">\n<td id=\"A1.T8.3.9.1\" class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span id=\"A1.T8.3.9.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">GradInv </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A1.T8.3.9.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib53\" title=\"\" class=\"ltx_ref\">53</a><span id=\"A1.T8.3.9.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n<td id=\"A1.T8.3.9.2\" class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.3.9.3\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.3.9.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">✓</span></td>\n<td id=\"A1.T8.3.9.4\" class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.3.9.5\" class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.3.9.6\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.3.9.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">✓</span></td>\n<td id=\"A1.T8.3.9.7\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.3.9.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">✓</span></td>\n<td id=\"A1.T8.3.9.8\" class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.3.9.9\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.3.9.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">✓</span></td>\n<td id=\"A1.T8.3.9.10\" class=\"ltx_td\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n</tr>\n<tr id=\"A1.T8.3.10\" class=\"ltx_tr\">\n<td id=\"A1.T8.3.10.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span id=\"A1.T8.3.10.1.1\" class=\"ltx_text ltx_font_sansserif\" style=\"font-size:70%;\">trap weights</span><span id=\"A1.T8.3.10.1.2\" class=\"ltx_text\" style=\"font-size:70%;\"> [Ours]</span>\n</td>\n<td id=\"A1.T8.3.10.2\" class=\"ltx_td ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.3.10.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.3.10.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">✓</span></td>\n<td id=\"A1.T8.3.10.4\" class=\"ltx_td ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.3.10.5\" class=\"ltx_td ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.3.10.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.3.10.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">✓</span></td>\n<td id=\"A1.T8.3.10.7\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.3.10.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">✓</span></td>\n<td id=\"A1.T8.3.10.8\" class=\"ltx_td ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"></td>\n<td id=\"A1.T8.3.10.9\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.3.10.9.1\" class=\"ltx_text\" style=\"font-size:70%;\">✓</span></td>\n<td id=\"A1.T8.3.10.10\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T8.3.10.10.1\" class=\"ltx_text\" style=\"font-size:70%;\">✓</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table VIII summarizes these data reconstruction attacks (described below) and compares them with our attack."
        ]
    },
    "A3.T9": {
        "caption": "TABLE IX: Architectures of models used in the experiments on image data. f: number of filters, k: kernel size, s: stride, p: padding act: activation function, n: number of neurons.",
        "table": "<table id=\"A3.T9.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"A3.T9.2.1\" class=\"ltx_tr\">\n<td id=\"A3.T9.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A3.T9.2.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">FC-NN Architecture</span></td>\n<td id=\"A3.T9.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A3.T9.2.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">VGG-inspired CNN Architecture</span></td>\n</tr>\n<tr id=\"A3.T9.2.2\" class=\"ltx_tr\">\n<td id=\"A3.T9.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A3.T9.2.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Dense(n=1000, act=relu)</span></td>\n<td id=\"A3.T9.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A3.T9.2.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">Conv(f=128, k=(3,3), s=1, p=same, act=relu)</span></td>\n</tr>\n<tr id=\"A3.T9.2.3\" class=\"ltx_tr\">\n<td id=\"A3.T9.2.3.1\" class=\"ltx_td ltx_align_center\"><span id=\"A3.T9.2.3.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Dense(n=3000, act=relu)</span></td>\n<td id=\"A3.T9.2.3.2\" class=\"ltx_td ltx_align_center\"><span id=\"A3.T9.2.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">Conv(f=256, k=(3,3), s=1, p=same, act=relu)</span></td>\n</tr>\n<tr id=\"A3.T9.2.4\" class=\"ltx_tr\">\n<td id=\"A3.T9.2.4.1\" class=\"ltx_td ltx_align_center\"><span id=\"A3.T9.2.4.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Dense(n=3000, act=relu)</span></td>\n<td id=\"A3.T9.2.4.2\" class=\"ltx_td ltx_align_center\"><span id=\"A3.T9.2.4.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">Conv(f=512, k=(3,3), s=1, p=same, act=relu)</span></td>\n</tr>\n<tr id=\"A3.T9.2.5\" class=\"ltx_tr\">\n<td id=\"A3.T9.2.5.1\" class=\"ltx_td ltx_align_center\"><span id=\"A3.T9.2.5.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Dense(n=2000, act=relu)</span></td>\n<td id=\"A3.T9.2.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"A3.T9.2.5.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">Flatten</span></td>\n</tr>\n<tr id=\"A3.T9.2.6\" class=\"ltx_tr\">\n<td id=\"A3.T9.2.6.1\" class=\"ltx_td ltx_align_center\"><span id=\"A3.T9.2.6.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Dense(n=1000, act=relu)</span></td>\n<td id=\"A3.T9.2.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"A3.T9.2.6.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">Dense(n=1000, act=relu)</span></td>\n</tr>\n<tr id=\"A3.T9.2.7\" class=\"ltx_tr\">\n<td id=\"A3.T9.2.7.1\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A3.T9.2.7.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Dense(n=#classes, act=None)</span></td>\n<td id=\"A3.T9.2.7.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A3.T9.2.7.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">Dense(n=#classes, act=None)</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Comparison to [37].\nNote that [37]’s main goal is not to extract large amounts individual user data points but user updates, by circumventing the secure aggregation [6] used to protect the FL protocol. The updates (gradients) that [37] recover do usually not correspond to full and perfectly individual data points.\nInstead, for FC-NNs, their extracted gradients will resemble our passive extraction results from Figure 8 where most of the gradients are a blurry overlay of all underlying data points.\nFor CNNs, their results will not be able to extract any individual data point since non-maliciously initialized convolution filters overlay input features.\nThereby, their attack mainly violates confidentiality of the users’ model updates in a setup where users believe to obtain protection though an aggregate with other users.\nStill, the resulting gradients can then be used as a departure point for additional privacy-attacks, such as reconstruction.\nIn contrast, our work directly violates the users’ privacy by manipulating the shared model weights to make individual data points directly extractable from the model updates sent from users to the central party.\nTo assess individual extractability in their setup, we use their gradient suppression and model inconsistency attack to make all but one user in a round of the FL protocol return zero gradients.\nThe one target-user receives a randomly initialized FC-NN (Gaussian with σ=0.5𝜎0.5\\sigma=0.5) with architecture from Table IX.\nNote that the data extraction from gradients in this setup corresponds to our passive extraction.\nFor MNIST, with B=100𝐵100B=100, extraction in their setup yields 55~{}5% of perfectly extractable data points, while our trap weights yield 545454%.\nIn the same setup for CIFAR10, their method yields 262626%, in contrast to our trap weights which yield again 545454%."
        ]
    },
    "A3.T10": {
        "caption": "TABLE X: Architecture of models used in the experiments on the IMDB dataset. feat: vocabulary size, dim: embedding size, act: activation function, n: number of neurons.",
        "table": "<table id=\"A3.T10.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"A3.T10.2.1\" class=\"ltx_tr\">\n<td id=\"A3.T10.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\">IMDB-Model Architecture</td>\n</tr>\n<tr id=\"A3.T10.2.2\" class=\"ltx_tr\">\n<td id=\"A3.T10.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\">Embedding(feat=10000, dim=250)</td>\n</tr>\n<tr id=\"A3.T10.2.3\" class=\"ltx_tr\">\n<td id=\"A3.T10.2.3.1\" class=\"ltx_td ltx_align_center\">Dense(n=1000, act=relu) )</td>\n</tr>\n<tr id=\"A3.T10.2.4\" class=\"ltx_tr\">\n<td id=\"A3.T10.2.4.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">Dense(n=1, act=None)</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "The following table describes the model architectures both for the FC-NNs and CNNs use throughout the paper. Note that the our method could also be applied to much larger CNNs with more layers: in fact, as long as each layer contains as many parameters as the data holds input features, our approach is applicable."
            ]
        ]
    },
    "A4.T11": {
        "caption": "TABLE XI: Effect of Mini-Batch Averaging. Success of our adversarial weight initialization on MNIST under averaging over multiple mini-batches on the same model parameters. The number of mini-batches is denoted by Num and their respective size by B. The results depict the percentage of active neurons (A), extraction-precision (P), and extraction-recall (R) for extracting from 1000 neurons at the first layer of the FC-NN depicted in Table IX. All numbers are averaged over 10 runs with different adversarial initializations.",
        "table": "<table id=\"A4.T11.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"A4.T11.2.1\" class=\"ltx_tr\">\n<td id=\"A4.T11.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A4.T11.2.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">B, Num</span></td>\n<td id=\"A4.T11.2.1.2\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"A4.T11.2.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">A</span></td>\n<td id=\"A4.T11.2.1.3\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"A4.T11.2.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">P</span></td>\n<td id=\"A4.T11.2.1.4\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"A4.T11.2.1.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">R</span></td>\n</tr>\n<tr id=\"A4.T11.2.2\" class=\"ltx_tr\">\n<td id=\"A4.T11.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A4.T11.2.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">(20,1)</span></td>\n<td id=\"A4.T11.2.2.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"A4.T11.2.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.496</span></td>\n<td id=\"A4.T11.2.2.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"A4.T11.2.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">.486</span></td>\n<td id=\"A4.T11.2.2.4\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"A4.T11.2.2.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.950</span></td>\n</tr>\n<tr id=\"A4.T11.2.3\" class=\"ltx_tr\">\n<td id=\"A4.T11.2.3.1\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T11.2.3.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">(20,5)</span></td>\n<td id=\"A4.T11.2.3.2\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.787</span></td>\n<td id=\"A4.T11.2.3.3\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">.213</span></td>\n<td id=\"A4.T11.2.3.4\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.3.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.572</span></td>\n</tr>\n<tr id=\"A4.T11.2.4\" class=\"ltx_tr\">\n<td id=\"A4.T11.2.4.1\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T11.2.4.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">(20,10)</span></td>\n<td id=\"A4.T11.2.4.2\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.4.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.851</span></td>\n<td id=\"A4.T11.2.4.3\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.4.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">.157</span></td>\n<td id=\"A4.T11.2.4.4\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.4.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.412</span></td>\n</tr>\n<tr id=\"A4.T11.2.5\" class=\"ltx_tr\">\n<td id=\"A4.T11.2.5.1\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T11.2.5.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">(20,20)</span></td>\n<td id=\"A4.T11.2.5.2\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.5.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.898</span></td>\n<td id=\"A4.T11.2.5.3\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.5.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">.116</span></td>\n<td id=\"A4.T11.2.5.4\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.5.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.251</span></td>\n</tr>\n<tr id=\"A4.T11.2.6\" class=\"ltx_tr\">\n<td id=\"A4.T11.2.6.1\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T11.2.6.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">(50,1)</span></td>\n<td id=\"A4.T11.2.6.2\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.6.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.687</span></td>\n<td id=\"A4.T11.2.6.3\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.6.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">.307</span></td>\n<td id=\"A4.T11.2.6.4\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.6.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.790</span></td>\n</tr>\n<tr id=\"A4.T11.2.7\" class=\"ltx_tr\">\n<td id=\"A4.T11.2.7.1\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T11.2.7.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">(50,5)</span></td>\n<td id=\"A4.T11.2.7.2\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.7.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.901</span></td>\n<td id=\"A4.T11.2.7.3\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.7.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">.107</span></td>\n<td id=\"A4.T11.2.7.4\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.7.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.230</span></td>\n</tr>\n<tr id=\"A4.T11.2.8\" class=\"ltx_tr\">\n<td id=\"A4.T11.2.8.1\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T11.2.8.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">(50,10)</span></td>\n<td id=\"A4.T11.2.8.2\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.8.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.928</span></td>\n<td id=\"A4.T11.2.8.3\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.8.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">.080</span></td>\n<td id=\"A4.T11.2.8.4\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.8.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.138</span></td>\n</tr>\n<tr id=\"A4.T11.2.9\" class=\"ltx_tr\">\n<td id=\"A4.T11.2.9.1\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T11.2.9.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">(50,20)</span></td>\n<td id=\"A4.T11.2.9.2\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.9.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.953</span></td>\n<td id=\"A4.T11.2.9.3\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.9.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">.053</span></td>\n<td id=\"A4.T11.2.9.4\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.9.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.067</span></td>\n</tr>\n<tr id=\"A4.T11.2.10\" class=\"ltx_tr\">\n<td id=\"A4.T11.2.10.1\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T11.2.10.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">(100,1)</span></td>\n<td id=\"A4.T11.2.10.2\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.10.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.800</span></td>\n<td id=\"A4.T11.2.10.3\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.10.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">.200</span></td>\n<td id=\"A4.T11.2.10.4\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.10.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.562</span></td>\n</tr>\n<tr id=\"A4.T11.2.11\" class=\"ltx_tr\">\n<td id=\"A4.T11.2.11.1\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T11.2.11.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">(100,5)</span></td>\n<td id=\"A4.T11.2.11.2\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.11.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.936</span></td>\n<td id=\"A4.T11.2.11.3\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.11.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">.066</span></td>\n<td id=\"A4.T11.2.11.4\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.11.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.116</span></td>\n</tr>\n<tr id=\"A4.T11.2.12\" class=\"ltx_tr\">\n<td id=\"A4.T11.2.12.1\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T11.2.12.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">(100,10)</span></td>\n<td id=\"A4.T11.2.12.2\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.12.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.966</span></td>\n<td id=\"A4.T11.2.12.3\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.12.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">.046</span></td>\n<td id=\"A4.T11.2.12.4\" class=\"ltx_td ltx_align_right\"><span id=\"A4.T11.2.12.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.054</span></td>\n</tr>\n<tr id=\"A4.T11.2.13\" class=\"ltx_tr\">\n<td id=\"A4.T11.2.13.1\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A4.T11.2.13.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">(100,20)</span></td>\n<td id=\"A4.T11.2.13.2\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"A4.T11.2.13.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.982</span></td>\n<td id=\"A4.T11.2.13.3\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"A4.T11.2.13.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">.028</span></td>\n<td id=\"A4.T11.2.13.4\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"A4.T11.2.13.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.020</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Figures ",
                "11",
                " and ",
                "13",
                " depict the extracted data points for MNIST and ImageNet, respectively.",
                "We, furthermore, study partial extractablity, ",
                "i.e",
                ".",
                ", the case when a data point is not individually extractable, but still leaks meaningful private information about a training data point.\nPartial leakage occurs when an extracted gradient represents the overlay of only a few data points.\nIn this case, the individual signal of each data point is still distinguishable, see for example the first data point in the third row of ",
                "Figure",
                " ",
                "8",
                ".\nWe plot in ",
                "Figure",
                " ",
                "10",
                " by how many data point each of the neurons with our trap weight initialization gets activated.\nThis corresponds to the number of data points that will be present in the overlay of the respective gradients.\nWe can see that nearly as many neurons get activated by two data points as by one data point (",
                "i.e",
                ".",
                " perfect extractability).\nIn general, with our trap weights, neurons get activated by small numbers of data points.\nThis indicates that the central party can still extract meaningful partial information on many data points, also if these are not perfectly extractable.\nResults an average over five runs with different trap weight initializations for a mini-batch of 100 data points from the ImageNet dataset.",
                "To provide additional insights on data points that can and cannot be individually extracted, in ",
                "Figure",
                " ",
                "14",
                " which data points can and cannot be extracted.\nFor the individually extractable data points, we, furthermore, depict how often each of them is individually extractable, ",
                "i.e",
                ".",
                " for how many neurons this data point is the only one activating it.\nWe see that our trap weights first amplify natural leakage ",
                "i.e",
                ".",
                ", data points that are extractable from random weights are usually also extractable with our trap weight and our trap weights make other data points extractable.\nSecond, our trap weights yield redundancy, ",
                "i.e",
                ".",
                " data points are extractable multiple times from different weight rows’ gradients.",
                "We depict extraction success for local averaging over multiple mini-batches in ",
                "Table",
                " ",
                "XI",
                ".",
                "We also study the non-IID setup where users hold data from a single class, different from other users in the protocol.\nWe present the ",
                "extraction-recall",
                " and ",
                "extraction-precision",
                " per class on the CIFAR10 dataset in ",
                "Table",
                " ",
                "XII",
                ".",
                "Finally, we study how an attacker without any prior knowledge can tune ",
                "s",
                "𝑠",
                "s",
                ".\nOne way to proceed is that the attacker does not adversarially initializes the model in the first FL iteration.\nIt then extracts data points from the gradients, which are not necessarily individually extracted data points.\nFrom these data points, the attacker keeps the one in a valid image input range with features in range [0, 1], and uses these data points for fine-tuning ",
                "s",
                "𝑠",
                "s",
                ".\nWe depict the resulting data points for MNIST, CIFAR10, and ImageNet in ",
                "Figure",
                " ",
                "9",
                " and show extraction success for different ",
                "s",
                "𝑠",
                "s",
                " on 100 such data point in ",
                "Table",
                " ",
                "XIII",
                "."
            ]
        ]
    },
    "A4.T12": {
        "caption": "TABLE XII: Non-IID Extraction on CIFAR10. Success of our adversarial weight initialization (active) versus non-manipulated model weights (passive) on CIFAR10 in a non-IID setup where each user only holds data from a single class, different from all other users. The results depict the extraction-precision (P) and extraction-recall (R) for extracting from 1000 neurons at the first layer of the FC-NN depicted in Table IX. All numbers are averaged over 10 runs with different (adversarial) initializations.\nWhile in both passive and active extraction, extraction success between the classes differs, our adversarial weight initialization significantly increases leakage over all classes.\nResults are averaged over 5 runs.\n",
        "table": "<table id=\"A4.T12.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"A4.T12.2.1\" class=\"ltx_tr\">\n<td id=\"A4.T12.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A4.T12.2.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Class</span></td>\n<td id=\"A4.T12.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A4.T12.2.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">P (Passive)</span></td>\n<td id=\"A4.T12.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A4.T12.2.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">P (Active)</span></td>\n<td id=\"A4.T12.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A4.T12.2.1.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">R (Passive)</span></td>\n<td id=\"A4.T12.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A4.T12.2.1.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">R (Active)</span></td>\n</tr>\n<tr id=\"A4.T12.2.2\" class=\"ltx_tr\">\n<td id=\"A4.T12.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A4.T12.2.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">0</span></td>\n<td id=\"A4.T12.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A4.T12.2.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.064</span></td>\n<td id=\"A4.T12.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A4.T12.2.2.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.570</span></td>\n<td id=\"A4.T12.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A4.T12.2.2.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.185</span></td>\n<td id=\"A4.T12.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A4.T12.2.2.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.352</span></td>\n</tr>\n<tr id=\"A4.T12.2.3\" class=\"ltx_tr\">\n<td id=\"A4.T12.2.3.1\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.3.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">1</span></td>\n<td id=\"A4.T12.2.3.2\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.3.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.041</span></td>\n<td id=\"A4.T12.2.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.3.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.276</span></td>\n<td id=\"A4.T12.2.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.3.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.208</span></td>\n<td id=\"A4.T12.2.3.5\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.3.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.560</span></td>\n</tr>\n<tr id=\"A4.T12.2.4\" class=\"ltx_tr\">\n<td id=\"A4.T12.2.4.1\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.4.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">2</span></td>\n<td id=\"A4.T12.2.4.2\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.4.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.056</span></td>\n<td id=\"A4.T12.2.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.4.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.480</span></td>\n<td id=\"A4.T12.2.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.4.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.195</span></td>\n<td id=\"A4.T12.2.4.5\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.4.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.384</span></td>\n</tr>\n<tr id=\"A4.T12.2.5\" class=\"ltx_tr\">\n<td id=\"A4.T12.2.5.1\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.5.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">3</span></td>\n<td id=\"A4.T12.2.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.5.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.044</span></td>\n<td id=\"A4.T12.2.5.3\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.5.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.318</span></td>\n<td id=\"A4.T12.2.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.5.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.208</span></td>\n<td id=\"A4.T12.2.5.5\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.5.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.489</span></td>\n</tr>\n<tr id=\"A4.T12.2.6\" class=\"ltx_tr\">\n<td id=\"A4.T12.2.6.1\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.6.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">4</span></td>\n<td id=\"A4.T12.2.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.6.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.056</span></td>\n<td id=\"A4.T12.2.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.6.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.516</span></td>\n<td id=\"A4.T12.2.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.6.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.225</span></td>\n<td id=\"A4.T12.2.6.5\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.6.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.426</span></td>\n</tr>\n<tr id=\"A4.T12.2.7\" class=\"ltx_tr\">\n<td id=\"A4.T12.2.7.1\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.7.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">5</span></td>\n<td id=\"A4.T12.2.7.2\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.7.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.045</span></td>\n<td id=\"A4.T12.2.7.3\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.7.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.356</span></td>\n<td id=\"A4.T12.2.7.4\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.7.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.238</span></td>\n<td id=\"A4.T12.2.7.5\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.7.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.534</span></td>\n</tr>\n<tr id=\"A4.T12.2.8\" class=\"ltx_tr\">\n<td id=\"A4.T12.2.8.1\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.8.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">6</span></td>\n<td id=\"A4.T12.2.8.2\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.8.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.049</span></td>\n<td id=\"A4.T12.2.8.3\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.8.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.358</span></td>\n<td id=\"A4.T12.2.8.4\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.8.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.209</span></td>\n<td id=\"A4.T12.2.8.5\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.8.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.442</span></td>\n</tr>\n<tr id=\"A4.T12.2.9\" class=\"ltx_tr\">\n<td id=\"A4.T12.2.9.1\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.9.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">7</span></td>\n<td id=\"A4.T12.2.9.2\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.9.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.051</span></td>\n<td id=\"A4.T12.2.9.3\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.9.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.367</span></td>\n<td id=\"A4.T12.2.9.4\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.9.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.205</span></td>\n<td id=\"A4.T12.2.9.5\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.9.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.515</span></td>\n</tr>\n<tr id=\"A4.T12.2.10\" class=\"ltx_tr\">\n<td id=\"A4.T12.2.10.1\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.10.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">8</span></td>\n<td id=\"A4.T12.2.10.2\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.10.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.055</span></td>\n<td id=\"A4.T12.2.10.3\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.10.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.536</span></td>\n<td id=\"A4.T12.2.10.4\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.10.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.209</span></td>\n<td id=\"A4.T12.2.10.5\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T12.2.10.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.386</span></td>\n</tr>\n<tr id=\"A4.T12.2.11\" class=\"ltx_tr\">\n<td id=\"A4.T12.2.11.1\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A4.T12.2.11.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">9</span></td>\n<td id=\"A4.T12.2.11.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A4.T12.2.11.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">.043</span></td>\n<td id=\"A4.T12.2.11.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A4.T12.2.11.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.395</span></td>\n<td id=\"A4.T12.2.11.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A4.T12.2.11.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">.240</span></td>\n<td id=\"A4.T12.2.11.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A4.T12.2.11.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">.559</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Figures ",
                "11",
                " and ",
                "13",
                " depict the extracted data points for MNIST and ImageNet, respectively.",
                "We, furthermore, study partial extractablity, ",
                "i.e",
                ".",
                ", the case when a data point is not individually extractable, but still leaks meaningful private information about a training data point.\nPartial leakage occurs when an extracted gradient represents the overlay of only a few data points.\nIn this case, the individual signal of each data point is still distinguishable, see for example the first data point in the third row of ",
                "Figure",
                " ",
                "8",
                ".\nWe plot in ",
                "Figure",
                " ",
                "10",
                " by how many data point each of the neurons with our trap weight initialization gets activated.\nThis corresponds to the number of data points that will be present in the overlay of the respective gradients.\nWe can see that nearly as many neurons get activated by two data points as by one data point (",
                "i.e",
                ".",
                " perfect extractability).\nIn general, with our trap weights, neurons get activated by small numbers of data points.\nThis indicates that the central party can still extract meaningful partial information on many data points, also if these are not perfectly extractable.\nResults an average over five runs with different trap weight initializations for a mini-batch of 100 data points from the ImageNet dataset.",
                "To provide additional insights on data points that can and cannot be individually extracted, in ",
                "Figure",
                " ",
                "14",
                " which data points can and cannot be extracted.\nFor the individually extractable data points, we, furthermore, depict how often each of them is individually extractable, ",
                "i.e",
                ".",
                " for how many neurons this data point is the only one activating it.\nWe see that our trap weights first amplify natural leakage ",
                "i.e",
                ".",
                ", data points that are extractable from random weights are usually also extractable with our trap weight and our trap weights make other data points extractable.\nSecond, our trap weights yield redundancy, ",
                "i.e",
                ".",
                " data points are extractable multiple times from different weight rows’ gradients.",
                "We depict extraction success for local averaging over multiple mini-batches in ",
                "Table",
                " ",
                "XI",
                ".",
                "We also study the non-IID setup where users hold data from a single class, different from other users in the protocol.\nWe present the ",
                "extraction-recall",
                " and ",
                "extraction-precision",
                " per class on the CIFAR10 dataset in ",
                "Table",
                " ",
                "XII",
                ".",
                "Finally, we study how an attacker without any prior knowledge can tune ",
                "s",
                "𝑠",
                "s",
                ".\nOne way to proceed is that the attacker does not adversarially initializes the model in the first FL iteration.\nIt then extracts data points from the gradients, which are not necessarily individually extracted data points.\nFrom these data points, the attacker keeps the one in a valid image input range with features in range [0, 1], and uses these data points for fine-tuning ",
                "s",
                "𝑠",
                "s",
                ".\nWe depict the resulting data points for MNIST, CIFAR10, and ImageNet in ",
                "Figure",
                " ",
                "9",
                " and show extraction success for different ",
                "s",
                "𝑠",
                "s",
                " on 100 such data point in ",
                "Table",
                " ",
                "XIII",
                "."
            ]
        ]
    },
    "A4.T13": {
        "caption": "TABLE XIII: Tuning Factor s𝑠s on Data Points from Passive Extraction. We model an attacker who does not hold auxiliary data to tune the scaling factor s𝑠s.\nSuch an attacker can, during the firs round, of the protocol extract the data points from the non-manipulated model weights’ gradients.\nThe points (we select those with features in range [0,1], see Figure 9), can be used to tune s𝑠s.\nThe identified optimal s𝑠s (bold) w.r.t. the extraction-recall are close (0.75 or 0.80, MNIST) or identical (0.95, CIFAR10 and 0.99, ImageNet) to the original datasets’ optimal s𝑠s, 0.7, 0.95, and 0.99 for MNIST, CIFAR10, ImageNet.",
        "table": "<table id=\"A4.T13.16\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"A4.T13.16.17\" class=\"ltx_tr\">\n<td id=\"A4.T13.16.17.1\" class=\"ltx_td ltx_align_left ltx_border_tt\"><span id=\"A4.T13.16.17.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">s</span></td>\n<td id=\"A4.T13.16.17.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A4.T13.16.17.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">MNIST R</span></td>\n<td id=\"A4.T13.16.17.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A4.T13.16.17.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">CIFAR10 R</span></td>\n<td id=\"A4.T13.16.17.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A4.T13.16.17.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">ImageNet R</span></td>\n</tr>\n<tr id=\"A4.T13.1.1\" class=\"ltx_tr\">\n<td id=\"A4.T13.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><math id=\"A4.T13.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\".650\" display=\"inline\"><semantics id=\"A4.T13.1.1.1.m1.1a\"><mn mathsize=\"70%\" id=\"A4.T13.1.1.1.m1.1.1\" xref=\"A4.T13.1.1.1.m1.1.1.cmml\">.650</mn><annotation-xml encoding=\"MathML-Content\" id=\"A4.T13.1.1.1.m1.1b\"><cn type=\"float\" id=\"A4.T13.1.1.1.m1.1.1.cmml\" xref=\"A4.T13.1.1.1.m1.1.1\">.650</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T13.1.1.1.m1.1c\">.650</annotation></semantics></math></td>\n<td id=\"A4.T13.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A4.T13.1.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.468</span></td>\n<td id=\"A4.T13.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A4.T13.1.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.0</span></td>\n<td id=\"A4.T13.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A4.T13.1.1.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.0</span></td>\n</tr>\n<tr id=\"A4.T13.2.2\" class=\"ltx_tr\">\n<td id=\"A4.T13.2.2.1\" class=\"ltx_td ltx_align_left\"><math id=\"A4.T13.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\".700\" display=\"inline\"><semantics id=\"A4.T13.2.2.1.m1.1a\"><mn mathsize=\"70%\" id=\"A4.T13.2.2.1.m1.1.1\" xref=\"A4.T13.2.2.1.m1.1.1.cmml\">.700</mn><annotation-xml encoding=\"MathML-Content\" id=\"A4.T13.2.2.1.m1.1b\"><cn type=\"float\" id=\"A4.T13.2.2.1.m1.1.1.cmml\" xref=\"A4.T13.2.2.1.m1.1.1\">.700</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T13.2.2.1.m1.1c\">.700</annotation></semantics></math></td>\n<td id=\"A4.T13.2.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.2.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.477</span></td>\n<td id=\"A4.T13.2.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.2.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.0</span></td>\n<td id=\"A4.T13.2.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.2.2.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.0</span></td>\n</tr>\n<tr id=\"A4.T13.3.3\" class=\"ltx_tr\">\n<td id=\"A4.T13.3.3.1\" class=\"ltx_td ltx_align_left\"><math id=\"A4.T13.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\".750\" display=\"inline\"><semantics id=\"A4.T13.3.3.1.m1.1a\"><mn mathsize=\"70%\" id=\"A4.T13.3.3.1.m1.1.1\" xref=\"A4.T13.3.3.1.m1.1.1.cmml\">.750</mn><annotation-xml encoding=\"MathML-Content\" id=\"A4.T13.3.3.1.m1.1b\"><cn type=\"float\" id=\"A4.T13.3.3.1.m1.1.1.cmml\" xref=\"A4.T13.3.3.1.m1.1.1\">.750</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T13.3.3.1.m1.1c\">.750</annotation></semantics></math></td>\n<td id=\"A4.T13.3.3.2\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.3.3.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.603</span></td>\n<td id=\"A4.T13.3.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.3.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.0</span></td>\n<td id=\"A4.T13.3.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.3.3.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.0</span></td>\n</tr>\n<tr id=\"A4.T13.4.4\" class=\"ltx_tr\">\n<td id=\"A4.T13.4.4.1\" class=\"ltx_td ltx_align_left\"><math id=\"A4.T13.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\".800\" display=\"inline\"><semantics id=\"A4.T13.4.4.1.m1.1a\"><mn mathsize=\"70%\" id=\"A4.T13.4.4.1.m1.1.1\" xref=\"A4.T13.4.4.1.m1.1.1.cmml\">.800</mn><annotation-xml encoding=\"MathML-Content\" id=\"A4.T13.4.4.1.m1.1b\"><cn type=\"float\" id=\"A4.T13.4.4.1.m1.1.1.cmml\" xref=\"A4.T13.4.4.1.m1.1.1\">.800</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T13.4.4.1.m1.1c\">.800</annotation></semantics></math></td>\n<td id=\"A4.T13.4.4.2\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.4.4.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.603</span></td>\n<td id=\"A4.T13.4.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.4.4.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.085</span></td>\n<td id=\"A4.T13.4.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.4.4.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.0</span></td>\n</tr>\n<tr id=\"A4.T13.5.5\" class=\"ltx_tr\">\n<td id=\"A4.T13.5.5.1\" class=\"ltx_td ltx_align_left\"><math id=\"A4.T13.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\".900\" display=\"inline\"><semantics id=\"A4.T13.5.5.1.m1.1a\"><mn mathsize=\"70%\" id=\"A4.T13.5.5.1.m1.1.1\" xref=\"A4.T13.5.5.1.m1.1.1.cmml\">.900</mn><annotation-xml encoding=\"MathML-Content\" id=\"A4.T13.5.5.1.m1.1b\"><cn type=\"float\" id=\"A4.T13.5.5.1.m1.1.1.cmml\" xref=\"A4.T13.5.5.1.m1.1.1\">.900</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T13.5.5.1.m1.1c\">.900</annotation></semantics></math></td>\n<td id=\"A4.T13.5.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.5.5.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.531</span></td>\n<td id=\"A4.T13.5.5.3\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.5.5.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.121</span></td>\n<td id=\"A4.T13.5.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.5.5.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.0</span></td>\n</tr>\n<tr id=\"A4.T13.6.6\" class=\"ltx_tr\">\n<td id=\"A4.T13.6.6.1\" class=\"ltx_td ltx_align_left\"><math id=\"A4.T13.6.6.1.m1.1\" class=\"ltx_Math\" alttext=\".910\" display=\"inline\"><semantics id=\"A4.T13.6.6.1.m1.1a\"><mn mathsize=\"70%\" id=\"A4.T13.6.6.1.m1.1.1\" xref=\"A4.T13.6.6.1.m1.1.1.cmml\">.910</mn><annotation-xml encoding=\"MathML-Content\" id=\"A4.T13.6.6.1.m1.1b\"><cn type=\"float\" id=\"A4.T13.6.6.1.m1.1.1.cmml\" xref=\"A4.T13.6.6.1.m1.1.1\">.910</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T13.6.6.1.m1.1c\">.910</annotation></semantics></math></td>\n<td id=\"A4.T13.6.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.6.6.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.504</span></td>\n<td id=\"A4.T13.6.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.6.6.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.147</span></td>\n<td id=\"A4.T13.6.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.6.6.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.0</span></td>\n</tr>\n<tr id=\"A4.T13.7.7\" class=\"ltx_tr\">\n<td id=\"A4.T13.7.7.1\" class=\"ltx_td ltx_align_left\"><math id=\"A4.T13.7.7.1.m1.1\" class=\"ltx_Math\" alttext=\".920\" display=\"inline\"><semantics id=\"A4.T13.7.7.1.m1.1a\"><mn mathsize=\"70%\" id=\"A4.T13.7.7.1.m1.1.1\" xref=\"A4.T13.7.7.1.m1.1.1.cmml\">.920</mn><annotation-xml encoding=\"MathML-Content\" id=\"A4.T13.7.7.1.m1.1b\"><cn type=\"float\" id=\"A4.T13.7.7.1.m1.1.1.cmml\" xref=\"A4.T13.7.7.1.m1.1.1\">.920</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T13.7.7.1.m1.1c\">.920</annotation></semantics></math></td>\n<td id=\"A4.T13.7.7.2\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.7.7.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.513</span></td>\n<td id=\"A4.T13.7.7.3\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.7.7.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.178</span></td>\n<td id=\"A4.T13.7.7.4\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.7.7.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.0</span></td>\n</tr>\n<tr id=\"A4.T13.8.8\" class=\"ltx_tr\">\n<td id=\"A4.T13.8.8.1\" class=\"ltx_td ltx_align_left\"><math id=\"A4.T13.8.8.1.m1.1\" class=\"ltx_Math\" alttext=\".930\" display=\"inline\"><semantics id=\"A4.T13.8.8.1.m1.1a\"><mn mathsize=\"70%\" id=\"A4.T13.8.8.1.m1.1.1\" xref=\"A4.T13.8.8.1.m1.1.1.cmml\">.930</mn><annotation-xml encoding=\"MathML-Content\" id=\"A4.T13.8.8.1.m1.1b\"><cn type=\"float\" id=\"A4.T13.8.8.1.m1.1.1.cmml\" xref=\"A4.T13.8.8.1.m1.1.1\">.930</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T13.8.8.1.m1.1c\">.930</annotation></semantics></math></td>\n<td id=\"A4.T13.8.8.2\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.8.8.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.459</span></td>\n<td id=\"A4.T13.8.8.3\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.8.8.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.210</span></td>\n<td id=\"A4.T13.8.8.4\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.8.8.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.0</span></td>\n</tr>\n<tr id=\"A4.T13.9.9\" class=\"ltx_tr\">\n<td id=\"A4.T13.9.9.1\" class=\"ltx_td ltx_align_left\"><math id=\"A4.T13.9.9.1.m1.1\" class=\"ltx_Math\" alttext=\".940\" display=\"inline\"><semantics id=\"A4.T13.9.9.1.m1.1a\"><mn mathsize=\"70%\" id=\"A4.T13.9.9.1.m1.1.1\" xref=\"A4.T13.9.9.1.m1.1.1.cmml\">.940</mn><annotation-xml encoding=\"MathML-Content\" id=\"A4.T13.9.9.1.m1.1b\"><cn type=\"float\" id=\"A4.T13.9.9.1.m1.1.1.cmml\" xref=\"A4.T13.9.9.1.m1.1.1\">.940</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T13.9.9.1.m1.1c\">.940</annotation></semantics></math></td>\n<td id=\"A4.T13.9.9.2\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.9.9.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.450</span></td>\n<td id=\"A4.T13.9.9.3\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.9.9.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.222</span></td>\n<td id=\"A4.T13.9.9.4\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.9.9.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.0</span></td>\n</tr>\n<tr id=\"A4.T13.10.10\" class=\"ltx_tr\">\n<td id=\"A4.T13.10.10.1\" class=\"ltx_td ltx_align_left\"><math id=\"A4.T13.10.10.1.m1.1\" class=\"ltx_Math\" alttext=\".950\" display=\"inline\"><semantics id=\"A4.T13.10.10.1.m1.1a\"><mn mathsize=\"70%\" id=\"A4.T13.10.10.1.m1.1.1\" xref=\"A4.T13.10.10.1.m1.1.1.cmml\">.950</mn><annotation-xml encoding=\"MathML-Content\" id=\"A4.T13.10.10.1.m1.1b\"><cn type=\"float\" id=\"A4.T13.10.10.1.m1.1.1.cmml\" xref=\"A4.T13.10.10.1.m1.1.1\">.950</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T13.10.10.1.m1.1c\">.950</annotation></semantics></math></td>\n<td id=\"A4.T13.10.10.2\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.10.10.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.513</span></td>\n<td id=\"A4.T13.10.10.3\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.10.10.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.238</span></td>\n<td id=\"A4.T13.10.10.4\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.10.10.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.0</span></td>\n</tr>\n<tr id=\"A4.T13.11.11\" class=\"ltx_tr\">\n<td id=\"A4.T13.11.11.1\" class=\"ltx_td ltx_align_left\"><math id=\"A4.T13.11.11.1.m1.1\" class=\"ltx_Math\" alttext=\".960\" display=\"inline\"><semantics id=\"A4.T13.11.11.1.m1.1a\"><mn mathsize=\"70%\" id=\"A4.T13.11.11.1.m1.1.1\" xref=\"A4.T13.11.11.1.m1.1.1.cmml\">.960</mn><annotation-xml encoding=\"MathML-Content\" id=\"A4.T13.11.11.1.m1.1b\"><cn type=\"float\" id=\"A4.T13.11.11.1.m1.1.1.cmml\" xref=\"A4.T13.11.11.1.m1.1.1\">.960</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T13.11.11.1.m1.1c\">.960</annotation></semantics></math></td>\n<td id=\"A4.T13.11.11.2\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.11.11.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.378</span></td>\n<td id=\"A4.T13.11.11.3\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.11.11.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.229</span></td>\n<td id=\"A4.T13.11.11.4\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.11.11.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.0</span></td>\n</tr>\n<tr id=\"A4.T13.12.12\" class=\"ltx_tr\">\n<td id=\"A4.T13.12.12.1\" class=\"ltx_td ltx_align_left\"><math id=\"A4.T13.12.12.1.m1.1\" class=\"ltx_Math\" alttext=\".970\" display=\"inline\"><semantics id=\"A4.T13.12.12.1.m1.1a\"><mn mathsize=\"70%\" id=\"A4.T13.12.12.1.m1.1.1\" xref=\"A4.T13.12.12.1.m1.1.1.cmml\">.970</mn><annotation-xml encoding=\"MathML-Content\" id=\"A4.T13.12.12.1.m1.1b\"><cn type=\"float\" id=\"A4.T13.12.12.1.m1.1.1.cmml\" xref=\"A4.T13.12.12.1.m1.1.1\">.970</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T13.12.12.1.m1.1c\">.970</annotation></semantics></math></td>\n<td id=\"A4.T13.12.12.2\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.12.12.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.450</span></td>\n<td id=\"A4.T13.12.12.3\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.12.12.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.191</span></td>\n<td id=\"A4.T13.12.12.4\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.12.12.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.073</span></td>\n</tr>\n<tr id=\"A4.T13.13.13\" class=\"ltx_tr\">\n<td id=\"A4.T13.13.13.1\" class=\"ltx_td ltx_align_left\"><math id=\"A4.T13.13.13.1.m1.1\" class=\"ltx_Math\" alttext=\".980\" display=\"inline\"><semantics id=\"A4.T13.13.13.1.m1.1a\"><mn mathsize=\"70%\" id=\"A4.T13.13.13.1.m1.1.1\" xref=\"A4.T13.13.13.1.m1.1.1.cmml\">.980</mn><annotation-xml encoding=\"MathML-Content\" id=\"A4.T13.13.13.1.m1.1b\"><cn type=\"float\" id=\"A4.T13.13.13.1.m1.1.1.cmml\" xref=\"A4.T13.13.13.1.m1.1.1\">.980</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T13.13.13.1.m1.1c\">.980</annotation></semantics></math></td>\n<td id=\"A4.T13.13.13.2\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.13.13.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.315</span></td>\n<td id=\"A4.T13.13.13.3\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.13.13.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.012</span></td>\n<td id=\"A4.T13.13.13.4\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.13.13.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.232</span></td>\n</tr>\n<tr id=\"A4.T13.14.14\" class=\"ltx_tr\">\n<td id=\"A4.T13.14.14.1\" class=\"ltx_td ltx_align_left\"><math id=\"A4.T13.14.14.1.m1.1\" class=\"ltx_Math\" alttext=\".990\" display=\"inline\"><semantics id=\"A4.T13.14.14.1.m1.1a\"><mn mathsize=\"70%\" id=\"A4.T13.14.14.1.m1.1.1\" xref=\"A4.T13.14.14.1.m1.1.1.cmml\">.990</mn><annotation-xml encoding=\"MathML-Content\" id=\"A4.T13.14.14.1.m1.1b\"><cn type=\"float\" id=\"A4.T13.14.14.1.m1.1.1.cmml\" xref=\"A4.T13.14.14.1.m1.1.1\">.990</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T13.14.14.1.m1.1c\">.990</annotation></semantics></math></td>\n<td id=\"A4.T13.14.14.2\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.14.14.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.360</span></td>\n<td id=\"A4.T13.14.14.3\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.14.14.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.0</span></td>\n<td id=\"A4.T13.14.14.4\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.14.14.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">0.422</span></td>\n</tr>\n<tr id=\"A4.T13.15.15\" class=\"ltx_tr\">\n<td id=\"A4.T13.15.15.1\" class=\"ltx_td ltx_align_left\"><math id=\"A4.T13.15.15.1.m1.1\" class=\"ltx_Math\" alttext=\".995\" display=\"inline\"><semantics id=\"A4.T13.15.15.1.m1.1a\"><mn mathsize=\"70%\" id=\"A4.T13.15.15.1.m1.1.1\" xref=\"A4.T13.15.15.1.m1.1.1.cmml\">.995</mn><annotation-xml encoding=\"MathML-Content\" id=\"A4.T13.15.15.1.m1.1b\"><cn type=\"float\" id=\"A4.T13.15.15.1.m1.1.1.cmml\" xref=\"A4.T13.15.15.1.m1.1.1\">.995</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T13.15.15.1.m1.1c\">.995</annotation></semantics></math></td>\n<td id=\"A4.T13.15.15.2\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.15.15.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.342</span></td>\n<td id=\"A4.T13.15.15.3\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.15.15.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.0</span></td>\n<td id=\"A4.T13.15.15.4\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T13.15.15.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.330</span></td>\n</tr>\n<tr id=\"A4.T13.16.16\" class=\"ltx_tr\">\n<td id=\"A4.T13.16.16.1\" class=\"ltx_td ltx_align_left ltx_border_bb\"><math id=\"A4.T13.16.16.1.m1.1\" class=\"ltx_Math\" alttext=\".999\" display=\"inline\"><semantics id=\"A4.T13.16.16.1.m1.1a\"><mn mathsize=\"70%\" id=\"A4.T13.16.16.1.m1.1.1\" xref=\"A4.T13.16.16.1.m1.1.1.cmml\">.999</mn><annotation-xml encoding=\"MathML-Content\" id=\"A4.T13.16.16.1.m1.1b\"><cn type=\"float\" id=\"A4.T13.16.16.1.m1.1.1.cmml\" xref=\"A4.T13.16.16.1.m1.1.1\">.999</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T13.16.16.1.m1.1c\">.999</annotation></semantics></math></td>\n<td id=\"A4.T13.16.16.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A4.T13.16.16.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.270</span></td>\n<td id=\"A4.T13.16.16.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A4.T13.16.16.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.0</span></td>\n<td id=\"A4.T13.16.16.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A4.T13.16.16.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">0.0</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Attacker without Auxiliary Data.\nWe experiment with an attacker who does not have access to a small mini-batch of data from the users’ distribution to tune the scaling factor s𝑠s of our trap weights.\nIn this setup, the only knowledge an attacker holds is about the dimensionality of the users’ data which it needs to instantiate an adequate model architecture.\nWe evaluate three attacks in this setup.\n1) Exploiting passive data leakage and composing a tuning dataset: the attacker randomly initializes the model in a first round of the protocol.\nOur results in Table I show that also randomly initialized models’ gradients leak significant fractions of the users’ data (MNIST 6.1%, CIFAR10 25.9%, and ImageNet 21.7%).\nBy plotting the user’s gradients and eyeballing which data points resemble natural images, the attacker can build a tuning set for s𝑠s.\nSince we only require a maximum of 100 data points to find the optimal values for s𝑠s per dataset in Table IV, the attacker only has to inspect the gradients of 17, 4, and 5 users for MNIST, CIFAR10, and ImageNet, respectively in the first round of the protocol.\nOn the selected data, they can tune s𝑠s and use it in every subsequent iteration.\nWe performed tuning on 100 data points obtained through passive extraction and obtained the same s𝑠s as through tuning on a random mini-batch of data (0.7, 0.95, and 0.99 for MNIST, CIFAR10, and ImageNet, respectively).\n2) Exploiting raw passive data leakage: Since manually, selecting suitable data points is time-consuming, we propose an alternative approach where the attacker uses all extracted data points with are in a valid range for input pixels ([0,1]) from the passive extraction on non-adversarially initialized model weights in the first round of the protocol.\nThese data points are not necessarily individually extracted user data points as we show in Figure 9 in Appendix D.2.\nBut the attacker can still consider them as a tuning dataset for s𝑠s and evaluate the extraction-recall on this dataset when initializing the shared model with different trap weights to tune s𝑠s.\nOur results in Table XIII show that for CIFAR10 and ImageNet, the best s𝑠s found on these passively reconstructed data points are equal to the best s𝑠s obtained directly by tuning on one mini-batch of the original data.\nFor MNIST, the s𝑠s on the extracted gradients differs slightly from the original best s𝑠s (0.75 vs. 0.7).\nWe suspect these changes to result from MNIST data being much sparser (many more zero features) than the extracted gradients in Figure 9(a).\n3) Using a surrogate dataset of same data dimensions: Lastly, the attacker can tune s𝑠s on a surrogate dataset of the same dimension (but potentially different distribution) than the users’ data.\nWe compare extraction-recall of an adversarial weight initialization with s𝑠s found on a surrogate dataset and the optimal s∗superscript𝑠s^{*} found on the actual dataset for Fashion MNIST, SVHN, CIFAR100, and Open Images\n [30] in Table VI.\nOur results highlight that extraction with the surrogate s𝑠s obtained through tuning on MNIST, CIFAR10, and ImageNet, already yields a significantly higher success than passive extraction on non-manipulated weights.\nFurthermore, the closer the surrogate dataset’s distribution is to the users’ dataset, the closer s𝑠s and s∗superscript𝑠s^{*}.\nEspecially for CIFAR10 and CIFAR100, and ImageNet and Open Images, we find that s=s∗𝑠superscript𝑠s=s^{*} which leads to highest extraction success."
        ]
    },
    "A4.T14": {
        "caption": "TABLE XIV: CNN Architecture by [4] used to evaluate the data extraction attack under the impact of Dropout and Pooling. f: number of filters, k: kernel size, s: stride, p: padding act: activation function, n: number of neurons.",
        "table": "<table id=\"A4.T14.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"A4.T14.2.1\" class=\"ltx_tr\">\n<td id=\"A4.T14.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"A4.T14.2.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">CNN Architecture by </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A4.T14.2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">[</span><a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\">4</a><span id=\"A4.T14.2.1.1.3.2\" class=\"ltx_text\" style=\"font-size:70%;\">]</span></cite>\n</td>\n</tr>\n<tr id=\"A4.T14.2.2\" class=\"ltx_tr\">\n<td id=\"A4.T14.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A4.T14.2.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Conv(f=32, k=(3,3), s=1, p=same, act=relu)</span></td>\n</tr>\n<tr id=\"A4.T14.2.3\" class=\"ltx_tr\">\n<td id=\"A4.T14.2.3.1\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T14.2.3.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">MaxPool()</span></td>\n</tr>\n<tr id=\"A4.T14.2.4\" class=\"ltx_tr\">\n<td id=\"A4.T14.2.4.1\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T14.2.4.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Conv(f=64, k=(3,3), s=1, p=same, act=relu)</span></td>\n</tr>\n<tr id=\"A4.T14.2.5\" class=\"ltx_tr\">\n<td id=\"A4.T14.2.5.1\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T14.2.5.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Dropout()</span></td>\n</tr>\n<tr id=\"A4.T14.2.6\" class=\"ltx_tr\">\n<td id=\"A4.T14.2.6.1\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T14.2.6.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Flatten</span></td>\n</tr>\n<tr id=\"A4.T14.2.7\" class=\"ltx_tr\">\n<td id=\"A4.T14.2.7.1\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T14.2.7.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Dense(n=1000, act=relu)</span></td>\n</tr>\n<tr id=\"A4.T14.2.8\" class=\"ltx_tr\">\n<td id=\"A4.T14.2.8.1\" class=\"ltx_td ltx_align_center\"><span id=\"A4.T14.2.8.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Dropout()</span></td>\n</tr>\n<tr id=\"A4.T14.2.9\" class=\"ltx_tr\">\n<td id=\"A4.T14.2.9.1\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A4.T14.2.9.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Dense(n=#classes, act=None)</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "We also study the effect of ”lossy” layers, such as dropout and pooling on our data extraction success.\nTherefore, we rely on the following architecture proposed by ",
                "[",
                "4",
                "]",
                " for FL, see ",
                "Table",
                " ",
                "XIV",
                ".",
                "Figures",
                " ",
                "15",
                " and ",
                "16",
                " and ",
                "Figures",
                " ",
                "17",
                " and ",
                "18",
                " show individual effects of dropout and pooling layers on a reconstructions for mini-batches of size 1 and 20 respectively.\nWe evaluated different dropout rates ",
                "p",
                "∈",
                "{",
                "0.0",
                ",",
                "0.1",
                ",",
                "0.3",
                ",",
                "0.5",
                ",",
                "0.7",
                ",",
                "0.9",
                "}",
                "𝑝",
                "0.0",
                "0.1",
                "0.3",
                "0.5",
                "0.7",
                "0.9",
                "p\\in\\{0.0,0.1,0.3,0.5,0.7,0.9\\}",
                ".\nNote that the second dropout layer does not have a significant impact on the success of our reconstruction since we extract from the first fully-connected model layer before information can get lost due to the second dropout.\nTo evaluate dropout without pooling, we remove the MaxPool layer, and to evaluate pooling without dropout, we set the dropout rate to ",
                "p",
                "=",
                "0.1",
                "𝑝",
                "0.1",
                "p=0.1",
                ".\nAlthough existence of non-invertible components compromises overall reconstruction fidelity, we observe it is often possible to still recognise individual data points."
            ]
        ]
    }
}