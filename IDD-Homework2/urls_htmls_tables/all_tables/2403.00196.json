{
    "S4.T1": {
        "caption": "TABLE I: Comparison of Model Architectures when training front-view.",
        "table": "<table id=\"S4.T1.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.4.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.4.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">Method</th>\n<th id=\"S4.T1.4.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">Average Test L1 Error</th>\n<th id=\"S4.T1.4.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Standard Deviation</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.4.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.4.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">CycleGAN</td>\n<td id=\"S4.T1.4.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.2179</td>\n<td id=\"S4.T1.4.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.0633</td>\n</tr>\n<tr id=\"S4.T1.4.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.4.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T1.4.3.2.1.1\" class=\"ltx_text ltx_font_bold\">pix2pix</span></td>\n<td id=\"S4.T1.4.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T1.4.3.2.2.1\" class=\"ltx_text ltx_font_bold\">0.0676</span></td>\n<td id=\"S4.T1.4.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.4.3.2.3.1\" class=\"ltx_text ltx_font_bold\">0.0106</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "We first compare the performance of the pix2pix architecture versus the CycleGAN architecture, which has proven useful in past data augmentation for driver state monitoring [26]. As shown in Table I, the performance of the CycleGAN is significantly subpar to pix2pix. We expect that this is due to the attempt by CycleGAN to reconstruct the original input from its generated output during training; because the thermal image is significantly lossy compared to the amount of information in the visible light images, this relationship is more difficult to model beyond approximation. For interpretation, we note that all experimental error values are on normalized pixel values in the range [0, 1], meaning that most errors range around 5-6% of the pixel range."
        ]
    },
    "S4.T2": {
        "caption": "TABLE II: Comparison of Input Style; Average Performance Across 17 Subjects.",
        "table": "<table id=\"S4.T2.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.4.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.4.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Dataset</th>\n<th id=\"S4.T2.4.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Average Test L1 Error</th>\n<th id=\"S4.T2.4.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Standard Deviation</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.4.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.4.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">Front-View</td>\n<td id=\"S4.T2.4.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">0.0676</td>\n<td id=\"S4.T2.4.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">0.0106</td>\n</tr>\n<tr id=\"S4.T2.4.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.4.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Four-View, Tessellated</td>\n<td id=\"S4.T2.4.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.0587</td>\n<td id=\"S4.T2.4.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.0109</td>\n</tr>\n<tr id=\"S4.T2.4.4.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.4.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T2.4.4.3.1.1\" class=\"ltx_text ltx_font_bold\">Four-View, Stacked</span></td>\n<td id=\"S4.T2.4.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T2.4.4.3.2.1\" class=\"ltx_text ltx_font_bold\">0.0559</span></td>\n<td id=\"S4.T2.4.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S4.T2.4.4.3.3.1\" class=\"ltx_text ltx_font_bold\">0.0093</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "The results of our experiments comparing the three different styles of input are presented in Table II. We find that a combination of views outperforms generation from a single-view, Figure 9, (perhaps assisting in understanding hand and posture positioning and respective heat signatures), and that the stacked-view and tesselated-view, Figures 7&10, of the images provides an efficient and effective input, evidenced by the lowest average L1 error (0.0559) and a comparatively low standard deviation (0.0093). This suggests that considering spatial relationships by multi-view information enhances the model\u2019s accuracy in thermal image generation, as seen in Figure 7."
        ]
    },
    "S4.T3": {
        "caption": "TABLE III: Comparison of Single vs. Multi-Subject Training on Front View.",
        "table": "<table id=\"S4.T3.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.4.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.4.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Dataset</th>\n<th id=\"S4.T3.4.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Average Test L1 Error</th>\n<th id=\"S4.T3.4.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Std. Deviation</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.4.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S4.T3.4.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Single-Subject Training</span></td>\n<td id=\"S4.T3.4.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S4.T3.4.2.1.2.1\" class=\"ltx_text ltx_font_bold\">0.0676</span></td>\n<td id=\"S4.T3.4.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T3.4.2.1.3.1\" class=\"ltx_text ltx_font_bold\">0.0106</span></td>\n</tr>\n<tr id=\"S4.T3.4.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">Multi-Subject Training</td>\n<td id=\"S4.T3.4.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.1116</td>\n<td id=\"S4.T3.4.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">0.0186</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "The results of our experiments on model generalizability to multi-subject training data is presented in Table III. We find that though the training dataset size grows significantly (17\u00d7\\times) and still includes the original training data, the additional subjects seem to contribute more to model confusion than generalized pattern creation, showing a higher average L1 error of 0.1116 and supporting the idea that these models are best trained on an individual basis. The relatively weak performance when trained on the more diverse set of data can be observed in Figure 8."
        ]
    }
}