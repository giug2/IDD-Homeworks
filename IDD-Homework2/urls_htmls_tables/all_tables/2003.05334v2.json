{
    "S4.T1": {
        "caption": "Table 1: Comparison of RL algorithms and their online meta-learning enhancements, including meta-learner PPO-LIRPG [44]. Max Average Return over 5 trials over all time steps. Max value for each comparison is in bold, and max value overall is underlined.",
        "table": null,
        "footnotes": [],
        "references": [
            "DDPG. Figure 2 shows the learning curves of DDPG and DDPG-MC. The results in each task are averaged over 5 random seeds (trials) and network initialisations. The standard deviation intervals are shown as shaded regions over time steps. Following Fujimoto et al. [9], curves are uniformly smoothed for clarity (window_size=10 for TORCS, 30 for others). We run MuJoCo-Gym tasks for 1-10 million depending on the environment, rllab tasks for 3 million and TORCS experiment for 100 thousand steps. Every 1000 steps we evaluate our policy over 10 episodes with no exploration noise. From Figure 2, DDPG-MC generally outperforms DDPG baseline in terms of the learning speed and asymptotic performance. Furthermore, -MC usually has smaller variance. The summary results for all tasks using the vanilla baseline and -MCs in terms of max average return are shown in Table 1. -MC usually provides consistently higher max return. We select seven tasks for plotting. The other MuJoCo tasks “Reacher”, “InvPend” and “InvDouPend” have reward upper bounds that all methods can reach quickly without obvious differences.",
            "TD3 and SAC.\nFigure 3 reports the learning curves for TD3. For some tasks the vanilla TD3’s performance declines in the long run, while TD3-MC shows improved stability with much higher asymptotic performance. Thus TD3-MC provides comparable or better learning performance in each case, while Table 1 shows the clear improvement in the max average return. For SAC in Figure 4, note that we use the most recent update of SAC [13], which is actually the combination of SAC+TD3. Although SAC+TD3 is arguably the strongest existing method, SAC-MC still gives a clear boost on the asymptotic performance for many tasks, especially the most challenging TORCS.",
            "Comparison vs PPO-LIRPG. Intrinsic Reward Learning for PPO [44] is the most related method to our work in performing online single-task meta-learning of an additional reward/loss. Their original PPO-LIRPG evaluated on a modified environment with hidden rewards. Here we apply it to the standard unmodified learning tasks that we aim to improve. Table 1 tells that: (i) In this conventional setting, PPO-LIRPG worsens rather than improves basic PPO performance. (ii) Overall OffP-AC methods generally perform better than on-policy PPO for most environments. This shows the importance of our meta-learning contribution to the off-policy setting. In general Meta-Critic is preferred compared to PPO-LIRPG because the latter only provides a scalar reward bonus that helps the policy indirectly via high-variance policy-gradient updates, while ours provides a direct loss.",
            "Summary. Table 1 and Figure 5 summarize all the results by max average return. SAC-MC generally performs best and -MCs are generally comparable or better than their corresponding vanilla alternatives. -MCs usually provide improved variance in return compared to their baselines."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Ablation study on different designs of Lωmcriticsubscriptsuperscript𝐿mcritic𝜔L^{\\text{mcritic}}_{\\omega} (i.e., hωsubscriptℎ𝜔h_{\\omega}) and Lmetasuperscript𝐿metaL^{\\text{meta}} applied to SAC-Walker2d. Max and Sum Average Return over 5 trials of all time steps. Max value in each row is in bold.",
        "table": null,
        "footnotes": [],
        "references": [
            "Ablation on hωsubscriptℎ𝜔h_{\\omega} design. We run Walker2d under SAC-MC with the alternative hωsubscriptℎ𝜔h_{\\omega} from Eq. (11) or in MetaReg [2] format (input actor parameters directly). In Table 2, we record the max average return and sum average return (area under the average reward curve) of evaluations over all time steps. Eq. (11) achieves the highest max average return and our default hωsubscriptℎ𝜔h_{\\omega} (Eq. (10)) attains the highest mean average return. We can also see some improvement for hω​(ϕ)subscriptℎ𝜔italic-ϕh_{\\omega}(\\phi) in MetaReg format, but the huge number (73484) of parameters is expensive. Overall, all meta-critic designs provide at least a small improvement on vanilla SAC.",
            "Ablation on meta-loss design. We considered two meta-loss designs in Eqs. (8&9).\nFor Lc​l​i​pmetasubscriptsuperscript𝐿meta𝑐𝑙𝑖𝑝L^{\\text{meta}}_{clip} in Eq. (9), we use Lcritic​(dv​a​l;ϕo​l​d)superscript𝐿criticsubscript𝑑𝑣𝑎𝑙subscriptitalic-ϕ𝑜𝑙𝑑L^{\\text{critic}}(d_{val};\\phi_{old}) as a baseline to improve numerical stability of the gradient update. To evaluate this design, we also compare using vanilla Lmetasuperscript𝐿metaL^{\\text{meta}} in Eq. (8). The last column in Table 2 shows vanilla Lmetasuperscript𝐿metaL^{\\text{meta}} barely improves on vanilla SAC, validating our meta-loss design."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Controlling for compute cost and parameter count. Max Average Return over 5 trials over all time steps. Max value for each comparison is in bold.",
        "table": null,
        "footnotes": [],
        "references": [
            "The max average return results for the seven tasks in these control experiments are shown in Table 3, and the detailed learning curves of the control experiments are in the supplementary material. Overall, there is no consistent benefit in providing the baseline with more compute iterations or parameters, and in many environments they perform worse than the baseline or even fail entirely, especially in ‘+updates’ condition. Thus -MCs’ good performance can not be simply replicated by a corresponding increase in gradient steps or parameter size taken by the baseline."
        ]
    }
}