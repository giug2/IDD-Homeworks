{
    "id_table_1": {
        "caption": "Table 1:  Main results of different methods on the test sets of GSM8K and MATH. The SFT training with external tool is based on (a subset of) Open-MathInstruct so the results are generally comparable to the previous SFT models.    \\dagger  : the model also serves as the starting checkpoint of other methods except for prompting and CoT without tool use. All the models are allowed to use code interpreter except for the CoT without tool use. The results of the CoT methods are borrowed from the technical reports  (Toshniwal et al.,  2024 ; Gou et al.,  2023b ) . The gains relative to the SFT starting checkpoint is marked by      \\uparrow  .",
        "table": "S1.E2",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "See Figure  1  for an example. The framework presented here is a Markov decision process (MDP), which offers a distinct approach from the contextual bandit model discussed in  Xiong et al.  . Formally, we define the following MDP.",
            "Further advancements include adapting direct preference learning algorithms to mathematical problem solving. For instance,  Jiao et al. ( 2024 ); Yuan et al. ( 2024 )  have applied the original DPO or KTO by taking the trajectory completion as a meta action.  Xie et al. ( 2024b ); Pang et al. ( 2024 )  further adapt the online iterative DPO originally designed for chat  ( Xiong et al.,  ; Xu et al.,  2023 ; Hoang Tran,  2024 )  and achieve better performance for CoT reasoning. Inspired by the success of PRMs, recent studies have explored generating proxy step-wise labels for the intermediate steps of the reasoning trajectories. For instance,  Xie et al. ( 2024b ); Chen et al. ( 2024a ); Lai et al. ( 2024 )  leverage Monte Carlo Tree Search (MCTS) and use the estimated Q value to generate the proxy labels for the intermediate steps.  Lai et al. ( 2024 )  proposes to use AI feedback like GPT-4  (Lai et al.,  2024 )  to find the first error step in the trajectory. Meanwhile,  Lu et al. ( 2024 )  identifies a trajectory with the correct final answer and no errors as preferable, and prompts the SFT model with a high temperature, starting from some intermediate step to collect a rejected trajectory with errors  (Pi et al.,  2024 ) . Finally, a very recent study by  Chen et al. ( 2024a )  proposes to use MCTS with a backward iteration from the final leaf node to compute the proxy unregularized value of each node. Preference pairs are then extracted from the tree by fixing the prefix and comparing  the next single reasoning step . Then, they run the original DPO on these intermediate actions with the proxy labels from MCTS. To summarize, these works present different ways of preference data collection and apply the original DPO algorithm (with some additional marginal loss and regularization adapted from the literature), thereby differing from our work in both algorithmic concepts and application scope. In contrast, we study preference learning in the context of trajectory-level comparison, where we derive the optimality condition and introduce a multi-turn DPO within an online iterative framework, specifically for tool-integrated mathematical problem solving. However, we remark that while we focus on the trajectory-level comparison, the preference signal itself can be generated in a step-by-step supervision (see Section  1.1  for the detailed examples). When preference signals for partial trajectories with shared prefixes are available, our method can also adapt to learn these step-level signals (see the optimality condition in ( 11 )). In particular, the algorithmic design presented in this paper can be readily combined with the MCTS-based data collection strategy outlined in recent literature, which we leave for future work.",
            "We develop the main algorithms of this paper in this section. We proceed to handle the general MDP formulation presented in Section  1.1 , which subsumes the tool-integrated mathematical reasoning problem as a special example. Therefore, the algorithms may also be applied to more general scenarios with external messages..",
            "We emphasize again that although the loss presented in ( 12 ) is identical to the one in ( 8 ), a rigorous derivation procedure (rather than a direct plug-in) is provided. To the best of our knowledge, ( 12 ) is new in the context of multi-turn reasoning task with external messages. In particular, it is noted that such a M-DPO loss is only valid upon deterministic transitions, i.e., term  ( C ) = 0 C 0 (C)=0 ( italic_C ) = 0 .",
            "Moreover, with ( 11 ) implying that with term  ( C ) = 0 C 0 (C)=0 ( italic_C ) = 0 , the implicit reward is given by  A =    h = 1 H log   h   ( a h | s h )  ref , h  ( a h | s h ) A  superscript subscript h 1 H subscript superscript  h conditional subscript a h subscript s h subscript  ref h conditional subscript a h subscript s h A=\\eta\\sum_{h=1}^{H}\\log\\frac{\\pi^{*}_{h}(a_{h}|s_{h})}{\\pi_{\\mathrm{ref},h}(a% _{h}|s_{h})} italic_A = italic_  start_POSTSUBSCRIPT italic_h = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT roman_log divide start_ARG italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ( italic_a start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT | italic_s start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) end_ARG start_ARG italic_ start_POSTSUBSCRIPT roman_ref , italic_h end_POSTSUBSCRIPT ( italic_a start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT | italic_s start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) end_ARG , a multi-turn version of KTO  (Ethayarajh et al.,  2024 ) , denoted as M-KTO, can also be naturally derived:",
            "The above discussions, in particular, M-DPO and M-KTO losses provided in ( 12 ) and ( 13 ), are focused on deterministic observations due to the deterministic nature of tool-integrated LLMs for mathematical reasoning. In contrast, some other applications may encounter stochastic observations, e.g., multi-turn chats with the external message provided by a human or another LLM  (Shani et al.,  2024 ) . In these scenarios, ( 12 ) is biased and cannot lead to the optimal policy since  term   ( C ) = 0 term  C 0 \\text{term }(C)\\neq 0 term ( italic_C ) = 0 . Instead, one should first construct a value network based on the Bellman equations provided in ( 6 ) and ( 7 ), similar to the approach in  Richemond et al. ( 2024 ) . Subsequently,  term   ( C ) term  C \\text{term }(C) term ( italic_C )  can be estimated using Monte-Carlo methods and serve as an adaptive margin in the preference training. Particularly, the distinctions between direct preference learning algorithms and classical deep RL methods become less clear. The exploration of this more complex algorithm and its application to general multi-turn learning scenarios is left for future research.",
            "Additionally, we note that a stronger KL regularization in the target ( 14 ) is known to be beneficial for mitigating over-fitting issue and forgetting on the  out-of-domain  tasks  (Gao et al.,  2023b ; Lin et al.,  2023 ; Coste et al.,  2023 ) . On the other hand, ( 15 ) allows the model to move more far away, thus achieving a better  in-domain  performance. Thus, from one perspective, the choice between the above two targets can be viewed as a tradeoff between out-of-domain and in-domain performances. This intuition is also verified by later experiments, where optimizing the second target in ( 15 ) leads to better performance on in-domain test sets. In the rest of this section, we discuss two learning objectives to fully develop the multi-turn preference learning framework. We also conduct an ablation study on these objectives in the experimental section.",
            "We present a general online iterative algorithmic framework in Algorithm  1 . This framework is termed as  Online Iterative Multi-turn Gibbs Sampling from Human Feedback (M-GSHF)  to highlight the online iterative training process and the optimal condition derived in ( 7 ) that the optimal policy is a layer-wise Gibbs distribution, which generalizes the bandit formulation in  Xiong et al.  . Specifically, starting from   0 subscript  0 \\pi_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , at each iteration, we first collect a pair of trajectories by the current policy pair, where the preference signal is also revealed according to Definition  1 . Then, we update our policy pair given the data collected so far and the next iteration begins. We now discuss some features of the framework as follows.",
            "Policy choice for exploration-exploitation trade-off.  We update our behavior policies in a non-symmetric way. The first agent, which aims to extract the historical information we have gathered so far, planning with respect to the empirically best model on the historical dataset  D D \\mathcal{D} caligraphic_D  to get   t 1 superscript subscript  t 1 \\pi_{t}^{1} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , where the planning algorithms have been discussed in Section  2.2 , e.g., optimizing the M-DPO or M-KTO loss in ( 12 ) or ( 13 ). However, it is widely recognized in RL studies  (Sutton and Barto,  2018 ; Auer et al.,  2002 )  that simply exploiting the historical data via following the empirically best model is not sufficient to obtain a good final policy, while it is also required to explore the environment so that new information can be collected to facilitate subsequent learning, i.e., the exploration-exploitation tradeoff. While the main agent targeting exploitation, we design the second agent, in contrast, to strategically incorporate the uncertainty of the future relative to   t 1 superscript subscript  t 1 \\pi_{t}^{1} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT  given the historical information we collect so far into its policy choice. We call the policy of the second agent   t 2 superscript subscript  t 2 \\pi_{t}^{2} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  as an exploration policy because it serves to explore the underlying environment and facilitate the first agents learning. In practice, this principle of exploration is generally interpreted as maximizing the difference between the two behavior policies or increasing the diversity of the collected data. We summarize some popular heuristic exploration policy adopted in the online iterative RLHF practice:",
            "Reference model choice for controlling regularization level.  Despite two different learning targets are discussed in ( 14 ) and ( 15 ) seperately, we note that one general algorithmic framework can be adopted with the reference model choice taking as a hyper-parameter to control the regularization level and account for the two targets:",
            "KL-regularized target in ( 14 ): if we fix the reference model as the initial policy, i.e.,   t , ref =  0 ,  t  [ T ] formulae-sequence subscript  t ref subscript  0 for-all t delimited-[] T \\pi_{t,\\mathrm{ref}}=\\pi_{0},\\forall t\\in[T] italic_ start_POSTSUBSCRIPT italic_t , roman_ref end_POSTSUBSCRIPT = italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ,  italic_t  [ italic_T ] , we always search the optimal policy within the KL ball centered at   0 subscript  0 \\pi_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , and thus optimize the KL-regularized target.",
            "Non-regularized target in ( 15 ): in contrast, inspired by the mirror descent  (Nemirovskij and Yudin,  1983 ) , if we update the reference policy every iteration to be the policy learned in the last iteration, i.e.,   t , ref =  t  1 1 ,  t  [ T ] formulae-sequence subscript  t ref subscript superscript  1 t 1 for-all t delimited-[] T \\pi_{t,\\mathrm{ref}}=\\pi^{1}_{t-1},\\forall t\\in[T] italic_ start_POSTSUBSCRIPT italic_t , roman_ref end_POSTSUBSCRIPT = italic_ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ,  italic_t  [ italic_T ] , the cumulative update can make the model to move away from the original   0 subscript  0 \\pi_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  (while a constraint is made on the per-iteration update magnitude) and we thus optimize the non-regularized target.",
            "The overall algorithm, i.e., the theoretical version of online iterative M-GSHF, is also summarized in Algorithm  1 . At each round  t t t italic_t , with  D =  i = 1 t  1 D i D superscript subscript i 1 t 1 subscript D i \\mathcal{D}=\\cup_{i=1}^{t-1}\\mathcal{D}_{i} caligraphic_D =  start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  as the aggregated dataset, it starts with performing a maximum likelihood estimation (MLE) of the reward function  u  superscript u u^{*} italic_u start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  over a set  U U \\mathcal{U} caligraphic_U , whose elements are bounded in  [ 0 , B ] 0 B [0,B] [ 0 , italic_B ] , as",
            "Based on Proposition  1 , the exploration policy   t 2 superscript subscript  t 2 \\pi_{t}^{2} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  is selected as",
            "We note that the Eluder coefficient and the generalized Eluder-type condition are standard and well-adopted conditions in the theoretical studies on RL  (Zhang,  2023 ; Zhong et al.,  2022 ; Liu et al.,  2023a ; Xie et al.,  2022 ; Agarwal et al.,  2023 )  and also RLHF  (Zhan et al.,  2023 ; Wang et al.,  2023b ; Ye et al.,  2024 ) . Moreover, for a board class of RL problems (see  Zhang ( 2023 ); Liu et al. ( 2023a )  for more details), the Eluder coefficient  d U subscript d U d_{\\mathcal{U}} italic_d start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT  is small and the condition is satisfied with    ( d P , T , c 2  log  ( | P |  H  T /  ) )  d P  T  log  ( | P |  H  T /  ) less-than-or-similar-to  subscript d P T subscript c 2 P H T  subscript d P T P H T  \\xi(d_{\\mathcal{P}},T,c_{2}\\log(|\\mathcal{P}|HT/\\delta))\\lesssim\\sqrt{d_{% \\mathcal{P}}T\\log(|\\mathcal{P}|HT/\\delta)} italic_ ( italic_d start_POSTSUBSCRIPT caligraphic_P end_POSTSUBSCRIPT , italic_T , italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_log ( | caligraphic_P | italic_H italic_T / italic_ ) )  square-root start_ARG italic_d start_POSTSUBSCRIPT caligraphic_P end_POSTSUBSCRIPT italic_T roman_log ( | caligraphic_P | italic_H italic_T / italic_ ) end_ARG , which implies that the regret of theoretical version of Algorithm  1  is sublinear in  T T T italic_T , further evidencing its statistical efficiency.",
            "We format the data into a multi-turn chat where the user initially ask the LLMs a question, and provide the messages returned by the Python interpreter in the subsequent user rounds of chat. In each model turn, the model reasons based the history gathered so far and can output a final answer enclosed in  \\ \\ \\backslash \\ boxed, or call the Python interpreter by writing a code wrapped in ```python and ```. After receiving the response of the model, we return the execution result of the code if the model calls the tool, and stop if the model outputs the final answer or reaches the maximal number of rounds H (6 in our setting). See Figure  1  for an illustration. We generated N=30 samples per prompt for each iteration using a temperature setting of 1.0, without employing top-K or top-p sampling. We employ a mixture sampling strategy, where the up-to-date model generates only 20 trajectories, and the remainder (10 trajectories) are collected using the model from the last iteration. For the initial iteration, we employed models fine-tuned for 3 epochs and 1 epoch, respectively, to conduct mixture sampling. Intuitively, the mixture sampling helps to improve the diversity of the collected samples, and have been employed in previous RLHF practices  (Bai et al.,  2022 ; Dong et al.,  2024 ) . For all the data generation process, we adopt the following constraints: (1) for each turn, the model can generate up to 512 tokens; (2) the maximal number of steps is H=6; (3) the maximal number of token for each trajectory is 2048.",
            "We evaluate the models in the zero-shot setting and report the main results in Table  1 .",
            "From the first two sections in Table  1 , we first observe that the tool-integrated LLMs significantly outperform their CoT counterparts with only SFT, demonstrating the benefits of leveraging external tools. In the subsequent discussions, we focus on the comparison within the scope of tool-integrated LLMs.",
            "We do not tune the prompt though we do observe that the prompt engineering can further improve the performance. For all the experiments, we simply adopt the chat template of the models as in Figure  1 .",
            "where the first inequality is from Proposition B.2 from  Liu et al. ( 2023a )  and the second inequality uses Lemma  1 . The result for  u ^ t subscript ^ u t \\hat{u}_{t} over^ start_ARG italic_u end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  can be similarly established.",
            "where the inequality is from the definition of   t 2 subscript superscript  2 t \\pi^{2}_{t} italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and the fact that  ( u  , P  )  U ~ t  P ~ t superscript u superscript P subscript ~ U t subscript ~ P t (u^{*},\\mathbb{P}^{*})\\in\\widetilde{\\mathcal{U}}_{t}\\times\\widetilde{\\mathcal{% P}}_{t} ( italic_u start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , blackboard_P start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT )  over~ start_ARG caligraphic_U end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  over~ start_ARG caligraphic_P end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  from Lemma  1 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Ablation study of the impact of KL regularization. The SFT policy is the starting checkpoint for all other experiments.",
        "table": "S1.E3",
        "footnotes": [],
        "references": [
            "We emphasize again that although the loss presented in ( 12 ) is identical to the one in ( 8 ), a rigorous derivation procedure (rather than a direct plug-in) is provided. To the best of our knowledge, ( 12 ) is new in the context of multi-turn reasoning task with external messages. In particular, it is noted that such a M-DPO loss is only valid upon deterministic transitions, i.e., term  ( C ) = 0 C 0 (C)=0 ( italic_C ) = 0 .",
            "The above discussions, in particular, M-DPO and M-KTO losses provided in ( 12 ) and ( 13 ), are focused on deterministic observations due to the deterministic nature of tool-integrated LLMs for mathematical reasoning. In contrast, some other applications may encounter stochastic observations, e.g., multi-turn chats with the external message provided by a human or another LLM  (Shani et al.,  2024 ) . In these scenarios, ( 12 ) is biased and cannot lead to the optimal policy since  term   ( C ) = 0 term  C 0 \\text{term }(C)\\neq 0 term ( italic_C ) = 0 . Instead, one should first construct a value network based on the Bellman equations provided in ( 6 ) and ( 7 ), similar to the approach in  Richemond et al. ( 2024 ) . Subsequently,  term   ( C ) term  C \\text{term }(C) term ( italic_C )  can be estimated using Monte-Carlo methods and serve as an adaptive margin in the preference training. Particularly, the distinctions between direct preference learning algorithms and classical deep RL methods become less clear. The exploration of this more complex algorithm and its application to general multi-turn learning scenarios is left for future research.",
            "Policy choice for exploration-exploitation trade-off.  We update our behavior policies in a non-symmetric way. The first agent, which aims to extract the historical information we have gathered so far, planning with respect to the empirically best model on the historical dataset  D D \\mathcal{D} caligraphic_D  to get   t 1 superscript subscript  t 1 \\pi_{t}^{1} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , where the planning algorithms have been discussed in Section  2.2 , e.g., optimizing the M-DPO or M-KTO loss in ( 12 ) or ( 13 ). However, it is widely recognized in RL studies  (Sutton and Barto,  2018 ; Auer et al.,  2002 )  that simply exploiting the historical data via following the empirically best model is not sufficient to obtain a good final policy, while it is also required to explore the environment so that new information can be collected to facilitate subsequent learning, i.e., the exploration-exploitation tradeoff. While the main agent targeting exploitation, we design the second agent, in contrast, to strategically incorporate the uncertainty of the future relative to   t 1 superscript subscript  t 1 \\pi_{t}^{1} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT  given the historical information we collect so far into its policy choice. We call the policy of the second agent   t 2 superscript subscript  t 2 \\pi_{t}^{2} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  as an exploration policy because it serves to explore the underlying environment and facilitate the first agents learning. In practice, this principle of exploration is generally interpreted as maximizing the difference between the two behavior policies or increasing the diversity of the collected data. We summarize some popular heuristic exploration policy adopted in the online iterative RLHF practice:",
            "A graphical illustration is provided in Figure  2  to facilitate the understanding.",
            "The effectiveness of (iterative) DPO is significantly influenced by the choice of reference model and the KL coefficient. Previous research by  Tunstall et al. ( 2023 )  on offline DPO for general chatbot applications suggests that a lower KL coefficient, specifically 0.01, yields superior performance by allowing the resulting model to move more far away from the SFT model   0 subscript  0 \\pi_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT . Meanwhile, for online iterative training,  Xiong et al.  ; Dong et al. ( 2024 )  adopt a fixed reference model of   0 subscript  0 \\pi_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , and achieves continuous improvements as the training iterates. In our ablation study, we consider two different choices: (1) using the fixed reference model   0 subscript  0 \\pi_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ; (2) updating the reference model to the last iterations model at each round. Moreover, we search the KL coefficient    { 0.01 , 0.1 , 0.5 }  0.01 0.1 0.5 \\eta\\in\\{0.01,0.1,0.5\\} italic_  { 0.01 , 0.1 , 0.5 } . The results are summarized in Table  2 . First, we notice that if we update the reference model at each iteration, the final model outperforms the one with a fixed reference model   0 subscript  0 \\pi_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  with a large margin. Essentially, this dynamic approach optimizes the non-regularized reward, while the one with a fixed reference model   0 subscript  0 \\pi_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  aims to maximize the KL-regularized reward. This can be viewed as a trade-off between the generation diversity and reward optimization. We suspect this performance difference is because for reasoning task, the correct reasoning paths are highly concentrated on a small subset of the generation space, and the diversity is less important in this case.",
            "where the first inequality is from the Hoeffding inequality, the second inequality uses the Eluder coefficient  d U := EC  ( 1 , U  U , T ) assign subscript d U EC 1 U U T d_{\\mathcal{U}}:=\\texttt{EC}(1,\\mathcal{U}-\\mathcal{U},T) italic_d start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT := EC ( 1 , caligraphic_U - caligraphic_U , italic_T )  from Definition  4 , the third inequality leverages the mean value theorem with   := 1 / ( 2 + exp  (  B ) + exp  ( B ) ) assign  1 2 B B \\kappa:=1/(2+\\exp(-B)+\\exp(B)) italic_ := 1 / ( 2 + roman_exp ( - italic_B ) + roman_exp ( italic_B ) )  representing the minimum derivative of    (  )   \\sigma(\\cdot) italic_ (  )  in the regime of  [ 0 , B ] 0 B [0,B] [ 0 , italic_B ] , and the last inequality incorporates Lemma  2 . A similar result can be obtained for  term (B) t subscript term (B) t \\text{term (B)}_{t} term (B) start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .",
            "where the last step is from the generalized Eluder-type condition in Definition  5  and Lemma  2 . A similar result can be obtained for  term (D) t subscript term (D) t \\text{term (D)}_{t} term (D) start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Ablation study of the impact of sampling strategy. The SFT policy is the starting checkpoint for all other experiments. Mixture sampling is adopted for the iterative M-DPO training by default and we run for three iterations in total.",
        "table": "S2.Ex4",
        "footnotes": [],
        "references": [
            "In the single-turn case (i.e.,  H = 1 H 1 H=1 italic_H = 1  and without transitions  P P \\mathbb{P} blackboard_P ),  Rafailov et al. ( 2023 ); Azar et al. ( 2023 )  show that the optimal solution with respect to a utility function  u u u italic_u  admits a closed-form solution, which is the  Gibbs distribution  (see Lemma  3 ):",
            "The idea is to take a backward iteration from  h = H = 2 h H 2 h=H=2 italic_h = italic_H = 2  to  h = 1 h 1 h=1 italic_h = 1 . Specifically, when we fix  s 2 subscript s 2 s_{2} italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  and consider the inner loop, we can leverage Lemma  3  to solve",
            "where  Z h  ( s h ) =  a h  A  ref , h  ( a h | s h )  exp  ( Q M , h  ( s h , a h )  ) subscript Z h subscript s h subscript subscript a h A  subscript  ref h conditional subscript a h subscript s h subscript Q M h subscript s h subscript a h  Z_{h}(s_{h})=\\sum_{a_{h}\\in\\mathcal{A}}\\pi_{\\mathrm{ref},h}(a_{h}|s_{h})\\cdot% \\exp\\big{(}\\frac{Q_{\\mathcal{M},h}(s_{h},a_{h})}{\\eta}\\big{)} italic_Z start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ( italic_s start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) =  start_POSTSUBSCRIPT italic_a start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT  caligraphic_A end_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT roman_ref , italic_h end_POSTSUBSCRIPT ( italic_a start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT | italic_s start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT )  roman_exp ( divide start_ARG italic_Q start_POSTSUBSCRIPT caligraphic_M , italic_h end_POSTSUBSCRIPT ( italic_s start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) end_ARG start_ARG italic_ end_ARG )  is the normalization constant. The second equality in the definition of the  V V V italic_V -value is from Lemma  3 . Then, by definition,  [  M , h ] h = 1 H superscript subscript delimited-[] subscript  M h h 1 H [\\pi_{\\mathcal{M},h}]_{h=1}^{H} [ italic_ start_POSTSUBSCRIPT caligraphic_M , italic_h end_POSTSUBSCRIPT ] start_POSTSUBSCRIPT italic_h = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT  is the optimal policy. Essentially, we solve  H H H italic_H  Gibbs distributions in terms of the  Q Q Q italic_Q -values 2 2 2 The definitions of  Q Q Q italic_Q -values are different from that of  Ziebart ( 2010 )  so that the optimal policy can be interpreted as the Gibbs distribution of  Q Q Q italic_Q -values. .",
            "The above discussions, in particular, M-DPO and M-KTO losses provided in ( 12 ) and ( 13 ), are focused on deterministic observations due to the deterministic nature of tool-integrated LLMs for mathematical reasoning. In contrast, some other applications may encounter stochastic observations, e.g., multi-turn chats with the external message provided by a human or another LLM  (Shani et al.,  2024 ) . In these scenarios, ( 12 ) is biased and cannot lead to the optimal policy since  term   ( C ) = 0 term  C 0 \\text{term }(C)\\neq 0 term ( italic_C ) = 0 . Instead, one should first construct a value network based on the Bellman equations provided in ( 6 ) and ( 7 ), similar to the approach in  Richemond et al. ( 2024 ) . Subsequently,  term   ( C ) term  C \\text{term }(C) term ( italic_C )  can be estimated using Monte-Carlo methods and serve as an adaptive margin in the preference training. Particularly, the distinctions between direct preference learning algorithms and classical deep RL methods become less clear. The exploration of this more complex algorithm and its application to general multi-turn learning scenarios is left for future research.",
            "Policy choice for exploration-exploitation trade-off.  We update our behavior policies in a non-symmetric way. The first agent, which aims to extract the historical information we have gathered so far, planning with respect to the empirically best model on the historical dataset  D D \\mathcal{D} caligraphic_D  to get   t 1 superscript subscript  t 1 \\pi_{t}^{1} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , where the planning algorithms have been discussed in Section  2.2 , e.g., optimizing the M-DPO or M-KTO loss in ( 12 ) or ( 13 ). However, it is widely recognized in RL studies  (Sutton and Barto,  2018 ; Auer et al.,  2002 )  that simply exploiting the historical data via following the empirically best model is not sufficient to obtain a good final policy, while it is also required to explore the environment so that new information can be collected to facilitate subsequent learning, i.e., the exploration-exploitation tradeoff. While the main agent targeting exploitation, we design the second agent, in contrast, to strategically incorporate the uncertainty of the future relative to   t 1 superscript subscript  t 1 \\pi_{t}^{1} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT  given the historical information we collect so far into its policy choice. We call the policy of the second agent   t 2 superscript subscript  t 2 \\pi_{t}^{2} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  as an exploration policy because it serves to explore the underlying environment and facilitate the first agents learning. In practice, this principle of exploration is generally interpreted as maximizing the difference between the two behavior policies or increasing the diversity of the collected data. We summarize some popular heuristic exploration policy adopted in the online iterative RLHF practice:",
            "where  P   (  ) superscript P   \\mathbb{P}^{\\pi}(\\tau) blackboard_P start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ( italic_ )  denotes the probability of trajectory    \\tau italic_  under policy    \\pi italic_  and transition kernel  P P \\mathbb{P} blackboard_P . With the obtained model  M ^ t = ( u ^ t , P ^ t ) subscript ^ M t subscript ^ u t subscript ^ P t \\hat{\\mathcal{M}}_{t}=(\\hat{u}_{t},\\hat{\\mathbb{P}}_{t}) over^ start_ARG caligraphic_M end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = ( over^ start_ARG italic_u end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , over^ start_ARG blackboard_P end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , the Oracle defined in Definition  3  is called with the reference policy   ref subscript  ref \\pi_{\\mathrm{ref}} italic_ start_POSTSUBSCRIPT roman_ref end_POSTSUBSCRIPT  set as the initial policy   0 subscript  0 \\pi_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , whose output is adopted as the main policy   t 1 subscript superscript  1 t \\pi^{1}_{t} italic_ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .",
            "We also observe that the iterative M-DPO and M-KTO surpass other existing RLHF baselines. First, they consistently and significantly outperform the RAFT algorithm across all four base models, which is known to be a robust and competitive baseline in the literature  (Dong et al.,  2023 ; Yuan et al.,  2023a ) . This is because the RAFT algorithm only utilizes the positive signal by imitating the correct trajectories, while the DPO-based and KTO-based algorithms further leverage the negative signal from those incorrect trajectories. We note that the SFT stage in our pipeline can also be viewed as an application of RAFT, an idea that further dates back to expert iteration  (Anthony et al.,  2017 ) . Consequently, our results should be interpreted to be that on the top of the first stage of SFT, algorithms with negative signal are more sample efficient. Moreover, while the online iterative single-turn DPO (KTO)  ( Xiong et al.,  ; Xu et al.,  2023 )  also gives a boost performance, it is generally worse than the multi-turn version. This suggests that learning to predict the off-policy external messages returned by the code interpreter usually has a negative impact on the reasoning ability improvement. Essentially, this corresponds to the fact that when deriving the optimality condition of the KL-regularized optimization problem, we are not allowed to optimize the external messages. Meanwhile, we present a representative example we encounter in Figure  3 , where LLMs generate poorly constructed code resulting in anomalous and lengthy external messages. Forcing LLMs to learn to predict these messages can significantly hurt the models reasoning abilities.",
            "In our experiments, we collected N trajectories per prompt to ensure the presence of both correct and incorrect reasoning paths for constructing the comparison pair. A larger N generally leads to a better coverage of the prompt set because for some difficult problem, we need to sample more responses to find a correct reasoning path. For instance, in iteration 1, with N=30, 92.5% of the prompts are covered, compared to 83.0% for N=12 and 60% for N=6. See Figure  4  for an illustration of the relationship between pass@1 and N. However, increasing N also incurs higher computational costs. To understand the impact of the parameter N, we conduct an ablation study with  N  { 6 , 12 , 30 } N 6 12 30 N\\in\\{6,12,30\\} italic_N  { 6 , 12 , 30 }  and summarize the results in Table  3 . We observe a substantial performance boost when increasing N from 6 to 12, reflecting a better coverage of the complex problems that require more attempts to find a correct path. In contrast, from N=12 to N=30, we only get very minor improvement in the test accuracy, suggesting that the incremental benefits of increasing N in best-of-N sampling diminish rapidly."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Ablation study of the impact of SFT epoch. Mixture sampling is adopted for the iterative M-DPO training and we run for three iterations in total. The gains relative to their starting SFT checkpoints are marked by      \\uparrow  .",
        "table": "A4.EGx1",
        "footnotes": [],
        "references": [
            "Additionally, we note that a stronger KL regularization in the target ( 14 ) is known to be beneficial for mitigating over-fitting issue and forgetting on the  out-of-domain  tasks  (Gao et al.,  2023b ; Lin et al.,  2023 ; Coste et al.,  2023 ) . On the other hand, ( 15 ) allows the model to move more far away, thus achieving a better  in-domain  performance. Thus, from one perspective, the choice between the above two targets can be viewed as a tradeoff between out-of-domain and in-domain performances. This intuition is also verified by later experiments, where optimizing the second target in ( 15 ) leads to better performance on in-domain test sets. In the rest of this section, we discuss two learning objectives to fully develop the multi-turn preference learning framework. We also conduct an ablation study on these objectives in the experimental section.",
            "Reference model choice for controlling regularization level.  Despite two different learning targets are discussed in ( 14 ) and ( 15 ) seperately, we note that one general algorithmic framework can be adopted with the reference model choice taking as a hyper-parameter to control the regularization level and account for the two targets:",
            "KL-regularized target in ( 14 ): if we fix the reference model as the initial policy, i.e.,   t , ref =  0 ,  t  [ T ] formulae-sequence subscript  t ref subscript  0 for-all t delimited-[] T \\pi_{t,\\mathrm{ref}}=\\pi_{0},\\forall t\\in[T] italic_ start_POSTSUBSCRIPT italic_t , roman_ref end_POSTSUBSCRIPT = italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ,  italic_t  [ italic_T ] , we always search the optimal policy within the KL ball centered at   0 subscript  0 \\pi_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , and thus optimize the KL-regularized target.",
            "where   := 1 / ( 2 + exp  (  B ) + exp  ( B ) ) assign  1 2 B B \\kappa:=1/(2+\\exp(-B)+\\exp(B)) italic_ := 1 / ( 2 + roman_exp ( - italic_B ) + roman_exp ( italic_B ) ) ,  c 2 subscript c 2 c_{2} italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  is an absolute constant,  d U subscript d U d_{\\mathcal{U}} italic_d start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT  is the Eluder coefficient defined in Definition  4  while  d P subscript d P d_{\\mathcal{P}} italic_d start_POSTSUBSCRIPT caligraphic_P end_POSTSUBSCRIPT  and    (  )   \\xi(\\cdot) italic_ (  )  are from the generalized Eluder-type condition defined in Definition  5 .",
            "We plot the pass@n accuracy in terms of the number of candidate trajectories n in Figure  4 . To evaluate the pass@n, for each question, we independently sample n trajectories, and the question is considered to be solved if there  exists  at least one trajectory with the correct final answer. We observe that the preference learning only improves the pass@n when n is relatively small. In particular, when  n > 16 n 16 n>16 italic_n > 16 , all the models perform similarly on both GSM8K and MATH. In other words, the iterative M-DPO does not inject new knowledge but elicits the models knowledge acquired in pre-training and SFT stages by boosting the quality of Top n responses. The observation is consistent with that of  Shao et al. ( 2024 ) , which studies the DRL-based GRPO method for the CoT mathematical reasoning task. Therefore, the success of preference learning is on top of a well-trained SFT model. We expect that the final model performance can be further improved with more high-quality SFT data.",
            "In our experiments, we collected N trajectories per prompt to ensure the presence of both correct and incorrect reasoning paths for constructing the comparison pair. A larger N generally leads to a better coverage of the prompt set because for some difficult problem, we need to sample more responses to find a correct reasoning path. For instance, in iteration 1, with N=30, 92.5% of the prompts are covered, compared to 83.0% for N=12 and 60% for N=6. See Figure  4  for an illustration of the relationship between pass@1 and N. However, increasing N also incurs higher computational costs. To understand the impact of the parameter N, we conduct an ablation study with  N  { 6 , 12 , 30 } N 6 12 30 N\\in\\{6,12,30\\} italic_N  { 6 , 12 , 30 }  and summarize the results in Table  3 . We observe a substantial performance boost when increasing N from 6 to 12, reflecting a better coverage of the complex problems that require more attempts to find a correct path. In contrast, from N=12 to N=30, we only get very minor improvement in the test accuracy, suggesting that the incremental benefits of increasing N in best-of-N sampling diminish rapidly.",
            "Tunstall et al. ( 2023 )  finds that if the SFT model is trained for more than one epoch, the subsequent DPO training will lead to performance regression with longer training in terms of instruction-following ability and benchmark for a general chatbot. In other words, there exists a trade-off between the SFT training epochs and the DPO training steps. Moreover, the best model is obtained by SFT for one epoch in their practice. We also conduct an ablation study on the impact of the SFT epoch and summarize the results in Table  4 . Consistently across all tested scenarios, the subsequent iterative M-DPO training leads to considerable model improvement compared to the SFT model. Meanwhile, we also observe a similar trade-off between SFT and RLHF training because with more SFT epochs, the gains from the RLHF stage decrease. However, in our case, the strongest model is obtained with three epochs of SFT, followed by fine-tuning through iterative M-DPO, which is different from the offline DPO training  (Tunstall et al.,  2023 )  or the iterative DPO for general chatbot  (Dong et al.,  2024 )  with only one epoch of SFT.",
            "where the first inequality is from the Hoeffding inequality, the second inequality uses the Eluder coefficient  d U := EC  ( 1 , U  U , T ) assign subscript d U EC 1 U U T d_{\\mathcal{U}}:=\\texttt{EC}(1,\\mathcal{U}-\\mathcal{U},T) italic_d start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT := EC ( 1 , caligraphic_U - caligraphic_U , italic_T )  from Definition  4 , the third inequality leverages the mean value theorem with   := 1 / ( 2 + exp  (  B ) + exp  ( B ) ) assign  1 2 B B \\kappa:=1/(2+\\exp(-B)+\\exp(B)) italic_ := 1 / ( 2 + roman_exp ( - italic_B ) + roman_exp ( italic_B ) )  representing the minimum derivative of    (  )   \\sigma(\\cdot) italic_ (  )  in the regime of  [ 0 , B ] 0 B [0,B] [ 0 , italic_B ] , and the last inequality incorporates Lemma  2 . A similar result can be obtained for  term (B) t subscript term (B) t \\text{term (B)}_{t} term (B) start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Other ablation studies. Mixture sampling is adopted for the iterative M-DPO training and we run for three iterations in total. The gains relative to the iterative M-DPO are marked by      \\uparrow  .",
        "table": "A4.EGx2",
        "footnotes": [],
        "references": [
            "Additionally, we note that a stronger KL regularization in the target ( 14 ) is known to be beneficial for mitigating over-fitting issue and forgetting on the  out-of-domain  tasks  (Gao et al.,  2023b ; Lin et al.,  2023 ; Coste et al.,  2023 ) . On the other hand, ( 15 ) allows the model to move more far away, thus achieving a better  in-domain  performance. Thus, from one perspective, the choice between the above two targets can be viewed as a tradeoff between out-of-domain and in-domain performances. This intuition is also verified by later experiments, where optimizing the second target in ( 15 ) leads to better performance on in-domain test sets. In the rest of this section, we discuss two learning objectives to fully develop the multi-turn preference learning framework. We also conduct an ablation study on these objectives in the experimental section.",
            "Reference model choice for controlling regularization level.  Despite two different learning targets are discussed in ( 14 ) and ( 15 ) seperately, we note that one general algorithmic framework can be adopted with the reference model choice taking as a hyper-parameter to control the regularization level and account for the two targets:",
            "Non-regularized target in ( 15 ): in contrast, inspired by the mirror descent  (Nemirovskij and Yudin,  1983 ) , if we update the reference policy every iteration to be the policy learned in the last iteration, i.e.,   t , ref =  t  1 1 ,  t  [ T ] formulae-sequence subscript  t ref subscript superscript  1 t 1 for-all t delimited-[] T \\pi_{t,\\mathrm{ref}}=\\pi^{1}_{t-1},\\forall t\\in[T] italic_ start_POSTSUBSCRIPT italic_t , roman_ref end_POSTSUBSCRIPT = italic_ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ,  italic_t  [ italic_T ] , the cumulative update can make the model to move away from the original   0 subscript  0 \\pi_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  (while a constraint is made on the per-iteration update magnitude) and we thus optimize the non-regularized target.",
            "where   := 1 / ( 2 + exp  (  B ) + exp  ( B ) ) assign  1 2 B B \\kappa:=1/(2+\\exp(-B)+\\exp(B)) italic_ := 1 / ( 2 + roman_exp ( - italic_B ) + roman_exp ( italic_B ) ) ,  c 2 subscript c 2 c_{2} italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  is an absolute constant,  d U subscript d U d_{\\mathcal{U}} italic_d start_POSTSUBSCRIPT caligraphic_U end_POSTSUBSCRIPT  is the Eluder coefficient defined in Definition  4  while  d P subscript d P d_{\\mathcal{P}} italic_d start_POSTSUBSCRIPT caligraphic_P end_POSTSUBSCRIPT  and    (  )   \\xi(\\cdot) italic_ (  )  are from the generalized Eluder-type condition defined in Definition  5 .",
            "We also find that the strongest model is obtained by a moderate KL coefficient of  0.1 0.1 0.1 0.1 , outperforming both 0.01 and 0.5. To understand this phenomena, we plot the test accuracy of GSM8K in Figure  5  along the way of iterative training. As we can see, for the first iteration, the results align with  Tunstall et al. ( 2023 ) s findings, where a smaller KL coefficient leads to a larger model improvement. However, the resulting intermediate model is further used to collect trajectories for subsequent iterative training. The models trained with very low KL coefficients tend to lose diversity rapidly, potentially reducing their capacity to collect diverse trajectories for subsequent training, leading to diminishing gains in the second and third iterations. In contrast, a higher KL coefficient of 0.5 imposes strong regularization between the resulting model and the reference model, and the model improvement is less compared to that of  0.1 0.1 0.1 0.1  for each iteration. To summarize, for online iterative training, we need to strike a balance between the per-iteration improvement and exploration efficiency to optimize the overall performance. We will see that such an intuition also extends to the choices of sampling strategy choice and other experimental tricks.",
            "Throughout our iterative training process of the Gemma-1.1-it-7B, we observed a steady increase in the percentage of correct trajectoriesfrom 47% in the first iteration to 76% in last iteration. Moreover, since we update the reference model at each iteration, the diversity of the generated trajectories also decrease rapidly. However, the diversity of the collected data is critical for DPO/KTO training due to their contrastive nature. Prior studies in online iterative DPO for general chatbots  (Dong et al.,  2024 )  recommend employing model variants with different sampling temperatures or training steps to enhance trajectory diversity. Motivated by this, we explored two data collection strategies: (1) on-policy sampling, where all trajectories are sampled using the current policy, and (2) mixture sampling, where 20 trajectories are collected using the current model and 10 from the last iterations model. We report the results in Table  5 , where with mixture sampling, the final model performance considerably outperform the one with only on-policy sampling. To understand this phenomena, we plot the MATH test accuracy in terms of the iteration in Figure  6 . We observe that on-policy sampling fails to improve MATH test accuracy in the third iteration, while we achieve considerable gain with the mixture sampling. This again demonstrates the importance of the diversity of the collected responses in the iterative training and also aligns with previous findings that advanced exploration strategies, which prevent diversity collapse, provide more meaningful signals for iterative preference learning  (Bai et al.,  2022 ; Touvron et al.,  2023 ;  Xiong et al.,  ; Pace et al.,  2024 ; Dong et al.,  2024 ) . It would be interested to explore more advanced exploration strategy like Monte Carlo tree search (MCTS) in the future study.",
            "The recent work  Pang et al. ( 2024 )  has introduced iterative RPO, specifically aimed at enhancing Chain of Thought (CoT) capabilities for solving mathematical problems. A key feature of this approach is the inclusion of an additional negative log-likelihood (NLL) loss for the preferred response. The main intuition for adding the NLL loss is that the original DPO algorithm  (Rafailov et al.,  2023 )  tends to reduce the likelihood of the preferred responses, and this is believed to hurt the reasoning ability  (Wang et al.,  2024 ) . Motivated by their results, we explored the applicability of this idea to our setup. We conduct an ablation study by adding the NLL loss into the iterative M-DPO training and observe performance regression as reported in Table  5 . We observe that the best model is obtained in the second iteration if we add the additional NLL loss even though we use the mixture sampling to increase the diversity of the collected data. With time-weighted exponential moving average for smoothing training record, we observe that the log probability of the chosen responses and rejected responses are (-126, -222) at the 200th step of the third iteration training when we add the NLL loss, as compared to (-166, -350) in the case without the NLL loss. This is consistent with the result of  Pang et al. ( 2024 )  where with the additional NLL loss, both the log probability of chosen responses and that of rejected responses increase. These evidences indicate that the NLL loss further contributes to the model distribution collapse and eventually hurt the overall performance of online iterative learning. Finally, we notice that the additional NLL loss can be viewed as an implementation of the pessimistic principle  (Liu et al.,  2024b ) . This also explains its inferior in-domain performance though it may be helpful to stable the training, which requires more in-depth studies.",
            "where the last step is from the generalized Eluder-type condition in Definition  5  and Lemma  2 . A similar result can be obtained for  term (D) t subscript term (D) t \\text{term (D)}_{t} term (D) start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  The table of notations used in this paper.",
        "table": "S2.E7",
        "footnotes": [],
        "references": [
            "Notations.  To improve the readability of this work, we provide a notable table in Table  6 .",
            "where two equalities uses the definition of the optimal policy   M , h subscript  M h \\pi_{\\mathcal{M},h} italic_ start_POSTSUBSCRIPT caligraphic_M , italic_h end_POSTSUBSCRIPT  and  V V V italic_V -value  V M , h subscript V M h V_{\\mathcal{M},h} italic_V start_POSTSUBSCRIPT caligraphic_M , italic_h end_POSTSUBSCRIPT  in ( 7 ), respectively. Furthermore, by the definition of  Q Q Q italic_Q -values  Q M , h subscript Q M h Q_{\\mathcal{M},h} italic_Q start_POSTSUBSCRIPT caligraphic_M , italic_h end_POSTSUBSCRIPT  in ( 6 ), we have",
            "The above discussions, in particular, M-DPO and M-KTO losses provided in ( 12 ) and ( 13 ), are focused on deterministic observations due to the deterministic nature of tool-integrated LLMs for mathematical reasoning. In contrast, some other applications may encounter stochastic observations, e.g., multi-turn chats with the external message provided by a human or another LLM  (Shani et al.,  2024 ) . In these scenarios, ( 12 ) is biased and cannot lead to the optimal policy since  term   ( C ) = 0 term  C 0 \\text{term }(C)\\neq 0 term ( italic_C ) = 0 . Instead, one should first construct a value network based on the Bellman equations provided in ( 6 ) and ( 7 ), similar to the approach in  Richemond et al. ( 2024 ) . Subsequently,  term   ( C ) term  C \\text{term }(C) term ( italic_C )  can be estimated using Monte-Carlo methods and serve as an adaptive margin in the preference training. Particularly, the distinctions between direct preference learning algorithms and classical deep RL methods become less clear. The exploration of this more complex algorithm and its application to general multi-turn learning scenarios is left for future research.",
            "Throughout our iterative training process of the Gemma-1.1-it-7B, we observed a steady increase in the percentage of correct trajectoriesfrom 47% in the first iteration to 76% in last iteration. Moreover, since we update the reference model at each iteration, the diversity of the generated trajectories also decrease rapidly. However, the diversity of the collected data is critical for DPO/KTO training due to their contrastive nature. Prior studies in online iterative DPO for general chatbots  (Dong et al.,  2024 )  recommend employing model variants with different sampling temperatures or training steps to enhance trajectory diversity. Motivated by this, we explored two data collection strategies: (1) on-policy sampling, where all trajectories are sampled using the current policy, and (2) mixture sampling, where 20 trajectories are collected using the current model and 10 from the last iterations model. We report the results in Table  5 , where with mixture sampling, the final model performance considerably outperform the one with only on-policy sampling. To understand this phenomena, we plot the MATH test accuracy in terms of the iteration in Figure  6 . We observe that on-policy sampling fails to improve MATH test accuracy in the third iteration, while we achieve considerable gain with the mixture sampling. This again demonstrates the importance of the diversity of the collected responses in the iterative training and also aligns with previous findings that advanced exploration strategies, which prevent diversity collapse, provide more meaningful signals for iterative preference learning  (Bai et al.,  2022 ; Touvron et al.,  2023 ;  Xiong et al.,  ; Pace et al.,  2024 ; Dong et al.,  2024 ) . It would be interested to explore more advanced exploration strategy like Monte Carlo tree search (MCTS) in the future study."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "S2.E8",
        "footnotes": [],
        "references": [
            "While ( 7 ) can be approximately solved with standard deep RL methods, here we are interested in the implementation in a direct preference learning manner like SLiC  (Zhao et al.,  2023 ) , DPO  (Rafailov et al.,  2023 )  or IPO  (Azar et al.,  2023 ) . The existing attempts  (e.g., Yuan et al.,  2024 )  take the completion  y y y italic_y  as a meta action and plug it into the single-step DPO loss. In other words, they treat the external messages as the regular texts generated by the model itself. Another natural idea is to plug the probability of the trajectory into the single-step DPO loss. To be specific, for a pair  ( x ,  w ,  l ) x superscript  w superscript  l (x,\\tau^{w},\\tau^{l}) ( italic_x , italic_ start_POSTSUPERSCRIPT italic_w end_POSTSUPERSCRIPT , italic_ start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ) , where   w superscript  w \\tau^{w} italic_ start_POSTSUPERSCRIPT italic_w end_POSTSUPERSCRIPT  refers to the preferred (i.e., winning) trajectory, we have",
            "where two equalities uses the definition of the optimal policy   M , h subscript  M h \\pi_{\\mathcal{M},h} italic_ start_POSTSUBSCRIPT caligraphic_M , italic_h end_POSTSUBSCRIPT  and  V V V italic_V -value  V M , h subscript V M h V_{\\mathcal{M},h} italic_V start_POSTSUBSCRIPT caligraphic_M , italic_h end_POSTSUBSCRIPT  in ( 7 ), respectively. Furthermore, by the definition of  Q Q Q italic_Q -values  Q M , h subscript Q M h Q_{\\mathcal{M},h} italic_Q start_POSTSUBSCRIPT caligraphic_M , italic_h end_POSTSUBSCRIPT  in ( 6 ), we have",
            "The above discussions, in particular, M-DPO and M-KTO losses provided in ( 12 ) and ( 13 ), are focused on deterministic observations due to the deterministic nature of tool-integrated LLMs for mathematical reasoning. In contrast, some other applications may encounter stochastic observations, e.g., multi-turn chats with the external message provided by a human or another LLM  (Shani et al.,  2024 ) . In these scenarios, ( 12 ) is biased and cannot lead to the optimal policy since  term   ( C ) = 0 term  C 0 \\text{term }(C)\\neq 0 term ( italic_C ) = 0 . Instead, one should first construct a value network based on the Bellman equations provided in ( 6 ) and ( 7 ), similar to the approach in  Richemond et al. ( 2024 ) . Subsequently,  term   ( C ) term  C \\text{term }(C) term ( italic_C )  can be estimated using Monte-Carlo methods and serve as an adaptive margin in the preference training. Particularly, the distinctions between direct preference learning algorithms and classical deep RL methods become less clear. The exploration of this more complex algorithm and its application to general multi-turn learning scenarios is left for future research.",
            "We present a general online iterative algorithmic framework in Algorithm  1 . This framework is termed as  Online Iterative Multi-turn Gibbs Sampling from Human Feedback (M-GSHF)  to highlight the online iterative training process and the optimal condition derived in ( 7 ) that the optimal policy is a layer-wise Gibbs distribution, which generalizes the bandit formulation in  Xiong et al.  . Specifically, starting from   0 subscript  0 \\pi_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , at each iteration, we first collect a pair of trajectories by the current policy pair, where the preference signal is also revealed according to Definition  1 . Then, we update our policy pair given the data collected so far and the next iteration begins. We now discuss some features of the framework as follows.",
            "For any model  M = ( S , A , H , P , d 0 , u ) M S A H P subscript d 0 u \\mathcal{M}=({\\mathcal{S}},\\mathcal{A},H,\\mathbb{P},d_{0},u) caligraphic_M = ( caligraphic_S , caligraphic_A , italic_H , blackboard_P , italic_d start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_u )  and a reference function   ref subscript  ref \\pi_{\\mathrm{ref}} italic_ start_POSTSUBSCRIPT roman_ref end_POSTSUBSCRIPT , we can compute the optimal policy associated with the model  [  M , h ] h = 1 H superscript subscript delimited-[] subscript  M h h 1 H [\\pi_{\\mathcal{M},h}]_{h=1}^{H} [ italic_ start_POSTSUBSCRIPT caligraphic_M , italic_h end_POSTSUBSCRIPT ] start_POSTSUBSCRIPT italic_h = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT  iteratively as in ( 7 )."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "S2.E9",
        "footnotes": [],
        "references": [
            "We emphasize again that although the loss presented in ( 12 ) is identical to the one in ( 8 ), a rigorous derivation procedure (rather than a direct plug-in) is provided. To the best of our knowledge, ( 12 ) is new in the context of multi-turn reasoning task with external messages. In particular, it is noted that such a M-DPO loss is only valid upon deterministic transitions, i.e., term  ( C ) = 0 C 0 (C)=0 ( italic_C ) = 0 ."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "S2.E10",
        "footnotes": [],
        "references": []
    },
    "id_table_10": {
        "caption": "",
        "table": "S2.E11",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "",
        "table": "S2.Ex10",
        "footnotes": [],
        "references": [
            "Further advancements include adapting direct preference learning algorithms to mathematical problem solving. For instance,  Jiao et al. ( 2024 ); Yuan et al. ( 2024 )  have applied the original DPO or KTO by taking the trajectory completion as a meta action.  Xie et al. ( 2024b ); Pang et al. ( 2024 )  further adapt the online iterative DPO originally designed for chat  ( Xiong et al.,  ; Xu et al.,  2023 ; Hoang Tran,  2024 )  and achieve better performance for CoT reasoning. Inspired by the success of PRMs, recent studies have explored generating proxy step-wise labels for the intermediate steps of the reasoning trajectories. For instance,  Xie et al. ( 2024b ); Chen et al. ( 2024a ); Lai et al. ( 2024 )  leverage Monte Carlo Tree Search (MCTS) and use the estimated Q value to generate the proxy labels for the intermediate steps.  Lai et al. ( 2024 )  proposes to use AI feedback like GPT-4  (Lai et al.,  2024 )  to find the first error step in the trajectory. Meanwhile,  Lu et al. ( 2024 )  identifies a trajectory with the correct final answer and no errors as preferable, and prompts the SFT model with a high temperature, starting from some intermediate step to collect a rejected trajectory with errors  (Pi et al.,  2024 ) . Finally, a very recent study by  Chen et al. ( 2024a )  proposes to use MCTS with a backward iteration from the final leaf node to compute the proxy unregularized value of each node. Preference pairs are then extracted from the tree by fixing the prefix and comparing  the next single reasoning step . Then, they run the original DPO on these intermediate actions with the proxy labels from MCTS. To summarize, these works present different ways of preference data collection and apply the original DPO algorithm (with some additional marginal loss and regularization adapted from the literature), thereby differing from our work in both algorithmic concepts and application scope. In contrast, we study preference learning in the context of trajectory-level comparison, where we derive the optimality condition and introduce a multi-turn DPO within an online iterative framework, specifically for tool-integrated mathematical problem solving. However, we remark that while we focus on the trajectory-level comparison, the preference signal itself can be generated in a step-by-step supervision (see Section  1.1  for the detailed examples). When preference signals for partial trajectories with shared prefixes are available, our method can also adapt to learn these step-level signals (see the optimality condition in ( 11 )). In particular, the algorithmic design presented in this paper can be readily combined with the MCTS-based data collection strategy outlined in recent literature, which we leave for future work.",
            "Moreover, with ( 11 ) implying that with term  ( C ) = 0 C 0 (C)=0 ( italic_C ) = 0 , the implicit reward is given by  A =    h = 1 H log   h   ( a h | s h )  ref , h  ( a h | s h ) A  superscript subscript h 1 H subscript superscript  h conditional subscript a h subscript s h subscript  ref h conditional subscript a h subscript s h A=\\eta\\sum_{h=1}^{H}\\log\\frac{\\pi^{*}_{h}(a_{h}|s_{h})}{\\pi_{\\mathrm{ref},h}(a% _{h}|s_{h})} italic_A = italic_  start_POSTSUBSCRIPT italic_h = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT roman_log divide start_ARG italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ( italic_a start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT | italic_s start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) end_ARG start_ARG italic_ start_POSTSUBSCRIPT roman_ref , italic_h end_POSTSUBSCRIPT ( italic_a start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT | italic_s start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) end_ARG , a multi-turn version of KTO  (Ethayarajh et al.,  2024 ) , denoted as M-KTO, can also be naturally derived:"
        ]
    },
    "id_table_12": {
        "caption": "",
        "table": "A4.EGx3",
        "footnotes": [],
        "references": [
            "We emphasize again that although the loss presented in ( 12 ) is identical to the one in ( 8 ), a rigorous derivation procedure (rather than a direct plug-in) is provided. To the best of our knowledge, ( 12 ) is new in the context of multi-turn reasoning task with external messages. In particular, it is noted that such a M-DPO loss is only valid upon deterministic transitions, i.e., term  ( C ) = 0 C 0 (C)=0 ( italic_C ) = 0 .",
            "The above discussions, in particular, M-DPO and M-KTO losses provided in ( 12 ) and ( 13 ), are focused on deterministic observations due to the deterministic nature of tool-integrated LLMs for mathematical reasoning. In contrast, some other applications may encounter stochastic observations, e.g., multi-turn chats with the external message provided by a human or another LLM  (Shani et al.,  2024 ) . In these scenarios, ( 12 ) is biased and cannot lead to the optimal policy since  term   ( C ) = 0 term  C 0 \\text{term }(C)\\neq 0 term ( italic_C ) = 0 . Instead, one should first construct a value network based on the Bellman equations provided in ( 6 ) and ( 7 ), similar to the approach in  Richemond et al. ( 2024 ) . Subsequently,  term   ( C ) term  C \\text{term }(C) term ( italic_C )  can be estimated using Monte-Carlo methods and serve as an adaptive margin in the preference training. Particularly, the distinctions between direct preference learning algorithms and classical deep RL methods become less clear. The exploration of this more complex algorithm and its application to general multi-turn learning scenarios is left for future research.",
            "Policy choice for exploration-exploitation trade-off.  We update our behavior policies in a non-symmetric way. The first agent, which aims to extract the historical information we have gathered so far, planning with respect to the empirically best model on the historical dataset  D D \\mathcal{D} caligraphic_D  to get   t 1 superscript subscript  t 1 \\pi_{t}^{1} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , where the planning algorithms have been discussed in Section  2.2 , e.g., optimizing the M-DPO or M-KTO loss in ( 12 ) or ( 13 ). However, it is widely recognized in RL studies  (Sutton and Barto,  2018 ; Auer et al.,  2002 )  that simply exploiting the historical data via following the empirically best model is not sufficient to obtain a good final policy, while it is also required to explore the environment so that new information can be collected to facilitate subsequent learning, i.e., the exploration-exploitation tradeoff. While the main agent targeting exploitation, we design the second agent, in contrast, to strategically incorporate the uncertainty of the future relative to   t 1 superscript subscript  t 1 \\pi_{t}^{1} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT  given the historical information we collect so far into its policy choice. We call the policy of the second agent   t 2 superscript subscript  t 2 \\pi_{t}^{2} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  as an exploration policy because it serves to explore the underlying environment and facilitate the first agents learning. In practice, this principle of exploration is generally interpreted as maximizing the difference between the two behavior policies or increasing the diversity of the collected data. We summarize some popular heuristic exploration policy adopted in the online iterative RLHF practice:"
        ]
    },
    "id_table_13": {
        "caption": "",
        "table": "A4.EGx4",
        "footnotes": [],
        "references": [
            "The above discussions, in particular, M-DPO and M-KTO losses provided in ( 12 ) and ( 13 ), are focused on deterministic observations due to the deterministic nature of tool-integrated LLMs for mathematical reasoning. In contrast, some other applications may encounter stochastic observations, e.g., multi-turn chats with the external message provided by a human or another LLM  (Shani et al.,  2024 ) . In these scenarios, ( 12 ) is biased and cannot lead to the optimal policy since  term   ( C ) = 0 term  C 0 \\text{term }(C)\\neq 0 term ( italic_C ) = 0 . Instead, one should first construct a value network based on the Bellman equations provided in ( 6 ) and ( 7 ), similar to the approach in  Richemond et al. ( 2024 ) . Subsequently,  term   ( C ) term  C \\text{term }(C) term ( italic_C )  can be estimated using Monte-Carlo methods and serve as an adaptive margin in the preference training. Particularly, the distinctions between direct preference learning algorithms and classical deep RL methods become less clear. The exploration of this more complex algorithm and its application to general multi-turn learning scenarios is left for future research.",
            "Policy choice for exploration-exploitation trade-off.  We update our behavior policies in a non-symmetric way. The first agent, which aims to extract the historical information we have gathered so far, planning with respect to the empirically best model on the historical dataset  D D \\mathcal{D} caligraphic_D  to get   t 1 superscript subscript  t 1 \\pi_{t}^{1} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , where the planning algorithms have been discussed in Section  2.2 , e.g., optimizing the M-DPO or M-KTO loss in ( 12 ) or ( 13 ). However, it is widely recognized in RL studies  (Sutton and Barto,  2018 ; Auer et al.,  2002 )  that simply exploiting the historical data via following the empirically best model is not sufficient to obtain a good final policy, while it is also required to explore the environment so that new information can be collected to facilitate subsequent learning, i.e., the exploration-exploitation tradeoff. While the main agent targeting exploitation, we design the second agent, in contrast, to strategically incorporate the uncertainty of the future relative to   t 1 superscript subscript  t 1 \\pi_{t}^{1} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT  given the historical information we collect so far into its policy choice. We call the policy of the second agent   t 2 superscript subscript  t 2 \\pi_{t}^{2} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  as an exploration policy because it serves to explore the underlying environment and facilitate the first agents learning. In practice, this principle of exploration is generally interpreted as maximizing the difference between the two behavior policies or increasing the diversity of the collected data. We summarize some popular heuristic exploration policy adopted in the online iterative RLHF practice:"
        ]
    },
    "id_table_14": {
        "caption": "",
        "table": "A4.EGx5",
        "footnotes": [],
        "references": [
            "Additionally, we note that a stronger KL regularization in the target ( 14 ) is known to be beneficial for mitigating over-fitting issue and forgetting on the  out-of-domain  tasks  (Gao et al.,  2023b ; Lin et al.,  2023 ; Coste et al.,  2023 ) . On the other hand, ( 15 ) allows the model to move more far away, thus achieving a better  in-domain  performance. Thus, from one perspective, the choice between the above two targets can be viewed as a tradeoff between out-of-domain and in-domain performances. This intuition is also verified by later experiments, where optimizing the second target in ( 15 ) leads to better performance on in-domain test sets. In the rest of this section, we discuss two learning objectives to fully develop the multi-turn preference learning framework. We also conduct an ablation study on these objectives in the experimental section.",
            "Reference model choice for controlling regularization level.  Despite two different learning targets are discussed in ( 14 ) and ( 15 ) seperately, we note that one general algorithmic framework can be adopted with the reference model choice taking as a hyper-parameter to control the regularization level and account for the two targets:",
            "KL-regularized target in ( 14 ): if we fix the reference model as the initial policy, i.e.,   t , ref =  0 ,  t  [ T ] formulae-sequence subscript  t ref subscript  0 for-all t delimited-[] T \\pi_{t,\\mathrm{ref}}=\\pi_{0},\\forall t\\in[T] italic_ start_POSTSUBSCRIPT italic_t , roman_ref end_POSTSUBSCRIPT = italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ,  italic_t  [ italic_T ] , we always search the optimal policy within the KL ball centered at   0 subscript  0 \\pi_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , and thus optimize the KL-regularized target."
        ]
    },
    "id_table_15": {
        "caption": "",
        "table": "A4.EGx6",
        "footnotes": [],
        "references": [
            "Additionally, we note that a stronger KL regularization in the target ( 14 ) is known to be beneficial for mitigating over-fitting issue and forgetting on the  out-of-domain  tasks  (Gao et al.,  2023b ; Lin et al.,  2023 ; Coste et al.,  2023 ) . On the other hand, ( 15 ) allows the model to move more far away, thus achieving a better  in-domain  performance. Thus, from one perspective, the choice between the above two targets can be viewed as a tradeoff between out-of-domain and in-domain performances. This intuition is also verified by later experiments, where optimizing the second target in ( 15 ) leads to better performance on in-domain test sets. In the rest of this section, we discuss two learning objectives to fully develop the multi-turn preference learning framework. We also conduct an ablation study on these objectives in the experimental section.",
            "Reference model choice for controlling regularization level.  Despite two different learning targets are discussed in ( 14 ) and ( 15 ) seperately, we note that one general algorithmic framework can be adopted with the reference model choice taking as a hyper-parameter to control the regularization level and account for the two targets:",
            "Non-regularized target in ( 15 ): in contrast, inspired by the mirror descent  (Nemirovskij and Yudin,  1983 ) , if we update the reference policy every iteration to be the policy learned in the last iteration, i.e.,   t , ref =  t  1 1 ,  t  [ T ] formulae-sequence subscript  t ref subscript superscript  1 t 1 for-all t delimited-[] T \\pi_{t,\\mathrm{ref}}=\\pi^{1}_{t-1},\\forall t\\in[T] italic_ start_POSTSUBSCRIPT italic_t , roman_ref end_POSTSUBSCRIPT = italic_ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ,  italic_t  [ italic_T ] , the cumulative update can make the model to move away from the original   0 subscript  0 \\pi_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  (while a constraint is made on the per-iteration update magnitude) and we thus optimize the non-regularized target."
        ]
    },
    "id_table_16": {
        "caption": "",
        "table": "S2.E21",
        "footnotes": [],
        "references": []
    },
    "id_table_17": {
        "caption": "",
        "table": "A4.EGx7",
        "footnotes": [],
        "references": []
    },
    "id_table_18": {
        "caption": "",
        "table": "S3.T1.32",
        "footnotes": [],
        "references": []
    },
    "id_table_19": {
        "caption": "",
        "table": "S3.T2.4",
        "footnotes": [],
        "references": []
    },
    "id_table_20": {
        "caption": "",
        "table": "S3.T3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_21": {
        "caption": "",
        "table": "S3.T4.8",
        "footnotes": [],
        "references": []
    },
    "id_table_22": {
        "caption": "",
        "table": "S3.T5.6",
        "footnotes": [],
        "references": []
    },
    "id_table_23": {
        "caption": "",
        "table": "S3.Ex20",
        "footnotes": [],
        "references": []
    },
    "id_table_24": {
        "caption": "",
        "table": "A1.T6.63",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": []
    },
    "id_table_25": {
        "caption": "",
        "table": "A4.EGx8",
        "footnotes": [],
        "references": []
    },
    "id_table_26": {
        "caption": "",
        "table": "A4.EGx9",
        "footnotes": [],
        "references": []
    },
    "id_table_27": {
        "caption": "",
        "table": "A4.EGx10",
        "footnotes": [],
        "references": []
    },
    "id_table_28": {
        "caption": "",
        "table": "A4.EGx11",
        "footnotes": [],
        "references": []
    },
    "id_table_29": {
        "caption": "",
        "table": "A4.EGx12",
        "footnotes": [],
        "references": []
    },
    "id_table_30": {
        "caption": "",
        "table": "A4.EGx13",
        "footnotes": [],
        "references": []
    },
    "id_table_31": {
        "caption": "",
        "table": "A4.EGx14",
        "footnotes": [],
        "references": []
    },
    "id_table_32": {
        "caption": "",
        "table": "A4.EGx15",
        "footnotes": [],
        "references": []
    },
    "id_table_33": {
        "caption": "",
        "table": "A4.EGx16",
        "footnotes": [],
        "references": []
    },
    "id_table_34": {
        "caption": "",
        "table": "A4.EGx17",
        "footnotes": [],
        "references": []
    },
    "id_table_35": {
        "caption": "",
        "table": "A4.EGx18",
        "footnotes": [],
        "references": []
    },
    "id_table_36": {
        "caption": "",
        "table": "A4.EGx19",
        "footnotes": [],
        "references": []
    },
    "id_table_37": {
        "caption": "",
        "table": "A4.EGx20",
        "footnotes": [],
        "references": []
    },
    "id_table_38": {
        "caption": "",
        "table": "A4.EGx21",
        "footnotes": [],
        "references": []
    },
    "id_table_39": {
        "caption": "",
        "table": "A4.EGx22",
        "footnotes": [],
        "references": []
    },
    "id_table_40": {
        "caption": "",
        "table": "A4.EGx23",
        "footnotes": [],
        "references": []
    },
    "id_table_41": {
        "caption": "",
        "table": "A4.EGx24",
        "footnotes": [],
        "references": []
    },
    "id_table_42": {
        "caption": "",
        "table": "A4.EGx25",
        "footnotes": [],
        "references": []
    },
    "id_table_43": {
        "caption": "",
        "table": "A4.EGx26",
        "footnotes": [],
        "references": []
    },
    "id_table_44": {
        "caption": "",
        "table": "A4.EGx27",
        "footnotes": [],
        "references": []
    },
    "id_table_45": {
        "caption": "",
        "table": "A4.EGx28",
        "footnotes": [],
        "references": []
    },
    "id_table_46": {
        "caption": "",
        "table": "A4.EGx29",
        "footnotes": [],
        "references": []
    },
    "id_table_47": {
        "caption": "",
        "table": "A4.EGx30",
        "footnotes": [],
        "references": []
    },
    "id_table_48": {
        "caption": "",
        "table": "A4.EGx31",
        "footnotes": [],
        "references": []
    }
}