{
    "id_table_1": {
        "caption": "Table 1:  Comparison of Different RAG Libraries and Frameworks. Fair Comparison refers to aligning all fundamental components during evaluation, including random seeds, generator, retriever, and instructions. Data Collector refers to the ability to gather or generate training and test data, either by sampling from existing raw datasets or by constructing labeled data using LLMs.",
        "table": "S1.T1.1",
        "footnotes": [],
        "references": [
            "While various current works are investigating these questions, such as LlamaIndex  (Liu,  2022 ) , LangChain (Chase,  2022 ) , Haystack (Pietsch et al.,  2019 ) , FastRAG (Izsak et al.,  2023 ) , RALLE  (Hoshi et al.,  2023 ) , LocalRQA (Yu et al.,  2024 ) , AutoRAG (Jeffrey Kim,  2024 ) , and FlashRAG (Jin et al.,  2024 ) .  LlamaIndex, LangChain, and Haystack are excessively encapsulation and lack transparency in internal operational mechanisms. Consequently, even experienced experts abandon tools like LangChain due to the lack of transparency (Woolf,  2023 ) . FastRAG and RALLE offer light and transparent frameworks that enable users to assemble their own RAG systems using core components. AutoRAG provides comprehensive metrics to assist users in selecting an optimal RAG system for customized data. LocalRAG provides a wide selection of model training algorithms and evaluation methods. However, LocalRAG, FastRAG, AutoRAG, and RALLE do not reproduce published algorithms. Researchers still need to invest time in replicating algorithms using the provided components. FlashRAG addressed this issue by reproducing a substantial number of existing algorithms. However, FlashRAG lacks training functionalities and fails to properly align generators during inference, leading to unfair comparisons among various algorithms. For a more detailed comparison, refer to Table  1 .",
            "The overall architecture of RAGLAB is illustrated in Figure  1 . We first introduce the core classes and concepts, then demonstrate an experimental case using a concise 5-line code snippet."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Tasks and Datasets.",
        "table": "S2.T2.1",
        "footnotes": [],
        "references": [
            "In this paper, we introduce the RAGLAB framework, giving an overview of core components and system workflows(section  2 ).  We standardized key experimental variables: generator fine-tuning, instructions, retrieval configurations, knowledge bases, and benchmark. As a result, we present a comprehensive and fair comparison of 6 RAG algorithms across 10 benchmarks(section  3 ).",
            "As shown in Table  2 , following a comprehensive investigation, RAGLAB collects 10 widely used benchmarks encompassing five distinct tasks.",
            "Algorithms inheriting from NaiveRAG can reuse the  inference()  method and all utility functions. Notably, the  inference()  method already provides automatic evaluation and interaction functionalities. This design enables researchers to focus solely on designing the  infer()  method to develop new algorithms. Section  2.3  will provide a detailed explanation of how to utilize the developed algorithm with just five lines of code.",
            "RAGLAB provides a user-friendly interface, allowing users to reproduce RAG algorithms for interaction or evaluation with just five lines of code. In Figure  2 , we present an example script for reproducing the Self-RAG algorithm in both interaction and evaluation modes.  The implementation process is as follows: (1) The  get_config()  function reads parameters from a YAML file and defines the args object. (2) The  SelfRag_Reproduction  class is defined to prepare all settings for the Self-RAG algorithm, based on the args parameters. (3) The  inference()  method in line 9 is called for the interaction mode. (4)  The  inference()  method in line 12 is called again for the evaluation mode.",
            "Additional Experimental Settings :  We employ ColBERT as the retriever, utilizing Wikipedia 2018 as the external knowledge database. Local models are loaded with float16 precision, and during inference, we fix the random seed and use greedy sampling. The number of retrieved passages and the maximum generation length vary for each benchmark, please refer to Appendix  B .  We strive to maintain consistent instructions across all algorithms. For specific instructions and parameter settings, please refer to Appendix  E  and   D , respectively.  We select 10 comprehensive benchmarks for evaluation experiments, as detailed in Table  2 .  Due to limited computation resources, we sequentially sampled 500 data points from each dataset. For evaluation, we employ a range of metrics, including Factscore, ACLE, accuracy (ACC), and F1 score across different datasets.  The task-specific instructions for each dataset are detailed in Appendix  F .  The results of Experiments  5 ,  5 , and  5  are presented in Tables  3 ,  4 , and  5 , respectively."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Performance comparison of various RAG algorithms using Llama3-8B as base language model.",
        "table": "S2.T3.1",
        "footnotes": [],
        "references": [
            "In this paper, we introduce the RAGLAB framework, giving an overview of core components and system workflows(section  2 ).  We standardized key experimental variables: generator fine-tuning, instructions, retrieval configurations, knowledge bases, and benchmark. As a result, we present a comprehensive and fair comparison of 6 RAG algorithms across 10 benchmarks(section  3 ).",
            "The design philosophy of RAGALB draws inspiration from the HuggingFace Transformer library. Users only need to define their model from the Transformer library, after which they can employ the  generate()  method for inference. RAGALB implements each RAG algorithm as a distinct class. Two critical methods in each algorithm class are  init()  and  infer() . The  init()  method serves to set parameters and load Generators, while the  infer()  method implements the algorithms inference process.  Based on this design framework, users can develop new algorithms through a few simple steps, as shown in Figure  3 :  (1) Define a  NewMethod()  class that inherits from  NaiveRAG .  (2) Add necessary parameters and components for the new algorithm by overriding the  init()  method.  (3) Implement the new algorithms inference logic by overriding the  infer()  method, utilizing the frameworks provided components.",
            "Algorithms inheriting from NaiveRAG can reuse the  inference()  method and all utility functions. Notably, the  inference()  method already provides automatic evaluation and interaction functionalities. This design enables researchers to focus solely on designing the  infer()  method to develop new algorithms. Section  2.3  will provide a detailed explanation of how to utilize the developed algorithm with just five lines of code.",
            "Additional Experimental Settings :  We employ ColBERT as the retriever, utilizing Wikipedia 2018 as the external knowledge database. Local models are loaded with float16 precision, and during inference, we fix the random seed and use greedy sampling. The number of retrieved passages and the maximum generation length vary for each benchmark, please refer to Appendix  B .  We strive to maintain consistent instructions across all algorithms. For specific instructions and parameter settings, please refer to Appendix  E  and   D , respectively.  We select 10 comprehensive benchmarks for evaluation experiments, as detailed in Table  2 .  Due to limited computation resources, we sequentially sampled 500 data points from each dataset. For evaluation, we employ a range of metrics, including Factscore, ACLE, accuracy (ACC), and F1 score across different datasets.  The task-specific instructions for each dataset are detailed in Appendix  F .  The results of Experiments  5 ,  5 , and  5  are presented in Tables  3 ,  4 , and  5 , respectively."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Performance comparison of various RAG algorithms using Llama3-70B as base language model.",
        "table": "S3.T4.1",
        "footnotes": [],
        "references": [
            "Additional Experimental Settings :  We employ ColBERT as the retriever, utilizing Wikipedia 2018 as the external knowledge database. Local models are loaded with float16 precision, and during inference, we fix the random seed and use greedy sampling. The number of retrieved passages and the maximum generation length vary for each benchmark, please refer to Appendix  B .  We strive to maintain consistent instructions across all algorithms. For specific instructions and parameter settings, please refer to Appendix  E  and   D , respectively.  We select 10 comprehensive benchmarks for evaluation experiments, as detailed in Table  2 .  Due to limited computation resources, we sequentially sampled 500 data points from each dataset. For evaluation, we employ a range of metrics, including Factscore, ACLE, accuracy (ACC), and F1 score across different datasets.  The task-specific instructions for each dataset are detailed in Appendix  F .  The results of Experiments  5 ,  5 , and  5  are presented in Tables  3 ,  4 , and  5 , respectively."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Performance comparison of various RAG algorithms using GPT3.5 as base language model.",
        "table": "S3.T5.1",
        "footnotes": [],
        "references": [
            "Experimental 1 Generator : In Experiment  5  , we select Llama3-8B as the base language model.  We utilize the open-source data provided by Self-RAG as training data.  The resulting fine-tuned model is designated as selfrag-llama3-8B, which serves as the generator for the Self-RAG algorithm. To ensure a fair comparison, we removed all special tokens from the training data, then full-weighted fine-tuned another model named Llama3-8B-baseline as the generator for other algorithms. For detailed training parameters, please refer to Appendix  A .",
            "Experimental 2 Generator :  In Experiment  5 , we selected Llama3-70B as the base language model. We select the QLoRA (Dettmers et al.,  2023 )  method to fine-tune selfrag-llama3-70B and Llama3-70B-baseline. We use the same training data as in Experiment  5 . For detailed training parameters, please refer to Appendix  C .",
            "Experimental 3 Generator :  In Experiment  5 , we selected GPT3.5 as base model. Additionally, we excluded the Self-RAG algorithm. Because closed-source models are not allowed to add special tokens during the training phase.",
            "Additional Experimental Settings :  We employ ColBERT as the retriever, utilizing Wikipedia 2018 as the external knowledge database. Local models are loaded with float16 precision, and during inference, we fix the random seed and use greedy sampling. The number of retrieved passages and the maximum generation length vary for each benchmark, please refer to Appendix  B .  We strive to maintain consistent instructions across all algorithms. For specific instructions and parameter settings, please refer to Appendix  E  and   D , respectively.  We select 10 comprehensive benchmarks for evaluation experiments, as detailed in Table  2 .  Due to limited computation resources, we sequentially sampled 500 data points from each dataset. For evaluation, we employ a range of metrics, including Factscore, ACLE, accuracy (ACC), and F1 score across different datasets.  The task-specific instructions for each dataset are detailed in Appendix  F .  The results of Experiments  5 ,  5 , and  5  are presented in Tables  3 ,  4 , and  5 , respectively.",
            "After analyzing the results from Experiments  5 , Experiments  5 , and Experiments  5 , we find several valuable insights.  When utilizing selfrag-llama3-8B as the generator for the Self-RAG algorithm, its performance across 10 benchmarks did not significantly surpass other RAG algorithms. However, when employing selfrag-llama3-70B as the generator, the Self-RAG algorithm significantly outperformed others in 10 benchmarks.  We also find that Naive RAG, RRR, Iter-RETGEN, and Active RAG demonstrate comparable performance across 10 datasets. Notably, the ITER-RETGEN algorithm exhibits superior performance in Multi-HopQA tasks.  Furthermore, our findings indicate that RAG systems underperform compared to direct LLMs in multiple-choice question tasks. This conclusion aligns with experimental results from other studies (Chan et al.,  2024 ; Asai et al.,  2024 ; Wang et al.,  2024 ) .  A possible explanation for this phenomenon is that multiple-choice questions include both the question and candidate answers. Additional retrieved information may mislead the generator."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Training Parameters for Llama3-8B.",
        "table": "A1.T6.1",
        "footnotes": [],
        "references": [
            "To comprehensively evaluate the user experience of the RAGLAB library, we implemented a user study. We developed a questionnaire comprising 12 questions, as shown in Figure  6 . The study participants consisted of 20 NLP researchers, each having utilized RAGLAB at least three days. The questionnaire was administered offline, achieving a  100%  response rate. The results indicated that  85%  of respondents perceived RAGLAB as significantly enhancing their research efficiency, and  90%  expressed willingness to recommend RABLAB to other researchers. Additionally, we gathered valuable suggestions for improvement, which will guide future system development.",
            "This appendix outlines the key training parameters used for fine-tuning the Llama3-8B model in our experiments. We employed full-weight fine-tuning on the Llama3-8B base model. The maximum sequence length was set to 4096 tokens, with a learning rate of 2e-5 and training conducted for 1 epoch. For a comprehensive list of training parameters, including computational resources and optimization settings, please refer to Table  6 ."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Inference Parameters for Different Benchmarks.",
        "table": "A2.T7.1",
        "footnotes": [],
        "references": [
            "The number of retrieved passages and the maximum generation length were adjusted for each benchmark to accommodate their specific requirements. Table  7  presents a comprehensive overview of these parameters across various benchmarks."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Training Parameters for Llama3-70B using QLoRA.",
        "table": "A3.T8.1",
        "footnotes": [],
        "references": [
            "We employed QLoRA fine-tuning on the Llama3-70B base model with a 4-bit quantization. The maximum sequence length was set to 4096 tokens, with a learning rate of 2e-5 and training conducted for 1 epoch. For the LoRA configuration, we used a rank of 64, an alpha of 16, and a dropout of 0.1. These parameters were consistently applied for both the self-rag-llama3-70B and Llama3-70B-baseline models. The training data remained the same as in Experiment 1. For a comprehensive list of training parameters, please refer to Table  8 ."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Configuration details for RAG methods.",
        "table": "A4.T9.1",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": [
        "Equal contribution",
        "Corresponding author"
    ]
}