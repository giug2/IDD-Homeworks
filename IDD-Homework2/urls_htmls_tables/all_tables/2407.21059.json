{
    "id_table_1": {
        "caption": "TABLE I:  Important Notation",
        "table": "S3.T1.16",
        "footnotes": [],
        "references": [
            "During the nascent stages of RAG , its core framework is constituted by indexing, retrieval, and generation, a paradigm referred to as Naive RAG  [ 7 ] . However, as the complexity of tasks and the demands of applications have escalated, the limitations of Naive RAG have become increasingly apparent. As depicted in Figure  1 , it predominantly hinges on the straightforward similarity of chunks, result in poor performance when confronted with complex queries and chunks with substantial variability. The primary challenges of Naive RAG include:   1) Shallow Understanding of Queries.  The semantic similarity between a query and document chunk is not always highly consistent. Relying solely on similarity calculations for retrieval lacks an in-depth exploration of the relationship between the query and the document  [ 8 ] .   2) Retrieval Redundancy and Noise.  Feeding all retrieved chunks directly into LLMs is not always beneficial. Research indicates that an excess of redundant and noisy information may interfere with the LLMs identification of key information, thereby increasing the risk of generating erroneous and hallucinated responses.  [ 9 ]",
            "To overcome the aforementioned limitations, Advanced RAG paradigm focuses on optimizing the retrieval phase, aiming to enhance retrieval efficiency and strengthen the utilization of retrieved chunks. As shown in Figure  1  ,typical strategies involve pre-retrieval processing and post-retrieval processing. For instance, query rewriting is used to make the queries more clear and specific, thereby increasing the accuracy of retrieval  [ 10 ] , and the reranking of retrieval results is employed to enhance the LLMs ability to identify and utilize key information  [ 11 ] .",
            "The modules in the modular RAG system are organized in a linear way, and can be described as Algorithm  1 .",
            "An exemplary case of iterative retrieval is ITER-RETGEN  [ 56 ]  (Figure  11 ), which iterates retrieval-augmented generation and generation-augmented retrieval. Retrieval-augmented generation outputs a response to a task input based on all retrieved knowledge. In each iteration, ITER-RETGEN leverages the model output from the previous iteration as a specific context to help retrieve more relevant knowledge. Termination of the loop is determined by a predefined number of iterations.",
            "A typical implementation of recursive retrieval, such as ToC  [ 13 ]  (see Figure  12  ), involves recursively executing RAC (Recursive Augmented Clarification) to gradually insert sub-nodes into the clarification tree from the initial ambiguous question (AQ). At each expansion step, paragraph re-ranking is performed based on the current query to generate a disambiguous Question (DQ). The exploration of the tree concludes upon reaching the maximum number of valid nodes or the maximum depth. Once the clarification tree is constructed, ToC gathers all valid nodes and generates a comprehensive long-text answer to address AQ.",
            "Tuning-base.  The tuning-based approach involves fine-tuning LLM to generate special tokens, thereby triggering retrieval or generation. This concept can be traced back to Toolformer  [ 50 ] , where the generation of specific content assists in invoking tools. In RAG systems, this approach is used to control both retrieval and generation steps. A typical case is Self-RAG  [ 28 ] (see Figure  14 ). Given an input prompt and the preceding generation result, first predict whether the special token  Retrieve  is helpful for enhancing the continued generation through retrieval. Then, if retrieval is needed, the model generates a  critique  token to evaluate the retrieved passages relevance. and a  critique  token to evaluate if the information in the response is supported by the retrieved passage. Finally, a  critique  token evaluates the overall utility of the response and selects the optimal result as the final output.",
            "In the RAG flow, common methods for fine-tuning the retriever is shown in Figure  15  ,which include:",
            "The primary methods for fine-tuning a generator in RAG flow is shown in Figure  16 , which include:"
        ]
    },
    "global_footnotes": []
}