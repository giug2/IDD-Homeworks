{
    "S4.T1": {
        "caption": "Table 1: Generalization performance of ABE, our proposed early-stopping method, compared to validation-based early-stopping (Baseline). For each setting, “Optimal” is the maximum achievable generalization performance, if early-stopping had been optimal (with an oracle). We show results per Meta-Learning algorithm (averaged over all source and target datasets), and per source dataset (averaged over all Meta-Learning algorithms and target datasets). The first row shows the maximum achievable target accuracy. Results show that for each source dataset and Meta-Learning algorithm, our method consistently outperforms the validation baseline. To measure the variability in performance of our method, for each experiment (algo, source dataset, target dataset), we compute the standard deviation among the generalization performances obtained with the 50 different target tasks (independent trials). We then average those standard deviations across all experiments. We present them on a per-algorithm basis. Those standard deviations are (in percentages of accuracy): 0.35% for MAML, 0.13% for Prototypical Network, and 0.38% for Matching Network.",
        "table": null,
        "footnotes": [],
        "references": [
            "In Fig. 4 we show the gain in target accuracy from our early-stopping method (ABE), compared to validation-based early-stopping (baseline). We also show the optimal performance, i.e. the performance obtained if early-stopping had been optimal in each experiment. For each setting (algorithm, source dataset, and target dataset), we show the accuracy difference against the generalization gap exhibited by the validation-based early-stopping. Results show that our method outperforms the baseline. When the generalization gap of the baseline is small and there is not much gain to have, our method is generally on par with validation-based early-stopping, and only in a very few instances do we suffer from a slight generalization drop. As the baseline generalization gap increases, our method tends to fill this gap. This results in better performance on the target task distribution as observed across all tested Meta-Learning algorithms. In Tab. 1 we show the average generalization performance of ABE, compared against validation-based early-stopping, and the optimal performance. Results indicate that ABE outperforms validation-based early-stopping, closing the generalization gap towards optimal performance. We break the results into two categories : performance per source dataset, averaged over all Meta-Learning algorithms and target datasets (Tab. 5(a)); performance per Meta-Learning algorithm (Tab. 5(b))."
        ]
    },
    "A2.T2": {
        "caption": "Table 2: Generalization performance of ABE, using a ResNet-18 as the neural architecture. For each setting, “Optimal” is the maximum achievable generalization performance, if early-stopping had been optimal (with an oracle). We used Mini-Imagenet as the source dataset and Omniglot as the target dataset. The first row shows the maximum achievable target accuracy. Results show that for each source dataset and Meta-Learning algorithm, our method consistently outperforms the validation baseline.",
        "table": "<table id=\"A2.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A2.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"A2.T2.1.1.1.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\" colspan=\"4\">ResNet-18</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A2.T2.1.2.1\" class=\"ltx_tr\">\n<td id=\"A2.T2.1.2.1.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Acc (%)</td>\n<td id=\"A2.T2.1.2.1.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">MAML</td>\n<td id=\"A2.T2.1.2.1.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Proto Net</td>\n<td id=\"A2.T2.1.2.1.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Matching Net</td>\n</tr>\n<tr id=\"A2.T2.1.3.2\" class=\"ltx_tr\">\n<td id=\"A2.T2.1.3.2.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Optimal</td>\n<td id=\"A2.T2.1.3.2.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">55.93</td>\n<td id=\"A2.T2.1.3.2.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">66.86</td>\n<td id=\"A2.T2.1.3.2.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">64.93</td>\n</tr>\n<tr id=\"A2.T2.1.4.3\" class=\"ltx_tr\">\n<td id=\"A2.T2.1.4.3.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<table id=\"A2.T2.1.4.3.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A2.T2.1.4.3.1.1.1\" class=\"ltx_tr\">\n<td id=\"A2.T2.1.4.3.1.1.1.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Validation</td>\n</tr>\n<tr id=\"A2.T2.1.4.3.1.1.2\" class=\"ltx_tr\">\n<td id=\"A2.T2.1.4.3.1.1.2.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Baseline</td>\n</tr>\n</table>\n</td>\n<td id=\"A2.T2.1.4.3.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">52.38</td>\n<td id=\"A2.T2.1.4.3.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">61.36</td>\n<td id=\"A2.T2.1.4.3.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">57.30</td>\n</tr>\n<tr id=\"A2.T2.1.5.4\" class=\"ltx_tr\">\n<td id=\"A2.T2.1.5.4.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">ABE (ours)</td>\n<td id=\"A2.T2.1.5.4.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span id=\"A2.T2.1.5.4.2.1\" class=\"ltx_text ltx_font_bold\">55.40</span></td>\n<td id=\"A2.T2.1.5.4.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span id=\"A2.T2.1.5.4.3.1\" class=\"ltx_text ltx_font_bold\">62.50</span></td>\n<td id=\"A2.T2.1.5.4.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span id=\"A2.T2.1.5.4.4.1\" class=\"ltx_text ltx_font_bold\">62.88</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Other neural architectures In addition, we tested our ABE on a different neural architecture: a deep residual network (ResNet-18), as used in the original Meta-Dataset paper. We tested it on a subset of our experiments. Our results show our method outperforming validation-based early-stopping, for all three algorithms. See Tab. 2 in App. B.",
            "We added experiments with an additional architecture: a deep residual network of 18 layers (ResNet-18), as used in the original Meta-Dataset paper. We used the few-shot transfer learning setting where Mini-Imagenet is used for meta-training (source), and Omniglot is the target dataset. We ran this experiment for all meta-training algorithms considered in our work, namely: MAML, prototypical network, and matching network. Our experiments show that our method outperforms validation-based early-stopping, for all three algorithms. See Tab. 2."
        ]
    },
    "A2.T3": {
        "caption": "Table 3: Variation of the optimal stopping times of the different target datasets, considering a fixed source dataset. For each experiment, i.e. each tuple (algorithm, source dataset), we computed the stopping times on the different target datasets, measured their standard deviation, and report this deviation as a percentage of the validation stopping time. This variations is significant, and show that for a fixed source dataset, and a target dataset of interest, even if we had access to an extra dataset to use as a proxy for our true target dataset (because it is ood), its stopping time may be significantly different than the optimal stopping time of our target dataset of interest.",
        "table": "<table id=\"A2.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A2.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"A2.T3.1.1.1.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Algorithm</th>\n<th id=\"A2.T3.1.1.1.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">MAML</th>\n<th id=\"A2.T3.1.1.1.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Proto Net</th>\n<th id=\"A2.T3.1.1.1.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Matching Net</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A2.T3.1.2.1\" class=\"ltx_tr\">\n<td id=\"A2.T3.1.2.1.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<table id=\"A2.T3.1.2.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A2.T3.1.2.1.1.1.1\" class=\"ltx_tr\">\n<td id=\"A2.T3.1.2.1.1.1.1.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Standard deviation</td>\n</tr>\n<tr id=\"A2.T3.1.2.1.1.1.2\" class=\"ltx_tr\">\n<td id=\"A2.T3.1.2.1.1.1.2.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">of target stopping-time</td>\n</tr>\n<tr id=\"A2.T3.1.2.1.1.1.3\" class=\"ltx_tr\">\n<td id=\"A2.T3.1.2.1.1.1.3.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">relative to validation stopping-time</td>\n</tr>\n<tr id=\"A2.T3.1.2.1.1.1.4\" class=\"ltx_tr\">\n<td id=\"A2.T3.1.2.1.1.1.4.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">(averaged over all source datasets)</td>\n</tr>\n</table>\n</td>\n<td id=\"A2.T3.1.2.1.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">38.97 %</td>\n<td id=\"A2.T3.1.2.1.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">30.05 %</td>\n<td id=\"A2.T3.1.2.1.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">20.77 %</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "As Fig. 14 suggests, the optimal stopping time for different target datasets can vary significantly. Here we quantitatively evaluate how they vary. For each experiment, i.e. each tuple (algorithm, source dataset), we computed the stopping times on the different target datasets, measured their standard deviation, and report this deviation as a percentage of the validation stopping time. Results show that those stopping times vary significantly, as some target datasets may peak very early on, while others can peak close to the validation stopping time. See the Tab. 3, where we summarize those results on a per-algorithm basis for compactness of the presentation. Those results suggest that using some extra ood dataset for validation (and early-stopping) may still to poor generalization, because of the large discrepancies between the different optimal stopping times."
        ]
    },
    "A2.T4": {
        "caption": "Table 4: OOD-Validation Oracle: Standard deviations for the oracle are computed on the obtained generalization performances, across the different ood dataset being used for early-stopping, then averaged across all experiments and presented on a per-algorithm basis. For results for ABE, the standard deviations in performances are computed across the different independent target tasks being used for early-stopping, and those deviations are averaged across all experiments. Those results show that our method generally performs better than the oracle, and show less variability in performance, meaning that using a single task gives a steady performance.",
        "table": "<table id=\"A2.T4.6\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A2.T4.6.7.1\" class=\"ltx_tr\">\n<th id=\"A2.T4.6.7.1.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Acc (%)</th>\n<th id=\"A2.T4.6.7.1.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">MAML</th>\n<th id=\"A2.T4.6.7.1.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Proto Net</th>\n<th id=\"A2.T4.6.7.1.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">Matching Net</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A2.T4.3.3\" class=\"ltx_tr\">\n<td id=\"A2.T4.3.3.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">OOD-Validation Oracle</td>\n<td id=\"A2.T4.1.1.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">39.59 <math id=\"A2.T4.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A2.T4.1.1.1.m1.1a\"><mo id=\"A2.T4.1.1.1.m1.1.1\" xref=\"A2.T4.1.1.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A2.T4.1.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"A2.T4.1.1.1.m1.1.1.cmml\" xref=\"A2.T4.1.1.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T4.1.1.1.m1.1c\">\\pm</annotation></semantics></math> 1.06</td>\n<td id=\"A2.T4.2.2.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">37.05 <math id=\"A2.T4.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A2.T4.2.2.2.m1.1a\"><mo id=\"A2.T4.2.2.2.m1.1.1\" xref=\"A2.T4.2.2.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A2.T4.2.2.2.m1.1b\"><csymbol cd=\"latexml\" id=\"A2.T4.2.2.2.m1.1.1.cmml\" xref=\"A2.T4.2.2.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T4.2.2.2.m1.1c\">\\pm</annotation></semantics></math> 0.68</td>\n<td id=\"A2.T4.3.3.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">42.48 <math id=\"A2.T4.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A2.T4.3.3.3.m1.1a\"><mo id=\"A2.T4.3.3.3.m1.1.1\" xref=\"A2.T4.3.3.3.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A2.T4.3.3.3.m1.1b\"><csymbol cd=\"latexml\" id=\"A2.T4.3.3.3.m1.1.1.cmml\" xref=\"A2.T4.3.3.3.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T4.3.3.3.m1.1c\">\\pm</annotation></semantics></math> 0.56</td>\n</tr>\n<tr id=\"A2.T4.6.6\" class=\"ltx_tr\">\n<td id=\"A2.T4.6.6.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span id=\"A2.T4.6.6.4.1\" class=\"ltx_text ltx_font_bold\">ABE (ours)</span></td>\n<td id=\"A2.T4.4.4.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">40.51 <math id=\"A2.T4.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A2.T4.4.4.1.m1.1a\"><mo id=\"A2.T4.4.4.1.m1.1.1\" xref=\"A2.T4.4.4.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A2.T4.4.4.1.m1.1b\"><csymbol cd=\"latexml\" id=\"A2.T4.4.4.1.m1.1.1.cmml\" xref=\"A2.T4.4.4.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T4.4.4.1.m1.1c\">\\pm</annotation></semantics></math> 0.35</td>\n<td id=\"A2.T4.5.5.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">37.35 <math id=\"A2.T4.5.5.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A2.T4.5.5.2.m1.1a\"><mo id=\"A2.T4.5.5.2.m1.1.1\" xref=\"A2.T4.5.5.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A2.T4.5.5.2.m1.1b\"><csymbol cd=\"latexml\" id=\"A2.T4.5.5.2.m1.1.1.cmml\" xref=\"A2.T4.5.5.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T4.5.5.2.m1.1c\">\\pm</annotation></semantics></math> 0.13</td>\n<td id=\"A2.T4.6.6.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">42.09 <math id=\"A2.T4.6.6.3.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"A2.T4.6.6.3.m1.1a\"><mo id=\"A2.T4.6.6.3.m1.1.1\" xref=\"A2.T4.6.6.3.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"A2.T4.6.6.3.m1.1b\"><csymbol cd=\"latexml\" id=\"A2.T4.6.6.3.m1.1.1.cmml\" xref=\"A2.T4.6.6.3.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T4.6.6.3.m1.1c\">\\pm</annotation></semantics></math> 0.38</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Early-stopping from an additional (oracle) OOD task distribution\nWhat happens if we perform early-stopping using a third distribution, i.e another dataset previously unseen (assuming we have access to it) ? Should its distributional shift with the training distribution result in a better early-stopping time ? Results in Appendix B.3 Tab. 4 that while this OOD validation early-stopping can outperform the in-distribution counterpart, our method generally outperforms it, the resulting early-stopping time can vary greatly depending on which dataset is used. The performance is generally lower too, compared to ABE, highlighting the importance of considering the specific target problem at hand.",
            "Another question arises: what happens if we perform early-stopping using a third distribution (assuming we have access to it)? Should its distributional shift with the training distribution result in a better early-stopping time?\nHaving access to some extra dataset for validation should be considered as an oracle method, since we cannot assume easy availability of that extra dataset. Nevertheless, for the sake of analysis, we computed how such oracle method, which we call the “ OOD-Validation Oracle “, would perform in terms of generalization. For each experiment, i.e. each tuple (algorithm, source dataset, target dataset), we measured, on average, what is the performance when early-stopping from another dataset than the target dataset. Thus for each experiment, we average the performance over all such extra datasets, and we compute the standard deviation of those performances, since the extra datasets will have different stopping times. We then average this over all experiments, and present them on a per-algorithm basis, for compactness of the presentation. See Tab. 4. The results show that, even with this oracle, our method generally performs better, as ABE shows better average performance for two out of the three algorithms, and shows less variance in its performance (for ABE, within each experiment, we compute the standard deviation across all the 50 target tasks that are used to test our method for early-stopping). This highlights the importance of considering the specific target problem at hand."
        ]
    }
}