{
    "id_table_1": {
        "caption": "List of NLP Techniques (Part 1)",
        "table": "<table id=\"S3.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.1.1.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\"><span id=\"S3.T1.1.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">ID</span></span>\n</span>\n</th>\n<th id=\"S3.T1.1.1.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.1.1.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\"><span id=\"S3.T1.1.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Name</span></span>\n</span>\n</th>\n<th id=\"S3.T1.1.1.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span id=\"S3.T1.1.1.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.1.1.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\"><span id=\"S3.T1.1.1.1.3.1.1.1\" class=\"ltx_text ltx_font_bold\">Explanation</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.2.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.2.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.2.1.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">1</span>\n</span>\n</td>\n<td id=\"S3.T1.1.2.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.2.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.2.1.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Part-of-Speech (POS) Tagging</span>\n</span>\n</td>\n<td id=\"S3.T1.1.2.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.2.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.2.1.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">POS Tagging (or Tagging) processes a sequence of words, and attaches a POS tag to each word. Parts of speech are also known as word classes or lexical categories.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.3.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.3.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.3.2.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">2</span>\n</span>\n</td>\n<td id=\"S3.T1.1.3.2.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.3.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.3.2.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Term Extraction</span>\n</span>\n</td>\n<td id=\"S3.T1.1.3.2.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.3.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.3.2.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">The process of extracting the most relevant words and expressions from text. Related terms: Keyword Extraction, Word Extraction</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.4.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.4.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.4.3.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">3</span>\n</span>\n</td>\n<td id=\"S3.T1.1.4.3.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.4.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.4.3.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Keyword Searching</span>\n</span>\n</td>\n<td id=\"S3.T1.1.4.3.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.4.3.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.4.3.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">The technique of finding strings that match a pattern. Related terms: Term Matching, Word Matching</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.5.4\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.5.4.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.5.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.5.4.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">4</span>\n</span>\n</td>\n<td id=\"S3.T1.1.5.4.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.5.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.5.4.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Chunking</span>\n</span>\n</td>\n<td id=\"S3.T1.1.5.4.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.5.4.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.5.4.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">Chunking (or text chunking) is a type of shallow parsing that analyses a sentence by first identifying its constituent parts (nouns, verbs, adjectives, etc.) and then links them to higher order units that have discrete grammatical meanings (noun groups or phrases, verb groups, etc.). Related term: Shallow Parsing.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.6.5\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.6.5.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.6.5.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.6.5.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">5</span>\n</span>\n</td>\n<td id=\"S3.T1.1.6.5.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.6.5.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.6.5.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Named Entity Recognition (NER)</span>\n</span>\n</td>\n<td id=\"S3.T1.1.6.5.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.6.5.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.6.5.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">Subtask of information extraction that is based to find and classify named entities in a certain text into pre-defined categories or class such as the names of persons, organizations, locations, etc. Related terms: Entity Identification, Concept Extraction.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.7.6\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.7.6.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.7.6.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.7.6.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">6</span>\n</span>\n</td>\n<td id=\"S3.T1.1.7.6.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.7.6.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.7.6.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Semantic Role Labelling (SRL)</span>\n</span>\n</td>\n<td id=\"S3.T1.1.7.6.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.7.6.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.7.6.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">The process of detecting the semantic arguments linked with the predicate or verb of a sentence and their classification into their specific roles. Related Term: Semantic parsing, semantic trees, shallow parsing, and shallow semantic analysis.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.8.7\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.8.7.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.8.7.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.8.7.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">7</span>\n</span>\n</td>\n<td id=\"S3.T1.1.8.7.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.8.7.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.8.7.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Temporal Tagging</span>\n</span>\n</td>\n<td id=\"S3.T1.1.8.7.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.8.7.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.8.7.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">The task of finding phrases with temporal meaning within the context of a larger document.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.9.8\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.9.8.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.9.8.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.9.8.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">8</span>\n</span>\n</td>\n<td id=\"S3.T1.1.9.8.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.9.8.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.9.8.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Dependency Parsing</span>\n</span>\n</td>\n<td id=\"S3.T1.1.9.8.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.9.8.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.9.8.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">Dependency parsing is the process of analyzing the grammatical structure of a sentence based on the dependencies between the words in a sentence. Related terms: Syntactic Patterns, Syntactic Structure</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.10.9\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.10.9.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.10.9.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.10.9.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">9</span>\n</span>\n</td>\n<td id=\"S3.T1.1.10.9.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.10.9.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.10.9.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Constituency Parsing</span>\n</span>\n</td>\n<td id=\"S3.T1.1.10.9.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.10.9.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.10.9.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">The process of analyzing the sentences by breaking down it into sub-phrases also known as constituents. These sub-phrases belong to a specific category of grammar like NP (noun phrase) and VP(verb phrase). Related terms: Phrase Parsing, Phrase Detection, Phrasal Verb Extraction</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.11.10\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.11.10.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.11.10.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.11.10.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">10</span>\n</span>\n</td>\n<td id=\"S3.T1.1.11.10.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.11.10.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.11.10.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Link Grammar</span>\n</span>\n</td>\n<td id=\"S3.T1.1.11.10.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.11.10.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.11.10.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">Builds relations between pairs of words, rather than constructing constituents in a phrase structure hierarchy.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.12.11\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.12.11.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.12.11.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.12.11.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">11</span>\n</span>\n</td>\n<td id=\"S3.T1.1.12.11.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.12.11.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.12.11.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Semantic Parsing</span>\n</span>\n</td>\n<td id=\"S3.T1.1.12.11.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.12.11.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.12.11.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">The task of converting a natural language utterance to a logical form: a machine-understandable representation of its meaning.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.13.12\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.13.12.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.13.12.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.13.12.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">12</span>\n</span>\n</td>\n<td id=\"S3.T1.1.13.12.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.13.12.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.13.12.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Sentiment Analysis</span>\n</span>\n</td>\n<td id=\"S3.T1.1.13.12.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.13.12.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.13.12.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">The process of computationally identifying and categorizing opinions expressed in a piece of text</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.14.13\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.14.13.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.14.13.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.14.13.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">13</span>\n</span>\n</td>\n<td id=\"S3.T1.1.14.13.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.14.13.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.14.13.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Text Annotation</span>\n</span>\n</td>\n<td id=\"S3.T1.1.14.13.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.14.13.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.14.13.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">The practice and the result of adding a note or gloss to a text, which may include highlights or underlining, comments, footnotes, tags, and links.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.15.14\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.15.14.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.15.14.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.15.14.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">14</span>\n</span>\n</td>\n<td id=\"S3.T1.1.15.14.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.15.14.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.15.14.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Semantic Annotation</span>\n</span>\n</td>\n<td id=\"S3.T1.1.15.14.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.15.14.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.15.14.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">The process of attaching to a text document or other unstructured content, metadata about concepts (e.g., people, places, organizations, products or topics) relevant to it.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.16.15\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.16.15.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.16.15.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.16.15.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">15</span>\n</span>\n</td>\n<td id=\"S3.T1.1.16.15.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.16.15.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.16.15.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Topic Modelling</span>\n</span>\n</td>\n<td id=\"S3.T1.1.16.15.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.16.15.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.16.15.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">A type of statistical model for discovering the abstract ”topics” that occur in a collection of documents</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.17.16\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.17.16.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.17.16.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.17.16.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">16</span>\n</span>\n</td>\n<td id=\"S3.T1.1.17.16.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.17.16.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.17.16.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Summarization</span>\n</span>\n</td>\n<td id=\"S3.T1.1.17.16.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.17.16.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.17.16.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">The practice of breaking down long publications into manageable paragraphs or sentences. The procedure extracts important information while also ensuring that the paragraph’s sense is preserved.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.18.17\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.18.17.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.18.17.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.18.17.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">17</span>\n</span>\n</td>\n<td id=\"S3.T1.1.18.17.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.18.17.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.18.17.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Latent Dirichlet Allocation (LDA)</span>\n</span>\n</td>\n<td id=\"S3.T1.1.18.17.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.18.17.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.18.17.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">The process of analysing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.19.18\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.19.18.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.19.18.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.19.18.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">18</span>\n</span>\n</td>\n<td id=\"S3.T1.1.19.18.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.19.18.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.19.18.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Latent Semantic Indexing (LSI)</span>\n</span>\n</td>\n<td id=\"S3.T1.1.19.18.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.19.18.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.19.18.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">A mathematical practice that helps classify and retrieve information on particular key terms and concepts using singular value decomposition (SVD). Related Term: Latent Semantic Analysis (LSA)</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.20.19\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.20.19.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.20.19.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.20.19.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">19</span>\n</span>\n</td>\n<td id=\"S3.T1.1.20.19.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.20.19.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.20.19.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Semantic Patterns</span>\n</span>\n</td>\n<td id=\"S3.T1.1.20.19.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.20.19.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.20.19.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">Semantic patterns are generated based on common matching concepts. The top matching concepts of each word are considered. One semantic pattern can relate to several concepts and a single semantic clique can contain several semantic patterns.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.21.20\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.21.20.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.21.20.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.21.20.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">20</span>\n</span>\n</td>\n<td id=\"S3.T1.1.21.20.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.21.20.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.21.20.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Case Grammar</span>\n</span>\n</td>\n<td id=\"S3.T1.1.21.20.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.21.20.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.21.20.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">A system of linguistic analysis, focusing on the link between the valence, or number of subjects, objects, etc., of a verb and the grammatical context it requires.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.22.21\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.22.21.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.22.21.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.22.21.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">21</span>\n</span>\n</td>\n<td id=\"S3.T1.1.22.21.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.22.21.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.22.21.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Semantic Frames</span>\n</span>\n</td>\n<td id=\"S3.T1.1.22.21.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.22.21.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.22.21.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">A coherent structure of concepts that are related such that without knowledge of all of them, one does not have complete knowledge of one of the either.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.23.22\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.23.22.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.23.22.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.23.22.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">22</span>\n</span>\n</td>\n<td id=\"S3.T1.1.23.22.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.23.22.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.23.22.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Knowledge Graph</span>\n</span>\n</td>\n<td id=\"S3.T1.1.23.22.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.23.22.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.23.22.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">A way of storing data that resulted from an information extraction task.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.24.23\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.24.23.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.24.23.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.24.23.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">23</span>\n</span>\n</td>\n<td id=\"S3.T1.1.24.23.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.24.23.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.24.23.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Bag-of-Words (BOW)</span>\n</span>\n</td>\n<td id=\"S3.T1.1.24.23.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.24.23.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.24.23.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">A representation that turns arbitrary text into fixed-length vectors by counting how many times each word appears. This process is often referred to as vectorization.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.25.24\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.25.24.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.25.24.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.25.24.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">24</span>\n</span>\n</td>\n<td id=\"S3.T1.1.25.24.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.25.24.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.25.24.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Word Frequency</span>\n</span>\n</td>\n<td id=\"S3.T1.1.25.24.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.25.24.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.25.24.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">How often a word appears in a document, divided by how many words there are. Related Terms: Term Frequency, Domain Term Frequency</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.26.25\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.26.25.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.26.25.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.26.25.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">25</span>\n</span>\n</td>\n<td id=\"S3.T1.1.26.25.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.26.25.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.26.25.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Term Frequency-Inverse Document Frequency (TF-IDF)</span>\n</span>\n</td>\n<td id=\"S3.T1.1.26.25.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.26.25.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.26.25.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">A statistical measure that evaluates how relevant a word is to a document in a collection of documents.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.27.26\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.27.26.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.27.26.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.27.26.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">26</span>\n</span>\n</td>\n<td id=\"S3.T1.1.27.26.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.27.26.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.27.26.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Co-location Analysis</span>\n</span>\n</td>\n<td id=\"S3.T1.1.27.26.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.27.26.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.27.26.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">A Co-location is an expression consisting of two or more words that correspond to some conventional way of saying things.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.28.27\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.28.27.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.28.27.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.28.27.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">27</span>\n</span>\n</td>\n<td id=\"S3.T1.1.28.27.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.28.27.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.28.27.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Term-Document Matrix</span>\n</span>\n</td>\n<td id=\"S3.T1.1.28.27.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.28.27.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.28.27.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">A mathematical matrix that describes the frequency of terms that occur in a collection of documents.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.29.28\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.29.28.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.29.28.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.29.28.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">28</span>\n</span>\n</td>\n<td id=\"S3.T1.1.29.28.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.29.28.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.29.28.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Character Counting</span>\n</span>\n</td>\n<td id=\"S3.T1.1.29.28.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.29.28.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.29.28.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">Counts the number of characters in a line of text, page or group of text.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.30.29\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.30.29.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.30.29.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.30.29.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">29</span>\n</span>\n</td>\n<td id=\"S3.T1.1.30.29.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.30.29.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.30.29.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Concordance</span>\n</span>\n</td>\n<td id=\"S3.T1.1.30.29.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T1.1.30.29.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.30.29.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">An alphabetical list of the words (especially the important ones) present in a text, usually with citations of the passages in which they are found.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.31.30\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.31.30.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.31.30.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.31.30.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">30</span>\n</span>\n</td>\n<td id=\"S3.T1.1.31.30.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.31.30.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.31.30.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Cosine Similarity</span>\n</span>\n</td>\n<td id=\"S3.T1.1.31.30.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t\">\n<span id=\"S3.T1.1.31.30.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.31.30.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">A metric used to measure how similar the documents are irrespective of their size.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": []
    },
    "id_table_2": {
        "caption": " List of NLP Techniques (Part 2)",
        "table": "<table id=\"S3.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.1.1.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\"><span id=\"S3.T2.1.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">ID</span></span>\n</span>\n</th>\n<th id=\"S3.T2.1.1.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.1.1.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\"><span id=\"S3.T2.1.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Name</span></span>\n</span>\n</th>\n<th id=\"S3.T2.1.1.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span id=\"S3.T2.1.1.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.1.1.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\"><span id=\"S3.T2.1.1.1.3.1.1.1\" class=\"ltx_text ltx_font_bold\">Explanation</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T2.1.2.1\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.2.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.2.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.2.1.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">31</span>\n</span>\n</td>\n<td id=\"S3.T2.1.2.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.2.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.2.1.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Lexical Affinity</span>\n</span>\n</td>\n<td id=\"S3.T2.1.2.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.2.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.2.1.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">Assigns to arbitrary words a probabilistic ’affinity’ for a particular category.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.3.2\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.3.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.3.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.3.2.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">32</span>\n</span>\n</td>\n<td id=\"S3.T2.1.3.2.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.3.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.3.2.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Similarity Distance</span>\n</span>\n</td>\n<td id=\"S3.T2.1.3.2.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.3.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.3.2.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">Determines the minimum number of single character edits required to change one word to another.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.4.3\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.4.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.4.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.4.3.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">33</span>\n</span>\n</td>\n<td id=\"S3.T2.1.4.3.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.4.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.4.3.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Document Similarity</span>\n</span>\n</td>\n<td id=\"S3.T2.1.4.3.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.4.3.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.4.3.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">Computing the similarity between two text documents by transforming the input documents into real-valued vectors.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.5.4\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.5.4.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.5.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.5.4.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">34</span>\n</span>\n</td>\n<td id=\"S3.T2.1.5.4.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.5.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.5.4.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Lexical Similarity</span>\n</span>\n</td>\n<td id=\"S3.T2.1.5.4.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.5.4.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.5.4.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">Provides a measure of the similarity of two texts based on the intersection of the word sets of same or different languages.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.6.5\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.6.5.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.6.5.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.6.5.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">35</span>\n</span>\n</td>\n<td id=\"S3.T2.1.6.5.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.6.5.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.6.5.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Regular Expression</span>\n</span>\n</td>\n<td id=\"S3.T2.1.6.5.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.6.5.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.6.5.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">A special series of strings for describing a a text pattern for the purpose of searching or replacing the described items.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.7.6\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.7.6.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.7.6.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.7.6.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">36</span>\n</span>\n</td>\n<td id=\"S3.T2.1.7.6.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.7.6.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.7.6.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Lexical Patterns</span>\n</span>\n</td>\n<td id=\"S3.T2.1.7.6.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.7.6.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.7.6.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">Words or chuck of text that occurs in language with high frequency and the meaning of the parts are sometime different than the meaning of the whole.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.8.7\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.8.7.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.8.7.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.8.7.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">37</span>\n</span>\n</td>\n<td id=\"S3.T2.1.8.7.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.8.7.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.8.7.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Generation Rules</span>\n</span>\n</td>\n<td id=\"S3.T2.1.8.7.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.8.7.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.8.7.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">Generation rules to produce meaningful sentences in Natural Language.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.9.8\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.9.8.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.9.8.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.9.8.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">38</span>\n</span>\n</td>\n<td id=\"S3.T2.1.9.8.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.9.8.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.9.8.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Stemming</span>\n</span>\n</td>\n<td id=\"S3.T2.1.9.8.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.9.8.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.9.8.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">A crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.10.9\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.10.9.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.10.9.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.10.9.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">39</span>\n</span>\n</td>\n<td id=\"S3.T2.1.10.9.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.10.9.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.10.9.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Lemmatization</span>\n</span>\n</td>\n<td id=\"S3.T2.1.10.9.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.10.9.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.10.9.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">Use a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.11.10\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.11.10.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.11.10.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.11.10.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">40</span>\n</span>\n</td>\n<td id=\"S3.T2.1.11.10.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.11.10.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.11.10.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Stop-Word Removal</span>\n</span>\n</td>\n<td id=\"S3.T2.1.11.10.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.11.10.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.11.10.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">Words which are filtered out before or after processing of natural language data (text).</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.12.11\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.12.11.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.12.11.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.12.11.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">41</span>\n</span>\n</td>\n<td id=\"S3.T2.1.12.11.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.12.11.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.12.11.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Noise Removal</span>\n</span>\n</td>\n<td id=\"S3.T2.1.12.11.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.12.11.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.12.11.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">Removing characters digits and pieces of text that can interfere with your text analysis.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.13.12\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.13.12.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.13.12.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.13.12.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">42</span>\n</span>\n</td>\n<td id=\"S3.T2.1.13.12.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.13.12.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.13.12.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Punctuation Removal</span>\n</span>\n</td>\n<td id=\"S3.T2.1.13.12.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.13.12.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.13.12.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">Removing puncuatations marks.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.14.13\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.14.13.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.14.13.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.14.13.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">43</span>\n</span>\n</td>\n<td id=\"S3.T2.1.14.13.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.14.13.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.14.13.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Lowercasing</span>\n</span>\n</td>\n<td id=\"S3.T2.1.14.13.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.14.13.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.14.13.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">Converting all your data to lowercase helps in the process of preprocessing and in later stages in the NLP application, when you are doing parsing.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.15.14\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.15.14.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.15.14.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.15.14.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">44</span>\n</span>\n</td>\n<td id=\"S3.T2.1.15.14.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.15.14.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.15.14.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Camel Case Splitting</span>\n</span>\n</td>\n<td id=\"S3.T2.1.15.14.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.15.14.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.15.14.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">Split CamelCase string to individual strings.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.16.15\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.16.15.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.16.15.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.16.15.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">45</span>\n</span>\n</td>\n<td id=\"S3.T2.1.16.15.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.16.15.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.16.15.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Tokenization</span>\n</span>\n</td>\n<td id=\"S3.T2.1.16.15.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.16.15.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.16.15.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">The process of breaking a stream of text into words, phrases, symbols, or other meaningful tokens. Related terms: Word Segmentation</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.17.16\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.17.16.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.17.16.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.17.16.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">46</span>\n</span>\n</td>\n<td id=\"S3.T2.1.17.16.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.17.16.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.17.16.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Sentence Segmentation</span>\n</span>\n</td>\n<td id=\"S3.T2.1.17.16.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.17.16.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.17.16.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">Split a document into sentences, each containing a list of tokens. Related terms: Sentence Splitting</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.18.17\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.18.17.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.18.17.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.18.17.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">47</span>\n</span>\n</td>\n<td id=\"S3.T2.1.18.17.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.18.17.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.18.17.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">n-gram</span>\n</span>\n</td>\n<td id=\"S3.T2.1.18.17.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.18.17.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.18.17.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">A representation of a text using a sequence of N words or N characters (character n-gram), where N can be any number. Thus we can have 1-gram (unigram), 2-gram (bigram), 3-gram (trigram), etc.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.19.18\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.19.18.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.19.18.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.19.18.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">48</span>\n</span>\n</td>\n<td id=\"S3.T2.1.19.18.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.19.18.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.19.18.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Word Embedding</span>\n</span>\n</td>\n<td id=\"S3.T2.1.19.18.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.19.18.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.19.18.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">One of the most popular technique to learn word embeddings using shallow neural network. Word embeddings are vector representations of a particular word. Related terms: Word2Vec</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.20.19\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.20.19.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.20.19.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.20.19.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">49</span>\n</span>\n</td>\n<td id=\"S3.T2.1.20.19.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.20.19.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.20.19.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Contextualized word embedding</span>\n</span>\n</td>\n<td id=\"S3.T2.1.20.19.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.20.19.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.20.19.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">A neural model that learns a generic embedding function for variable length contexts of target words. Related terms: Context2Vec</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.21.20\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.21.20.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.21.20.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.21.20.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">50</span>\n</span>\n</td>\n<td id=\"S3.T2.1.21.20.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.21.20.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.21.20.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Sentence and document Embedding</span>\n</span>\n</td>\n<td id=\"S3.T2.1.21.20.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.21.20.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.21.20.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">A generalized word2vec method, for representing documents as a vector. Related term: Doc2Vec</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.22.21\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.22.21.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.22.21.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.22.21.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">51</span>\n</span>\n</td>\n<td id=\"S3.T2.1.22.21.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.22.21.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.22.21.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">GloVe</span>\n</span>\n</td>\n<td id=\"S3.T2.1.22.21.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.22.21.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.22.21.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">An alternative to word2vec for the representation of the distributed words.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.23.22\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.23.22.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.23.22.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.23.22.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">52</span>\n</span>\n</td>\n<td id=\"S3.T2.1.23.22.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.23.22.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.23.22.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">FastText</span>\n</span>\n</td>\n<td id=\"S3.T2.1.23.22.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.23.22.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.23.22.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">An alternative to word2vec, FastText represents each word as a bag of character n-gram.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.24.23\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.24.23.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.24.23.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.24.23.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">53</span>\n</span>\n</td>\n<td id=\"S3.T2.1.24.23.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.24.23.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.24.23.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Textual Entailment Recognition</span>\n</span>\n</td>\n<td id=\"S3.T2.1.24.23.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.24.23.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.24.23.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">Deciding, given two text fragments, whether the meaning of one text is entailed (can be inferred) from another text.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.25.24\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.25.24.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.25.24.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.25.24.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">54</span>\n</span>\n</td>\n<td id=\"S3.T2.1.25.24.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.25.24.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.25.24.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Homonym Detection</span>\n</span>\n</td>\n<td id=\"S3.T2.1.25.24.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.25.24.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.25.24.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">Detecting the words that are pronounced the same as each other (e.g., ”maid” and ”made”) or have the same spelling (e.g., ”lead weight” and ”to lead”).</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.26.25\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.26.25.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.26.25.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.26.25.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">55</span>\n</span>\n</td>\n<td id=\"S3.T2.1.26.25.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.26.25.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.26.25.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Synonym Detection</span>\n</span>\n</td>\n<td id=\"S3.T2.1.26.25.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.26.25.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.26.25.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">Finding a a word or phrase that means exactly or nearly the same as another word or phrase in a text.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.27.26\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.27.26.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.27.26.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.27.26.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">56</span>\n</span>\n</td>\n<td id=\"S3.T2.1.27.26.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.27.26.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.27.26.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Coreference Resolution</span>\n</span>\n</td>\n<td id=\"S3.T2.1.27.26.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S3.T2.1.27.26.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.27.26.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">Finding all expressions that refer to the same entity in a discourse.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.1.28.27\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.28.27.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.28.27.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.28.27.1.1.1\" class=\"ltx_p\" style=\"width:11.4pt;\">57</span>\n</span>\n</td>\n<td id=\"S3.T2.1.28.27.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.28.27.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.28.27.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">Anaphora Resolution</span>\n</span>\n</td>\n<td id=\"S3.T2.1.28.27.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t\">\n<span id=\"S3.T2.1.28.27.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.28.27.3.1.1\" class=\"ltx_p\" style=\"width:313.0pt;\">Resolving what a pronoun, or a noun phrase refers to in a discourse.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": []
    },
    "id_table_3": {
        "caption": ": Classifying NLP Techniques by Tasks.",
        "table": "<table id=\"S4.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span id=\"S4.T3.1.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.1.1.1.1.1\" class=\"ltx_p\" style=\"width:85.4pt;\"><span id=\"S4.T3.1.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">NLP Tasks</span></span>\n</span>\n</th>\n<th id=\"S4.T3.1.1.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span id=\"S4.T3.1.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.1.1.2.1.1\" class=\"ltx_p\" style=\"width:156.5pt;\"><span id=\"S4.T3.1.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Explanation</span></span>\n</span>\n</th>\n<th id=\"S4.T3.1.1.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span id=\"S4.T3.1.1.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.1.1.3.1.1\" class=\"ltx_p\" style=\"width:213.4pt;\"><span id=\"S4.T3.1.1.1.3.1.1.1\" class=\"ltx_text ltx_font_bold\">NLP Techniques</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.1.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.2.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S4.T3.1.2.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.2.1.1.1.1\" class=\"ltx_p\" style=\"width:85.4pt;\"><span class=\"ltx_rule\" style=\"width:0.0pt;height:12.9pt;background:black;display:inline-block;\"></span><span id=\"S4.T3.1.2.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Part-of-Speech Tagging</span></span>\n</span>\n</td>\n<td id=\"S4.T3.1.2.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S4.T3.1.2.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.2.1.2.1.1\" class=\"ltx_p\" style=\"width:156.5pt;\">Associate words with part-of-speech (POS) tags to distinguish between nouns, verbs, adjectives, adverbs, etc.</span>\n</span>\n</td>\n<td id=\"S4.T3.1.2.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S4.T3.1.2.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.2.1.3.1.1\" class=\"ltx_p\" style=\"width:213.4pt;\">Part-of-Speech (POS) Tagging</span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T3.1.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.3.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S4.T3.1.3.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.3.2.1.1.1\" class=\"ltx_p\" style=\"width:85.4pt;\"><span class=\"ltx_rule\" style=\"width:0.0pt;height:12.9pt;background:black;display:inline-block;\"></span><span id=\"S4.T3.1.3.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Semantic Tagging</span></span>\n</span>\n</td>\n<td id=\"S4.T3.1.3.2.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S4.T3.1.3.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.3.2.2.1.1\" class=\"ltx_p\" style=\"width:156.5pt;\">Extract useful bits of information (words, terms, relations, etc.) from the text.</span>\n</span>\n</td>\n<td id=\"S4.T3.1.3.2.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S4.T3.1.3.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.3.2.3.1.1\" class=\"ltx_p\" style=\"width:213.4pt;\">Term Extraction, Term Matching, Chunking, Concept Extraction, Named Entity Recognition (NER), Semantic Role Labelling (SRL), Temporal Tagging</span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T3.1.4.3\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.4.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S4.T3.1.4.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.4.3.1.1.1\" class=\"ltx_p\" style=\"width:85.4pt;\"><span class=\"ltx_rule\" style=\"width:0.0pt;height:12.9pt;background:black;display:inline-block;\"></span><span id=\"S4.T3.1.4.3.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Syntactic Analysis</span></span>\n</span>\n</td>\n<td id=\"S4.T3.1.4.3.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S4.T3.1.4.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.4.3.2.1.1\" class=\"ltx_p\" style=\"width:156.5pt;\">Construct a syntactic structure representing the relationship between the logical components in a stream of text, such as the parse tree, or the dependency parsing graph.</span>\n</span>\n</td>\n<td id=\"S4.T3.1.4.3.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S4.T3.1.4.3.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.4.3.3.1.1\" class=\"ltx_p\" style=\"width:213.4pt;\">Dependency Parsing, Constituency Parsing, Link Grammar</span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T3.1.5.4\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.5.4.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S4.T3.1.5.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.5.4.1.1.1\" class=\"ltx_p\" style=\"width:85.4pt;\"><span class=\"ltx_rule\" style=\"width:0.0pt;height:12.9pt;background:black;display:inline-block;\"></span><span id=\"S4.T3.1.5.4.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Semantic Analysis</span></span>\n</span>\n</td>\n<td id=\"S4.T3.1.5.4.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S4.T3.1.5.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.5.4.2.1.1\" class=\"ltx_p\" style=\"width:156.5pt;\">Identify and label semantically relevant components and relations in the text.</span>\n</span>\n</td>\n<td id=\"S4.T3.1.5.4.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S4.T3.1.5.4.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.5.4.3.1.1\" class=\"ltx_p\" style=\"width:213.4pt;\">Semantic Parsing, Sentiment Analysis, Text Annotation, Semantic Annotation, Topic Modelling, Summarization, Latent Dirichlet Allocation (LDA), Latent Semantic Indexing (LSI), Semantic Patterns, Case Grammar, Semantic Frames, Knowledge Graph, Textual Entailment Recognition (TER), Homonym Detection, Synonym Detection, Coreference Resolution, Anaphora Resolution</span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T3.1.6.5\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.6.5.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S4.T3.1.6.5.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.6.5.1.1.1\" class=\"ltx_p\" style=\"width:85.4pt;\"><span class=\"ltx_rule\" style=\"width:0.0pt;height:12.9pt;background:black;display:inline-block;\"></span><span id=\"S4.T3.1.6.5.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Frequency Analysis</span></span>\n</span>\n</td>\n<td id=\"S4.T3.1.6.5.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S4.T3.1.6.5.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.6.5.2.1.1\" class=\"ltx_p\" style=\"width:156.5pt;\">Analyse the frequency of occurrence of lexical elements (e.g., words and characters) and groups of elements (e.g., phrases and multiwords) in a given text.</span>\n</span>\n</td>\n<td id=\"S4.T3.1.6.5.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S4.T3.1.6.5.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.6.5.3.1.1\" class=\"ltx_p\" style=\"width:213.4pt;\">Bag-of-Words (BOW), Word Frequency, Term Frequency (TF), Term Frequency &amp; Inverse Document Frequency (TF-IDF), Co-location Analysis, Term-Document Matrix, Character Counting, Concordance</span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T3.1.7.6\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.7.6.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S4.T3.1.7.6.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.7.6.1.1.1\" class=\"ltx_p\" style=\"width:85.4pt;\"><span class=\"ltx_rule\" style=\"width:0.0pt;height:12.9pt;background:black;display:inline-block;\"></span><span id=\"S4.T3.1.7.6.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Similarity Analysis</span></span>\n</span>\n</td>\n<td id=\"S4.T3.1.7.6.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S4.T3.1.7.6.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.7.6.2.1.1\" class=\"ltx_p\" style=\"width:156.5pt;\">Calculate numerical values of the similarity between text elements, such as to identify semantic relatedness.</span>\n</span>\n</td>\n<td id=\"S4.T3.1.7.6.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S4.T3.1.7.6.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.7.6.3.1.1\" class=\"ltx_p\" style=\"width:213.4pt;\">Cosine Similarity, Lexical Affinity, Similarity Distance, Document Similarity, Lexical Similarity</span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T3.1.8.7\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.8.7.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S4.T3.1.8.7.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.8.7.1.1.1\" class=\"ltx_p\" style=\"width:85.4pt;\"><span class=\"ltx_rule\" style=\"width:0.0pt;height:12.9pt;background:black;display:inline-block;\"></span><span id=\"S4.T3.1.8.7.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Rule-Based Analysis</span></span>\n</span>\n</td>\n<td id=\"S4.T3.1.8.7.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S4.T3.1.8.7.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.8.7.2.1.1\" class=\"ltx_p\" style=\"width:156.5pt;\">Use rules or patterns to analyse the syntax or semantics of a text or transform the text.</span>\n</span>\n</td>\n<td id=\"S4.T3.1.8.7.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S4.T3.1.8.7.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.8.7.3.1.1\" class=\"ltx_p\" style=\"width:213.4pt;\">Regular Expression, Lexical Patterns, Generation Rules</span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T3.1.9.8\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.9.8.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S4.T3.1.9.8.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.9.8.1.1.1\" class=\"ltx_p\" style=\"width:85.4pt;\"><span class=\"ltx_rule\" style=\"width:0.0pt;height:12.9pt;background:black;display:inline-block;\"></span><span id=\"S4.T3.1.9.8.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Text Normalization</span></span>\n</span>\n</td>\n<td id=\"S4.T3.1.9.8.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S4.T3.1.9.8.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.9.8.2.1.1\" class=\"ltx_p\" style=\"width:156.5pt;\">Convert the words into their original form and remove unnecessary words or characters from the text.</span>\n</span>\n</td>\n<td id=\"S4.T3.1.9.8.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S4.T3.1.9.8.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.9.8.3.1.1\" class=\"ltx_p\" style=\"width:213.4pt;\">Stemming, Lemmatization, Stop-Word Removal, Noise Removal, Punctuation Removal, Lowercasing, Camel Case Splitting</span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T3.1.10.9\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.10.9.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S4.T3.1.10.9.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.10.9.1.1.1\" class=\"ltx_p\" style=\"width:85.4pt;\"><span class=\"ltx_rule\" style=\"width:0.0pt;height:12.9pt;background:black;display:inline-block;\"></span><span id=\"S4.T3.1.10.9.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Text Segmentation</span></span>\n</span>\n</td>\n<td id=\"S4.T3.1.10.9.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S4.T3.1.10.9.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.10.9.2.1.1\" class=\"ltx_p\" style=\"width:156.5pt;\">Break down a text into a sequence of individual tokens (i.e., words or sentences).</span>\n</span>\n</td>\n<td id=\"S4.T3.1.10.9.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S4.T3.1.10.9.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.10.9.3.1.1\" class=\"ltx_p\" style=\"width:213.4pt;\">Tokenization, Sentence Segmentation</span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T3.1.11.10\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.11.10.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"S4.T3.1.11.10.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.11.10.1.1.1\" class=\"ltx_p\" style=\"width:85.4pt;\"><span class=\"ltx_rule\" style=\"width:0.0pt;height:12.9pt;background:black;display:inline-block;\"></span><span id=\"S4.T3.1.11.10.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Text Representation</span></span>\n</span>\n</td>\n<td id=\"S4.T3.1.11.10.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"S4.T3.1.11.10.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.11.10.2.1.1\" class=\"ltx_p\" style=\"width:156.5pt;\">Represent words, sentences or documents using vectors of real numbers.</span>\n</span>\n</td>\n<td id=\"S4.T3.1.11.10.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t\">\n<span id=\"S4.T3.1.11.10.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.11.10.3.1.1\" class=\"ltx_p\" style=\"width:213.4pt;\">N-gram, Word2Vec, Context2Vec, Doc2Vec, GloVe, FastText</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": []
    },
    "id_table_4": {
        "caption": " Classifying NLP Techniques by Levels of Analysis.",
        "table": "<table id=\"S5.T4.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span id=\"S5.T4.1.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T4.1.1.1.1.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\"><span id=\"S5.T4.1.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Analysis Level</span></span>\n</span>\n</th>\n<th id=\"S5.T4.1.1.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span id=\"S5.T4.1.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T4.1.1.1.2.1.1\" class=\"ltx_p\" style=\"width:213.4pt;\"><span id=\"S5.T4.1.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Explanation</span></span>\n</span>\n</th>\n<th id=\"S5.T4.1.1.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span id=\"S5.T4.1.1.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T4.1.1.1.3.1.1\" class=\"ltx_p\" style=\"width:199.2pt;\"><span id=\"S5.T4.1.1.1.3.1.1.1\" class=\"ltx_text ltx_font_bold\">NLP Techniques</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T4.1.2.1\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.2.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S5.T4.1.2.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T4.1.2.1.1.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\"><span class=\"ltx_rule\" style=\"width:0.0pt;height:12.9pt;background:black;display:inline-block;\"></span><span id=\"S5.T4.1.2.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Morphology</span></span>\n</span>\n</td>\n<td id=\"S5.T4.1.2.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S5.T4.1.2.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T4.1.2.1.2.1.1\" class=\"ltx_p\" style=\"width:213.4pt;\">This is the lowest level of text analysis, dealing with the smallest parts of words that carry meaning. All the techniques used for text normalization belong to this category.</span>\n</span>\n</td>\n<td id=\"S5.T4.1.2.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S5.T4.1.2.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T4.1.2.1.3.1.1\" class=\"ltx_p\" style=\"width:199.2pt;\">Stemming, Lemmatization, Stop-Word Removal, Noise Removal, Punctuation Removal, Lowercasing, Camel Case Splitting</span>\n</span>\n</td>\n</tr>\n<tr id=\"S5.T4.1.3.2\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.3.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S5.T4.1.3.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T4.1.3.2.1.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\"><span class=\"ltx_rule\" style=\"width:0.0pt;height:12.9pt;background:black;display:inline-block;\"></span><span id=\"S5.T4.1.3.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Lexical</span></span>\n</span>\n</td>\n<td id=\"S5.T4.1.3.2.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S5.T4.1.3.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T4.1.3.2.2.1.1\" class=\"ltx_p\" style=\"width:213.4pt;\">This is the word-level of text analysis, interpreting the meaning of individual words to gain word-level understanding. All the techniques used for frequency analysis belong to this category. In addition, Tokenization and n-gram should also be in this category.</span>\n</span>\n</td>\n<td id=\"S5.T4.1.3.2.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S5.T4.1.3.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T4.1.3.2.3.1.1\" class=\"ltx_p\" style=\"width:199.2pt;\">BOW, TF, TF-IDF, Co-location Analysis, Term-Document Matrix, Character Counting, Concordance, n-gram</span>\n</span>\n</td>\n</tr>\n<tr id=\"S5.T4.1.4.3\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.4.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S5.T4.1.4.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T4.1.4.3.1.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\"><span class=\"ltx_rule\" style=\"width:0.0pt;height:12.9pt;background:black;display:inline-block;\"></span><span id=\"S5.T4.1.4.3.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Syntactic</span></span>\n</span>\n</td>\n<td id=\"S5.T4.1.4.3.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S5.T4.1.4.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T4.1.4.3.2.1.1\" class=\"ltx_p\" style=\"width:213.4pt;\">This level focuses on analyzing\nthe words in a sentence through the grammatical structure of the sentence. All the techniques used for syntactic analysis belong to this category. In addition, the techniques used for text segmentation and Regular Expression for Rule-Based Analysis should also belong to this category.</span>\n</span>\n</td>\n<td id=\"S5.T4.1.4.3.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S5.T4.1.4.3.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T4.1.4.3.3.1.1\" class=\"ltx_p\" style=\"width:199.2pt;\">POS Tagging, Dependency Parsing, Constituency Parsing, Link Grammar, Regular Expression, Tokenization, Sentence Segmentation</span>\n</span>\n</td>\n</tr>\n<tr id=\"S5.T4.1.5.4\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.5.4.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S5.T4.1.5.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T4.1.5.4.1.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\"><span class=\"ltx_rule\" style=\"width:0.0pt;height:12.9pt;background:black;display:inline-block;\"></span></span>\n<span id=\"S5.T4.1.5.4.1.1.2\" class=\"ltx_p ltx_align_left\"><span id=\"S5.T4.1.5.4.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Semantic (Word-Level)</span></span>\n</span>\n</td>\n<td id=\"S5.T4.1.5.4.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S5.T4.1.5.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T4.1.5.4.2.1.1\" class=\"ltx_p\" style=\"width:213.4pt;\">We split the semantic level into word-level semantic and sentence-level semantic. The word-level semantic focuses on the meanings of individual words (e.g., dictionary definitions of words and word-sense disambiguation). Most techniques used for semantic tagging and similarly analysis belong to this level. In addition, apart from n-gram, the techniques used for text representation belong to this category.</span>\n</span>\n</td>\n<td id=\"S5.T4.1.5.4.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S5.T4.1.5.4.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T4.1.5.4.3.1.1\" class=\"ltx_p\" style=\"width:199.2pt;\">Term Extraction, Keyword Searching, Chunking, NER, Temporal Tagging, Lexical Patterns, Cosine Similarity, Lexical Affinity, Similarity Distance, Document Similarity, Lexical Similarity, Word2Vec, Context2Vec, Doc2Vec, GloVe, FastText</span>\n</span>\n</td>\n</tr>\n<tr id=\"S5.T4.1.6.5\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.6.5.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S5.T4.1.6.5.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T4.1.6.5.1.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\"><span class=\"ltx_rule\" style=\"width:0.0pt;height:12.9pt;background:black;display:inline-block;\"></span></span>\n<span id=\"S5.T4.1.6.5.1.1.2\" class=\"ltx_p ltx_align_left\"><span id=\"S5.T4.1.6.5.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Semantic (Sentence-Level)</span></span>\n</span>\n</td>\n<td id=\"S5.T4.1.6.5.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S5.T4.1.6.5.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T4.1.6.5.2.1.1\" class=\"ltx_p\" style=\"width:213.4pt;\">This level deals with the compositional semantics, which looks at the interactions among word-level meanings in sentences (e.g., semantic role labeling). Most techniques used for semantic analysis belong to this category. In addition, SRL and most techniques for disambiguation should also belong to this category.</span>\n</span>\n</td>\n<td id=\"S5.T4.1.6.5.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S5.T4.1.6.5.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T4.1.6.5.3.1.1\" class=\"ltx_p\" style=\"width:199.2pt;\">Semantic Parsing, Sentiment Analysis, Text Annotation, Semantic Annotation, Topic Modelling, SRL, Summarization, LDA, LSI, Semantic Patterns, Case Grammar, Semantic Frames, Knowledge Graph, TER, Homonym Detection, Synonymy Detection</span>\n</span>\n</td>\n</tr>\n<tr id=\"S5.T4.1.7.6\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.7.6.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"S5.T4.1.7.6.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T4.1.7.6.1.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\"><span class=\"ltx_rule\" style=\"width:0.0pt;height:12.9pt;background:black;display:inline-block;\"></span><span id=\"S5.T4.1.7.6.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Discourse</span></span>\n</span>\n</td>\n<td id=\"S5.T4.1.7.6.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"S5.T4.1.7.6.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T4.1.7.6.2.1.1\" class=\"ltx_p\" style=\"width:213.4pt;\">This level focuses on the properties of the text as a whole that convey meaning by making connections between component sentences. Only three techniques belong to this category.</span>\n</span>\n</td>\n<td id=\"S5.T4.1.7.6.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t\">\n<span id=\"S5.T4.1.7.6.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T4.1.7.6.3.1.1\" class=\"ltx_p\" style=\"width:199.2pt;\">Coreference Resolution, Anaphora Resolution, Generation Rules</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": []
    }
}