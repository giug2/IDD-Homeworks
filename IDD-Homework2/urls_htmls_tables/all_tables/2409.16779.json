{
    "id_table_1": {
        "caption": "Table 1:  Accuracy scores of the models on 1000 samples of the test set.",
        "table": "S3.T1.1",
        "footnotes": [],
        "references": [
            "Our approach consisted of performing SFT training on both Mistral-7B and LLaMa-3-8B. We then compared the performance of two models with SFT and DPO training and proceeded with LLaMa-3-8B which performed better on the evaluation set (Figure  1 ). In this section, we outline the details of the models and their fine-tuning strategy with emphasis on LLaMa-3-8B.",
            "The training pipeline for LLaMa-3-8B model is demonstrated in Figure  1 . We first performed Supervised Fine-tuning on a mix of specialized maths and science datasets. We then performed DPO training using preference data generated and annotated by students via cDPO loss. Finally, we gauged the performance of the model on  AQuA-Rat   Ling et al. ( 2017 )  dataset which contains STEM-related MCQ questions.",
            "Mistral-7B used the same process as LLaMa, except for the final SFT, since LLaMa showed superior performance (Figure  1 ). Ultimately, we have not implemented both due to time constraints.",
            "The results of supervised fine-tuning of two models are demonstrated in Table  1 .",
            "We augment LLaMa-SciQ by incorporating Retrieval-Augmented Generation (RAG) methods. This approach stands out as one of the most effective means to enhance the predictive capabilities of our model. RAG combines the capabilities of generative models, dense vector indices of a racorpus of documents, and pre-trained neural retrievers.  Figure  2  summarizes our RAG pipeline, known as the  Naive RAG .  We use the dataset described in  4.1.4  as our Dense Passage Retrieval (DPR) corpus of documents over which we create an index using Facebooks FAISS library  Douze et al. ( 2024 ) . Documents are then retrieved using Facebooks DPR question encoder  Karpukhin et al. ( 2020 )  and added to the prompt in the format detailed in Appendix  D.1 .",
            "For the DPO training procedure, we split the DPO dataset described in Section  4.1.2  into 45,000 samples for training and the remaining for testing. The hyperparameter settings are described in Table  9 .",
            "Table  10  presents the MCQA accuracy of each configuration tested on a 10-subset of the EPFL MCQA dataset"
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Dataset sizes and their ratios of the SFT dataset",
        "table": "S4.T2.1",
        "footnotes": [],
        "references": [
            "We augment LLaMa-SciQ by incorporating Retrieval-Augmented Generation (RAG) methods. This approach stands out as one of the most effective means to enhance the predictive capabilities of our model. RAG combines the capabilities of generative models, dense vector indices of a racorpus of documents, and pre-trained neural retrievers.  Figure  2  summarizes our RAG pipeline, known as the  Naive RAG .  We use the dataset described in  4.1.4  as our Dense Passage Retrieval (DPR) corpus of documents over which we create an index using Facebooks FAISS library  Douze et al. ( 2024 ) . Documents are then retrieved using Facebooks DPR question encoder  Karpukhin et al. ( 2020 )  and added to the prompt in the format detailed in Appendix  D.1 .",
            "We first introduce  StemQA , a specialized dataset to extend our models performance on math and coding questions. This dataset is a blend of  MetaMathQA   Yu et al. ( 2023 )  and  CodeFeedback-Filtered-Instruction   Zheng et al. ( 2024 )  datasets. It is balanced so that 75% of the questions are math-related, while the remaining 25% are coding-related. Table  2  presents these proportions. The answers now include the rationale followed by  \"The answer is: <Maths/Code>\"  to simplify future answer extraction.",
            "We conduct two SFT sessions for each model. The best models are selected from the 100,000-sample-size runs, showing the best results in the generation (see an example in  D.2 ).",
            "For the DPO training procedure, we split the DPO dataset described in Section  4.1.2  into 45,000 samples for training and the remaining for testing. The hyperparameter settings are described in Table  9 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Dataset sizes and their ratios of the DPO dataset",
        "table": "S4.T3.1",
        "footnotes": [],
        "references": [
            "Then, we introduce  StemDPO , a dataset to align our model with human preferences, focusing particularly on STEM questions. This dataset combines our class preference pairs with the  PyDPO  and  MetaMathDPO  datasets. Our objective was to expand this dataset to a size of 50,000 samples, maintaining the same distribution proportions as the SFT dataset, assuming our class preferences are similarly balanced (see Table  3 ).",
            "We compare LLaMa-SciQ with the candidate base models:  LLaMa-3-8B   AI ( 2024 )  and  Mistral-7B   Jiang et al. ( 2023 ) . At each step of the training pipeline (described in Section  3 ), we conduct ablation studies by comparing the newly trained model with the model from the previous step.",
            "Finally, using the same hyperparameter setup as in the first SFT sessions, we perform the final SFT training for MCQA specialization using 97,500 MCQ samples. Figure  3  presents the training loss of the kept run."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Dataset sizes and their ratios of the MCQ dataset",
        "table": "S4.T4.1",
        "footnotes": [],
        "references": [
            "We augment LLaMa-SciQ by incorporating Retrieval-Augmented Generation (RAG) methods. This approach stands out as one of the most effective means to enhance the predictive capabilities of our model. RAG combines the capabilities of generative models, dense vector indices of a racorpus of documents, and pre-trained neural retrievers.  Figure  2  summarizes our RAG pipeline, known as the  Naive RAG .  We use the dataset described in  4.1.4  as our Dense Passage Retrieval (DPR) corpus of documents over which we create an index using Facebooks FAISS library  Douze et al. ( 2024 ) . Documents are then retrieved using Facebooks DPR question encoder  Karpukhin et al. ( 2020 )  and added to the prompt in the format detailed in Appendix  D.1 .",
            "We present  StemMCQ , a modified version of the well-known  AQuA-RAT  dataset  Ling et al. ( 2017 ) , specifically designed to align the model with its primary purpose: answering STEM multiple-choice questions. The answers include the  AQuA-RAT  rationale followed by our extraction flag:  \"The answer is: <MCQ Letter>\" . We chose to include the rationale in our responses, as the Chain-of-Thought approach has demonstrated improved results compared to simply providing the answer  Wei et al. ( 2022 ) . Table  4  presents the dataset size.",
            "For the DPO training procedure, we split the DPO dataset described in Section  4.1.2  into 45,000 samples for training and the remaining for testing. The hyperparameter settings are described in Table  9 .",
            "Figure  4  presents the most important training metrics values of the best Maths-SFT runs of each model."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Performance comparison of different models on MATH benchmark",
        "table": "S4.T5.1",
        "footnotes": [],
        "references": [
            "To generate answers for the dataset of questions in mathematics, physics, and computer science, we have developed a prompting strategy that incorporates several techniques. Firstly, we create a separate chat for each subject id. Secondly, we use Chain-of-Thought (CoT)  Wei et al. ( 2022 ) , which guides the model to reach conclusions in a step-by-step manner. Thirdly, the model is prompted with the instruction provided in  B.5 . Finally, for generating preference pairs, we employ the following method: to achieve a better response, we prompt the model to re-read the question before attempting to solve it. This methods was shown by  Xu et al. ( 2024 )  to consistently improve performance for LLMs, except for Vanilla ChatGPT. For the worst answer, the model is instructed to provide a very brief explanation.",
            "The intermediate and final results can be found in Table  6 , Table  5 , and Table  7 .",
            "Figure  5  presents the training metrics of the MCQ-SFT."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Performance comparison of different models on GSM8k benchmark",
        "table": "S4.T6.1",
        "footnotes": [],
        "references": [
            "The intermediate and final results can be found in Table  6 , Table  5 , and Table  7 ."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Models performance on MCQA EPFL benchmark",
        "table": "S4.T7.1",
        "footnotes": [],
        "references": [
            "The intermediate and final results can be found in Table  6 , Table  5 , and Table  7 .",
            "We noted that our model exhibits reasonable generation capabilities and demonstrates sound reasoning when answering questions. During our SFT and DPO training, which frequently involved mathematical questions, our model proved particularly adept at handling them. However, as the benchmark (Table  7 ) included questions from a wide range of disciplines, the results were generally acceptable."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Accuracy of Different Sampling Methods on the 10-sample of EFPL MCQA",
        "table": "S5.T8.1",
        "footnotes": [],
        "references": [
            "We believe our quantized model maintained the accuracy of the LLaMa-SciQ model, as it occasionally achieved higher accuracy in our tests. During development, we experimented with various configuration settings, including adjustments to the temperature, to optimize performance. Table  8  presents the best results of the generation tested on a 10-subsample of the EPFL MCQ dataset; the full test results are presented in Appendix  E ). Despite these efforts, we think the generation configuration could still benefit from fine-tuning. With a large beam size in the beam search, the quantized models performance was comparable to that of LLaMa-SciQ. However, due to resource constraints, we reduced the beam size to 1 for our final benchmark."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  SFT and DPO Hyperparameters",
        "table": "A3.T9.1",
        "footnotes": [],
        "references": [
            "Therefore, the SFT hyperparameters (see Table  9  in the appendix) are chosen based on the state-of-the-art SFT of the models. For similar reasons, we train our models using two relatively small, random sample sizes from the full SFT dataset (described in Section 2.1): 10,000 and 100,000 examples.",
            "For the DPO training procedure, we split the DPO dataset described in Section  4.1.2  into 45,000 samples for training and the remaining for testing. The hyperparameter settings are described in Table  9 .",
            "Table  9  presents the hyperparameters that we used for each training."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:  Accuracy of Different Sampling Methods on 10-sample of EFPL MCQA",
        "table": "A5.T10.1",
        "footnotes": [],
        "references": [
            "Table  10  presents the MCQA accuracy of each configuration tested on a 10-subset of the EPFL MCQA dataset"
        ]
    },
    "global_footnotes": [
        "See the RUG",
        "."
    ]
}