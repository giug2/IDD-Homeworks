{
    "S2.T1.2": {
        "caption": [
            "Table 1",
            "Data collection mediums."
        ],
        "table": "<table id=\"S2.T1.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S2.T1.2.1.1\" class=\"ltx_tr\">\n<th id=\"S2.T1.2.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T1.2.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.2.1.1.1.1.1\" class=\"ltx_p\" style=\"width:78.0pt;\"><span id=\"S2.T1.2.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Medium</span></span>\n</span>\n</th>\n<th id=\"S2.T1.2.1.1.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T1.2.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.2.1.1.2.1.1\" class=\"ltx_p\" style=\"width:329.5pt;\"><span id=\"S2.T1.2.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Definition</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S2.T1.2.2.1\" class=\"ltx_tr\">\n<td id=\"S2.T1.2.2.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T1.2.2.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.2.2.1.1.1.1\" class=\"ltx_p\" style=\"width:78.0pt;\">Video</span>\n</span>\n</td>\n<td id=\"S2.T1.2.2.1.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T1.2.2.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.2.2.1.2.1.1\" class=\"ltx_p\" style=\"width:329.5pt;\">Sequences of image frames captured from a camera source <cite class=\"ltx_cite ltx_citemacro_citep\">(Pham and Wang, <a href=\"#bib.bib118\" title=\"\" class=\"ltx_ref\">2018</a>; Emerson et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib56\" title=\"\" class=\"ltx_ref\">2020b</a>; Closser et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.2.3.2\" class=\"ltx_tr\">\n<td id=\"S2.T1.2.3.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T1.2.3.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.2.3.2.1.1.1\" class=\"ltx_p\" style=\"width:78.0pt;\">Audio</span>\n</span>\n</td>\n<td id=\"S2.T1.2.3.2.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T1.2.3.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.2.3.2.2.1.1\" class=\"ltx_p\" style=\"width:329.5pt;\">Audio signals captured by a microphone <cite class=\"ltx_cite ltx_citemacro_citep\">(Tanaka et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib144\" title=\"\" class=\"ltx_ref\">2017</a>; Petukhova et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib116\" title=\"\" class=\"ltx_ref\">2017b</a>, <a href=\"#bib.bib115\" title=\"\" class=\"ltx_ref\">a</a>)</cite>.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.2.4.3\" class=\"ltx_tr\">\n<td id=\"S2.T1.2.4.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T1.2.4.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.2.4.3.1.1.1\" class=\"ltx_p\" style=\"width:78.0pt;\">Screen Recording</span>\n</span>\n</td>\n<td id=\"S2.T1.2.4.3.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T1.2.4.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.2.4.3.2.1.1\" class=\"ltx_p\" style=\"width:329.5pt;\">Sequences of image frames displaying a device’s screen contents <cite class=\"ltx_cite ltx_citemacro_citep\">(Alyuz et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">2017</a>; Liu et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib87\" title=\"\" class=\"ltx_ref\">2018b</a>; Jiang et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib75\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.2.5.4\" class=\"ltx_tr\">\n<td id=\"S2.T1.2.5.4.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T1.2.5.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.2.5.4.1.1.1\" class=\"ltx_p\" style=\"width:78.0pt;\">Eye</span>\n</span>\n</td>\n<td id=\"S2.T1.2.5.4.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T1.2.5.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.2.5.4.2.1.1\" class=\"ltx_p\" style=\"width:329.5pt;\">Eye movement data and gaze points captured by tracking devices <cite class=\"ltx_cite ltx_citemacro_citep\">(Chango et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">2021b</a>; Tancredi et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib145\" title=\"\" class=\"ltx_ref\">2022</a>; Papamitsiou et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib113\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.2.6.5\" class=\"ltx_tr\">\n<td id=\"S2.T1.2.6.5.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T1.2.6.5.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.2.6.5.1.1.1\" class=\"ltx_p\" style=\"width:78.0pt;\">Logs</span>\n</span>\n</td>\n<td id=\"S2.T1.2.6.5.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T1.2.6.5.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.2.6.5.2.1.1\" class=\"ltx_p\" style=\"width:329.5pt;\">Participant’s actions within the system and its state data <cite class=\"ltx_cite ltx_citemacro_citep\">(Psaltis et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib121\" title=\"\" class=\"ltx_ref\">2018</a>; Spikol et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib137\" title=\"\" class=\"ltx_ref\">2017a</a>; Azcona et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">2018</a>)</cite>.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.2.7.6\" class=\"ltx_tr\">\n<td id=\"S2.T1.2.7.6.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T1.2.7.6.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.2.7.6.1.1.1\" class=\"ltx_p\" style=\"width:78.0pt;\">Sensor</span>\n</span>\n</td>\n<td id=\"S2.T1.2.7.6.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T1.2.7.6.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.2.7.6.2.1.1\" class=\"ltx_p\" style=\"width:329.5pt;\">Specialized sensors used to gather participants’ physiological data <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib88\" title=\"\" class=\"ltx_ref\">2018a</a>; Henderson et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib70\" title=\"\" class=\"ltx_ref\">2019</a>; Järvelä et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib76\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.2.8.7\" class=\"ltx_tr\">\n<td id=\"S2.T1.2.8.7.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T1.2.8.7.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.2.8.7.1.1.1\" class=\"ltx_p\" style=\"width:78.0pt;\">Interview</span>\n</span>\n</td>\n<td id=\"S2.T1.2.8.7.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T1.2.8.7.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.2.8.7.2.1.1\" class=\"ltx_p\" style=\"width:329.5pt;\">Structured or unstructured conversations between researchers and participants <cite class=\"ltx_cite ltx_citemacro_citep\">(Birt et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">2018</a>; Mat Sanusi et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib96\" title=\"\" class=\"ltx_ref\">2021</a>; Noël et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib106\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.2.9.8\" class=\"ltx_tr\">\n<td id=\"S2.T1.2.9.8.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T1.2.9.8.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.2.9.8.1.1.1\" class=\"ltx_p\" style=\"width:78.0pt;\">Survey</span>\n</span>\n</td>\n<td id=\"S2.T1.2.9.8.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T1.2.9.8.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.2.9.8.2.1.1\" class=\"ltx_p\" style=\"width:329.5pt;\">Standardized sets of questions administered to participants <cite class=\"ltx_cite ltx_citemacro_citep\">(Pham and Wang, <a href=\"#bib.bib117\" title=\"\" class=\"ltx_ref\">2017</a>; Cornide-Reyes et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">2019</a>; Cukurova et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib44\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.2.10.9\" class=\"ltx_tr\">\n<td id=\"S2.T1.2.10.9.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T1.2.10.9.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.2.10.9.1.1.1\" class=\"ltx_p\" style=\"width:78.0pt;\">Participant-Produced Artifacts</span>\n</span>\n</td>\n<td id=\"S2.T1.2.10.9.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T1.2.10.9.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.2.10.9.2.1.1\" class=\"ltx_p\" style=\"width:329.5pt;\">Materials produced by study participants using various mediums, including physical objects created for a task or written responses to formative assessment questions <cite class=\"ltx_cite ltx_citemacro_citep\">(Ashwin and Guddeti, <a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">2020</a>; Chango et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">2021a</a>; Ochoa and Dominguez, <a href=\"#bib.bib107\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.2.11.10\" class=\"ltx_tr\">\n<td id=\"S2.T1.2.11.10.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T1.2.11.10.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.2.11.10.1.1.1\" class=\"ltx_p\" style=\"width:78.0pt;\">Researcher-Produced Artifacts</span>\n</span>\n</td>\n<td id=\"S2.T1.2.11.10.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T1.2.11.10.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.2.11.10.2.1.1\" class=\"ltx_p\" style=\"width:329.5pt;\">Materials produced by the researchers that contribute to analysis and findings, such as observational notes <cite class=\"ltx_cite ltx_citemacro_citep\">(Henderson et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib70\" title=\"\" class=\"ltx_ref\">2019</a>; Standen et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib140\" title=\"\" class=\"ltx_ref\">2020</a>; Martinez-Maldonado et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib94\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.2.12.11\" class=\"ltx_tr\">\n<td id=\"S2.T1.2.12.11.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T1.2.12.11.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.2.12.11.1.1.1\" class=\"ltx_p\" style=\"width:78.0pt;\">Motion</span>\n</span>\n</td>\n<td id=\"S2.T1.2.12.11.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T1.2.12.11.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.2.12.11.2.1.1\" class=\"ltx_p\" style=\"width:329.5pt;\">Raw motion data collected via various different devices/technologies <cite class=\"ltx_cite ltx_citemacro_citep\">(Vujovic et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib156\" title=\"\" class=\"ltx_ref\">2020</a>; Mat Sanusi et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib96\" title=\"\" class=\"ltx_ref\">2021</a>; Di Mitri et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib52\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.2.13.12\" class=\"ltx_tr\">\n<td id=\"S2.T1.2.13.12.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T1.2.13.12.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.2.13.12.1.1.1\" class=\"ltx_p\" style=\"width:78.0pt;\">Text</span>\n</span>\n</td>\n<td id=\"S2.T1.2.13.12.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T1.2.13.12.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.2.13.12.2.1.1\" class=\"ltx_p\" style=\"width:329.5pt;\">Raw textual input <cite class=\"ltx_cite ltx_citemacro_citep\">(Worsley et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib160\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Current learning and training environments use several computational measures of performance and behaviors such as evaluating learning gains, establishing and progressing toward desired objectives, and employing effective plans of action to achieve these objectives. Multimodal data can provide the basis for computing these measures, ranging from logs and surveys to analyses of student artifacts. A diverse array of data collection mediums plays a pivotal role in gaining a comprehensive understanding of learners’ progress, interactions, strategies, and struggles within these environments. The mediums listed in Table 1 (and all definitions in Section 2.2) were identified through our qualitative analysis of the corpus."
        ]
    }
}{
    "S2.T2.2": {
        "caption": [
            "Table 2",
            "Modalities, their definitions, and the modality groups they fall into (detailed in Section 4.2).",
            "4.2"
        ],
        "table": "<table id=\"S2.T2.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S2.T2.2.1.1\" class=\"ltx_tr\">\n<th id=\"S2.T2.2.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.1.1.1.1.1\" class=\"ltx_p\" style=\"width:65.0pt;\"><span id=\"S2.T2.2.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Modality</span></span>\n</span>\n</th>\n<th id=\"S2.T2.2.1.1.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.1.1.2.1.1\" class=\"ltx_p\"><span id=\"S2.T2.2.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Description</span></span>\n</span>\n</th>\n<th id=\"S2.T2.2.1.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.1.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.1.1.3.1.1\" class=\"ltx_p\" style=\"width:70.2pt;\"><span id=\"S2.T2.2.1.1.3.1.1.1\" class=\"ltx_text ltx_font_bold\">Modality Group</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S2.T2.2.2.1\" class=\"ltx_tr\">\n<td id=\"S2.T2.2.2.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.2.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.2.1.1.1.1\" class=\"ltx_p\" style=\"width:65.0pt;\">Affect</span>\n</span>\n</td>\n<td id=\"S2.T2.2.2.1.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.2.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.2.1.2.1.1\" class=\"ltx_p\">Participant’s emotional or affective state <cite class=\"ltx_cite ltx_citemacro_citep\">(Psaltis et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib121\" title=\"\" class=\"ltx_ref\">2018</a>; Di Mitri et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">2017</a>; Tanaka et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib144\" title=\"\" class=\"ltx_ref\">2017</a>)</cite>.</span>\n</span>\n</td>\n<td id=\"S2.T2.2.2.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.2.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.2.1.3.1.1\" class=\"ltx_p\" style=\"width:70.2pt;\">NLP, Vision, Sensor</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T2.2.3.2\" class=\"ltx_tr\">\n<td id=\"S2.T2.2.3.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.3.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.3.2.1.1.1\" class=\"ltx_p\" style=\"width:65.0pt;\">Pose</span>\n</span>\n</td>\n<td id=\"S2.T2.2.3.2.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.3.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.3.2.2.1.1\" class=\"ltx_p\">Participant’s physical position, location, or body posture <cite class=\"ltx_cite ltx_citemacro_citep\">(Alyuz et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">2017</a>; Spikol et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib138\" title=\"\" class=\"ltx_ref\">2018</a>; Starr et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib141\" title=\"\" class=\"ltx_ref\">2018</a>)</cite>.</span>\n</span>\n</td>\n<td id=\"S2.T2.2.3.2.3\" class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.3.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.3.2.3.1.1\" class=\"ltx_p\" style=\"width:70.2pt;\">Vision, Sensor</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T2.2.4.3\" class=\"ltx_tr\">\n<td id=\"S2.T2.2.4.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.4.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.4.3.1.1.1\" class=\"ltx_p\" style=\"width:65.0pt;\">Gesture</span>\n</span>\n</td>\n<td id=\"S2.T2.2.4.3.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.4.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.4.3.2.1.1\" class=\"ltx_p\">Participant’s gestures and body language <cite class=\"ltx_cite ltx_citemacro_citep\">(Andrade, <a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">2017</a>; Worsley and Blikstein, <a href=\"#bib.bib159\" title=\"\" class=\"ltx_ref\">2018</a>; Petukhova et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib116\" title=\"\" class=\"ltx_ref\">2017b</a>)</cite>.</span>\n</span>\n</td>\n<td id=\"S2.T2.2.4.3.3\" class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.4.3.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.4.3.3.1.1\" class=\"ltx_p\" style=\"width:70.2pt;\">Vision</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T2.2.5.4\" class=\"ltx_tr\">\n<td id=\"S2.T2.2.5.4.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.5.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.5.4.1.1.1\" class=\"ltx_p\" style=\"width:65.0pt;\">Activity</span>\n</span>\n</td>\n<td id=\"S2.T2.2.5.4.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.5.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.5.4.2.1.1\" class=\"ltx_p\">Participant’s observable actions or activities <cite class=\"ltx_cite ltx_citemacro_citep\">(Fwa, Hua Leong and Lindsay Marshall, <a href=\"#bib.bib63\" title=\"\" class=\"ltx_ref\">2018</a>; Prieto et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib120\" title=\"\" class=\"ltx_ref\">2018</a>; Liu et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib87\" title=\"\" class=\"ltx_ref\">2018b</a>)</cite>.</span>\n</span>\n</td>\n<td id=\"S2.T2.2.5.4.3\" class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.5.4.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.5.4.3.1.1\" class=\"ltx_p\" style=\"width:70.2pt;\">Vision, Sensor</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T2.2.6.5\" class=\"ltx_tr\">\n<td id=\"S2.T2.2.6.5.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.6.5.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.6.5.1.1.1\" class=\"ltx_p\" style=\"width:65.0pt;\">Prosodic Speech</span>\n</span>\n</td>\n<td id=\"S2.T2.2.6.5.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.6.5.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.6.5.2.1.1\" class=\"ltx_p\">Elements of speech beyond word meaning, e.g. volume, pauses, and intonation <cite class=\"ltx_cite ltx_citemacro_citep\">(Spikol et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib137\" title=\"\" class=\"ltx_ref\">2017a</a>, <a href=\"#bib.bib139\" title=\"\" class=\"ltx_ref\">b</a>; Noel et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib105\" title=\"\" class=\"ltx_ref\">2018</a>)</cite>.</span>\n</span>\n</td>\n<td id=\"S2.T2.2.6.5.3\" class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.6.5.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.6.5.3.1.1\" class=\"ltx_p\" style=\"width:70.2pt;\">NLP</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T2.2.7.6\" class=\"ltx_tr\">\n<td id=\"S2.T2.2.7.6.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.7.6.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.7.6.1.1.1\" class=\"ltx_p\" style=\"width:65.0pt;\">Transcribed Speech</span>\n</span>\n</td>\n<td id=\"S2.T2.2.7.6.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.7.6.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.7.6.2.1.1\" class=\"ltx_p\">Textual speech transcribed from audio <cite class=\"ltx_cite ltx_citemacro_citep\">(Birt et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">2018</a>; Liu et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib86\" title=\"\" class=\"ltx_ref\">2019</a>; Cornide-Reyes et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>.</span>\n</span>\n</td>\n<td id=\"S2.T2.2.7.6.3\" class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.7.6.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.7.6.3.1.1\" class=\"ltx_p\" style=\"width:70.2pt;\">NLP</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T2.2.8.7\" class=\"ltx_tr\">\n<td id=\"S2.T2.2.8.7.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.8.7.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.8.7.1.1.1\" class=\"ltx_p\" style=\"width:65.0pt;\">Qualitative Observations</span>\n</span>\n</td>\n<td id=\"S2.T2.2.8.7.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.8.7.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.8.7.2.1.1\" class=\"ltx_p\">Researcher observations about the participant and study task <cite class=\"ltx_cite ltx_citemacro_citep\">(Worsley, <a href=\"#bib.bib158\" title=\"\" class=\"ltx_ref\">2018</a>; Martin et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib93\" title=\"\" class=\"ltx_ref\">2019</a>; Järvelä et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib76\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>.</span>\n</span>\n</td>\n<td id=\"S2.T2.2.8.7.3\" class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.8.7.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.8.7.3.1.1\" class=\"ltx_p\" style=\"width:70.2pt;\">Human-centered</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T2.2.9.8\" class=\"ltx_tr\">\n<td id=\"S2.T2.2.9.8.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.9.8.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.9.8.1.1.1\" class=\"ltx_p\" style=\"width:65.0pt;\">Logs</span>\n</span>\n</td>\n<td id=\"S2.T2.2.9.8.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.9.8.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.9.8.2.1.1\" class=\"ltx_p\">Participant’s environment actions and system state data <cite class=\"ltx_cite ltx_citemacro_citep\">(Azcona et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">2018</a>; Mitri, <a href=\"#bib.bib99\" title=\"\" class=\"ltx_ref\">2019</a>; Giannakos et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib66\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>.</span>\n</span>\n</td>\n<td id=\"S2.T2.2.9.8.3\" class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.9.8.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.9.8.3.1.1\" class=\"ltx_p\" style=\"width:70.2pt;\">Logs</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T2.2.10.9\" class=\"ltx_tr\">\n<td id=\"S2.T2.2.10.9.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.10.9.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.10.9.1.1.1\" class=\"ltx_p\" style=\"width:65.0pt;\">Gaze</span>\n</span>\n</td>\n<td id=\"S2.T2.2.10.9.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.10.9.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.10.9.2.1.1\" class=\"ltx_p\">Participant’s eye gaze, e.g., movement, direction and focus <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib163\" title=\"\" class=\"ltx_ref\">2020</a>; Emerson et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib56\" title=\"\" class=\"ltx_ref\">2020b</a>, <a href=\"#bib.bib55\" title=\"\" class=\"ltx_ref\">a</a>)</cite>.</span>\n</span>\n</td>\n<td id=\"S2.T2.2.10.9.3\" class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.10.9.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.10.9.3.1.1\" class=\"ltx_p\" style=\"width:70.2pt;\">Vision, Sensor</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T2.2.11.10\" class=\"ltx_tr\">\n<td id=\"S2.T2.2.11.10.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.11.10.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.11.10.1.1.1\" class=\"ltx_p\" style=\"width:65.0pt;\">Interview</span>\n</span>\n</td>\n<td id=\"S2.T2.2.11.10.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.11.10.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.11.10.2.1.1\" class=\"ltx_p\">Notes from interviews between researchers and participants <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib75\" title=\"\" class=\"ltx_ref\">2021</a>; Aslan et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">2019</a>; Echeverria et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib54\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>.</span>\n</span>\n</td>\n<td id=\"S2.T2.2.11.10.3\" class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.11.10.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.11.10.3.1.1\" class=\"ltx_p\" style=\"width:70.2pt;\">Human-centered</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T2.2.12.11\" class=\"ltx_tr\">\n<td id=\"S2.T2.2.12.11.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.12.11.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.12.11.1.1.1\" class=\"ltx_p\" style=\"width:65.0pt;\">Survey</span>\n</span>\n</td>\n<td id=\"S2.T2.2.12.11.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.12.11.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.12.11.2.1.1\" class=\"ltx_p\">Participant’s responses to surveys/questionnaires <cite class=\"ltx_cite ltx_citemacro_citep\">(Pham and Wang, <a href=\"#bib.bib117\" title=\"\" class=\"ltx_ref\">2017</a>; Petukhova et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib115\" title=\"\" class=\"ltx_ref\">2017a</a>; Papamitsiou et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib113\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>.</span>\n</span>\n</td>\n<td id=\"S2.T2.2.12.11.3\" class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.12.11.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.12.11.3.1.1\" class=\"ltx_p\" style=\"width:70.2pt;\">Human-centered</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T2.2.13.12\" class=\"ltx_tr\">\n<td id=\"S2.T2.2.13.12.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.13.12.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.13.12.1.1.1\" class=\"ltx_p\" style=\"width:65.0pt;\">Pulse</span>\n</span>\n</td>\n<td id=\"S2.T2.2.13.12.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.13.12.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.13.12.2.1.1\" class=\"ltx_p\">The participant’s pulse, indicating their heart rate <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee-Cultura et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib82\" title=\"\" class=\"ltx_ref\">2021</a>, <a href=\"#bib.bib83\" title=\"\" class=\"ltx_ref\">2022</a>; Tisza et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib149\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>.</span>\n</span>\n</td>\n<td id=\"S2.T2.2.13.12.3\" class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.13.12.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.13.12.3.1.1\" class=\"ltx_p\" style=\"width:70.2pt;\">Sensor</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T2.2.14.13\" class=\"ltx_tr\">\n<td id=\"S2.T2.2.14.13.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.14.13.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.14.13.1.1.1\" class=\"ltx_p\" style=\"width:65.0pt;\">EDA</span>\n</span>\n</td>\n<td id=\"S2.T2.2.14.13.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.14.13.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.14.13.2.1.1\" class=\"ltx_p\">Participant’s electrodermal activity <cite class=\"ltx_cite ltx_citemacro_citep\">(Larmuseau et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib81\" title=\"\" class=\"ltx_ref\">2020</a>; Mangaroska et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib92\" title=\"\" class=\"ltx_ref\">2020</a>; Sharma et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib133\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>.</span>\n</span>\n</td>\n<td id=\"S2.T2.2.14.13.3\" class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.14.13.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.14.13.3.1.1\" class=\"ltx_p\" style=\"width:70.2pt;\">Sensor</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T2.2.15.14\" class=\"ltx_tr\">\n<td id=\"S2.T2.2.15.14.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.15.14.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.15.14.1.1.1\" class=\"ltx_p\" style=\"width:65.0pt;\">Temperature</span>\n</span>\n</td>\n<td id=\"S2.T2.2.15.14.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.15.14.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.15.14.2.1.1\" class=\"ltx_p\">Participant’s body temperature <cite class=\"ltx_cite ltx_citemacro_citep\">(Sharma et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib133\" title=\"\" class=\"ltx_ref\">2020</a>; Lee-Cultura et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib84\" title=\"\" class=\"ltx_ref\">2020</a>; Papamitsiou et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib113\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>.</span>\n</span>\n</td>\n<td id=\"S2.T2.2.15.14.3\" class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.15.14.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.15.14.3.1.1\" class=\"ltx_p\" style=\"width:70.2pt;\">Sensor</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T2.2.16.15\" class=\"ltx_tr\">\n<td id=\"S2.T2.2.16.15.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.16.15.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.16.15.1.1.1\" class=\"ltx_p\" style=\"width:65.0pt;\">Blood Pressure</span>\n</span>\n</td>\n<td id=\"S2.T2.2.16.15.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.16.15.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.16.15.2.1.1\" class=\"ltx_p\">Participant’s blood pressure <cite class=\"ltx_cite ltx_citemacro_citep\">(Papamitsiou et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib113\" title=\"\" class=\"ltx_ref\">2020</a>; Lee-Cultura et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib83\" title=\"\" class=\"ltx_ref\">2022</a>; Tisza et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib149\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>.</span>\n</span>\n</td>\n<td id=\"S2.T2.2.16.15.3\" class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.16.15.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.16.15.3.1.1\" class=\"ltx_p\" style=\"width:70.2pt;\">Sensor</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T2.2.17.16\" class=\"ltx_tr\">\n<td id=\"S2.T2.2.17.16.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.17.16.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.17.16.1.1.1\" class=\"ltx_p\" style=\"width:65.0pt;\">EEG</span>\n</span>\n</td>\n<td id=\"S2.T2.2.17.16.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.17.16.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.17.16.2.1.1\" class=\"ltx_p\">Participant’s electroencephalography activity <cite class=\"ltx_cite ltx_citemacro_citep\">(Giannakos et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib66\" title=\"\" class=\"ltx_ref\">2019</a>; Sharma et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib133\" title=\"\" class=\"ltx_ref\">2020</a>; Papamitsiou et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib113\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>.</span>\n</span>\n</td>\n<td id=\"S2.T2.2.17.16.3\" class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.17.16.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.17.16.3.1.1\" class=\"ltx_p\" style=\"width:70.2pt;\">Sensor</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T2.2.18.17\" class=\"ltx_tr\">\n<td id=\"S2.T2.2.18.17.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.18.17.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.18.17.1.1.1\" class=\"ltx_p\" style=\"width:65.0pt;\">Fatigue</span>\n</span>\n</td>\n<td id=\"S2.T2.2.18.17.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.18.17.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.18.17.2.1.1\" class=\"ltx_p\">The level of fatigue experienced during the activity <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee-Cultura et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib82\" title=\"\" class=\"ltx_ref\">2021</a>, <a href=\"#bib.bib83\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>.</span>\n</span>\n</td>\n<td id=\"S2.T2.2.18.17.3\" class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.18.17.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.18.17.3.1.1\" class=\"ltx_p\" style=\"width:70.2pt;\">Vision, Sensor</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T2.2.19.18\" class=\"ltx_tr\">\n<td id=\"S2.T2.2.19.18.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.19.18.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.19.18.1.1.1\" class=\"ltx_p\" style=\"width:65.0pt;\">EMG</span>\n</span>\n</td>\n<td id=\"S2.T2.2.19.18.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.19.18.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.19.18.2.1.1\" class=\"ltx_p\">Participant’s electromyography activity <cite class=\"ltx_cite ltx_citemacro_citep\">(Di Mitri et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib52\" title=\"\" class=\"ltx_ref\">2020</a>, <a href=\"#bib.bib50\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>.</span>\n</span>\n</td>\n<td id=\"S2.T2.2.19.18.3\" class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.19.18.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.19.18.3.1.1\" class=\"ltx_p\" style=\"width:70.2pt;\">Sensor</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T2.2.20.19\" class=\"ltx_tr\">\n<td id=\"S2.T2.2.20.19.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.20.19.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.20.19.1.1.1\" class=\"ltx_p\" style=\"width:65.0pt;\">Participant Produced Artifacts</span>\n</span>\n</td>\n<td id=\"S2.T2.2.20.19.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.20.19.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.20.19.2.1.1\" class=\"ltx_p\">Artifacts produced by the participant during the study, e.g., pre/post-tests <cite class=\"ltx_cite ltx_citemacro_citep\">(Ochoa and Dominguez, <a href=\"#bib.bib107\" title=\"\" class=\"ltx_ref\">2020</a>; Chango et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">2021b</a>; Morell et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib100\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>.</span>\n</span>\n</td>\n<td id=\"S2.T2.2.20.19.3\" class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.20.19.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.20.19.3.1.1\" class=\"ltx_p\" style=\"width:70.2pt;\">Human-centered</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T2.2.21.20\" class=\"ltx_tr\">\n<td id=\"S2.T2.2.21.20.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.21.20.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.21.20.1.1.1\" class=\"ltx_p\" style=\"width:65.0pt;\">Researcher Produced Artifacts</span>\n</span>\n</td>\n<td id=\"S2.T2.2.21.20.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.21.20.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.21.20.2.1.1\" class=\"ltx_p\">Artifacts produced by the researcher about the study and participants, e.g., field notes <cite class=\"ltx_cite ltx_citemacro_citep\">(Closser et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">2022</a>; Fernandez-Nieto et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib58\" title=\"\" class=\"ltx_ref\">2021</a>; Noël et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib106\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>.</span>\n</span>\n</td>\n<td id=\"S2.T2.2.21.20.3\" class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.21.20.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.21.20.3.1.1\" class=\"ltx_p\" style=\"width:70.2pt;\">Human-centered</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T2.2.22.21\" class=\"ltx_tr\">\n<td id=\"S2.T2.2.22.21.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.22.21.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.22.21.1.1.1\" class=\"ltx_p\" style=\"width:65.0pt;\">Spectrogram</span>\n</span>\n</td>\n<td id=\"S2.T2.2.22.21.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.22.21.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.22.21.2.1.1\" class=\"ltx_p\">Representation of audio frequencies in the form of a spectrogram <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib91\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>.</span>\n</span>\n</td>\n<td id=\"S2.T2.2.22.21.3\" class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.22.21.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.22.21.3.1.1\" class=\"ltx_p\" style=\"width:70.2pt;\">NLP</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T2.2.23.22\" class=\"ltx_tr\">\n<td id=\"S2.T2.2.23.22.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.23.22.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.23.22.1.1.1\" class=\"ltx_p\" style=\"width:65.0pt;\">Text</span>\n</span>\n</td>\n<td id=\"S2.T2.2.23.22.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.23.22.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.23.22.2.1.1\" class=\"ltx_p\">Participant’s raw text data generated in the study environment <cite class=\"ltx_cite ltx_citemacro_citep\">(Worsley et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib160\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>.</span>\n</span>\n</td>\n<td id=\"S2.T2.2.23.22.3\" class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.23.22.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.23.22.3.1.1\" class=\"ltx_p\" style=\"width:70.2pt;\">NLP</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T2.2.24.23\" class=\"ltx_tr\">\n<td id=\"S2.T2.2.24.23.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.24.23.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.24.23.1.1.1\" class=\"ltx_p\" style=\"width:65.0pt;\">Pixel</span>\n</span>\n</td>\n<td id=\"S2.T2.2.24.23.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_border_bb ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.24.23.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.24.23.2.1.1\" class=\"ltx_p\">RGB pixel values from cameras or sensors <cite class=\"ltx_cite ltx_citemacro_citep\">(Prieto et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib120\" title=\"\" class=\"ltx_ref\">2018</a>)</cite>.</span>\n</span>\n</td>\n<td id=\"S2.T2.2.24.23.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T2.2.24.23.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.2.24.23.3.1.1\" class=\"ltx_p\" style=\"width:70.2pt;\">Vision</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "We previously defined modalities as unique attributes characterized by one or more data streams, where each modality conveys different information. Table 2 shows several modalities that are used for analyzing and understanding participants’ interactions with and within learning and training environments. In this context, it is important to note that multimodality can arise from a combination of multiple modalities and multiple data streams. For example, the same video data stream could be used to derive both the affect and pose modalities. Similarly, affect can be derived from separate audio and video data streams."
        ]
    }
}{
    "S2.T3.2": {
        "caption": [
            "Table 3",
            "Analysis methods."
        ],
        "table": "<table id=\"S2.T3.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S2.T3.2.1.1\" class=\"ltx_tr\">\n<th id=\"S2.T3.2.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T3.2.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T3.2.1.1.1.1.1\" class=\"ltx_p\" style=\"width:67.2pt;\"><span id=\"S2.T3.2.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Method</span></span>\n</span>\n</th>\n<th id=\"S2.T3.2.1.1.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T3.2.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T3.2.1.1.2.1.1\" class=\"ltx_p\" style=\"width:338.2pt;\"><span id=\"S2.T3.2.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Definition</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S2.T3.2.2.1\" class=\"ltx_tr\">\n<td id=\"S2.T3.2.2.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T3.2.2.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T3.2.2.1.1.1.1\" class=\"ltx_p\" style=\"width:67.2pt;\">Classification</span>\n</span>\n</td>\n<td id=\"S2.T3.2.2.1.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T3.2.2.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T3.2.2.1.2.1.1\" class=\"ltx_p\" style=\"width:338.2pt;\">Assigning pre-defined labels to input data based on feature analysis through supervised learning (often via deep learning approaches) <cite class=\"ltx_cite ltx_citemacro_citep\">(Psaltis et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib121\" title=\"\" class=\"ltx_ref\">2018</a>; Spikol et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib139\" title=\"\" class=\"ltx_ref\">2017b</a>; Alyuz et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">2017</a>)</cite>.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T3.2.3.2\" class=\"ltx_tr\">\n<td id=\"S2.T3.2.3.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T3.2.3.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T3.2.3.2.1.1.1\" class=\"ltx_p\" style=\"width:67.2pt;\">Regression</span>\n</span>\n</td>\n<td id=\"S2.T3.2.3.2.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T3.2.3.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T3.2.3.2.2.1.1\" class=\"ltx_p\" style=\"width:338.2pt;\">Predicting continuous numerical values through supervised learning to understand input-output relationships <cite class=\"ltx_cite ltx_citemacro_citep\">(Spikol et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib137\" title=\"\" class=\"ltx_ref\">2017a</a>; Di Mitri et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">2017</a>; Pham and Wang, <a href=\"#bib.bib118\" title=\"\" class=\"ltx_ref\">2018</a>)</cite>.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T3.2.4.3\" class=\"ltx_tr\">\n<td id=\"S2.T3.2.4.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T3.2.4.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T3.2.4.3.1.1.1\" class=\"ltx_p\" style=\"width:67.2pt;\">Clustering</span>\n</span>\n</td>\n<td id=\"S2.T3.2.4.3.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T3.2.4.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T3.2.4.3.2.1.1\" class=\"ltx_p\" style=\"width:338.2pt;\">Grouping data based on patterns or similarities using unsupervised learning <cite class=\"ltx_cite ltx_citemacro_citep\">(Andrade, <a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">2017</a>; Chan et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib20\" title=\"\" class=\"ltx_ref\">2020</a>; Closser et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T3.2.5.4\" class=\"ltx_tr\">\n<td id=\"S2.T3.2.5.4.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T3.2.5.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T3.2.5.4.1.1.1\" class=\"ltx_p\" style=\"width:67.2pt;\">Qualitative</span>\n</span>\n</td>\n<td id=\"S2.T3.2.5.4.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T3.2.5.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T3.2.5.4.2.1.1\" class=\"ltx_p\" style=\"width:338.2pt;\">Manually examining and interpreting data to uncover patterns or themes <cite class=\"ltx_cite ltx_citemacro_citep\">(Martin et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib93\" title=\"\" class=\"ltx_ref\">2019</a>; Järvelä et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib76\" title=\"\" class=\"ltx_ref\">2021</a>; Jiang et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib75\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T3.2.6.5\" class=\"ltx_tr\">\n<td id=\"S2.T3.2.6.5.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T3.2.6.5.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T3.2.6.5.1.1.1\" class=\"ltx_p\" style=\"width:67.2pt;\">Statistical</span>\n</span>\n</td>\n<td id=\"S2.T3.2.6.5.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T3.2.6.5.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T3.2.6.5.2.1.1\" class=\"ltx_p\" style=\"width:338.2pt;\">Using statistical methods (e.g., correlation) to analyze data and draw conclusions <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib86\" title=\"\" class=\"ltx_ref\">2019</a>; Ochoa and Dominguez, <a href=\"#bib.bib107\" title=\"\" class=\"ltx_ref\">2020</a>; López et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib90\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T3.2.7.6\" class=\"ltx_tr\">\n<td id=\"S2.T3.2.7.6.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T3.2.7.6.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T3.2.7.6.1.1.1\" class=\"ltx_p\" style=\"width:67.2pt;\">Network analysis</span>\n</span>\n</td>\n<td id=\"S2.T3.2.7.6.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T3.2.7.6.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T3.2.7.6.2.1.1\" class=\"ltx_p\" style=\"width:338.2pt;\">Studying relationships and interactions using graph-based approaches <cite class=\"ltx_cite ltx_citemacro_citep\">(Noel et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib105\" title=\"\" class=\"ltx_ref\">2018</a>; Cornide-Reyes et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">2019</a>; Chen, <a href=\"#bib.bib24\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T3.2.8.7\" class=\"ltx_tr\">\n<td id=\"S2.T3.2.8.7.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T3.2.8.7.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T3.2.8.7.1.1.1\" class=\"ltx_p\" style=\"width:67.2pt;\">Pattern Extraction</span>\n</span>\n</td>\n<td id=\"S2.T3.2.8.7.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T3.2.8.7.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T3.2.8.7.2.1.1\" class=\"ltx_p\" style=\"width:338.2pt;\">Identifying meaningful patterns or structures within data, including techniques like Markov analysis and sequence mining <cite class=\"ltx_cite ltx_citemacro_citep\">(Papamitsiou et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib113\" title=\"\" class=\"ltx_ref\">2020</a>; Nguyen et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib103\" title=\"\" class=\"ltx_ref\">2023</a>; Tancredi et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib145\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "We use the term analysis method to refer to specific techniques for deriving insights from multimodal data in learning and training contexts, which vary depending on research goals and data characteristics, and are presented in Table 3. The methods range from supervised and unsupervised techniques (like classification and clustering) to qualitative analyses. More recently, deep learning algorithms have been developed for analyzing multiple data streams (Gao et al., 2020; Giannakos et al., 2022), and reinforcement learning techniques are being developed for educational recommendations (Liu et al., 2018a). Evaluating these methods is essential for understanding current trends in data analysis and informing future research. This review concentrates on the examination and interpretation of the data through these methods and not on the analytical techniques themselves, unless such meta-analysis yields further valuable insights."
        ]
    }
}{
    "S2.T4.2": {
        "caption": [
            "Table 4",
            "Data fusion approaches."
        ],
        "table": "<table id=\"S2.T4.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S2.T4.2.1.1\" class=\"ltx_tr\">\n<th id=\"S2.T4.2.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T4.2.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T4.2.1.1.1.1.1\" class=\"ltx_p\" style=\"width:52.0pt;\"><span id=\"S2.T4.2.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Category</span></span>\n</span>\n</th>\n<th id=\"S2.T4.2.1.1.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T4.2.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T4.2.1.1.2.1.1\" class=\"ltx_p\" style=\"width:338.2pt;\"><span id=\"S2.T4.2.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Description</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S2.T4.2.2.1\" class=\"ltx_tr\">\n<td id=\"S2.T4.2.2.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T4.2.2.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T4.2.2.1.1.1.1\" class=\"ltx_p\" style=\"width:52.0pt;\">Early Fusion</span>\n</span>\n</td>\n<td id=\"S2.T4.2.2.1.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T4.2.2.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T4.2.2.1.2.1.1\" class=\"ltx_p\" style=\"width:338.2pt;\">Draws inferences and computes analytics from multiple sources of raw data at the earliest stage of processing before any modality-specific analysis <cite class=\"ltx_cite ltx_citemacro_citep\">(Worsley and Blikstein, <a href=\"#bib.bib159\" title=\"\" class=\"ltx_ref\">2018</a>; Larmuseau et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib81\" title=\"\" class=\"ltx_ref\">2020</a>; Sümer et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib143\" title=\"\" class=\"ltx_ref\">2023</a>)</cite>.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T4.2.3.2\" class=\"ltx_tr\">\n<td id=\"S2.T4.2.3.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T4.2.3.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T4.2.3.2.1.1.1\" class=\"ltx_p\" style=\"width:52.0pt;\">Mid Fusion</span>\n</span>\n</td>\n<td id=\"S2.T4.2.3.2.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T4.2.3.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T4.2.3.2.2.1.1\" class=\"ltx_p\" style=\"width:338.2pt;\">Represents a compromise that mixes early and late fusion for analysis. Combines processed, observable features generated from individual sources with analysis using other sources of data within the input space <cite class=\"ltx_cite ltx_citemacro_citep\">(Cukurova et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib44\" title=\"\" class=\"ltx_ref\">2019</a>; Emerson et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib56\" title=\"\" class=\"ltx_ref\">2020b</a>, <a href=\"#bib.bib55\" title=\"\" class=\"ltx_ref\">a</a>)</cite>.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T4.2.4.3\" class=\"ltx_tr\">\n<td id=\"S2.T4.2.4.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T4.2.4.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T4.2.4.3.1.1.1\" class=\"ltx_p\" style=\"width:52.0pt;\">Late Fusion</span>\n</span>\n</td>\n<td id=\"S2.T4.2.4.3.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T4.2.4.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T4.2.4.3.2.1.1\" class=\"ltx_p\" style=\"width:338.2pt;\">Analysis is performed on individual modalities, and the inferences generated are combined to generate outcomes at a later stage, i.e., in the hypothesis space <cite class=\"ltx_cite ltx_citemacro_citep\">(Psaltis et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib121\" title=\"\" class=\"ltx_ref\">2018</a>; Pham and Wang, <a href=\"#bib.bib118\" title=\"\" class=\"ltx_ref\">2018</a>; Ochoa et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib108\" title=\"\" class=\"ltx_ref\">2018</a>)</cite>.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T4.2.5.4\" class=\"ltx_tr\">\n<td id=\"S2.T4.2.5.4.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T4.2.5.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T4.2.5.4.1.1.1\" class=\"ltx_p\" style=\"width:52.0pt;\">Hybrid Fusion</span>\n</span>\n</td>\n<td id=\"S2.T4.2.5.4.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T4.2.5.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T4.2.5.4.2.1.1\" class=\"ltx_p\" style=\"width:338.2pt;\">Combines the strengths of both early and late fusion methods. Data from various sources are combined at multiple stages of processing <cite class=\"ltx_cite ltx_citemacro_citep\">(Andrade, <a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">2017</a>; Alyuz et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">2017</a>; Prieto et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib120\" title=\"\" class=\"ltx_ref\">2018</a>)</cite>.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T4.2.6.5\" class=\"ltx_tr\">\n<td id=\"S2.T4.2.6.5.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T4.2.6.5.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T4.2.6.5.1.1.1\" class=\"ltx_p\" style=\"width:52.0pt;\">Other</span>\n</span>\n</td>\n<td id=\"S2.T4.2.6.5.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"S2.T4.2.6.5.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T4.2.6.5.2.1.1\" class=\"ltx_p\" style=\"width:338.2pt;\">Studies that do not fit into the early, mid, late, or hybrid categories, or where the fusion point was not specified or fusion was not performed <cite class=\"ltx_cite ltx_citemacro_citep\">(Martin et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib93\" title=\"\" class=\"ltx_ref\">2019</a>; Järvelä et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib76\" title=\"\" class=\"ltx_ref\">2021</a>; Jiang et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib75\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "For example, a Kinect sensor’s raw pixel or depth data are suitable for early fusion, while joint position data, processed but observable, fit mid fusion. In contrast, inferred constructs like motivation, derived from joint data, align with late fusion. The mid fusion category, while interpretatively flexible, clarifies ambiguities and aids in identifying MMLA sub-communities by their fusion methods. For a detailed definition of observable modalities, see section 2.2.3. Following Chango et al.’s methodology (Chango et al., 2022), we also introduce an other category for studies not conforming to the four primary groups or lacking specified fusion points. These categories are summarized in Table 4 and illustrated in Figure 3."
        ]
    }
}{
    "A1.T5.4": {
        "caption": [
            "Table 5",
            "Each of the 73 works in our corpus."
        ],
        "table": "<table id=\"A1.T5.4\" class=\"ltx_tabular\">\n<tr id=\"A1.T5.4.1\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\">UUID</td>\n<td id=\"A1.T5.4.1.2\" class=\"ltx_td ltx_align_left ltx_border_tt\">First Author</td>\n<td id=\"A1.T5.4.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span id=\"A1.T5.4.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.1.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Title</span>\n</span>\n</td>\n<td id=\"A1.T5.4.1.4\" class=\"ltx_td ltx_align_right ltx_border_tt\">Year</td>\n<td id=\"A1.T5.4.1.5\" class=\"ltx_td ltx_align_left ltx_border_tt\">Publication</td>\n</tr>\n<tr id=\"A1.T5.4.2\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\">2456887548 <cite class=\"ltx_cite ltx_citemacro_citep\">(Alyuz et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">2017</a>)</cite>\n</td>\n<td id=\"A1.T5.4.2.2\" class=\"ltx_td ltx_align_left ltx_border_t\">Alyuz</td>\n<td id=\"A1.T5.4.2.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"A1.T5.4.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.2.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">An Unobtrusive And Multimodal Approach For Behavioral Engagement Detection Of Students</span>\n</span>\n</td>\n<td id=\"A1.T5.4.2.4\" class=\"ltx_td ltx_align_right ltx_border_t\">2017</td>\n<td id=\"A1.T5.4.2.5\" class=\"ltx_td ltx_align_left ltx_border_t\">MIE</td>\n</tr>\n<tr id=\"A1.T5.4.3\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.3.1\" class=\"ltx_td ltx_align_left\">818492192 <cite class=\"ltx_cite ltx_citemacro_citep\">(Andrade, <a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">2017</a>)</cite>\n</td>\n<td id=\"A1.T5.4.3.2\" class=\"ltx_td ltx_align_left\">Andrade</td>\n<td id=\"A1.T5.4.3.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.3.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.3.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Understanding Student Learning Trajectories Using Multimodal Learning Analytics Within An Embodied-Interaction Learning Environment</span>\n</span>\n</td>\n<td id=\"A1.T5.4.3.4\" class=\"ltx_td ltx_align_right\">2017</td>\n<td id=\"A1.T5.4.3.5\" class=\"ltx_td ltx_align_left\">LAK</td>\n</tr>\n<tr id=\"A1.T5.4.4\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.4.1\" class=\"ltx_td ltx_align_left\">3637456466 <cite class=\"ltx_cite ltx_citemacro_citep\">(Ashwin and Guddeti, <a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>\n</td>\n<td id=\"A1.T5.4.4.2\" class=\"ltx_td ltx_align_left\">Ashwin</td>\n<td id=\"A1.T5.4.4.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.4.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.4.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Impact Of Inquiry Interventions On Students In E-Learning And Classroom Environments Using Affective Computing Framework</span>\n</span>\n</td>\n<td id=\"A1.T5.4.4.4\" class=\"ltx_td ltx_align_right\">2020</td>\n<td id=\"A1.T5.4.4.5\" class=\"ltx_td ltx_align_left\">UMUAI</td>\n</tr>\n<tr id=\"A1.T5.4.5\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.5.1\" class=\"ltx_td ltx_align_left\">3448122334 <cite class=\"ltx_cite ltx_citemacro_citep\">(Aslan et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>\n</td>\n<td id=\"A1.T5.4.5.2\" class=\"ltx_td ltx_align_left\">Aslan</td>\n<td id=\"A1.T5.4.5.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.5.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.5.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Investigating The Impact Of A Real-Time, Multimodal Student Engagement Analytics Technology In Authentic Classrooms</span>\n</span>\n</td>\n<td id=\"A1.T5.4.5.4\" class=\"ltx_td ltx_align_right\">2019</td>\n<td id=\"A1.T5.4.5.5\" class=\"ltx_td ltx_align_left\">CHI</td>\n</tr>\n<tr id=\"A1.T5.4.6\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.6.1\" class=\"ltx_td ltx_align_left\">1886134458 <cite class=\"ltx_cite ltx_citemacro_citep\">(Azcona et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">2018</a>)</cite>\n</td>\n<td id=\"A1.T5.4.6.2\" class=\"ltx_td ltx_align_left\">Azcona</td>\n<td id=\"A1.T5.4.6.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.6.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.6.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Personalizing Computer Science Education By Leveraging Multimodal Learning Analytics</span>\n</span>\n</td>\n<td id=\"A1.T5.4.6.4\" class=\"ltx_td ltx_align_right\">2018</td>\n<td id=\"A1.T5.4.6.5\" class=\"ltx_td ltx_align_left\">FIE</td>\n</tr>\n<tr id=\"A1.T5.4.7\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.7.1\" class=\"ltx_td ltx_align_left\">3146393211 <cite class=\"ltx_cite ltx_citemacro_citep\">(Birt et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">2018</a>)</cite>\n</td>\n<td id=\"A1.T5.4.7.2\" class=\"ltx_td ltx_align_left\">Birt</td>\n<td id=\"A1.T5.4.7.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.7.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.7.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Mobile Mixed Reality For Experiential Learning And Simulation In Medical And Health Sciences Education</span>\n</span>\n</td>\n<td id=\"A1.T5.4.7.4\" class=\"ltx_td ltx_align_right\">2018</td>\n<td id=\"A1.T5.4.7.5\" class=\"ltx_td ltx_align_left\">Information</td>\n</tr>\n<tr id=\"A1.T5.4.8\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.8.1\" class=\"ltx_td ltx_align_left\">1326191931 <cite class=\"ltx_cite ltx_citemacro_citep\">(Chan et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib20\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>\n</td>\n<td id=\"A1.T5.4.8.2\" class=\"ltx_td ltx_align_left\">Chan</td>\n<td id=\"A1.T5.4.8.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.8.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.8.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Multimodal Learning Analytics In A Laboratory Classroom</span>\n</span>\n</td>\n<td id=\"A1.T5.4.8.4\" class=\"ltx_td ltx_align_right\">2019</td>\n<td id=\"A1.T5.4.8.5\" class=\"ltx_td ltx_align_left\">MLPALA</td>\n</tr>\n<tr id=\"A1.T5.4.9\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.9.1\" class=\"ltx_td ltx_align_left\">2936220551 <cite class=\"ltx_cite ltx_citemacro_citep\">(Chango et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">2021a</a>)</cite>\n</td>\n<td id=\"A1.T5.4.9.2\" class=\"ltx_td ltx_align_left\">Chango</td>\n<td id=\"A1.T5.4.9.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.9.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.9.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Multi-Source And Multimodal Data Fusion For Predicting Academic Performance In Blended Learning University Courses</span>\n</span>\n</td>\n<td id=\"A1.T5.4.9.4\" class=\"ltx_td ltx_align_right\">2020</td>\n<td id=\"A1.T5.4.9.5\" class=\"ltx_td ltx_align_left\">CEE</td>\n</tr>\n<tr id=\"A1.T5.4.10\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.10.1\" class=\"ltx_td ltx_align_left\">4277812050 <cite class=\"ltx_cite ltx_citemacro_citep\">(Chango et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">2021b</a>)</cite>\n</td>\n<td id=\"A1.T5.4.10.2\" class=\"ltx_td ltx_align_left\">Chango</td>\n<td id=\"A1.T5.4.10.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.10.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.10.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Improving Prediction Of Students’ Performance In Intelligent Tutoring Systems Using Attribute Selection And Ensembles Of Different Multimodal Data Sources</span>\n</span>\n</td>\n<td id=\"A1.T5.4.10.4\" class=\"ltx_td ltx_align_right\">2021</td>\n<td id=\"A1.T5.4.10.5\" class=\"ltx_td ltx_align_left\">JCHE</td>\n</tr>\n<tr id=\"A1.T5.4.11\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.11.1\" class=\"ltx_td ltx_align_left\">1426267857 <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen, <a href=\"#bib.bib24\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>\n</td>\n<td id=\"A1.T5.4.11.2\" class=\"ltx_td ltx_align_left\">Chen</td>\n<td id=\"A1.T5.4.11.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.11.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.11.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Affect, Support, And Personal Factors: Multimodal Causal Models Of One-On-One Coaching</span>\n</span>\n</td>\n<td id=\"A1.T5.4.11.4\" class=\"ltx_td ltx_align_right\">2021</td>\n<td id=\"A1.T5.4.11.5\" class=\"ltx_td ltx_align_left\">JEDM</td>\n</tr>\n<tr id=\"A1.T5.4.12\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.12.1\" class=\"ltx_td ltx_align_left\">3809293172 <cite class=\"ltx_cite ltx_citemacro_citep\">(Closser et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>\n</td>\n<td id=\"A1.T5.4.12.2\" class=\"ltx_td ltx_align_left\">Closser</td>\n<td id=\"A1.T5.4.12.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.12.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.12.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Blending Learning Analytics And Embodied Design To Model Students’ Comprehension Of Measurement Using Their Actions, Speech, And Gestures</span>\n</span>\n</td>\n<td id=\"A1.T5.4.12.4\" class=\"ltx_td ltx_align_right\">2021</td>\n<td id=\"A1.T5.4.12.5\" class=\"ltx_td ltx_align_left\">IJCCI</td>\n</tr>\n<tr id=\"A1.T5.4.13\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.13.1\" class=\"ltx_td ltx_align_left\">4019205162 <cite class=\"ltx_cite ltx_citemacro_citep\">(Cornide-Reyes et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>\n</td>\n<td id=\"A1.T5.4.13.2\" class=\"ltx_td ltx_align_left\">Cornide-Reyes</td>\n<td id=\"A1.T5.4.13.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.13.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.13.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Introducing Low-Cost Sensors Into The Classroom Settings: Improving The Assessment In Agile Practices With Multimodal Learning Analytics</span>\n</span>\n</td>\n<td id=\"A1.T5.4.13.4\" class=\"ltx_td ltx_align_right\">2019</td>\n<td id=\"A1.T5.4.13.5\" class=\"ltx_td ltx_align_left\">Sensors</td>\n</tr>\n<tr id=\"A1.T5.4.14\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.14.1\" class=\"ltx_td ltx_align_left\">1576545447 <cite class=\"ltx_cite ltx_citemacro_citep\">(Cukurova et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib44\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>\n</td>\n<td id=\"A1.T5.4.14.2\" class=\"ltx_td ltx_align_left\">Cukurova</td>\n<td id=\"A1.T5.4.14.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.14.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.14.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Artificial Intelligence And Multimodal Data In The Service Of Human Decision-Making: A Case Study In Debate Tutoring</span>\n</span>\n</td>\n<td id=\"A1.T5.4.14.4\" class=\"ltx_td ltx_align_right\">2019</td>\n<td id=\"A1.T5.4.14.5\" class=\"ltx_td ltx_align_left\">BJET</td>\n</tr>\n<tr id=\"A1.T5.4.15\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.15.1\" class=\"ltx_td ltx_align_left\">1609706685 <cite class=\"ltx_cite ltx_citemacro_citep\">(Di Mitri et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">2017</a>)</cite>\n</td>\n<td id=\"A1.T5.4.15.2\" class=\"ltx_td ltx_align_left\">Di Mitri</td>\n<td id=\"A1.T5.4.15.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.15.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.15.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Learning Pulse: A Machine Learning Approach For Predicting Performance In Self-Regulated Learning Using Multimodal Data</span>\n</span>\n</td>\n<td id=\"A1.T5.4.15.4\" class=\"ltx_td ltx_align_right\">2017</td>\n<td id=\"A1.T5.4.15.5\" class=\"ltx_td ltx_align_left\">LAK</td>\n</tr>\n<tr id=\"A1.T5.4.16\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.16.1\" class=\"ltx_td ltx_align_left\">2070224207 <cite class=\"ltx_cite ltx_citemacro_citep\">(Mitri, <a href=\"#bib.bib99\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>\n</td>\n<td id=\"A1.T5.4.16.2\" class=\"ltx_td ltx_align_left\">Di Mitri</td>\n<td id=\"A1.T5.4.16.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.16.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.16.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Detecting Medical Simulation Errors With Machine Learning And Multimodal Data</span>\n</span>\n</td>\n<td id=\"A1.T5.4.16.4\" class=\"ltx_td ltx_align_right\">2019</td>\n<td id=\"A1.T5.4.16.5\" class=\"ltx_td ltx_align_left\">CAIM</td>\n</tr>\n<tr id=\"A1.T5.4.17\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.17.1\" class=\"ltx_td ltx_align_left\">3009548670 <cite class=\"ltx_cite ltx_citemacro_citep\">(Di Mitri et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib52\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>\n</td>\n<td id=\"A1.T5.4.17.2\" class=\"ltx_td ltx_align_left\">Di Mitri</td>\n<td id=\"A1.T5.4.17.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.17.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.17.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Real-Time Multimodal Feedback With The Cpr Tutor</span>\n</span>\n</td>\n<td id=\"A1.T5.4.17.4\" class=\"ltx_td ltx_align_right\">2020</td>\n<td id=\"A1.T5.4.17.5\" class=\"ltx_td ltx_align_left\">AIED</td>\n</tr>\n<tr id=\"A1.T5.4.18\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.18.1\" class=\"ltx_td ltx_align_left\">1763513559 <cite class=\"ltx_cite ltx_citemacro_citep\">(Di Mitri et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib50\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>\n</td>\n<td id=\"A1.T5.4.18.2\" class=\"ltx_td ltx_align_left\">Di Mitri</td>\n<td id=\"A1.T5.4.18.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.18.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.18.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Keep Me In The Loop: Real-Time Feedback With Multimodal Data</span>\n</span>\n</td>\n<td id=\"A1.T5.4.18.4\" class=\"ltx_td ltx_align_right\">2021</td>\n<td id=\"A1.T5.4.18.5\" class=\"ltx_td ltx_align_left\">IJAIED</td>\n</tr>\n<tr id=\"A1.T5.4.19\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.19.1\" class=\"ltx_td ltx_align_left\">1296637108 <cite class=\"ltx_cite ltx_citemacro_citep\">(Echeverria et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib54\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>\n</td>\n<td id=\"A1.T5.4.19.2\" class=\"ltx_td ltx_align_left\">Echeverria</td>\n<td id=\"A1.T5.4.19.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.19.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.19.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Towards Collaboration Translucence: Giving Meaning To Multimodal Group Data</span>\n</span>\n</td>\n<td id=\"A1.T5.4.19.4\" class=\"ltx_td ltx_align_right\">2019</td>\n<td id=\"A1.T5.4.19.5\" class=\"ltx_td ltx_align_left\">CHI</td>\n</tr>\n<tr id=\"A1.T5.4.20\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.20.1\" class=\"ltx_td ltx_align_left\">1581261659 <cite class=\"ltx_cite ltx_citemacro_citep\">(Emerson et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib56\" title=\"\" class=\"ltx_ref\">2020b</a>)</cite>\n</td>\n<td id=\"A1.T5.4.20.2\" class=\"ltx_td ltx_align_left\">Emerson</td>\n<td id=\"A1.T5.4.20.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.20.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.20.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Early Prediction Of Visitor Engagement In Science Museums With Multimodal Learning Analytics</span>\n</span>\n</td>\n<td id=\"A1.T5.4.20.4\" class=\"ltx_td ltx_align_right\">2020</td>\n<td id=\"A1.T5.4.20.5\" class=\"ltx_td ltx_align_left\">ICMI</td>\n</tr>\n<tr id=\"A1.T5.4.21\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.21.1\" class=\"ltx_td ltx_align_left\">1598166515 <cite class=\"ltx_cite ltx_citemacro_citep\">(Emerson et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib55\" title=\"\" class=\"ltx_ref\">2020a</a>)</cite>\n</td>\n<td id=\"A1.T5.4.21.2\" class=\"ltx_td ltx_align_left\">Emerson</td>\n<td id=\"A1.T5.4.21.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.21.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.21.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Multimodal Learning Analytics For Game-Based Learning</span>\n</span>\n</td>\n<td id=\"A1.T5.4.21.4\" class=\"ltx_td ltx_align_right\">2020</td>\n<td id=\"A1.T5.4.21.5\" class=\"ltx_td ltx_align_left\">BJET</td>\n</tr>\n<tr id=\"A1.T5.4.22\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.22.1\" class=\"ltx_td ltx_align_left\">4035649049 <cite class=\"ltx_cite ltx_citemacro_citep\">(Fernandez-Nieto et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib58\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>\n</td>\n<td id=\"A1.T5.4.22.2\" class=\"ltx_td ltx_align_left\">Fernández-Nieto</td>\n<td id=\"A1.T5.4.22.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.22.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.22.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Storytelling With Learner Data: Guiding Student Reflection On Multimodal Team Data</span>\n</span>\n</td>\n<td id=\"A1.T5.4.22.4\" class=\"ltx_td ltx_align_right\">2021</td>\n<td id=\"A1.T5.4.22.5\" class=\"ltx_td ltx_align_left\">TLT</td>\n</tr>\n<tr id=\"A1.T5.4.23\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.23.1\" class=\"ltx_td ltx_align_left\">483140962 <cite class=\"ltx_cite ltx_citemacro_citep\">(Fwa, Hua Leong and Lindsay Marshall, <a href=\"#bib.bib63\" title=\"\" class=\"ltx_ref\">2018</a>)</cite>\n</td>\n<td id=\"A1.T5.4.23.2\" class=\"ltx_td ltx_align_left\">Fwa</td>\n<td id=\"A1.T5.4.23.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.23.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.23.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Investigating Multimodal Affect Sensing In An Affective Tutoring System Using Unobtrusive Sensors</span>\n</span>\n</td>\n<td id=\"A1.T5.4.23.4\" class=\"ltx_td ltx_align_right\">2018</td>\n<td id=\"A1.T5.4.23.5\" class=\"ltx_td ltx_align_left\">PPIG</td>\n</tr>\n<tr id=\"A1.T5.4.24\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.24.1\" class=\"ltx_td ltx_align_left\">4278392816 <cite class=\"ltx_cite ltx_citemacro_citep\">(Giannakos et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib66\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>\n</td>\n<td id=\"A1.T5.4.24.2\" class=\"ltx_td ltx_align_left\">Giannakos</td>\n<td id=\"A1.T5.4.24.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.24.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.24.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Multimodal Data As A Means To Understand The Learning Experience</span>\n</span>\n</td>\n<td id=\"A1.T5.4.24.4\" class=\"ltx_td ltx_align_right\">2019</td>\n<td id=\"A1.T5.4.24.5\" class=\"ltx_td ltx_align_left\">IJIM</td>\n</tr>\n<tr id=\"A1.T5.4.25\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.25.1\" class=\"ltx_td ltx_align_left\">853680639 <cite class=\"ltx_cite ltx_citemacro_citep\">(Henderson et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib70\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>\n</td>\n<td id=\"A1.T5.4.25.2\" class=\"ltx_td ltx_align_left\">Henderson</td>\n<td id=\"A1.T5.4.25.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.25.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.25.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Sensor-Based Data Fusion For Multimodal Affect Detection In Game-Based Learning Environments</span>\n</span>\n</td>\n<td id=\"A1.T5.4.25.4\" class=\"ltx_td ltx_align_right\">2019</td>\n<td id=\"A1.T5.4.25.5\" class=\"ltx_td ltx_align_left\">EDM</td>\n</tr>\n<tr id=\"A1.T5.4.26\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.26.1\" class=\"ltx_td ltx_align_left\">86191824 <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib75\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>\n</td>\n<td id=\"A1.T5.4.26.2\" class=\"ltx_td ltx_align_left\">Jiang</td>\n<td id=\"A1.T5.4.26.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.26.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.26.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Examining How Different Modes Mediate Adolescents’ Interactions During Their Collaborative Multimodal Composing Processes</span>\n</span>\n</td>\n<td id=\"A1.T5.4.26.4\" class=\"ltx_td ltx_align_right\">2019</td>\n<td id=\"A1.T5.4.26.5\" class=\"ltx_td ltx_align_left\">ILE</td>\n</tr>\n<tr id=\"A1.T5.4.27\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.27.1\" class=\"ltx_td ltx_align_left\">3398902089 <cite class=\"ltx_cite ltx_citemacro_citep\">(Järvelä et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib76\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>\n</td>\n<td id=\"A1.T5.4.27.2\" class=\"ltx_td ltx_align_left\">Järvelä</td>\n<td id=\"A1.T5.4.27.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.27.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.27.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">What Multimodal Data Can Tell Us About The Students’ Regulation Of Their Learning Process?</span>\n</span>\n</td>\n<td id=\"A1.T5.4.27.4\" class=\"ltx_td ltx_align_right\">2019</td>\n<td id=\"A1.T5.4.27.5\" class=\"ltx_td ltx_align_left\">LAI</td>\n</tr>\n<tr id=\"A1.T5.4.28\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.28.1\" class=\"ltx_td ltx_align_left\">32184286 <cite class=\"ltx_cite ltx_citemacro_citep\">(Kubsch et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib80\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>\n</td>\n<td id=\"A1.T5.4.28.2\" class=\"ltx_td ltx_align_left\">Kubsch</td>\n<td id=\"A1.T5.4.28.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.28.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.28.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Once More With Feeling: Emotions In Multimodal Learning Analytics</span>\n</span>\n</td>\n<td id=\"A1.T5.4.28.4\" class=\"ltx_td ltx_align_right\">2022</td>\n<td id=\"A1.T5.4.28.5\" class=\"ltx_td ltx_align_left\">MMLA Handbook</td>\n</tr>\n<tr id=\"A1.T5.4.29\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.29.1\" class=\"ltx_td ltx_align_left\">205660768 <cite class=\"ltx_cite ltx_citemacro_citep\">(Larmuseau et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib81\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>\n</td>\n<td id=\"A1.T5.4.29.2\" class=\"ltx_td ltx_align_left\">Larmuseau</td>\n<td id=\"A1.T5.4.29.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.29.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.29.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Multimodal Learning Analytics To Investigate Cognitive Load During Online Problem Solving</span>\n</span>\n</td>\n<td id=\"A1.T5.4.29.4\" class=\"ltx_td ltx_align_right\">2020</td>\n<td id=\"A1.T5.4.29.5\" class=\"ltx_td ltx_align_left\">BJET</td>\n</tr>\n<tr id=\"A1.T5.4.30\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.30.1\" class=\"ltx_td ltx_align_left\">1877483551 <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee-Cultura et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib84\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>\n</td>\n<td id=\"A1.T5.4.30.2\" class=\"ltx_td ltx_align_left\">Lee-Cultura</td>\n<td id=\"A1.T5.4.30.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.30.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.30.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Motion-Based Educational Games: Using Multi-Modal Data To Predict Player’S Performance</span>\n</span>\n</td>\n<td id=\"A1.T5.4.30.4\" class=\"ltx_td ltx_align_right\">2020</td>\n<td id=\"A1.T5.4.30.5\" class=\"ltx_td ltx_align_left\">COG</td>\n</tr>\n<tr id=\"A1.T5.4.31\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.31.1\" class=\"ltx_td ltx_align_left\">3660066725 <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee-Cultura et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib82\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>\n</td>\n<td id=\"A1.T5.4.31.2\" class=\"ltx_td ltx_align_left\">Lee-Cultura</td>\n<td id=\"A1.T5.4.31.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.31.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.31.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Children’S Play And Problem Solving In Motion-Based Educational Games: Synergies Between Human Annotations And Multi-Modal Data</span>\n</span>\n</td>\n<td id=\"A1.T5.4.31.4\" class=\"ltx_td ltx_align_right\">2021</td>\n<td id=\"A1.T5.4.31.5\" class=\"ltx_td ltx_align_left\">IDC</td>\n</tr>\n<tr id=\"A1.T5.4.32\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.32.1\" class=\"ltx_td ltx_align_left\">3856280479 <cite class=\"ltx_cite ltx_citemacro_citep\">(Lee-Cultura et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib83\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>\n</td>\n<td id=\"A1.T5.4.32.2\" class=\"ltx_td ltx_align_left\">Lee-Cultura</td>\n<td id=\"A1.T5.4.32.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.32.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.32.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Children’S Play And Problem-Solving In Motion-Based Learning Technologies Using A Multi-Modal Mixed Methods Approach</span>\n</span>\n</td>\n<td id=\"A1.T5.4.32.4\" class=\"ltx_td ltx_align_right\">2021</td>\n<td id=\"A1.T5.4.32.5\" class=\"ltx_td ltx_align_left\">IJCCI</td>\n</tr>\n<tr id=\"A1.T5.4.33\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.33.1\" class=\"ltx_td ltx_align_left\">804659204 <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib88\" title=\"\" class=\"ltx_ref\">2018a</a>)</cite>\n</td>\n<td id=\"A1.T5.4.33.2\" class=\"ltx_td ltx_align_left\">Liu</td>\n<td id=\"A1.T5.4.33.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.33.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.33.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Towards Smart Educational Recommendations With Reinforcement Learning In Classroom</span>\n</span>\n</td>\n<td id=\"A1.T5.4.33.4\" class=\"ltx_td ltx_align_right\">2018</td>\n<td id=\"A1.T5.4.33.5\" class=\"ltx_td ltx_align_left\">TALE</td>\n</tr>\n<tr id=\"A1.T5.4.34\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.34.1\" class=\"ltx_td ltx_align_left\">3783339081 <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib87\" title=\"\" class=\"ltx_ref\">2018b</a>)</cite>\n</td>\n<td id=\"A1.T5.4.34.2\" class=\"ltx_td ltx_align_left\">Liu</td>\n<td id=\"A1.T5.4.34.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.34.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.34.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">A Novel Method For The In-Depth Multimodal Analysis Of Student Learning Trajectories In Intelligent Tutoring Systems</span>\n</span>\n</td>\n<td id=\"A1.T5.4.34.4\" class=\"ltx_td ltx_align_right\">2018</td>\n<td id=\"A1.T5.4.34.5\" class=\"ltx_td ltx_align_left\">JLA</td>\n</tr>\n<tr id=\"A1.T5.4.35\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.35.1\" class=\"ltx_td ltx_align_left\">3796180663 <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib86\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>\n</td>\n<td id=\"A1.T5.4.35.2\" class=\"ltx_td ltx_align_left\">Liu</td>\n<td id=\"A1.T5.4.35.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.35.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.35.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Learning Linkages: Integrating Data Streams Of Multiple Modalities And Timescales</span>\n</span>\n</td>\n<td id=\"A1.T5.4.35.4\" class=\"ltx_td ltx_align_right\">2018</td>\n<td id=\"A1.T5.4.35.5\" class=\"ltx_td ltx_align_left\">JCAL</td>\n</tr>\n<tr id=\"A1.T5.4.36\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.36.1\" class=\"ltx_td ltx_align_left\">518268671 <cite class=\"ltx_cite ltx_citemacro_citep\">(López et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib90\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>\n</td>\n<td id=\"A1.T5.4.36.2\" class=\"ltx_td ltx_align_left\">López</td>\n<td id=\"A1.T5.4.36.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.36.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.36.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Using Multimodal Learning Analytics To Explore Collaboration In A Sustainability Co-Located Tabletop Game</span>\n</span>\n</td>\n<td id=\"A1.T5.4.36.4\" class=\"ltx_td ltx_align_right\">2021</td>\n<td id=\"A1.T5.4.36.5\" class=\"ltx_td ltx_align_left\">ECGBL</td>\n</tr>\n<tr id=\"A1.T5.4.37\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.37.1\" class=\"ltx_td ltx_align_left\">566043228 <cite class=\"ltx_cite ltx_citemacro_citep\">(Capital Normal University, Beijing, China et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib19\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>\n</td>\n<td id=\"A1.T5.4.37.2\" class=\"ltx_td ltx_align_left\">Ma</td>\n<td id=\"A1.T5.4.37.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.37.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.37.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Automatic Student Engagement In Online Learning Environment Based On Neural Turing Machine</span>\n</span>\n</td>\n<td id=\"A1.T5.4.37.4\" class=\"ltx_td ltx_align_right\">2021</td>\n<td id=\"A1.T5.4.37.5\" class=\"ltx_td ltx_align_left\">IJIET</td>\n</tr>\n<tr id=\"A1.T5.4.38\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.38.1\" class=\"ltx_td ltx_align_left\">3754172825 <cite class=\"ltx_cite ltx_citemacro_citep\">(Ma et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib91\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>\n</td>\n<td id=\"A1.T5.4.38.2\" class=\"ltx_td ltx_align_left\">Ma</td>\n<td id=\"A1.T5.4.38.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.38.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.38.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Detecting Impasse During Collaborative Problem Solving With Multimodal Learning Analytics</span>\n</span>\n</td>\n<td id=\"A1.T5.4.38.4\" class=\"ltx_td ltx_align_right\">2022</td>\n<td id=\"A1.T5.4.38.5\" class=\"ltx_td ltx_align_left\">LAK</td>\n</tr>\n<tr id=\"A1.T5.4.39\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.39.1\" class=\"ltx_td ltx_align_left\">147203129 <cite class=\"ltx_cite ltx_citemacro_citep\">(Mangaroska et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib92\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>\n</td>\n<td id=\"A1.T5.4.39.2\" class=\"ltx_td ltx_align_left\">Mangaroska</td>\n<td id=\"A1.T5.4.39.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.39.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.39.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Multimodal Learning Analytics To Inform Learning Design: Lessons Learned From Computing Education</span>\n</span>\n</td>\n<td id=\"A1.T5.4.39.4\" class=\"ltx_td ltx_align_right\">2020</td>\n<td id=\"A1.T5.4.39.5\" class=\"ltx_td ltx_align_left\">JLA</td>\n</tr>\n<tr id=\"A1.T5.4.40\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.40.1\" class=\"ltx_td ltx_align_left\">1847468084 <cite class=\"ltx_cite ltx_citemacro_citep\">(Martin et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib93\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>\n</td>\n<td id=\"A1.T5.4.40.2\" class=\"ltx_td ltx_align_left\">Martin</td>\n<td id=\"A1.T5.4.40.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.40.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.40.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Computationally Augmented Ethnography: Emotion Tracking And Learning In Museum Games</span>\n</span>\n</td>\n<td id=\"A1.T5.4.40.4\" class=\"ltx_td ltx_align_right\">2019</td>\n<td id=\"A1.T5.4.40.5\" class=\"ltx_td ltx_align_left\">ICQE</td>\n</tr>\n<tr id=\"A1.T5.4.41\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.41.1\" class=\"ltx_td ltx_align_left\">2879332689 <cite class=\"ltx_cite ltx_citemacro_citep\">(Martinez-Maldonado et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib94\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>\n</td>\n<td id=\"A1.T5.4.41.2\" class=\"ltx_td ltx_align_left\">Martinez-Maldonado</td>\n<td id=\"A1.T5.4.41.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.41.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.41.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">From Data To Insights: A Layered Storytelling Approach For Multimodal Learning Analytics</span>\n</span>\n</td>\n<td id=\"A1.T5.4.41.4\" class=\"ltx_td ltx_align_right\">2020</td>\n<td id=\"A1.T5.4.41.5\" class=\"ltx_td ltx_align_left\">CHI</td>\n</tr>\n<tr id=\"A1.T5.4.42\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.42.1\" class=\"ltx_td ltx_align_left\">2155422499 <cite class=\"ltx_cite ltx_citemacro_citep\">(Morell et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib100\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>\n</td>\n<td id=\"A1.T5.4.42.2\" class=\"ltx_td ltx_align_left\">Morell</td>\n<td id=\"A1.T5.4.42.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.42.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.42.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">A Multimodal Analysis Of Pair Work Engagement Episodes: Implications For Emi Lecturer Training</span>\n</span>\n</td>\n<td id=\"A1.T5.4.42.4\" class=\"ltx_td ltx_align_right\">2022</td>\n<td id=\"A1.T5.4.42.5\" class=\"ltx_td ltx_align_left\">JEAP</td>\n</tr>\n<tr id=\"A1.T5.4.43\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.43.1\" class=\"ltx_td ltx_align_left\">2273914836 <cite class=\"ltx_cite ltx_citemacro_citep\">(Nasir et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib102\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>\n</td>\n<td id=\"A1.T5.4.43.2\" class=\"ltx_td ltx_align_left\">Nasir</td>\n<td id=\"A1.T5.4.43.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.43.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.43.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Many Are The Ways To Learn Identifying Multi-Modal Behavioral Profiles Of Collaborative Learning In Constructivist Activities</span>\n</span>\n</td>\n<td id=\"A1.T5.4.43.4\" class=\"ltx_td ltx_align_right\">2022</td>\n<td id=\"A1.T5.4.43.5\" class=\"ltx_td ltx_align_left\">IJCSCL</td>\n</tr>\n<tr id=\"A1.T5.4.44\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.44.1\" class=\"ltx_td ltx_align_left\">1469065963 <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib103\" title=\"\" class=\"ltx_ref\">2023</a>)</cite>\n</td>\n<td id=\"A1.T5.4.44.2\" class=\"ltx_td ltx_align_left\">Nguyen</td>\n<td id=\"A1.T5.4.44.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.44.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.44.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Examining Socially Shared Regulation And Shared Physiological Arousal Events With Multimodal Learning Analytics</span>\n</span>\n</td>\n<td id=\"A1.T5.4.44.4\" class=\"ltx_td ltx_align_right\">2022</td>\n<td id=\"A1.T5.4.44.5\" class=\"ltx_td ltx_align_left\">BJET</td>\n</tr>\n<tr id=\"A1.T5.4.45\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.45.1\" class=\"ltx_td ltx_align_left\">2345021698 <cite class=\"ltx_cite ltx_citemacro_citep\">(Noel et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib105\" title=\"\" class=\"ltx_ref\">2018</a>)</cite>\n</td>\n<td id=\"A1.T5.4.45.2\" class=\"ltx_td ltx_align_left\">Noël</td>\n<td id=\"A1.T5.4.45.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.45.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.45.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Exploring Collaborative Writing Of User Stories With Multimodal Learning Analytics: A Case Study On A Software Engineering Course</span>\n</span>\n</td>\n<td id=\"A1.T5.4.45.4\" class=\"ltx_td ltx_align_right\">2018</td>\n<td id=\"A1.T5.4.45.5\" class=\"ltx_td ltx_align_left\">Access</td>\n</tr>\n<tr id=\"A1.T5.4.46\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.46.1\" class=\"ltx_td ltx_align_left\">2609260641 <cite class=\"ltx_cite ltx_citemacro_citep\">(Noël et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib106\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>\n</td>\n<td id=\"A1.T5.4.46.2\" class=\"ltx_td ltx_align_left\">Noël</td>\n<td id=\"A1.T5.4.46.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.46.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.46.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Visualizing Collaboration In Teamwork: A Multimodal Learning Analytics Platform For Non-Verbal Communication</span>\n</span>\n</td>\n<td id=\"A1.T5.4.46.4\" class=\"ltx_td ltx_align_right\">2022</td>\n<td id=\"A1.T5.4.46.5\" class=\"ltx_td ltx_align_left\">DAMLE</td>\n</tr>\n<tr id=\"A1.T5.4.47\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.47.1\" class=\"ltx_td ltx_align_left\">2497456347 <cite class=\"ltx_cite ltx_citemacro_citep\">(Ochoa et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib108\" title=\"\" class=\"ltx_ref\">2018</a>)</cite>\n</td>\n<td id=\"A1.T5.4.47.2\" class=\"ltx_td ltx_align_left\">Ochoa</td>\n<td id=\"A1.T5.4.47.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.47.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.47.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">The Rap System: Automatic Feedback Of Oral Presentation Skills Using Multimodal Analysis And Low-Cost Sensors</span>\n</span>\n</td>\n<td id=\"A1.T5.4.47.4\" class=\"ltx_td ltx_align_right\">2018</td>\n<td id=\"A1.T5.4.47.5\" class=\"ltx_td ltx_align_left\">LAK</td>\n</tr>\n<tr id=\"A1.T5.4.48\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.48.1\" class=\"ltx_td ltx_align_left\">2634033325 <cite class=\"ltx_cite ltx_citemacro_citep\">(Ochoa and Dominguez, <a href=\"#bib.bib107\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>\n</td>\n<td id=\"A1.T5.4.48.2\" class=\"ltx_td ltx_align_left\">Ochoa</td>\n<td id=\"A1.T5.4.48.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.48.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.48.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Controlled Evaluation Of A Multimodal System To Improve Oral Presentation Skills In A Real Learning Setting</span>\n</span>\n</td>\n<td id=\"A1.T5.4.48.4\" class=\"ltx_td ltx_align_right\">2020</td>\n<td id=\"A1.T5.4.48.5\" class=\"ltx_td ltx_align_left\">BJET</td>\n</tr>\n<tr id=\"A1.T5.4.49\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.49.1\" class=\"ltx_td ltx_align_left\">3051560548 <cite class=\"ltx_cite ltx_citemacro_citep\">(Olsen et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib111\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>\n</td>\n<td id=\"A1.T5.4.49.2\" class=\"ltx_td ltx_align_left\">Olsen</td>\n<td id=\"A1.T5.4.49.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.49.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.49.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Temporal Analysis Of Multimodal Data To Predict Collaborative Learning Outcomes</span>\n</span>\n</td>\n<td id=\"A1.T5.4.49.4\" class=\"ltx_td ltx_align_right\">2020</td>\n<td id=\"A1.T5.4.49.5\" class=\"ltx_td ltx_align_left\">BJET</td>\n</tr>\n<tr id=\"A1.T5.4.50\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.50.1\" class=\"ltx_td ltx_align_left\">123412197 <cite class=\"ltx_cite ltx_citemacro_citep\">(Papamitsiou et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib113\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>\n</td>\n<td id=\"A1.T5.4.50.2\" class=\"ltx_td ltx_align_left\">Papamitsiou</td>\n<td id=\"A1.T5.4.50.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.50.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.50.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Utilizing Multimodal Data Through Fsqca To Explain Engagement In Adaptive Learning</span>\n</span>\n</td>\n<td id=\"A1.T5.4.50.4\" class=\"ltx_td ltx_align_right\">2020</td>\n<td id=\"A1.T5.4.50.5\" class=\"ltx_td ltx_align_left\">TLT</td>\n</tr>\n<tr id=\"A1.T5.4.51\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.51.1\" class=\"ltx_td ltx_align_left\">85990093 <cite class=\"ltx_cite ltx_citemacro_citep\">(Petukhova et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib116\" title=\"\" class=\"ltx_ref\">2017b</a>)</cite>\n</td>\n<td id=\"A1.T5.4.51.2\" class=\"ltx_td ltx_align_left\">Petukhova</td>\n<td id=\"A1.T5.4.51.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.51.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.51.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Multimodal Markers Of Persuasive Speech : Designing A Virtual Debate Coach</span>\n</span>\n</td>\n<td id=\"A1.T5.4.51.4\" class=\"ltx_td ltx_align_right\">2017</td>\n<td id=\"A1.T5.4.51.5\" class=\"ltx_td ltx_align_left\">INTERSPEECH</td>\n</tr>\n<tr id=\"A1.T5.4.52\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.52.1\" class=\"ltx_td ltx_align_left\">957160695 <cite class=\"ltx_cite ltx_citemacro_citep\">(Petukhova et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib115\" title=\"\" class=\"ltx_ref\">2017a</a>)</cite>\n</td>\n<td id=\"A1.T5.4.52.2\" class=\"ltx_td ltx_align_left\">Petukhova</td>\n<td id=\"A1.T5.4.52.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.52.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.52.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Virtual Debate Coach Design: Assessing Multimodal Argumentation Performance</span>\n</span>\n</td>\n<td id=\"A1.T5.4.52.4\" class=\"ltx_td ltx_align_right\">2017</td>\n<td id=\"A1.T5.4.52.5\" class=\"ltx_td ltx_align_left\">ICMI</td>\n</tr>\n<tr id=\"A1.T5.4.53\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.53.1\" class=\"ltx_td ltx_align_left\">1374035721 <cite class=\"ltx_cite ltx_citemacro_citep\">(Pham and Wang, <a href=\"#bib.bib117\" title=\"\" class=\"ltx_ref\">2017</a>)</cite>\n</td>\n<td id=\"A1.T5.4.53.2\" class=\"ltx_td ltx_align_left\">Pham</td>\n<td id=\"A1.T5.4.53.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.53.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.53.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Attentivelearner2: A Multimodal Approach For Improving Mooc Learning On Mobile Devices</span>\n</span>\n</td>\n<td id=\"A1.T5.4.53.4\" class=\"ltx_td ltx_align_right\">2017</td>\n<td id=\"A1.T5.4.53.5\" class=\"ltx_td ltx_align_left\">AIED</td>\n</tr>\n<tr id=\"A1.T5.4.54\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.54.1\" class=\"ltx_td ltx_align_left\">2836996318 <cite class=\"ltx_cite ltx_citemacro_citep\">(Pham and Wang, <a href=\"#bib.bib118\" title=\"\" class=\"ltx_ref\">2018</a>)</cite>\n</td>\n<td id=\"A1.T5.4.54.2\" class=\"ltx_td ltx_align_left\">Pham</td>\n<td id=\"A1.T5.4.54.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.54.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.54.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Predicting Learners’ Emotions In Mobile Mooc Learning Via A Multimodal Intelligent Tutor</span>\n</span>\n</td>\n<td id=\"A1.T5.4.54.4\" class=\"ltx_td ltx_align_right\">2018</td>\n<td id=\"A1.T5.4.54.5\" class=\"ltx_td ltx_align_left\">ITS</td>\n</tr>\n<tr id=\"A1.T5.4.55\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.55.1\" class=\"ltx_td ltx_align_left\">3135645357 <cite class=\"ltx_cite ltx_citemacro_citep\">(Prieto et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib120\" title=\"\" class=\"ltx_ref\">2018</a>)</cite>\n</td>\n<td id=\"A1.T5.4.55.2\" class=\"ltx_td ltx_align_left\">Prieto</td>\n<td id=\"A1.T5.4.55.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.55.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.55.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Multimodal Teaching Analytics: Automated Extraction Of Orchestration Graphs From Wearable Sensor Data</span>\n</span>\n</td>\n<td id=\"A1.T5.4.55.4\" class=\"ltx_td ltx_align_right\">2018</td>\n<td id=\"A1.T5.4.55.5\" class=\"ltx_td ltx_align_left\">JCAL</td>\n</tr>\n<tr id=\"A1.T5.4.56\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.56.1\" class=\"ltx_td ltx_align_left\">3408664396 <cite class=\"ltx_cite ltx_citemacro_citep\">(Psaltis et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib121\" title=\"\" class=\"ltx_ref\">2018</a>)</cite>\n</td>\n<td id=\"A1.T5.4.56.2\" class=\"ltx_td ltx_align_left\">Psaltis</td>\n<td id=\"A1.T5.4.56.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.56.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.56.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Multimodal Student Engagement Recognition In Prosocial Games</span>\n</span>\n</td>\n<td id=\"A1.T5.4.56.4\" class=\"ltx_td ltx_align_right\">2017</td>\n<td id=\"A1.T5.4.56.5\" class=\"ltx_td ltx_align_left\">T-CIAIG</td>\n</tr>\n<tr id=\"A1.T5.4.57\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.57.1\" class=\"ltx_td ltx_align_left\">3308658121 <cite class=\"ltx_cite ltx_citemacro_citep\">(Reilly et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib124\" title=\"\" class=\"ltx_ref\">2018</a>)</cite>\n</td>\n<td id=\"A1.T5.4.57.2\" class=\"ltx_td ltx_align_left\">Reilly</td>\n<td id=\"A1.T5.4.57.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.57.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.57.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Exploring Collaboration Using Motion Sensors And Multi-Modal Learning Analytics</span>\n</span>\n</td>\n<td id=\"A1.T5.4.57.4\" class=\"ltx_td ltx_align_right\">2018</td>\n<td id=\"A1.T5.4.57.5\" class=\"ltx_td ltx_align_left\">EDM</td>\n</tr>\n<tr id=\"A1.T5.4.58\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.58.1\" class=\"ltx_td ltx_align_left\">3625722965 <cite class=\"ltx_cite ltx_citemacro_citep\">(Mat Sanusi et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib96\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>\n</td>\n<td id=\"A1.T5.4.58.2\" class=\"ltx_td ltx_align_left\">Sanusi</td>\n<td id=\"A1.T5.4.58.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.58.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.58.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Table Tennis Tutor: Forehand Strokes Classification Based On Multimodal Data And Neural Networks</span>\n</span>\n</td>\n<td id=\"A1.T5.4.58.4\" class=\"ltx_td ltx_align_right\">2021</td>\n<td id=\"A1.T5.4.58.5\" class=\"ltx_td ltx_align_left\">Sensors</td>\n</tr>\n<tr id=\"A1.T5.4.59\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.59.1\" class=\"ltx_td ltx_align_left\">2000036002 <cite class=\"ltx_cite ltx_citemacro_citep\">(Sharma et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib133\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>\n</td>\n<td id=\"A1.T5.4.59.2\" class=\"ltx_td ltx_align_left\">Sharma</td>\n<td id=\"A1.T5.4.59.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.59.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.59.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Predicting Learners’ Effortful Behaviour In Adaptive Assessment Using Multimodal Data</span>\n</span>\n</td>\n<td id=\"A1.T5.4.59.4\" class=\"ltx_td ltx_align_right\">2020</td>\n<td id=\"A1.T5.4.59.5\" class=\"ltx_td ltx_align_left\">LAK</td>\n</tr>\n<tr id=\"A1.T5.4.60\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.60.1\" class=\"ltx_td ltx_align_left\">1118315889 <cite class=\"ltx_cite ltx_citemacro_citep\">(Spikol et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib137\" title=\"\" class=\"ltx_ref\">2017a</a>)</cite>\n</td>\n<td id=\"A1.T5.4.60.2\" class=\"ltx_td ltx_align_left\">Spikol</td>\n<td id=\"A1.T5.4.60.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.60.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.60.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Using Multimodal Learning Analytics To Identify Aspects Of Collaboration In Project-Based Learning</span>\n</span>\n</td>\n<td id=\"A1.T5.4.60.4\" class=\"ltx_td ltx_align_right\">2017</td>\n<td id=\"A1.T5.4.60.5\" class=\"ltx_td ltx_align_left\">CSCL</td>\n</tr>\n<tr id=\"A1.T5.4.61\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.61.1\" class=\"ltx_td ltx_align_left\">3339002981 <cite class=\"ltx_cite ltx_citemacro_citep\">(Spikol et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib139\" title=\"\" class=\"ltx_ref\">2017b</a>)</cite>\n</td>\n<td id=\"A1.T5.4.61.2\" class=\"ltx_td ltx_align_left\">Spikol</td>\n<td id=\"A1.T5.4.61.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.61.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.61.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Estimation Of Success In Collaborative Learning Based On Multimodal Learning Analytics Features</span>\n</span>\n</td>\n<td id=\"A1.T5.4.61.4\" class=\"ltx_td ltx_align_right\">2017</td>\n<td id=\"A1.T5.4.61.5\" class=\"ltx_td ltx_align_left\">ICALT</td>\n</tr>\n<tr id=\"A1.T5.4.62\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.62.1\" class=\"ltx_td ltx_align_left\">1637690235 <cite class=\"ltx_cite ltx_citemacro_citep\">(Spikol et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib138\" title=\"\" class=\"ltx_ref\">2018</a>)</cite>\n</td>\n<td id=\"A1.T5.4.62.2\" class=\"ltx_td ltx_align_left\">Spikol</td>\n<td id=\"A1.T5.4.62.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.62.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.62.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Supervised Machine Learning In Multimodal Learning Analytics For Estimating Success In Project-Based Learning</span>\n</span>\n</td>\n<td id=\"A1.T5.4.62.4\" class=\"ltx_td ltx_align_right\">2018</td>\n<td id=\"A1.T5.4.62.5\" class=\"ltx_td ltx_align_left\">JCAL</td>\n</tr>\n<tr id=\"A1.T5.4.63\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.63.1\" class=\"ltx_td ltx_align_left\">3796643912 <cite class=\"ltx_cite ltx_citemacro_citep\">(Standen et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib140\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>\n</td>\n<td id=\"A1.T5.4.63.2\" class=\"ltx_td ltx_align_left\">Standen</td>\n<td id=\"A1.T5.4.63.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.63.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.63.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">An Evaluation Of An Adaptive Learning System Based On Multimodal Affect Recognition For Learners With Intellectual Disabilities</span>\n</span>\n</td>\n<td id=\"A1.T5.4.63.4\" class=\"ltx_td ltx_align_right\">2020</td>\n<td id=\"A1.T5.4.63.5\" class=\"ltx_td ltx_align_left\">BJET</td>\n</tr>\n<tr id=\"A1.T5.4.64\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.64.1\" class=\"ltx_td ltx_align_left\">2181637610 <cite class=\"ltx_cite ltx_citemacro_citep\">(Starr et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib141\" title=\"\" class=\"ltx_ref\">2018</a>)</cite>\n</td>\n<td id=\"A1.T5.4.64.2\" class=\"ltx_td ltx_align_left\">Starr</td>\n<td id=\"A1.T5.4.64.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.64.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.64.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Toward Using Multi-Modal Learning Analytics To Support And Measure Collaboration In Co-Located Dyads</span>\n</span>\n</td>\n<td id=\"A1.T5.4.64.4\" class=\"ltx_td ltx_align_right\">2018</td>\n<td id=\"A1.T5.4.64.5\" class=\"ltx_td ltx_align_left\">ICLS</td>\n</tr>\n<tr id=\"A1.T5.4.65\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.65.1\" class=\"ltx_td ltx_align_left\">1315379489 <cite class=\"ltx_cite ltx_citemacro_citep\">(Sümer et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib143\" title=\"\" class=\"ltx_ref\">2023</a>)</cite>\n</td>\n<td id=\"A1.T5.4.65.2\" class=\"ltx_td ltx_align_left\">Sümer</td>\n<td id=\"A1.T5.4.65.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.65.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.65.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Multimodal Engagement Analysis From Facial Videos In The Classroom</span>\n</span>\n</td>\n<td id=\"A1.T5.4.65.4\" class=\"ltx_td ltx_align_right\">2021</td>\n<td id=\"A1.T5.4.65.5\" class=\"ltx_td ltx_align_left\">TAC</td>\n</tr>\n<tr id=\"A1.T5.4.66\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.66.1\" class=\"ltx_td ltx_align_left\">3093310941 <cite class=\"ltx_cite ltx_citemacro_citep\">(Tanaka et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib144\" title=\"\" class=\"ltx_ref\">2017</a>)</cite>\n</td>\n<td id=\"A1.T5.4.66.2\" class=\"ltx_td ltx_align_left\">Tanaka</td>\n<td id=\"A1.T5.4.66.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.66.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.66.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Embodied Conversational Agents For Multimodal Automated Social Skills Training In People With Autism Spectrum Disorders</span>\n</span>\n</td>\n<td id=\"A1.T5.4.66.4\" class=\"ltx_td ltx_align_right\">2017</td>\n<td id=\"A1.T5.4.66.5\" class=\"ltx_td ltx_align_left\">PLOS</td>\n</tr>\n<tr id=\"A1.T5.4.67\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.67.1\" class=\"ltx_td ltx_align_left\">1345598079 <cite class=\"ltx_cite ltx_citemacro_citep\">(Tancredi et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib145\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>\n</td>\n<td id=\"A1.T5.4.67.2\" class=\"ltx_td ltx_align_left\">Tancredi</td>\n<td id=\"A1.T5.4.67.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.67.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.67.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Intermodality In Multimodal Learning Analytics For Cognitive Theory Development: A Case From Embodied Design For Mathematics Learning</span>\n</span>\n</td>\n<td id=\"A1.T5.4.67.4\" class=\"ltx_td ltx_align_right\">2022</td>\n<td id=\"A1.T5.4.67.5\" class=\"ltx_td ltx_align_left\">MMLA Handbook</td>\n</tr>\n<tr id=\"A1.T5.4.68\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.68.1\" class=\"ltx_td ltx_align_left\">433919853 <cite class=\"ltx_cite ltx_citemacro_citep\">(Tisza et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib149\" title=\"\" class=\"ltx_ref\">2022</a>)</cite>\n</td>\n<td id=\"A1.T5.4.68.2\" class=\"ltx_td ltx_align_left\">Tisza</td>\n<td id=\"A1.T5.4.68.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.68.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.68.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Understanding Fun In Learning To Code: A Multi-Modal Data Approach</span>\n</span>\n</td>\n<td id=\"A1.T5.4.68.4\" class=\"ltx_td ltx_align_right\">2022</td>\n<td id=\"A1.T5.4.68.5\" class=\"ltx_td ltx_align_left\">IDC</td>\n</tr>\n<tr id=\"A1.T5.4.69\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.69.1\" class=\"ltx_td ltx_align_left\">1770989706 <cite class=\"ltx_cite ltx_citemacro_citep\">(Vrzakova et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib155\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>\n</td>\n<td id=\"A1.T5.4.69.2\" class=\"ltx_td ltx_align_left\">Vrzakova</td>\n<td id=\"A1.T5.4.69.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.69.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.69.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Focused Or Stuck Together: Multimodal Patterns Reveal Triads’ Performance In Collaborative Problem Solving</span>\n</span>\n</td>\n<td id=\"A1.T5.4.69.4\" class=\"ltx_td ltx_align_right\">2020</td>\n<td id=\"A1.T5.4.69.5\" class=\"ltx_td ltx_align_left\">LAK</td>\n</tr>\n<tr id=\"A1.T5.4.70\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.70.1\" class=\"ltx_td ltx_align_left\">2055153191 <cite class=\"ltx_cite ltx_citemacro_citep\">(Vujovic et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib156\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>\n</td>\n<td id=\"A1.T5.4.70.2\" class=\"ltx_td ltx_align_left\">Vujovic</td>\n<td id=\"A1.T5.4.70.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.70.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.70.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Round Or Rectangular Tables For Collaborative Problem Solving? A Multimodal Learning Analytics Study</span>\n</span>\n</td>\n<td id=\"A1.T5.4.70.4\" class=\"ltx_td ltx_align_right\">2020</td>\n<td id=\"A1.T5.4.70.5\" class=\"ltx_td ltx_align_left\">BJET</td>\n</tr>\n<tr id=\"A1.T5.4.71\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.71.1\" class=\"ltx_td ltx_align_left\">3095923626 <cite class=\"ltx_cite ltx_citemacro_citep\">(Worsley and Blikstein, <a href=\"#bib.bib159\" title=\"\" class=\"ltx_ref\">2018</a>)</cite>\n</td>\n<td id=\"A1.T5.4.71.2\" class=\"ltx_td ltx_align_left\">Worsley</td>\n<td id=\"A1.T5.4.71.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.71.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.71.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">A Multimodal Analysis Of Making</span>\n</span>\n</td>\n<td id=\"A1.T5.4.71.4\" class=\"ltx_td ltx_align_right\">2017</td>\n<td id=\"A1.T5.4.71.5\" class=\"ltx_td ltx_align_left\">IJAIED</td>\n</tr>\n<tr id=\"A1.T5.4.72\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.72.1\" class=\"ltx_td ltx_align_left\">3309250332 <cite class=\"ltx_cite ltx_citemacro_citep\">(Worsley, <a href=\"#bib.bib158\" title=\"\" class=\"ltx_ref\">2018</a>)</cite>\n</td>\n<td id=\"A1.T5.4.72.2\" class=\"ltx_td ltx_align_left\">Worsley</td>\n<td id=\"A1.T5.4.72.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.72.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.72.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">(Dis)Engagement Matters: Identifying Efficacious Learning Practices With Multimodal Learning Analytics</span>\n</span>\n</td>\n<td id=\"A1.T5.4.72.4\" class=\"ltx_td ltx_align_right\">2018</td>\n<td id=\"A1.T5.4.72.5\" class=\"ltx_td ltx_align_left\">LAK</td>\n</tr>\n<tr id=\"A1.T5.4.73\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.73.1\" class=\"ltx_td ltx_align_left\">666050348 <cite class=\"ltx_cite ltx_citemacro_citep\">(Worsley et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib160\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>\n</td>\n<td id=\"A1.T5.4.73.2\" class=\"ltx_td ltx_align_left\">Worsley</td>\n<td id=\"A1.T5.4.73.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"A1.T5.4.73.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.73.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Multicraft: A Multimodal Interface For Supporting And Studying Learning In Minecraft</span>\n</span>\n</td>\n<td id=\"A1.T5.4.73.4\" class=\"ltx_td ltx_align_right\">2021</td>\n<td id=\"A1.T5.4.73.5\" class=\"ltx_td ltx_align_left\">HCII</td>\n</tr>\n<tr id=\"A1.T5.4.74\" class=\"ltx_tr\">\n<td id=\"A1.T5.4.74.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">1019093033 <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib163\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>\n</td>\n<td id=\"A1.T5.4.74.2\" class=\"ltx_td ltx_align_left ltx_border_bb\">Yang</td>\n<td id=\"A1.T5.4.74.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span id=\"A1.T5.4.74.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A1.T5.4.74.3.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Prime: Block-Wise Missingness Handling For Multi-Modalities In Intelligent Tutoring Systems</span>\n</span>\n</td>\n<td id=\"A1.T5.4.74.4\" class=\"ltx_td ltx_align_right ltx_border_bb\">2019</td>\n<td id=\"A1.T5.4.74.5\" class=\"ltx_td ltx_align_left ltx_border_bb\">MMM</td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": []
    }
}{
    "A2.T6.2": {
        "caption": [
            "Table 6",
            "Search strings used for the initial literature search."
        ],
        "table": "<table id=\"A2.T6.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A2.T6.2.1.1\" class=\"ltx_tr\">\n<th id=\"A2.T6.2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">education technology</th>\n<th id=\"A2.T6.2.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">explainable artificial intelligence</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A2.T6.2.2.1\" class=\"ltx_tr\">\n<td id=\"A2.T6.2.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\">learning analytics</td>\n<td id=\"A2.T6.2.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_t\">learning environments</td>\n</tr>\n<tr id=\"A2.T6.2.3.2\" class=\"ltx_tr\">\n<td id=\"A2.T6.2.3.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\">learning environments literature review</td>\n<td id=\"A2.T6.2.3.2.2\" class=\"ltx_td ltx_align_left ltx_border_t\">learning environments survey</td>\n</tr>\n<tr id=\"A2.T6.2.4.3\" class=\"ltx_tr\">\n<td id=\"A2.T6.2.4.3.1\" class=\"ltx_td ltx_align_left ltx_border_t\">literature review</td>\n<td id=\"A2.T6.2.4.3.2\" class=\"ltx_td ltx_align_left ltx_border_t\">simulation environments</td>\n</tr>\n<tr id=\"A2.T6.2.5.4\" class=\"ltx_tr\">\n<td id=\"A2.T6.2.5.4.1\" class=\"ltx_td ltx_align_left ltx_border_t\">survey</td>\n<td id=\"A2.T6.2.5.4.2\" class=\"ltx_td ltx_align_left ltx_border_t\">training environments</td>\n</tr>\n<tr id=\"A2.T6.2.6.5\" class=\"ltx_tr\">\n<td id=\"A2.T6.2.6.5.1\" class=\"ltx_td ltx_align_left ltx_border_t\">training environments literature review</td>\n<td id=\"A2.T6.2.6.5.2\" class=\"ltx_td ltx_align_left ltx_border_t\">training environments survey</td>\n</tr>\n<tr id=\"A2.T6.2.7.6\" class=\"ltx_tr\">\n<td id=\"A2.T6.2.7.6.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">tutoring systems</td>\n<td id=\"A2.T6.2.7.6.2\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">xai</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "For the literature search, this review’s authors decided on 14 distinct search phrases, and each phrase was searched 3 times with a different spelling of the word multimodal — multimodal, multi-modal, and multi modal — prepended to it. The 14 search phrases are enumerated in Table 6.222The term ”xai” was included in the search due to the authors’ interest in exploring explainable AI methods applied to learning and training environments. Unfortunately, the field is still nascent, and no usable query results were returned with this search string."
        ]
    }
}{
    "A2.T7.2": {
        "caption": [
            "Table 7",
            "Our corpus reduction procedure. Step ID 0 is the literature search. Steps 1 and 2 used programmatic filtering via Python packages. Steps 3-5 were performed quantitatively via CGP (see Section 3). Step 6 uses human-in-the-loop regex filtering. Steps 7-9 were performed qualitatively via our quality control procedure. Each Step ID lists the number of papers removed and remaining.",
            "3"
        ],
        "table": "<table id=\"A2.T7.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A2.T7.2.1.1\" class=\"ltx_tr\">\n<th id=\"A2.T7.2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Step ID</th>\n<th id=\"A2.T7.2.1.1.2\" class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Procedure</th>\n<th id=\"A2.T7.2.1.1.3\" class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Removed</th>\n<th id=\"A2.T7.2.1.1.4\" class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Remaining</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A2.T7.2.2.1\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0</td>\n<td id=\"A2.T7.2.2.1.2\" class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Literature search</td>\n<td id=\"A2.T7.2.2.1.3\" class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0</td>\n<td id=\"A2.T7.2.2.1.4\" class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">4200</td>\n</tr>\n<tr id=\"A2.T7.2.3.2\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.3.2.1\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1</td>\n<td id=\"A2.T7.2.3.2.2\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Remove duplicates</td>\n<td id=\"A2.T7.2.3.2.3\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2079</td>\n<td id=\"A2.T7.2.3.2.4\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2121</td>\n</tr>\n<tr id=\"A2.T7.2.4.3\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.4.3.1\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2</td>\n<td id=\"A2.T7.2.4.3.2\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Remove non-English</td>\n<td id=\"A2.T7.2.4.3.3\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1</td>\n<td id=\"A2.T7.2.4.3.4\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2120</td>\n</tr>\n<tr id=\"A2.T7.2.5.4\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.5.4.1\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3</td>\n<td id=\"A2.T7.2.5.4.2\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Remove degree-0 nodes</td>\n<td id=\"A2.T7.2.5.4.3\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">488</td>\n<td id=\"A2.T7.2.5.4.4\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1632</td>\n</tr>\n<tr id=\"A2.T7.2.6.5\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.6.5.1\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">4</td>\n<td id=\"A2.T7.2.6.5.2\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Remove disconnected components</td>\n<td id=\"A2.T7.2.6.5.3\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">101</td>\n<td id=\"A2.T7.2.6.5.4\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1531</td>\n</tr>\n<tr id=\"A2.T7.2.7.6\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.7.6.1\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">5</td>\n<td id=\"A2.T7.2.7.6.2\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Iteratively remove degree-1 nodes</td>\n<td id=\"A2.T7.2.7.6.3\" class=\"ltx_td ltx_nopad_l\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"></td>\n<td id=\"A2.T7.2.7.6.4\" class=\"ltx_td ltx_nopad_l\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"></td>\n</tr>\n<tr id=\"A2.T7.2.8.7\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.8.7.1\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">   5.1</td>\n<td id=\"A2.T7.2.8.7.2\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Iteration 1</td>\n<td id=\"A2.T7.2.8.7.3\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">373</td>\n<td id=\"A2.T7.2.8.7.4\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1158</td>\n</tr>\n<tr id=\"A2.T7.2.9.8\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.9.8.1\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">   5.2</td>\n<td id=\"A2.T7.2.9.8.2\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Iteration 2</td>\n<td id=\"A2.T7.2.9.8.3\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">74</td>\n<td id=\"A2.T7.2.9.8.4\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1084</td>\n</tr>\n<tr id=\"A2.T7.2.10.9\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.10.9.1\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">   5.3</td>\n<td id=\"A2.T7.2.10.9.2\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Iteration 3</td>\n<td id=\"A2.T7.2.10.9.3\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">19</td>\n<td id=\"A2.T7.2.10.9.4\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1065</td>\n</tr>\n<tr id=\"A2.T7.2.11.10\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.11.10.1\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">   5.4</td>\n<td id=\"A2.T7.2.11.10.2\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Iteration 4</td>\n<td id=\"A2.T7.2.11.10.3\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2</td>\n<td id=\"A2.T7.2.11.10.4\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1063</td>\n</tr>\n<tr id=\"A2.T7.2.12.11\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.12.11.1\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">6</td>\n<td id=\"A2.T7.2.12.11.2\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Remove titles with keywords</td>\n<td id=\"A2.T7.2.12.11.3\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">204</td>\n<td id=\"A2.T7.2.12.11.4\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">859</td>\n</tr>\n<tr id=\"A2.T7.2.13.12\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.13.12.1\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">7</td>\n<td id=\"A2.T7.2.13.12.2\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Title reads</td>\n<td id=\"A2.T7.2.13.12.3\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">471</td>\n<td id=\"A2.T7.2.13.12.4\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">388</td>\n</tr>\n<tr id=\"A2.T7.2.14.13\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.14.13.1\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">8</td>\n<td id=\"A2.T7.2.14.13.2\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Abstract reads</td>\n<td id=\"A2.T7.2.14.13.3\" class=\"ltx_td ltx_nopad_l\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"></td>\n<td id=\"A2.T7.2.14.13.4\" class=\"ltx_td ltx_nopad_l\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"></td>\n</tr>\n<tr id=\"A2.T7.2.15.14\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.15.14.1\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">   8.1</td>\n<td id=\"A2.T7.2.15.14.2\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Remove inaccessible abstracts</td>\n<td id=\"A2.T7.2.15.14.3\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">10</td>\n<td id=\"A2.T7.2.15.14.4\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">378</td>\n</tr>\n<tr id=\"A2.T7.2.16.15\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.16.15.1\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">   8.2</td>\n<td id=\"A2.T7.2.16.15.2\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">First abstract round</td>\n<td id=\"A2.T7.2.16.15.3\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">211</td>\n<td id=\"A2.T7.2.16.15.4\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">167</td>\n</tr>\n<tr id=\"A2.T7.2.17.16\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.17.16.1\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">   8.3</td>\n<td id=\"A2.T7.2.17.16.2\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Second abstract round</td>\n<td id=\"A2.T7.2.17.16.3\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">40</td>\n<td id=\"A2.T7.2.17.16.4\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">127</td>\n</tr>\n<tr id=\"A2.T7.2.18.17\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.18.17.1\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">9</td>\n<td id=\"A2.T7.2.18.17.2\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Full paper reads</td>\n<td id=\"A2.T7.2.18.17.3\" class=\"ltx_td ltx_nopad_l\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"></td>\n<td id=\"A2.T7.2.18.17.4\" class=\"ltx_td ltx_nopad_l\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"></td>\n</tr>\n<tr id=\"A2.T7.2.19.18\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.19.18.1\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">   9.1</td>\n<td id=\"A2.T7.2.19.18.2\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">First full paper round</td>\n<td id=\"A2.T7.2.19.18.3\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">52</td>\n<td id=\"A2.T7.2.19.18.4\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">75</td>\n</tr>\n<tr id=\"A2.T7.2.20.19\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.20.19.1\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">   9.2</td>\n<td id=\"A2.T7.2.20.19.2\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Feature discretization and extraction</td>\n<td id=\"A2.T7.2.20.19.3\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2</td>\n<td id=\"A2.T7.2.20.19.4\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">73</td>\n</tr>\n<tr id=\"A2.T7.2.21.20\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.21.20.1\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">   9.3</td>\n<td id=\"A2.T7.2.21.20.2\" class=\"ltx_td ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Second full paper round</td>\n<td id=\"A2.T7.2.21.20.3\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0</td>\n<td id=\"A2.T7.2.21.20.4\" class=\"ltx_td ltx_nopad_l ltx_align_left\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">73</td>\n</tr>\n<tr id=\"A2.T7.2.22.21\" class=\"ltx_tr\">\n<td id=\"A2.T7.2.22.21.1\" class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">   9.4</td>\n<td id=\"A2.T7.2.22.21.2\" class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Second feature extraction round</td>\n<td id=\"A2.T7.2.22.21.3\" class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_bb\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">0</td>\n<td id=\"A2.T7.2.22.21.4\" class=\"ltx_td ltx_nopad_l ltx_align_left ltx_border_bb\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">73</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Our initial search yielded a total of 4,200 papers (14 unique search terms * 3 spellings of multimodal * 100 publications per search string). Our corpus reduction procedure is enumerated in Table 7 and discussed in the following subappendices. Throughout this appendix, each step of our corpus reduction procedure is identified via its Step ID in Table 7.",
            "Our initial corpus contained 2,079 duplicates, which were removed by hashing paper titles (Table 7, Step ID 1). If a paper had multiple versions (or other duplicates), we used the official source (e.g., journal or conference) of publication. We removed 1 non-English paper (Table 7, Step ID 2) due to pragmatism (English is the only language shared between all of this review’s authors). Non-English papers were identified using spaCy FastLang (Thomas Thiebaud”, 2020), where any paper whose title was identified as having less than a 100% chance of being English was selected for manual review and potential exclusion. In total, our initial search yielded 2,120 unique English papers published within our search window.",
            "Once the DAG was constructed, we removed all 0-degree nodes (Table 7, Step ID 3; i.e., nodes with no edges coming in or going out). We felt it reasonable that if a paper did not cite (or was not cited by) any other papers in the field (as determined by our literature search), then the paper was either not relevant to the field or did not yield methods or findings referenced by subsequent works. Importantly, our approach strikes a balance between incoming and outgoing citations, as earlier works are unable to cite many works in the corpus, and later works are unable to be cited by many works in the corpus. For example, works from early 2017 may not have any outgoing edges simply due to being some of the earliest works in the corpus, which would have prevented them from citing papers that had not yet been published. However, these same papers had a greater opportunity to be cited by subsequent papers, which is why we felt it important to consider both incoming and outgoing edges. We expected earlier papers to have more incoming edges and later papers to have more outgoing edges, which was supported by our final corpus’s relatively uniform distribution over publication years. Altogether, pruning 0-degree nodes from the DAG reduced our corpus by 488, dropping our corpus count to 1,632 works.",
            "After removing 0-degree nodes, we examined the DAG’s connectivity (Table 7, Step ID 4) to identify disconnected components deemed irrelevant to the field, which was necessary to account for overlapping terminology across domains. For example, a cursory look at our initial search results included several ”multimodal training” papers related to deep learning (DL), where artificial neural networks (ANNs) are trained using data across multiple modalities but are not applied to multimodal learning or training environments. Our hypothesis, based on our search strings, was that the works relevant to this review would comprise the largest component of the DAG, leaving other smaller, disconnected components to be discarded as irrelevant because they lacked any edge to or from the DAG’s primary component.",
            "Once we had our single component graph, we removed 1-degree nodes to further prune it. This created new 1-degree nodes, which were also removed. This process of removing 1-degree nodes was repeated four times (Table 7, Step ID 5) until the graph was stable (i.e., removing 1-degree nodes did not create any new 1-degree nodes). By iteratively removing 1-degree nodes, we felt we could effectively identify and remove works outside the scope of our literature review without losing works directly related to multimodal learning and training environments. This is because the field of multimodal learning and training environments spans several sub-fields across computer science, education, psychology, etc., and the authors agreed it was unlikely papers with so few edges would be relevant to our review. We removed 373 nodes in the first iteration (Table 7, Step ID 5.1), 74 nodes in the second iteration (Table 7, Step ID 5.2), 19 nodes in the third iteration (Table 7, Step ID 5.3), and 2 nodes in the fourth and final iteration (Table 7, Step ID 5.4). Altogether, we removed 468 papers over four iterations, reducing our corpus from 1,531 papers to 1,063. The CGP pseudocode is presented in Section 3.2.1 (Algorithm 1). At this point we concluded our quantitative pruning procedure and began qualitatively reducing the corpus.",
            "Manually examining the remaining 1,063 titles informed us that a large part of our corpus was still outside the scope of our review. First, we noticed there were still many papers related to training multimodal neural networks. We also noticed many works applying multimodal methods to the medical field, usually in terms of medical imaging. To remove papers pertaining to multimodal neural network training and multimodal medical applications, we programmatically identified 217 titles via regex keyword search (Table 7, Step ID 6) that contained at least one of the six following words: neural, deep, machine, medical, medicine, and healthcare. We then evaluated the selected titles by hand. Of the 217, 13 were kept in the corpus due to their potential relevance to our review. Papers employing deep learning methods in MMLA or applying multimodal methods to medical learning or training environments were within our scope, for example. Specific examples include removing one paper titled, ”deep learning for object detection and scene perception in self-driving cars: survey, challenges, and open issues” (Gupta et al., 2021); and keeping one titled, ”supervised machine learning in multimodal learning analytics for estimating success in project‐based learning” (Spikol et al., 2018). The remaining 204 papers were removed from the corpus, reducing it to 859 potentially relevant works.",
            "Next, we selected papers for exclusion based on consensus. Pursuant to Kitchenham (Kitchenham, 2004), we initially excluded works based on reading papers’ titles, then abstracts, and eventually full manuscripts. The first five authors of this review acted as reviewers (henceforth referred to as ”the Reviewers”) for the quality control procedure. For the title reads (Table 7, Step ID 7), four of the Reviewers read all 859 titles. For each title, each Reviewer independently determined whether the title was likely to fall inside the scope of the review. The results were tallied, and papers were then selected for inclusion/exclusion based on majority voting, i.e., papers with at least three votes ”for” were automatically included, and papers with at least three votes ”against” were automatically excluded. For the papers with a 2-2 tie, a fifth reviewer was used as a tie breaker. The\nReviewers selected 347 papers for inclusion and 372 papers for exclusion. 140 papers were tied, and a fifth reviewer selected 41 of those for inclusion. In total, 388 papers were selected for inclusion after the title reads — 347 by majority vote, and 41 by tie-breaker.",
            "Before conducting the abstract reads (Table 7, Step ID 8), several works were excluded due to their inaccessibility (Table 7, Step ID 8.1). While gathering the abstracts, we noticed not all papers were publicly available. Several were defined by invalid URLs or behind paywalls. Whenever a paper’s abstract (or introduction, in the case of a book or book chapter) was unavailable via its SerpAPI URL, a Google search was conducted in order to obtain the abstract manually through websites such as ResearchGate and other academic repositories. When this failed, we relied on the Vanderbilt University Library’s proxy to access papers behind paywalls. If we were unable to freely access a paper’s abstract online through Google search or via Vanderbilt’s proxy, the paper was excluded from the corpus. Altogether, 10 papers were removed due to inaccessibility, leaving 378 papers for abstract reads.",
            "The ”abstracts” quality control procedure consisted of two rounds. Similar to the procedure for the title reads, each of the remaining 378 abstracts was first assigned to two Reviewers, and a majority voting scheme was employed (Table 7, Step ID 8.2). Papers were then selected for inclusion or exclusion based on a predefined set of exclusion criteria. The exclusion criteria for the abstracts is listed in Table 9. Exclusion criteria are cumulative, so each criterion applies to subsequent steps in our corpus reduction procedure. An exclusion criterion for the abstracts will similarly apply to full paper reads later on, for example.",
            "Of the 378 abstracts, Reviewers agreed to keep 96 papers (i.e., both Reviewers selected the work for inclusion) and discard 211 (i.e., both Reviewers selected the work for exclusion). 71 were selected for further review (i.e., one reviewer selected the work for inclusion and one reviewer selected the work for exclusion). To address the 71 abstracts that did not receive unanimous agreement among Reviewers, a second round of abstract reads was performed (Table 7, Step ID 8.3). This round consisted of each of the 71 abstracts without unanimous agreement receiving three additional reads: one read from each of the three Reviewers who did not read the abstract in the initial abstract round. Each of the 71 papers was subsequently included or excluded based on majority voting (i.e., papers were kept if and only if at least two out of the three second abstract round Reviewers elected to keep the abstract in the corpus). Of the 71 second abstract round papers, 31 were selected for inclusion, and 40 were removed from the corpus. With 96 papers selected for inclusion from the first round of abstract reads, and 31 papers selected from the second round, 127 papers in total were kept in the corpus for the next round of quality control: full paper reads.",
            "The ”full paper” quality control procedure also involved two rounds of review. To conduct full paper reads (Table 7, Step ID 9), the 127 papers kept from the abstract round were split into 5 approximately equal partitions and randomly assigned to the 5 Reviewers. Conducting full paper reads took several weeks, during which two additional exclusion criteria were defined. They are enumerated in Table 10.",
            "During the first round of full paper reads (Table 7, Step ID 9.1), Reviewers marked each paper as ”immediate exclude,” ”immediate accept,” ”borderline exclude,” or ”borderline accept.” Papers marked as ”immediate exclude” were discussed by all 5 Reviewers and excluded only if all agreed. These were papers with easily identifiable reasons for exclusion based on our criteria (for instance, a proposed theoretical framework with no analysis or a doctoral consortium presenting ideas for future research). No papers were ever excluded from our corpus during full paper reads without unanimous agreement from all five Reviewers. Papers marked as ”immediate accept” were kept in the corpus for the second full paper read round. Papers marked as ”borderline exclude” or ”borderline accept” were assigned to a separate reader for further review and were subsequently discussed. Similar to papers marked for immediate exclusion, borderline papers were excluded prior to the second full paper read round only if all Reviewers agreed. Altogether, 52 papers were excluded during the first round of full paper reads, which left 75 works remaining in the corpus.",
            "During the first full paper read round, several features were extracted from each paper (Table 7, Step ID 9.2). Features included identifying information (e.g., title, first author, publication year), and information related to the paper’s methods (e.g., data collection mediums, modalities, and analysis methods). The extracted features and their descriptions are found in Table 11.444For the ”Year” category, we used the date the manuscript was first publicly available (if listed, otherwise we used the publication date) in order to most accurately represent when the methods were performed. In some instances, the first date of online availability preceded the official publication date by over a year. Additionally, only data that was ultimately used in the paper’s analysis was considered for the ”Data Collection Mediums” category (i.e., if data was collected but never analyzed, we did not include it).",
            "After the first read, the Reviewers discussed their extracted features. To ensure alignment and understanding between the Reviewers with respect to the features, feature categories were discretized via inductive coding (Thomas, 2006), where four Reviewers each extracted initial feature sets from 25% of the corpus’s papers. For example, the initially extracted data collection mediums feature included instances of video camera, web camera, and Kinect camera, all of which were mapped to the ”VIDEO” data collection medium. Once the Reviewers agreed on the discrete sets of features, papers were reread by their original Reviewers, and their features were extracted into the discrete sets. The initial feature-space is described below in Table 12. We call these features circumscribing features to delineate them relative to the identifying features (e.g., UUID, paper title, author, etc.) that were extracted for identification purposes but not used in our analysis. In total, two sets of circumscribing features were extracted from the corpus to gather the information needed to conduct our analysis (Table 7, Step IDs 9.2 and 9.4).",
            "During feature discretization and extraction (Table 7, Step ID 9.2), additional papers were newly identified for possible exclusion pursuant to our aforementioned criteria. After discussing each paper selected for possible exclusion, 2 papers were removed from the corpus due to all five Reviewers agreeing that each paper violated at least one exclusion criterion. After the two removals, 73 papers remained in the corpus, all of whose features were extracted into discrete sets pursuant to Table 11 by the first full paper read round reviewer. At this point, a second and final quality control round was performed for full paper reads (Table 7, Step ID 9.3), where each of the 73 papers remaining in the corpus was assigned to a reviewer who had not yet read that particular paper. For this round, Reviewers were instructed to perform two tasks: identify any papers remaining in the corpus that violated any of the exclusion criteria (to discuss later for possible exclusion), and perform a round of feature extraction (to determine inter-rater reliability, or IRR, with respect to the initial feature extraction via Cohen’s k𝑘k (Cohen, 1960)). For this round, no additional papers were identified for exclusion, resulting in a final corpus of 73 works. Each paper’s discrete feature sets were ultimately determined via consensus coding (Chinh et al., 2019) by the two Reviewers who read that particular paper (i.e., for each paper, both Reviewers needed to agree on the presence or absence of each item in each feature’s feature set). For reference, Cohen’s k𝑘k before consensus for the first round of feature extraction was k=0.873𝑘0.873k=0.873.",
            "Once our corpus was finalized, we performed one additional round of feature extraction (Table 7, Step ID 9.4) to allow for greater insight into the corpus via a more in depth analysis. The features we extracted are: Environment Setting, Domain of Study, Participant Interaction Structure, Didactic Nature, Level of Instruction or Training, Analysis Approach, and Analysis Results (the findings reported from each paper). All of these features are explained in Section 2.1 and presented again here in Table 13 for readability alongside their discrete values. The one exception is Analysis Results, which was not discretized due to the wide degree of variability across each paper’s findings. Instead, we noted each paper’s findings, and used them in our thematic analysis (Braun and Clarke, 2006), which we describe in Section 3.4."
        ]
    }
}{
    "A2.T8.2": {
        "caption": [
            "Table 8",
            "Disconnected DAG components by number of nodes in the component (size), frequency of occurrence (#), and total number of papers (papers). For instance, the first row indicates that there were 35 disconnected components of size 2 in the graph, totaling to 70 papers."
        ],
        "table": "<table id=\"A2.T8.2\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A2.T8.2.1.1\" class=\"ltx_tr\">\n<th id=\"A2.T8.2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column\">Size</th>\n<th id=\"A2.T8.2.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column\">#</th>\n<th id=\"A2.T8.2.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column\">Papers</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A2.T8.2.2.1\" class=\"ltx_tr\">\n<td id=\"A2.T8.2.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\">2</td>\n<td id=\"A2.T8.2.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_t\">35</td>\n<td id=\"A2.T8.2.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_t\">70</td>\n</tr>\n<tr id=\"A2.T8.2.3.2\" class=\"ltx_tr\">\n<td id=\"A2.T8.2.3.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\">3</td>\n<td id=\"A2.T8.2.3.2.2\" class=\"ltx_td ltx_align_left ltx_border_t\">6</td>\n<td id=\"A2.T8.2.3.2.3\" class=\"ltx_td ltx_align_left ltx_border_t\">18</td>\n</tr>\n<tr id=\"A2.T8.2.4.3\" class=\"ltx_tr\">\n<td id=\"A2.T8.2.4.3.1\" class=\"ltx_td ltx_align_left ltx_border_t\">4</td>\n<td id=\"A2.T8.2.4.3.2\" class=\"ltx_td ltx_align_left ltx_border_t\">2</td>\n<td id=\"A2.T8.2.4.3.3\" class=\"ltx_td ltx_align_left ltx_border_t\">8</td>\n</tr>\n<tr id=\"A2.T8.2.5.4\" class=\"ltx_tr\">\n<td id=\"A2.T8.2.5.4.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">5</td>\n<td id=\"A2.T8.2.5.4.2\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">1</td>\n<td id=\"A2.T8.2.5.4.3\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">5</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Evaluating the DAG’s connectivity, we found one large component consisting of 1,531 nodes (papers) and 44 smaller, disconnected components of various sizes totaling 101 papers. The sizes of the disconnected components, their frequencies of occurrence in the DAG, and the total number of papers for each component size are listed in Table 8. All 101 papers were removed from the corpus by pruning the DAG’s disconnected components, which left 1,531 papers represented by a single, connected graph."
        ]
    }
}{
    "A2.T9.2": {
        "caption": [
            "Table 9",
            "Exclusion criteria for the abstract reads. Each of the 378 abstracts was assigned to two different Reviewers. Each reviewer was instructed to exclude works based on this set of criteria."
        ],
        "table": "<table id=\"A2.T9.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"A2.T9.2.1.1\" class=\"ltx_tr\">\n<td id=\"A2.T9.2.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\">1. Paper does not deal with learning or training environments</td>\n</tr>\n<tr id=\"A2.T9.2.2.2\" class=\"ltx_tr\">\n<td id=\"A2.T9.2.2.2.1\" class=\"ltx_td ltx_align_left\">2. Paper’s environment is VR-only</td>\n</tr>\n<tr id=\"A2.T9.2.3.3\" class=\"ltx_tr\">\n<td id=\"A2.T9.2.3.3.1\" class=\"ltx_td ltx_align_left\">3. Paper does not analyze multimodal data</td>\n</tr>\n<tr id=\"A2.T9.2.4.4\" class=\"ltx_tr\">\n<td id=\"A2.T9.2.4.4.1\" class=\"ltx_td ltx_align_left\">4. Paper does not apply multimodal analysis methods</td>\n</tr>\n<tr id=\"A2.T9.2.5.5\" class=\"ltx_tr\">\n<td id=\"A2.T9.2.5.5.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">5. Paper is not original applied research</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "The ”abstracts” quality control procedure consisted of two rounds. Similar to the procedure for the title reads, each of the remaining 378 abstracts was first assigned to two Reviewers, and a majority voting scheme was employed (Table 7, Step ID 8.2). Papers were then selected for inclusion or exclusion based on a predefined set of exclusion criteria. The exclusion criteria for the abstracts is listed in Table 9. Exclusion criteria are cumulative, so each criterion applies to subsequent steps in our corpus reduction procedure. An exclusion criterion for the abstracts will similarly apply to full paper reads later on, for example."
        ]
    }
}{
    "A2.T10.2": {
        "caption": [
            "Table 10",
            "Exclusion criteria for full paper reads. Each reviewer was instructed to recommend works for exclusion based on this set of criteria and the previously established exclusion criteria."
        ],
        "table": "<table id=\"A2.T10.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"A2.T10.2.1.1\" class=\"ltx_tr\">\n<td id=\"A2.T10.2.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\">1. Paper’s results are not informative with respect to learning or training</td>\n</tr>\n<tr id=\"A2.T10.2.2.2\" class=\"ltx_tr\">\n<td id=\"A2.T10.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">2. Paper’s analysis methods are not able to be determined from the manuscript</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "The ”full paper” quality control procedure also involved two rounds of review. To conduct full paper reads (Table 7, Step ID 9), the 127 papers kept from the abstract round were split into 5 approximately equal partitions and randomly assigned to the 5 Reviewers. Conducting full paper reads took several weeks, during which two additional exclusion criteria were defined. They are enumerated in Table 10."
        ]
    }
}{
    "A2.T11.2": {
        "caption": [
            "Table 11",
            "Initial features extracted from each paper."
        ],
        "table": "<table id=\"A2.T11.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A2.T11.2.1.1\" class=\"ltx_tr\">\n<th id=\"A2.T11.2.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T11.2.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T11.2.1.1.1.1.1\" class=\"ltx_p\" style=\"width:108.4pt;\">Feature</span>\n</span>\n</th>\n<th id=\"A2.T11.2.1.1.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T11.2.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T11.2.1.1.2.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Description</span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A2.T11.2.2.1\" class=\"ltx_tr\">\n<td id=\"A2.T11.2.2.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T11.2.2.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T11.2.2.1.1.1.1\" class=\"ltx_p\" style=\"width:108.4pt;\">UUID</span>\n</span>\n</td>\n<td id=\"A2.T11.2.2.1.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T11.2.2.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T11.2.2.1.2.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Universally unique identifier on Google Scholar</span>\n</span>\n</td>\n</tr>\n<tr id=\"A2.T11.2.3.2\" class=\"ltx_tr\">\n<td id=\"A2.T11.2.3.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T11.2.3.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T11.2.3.2.1.1.1\" class=\"ltx_p\" style=\"width:108.4pt;\">Title</span>\n</span>\n</td>\n<td id=\"A2.T11.2.3.2.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T11.2.3.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T11.2.3.2.2.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Publication title</span>\n</span>\n</td>\n</tr>\n<tr id=\"A2.T11.2.4.3\" class=\"ltx_tr\">\n<td id=\"A2.T11.2.4.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T11.2.4.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T11.2.4.3.1.1.1\" class=\"ltx_p\" style=\"width:108.4pt;\">First Author</span>\n</span>\n</td>\n<td id=\"A2.T11.2.4.3.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T11.2.4.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T11.2.4.3.2.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Publication’s first author</span>\n</span>\n</td>\n</tr>\n<tr id=\"A2.T11.2.5.4\" class=\"ltx_tr\">\n<td id=\"A2.T11.2.5.4.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T11.2.5.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T11.2.5.4.1.1.1\" class=\"ltx_p\" style=\"width:108.4pt;\">Year</span>\n</span>\n</td>\n<td id=\"A2.T11.2.5.4.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T11.2.5.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T11.2.5.4.2.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Year publication was first publicly available</span>\n</span>\n</td>\n</tr>\n<tr id=\"A2.T11.2.6.5\" class=\"ltx_tr\">\n<td id=\"A2.T11.2.6.5.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T11.2.6.5.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T11.2.6.5.1.1.1\" class=\"ltx_p\" style=\"width:108.4pt;\">Environment Type</span>\n</span>\n</td>\n<td id=\"A2.T11.2.6.5.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T11.2.6.5.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T11.2.6.5.2.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Type of environment analyzed in the publication</span>\n</span>\n</td>\n</tr>\n<tr id=\"A2.T11.2.7.6\" class=\"ltx_tr\">\n<td id=\"A2.T11.2.7.6.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T11.2.7.6.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T11.2.7.6.1.1.1\" class=\"ltx_p\" style=\"width:108.4pt;\">Data Collection Mediums</span>\n</span>\n</td>\n<td id=\"A2.T11.2.7.6.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T11.2.7.6.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T11.2.7.6.2.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Types of data collected from the environment</span>\n</span>\n</td>\n</tr>\n<tr id=\"A2.T11.2.8.7\" class=\"ltx_tr\">\n<td id=\"A2.T11.2.8.7.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T11.2.8.7.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T11.2.8.7.1.1.1\" class=\"ltx_p\" style=\"width:108.4pt;\">Modalities</span>\n</span>\n</td>\n<td id=\"A2.T11.2.8.7.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T11.2.8.7.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T11.2.8.7.2.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">List of the different modalities used during analysis</span>\n</span>\n</td>\n</tr>\n<tr id=\"A2.T11.2.9.8\" class=\"ltx_tr\">\n<td id=\"A2.T11.2.9.8.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T11.2.9.8.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T11.2.9.8.1.1.1\" class=\"ltx_p\" style=\"width:108.4pt;\">Analysis Methods</span>\n</span>\n</td>\n<td id=\"A2.T11.2.9.8.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T11.2.9.8.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T11.2.9.8.2.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">List of the analysis methods used in the publication</span>\n</span>\n</td>\n</tr>\n<tr id=\"A2.T11.2.10.9\" class=\"ltx_tr\">\n<td id=\"A2.T11.2.10.9.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T11.2.10.9.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T11.2.10.9.1.1.1\" class=\"ltx_p\" style=\"width:108.4pt;\">Fusion Type</span>\n</span>\n</td>\n<td id=\"A2.T11.2.10.9.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T11.2.10.9.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T11.2.10.9.2.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">List of data fusion types used in the publication</span>\n</span>\n</td>\n</tr>\n<tr id=\"A2.T11.2.11.10\" class=\"ltx_tr\">\n<td id=\"A2.T11.2.11.10.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T11.2.11.10.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T11.2.11.10.1.1.1\" class=\"ltx_p\" style=\"width:108.4pt;\">Publication Source</span>\n</span>\n</td>\n<td id=\"A2.T11.2.11.10.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T11.2.11.10.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T11.2.11.10.2.1.1\" class=\"ltx_p\" style=\"width:281.9pt;\">Publication journal, conference, workshop, etc.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "During the first full paper read round, several features were extracted from each paper (Table 7, Step ID 9.2). Features included identifying information (e.g., title, first author, publication year), and information related to the paper’s methods (e.g., data collection mediums, modalities, and analysis methods). The extracted features and their descriptions are found in Table 11.444For the ”Year” category, we used the date the manuscript was first publicly available (if listed, otherwise we used the publication date) in order to most accurately represent when the methods were performed. In some instances, the first date of online availability preceded the official publication date by over a year. Additionally, only data that was ultimately used in the paper’s analysis was considered for the ”Data Collection Mediums” category (i.e., if data was collected but never analyzed, we did not include it).",
            "During feature discretization and extraction (Table 7, Step ID 9.2), additional papers were newly identified for possible exclusion pursuant to our aforementioned criteria. After discussing each paper selected for possible exclusion, 2 papers were removed from the corpus due to all five Reviewers agreeing that each paper violated at least one exclusion criterion. After the two removals, 73 papers remained in the corpus, all of whose features were extracted into discrete sets pursuant to Table 11 by the first full paper read round reviewer. At this point, a second and final quality control round was performed for full paper reads (Table 7, Step ID 9.3), where each of the 73 papers remaining in the corpus was assigned to a reviewer who had not yet read that particular paper. For this round, Reviewers were instructed to perform two tasks: identify any papers remaining in the corpus that violated any of the exclusion criteria (to discuss later for possible exclusion), and perform a round of feature extraction (to determine inter-rater reliability, or IRR, with respect to the initial feature extraction via Cohen’s k𝑘k (Cohen, 1960)). For this round, no additional papers were identified for exclusion, resulting in a final corpus of 73 works. Each paper’s discrete feature sets were ultimately determined via consensus coding (Chinh et al., 2019) by the two Reviewers who read that particular paper (i.e., for each paper, both Reviewers needed to agree on the presence or absence of each item in each feature’s feature set). For reference, Cohen’s k𝑘k before consensus for the first round of feature extraction was k=0.873𝑘0.873k=0.873."
        ]
    }
}{
    "A2.T12.2": {
        "caption": [
            "Table 12",
            "The first set of circumscribing features and their corresponding feature sets. For Environment Type, items in the feature set are mutually exclusive (i.e., an environment can either be a learning or training environment for the purposes of this paper, but it cannot be both). All other circumscribing features can consist of multiple items in the feature set (e.g., each paper in our corpus will contain multiple data collection mediums or modalities). Features are discussed individually in Section 2.2.",
            "Environment Type",
            "or",
            "2.2"
        ],
        "table": "<table id=\"A2.T12.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A2.T12.2.1.1\" class=\"ltx_tr\">\n<th id=\"A2.T12.2.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T12.2.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T12.2.1.1.1.1.1\" class=\"ltx_p\" style=\"width:69.4pt;\">Feature</span>\n</span>\n</th>\n<th id=\"A2.T12.2.1.1.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T12.2.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T12.2.1.1.2.1.1\" class=\"ltx_p\" style=\"width:325.2pt;\">Feature Set</span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A2.T12.2.2.1\" class=\"ltx_tr\">\n<td id=\"A2.T12.2.2.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T12.2.2.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T12.2.2.1.1.1.1\" class=\"ltx_p\" style=\"width:69.4pt;\">Environment Type</span>\n</span>\n</td>\n<td id=\"A2.T12.2.2.1.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T12.2.2.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T12.2.2.1.2.1.1\" class=\"ltx_p\" style=\"width:325.2pt;\">learning, training</span>\n</span>\n</td>\n</tr>\n<tr id=\"A2.T12.2.3.2\" class=\"ltx_tr\">\n<td id=\"A2.T12.2.3.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T12.2.3.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T12.2.3.2.1.1.1\" class=\"ltx_p\" style=\"width:69.4pt;\">Data Collection Mediums</span>\n</span>\n</td>\n<td id=\"A2.T12.2.3.2.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T12.2.3.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T12.2.3.2.2.1.1\" class=\"ltx_p\" style=\"width:325.2pt;\">video, audio, screen recording, eye tracking, logs, physiological sensor, interview, survey, participant produced artifacts, researcher produced artifacts, motion, text</span>\n</span>\n</td>\n</tr>\n<tr id=\"A2.T12.2.4.3\" class=\"ltx_tr\">\n<td id=\"A2.T12.2.4.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T12.2.4.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T12.2.4.3.1.1.1\" class=\"ltx_p\" style=\"width:69.4pt;\">Modalities</span>\n</span>\n</td>\n<td id=\"A2.T12.2.4.3.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T12.2.4.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T12.2.4.3.2.1.1\" class=\"ltx_p\" style=\"width:325.2pt;\">affect, pose, gesture, activity, prosodic speech, transcribed speech, qualitative observation, logs, gaze, interview notes, survey, pulse, EDA, body temperature, blood pressure, EEG, fatigue, EMG, participant artifacts, researcher artifacts, audio spectrogram, text, pixel</span>\n</span>\n</td>\n</tr>\n<tr id=\"A2.T12.2.5.4\" class=\"ltx_tr\">\n<td id=\"A2.T12.2.5.4.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T12.2.5.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T12.2.5.4.1.1.1\" class=\"ltx_p\" style=\"width:69.4pt;\">Analysis Methods</span>\n</span>\n</td>\n<td id=\"A2.T12.2.5.4.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T12.2.5.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T12.2.5.4.2.1.1\" class=\"ltx_p\" style=\"width:325.2pt;\">Classification, regression, clustering, qualitative, statistical methods, network analysis, pattern extraction</span>\n</span>\n</td>\n</tr>\n<tr id=\"A2.T12.2.6.5\" class=\"ltx_tr\">\n<td id=\"A2.T12.2.6.5.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T12.2.6.5.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T12.2.6.5.1.1.1\" class=\"ltx_p\" style=\"width:69.4pt;\">Fusion Type</span>\n</span>\n</td>\n<td id=\"A2.T12.2.6.5.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T12.2.6.5.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T12.2.6.5.2.1.1\" class=\"ltx_p\" style=\"width:325.2pt;\">early, mid, late, hybrid, other</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "After the first read, the Reviewers discussed their extracted features. To ensure alignment and understanding between the Reviewers with respect to the features, feature categories were discretized via inductive coding (Thomas, 2006), where four Reviewers each extracted initial feature sets from 25% of the corpus’s papers. For example, the initially extracted data collection mediums feature included instances of video camera, web camera, and Kinect camera, all of which were mapped to the ”VIDEO” data collection medium. Once the Reviewers agreed on the discrete sets of features, papers were reread by their original Reviewers, and their features were extracted into the discrete sets. The initial feature-space is described below in Table 12. We call these features circumscribing features to delineate them relative to the identifying features (e.g., UUID, paper title, author, etc.) that were extracted for identification purposes but not used in our analysis. In total, two sets of circumscribing features were extracted from the corpus to gather the information needed to conduct our analysis (Table 7, Step IDs 9.2 and 9.4)."
        ]
    }
}{
    "A2.T13.2": {
        "caption": [
            "Table 13",
            "The second set of circumscribing features, all of which are multi-label, and their corresponding feature sets. Features are discussed individually in Section 2.2.",
            "2.2"
        ],
        "table": "<table id=\"A2.T13.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A2.T13.2.1.1\" class=\"ltx_tr\">\n<th id=\"A2.T13.2.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T13.2.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T13.2.1.1.1.1.1\" class=\"ltx_p\" style=\"width:117.1pt;\">Feature</span>\n</span>\n</th>\n<th id=\"A2.T13.2.1.1.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T13.2.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T13.2.1.1.2.1.1\" class=\"ltx_p\" style=\"width:208.1pt;\">Feature Set</span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A2.T13.2.2.1\" class=\"ltx_tr\">\n<td id=\"A2.T13.2.2.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T13.2.2.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T13.2.2.1.1.1.1\" class=\"ltx_p\" style=\"width:117.1pt;\">Environment Setting</span>\n</span>\n</td>\n<td id=\"A2.T13.2.2.1.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T13.2.2.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T13.2.2.1.2.1.1\" class=\"ltx_p\" style=\"width:208.1pt;\">physical, virtual, blended, unspecified</span>\n</span>\n</td>\n</tr>\n<tr id=\"A2.T13.2.3.2\" class=\"ltx_tr\">\n<td id=\"A2.T13.2.3.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T13.2.3.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T13.2.3.2.1.1.1\" class=\"ltx_p\" style=\"width:117.1pt;\">Domain of Study</span>\n</span>\n</td>\n<td id=\"A2.T13.2.3.2.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T13.2.3.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T13.2.3.2.2.1.1\" class=\"ltx_p\" style=\"width:208.1pt;\">STEM, humanities, psychomotor skills, other, unspecified</span>\n</span>\n</td>\n</tr>\n<tr id=\"A2.T13.2.4.3\" class=\"ltx_tr\">\n<td id=\"A2.T13.2.4.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T13.2.4.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T13.2.4.3.1.1.1\" class=\"ltx_p\" style=\"width:117.1pt;\">Participant Interaction Structure</span>\n</span>\n</td>\n<td id=\"A2.T13.2.4.3.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T13.2.4.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T13.2.4.3.2.1.1\" class=\"ltx_p\" style=\"width:208.1pt;\">individual, multi-person</span>\n</span>\n</td>\n</tr>\n<tr id=\"A2.T13.2.5.4\" class=\"ltx_tr\">\n<td id=\"A2.T13.2.5.4.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T13.2.5.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T13.2.5.4.1.1.1\" class=\"ltx_p\" style=\"width:117.1pt;\">Didactic Nature</span>\n</span>\n</td>\n<td id=\"A2.T13.2.5.4.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T13.2.5.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T13.2.5.4.2.1.1\" class=\"ltx_p\" style=\"width:208.1pt;\">instructional, training, informal, unspecified</span>\n</span>\n</td>\n</tr>\n<tr id=\"A2.T13.2.6.5\" class=\"ltx_tr\">\n<td id=\"A2.T13.2.6.5.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T13.2.6.5.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T13.2.6.5.1.1.1\" class=\"ltx_p\" style=\"width:117.1pt;\">Level of Instruction or Training</span>\n</span>\n</td>\n<td id=\"A2.T13.2.6.5.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T13.2.6.5.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T13.2.6.5.2.1.1\" class=\"ltx_p\" style=\"width:208.1pt;\">K-12, university, professional development, unspecified</span>\n</span>\n</td>\n</tr>\n<tr id=\"A2.T13.2.7.6\" class=\"ltx_tr\">\n<td id=\"A2.T13.2.7.6.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T13.2.7.6.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T13.2.7.6.1.1.1\" class=\"ltx_p\" style=\"width:117.1pt;\">Analysis Approach</span>\n</span>\n</td>\n<td id=\"A2.T13.2.7.6.2\" class=\"ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">\n<span id=\"A2.T13.2.7.6.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"A2.T13.2.7.6.2.1.1\" class=\"ltx_p\" style=\"width:208.1pt;\">model-free, model-based</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Once our corpus was finalized, we performed one additional round of feature extraction (Table 7, Step ID 9.4) to allow for greater insight into the corpus via a more in depth analysis. The features we extracted are: Environment Setting, Domain of Study, Participant Interaction Structure, Didactic Nature, Level of Instruction or Training, Analysis Approach, and Analysis Results (the findings reported from each paper). All of these features are explained in Section 2.1 and presented again here in Table 13 for readability alongside their discrete values. The one exception is Analysis Results, which was not discretized due to the wide degree of variability across each paper’s findings. Instead, we noted each paper’s findings, and used them in our thematic analysis (Braun and Clarke, 2006), which we describe in Section 3.4."
        ]
    }
}