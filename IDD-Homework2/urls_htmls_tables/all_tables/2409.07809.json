{
    "id_table_1": {
        "caption": "Table 1:  F1 values for NER tasks after training encoder models generated data, base MLM model is RoBERTa",
        "table": "S4.T1.3",
        "footnotes": [],
        "references": [
            "Instruction tuning is a technique designed to enhance the usefulness of language models by training them on Instruction/Response Tuples, as illustrated in Figure  1  (example taken from OpenAI grade school math dataset  Cobbe et al. ( 2021 ) ).",
            "In table  1  we present the weighted F1 scores of each experiment. Using the original MIMIC data to domain adapt the RoBERTa model yields the best performance while not adapting at yields the worst results. The utility of domain adapting the both GPT-3 and PHI-2 is clear while the use of DP causes only a slight drop and even improvement over w/o DP. In our setup decreasing the   italic- \\epsilon italic_  value seems to yield stable utility results where the best preforming   italic- \\epsilon italic_  value on average is 4."
        ]
    }
}