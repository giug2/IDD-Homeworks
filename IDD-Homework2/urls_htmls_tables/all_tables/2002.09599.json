{
    "S1.T1": {
        "caption": "Table 1:  Questions generated by models of increasing capacity with the ground truth answer highlighted in bold. As model size grows, question quality becomes increasingly coherent, complex, and factually relevant.",
        "table": "<table id=\"S1.T1.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S1.T1.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S1.T1.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Text</th>\n<td id=\"S1.T1.1.1.1.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S1.T1.1.1.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T1.1.1.1.1.2.1.1\" class=\"ltx_p\" style=\"width:227.6pt;\">Albert Einstein is known for his theories of special relativity and general relativity. He also made important contributions to statistical mechanics, especially his mathematical treatment of Brownian motion, his resolution of the paradox of specific heats, and his connection of fluctuations and dissipation. Despite his reservations about its interpretation, Einstein also made contributions to quantum mechanics and, indirectly, <span id=\"S1.T1.1.1.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">quantum field theory</span>, primarily through his theoretical studies of the photon.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S1.T1.1.1.2.2\" class=\"ltx_tr\">\n<th id=\"S1.T1.1.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">117M</th>\n<td id=\"S1.T1.1.1.2.2.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S1.T1.1.1.2.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T1.1.1.2.2.2.1.1\" class=\"ltx_p\" style=\"width:227.6pt;\">Which two concepts made Einstein&#8217;s post on quantum mechanics relevant?</span>\n</span>\n</td>\n</tr>\n<tr id=\"S1.T1.1.1.3.3\" class=\"ltx_tr\">\n<th id=\"S1.T1.1.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">768M</th>\n<td id=\"S1.T1.1.1.3.3.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S1.T1.1.1.3.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T1.1.1.3.3.2.1.1\" class=\"ltx_p\" style=\"width:227.6pt;\">Albert Einstein also made significant contributions to which field of theory?</span>\n</span>\n</td>\n</tr>\n<tr id=\"S1.T1.1.1.4.4\" class=\"ltx_tr\">\n<th id=\"S1.T1.1.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">8.3B</th>\n<td id=\"S1.T1.1.1.4.4.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S1.T1.1.1.4.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T1.1.1.4.4.2.1.1\" class=\"ltx_p\" style=\"width:227.6pt;\">Because of his work with the photon, what theory did he indirectly contribute to?</span>\n</span>\n</td>\n</tr>\n<tr id=\"S1.T1.1.1.5.5\" class=\"ltx_tr\">\n<th id=\"S1.T1.1.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">Human</th>\n<td id=\"S1.T1.1.1.5.5.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"S1.T1.1.1.5.5.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S1.T1.1.1.5.5.2.1.1\" class=\"ltx_p\" style=\"width:227.6pt;\">What theory did Einstein have reservations about?</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "We demonstrate generated questions to be comparable to supervised training with real data. For answerable SQuAD1.1 questions we recover 100.4% of fully supervised EM and F1 scores, when training on purely synthetic questions and answers generated from unlabeled data. Specifically, we achieve scores of 88.4 and 94.1 versus supervised training which achieves 87.7 EM and 94.0 F1. Finetuning the resulting model on real SQuAD1.1 data reaches 89.4 EM and 95.1 F1 score, which is higher than any prior BERT-based approach. In Table 1, we show that the generated questions are qualitatively similar to ground truth questions, with quality improving as a function of model size."
        ]
    },
    "S4.T2": {
        "caption": "Table 2:  Finetuning BERT-345M on synthetic and human-generated data. Using 1.2B parameter models we synthesize question answer pairs from real Wikipedia corpus and synthetic cospus generated from an 8.3B GPT-2 model. Completely synthetic data does better than training with real data. Finetuning with real SQuAD1.1 data afterwards further boosts performance. ",
        "table": "<table id=\"S4.T2.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">Text</td>\n<td id=\"S4.T2.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">Source</td>\n<td id=\"S4.T2.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">finetune</td>\n<td id=\"S4.T2.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\"><span id=\"S4.T2.1.1.1.1.4.1\" class=\"ltx_text\"># Questions</span></td>\n<td id=\"S4.T2.1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\"><span id=\"S4.T2.1.1.1.1.5.1\" class=\"ltx_text\">EM</span></td>\n<td id=\"S4.T2.1.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\"><span id=\"S4.T2.1.1.1.1.6.1\" class=\"ltx_text\">F1</span></td>\n</tr>\n<tr id=\"S4.T2.1.1.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.1.2.2.1\" class=\"ltx_td ltx_align_center\">Source</td>\n<td id=\"S4.T2.1.1.2.2.2\" class=\"ltx_td ltx_align_center\">Data Size</td>\n<td id=\"S4.T2.1.1.2.2.3\" class=\"ltx_td ltx_align_center\">data</td>\n</tr>\n<tr id=\"S4.T2.1.1.3.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T2.1.1.3.3.1.1\" class=\"ltx_text\">Wikipedia</span></td>\n<td id=\"S4.T2.1.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T2.1.1.3.3.2.1\" class=\"ltx_text\">638 MB</span></td>\n<td id=\"S4.T2.1.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">Synthetic</td>\n<td id=\"S4.T2.1.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">19,925,130</td>\n<td id=\"S4.T2.1.1.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_tt\">88.4</td>\n<td id=\"S4.T2.1.1.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_tt\">94.1</td>\n</tr>\n<tr id=\"S4.T2.1.1.4.4\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.1.4.4.1\" class=\"ltx_td ltx_align_center\">+<span id=\"S4.T2.1.1.4.4.1.1\" class=\"ltx_text ltx_font_smallcaps\">SQuAD</span>\n</td>\n<td id=\"S4.T2.1.1.4.4.2\" class=\"ltx_td ltx_align_center\">20,012,729</td>\n<td id=\"S4.T2.1.1.4.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.1.1.4.4.3.1\" class=\"ltx_text ltx_font_bold\">89.4</span></td>\n<td id=\"S4.T2.1.1.4.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.1.1.4.4.4.1\" class=\"ltx_text ltx_font_bold\">95.2</span></td>\n</tr>\n<tr id=\"S4.T2.1.1.5.5\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\"><span id=\"S4.T2.1.1.5.5.1.1\" class=\"ltx_text\">8.3B GPT-2</span></td>\n<td id=\"S4.T2.1.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\"><span id=\"S4.T2.1.1.5.5.2.1\" class=\"ltx_text\">480 MB</span></td>\n<td id=\"S4.T2.1.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\">Synthetic</td>\n<td id=\"S4.T2.1.1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\">17,400,016</td>\n<td id=\"S4.T2.1.1.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_t\">88.4</td>\n<td id=\"S4.T2.1.1.5.5.6\" class=\"ltx_td ltx_align_center ltx_border_t\">93.9</td>\n</tr>\n<tr id=\"S4.T2.1.1.6.6\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.1.6.6.1\" class=\"ltx_td ltx_align_center\">+<span id=\"S4.T2.1.1.6.6.1.1\" class=\"ltx_text ltx_font_smallcaps\">SQuAD</span>\n</td>\n<td id=\"S4.T2.1.1.6.6.2\" class=\"ltx_td ltx_align_center\">17,487,615</td>\n<td id=\"S4.T2.1.1.6.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.1.1.6.6.3.1\" class=\"ltx_text ltx_font_bold\">89.1</span></td>\n<td id=\"S4.T2.1.1.6.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.1.1.6.6.4.1\" class=\"ltx_text ltx_font_bold\">94.9</span></td>\n</tr>\n<tr id=\"S4.T2.1.1.7.7\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.1.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S4.T2.1.1.7.7.1.1\" class=\"ltx_text ltx_font_smallcaps\">SQuAD1.1</span></td>\n<td id=\"S4.T2.1.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">14MB</td>\n<td id=\"S4.T2.1.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S4.T2.1.1.7.7.3.1\" class=\"ltx_text ltx_font_smallcaps\">SQuAD</span></td>\n<td id=\"S4.T2.1.1.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">87,599</td>\n<td id=\"S4.T2.1.1.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">87.7</td>\n<td id=\"S4.T2.1.1.7.7.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">94.0</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Table 2 shows results when the synthetic data is finetuned on a BERT-345M QA model. We showcase that we are able to recover and surpass the performance of real data by only using synthetic data generated from synthetic corpus. Using real questions synthesized on real Wikipedia data we do even better. Finetuning this model afterwards on the actual SQuAD1.1 dataset allows us to achieve a 1.7 and 1.2 point boost to our EM and F1 scores. In Figure 3 we examine the relationship between SQuAD1.1 score and the amount of text labeled. We find that the performance of training with purely synthetic data observes a log\\log-linear relationship that begins to saturate at approximately 100 MB of text labeled. However, finetuning these models on labeled SQuAD1.1 data demonstrates continued improvement even beyond saturation. The performance of these post finetuned models continues to improve even past 500 MB of data labeled."
        ]
    },
    "S4.T3": {
        "caption": "Table 3:  Comparison with prior work.\nImprovements in question generation allow for improved SQuAD2.0 score even without generating unanswerable questions.\n",
        "table": "<table id=\"S4.T3.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">Implementation</th>\n<th id=\"S4.T3.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">EM</th>\n<th id=\"S4.T3.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">F1</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\">BERT-Large <cite class=\"ltx_cite ltx_citemacro_citep\">(Alberti et&#160;al., <a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">2019a</a>)</cite>\n</td>\n<td id=\"S4.T3.1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">78.7</td>\n<td id=\"S4.T3.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">81.9</td>\n</tr>\n<tr id=\"S4.T3.1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.3.2.1\" class=\"ltx_td ltx_align_left\">&#160;&#160;&#160;&#160;&#160;+ 3M Questions</td>\n<td id=\"S4.T3.1.1.3.2.2\" class=\"ltx_td ltx_align_center\">80.1</td>\n<td id=\"S4.T3.1.1.3.2.3\" class=\"ltx_td ltx_align_center\">82.8</td>\n</tr>\n<tr id=\"S4.T3.1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_border_t\">UniLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Dong et&#160;al., <a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>\n</td>\n<td id=\"S4.T3.1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">80.5</td>\n<td id=\"S4.T3.1.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">83.4</td>\n</tr>\n<tr id=\"S4.T3.1.1.5.4\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.5.4.1\" class=\"ltx_td ltx_align_left\">&#160;&#160;&#160;&#160;&#160;+ 9M Questions</td>\n<td id=\"S4.T3.1.1.5.4.2\" class=\"ltx_td ltx_align_center\">84.7</td>\n<td id=\"S4.T3.1.1.5.4.3\" class=\"ltx_td ltx_align_center\">87.6</td>\n</tr>\n<tr id=\"S4.T3.1.1.6.5\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_border_t\">BERT-Large</td>\n<td id=\"S4.T3.1.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\">77.4</td>\n<td id=\"S4.T3.1.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\">80.6</td>\n</tr>\n<tr id=\"S4.T3.1.1.7.6\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.7.6.1\" class=\"ltx_td ltx_align_left\">&#160;&#160;&#160;&#160;&#160;+ 3M Questions</td>\n<td id=\"S4.T3.1.1.7.6.2\" class=\"ltx_td ltx_align_center\">81.6</td>\n<td id=\"S4.T3.1.1.7.6.3\" class=\"ltx_td ltx_align_center\">84.5</td>\n</tr>\n<tr id=\"S4.T3.1.1.8.7\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.8.7.1\" class=\"ltx_td ltx_align_left ltx_border_t\">BERT-345M</td>\n<td id=\"S4.T3.1.1.8.7.2\" class=\"ltx_td ltx_align_center ltx_border_t\">84.9</td>\n<td id=\"S4.T3.1.1.8.7.3\" class=\"ltx_td ltx_align_center ltx_border_t\">88.2</td>\n</tr>\n<tr id=\"S4.T3.1.1.9.8\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.9.8.1\" class=\"ltx_td ltx_align_left\">&#160;&#160;&#160;&#160;&#160;+ 3M Questions</td>\n<td id=\"S4.T3.1.1.9.8.2\" class=\"ltx_td ltx_align_center\">85.8</td>\n<td id=\"S4.T3.1.1.9.8.3\" class=\"ltx_td ltx_align_center\">88.6</td>\n</tr>\n<tr id=\"S4.T3.1.1.10.9\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.10.9.1\" class=\"ltx_td ltx_align_left ltx_border_b\">&#160;&#160;<span id=\"S4.T3.1.1.10.9.1.1\" class=\"ltx_text ltx_font_bold\">&#160;&#160;&#160;+ 8M Questions</span>\n</td>\n<td id=\"S4.T3.1.1.10.9.2\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S4.T3.1.1.10.9.2.1\" class=\"ltx_text ltx_font_bold\">86.4</span></td>\n<td id=\"S4.T3.1.1.10.9.3\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S4.T3.1.1.10.9.3.1\" class=\"ltx_text ltx_font_bold\">89.2</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "To quantify the improvements in question generation quality derived from improvements to language models and our generation techniques we compare our results to the original roundtrip consistency work from (Alberti et\u00a0al., 2019a). We generate 3 million questions from real Wikipedia text and finetune the public BERT-Large model on this data. We then finetune the model on the human-generated SQuAD2.0 dataset and evaluate on the dev set. Unlike prior work we do not generate any unanswerable questions, yet we find in Table 3 that our synthetic data approach outperforms the prior work. This is despite our BERT-Large baseline underperforming the numbers reported in (Alberti et\u00a0al., 2019a) by a full point. We also compare our methods with the state of the art in synthetically trained SQuAD2.0 (Dong et\u00a0al., 2019) and find that with a similar number of questions we outperform existing methods, and with even more labeled data this trend persists."
        ]
    },
    "S5.T4": {
        "caption": "Table 4:  SQuAD1.1 performance using synthetic data. Downstream QA models used in all experiments are 345M parameters.",
        "table": "<table id=\"S5.T4.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T4.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"4\">Model Size</th>\n<th id=\"S5.T4.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" rowspan=\"2\"><span id=\"S5.T4.1.1.1.1.2.1\" class=\"ltx_text\"># Questions</span></th>\n<th id=\"S5.T4.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" rowspan=\"2\"><span id=\"S5.T4.1.1.1.1.3.1\" class=\"ltx_text\">EM</span></th>\n<th id=\"S5.T4.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" rowspan=\"2\"><span id=\"S5.T4.1.1.1.1.4.1\" class=\"ltx_text\">F1</span></th>\n</tr>\n<tr id=\"S5.T4.1.1.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Answer</th>\n<th id=\"S5.T4.1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Question</th>\n<th id=\"S5.T4.1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Filter</th>\n<th id=\"S5.T4.1.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">QA</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T4.1.1.3.1\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.1.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\">345M</td>\n<td id=\"S5.T4.1.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">345M</td>\n<td id=\"S5.T4.1.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">345M</td>\n<td id=\"S5.T4.1.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">345M</td>\n<td id=\"S5.T4.1.1.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\">116721</td>\n<td id=\"S5.T4.1.1.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_tt\">85.3</td>\n<td id=\"S5.T4.1.1.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_tt\">92.0</td>\n</tr>\n<tr id=\"S5.T4.1.1.4.2\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.1.4.2.1\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T4.1.1.4.2.1.1\" class=\"ltx_text ltx_font_bold\">1.2B</span></td>\n<td id=\"S5.T4.1.1.4.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T4.1.1.4.2.2.1\" class=\"ltx_text ltx_font_bold\">1.2B</span></td>\n<td id=\"S5.T4.1.1.4.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T4.1.1.4.2.3.1\" class=\"ltx_text ltx_font_bold\">1.2B</span></td>\n<td id=\"S5.T4.1.1.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T4.1.1.4.2.4.1\" class=\"ltx_text ltx_font_bold\">345M</span></td>\n<td id=\"S5.T4.1.1.4.2.5\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T4.1.1.4.2.5.1\" class=\"ltx_text ltx_font_bold\">184992</span></td>\n<td id=\"S5.T4.1.1.4.2.6\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T4.1.1.4.2.6.1\" class=\"ltx_text ltx_font_bold\">87.1</span></td>\n<td id=\"S5.T4.1.1.4.2.7\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T4.1.1.4.2.7.1\" class=\"ltx_text ltx_font_bold\">93.2</span></td>\n</tr>\n<tr id=\"S5.T4.1.1.5.3\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.1.5.3.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" colspan=\"3\">Human Generated Data</td>\n<td id=\"S5.T4.1.1.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">345M</td>\n<td id=\"S5.T4.1.1.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">42472</td>\n<td id=\"S5.T4.1.1.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">86.3</td>\n<td id=\"S5.T4.1.1.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">93.2</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "A central premise of our work is that language models have advanced to the point where they are now able to generate diverse, coherent language and as a result can generate high quality question answering curricula. We show in this section that as we improve pretraining tasks, pretraining scale, and model scale, synthetic data also improves. To show improvements in question generation we track the resulting SQuAD1.1 evaluation score when a BERT-style model is finetuned on the synthetic data. Table 4 summarizes the benefits of using larger models for answer generation, question generation, and question filtration. The following subsections ablate this result to show the contributions from scaling individual components of the synthetic data pipeline."
        ]
    },
    "S5.T5": {
        "caption": "Table 5:  Effect of question generator scale on SQuAD1.1 performance. Ground truth answers are used to generate questions without filtration for finetuning.",
        "table": "<table id=\"S5.T5.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T5.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\">Question Generator</th>\n<th id=\"S5.T5.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\"># Questions</th>\n<th id=\"S5.T5.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">EM</th>\n<th id=\"S5.T5.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">F1</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T5.1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\">117M</th>\n<th id=\"S5.T5.1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\">42345</th>\n<td id=\"S5.T5.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">76.6</td>\n<td id=\"S5.T5.1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">85.0</td>\n</tr>\n<tr id=\"S5.T5.1.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">345M <cite class=\"ltx_cite ltx_citemacro_citep\">(Klein &amp; Nabi, <a href=\"#bib.bib19\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>\n</th>\n<th id=\"S5.T5.1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">-</th>\n<td id=\"S5.T5.1.1.3.2.3\" class=\"ltx_td ltx_align_center\">75.4</td>\n<td id=\"S5.T5.1.1.3.2.4\" class=\"ltx_td ltx_align_center\">84.4</td>\n</tr>\n<tr id=\"S5.T5.1.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">345M (w/ BERT QA model)</th>\n<th id=\"S5.T5.1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">42414</th>\n<td id=\"S5.T5.1.1.4.3.3\" class=\"ltx_td ltx_align_center\">76.6</td>\n<td id=\"S5.T5.1.1.4.3.4\" class=\"ltx_td ltx_align_center\">84.8</td>\n</tr>\n<tr id=\"S5.T5.1.1.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">345M</th>\n<th id=\"S5.T5.1.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">42414</th>\n<td id=\"S5.T5.1.1.5.4.3\" class=\"ltx_td ltx_align_center\">80.7</td>\n<td id=\"S5.T5.1.1.5.4.4\" class=\"ltx_td ltx_align_center\">88.6</td>\n</tr>\n<tr id=\"S5.T5.1.1.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">768M</th>\n<th id=\"S5.T5.1.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">42465</th>\n<td id=\"S5.T5.1.1.6.5.3\" class=\"ltx_td ltx_align_center\">81.0</td>\n<td id=\"S5.T5.1.1.6.5.4\" class=\"ltx_td ltx_align_center\">89.0</td>\n</tr>\n<tr id=\"S5.T5.1.1.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.1.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">1.2B</th>\n<th id=\"S5.T5.1.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">42472</th>\n<td id=\"S5.T5.1.1.7.6.3\" class=\"ltx_td ltx_align_center\">83.4</td>\n<td id=\"S5.T5.1.1.7.6.4\" class=\"ltx_td ltx_align_center\">90.9</td>\n</tr>\n<tr id=\"S5.T5.1.1.8.7\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.1.8.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><span id=\"S5.T5.1.1.8.7.1.1\" class=\"ltx_text ltx_font_bold\">8.3B</span></th>\n<th id=\"S5.T5.1.1.8.7.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><span id=\"S5.T5.1.1.8.7.2.1\" class=\"ltx_text ltx_font_bold\">42478</span></th>\n<td id=\"S5.T5.1.1.8.7.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T5.1.1.8.7.3.1\" class=\"ltx_text ltx_font_bold\">84.9</span></td>\n<td id=\"S5.T5.1.1.8.7.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T5.1.1.8.7.4.1\" class=\"ltx_text ltx_font_bold\">92.0</span></td>\n</tr>\n<tr id=\"S5.T5.1.1.9.8\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.1.9.8.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t\">Human Generated Data</th>\n<th id=\"S5.T5.1.1.9.8.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t\">42472</th>\n<td id=\"S5.T5.1.1.9.8.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">86.3</td>\n<td id=\"S5.T5.1.1.9.8.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">93.2</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Question generation plays a critical role in our synthetic data pipeline: it must synthesize linguistically and logically coherent text even if the text does not exist within the provided context. In this section we investigate the relationship between question generator scale and downstream SQuAD1.1 performance. We isolate the quality of question generation by using ground truth answers from the SQuAD1.1 dataset to generate questions and finetune a BERT model before evaluating it on the SQuAD1.1 dev set. We perform no question filtration in between generation and finetuning. From our experiments in Table 5 we find that SQuAD1.1 performance increases monotonically. Additionally, the number of valid samples that pass stopword filtration increase with larger models, indicating bigger models maintain coherency during sampling. For comparisons with prior work we train a question answering model with our ALBERT-style BERT model (BERT-345M) and the original BERT-Large model. (Klein & Nabi, 2019) use a feedback loop to improve the question generator and BERT-Large question answering model. Compared to our work we find that a similarly parameterized set of models achieve equal if not better performance despite using only a single supervised pass through the data and no feedback loop."
        ]
    },
    "S5.T6": {
        "caption": "Table 6:  Comparison of answer generator pretraining and scale.\nOur 1.2 billion parameter question generator is used for generating questions.\n",
        "table": "<table id=\"S5.T6.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T6.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T6.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\">Answer Generator</th>\n<th id=\"S5.T6.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\">#Questions</th>\n<th id=\"S5.T6.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">EM</th>\n<th id=\"S5.T6.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">F1</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T6.1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T6.1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\">BERT-Large</th>\n<th id=\"S5.T6.1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\">227063</th>\n<td id=\"S5.T6.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">77.7</td>\n<td id=\"S5.T6.1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">87.6</td>\n</tr>\n<tr id=\"S5.T6.1.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T6.1.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">BERT-345M</th>\n<th id=\"S5.T6.1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">229297</th>\n<td id=\"S5.T6.1.1.3.2.3\" class=\"ltx_td ltx_align_center\">79.1</td>\n<td id=\"S5.T6.1.1.3.2.4\" class=\"ltx_td ltx_align_center\">87.9</td>\n</tr>\n<tr id=\"S5.T6.1.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T6.1.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><span id=\"S5.T6.1.1.4.3.1.1\" class=\"ltx_text ltx_font_bold\">BERT-1.2B</span></th>\n<th id=\"S5.T6.1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><span id=\"S5.T6.1.1.4.3.2.1\" class=\"ltx_text ltx_font_bold\">229067</span></th>\n<td id=\"S5.T6.1.1.4.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T6.1.1.4.3.3.1\" class=\"ltx_text ltx_font_bold\">79.2</span></td>\n<td id=\"S5.T6.1.1.4.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T6.1.1.4.3.4.1\" class=\"ltx_text ltx_font_bold\">88.3</span></td>\n</tr>\n<tr id=\"S5.T6.1.1.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T6.1.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t\">Human Generated Answers</th>\n<th id=\"S5.T6.1.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t\">42472</th>\n<td id=\"S5.T6.1.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">83.7</td>\n<td id=\"S5.T6.1.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">91.1</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Answer generation is equally important in our data generation pipeline. Answer generation is the first component of the pipeline and must be precise to avoid compounding errors. For answer generation we use an unconditional extractive BERT model that predicts start and end spans jointly over a given sentence. From each probability distribution we sample the entire nucleus (p=0.9\ud835\udc5d0.9p=0.9) or the top-5 spans, choosing whichever is smaller. We arrive at this implementation based on our ablation studies in section 6.3. To test the quality of the selected answers we generate questions from our 1.2 billion parameter question generator and finetune a question answering model on the synthesized questions without any filtration. In Table 6 we compare answer generation quality using our two trained models and the original BERT-Large model from (Devlin et\u00a0al., 2018). We find that improvements in pretraining data and tasks dramatically improve answer generation quality by 1.4 EM and 0.3 F1 between BERT-Large and our 345 million parameter answer generation model. We find that increasing model scale further to 1.2 billion parameters improves answer generation quality F1 by 0.4 while EM only improves by 0.1.\nAlthough these represent improvements in question quality only achieved by newer models, answer generation seems to be a large bottleneck as we discuss in section 6.1."
        ]
    },
    "S5.T7": {
        "caption": "Table 7:  Effect of pretraining and scale on question filtration. Synthetic questions and answers were both generated with 1.2 billion parameter models. Before finetuning, overgeneration and filtration were performed with the models ablated here.\n",
        "table": "<table id=\"S5.T7.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T7.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T7.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\">Filter Model</th>\n<th id=\"S5.T7.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\"># Questions</th>\n<th id=\"S5.T7.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">EM</th>\n<th id=\"S5.T7.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">F1</th>\n</tr>\n<tr id=\"S5.T7.1.1.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T7.1.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" colspan=\"4\">Synthetic Questions + Real Answers</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T7.1.1.3.1\" class=\"ltx_tr\">\n<th id=\"S5.T7.1.1.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">BERT-Large</th>\n<th id=\"S5.T7.1.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">45888</th>\n<td id=\"S5.T7.1.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">84.5</td>\n<td id=\"S5.T7.1.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">91.4</td>\n</tr>\n<tr id=\"S5.T7.1.1.4.2\" class=\"ltx_tr\">\n<th id=\"S5.T7.1.1.4.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">BERT-345M</th>\n<th id=\"S5.T7.1.1.4.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">34341</th>\n<td id=\"S5.T7.1.1.4.2.3\" class=\"ltx_td ltx_align_center\">84.2</td>\n<td id=\"S5.T7.1.1.4.2.4\" class=\"ltx_td ltx_align_center\">91.4</td>\n</tr>\n<tr id=\"S5.T7.1.1.5.3\" class=\"ltx_tr\">\n<th id=\"S5.T7.1.1.5.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><span id=\"S5.T7.1.1.5.3.1.1\" class=\"ltx_text ltx_font_bold\">BERT-1.2B</span></th>\n<th id=\"S5.T7.1.1.5.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><span id=\"S5.T7.1.1.5.3.2.1\" class=\"ltx_text ltx_font_bold\">47772</span></th>\n<td id=\"S5.T7.1.1.5.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T7.1.1.5.3.3.1\" class=\"ltx_text ltx_font_bold\">85.6</span></td>\n<td id=\"S5.T7.1.1.5.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T7.1.1.5.3.4.1\" class=\"ltx_text ltx_font_bold\">92.4</span></td>\n</tr>\n<tr id=\"S5.T7.1.1.6.4\" class=\"ltx_tr\">\n<th id=\"S5.T7.1.1.6.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"4\">Synthetic Questions + Synthetic Answers</th>\n</tr>\n<tr id=\"S5.T7.1.1.7.5\" class=\"ltx_tr\">\n<th id=\"S5.T7.1.1.7.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">BERT-Large</th>\n<th id=\"S5.T7.1.1.7.5.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">177712</th>\n<td id=\"S5.T7.1.1.7.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\">85.5</td>\n<td id=\"S5.T7.1.1.7.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\">91.9</td>\n</tr>\n<tr id=\"S5.T7.1.1.8.6\" class=\"ltx_tr\">\n<th id=\"S5.T7.1.1.8.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">BERT-345M</th>\n<th id=\"S5.T7.1.1.8.6.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">144322</th>\n<td id=\"S5.T7.1.1.8.6.3\" class=\"ltx_td ltx_align_center\">85.9</td>\n<td id=\"S5.T7.1.1.8.6.4\" class=\"ltx_td ltx_align_center\">92.5</td>\n</tr>\n<tr id=\"S5.T7.1.1.9.7\" class=\"ltx_tr\">\n<th id=\"S5.T7.1.1.9.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><span id=\"S5.T7.1.1.9.7.1.1\" class=\"ltx_text ltx_font_bold\">BERT-1.2B</span></th>\n<th id=\"S5.T7.1.1.9.7.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><span id=\"S5.T7.1.1.9.7.2.1\" class=\"ltx_text ltx_font_bold\">184992</span></th>\n<td id=\"S5.T7.1.1.9.7.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T7.1.1.9.7.3.1\" class=\"ltx_text ltx_font_bold\">87.1</span></td>\n<td id=\"S5.T7.1.1.9.7.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T7.1.1.9.7.4.1\" class=\"ltx_text ltx_font_bold\">93.2</span></td>\n</tr>\n<tr id=\"S5.T7.1.1.10.8\" class=\"ltx_tr\">\n<th id=\"S5.T7.1.1.10.8.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t\">Human Generated Data</th>\n<th id=\"S5.T7.1.1.10.8.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t\">42472</th>\n<td id=\"S5.T7.1.1.10.8.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">86.3</td>\n<td id=\"S5.T7.1.1.10.8.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">93.2</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "We use the 1.2 billion parameter question generator from section 5.1 to generate questions for filtration. As described in more detail in section 6.3 we overgenerate two questions for every answer. We then filter these questions with roundtrip filtration before finetuning a question answering model. In Table 7 we find that our 345 million parameter BERT model modestly outperforms the public BERT-Large model when using synthetic answers to generate questions while our 1.2 billion parameter BERT model further improves on this score by more than a whole point. Interestingly, these results follow an opposite trend compared to the previous section. In the previous section improvements to pretraining scale and tasks made a larger difference on answer generation than increasing model scale. However, here we see the opposite: improvements to pretraining tasks results only in a modest improvement to question filtration, while increasing model size results in much more substantive improvements. We hypothesize that this is due to the larger model\u2019s ability to correctly answer more questions, and therefore allow more valid and high quality samples through to the finetuning phase as indicated by the number of questions generated by the technique."
        ]
    },
    "S6.T10": {
        "caption": "Table 10:  Effect of filtration modeling choices on questions generated from ground truth and synthetic answers. 1.2 billion parameter models are used for every stage of the generation pipeline. Questions from no filtration are used in the other experiments with a second set of questions generated in overgeneration experiments.",
        "table": "<table id=\"S6.T10.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T10.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S6.T10.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S6.T10.1.1.1.1.1.1\" class=\"ltx_text\">Filter Model</span></th>\n<th id=\"S6.T10.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S6.T10.1.1.1.1.2.1\" class=\"ltx_text\"># Questions</span></th>\n<td id=\"S6.T10.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" colspan=\"2\">345M QA</td>\n<td id=\"S6.T10.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_t\" colspan=\"2\">Large QA</td>\n</tr>\n<tr id=\"S6.T10.1.1.2.2\" class=\"ltx_tr\">\n<td id=\"S6.T10.1.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\">EM</td>\n<td id=\"S6.T10.1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">F1</td>\n<td id=\"S6.T10.1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">EM</td>\n<td id=\"S6.T10.1.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">F1</td>\n</tr>\n<tr id=\"S6.T10.1.1.3.3\" class=\"ltx_tr\">\n<th id=\"S6.T10.1.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" colspan=\"6\">Synthetic Questions + Real Answers</th>\n</tr>\n<tr id=\"S6.T10.1.1.4.4\" class=\"ltx_tr\">\n<th id=\"S6.T10.1.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">None</th>\n<th id=\"S6.T10.1.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">42472</th>\n<td id=\"S6.T10.1.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\">83.4</td>\n<td id=\"S6.T10.1.1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">90.9</td>\n<td id=\"S6.T10.1.1.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\">79.0</td>\n<td id=\"S6.T10.1.1.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\">87.0</td>\n</tr>\n<tr id=\"S6.T10.1.1.5.5\" class=\"ltx_tr\">\n<th id=\"S6.T10.1.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Roundtrip (RT)</th>\n<th id=\"S6.T10.1.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">24310</th>\n<td id=\"S6.T10.1.1.5.5.3\" class=\"ltx_td ltx_align_center\">84.0</td>\n<td id=\"S6.T10.1.1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_r\">91.3</td>\n<td id=\"S6.T10.1.1.5.5.5\" class=\"ltx_td ltx_align_center\">76.5</td>\n<td id=\"S6.T10.1.1.5.5.6\" class=\"ltx_td ltx_align_center\">84.4</td>\n</tr>\n<tr id=\"S6.T10.1.1.6.6\" class=\"ltx_tr\">\n<th id=\"S6.T10.1.1.6.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S6.T10.1.1.6.6.1.1\" class=\"ltx_text ltx_font_bold\">Overgenerate &amp; RT</span></th>\n<th id=\"S6.T10.1.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S6.T10.1.1.6.6.2.1\" class=\"ltx_text ltx_font_bold\">47772</span></th>\n<td id=\"S6.T10.1.1.6.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T10.1.1.6.6.3.1\" class=\"ltx_text ltx_font_bold\">85.6</span></td>\n<td id=\"S6.T10.1.1.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S6.T10.1.1.6.6.4.1\" class=\"ltx_text ltx_font_bold\">92.4</span></td>\n<td id=\"S6.T10.1.1.6.6.5\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T10.1.1.6.6.5.1\" class=\"ltx_text ltx_font_bold\">81.7</span></td>\n<td id=\"S6.T10.1.1.6.6.6\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T10.1.1.6.6.6.1\" class=\"ltx_text ltx_font_bold\">88.7</span></td>\n</tr>\n<tr id=\"S6.T10.1.1.7.7\" class=\"ltx_tr\">\n<th id=\"S6.T10.1.1.7.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"6\">Synthetic Questions + Synthetic Answers</th>\n</tr>\n<tr id=\"S6.T10.1.1.8.8\" class=\"ltx_tr\">\n<th id=\"S6.T10.1.1.8.8.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">None</th>\n<th id=\"S6.T10.1.1.8.8.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">229297</th>\n<td id=\"S6.T10.1.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_t\">79.1</td>\n<td id=\"S6.T10.1.1.8.8.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">87.9</td>\n<td id=\"S6.T10.1.1.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_t\">78.2</td>\n<td id=\"S6.T10.1.1.8.8.6\" class=\"ltx_td ltx_align_center ltx_border_t\">86.8</td>\n</tr>\n<tr id=\"S6.T10.1.1.9.9\" class=\"ltx_tr\">\n<th id=\"S6.T10.1.1.9.9.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Roundtrip (RT)</th>\n<th id=\"S6.T10.1.1.9.9.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">93866</th>\n<td id=\"S6.T10.1.1.9.9.3\" class=\"ltx_td ltx_align_center\">86.3</td>\n<td id=\"S6.T10.1.1.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_r\">92.7</td>\n<td id=\"S6.T10.1.1.9.9.5\" class=\"ltx_td ltx_align_center\">84.1</td>\n<td id=\"S6.T10.1.1.9.9.6\" class=\"ltx_td ltx_align_center\">90.5</td>\n</tr>\n<tr id=\"S6.T10.1.1.10.10\" class=\"ltx_tr\">\n<th id=\"S6.T10.1.1.10.10.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S6.T10.1.1.10.10.1.1\" class=\"ltx_text ltx_font_bold\">Overgenerate &amp; RT</span></th>\n<th id=\"S6.T10.1.1.10.10.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S6.T10.1.1.10.10.2.1\" class=\"ltx_text ltx_font_bold\">184992</span></th>\n<td id=\"S6.T10.1.1.10.10.3\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T10.1.1.10.10.3.1\" class=\"ltx_text ltx_font_bold\">87.1</span></td>\n<td id=\"S6.T10.1.1.10.10.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S6.T10.1.1.10.10.4.1\" class=\"ltx_text ltx_font_bold\">93.2</span></td>\n<td id=\"S6.T10.1.1.10.10.5\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T10.1.1.10.10.5.1\" class=\"ltx_text ltx_font_bold\">85.2</span></td>\n<td id=\"S6.T10.1.1.10.10.6\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T10.1.1.10.10.6.1\" class=\"ltx_text ltx_font_bold\">91.5</span></td>\n</tr>\n<tr id=\"S6.T10.1.1.11.11\" class=\"ltx_tr\">\n<th id=\"S6.T10.1.1.11.11.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\">Human Generated Data</th>\n<th id=\"S6.T10.1.1.11.11.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\">42472</th>\n<td id=\"S6.T10.1.1.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">86.3</td>\n<td id=\"S6.T10.1.1.11.11.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">93.2</td>\n<td id=\"S6.T10.1.1.11.11.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">82.4</td>\n<td id=\"S6.T10.1.1.11.11.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">89.7</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Both question generation and answer generation sometimes produces poor answers. As we show in Table 10 generating synthetic data from synthetic answers without filtering deteriorates significantly, while roundtrip consistency combats this effect to perform 7.2 EM points better. However, we find that even on questions generated from ground truth answers roundtrip filtering throws away questions associated with perfectly good answers. Throwing away data significantly hurts BERT-Large whose pretrained features are not as robust as our BERT-345M model and require more finetuning data. To combat this we take an approach similar to overgeneration and reranking (Heilman & Smith, 2010b) where we generate two questions per answer and feed each into roundtrip filtration independently. We term this overgeneration and filtration. This helps avoid losing important answers in our synthesized training set. To perform overgeneration we sample one question with top-k\ud835\udc58k (k=40\ud835\udc5840k=40) sampling and one with top-p\ud835\udc5dp (p=0.9\ud835\udc5d0.9p=0.9) nucleus sampling. This leads approximately to a whole point of improvement for our model in both the case with and without ground truth answers, and allows us to surpass training with real SQuAD1.1 data."
        ]
    },
    "S6.T8": {
        "caption": "Table 8:  Effect of question generator modeling choices. Questions are generated from ground truth answers without any filtration.",
        "table": "<table id=\"S6.T8.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S6.T8.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S6.T8.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Question Generator</th>\n<th id=\"S6.T8.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"># Questions</th>\n<th id=\"S6.T8.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">EM</th>\n<th id=\"S6.T8.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">F1</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T8.1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S6.T8.1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S6.T8.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">345M</span></td>\n<td id=\"S6.T8.1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S6.T8.1.1.2.1.2.1\" class=\"ltx_text ltx_font_bold\">42414</span></td>\n<td id=\"S6.T8.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S6.T8.1.1.2.1.3.1\" class=\"ltx_text ltx_font_bold\">80.7</span></td>\n<td id=\"S6.T8.1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S6.T8.1.1.2.1.4.1\" class=\"ltx_text ltx_font_bold\">88.6</span></td>\n</tr>\n<tr id=\"S6.T8.1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S6.T8.1.1.3.2.1\" class=\"ltx_td ltx_align_center\">345M (no pretraining)</td>\n<td id=\"S6.T8.1.1.3.2.2\" class=\"ltx_td ltx_align_center\">42408</td>\n<td id=\"S6.T8.1.1.3.2.3\" class=\"ltx_td ltx_align_center\">42.7</td>\n<td id=\"S6.T8.1.1.3.2.4\" class=\"ltx_td ltx_align_center\">51.4</td>\n</tr>\n<tr id=\"S6.T8.1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S6.T8.1.1.4.3.1\" class=\"ltx_td ltx_align_center\">345M (no stopwords)</td>\n<td id=\"S6.T8.1.1.4.3.2\" class=\"ltx_td ltx_align_center\">42486</td>\n<td id=\"S6.T8.1.1.4.3.3\" class=\"ltx_td ltx_align_center\">75.5</td>\n<td id=\"S6.T8.1.1.4.3.4\" class=\"ltx_td ltx_align_center\">84.5</td>\n</tr>\n<tr id=\"S6.T8.1.1.5.4\" class=\"ltx_tr\">\n<td id=\"S6.T8.1.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">Human Generated Questions</td>\n<td id=\"S6.T8.1.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">42472</td>\n<td id=\"S6.T8.1.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">86.3</td>\n<td id=\"S6.T8.1.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">93.2</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "To study question generation in isolation we used our 345 million parameter model to generate questions from ground truth SQuAD1.1 answers. The results of our analysis can be found in Table 8. We first investigated the use of pretrained models and found that pretraining our GPT-2 model was crucial for achieving reasonable question generation. We then examined the effect of stopword filtration in our question generator. We found that this provided a substantial boost to EM and F1 scores of 5.2 and 4.1 respectively. The goal of employing this technique was to catch generations that ramble onwards without stopping, or produce end of text prematurely in the middle of a question. On manual inspection we found qualitatively that this technique helped when generating questions on text that featured heavy use of symbols and foreign language. In these cases the model struggled with out of distribution vocabulary, autoregressive sampling degenerated, and no stopword was produced."
        ]
    },
    "S6.T9": {
        "caption": "Table 9:  Effect of answer generator modeling choices. Model based answer generation is performed with BERT-345M and questions are generated using a 1.2B parameter model. No filtration is applied to the generated questions.",
        "table": "<table id=\"S6.T9.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S6.T9.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S6.T9.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Answer Generator</th>\n<th id=\"S6.T9.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"># Questions</th>\n<th id=\"S6.T9.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">EM</th>\n<th id=\"S6.T9.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">F1</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T9.1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S6.T9.1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\">NER</td>\n<td id=\"S6.T9.1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">132729</td>\n<td id=\"S6.T9.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">59.3</td>\n<td id=\"S6.T9.1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">70.5</td>\n</tr>\n<tr id=\"S6.T9.1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S6.T9.1.1.3.2.1\" class=\"ltx_td ltx_align_center\">Independent Spans</td>\n<td id=\"S6.T9.1.1.3.2.2\" class=\"ltx_td ltx_align_center\">83534</td>\n<td id=\"S6.T9.1.1.3.2.3\" class=\"ltx_td ltx_align_center\">77.2</td>\n<td id=\"S6.T9.1.1.3.2.4\" class=\"ltx_td ltx_align_center\">87.1</td>\n</tr>\n<tr id=\"S6.T9.1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S6.T9.1.1.4.3.1\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T9.1.1.4.3.1.1\" class=\"ltx_text ltx_font_bold\">Joint Spans</span></td>\n<td id=\"S6.T9.1.1.4.3.2\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T9.1.1.4.3.2.1\" class=\"ltx_text ltx_font_bold\">229297</span></td>\n<td id=\"S6.T9.1.1.4.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T9.1.1.4.3.3.1\" class=\"ltx_text ltx_font_bold\">79.1</span></td>\n<td id=\"S6.T9.1.1.4.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T9.1.1.4.3.4.1\" class=\"ltx_text ltx_font_bold\">87.9</span></td>\n</tr>\n<tr id=\"S6.T9.1.1.5.4\" class=\"ltx_tr\">\n<td id=\"S6.T9.1.1.5.4.1\" class=\"ltx_td ltx_align_center\">Paragraph-level Joint Spans</td>\n<td id=\"S6.T9.1.1.5.4.2\" class=\"ltx_td ltx_align_center\">226672</td>\n<td id=\"S6.T9.1.1.5.4.3\" class=\"ltx_td ltx_align_center\">77.3</td>\n<td id=\"S6.T9.1.1.5.4.4\" class=\"ltx_td ltx_align_center\">86.9</td>\n</tr>\n<tr id=\"S6.T9.1.1.6.5\" class=\"ltx_tr\">\n<td id=\"S6.T9.1.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">Human Generated Answers</td>\n<td id=\"S6.T9.1.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">42472</td>\n<td id=\"S6.T9.1.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">83.7</td>\n<td id=\"S6.T9.1.1.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">91.1</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "In our experiments we found answer generation to be a significant bottleneck in performance. In section 5.2 we found that scaling up model size allows us to close the gap between human and synthetic training performance. However, these scaling analysis were performed with our best model. In Table 9 we show that the choice of model is critical to closing the gap. Starting with a Named Entity Recognition (NER) model we find that it gets a dismal EM and F1 score. This is due to entities comprising only of \u223csimilar-to\\sim 50% of the answer distribution for SQuAD1.1 . It\u2019s necessary to use a learned model to model the diverse set of answers present SQuAD1.1 . We then tried to use the most common SQuAD1.1 model, which models the start and end of a span independently, to extract answers from individual sentences. This performed noticeably better, boosting our score to 77.2 EM, despite producing fewer answers than NER extraction. However, upon inspection we found that modeling the span independently resulted in sampling repetitive answers. We then tried using the answer generator from (Alberti et\u00a0al., 2019a) which models the start and end of a span jointly as a conditional random field. This is the model we ended up choosing as it performed the best with an exact match score of 79.1. Lastly, we also considered jointly modeling answer spans over an entire paragraph instead of a single sentence. However, we found that it performed worse than independent span modeling over sentences."
        ]
    }
}