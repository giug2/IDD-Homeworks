{
    "id_table_1": {
        "caption": "Table 1:  Zero-shot one-to-one and two-to-one retrieval accuracy. ( V V \\mathcal{V} caligraphic_V : video,  A A \\mathcal{A} caligraphic_A : audio,  T T \\mathcal{T} caligraphic_T : text)",
        "table": "A3.EGx1",
        "footnotes": [],
        "references": [
            "We propose a simple yet effective modification by replacing fixed anchors with  dynamic  centroid-based anchors computed from paired samples. Our method,  CentroBind , described in Section  3  removes the need for selecting a fixed anchor modality, instead calculates the centroid of all modality representations and generates an anchor representation, as shown in Figure  1(a) . Encoders are then trained to minimize the ensemble of InfoNCE loss  (Oord et al.,  2018 )  between this dynamic anchor and other representations, using the centroid as the anchor. The main intuition is that  a desirable anchor should be representative of all modalities , capturing the most comprehensive information, with well-trained encoders producing representations that naturally cluster around this shared centroid, reflecting their underlying semantic alignment.",
            "We note that if  i = l i l i=l italic_i = italic_l , the sufficient encoder provides embeddings with maximum intra information, and if  i = l i l i\\neq l italic_i = italic_l , it gives embeddings with maximum shared information between  i i i italic_i -th and  l l l italic_l -th modalities. 2 2 2 With a proper choice of  Z i subscript Z i \\mathcal{Z}_{i} caligraphic_Z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  ensuring  max f : X i  Z i  I  ( f  ( X i ) ; X l ) = I  ( X i ; X l ) subscript : f  subscript X i subscript Z i I f subscript X i subscript X l I subscript X i subscript X l \\max_{f:\\mathcal{X}_{i}\\to\\mathcal{Z}_{i}}I(f({\\bf X}_{i});{\\bf X}_{l})=I({\\bf X% }_{i};{\\bf X}_{l}) roman_max start_POSTSUBSCRIPT italic_f : caligraphic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  caligraphic_Z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_I ( italic_f ( bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ; bold_X start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ) = italic_I ( bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ; bold_X start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ) , Definition  1  says that  z i , j = f i  ( x i , j ) subscript z i j subscript f i subscript x i j {\\boldsymbol{z}}_{i,j}=f_{i}({\\boldsymbol{x}}_{i,j}) bold_italic_z start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT )  is a sufficient statistic  (Polyanskiy & Wu,  2024 )  of  x i , j subscript x i j {\\boldsymbol{x}}_{i,j} bold_italic_x start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT  for  x l , j subscript x l j {\\boldsymbol{x}}_{l,j} bold_italic_x start_POSTSUBSCRIPT italic_l , italic_j end_POSTSUBSCRIPT  as the encoding entails no information loss.",
            "In the context of contrastive representation learning, with a goal of attaining sufficient encoders in Definition  1 , InfoNCE loss  I NCE  ( X ; Y ) subscript I NCE X Y I_{\\rm NCE}(X;Y) italic_I start_POSTSUBSCRIPT roman_NCE end_POSTSUBSCRIPT ( italic_X ; italic_Y )  is often employed since it relates to mutual information. Specifically, InfoNCE provides a lower bound on mutual information, i.e.,  I  ( X ; Y )   I NCE  ( X ; Y ) I X Y subscript I NCE X Y I({\\bf X};{\\bf Y})\\geq-I_{\\rm NCE}({\\bf X};{\\bf Y}) italic_I ( bold_X ; bold_Y )  - italic_I start_POSTSUBSCRIPT roman_NCE end_POSTSUBSCRIPT ( bold_X ; bold_Y )   (Oord et al.,  2018 ) , thus minimizing InfoNCE leads to an increase in mutual information. InfoNCE loss between embeddings  U U {\\bf U} bold_U  and  V V {\\bf V} bold_V  can be written as follows:",
            "In addition to the objective of intra and shared information, multimodal learning often takes into account multimodal alignment  (Radford et al.,  2021 ; Duan et al.,  2022 ) . Without multimodal alignment, each modality has its own embedding structure depending on its encoder. For example, embeddings of cat and dog images, respectively, locate around  ( 1 , 0 ) 1 0 (1,0) ( 1 , 0 )  and  ( 0 , 2 ) 0 2 (0,2) ( 0 , 2 )  in  R 2 superscript R 2 \\mathbb{R}^{2} blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , whereas embeddings of cat and dog text can lie around  ( 0 , 2 ) 0 2 (0,2) ( 0 , 2 )  and  ( 1 , 0 ) 1 0 (1,0) ( 1 , 0 ) . Such a misalignment can happen even for sufficient encoders (Definition  1 ), since the mutual information is invariant to one-to-one mappings  (Polyanskiy & Wu,  2024 ) .",
            "reflecting the fact that minimizing InfoNCE loss leads to maximizing mutual information. 3 3 3 In contrast to ( 3 ),  f i FB superscript subscript f i FB f_{i}^{\\rm FB} italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_FB end_POSTSUPERSCRIPT  in ( 4 ) might not be aligned with other modalities due to the one-to-one mapping invariant property of mutual information. However, we here do not analyze multimodal alignment of  FABind  from ( 4 ), but rather investigate the performance of encoders in terms of the sufficiency in Definition  1 .  Let  FABind  encoders from ( 4 ) for each modality be  F FB = { f 1 , f 2 FB ,  , f M FB } superscript F FB subscript f 1 superscript subscript f 2 FB  superscript subscript f M FB \\mathcal{F}^{\\rm FB}=\\{f_{1},f_{2}^{\\rm FB},\\cdots,f_{M}^{\\rm FB}\\} caligraphic_F start_POSTSUPERSCRIPT roman_FB end_POSTSUPERSCRIPT = { italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_FB end_POSTSUPERSCRIPT ,  , italic_f start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_FB end_POSTSUPERSCRIPT } . The anchor encoder  f 1 subscript f 1 f_{1} italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  is fixed during the entire  FABind  procedure. Moreover, we assume that  I  ( f 1  ( X 1 ) ; f i FB  ( X i ) ) = I  ( f 1  ( X 1 ) ; X i ) I subscript f 1 subscript X 1 superscript subscript f i FB subscript X i I subscript f 1 subscript X 1 subscript X i I(f_{1}({\\bf X}_{1});f_{i}^{\\rm FB}({\\bf X}_{i}))=I(f_{1}({\\bf X}_{1});{\\bf X}% _{i}) italic_I ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ; italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_FB end_POSTSUPERSCRIPT ( bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) = italic_I ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ; bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  is the maximum value that can be achieved by ( 4 ) due to data processing inequality  (Polyanskiy & Wu,  2024 ) . We next demonstrate that the quality of anchor embedding  f 1  ( X 1 ) subscript f 1 subscript X 1 f_{1}({\\bf X}_{1}) italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT )  significantly impacts the performance of  F FB superscript F FB \\mathcal{F}^{\\rm FB} caligraphic_F start_POSTSUPERSCRIPT roman_FB end_POSTSUPERSCRIPT  in terms of shared information. The following propositions show the dependency of  FABind  on anchor embedding quality.",
            "The proof can be found in Appendix  B.1 .",
            "Proposition  1  shows that the  FABind  encoders  F FB superscript F FB \\mathcal{F}^{\\rm FB} caligraphic_F start_POSTSUPERSCRIPT roman_FB end_POSTSUPERSCRIPT  learned with a sufficient anchor embedding can achieve the maximum shared information between the anchor and the other modalities. However, it does not guarantee the shared information between  non-anchored  modalities  I  ( f i  ( X i ) ; f l  ( X l ) ) , i , l = 1 I subscript f i subscript X i subscript f l subscript X l i l 1 I(f_{i}({\\bf X}_{i});f_{l}({\\bf X}_{l})),~{}i,l\\neq 1 italic_I ( italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ; italic_f start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( bold_X start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ) ) , italic_i , italic_l = 1 , which can also be seen from ( 4 ). Proposition  2  establishes that an insufficient anchor may lead to a reduction of shared information between the anchor and the other modalities, implying that the performance of  FABind  solely depends on the quality of the anchor.",
            "To present a general framework for  CentroBind , we consider  M M M italic_M  modalities with corresponding encoders  { f i } i = 1 M superscript subscript subscript f i i 1 M \\{f_{i}\\}_{i=1}^{M} { italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT . The algorithmic presentation of  CentroBind  is in Algorithm  1 , and a graphical illustration is given in Figure  1(b) . In the following, we elaborate detailed steps of  CentroBind .",
            "We start by providing a lower bound of  CentroBind s objective function  L CB  ( f i |  ) subscript L CB conditional subscript f i  \\mathcal{L}_{\\rm CB}(f_{i}|\\tau) caligraphic_L start_POSTSUBSCRIPT roman_CB end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_ )  ( 8 ) in Theorem  1 , followed by an analysis of minimizing  L CB  ( f i |  ) subscript L CB conditional subscript f i  \\mathcal{L}_{\\rm CB}(f_{i}|\\tau) caligraphic_L start_POSTSUBSCRIPT roman_CB end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_ ) .",
            "Theorem  1  provides a lower bound of  I NCE ( A ; f i ( X i ) |  ) I_{\\rm NCE}\\left({\\bf A};f_{i}({\\bf X}_{i})\\;\\middle|\\;{\\tau}\\right) italic_I start_POSTSUBSCRIPT roman_NCE end_POSTSUBSCRIPT ( bold_A ; italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) | italic_ )  in ( 9a ), which is a part of  CentroBind  objective  L CB  ( f i |  ) subscript L CB conditional subscript f i  \\mathcal{L}_{\\rm CB}(f_{i}|\\tau) caligraphic_L start_POSTSUBSCRIPT roman_CB end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_ ) . Thus  CentroBind  consequentially minimizes the lower bound ( 10 ) that consists of two terms,   l = 1 M I NCE ( f l ( X l  ) ; f i ( X i ) |   M | I B | ) \\sum_{l=1}^{M}I_{\\rm NCE}\\left(f_{l}({\\bf X}_{l}^{\\prime});f_{i}({\\bf X}_{i})% \\;\\middle|\\;\\frac{{\\tau}M}{|\\mathcal{I}_{B}|}\\right)  start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT italic_I start_POSTSUBSCRIPT roman_NCE end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( bold_X start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) ; italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) | divide start_ARG italic_ italic_M end_ARG start_ARG | caligraphic_I start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT | end_ARG )  and    k = 1 | I B | log  C F , k , i superscript subscript k 1 subscript I B subscript C F k i -\\sum_{k=1}^{|\\mathcal{I}_{B}|}\\log C_{\\mathcal{F},k,i} -  start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | caligraphic_I start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT | end_POSTSUPERSCRIPT roman_log italic_C start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT . We next provide some intuitive explanation on the effect of such a minimization of the lower bound.",
            "We show the effect of growing  C F , k , i subscript C F k i C_{\\mathcal{F},k,i} italic_C start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT  in terms of cosine similarity score between embeddings. Since  C F , k , i = 1 4  (  + 1  ) 2 subscript C F k i 1 4 superscript  1  2 C_{\\mathcal{F},k,i}=\\frac{1}{4}\\left(\\sqrt{\\gamma}+\\sqrt{\\frac{1}{\\gamma}}% \\right)^{2} italic_C start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG 4 end_ARG ( square-root start_ARG italic_ end_ARG + square-root start_ARG divide start_ARG 1 end_ARG start_ARG italic_ end_ARG end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  with   = c F , k , i max c F , k , i min  1  superscript subscript c F k i superscript subscript c F k i 1 \\gamma=\\frac{c_{\\mathcal{F},k,i}^{\\max}}{c_{\\mathcal{F},k,i}^{\\min}}\\geq 1 italic_ = divide start_ARG italic_c start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_max end_POSTSUPERSCRIPT end_ARG start_ARG italic_c start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_min end_POSTSUPERSCRIPT end_ARG  1 , maximizing  C F , k , i subscript C F k i C_{\\mathcal{F},k,i} italic_C start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT  is equivalent to simultaneously maximizing  c F , k , i max superscript subscript c F k i c_{\\mathcal{F},k,i}^{\\max} italic_c start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_max end_POSTSUPERSCRIPT  and minimizing  c F , k , i min superscript subscript c F k i c_{\\mathcal{F},k,i}^{\\min} italic_c start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_min end_POSTSUPERSCRIPT . For ease of the analysis, we assume that the encoders are reasonably well-trained. Then, since a positive pair of embeddings normally yields higher similarity score,  c F , k , i max superscript subscript c F k i c_{\\mathcal{F},k,i}^{\\max} italic_c start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_max end_POSTSUPERSCRIPT  should be obtained by choosing  l = i l i l=i italic_l = italic_i  and  j = k j k j=k italic_j = italic_k  in ( 11 ) as such choices make  x l , k  superscript subscript x l k  {\\boldsymbol{x}}_{l,k}^{\\prime} bold_italic_x start_POSTSUBSCRIPT italic_l , italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  be positive pair with  x i , j subscript x i j {\\boldsymbol{x}}_{i,j} bold_italic_x start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT . Thus,  c F , k , i max superscript subscript c F k i c_{\\mathcal{F},k,i}^{\\max} italic_c start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_max end_POSTSUPERSCRIPT  is roughly proportional to the similarity score of a positive pair of embeddings. Conversely,  c F , k , i min superscript subscript c F k i c_{\\mathcal{F},k,i}^{\\min} italic_c start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_min end_POSTSUPERSCRIPT  corresponds to the similarity scores of negative pairs, which tend to be low. Hence, minimizing    k = 1 | I B | log  C F , k , i superscript subscript k 1 subscript I B subscript C F k i -\\sum_{k=1}^{|\\mathcal{I}_{B}|}\\log C_{\\mathcal{F},k,i} -  start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | caligraphic_I start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT | end_POSTSUPERSCRIPT roman_log italic_C start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT  enhances the similarity scores for positive pairs and reduces those for negative pairs, improving the overall multimodal alignment.",
            "We employ a latent variable model  (Bishop & Nasrabadi,  2006 )  for generating synthetic multimodal datasets. A latent variable model is a statistical model for data  X  R d x X superscript R subscript d x {\\bf X}\\in\\mathbb{R}^{d_{x}} bold_X  blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT end_POSTSUPERSCRIPT , under which  X X {\\bf X} bold_X  is generated according to a conditional probability distribution  P X | Z subscript P conditional X Z P_{{\\bf X}|{\\bf Z}} italic_P start_POSTSUBSCRIPT bold_X | bold_Z end_POSTSUBSCRIPT , where  Z  R d z Z superscript R subscript d z {\\bf Z}\\in\\mathbb{R}^{d_{z}} bold_Z  blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  is the latent variable. In terms of the representation learning framework,  Z Z {\\bf Z} bold_Z  can be seen as the true representation of  X X {\\bf X} bold_X . We assume that the class label  Y  [ K ] Y delimited-[] K {\\bf Y}\\in[K] bold_Y  [ italic_K ]  and the latent variable  Z Z {\\bf Z} bold_Z  are jointly distributed according to  P Z , Y subscript P Z Y P_{{\\bf Z},{\\bf Y}} italic_P start_POSTSUBSCRIPT bold_Z , bold_Y end_POSTSUBSCRIPT . In our setting, we exploit Gaussian mixture model (GMM)  (Bishop & Nasrabadi,  2006 )  for the latent variable  Z Z {\\bf Z} bold_Z , and we generate  M M M italic_M  modalities  X i = g i  ( Z ) + N , i  [ M ] formulae-sequence subscript X i subscript g i Z N i delimited-[] M {\\bf X}_{i}=g_{i}({\\bf Z})+{\\bf N},i\\in[M] bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_Z ) + bold_N , italic_i  [ italic_M ]  with random noise  N N {\\bf N} bold_N  and some non-linear projections  g i : R d z  R d x : subscript g i  superscript R subscript d z superscript R subscript d x g_{i}:\\mathbb{R}^{d_{z}}\\to\\mathbb{R}^{d_{x}} italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT : blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT end_POSTSUPERSCRIPT . We choose the projections in a way such that each model can be ranked in ascending order, i.e.,  X 1 subscript X 1 {\\bf X}_{1} bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  is the worst, and  X 4 subscript X 4 {\\bf X}_{4} bold_X start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT  is the best modality in terms of their inherent correlation with the latent variable. The class label  Y Y {\\bf Y} bold_Y  is set to the component id of GMM (for details, see Appendix  C.1 ).",
            "Figure  2(b)  includes accuracies of  FABind  and CB with random backbones. Similarly, CB outperforms all baselines. Somewhat surprisingly, CB with random backbones (green curves) also performs better than  FABind  with pretrained backbones (red curves). This further supports our analysis that  CentroBind  is robust to the backbone quality as it optimizes intra and shared information, whereas  FABind  is sensitive to the backbone quality. Overall, these empirical results validate our findings. We provide additional experimental results on synthetic datasets with  M = 6 , 8 M 6 8 M=6,8 italic_M = 6 , 8  in Appendix  C.1 . With the larger number of modalities, CB still outperforms the baselines, strengthening our analysis of CB.",
            "We perform evaluations in zero-shot binary and multi-class classification tasks, One-to-One cross-modal retrieval, and Two-to-One cross-modal retrieval. For classification tasks, we use a Multi-Layer Perceptron (MLP) to perform sarcasm detection as a binary classification and speaker classification with  23 23 23 23  multi-class categories. In particular, MLP is trained on embeddings in a single modality (denoted by  Tr  in Table  2 ) and accuracy is evaluated on another modality (denoted by  Ev  in Table  2 ). In retrieval tasks, we measure the accuracy of correct retrieval. For One-to-One case, we retrieve data sample in different modality by choosing the closest embedding from a single input embedding, while for Two-to-One case we choose the closest embedding from the centroid of two input embeddings in two modalities. We denote input and target modalities with    \\rightarrow   in Table  1 .",
            "Table  1  shows the performance for one-to-one and two-to-one retrieval tasks.  CentroBind  consistently excels in one-to-one retrieval for text and audio modalities, while  FABind  performs better for video retrieval. This might be due to a strong dependency between text and video, which may be suitable for  FABind  anchored at text modality. A notable observation is that the centroid of video and audio embeddings achieves the best text retrieval performance including one-to-one retrieval. This implies complementary information exists and is captured by  CentroBind .",
            "Moreover,  f 1 suf superscript subscript f 1 suf f_{1}^{\\rm suf} italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_suf end_POSTSUPERSCRIPT  obtained in Definition  1  with proper choice of  Z Z \\mathcal{Z} caligraphic_Z  achieves the maximum mutual information, implying together with  I  ( X ; Y )  min  { H  ( X ) , H  ( Y ) } I X Y H X H Y I({\\bf X};{\\bf Y})\\leq\\min\\{H({\\bf X}),H({\\bf Y})\\} italic_I ( bold_X ; bold_Y )  roman_min { italic_H ( bold_X ) , italic_H ( bold_Y ) }  that  I  ( f 1 suf  ( X 1 ) ; X 1 ) = H  ( X 1 ) I superscript subscript f 1 suf subscript X 1 subscript X 1 H subscript X 1 I(f_{1}^{\\rm suf}({\\bf X}_{1});{\\bf X}_{1})=H({\\bf X}_{1}) italic_I ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_suf end_POSTSUPERSCRIPT ( bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ; bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) = italic_H ( bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , where  H  ( X 1 ) H subscript X 1 H({\\bf X}_{1}) italic_H ( bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT )  is the entropy of  X 1 subscript X 1 {\\bf X}_{1} bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT   (Polyanskiy & Wu,  2024 ) . In other words, we have  H  ( X 1 | f 1 suf  ( X 1 ) ) = H  ( X 1 )  I  ( f 1 suf  ( X 1 ) ; X 1 ) = 0 H conditional subscript X 1 superscript subscript f 1 suf subscript X 1 H subscript X 1 I superscript subscript f 1 suf subscript X 1 subscript X 1 0 H({\\bf X}_{1}|f_{1}^{\\rm suf}({\\bf X}_{1}))=H({\\bf X}_{1})-I(f_{1}^{\\rm suf}({% \\bf X}_{1});{\\bf X}_{1})=0 italic_H ( bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_suf end_POSTSUPERSCRIPT ( bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ) = italic_H ( bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - italic_I ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_suf end_POSTSUPERSCRIPT ( bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ; bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) = 0 . This gives",
            "Substituting ( 13 ) and ( B.1 ) into ( B.1 ) yields",
            "We conclude the proof of Proposition  1  by noting that the optimality of  FABind  (i.e.,  I  ( f 1 suf  ( X 1 ) ; X i ) = I  ( f 1 suf  ( X 1 ) ; f i FB  ( X i ) ) ,  i  { 2 ,  , M } formulae-sequence I superscript subscript f 1 suf subscript X 1 subscript X i I superscript subscript f 1 suf subscript X 1 superscript subscript f i FB subscript X i for-all i 2  M I(f_{1}^{\\rm suf}({\\bf X}_{1});{\\bf X}_{i})=I(f_{1}^{\\rm suf}({\\bf X}_{1});f_{% i}^{\\rm FB}({\\bf X}_{i})),~{}\\forall i\\in\\{2,\\cdots,M\\} italic_I ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_suf end_POSTSUPERSCRIPT ( bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ; bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = italic_I ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_suf end_POSTSUPERSCRIPT ( bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ; italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_FB end_POSTSUPERSCRIPT ( bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) ,  italic_i  { 2 ,  , italic_M } ) yields",
            "To prove Theorem  1 , we leverage the reverse inequality of  M M M italic_M -variable Holder inequality  (Seo,  2013 , eq. (2.8)) . For the sake of completeness, we state the inequality in Lemma  1 .",
            "Then, the inner summation in ( 21 ) is bounded as",
            "where the labeled (in)equalities follow from:  ( a ) a \\rm(a) ( roman_a )  Lemma  1  and  C F , k , i = ( c F , k , i min + c F , k , i max ) 2 4  c F , k , i min  c F , k , i max subscript C F k i superscript superscript subscript c F k i superscript subscript c F k i 2 4 superscript subscript c F k i superscript subscript c F k i C_{\\mathcal{F},k,i}=\\frac{(c_{\\mathcal{F},k,i}^{\\min}+c_{\\mathcal{F},k,i}^{% \\max})^{2}}{4c_{\\mathcal{F},k,i}^{\\min}c_{\\mathcal{F},k,i}^{\\max}} italic_C start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT = divide start_ARG ( italic_c start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_min end_POSTSUPERSCRIPT + italic_c start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_max end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 4 italic_c start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_min end_POSTSUPERSCRIPT italic_c start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_max end_POSTSUPERSCRIPT end_ARG  with",
            "and  ( b ) b \\rm(b) ( roman_b )  the definition of anchor embedding ( 7 ). Substituting ( B.3 ) into ( 21 ) gives",
            "which concludes the proof of Theorem  1 ."
        ]
    },
    "id_table_2": {
        "caption": "",
        "table": "A3.EGx2",
        "footnotes": [],
        "references": [
            "First, selecting an anchor modality is crucial but challenging, as it depends on both embedding quality and task suitability. Common choices like images or text may still be suboptimal, especially for less common modalities lacking high-quality embeddings. Second, fixing an anchor can result in lost semantic information that might be better represented by other modalities. For instance, while  text  may describe a dog barks loudly,  sound  could reveal mood, and an  image  could add facial expression, which fixed alignment might miss. Third, optimizing only for anchor-to-other-modality overlooks information similarity between non-anchored modalities, leading to a loss of complementary insights. Addressing this among these non-anchored modalities would disrupt overall multimodal alignment, contradicting the primary goal of  CentroBind . We formally analyze deficiencies of the fixed anchor bind approaches in Section  2 .",
            "where the expectation is taken with respect to  P U , V   i = 1 N P V i subscript P U V superscript subscript product i 1 N subscript P subscript V i P_{{\\bf U},{\\bf V}}\\prod_{i=1}^{N}P_{{\\bf V}_{i}} italic_P start_POSTSUBSCRIPT bold_U , bold_V end_POSTSUBSCRIPT  start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_P start_POSTSUBSCRIPT bold_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT . Here, we say  ( U , V )  P U , V similar-to U V subscript P U V ({\\bf U},{\\bf V})\\sim P_{{\\bf U},{\\bf V}} ( bold_U , bold_V )  italic_P start_POSTSUBSCRIPT bold_U , bold_V end_POSTSUBSCRIPT  a positive pair and  ( U , V i )  P U  P V i similar-to U subscript V i subscript P U subscript P subscript V i ({\\bf U},{\\bf V}_{i})\\sim P_{\\bf U}P_{{\\bf V}_{i}} ( bold_U , bold_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  italic_P start_POSTSUBSCRIPT bold_U end_POSTSUBSCRIPT italic_P start_POSTSUBSCRIPT bold_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT  a negative pair. Moreover,  N  1 N 1 N\\geq 1 italic_N  1  and   > 0  0 \\tau>0 italic_ > 0  are hyper-parameters, specifying the number of negative samples and the temperature parameter. For simplicity, we assume that embeddings are normalized  (Wang & Isola,  2020 )  and are of the same dimensionality in this paper. Under the assumption of   = 1  1 \\tau=1 italic_ = 1 , the exponent  U   V /  superscript U top V  {\\bf U}^{\\top}{\\bf V}/\\tau bold_U start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT bold_V / italic_  in ( 2 ) is the cosine similarity score between  U U {\\bf U} bold_U  and  V V {\\bf V} bold_V .",
            "The proof can be found in Appendix  B.2 .",
            "Proposition  1  shows that the  FABind  encoders  F FB superscript F FB \\mathcal{F}^{\\rm FB} caligraphic_F start_POSTSUPERSCRIPT roman_FB end_POSTSUPERSCRIPT  learned with a sufficient anchor embedding can achieve the maximum shared information between the anchor and the other modalities. However, it does not guarantee the shared information between  non-anchored  modalities  I  ( f i  ( X i ) ; f l  ( X l ) ) , i , l = 1 I subscript f i subscript X i subscript f l subscript X l i l 1 I(f_{i}({\\bf X}_{i});f_{l}({\\bf X}_{l})),~{}i,l\\neq 1 italic_I ( italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ; italic_f start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( bold_X start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ) ) , italic_i , italic_l = 1 , which can also be seen from ( 4 ). Proposition  2  establishes that an insufficient anchor may lead to a reduction of shared information between the anchor and the other modalities, implying that the performance of  FABind  solely depends on the quality of the anchor.",
            "By optimizing this summation,  CentroBind  effectively captures both intra and shared information. This results in a more accurate representation for each modality. In contrast, as noted in Section  2.3 ,  FABind  does not adequately capture intra information and shared information between non-anchored modalities. This limitation highlights the advantage of  CentroBind  in achieving a more integrated multimodal representation than fixed anchor binding methods.",
            "The preceding analyses demonstrate that  CentroBind  addresses the limitations  P1 , P2 , P3 P1 P2 P3 \\bf{P1,P2,P3} bold_P1 , bold_P2 , bold_P3 , and  P4 P4 \\bf{P4} bold_P4  of  FABind  identified in Section  2.3 . We now conclude the analysis with a discussion that  CentroBind s unified representation is likely closer to an ideal platonic representation  (Huh et al.,  2024 )  compared to  FABind s. A platonic representation is defined as an ideal representation of reality that induces information in all modalities. From this perspective, a representation derived solely from a single modality, without leveraging others, may not be sufficient. In contrast to  FABind s approach, which relies exclusively on the fixed anchor modality,  CentroBind  constructs its unified representation space by incorporating information from all modalities. This suggests that  CentroBind s unified space is likely to retain a more comprehensive representation of all modalities. Thus,  CentroBind  emerges as a promising approach for developing a Planotic representation.",
            "Figure  2  shows the classification accuracies with a synthetic dataset of  M = 4 M 4 M=4 italic_M = 4  modalities. To obtain the results in Figure  2(a) , we initialize pretrained backbones for all modalities, apply  FABind  ( X i subscript X i {\\bf X}_{i} bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT -B) with anchor  X i subscript X i {\\bf X}_{i} bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  or  CentroBind  (CB), and evaluate accuracy (acc( Z i subscript Z i {\\bf Z}_{i} bold_Z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )) with embeddings from  i i i italic_i -th modality. We provide acc( Z i subscript Z i {\\bf Z}_{i} bold_Z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ), without any binding, for a reference. Figure  2(a)  verifies our analysis of  FABind  (Section  2.3 ) and CentroBind (Section  3.2 ): (1) the comparison between  X 1 subscript X 1 {\\bf X}_{1} bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT -B and  X 4 subscript X 4 {\\bf X}_{4} bold_X start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT -B shows the importance of choosing anchor modality; (2) the comparison between acc( Z 4 subscript Z 4 {\\bf Z}_{4} bold_Z start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT ) and  X 1 subscript X 1 {\\bf X}_{1} bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT -B: acc( Z 4 subscript Z 4 {\\bf Z}_{4} bold_Z start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT ) shows a performance deterioration by  FABind  , demonstrating that  FABind  does not capture intra information; (3) CB consistently outperform all  FABind  , indicating that CB successfully captures elements that  FABind  overlooks, including intra information and shared information among non-anchored modalities.",
            "Figure  2(b)  includes accuracies of  FABind  and CB with random backbones. Similarly, CB outperforms all baselines. Somewhat surprisingly, CB with random backbones (green curves) also performs better than  FABind  with pretrained backbones (red curves). This further supports our analysis that  CentroBind  is robust to the backbone quality as it optimizes intra and shared information, whereas  FABind  is sensitive to the backbone quality. Overall, these empirical results validate our findings. We provide additional experimental results on synthetic datasets with  M = 6 , 8 M 6 8 M=6,8 italic_M = 6 , 8  in Appendix  C.1 . With the larger number of modalities, CB still outperforms the baselines, strengthening our analysis of CB.",
            "In this section, we provide experiment results with a real-world dataset. We compare  CentroBind  and  FABind  anchored at text modality. We utilize the MUStARD dataset   (Castro et al.,  2019 )  for its rich combination of multimodal data with more than two modalities. It consists of 690 video clips (including audio) and text for sarcasm detection with labels such as sarcasm indicators and speaker names. For the backbones in  FABind  and  CentroBind  , we use the pretrained VideoMAE model  (Tong et al.,  2022 )  for video data, the pretrained WaveLM model  (Chen et al.,  2022 )  for audio data, and the pretrained BERT model  (Devlin et al.,  2019 )  for text data. A detailed description of the training setting is provided in Appendix  C.2 .",
            "We perform evaluations in zero-shot binary and multi-class classification tasks, One-to-One cross-modal retrieval, and Two-to-One cross-modal retrieval. For classification tasks, we use a Multi-Layer Perceptron (MLP) to perform sarcasm detection as a binary classification and speaker classification with  23 23 23 23  multi-class categories. In particular, MLP is trained on embeddings in a single modality (denoted by  Tr  in Table  2 ) and accuracy is evaluated on another modality (denoted by  Ev  in Table  2 ). In retrieval tasks, we measure the accuracy of correct retrieval. For One-to-One case, we retrieve data sample in different modality by choosing the closest embedding from a single input embedding, while for Two-to-One case we choose the closest embedding from the centroid of two input embeddings in two modalities. We denote input and target modalities with    \\rightarrow   in Table  1 .",
            "Table  2  presents results for sarcasm detection and speaker classification tasks, where Sar-1 indicates Top-1 accuracy for sarcasm, and Spk-1, Spk-3, and Spk-5 represent Top-1, Top-3, and Top-5 accuracies for speaker classification. In this experiment,  CentroBind  consistently outperforms  FABind  across all pairs of train and evaluation modalities, which can be distributed to  CentroBind  generally learning a better shared embedding space than  FABind .",
            "As analyzed in Section  2.3  and Section  3.2 , these results highlight the  CentroBind s ability to preserve intra and shared information among modalities, which are useful in unknown downstream tasks. Moreover, the zero-shot setting verifies the multimodal alignment of both  CentroBind  and  FABind .",
            "where the labeled inequalities follow from:  ( a ) a \\rm(a) ( roman_a )  the non-negativity of mutual information;  ( b ) b \\rm(b) ( roman_b )  the data processing inequality. This concludes the proof of Proposition  2 .",
            "Then, the inner summation in ( 21 ) is bounded as",
            "and  ( b ) b \\rm(b) ( roman_b )  the definition of anchor embedding ( 7 ). Substituting ( B.3 ) into ( 21 ) gives",
            "Once a latent variable  z z {\\boldsymbol{z}} bold_italic_z  is generated from GMM in ( 26 ), we generate data samples  ( x i , 1 , x i , 2 ,  , x i , N ) subscript x i 1 subscript x i 2  subscript x i N ({\\boldsymbol{x}}_{i,1},{\\boldsymbol{x}}_{i,2},\\cdots,{\\boldsymbol{x}}_{i,N}) ( bold_italic_x start_POSTSUBSCRIPT italic_i , 1 end_POSTSUBSCRIPT , bold_italic_x start_POSTSUBSCRIPT italic_i , 2 end_POSTSUBSCRIPT ,  , bold_italic_x start_POSTSUBSCRIPT italic_i , italic_N end_POSTSUBSCRIPT )  for  i i i italic_i -th modality using the conditional PDFs of  X i subscript X i {\\bf X}_{i} bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  given  z z {\\boldsymbol{z}} bold_italic_z , denoted by  p X i | Z  ( x i | z ) subscript p conditional subscript X i Z conditional subscript x i z p_{{\\bf X}_{i}|{\\bf Z}}({\\boldsymbol{x}}_{i}|{\\boldsymbol{z}}) italic_p start_POSTSUBSCRIPT bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | bold_Z end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | bold_italic_z ) . Specifically, we use the model  X i = g i  ( Z i ) + N subscript X i subscript g i subscript Z i N {\\bf X}_{i}=g_{i}({\\bf Z}_{i})+{\\bf N} bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_Z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) + bold_N , where  g i : R d z  R d x : subscript g i  superscript R subscript d z superscript R subscript d x g_{i}:\\mathbb{R}^{d_{z}}\\to\\mathbb{R}^{d_{x}} italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT : blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  is a non-linear projection from latent space to observation space, and  N  N  ( 0 , I d x ) similar-to N N 0 subscript I subscript d x {\\bf N}\\sim\\mathcal{N}({\\bf 0},I_{d_{x}}) bold_N  caligraphic_N ( bold_0 , italic_I start_POSTSUBSCRIPT italic_d start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT end_POSTSUBSCRIPT )  is Gaussian noise with zero-mean and identity covariance matrix. To make the inherent correlation between  X i subscript X i {\\bf X}_{i} bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and  Z i subscript Z i {\\bf Z}_{i} bold_Z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  different among modalities, we choose  g i subscript g i g_{i} italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  such that",
            "where  sigmoid  ( x ) = 1 1 + e  x sigmoid x 1 1 superscript e x {\\rm sigmoid}(x)=\\frac{1}{1+e^{-x}} roman_sigmoid ( italic_x ) = divide start_ARG 1 end_ARG start_ARG 1 + italic_e start_POSTSUPERSCRIPT - italic_x end_POSTSUPERSCRIPT end_ARG  is applied element-wise, and   i ( 1 )  R d x  d z superscript subscript  i 1 superscript R subscript d x subscript d z \\Theta_{i}^{(1)}\\in\\mathbb{R}^{d_{x}\\times d_{z}} roman_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT  italic_d start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  and   i ( 2 )  R d x  d x superscript subscript  i 2 superscript R subscript d x subscript d x \\Theta_{i}^{(2)}\\in\\mathbb{R}^{d_{x}\\times d_{x}} roman_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT  italic_d start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  are matrices randomly generated from Gaussian distribution. Moreover, after   i ( 1 ) , i  [ M ] superscript subscript  i 1 i delimited-[] M \\Theta_{i}^{(1)},i\\in[M] roman_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT , italic_i  [ italic_M ]  are generated, we set arbitrary columns of them all zero, so that the number of all zero columns decreases in  i i i italic_i . For example,  60 % percent 60 60\\% 60 %  of columns of   1 ( 1 ) superscript subscript  1 1 \\Theta_{1}^{(1)} roman_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT  are all-zero, while only  10 % percent 10 10\\% 10 %  of columns of   M ( 1 ) superscript subscript  M 1 \\Theta_{M}^{(1)} roman_ start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT  are all-zero. This enables approximate control the correlation between  X i subscript X i {\\bf X}_{i} bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and  Z Z {\\bf Z} bold_Z , providing estimates of best modality ( X M subscript X M {\\bf X}_{M} bold_X start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT ) or worst modality ( X 1 subscript X 1 {\\bf X}_{1} bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ). To have meaningful labels for this latent model, which requires for downstream tasks, we set the labels  Y Y {\\bf Y} bold_Y  being the component index in GMM. In particular, since there are  K K K italic_K  components in GMM ( 26 ), there exists  K K K italic_K  categories in  Y Y {\\bf Y} bold_Y . We conduct experiments with three different synthetic datasets by setting  M = 4 , 6 , 8 M 4 6 8 M=4,6,8 italic_M = 4 , 6 , 8 . For all synthetic datasets, we fix  d x = 16 subscript d x 16 d_{x}=16 italic_d start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT = 16 ,  d z = 8 subscript d z 8 d_{z}=8 italic_d start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT = 8 , and  K = 50 K 50 K=50 italic_K = 50 ."
        ]
    },
    "id_table_3": {
        "caption": "",
        "table": "A3.EGx3",
        "footnotes": [],
        "references": [
            "We propose a simple yet effective modification by replacing fixed anchors with  dynamic  centroid-based anchors computed from paired samples. Our method,  CentroBind , described in Section  3  removes the need for selecting a fixed anchor modality, instead calculates the centroid of all modality representations and generates an anchor representation, as shown in Figure  1(a) . Encoders are then trained to minimize the ensemble of InfoNCE loss  (Oord et al.,  2018 )  between this dynamic anchor and other representations, using the centroid as the anchor. The main intuition is that  a desirable anchor should be representative of all modalities , capturing the most comprehensive information, with well-trained encoders producing representations that naturally cluster around this shared centroid, reflecting their underlying semantic alignment.",
            "In this section, we characterize limitations of  FABind  regarding the important objective in multimodal representation learning, such as intra and shared information. To this end, we consider ( 3 ) as",
            "reflecting the fact that minimizing InfoNCE loss leads to maximizing mutual information. 3 3 3 In contrast to ( 3 ),  f i FB superscript subscript f i FB f_{i}^{\\rm FB} italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_FB end_POSTSUPERSCRIPT  in ( 4 ) might not be aligned with other modalities due to the one-to-one mapping invariant property of mutual information. However, we here do not analyze multimodal alignment of  FABind  from ( 4 ), but rather investigate the performance of encoders in terms of the sufficiency in Definition  1 .  Let  FABind  encoders from ( 4 ) for each modality be  F FB = { f 1 , f 2 FB ,  , f M FB } superscript F FB subscript f 1 superscript subscript f 2 FB  superscript subscript f M FB \\mathcal{F}^{\\rm FB}=\\{f_{1},f_{2}^{\\rm FB},\\cdots,f_{M}^{\\rm FB}\\} caligraphic_F start_POSTSUPERSCRIPT roman_FB end_POSTSUPERSCRIPT = { italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_FB end_POSTSUPERSCRIPT ,  , italic_f start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_FB end_POSTSUPERSCRIPT } . The anchor encoder  f 1 subscript f 1 f_{1} italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  is fixed during the entire  FABind  procedure. Moreover, we assume that  I  ( f 1  ( X 1 ) ; f i FB  ( X i ) ) = I  ( f 1  ( X 1 ) ; X i ) I subscript f 1 subscript X 1 superscript subscript f i FB subscript X i I subscript f 1 subscript X 1 subscript X i I(f_{1}({\\bf X}_{1});f_{i}^{\\rm FB}({\\bf X}_{i}))=I(f_{1}({\\bf X}_{1});{\\bf X}% _{i}) italic_I ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ; italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_FB end_POSTSUPERSCRIPT ( bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) = italic_I ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ; bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  is the maximum value that can be achieved by ( 4 ) due to data processing inequality  (Polyanskiy & Wu,  2024 ) . We next demonstrate that the quality of anchor embedding  f 1  ( X 1 ) subscript f 1 subscript X 1 f_{1}({\\bf X}_{1}) italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT )  significantly impacts the performance of  F FB superscript F FB \\mathcal{F}^{\\rm FB} caligraphic_F start_POSTSUPERSCRIPT roman_FB end_POSTSUPERSCRIPT  in terms of shared information. The following propositions show the dependency of  FABind  on anchor embedding quality.",
            "The proof is provided in Appendix  B.3 .",
            "By optimizing this summation,  CentroBind  effectively captures both intra and shared information. This results in a more accurate representation for each modality. In contrast, as noted in Section  2.3 ,  FABind  does not adequately capture intra information and shared information between non-anchored modalities. This limitation highlights the advantage of  CentroBind  in achieving a more integrated multimodal representation than fixed anchor binding methods.",
            "The preceding analyses demonstrate that  CentroBind  addresses the limitations  P1 , P2 , P3 P1 P2 P3 \\bf{P1,P2,P3} bold_P1 , bold_P2 , bold_P3 , and  P4 P4 \\bf{P4} bold_P4  of  FABind  identified in Section  2.3 . We now conclude the analysis with a discussion that  CentroBind s unified representation is likely closer to an ideal platonic representation  (Huh et al.,  2024 )  compared to  FABind s. A platonic representation is defined as an ideal representation of reality that induces information in all modalities. From this perspective, a representation derived solely from a single modality, without leveraging others, may not be sufficient. In contrast to  FABind s approach, which relies exclusively on the fixed anchor modality,  CentroBind  constructs its unified representation space by incorporating information from all modalities. This suggests that  CentroBind s unified space is likely to retain a more comprehensive representation of all modalities. Thus,  CentroBind  emerges as a promising approach for developing a Planotic representation.",
            "Figure  2  shows the classification accuracies with a synthetic dataset of  M = 4 M 4 M=4 italic_M = 4  modalities. To obtain the results in Figure  2(a) , we initialize pretrained backbones for all modalities, apply  FABind  ( X i subscript X i {\\bf X}_{i} bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT -B) with anchor  X i subscript X i {\\bf X}_{i} bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  or  CentroBind  (CB), and evaluate accuracy (acc( Z i subscript Z i {\\bf Z}_{i} bold_Z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )) with embeddings from  i i i italic_i -th modality. We provide acc( Z i subscript Z i {\\bf Z}_{i} bold_Z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ), without any binding, for a reference. Figure  2(a)  verifies our analysis of  FABind  (Section  2.3 ) and CentroBind (Section  3.2 ): (1) the comparison between  X 1 subscript X 1 {\\bf X}_{1} bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT -B and  X 4 subscript X 4 {\\bf X}_{4} bold_X start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT -B shows the importance of choosing anchor modality; (2) the comparison between acc( Z 4 subscript Z 4 {\\bf Z}_{4} bold_Z start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT ) and  X 1 subscript X 1 {\\bf X}_{1} bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT -B: acc( Z 4 subscript Z 4 {\\bf Z}_{4} bold_Z start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT ) shows a performance deterioration by  FABind  , demonstrating that  FABind  does not capture intra information; (3) CB consistently outperform all  FABind  , indicating that CB successfully captures elements that  FABind  overlooks, including intra information and shared information among non-anchored modalities.",
            "As analyzed in Section  2.3  and Section  3.2 , these results highlight the  CentroBind s ability to preserve intra and shared information among modalities, which are useful in unknown downstream tasks. Moreover, the zero-shot setting verifies the multimodal alignment of both  CentroBind  and  FABind .",
            "Substituting ( 13 ) and ( B.1 ) into ( B.1 ) yields",
            "and  ( b ) b \\rm(b) ( roman_b )  the definition of anchor embedding ( 7 ). Substituting ( B.3 ) into ( 21 ) gives",
            "Rearranging ( B.3 ) and setting   ~ =   | I B | M ~   subscript I B M \\tilde{\\tau}=\\frac{\\tau|\\mathcal{I}_{B}|}{M} over~ start_ARG italic_ end_ARG = divide start_ARG italic_ | caligraphic_I start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT | end_ARG start_ARG italic_M end_ARG  in ( B.3 ) and ( B.3 ) yield",
            "Additional experimental results on synthetic datasets with  M = 6 , 8 M 6 8 M=6,8 italic_M = 6 , 8  number of modalities are shown in Figure  3  and Figure  4 ."
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "A3.EGx4",
        "footnotes": [],
        "references": [
            "reflecting the fact that minimizing InfoNCE loss leads to maximizing mutual information. 3 3 3 In contrast to ( 3 ),  f i FB superscript subscript f i FB f_{i}^{\\rm FB} italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_FB end_POSTSUPERSCRIPT  in ( 4 ) might not be aligned with other modalities due to the one-to-one mapping invariant property of mutual information. However, we here do not analyze multimodal alignment of  FABind  from ( 4 ), but rather investigate the performance of encoders in terms of the sufficiency in Definition  1 .  Let  FABind  encoders from ( 4 ) for each modality be  F FB = { f 1 , f 2 FB ,  , f M FB } superscript F FB subscript f 1 superscript subscript f 2 FB  superscript subscript f M FB \\mathcal{F}^{\\rm FB}=\\{f_{1},f_{2}^{\\rm FB},\\cdots,f_{M}^{\\rm FB}\\} caligraphic_F start_POSTSUPERSCRIPT roman_FB end_POSTSUPERSCRIPT = { italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_FB end_POSTSUPERSCRIPT ,  , italic_f start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_FB end_POSTSUPERSCRIPT } . The anchor encoder  f 1 subscript f 1 f_{1} italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  is fixed during the entire  FABind  procedure. Moreover, we assume that  I  ( f 1  ( X 1 ) ; f i FB  ( X i ) ) = I  ( f 1  ( X 1 ) ; X i ) I subscript f 1 subscript X 1 superscript subscript f i FB subscript X i I subscript f 1 subscript X 1 subscript X i I(f_{1}({\\bf X}_{1});f_{i}^{\\rm FB}({\\bf X}_{i}))=I(f_{1}({\\bf X}_{1});{\\bf X}% _{i}) italic_I ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ; italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_FB end_POSTSUPERSCRIPT ( bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) = italic_I ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ; bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  is the maximum value that can be achieved by ( 4 ) due to data processing inequality  (Polyanskiy & Wu,  2024 ) . We next demonstrate that the quality of anchor embedding  f 1  ( X 1 ) subscript f 1 subscript X 1 f_{1}({\\bf X}_{1}) italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT )  significantly impacts the performance of  F FB superscript F FB \\mathcal{F}^{\\rm FB} caligraphic_F start_POSTSUPERSCRIPT roman_FB end_POSTSUPERSCRIPT  in terms of shared information. The following propositions show the dependency of  FABind  on anchor embedding quality.",
            "Let  f 1 suf  ( X 1 ) superscript subscript f 1 suf subscript X 1 f_{1}^{\\rm suf}({\\bf X}_{1}) italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_suf end_POSTSUPERSCRIPT ( bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT )  be a sufficient embedding of the anchor  X 1 subscript X 1 {\\bf X}_{1} bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , and let  X i , i  [ M ] subscript X i i delimited-[] M {\\bf X}_{i},i\\in[M] bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_i  [ italic_M ]  be discrete. Assume that  f i FB , i  { 2 ,  , M } superscript subscript f i FB i 2  M f_{i}^{\\rm FB},i\\in\\{2,\\cdots,M\\} italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_FB end_POSTSUPERSCRIPT , italic_i  { 2 ,  , italic_M }  are obtained by ( 4 ) with a sufficient anchor encoder  f 1 = f 1 suf subscript f 1 superscript subscript f 1 suf f_{1}=f_{1}^{\\rm suf} italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_suf end_POSTSUPERSCRIPT , i.e.,  I  ( f 1 suf  ( X 1 ) ; f i FB  ( X i ) ) = I  ( f 1 suf  ( X 1 ) ; X i ) I superscript subscript f 1 suf subscript X 1 superscript subscript f i FB subscript X i I superscript subscript f 1 suf subscript X 1 subscript X i I(f_{1}^{\\rm suf}({\\bf X}_{1});f_{i}^{\\rm FB}({\\bf X}_{i}))=I(f_{1}^{\\rm suf}(% {\\bf X}_{1});{\\bf X}_{i}) italic_I ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_suf end_POSTSUPERSCRIPT ( bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ; italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_FB end_POSTSUPERSCRIPT ( bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) = italic_I ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_suf end_POSTSUPERSCRIPT ( bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ; bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) . Then,",
            "Let  f 1 ins  ( X 1 ) superscript subscript f 1 ins subscript X 1 f_{1}^{\\rm ins}({\\bf X}_{1}) italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ins end_POSTSUPERSCRIPT ( bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT )  be insufficient embedding of the anchor  X 1 subscript X 1 {\\bf X}_{1} bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  for  X 1 subscript X 1 {\\bf X}_{1} bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , in the sense that there exists some   > 0 italic- 0 \\epsilon>0 italic_ > 0  such that  I  ( f 1 ins  ( X 1 ) ; X 1 ) <   max f  I  ( f  ( X 1 ) ; X 1 ) I superscript subscript f 1 ins subscript X 1 subscript X 1 italic- subscript f I f subscript X 1 subscript X 1 I(f_{1}^{\\rm ins}({\\bf X}_{1});{\\bf X}_{1})<\\epsilon\\leq\\max_{f}I(f({\\bf X}_{1% });{\\bf X}_{1}) italic_I ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ins end_POSTSUPERSCRIPT ( bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ; bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) < italic_  roman_max start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT italic_I ( italic_f ( bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ; bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) . Assume that  f i FB , i  { 2 ,  , M } superscript subscript f i FB i 2  M f_{i}^{\\rm FB},i\\in\\{2,\\cdots,M\\} italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_FB end_POSTSUPERSCRIPT , italic_i  { 2 ,  , italic_M }  are obtained by ( 4 ) with  f 1 = f 1 ins subscript f 1 superscript subscript f 1 ins f_{1}=f_{1}^{\\rm ins} italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ins end_POSTSUPERSCRIPT , i.e.,  I  ( f 1 ins  ( X 1 ) ; f i FB  ( X i ) ) = I  ( f 1 ins  ( X 1 ) ; X i ) I superscript subscript f 1 ins subscript X 1 superscript subscript f i FB subscript X i I superscript subscript f 1 ins subscript X 1 subscript X i I(f_{1}^{\\rm ins}({\\bf X}_{1});f_{i}^{\\rm FB}({\\bf X}_{i}))=I(f_{1}^{\\rm ins}(% {\\bf X}_{1});{\\bf X}_{i}) italic_I ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ins end_POSTSUPERSCRIPT ( bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ; italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_FB end_POSTSUPERSCRIPT ( bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) = italic_I ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ins end_POSTSUPERSCRIPT ( bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ; bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) . Then,",
            "Proposition  1  shows that the  FABind  encoders  F FB superscript F FB \\mathcal{F}^{\\rm FB} caligraphic_F start_POSTSUPERSCRIPT roman_FB end_POSTSUPERSCRIPT  learned with a sufficient anchor embedding can achieve the maximum shared information between the anchor and the other modalities. However, it does not guarantee the shared information between  non-anchored  modalities  I  ( f i  ( X i ) ; f l  ( X l ) ) , i , l = 1 I subscript f i subscript X i subscript f l subscript X l i l 1 I(f_{i}({\\bf X}_{i});f_{l}({\\bf X}_{l})),~{}i,l\\neq 1 italic_I ( italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ; italic_f start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( bold_X start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ) ) , italic_i , italic_l = 1 , which can also be seen from ( 4 ). Proposition  2  establishes that an insufficient anchor may lead to a reduction of shared information between the anchor and the other modalities, implying that the performance of  FABind  solely depends on the quality of the anchor.",
            "The analysis reveals several limitations in  FABind . Firstly, achieving maximum shared information requires sufficient anchor representation, which depends on having both an informative modality and a sufficient encoder. Without these conditions,  FABind  may not effectively capture shared information. Secondly, even with sufficient anchor representation,  FABind  may not provide encoders with maximum intra information. This is because its objective function ( 4 ) does not take into account intra information. Thirdly,  FABind s objective function ( 4 ) focuses solely on learning shared information between pairs of anchor and non-anchored modalities, while disregarding shared information among non-anchor modalities. This limitation renders  FABind  less effective when significant shared information exists among non-anchored modalities. Lastly, the representation produced by  FABind  may not approximate an ideal representation, such as Platonic representation  (Huh et al.,  2024 ) . The Platonic representation is an idealized depiction of reality, which generates all modalities through projections. While integrating all modalities is crucial for constructing such a comprehensive representation,  FABind s approach, which focuses solely on the anchor modality, falls short of this ideal. A more promising direction, which we pursue in this paper, involves learning representations from multiple modalities, fully leveraging fine-grained, sample-level information.",
            "Additional experimental results on synthetic datasets with  M = 6 , 8 M 6 8 M=6,8 italic_M = 6 , 8  number of modalities are shown in Figure  3  and Figure  4 ."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "A3.EGx5",
        "footnotes": [],
        "references": []
    },
    "id_table_6": {
        "caption": "",
        "table": "A3.EGx6",
        "footnotes": [],
        "references": [
            "Once a latent variable  z z {\\boldsymbol{z}} bold_italic_z  is generated from GMM in ( 26 ), we generate data samples  ( x i , 1 , x i , 2 ,  , x i , N ) subscript x i 1 subscript x i 2  subscript x i N ({\\boldsymbol{x}}_{i,1},{\\boldsymbol{x}}_{i,2},\\cdots,{\\boldsymbol{x}}_{i,N}) ( bold_italic_x start_POSTSUBSCRIPT italic_i , 1 end_POSTSUBSCRIPT , bold_italic_x start_POSTSUBSCRIPT italic_i , 2 end_POSTSUBSCRIPT ,  , bold_italic_x start_POSTSUBSCRIPT italic_i , italic_N end_POSTSUBSCRIPT )  for  i i i italic_i -th modality using the conditional PDFs of  X i subscript X i {\\bf X}_{i} bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  given  z z {\\boldsymbol{z}} bold_italic_z , denoted by  p X i | Z  ( x i | z ) subscript p conditional subscript X i Z conditional subscript x i z p_{{\\bf X}_{i}|{\\bf Z}}({\\boldsymbol{x}}_{i}|{\\boldsymbol{z}}) italic_p start_POSTSUBSCRIPT bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | bold_Z end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | bold_italic_z ) . Specifically, we use the model  X i = g i  ( Z i ) + N subscript X i subscript g i subscript Z i N {\\bf X}_{i}=g_{i}({\\bf Z}_{i})+{\\bf N} bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_Z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) + bold_N , where  g i : R d z  R d x : subscript g i  superscript R subscript d z superscript R subscript d x g_{i}:\\mathbb{R}^{d_{z}}\\to\\mathbb{R}^{d_{x}} italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT : blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  is a non-linear projection from latent space to observation space, and  N  N  ( 0 , I d x ) similar-to N N 0 subscript I subscript d x {\\bf N}\\sim\\mathcal{N}({\\bf 0},I_{d_{x}}) bold_N  caligraphic_N ( bold_0 , italic_I start_POSTSUBSCRIPT italic_d start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT end_POSTSUBSCRIPT )  is Gaussian noise with zero-mean and identity covariance matrix. To make the inherent correlation between  X i subscript X i {\\bf X}_{i} bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and  Z i subscript Z i {\\bf Z}_{i} bold_Z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  different among modalities, we choose  g i subscript g i g_{i} italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  such that",
            "where  sigmoid  ( x ) = 1 1 + e  x sigmoid x 1 1 superscript e x {\\rm sigmoid}(x)=\\frac{1}{1+e^{-x}} roman_sigmoid ( italic_x ) = divide start_ARG 1 end_ARG start_ARG 1 + italic_e start_POSTSUPERSCRIPT - italic_x end_POSTSUPERSCRIPT end_ARG  is applied element-wise, and   i ( 1 )  R d x  d z superscript subscript  i 1 superscript R subscript d x subscript d z \\Theta_{i}^{(1)}\\in\\mathbb{R}^{d_{x}\\times d_{z}} roman_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT  italic_d start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  and   i ( 2 )  R d x  d x superscript subscript  i 2 superscript R subscript d x subscript d x \\Theta_{i}^{(2)}\\in\\mathbb{R}^{d_{x}\\times d_{x}} roman_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT  italic_d start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  are matrices randomly generated from Gaussian distribution. Moreover, after   i ( 1 ) , i  [ M ] superscript subscript  i 1 i delimited-[] M \\Theta_{i}^{(1)},i\\in[M] roman_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT , italic_i  [ italic_M ]  are generated, we set arbitrary columns of them all zero, so that the number of all zero columns decreases in  i i i italic_i . For example,  60 % percent 60 60\\% 60 %  of columns of   1 ( 1 ) superscript subscript  1 1 \\Theta_{1}^{(1)} roman_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT  are all-zero, while only  10 % percent 10 10\\% 10 %  of columns of   M ( 1 ) superscript subscript  M 1 \\Theta_{M}^{(1)} roman_ start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT  are all-zero. This enables approximate control the correlation between  X i subscript X i {\\bf X}_{i} bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and  Z Z {\\bf Z} bold_Z , providing estimates of best modality ( X M subscript X M {\\bf X}_{M} bold_X start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT ) or worst modality ( X 1 subscript X 1 {\\bf X}_{1} bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ). To have meaningful labels for this latent model, which requires for downstream tasks, we set the labels  Y Y {\\bf Y} bold_Y  being the component index in GMM. In particular, since there are  K K K italic_K  components in GMM ( 26 ), there exists  K K K italic_K  categories in  Y Y {\\bf Y} bold_Y . We conduct experiments with three different synthetic datasets by setting  M = 4 , 6 , 8 M 4 6 8 M=4,6,8 italic_M = 4 , 6 , 8 . For all synthetic datasets, we fix  d x = 16 subscript d x 16 d_{x}=16 italic_d start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT = 16 ,  d z = 8 subscript d z 8 d_{z}=8 italic_d start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT = 8 , and  K = 50 K 50 K=50 italic_K = 50 ."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "A3.EGx7",
        "footnotes": [],
        "references": [
            "and  ( b ) b \\rm(b) ( roman_b )  the definition of anchor embedding ( 7 ). Substituting ( B.3 ) into ( 21 ) gives"
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "A3.EGx8",
        "footnotes": [],
        "references": [
            "Once anchor embeddings  { a j } j subscript subscript a j j \\{{\\boldsymbol{a}}_{j}\\}_{j} { bold_italic_a start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  are derived from a batch of data  B = { x i , j } i , j B subscript subscript x i j i j B=\\{{\\boldsymbol{x}}_{i,j}\\}_{i,j} italic_B = { bold_italic_x start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT ,  CentroBind  aligns each modality-specific encoders embedding with the anchor embedding by minimizing the InfoNCE loss. Specifically, let  A = mean  ( { f i  ( X i ) } i ) A mean subscript subscript f i subscript X i i {\\bf A}=\\text{mean}(\\{f_{i}({\\bf X}_{i})\\}_{i}) bold_A = mean ( { italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  represent the anchor embedding variable. Then,  CentroBind  aims to minimize the InfoNCE loss  I NCE  ( A ; f i  ( X i ) ) subscript I NCE A subscript f i subscript X i I_{\\rm NCE}({\\bf A};f_{i}({\\bf X}_{i})) italic_I start_POSTSUBSCRIPT roman_NCE end_POSTSUBSCRIPT ( bold_A ; italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) )  across all modalities  i  [ M ] i delimited-[] M i\\in[M] italic_i  [ italic_M ] . A detailed expression for this loss is provided in ( 8 ).",
            "We start by providing a lower bound of  CentroBind s objective function  L CB  ( f i |  ) subscript L CB conditional subscript f i  \\mathcal{L}_{\\rm CB}(f_{i}|\\tau) caligraphic_L start_POSTSUBSCRIPT roman_CB end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_ )  ( 8 ) in Theorem  1 , followed by an analysis of minimizing  L CB  ( f i |  ) subscript L CB conditional subscript f i  \\mathcal{L}_{\\rm CB}(f_{i}|\\tau) caligraphic_L start_POSTSUBSCRIPT roman_CB end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_ ) ."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "S3.E9",
        "footnotes": [],
        "references": [
            "Consider  B = { x i , j : i  [ M ] , j  I B } B conditional-set subscript x i j formulae-sequence i delimited-[] M j subscript I B B=\\{{\\boldsymbol{x}}_{i,j}:i\\in[M],j\\in\\mathcal{I}_{B}\\} italic_B = { bold_italic_x start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT : italic_i  [ italic_M ] , italic_j  caligraphic_I start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT }  with a set of indices  I B subscript I B \\mathcal{I}_{B} caligraphic_I start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT , where  x i , j subscript x i j {\\boldsymbol{x}}_{i,j} bold_italic_x start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT  is the  j j j italic_j -th sample of  i i i italic_i -th modality. Then, for any encoders  { f i } i subscript subscript f i i \\{f_{i}\\}_{i} { italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and for any   > 0  0 \\tau>0 italic_ > 0 , ( 9a ) is bounded as",
            "Theorem  1  provides a lower bound of  I NCE ( A ; f i ( X i ) |  ) I_{\\rm NCE}\\left({\\bf A};f_{i}({\\bf X}_{i})\\;\\middle|\\;{\\tau}\\right) italic_I start_POSTSUBSCRIPT roman_NCE end_POSTSUBSCRIPT ( bold_A ; italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) | italic_ )  in ( 9a ), which is a part of  CentroBind  objective  L CB  ( f i |  ) subscript L CB conditional subscript f i  \\mathcal{L}_{\\rm CB}(f_{i}|\\tau) caligraphic_L start_POSTSUBSCRIPT roman_CB end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_ ) . Thus  CentroBind  consequentially minimizes the lower bound ( 10 ) that consists of two terms,   l = 1 M I NCE ( f l ( X l  ) ; f i ( X i ) |   M | I B | ) \\sum_{l=1}^{M}I_{\\rm NCE}\\left(f_{l}({\\bf X}_{l}^{\\prime});f_{i}({\\bf X}_{i})% \\;\\middle|\\;\\frac{{\\tau}M}{|\\mathcal{I}_{B}|}\\right)  start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT italic_I start_POSTSUBSCRIPT roman_NCE end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( bold_X start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) ; italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) | divide start_ARG italic_ italic_M end_ARG start_ARG | caligraphic_I start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT | end_ARG )  and    k = 1 | I B | log  C F , k , i superscript subscript k 1 subscript I B subscript C F k i -\\sum_{k=1}^{|\\mathcal{I}_{B}|}\\log C_{\\mathcal{F},k,i} -  start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | caligraphic_I start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT | end_POSTSUPERSCRIPT roman_log italic_C start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT . We next provide some intuitive explanation on the effect of such a minimization of the lower bound."
        ]
    },
    "id_table_10": {
        "caption": "",
        "table": "A3.EGx10",
        "footnotes": [],
        "references": [
            "Theorem  1  provides a lower bound of  I NCE ( A ; f i ( X i ) |  ) I_{\\rm NCE}\\left({\\bf A};f_{i}({\\bf X}_{i})\\;\\middle|\\;{\\tau}\\right) italic_I start_POSTSUBSCRIPT roman_NCE end_POSTSUBSCRIPT ( bold_A ; italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) | italic_ )  in ( 9a ), which is a part of  CentroBind  objective  L CB  ( f i |  ) subscript L CB conditional subscript f i  \\mathcal{L}_{\\rm CB}(f_{i}|\\tau) caligraphic_L start_POSTSUBSCRIPT roman_CB end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_ ) . Thus  CentroBind  consequentially minimizes the lower bound ( 10 ) that consists of two terms,   l = 1 M I NCE ( f l ( X l  ) ; f i ( X i ) |   M | I B | ) \\sum_{l=1}^{M}I_{\\rm NCE}\\left(f_{l}({\\bf X}_{l}^{\\prime});f_{i}({\\bf X}_{i})% \\;\\middle|\\;\\frac{{\\tau}M}{|\\mathcal{I}_{B}|}\\right)  start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT italic_I start_POSTSUBSCRIPT roman_NCE end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( bold_X start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) ; italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) | divide start_ARG italic_ italic_M end_ARG start_ARG | caligraphic_I start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT | end_ARG )  and    k = 1 | I B | log  C F , k , i superscript subscript k 1 subscript I B subscript C F k i -\\sum_{k=1}^{|\\mathcal{I}_{B}|}\\log C_{\\mathcal{F},k,i} -  start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | caligraphic_I start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT | end_POSTSUPERSCRIPT roman_log italic_C start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT . We next provide some intuitive explanation on the effect of such a minimization of the lower bound."
        ]
    },
    "id_table_11": {
        "caption": "",
        "table": "A3.EGx11",
        "footnotes": [],
        "references": [
            "We show the effect of growing  C F , k , i subscript C F k i C_{\\mathcal{F},k,i} italic_C start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT  in terms of cosine similarity score between embeddings. Since  C F , k , i = 1 4  (  + 1  ) 2 subscript C F k i 1 4 superscript  1  2 C_{\\mathcal{F},k,i}=\\frac{1}{4}\\left(\\sqrt{\\gamma}+\\sqrt{\\frac{1}{\\gamma}}% \\right)^{2} italic_C start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG 4 end_ARG ( square-root start_ARG italic_ end_ARG + square-root start_ARG divide start_ARG 1 end_ARG start_ARG italic_ end_ARG end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  with   = c F , k , i max c F , k , i min  1  superscript subscript c F k i superscript subscript c F k i 1 \\gamma=\\frac{c_{\\mathcal{F},k,i}^{\\max}}{c_{\\mathcal{F},k,i}^{\\min}}\\geq 1 italic_ = divide start_ARG italic_c start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_max end_POSTSUPERSCRIPT end_ARG start_ARG italic_c start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_min end_POSTSUPERSCRIPT end_ARG  1 , maximizing  C F , k , i subscript C F k i C_{\\mathcal{F},k,i} italic_C start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT  is equivalent to simultaneously maximizing  c F , k , i max superscript subscript c F k i c_{\\mathcal{F},k,i}^{\\max} italic_c start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_max end_POSTSUPERSCRIPT  and minimizing  c F , k , i min superscript subscript c F k i c_{\\mathcal{F},k,i}^{\\min} italic_c start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_min end_POSTSUPERSCRIPT . For ease of the analysis, we assume that the encoders are reasonably well-trained. Then, since a positive pair of embeddings normally yields higher similarity score,  c F , k , i max superscript subscript c F k i c_{\\mathcal{F},k,i}^{\\max} italic_c start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_max end_POSTSUPERSCRIPT  should be obtained by choosing  l = i l i l=i italic_l = italic_i  and  j = k j k j=k italic_j = italic_k  in ( 11 ) as such choices make  x l , k  superscript subscript x l k  {\\boldsymbol{x}}_{l,k}^{\\prime} bold_italic_x start_POSTSUBSCRIPT italic_l , italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  be positive pair with  x i , j subscript x i j {\\boldsymbol{x}}_{i,j} bold_italic_x start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT . Thus,  c F , k , i max superscript subscript c F k i c_{\\mathcal{F},k,i}^{\\max} italic_c start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_max end_POSTSUPERSCRIPT  is roughly proportional to the similarity score of a positive pair of embeddings. Conversely,  c F , k , i min superscript subscript c F k i c_{\\mathcal{F},k,i}^{\\min} italic_c start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_min end_POSTSUPERSCRIPT  corresponds to the similarity scores of negative pairs, which tend to be low. Hence, minimizing    k = 1 | I B | log  C F , k , i superscript subscript k 1 subscript I B subscript C F k i -\\sum_{k=1}^{|\\mathcal{I}_{B}|}\\log C_{\\mathcal{F},k,i} -  start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | caligraphic_I start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT | end_POSTSUPERSCRIPT roman_log italic_C start_POSTSUBSCRIPT caligraphic_F , italic_k , italic_i end_POSTSUBSCRIPT  enhances the similarity scores for positive pairs and reduces those for negative pairs, improving the overall multimodal alignment."
        ]
    },
    "id_table_12": {
        "caption": "",
        "table": "A3.EGx12",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "",
        "table": "A3.EGx13",
        "footnotes": [],
        "references": [
            "Substituting ( 13 ) and ( B.1 ) into ( B.1 ) yields"
        ]
    },
    "id_table_14": {
        "caption": "",
        "table": "A3.EGx14",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "",
        "table": "A3.EGx15",
        "footnotes": [],
        "references": []
    },
    "id_table_16": {
        "caption": "",
        "table": "A3.EGx16",
        "footnotes": [],
        "references": []
    },
    "id_table_17": {
        "caption": "",
        "table": "A3.EGx17",
        "footnotes": [],
        "references": []
    },
    "id_table_18": {
        "caption": "",
        "table": "A3.EGx18",
        "footnotes": [],
        "references": []
    },
    "id_table_19": {
        "caption": "",
        "table": "A3.EGx19",
        "footnotes": [],
        "references": []
    },
    "id_table_20": {
        "caption": "",
        "table": "A3.EGx20",
        "footnotes": [],
        "references": []
    },
    "id_table_21": {
        "caption": "",
        "table": "A3.EGx21",
        "footnotes": [],
        "references": [
            "Then, the inner summation in ( 21 ) is bounded as",
            "and  ( b ) b \\rm(b) ( roman_b )  the definition of anchor embedding ( 7 ). Substituting ( B.3 ) into ( 21 ) gives"
        ]
    },
    "id_table_22": {
        "caption": "",
        "table": "A3.EGx22",
        "footnotes": [],
        "references": []
    },
    "id_table_23": {
        "caption": "",
        "table": "A3.EGx23",
        "footnotes": [],
        "references": []
    },
    "id_table_24": {
        "caption": "",
        "table": "A3.EGx24",
        "footnotes": [],
        "references": []
    },
    "id_table_25": {
        "caption": "",
        "table": "A3.EGx25",
        "footnotes": [],
        "references": []
    },
    "id_table_26": {
        "caption": "",
        "table": "A3.EGx26",
        "footnotes": [],
        "references": [
            "Once a latent variable  z z {\\boldsymbol{z}} bold_italic_z  is generated from GMM in ( 26 ), we generate data samples  ( x i , 1 , x i , 2 ,  , x i , N ) subscript x i 1 subscript x i 2  subscript x i N ({\\boldsymbol{x}}_{i,1},{\\boldsymbol{x}}_{i,2},\\cdots,{\\boldsymbol{x}}_{i,N}) ( bold_italic_x start_POSTSUBSCRIPT italic_i , 1 end_POSTSUBSCRIPT , bold_italic_x start_POSTSUBSCRIPT italic_i , 2 end_POSTSUBSCRIPT ,  , bold_italic_x start_POSTSUBSCRIPT italic_i , italic_N end_POSTSUBSCRIPT )  for  i i i italic_i -th modality using the conditional PDFs of  X i subscript X i {\\bf X}_{i} bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  given  z z {\\boldsymbol{z}} bold_italic_z , denoted by  p X i | Z  ( x i | z ) subscript p conditional subscript X i Z conditional subscript x i z p_{{\\bf X}_{i}|{\\bf Z}}({\\boldsymbol{x}}_{i}|{\\boldsymbol{z}}) italic_p start_POSTSUBSCRIPT bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | bold_Z end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | bold_italic_z ) . Specifically, we use the model  X i = g i  ( Z i ) + N subscript X i subscript g i subscript Z i N {\\bf X}_{i}=g_{i}({\\bf Z}_{i})+{\\bf N} bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_Z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) + bold_N , where  g i : R d z  R d x : subscript g i  superscript R subscript d z superscript R subscript d x g_{i}:\\mathbb{R}^{d_{z}}\\to\\mathbb{R}^{d_{x}} italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT : blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  is a non-linear projection from latent space to observation space, and  N  N  ( 0 , I d x ) similar-to N N 0 subscript I subscript d x {\\bf N}\\sim\\mathcal{N}({\\bf 0},I_{d_{x}}) bold_N  caligraphic_N ( bold_0 , italic_I start_POSTSUBSCRIPT italic_d start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT end_POSTSUBSCRIPT )  is Gaussian noise with zero-mean and identity covariance matrix. To make the inherent correlation between  X i subscript X i {\\bf X}_{i} bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and  Z i subscript Z i {\\bf Z}_{i} bold_Z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  different among modalities, we choose  g i subscript g i g_{i} italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  such that",
            "where  sigmoid  ( x ) = 1 1 + e  x sigmoid x 1 1 superscript e x {\\rm sigmoid}(x)=\\frac{1}{1+e^{-x}} roman_sigmoid ( italic_x ) = divide start_ARG 1 end_ARG start_ARG 1 + italic_e start_POSTSUPERSCRIPT - italic_x end_POSTSUPERSCRIPT end_ARG  is applied element-wise, and   i ( 1 )  R d x  d z superscript subscript  i 1 superscript R subscript d x subscript d z \\Theta_{i}^{(1)}\\in\\mathbb{R}^{d_{x}\\times d_{z}} roman_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT  italic_d start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  and   i ( 2 )  R d x  d x superscript subscript  i 2 superscript R subscript d x subscript d x \\Theta_{i}^{(2)}\\in\\mathbb{R}^{d_{x}\\times d_{x}} roman_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT  italic_d start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  are matrices randomly generated from Gaussian distribution. Moreover, after   i ( 1 ) , i  [ M ] superscript subscript  i 1 i delimited-[] M \\Theta_{i}^{(1)},i\\in[M] roman_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT , italic_i  [ italic_M ]  are generated, we set arbitrary columns of them all zero, so that the number of all zero columns decreases in  i i i italic_i . For example,  60 % percent 60 60\\% 60 %  of columns of   1 ( 1 ) superscript subscript  1 1 \\Theta_{1}^{(1)} roman_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT  are all-zero, while only  10 % percent 10 10\\% 10 %  of columns of   M ( 1 ) superscript subscript  M 1 \\Theta_{M}^{(1)} roman_ start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT  are all-zero. This enables approximate control the correlation between  X i subscript X i {\\bf X}_{i} bold_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and  Z Z {\\bf Z} bold_Z , providing estimates of best modality ( X M subscript X M {\\bf X}_{M} bold_X start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT ) or worst modality ( X 1 subscript X 1 {\\bf X}_{1} bold_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ). To have meaningful labels for this latent model, which requires for downstream tasks, we set the labels  Y Y {\\bf Y} bold_Y  being the component index in GMM. In particular, since there are  K K K italic_K  components in GMM ( 26 ), there exists  K K K italic_K  categories in  Y Y {\\bf Y} bold_Y . We conduct experiments with three different synthetic datasets by setting  M = 4 , 6 , 8 M 4 6 8 M=4,6,8 italic_M = 4 , 6 , 8 . For all synthetic datasets, we fix  d x = 16 subscript d x 16 d_{x}=16 italic_d start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT = 16 ,  d z = 8 subscript d z 8 d_{z}=8 italic_d start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT = 8 , and  K = 50 K 50 K=50 italic_K = 50 ."
        ]
    },
    "id_table_27": {
        "caption": "",
        "table": "A3.EGx27",
        "footnotes": [],
        "references": []
    }
}