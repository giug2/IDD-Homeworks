{
    "id_table_1": {
        "caption": "Table 1 .  Comparison of Results across the LLM Enhanced Thematic Analysis Strategies Employed",
        "table": "S4.T1.2",
        "footnotes": [],
        "references": [
            "In Table 1, the performance of various LLM prompting techniques including Chain of Thought, Few Shot, Zero Shot and RAG, are compared across different embedding models (Distillbert-base-uncased, Bert-base-uncased, and Roberta-large). This comparison aims to evaluate the robustness and effectiveness of these prompting techniques. Our results indicate that while each prompting technique shows varying level of precision, recall and F1-score, RAG consistently outperform the others on all three metrics, achieving highest performance across all models."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 .  Example Output from LLM approach compared to Keywords from LDA Topic One",
        "table": "S5.T2.2",
        "footnotes": [],
        "references": [
            "Using a RAG approach towards an LLM-augmented qualitative research analyzing semi-structure interviews shows great promise compared to natural language processing methods like Latent Dirichlet allocation (LDA). Currently, there are no widely accepted methods for comparing the two approaches as there is no bridge to compare keywords to themes, except from a human-evaluator ease of interpretability standpoint. We performed topic modeling analysis on the same dataset with the broader aim of finding themes. Manually comparing both approaches, each researcher of this workstream independently found that any of the approaches using an LLM yielded much greater context and consequently, better interpretability than the traditional LDA approach. This is likely because, with LDA, the model outputs a list of words and probability for each topic. With these words, the researcher would then have to manually define the topic. While this approach increases researcher flexibility, it remains time and resource consuming. In contrast, with the LLM approach, the output is richer in context of what particular topics mean. For example, our LDA model yielded 5 topics (see: Appendix A Figure 3). The first 10 words for topic 1 can also be seen in Table 2. Putting these words together into a comprehensive theme can be challenging without more context. However, an LLM is able to generate context grounded in the participants voice for researchers to work with. An example of an extracted theme and its corresponding anecdote using an LLM can also be seen in Table 2, above."
        ]
    },
    "global_footnotes": []
}