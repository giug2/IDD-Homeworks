{
    "id_table_1": {
        "caption": "Table 1.  Evaluation (NDCG@10) of multi-stage text retrieval pipelines with different embedding and ranking models on text Q&A datasets from BEIR",
        "table": "S3.T1.1",
        "footnotes": [],
        "references": [
            "For that reason, multi-stage text retrieval pipelines have been proposed to increase indexing and serving throughput, as well as improving the retrieval accuracy. In those pipelines, a sparse and/or dense embedding model are first used to retrieve top-k candidate passages, followed by a ranking model that refines the final ranking of those passages, as illustrated in Figure  1 .",
            "The evaluation setup mimics the typical text retrieval indexing and querying pipelines, as previously illustrated in Figure  1 .",
            "In Table  1 , we compare pipelines with three commercially usable embedding models (Section  3.1 ) and their combination with a number of ranking models (Section  3.2 ). Retrieval accuracy is measured with NDCG@10 for Q&A BEIR datasets.",
            "Cross-encoder ranking models are binary classifiers that discriminate between positive and negative passages. They typically are trained with the binary cross-entropy loss as in Equation  1 , where  p =   ( q , d ) p italic- q d p=\\phi(q,d) italic_p = italic_ ( italic_q , italic_d )  is the model predicted likelihood of the passage  d d d italic_d  being relevant to query  q q q italic_q .",
            "For broader comparison, we evaluate the ranking models with the three different embedding models described in Section  3.1 .",
            "Cross-encoders are typically trained with the point-wise Binary Cross-Entropy (BCE) loss (Equation  1 ), as we discussed in Section  4 .",
            "Thus, by using a two-stage pipeline with  NV-EmbedQA-E5-v5  embedding and  NV-RerankQA-Mistral-4B-v3  ranking models instead of a single-stage pipeline with  NV-EmbedQA-Mistral7B-v2 , in addition to achieving higher retrieval accuracy (see Table  1 ), we will also reduce the indexing time by 8.2x."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.  Comparing multi-stage text retrieval pipelines with different-sized ranking models, fine-tuned with the same train set and compute budget. Metric is NDCG@10.",
        "table": "S5.T2.1",
        "footnotes": [],
        "references": [
            "In Table  1 , we compare pipelines with three commercially usable embedding models (Section  3.1 ) and their combination with a number of ranking models (Section  3.2 ). Retrieval accuracy is measured with NDCG@10 for Q&A BEIR datasets.",
            "Instead, for  NV-RerankQA-Mistral-4B-v3  we follow  (Wang et al . ,  2022a )  and train the reranker with contrastive learning over the positive and its negative candidates scores using the list-wise InfoNCE loss  (Oord et al . ,  2018 ) , shown in Equation  2 , where  d + superscript d d^{+} italic_d start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT  is a positive relevant passage,  d  superscript d d^{-} italic_d start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT  is one of the  N N N italic_N  negative passages and    \\tau italic_  is the temperature parameter.",
            "In Table  2 , we present the comparison of fine-tuning those different ranking models, with the same train set and compute budget. Although the pre-training of those base models is different, it is possible to observe a pattern where larger ranking models provide higher retrieval accuracy.",
            "On the other hand, we fine-tune  Mistral 4B  with the list-wise InfoNCE loss (Oord et al . ,  2018 )  (Equation  2 ) and contrastive learning.",
            "We also have to consider the latency the ranking model adds to the query pipeline. In our example, after the query is embedded with  NV-EmbedQA-E5-v5  (in 5.1 ms), candidate relevant passages are retrieved from a vector database using ANN, and the top-40 passages will be provided to a ranking model for final ranking. The 40 candidate passages would be scored on their relevancy with respect to the query by the  NV-RerankQA-Mistral-4B-v3  model in total 266 ms on average, which might add a reasonable time to the overall query pipeline latency depending on the non-functional requirements for the system. That could be improved by distributing the ranking scoring requests of those 40 candidates among multiple GPUs / Triton Inference Server instances. Another option would be deploying a model with a good trade-off between size/latency and accuracy, like  deberta-v3-large 25 25 25 We are actually building a ranking model based on  deberta-v3-large  and plan to release it soon. , which is much smaller (435M) than  NV-RerankQA-Mistral-4B-v3  (4B) and might provide a good ranking accuracy as discussed in the ablation study on Table  2 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3.  Comparing NDCG@10 of Mistral 4B reranker fine-tuned with different attention mechanisms.",
        "table": "S5.T3.1",
        "footnotes": [],
        "references": [
            "In Table  1 , we compare pipelines with three commercially usable embedding models (Section  3.1 ) and their combination with a number of ranking models (Section  3.2 ). Retrieval accuracy is measured with NDCG@10 for Q&A BEIR datasets.",
            "We introduce in this paper the state-of-the-art  NV-RerankQA-Mistral-4B-v3 , that performs best in our benchmark on text retrieval for Q&A (Section  3.4 ).",
            "For broader comparison, we evaluate the ranking models with the three different embedding models described in Section  3.1 .",
            "We compare the accuracy with those two self-attention mechanisms in Table  3 , both using average pooling, and demonstrate the effectiveness of bi-directional attention for allowing deeper interaction among input query and passage tokens."
        ]
    },
    "id_table_4": {
        "caption": "Table 4.  Comparing NDCG@10 of Mistral 4B reranker with bi-directional attention fine-tuned with different losses.",
        "table": "S5.T4.1",
        "footnotes": [],
        "references": [
            "NV-RerankQA-Mistral-4B-v3   16 16 16 https://build.nvidia.com/explore/retrieval#nv-rerankqa-mistral-4b-v3  (4B params) - Large and powerful re-ranker that takes as base model a pruned version of Mistral 7B and is fine-tuned with a blend of supervised data with contrastive learning. It is fully described in Section  4 .",
            "We introduce in this paper the state-of-the-art  NV-RerankQA-Mistral-4B-v3 , that performs best in our benchmark on text retrieval for Q&A (Section  3.4 ).",
            "Model size is an important aspect for trading-off model accuracy and inference throughput. For this section we fine-tune and compare three base models with different sizes as ranking models:  MiniLM-L12-H384-uncased   (Wang et al . ,  2020b )  (33M params)  18 18 18 https://huggingface.co/microsoft/MiniLM-L12-H384-uncased ,  deberta-v3-large   (He et al . ,  2021 )   19 19 19 https://huggingface.co/microsoft/deberta-v3-large  (435M) and  Mistral 4B  (4B params), the latter pruned and modified from Mistral 7B v0.1  (Jiang et al . ,  2023 )  as described in Section  4 .",
            "In Section  4  we describe that for our adapted  Mistral 4B  we modified the standard self-attention mechanism of Mistral from uni-directional (causal) to bi-directional attention.",
            "Cross-encoders are typically trained with the point-wise Binary Cross-Entropy (BCE) loss (Equation  1 ), as we discussed in Section  4 .",
            "We experiment with those two losses, both using the same sets of hard-negative passages mined from the corpus, as described in Section  4 .",
            "In Table  4 , we can clearly observe the higher retrieval accuracy obtained when using InfoNCE, a list-wise contrastive learning loss trained to maximize the relevance score of the question and positive passage pair, while minimizing the score for question and negative passage pairs."
        ]
    },
    "global_footnotes": [
        "We also tried pruning different number of top layers from Mistral 7B model, the more layers we remove the lower the accuracy. We found out that keeping bottom 16 layers provides a good trade-off between accuracy penalty (-1%) and model size reduction (-50% # parameters) compared to the original 32-layer model.",
        "You can find Nemo Retriever embedding and ranking models optimized for fast serving with TensorRT and Triton Inference Server in",
        "You can find more information on Nemo Retriever performance benchmarks with other GPUs / batch sizes for embedding models in",
        "and for ranking models in"
    ]
}