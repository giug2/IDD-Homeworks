{
    "S5.T1": {
        "caption": "Table 1: Schematic of the neural network used with kernel 20. All other models follow the same architectural structure and change only for the dimension of the kernel. The final prediction is given by the consensus of the models with kernel from 20 through 30.",
        "table": "<table id=\"S5.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><em id=\"S5.T1.1.1.1.1.1\" class=\"ltx_emph ltx_font_italic\">Layer (type)</em></th>\n<th id=\"S5.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><em id=\"S5.T1.1.1.1.2.1\" class=\"ltx_emph ltx_font_italic\">Output Shape</em></th>\n<th id=\"S5.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><em id=\"S5.T1.1.1.1.3.1\" class=\"ltx_emph ltx_font_italic\">Param #</em></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">Layer Normalization</td>\n<td id=\"S5.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">(None, 160, 2, 1)</td>\n<td id=\"S5.T1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">320</td>\n</tr>\n<tr id=\"S5.T1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.3.2.1\" class=\"ltx_td ltx_align_center\">Conv2D</td>\n<td id=\"S5.T1.1.3.2.2\" class=\"ltx_td ltx_align_center\">(None, 141, 2, 32)</td>\n<td id=\"S5.T1.1.3.2.3\" class=\"ltx_td ltx_align_center\">672</td>\n</tr>\n<tr id=\"S5.T1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.4.3.1\" class=\"ltx_td ltx_align_center\">MaxPooling2D</td>\n<td id=\"S5.T1.1.4.3.2\" class=\"ltx_td ltx_align_center\">(None, 70, 2, 32)</td>\n<td id=\"S5.T1.1.4.3.3\" class=\"ltx_td ltx_align_center\">0</td>\n</tr>\n<tr id=\"S5.T1.1.5.4\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.5.4.1\" class=\"ltx_td ltx_align_center\">Conv2D</td>\n<td id=\"S5.T1.1.5.4.2\" class=\"ltx_td ltx_align_center\">(None, 51, 2, 64)</td>\n<td id=\"S5.T1.1.5.4.3\" class=\"ltx_td ltx_align_center\">41024</td>\n</tr>\n<tr id=\"S5.T1.1.6.5\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.6.5.1\" class=\"ltx_td ltx_align_center\">MaxPooling2D</td>\n<td id=\"S5.T1.1.6.5.2\" class=\"ltx_td ltx_align_center\">(None, 25, 1, 64)</td>\n<td id=\"S5.T1.1.6.5.3\" class=\"ltx_td ltx_align_center\">0</td>\n</tr>\n<tr id=\"S5.T1.1.7.6\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.7.6.1\" class=\"ltx_td ltx_align_center\">Flatten</td>\n<td id=\"S5.T1.1.7.6.2\" class=\"ltx_td ltx_align_center\">(None, 1600)</td>\n<td id=\"S5.T1.1.7.6.3\" class=\"ltx_td ltx_align_center\">0</td>\n</tr>\n<tr id=\"S5.T1.1.8.7\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.8.7.1\" class=\"ltx_td ltx_align_center\">Dropout</td>\n<td id=\"S5.T1.1.8.7.2\" class=\"ltx_td ltx_align_center\">(None, 1600)</td>\n<td id=\"S5.T1.1.8.7.3\" class=\"ltx_td ltx_align_center\">0</td>\n</tr>\n<tr id=\"S5.T1.1.9.8\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.9.8.1\" class=\"ltx_td ltx_align_center\">Dense</td>\n<td id=\"S5.T1.1.9.8.2\" class=\"ltx_td ltx_align_center\">(None, 2)</td>\n<td id=\"S5.T1.1.9.8.3\" class=\"ltx_td ltx_align_center\">3202</td>\n</tr>\n<tr id=\"S5.T1.1.10.9\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.10.9.1\" class=\"ltx_td ltx_align_center ltx_border_t\"><em id=\"S5.T1.1.10.9.1.1\" class=\"ltx_emph ltx_font_italic\">Total Params</em></td>\n<td id=\"S5.T1.1.10.9.2\" class=\"ltx_td ltx_align_center ltx_border_t\">45218</td>\n<td id=\"S5.T1.1.10.9.3\" class=\"ltx_td ltx_border_t\"></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Considering these advantages, the authors opted for a CNN architecture for the somewhat atypical application of numerical pattern recognition, specifically for analyzing the electrical signals from neuronal cells. This architecture comprises a series of layers typical in image recognition with deep learning using CNNs. The implementation was carried out using the Keras library within TensorFlow 2.15. The network includes a normalization layer to stabilize learning and expedite training, two sets of a 2D convolutional layer with 32 filters each, followed by a max pooling layer with a pool size of (2x1). It also features a flatten layer that connects to a dropout layer and subsequently to dense layers, which have two output units for the binary classification task. The activation function for the convolutional layers is ReLU, while the dense layers utilize the sigmoid function, detailed in Table 1. For training, the authors employed the \u2019binary crossentropy\u2019 loss function, a standard in binary classification tasks, and \u2019Adam\u2019 (Adaptive Moment Estimation) as the optimizer, given its widespread use and effectiveness."
        ]
    }
}