{
    "PAPER'S NUMBER OF TABLES": 10,
    "S6.T1": {
        "caption": "Table 1: Testing accuracy of different FL schemes on CIFAR-10 dataset. The mini-batch SGD with momentum and weight decay is adopted.",
        "table": "<table id=\"S6.T1.12\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S6.T1.12.13\" class=\"ltx_tr\">\n<td id=\"S6.T1.12.13.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S6.T1.12.13.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">FL scheme</span></td>\n<td id=\"S6.T1.12.13.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S6.T1.12.13.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">i.i.d. (%)</span></td>\n<td id=\"S6.T1.12.13.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S6.T1.12.13.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Non-i.i.d. (%)</span></td>\n</tr>\n<tr id=\"S6.T1.2.2\" class=\"ltx_tr\">\n<td id=\"S6.T1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S6.T1.2.2.3.1\" class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Centralized+BN</span></td>\n<td id=\"S6.T1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S6.T1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">91.53 </span><math id=\"S6.T1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S6.T1.1.1.1.m1.1a\"><mo mathsize=\"90%\" id=\"S6.T1.1.1.1.m1.1.1\" xref=\"S6.T1.1.1.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T1.1.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S6.T1.1.1.1.m1.1.1.cmml\" xref=\"S6.T1.1.1.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T1.1.1.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"S6.T1.1.1.1.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 0.50</span>\n</td>\n<td id=\"S6.T1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S6.T1.2.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">91.53 </span><math id=\"S6.T1.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S6.T1.2.2.2.m1.1a\"><mo mathsize=\"90%\" id=\"S6.T1.2.2.2.m1.1.1\" xref=\"S6.T1.2.2.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T1.2.2.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S6.T1.2.2.2.m1.1.1.cmml\" xref=\"S6.T1.2.2.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T1.2.2.2.m1.1c\">\\pm</annotation></semantics></math><span id=\"S6.T1.2.2.2.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 0.50</span>\n</td>\n</tr>\n<tr id=\"S6.T1.4.4\" class=\"ltx_tr\">\n<td id=\"S6.T1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S6.T1.4.4.3.1\" class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">FedTAN</span></td>\n<td id=\"S6.T1.3.3.1\" class=\"ltx_td ltx_align_center\">\n<span id=\"S6.T1.3.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">91.26 </span><math id=\"S6.T1.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S6.T1.3.3.1.m1.1a\"><mo mathsize=\"90%\" id=\"S6.T1.3.3.1.m1.1.1\" xref=\"S6.T1.3.3.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T1.3.3.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S6.T1.3.3.1.m1.1.1.cmml\" xref=\"S6.T1.3.3.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T1.3.3.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"S6.T1.3.3.1.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 0.21</span>\n</td>\n<td id=\"S6.T1.4.4.2\" class=\"ltx_td ltx_align_center\">\n<span id=\"S6.T1.4.4.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">87.66</span><span id=\"S6.T1.4.4.2.2\" class=\"ltx_text\" style=\"font-size:90%;\"> </span><math id=\"S6.T1.4.4.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S6.T1.4.4.2.m1.1a\"><mo mathsize=\"90%\" id=\"S6.T1.4.4.2.m1.1.1\" xref=\"S6.T1.4.4.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T1.4.4.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S6.T1.4.4.2.m1.1.1.cmml\" xref=\"S6.T1.4.4.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T1.4.4.2.m1.1c\">\\pm</annotation></semantics></math><span id=\"S6.T1.4.4.2.3\" class=\"ltx_text\" style=\"font-size:90%;\"> 0.43</span>\n</td>\n</tr>\n<tr id=\"S6.T1.6.6\" class=\"ltx_tr\">\n<td id=\"S6.T1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S6.T1.6.6.3.1\" class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">FedAvg+BN</span></td>\n<td id=\"S6.T1.5.5.1\" class=\"ltx_td ltx_align_center\">\n<span id=\"S6.T1.5.5.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">91.35 </span><math id=\"S6.T1.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S6.T1.5.5.1.m1.1a\"><mo mathsize=\"90%\" id=\"S6.T1.5.5.1.m1.1.1\" xref=\"S6.T1.5.5.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T1.5.5.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S6.T1.5.5.1.m1.1.1.cmml\" xref=\"S6.T1.5.5.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T1.5.5.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"S6.T1.5.5.1.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 0.24</span>\n</td>\n<td id=\"S6.T1.6.6.2\" class=\"ltx_td ltx_align_center\">\n<span id=\"S6.T1.6.6.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">45.90 </span><math id=\"S6.T1.6.6.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S6.T1.6.6.2.m1.1a\"><mo mathsize=\"90%\" id=\"S6.T1.6.6.2.m1.1.1\" xref=\"S6.T1.6.6.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T1.6.6.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S6.T1.6.6.2.m1.1.1.cmml\" xref=\"S6.T1.6.6.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T1.6.6.2.m1.1c\">\\pm</annotation></semantics></math><span id=\"S6.T1.6.6.2.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 6.39</span>\n</td>\n</tr>\n<tr id=\"S6.T1.8.8\" class=\"ltx_tr\">\n<td id=\"S6.T1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S6.T1.8.8.3.1\" class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">FedAvg+Algorithm <a href=\"#alg2\" title=\"Algorithm 2 ‣ 5.2 Layer-wise Aggregation in Forward Propagation ‣ 5 Proposed FedTAN ‣ Why Batch Normalization Damage Federated Learning on Non-IID Data?\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a></span></td>\n<td id=\"S6.T1.7.7.1\" class=\"ltx_td ltx_align_center\">\n<span id=\"S6.T1.7.7.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">91.42 </span><math id=\"S6.T1.7.7.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S6.T1.7.7.1.m1.1a\"><mo mathsize=\"90%\" id=\"S6.T1.7.7.1.m1.1.1\" xref=\"S6.T1.7.7.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T1.7.7.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S6.T1.7.7.1.m1.1.1.cmml\" xref=\"S6.T1.7.7.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T1.7.7.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"S6.T1.7.7.1.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 0.15</span>\n</td>\n<td id=\"S6.T1.8.8.2\" class=\"ltx_td ltx_align_center\">\n<span id=\"S6.T1.8.8.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">76.01 </span><math id=\"S6.T1.8.8.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S6.T1.8.8.2.m1.1a\"><mo mathsize=\"90%\" id=\"S6.T1.8.8.2.m1.1.1\" xref=\"S6.T1.8.8.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T1.8.8.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S6.T1.8.8.2.m1.1.1.cmml\" xref=\"S6.T1.8.8.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T1.8.8.2.m1.1c\">\\pm</annotation></semantics></math><span id=\"S6.T1.8.8.2.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 1.37</span>\n</td>\n</tr>\n<tr id=\"S6.T1.10.10\" class=\"ltx_tr\">\n<td id=\"S6.T1.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S6.T1.10.10.3.1\" class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">FedBN</span></td>\n<td id=\"S6.T1.9.9.1\" class=\"ltx_td ltx_align_center\">\n<span id=\"S6.T1.9.9.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">90.74 </span><math id=\"S6.T1.9.9.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S6.T1.9.9.1.m1.1a\"><mo mathsize=\"90%\" id=\"S6.T1.9.9.1.m1.1.1\" xref=\"S6.T1.9.9.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T1.9.9.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S6.T1.9.9.1.m1.1.1.cmml\" xref=\"S6.T1.9.9.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T1.9.9.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"S6.T1.9.9.1.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 0.28</span>\n</td>\n<td id=\"S6.T1.10.10.2\" class=\"ltx_td ltx_align_center\">\n<span id=\"S6.T1.10.10.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">19.46 </span><math id=\"S6.T1.10.10.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S6.T1.10.10.2.m1.1a\"><mo mathsize=\"90%\" id=\"S6.T1.10.10.2.m1.1.1\" xref=\"S6.T1.10.10.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T1.10.10.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S6.T1.10.10.2.m1.1.1.cmml\" xref=\"S6.T1.10.10.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T1.10.10.2.m1.1c\">\\pm</annotation></semantics></math><span id=\"S6.T1.10.10.2.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 0.05</span>\n</td>\n</tr>\n<tr id=\"S6.T1.12.12\" class=\"ltx_tr\">\n<td id=\"S6.T1.12.12.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S6.T1.12.12.3.1\" class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">SiloBN</span></td>\n<td id=\"S6.T1.11.11.1\" class=\"ltx_td ltx_align_center ltx_border_b\">\n<span id=\"S6.T1.11.11.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">91.31 </span><math id=\"S6.T1.11.11.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S6.T1.11.11.1.m1.1a\"><mo mathsize=\"90%\" id=\"S6.T1.11.11.1.m1.1.1\" xref=\"S6.T1.11.11.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T1.11.11.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S6.T1.11.11.1.m1.1.1.cmml\" xref=\"S6.T1.11.11.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T1.11.11.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"S6.T1.11.11.1.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 0.29</span>\n</td>\n<td id=\"S6.T1.12.12.2\" class=\"ltx_td ltx_align_center ltx_border_b\">\n<span id=\"S6.T1.12.12.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">19.43 </span><math id=\"S6.T1.12.12.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S6.T1.12.12.2.m1.1a\"><mo mathsize=\"90%\" id=\"S6.T1.12.12.2.m1.1.1\" xref=\"S6.T1.12.12.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T1.12.12.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S6.T1.12.12.2.m1.1.1.cmml\" xref=\"S6.T1.12.12.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T1.12.12.2.m1.1c\">\\pm</annotation></semantics></math><span id=\"S6.T1.12.12.2.2\" class=\"ltx_text\" style=\"font-size:90%;\"> 0.09</span>\n</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We further demonstrate the effectiveness of proposed FedTAN by adopting mini-batch SGD with momentum and weight decay.\nTable 1 compares the testing accuracy of different FL schemes, based on the mean and standard deviation of five independent experiments.\nAs can be observed, FedTAN performs well for both data cases, while other FL benchmarks significantly degrade in the non-i.i.d. data case.\nIn particular, under the non-i.i.d. data, FedTAN increases its average testing accuracy by 41.76% compared to FedAvg+BN."
        ]
    },
    "S6.T2": {
        "caption": "Table 2: Testing accuracy of FedTAN and FedAvg+GN on CIFAR-10 dataset.",
        "table": "<table id=\"S6.T2.8\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S6.T2.8.9\" class=\"ltx_tr\">\n<td id=\"S6.T2.8.9.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S6.T2.8.9.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">FL scheme</span></td>\n<td id=\"S6.T2.8.9.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\"><span id=\"S6.T2.8.9.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Without momentum (%)</span></td>\n<td id=\"S6.T2.8.9.3\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\"><span id=\"S6.T2.8.9.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">With momentum (%)</span></td>\n</tr>\n<tr id=\"S6.T2.8.10\" class=\"ltx_tr\">\n<td id=\"S6.T2.8.10.1\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T2.8.10.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">i.i.d.</span></td>\n<td id=\"S6.T2.8.10.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S6.T2.8.10.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Non-i.i.d.</span></td>\n<td id=\"S6.T2.8.10.3\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T2.8.10.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">i.i.d.</span></td>\n<td id=\"S6.T2.8.10.4\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T2.8.10.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Non-i.i.d.</span></td>\n</tr>\n<tr id=\"S6.T2.4.4\" class=\"ltx_tr\">\n<td id=\"S6.T2.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S6.T2.4.4.5.1\" class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">FedTAN</span></td>\n<td id=\"S6.T2.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S6.T2.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">89.32<math id=\"S6.T2.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S6.T2.1.1.1.1.m1.1a\"><mo id=\"S6.T2.1.1.1.1.m1.1.1\" xref=\"S6.T2.1.1.1.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T2.1.1.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S6.T2.1.1.1.1.m1.1.1.cmml\" xref=\"S6.T2.1.1.1.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T2.1.1.1.1.m1.1c\">\\pm</annotation></semantics></math></span><span id=\"S6.T2.1.1.1.2\" class=\"ltx_text\" style=\"font-size:90%;\">0.28</span>\n</td>\n<td id=\"S6.T2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S6.T2.2.2.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">86.69<math id=\"S6.T2.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S6.T2.2.2.2.1.m1.1a\"><mo id=\"S6.T2.2.2.2.1.m1.1.1\" xref=\"S6.T2.2.2.2.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T2.2.2.2.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S6.T2.2.2.2.1.m1.1.1.cmml\" xref=\"S6.T2.2.2.2.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T2.2.2.2.1.m1.1c\">\\pm</annotation></semantics></math></span><span id=\"S6.T2.2.2.2.2\" class=\"ltx_text\" style=\"font-size:90%;\">0.82</span>\n</td>\n<td id=\"S6.T2.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S6.T2.3.3.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">91.26<math id=\"S6.T2.3.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S6.T2.3.3.3.1.m1.1a\"><mo id=\"S6.T2.3.3.3.1.m1.1.1\" xref=\"S6.T2.3.3.3.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T2.3.3.3.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S6.T2.3.3.3.1.m1.1.1.cmml\" xref=\"S6.T2.3.3.3.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T2.3.3.3.1.m1.1c\">\\pm</annotation></semantics></math></span><span id=\"S6.T2.3.3.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">0.21</span>\n</td>\n<td id=\"S6.T2.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S6.T2.4.4.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">87.66<math id=\"S6.T2.4.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S6.T2.4.4.4.1.m1.1a\"><mo id=\"S6.T2.4.4.4.1.m1.1.1\" xref=\"S6.T2.4.4.4.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T2.4.4.4.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S6.T2.4.4.4.1.m1.1.1.cmml\" xref=\"S6.T2.4.4.4.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T2.4.4.4.1.m1.1c\">\\pm</annotation></semantics></math></span><span id=\"S6.T2.4.4.4.2\" class=\"ltx_text\" style=\"font-size:90%;\">0.43</span>\n</td>\n</tr>\n<tr id=\"S6.T2.8.8\" class=\"ltx_tr\">\n<td id=\"S6.T2.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S6.T2.8.8.5.1\" class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">FedAvg+GN</span></td>\n<td id=\"S6.T2.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_b\">\n<span id=\"S6.T2.5.5.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">85.99</span><math id=\"S6.T2.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S6.T2.5.5.1.m1.1a\"><mo mathsize=\"90%\" id=\"S6.T2.5.5.1.m1.1.1\" xref=\"S6.T2.5.5.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T2.5.5.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S6.T2.5.5.1.m1.1.1.cmml\" xref=\"S6.T2.5.5.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T2.5.5.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"S6.T2.5.5.1.2\" class=\"ltx_text\" style=\"font-size:90%;\">0.71</span>\n</td>\n<td id=\"S6.T2.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">\n<span id=\"S6.T2.6.6.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">79.01</span><math id=\"S6.T2.6.6.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S6.T2.6.6.2.m1.1a\"><mo mathsize=\"90%\" id=\"S6.T2.6.6.2.m1.1.1\" xref=\"S6.T2.6.6.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T2.6.6.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S6.T2.6.6.2.m1.1.1.cmml\" xref=\"S6.T2.6.6.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T2.6.6.2.m1.1c\">\\pm</annotation></semantics></math><span id=\"S6.T2.6.6.2.2\" class=\"ltx_text\" style=\"font-size:90%;\">1.63</span>\n</td>\n<td id=\"S6.T2.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_b\">\n<span id=\"S6.T2.7.7.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">88.41</span><math id=\"S6.T2.7.7.3.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S6.T2.7.7.3.m1.1a\"><mo mathsize=\"90%\" id=\"S6.T2.7.7.3.m1.1.1\" xref=\"S6.T2.7.7.3.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T2.7.7.3.m1.1b\"><csymbol cd=\"latexml\" id=\"S6.T2.7.7.3.m1.1.1.cmml\" xref=\"S6.T2.7.7.3.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T2.7.7.3.m1.1c\">\\pm</annotation></semantics></math><span id=\"S6.T2.7.7.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">0.46</span>\n</td>\n<td id=\"S6.T2.8.8.4\" class=\"ltx_td ltx_align_center ltx_border_b\">\n<span id=\"S6.T2.8.8.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">82.66</span><math id=\"S6.T2.8.8.4.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S6.T2.8.8.4.m1.1a\"><mo mathsize=\"90%\" id=\"S6.T2.8.8.4.m1.1.1\" xref=\"S6.T2.8.8.4.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T2.8.8.4.m1.1b\"><csymbol cd=\"latexml\" id=\"S6.T2.8.8.4.m1.1.1.cmml\" xref=\"S6.T2.8.8.4.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T2.8.8.4.m1.1c\">\\pm</annotation></semantics></math><span id=\"S6.T2.8.8.4.2\" class=\"ltx_text\" style=\"font-size:90%;\">1.48</span>\n</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "In comparison to BN, GN [13] is less sensitive to data distribution and also widely used.\nDespite this, GN is still unable to match the performance of BN in many recognition tasks.\nFor instance, replacing BN with GN during ResNet training on the CIFAR dataset leads to performance degradation [43].\nBased on CIFAR-10 dataset, Fig. 9 compares the performance of the ResNet-20 with BN trained by FedTAN and the one with GN trained by FedAvg, in terms of iterations.\nAdditionally, Table 2 compares the testing accuracy of the global models obtained through these two schemes after the entire training process.\nIt can be seen from Fig. 9 and Table 2 that, for both i.i.d. and non-i.i.d data cases, FedTAN maintains the superiority of BN and achieves higher testing accuracy with faster convergence rates.\nFurthermore, employing mini-batch SGD with momentum enhances the learning performance of both FedTAN and FedAvg+GN compared to the case without momentum."
        ]
    },
    "S6.T3": {
        "caption": "Table 3: Communication overhead per iteration of FL schemes on CIFAR-10 dataset.",
        "table": "<table id=\"S6.T3.3\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S6.T3.3.1\" class=\"ltx_tr\">\n<td id=\"S6.T3.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S6.T3.3.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">FL scheme</span></td>\n<td id=\"S6.T3.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S6.T3.3.1.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Exchanged bits</span></td>\n<td id=\"S6.T3.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\"><span id=\"S6.T3.3.1.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Percentage higher than</span></td>\n</tr>\n<tr id=\"S6.T3.3.2\" class=\"ltx_tr\">\n<td id=\"S6.T3.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S6.T3.3.2.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">(MB)</span></td>\n<td id=\"S6.T3.3.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T3.3.2.2.1\" class=\"ltx_text ltx_font_typewriter ltx_font_bold\" style=\"font-size:90%;\">FedAvg+BN</span></td>\n<td id=\"S6.T3.3.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T3.3.2.3.1\" class=\"ltx_text ltx_font_typewriter ltx_font_bold\" style=\"font-size:90%;\">FedAvg+GN</span></td>\n</tr>\n<tr id=\"S6.T3.3.3\" class=\"ltx_tr\">\n<td id=\"S6.T3.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S6.T3.3.3.1.1\" class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">FedTAN</span></td>\n<td id=\"S6.T3.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S6.T3.3.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">6.2679</span></td>\n<td id=\"S6.T3.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S6.T3.3.3.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.02%</span></td>\n<td id=\"S6.T3.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S6.T3.3.3.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.52%</span></td>\n</tr>\n<tr id=\"S6.T3.3.4\" class=\"ltx_tr\">\n<td id=\"S6.T3.3.4.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S6.T3.3.4.1.1\" class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">FedAvg+BN</span></td>\n<td id=\"S6.T3.3.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S6.T3.3.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">6.2049</span></td>\n<td id=\"S6.T3.3.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T3.3.4.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"S6.T3.3.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T3.3.4.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.51%</span></td>\n</tr>\n<tr id=\"S6.T3.3.5\" class=\"ltx_tr\">\n<td id=\"S6.T3.3.5.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S6.T3.3.5.1.1\" class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">FedAvg+GN</span></td>\n<td id=\"S6.T3.3.5.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S6.T3.3.5.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">6.1734</span></td>\n<td id=\"S6.T3.3.5.3\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S6.T3.3.5.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">-0.51%</span></td>\n<td id=\"S6.T3.3.5.4\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S6.T3.3.5.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "In FedTAN, as depicted in Algorithms 1, 2 and 3, the modified first local updating step brings extra transmission of statistical parameters and their gradients between the server and the clients.\nTable 3 compares the number of bits exchanged between the server and five clients per iteration required by different FL schemes when training ResNet-20 with the CIFAR-10 dataset, where the detailed calculation process of each exchanged data volume is provided in Section G of the Supplementary Material.\nTable 3 shows that, compared to FedAvg+BN and FedAvg+GN, the transmitted data volume between the server and clients in FedTAN only increases by 1.02% and 1.52%, respectively.\nThis slight increase is due to statistical parameters comprising a small portion of all parameters, especially in large-scale DNNs.\nNext, Fig. 10 compares the testing accuracy of various FL schemes concerning the number of bits exchanged between the server and all clients.\nThe figure shows that with negligible additional exchanged bits, FedTAN is still capable of maintaining the faster convergence rate w.r.t. the transmission data volume.\nSpecifically, given an expected level of testing accuracy, FedAvg+GN requires significantly more bits to be exchanged between the server and clients due to its slower convergence rate compared to FedTAN.\nMoreover, while FedTAN achieves higher learning accuracy, the extra communication rounds caused by the layer-wise aggregations in Algorithms 2 and 3 will increase the total communication rounds required for FedTAN.\nThus, FedAvg+GN is recommended if the number of communication rounds is critical."
        ]
    },
    "S6.T4": {
        "caption": "Table 4: Communication overhead of FedTAN-II on CIFAR-10 dataset.",
        "table": "<table id=\"S6.T4.6\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S6.T4.6.7\" class=\"ltx_tr\">\n<td id=\"S6.T4.6.7.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S6.T4.6.7.1.1\" class=\"ltx_text ltx_font_bold\">FL scheme</span></td>\n<td id=\"S6.T4.6.7.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S6.T4.6.7.2.1\" class=\"ltx_text ltx_font_bold\">Exchanged</span></td>\n<td id=\"S6.T4.6.7.3\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\"><span id=\"S6.T4.6.7.3.1\" class=\"ltx_text ltx_font_bold\">Percentage of extra</span></td>\n</tr>\n<tr id=\"S6.T4.6.8\" class=\"ltx_tr\">\n<td id=\"S6.T4.6.8.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S6.T4.6.8.1.1\" class=\"ltx_text ltx_font_bold\">bits (GB)</span></td>\n<td id=\"S6.T4.6.8.2\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T4.6.8.2.1\" class=\"ltx_text ltx_font_bold\">Rounds</span></td>\n<td id=\"S6.T4.6.8.3\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T4.6.8.3.1\" class=\"ltx_text ltx_font_bold\">Bits</span></td>\n</tr>\n<tr id=\"S6.T4.2.2\" class=\"ltx_tr\">\n<td id=\"S6.T4.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S6.T4.2.2.2.1\" class=\"ltx_text ltx_font_typewriter\">FedTAN-II</span> (<math id=\"S6.T4.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"M\" display=\"inline\"><semantics id=\"S6.T4.1.1.1.m1.1a\"><mi id=\"S6.T4.1.1.1.m1.1.1\" xref=\"S6.T4.1.1.1.m1.1.1.cmml\">M</mi><annotation-xml encoding=\"MathML-Content\" id=\"S6.T4.1.1.1.m1.1b\"><ci id=\"S6.T4.1.1.1.m1.1.1.cmml\" xref=\"S6.T4.1.1.1.m1.1.1\">𝑀</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T4.1.1.1.m1.1c\">M</annotation></semantics></math><math id=\"S6.T4.2.2.2.m2.1\" class=\"ltx_Math\" alttext=\"=\" display=\"inline\"><semantics id=\"S6.T4.2.2.2.m2.1a\"><mo id=\"S6.T4.2.2.2.m2.1.1\" xref=\"S6.T4.2.2.2.m2.1.1.cmml\">=</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T4.2.2.2.m2.1b\"><eq id=\"S6.T4.2.2.2.m2.1.1.cmml\" xref=\"S6.T4.2.2.2.m2.1.1\"></eq></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T4.2.2.2.m2.1c\">=</annotation></semantics></math>1000)</td>\n<td id=\"S6.T4.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">60.6563</td>\n<td id=\"S6.T4.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">85.07%</td>\n<td id=\"S6.T4.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.1014%</td>\n</tr>\n<tr id=\"S6.T4.4.4\" class=\"ltx_tr\">\n<td id=\"S6.T4.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">\n<span id=\"S6.T4.4.4.2.1\" class=\"ltx_text ltx_font_typewriter\">FedTAN-II</span> (<math id=\"S6.T4.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"M\" display=\"inline\"><semantics id=\"S6.T4.3.3.1.m1.1a\"><mi id=\"S6.T4.3.3.1.m1.1.1\" xref=\"S6.T4.3.3.1.m1.1.1.cmml\">M</mi><annotation-xml encoding=\"MathML-Content\" id=\"S6.T4.3.3.1.m1.1b\"><ci id=\"S6.T4.3.3.1.m1.1.1.cmml\" xref=\"S6.T4.3.3.1.m1.1.1\">𝑀</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T4.3.3.1.m1.1c\">M</annotation></semantics></math><math id=\"S6.T4.4.4.2.m2.1\" class=\"ltx_Math\" alttext=\"=\" display=\"inline\"><semantics id=\"S6.T4.4.4.2.m2.1a\"><mo id=\"S6.T4.4.4.2.m2.1.1\" xref=\"S6.T4.4.4.2.m2.1.1.cmml\">=</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T4.4.4.2.m2.1b\"><eq id=\"S6.T4.4.4.2.m2.1.1.cmml\" xref=\"S6.T4.4.4.2.m2.1.1\"></eq></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T4.4.4.2.m2.1c\">=</annotation></semantics></math>2000)</td>\n<td id=\"S6.T4.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">60.7178</td>\n<td id=\"S6.T4.4.4.4\" class=\"ltx_td ltx_align_center\">91.94%</td>\n<td id=\"S6.T4.4.4.5\" class=\"ltx_td ltx_align_center\">0.2027%</td>\n</tr>\n<tr id=\"S6.T4.6.6\" class=\"ltx_tr\">\n<td id=\"S6.T4.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\">\n<span id=\"S6.T4.6.6.2.1\" class=\"ltx_text ltx_font_typewriter\">FedTAN-II</span> (<math id=\"S6.T4.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"M\" display=\"inline\"><semantics id=\"S6.T4.5.5.1.m1.1a\"><mi id=\"S6.T4.5.5.1.m1.1.1\" xref=\"S6.T4.5.5.1.m1.1.1.cmml\">M</mi><annotation-xml encoding=\"MathML-Content\" id=\"S6.T4.5.5.1.m1.1b\"><ci id=\"S6.T4.5.5.1.m1.1.1.cmml\" xref=\"S6.T4.5.5.1.m1.1.1\">𝑀</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T4.5.5.1.m1.1c\">M</annotation></semantics></math><math id=\"S6.T4.6.6.2.m2.1\" class=\"ltx_Math\" alttext=\"=\" display=\"inline\"><semantics id=\"S6.T4.6.6.2.m2.1a\"><mo id=\"S6.T4.6.6.2.m2.1.1\" xref=\"S6.T4.6.6.2.m2.1.1.cmml\">=</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T4.6.6.2.m2.1b\"><eq id=\"S6.T4.6.6.2.m2.1.1.cmml\" xref=\"S6.T4.6.6.2.m2.1.1\"></eq></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T4.6.6.2.m2.1c\">=</annotation></semantics></math>4000)</td>\n<td id=\"S6.T4.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\">60.8408</td>\n<td id=\"S6.T4.6.6.4\" class=\"ltx_td ltx_align_center\">95.80%</td>\n<td id=\"S6.T4.6.6.5\" class=\"ltx_td ltx_align_center\">0.4045%</td>\n</tr>\n<tr id=\"S6.T4.6.9\" class=\"ltx_tr\">\n<td id=\"S6.T4.6.9.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S6.T4.6.9.1.1\" class=\"ltx_text ltx_font_typewriter\">FedTAN</span></td>\n<td id=\"S6.T4.6.9.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">61.2100</td>\n<td id=\"S6.T4.6.9.3\" class=\"ltx_td ltx_align_center ltx_border_b\">98.28%</td>\n<td id=\"S6.T4.6.9.4\" class=\"ltx_td ltx_align_center ltx_border_b\">1.0051%</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Next, Table 4 compares the total exchanged bits, the percentages of extra communication rounds and exchanged bits caused by layer-aggregations in Algorithms 2 and 3 during the entire training process of FedTAN-II for varying values of M𝑀M.\nThe table shows that the difference in total exchanged bits between FedTAN and FedTAN-II under different M𝑀M is negligible, as the statistical parameters in BN layers occupy only a small portion of the total model size.\nMoreover, we find that although the extra communication rounds caused by layer-aggregations in Algorithms 2 and 3 account for a large percentage of the total exchanged rounds during the entire training process, the corresponding extra exchanged bits are negligible.\nThus, the layer-aggregations in Algorithms 2 and 3 constitute a high-frequent low-volume communication mode, which effectively mitigates divergence among local models, as highlighted in [7].",
            "Next, In Table 5, we contrast the aggregate exchanged bits, the proportions of supplementary communication rounds, and the exchanged bits resulting from layer-aggregations in Algorithms 2 and 3 throughout the complete training procedure of FedTAN-II across different M𝑀M values.\nAs observed in Table 4, Table 5 further illustrates the insubstantial variance in total exchanged bits between FedTAN and FedTAN-II across varying M𝑀M values, owing to the minor proportion occupied by statistical parameters in BN layers within the total model size.\nFurthermore, while the additional communication rounds resulting from layer-aggregations in Algorithms 2 and 3 contribute to a specific proportion of the total exchanged bits throughout the training process, particularly evident with larger values of M𝑀M, the accompanying extra exchanged bits remain negligible."
        ]
    },
    "A4.I1.i1.tab1": {
        "caption": "",
        "table": "<table id=\"A4.I1.i1.tab1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A4.I1.i1.tab1.1.1\" class=\"ltx_tr\">\n<td id=\"A4.I1.i1.tab1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"A4.I1.i1.tab1.1.1.1.1\" class=\"ltx_text\">Client</span></td>\n<td id=\"A4.I1.i1.tab1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"10\">Label</td>\n</tr>\n<tr id=\"A4.I1.i1.tab1.1.2\" class=\"ltx_tr\">\n<td id=\"A4.I1.i1.tab1.1.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\">1</td>\n<td id=\"A4.I1.i1.tab1.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">2</td>\n<td id=\"A4.I1.i1.tab1.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">3</td>\n<td id=\"A4.I1.i1.tab1.1.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">4</td>\n<td id=\"A4.I1.i1.tab1.1.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">5</td>\n<td id=\"A4.I1.i1.tab1.1.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\">6</td>\n<td id=\"A4.I1.i1.tab1.1.2.7\" class=\"ltx_td ltx_align_center ltx_border_t\">7</td>\n<td id=\"A4.I1.i1.tab1.1.2.8\" class=\"ltx_td ltx_align_center ltx_border_t\">8</td>\n<td id=\"A4.I1.i1.tab1.1.2.9\" class=\"ltx_td ltx_align_center ltx_border_t\">9</td>\n<td id=\"A4.I1.i1.tab1.1.2.10\" class=\"ltx_td ltx_align_center ltx_border_t\">10</td>\n</tr>\n<tr id=\"A4.I1.i1.tab1.1.3\" class=\"ltx_tr\">\n<td id=\"A4.I1.i1.tab1.1.3.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"A4.I1.i1.tab1.1.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">1/2</td>\n<td id=\"A4.I1.i1.tab1.1.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">1/2</td>\n<td id=\"A4.I1.i1.tab1.1.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0</td>\n<td id=\"A4.I1.i1.tab1.1.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0</td>\n<td id=\"A4.I1.i1.tab1.1.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\">0</td>\n<td id=\"A4.I1.i1.tab1.1.3.7\" class=\"ltx_td ltx_align_center ltx_border_t\">0</td>\n<td id=\"A4.I1.i1.tab1.1.3.8\" class=\"ltx_td ltx_align_center ltx_border_t\">0</td>\n<td id=\"A4.I1.i1.tab1.1.3.9\" class=\"ltx_td ltx_align_center ltx_border_t\">0</td>\n<td id=\"A4.I1.i1.tab1.1.3.10\" class=\"ltx_td ltx_align_center ltx_border_t\">0</td>\n<td id=\"A4.I1.i1.tab1.1.3.11\" class=\"ltx_td ltx_align_center ltx_border_t\">0</td>\n</tr>\n<tr id=\"A4.I1.i1.tab1.1.4\" class=\"ltx_tr\">\n<td id=\"A4.I1.i1.tab1.1.4.1\" class=\"ltx_td ltx_align_center ltx_border_r\">2</td>\n<td id=\"A4.I1.i1.tab1.1.4.2\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i1.tab1.1.4.3\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i1.tab1.1.4.4\" class=\"ltx_td ltx_align_center\">1/2</td>\n<td id=\"A4.I1.i1.tab1.1.4.5\" class=\"ltx_td ltx_align_center\">1/2</td>\n<td id=\"A4.I1.i1.tab1.1.4.6\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i1.tab1.1.4.7\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i1.tab1.1.4.8\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i1.tab1.1.4.9\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i1.tab1.1.4.10\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i1.tab1.1.4.11\" class=\"ltx_td ltx_align_center\">0</td>\n</tr>\n<tr id=\"A4.I1.i1.tab1.1.5\" class=\"ltx_tr\">\n<td id=\"A4.I1.i1.tab1.1.5.1\" class=\"ltx_td ltx_align_center ltx_border_r\">3</td>\n<td id=\"A4.I1.i1.tab1.1.5.2\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i1.tab1.1.5.3\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i1.tab1.1.5.4\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i1.tab1.1.5.5\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i1.tab1.1.5.6\" class=\"ltx_td ltx_align_center\">1/2</td>\n<td id=\"A4.I1.i1.tab1.1.5.7\" class=\"ltx_td ltx_align_center\">1/2</td>\n<td id=\"A4.I1.i1.tab1.1.5.8\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i1.tab1.1.5.9\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i1.tab1.1.5.10\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i1.tab1.1.5.11\" class=\"ltx_td ltx_align_center\">0</td>\n</tr>\n<tr id=\"A4.I1.i1.tab1.1.6\" class=\"ltx_tr\">\n<td id=\"A4.I1.i1.tab1.1.6.1\" class=\"ltx_td ltx_align_center ltx_border_r\">4</td>\n<td id=\"A4.I1.i1.tab1.1.6.2\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i1.tab1.1.6.3\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i1.tab1.1.6.4\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i1.tab1.1.6.5\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i1.tab1.1.6.6\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i1.tab1.1.6.7\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i1.tab1.1.6.8\" class=\"ltx_td ltx_align_center\">1/2</td>\n<td id=\"A4.I1.i1.tab1.1.6.9\" class=\"ltx_td ltx_align_center\">1/2</td>\n<td id=\"A4.I1.i1.tab1.1.6.10\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i1.tab1.1.6.11\" class=\"ltx_td ltx_align_center\">0</td>\n</tr>\n<tr id=\"A4.I1.i1.tab1.1.7\" class=\"ltx_tr\">\n<td id=\"A4.I1.i1.tab1.1.7.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">5</td>\n<td id=\"A4.I1.i1.tab1.1.7.2\" class=\"ltx_td ltx_align_center ltx_border_b\">0</td>\n<td id=\"A4.I1.i1.tab1.1.7.3\" class=\"ltx_td ltx_align_center ltx_border_b\">0</td>\n<td id=\"A4.I1.i1.tab1.1.7.4\" class=\"ltx_td ltx_align_center ltx_border_b\">0</td>\n<td id=\"A4.I1.i1.tab1.1.7.5\" class=\"ltx_td ltx_align_center ltx_border_b\">0</td>\n<td id=\"A4.I1.i1.tab1.1.7.6\" class=\"ltx_td ltx_align_center ltx_border_b\">0</td>\n<td id=\"A4.I1.i1.tab1.1.7.7\" class=\"ltx_td ltx_align_center ltx_border_b\">0</td>\n<td id=\"A4.I1.i1.tab1.1.7.8\" class=\"ltx_td ltx_align_center ltx_border_b\">0</td>\n<td id=\"A4.I1.i1.tab1.1.7.9\" class=\"ltx_td ltx_align_center ltx_border_b\">0</td>\n<td id=\"A4.I1.i1.tab1.1.7.10\" class=\"ltx_td ltx_align_center ltx_border_b\">1/2</td>\n<td id=\"A4.I1.i1.tab1.1.7.11\" class=\"ltx_td ltx_align_center ltx_border_b\">1/2</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "This two-step strategy yields notable benefits.\nBy executing FedTAN initially, we acquire more precise statistical parameters for BN layers.\nSubsequently, applying FedAvg in later iterations avoids the need for layer-wise aggregations, effectively reducing communication rounds.\nWe denote this communication-efficient scheme of FedTAN as FedTAN-II.",
            "We further demonstrate the effectiveness of proposed FedTAN by adopting mini-batch SGD with momentum and weight decay.\nTable 1 compares the testing accuracy of different FL schemes, based on the mean and standard deviation of five independent experiments.\nAs can be observed, FedTAN performs well for both data cases, while other FL benchmarks significantly degrade in the non-i.i.d. data case.\nIn particular, under the non-i.i.d. data, FedTAN increases its average testing accuracy by 41.76% compared to FedAvg+BN.",
            "In comparison to BN, GN [13] is less sensitive to data distribution and also widely used.\nDespite this, GN is still unable to match the performance of BN in many recognition tasks.\nFor instance, replacing BN with GN during ResNet training on the CIFAR dataset leads to performance degradation [43].\nBased on CIFAR-10 dataset, Fig. 9 compares the performance of the ResNet-20 with BN trained by FedTAN and the one with GN trained by FedAvg, in terms of iterations.\nAdditionally, Table 2 compares the testing accuracy of the global models obtained through these two schemes after the entire training process.\nIt can be seen from Fig. 9 and Table 2 that, for both i.i.d. and non-i.i.d data cases, FedTAN maintains the superiority of BN and achieves higher testing accuracy with faster convergence rates.\nFurthermore, employing mini-batch SGD with momentum enhances the learning performance of both FedTAN and FedAvg+GN compared to the case without momentum.",
            "In FedTAN, as depicted in Algorithms 1, 2 and 3, the modified first local updating step brings extra transmission of statistical parameters and their gradients between the server and the clients.\nTable 3 compares the number of bits exchanged between the server and five clients per iteration required by different FL schemes when training ResNet-20 with the CIFAR-10 dataset, where the detailed calculation process of each exchanged data volume is provided in Section G of the Supplementary Material.\nTable 3 shows that, compared to FedAvg+BN and FedAvg+GN, the transmitted data volume between the server and clients in FedTAN only increases by 1.02% and 1.52%, respectively.\nThis slight increase is due to statistical parameters comprising a small portion of all parameters, especially in large-scale DNNs.\nNext, Fig. 10 compares the testing accuracy of various FL schemes concerning the number of bits exchanged between the server and all clients.\nThe figure shows that with negligible additional exchanged bits, FedTAN is still capable of maintaining the faster convergence rate w.r.t. the transmission data volume.\nSpecifically, given an expected level of testing accuracy, FedAvg+GN requires significantly more bits to be exchanged between the server and clients due to its slower convergence rate compared to FedTAN.\nMoreover, while FedTAN achieves higher learning accuracy, the extra communication rounds caused by the layer-wise aggregations in Algorithms 2 and 3 will increase the total communication rounds required for FedTAN.\nThus, FedAvg+GN is recommended if the number of communication rounds is critical.",
            "Next, Table 4 compares the total exchanged bits, the percentages of extra communication rounds and exchanged bits caused by layer-aggregations in Algorithms 2 and 3 during the entire training process of FedTAN-II for varying values of M𝑀M.\nThe table shows that the difference in total exchanged bits between FedTAN and FedTAN-II under different M𝑀M is negligible, as the statistical parameters in BN layers occupy only a small portion of the total model size.\nMoreover, we find that although the extra communication rounds caused by layer-aggregations in Algorithms 2 and 3 account for a large percentage of the total exchanged rounds during the entire training process, the corresponding extra exchanged bits are negligible.\nThus, the layer-aggregations in Algorithms 2 and 3 constitute a high-frequent low-volume communication mode, which effectively mitigates divergence among local models, as highlighted in [7].",
            "Fig. 12 compares the performance of various FL schemes on CIFAR-10 dataset across different batch sizes.\nIt can be observed that all FL schemes exhibit relatively stable performance despite variations in batch size.\nNotably, the proposed FedTAN consistently performs comparably to centralized learning across all cases.\nHowever, both FedAvg+BN and FedBN exhibit poor performance under the non-i.i.d. data distribution, regardless of the batch size value.",
            "Next, In Table 5, we contrast the aggregate exchanged bits, the proportions of supplementary communication rounds, and the exchanged bits resulting from layer-aggregations in Algorithms 2 and 3 throughout the complete training procedure of FedTAN-II across different M𝑀M values.\nAs observed in Table 4, Table 5 further illustrates the insubstantial variance in total exchanged bits between FedTAN and FedTAN-II across varying M𝑀M values, owing to the minor proportion occupied by statistical parameters in BN layers within the total model size.\nFurthermore, while the additional communication rounds resulting from layer-aggregations in Algorithms 2 and 3 contribute to a specific proportion of the total exchanged bits throughout the training process, particularly evident with larger values of M𝑀M, the accompanying extra exchanged bits remain negligible."
        ]
    },
    "A4.I1.i2.tab1": {
        "caption": "",
        "table": "<table id=\"A4.I1.i2.tab1.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"A4.I1.i2.tab1.1.1\" class=\"ltx_tr\">\n<td id=\"A4.I1.i2.tab1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"A4.I1.i2.tab1.1.1.1.1\" class=\"ltx_text\">Client</span></td>\n<td id=\"A4.I1.i2.tab1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"10\">Label</td>\n</tr>\n<tr id=\"A4.I1.i2.tab1.1.2\" class=\"ltx_tr\">\n<td id=\"A4.I1.i2.tab1.1.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\">1</td>\n<td id=\"A4.I1.i2.tab1.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">2</td>\n<td id=\"A4.I1.i2.tab1.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">3</td>\n<td id=\"A4.I1.i2.tab1.1.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">4</td>\n<td id=\"A4.I1.i2.tab1.1.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">5</td>\n<td id=\"A4.I1.i2.tab1.1.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\">6</td>\n<td id=\"A4.I1.i2.tab1.1.2.7\" class=\"ltx_td ltx_align_center ltx_border_t\">7</td>\n<td id=\"A4.I1.i2.tab1.1.2.8\" class=\"ltx_td ltx_align_center ltx_border_t\">8</td>\n<td id=\"A4.I1.i2.tab1.1.2.9\" class=\"ltx_td ltx_align_center ltx_border_t\">9</td>\n<td id=\"A4.I1.i2.tab1.1.2.10\" class=\"ltx_td ltx_align_center ltx_border_t\">10</td>\n</tr>\n<tr id=\"A4.I1.i2.tab1.1.3\" class=\"ltx_tr\">\n<td id=\"A4.I1.i2.tab1.1.3.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"A4.I1.i2.tab1.1.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">1/4</td>\n<td id=\"A4.I1.i2.tab1.1.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">1/4</td>\n<td id=\"A4.I1.i2.tab1.1.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">1/4</td>\n<td id=\"A4.I1.i2.tab1.1.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\">1/4</td>\n<td id=\"A4.I1.i2.tab1.1.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\">0</td>\n<td id=\"A4.I1.i2.tab1.1.3.7\" class=\"ltx_td ltx_align_center ltx_border_t\">0</td>\n<td id=\"A4.I1.i2.tab1.1.3.8\" class=\"ltx_td ltx_align_center ltx_border_t\">0</td>\n<td id=\"A4.I1.i2.tab1.1.3.9\" class=\"ltx_td ltx_align_center ltx_border_t\">0</td>\n<td id=\"A4.I1.i2.tab1.1.3.10\" class=\"ltx_td ltx_align_center ltx_border_t\">0</td>\n<td id=\"A4.I1.i2.tab1.1.3.11\" class=\"ltx_td ltx_align_center ltx_border_t\">0</td>\n</tr>\n<tr id=\"A4.I1.i2.tab1.1.4\" class=\"ltx_tr\">\n<td id=\"A4.I1.i2.tab1.1.4.1\" class=\"ltx_td ltx_align_center ltx_border_r\">2</td>\n<td id=\"A4.I1.i2.tab1.1.4.2\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i2.tab1.1.4.3\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i2.tab1.1.4.4\" class=\"ltx_td ltx_align_center\">1/4</td>\n<td id=\"A4.I1.i2.tab1.1.4.5\" class=\"ltx_td ltx_align_center\">1/4</td>\n<td id=\"A4.I1.i2.tab1.1.4.6\" class=\"ltx_td ltx_align_center\">1/4</td>\n<td id=\"A4.I1.i2.tab1.1.4.7\" class=\"ltx_td ltx_align_center\">1/4</td>\n<td id=\"A4.I1.i2.tab1.1.4.8\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i2.tab1.1.4.9\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i2.tab1.1.4.10\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i2.tab1.1.4.11\" class=\"ltx_td ltx_align_center\">0</td>\n</tr>\n<tr id=\"A4.I1.i2.tab1.1.5\" class=\"ltx_tr\">\n<td id=\"A4.I1.i2.tab1.1.5.1\" class=\"ltx_td ltx_align_center ltx_border_r\">3</td>\n<td id=\"A4.I1.i2.tab1.1.5.2\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i2.tab1.1.5.3\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i2.tab1.1.5.4\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i2.tab1.1.5.5\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i2.tab1.1.5.6\" class=\"ltx_td ltx_align_center\">1/4</td>\n<td id=\"A4.I1.i2.tab1.1.5.7\" class=\"ltx_td ltx_align_center\">1/4</td>\n<td id=\"A4.I1.i2.tab1.1.5.8\" class=\"ltx_td ltx_align_center\">1/4</td>\n<td id=\"A4.I1.i2.tab1.1.5.9\" class=\"ltx_td ltx_align_center\">1/4</td>\n<td id=\"A4.I1.i2.tab1.1.5.10\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i2.tab1.1.5.11\" class=\"ltx_td ltx_align_center\">0</td>\n</tr>\n<tr id=\"A4.I1.i2.tab1.1.6\" class=\"ltx_tr\">\n<td id=\"A4.I1.i2.tab1.1.6.1\" class=\"ltx_td ltx_align_center ltx_border_r\">4</td>\n<td id=\"A4.I1.i2.tab1.1.6.2\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i2.tab1.1.6.3\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i2.tab1.1.6.4\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i2.tab1.1.6.5\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i2.tab1.1.6.6\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i2.tab1.1.6.7\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i2.tab1.1.6.8\" class=\"ltx_td ltx_align_center\">1/4</td>\n<td id=\"A4.I1.i2.tab1.1.6.9\" class=\"ltx_td ltx_align_center\">1/4</td>\n<td id=\"A4.I1.i2.tab1.1.6.10\" class=\"ltx_td ltx_align_center\">1/4</td>\n<td id=\"A4.I1.i2.tab1.1.6.11\" class=\"ltx_td ltx_align_center\">1/4</td>\n</tr>\n<tr id=\"A4.I1.i2.tab1.1.7\" class=\"ltx_tr\">\n<td id=\"A4.I1.i2.tab1.1.7.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">5</td>\n<td id=\"A4.I1.i2.tab1.1.7.2\" class=\"ltx_td ltx_align_center ltx_border_b\">1/4</td>\n<td id=\"A4.I1.i2.tab1.1.7.3\" class=\"ltx_td ltx_align_center ltx_border_b\">1/4</td>\n<td id=\"A4.I1.i2.tab1.1.7.4\" class=\"ltx_td ltx_align_center ltx_border_b\">0</td>\n<td id=\"A4.I1.i2.tab1.1.7.5\" class=\"ltx_td ltx_align_center ltx_border_b\">0</td>\n<td id=\"A4.I1.i2.tab1.1.7.6\" class=\"ltx_td ltx_align_center ltx_border_b\">0</td>\n<td id=\"A4.I1.i2.tab1.1.7.7\" class=\"ltx_td ltx_align_center ltx_border_b\">0</td>\n<td id=\"A4.I1.i2.tab1.1.7.8\" class=\"ltx_td ltx_align_center ltx_border_b\">0</td>\n<td id=\"A4.I1.i2.tab1.1.7.9\" class=\"ltx_td ltx_align_center ltx_border_b\">0</td>\n<td id=\"A4.I1.i2.tab1.1.7.10\" class=\"ltx_td ltx_align_center ltx_border_b\">1/4</td>\n<td id=\"A4.I1.i2.tab1.1.7.11\" class=\"ltx_td ltx_align_center ltx_border_b\">1/4</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "This two-step strategy yields notable benefits.\nBy executing FedTAN initially, we acquire more precise statistical parameters for BN layers.\nSubsequently, applying FedAvg in later iterations avoids the need for layer-wise aggregations, effectively reducing communication rounds.\nWe denote this communication-efficient scheme of FedTAN as FedTAN-II.",
            "We further demonstrate the effectiveness of proposed FedTAN by adopting mini-batch SGD with momentum and weight decay.\nTable 1 compares the testing accuracy of different FL schemes, based on the mean and standard deviation of five independent experiments.\nAs can be observed, FedTAN performs well for both data cases, while other FL benchmarks significantly degrade in the non-i.i.d. data case.\nIn particular, under the non-i.i.d. data, FedTAN increases its average testing accuracy by 41.76% compared to FedAvg+BN.",
            "In comparison to BN, GN [13] is less sensitive to data distribution and also widely used.\nDespite this, GN is still unable to match the performance of BN in many recognition tasks.\nFor instance, replacing BN with GN during ResNet training on the CIFAR dataset leads to performance degradation [43].\nBased on CIFAR-10 dataset, Fig. 9 compares the performance of the ResNet-20 with BN trained by FedTAN and the one with GN trained by FedAvg, in terms of iterations.\nAdditionally, Table 2 compares the testing accuracy of the global models obtained through these two schemes after the entire training process.\nIt can be seen from Fig. 9 and Table 2 that, for both i.i.d. and non-i.i.d data cases, FedTAN maintains the superiority of BN and achieves higher testing accuracy with faster convergence rates.\nFurthermore, employing mini-batch SGD with momentum enhances the learning performance of both FedTAN and FedAvg+GN compared to the case without momentum.",
            "In FedTAN, as depicted in Algorithms 1, 2 and 3, the modified first local updating step brings extra transmission of statistical parameters and their gradients between the server and the clients.\nTable 3 compares the number of bits exchanged between the server and five clients per iteration required by different FL schemes when training ResNet-20 with the CIFAR-10 dataset, where the detailed calculation process of each exchanged data volume is provided in Section G of the Supplementary Material.\nTable 3 shows that, compared to FedAvg+BN and FedAvg+GN, the transmitted data volume between the server and clients in FedTAN only increases by 1.02% and 1.52%, respectively.\nThis slight increase is due to statistical parameters comprising a small portion of all parameters, especially in large-scale DNNs.\nNext, Fig. 10 compares the testing accuracy of various FL schemes concerning the number of bits exchanged between the server and all clients.\nThe figure shows that with negligible additional exchanged bits, FedTAN is still capable of maintaining the faster convergence rate w.r.t. the transmission data volume.\nSpecifically, given an expected level of testing accuracy, FedAvg+GN requires significantly more bits to be exchanged between the server and clients due to its slower convergence rate compared to FedTAN.\nMoreover, while FedTAN achieves higher learning accuracy, the extra communication rounds caused by the layer-wise aggregations in Algorithms 2 and 3 will increase the total communication rounds required for FedTAN.\nThus, FedAvg+GN is recommended if the number of communication rounds is critical.",
            "Next, Table 4 compares the total exchanged bits, the percentages of extra communication rounds and exchanged bits caused by layer-aggregations in Algorithms 2 and 3 during the entire training process of FedTAN-II for varying values of M𝑀M.\nThe table shows that the difference in total exchanged bits between FedTAN and FedTAN-II under different M𝑀M is negligible, as the statistical parameters in BN layers occupy only a small portion of the total model size.\nMoreover, we find that although the extra communication rounds caused by layer-aggregations in Algorithms 2 and 3 account for a large percentage of the total exchanged rounds during the entire training process, the corresponding extra exchanged bits are negligible.\nThus, the layer-aggregations in Algorithms 2 and 3 constitute a high-frequent low-volume communication mode, which effectively mitigates divergence among local models, as highlighted in [7].",
            "Fig. 12 compares the performance of various FL schemes on CIFAR-10 dataset across different batch sizes.\nIt can be observed that all FL schemes exhibit relatively stable performance despite variations in batch size.\nNotably, the proposed FedTAN consistently performs comparably to centralized learning across all cases.\nHowever, both FedAvg+BN and FedBN exhibit poor performance under the non-i.i.d. data distribution, regardless of the batch size value.",
            "Next, In Table 5, we contrast the aggregate exchanged bits, the proportions of supplementary communication rounds, and the exchanged bits resulting from layer-aggregations in Algorithms 2 and 3 throughout the complete training procedure of FedTAN-II across different M𝑀M values.\nAs observed in Table 4, Table 5 further illustrates the insubstantial variance in total exchanged bits between FedTAN and FedTAN-II across varying M𝑀M values, owing to the minor proportion occupied by statistical parameters in BN layers within the total model size.\nFurthermore, while the additional communication rounds resulting from layer-aggregations in Algorithms 2 and 3 contribute to a specific proportion of the total exchanged bits throughout the training process, particularly evident with larger values of M𝑀M, the accompanying extra exchanged bits remain negligible."
        ]
    },
    "A4.I1.i3.tab1": {
        "caption": "",
        "table": "<table id=\"A4.I1.i3.tab1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A4.I1.i3.tab1.1.1\" class=\"ltx_tr\">\n<td id=\"A4.I1.i3.tab1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"A4.I1.i3.tab1.1.1.1.1\" class=\"ltx_text\">Client</span></td>\n<td id=\"A4.I1.i3.tab1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"10\">Label</td>\n</tr>\n<tr id=\"A4.I1.i3.tab1.1.2\" class=\"ltx_tr\">\n<td id=\"A4.I1.i3.tab1.1.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\">1</td>\n<td id=\"A4.I1.i3.tab1.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">2</td>\n<td id=\"A4.I1.i3.tab1.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">3</td>\n<td id=\"A4.I1.i3.tab1.1.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">4</td>\n<td id=\"A4.I1.i3.tab1.1.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">5</td>\n<td id=\"A4.I1.i3.tab1.1.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\">6</td>\n<td id=\"A4.I1.i3.tab1.1.2.7\" class=\"ltx_td ltx_align_center ltx_border_t\">7</td>\n<td id=\"A4.I1.i3.tab1.1.2.8\" class=\"ltx_td ltx_align_center ltx_border_t\">8</td>\n<td id=\"A4.I1.i3.tab1.1.2.9\" class=\"ltx_td ltx_align_center ltx_border_t\">9</td>\n<td id=\"A4.I1.i3.tab1.1.2.10\" class=\"ltx_td ltx_align_center ltx_border_t\">10</td>\n</tr>\n<tr id=\"A4.I1.i3.tab1.1.3\" class=\"ltx_tr\">\n<td id=\"A4.I1.i3.tab1.1.3.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"A4.I1.i3.tab1.1.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">1/6</td>\n<td id=\"A4.I1.i3.tab1.1.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">1/6</td>\n<td id=\"A4.I1.i3.tab1.1.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">1/6</td>\n<td id=\"A4.I1.i3.tab1.1.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\">1/6</td>\n<td id=\"A4.I1.i3.tab1.1.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\">1/6</td>\n<td id=\"A4.I1.i3.tab1.1.3.7\" class=\"ltx_td ltx_align_center ltx_border_t\">1/6</td>\n<td id=\"A4.I1.i3.tab1.1.3.8\" class=\"ltx_td ltx_align_center ltx_border_t\">0</td>\n<td id=\"A4.I1.i3.tab1.1.3.9\" class=\"ltx_td ltx_align_center ltx_border_t\">0</td>\n<td id=\"A4.I1.i3.tab1.1.3.10\" class=\"ltx_td ltx_align_center ltx_border_t\">0</td>\n<td id=\"A4.I1.i3.tab1.1.3.11\" class=\"ltx_td ltx_align_center ltx_border_t\">0</td>\n</tr>\n<tr id=\"A4.I1.i3.tab1.1.4\" class=\"ltx_tr\">\n<td id=\"A4.I1.i3.tab1.1.4.1\" class=\"ltx_td ltx_align_center ltx_border_r\">2</td>\n<td id=\"A4.I1.i3.tab1.1.4.2\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i3.tab1.1.4.3\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i3.tab1.1.4.4\" class=\"ltx_td ltx_align_center\">1/6</td>\n<td id=\"A4.I1.i3.tab1.1.4.5\" class=\"ltx_td ltx_align_center\">1/6</td>\n<td id=\"A4.I1.i3.tab1.1.4.6\" class=\"ltx_td ltx_align_center\">1/6</td>\n<td id=\"A4.I1.i3.tab1.1.4.7\" class=\"ltx_td ltx_align_center\">1/6</td>\n<td id=\"A4.I1.i3.tab1.1.4.8\" class=\"ltx_td ltx_align_center\">1/6</td>\n<td id=\"A4.I1.i3.tab1.1.4.9\" class=\"ltx_td ltx_align_center\">1/6</td>\n<td id=\"A4.I1.i3.tab1.1.4.10\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i3.tab1.1.4.11\" class=\"ltx_td ltx_align_center\">0</td>\n</tr>\n<tr id=\"A4.I1.i3.tab1.1.5\" class=\"ltx_tr\">\n<td id=\"A4.I1.i3.tab1.1.5.1\" class=\"ltx_td ltx_align_center ltx_border_r\">3</td>\n<td id=\"A4.I1.i3.tab1.1.5.2\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i3.tab1.1.5.3\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i3.tab1.1.5.4\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i3.tab1.1.5.5\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i3.tab1.1.5.6\" class=\"ltx_td ltx_align_center\">1/6</td>\n<td id=\"A4.I1.i3.tab1.1.5.7\" class=\"ltx_td ltx_align_center\">1/6</td>\n<td id=\"A4.I1.i3.tab1.1.5.8\" class=\"ltx_td ltx_align_center\">1/6</td>\n<td id=\"A4.I1.i3.tab1.1.5.9\" class=\"ltx_td ltx_align_center\">1/6</td>\n<td id=\"A4.I1.i3.tab1.1.5.10\" class=\"ltx_td ltx_align_center\">1/6</td>\n<td id=\"A4.I1.i3.tab1.1.5.11\" class=\"ltx_td ltx_align_center\">1/6</td>\n</tr>\n<tr id=\"A4.I1.i3.tab1.1.6\" class=\"ltx_tr\">\n<td id=\"A4.I1.i3.tab1.1.6.1\" class=\"ltx_td ltx_align_center ltx_border_r\">4</td>\n<td id=\"A4.I1.i3.tab1.1.6.2\" class=\"ltx_td ltx_align_center\">1/6</td>\n<td id=\"A4.I1.i3.tab1.1.6.3\" class=\"ltx_td ltx_align_center\">1/6</td>\n<td id=\"A4.I1.i3.tab1.1.6.4\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i3.tab1.1.6.5\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i3.tab1.1.6.6\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i3.tab1.1.6.7\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i3.tab1.1.6.8\" class=\"ltx_td ltx_align_center\">1/6</td>\n<td id=\"A4.I1.i3.tab1.1.6.9\" class=\"ltx_td ltx_align_center\">1/6</td>\n<td id=\"A4.I1.i3.tab1.1.6.10\" class=\"ltx_td ltx_align_center\">1/6</td>\n<td id=\"A4.I1.i3.tab1.1.6.11\" class=\"ltx_td ltx_align_center\">1/6</td>\n</tr>\n<tr id=\"A4.I1.i3.tab1.1.7\" class=\"ltx_tr\">\n<td id=\"A4.I1.i3.tab1.1.7.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">5</td>\n<td id=\"A4.I1.i3.tab1.1.7.2\" class=\"ltx_td ltx_align_center ltx_border_b\">1/6</td>\n<td id=\"A4.I1.i3.tab1.1.7.3\" class=\"ltx_td ltx_align_center ltx_border_b\">1/6</td>\n<td id=\"A4.I1.i3.tab1.1.7.4\" class=\"ltx_td ltx_align_center ltx_border_b\">1/6</td>\n<td id=\"A4.I1.i3.tab1.1.7.5\" class=\"ltx_td ltx_align_center ltx_border_b\">1/6</td>\n<td id=\"A4.I1.i3.tab1.1.7.6\" class=\"ltx_td ltx_align_center ltx_border_b\">0</td>\n<td id=\"A4.I1.i3.tab1.1.7.7\" class=\"ltx_td ltx_align_center ltx_border_b\">0</td>\n<td id=\"A4.I1.i3.tab1.1.7.8\" class=\"ltx_td ltx_align_center ltx_border_b\">0</td>\n<td id=\"A4.I1.i3.tab1.1.7.9\" class=\"ltx_td ltx_align_center ltx_border_b\">0</td>\n<td id=\"A4.I1.i3.tab1.1.7.10\" class=\"ltx_td ltx_align_center ltx_border_b\">1/6</td>\n<td id=\"A4.I1.i3.tab1.1.7.11\" class=\"ltx_td ltx_align_center ltx_border_b\">1/6</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "This two-step strategy yields notable benefits.\nBy executing FedTAN initially, we acquire more precise statistical parameters for BN layers.\nSubsequently, applying FedAvg in later iterations avoids the need for layer-wise aggregations, effectively reducing communication rounds.\nWe denote this communication-efficient scheme of FedTAN as FedTAN-II.",
            "We further demonstrate the effectiveness of proposed FedTAN by adopting mini-batch SGD with momentum and weight decay.\nTable 1 compares the testing accuracy of different FL schemes, based on the mean and standard deviation of five independent experiments.\nAs can be observed, FedTAN performs well for both data cases, while other FL benchmarks significantly degrade in the non-i.i.d. data case.\nIn particular, under the non-i.i.d. data, FedTAN increases its average testing accuracy by 41.76% compared to FedAvg+BN.",
            "In comparison to BN, GN [13] is less sensitive to data distribution and also widely used.\nDespite this, GN is still unable to match the performance of BN in many recognition tasks.\nFor instance, replacing BN with GN during ResNet training on the CIFAR dataset leads to performance degradation [43].\nBased on CIFAR-10 dataset, Fig. 9 compares the performance of the ResNet-20 with BN trained by FedTAN and the one with GN trained by FedAvg, in terms of iterations.\nAdditionally, Table 2 compares the testing accuracy of the global models obtained through these two schemes after the entire training process.\nIt can be seen from Fig. 9 and Table 2 that, for both i.i.d. and non-i.i.d data cases, FedTAN maintains the superiority of BN and achieves higher testing accuracy with faster convergence rates.\nFurthermore, employing mini-batch SGD with momentum enhances the learning performance of both FedTAN and FedAvg+GN compared to the case without momentum.",
            "In FedTAN, as depicted in Algorithms 1, 2 and 3, the modified first local updating step brings extra transmission of statistical parameters and their gradients between the server and the clients.\nTable 3 compares the number of bits exchanged between the server and five clients per iteration required by different FL schemes when training ResNet-20 with the CIFAR-10 dataset, where the detailed calculation process of each exchanged data volume is provided in Section G of the Supplementary Material.\nTable 3 shows that, compared to FedAvg+BN and FedAvg+GN, the transmitted data volume between the server and clients in FedTAN only increases by 1.02% and 1.52%, respectively.\nThis slight increase is due to statistical parameters comprising a small portion of all parameters, especially in large-scale DNNs.\nNext, Fig. 10 compares the testing accuracy of various FL schemes concerning the number of bits exchanged between the server and all clients.\nThe figure shows that with negligible additional exchanged bits, FedTAN is still capable of maintaining the faster convergence rate w.r.t. the transmission data volume.\nSpecifically, given an expected level of testing accuracy, FedAvg+GN requires significantly more bits to be exchanged between the server and clients due to its slower convergence rate compared to FedTAN.\nMoreover, while FedTAN achieves higher learning accuracy, the extra communication rounds caused by the layer-wise aggregations in Algorithms 2 and 3 will increase the total communication rounds required for FedTAN.\nThus, FedAvg+GN is recommended if the number of communication rounds is critical.",
            "Next, Table 4 compares the total exchanged bits, the percentages of extra communication rounds and exchanged bits caused by layer-aggregations in Algorithms 2 and 3 during the entire training process of FedTAN-II for varying values of M𝑀M.\nThe table shows that the difference in total exchanged bits between FedTAN and FedTAN-II under different M𝑀M is negligible, as the statistical parameters in BN layers occupy only a small portion of the total model size.\nMoreover, we find that although the extra communication rounds caused by layer-aggregations in Algorithms 2 and 3 account for a large percentage of the total exchanged rounds during the entire training process, the corresponding extra exchanged bits are negligible.\nThus, the layer-aggregations in Algorithms 2 and 3 constitute a high-frequent low-volume communication mode, which effectively mitigates divergence among local models, as highlighted in [7].",
            "Fig. 12 compares the performance of various FL schemes on CIFAR-10 dataset across different batch sizes.\nIt can be observed that all FL schemes exhibit relatively stable performance despite variations in batch size.\nNotably, the proposed FedTAN consistently performs comparably to centralized learning across all cases.\nHowever, both FedAvg+BN and FedBN exhibit poor performance under the non-i.i.d. data distribution, regardless of the batch size value.",
            "Next, In Table 5, we contrast the aggregate exchanged bits, the proportions of supplementary communication rounds, and the exchanged bits resulting from layer-aggregations in Algorithms 2 and 3 throughout the complete training procedure of FedTAN-II across different M𝑀M values.\nAs observed in Table 4, Table 5 further illustrates the insubstantial variance in total exchanged bits between FedTAN and FedTAN-II across varying M𝑀M values, owing to the minor proportion occupied by statistical parameters in BN layers within the total model size.\nFurthermore, while the additional communication rounds resulting from layer-aggregations in Algorithms 2 and 3 contribute to a specific proportion of the total exchanged bits throughout the training process, particularly evident with larger values of M𝑀M, the accompanying extra exchanged bits remain negligible."
        ]
    },
    "A4.I1.i4.tab1": {
        "caption": "",
        "table": "<table id=\"A4.I1.i4.tab1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A4.I1.i4.tab1.1.1\" class=\"ltx_tr\">\n<td id=\"A4.I1.i4.tab1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"A4.I1.i4.tab1.1.1.1.1\" class=\"ltx_text\">Client</span></td>\n<td id=\"A4.I1.i4.tab1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"10\">Label</td>\n</tr>\n<tr id=\"A4.I1.i4.tab1.1.2\" class=\"ltx_tr\">\n<td id=\"A4.I1.i4.tab1.1.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\">1</td>\n<td id=\"A4.I1.i4.tab1.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">2</td>\n<td id=\"A4.I1.i4.tab1.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">3</td>\n<td id=\"A4.I1.i4.tab1.1.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">4</td>\n<td id=\"A4.I1.i4.tab1.1.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">5</td>\n<td id=\"A4.I1.i4.tab1.1.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\">6</td>\n<td id=\"A4.I1.i4.tab1.1.2.7\" class=\"ltx_td ltx_align_center ltx_border_t\">7</td>\n<td id=\"A4.I1.i4.tab1.1.2.8\" class=\"ltx_td ltx_align_center ltx_border_t\">8</td>\n<td id=\"A4.I1.i4.tab1.1.2.9\" class=\"ltx_td ltx_align_center ltx_border_t\">9</td>\n<td id=\"A4.I1.i4.tab1.1.2.10\" class=\"ltx_td ltx_align_center ltx_border_t\">10</td>\n</tr>\n<tr id=\"A4.I1.i4.tab1.1.3\" class=\"ltx_tr\">\n<td id=\"A4.I1.i4.tab1.1.3.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"A4.I1.i4.tab1.1.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.3.7\" class=\"ltx_td ltx_align_center ltx_border_t\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.3.8\" class=\"ltx_td ltx_align_center ltx_border_t\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.3.9\" class=\"ltx_td ltx_align_center ltx_border_t\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.3.10\" class=\"ltx_td ltx_align_center ltx_border_t\">0</td>\n<td id=\"A4.I1.i4.tab1.1.3.11\" class=\"ltx_td ltx_align_center ltx_border_t\">0</td>\n</tr>\n<tr id=\"A4.I1.i4.tab1.1.4\" class=\"ltx_tr\">\n<td id=\"A4.I1.i4.tab1.1.4.1\" class=\"ltx_td ltx_align_center ltx_border_r\">2</td>\n<td id=\"A4.I1.i4.tab1.1.4.2\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i4.tab1.1.4.3\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i4.tab1.1.4.4\" class=\"ltx_td ltx_align_center\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.4.5\" class=\"ltx_td ltx_align_center\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.4.6\" class=\"ltx_td ltx_align_center\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.4.7\" class=\"ltx_td ltx_align_center\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.4.8\" class=\"ltx_td ltx_align_center\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.4.9\" class=\"ltx_td ltx_align_center\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.4.10\" class=\"ltx_td ltx_align_center\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.4.11\" class=\"ltx_td ltx_align_center\">1/8</td>\n</tr>\n<tr id=\"A4.I1.i4.tab1.1.5\" class=\"ltx_tr\">\n<td id=\"A4.I1.i4.tab1.1.5.1\" class=\"ltx_td ltx_align_center ltx_border_r\">3</td>\n<td id=\"A4.I1.i4.tab1.1.5.2\" class=\"ltx_td ltx_align_center\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.5.3\" class=\"ltx_td ltx_align_center\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.5.4\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i4.tab1.1.5.5\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i4.tab1.1.5.6\" class=\"ltx_td ltx_align_center\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.5.7\" class=\"ltx_td ltx_align_center\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.5.8\" class=\"ltx_td ltx_align_center\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.5.9\" class=\"ltx_td ltx_align_center\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.5.10\" class=\"ltx_td ltx_align_center\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.5.11\" class=\"ltx_td ltx_align_center\">1/8</td>\n</tr>\n<tr id=\"A4.I1.i4.tab1.1.6\" class=\"ltx_tr\">\n<td id=\"A4.I1.i4.tab1.1.6.1\" class=\"ltx_td ltx_align_center ltx_border_r\">4</td>\n<td id=\"A4.I1.i4.tab1.1.6.2\" class=\"ltx_td ltx_align_center\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.6.3\" class=\"ltx_td ltx_align_center\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.6.4\" class=\"ltx_td ltx_align_center\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.6.5\" class=\"ltx_td ltx_align_center\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.6.6\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i4.tab1.1.6.7\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"A4.I1.i4.tab1.1.6.8\" class=\"ltx_td ltx_align_center\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.6.9\" class=\"ltx_td ltx_align_center\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.6.10\" class=\"ltx_td ltx_align_center\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.6.11\" class=\"ltx_td ltx_align_center\">1/8</td>\n</tr>\n<tr id=\"A4.I1.i4.tab1.1.7\" class=\"ltx_tr\">\n<td id=\"A4.I1.i4.tab1.1.7.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">5</td>\n<td id=\"A4.I1.i4.tab1.1.7.2\" class=\"ltx_td ltx_align_center ltx_border_b\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.7.3\" class=\"ltx_td ltx_align_center ltx_border_b\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.7.4\" class=\"ltx_td ltx_align_center ltx_border_b\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.7.5\" class=\"ltx_td ltx_align_center ltx_border_b\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.7.6\" class=\"ltx_td ltx_align_center ltx_border_b\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.7.7\" class=\"ltx_td ltx_align_center ltx_border_b\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.7.8\" class=\"ltx_td ltx_align_center ltx_border_b\">0</td>\n<td id=\"A4.I1.i4.tab1.1.7.9\" class=\"ltx_td ltx_align_center ltx_border_b\">0</td>\n<td id=\"A4.I1.i4.tab1.1.7.10\" class=\"ltx_td ltx_align_center ltx_border_b\">1/8</td>\n<td id=\"A4.I1.i4.tab1.1.7.11\" class=\"ltx_td ltx_align_center ltx_border_b\">1/8</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "This two-step strategy yields notable benefits.\nBy executing FedTAN initially, we acquire more precise statistical parameters for BN layers.\nSubsequently, applying FedAvg in later iterations avoids the need for layer-wise aggregations, effectively reducing communication rounds.\nWe denote this communication-efficient scheme of FedTAN as FedTAN-II.",
            "We further demonstrate the effectiveness of proposed FedTAN by adopting mini-batch SGD with momentum and weight decay.\nTable 1 compares the testing accuracy of different FL schemes, based on the mean and standard deviation of five independent experiments.\nAs can be observed, FedTAN performs well for both data cases, while other FL benchmarks significantly degrade in the non-i.i.d. data case.\nIn particular, under the non-i.i.d. data, FedTAN increases its average testing accuracy by 41.76% compared to FedAvg+BN.",
            "In comparison to BN, GN [13] is less sensitive to data distribution and also widely used.\nDespite this, GN is still unable to match the performance of BN in many recognition tasks.\nFor instance, replacing BN with GN during ResNet training on the CIFAR dataset leads to performance degradation [43].\nBased on CIFAR-10 dataset, Fig. 9 compares the performance of the ResNet-20 with BN trained by FedTAN and the one with GN trained by FedAvg, in terms of iterations.\nAdditionally, Table 2 compares the testing accuracy of the global models obtained through these two schemes after the entire training process.\nIt can be seen from Fig. 9 and Table 2 that, for both i.i.d. and non-i.i.d data cases, FedTAN maintains the superiority of BN and achieves higher testing accuracy with faster convergence rates.\nFurthermore, employing mini-batch SGD with momentum enhances the learning performance of both FedTAN and FedAvg+GN compared to the case without momentum.",
            "In FedTAN, as depicted in Algorithms 1, 2 and 3, the modified first local updating step brings extra transmission of statistical parameters and their gradients between the server and the clients.\nTable 3 compares the number of bits exchanged between the server and five clients per iteration required by different FL schemes when training ResNet-20 with the CIFAR-10 dataset, where the detailed calculation process of each exchanged data volume is provided in Section G of the Supplementary Material.\nTable 3 shows that, compared to FedAvg+BN and FedAvg+GN, the transmitted data volume between the server and clients in FedTAN only increases by 1.02% and 1.52%, respectively.\nThis slight increase is due to statistical parameters comprising a small portion of all parameters, especially in large-scale DNNs.\nNext, Fig. 10 compares the testing accuracy of various FL schemes concerning the number of bits exchanged between the server and all clients.\nThe figure shows that with negligible additional exchanged bits, FedTAN is still capable of maintaining the faster convergence rate w.r.t. the transmission data volume.\nSpecifically, given an expected level of testing accuracy, FedAvg+GN requires significantly more bits to be exchanged between the server and clients due to its slower convergence rate compared to FedTAN.\nMoreover, while FedTAN achieves higher learning accuracy, the extra communication rounds caused by the layer-wise aggregations in Algorithms 2 and 3 will increase the total communication rounds required for FedTAN.\nThus, FedAvg+GN is recommended if the number of communication rounds is critical.",
            "Next, Table 4 compares the total exchanged bits, the percentages of extra communication rounds and exchanged bits caused by layer-aggregations in Algorithms 2 and 3 during the entire training process of FedTAN-II for varying values of M𝑀M.\nThe table shows that the difference in total exchanged bits between FedTAN and FedTAN-II under different M𝑀M is negligible, as the statistical parameters in BN layers occupy only a small portion of the total model size.\nMoreover, we find that although the extra communication rounds caused by layer-aggregations in Algorithms 2 and 3 account for a large percentage of the total exchanged rounds during the entire training process, the corresponding extra exchanged bits are negligible.\nThus, the layer-aggregations in Algorithms 2 and 3 constitute a high-frequent low-volume communication mode, which effectively mitigates divergence among local models, as highlighted in [7].",
            "Fig. 12 compares the performance of various FL schemes on CIFAR-10 dataset across different batch sizes.\nIt can be observed that all FL schemes exhibit relatively stable performance despite variations in batch size.\nNotably, the proposed FedTAN consistently performs comparably to centralized learning across all cases.\nHowever, both FedAvg+BN and FedBN exhibit poor performance under the non-i.i.d. data distribution, regardless of the batch size value.",
            "Next, In Table 5, we contrast the aggregate exchanged bits, the proportions of supplementary communication rounds, and the exchanged bits resulting from layer-aggregations in Algorithms 2 and 3 throughout the complete training procedure of FedTAN-II across different M𝑀M values.\nAs observed in Table 4, Table 5 further illustrates the insubstantial variance in total exchanged bits between FedTAN and FedTAN-II across varying M𝑀M values, owing to the minor proportion occupied by statistical parameters in BN layers within the total model size.\nFurthermore, while the additional communication rounds resulting from layer-aggregations in Algorithms 2 and 3 contribute to a specific proportion of the total exchanged bits throughout the training process, particularly evident with larger values of M𝑀M, the accompanying extra exchanged bits remain negligible."
        ]
    },
    "A4.I1.i5.tab1": {
        "caption": "",
        "table": "<table id=\"A4.I1.i5.tab1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A4.I1.i5.tab1.1.1\" class=\"ltx_tr\">\n<td id=\"A4.I1.i5.tab1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"A4.I1.i5.tab1.1.1.1.1\" class=\"ltx_text\">Client</span></td>\n<td id=\"A4.I1.i5.tab1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"10\">Label</td>\n</tr>\n<tr id=\"A4.I1.i5.tab1.1.2\" class=\"ltx_tr\">\n<td id=\"A4.I1.i5.tab1.1.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\">1</td>\n<td id=\"A4.I1.i5.tab1.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">2</td>\n<td id=\"A4.I1.i5.tab1.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">3</td>\n<td id=\"A4.I1.i5.tab1.1.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">4</td>\n<td id=\"A4.I1.i5.tab1.1.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">5</td>\n<td id=\"A4.I1.i5.tab1.1.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\">6</td>\n<td id=\"A4.I1.i5.tab1.1.2.7\" class=\"ltx_td ltx_align_center ltx_border_t\">7</td>\n<td id=\"A4.I1.i5.tab1.1.2.8\" class=\"ltx_td ltx_align_center ltx_border_t\">8</td>\n<td id=\"A4.I1.i5.tab1.1.2.9\" class=\"ltx_td ltx_align_center ltx_border_t\">9</td>\n<td id=\"A4.I1.i5.tab1.1.2.10\" class=\"ltx_td ltx_align_center ltx_border_t\">10</td>\n</tr>\n<tr id=\"A4.I1.i5.tab1.1.3\" class=\"ltx_tr\">\n<td id=\"A4.I1.i5.tab1.1.3.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"A4.I1.i5.tab1.1.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.3.7\" class=\"ltx_td ltx_align_center ltx_border_t\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.3.8\" class=\"ltx_td ltx_align_center ltx_border_t\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.3.9\" class=\"ltx_td ltx_align_center ltx_border_t\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.3.10\" class=\"ltx_td ltx_align_center ltx_border_t\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.3.11\" class=\"ltx_td ltx_align_center ltx_border_t\">1/10</td>\n</tr>\n<tr id=\"A4.I1.i5.tab1.1.4\" class=\"ltx_tr\">\n<td id=\"A4.I1.i5.tab1.1.4.1\" class=\"ltx_td ltx_align_center ltx_border_r\">2</td>\n<td id=\"A4.I1.i5.tab1.1.4.2\" class=\"ltx_td ltx_align_center\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.4.3\" class=\"ltx_td ltx_align_center\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.4.4\" class=\"ltx_td ltx_align_center\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.4.5\" class=\"ltx_td ltx_align_center\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.4.6\" class=\"ltx_td ltx_align_center\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.4.7\" class=\"ltx_td ltx_align_center\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.4.8\" class=\"ltx_td ltx_align_center\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.4.9\" class=\"ltx_td ltx_align_center\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.4.10\" class=\"ltx_td ltx_align_center\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.4.11\" class=\"ltx_td ltx_align_center\">1/10</td>\n</tr>\n<tr id=\"A4.I1.i5.tab1.1.5\" class=\"ltx_tr\">\n<td id=\"A4.I1.i5.tab1.1.5.1\" class=\"ltx_td ltx_align_center ltx_border_r\">3</td>\n<td id=\"A4.I1.i5.tab1.1.5.2\" class=\"ltx_td ltx_align_center\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.5.3\" class=\"ltx_td ltx_align_center\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.5.4\" class=\"ltx_td ltx_align_center\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.5.5\" class=\"ltx_td ltx_align_center\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.5.6\" class=\"ltx_td ltx_align_center\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.5.7\" class=\"ltx_td ltx_align_center\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.5.8\" class=\"ltx_td ltx_align_center\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.5.9\" class=\"ltx_td ltx_align_center\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.5.10\" class=\"ltx_td ltx_align_center\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.5.11\" class=\"ltx_td ltx_align_center\">1/10</td>\n</tr>\n<tr id=\"A4.I1.i5.tab1.1.6\" class=\"ltx_tr\">\n<td id=\"A4.I1.i5.tab1.1.6.1\" class=\"ltx_td ltx_align_center ltx_border_r\">4</td>\n<td id=\"A4.I1.i5.tab1.1.6.2\" class=\"ltx_td ltx_align_center\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.6.3\" class=\"ltx_td ltx_align_center\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.6.4\" class=\"ltx_td ltx_align_center\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.6.5\" class=\"ltx_td ltx_align_center\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.6.6\" class=\"ltx_td ltx_align_center\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.6.7\" class=\"ltx_td ltx_align_center\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.6.8\" class=\"ltx_td ltx_align_center\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.6.9\" class=\"ltx_td ltx_align_center\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.6.10\" class=\"ltx_td ltx_align_center\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.6.11\" class=\"ltx_td ltx_align_center\">1/10</td>\n</tr>\n<tr id=\"A4.I1.i5.tab1.1.7\" class=\"ltx_tr\">\n<td id=\"A4.I1.i5.tab1.1.7.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">5</td>\n<td id=\"A4.I1.i5.tab1.1.7.2\" class=\"ltx_td ltx_align_center ltx_border_b\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.7.3\" class=\"ltx_td ltx_align_center ltx_border_b\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.7.4\" class=\"ltx_td ltx_align_center ltx_border_b\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.7.5\" class=\"ltx_td ltx_align_center ltx_border_b\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.7.6\" class=\"ltx_td ltx_align_center ltx_border_b\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.7.7\" class=\"ltx_td ltx_align_center ltx_border_b\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.7.8\" class=\"ltx_td ltx_align_center ltx_border_b\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.7.9\" class=\"ltx_td ltx_align_center ltx_border_b\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.7.10\" class=\"ltx_td ltx_align_center ltx_border_b\">1/10</td>\n<td id=\"A4.I1.i5.tab1.1.7.11\" class=\"ltx_td ltx_align_center ltx_border_b\">1/10</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "This two-step strategy yields notable benefits.\nBy executing FedTAN initially, we acquire more precise statistical parameters for BN layers.\nSubsequently, applying FedAvg in later iterations avoids the need for layer-wise aggregations, effectively reducing communication rounds.\nWe denote this communication-efficient scheme of FedTAN as FedTAN-II.",
            "We further demonstrate the effectiveness of proposed FedTAN by adopting mini-batch SGD with momentum and weight decay.\nTable 1 compares the testing accuracy of different FL schemes, based on the mean and standard deviation of five independent experiments.\nAs can be observed, FedTAN performs well for both data cases, while other FL benchmarks significantly degrade in the non-i.i.d. data case.\nIn particular, under the non-i.i.d. data, FedTAN increases its average testing accuracy by 41.76% compared to FedAvg+BN.",
            "In comparison to BN, GN [13] is less sensitive to data distribution and also widely used.\nDespite this, GN is still unable to match the performance of BN in many recognition tasks.\nFor instance, replacing BN with GN during ResNet training on the CIFAR dataset leads to performance degradation [43].\nBased on CIFAR-10 dataset, Fig. 9 compares the performance of the ResNet-20 with BN trained by FedTAN and the one with GN trained by FedAvg, in terms of iterations.\nAdditionally, Table 2 compares the testing accuracy of the global models obtained through these two schemes after the entire training process.\nIt can be seen from Fig. 9 and Table 2 that, for both i.i.d. and non-i.i.d data cases, FedTAN maintains the superiority of BN and achieves higher testing accuracy with faster convergence rates.\nFurthermore, employing mini-batch SGD with momentum enhances the learning performance of both FedTAN and FedAvg+GN compared to the case without momentum.",
            "In FedTAN, as depicted in Algorithms 1, 2 and 3, the modified first local updating step brings extra transmission of statistical parameters and their gradients between the server and the clients.\nTable 3 compares the number of bits exchanged between the server and five clients per iteration required by different FL schemes when training ResNet-20 with the CIFAR-10 dataset, where the detailed calculation process of each exchanged data volume is provided in Section G of the Supplementary Material.\nTable 3 shows that, compared to FedAvg+BN and FedAvg+GN, the transmitted data volume between the server and clients in FedTAN only increases by 1.02% and 1.52%, respectively.\nThis slight increase is due to statistical parameters comprising a small portion of all parameters, especially in large-scale DNNs.\nNext, Fig. 10 compares the testing accuracy of various FL schemes concerning the number of bits exchanged between the server and all clients.\nThe figure shows that with negligible additional exchanged bits, FedTAN is still capable of maintaining the faster convergence rate w.r.t. the transmission data volume.\nSpecifically, given an expected level of testing accuracy, FedAvg+GN requires significantly more bits to be exchanged between the server and clients due to its slower convergence rate compared to FedTAN.\nMoreover, while FedTAN achieves higher learning accuracy, the extra communication rounds caused by the layer-wise aggregations in Algorithms 2 and 3 will increase the total communication rounds required for FedTAN.\nThus, FedAvg+GN is recommended if the number of communication rounds is critical.",
            "Next, Table 4 compares the total exchanged bits, the percentages of extra communication rounds and exchanged bits caused by layer-aggregations in Algorithms 2 and 3 during the entire training process of FedTAN-II for varying values of M𝑀M.\nThe table shows that the difference in total exchanged bits between FedTAN and FedTAN-II under different M𝑀M is negligible, as the statistical parameters in BN layers occupy only a small portion of the total model size.\nMoreover, we find that although the extra communication rounds caused by layer-aggregations in Algorithms 2 and 3 account for a large percentage of the total exchanged rounds during the entire training process, the corresponding extra exchanged bits are negligible.\nThus, the layer-aggregations in Algorithms 2 and 3 constitute a high-frequent low-volume communication mode, which effectively mitigates divergence among local models, as highlighted in [7].",
            "Fig. 12 compares the performance of various FL schemes on CIFAR-10 dataset across different batch sizes.\nIt can be observed that all FL schemes exhibit relatively stable performance despite variations in batch size.\nNotably, the proposed FedTAN consistently performs comparably to centralized learning across all cases.\nHowever, both FedAvg+BN and FedBN exhibit poor performance under the non-i.i.d. data distribution, regardless of the batch size value.",
            "Next, In Table 5, we contrast the aggregate exchanged bits, the proportions of supplementary communication rounds, and the exchanged bits resulting from layer-aggregations in Algorithms 2 and 3 throughout the complete training procedure of FedTAN-II across different M𝑀M values.\nAs observed in Table 4, Table 5 further illustrates the insubstantial variance in total exchanged bits between FedTAN and FedTAN-II across varying M𝑀M values, owing to the minor proportion occupied by statistical parameters in BN layers within the total model size.\nFurthermore, while the additional communication rounds resulting from layer-aggregations in Algorithms 2 and 3 contribute to a specific proportion of the total exchanged bits throughout the training process, particularly evident with larger values of M𝑀M, the accompanying extra exchanged bits remain negligible."
        ]
    },
    "A8.T5": {
        "caption": "Table 5: Communication overhead of FedTAN-II on MNIST dataset.",
        "table": "<table id=\"A8.T5.6\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"A8.T5.6.7\" class=\"ltx_tr\">\n<td id=\"A8.T5.6.7.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"A8.T5.6.7.1.1\" class=\"ltx_text ltx_font_bold\">FL scheme</span></td>\n<td id=\"A8.T5.6.7.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"A8.T5.6.7.2.1\" class=\"ltx_text ltx_font_bold\">Exchanged bits (GB)</span></td>\n<td id=\"A8.T5.6.7.3\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\"><span id=\"A8.T5.6.7.3.1\" class=\"ltx_text ltx_font_bold\">Percentage of extra</span></td>\n</tr>\n<tr id=\"A8.T5.6.8\" class=\"ltx_tr\">\n<td id=\"A8.T5.6.8.1\" class=\"ltx_td ltx_align_center\"><span id=\"A8.T5.6.8.1.1\" class=\"ltx_text ltx_font_bold\">Rounds</span></td>\n<td id=\"A8.T5.6.8.2\" class=\"ltx_td ltx_align_center\"><span id=\"A8.T5.6.8.2.1\" class=\"ltx_text ltx_font_bold\">Bits</span></td>\n</tr>\n<tr id=\"A8.T5.2.2\" class=\"ltx_tr\">\n<td id=\"A8.T5.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"A8.T5.2.2.2.1\" class=\"ltx_text ltx_font_typewriter\">FedTAN-II</span> (<math id=\"A8.T5.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"M\" display=\"inline\"><semantics id=\"A8.T5.1.1.1.m1.1a\"><mi id=\"A8.T5.1.1.1.m1.1.1\" xref=\"A8.T5.1.1.1.m1.1.1.cmml\">M</mi><annotation-xml encoding=\"MathML-Content\" id=\"A8.T5.1.1.1.m1.1b\"><ci id=\"A8.T5.1.1.1.m1.1.1.cmml\" xref=\"A8.T5.1.1.1.m1.1.1\">𝑀</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A8.T5.1.1.1.m1.1c\">M</annotation></semantics></math><math id=\"A8.T5.2.2.2.m2.1\" class=\"ltx_Math\" alttext=\"=\" display=\"inline\"><semantics id=\"A8.T5.2.2.2.m2.1a\"><mo id=\"A8.T5.2.2.2.m2.1.1\" xref=\"A8.T5.2.2.2.m2.1.1.cmml\">=</mo><annotation-xml encoding=\"MathML-Content\" id=\"A8.T5.2.2.2.m2.1b\"><eq id=\"A8.T5.2.2.2.m2.1.1.cmml\" xref=\"A8.T5.2.2.2.m2.1.1\"></eq></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A8.T5.2.2.2.m2.1c\">=</annotation></semantics></math>10)</td>\n<td id=\"A8.T5.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.1388</td>\n<td id=\"A8.T5.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">5.66%</td>\n<td id=\"A8.T5.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.0100%</td>\n</tr>\n<tr id=\"A8.T5.4.4\" class=\"ltx_tr\">\n<td id=\"A8.T5.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">\n<span id=\"A8.T5.4.4.2.1\" class=\"ltx_text ltx_font_typewriter\">FedTAN-II</span> (<math id=\"A8.T5.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"M\" display=\"inline\"><semantics id=\"A8.T5.3.3.1.m1.1a\"><mi id=\"A8.T5.3.3.1.m1.1.1\" xref=\"A8.T5.3.3.1.m1.1.1.cmml\">M</mi><annotation-xml encoding=\"MathML-Content\" id=\"A8.T5.3.3.1.m1.1b\"><ci id=\"A8.T5.3.3.1.m1.1.1.cmml\" xref=\"A8.T5.3.3.1.m1.1.1\">𝑀</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A8.T5.3.3.1.m1.1c\">M</annotation></semantics></math><math id=\"A8.T5.4.4.2.m2.1\" class=\"ltx_Math\" alttext=\"=\" display=\"inline\"><semantics id=\"A8.T5.4.4.2.m2.1a\"><mo id=\"A8.T5.4.4.2.m2.1.1\" xref=\"A8.T5.4.4.2.m2.1.1.cmml\">=</mo><annotation-xml encoding=\"MathML-Content\" id=\"A8.T5.4.4.2.m2.1b\"><eq id=\"A8.T5.4.4.2.m2.1.1.cmml\" xref=\"A8.T5.4.4.2.m2.1.1\"></eq></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A8.T5.4.4.2.m2.1c\">=</annotation></semantics></math>50)</td>\n<td id=\"A8.T5.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">2.1397</td>\n<td id=\"A8.T5.4.4.4\" class=\"ltx_td ltx_align_center\">23.08%</td>\n<td id=\"A8.T5.4.4.5\" class=\"ltx_td ltx_align_center\">0.0501%</td>\n</tr>\n<tr id=\"A8.T5.6.6\" class=\"ltx_tr\">\n<td id=\"A8.T5.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\">\n<span id=\"A8.T5.6.6.2.1\" class=\"ltx_text ltx_font_typewriter\">FedTAN-II</span> (<math id=\"A8.T5.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"M\" display=\"inline\"><semantics id=\"A8.T5.5.5.1.m1.1a\"><mi id=\"A8.T5.5.5.1.m1.1.1\" xref=\"A8.T5.5.5.1.m1.1.1.cmml\">M</mi><annotation-xml encoding=\"MathML-Content\" id=\"A8.T5.5.5.1.m1.1b\"><ci id=\"A8.T5.5.5.1.m1.1.1.cmml\" xref=\"A8.T5.5.5.1.m1.1.1\">𝑀</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A8.T5.5.5.1.m1.1c\">M</annotation></semantics></math><math id=\"A8.T5.6.6.2.m2.1\" class=\"ltx_Math\" alttext=\"=\" display=\"inline\"><semantics id=\"A8.T5.6.6.2.m2.1a\"><mo id=\"A8.T5.6.6.2.m2.1.1\" xref=\"A8.T5.6.6.2.m2.1.1.cmml\">=</mo><annotation-xml encoding=\"MathML-Content\" id=\"A8.T5.6.6.2.m2.1b\"><eq id=\"A8.T5.6.6.2.m2.1.1.cmml\" xref=\"A8.T5.6.6.2.m2.1.1\"></eq></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A8.T5.6.6.2.m2.1c\">=</annotation></semantics></math>100)</td>\n<td id=\"A8.T5.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\">2.1408</td>\n<td id=\"A8.T5.6.6.4\" class=\"ltx_td ltx_align_center\">37.50%</td>\n<td id=\"A8.T5.6.6.5\" class=\"ltx_td ltx_align_center\">0.1002%</td>\n</tr>\n<tr id=\"A8.T5.6.9\" class=\"ltx_tr\">\n<td id=\"A8.T5.6.9.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"A8.T5.6.9.1.1\" class=\"ltx_text ltx_font_typewriter\">FedTAN</span></td>\n<td id=\"A8.T5.6.9.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">2.1386</td>\n<td id=\"A8.T5.6.9.3\" class=\"ltx_td ltx_align_center ltx_border_b\">75%</td>\n<td id=\"A8.T5.6.9.4\" class=\"ltx_td ltx_align_center ltx_border_b\">0.4992%</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Next, In Table 5, we contrast the aggregate exchanged bits, the proportions of supplementary communication rounds, and the exchanged bits resulting from layer-aggregations in Algorithms 2 and 3 throughout the complete training procedure of FedTAN-II across different M𝑀M values.\nAs observed in Table 4, Table 5 further illustrates the insubstantial variance in total exchanged bits between FedTAN and FedTAN-II across varying M𝑀M values, owing to the minor proportion occupied by statistical parameters in BN layers within the total model size.\nFurthermore, while the additional communication rounds resulting from layer-aggregations in Algorithms 2 and 3 contribute to a specific proportion of the total exchanged bits throughout the training process, particularly evident with larger values of M𝑀M, the accompanying extra exchanged bits remain negligible."
        ]
    }
}