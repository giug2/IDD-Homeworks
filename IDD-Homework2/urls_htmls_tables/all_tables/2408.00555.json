{
    "id_table_1": {
        "caption": "Table 1.  Results on POPE.  Regular  decoding denotes greedy decoding, whereas  RAR  refers to our method with augmentation.",
        "table": "S3.T1.2.2",
        "footnotes": [],
        "references": [
            "To mitigate hallucinations in LVLMs, a series of attempts have been recently proposed and can be broadly classified into two categories.  The first class of approach retrains the LVLMs with constructed hallucination-related datasets by  supervised fine-tuning (SFT)   (Liu et al . ,  2023c ; Gunjal et al . ,  2024 ; Wang et al . ,  2024b ) , or Reinforcement Learning from Human Feedback (RLHF)  (Sun et al . ,  2023 ; Yu et al . ,  2023 ) .  Although effective, these methods introduce significantly additional training costs.  The other solutions focus on designing more robust decoding strategies  (Chen et al . ,  2024b ; Huang et al . ,  2023 ; Yin et al . ,  2023b ; Leng et al . ,  2023 ) . For example, VCD   (Leng et al . ,  2023 )  introduces visual contrastive decoding to contrast output distributions derived from original and distorted visual inputs.  In this way, the LVLM reduces the over-reliance on statistical bias and unimodal prior.  Although these methods are training-free, they still suffer from the limitations of LVLMs static parametric capacity.  Recently, in the realm of large language models (LMs), augmenting LLMs  (Karpukhin et al . ,  2020 ; Izacard et al . ,  2022 ; Ram et al . ,  2023 )  by retrieving information from external knowledge resources has shown promise in reducing language hallucinations. Furthermore, this retrieval augmentation serves as a flexible way to extend beyond the models inherent knowledge without the burden of extensive training costs.  In this paper, we aim to propose a novel framework to augment LVLMs with external knowledge by introducing an Active Retrieval-Augmented large vision-language model (ARA) for mitigating hallucinations. As shown in Figure  1 , given an input image and query, the LVLM tends to produce hallucinated answers. Instead, our model can accurately identify the most relevant pairs and output the correct answer.",
            "Results on POPE.  Experimental results on POPE under the random, popular, and adversarial settings are summarized in Table  1 . This benchmark mainly focuses on the object-level hallucination.  Specifically, the performances of our ARA consistently surpass the baseline results on all of the LVLMs.  This suggests its pivotal role in augmenting LVLMs with retrieval, thereby reducing instances of object-level hallucination.  In a more detailed model-specific analysis, RAR demonstrates varied effects across different LVLMs. For all three LVLMs, the F1 score elevation is predominantly driven by a recall boost (e.g., up to 10 points for mPLUG-Owl2), showcasing retrieving to external knowledge base can effectively help detect object presences."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.  Results on the hallucination subset of MME.",
        "table": "S4.T2.5.5",
        "footnotes": [],
        "references": [
            "In this section, we first describe the generation procedure of the LVLMs, laying the groundwork for comprehending our approach. Subsequently, we will introduce our Active Retrieval-Augmentation large vision-language model (ARA) in detail. As shown in Figure  2 , given the input image and query, the LVLM first analyzes them based on mutual information and determines whether to trigger the retrieval process. If the retrieval is necessary, our ARA will follow a coarse-to-fine paradigm to dissect the retrieval targets and conduct jointly coarse-grained and fine-grained retrieval. Following this, a reranking of the retrieval outcomes is performed for additional refinement. Finally, the LVLM harnesses this externally sourced knowledge to generate the final response.",
            "As shown in Figure  2 , given the input image and query, our ARA model first determines whether the input pair requires retrieval. In this way, we can circumvent unnecessary retrieval during periods of high certainty.  In this section, we propose three active retrieval methods to explore the difficulty metrics that influence when to retrieve, including Confidence-aware Active Retrieval,  Question-aware Active Retrieval, and Image-aware Active Retrieval. The first method relies on the confidence of the model output, and the latter two matrices depend on the mutual information between the inputs.  An ideal metric should effectively reduce the frequency of retrievals and can easily apply to different LVLMs.",
            "Given the inherently hierarchical nature of images, simple full-image retrieval may result in noise and irrelevant outcomes. As shown in Figure  2 , using the full image for retrieval can not obtain the most relevant information for reasoning, namely red shirt. Thus it is imperative to decompose the target object causing hallucination from the input image for more accurate retrieval.  In the following, we will first describe the coarse-grained retrieval and then introduce the fine-grained retrieve.",
            "Fine-grained Retrieve.  While full-image retrieval can incorporate valuable information, it may overlook fine-grained details, such as diminutive objects and intricate attribute data. Therefore, it is crucial to propose a fine-grained retrieval mechanism that concentrates on the distinct objects within an image.  To conduct fine-grained retrieval, we initially extract key entities from the input query utilizing the large language model LLaMA2-7B  (Touvron et al . ,  2023 ) . This extraction is facilitated by in-context learning  (Dong et al . ,  2022 ) , where demonstrations are presented to the LLM, which subsequently outputs the specific entities from the input query.  Following this, Grounding Dino  (Liu et al . ,  2023e )  is deployed to identify the objects in the image that correspond to the extracted entities. As illustrated in Figure  2 , we crop the targeted object and then perform retrieval in a manner akin to the previously described coarse-grained retrieval.",
            "Results on MME Hallucination Subset.   The MME subset evaluations extend beyond POPEs scope, encompassing both object-level and attribute-level hallucination. Results in Table  2  show that our ARA leads to a uniform enhancement in addressing object-level hallucination for all models, except the count score of Qwen-VL. Meanwhile, our ARA demonstrates significant improvements in attribute-level Color scores, contributing to substantial overall performance gains. These improvements are attributable to our designed coarse-to-fine retrieval paradigms, which effectively focus on the target object."
        ]
    },
    "id_table_3": {
        "caption": "Table 3.  Results on the MMStar benchmark. CP (coarse perception), FP (fine-grained perception), IR (instance reasoning), LR (logical reasoning), MA (mathematics), ST (science & technology).",
        "table": "S4.T3.1.1",
        "footnotes": [],
        "references": [
            "Results on MMStar.  In a more difficult benchmark MMStar which rigorously assesses the multi-modal proficiency, as depicted in Table  3 , our method ARA secures a notable enhancement across all six subsets. For instance, LLaVA-1.5 realizes an average improvement of 8.8 points. Unlike POPE, which may include statistical bias, answering questions in MMStar necessitates reliance on image information. Hence, our retrieval augmentation method introducing external image and text knowledge significantly bolsters model performance.",
            "How much external information do we need?   With the above retrieval, we can obtain multiple image-caption pairs from the database. This research delves into the efficacy of varying retrieved pair configurations.  Considering that different LVLMs may be sensitive to retrieval configurations, our study incorporates both (T) and (I+T) configurations for a thorough evaluation. The analysis is constrained to examining between one to five retrieved pairs due to the input token limitations inherent in contemporary LVLMs.  As depicted in Figure  3 , LLaVA (T) reaches peak accuracy with three retrieved texts, whereas mPLUG (T) begins to saturate when there are four retrieval texts.  Conversely, QwenVL (I+T) demonstrates improved outcomes with a greater number of retrieval pairs; however, it still lags considerably behind the performance achieved by the other two LVLM models."
        ]
    },
    "id_table_4": {
        "caption": "Table 4.  Results on the hallucination subsets of MMbench benchmark. OL (object localization),  ATR (attribute recognition), SR (spatial relationship), and ACR (action recognition).",
        "table": "S4.T4.1",
        "footnotes": [],
        "references": [
            "Results on MMBench Hallucination Subset.   This benchmark extends our hallucination evaluation to incorporate the relation level in addition to the above object and attribute levels, providing a more holistic assessment for reducing hallucinations. As evidenced in Table  4 , benefiting from coarse-grained retrieval, our ARA manifests discernible enhancements in the object and relation-level subsets, such as OL, SR, and ACR. For instance, our ARA model surpasses the vanilla LLaVA-1.5 with 5.08 percent points on object localization (OL).  Concurrently, the integration of fine-grained retrieval into our model facilitates superior performance in attribute-level subsets, namely attribute recognition (ATR).",
            "To substantiate the efficacy and indispensability of reranking, we begin by providing a case study within the POPE benchmark.  As depicted in Figure  4 , in instances where an image portrays people in a pool, leveraging CLIP for image retrieval may inadvertently introduce irrelevant content due to its predominant emphasis on visual congruence.  For instance, it might retrieve an incongruent image captioned fruit and a bowl of bread sit on the table. Therefore, an adept reranking methodology can proficiently filter and organize the retrieved images in a more relevant manner. In this particular scenario, our reranking strategy aligns the input image caption with those of the retrieved images.  By assessing these similarity metrics, we are able to reorganize the pool of retrieved images into a more accurately ranked series, thereby enhancing the precision of the retrieval process."
        ]
    },
    "id_table_5": {
        "caption": "Table 5.  Ablation studies on different retrieving methods. The results are obtained on POPE MSCOCO Popular by LLaVA 1.5. For this study, we only conduct coarse-grained retrieval.",
        "table": "S4.T5.4",
        "footnotes": [],
        "references": [
            "Which kind of retrieval is better?   Due to the multimodal characteristics of the input, there are various retrieval methods at our disposal. As shown in Table  5 , we attempt to use the input query and image to search for images and descriptions in the database. For this study, we only conduct coarse-grained retrieval and we use the COCO dataset as the database which contains a caption for each image.  As a result, there are four retrieval methods available to us.  As reflected in the results within this table, we adopt the input image as our primary retrieval modality for images in the database throughout our series of experiments.  Surprisingly, using the input query to search the database for image leads to substantially poorer outcomes, achieving only 51.6% accuracy.  This lower performance may stem from the fact that, despite the presence of relevant keywords, the images retrieved this way tend to be surrounded by a significant degree of noise.",
            "As excessive retrieval activations may lead to undue time expenditure and unnecessary retrieval, in this paper, retrieval is activated using a difficulty metric. In this ablation study, we compare three kinds of active retrieval methods, including confidence-aware active retrieval, query-aware active retrieval, and image-aware active retrieval. As shown in Figure  5 , we have drawn the proportion of queries that need to be retrieved to achieve peak performance. Firstly, it is quite intuitive, the fluctuations of confidence-aware active retrieval among different models are very significant. For mPLUG-Owl2, we even need to retrieve all the queries, which indicates that confidence-aware active retrieval is not a good indicator of whether to perform a retrieval. For both query-aware active retrieval and image-aware active retrieval, the LVLMs can better determine a trigger threshold.  Considering that query-aware active retrieval is more concise and the process of adding noise is omitted, we use query-aware active retrieval in all our experiments.  It is worth noting that the very slight decline in the models performance after continuing to use retrieval beyond the trigger threshold is due to the fact that the model itself has a high level of confidence, but the retrieval has introduced some redundant noise information, which we cannot filter out through reranking."
        ]
    },
    "id_table_6": {
        "caption": "Table 6.  Performance on the LLaVA-1.5. In this paper, we compare different kinds of retrieval schemes and regular decoding. I indicates augmentation with only retrieved texts and I+T means enhancing with both texts and images.  The experiments are performed on the popular part of POPE MSCOCO dataset. Coarse and Fine indicate coarse-grained and fine-grained retrieval, separately. Coarse+Fine means the instance-level fusion, while Coarse & Fine corresponds to the probabilistic-level fusion.",
        "table": "S5.T6.1",
        "footnotes": [],
        "references": [
            "How to augment the LVLM?  After retrieving, we can obtain retrieved images and paired captions. In this section, we explore how to augment the LVLM with this external information. For a fair comparison, in this study, we fix the number of retrieved pairs to 3 for both coarse-grained and fine-grained retrieval. As demonstrated in Table 6, when using coarse-grained retrieval, we observe that the image and caption pair perform worse than the single caption (86.93 vs 87.43).  However, the opposite results are observed in fine-grained retrieval and the Fine (I+T) achieves 86.53 compared with 86.20 of Fine (T). These results imply that  retrieval augmentation is sensitive to specific configurations. In this experiment, we do not carefully compare with Coarse (I) or Fine (I) as we have found that the LVLMs we use in our experiments are still quite weak in multi-image reasoning.",
            "In Section 3.5, we propose a joint coarse-grained and fine-grained decoding, designed to optimally utilize the two retrieval mechanisms.  In Table  6 , we also investigate the effectiveness of probabilistic-level fusion denoted as Coarse & Fine and instance-level fusion labeled as Coarse+Fine.  As shown from the results, we observe that Coarse+Fine even performs than Coarse retrieval, signifying that simply integrating the coarse-grained and fine-grained retrieved results in a prompt fails to enhance performance, given the distinct granularity of the information retrieved through these two approaches. Instead, the probabilistic-level fusion Coarse & Fine achieves better accuracy than both two kinds of retrieval.  Thus, in our paper, we use probabilistic-level fusion for the following experiments. It is worth noting that Coarse & Fine (T) performs better than Coarse & Fine (I+T) for LLaVA-1.5 but Coarse & Fine (I+T) achieves better for Qwen-VL (we will describe in Figure 3 later). This result further validates the necessity of carefully selecting retrieval configurations to obtain the most favorable experimental results.",
            "To qualitatively verify the effectiveness of our ARA method on downstream tasks, we presented five examples from the POPE MSCOCO dataset and MME benchmark. As shown in Figure  6 , we present three image-caption pairs for both coarse-grained retrieval and fine-grained retrieval. In the above three examples of determining the existence of objects, fine-grained retrieval helps to increase the models existence information about specific objects. For the last two examples, the coarse-grained retrieval contributed effectively to capturing the overall information of the images. Thus, with both kinds of retrieval, our RAR model can effectively increase the accuracy compared with vanilla LLaVA-1.5."
        ]
    },
    "id_table_7": {
        "caption": "Table 7.  Ablation studies on different reranking methods. The results are obtained on POPE MSCOCO Popular by LLaVA-1.5. In this experiment, we incorporate a reranking strategy on top of the coarse-grained retrieval. Thus, the result of the first line (None) aligns with the results in Table 5.",
        "table": "S5.T7.1",
        "footnotes": [],
        "references": [
            "In order to further quantify the effect of reranking and compare different reranking methods, we conduct experiments in Table  7 .  The result reveals that the omission of re-ranking precipitates a decline in accuracy, dropping from 87.17% to 86.93%  This suggests that re-ranking effectively mitigates noise, particularly when employing the top three re-ordered retrieval pairs.  For comparative purposes, we executed two alternative methodologies. The k-Reciprocal approach  (Zhong et al . ,  2017 ) , a prevalent offline person re-identification method, re-ranks images using the k-Reciprocal nearest neighbor algorithm. While this method offers a swifter execution by circumventing the captioning process of input images, its efficacy slightly underperforms our strategy. Additionally, we examined the employment of the external tool VSR  (Liu et al . ,  2023b ) , which determines the entailment between the caption of the input image and the retrieved images. As shown in Table 7, this method gains only a slight increase in accuracy (86.93 vs 86.97). This may be due to the visual entailment model not being sufficiently versatile."
        ]
    },
    "id_table_8": {
        "caption": "Table 8.  The performance comparison between our method RAR and VCD in POPE MSCOCO benchmark. VCD is the state-of-the-art method published in CVPR24. The results of VCD-sampling are from their paper. For a fair comparison with our method, we rerun VCD with greedy decoding.",
        "table": "S5.T8.2",
        "footnotes": [],
        "references": [
            "In this paper, we augment LVLMs with external knowledge to mitigate hallucinations.  To further demonstrate the effectiveness of our method, we compare it with VCD  (Leng et al . ,  2023 )  and the results are demonstrated in Table  8 . VCD is a state-of-the-art training-free method that explores intra-model knowledge with visual contrastive decoding. From the results, we can notice that our method significantly surpasses VCD in all three settings. For instance, in popular settings, our method gains a 4.06 percent improvement. It is worth noting that VCD performs comparably with the vanilla LLaVA-1.5 when using greedy decoding. This phenomenon is also observed in  (Chen et al . ,  2024b ) . Instead, our method significantly surpasses the vanilla LLaVA-1.5 model as presented in Tabe 1. This result further verifies the robust improvement of introducing external information when applying our ARA method."
        ]
    },
    "id_table_9": {
        "caption": "Table 9.  Text generation quality comparison. Results on the MS-COCO dataset.",
        "table": "S5.T9.1",
        "footnotes": [],
        "references": [
            "In this section, we evaluated the effectiveness of our method in improving text generation quality on the MS-COCO validation set 1 1 1 We use the official COCO evaluation package. . As shown in Table 9, all models LLaVA, Qwen-VL, and mPLUG exhibited significant improvements after applying our method, with an average improvement of 7.7% across the three models. Notably, the improvement was most pronounced for Qwen-VL, with an average increase of approximately 14.9%."
        ]
    },
    "global_footnotes": [
        "",
        "The first two authors contribute equally.",
        "Corresponding author.",
        "We use the official COCO evaluation package."
    ]
}