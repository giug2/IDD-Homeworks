{
    "id_table_1": {
        "caption": "Table 1:  Performance comparison of different baselines on the FeTaQA dataset. GPT-3.5-turbo-0125 with the triples and RAG achieves  significantly better S-BLEU and ROUGE metrics.",
        "table": "S5.T1.1",
        "footnotes": [],
        "references": [
            "The performance of our proposed approach, which integrates extracted triples with RAG in a fine-tuned GPT-3.5-turbo-0125 model, was evaluated using S-BLEU and ROUGE metrics. The results in Table  1  demonstrates significant improvements over existing approaches that have experiments on the FeTaQA dataset. The reported models are fine-tuned on the FeTaQA dataset and evaluated their model on the S-BLEU and ROUGE metrics.  Furthermore, we evaluated the test set on the only fine-tuned GPT-3.5-turbo-0125 model without the triples and RAG. The comparison between GPT-3.5-turbo-0125 fine-tuned with and without triples and RAG integration highlights the importance of these components. The model with RAG integration shows a substantial improvement in all evaluation metrics.",
            "Figure  1  shows two comparisons between the generated answer from GPT-3.5-turbo-0125 with the triples and RAG and the actual answer from the FeTaQA dataset. In the first example, we have a table with 29 rows with 4.31% important cells to answer the provided question. A sophisticated method is needed to identify the relevant information while ignoring it. Our method could retrieve related information based on the question and a table from the RAG. The table representations in the text format allow the LLM to understand the table. A fine-tuned LLM model could generate more contextual answers. The generated answer has information related to the question and is in the format of the preferred answer in the FeTaQA actual answers. In the second example, we can see that fine-tuned GPT-3.5-turbo-0125 could generate the answers in the preferred context instead of short answers, but the generated answer is wrong. The table has only 5 rows, with 46% of essential cells answering the question. This indicates that the model did not adequately understand the table. With the triples and RAG, this model can better understand the tables relations and answer the question more accurately using the same contextual preferences."
        ]
    },
    "global_footnotes": [
        "Available at:",
        "(Accessed: August 18, 2024)"
    ]
}