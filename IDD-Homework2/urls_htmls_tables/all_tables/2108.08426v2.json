{
    "S4.T1": {
        "caption": "Table 1: Comparisons between MCN and baseline with different backbones on video action recognition task.",
        "table": null,
        "footnotes": [],
        "references": [
            "Comparison with Baseline. We compare the action recognition results of self-supervised training with and without MCN in Table 1. We use three backbones, i.e., S3D, R(2+1)D and R3D-18, to demonstrate the performance boost of MCN. As shown in this table, the accuracies of baseline with S3D, R(2+1)D and R3D-18 are 76.7%, 77.3% and 78.6% on UCF101 dataset respectively. while the accuracies of MCN are 82.9%, 84.8% and 85.4% respectively. Relevant results of HMDB51 dataset can also be observed. There is consistent performance boost when using MCN on different backbones and data sets.",
            "As shown in Table 9, when compared with other state-of-art methods, our method achieves superior or comparable performance in UCF101 dataset. We observe that the top-1 accuracy of CoCLR is slightly better than our R(2+1)D backbone. Actually, our method is orthogonal to CoCLR. In other words, MCN can take the model trained by CoCLR as baseline to take additional improvement. The results of HMDB51 dataset have been shown in Table 10, which demonstrate the superior performance of our proposed MCN."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Linear probe evaluation results of different backbones on video action recognition task.",
        "table": null,
        "footnotes": [],
        "references": [
            "Furthermore, we also evaluate the linear probe results in Table 2, in which only the fully connected layers are fine-tuned. Significant performance boost of MCN can also be observed on both UCF101 and HMDB51 dataset."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Results of different Œ±ùõº\\alpha settings on UCF101 dataset.",
        "table": null,
        "footnotes": [],
        "references": [
            "Influence of Œ±ùõº\\alpha. As depicted in Equation 5, Œ±ùõº\\alpha is introduced to modulate meta loss. We also conducted experiments to demonstrate the influence of this hyper-parameter. Table 3 shows the results of 4 settings of Œ±ùõº\\alpha with R(2+1)D backbone."
        ]
    },
    "S4.T4": {
        "caption": "Table 4: Results of different input frames for MCN and baseline on video action recognition task.",
        "table": null,
        "footnotes": [],
        "references": [
            "In Table 4, we can see that more input frames bring better performance. As the input length increases, MCN takes additional improvement."
        ]
    },
    "S4.T5": {
        "caption": "Table 5: Ablation study for different components of MCN on video recognition task.",
        "table": null,
        "footnotes": [],
        "references": [
            "Influence of Individual Component. We also test each component of MCN to figure out their contributions to the final performance. The results of video action recognition on UCF101 are demonstrated in Table 5. R(2+1)D is selected as backbone.",
            "As shown in Table 5. CL represents contrastive loss. BL represents binary loss from proposed meta branch. Combining CL and BL without meta stages takes 1.9% accuracy improvement. By adding meta stages, additional 5.6% improvement is achieved, which proves the efficiency of meta learning. These experiments can demonstrate the effectiveness of proposed MCN method."
        ]
    },
    "S4.T6": {
        "caption": "Table 6: Linear probe comparisons with state-of-the-art methods on UCF101 and HMDB51 datasets.",
        "table": null,
        "footnotes": [],
        "references": [
            "We first compare our linear probe evaluation results with other state-of-the-art approaches so that we can verify the transferability of the video representations learned with our approach. Results in Table 6 demonstrate that the proposed MCN method outperforms state-of-the-art approaches on both UCF101 and HMDB51."
        ]
    },
    "S4.T7": {
        "caption": "Table 7: Comparisons with state-of-the-art methods for video action recognition on UCF101 and HMDB51 datasets (models are pre-trained on UCF101).",
        "table": null,
        "footnotes": [],
        "references": [
            "We then compare the results of fine-tuning all parameters with other state-of-the-art methods with different pre-training datasets. In specific, we pre-train our models on both UCF101 and Kinetics-400, and then fine-tune the pre-trained models on UCF101 and HMDB51. Table 7 and Table 8 show the results respectively. From the tables, we can observe that the results pre-trained on Kinetics-400 are much better than that pre-trained on UCF101. Kinetics contains much more videos than UCF101. The results demonstrates that MCN can better leverage large volume of unlabeled videos. In both tables, our method outperforms or achieves comparable performance with other state-of-the-art self-supervised approaches. In Table 8, CVRL shows better result than ours. This may be due to three reasons: (1) larger input image resolution (224√ó\\times224) compared with ours (128√ó\\times128); (2) more powerful and deeper backbone network (R3D-50) than ours (R(2+1)D and R3D-18); (3) more efficient data augmentation approaches. These experimental results can shed a light for combining meta learning with self-supervised learning approaches."
        ]
    },
    "S4.T8": {
        "caption": "Table 8: Comparisons with state-of-the-art methods for video action recognition on UCF101 and HMDB51 datasets (models are pre-trained on Kinetics-400).",
        "table": null,
        "footnotes": [],
        "references": [
            "We then compare the results of fine-tuning all parameters with other state-of-the-art methods with different pre-training datasets. In specific, we pre-train our models on both UCF101 and Kinetics-400, and then fine-tune the pre-trained models on UCF101 and HMDB51. Table 7 and Table 8 show the results respectively. From the tables, we can observe that the results pre-trained on Kinetics-400 are much better than that pre-trained on UCF101. Kinetics contains much more videos than UCF101. The results demonstrates that MCN can better leverage large volume of unlabeled videos. In both tables, our method outperforms or achieves comparable performance with other state-of-the-art self-supervised approaches. In Table 8, CVRL shows better result than ours. This may be due to three reasons: (1) larger input image resolution (224√ó\\times224) compared with ours (128√ó\\times128); (2) more powerful and deeper backbone network (R3D-50) than ours (R(2+1)D and R3D-18); (3) more efficient data augmentation approaches. These experimental results can shed a light for combining meta learning with self-supervised learning approaches."
        ]
    },
    "S4.T9": {
        "caption": "Table 9: Comparisons with state-of-the-art approaches for video retrieval on UCF101 dataset.",
        "table": null,
        "footnotes": [],
        "references": [
            "As shown in Table 9, when compared with other state-of-art methods, our method achieves superior or comparable performance in UCF101 dataset. We observe that the top-1 accuracy of CoCLR is slightly better than our R(2+1)D backbone. Actually, our method is orthogonal to CoCLR. In other words, MCN can take the model trained by CoCLR as baseline to take additional improvement. The results of HMDB51 dataset have been shown in Table 10, which demonstrate the superior performance of our proposed MCN."
        ]
    },
    "S4.T10": {
        "caption": "Table 10: Comparisons with state-of-the-art approaches for video retrieval on HMDB51 dataset.",
        "table": null,
        "footnotes": [],
        "references": [
            "As shown in Table 9, when compared with other state-of-art methods, our method achieves superior or comparable performance in UCF101 dataset. We observe that the top-1 accuracy of CoCLR is slightly better than our R(2+1)D backbone. Actually, our method is orthogonal to CoCLR. In other words, MCN can take the model trained by CoCLR as baseline to take additional improvement. The results of HMDB51 dataset have been shown in Table 10, which demonstrate the superior performance of our proposed MCN."
        ]
    }
}