{
    "id_table_1": {
        "caption": "Table 1:  Results of Majority, direct prompting LLaVA and GPT-4V, full-dataset fine-tuning on the other real-world dataset ( LLaVA M  S subscript LLaVA  M S \\text{LLaVA}_{\\text{M}\\rightleftharpoons\\text{S}} LLaVA start_POSTSUBSCRIPT M  S end_POSTSUBSCRIPT ), and fine-tuning on a small set of selected synthetic data ( LLaVA R subscript LLaVA R \\text{LLaVA}_{\\text{R}} LLaVA start_POSTSUBSCRIPT R end_POSTSUBSCRIPT : Random,  LLaVA S subscript LLaVA S \\text{LLaVA}_{\\text{S}} LLaVA start_POSTSUBSCRIPT S end_POSTSUBSCRIPT : SemSim,  LLaVA D subscript LLaVA D \\text{LLaVA}_{\\text{D}} LLaVA start_POSTSUBSCRIPT D end_POSTSUBSCRIPT : DisSim). We report the macro-F1 averaged over 3 trials with different random seeds. The best results for each dataset are in  bold  while the second-best results are  underlined . The standard deviation is in (.).",
        "table": "A4.EGx1",
        "footnotes": [],
        "references": [
            "Multimodal misinformation, which appears more credible and spreads faster than text-only misinformation, has become a significant concern. About one-third of verification claims include multimodal data, with the primary modality being image-text pairs  Akhtar et al. ( 2023 ) . This underscores the importance of Multimodal Misinformation Detection (MMD), which involves determining the veracity of such image-text pairs. These pairs can be created by pairing a textual claim with an out-of-context image, or by manipulating the content of the image, the text, or both. An illustrative example is depicted in Figure  1 , where an image shows Elon Musk holding a flag bearing the statement \"Trump Won, Democrats Cheated,\" a false claim debunked by fact-checkers 1 1 1 https://www.snopes.com/fact-check/elon-musk-trump-won-flag/ .",
            "There is no standard categorizations for multimodal misinformation. We categorize it into two types based on whether the image content are falsely altered or counterfeited, as briefly described below. Appendix  A.1  contains a more formulated and detailed description.",
            "We employ LLaVA-NeXT-13B 5 5 5 In the rest of the paper, we refer to LLaVA-NeXT-13B as LLaVA for brevity.   Liu et al. ( 2024a )  as the base MLLM for fine-tuning, given its excellent performance on various multimodal tasks compared to other MLLMs. Additionally, we perform an ablation study on different base models, including LLaVA-7B and mPLUG-Owl2  Ye et al. ( 2023 ) . More details are in Appendix  B.1.1 .",
            "We utilize the off-the-shelf CLIP model (ViT-L/14)  Radford et al. ( 2021b )  for feature extraction. We use the same prompt template for all models to ensure a fair comparison. Each data selection method empirically selects  750 750 750 750  synthetic instances from the synthetic dataset  D s subscript D s \\mathcal{D}_{s} caligraphic_D start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  to construct  D v subscript D v \\mathcal{D}_{v} caligraphic_D start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT  for fine-tuning, and we ensure the class distribution of selected set is balanced. The unlabeled validation set  D u subscript D u \\mathcal{D}_{u} caligraphic_D start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT  contains 5% instances randomly sampled from each test set. We conduct all experiments with the selection methods three times using different random seeds, and report the mean macro-F1 and standard deviation of each metric across these three runs. More details are provided in Appendix  B.1.2 .",
            "We present main results in Table  1 , and provide an in-depth analysis.",
            "In Table  1 , we observe that directly prompting the LLaVA model does not yield satisfactory performance, indicating that merely relying on the inherent knowledge of base MLLM is insufficient for MMD, especially when the model size is not large enough. We conjecture that task-specific information might be necessary to induce this capability in the base model.",
            "However, incorporating task-specific knowledge by training on one real-world fact-checking dataset does not consistently enhance performance on another; it may even impede it. As shown in Table  1 , training LLaVA on the MediaEval dataset enhances its detection performance on Snopes and Snopes (O+) with an absolute increase of  0.104 0.104 0.104 0.104  and  0.149 0.149 0.149 0.149  respectively, but the reverse adversely affects its performance, where we observe a decline of  0.09 0.09 0.09 0.09 . We examine this inconsistency by checking the distributions of Snopes and MediaEval datasets.",
            "Compared to using OOD real-world data, a small amount of selected synthetic data consistently enhances base models performance on various real-world datasets, with absolute F1 score improvements ranging from  0.133 0.133 0.133 0.133  to  0.414 0.414 0.414 0.414 , as observed in Table  1 . This highlights the promising potential of leveraging synthetic data for real-world MMD. Next, we provide a detailed analysis of specific results from the standpoint of data distribution, offering insights for selecting synthetic data for real-world MMD.",
            "Firstly, both similarity-based data selection methods, SemSim and DisSim, enable the base MLLM to outperform GPT-4V on MediaEval and Snopes (O+) datasets respectively. This confirms the effectiveness of these methods and demonstrate their applicability in real-world scenarios. In Table  1 , SemSim achieves the best F1 score on the MediaEval dataset ( 0.687 0.687 0.687 0.687 ) surpassing DisSim ( 0.611 0.611 0.611 0.611 ), while DisSim exhibits the best on the Snopes (O+) dataset ( 0.813 0.813 0.813 0.813 ) outperforming SemSim ( 0.716 0.716 0.716 0.716 ). We hypothesize this phenomenon is influenced by the choice of similarity metrics.",
            "Secondly, all the data selection methods improve the base model on Snopes dataset, narrowing the performance gap between the base MLLM and GPT-4V. However, the magnitude of the improvements become smaller compared to the MediaEval and Snopes (O+) datasets. For example, as shown in Table  1 , DisSim improves the base model performance from  0.399 0.399 0.399 0.399  to  0.813 0.813 0.813 0.813  on Snopes (O+), whereas its improvement is from  0.407 0.407 0.407 0.407  to  0.496 0.496 0.496 0.496  on Snopes. The reason is likely two-fold: 1) These two datasets have different evaluation focusesSnopes requires models to have strong manipulation detection capabilities while Snopes (O+) emphasizes OOC detection. Additionally, OOC instances are easier to obtain than manipulation instances requiring data-specific adjustments, which may result in more OOC instances in the synthetic data. Consequently, when the synthetic data distribution has more OOC instances as support, DisSim can better identify valuable instances based on gradient direction and magnitude, aligning the training distribution of the base MLLM closer to the target distribution. 2) Detecting image manipulation usually requires the base MLLMs have strong visual grounding capabilities, while most open-source MLLMs exhibit some systematic visual shortcomings because their pre-trained CLIP vision encoders might overlook visual details in images  Tong et al. ( 2024 ) . These observations suggest the importance of increasing diversity of misinformation data in synthetic datasets and enhancing the capability of vision encoder for better real-world MMD."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Results of MLLMs with different model scales and families. (.) encloses standard deviation.",
        "table": "S5.T2.st1.1",
        "footnotes": [],
        "references": [
            "For a comprehensive assessment of models performance across various sources and distributions of misinformation, we utilize two real-world fact-checking datasets collected from social media and fact-checking websites, both of which provide sufficient data for reliable evaluation. Appendix  A.2  contains more datasets details.",
            "We utilize the off-the-shelf CLIP model (ViT-L/14)  Radford et al. ( 2021b )  for feature extraction. We use the same prompt template for all models to ensure a fair comparison. Each data selection method empirically selects  750 750 750 750  synthetic instances from the synthetic dataset  D s subscript D s \\mathcal{D}_{s} caligraphic_D start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  to construct  D v subscript D v \\mathcal{D}_{v} caligraphic_D start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT  for fine-tuning, and we ensure the class distribution of selected set is balanced. The unlabeled validation set  D u subscript D u \\mathcal{D}_{u} caligraphic_D start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT  contains 5% instances randomly sampled from each test set. We conduct all experiments with the selection methods three times using different random seeds, and report the mean macro-F1 and standard deviation of each metric across these three runs. More details are provided in Appendix  B.1.2 .",
            "Specifically, we employ Principal Component Analysis (PCA)  Pearson ( 1901 )  to project the multimodal features of each instance from both two datasets. As depicted in Figure  2(a) , the MediaEval dataset exhibits a feature distribution characterized by multiple clusters, displaying OOD traits when compared across different clusters. This is attributed to MediaEval encompassing various events, each comprising posts with high internal density. This observation is further supported by the visualization of the top-10 events with the most instances in MediaEval in Figure  2(b) .",
            "Semantic similarity and Wasserstein distance, provide distinct criteria for evaluating the similarities of data points. Semantic similarity primarily assesses the resemblance of individual points in feature space, selecting data samples that closely match the representations of the target set. In contrast, Wasserstein distance excels at discerning similarities across different distributions, prioritizing the construction of the mean distribution among all probability measures to minimize the total transport cost, rather than matching individual clusters. To delve deeper, we perform data selection and evaluation for each of the top-3 events (as other events lack sufficient instances) and report the macro-F1 in Table  2(a) . We find that when selecting synthetic data for a specific event, DisSim outperforms SemSim by  5 % percent 5 5\\% 5 %  on average, aligning with our earlier hypothesis. These insights suggest the importance of employing appropriate criteria for data selection to enhance results on real-world data.",
            "In Table  2 , we report the results of SemSim and DisSim across different base models. Fine-tuning on a small number of synthetic data selected by both methods consistently improves the performance of the base models, including MLLMs with smaller model size (i.e., LLaVA-7B) and from different families (i.e., mPLUG-Owl2  Ye et al. ( 2023 ) ). The maximum F1 score improvement on all datasets for LLaVA-7B is  0.26 0.26 0.26 0.26  on average and for mPLUG-Owl2 is  0.19 0.19 0.19 0.19 , which indicates that the selected synthetic data is generalizably useful, enhancing the MMD performance of various MLLMs. Notably, as SemSim and DisSim are model-agnostic data selection approaches, the selected synthetic data can be reused without further selection costs, endowing efficiency to their deployment on real-world data."
        ]
    }
}