{
    "S3.T1": {
        "caption": "Table 1. The construction of semantically similar pair for pre-training.",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S3.T1.1.1.1.1\">modality</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.1.1.1.2\">semantically similar pair</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.1.2.1.1\">Image</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.2.1.2\">&#161;user&#8217;s image query, image of the purchased item&#191;</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" id=\"S3.T1.1.3.2.1\">Text</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T1.1.3.2.2\">&#161;user&#8217;s text query, title of the purchased item&#191;</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "In the context of e-commerce, a user\u2019s search query and subsequent purchase action often signifies a strong semantic similarity between the query and the purchased item. For example, if a user searches for an image of a pillow and subsequently purchases a pillow, this sequence of actions indicates that the two images (the queried image and the image of the purchased item) are semantically similar enough to satisfy the user\u2019s purchase intentions. Thus, as shown in Table\u00a01, in training the text encoder, we pair the text of the user\u2019s search query with the title of the item they ultimately purchased as the semantically similar pair. Similarly, for the image modality, user\u2019s image query (obtained from the image search scenario of Taobao) is paired with the image of the subsequent purchased item. This pairing strategy naturally captures the dimensions of semantic similarity that are most relevant to users in e-commerce scenarios, reflecting the elements that influence their purchasing decisions."
        ]
    },
    "S6.T2": {
        "caption": "Table 2. Pre-training performance of different methods",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S6.T2.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S6.T2.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S6.T2.1.1.1.1\">Method</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T2.1.1.1.2\">Acc@1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T2.1.1.1.3\">Acc@5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S6.T2.1.2.2.1\">CLIP-O</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.1.2.2.2\">0.2559</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.1.2.2.3\">0.3575</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.1.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S6.T2.1.3.3.1\">CLIP-E</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.1.3.3.2\">0.2952</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.1.3.3.3\">0.3917</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.1.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S6.T2.1.4.4.1\">SCL</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.1.4.4.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.1.4.4.2.1\">0.7474</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T2.1.4.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.1.4.4.3.1\">0.8850</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.1.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S6.T2.1.5.5.1\">&#160;&#160;&#8195;w/o Triplet Loss</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.1.5.5.2\">0.6957</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T2.1.5.5.3\">0.8604</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.1.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S6.T2.1.6.6.1\">&#160;&#160;&#8195;w/o Triplet Loss &amp; MoCo</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T2.1.6.6.2\">0.5760</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T2.1.6.6.3\">0.7590</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "To investigate how different pre-training tasks affect the quality of multimodal representations, we conducted a series of experiments.\nThe results, presented in Table\u00a02, offer two important observations. First, the proposed SCL pre-training method surpasses other semantic-similarity-agnostic methods, emphasizing the necessity of the semantic-aware learning.. Second, incorporating techniques like Momentum Contrast (MoCo)\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(He\net&#160;al<span class=\"ltx_text\">.</span>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2407.19467v1#bib.bib13\" title=\"\">2020</a>)</cite> and Triplet Loss&#160;\n(He\net\u00a0al., 2020) and Triplet Loss\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Schroff\net&#160;al<span class=\"ltx_text\">.</span>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2407.19467v1#bib.bib24\" title=\"\">2015</a>)</cite> further enhances the quality of the multimodal representations, demonstrating the choice of negative sample greatly impacts the performance.\n(Schroff\net\u00a0al., 2015) further enhances the quality of the multimodal representations, demonstrating the choice of negative sample greatly impacts the performance."
        ]
    },
    "S6.T3": {
        "caption": "Table 3. Overall performance on CTR prediction dataset.",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S6.T3.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S6.T3.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S6.T3.1.1.1.1\">Method</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T3.1.1.1.2\">GAUC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S6.T3.1.1.1.3\">AUC</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T3.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S6.T3.1.2.2.1\">ID-based Model</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T3.1.2.2.2\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T3.1.2.2.3\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T3.1.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S6.T3.1.3.3.1\">Vector</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.3.3.2\">+0.29%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.3.3.3\">+0.18%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T3.1.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S6.T3.1.4.4.1\">SimScore</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.4.4.2\">+0.77%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.4.4.3\">+0.40%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T3.1.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S6.T3.1.5.5.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S6.T3.1.5.5.1.1\">SimTier</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T3.1.5.5.2\">+0.96%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T3.1.5.5.3\">+0.59%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T3.1.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S6.T3.1.6.6.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S6.T3.1.6.6.1.1\">MAKE</span></th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.6.6.2\">+0.93%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T3.1.6.6.3\">+0.51%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T3.1.7.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" id=\"S6.T3.1.7.7.1\">\n<span class=\"ltx_text ltx_font_smallcaps\" id=\"S6.T3.1.7.7.1.1\">SimTier</span>+<span class=\"ltx_text ltx_font_smallcaps\" id=\"S6.T3.1.7.7.1.2\">MAKE</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S6.T3.1.7.7.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T3.1.7.7.2.1\">+1.25%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S6.T3.1.7.7.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T3.1.7.7.3.1\">+0.75%</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "In the CTR prediction dataset, we evaluate the proposed SimTier and MAKE against other methods. The overall results are shown in Table\u00a03, from which two observations can be noted. Firstly, SimTier and MAKE outperform other methods significantly.\nSecondly, the combination of SimTier and MAKE can further improve the performance, with a 1.25% increase in GAUC, 0.75% increase in AUC compared with the ID-based model. The above results demonstrate the effectiveness of the proposed methods on integrating multimodal representations into the ID-based model."
        ]
    }
}