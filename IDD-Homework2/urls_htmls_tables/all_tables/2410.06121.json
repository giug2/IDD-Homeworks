{
    "id_table_1": {
        "caption": "Table 1:  Subgraph retrieval results. We set beam size  k k k italic_k =3 for fair comparison with baseline.",
        "table": "S6.T1.3",
        "footnotes": [],
        "references": [
            "To simplify the task of generating relation chain given the complex relation names and to improve the representation of relations in language models, we treat each relation as a special token (relation ID) in the small language model. This transforms the subgraph retrieval task into predicting a sequence of relation IDs, which we define as the identifier of a subgraph (subgraph ID). As demonstrated in Figure  1 , our redefined task requires much  less  tokens (2 tokens vs. 35 tokens) as target, makes the training easier and increases the inference efficiency.  Our subgraph retriever, named Generative Subgraph Retriever (GSR), is trained jointly using two types of data: indexing data and subgraph retrieval data. In addition, we observe a large amount of noise inside the subgraph retrieval data used by most previous works  (Zhang et al.,  2022 ; Luo et al.,  2024b ) . To mitigate this issue, we proposed two data pruning methods to obtain denoised retreival data with  less  training samples.",
            "We build our GSR model on top of T5 models with size base, large and 3b, which consist of 220M, 770M and 3B parameters respectively. For beam search, we retrieve top-10 result and keep at most 3 valid relation chains for end-to-end evaluation 4 4 4 Some relation chains are not executable on Freebase. This means that there is no reasoning path starting from the topic entity with the decoded relation chain. Detailed results under different beam size settings can be found in Appendix  D.1 . For subgraph retrieval evaluation, we set the beam size to 3 for a fair comparison against RoG. For the LLM reader, we fine-tuned LLaMA2-chat-7B and LLaMA3-instruct-8B with QLora  (Dettmers et al.,  2023 )  using the unsloth library 5 5 5 https://github.com/unslothai/unsloth . More implementation details can be found in Appendix  A .",
            "Table  1  shows results on subgraph retrieval.",
            "Compared to RoG that performs relation chain retrieval with an LLM, the proposed GSR models achieve better performance across most metrics with less model parameters. This is observed even in the case of the smallest GSR-base models with approximate 30  \\times   less parameters than RoG, where we can obtain an average of +4.5% more recalled answers in the retrieved subgraph when training with the same retrieval data (raw). In addition, we can observe that within the GSR variant trained on the same retrieval data, larger model generally works better. We note a few cases in which GSR-large works slightly worse than GSR-base (e.g., in ours w/ GPT-selected data). We attribute this to the more invalid paths that are generated by GSR-large, which can be mitigated by setting beam size  k = 10 k 10 k=10 italic_k = 10  (cf. Figure  11  in Appendix  D.3 ).",
            "We analyse in detail the effectiveness of several variants of our proposed method, to answer the following research questions.   RQ1 : How to efficiently and effectively prompt LLMs with a retrieved subgraph? (Sec  7.1 )   RQ2 : How does the indexing data contribute to the performance on subgraph retrieval?   RQ3 : What is the performance upper-bound of the proposed methods? (Sec  7.3 )",
            "Table  10  shows extended end-to-end evaluation results under GSR models trained with different retrieval data.",
            "Table  11  shows the subgraph retrieval results under beam size  k k k italic_k =10. Noted that we still limit at most 3 valid paths per question.",
            "In this section, we investigate whether the proposed method can be generalized to other datasets and other KGs. In this part, we follow RoG  (Luo et al.,  2024b )  to use MetaQA datasets  (Zhang et al.,  2018 ) . Specifically, MetaQA is a KGQA dataset in the movie domain, based on the WikiMovies KG. To better show the effectiveness of our proposed method and for a fair comparison, we follow the same low resource setting from RoG by using only 1,000 randomly sampled data instances for training, without leveraging the gold annotated relation chain. In addition, we use the 3-hop subset of MetaQA, which requires 3 hops from topic entity to target entity. Table  12  shows the results, we use fine-tuned LLaMA2 as reader for fair comparison. We use Hits and F1 as the metrics to measure the performance of baseline (RoG) and proposed methods.",
            "Table  13  shows some subgraph ID generation sampled from WebQSP and CWQ test set. The first 2 is from WebQSP and the latter 2 is from CWQ. We can observe that GSR trained with raw data usually generate loop path which is helpless to the question and will bring unreliable recall (e.g., all person with same gender as topic entity will be retrieved in first example).",
            "Table  14  shows a success example sampled from the CWQ dataset. Specially, when represent retrieved subgraph as triple sets, we only present repeated triples once. For example, (Lou Seal, sports.mascot.team, San Francisco Giants) appears 6 times when represent retrieved subgraph as paths (first 6 paths), but only appear once when represent retrieved subgraph as triples (first triple).",
            "In this part, we analyse when our proposed method fail in KGQA task.    SR-based KGQA methods typically fail in following constraint in complex questions. Table  15  shows an example that our proposed GSR model fails in this case. The question has a constraint of answer entity, which is has a GDP eflator change rate of 2.32. However, since in building training samples, we only care about shortest path between topic entity and answer entity, which means that the GDP information will not be considered in training (if this question is in training set) and in inference our model will miss such information. To this end, the retrieved subgraph do have the answer entity, but not enough for the reader to answer the question."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Subgraph retrieval performance (Recall).  Inf. Time  is the inference time on WebQSP + CWQ.",
        "table": "S6.T2.1",
        "footnotes": [],
        "references": [
            "A KG subgraph  S  G  K  G S G K G \\mathcal{SG}\\subseteq\\mathcal{KG} caligraphic_S caligraphic_G  caligraphic_K caligraphic_G  is a subset of all triples in the original KG. Specially, in this work, we made a further constraint on the definition of subgraph, which we called path constrained subgraph 1 1 1 In other part of the paper, we use the term subgraph to refer to path constrained subgraph if not specially mentioned. . In short, path constrained subgraph is a set of special subgraph that can be identified with a simple identifier  ( e , c ) e c (e,c) ( italic_e , italic_c ) , where  e e e italic_e  is the entity and  c = r 1 , r 2 , ... , r n c subscript r 1 subscript r 2 ... subscript r n c=r_{1},r_{2},...,r_{n} italic_c = italic_r start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , italic_r start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT  is an ordered relation chain that indicates  n n n italic_n -hop reasoning. By moving from the entity  e e e italic_e  through the relation chain  c c c italic_c , we can identify the whole subgraph with the simple identifier.  The middle part of Figure  2  shows an example of identify subgraph with topic entity  James K. Polk  and relation chain ( office_holder ,  office_position_or_title ), which can be identified with identifier  [James K. Polk, office_holder, office_position_or_title] . In addition, for better flexibility, we decouple the topic entity  e e e italic_e  and relation chain  c c c italic_c  in the subgraph identifier, which means that the two part can be retrieved separately. We assume the topic entity  e e e italic_e  is known in this work for aligning other SR-based KGQA works  (Zhang et al.,  2022 ; Luo et al.,  2024b ) .",
            "We further trained a T5 model with the same input and output settings as RoG  (Luo et al.,  2024b )  using GPT-selected retrieval data to explore the benefits of our apporach. Table  2  shows the subgraph retrieval performance and inference time when inferring these two base-sized models. It is evident from the results that our GSR architecture achieves a +3.3% average improvement of recall while being 7.4 times more efficient during inference, demonstrating that incorporating less target tokens for the subgraph ID generation task brings in both effectiveness and efficiency.",
            "We evaluate the effectiveness of the indexing step. In addition, we explore different training strategies along the joint-training strategy (discussed in Section  4.2 ) for the GSR model using the indexing and retrieval data. 1) We directly finetune GSR with retrieval data without indexing data ( retrieval ); 2) we first pretrain GSR with indexing data, and then finetune it with retrieval data ( index => retrieval ). 3) we first pretrain GSR with indexing data, and then finetune it jointly with indexing and retrieval data ( index => joint ). To better highlight the importance of the indexing data, we further define low resource settings, by limiting the availability of retrieval data to 10%, 20%, 50% and 100%. We use GSR-base in this part for these experiments.",
            "In this section, we investigate whether the proposed method can be generalized to other datasets and other KGs. In this part, we follow RoG  (Luo et al.,  2024b )  to use MetaQA datasets  (Zhang et al.,  2018 ) . Specifically, MetaQA is a KGQA dataset in the movie domain, based on the WikiMovies KG. To better show the effectiveness of our proposed method and for a fair comparison, we follow the same low resource setting from RoG by using only 1,000 randomly sampled data instances for training, without leveraging the gold annotated relation chain. In addition, we use the 3-hop subset of MetaQA, which requires 3 hops from topic entity to target entity. Table  12  shows the results, we use fine-tuned LLaMA2 as reader for fair comparison. We use Hits and F1 as the metrics to measure the performance of baseline (RoG) and proposed methods."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  End-to-end KGQA performance with LLM reader. The GSR models are trained by gpt selected weakly supervised data. H@1 stands for Hits@1.  None  indicating that we use QLora finetuned LLM without any subgraph information for ablation studies.",
        "table": "S6.T3.1",
        "footnotes": [],
        "references": [
            "We adapt an jointly training strategy to train the GSR model, where the indexing data and the retrieval data is used at the same time for training the GSR model. Since we need to train the GSR model with both indexing data and retrieval data, we use a common strategy to distinguish the two task in the model level. Specifically, we design two special prefix token in front of the two different task,  [Index]  indicating this is a indexing task and  [Retrieval]  indicating this is a subgraph retrieval task. Figure  3  demonstrate how we train the GSR model.",
            "Compared to RoG that performs relation chain retrieval with an LLM, the proposed GSR models achieve better performance across most metrics with less model parameters. This is observed even in the case of the smallest GSR-base models with approximate 30  \\times   less parameters than RoG, where we can obtain an average of +4.5% more recalled answers in the retrieved subgraph when training with the same retrieval data (raw). In addition, we can observe that within the GSR variant trained on the same retrieval data, larger model generally works better. We note a few cases in which GSR-large works slightly worse than GSR-base (e.g., in ours w/ GPT-selected data). We attribute this to the more invalid paths that are generated by GSR-large, which can be mitigated by setting beam size  k = 10 k 10 k=10 italic_k = 10  (cf. Figure  11  in Appendix  D.3 ).",
            "Table  3  shows the end-to-end KGQA performance. Generally, on the WebQSP dataset, our best GSR-3b model with LLaMA 3 8B reader achieves the best performance among all SR-based methods. In particular, our best model achieves +5.2% improvement for Hits@1 compared to ToG with GPT-4 and +9.3% improvement for F1 compared to RoG. Even the GSR-base model with LLaMA 2 7B reader outperforms RoG which uses a finetuned LLaMA 2 7B model as both its retriever and reader, showing that the subgraph retrieval task can be effectively handled by small language models. Our systems performance is also on par with, if not exceeds, SOTA SP-based methods. The best GSR system manages to achieve higher Hits@1 score, even though it is trained using only weakly supervised data.",
            "The performance of our model on the more challenging CWQ dataset is still promising, where we achieve best performance among the SR-based baselines, except against ToG GPT-4 . This indicates that even though more complex questions would require better subgraph retrieval ability, the GSR model is still competitive enough to go head-to-head against much larger LLMs. However, the performance gap between all SR-based methods, including ours, against SOTA SP-based methods is still large, indicating that SP-based methods are still dominating complex KGQA. This can be attributed to the fact that SR-based methods, including the SOTA RoG, still struggle with following some specific constraints like less than limiting their overall performance. 6 6 6 More details can be found in Appendix  F.3 .",
            "We analyse in detail the effectiveness of several variants of our proposed method, to answer the following research questions.   RQ1 : How to efficiently and effectively prompt LLMs with a retrieved subgraph? (Sec  7.1 )   RQ2 : How does the indexing data contribute to the performance on subgraph retrieval?   RQ3 : What is the performance upper-bound of the proposed methods? (Sec  7.3 )",
            "Table  13  shows some subgraph ID generation sampled from WebQSP and CWQ test set. The first 2 is from WebQSP and the latter 2 is from CWQ. We can observe that GSR trained with raw data usually generate loop path which is helpless to the question and will bring unreliable recall (e.g., all person with same gender as topic entity will be retrieved in first example)."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  KGQA performance with different ways of prompting LLM with retrieved subgraph.",
        "table": "S7.T4.1",
        "footnotes": [],
        "references": [
            "We conduct an in-depth analysis to identify the most suitable format for representing a retrieved subgraph for the LLM reader. Table  4  shows how different ways of prompting an LLM reader  can affect the final performance. We observe that representing a retrieved subgraph as reasoning paths generally works better than representing it as triples. Prompting LLMs with triples is more compact. 7 7 7 Appendix  F  shows an example of why triples are more compact than paths.  As such it offers higher chances to cover an answer if the LLMs context window is limited. This is also demonstrated by the competitive performance on WebQSP. However, the compressed triples form can easily result in relevant triples, that belong to the same reasoning path, ending up far away in the context of this serialised representation, making the reasoning task harder for the LLM. The effect of this drawback is more evident on the more complex CWQ dataset, where the performance gap between the two approaches becomes larger. Nonetheless, depending on the use-case, using triples as input format can navigate a healthy trade-off between performance and efficiency.",
            "We evaluate the effectiveness of the indexing step. In addition, we explore different training strategies along the joint-training strategy (discussed in Section  4.2 ) for the GSR model using the indexing and retrieval data. 1) We directly finetune GSR with retrieval data without indexing data ( retrieval ); 2) we first pretrain GSR with indexing data, and then finetune it with retrieval data ( index => retrieval ). 3) we first pretrain GSR with indexing data, and then finetune it jointly with indexing and retrieval data ( index => joint ). To better highlight the importance of the indexing data, we further define low resource settings, by limiting the availability of retrieval data to 10%, 20%, 50% and 100%. We use GSR-base in this part for these experiments.",
            "Figure  4  shows the recall of GSR-base under different training settings according to the proportion of retrieval data that is trained on. Generally, training the GSR model with retrieval data brings in competitive performance when 100% of the data is used, but has a large gap with variants using the indexing data when the retrieval data is limitedthe gap increases as the data proportion decreases. With only the indexing step, the trained GSR model is able to perform the simplest subgraph retrieval task where the maximum relation hop is 1, with a recall of 42.54 on WebQSP and 21.18 on CWQ. Joint model training with indexing and retrieval data contributes to the best performance, especially on the harder CWQ dataset. This observation is in line with findings from  Allen-Zhu and Li ( 2024 ) , that including finetuning-relevant data instances during pre-training can benefit the model to make better use of the learned relation-level knowledge.",
            "Table  14  shows a success example sampled from the CWQ dataset. Specially, when represent retrieved subgraph as triple sets, we only present repeated triples once. For example, (Lou Seal, sports.mascot.team, San Francisco Giants) appears 6 times when represent retrieved subgraph as paths (first 6 paths), but only appear once when represent retrieved subgraph as triples (first triple)."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Subgraph retrieval performance under different KGs.",
        "table": "S7.T5.1",
        "footnotes": [],
        "references": [
            "To further investigate the upper bound of the proposed GSR method, we apply a full Freebase setting to fetch subgraph based on the subgraph ID from the full Freebase instead of the simplified subgraph (Freebase-SG). Table  5  shows the evaluation results. It is intuitive that the recall increases on both datasets when fetching subgraphs from the full Freebase. On the other hand, the precision on CWQ also increases, which can be attributed to the low question coverage of Freebase-SG that at least 20% of the questions will always have precision 0 even if the subgraph ID is correct.",
            "Table  9  shows the statistic of different types of retrieval data. While Figure  5  shows an example of how we process the weakly supervised data to get the filtered data and GPT-selected data.",
            "In this part, we analyse when our proposed method fail in KGQA task.    SR-based KGQA methods typically fail in following constraint in complex questions. Table  15  shows an example that our proposed GSR model fails in this case. The question has a constraint of answer entity, which is has a GDP eflator change rate of 2.32. However, since in building training samples, we only care about shortest path between topic entity and answer entity, which means that the GDP information will not be considered in training (if this question is in training set) and in inference our model will miss such information. To this end, the retrieved subgraph do have the answer entity, but not enough for the reader to answer the question."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Training time spent on single NVIDIA A100 80G GPU. GSR models are jointly trained with indexing data and GPT-selected retreival data. RoG+T5-base is trained with GPT-selected retrieval data only.",
        "table": "A1.T6.1",
        "footnotes": [],
        "references": [
            "Table  6  shows the training cost of both GSR retriever and LLM reader used in this work.",
            "In this part we discuss how the different beam size affect both subgraph retrieval performance and end-to-end KGQA performance. Noted that no matter how large the beam size  k k k italic_k  is, we always keep the max size of valid subgraph IDs to 3. Figure  6  shows the evaluation results when changing beam size from 3 to 10. From the result, we can observe a clear precision drop as well as little recall improvement on subgraph retrieval performance on WebQSP dataset. While for CWQ dataset, we can find the drop and improvement are more balanced. The seesaw of precision and recall then affect the end-to-end KGQA performance. On WebQSP dataset, when we increase  k k k italic_k , the performance is quite unstable, since we bring a lot more noise with lower precision. While on the more complex CWQ dataset, the performance improvement along with  k k k italic_k  is clear and consistent. This findings provide an insight that for the choice of  k k k italic_k  is conditioned on the complexity of the task, which we should choose a relatively low  k k k italic_k  for simple task while a larger  k k k italic_k  for hard task."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Inference cost on both datasets. Measured on single NVIDIA A100 80G GPU.",
        "table": "A1.T7.1",
        "footnotes": [],
        "references": [
            "We analyse in detail the effectiveness of several variants of our proposed method, to answer the following research questions.   RQ1 : How to efficiently and effectively prompt LLMs with a retrieved subgraph? (Sec  7.1 )   RQ2 : How does the indexing data contribute to the performance on subgraph retrieval?   RQ3 : What is the performance upper-bound of the proposed methods? (Sec  7.3 )",
            "Table  7  shows the inference cost of subgraph retrieval step on both datasets."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Statistic of original dataset.",
        "table": "A2.T8.1",
        "footnotes": [],
        "references": [
            "In this work, we use the WebQSP and CWQ datasets processed by  Luo et al. ( 2024b )  for all the experiments. Following previous studies  (Zhang et al.,  2022 ; Luo et al.,  2024b ) , we utilize the subgraph of Freebase (Freebase-SG) instead of the full Freebase for most of the experiments to enhance efficiency. Table  8  shows the statistic of the original dataset. Coverage refers to the question coverage rate of Freebase-SG, which is the proportion of questions where at least one answer entity appears in Freebase-SG."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Statistic of retrieval data.",
        "table": "A2.T9.1",
        "footnotes": [],
        "references": [
            "Table  9  shows the statistic of different types of retrieval data. While Figure  5  shows an example of how we process the weakly supervised data to get the filtered data and GPT-selected data."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:  End-to-end KGQA performance with LLM reader. H@1 stands for Hits@1.",
        "table": "A4.T10.1",
        "footnotes": [],
        "references": [
            "Table  10  shows extended end-to-end evaluation results under GSR models trained with different retrieval data."
        ]
    },
    "id_table_11": {
        "caption": "Table 11:  Subgraph retrieval results under beam size  k k k italic_k =10.",
        "table": "A4.T11.3",
        "footnotes": [],
        "references": [
            "Compared to RoG that performs relation chain retrieval with an LLM, the proposed GSR models achieve better performance across most metrics with less model parameters. This is observed even in the case of the smallest GSR-base models with approximate 30  \\times   less parameters than RoG, where we can obtain an average of +4.5% more recalled answers in the retrieved subgraph when training with the same retrieval data (raw). In addition, we can observe that within the GSR variant trained on the same retrieval data, larger model generally works better. We note a few cases in which GSR-large works slightly worse than GSR-base (e.g., in ours w/ GPT-selected data). We attribute this to the more invalid paths that are generated by GSR-large, which can be mitigated by setting beam size  k = 10 k 10 k=10 italic_k = 10  (cf. Figure  11  in Appendix  D.3 ).",
            "Table  11  shows the subgraph retrieval results under beam size  k k k italic_k =10. Noted that we still limit at most 3 valid paths per question."
        ]
    },
    "id_table_12": {
        "caption": "Table 12:  End-to-end KGQA performance on MetaQA 3-hop test set. All models are trained with 1,000 training samples.",
        "table": "A5.T12.1",
        "footnotes": [],
        "references": [
            "In this section, we investigate whether the proposed method can be generalized to other datasets and other KGs. In this part, we follow RoG  (Luo et al.,  2024b )  to use MetaQA datasets  (Zhang et al.,  2018 ) . Specifically, MetaQA is a KGQA dataset in the movie domain, based on the WikiMovies KG. To better show the effectiveness of our proposed method and for a fair comparison, we follow the same low resource setting from RoG by using only 1,000 randomly sampled data instances for training, without leveraging the gold annotated relation chain. In addition, we use the 3-hop subset of MetaQA, which requires 3 hops from topic entity to target entity. Table  12  shows the results, we use fine-tuned LLaMA2 as reader for fair comparison. We use Hits and F1 as the metrics to measure the performance of baseline (RoG) and proposed methods."
        ]
    },
    "id_table_13": {
        "caption": "Table 13:  Subgraph ID generation cases.",
        "table": "A6.T13.1",
        "footnotes": [],
        "references": [
            "Table  13  shows some subgraph ID generation sampled from WebQSP and CWQ test set. The first 2 is from WebQSP and the latter 2 is from CWQ. We can observe that GSR trained with raw data usually generate loop path which is helpless to the question and will bring unreliable recall (e.g., all person with same gender as topic entity will be retrieved in first example)."
        ]
    },
    "id_table_14": {
        "caption": "Table 14:  One success example sampled from CWQ test set.",
        "table": "A6.T14.36",
        "footnotes": [],
        "references": [
            "Table  14  shows a success example sampled from the CWQ dataset. Specially, when represent retrieved subgraph as triple sets, we only present repeated triples once. For example, (Lou Seal, sports.mascot.team, San Francisco Giants) appears 6 times when represent retrieved subgraph as paths (first 6 paths), but only appear once when represent retrieved subgraph as triples (first triple)."
        ]
    },
    "id_table_15": {
        "caption": "Table 15:  One error example due to the missing of constraint information sampled from CWQ test set. This is a common issue among sr-based KGQA works training with weakly supervised data.",
        "table": "A6.T15.12",
        "footnotes": [],
        "references": [
            "In this part, we analyse when our proposed method fail in KGQA task.    SR-based KGQA methods typically fail in following constraint in complex questions. Table  15  shows an example that our proposed GSR model fails in this case. The question has a constraint of answer entity, which is has a GDP eflator change rate of 2.32. However, since in building training samples, we only care about shortest path between topic entity and answer entity, which means that the GDP information will not be considered in training (if this question is in training set) and in inference our model will miss such information. To this end, the retrieved subgraph do have the answer entity, but not enough for the reader to answer the question."
        ]
    },
    "global_footnotes": [
        "In other part of the paper, we use the term subgraph to refer to path constrained subgraph if not specially mentioned.",
        "Appendix",
        "shows some examples of inverse relations.",
        "Some relation chains are not executable on Freebase. This means that there is no reasoning path starting from the topic entity with the decoded relation chain. Detailed results under different beam size settings can be found in Appendix",
        "More details can be found in Appendix",
        ".",
        "Appendix",
        "shows an example of why triples are more compact than paths."
    ]
}