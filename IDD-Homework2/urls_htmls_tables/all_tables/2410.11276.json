{
    "id_table_1": {
        "caption": "Table 1.  Expert EDA session and the interestingness measure maximized by each operation in the analysis sequence. Measures with relative score greater than 0.8 are mentioned",
        "table": "S2.T1.1.1",
        "footnotes": [],
        "references": [
            "To understand futher shortcomings of reward-based modeling, we analyze an expert EDA session using the interestingness measures described in Section  2 . Table  1  shows sequence of operations from an EDA session performed by an expert on the dataset mentioned above. At each step of the session, we calculate the scores of interestingness measures and examine which of them are optimized at each step. The scores are normalized  (Somech et al . ,  2019 )  in the range of  [ 0 , 1 ] 0 1 [0,1] [ 0 , 1 ]  for comparison. At every step, measures with a normalized score exceeding 0.8 are shown. We observe that:",
            "Some actions optimize the Peculiarity and Readability scores, which are not part of the reward definition in  1 .",
            "The overall architecture of the proposed approach ILAEDA is provided in Figure  1 . Our main goal is to learn a policy directly from the expert EDA sessions (from here on, also referred to as  expert trajectories ) using Imitation Learning, allowing the system to learn to imitate a human expert. We have a set of expert trajectories  E = {  1 ,  2 , ...   N } E subscript  1 subscript  2 ... subscript  N \\mathcal{E}=\\{\\tau_{1},\\tau_{2},\\dots\\tau_{N}\\} caligraphic_E = { italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... italic_ start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT }  where each   i subscript  i \\tau_{i} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is a sequence of state-action pairs  { ( s 1 , a 1 ) , ( s 2 , a 2 )  ...  ( s T , a T ) } subscript s 1 subscript a 1 subscript s 2 subscript a 2 ... subscript s T subscript a T \\{(s_{1},a_{1}),(s_{2},a_{2})\\dots(s_{T},a_{T})\\} { ( italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , ( italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) ... ( italic_s start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) }  where each  s i  S subscript s i S s_{i}\\in\\mathcal{S} italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  caligraphic_S  and  a i  A subscript a i A a_{i}\\in\\mathcal{A} italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  caligraphic_A . We assume that expert actions come from an underlying expert policy   E subscript  E \\pi_{E} italic_ start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT , i.e.  a t   E (  | s t ) ,  ( s t , a t )   ,    E a_{t}\\sim\\pi_{E}(\\cdot|s_{t}),\\;\\forall\\,(s_{t},a_{t})\\in\\tau,\\;\\forall\\tau\\in% \\mathcal{E} italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  italic_ start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT (  | italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ,  ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  italic_ ,  italic_  caligraphic_E . ILAEDA uses three neural networks: Policy    subscript   \\pi_{\\theta} italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT , Value  V  subscript V italic- V_{\\phi} italic_V start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  and Discriminator  D w subscript D w D_{w} italic_D start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT  parameterized by   ,  , w  italic- w \\theta,\\phi,w italic_ , italic_ , italic_w  respectively. The policy and discriminator networks undergo training at alternate steps. The discriminator is trained to minimize the loss:",
            "We filter the expert sessions from the Synthetic Datasets (refer to section  5.1.2 ) to obtain subsets where the sessions maximize one particular interestingness metric over others. We consider A-INT, Diversity, and Readability as the metrics for this analysis. To filter the expert sessions, we classify each session into one of three categories. (1)  Category 1:  Maximizing A-INT more than others. (2)  Category 2:  Maximizing Diversity more than others. (3)  Category 3:  Maximizing Readability more than others.",
            "Algorithm  1  gives a detailed description of the ILAEDA procedure."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.  Comparison of ILAEDA with baselines on Cyber Security (CS 1-4) and Synthetic Datasets (SD 6-7)",
        "table": "S2.T1.1.1.1.1.1.1",
        "footnotes": [],
        "references": [
            "Here,  r i  n  t , r d  i  v subscript r i n t subscript r d i v r_{int},\\;r_{div} italic_r start_POSTSUBSCRIPT italic_i italic_n italic_t end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT italic_d italic_i italic_v end_POSTSUBSCRIPT  are the A-INT and Diversity score respectively (refer Section  2 ).  r c  o  h subscript r c o h r_{coh} italic_r start_POSTSUBSCRIPT italic_c italic_o italic_h end_POSTSUBSCRIPT  is the output of a binary classifier trained using rules based on the Coherence score.   1 ,  2 subscript  1 subscript  2 \\lambda_{1},\\lambda_{2} italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  are constants that are tuned to calibrate the reward. As described before, some of the rules defined for the Coherence reward include:",
            "To understand futher shortcomings of reward-based modeling, we analyze an expert EDA session using the interestingness measures described in Section  2 . Table  1  shows sequence of operations from an EDA session performed by an expert on the dataset mentioned above. At each step of the session, we calculate the scores of interestingness measures and examine which of them are optimized at each step. The scores are normalized  (Somech et al . ,  2019 )  in the range of  [ 0 , 1 ] 0 1 [0,1] [ 0 , 1 ]  for comparison. At every step, measures with a normalized score exceeding 0.8 are shown. We observe that:",
            "Table  2  shows a performance comparison between ILAEDA and the baselines on the evaluation metrics discussed in Section  5.3 . We see that our method overall delivers a significantly better performance than the baselines. The results are all the more significant because, in each setting, ILAEDA is evaluated on datasets that it has not seen during training. In contrast, ATENA is evaluated on the same datasets it trains on. This indicates that ILAEDA generates better EDA sessions and has better generalization capability.",
            "We filter the expert sessions from the Synthetic Datasets (refer to section  5.1.2 ) to obtain subsets where the sessions maximize one particular interestingness metric over others. We consider A-INT, Diversity, and Readability as the metrics for this analysis. To filter the expert sessions, we classify each session into one of three categories. (1)  Category 1:  Maximizing A-INT more than others. (2)  Category 2:  Maximizing Diversity more than others. (3)  Category 3:  Maximizing Readability more than others.",
            "We do this classification for all Synthetic datasets 1-5 sessions and obtain three subsets. We then trained an ILAEDA model on these subsets using the default settings described before. We use the trained models to generate 100 EDA sessions each on Synthetic Dataset 7. For each generated session, we compare the median normalized score obtained for each metric across all the steps in the trajectory of that session. If the median normalized score is highest for A-INT, we say this session maximizes A-INT. Similarly, a session can also be classified as maximizing Readability/Diversity more than other metrics. Figure  2  shows the distribution of most-maximized metrics for sessions generated using each of the three ILAEDA models. We see that in all three cases, most sessions maximize that interestingness metric more than others, for which the training sessions used to train the model generating those sessions were filtered. A model trained on sessions filtered for maximizing A-INT more than others tends to maximize A-INT more than other metrics when tested on an unseen dataset. This analysis strongly suggests that our method can capture specific underlying interestingness measures being optimized for in a set of expert demonstrations, and the trained model can also optimize for those measures on unseen datasets."
        ]
    },
    "id_table_3": {
        "caption": "Table 4.  Ablation study scores for Cyber Security datasets. The low performance of ablated models validates our choice of penalties and pretraining using BC.",
        "table": "S4.T2.1",
        "footnotes": [],
        "references": [
            "This is a collection of four mutually exclusive datasets  (Spitzner,  2003 ) , which we will refer to as the  cyber security datasets . Expert EDA sessions have been generated in two ways for each of the four datasets:  (1) REACT Dataset:   56 cyber security analysts were asked to explore the four cyber security datasets. Each dataset may reveal a unique security event, and analysts were asked to discover the specifics of the underlying security event for each dataset using as many as necessary actions. The REACT dataset is a collection of EDA session traces obtained from these experienced analysts data exploration on the cyber security datasets and was curated by authors of  (Milo and Somech,  2018 ) .  (2) Gold Dataset:  The gold-standard EDA sessions are generated from walkthrough documents from cyber-security experts, which guide the EDA process for viewers and highlight essential insights in the dataset. Authors curate this from  (Bar El et al . ,  2020b ) . Table  3  shows the number of EDA trajectories found for each of these datasets. We discuss in the following sections how we use these different trajectories for training and testing purposes.",
            "Table  2  shows a performance comparison between ILAEDA and the baselines on the evaluation metrics discussed in Section  5.3 . We see that our method overall delivers a significantly better performance than the baselines. The results are all the more significant because, in each setting, ILAEDA is evaluated on datasets that it has not seen during training. In contrast, ATENA is evaluated on the same datasets it trains on. This indicates that ILAEDA generates better EDA sessions and has better generalization capability.",
            "We trained a version of ILAEDA without the coherence penalties defined in equation  3  and kept all other configurations the same to study the impact of our design choice. We see that this model underperforms in comparison to ILAEDA trained with coherence penalties (Table  4 ). We explain the lower scores by inspecting the generated EDA session (Table  5 ). The EDA views 2 and 3 are identical because the same GROUP operators have been repeated consecutively. Such actions are penalized in our original model."
        ]
    },
    "id_table_4": {
        "caption": "Table 5.  EDA session without penalty score on Cyber Security dataset 1. Spurious repetition (red colored) of actions is seen.",
        "table": "S5.T3.1",
        "footnotes": [],
        "references": [
            "We trained a version of ILAEDA without the coherence penalties defined in equation  3  and kept all other configurations the same to study the impact of our design choice. We see that this model underperforms in comparison to ILAEDA trained with coherence penalties (Table  4 ). We explain the lower scores by inspecting the generated EDA session (Table  5 ). The EDA views 2 and 3 are identical because the same GROUP operators have been repeated consecutively. Such actions are penalized in our original model."
        ]
    },
    "id_table_5": {
        "caption": "Table 6.  Trajectory generated by the ILAEDA for Cyber Security dataset 1 and normalized values attained by different interestingness measures for each action.",
        "table": "S6.T4.1",
        "footnotes": [],
        "references": [
            "Table  2  shows a performance comparison between ILAEDA and the baselines on the evaluation metrics discussed in Section  5.3 . We see that our method overall delivers a significantly better performance than the baselines. The results are all the more significant because, in each setting, ILAEDA is evaluated on datasets that it has not seen during training. In contrast, ATENA is evaluated on the same datasets it trains on. This indicates that ILAEDA generates better EDA sessions and has better generalization capability.",
            "We trained a version of ILAEDA without the coherence penalties defined in equation  3  and kept all other configurations the same to study the impact of our design choice. We see that this model underperforms in comparison to ILAEDA trained with coherence penalties (Table  4 ). We explain the lower scores by inspecting the generated EDA session (Table  5 ). The EDA views 2 and 3 are identical because the same GROUP operators have been repeated consecutively. Such actions are penalized in our original model.",
            "We filter the expert sessions from the Synthetic Datasets (refer to section  5.1.2 ) to obtain subsets where the sessions maximize one particular interestingness metric over others. We consider A-INT, Diversity, and Readability as the metrics for this analysis. To filter the expert sessions, we classify each session into one of three categories. (1)  Category 1:  Maximizing A-INT more than others. (2)  Category 2:  Maximizing Diversity more than others. (3)  Category 3:  Maximizing Readability more than others."
        ]
    },
    "id_table_6": {
        "caption": "Table 7.  Trajectory generated by ATENA model for Cyber Security Dataset 1 and normalized values attained by different interestingness measures for each action.",
        "table": "S6.T5.1",
        "footnotes": [],
        "references": [
            "In Table  6 , we show a session generated by our model on Cyber Security dataset 1 and normalized scores obtained on different interestingness metrics. We highlight scores greater than 0.7. Throughout this session, generated views obtained high readability, diversity, and peculiarity scores. This indicates that our model can effectively filter the dataset and present important tuples to users. Views presented are diverse, showing different insights from the dataset."
        ]
    },
    "id_table_7": {
        "caption": "Table 8.  Trajectory distribution for synthetic datasets",
        "table": "S6.T6.1.1",
        "footnotes": [],
        "references": [
            "Table  7  shows a session generated using ATENA on the same dataset. We observe that views produced have high A-INT and diversity scores. This is because these scores were used as reward signals to train ATENA. In comparison, each action taken by our model shows different scores being highlighted, which shows that our model can capture a wider set of interestingness measures than ATENA."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "S6.T7.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_9": {
        "caption": "",
        "table": "A3.EGx1",
        "footnotes": [],
        "references": []
    },
    "id_table_10": {
        "caption": "",
        "table": "A3.T8.1",
        "footnotes": [],
        "references": []
    }
}