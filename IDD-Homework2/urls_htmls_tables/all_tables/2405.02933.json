{
    "S2.Ex1": {
        "caption": null,
        "table": null,
        "footnotes": [],
        "references": []
    },
    "S2.T1.1": {
        "caption": "The result of RD Method for Zh-Fr, Zh-De, Zh-Cs translation tasks on Multi30k dataset. ",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S2.T1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S2.T1.1.1.1.1\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.1.1.1\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\" id=\"S2.T1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.1.2.1\">Zh-Fr</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\" id=\"S2.T1.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.1.3.1\">Zh-De</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S2.T1.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.1.4.1\">Zh-Cs</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.1.2.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.2.2.1.1\">BLEU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.1.2.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.2.2.2.1\">chrF</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.1.2.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.2.2.3.1\">BLEU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.1.2.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.2.2.4.1\">chrF</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.1.2.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.2.2.5.1\">BLEU</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S2.T1.1.2.2.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.2.2.6.1\">chrF</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.1.3.3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.3.3.1.1\">Bilingual</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.1.3.3.2\">20.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.1.3.3.3\">47.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.1.3.3.4\">10.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.1.3.3.5\">38.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S2.T1.1.3.3.6\">7.52</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S2.T1.1.3.3.7\">27.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.4.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.4.4.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.4.4.1.1\">Aquila2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.4.4.2\">19.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.4.4.3\">49.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.4.4.4\">10.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.4.4.5\">43.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.4.4.6\">8.75</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S2.T1.1.4.4.7\">36.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.5.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.5.5.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.5.5.1.1\">LLaMA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.5.5.2\">25.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.5.5.3\">53.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.5.5.4\">15.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.5.5.5\">49.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S2.T1.1.5.5.6\">10.08</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S2.T1.1.5.5.7\">38.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.6.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S2.T1.1.6.6.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.6.6.1.1\">RD (Aquila2+LLaMA)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S2.T1.1.6.6.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.6.6.2.1\">27.36</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S2.T1.1.6.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.6.6.3.1\">55.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S2.T1.1.6.6.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.6.6.4.1\">17.87</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S2.T1.1.6.6.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.6.6.5.1\">49.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S2.T1.1.6.6.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.6.6.6.1\">13.44</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"S2.T1.1.6.6.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.6.6.7.1\">39.1</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "The experimental results on Multi30k dataset for Zh-Fr, Zh-De, and Zh-Cs are presented in Table 1. From the table, we observe that our RD method achieves the best results. When comparing the last three rows with the first row, which represents the bilingual transformer approach, we find that utilizing large models with the same parallel corpus outperforms training from scratch. This indicates that the language alignment capability of the large models is indeed utilized during training, even though they were pretrained only on monolingual data. The results of fine-tuning large models specialized in one language (rows 2 and 3) show that these models still have some limitations in completing translation tasks. Additionally, we have also discovered that LLMs specialized in the target language tend to exhibit superior performance in translation tasks. Our concatenation method also surpasses the performance of fine-tuning with a single large model, demonstrating the need for pretraining large models on both the source and target languages to achieve better translation performance and this further validates the effectiveness of our proposed concatenation method.\n"
        ]
    },
    "S2.Ex2": {
        "caption": null,
        "table": null,
        "footnotes": [],
        "references": []
    },
    "S2.Ex3": {
        "caption": null,
        "table": null,
        "footnotes": [],
        "references": []
    },
    "S3.T2.1": {
        "caption": "The BLEU scores of different finetune settings on Multi30k dataset.",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T2.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S3.T2.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.1.1\">Aquila2 (ZH)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S3.T2.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.2.1\">LLaMA (FR)</span></th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.3.1\">Zh-Fr</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T2.1.2.1.1\">Not Finetune</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T2.1.2.1.2\">Not Finetune</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S3.T2.1.2.1.3\">25.94</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.1.3.2.1\">Not Finetune</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.1.3.2.2\">Finetune</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.1.3.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.3.2.3.1\">27.36</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.1.4.3.1\">Finetune</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.1.4.3.2\">Not Finetune</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T2.1.4.3.3\">23.37</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S3.T2.1.5.4.1\">Finetune</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S3.T2.1.5.4.2\">Finetune</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"S3.T2.1.5.4.3\">26.88</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "Our approach involves training a mapping layer to connect two large models, but during training, we also need to consider whether to adjust the parameters of the large models. As shown in Table 2, we test the translation performance of Zh-Fr under different finetuning conditions on Multi30K datasets and find that simultaneously finetuning the parameters of the second large model (i.e., the one specialized in the target language) yield better results. On the other hand, fine-tuning the parameters of the first large model has a less significant impact. For finetuning, we utilized the efficient finetuning method known as LORA due to its higher efficiency.\n"
        ]
    },
    "S3.T3.1": {
        "caption": "The BLEU scores associated with varying WikiMatrix dataset sizes.",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T3.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S3.T3.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.1.1\">#Dataset</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.2.1\">2W</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.3.1\">3W</span></th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.4.1\">4W</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T3.1.2.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S3.T3.1.2.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.2.1.1.1\">RD</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.2.1.2\">11.79</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.2.1.3\">12.98</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S3.T3.1.2.1.4\">13.64</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.3.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S3.T3.1.3.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.3.2.1.1\">#Dataset</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.3.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.3.2.2.1\">5W</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.3.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.3.2.3.1\">6W</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S3.T3.1.3.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.3.2.4.1\">7W</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.4.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" id=\"S3.T3.1.4.3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.4.3.1.1\">RD</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T3.1.4.3.2\">14.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T3.1.4.3.3\">15.44</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T3.1.4.3.4\">15.52</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "As presented in Table 3, we conducted Zh-Fr translation experiments using training sets of different sizes on WikiMatrix datasets. The findings reveal that on the WikiMatrix dataset, training with approximately 60,000 data points is adequate for training the mapping layer. This requirement is considerably smaller compared to the dataset size typically needed by traditional bilingual methods. Moreover, our method surpasses these methods in performance.\n"
        ]
    },
    "A1.T4.1": {
        "caption": "The BLEU score of different mapping method on Multi30k dataset.",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A1.T4.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T4.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"A1.T4.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T4.1.1.1.1.1\">Mapping Method</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A1.T4.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T4.1.1.1.2.1\">FC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A1.T4.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T4.1.1.1.3.1\">CA</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"A1.T4.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T4.1.1.1.4.1\">CA-Q</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T4.1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" id=\"A1.T4.1.2.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A1.T4.1.2.2.1.1\">Zh-Fr</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"A1.T4.1.2.2.2\">27.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"A1.T4.1.2.2.3\">11.80</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" id=\"A1.T4.1.2.2.4\">17.92</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "We also conducted Zh-Fr translation experiments on the Multi30k dataset, and the experimental results are presented in Table 4.\n"
        ]
    }
}