{
    "S5.T1": {
        "caption": "TABLE I: Contribution of Uncertainty in VQA",
        "table": "<table id=\"S5.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Models</span></th>\n<th id=\"S5.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">All</span></th>\n<th id=\"S5.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Y/N</span></th>\n<th id=\"S5.T1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">Num</span></th>\n<th id=\"S5.T1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.1.1.5.1\" class=\"ltx_text ltx_font_bold\">Oth</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">LSTM Q+I<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">8</a>]</cite>\n</th>\n<td id=\"S5.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">53.7</td>\n<td id=\"S5.T1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">78.9</td>\n<td id=\"S5.T1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">35.2</td>\n<td id=\"S5.T1.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">36.4</td>\n</tr>\n<tr id=\"S5.T1.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">SAN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib29\" title=\"\" class=\"ltx_ref\">29</a>]</cite>\n</th>\n<td id=\"S5.T1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">58.7</td>\n<td id=\"S5.T1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">79.3</td>\n<td id=\"S5.T1.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\">36.6</td>\n<td id=\"S5.T1.1.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\">46.1</td>\n</tr>\n<tr id=\"S5.T1.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">LSTM Q+I+Uncertainty</th>\n<td id=\"S5.T1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">54.1</td>\n<td id=\"S5.T1.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">79.5</td>\n<td id=\"S5.T1.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\">35.3</td>\n<td id=\"S5.T1.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_r\">37.0</td>\n</tr>\n<tr id=\"S5.T1.1.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">SAN+Uncertainty</th>\n<td id=\"S5.T1.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">59.5</td>\n<td id=\"S5.T1.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">80.1</td>\n<td id=\"S5.T1.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_r\">36.4</td>\n<td id=\"S5.T1.1.5.4.5\" class=\"ltx_td ltx_align_center ltx_border_r\">46.7</td>\n</tr>\n<tr id=\"S5.T1.1.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\">SAN + P-GCA (ours)</th>\n<td id=\"S5.T1.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">60.4</td>\n<td id=\"S5.T1.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">80.7</td>\n<td id=\"S5.T1.1.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">36.8</td>\n<td id=\"S5.T1.1.6.5.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">47.9</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "We observe that uncertainty in the answer prediction is too high when we have noise in the input data or the model. For example, in the baseline model if you add slight noise in either the input image or question, the baseline VQA model predicts wrong output, that is the answer may change from “Napkin” to “Paper” or “Wii” to “Tennis” as shown in the figure-4. We minimise uncertainty in the answer prediction using the Gradient certainty method as mentioned in section-IV-A and obtain result as shown in the right side of figure-4. As can be observed, the predictions for our model are not only correct, and they are also more robust with the second (wrong) answer having far lesser scores. In table-I, we start with a simple VQA model (LSTM Q+I)[8], in which we obtain image feature using a pretrained CNN model (VGG-16) and a question feature using standard LSTM network. We then use a fused network to bring these two embedding features close to each other and predict the answer. The second method we tried was with a stacked attention network (SAN) [29]. We train our VQA model using uncertainty loss, and we observe that there is an increase in accuracy in VQA. We further improve our model accuracy and attention map using the Gradient certainty based attention map mechanism. The results are shown in the last row of the table-I. So in this work, we thoroughly analyse uncertainty-CAM with attention and without attention for VQA models. The observations that we can draw from our analysis are as follows: a) A baseline VQA model (without attention) improves marginally (0.4%) by incorporating uncertainty minimization loss. b) The improvement on a VQA model with attention (SAN) by using uncertainty is more significant (0.8%). c) Jointly considering attention and uncertainty through our proposed model on a stacked attention network (SAN) model results in further improvement. The resulting improvement is 1.7%."
        ]
    },
    "S7.T2": {
        "caption": "TABLE II: Ablation analysis for Open-Ended VQA1.0 accuracy on test-dev",
        "table": "<table id=\"S7.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S7.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S7.T2.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S7.T2.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Models</span></th>\n<td id=\"S7.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S7.T2.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">All</span></td>\n<td id=\"S7.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S7.T2.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Yes/No</span></td>\n<td id=\"S7.T2.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S7.T2.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">Number</span></td>\n<td id=\"S7.T2.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S7.T2.1.1.1.5.1\" class=\"ltx_text ltx_font_bold\">Others</span></td>\n</tr>\n<tr id=\"S7.T2.1.2.2\" class=\"ltx_tr\">\n<th id=\"S7.T2.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Baseline</th>\n<td id=\"S7.T2.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">63.8</td>\n<td id=\"S7.T2.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">82.2</td>\n<td id=\"S7.T2.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">37.3</td>\n<td id=\"S7.T2.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">54.2</td>\n</tr>\n<tr id=\"S7.T2.1.3.3\" class=\"ltx_tr\">\n<th id=\"S7.T2.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">VE</th>\n<td id=\"S7.T2.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">64.1</td>\n<td id=\"S7.T2.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">82.3</td>\n<td id=\"S7.T2.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\">37.2</td>\n<td id=\"S7.T2.1.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r\">54.3</td>\n</tr>\n<tr id=\"S7.T2.1.4.4\" class=\"ltx_tr\">\n<th id=\"S7.T2.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">UDL</th>\n<td id=\"S7.T2.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">64.4</td>\n<td id=\"S7.T2.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">82.6</td>\n<td id=\"S7.T2.1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_r\">37.2</td>\n<td id=\"S7.T2.1.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_r\">54.5</td>\n</tr>\n<tr id=\"S7.T2.1.5.5\" class=\"ltx_tr\">\n<th id=\"S7.T2.1.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">AUL</th>\n<td id=\"S7.T2.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\">64.7</td>\n<td id=\"S7.T2.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\">82.9</td>\n<td id=\"S7.T2.1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_r\">37.4</td>\n<td id=\"S7.T2.1.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_r\">54.6</td>\n</tr>\n<tr id=\"S7.T2.1.6.6\" class=\"ltx_tr\">\n<th id=\"S7.T2.1.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">PUL</th>\n<td id=\"S7.T2.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\">64.9</td>\n<td id=\"S7.T2.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\">83.0</td>\n<td id=\"S7.T2.1.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_r\">37.5</td>\n<td id=\"S7.T2.1.6.6.5\" class=\"ltx_td ltx_align_center ltx_border_r\">54.6</td>\n</tr>\n<tr id=\"S7.T2.1.7.7\" class=\"ltx_tr\">\n<th id=\"S7.T2.1.7.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">UDL+VE</th>\n<td id=\"S7.T2.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">64.8</td>\n<td id=\"S7.T2.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">82.8</td>\n<td id=\"S7.T2.1.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">37.4</td>\n<td id=\"S7.T2.1.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">54.5</td>\n</tr>\n<tr id=\"S7.T2.1.8.8\" class=\"ltx_tr\">\n<th id=\"S7.T2.1.8.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">AUL+VE</th>\n<td id=\"S7.T2.1.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_r\">65.0</td>\n<td id=\"S7.T2.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r\">83.3</td>\n<td id=\"S7.T2.1.8.8.4\" class=\"ltx_td ltx_align_center ltx_border_r\">37.8</td>\n<td id=\"S7.T2.1.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_r\">54.7</td>\n</tr>\n<tr id=\"S7.T2.1.9.9\" class=\"ltx_tr\">\n<th id=\"S7.T2.1.9.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">PUL+ VE</th>\n<td id=\"S7.T2.1.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_r\">65.3</td>\n<td id=\"S7.T2.1.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r\">83.3</td>\n<td id=\"S7.T2.1.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_r\">37.9</td>\n<td id=\"S7.T2.1.9.9.5\" class=\"ltx_td ltx_align_center ltx_border_r\">54.9</td>\n</tr>\n<tr id=\"S7.T2.1.10.10\" class=\"ltx_tr\">\n<th id=\"S7.T2.1.10.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">AUL +UDL</th>\n<td id=\"S7.T2.1.10.10.2\" class=\"ltx_td ltx_align_center ltx_border_r\">65.6</td>\n<td id=\"S7.T2.1.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r\">83.3</td>\n<td id=\"S7.T2.1.10.10.4\" class=\"ltx_td ltx_align_center ltx_border_r\">37.6</td>\n<td id=\"S7.T2.1.10.10.5\" class=\"ltx_td ltx_align_center ltx_border_r\">55.0</td>\n</tr>\n<tr id=\"S7.T2.1.11.11\" class=\"ltx_tr\">\n<th id=\"S7.T2.1.11.11.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">PUL + UDL</th>\n<td id=\"S7.T2.1.11.11.2\" class=\"ltx_td ltx_align_center ltx_border_r\">65.9</td>\n<td id=\"S7.T2.1.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_r\">83.7</td>\n<td id=\"S7.T2.1.11.11.4\" class=\"ltx_td ltx_align_center ltx_border_r\">37.8</td>\n<td id=\"S7.T2.1.11.11.5\" class=\"ltx_td ltx_align_center ltx_border_r\">55.2</td>\n</tr>\n<tr id=\"S7.T2.1.12.12\" class=\"ltx_tr\">\n<th id=\"S7.T2.1.12.12.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">A-GCA (ours)</th>\n<td id=\"S7.T2.1.12.12.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">66.3</td>\n<td id=\"S7.T2.1.12.12.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">84.2</td>\n<td id=\"S7.T2.1.12.12.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">38.0</td>\n<td id=\"S7.T2.1.12.12.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">55.5</td>\n</tr>\n<tr id=\"S7.T2.1.13.13\" class=\"ltx_tr\">\n<th id=\"S7.T2.1.13.13.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\">P-GCA (ours)</th>\n<td id=\"S7.T2.1.13.13.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S7.T2.1.13.13.2.1\" class=\"ltx_text ltx_font_bold\">66.5</span></td>\n<td id=\"S7.T2.1.13.13.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S7.T2.1.13.13.3.1\" class=\"ltx_text ltx_font_bold\">84.7</span></td>\n<td id=\"S7.T2.1.13.13.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S7.T2.1.13.13.4.1\" class=\"ltx_text ltx_font_bold\">38.4</span></td>\n<td id=\"S7.T2.1.13.13.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S7.T2.1.13.13.5.1\" class=\"ltx_text ltx_font_bold\">55.9</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Our proposed GCA model’s loss consists of undistorted and distorted loss. The undistorted loss is the Standard Cross-Entropy (SCE) loss. The distorted loss consists of uncertain loss (either aleatoric uncertainty loss (AUL) or predictive uncertainly loss (PUL)), Variance Equalizer (VE) loss, and Uncertainty Distorted loss (UDL). In the first block of the Table- II, we report the results when these losses are used individually. (Only SCE loss is there in the Baseline). We use a variant of the MCB [6] model as our baseline method. As seen, PUL, when used individually, outperforms the other 4. This could be attributed to PUL guiding the model to minimize both the data and the model uncertainty. The second block of the Table- II depicts the results when we tried while combining two different individual losses. The model variant, which is guided using the combination of PUL and UDL loss, performs best among the five variants. Then finally, after combining (AUL+UDL+VE+SCE), denoting it as A-GCA model and combining (PUL+UDL+VE+SCE), indicating it as P-GCA, we report an improvement of around 2.5% and 2.7% accuracy score respectively.",
            "We compare attention maps produced by our proposed GCA model, and it’s variants with the base model and reports them in Table-IV. Rank correlation and EMD scores are calculated for the produced attention map against human-annotated attention (HAT) maps  [34]. In the table, as we approach the best-proposed GCA model, Rank correlation (RC) is increasing. EMD is also decreasing (Lower the better) as we move towards GCA. To verify our intuition, that we can learn better attention masks by minimizing the uncertainty present in the attention mask; we start with VE and observe that both rank correlation and answer accuracy increase by 0.42 and 0.3 % from baseline, respectively. We also observe that with UDL, AUL, and PUL based loss minimization technique, both RC and EMD improves, as shown in the Table- IV.\nAleatoric-GCA (A-GCA) improves 5.21% in terms of RC and 2.5% in terms of accuracy. Finally, the proposed Predictive-GCA (P-GCA), which is modeled to consider both data and the model uncertainty, improves the RC by 5.51% and accuracy by 2.7% as shown in the Table- IV and Table- II. Since HAT maps are only available for the VQA-v1 dataset, thus, this ablation analysis has been performed only for VQA-v1. We also providing SOTA results for VQA-v1 and VQA-v2 dataset as shown in Table- VII and Table- VI respectively.\nAlso, we compare with our gradient certainty explanation with human explanation present in the VQA-v2 dataset for the various model, as mentioned in Table- III. This human explanation mask only available for the VQA-v2 dataset. We observe that our attention (P-GCA) mask performs better than others as well."
        ]
    },
    "S7.T3": {
        "caption": "TABLE III: Rank Correlation for explanation mask in VQA-X  [46] data with our explanation mask using Grad-Cam.",
        "table": "<table id=\"S7.T3.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S7.T3.2.2\" class=\"ltx_tr\">\n<th id=\"S7.T3.2.2.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S7.T3.2.2.3.1\" class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th id=\"S7.T3.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S7.T3.1.1.1.1\" class=\"ltx_text ltx_font_bold\">RC(<math id=\"S7.T3.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S7.T3.1.1.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S7.T3.1.1.1.1.m1.1.1\" xref=\"S7.T3.1.1.1.1.m1.1.1.cmml\">↑</mo><annotation-xml encoding=\"MathML-Content\" id=\"S7.T3.1.1.1.1.m1.1b\"><ci id=\"S7.T3.1.1.1.1.m1.1.1.cmml\" xref=\"S7.T3.1.1.1.1.m1.1.1\">↑</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T3.1.1.1.1.m1.1c\">\\uparrow</annotation></semantics></math>)</span></th>\n<th id=\"S7.T3.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S7.T3.2.2.2.1\" class=\"ltx_text ltx_font_bold\">EMD(<math id=\"S7.T3.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S7.T3.2.2.2.1.m1.1a\"><mo stretchy=\"false\" id=\"S7.T3.2.2.2.1.m1.1.1\" xref=\"S7.T3.2.2.2.1.m1.1.1.cmml\">↓</mo><annotation-xml encoding=\"MathML-Content\" id=\"S7.T3.2.2.2.1.m1.1b\"><ci id=\"S7.T3.2.2.2.1.m1.1.1.cmml\" xref=\"S7.T3.2.2.2.1.m1.1.1\">↓</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T3.2.2.2.1.m1.1c\">\\downarrow</annotation></semantics></math>)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S7.T3.2.3.1\" class=\"ltx_tr\">\n<th id=\"S7.T3.2.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Baseline</th>\n<td id=\"S7.T3.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.3017</td>\n<td id=\"S7.T3.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.3825</td>\n</tr>\n<tr id=\"S7.T3.2.4.2\" class=\"ltx_tr\">\n<th id=\"S7.T3.2.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">Deconv ReLU</th>\n<td id=\"S7.T3.2.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.3198</td>\n<td id=\"S7.T3.2.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">0.3801</td>\n</tr>\n<tr id=\"S7.T3.2.5.3\" class=\"ltx_tr\">\n<th id=\"S7.T3.2.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">Guided GradCAM</th>\n<td id=\"S7.T3.2.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.3275</td>\n<td id=\"S7.T3.2.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">0.3781</td>\n</tr>\n<tr id=\"S7.T3.2.6.4\" class=\"ltx_tr\">\n<th id=\"S7.T3.2.6.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">Aleatoric mask</th>\n<td id=\"S7.T3.2.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.3571</td>\n<td id=\"S7.T3.2.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">0.3763</td>\n</tr>\n<tr id=\"S7.T3.2.7.5\" class=\"ltx_tr\">\n<th id=\"S7.T3.2.7.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\">Predictive mask</th>\n<td id=\"S7.T3.2.7.5.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S7.T3.2.7.5.2.1\" class=\"ltx_text ltx_font_bold\">0.3718</span></td>\n<td id=\"S7.T3.2.7.5.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S7.T3.2.7.5.3.1\" class=\"ltx_text ltx_font_bold\">0.3714</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "We evaluate the proposed GCA methods and have provided both quantitative analysis and qualitative analysis.\nThe former includes: i) Ablation analysis of proposed models (Section- VII-B1), ii) Analysis of uncertainty effect on answer predictions (Figure- 7 (a,b)), iii) Differences of Top-2 softmax scores for answers for some representative questions (Figure- 7 (c,d)) and iv) Comparison of attention map of our proposed uncertainty model against other variants using Rank correlation (RC) and Earth Mover Distance (EMD)  [45] as shown in Table-IV for VQA-HAT [34] and in Table- III for VQA-X [46] . Finally, we compare PGCA with state of the art methods, as mentioned in Section-VII-D. The qualitative analysis includes visualization of certainty activation maps for some representative images as we move from our basic model to the P-GCA model. (Section VII-C)",
            "We compare attention maps produced by our proposed GCA model, and it’s variants with the base model and reports them in Table-IV. Rank correlation and EMD scores are calculated for the produced attention map against human-annotated attention (HAT) maps  [34]. In the table, as we approach the best-proposed GCA model, Rank correlation (RC) is increasing. EMD is also decreasing (Lower the better) as we move towards GCA. To verify our intuition, that we can learn better attention masks by minimizing the uncertainty present in the attention mask; we start with VE and observe that both rank correlation and answer accuracy increase by 0.42 and 0.3 % from baseline, respectively. We also observe that with UDL, AUL, and PUL based loss minimization technique, both RC and EMD improves, as shown in the Table- IV.\nAleatoric-GCA (A-GCA) improves 5.21% in terms of RC and 2.5% in terms of accuracy. Finally, the proposed Predictive-GCA (P-GCA), which is modeled to consider both data and the model uncertainty, improves the RC by 5.51% and accuracy by 2.7% as shown in the Table- IV and Table- II. Since HAT maps are only available for the VQA-v1 dataset, thus, this ablation analysis has been performed only for VQA-v1. We also providing SOTA results for VQA-v1 and VQA-v2 dataset as shown in Table- VII and Table- VI respectively.\nAlso, we compare with our gradient certainty explanation with human explanation present in the VQA-v2 dataset for the various model, as mentioned in Table- III. This human explanation mask only available for the VQA-v2 dataset. We observe that our attention (P-GCA) mask performs better than others as well."
        ]
    },
    "S7.T4": {
        "caption": "TABLE IV: Ablation analysis and SOTA between HAT[34] attention and generated attention mask",
        "table": "<table id=\"S7.T4.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S7.T4.3.3\" class=\"ltx_tr\">\n<th id=\"S7.T4.3.3.4\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S7.T4.3.3.4.1\" class=\"ltx_text ltx_font_bold\">Model</span></th>\n<td id=\"S7.T4.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S7.T4.1.1.1.1\" class=\"ltx_text ltx_font_bold\">RC(<math id=\"S7.T4.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S7.T4.1.1.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S7.T4.1.1.1.1.m1.1.1\" xref=\"S7.T4.1.1.1.1.m1.1.1.cmml\">↑</mo><annotation-xml encoding=\"MathML-Content\" id=\"S7.T4.1.1.1.1.m1.1b\"><ci id=\"S7.T4.1.1.1.1.m1.1.1.cmml\" xref=\"S7.T4.1.1.1.1.m1.1.1\">↑</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T4.1.1.1.1.m1.1c\">\\uparrow</annotation></semantics></math>)</span></td>\n<td id=\"S7.T4.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S7.T4.2.2.2.1\" class=\"ltx_text ltx_font_bold\">EMD(<math id=\"S7.T4.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S7.T4.2.2.2.1.m1.1a\"><mo stretchy=\"false\" id=\"S7.T4.2.2.2.1.m1.1.1\" xref=\"S7.T4.2.2.2.1.m1.1.1.cmml\">↓</mo><annotation-xml encoding=\"MathML-Content\" id=\"S7.T4.2.2.2.1.m1.1b\"><ci id=\"S7.T4.2.2.2.1.m1.1.1.cmml\" xref=\"S7.T4.2.2.2.1.m1.1.1\">↓</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T4.2.2.2.1.m1.1c\">\\downarrow</annotation></semantics></math>)</span></td>\n<td id=\"S7.T4.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S7.T4.3.3.3.1\" class=\"ltx_text ltx_font_bold\">CD(<math id=\"S7.T4.3.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S7.T4.3.3.3.1.m1.1a\"><mo stretchy=\"false\" id=\"S7.T4.3.3.3.1.m1.1.1\" xref=\"S7.T4.3.3.3.1.m1.1.1.cmml\">↑</mo><annotation-xml encoding=\"MathML-Content\" id=\"S7.T4.3.3.3.1.m1.1b\"><ci id=\"S7.T4.3.3.3.1.m1.1.1.cmml\" xref=\"S7.T4.3.3.3.1.m1.1.1\">↑</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T4.3.3.3.1.m1.1c\">\\uparrow</annotation></semantics></math>)</span></td>\n</tr>\n<tr id=\"S7.T4.3.4.1\" class=\"ltx_tr\">\n<th id=\"S7.T4.3.4.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">SAN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib34\" title=\"\" class=\"ltx_ref\">34</a>]</cite>\n</th>\n<td id=\"S7.T4.3.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.2432</td>\n<td id=\"S7.T4.3.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.4013</td>\n<td id=\"S7.T4.3.4.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">–</td>\n</tr>\n<tr id=\"S7.T4.3.5.2\" class=\"ltx_tr\">\n<th id=\"S7.T4.3.5.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">CoAtt-W<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">27</a>]</cite>\n</th>\n<td id=\"S7.T4.3.5.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.246</td>\n<td id=\"S7.T4.3.5.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">–</td>\n<td id=\"S7.T4.3.5.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\">–</td>\n</tr>\n<tr id=\"S7.T4.3.6.3\" class=\"ltx_tr\">\n<th id=\"S7.T4.3.6.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">CoAtt-P <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">27</a>]</cite>\n</th>\n<td id=\"S7.T4.3.6.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.256</td>\n<td id=\"S7.T4.3.6.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">–</td>\n<td id=\"S7.T4.3.6.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\">–</td>\n</tr>\n<tr id=\"S7.T4.3.7.4\" class=\"ltx_tr\">\n<th id=\"S7.T4.3.7.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">CoAtt-Q<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">27</a>]</cite>\n</th>\n<td id=\"S7.T4.3.7.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.264</td>\n<td id=\"S7.T4.3.7.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">–</td>\n<td id=\"S7.T4.3.7.4.4\" class=\"ltx_td ltx_align_center ltx_border_r\">–</td>\n</tr>\n<tr id=\"S7.T4.3.8.5\" class=\"ltx_tr\">\n<th id=\"S7.T4.3.8.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">DVQA(K=1)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib31\" title=\"\" class=\"ltx_ref\">31</a>]</cite>\n</th>\n<td id=\"S7.T4.3.8.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.328</td>\n<td id=\"S7.T4.3.8.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\">–</td>\n<td id=\"S7.T4.3.8.5.4\" class=\"ltx_td ltx_align_center ltx_border_r\">–</td>\n</tr>\n<tr id=\"S7.T4.3.9.6\" class=\"ltx_tr\">\n<th id=\"S7.T4.3.9.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Baseline (MCB)</th>\n<td id=\"S7.T4.3.9.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.2790</td>\n<td id=\"S7.T4.3.9.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.3931</td>\n<td id=\"S7.T4.3.9.6.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">–</td>\n</tr>\n<tr id=\"S7.T4.3.10.7\" class=\"ltx_tr\">\n<th id=\"S7.T4.3.10.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">VE (ours)</th>\n<td id=\"S7.T4.3.10.7.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.2832</td>\n<td id=\"S7.T4.3.10.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\">0.3931</td>\n<td id=\"S7.T4.3.10.7.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0.1013</td>\n</tr>\n<tr id=\"S7.T4.3.11.8\" class=\"ltx_tr\">\n<th id=\"S7.T4.3.11.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">UDL (ours)</th>\n<td id=\"S7.T4.3.11.8.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.2850</td>\n<td id=\"S7.T4.3.11.8.3\" class=\"ltx_td ltx_align_center ltx_border_r\">0.3914</td>\n<td id=\"S7.T4.3.11.8.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0.1229</td>\n</tr>\n<tr id=\"S7.T4.3.12.9\" class=\"ltx_tr\">\n<th id=\"S7.T4.3.12.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">AUL (ours)</th>\n<td id=\"S7.T4.3.12.9.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.2937</td>\n<td id=\"S7.T4.3.12.9.3\" class=\"ltx_td ltx_align_center ltx_border_r\">0.3867</td>\n<td id=\"S7.T4.3.12.9.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0.1502</td>\n</tr>\n<tr id=\"S7.T4.3.13.10\" class=\"ltx_tr\">\n<th id=\"S7.T4.3.13.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">PUL(ours)</th>\n<td id=\"S7.T4.3.13.10.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.3012</td>\n<td id=\"S7.T4.3.13.10.3\" class=\"ltx_td ltx_align_center ltx_border_r\">0.3805</td>\n<td id=\"S7.T4.3.13.10.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0.1585</td>\n</tr>\n<tr id=\"S7.T4.3.14.11\" class=\"ltx_tr\">\n<th id=\"S7.T4.3.14.11.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">PUL + VE (ours)</th>\n<td id=\"S7.T4.3.14.11.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.3139</td>\n<td id=\"S7.T4.3.14.11.3\" class=\"ltx_td ltx_align_center ltx_border_r\">0.3851</td>\n<td id=\"S7.T4.3.14.11.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0.1631</td>\n</tr>\n<tr id=\"S7.T4.3.15.12\" class=\"ltx_tr\">\n<th id=\"S7.T4.3.15.12.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">PUL + UDL(ours)</th>\n<td id=\"S7.T4.3.15.12.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.3243</td>\n<td id=\"S7.T4.3.15.12.3\" class=\"ltx_td ltx_align_center ltx_border_r\">0.3824</td>\n<td id=\"S7.T4.3.15.12.4\" class=\"ltx_td ltx_align_center ltx_border_r\">0.1630</td>\n</tr>\n<tr id=\"S7.T4.3.16.13\" class=\"ltx_tr\">\n<th id=\"S7.T4.3.16.13.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">A-GCA (ours)</th>\n<td id=\"S7.T4.3.16.13.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.3311</td>\n<td id=\"S7.T4.3.16.13.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.3784</td>\n<td id=\"S7.T4.3.16.13.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.1683</td>\n</tr>\n<tr id=\"S7.T4.3.17.14\" class=\"ltx_tr\">\n<th id=\"S7.T4.3.17.14.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">P-GCA (ours)</th>\n<td id=\"S7.T4.3.17.14.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T4.3.17.14.2.1\" class=\"ltx_text ltx_font_bold\">0.3341</span></td>\n<td id=\"S7.T4.3.17.14.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T4.3.17.14.3.1\" class=\"ltx_text ltx_font_bold\">0.3721</span></td>\n<td id=\"S7.T4.3.17.14.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T4.3.17.14.4.1\" class=\"ltx_text ltx_font_bold\">0.1710</span></td>\n</tr>\n<tr id=\"S7.T4.3.18.15\" class=\"ltx_tr\">\n<th id=\"S7.T4.3.18.15.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">Human <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib34\" title=\"\" class=\"ltx_ref\">34</a>]</cite>\n</th>\n<td id=\"S7.T4.3.18.15.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.623</td>\n<td id=\"S7.T4.3.18.15.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">–</td>\n<td id=\"S7.T4.3.18.15.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">–</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "We evaluate the proposed GCA methods and have provided both quantitative analysis and qualitative analysis.\nThe former includes: i) Ablation analysis of proposed models (Section- VII-B1), ii) Analysis of uncertainty effect on answer predictions (Figure- 7 (a,b)), iii) Differences of Top-2 softmax scores for answers for some representative questions (Figure- 7 (c,d)) and iv) Comparison of attention map of our proposed uncertainty model against other variants using Rank correlation (RC) and Earth Mover Distance (EMD)  [45] as shown in Table-IV for VQA-HAT [34] and in Table- III for VQA-X [46] . Finally, we compare PGCA with state of the art methods, as mentioned in Section-VII-D. The qualitative analysis includes visualization of certainty activation maps for some representative images as we move from our basic model to the P-GCA model. (Section VII-C)",
            "We compare attention maps produced by our proposed GCA model, and it’s variants with the base model and reports them in Table-IV. Rank correlation and EMD scores are calculated for the produced attention map against human-annotated attention (HAT) maps  [34]. In the table, as we approach the best-proposed GCA model, Rank correlation (RC) is increasing. EMD is also decreasing (Lower the better) as we move towards GCA. To verify our intuition, that we can learn better attention masks by minimizing the uncertainty present in the attention mask; we start with VE and observe that both rank correlation and answer accuracy increase by 0.42 and 0.3 % from baseline, respectively. We also observe that with UDL, AUL, and PUL based loss minimization technique, both RC and EMD improves, as shown in the Table- IV.\nAleatoric-GCA (A-GCA) improves 5.21% in terms of RC and 2.5% in terms of accuracy. Finally, the proposed Predictive-GCA (P-GCA), which is modeled to consider both data and the model uncertainty, improves the RC by 5.51% and accuracy by 2.7% as shown in the Table- IV and Table- II. Since HAT maps are only available for the VQA-v1 dataset, thus, this ablation analysis has been performed only for VQA-v1. We also providing SOTA results for VQA-v1 and VQA-v2 dataset as shown in Table- VII and Table- VI respectively.\nAlso, we compare with our gradient certainty explanation with human explanation present in the VQA-v2 dataset for the various model, as mentioned in Table- III. This human explanation mask only available for the VQA-v2 dataset. We observe that our attention (P-GCA) mask performs better than others as well.",
            "We obtain the initial comparison with the baselines on the rank correlation on human attention (HAT) dataset [34] that provides human attention while solving for VQA. Between humans, the rank correlation is 62.3%. The comparison of various state-of-the-art methods and baselines are provided in Table IV. We use a variant of MCB [6] model as our baseline method. We obtain an improvement of around 5.2% using the A-GCA model and 5.51% using the P-GCA model in terms of rank correlation with human attention. From this, we justify that our attention map is more similar to the human attention map. We also compare with the baselines on the answer accuracy on the VQA-v1[8] dataset, as shown in Table- VII. We obtain an improvement of around 2.7% over the comparable MCB baseline. Our MCB based model A-GCA and P-GCA improves by 0.9% and 1.1% accuracy as compared to state of the art model DVQA [31] on VQA-v1. However, using a saliency-based method  [53] that is trained on eye-tracking data to obtain a measure of where people look in a task-independent manner, results in more correlation with human attention (0.49), as noted by [34]. However, this is explicitly trained using human attention and is not task-dependent. In our approach, we aim to obtain a method that can simulate human cognitive abilities for solving the tasks. We provide state of the art results for VQA-v2 in Table- VI. This table shows that using the GCA method, the VQA result improves. We have provided more results for attention map visualization for both types of uncertainty, training setup, dataset, and evaluation methods here.222https://delta-lab-iitk.github.io/U-CAM/ ."
        ]
    },
    "S7.T5": {
        "caption": "TABLE V:  Aleatoric & Epistemic uncertainty measurement score.",
        "table": "<table id=\"S7.T5.7\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S7.T5.7.8.1\" class=\"ltx_tr\">\n<th id=\"S7.T5.7.8.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S7.T5.7.8.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T5.7.8.1.1.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\"><span id=\"S7.T5.7.8.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Type of Uncertainty</span></span>\n</span>\n</th>\n<th id=\"S7.T5.7.8.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S7.T5.7.8.1.2.1\" class=\"ltx_text ltx_font_bold\">Variance</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S7.T5.1.1\" class=\"ltx_tr\">\n<th id=\"S7.T5.1.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S7.T5.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T5.1.1.2.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\">Aleatoric (with VE)</span>\n</span>\n</th>\n<td id=\"S7.T5.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.277 <math id=\"S7.T5.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times 10^{-}3\" display=\"inline\"><semantics id=\"S7.T5.1.1.1.m1.1a\"><mrow id=\"S7.T5.1.1.1.m1.1.1\" xref=\"S7.T5.1.1.1.m1.1.1.cmml\"><mi id=\"S7.T5.1.1.1.m1.1.1.2\" xref=\"S7.T5.1.1.1.m1.1.1.2.cmml\"></mi><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S7.T5.1.1.1.m1.1.1.1\" xref=\"S7.T5.1.1.1.m1.1.1.1.cmml\">×</mo><mrow id=\"S7.T5.1.1.1.m1.1.1.3\" xref=\"S7.T5.1.1.1.m1.1.1.3.cmml\"><msup id=\"S7.T5.1.1.1.m1.1.1.3.2\" xref=\"S7.T5.1.1.1.m1.1.1.3.2.cmml\"><mn id=\"S7.T5.1.1.1.m1.1.1.3.2.2\" xref=\"S7.T5.1.1.1.m1.1.1.3.2.2.cmml\">10</mn><mo id=\"S7.T5.1.1.1.m1.1.1.3.2.3\" xref=\"S7.T5.1.1.1.m1.1.1.3.2.3.cmml\">−</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S7.T5.1.1.1.m1.1.1.3.1\" xref=\"S7.T5.1.1.1.m1.1.1.3.1.cmml\">​</mo><mn id=\"S7.T5.1.1.1.m1.1.1.3.3\" xref=\"S7.T5.1.1.1.m1.1.1.3.3.cmml\">3</mn></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S7.T5.1.1.1.m1.1b\"><apply id=\"S7.T5.1.1.1.m1.1.1.cmml\" xref=\"S7.T5.1.1.1.m1.1.1\"><times id=\"S7.T5.1.1.1.m1.1.1.1.cmml\" xref=\"S7.T5.1.1.1.m1.1.1.1\"></times><csymbol cd=\"latexml\" id=\"S7.T5.1.1.1.m1.1.1.2.cmml\" xref=\"S7.T5.1.1.1.m1.1.1.2\">absent</csymbol><apply id=\"S7.T5.1.1.1.m1.1.1.3.cmml\" xref=\"S7.T5.1.1.1.m1.1.1.3\"><times id=\"S7.T5.1.1.1.m1.1.1.3.1.cmml\" xref=\"S7.T5.1.1.1.m1.1.1.3.1\"></times><apply id=\"S7.T5.1.1.1.m1.1.1.3.2.cmml\" xref=\"S7.T5.1.1.1.m1.1.1.3.2\"><csymbol cd=\"ambiguous\" id=\"S7.T5.1.1.1.m1.1.1.3.2.1.cmml\" xref=\"S7.T5.1.1.1.m1.1.1.3.2\">superscript</csymbol><cn type=\"integer\" id=\"S7.T5.1.1.1.m1.1.1.3.2.2.cmml\" xref=\"S7.T5.1.1.1.m1.1.1.3.2.2\">10</cn><minus id=\"S7.T5.1.1.1.m1.1.1.3.2.3.cmml\" xref=\"S7.T5.1.1.1.m1.1.1.3.2.3\"></minus></apply><cn type=\"integer\" id=\"S7.T5.1.1.1.m1.1.1.3.3.cmml\" xref=\"S7.T5.1.1.1.m1.1.1.3.3\">3</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T5.1.1.1.m1.1c\">\\times 10^{-}3</annotation></semantics></math>\n</td>\n</tr>\n<tr id=\"S7.T5.2.2\" class=\"ltx_tr\">\n<th id=\"S7.T5.2.2.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r\">\n<span id=\"S7.T5.2.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T5.2.2.2.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\">Aleatoric (with UDL)</span>\n</span>\n</th>\n<td id=\"S7.T5.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_r\">5.743<math id=\"S7.T5.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times 10^{-}3\" display=\"inline\"><semantics id=\"S7.T5.2.2.1.m1.1a\"><mrow id=\"S7.T5.2.2.1.m1.1.1\" xref=\"S7.T5.2.2.1.m1.1.1.cmml\"><mi id=\"S7.T5.2.2.1.m1.1.1.2\" xref=\"S7.T5.2.2.1.m1.1.1.2.cmml\"></mi><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S7.T5.2.2.1.m1.1.1.1\" xref=\"S7.T5.2.2.1.m1.1.1.1.cmml\">×</mo><mrow id=\"S7.T5.2.2.1.m1.1.1.3\" xref=\"S7.T5.2.2.1.m1.1.1.3.cmml\"><msup id=\"S7.T5.2.2.1.m1.1.1.3.2\" xref=\"S7.T5.2.2.1.m1.1.1.3.2.cmml\"><mn id=\"S7.T5.2.2.1.m1.1.1.3.2.2\" xref=\"S7.T5.2.2.1.m1.1.1.3.2.2.cmml\">10</mn><mo id=\"S7.T5.2.2.1.m1.1.1.3.2.3\" xref=\"S7.T5.2.2.1.m1.1.1.3.2.3.cmml\">−</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S7.T5.2.2.1.m1.1.1.3.1\" xref=\"S7.T5.2.2.1.m1.1.1.3.1.cmml\">​</mo><mn id=\"S7.T5.2.2.1.m1.1.1.3.3\" xref=\"S7.T5.2.2.1.m1.1.1.3.3.cmml\">3</mn></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S7.T5.2.2.1.m1.1b\"><apply id=\"S7.T5.2.2.1.m1.1.1.cmml\" xref=\"S7.T5.2.2.1.m1.1.1\"><times id=\"S7.T5.2.2.1.m1.1.1.1.cmml\" xref=\"S7.T5.2.2.1.m1.1.1.1\"></times><csymbol cd=\"latexml\" id=\"S7.T5.2.2.1.m1.1.1.2.cmml\" xref=\"S7.T5.2.2.1.m1.1.1.2\">absent</csymbol><apply id=\"S7.T5.2.2.1.m1.1.1.3.cmml\" xref=\"S7.T5.2.2.1.m1.1.1.3\"><times id=\"S7.T5.2.2.1.m1.1.1.3.1.cmml\" xref=\"S7.T5.2.2.1.m1.1.1.3.1\"></times><apply id=\"S7.T5.2.2.1.m1.1.1.3.2.cmml\" xref=\"S7.T5.2.2.1.m1.1.1.3.2\"><csymbol cd=\"ambiguous\" id=\"S7.T5.2.2.1.m1.1.1.3.2.1.cmml\" xref=\"S7.T5.2.2.1.m1.1.1.3.2\">superscript</csymbol><cn type=\"integer\" id=\"S7.T5.2.2.1.m1.1.1.3.2.2.cmml\" xref=\"S7.T5.2.2.1.m1.1.1.3.2.2\">10</cn><minus id=\"S7.T5.2.2.1.m1.1.1.3.2.3.cmml\" xref=\"S7.T5.2.2.1.m1.1.1.3.2.3\"></minus></apply><cn type=\"integer\" id=\"S7.T5.2.2.1.m1.1.1.3.3.cmml\" xref=\"S7.T5.2.2.1.m1.1.1.3.3\">3</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T5.2.2.1.m1.1c\">\\times 10^{-}3</annotation></semantics></math>\n</td>\n</tr>\n<tr id=\"S7.T5.3.3\" class=\"ltx_tr\">\n<th id=\"S7.T5.3.3.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r\">\n<span id=\"S7.T5.3.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T5.3.3.2.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\">Aleatoric (with AUL)</span>\n</span>\n</th>\n<td id=\"S7.T5.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_r\">2.561<math id=\"S7.T5.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times 10^{-}3\" display=\"inline\"><semantics id=\"S7.T5.3.3.1.m1.1a\"><mrow id=\"S7.T5.3.3.1.m1.1.1\" xref=\"S7.T5.3.3.1.m1.1.1.cmml\"><mi id=\"S7.T5.3.3.1.m1.1.1.2\" xref=\"S7.T5.3.3.1.m1.1.1.2.cmml\"></mi><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S7.T5.3.3.1.m1.1.1.1\" xref=\"S7.T5.3.3.1.m1.1.1.1.cmml\">×</mo><mrow id=\"S7.T5.3.3.1.m1.1.1.3\" xref=\"S7.T5.3.3.1.m1.1.1.3.cmml\"><msup id=\"S7.T5.3.3.1.m1.1.1.3.2\" xref=\"S7.T5.3.3.1.m1.1.1.3.2.cmml\"><mn id=\"S7.T5.3.3.1.m1.1.1.3.2.2\" xref=\"S7.T5.3.3.1.m1.1.1.3.2.2.cmml\">10</mn><mo id=\"S7.T5.3.3.1.m1.1.1.3.2.3\" xref=\"S7.T5.3.3.1.m1.1.1.3.2.3.cmml\">−</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S7.T5.3.3.1.m1.1.1.3.1\" xref=\"S7.T5.3.3.1.m1.1.1.3.1.cmml\">​</mo><mn id=\"S7.T5.3.3.1.m1.1.1.3.3\" xref=\"S7.T5.3.3.1.m1.1.1.3.3.cmml\">3</mn></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S7.T5.3.3.1.m1.1b\"><apply id=\"S7.T5.3.3.1.m1.1.1.cmml\" xref=\"S7.T5.3.3.1.m1.1.1\"><times id=\"S7.T5.3.3.1.m1.1.1.1.cmml\" xref=\"S7.T5.3.3.1.m1.1.1.1\"></times><csymbol cd=\"latexml\" id=\"S7.T5.3.3.1.m1.1.1.2.cmml\" xref=\"S7.T5.3.3.1.m1.1.1.2\">absent</csymbol><apply id=\"S7.T5.3.3.1.m1.1.1.3.cmml\" xref=\"S7.T5.3.3.1.m1.1.1.3\"><times id=\"S7.T5.3.3.1.m1.1.1.3.1.cmml\" xref=\"S7.T5.3.3.1.m1.1.1.3.1\"></times><apply id=\"S7.T5.3.3.1.m1.1.1.3.2.cmml\" xref=\"S7.T5.3.3.1.m1.1.1.3.2\"><csymbol cd=\"ambiguous\" id=\"S7.T5.3.3.1.m1.1.1.3.2.1.cmml\" xref=\"S7.T5.3.3.1.m1.1.1.3.2\">superscript</csymbol><cn type=\"integer\" id=\"S7.T5.3.3.1.m1.1.1.3.2.2.cmml\" xref=\"S7.T5.3.3.1.m1.1.1.3.2.2\">10</cn><minus id=\"S7.T5.3.3.1.m1.1.1.3.2.3.cmml\" xref=\"S7.T5.3.3.1.m1.1.1.3.2.3\"></minus></apply><cn type=\"integer\" id=\"S7.T5.3.3.1.m1.1.1.3.3.cmml\" xref=\"S7.T5.3.3.1.m1.1.1.3.3\">3</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T5.3.3.1.m1.1c\">\\times 10^{-}3</annotation></semantics></math>\n</td>\n</tr>\n<tr id=\"S7.T5.4.4\" class=\"ltx_tr\">\n<th id=\"S7.T5.4.4.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r\">\n<span id=\"S7.T5.4.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T5.4.4.2.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\">Aleatoric (with PUL)</span>\n</span>\n</th>\n<td id=\"S7.T5.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_r\">1.841<math id=\"S7.T5.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times 10^{-}3\" display=\"inline\"><semantics id=\"S7.T5.4.4.1.m1.1a\"><mrow id=\"S7.T5.4.4.1.m1.1.1\" xref=\"S7.T5.4.4.1.m1.1.1.cmml\"><mi id=\"S7.T5.4.4.1.m1.1.1.2\" xref=\"S7.T5.4.4.1.m1.1.1.2.cmml\"></mi><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S7.T5.4.4.1.m1.1.1.1\" xref=\"S7.T5.4.4.1.m1.1.1.1.cmml\">×</mo><mrow id=\"S7.T5.4.4.1.m1.1.1.3\" xref=\"S7.T5.4.4.1.m1.1.1.3.cmml\"><msup id=\"S7.T5.4.4.1.m1.1.1.3.2\" xref=\"S7.T5.4.4.1.m1.1.1.3.2.cmml\"><mn id=\"S7.T5.4.4.1.m1.1.1.3.2.2\" xref=\"S7.T5.4.4.1.m1.1.1.3.2.2.cmml\">10</mn><mo id=\"S7.T5.4.4.1.m1.1.1.3.2.3\" xref=\"S7.T5.4.4.1.m1.1.1.3.2.3.cmml\">−</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S7.T5.4.4.1.m1.1.1.3.1\" xref=\"S7.T5.4.4.1.m1.1.1.3.1.cmml\">​</mo><mn id=\"S7.T5.4.4.1.m1.1.1.3.3\" xref=\"S7.T5.4.4.1.m1.1.1.3.3.cmml\">3</mn></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S7.T5.4.4.1.m1.1b\"><apply id=\"S7.T5.4.4.1.m1.1.1.cmml\" xref=\"S7.T5.4.4.1.m1.1.1\"><times id=\"S7.T5.4.4.1.m1.1.1.1.cmml\" xref=\"S7.T5.4.4.1.m1.1.1.1\"></times><csymbol cd=\"latexml\" id=\"S7.T5.4.4.1.m1.1.1.2.cmml\" xref=\"S7.T5.4.4.1.m1.1.1.2\">absent</csymbol><apply id=\"S7.T5.4.4.1.m1.1.1.3.cmml\" xref=\"S7.T5.4.4.1.m1.1.1.3\"><times id=\"S7.T5.4.4.1.m1.1.1.3.1.cmml\" xref=\"S7.T5.4.4.1.m1.1.1.3.1\"></times><apply id=\"S7.T5.4.4.1.m1.1.1.3.2.cmml\" xref=\"S7.T5.4.4.1.m1.1.1.3.2\"><csymbol cd=\"ambiguous\" id=\"S7.T5.4.4.1.m1.1.1.3.2.1.cmml\" xref=\"S7.T5.4.4.1.m1.1.1.3.2\">superscript</csymbol><cn type=\"integer\" id=\"S7.T5.4.4.1.m1.1.1.3.2.2.cmml\" xref=\"S7.T5.4.4.1.m1.1.1.3.2.2\">10</cn><minus id=\"S7.T5.4.4.1.m1.1.1.3.2.3.cmml\" xref=\"S7.T5.4.4.1.m1.1.1.3.2.3\"></minus></apply><cn type=\"integer\" id=\"S7.T5.4.4.1.m1.1.1.3.3.cmml\" xref=\"S7.T5.4.4.1.m1.1.1.3.3\">3</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T5.4.4.1.m1.1c\">\\times 10^{-}3</annotation></semantics></math>\n</td>\n</tr>\n<tr id=\"S7.T5.5.5\" class=\"ltx_tr\">\n<th id=\"S7.T5.5.5.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S7.T5.5.5.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T5.5.5.2.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\">Epistemic (50% training)</span>\n</span>\n</th>\n<td id=\"S7.T5.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4.371 <math id=\"S7.T5.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times 10^{-}4\" display=\"inline\"><semantics id=\"S7.T5.5.5.1.m1.1a\"><mrow id=\"S7.T5.5.5.1.m1.1.1\" xref=\"S7.T5.5.5.1.m1.1.1.cmml\"><mi id=\"S7.T5.5.5.1.m1.1.1.2\" xref=\"S7.T5.5.5.1.m1.1.1.2.cmml\"></mi><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S7.T5.5.5.1.m1.1.1.1\" xref=\"S7.T5.5.5.1.m1.1.1.1.cmml\">×</mo><mrow id=\"S7.T5.5.5.1.m1.1.1.3\" xref=\"S7.T5.5.5.1.m1.1.1.3.cmml\"><msup id=\"S7.T5.5.5.1.m1.1.1.3.2\" xref=\"S7.T5.5.5.1.m1.1.1.3.2.cmml\"><mn id=\"S7.T5.5.5.1.m1.1.1.3.2.2\" xref=\"S7.T5.5.5.1.m1.1.1.3.2.2.cmml\">10</mn><mo id=\"S7.T5.5.5.1.m1.1.1.3.2.3\" xref=\"S7.T5.5.5.1.m1.1.1.3.2.3.cmml\">−</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S7.T5.5.5.1.m1.1.1.3.1\" xref=\"S7.T5.5.5.1.m1.1.1.3.1.cmml\">​</mo><mn id=\"S7.T5.5.5.1.m1.1.1.3.3\" xref=\"S7.T5.5.5.1.m1.1.1.3.3.cmml\">4</mn></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S7.T5.5.5.1.m1.1b\"><apply id=\"S7.T5.5.5.1.m1.1.1.cmml\" xref=\"S7.T5.5.5.1.m1.1.1\"><times id=\"S7.T5.5.5.1.m1.1.1.1.cmml\" xref=\"S7.T5.5.5.1.m1.1.1.1\"></times><csymbol cd=\"latexml\" id=\"S7.T5.5.5.1.m1.1.1.2.cmml\" xref=\"S7.T5.5.5.1.m1.1.1.2\">absent</csymbol><apply id=\"S7.T5.5.5.1.m1.1.1.3.cmml\" xref=\"S7.T5.5.5.1.m1.1.1.3\"><times id=\"S7.T5.5.5.1.m1.1.1.3.1.cmml\" xref=\"S7.T5.5.5.1.m1.1.1.3.1\"></times><apply id=\"S7.T5.5.5.1.m1.1.1.3.2.cmml\" xref=\"S7.T5.5.5.1.m1.1.1.3.2\"><csymbol cd=\"ambiguous\" id=\"S7.T5.5.5.1.m1.1.1.3.2.1.cmml\" xref=\"S7.T5.5.5.1.m1.1.1.3.2\">superscript</csymbol><cn type=\"integer\" id=\"S7.T5.5.5.1.m1.1.1.3.2.2.cmml\" xref=\"S7.T5.5.5.1.m1.1.1.3.2.2\">10</cn><minus id=\"S7.T5.5.5.1.m1.1.1.3.2.3.cmml\" xref=\"S7.T5.5.5.1.m1.1.1.3.2.3\"></minus></apply><cn type=\"integer\" id=\"S7.T5.5.5.1.m1.1.1.3.3.cmml\" xref=\"S7.T5.5.5.1.m1.1.1.3.3\">4</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T5.5.5.1.m1.1c\">\\times 10^{-}4</annotation></semantics></math>\n</td>\n</tr>\n<tr id=\"S7.T5.6.6\" class=\"ltx_tr\">\n<th id=\"S7.T5.6.6.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r\">\n<span id=\"S7.T5.6.6.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T5.6.6.2.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\">Epistemic (75% training)</span>\n</span>\n</th>\n<td id=\"S7.T5.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_r\">3.369<math id=\"S7.T5.6.6.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times 10^{-}4\" display=\"inline\"><semantics id=\"S7.T5.6.6.1.m1.1a\"><mrow id=\"S7.T5.6.6.1.m1.1.1\" xref=\"S7.T5.6.6.1.m1.1.1.cmml\"><mi id=\"S7.T5.6.6.1.m1.1.1.2\" xref=\"S7.T5.6.6.1.m1.1.1.2.cmml\"></mi><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S7.T5.6.6.1.m1.1.1.1\" xref=\"S7.T5.6.6.1.m1.1.1.1.cmml\">×</mo><mrow id=\"S7.T5.6.6.1.m1.1.1.3\" xref=\"S7.T5.6.6.1.m1.1.1.3.cmml\"><msup id=\"S7.T5.6.6.1.m1.1.1.3.2\" xref=\"S7.T5.6.6.1.m1.1.1.3.2.cmml\"><mn id=\"S7.T5.6.6.1.m1.1.1.3.2.2\" xref=\"S7.T5.6.6.1.m1.1.1.3.2.2.cmml\">10</mn><mo id=\"S7.T5.6.6.1.m1.1.1.3.2.3\" xref=\"S7.T5.6.6.1.m1.1.1.3.2.3.cmml\">−</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S7.T5.6.6.1.m1.1.1.3.1\" xref=\"S7.T5.6.6.1.m1.1.1.3.1.cmml\">​</mo><mn id=\"S7.T5.6.6.1.m1.1.1.3.3\" xref=\"S7.T5.6.6.1.m1.1.1.3.3.cmml\">4</mn></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S7.T5.6.6.1.m1.1b\"><apply id=\"S7.T5.6.6.1.m1.1.1.cmml\" xref=\"S7.T5.6.6.1.m1.1.1\"><times id=\"S7.T5.6.6.1.m1.1.1.1.cmml\" xref=\"S7.T5.6.6.1.m1.1.1.1\"></times><csymbol cd=\"latexml\" id=\"S7.T5.6.6.1.m1.1.1.2.cmml\" xref=\"S7.T5.6.6.1.m1.1.1.2\">absent</csymbol><apply id=\"S7.T5.6.6.1.m1.1.1.3.cmml\" xref=\"S7.T5.6.6.1.m1.1.1.3\"><times id=\"S7.T5.6.6.1.m1.1.1.3.1.cmml\" xref=\"S7.T5.6.6.1.m1.1.1.3.1\"></times><apply id=\"S7.T5.6.6.1.m1.1.1.3.2.cmml\" xref=\"S7.T5.6.6.1.m1.1.1.3.2\"><csymbol cd=\"ambiguous\" id=\"S7.T5.6.6.1.m1.1.1.3.2.1.cmml\" xref=\"S7.T5.6.6.1.m1.1.1.3.2\">superscript</csymbol><cn type=\"integer\" id=\"S7.T5.6.6.1.m1.1.1.3.2.2.cmml\" xref=\"S7.T5.6.6.1.m1.1.1.3.2.2\">10</cn><minus id=\"S7.T5.6.6.1.m1.1.1.3.2.3.cmml\" xref=\"S7.T5.6.6.1.m1.1.1.3.2.3\"></minus></apply><cn type=\"integer\" id=\"S7.T5.6.6.1.m1.1.1.3.3.cmml\" xref=\"S7.T5.6.6.1.m1.1.1.3.3\">4</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T5.6.6.1.m1.1c\">\\times 10^{-}4</annotation></semantics></math>\n</td>\n</tr>\n<tr id=\"S7.T5.7.7\" class=\"ltx_tr\">\n<th id=\"S7.T5.7.7.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\">\n<span id=\"S7.T5.7.7.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T5.7.7.2.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\">Epistemic (100% training)</span>\n</span>\n</th>\n<td id=\"S7.T5.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">1.972<math id=\"S7.T5.7.7.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times 10^{-}4\" display=\"inline\"><semantics id=\"S7.T5.7.7.1.m1.1a\"><mrow id=\"S7.T5.7.7.1.m1.1.1\" xref=\"S7.T5.7.7.1.m1.1.1.cmml\"><mi id=\"S7.T5.7.7.1.m1.1.1.2\" xref=\"S7.T5.7.7.1.m1.1.1.2.cmml\"></mi><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S7.T5.7.7.1.m1.1.1.1\" xref=\"S7.T5.7.7.1.m1.1.1.1.cmml\">×</mo><mrow id=\"S7.T5.7.7.1.m1.1.1.3\" xref=\"S7.T5.7.7.1.m1.1.1.3.cmml\"><msup id=\"S7.T5.7.7.1.m1.1.1.3.2\" xref=\"S7.T5.7.7.1.m1.1.1.3.2.cmml\"><mn id=\"S7.T5.7.7.1.m1.1.1.3.2.2\" xref=\"S7.T5.7.7.1.m1.1.1.3.2.2.cmml\">10</mn><mo id=\"S7.T5.7.7.1.m1.1.1.3.2.3\" xref=\"S7.T5.7.7.1.m1.1.1.3.2.3.cmml\">−</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S7.T5.7.7.1.m1.1.1.3.1\" xref=\"S7.T5.7.7.1.m1.1.1.3.1.cmml\">​</mo><mn id=\"S7.T5.7.7.1.m1.1.1.3.3\" xref=\"S7.T5.7.7.1.m1.1.1.3.3.cmml\">4</mn></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S7.T5.7.7.1.m1.1b\"><apply id=\"S7.T5.7.7.1.m1.1.1.cmml\" xref=\"S7.T5.7.7.1.m1.1.1\"><times id=\"S7.T5.7.7.1.m1.1.1.1.cmml\" xref=\"S7.T5.7.7.1.m1.1.1.1\"></times><csymbol cd=\"latexml\" id=\"S7.T5.7.7.1.m1.1.1.2.cmml\" xref=\"S7.T5.7.7.1.m1.1.1.2\">absent</csymbol><apply id=\"S7.T5.7.7.1.m1.1.1.3.cmml\" xref=\"S7.T5.7.7.1.m1.1.1.3\"><times id=\"S7.T5.7.7.1.m1.1.1.3.1.cmml\" xref=\"S7.T5.7.7.1.m1.1.1.3.1\"></times><apply id=\"S7.T5.7.7.1.m1.1.1.3.2.cmml\" xref=\"S7.T5.7.7.1.m1.1.1.3.2\"><csymbol cd=\"ambiguous\" id=\"S7.T5.7.7.1.m1.1.1.3.2.1.cmml\" xref=\"S7.T5.7.7.1.m1.1.1.3.2\">superscript</csymbol><cn type=\"integer\" id=\"S7.T5.7.7.1.m1.1.1.3.2.2.cmml\" xref=\"S7.T5.7.7.1.m1.1.1.3.2.2\">10</cn><minus id=\"S7.T5.7.7.1.m1.1.1.3.2.3.cmml\" xref=\"S7.T5.7.7.1.m1.1.1.3.2.3\"></minus></apply><cn type=\"integer\" id=\"S7.T5.7.7.1.m1.1.1.3.3.cmml\" xref=\"S7.T5.7.7.1.m1.1.1.3.3\">4</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T5.7.7.1.m1.1c\">\\times 10^{-}4</annotation></semantics></math>\n</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "We measure the uncertainty(entropy) in terms of mean and variance for all the answer prediction in the validation dataset. We also measure uncertainty for an individual question in the dataset. Here, we split our training data into three parts. In the first part, the model is trained with 50% of the training data. In the second part, the model is trained with 75% of training data, and in the third part, the model is trained with the full dataset as shown in the second block of the table- V. It is observed that the model uncertainty(epistemic uncertainty) decreases as training data increases.",
            "We analyze the contribution of each term in the uncertainty loss to estimate data uncertainty, as shown in the first block of the table V. From measurements, it can be seen that comparing aleatoric uncertainty of an image with the epistemic uncertainty of another image doesn’t make sense because of the significant difference in their values. However, both the uncertainties can be individually compared with the corresponding uncertainties of other images. We capture predictive uncertainty by combining aleatoric uncertainty with the entropy of the prediction (epistemic uncertainty), as mentioned in equation 5 of this paper. Finally, we provide a variation of aleatoric uncertainty and uncertainty distorted loss over a number of epochs, as shown in figure- 10."
        ]
    },
    "S7.T6": {
        "caption": "TABLE VI: SOTA: Open-Ended VQA2.0 accuracy on test-dev",
        "table": "<table id=\"S7.T6.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S7.T6.1.1.1\" class=\"ltx_tr\">\n<th id=\"S7.T6.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S7.T6.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Models</span></th>\n<td id=\"S7.T6.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S7.T6.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">All</span></td>\n<td id=\"S7.T6.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S7.T6.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Y/N</span></td>\n<td id=\"S7.T6.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S7.T6.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">Num</span></td>\n<td id=\"S7.T6.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S7.T6.1.1.1.5.1\" class=\"ltx_text ltx_font_bold\">Oth</span></td>\n</tr>\n<tr id=\"S7.T6.1.2.2\" class=\"ltx_tr\">\n<th id=\"S7.T6.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">SAN-2<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib29\" title=\"\" class=\"ltx_ref\">29</a>]</cite>\n</th>\n<td id=\"S7.T6.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">56.9</td>\n<td id=\"S7.T6.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">74.1</td>\n<td id=\"S7.T6.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">35.5</td>\n<td id=\"S7.T6.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">44.5</td>\n</tr>\n<tr id=\"S7.T6.1.3.3\" class=\"ltx_tr\">\n<th id=\"S7.T6.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">MCB <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">6</a>]</cite>\n</th>\n<td id=\"S7.T6.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">64.0</td>\n<td id=\"S7.T6.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">78.8</td>\n<td id=\"S7.T6.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\">38.3</td>\n<td id=\"S7.T6.1.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r\">53.3</td>\n</tr>\n<tr id=\"S7.T6.1.4.4\" class=\"ltx_tr\">\n<th id=\"S7.T6.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">Bottom[<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">49</a>]</cite>]</th>\n<td id=\"S7.T6.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">65.3</td>\n<td id=\"S7.T6.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">81.8</td>\n<td id=\"S7.T6.1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_r\">44.2</td>\n<td id=\"S7.T6.1.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_r\">56.0</td>\n</tr>\n<tr id=\"S7.T6.1.5.5\" class=\"ltx_tr\">\n<th id=\"S7.T6.1.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">DVQA<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib31\" title=\"\" class=\"ltx_ref\">31</a>]</cite>\n</th>\n<td id=\"S7.T6.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\">65.9</td>\n<td id=\"S7.T6.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\">82.4</td>\n<td id=\"S7.T6.1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_r\">43.2</td>\n<td id=\"S7.T6.1.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_r\">56.8</td>\n</tr>\n<tr id=\"S7.T6.1.6.6\" class=\"ltx_tr\">\n<th id=\"S7.T6.1.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">MLB <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib32\" title=\"\" class=\"ltx_ref\">32</a>]</cite>\n</th>\n<td id=\"S7.T6.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\">66.3</td>\n<td id=\"S7.T6.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\">83.6</td>\n<td id=\"S7.T6.1.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_r\">44.9</td>\n<td id=\"S7.T6.1.6.6.5\" class=\"ltx_td ltx_align_center ltx_border_r\">56.3</td>\n</tr>\n<tr id=\"S7.T6.1.7.7\" class=\"ltx_tr\">\n<th id=\"S7.T6.1.7.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">DA-NTN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib50\" title=\"\" class=\"ltx_ref\">50</a>]</cite>\n</th>\n<td id=\"S7.T6.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_r\">67.5</td>\n<td id=\"S7.T6.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\">84.3</td>\n<td id=\"S7.T6.1.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_r\">47.1</td>\n<td id=\"S7.T6.1.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_r\">57.9</td>\n</tr>\n<tr id=\"S7.T6.1.8.8\" class=\"ltx_tr\">\n<th id=\"S7.T6.1.8.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">Counter<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib51\" title=\"\" class=\"ltx_ref\">51</a>]</cite>\n</th>\n<td id=\"S7.T6.1.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_r\">68.0</td>\n<td id=\"S7.T6.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r\">83.1</td>\n<td id=\"S7.T6.1.8.8.4\" class=\"ltx_td ltx_align_center ltx_border_r\">51.6</td>\n<td id=\"S7.T6.1.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_r\">58.9</td>\n</tr>\n<tr id=\"S7.T6.1.9.9\" class=\"ltx_tr\">\n<th id=\"S7.T6.1.9.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">BAN<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib33\" title=\"\" class=\"ltx_ref\">33</a>]</cite>\n</th>\n<td id=\"S7.T6.1.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T6.1.9.9.2.1\" class=\"ltx_text ltx_font_bold\">69.5</span></td>\n<td id=\"S7.T6.1.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r\">85.3</td>\n<td id=\"S7.T6.1.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T6.1.9.9.4.1\" class=\"ltx_text ltx_font_bold\">50.9</span></td>\n<td id=\"S7.T6.1.9.9.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S7.T6.1.9.9.5.1\" class=\"ltx_text ltx_font_bold\">60.2</span></td>\n</tr>\n<tr id=\"S7.T6.1.10.10\" class=\"ltx_tr\">\n<th id=\"S7.T6.1.10.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">P-GCA + SAN (ours)</th>\n<td id=\"S7.T6.1.10.10.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">59.2</td>\n<td id=\"S7.T6.1.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">75.7</td>\n<td id=\"S7.T6.1.10.10.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">36.6</td>\n<td id=\"S7.T6.1.10.10.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">46.8</td>\n</tr>\n<tr id=\"S7.T6.1.11.11\" class=\"ltx_tr\">\n<th id=\"S7.T6.1.11.11.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">P-GCA + MCB (ours)</th>\n<td id=\"S7.T6.1.11.11.2\" class=\"ltx_td ltx_align_center ltx_border_r\">65.7</td>\n<td id=\"S7.T6.1.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_r\">79.6</td>\n<td id=\"S7.T6.1.11.11.4\" class=\"ltx_td ltx_align_center ltx_border_r\">40.1</td>\n<td id=\"S7.T6.1.11.11.5\" class=\"ltx_td ltx_align_center ltx_border_r\">54.7</td>\n</tr>\n<tr id=\"S7.T6.1.12.12\" class=\"ltx_tr\">\n<th id=\"S7.T6.1.12.12.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\">P-GCA + Counter (ours)</th>\n<td id=\"S7.T6.1.12.12.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">69.2</td>\n<td id=\"S7.T6.1.12.12.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S7.T6.1.12.12.3.1\" class=\"ltx_text ltx_font_bold\">85.4</span></td>\n<td id=\"S7.T6.1.12.12.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">50.1</td>\n<td id=\"S7.T6.1.12.12.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">59.4</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "VQA-v2[10]: We provide benchmark result on VQA-v2[10] dataset. This dataset removes bias present in VQA-v1 by adding a conjugate image pair. It contains 443,757 image-question pairs on the training set, 214,354 pairs on the validation set, and 447,793 pairs on the test set, which is more than twice the first version. All the questions and answers pairs are annotated by human annotators. The benchmark results on the VQA-v2 dataset is presented in table-VI.",
            "We compare attention maps produced by our proposed GCA model, and it’s variants with the base model and reports them in Table-IV. Rank correlation and EMD scores are calculated for the produced attention map against human-annotated attention (HAT) maps  [34]. In the table, as we approach the best-proposed GCA model, Rank correlation (RC) is increasing. EMD is also decreasing (Lower the better) as we move towards GCA. To verify our intuition, that we can learn better attention masks by minimizing the uncertainty present in the attention mask; we start with VE and observe that both rank correlation and answer accuracy increase by 0.42 and 0.3 % from baseline, respectively. We also observe that with UDL, AUL, and PUL based loss minimization technique, both RC and EMD improves, as shown in the Table- IV.\nAleatoric-GCA (A-GCA) improves 5.21% in terms of RC and 2.5% in terms of accuracy. Finally, the proposed Predictive-GCA (P-GCA), which is modeled to consider both data and the model uncertainty, improves the RC by 5.51% and accuracy by 2.7% as shown in the Table- IV and Table- II. Since HAT maps are only available for the VQA-v1 dataset, thus, this ablation analysis has been performed only for VQA-v1. We also providing SOTA results for VQA-v1 and VQA-v2 dataset as shown in Table- VII and Table- VI respectively.\nAlso, we compare with our gradient certainty explanation with human explanation present in the VQA-v2 dataset for the various model, as mentioned in Table- III. This human explanation mask only available for the VQA-v2 dataset. We observe that our attention (P-GCA) mask performs better than others as well.",
            "We obtain the initial comparison with the baselines on the rank correlation on human attention (HAT) dataset [34] that provides human attention while solving for VQA. Between humans, the rank correlation is 62.3%. The comparison of various state-of-the-art methods and baselines are provided in Table IV. We use a variant of MCB [6] model as our baseline method. We obtain an improvement of around 5.2% using the A-GCA model and 5.51% using the P-GCA model in terms of rank correlation with human attention. From this, we justify that our attention map is more similar to the human attention map. We also compare with the baselines on the answer accuracy on the VQA-v1[8] dataset, as shown in Table- VII. We obtain an improvement of around 2.7% over the comparable MCB baseline. Our MCB based model A-GCA and P-GCA improves by 0.9% and 1.1% accuracy as compared to state of the art model DVQA [31] on VQA-v1. However, using a saliency-based method  [53] that is trained on eye-tracking data to obtain a measure of where people look in a task-independent manner, results in more correlation with human attention (0.49), as noted by [34]. However, this is explicitly trained using human attention and is not task-dependent. In our approach, we aim to obtain a method that can simulate human cognitive abilities for solving the tasks. We provide state of the art results for VQA-v2 in Table- VI. This table shows that using the GCA method, the VQA result improves. We have provided more results for attention map visualization for both types of uncertainty, training setup, dataset, and evaluation methods here.222https://delta-lab-iitk.github.io/U-CAM/ ."
        ]
    },
    "S7.T7": {
        "caption": "TABLE VII: SOTA: Open-Ended VQA1.0 accuracy on test-dev",
        "table": "<table id=\"S7.T7.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S7.T7.1.1.1\" class=\"ltx_tr\">\n<th id=\"S7.T7.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S7.T7.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Models</span></th>\n<td id=\"S7.T7.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S7.T7.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">All</span></td>\n<td id=\"S7.T7.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S7.T7.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Y/N</span></td>\n<td id=\"S7.T7.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S7.T7.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">Num</span></td>\n<td id=\"S7.T7.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S7.T7.1.1.1.5.1\" class=\"ltx_text ltx_font_bold\">Oth</span></td>\n</tr>\n<tr id=\"S7.T7.1.2.2\" class=\"ltx_tr\">\n<th id=\"S7.T7.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">DPPnet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>]</cite>\n</th>\n<td id=\"S7.T7.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">57.2</td>\n<td id=\"S7.T7.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">80.7</td>\n<td id=\"S7.T7.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">37.2</td>\n<td id=\"S7.T7.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">41.7</td>\n</tr>\n<tr id=\"S7.T7.1.3.3\" class=\"ltx_tr\">\n<th id=\"S7.T7.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">SMem[<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">26</a>]</cite>]</th>\n<td id=\"S7.T7.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">58.0</td>\n<td id=\"S7.T7.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">80.9</td>\n<td id=\"S7.T7.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\">37.3</td>\n<td id=\"S7.T7.1.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r\">43.1</td>\n</tr>\n<tr id=\"S7.T7.1.4.4\" class=\"ltx_tr\">\n<th id=\"S7.T7.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">SAN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib29\" title=\"\" class=\"ltx_ref\">29</a>]</cite>\n</th>\n<td id=\"S7.T7.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">58.7</td>\n<td id=\"S7.T7.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">79.3</td>\n<td id=\"S7.T7.1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_r\">36.6</td>\n<td id=\"S7.T7.1.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_r\">46.1</td>\n</tr>\n<tr id=\"S7.T7.1.5.5\" class=\"ltx_tr\">\n<th id=\"S7.T7.1.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">DMN<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib52\" title=\"\" class=\"ltx_ref\">52</a>]</cite>\n</th>\n<td id=\"S7.T7.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\">60.3</td>\n<td id=\"S7.T7.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\">80.5</td>\n<td id=\"S7.T7.1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_r\">36.8</td>\n<td id=\"S7.T7.1.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_r\">48.3</td>\n</tr>\n<tr id=\"S7.T7.1.6.6\" class=\"ltx_tr\">\n<th id=\"S7.T7.1.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">QRU(2)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">30</a>]</cite>\n</th>\n<td id=\"S7.T7.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\">60.7</td>\n<td id=\"S7.T7.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\">82.3</td>\n<td id=\"S7.T7.1.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_r\">37.0</td>\n<td id=\"S7.T7.1.6.6.5\" class=\"ltx_td ltx_align_center ltx_border_r\">47.7</td>\n</tr>\n<tr id=\"S7.T7.1.7.7\" class=\"ltx_tr\">\n<th id=\"S7.T7.1.7.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">HieCoAtt <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">27</a>]</cite>\n</th>\n<td id=\"S7.T7.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_r\">61.8</td>\n<td id=\"S7.T7.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\">79.7</td>\n<td id=\"S7.T7.1.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_r\">38.9</td>\n<td id=\"S7.T7.1.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_r\">51.7</td>\n</tr>\n<tr id=\"S7.T7.1.8.8\" class=\"ltx_tr\">\n<th id=\"S7.T7.1.8.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">MCB <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">6</a>]</cite>\n</th>\n<td id=\"S7.T7.1.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_r\">64.2</td>\n<td id=\"S7.T7.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r\">82.2</td>\n<td id=\"S7.T7.1.8.8.4\" class=\"ltx_td ltx_align_center ltx_border_r\">37.7</td>\n<td id=\"S7.T7.1.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_r\">54.8</td>\n</tr>\n<tr id=\"S7.T7.1.9.9\" class=\"ltx_tr\">\n<th id=\"S7.T7.1.9.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">MLB <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib32\" title=\"\" class=\"ltx_ref\">32</a>]</cite>\n</th>\n<td id=\"S7.T7.1.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_r\">65.0</td>\n<td id=\"S7.T7.1.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r\">84.0</td>\n<td id=\"S7.T7.1.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_r\">37.9</td>\n<td id=\"S7.T7.1.9.9.5\" class=\"ltx_td ltx_align_center ltx_border_r\">54.7</td>\n</tr>\n<tr id=\"S7.T7.1.10.10\" class=\"ltx_tr\">\n<th id=\"S7.T7.1.10.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">DVQA<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib31\" title=\"\" class=\"ltx_ref\">31</a>]</cite>\n</th>\n<td id=\"S7.T7.1.10.10.2\" class=\"ltx_td ltx_align_center ltx_border_r\">65.4</td>\n<td id=\"S7.T7.1.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r\">83.8</td>\n<td id=\"S7.T7.1.10.10.4\" class=\"ltx_td ltx_align_center ltx_border_r\">38.1</td>\n<td id=\"S7.T7.1.10.10.5\" class=\"ltx_td ltx_align_center ltx_border_r\">55.2</td>\n</tr>\n<tr id=\"S7.T7.1.11.11\" class=\"ltx_tr\">\n<th id=\"S7.T7.1.11.11.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">P-GCA + SAN (ours)</th>\n<td id=\"S7.T7.1.11.11.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">60.4</td>\n<td id=\"S7.T7.1.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">80.7</td>\n<td id=\"S7.T7.1.11.11.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">36.6</td>\n<td id=\"S7.T7.1.11.11.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">47.9</td>\n</tr>\n<tr id=\"S7.T7.1.12.12\" class=\"ltx_tr\">\n<th id=\"S7.T7.1.12.12.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">A-GCA + MCB (ours)</th>\n<td id=\"S7.T7.1.12.12.2\" class=\"ltx_td ltx_align_center ltx_border_r\">66.3</td>\n<td id=\"S7.T7.1.12.12.3\" class=\"ltx_td ltx_align_center ltx_border_r\">84.2</td>\n<td id=\"S7.T7.1.12.12.4\" class=\"ltx_td ltx_align_center ltx_border_r\">38.0</td>\n<td id=\"S7.T7.1.12.12.5\" class=\"ltx_td ltx_align_center ltx_border_r\">55.5</td>\n</tr>\n<tr id=\"S7.T7.1.13.13\" class=\"ltx_tr\">\n<th id=\"S7.T7.1.13.13.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\">P-GCA + MCB (ours)</th>\n<td id=\"S7.T7.1.13.13.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S7.T7.1.13.13.2.1\" class=\"ltx_text ltx_font_bold\">66.5</span></td>\n<td id=\"S7.T7.1.13.13.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S7.T7.1.13.13.3.1\" class=\"ltx_text ltx_font_bold\">84.6</span></td>\n<td id=\"S7.T7.1.13.13.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S7.T7.1.13.13.4.1\" class=\"ltx_text ltx_font_bold\">38.4</span></td>\n<td id=\"S7.T7.1.13.13.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S7.T7.1.13.13.5.1\" class=\"ltx_text ltx_font_bold\">55.9</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "We compare attention maps produced by our proposed GCA model, and it’s variants with the base model and reports them in Table-IV. Rank correlation and EMD scores are calculated for the produced attention map against human-annotated attention (HAT) maps  [34]. In the table, as we approach the best-proposed GCA model, Rank correlation (RC) is increasing. EMD is also decreasing (Lower the better) as we move towards GCA. To verify our intuition, that we can learn better attention masks by minimizing the uncertainty present in the attention mask; we start with VE and observe that both rank correlation and answer accuracy increase by 0.42 and 0.3 % from baseline, respectively. We also observe that with UDL, AUL, and PUL based loss minimization technique, both RC and EMD improves, as shown in the Table- IV.\nAleatoric-GCA (A-GCA) improves 5.21% in terms of RC and 2.5% in terms of accuracy. Finally, the proposed Predictive-GCA (P-GCA), which is modeled to consider both data and the model uncertainty, improves the RC by 5.51% and accuracy by 2.7% as shown in the Table- IV and Table- II. Since HAT maps are only available for the VQA-v1 dataset, thus, this ablation analysis has been performed only for VQA-v1. We also providing SOTA results for VQA-v1 and VQA-v2 dataset as shown in Table- VII and Table- VI respectively.\nAlso, we compare with our gradient certainty explanation with human explanation present in the VQA-v2 dataset for the various model, as mentioned in Table- III. This human explanation mask only available for the VQA-v2 dataset. We observe that our attention (P-GCA) mask performs better than others as well.",
            "We obtain the initial comparison with the baselines on the rank correlation on human attention (HAT) dataset [34] that provides human attention while solving for VQA. Between humans, the rank correlation is 62.3%. The comparison of various state-of-the-art methods and baselines are provided in Table IV. We use a variant of MCB [6] model as our baseline method. We obtain an improvement of around 5.2% using the A-GCA model and 5.51% using the P-GCA model in terms of rank correlation with human attention. From this, we justify that our attention map is more similar to the human attention map. We also compare with the baselines on the answer accuracy on the VQA-v1[8] dataset, as shown in Table- VII. We obtain an improvement of around 2.7% over the comparable MCB baseline. Our MCB based model A-GCA and P-GCA improves by 0.9% and 1.1% accuracy as compared to state of the art model DVQA [31] on VQA-v1. However, using a saliency-based method  [53] that is trained on eye-tracking data to obtain a measure of where people look in a task-independent manner, results in more correlation with human attention (0.49), as noted by [34]. However, this is explicitly trained using human attention and is not task-dependent. In our approach, we aim to obtain a method that can simulate human cognitive abilities for solving the tasks. We provide state of the art results for VQA-v2 in Table- VI. This table shows that using the GCA method, the VQA result improves. We have provided more results for attention map visualization for both types of uncertainty, training setup, dataset, and evaluation methods here.222https://delta-lab-iitk.github.io/U-CAM/ ."
        ]
    },
    "S7.T8": {
        "caption": "TABLE VIII:  Reference for the Figure- 7(c).",
        "table": "<table id=\"S7.T8.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S7.T8.1.1.1\" class=\"ltx_tr\">\n<th id=\"S7.T8.1.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S7.T8.1.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T8.1.1.1.1.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\"><span id=\"S7.T8.1.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Question</span></span>\n</span>\n</th>\n<th id=\"S7.T8.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S7.T8.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">ID</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S7.T8.1.2.1\" class=\"ltx_tr\">\n<td id=\"S7.T8.1.2.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S7.T8.1.2.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T8.1.2.1.1.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\">What does the person in this picture have on his face?</span>\n</span>\n</td>\n<td id=\"S7.T8.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S7.T8.1.3.2\" class=\"ltx_tr\">\n<td id=\"S7.T8.1.3.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r\">\n<span id=\"S7.T8.1.3.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T8.1.3.2.1.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\">How many baby elephants are there?</span>\n</span>\n</td>\n<td id=\"S7.T8.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">2</td>\n</tr>\n<tr id=\"S7.T8.1.4.3\" class=\"ltx_tr\">\n<td id=\"S7.T8.1.4.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r\">\n<span id=\"S7.T8.1.4.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T8.1.4.3.1.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\">What is in the bowl?</span>\n</span>\n</td>\n<td id=\"S7.T8.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">3</td>\n</tr>\n<tr id=\"S7.T8.1.5.4\" class=\"ltx_tr\">\n<td id=\"S7.T8.1.5.4.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r\">\n<span id=\"S7.T8.1.5.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T8.1.5.4.1.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\">Is the television on or off?</span>\n</span>\n</td>\n<td id=\"S7.T8.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">4</td>\n</tr>\n<tr id=\"S7.T8.1.6.5\" class=\"ltx_tr\">\n<td id=\"S7.T8.1.6.5.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r\">\n<span id=\"S7.T8.1.6.5.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T8.1.6.5.1.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\">What color is the walk light?</span>\n</span>\n</td>\n<td id=\"S7.T8.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\">5</td>\n</tr>\n<tr id=\"S7.T8.1.7.6\" class=\"ltx_tr\">\n<td id=\"S7.T8.1.7.6.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r\">\n<span id=\"S7.T8.1.7.6.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T8.1.7.6.1.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\">Which way is its head turned?</span>\n</span>\n</td>\n<td id=\"S7.T8.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\">6</td>\n</tr>\n<tr id=\"S7.T8.1.8.7\" class=\"ltx_tr\">\n<td id=\"S7.T8.1.8.7.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r\">\n<span id=\"S7.T8.1.8.7.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T8.1.8.7.1.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\">How many people are riding on each bike?</span>\n</span>\n</td>\n<td id=\"S7.T8.1.8.7.2\" class=\"ltx_td ltx_align_center ltx_border_r\">7</td>\n</tr>\n<tr id=\"S7.T8.1.9.8\" class=\"ltx_tr\">\n<td id=\"S7.T8.1.9.8.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r\">\n<span id=\"S7.T8.1.9.8.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T8.1.9.8.1.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\">What animal is in this picture?</span>\n</span>\n</td>\n<td id=\"S7.T8.1.9.8.2\" class=\"ltx_td ltx_align_center ltx_border_r\">8</td>\n</tr>\n<tr id=\"S7.T8.1.10.9\" class=\"ltx_tr\">\n<td id=\"S7.T8.1.10.9.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r\">\n<span id=\"S7.T8.1.10.9.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T8.1.10.9.1.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\">What color is the road?</span>\n</span>\n</td>\n<td id=\"S7.T8.1.10.9.2\" class=\"ltx_td ltx_align_center ltx_border_r\">9</td>\n</tr>\n<tr id=\"S7.T8.1.11.10\" class=\"ltx_tr\">\n<td id=\"S7.T8.1.11.10.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r\">\n<span id=\"S7.T8.1.11.10.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T8.1.11.10.1.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\">What color is the boy’s hair?</span>\n</span>\n</td>\n<td id=\"S7.T8.1.11.10.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">10</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Further, we plotted Predictive uncertainty (Figure- 7(a,b)) of some randomly chosen samples against the Classification error (error=log⁡11−p11𝑝\\log{\\frac{1}{1-p}}, where p is the probability of misclassification). As seen, when the samples are correct, they are also certain and have less Classification Error (CE). To visualize the direct effect of decreased uncertainty, we plotted (Figure- 7(c, d)). It can be seen that how similar classes, like (glasses, sunglasses) and (black, gray), etc., thus leading to uncertainty, got separated more in the logit space in the proposed model. Table- VIII and - IX shows list of the questions and its corresponding id’s is present in the figure-5."
        ]
    },
    "S7.T9": {
        "caption": "TABLE IX:  Reference for the Figure- 7(d).",
        "table": "<table id=\"S7.T9.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S7.T9.1.1.1\" class=\"ltx_tr\">\n<th id=\"S7.T9.1.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S7.T9.1.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T9.1.1.1.1.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\"><span id=\"S7.T9.1.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Question</span></span>\n</span>\n</th>\n<th id=\"S7.T9.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S7.T9.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">ID</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S7.T9.1.2.1\" class=\"ltx_tr\">\n<td id=\"S7.T9.1.2.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S7.T9.1.2.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T9.1.2.1.1.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\">Is this wheat bread?</span>\n</span>\n</td>\n<td id=\"S7.T9.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n</tr>\n<tr id=\"S7.T9.1.3.2\" class=\"ltx_tr\">\n<td id=\"S7.T9.1.3.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r\">\n<span id=\"S7.T9.1.3.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T9.1.3.2.1.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\">Is the cat looking at the camera?</span>\n</span>\n</td>\n<td id=\"S7.T9.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">2</td>\n</tr>\n<tr id=\"S7.T9.1.4.3\" class=\"ltx_tr\">\n<td id=\"S7.T9.1.4.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r\">\n<span id=\"S7.T9.1.4.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T9.1.4.3.1.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\">Is this chair broken?</span>\n</span>\n</td>\n<td id=\"S7.T9.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">3</td>\n</tr>\n<tr id=\"S7.T9.1.5.4\" class=\"ltx_tr\">\n<td id=\"S7.T9.1.5.4.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r\">\n<span id=\"S7.T9.1.5.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T9.1.5.4.1.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\">Are these animals monitored?</span>\n</span>\n</td>\n<td id=\"S7.T9.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">4</td>\n</tr>\n<tr id=\"S7.T9.1.6.5\" class=\"ltx_tr\">\n<td id=\"S7.T9.1.6.5.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r\">\n<span id=\"S7.T9.1.6.5.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T9.1.6.5.1.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\">Does the cat recognize someone?</span>\n</span>\n</td>\n<td id=\"S7.T9.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\">5</td>\n</tr>\n<tr id=\"S7.T9.1.7.6\" class=\"ltx_tr\">\n<td id=\"S7.T9.1.7.6.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r\">\n<span id=\"S7.T9.1.7.6.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T9.1.7.6.1.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\">Is the figurine life size?</span>\n</span>\n</td>\n<td id=\"S7.T9.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\">6</td>\n</tr>\n<tr id=\"S7.T9.1.8.7\" class=\"ltx_tr\">\n<td id=\"S7.T9.1.8.7.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r\">\n<span id=\"S7.T9.1.8.7.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T9.1.8.7.1.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\">Is the smaller dog on a leash?</span>\n</span>\n</td>\n<td id=\"S7.T9.1.8.7.2\" class=\"ltx_td ltx_align_center ltx_border_r\">7</td>\n</tr>\n<tr id=\"S7.T9.1.9.8\" class=\"ltx_tr\">\n<td id=\"S7.T9.1.9.8.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r\">\n<span id=\"S7.T9.1.9.8.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T9.1.9.8.1.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\">Is this in the mountains?</span>\n</span>\n</td>\n<td id=\"S7.T9.1.9.8.2\" class=\"ltx_td ltx_align_center ltx_border_r\">8</td>\n</tr>\n<tr id=\"S7.T9.1.10.9\" class=\"ltx_tr\">\n<td id=\"S7.T9.1.10.9.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r\">\n<span id=\"S7.T9.1.10.9.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T9.1.10.9.1.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\">Is the woman sitting on the bench?</span>\n</span>\n</td>\n<td id=\"S7.T9.1.10.9.2\" class=\"ltx_td ltx_align_center ltx_border_r\">9</td>\n</tr>\n<tr id=\"S7.T9.1.11.10\" class=\"ltx_tr\">\n<td id=\"S7.T9.1.11.10.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r\">\n<span id=\"S7.T9.1.11.10.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S7.T9.1.11.10.1.1.1\" class=\"ltx_p\" style=\"width:119.5pt;\">Is the church empty?</span>\n</span>\n</td>\n<td id=\"S7.T9.1.11.10.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">10</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Further, we plotted Predictive uncertainty (Figure- 7(a,b)) of some randomly chosen samples against the Classification error (error=log⁡11−p11𝑝\\log{\\frac{1}{1-p}}, where p is the probability of misclassification). As seen, when the samples are correct, they are also certain and have less Classification Error (CE). To visualize the direct effect of decreased uncertainty, we plotted (Figure- 7(c, d)). It can be seen that how similar classes, like (glasses, sunglasses) and (black, gray), etc., thus leading to uncertainty, got separated more in the logit space in the proposed model. Table- VIII and - IX shows list of the questions and its corresponding id’s is present in the figure-5."
        ]
    }
}