{
    "id_table_1": {
        "caption": "Table 1:  Features used in Depth Estimator.",
        "table": "S2.T1.1",
        "footnotes": [],
        "references": [
            "The use of generative models and machine learning has gained significant attention in flood risk analysis. Recent studies have adopted Generative Adversarial Networks (GANs) to improve data generation, such as  [ 4 ] , which uses GANs to generate synthetic precipitation data integrated with the Soil and Water Assessment Tool (SWAT) to enhance flood frequency analysis. Similarly,  [ 5 ]  employs machine learning models within a GAN framework for real-time flood forecasting in urban environments, improving prediction accuracy and emergency response strategies through event-based data modeling. Deep learning models, including Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), have been used for predicting flash flood probabilities and integrating with hydrodynamic models for efficient flood mapping and risk assessment  [ 6 ] . Machine learning algorithms like Random Forest and Extreme Gradient Boosting (XGBoost) have been applied to assess flood susceptibility and generate flood risk maps by analyzing key flood conditioning factors  [ 7 ,  8 ] . Furthermore, studies such as  [ 9 ,  10 ]  have developed ML-based models, including Long Short-Term Memory (LSTM) networks and conditional GANs, to enhance flood prediction accuracy in specific urban settings. Despite these advancements, there is currently no model for simulating synthetic flood data specifically for creating probabilistic flood maps. To address these challenges, we introduce  Flood-Precip GAN , a novel methodology leveraging generative machine learning to synthesize large-scale inundation data. The main workflow is illustrated in Fig.  1 .  Flood-Precip GAN  starts with the training of a depth estimator using a number of physics-based model-generated flood events, incorporating precipitation-based, spatial, and region-specific features. Following this, a Conditional GAN (CTGAN) with constraints is employed to generate synthetic rainfall precipitation events. Strategic thresholds are established to filter these synthetic records, ensuring close alignment with true precipitation patterns. For each location, synthetic events are smoothed using a K-nearest neighbors (KNN) algorithm and processed through the depth estimator to derive synthetic depth distributions. By iterating this procedure across multiple synthetic rainfall events, we construct flood probability maps for different rainfall categories with certain duration, maximum, and cumulative precipitation ranges. These maps are validated using similarity and correlation metrics to confirm the correspondence of synthetic depth distributions to true data. The novel contributions of the model presented in this paper are threefold:",
            "For the depth estimator component of  Flood-Precip GAN , we chose a tree-based machine learning model trained on simulated scenarios from the HEC-RAS 2D model. Compared to traditional machine learning models, tree-based models have shown superior performance due to their ability to manage complex, non-linear relationships and interactions within the data. XGBoost  [ 16 ,  17 ] , in particular, excels with its gradient boosting framework, which iteratively improves model accuracy by minimizing residual errors from previous iterations. Additionally, deep learning models in regression, such as the Transformer  [ 18 ,  19 ] , utilize attention mechanisms to capture long-range dependencies and complex feature interactions, providing substantial benefits in predictive performance. In this study, we employed the XGBoost regressor and the regression transformer as universal depth estimators, training them on physics-based model-generated events that include both spatial features incorporating channel presence and terrain elevation, and precipitation-based features as shown in Table  1 . This integration aims to enhance the accuracy and robustness of maximum flood inundation estimations. Furthermore, we compared these universal models with a cell-wise model, MaxFloodCast V2  [ 20 ] , an XGBoost-based depth estimator trained specifically on individual cell meshes, resulting in 26,301 models corresponding to the 26,301 cells. Due to the unique nature of cell-wise model implementation, we assumed geospatial changes to be negligible across all events, thereby excluding spatial features. To account for the influence of inter-watershed precipitation without directly utilizing static geospatial features, we introduced cumulative and peak precipitation ratios as shown in Eq.  1  and Eq.  2  to effectively capture interactions among watersheds.",
            "We assessed the predictive performance of the depth estimators (in Section  3.1 ) using Root Mean Squared Error (RMSE) and R-squared ( R 2 superscript R 2 R^{2} italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) scores  [ 21 ]  to evaluate both accuracy and explainability. RMSE measures the average magnitude of the prediction errors, providing insight into the estimators precision, while  R 2 superscript R 2 R^{2} italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  indicates the proportion of variance in the observed data that is predictable from the input features, reflecting the models explanatory power. In addition, we conducted a comprehensive analysis of the aggregated performance of these models on both channel and non-channel cells. This distinction is crucial due to the varying hydrological and ground texture characteristics inherent to these environments, which influence the uncertainty and behavior of flood dynamics."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Definition of LMH Thresholds for Each Precipitation-Based Feature within a Cell Mesh.",
        "table": "S2.T2.3",
        "footnotes": [],
        "references": [
            "The cell-wise machine learning modeling, introduced in Section  2.3 , presents an innovative approach to feature engineering, yielding enhanced prediction performance compared to global models.",
            "As Fig.  2  depicts, the test bed for creating the  Flood-Precip GAN  model is Harris County in Texas. The main watersheds are Cypress Creek and Buffalo Bayou. Harris County, the core county of the Greater Houston Metropolitan Statistical Area, spans 1,778 square miles (4,605 square kilometers) and has grown to over 4.5 million residents in the past decade. The countys flat topography, with elevations ranging from -40 ft (-12.19 meters) to 300 ft (91.44 meters), and predominantly developed land (over two-thirds), along with 20% pasture and cultivated lands  [ 11 ] , contribute to its flood risk. The Cypress Creek watershed in the north and the Buffalo Bayou system in the central and southern parts flow into the San Jacinto River and the Ship Channel, eventually reaching the Gulf of Mexico. The areas dense development, poor natural drainage, limited soil infiltration, and subtropical climate significantly heighten its susceptibility to chronic flooding.",
            "For the depth estimator component of  Flood-Precip GAN , we chose a tree-based machine learning model trained on simulated scenarios from the HEC-RAS 2D model. Compared to traditional machine learning models, tree-based models have shown superior performance due to their ability to manage complex, non-linear relationships and interactions within the data. XGBoost  [ 16 ,  17 ] , in particular, excels with its gradient boosting framework, which iteratively improves model accuracy by minimizing residual errors from previous iterations. Additionally, deep learning models in regression, such as the Transformer  [ 18 ,  19 ] , utilize attention mechanisms to capture long-range dependencies and complex feature interactions, providing substantial benefits in predictive performance. In this study, we employed the XGBoost regressor and the regression transformer as universal depth estimators, training them on physics-based model-generated events that include both spatial features incorporating channel presence and terrain elevation, and precipitation-based features as shown in Table  1 . This integration aims to enhance the accuracy and robustness of maximum flood inundation estimations. Furthermore, we compared these universal models with a cell-wise model, MaxFloodCast V2  [ 20 ] , an XGBoost-based depth estimator trained specifically on individual cell meshes, resulting in 26,301 models corresponding to the 26,301 cells. Due to the unique nature of cell-wise model implementation, we assumed geospatial changes to be negligible across all events, thereby excluding spatial features. To account for the influence of inter-watershed precipitation without directly utilizing static geospatial features, we introduced cumulative and peak precipitation ratios as shown in Eq.  1  and Eq.  2  to effectively capture interactions among watersheds.",
            "Where   i subscript  i \\mu_{i} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is the mean of a precipitation-based feature  x i subscript x i x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  from training dataset, and   i subscript  i \\sigma_{i} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  refers to its standard deviation.   1 subscript  1 \\theta_{1} italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and   2 subscript  2 \\theta_{2} italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  are two positive constants. Consequently, for each cell mesh, we established low (L), medium (M), and high (H) thresholds based on the distribution of precipitation-based features (cumulative precipitation, peak precipitation and duration) from the training dataset. These thresholds were then applied to the synthetic point dataset generated as described in section  2.4 . We conducted a comprehensive iteration over all synthetic point records, retaining the index of a point record to corresponding cell-level feature pool only if all its synthetic precipitation-based features consistently matched within the same LMH classification. This rigorous iterative screening ensured that each cells pool included indices of synthetic records that not only covered a range of precipitation levels but also met stringent quality criteria.",
            "With the establishment of the synthetic indices pool for each cell, we were enabled to generate global precipitation events across the study area. For each event generation, a specific duration was sampled randomly from the aggregated pool to set as the global rainfall duration, constraining the potential pool. Subsequently, each cell randomly sampled a synthetic record from its tailored pool to represent the entire cell mesh. In instances where some cells returned an empty record, we employed a K-Nearest Neighbors (KNN) approach  [ 25 ]  to generate records by averaging the attributes of surrounding neighbors, effectively filling any gaps. This strategic sampling allowed us to create isolated synthetic precipitation events from a large point cloud, transitioning from point-level to cell-level representation. To enhance the approximation of true precipitation events, we applied the KNN method as a smoother  [ 26 ]  to both cumulative and peak precipitation data. Additionally, we computed heavy precipitation ratios from these smoothed values to highlight interactions among various watersheds. Following this cell-level processing, we utilized the depth estimator with optimal predictive performance as identified in Section  2.3  to generate inundation depths from the aggregated synthetic features, thus completing the precipitation-flood event. After thousands of iterations of individual rainfall event generation, we built synthetic inundation depth distribution for each cell, enabling the estimation of the frequency of rainfall events that cause certain flood depth levels in different areas in producing the synthetic flood probability maps.",
            "where  p p \\mathbf{p} bold_p  and  q q \\mathbf{q} bold_q  are the normalized versions of  d t subscript d t \\mathbf{d_{t}} bold_d start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT  and  d s  superscript subscript d s  \\mathbf{d_{s}^{\\prime}} bold_d start_POSTSUBSCRIPT bold_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , respectively. Besides the averaged synthetic depth distribution across the study region, we further explore the performance by aggregating the results into channel and non-channel levels, as described in Section  2.3 . This detailed analysis allows us to assess how well the synthetic data captures the unique characteristics of different regions in the study area.",
            "To understand the advancements of tree-based models and transformers in regression problems, specifically in the context of inundation depth estimation, we compared the predictive performance of two universal models, XGBoost Regressor and Regression Transformer, with one cell-wise model, MaxFloodCast V2. MaxFloodCast V2 utilizes an XGBoost-based architecture and incorporates heavy precipitation ratio features. We trained the Regression Transformer, configured with 4 layers in the Transformer encoder, a dimensionality of 128, 2,048 in the feed-forward layer, and 8 attention heads with 25% dropout, on 8 NVIDIA RTX A100 GPUs. The XGBoost Regressor was trained on an NVIDIA RTX A6000, while MaxFloodCast V2 was trained in parallel on an AMD EPYC 7702P 64-Core Processor, sharing the same hyperparameter settings as the XGBoost Regressor. The objective function minimized squared errors with a learning rate of 0.01. The XGBoost model built 1000 trees with a maximum depth of 5, incorporating L1 regularization to prevent overfitting and a subsample ratio of 0.3 to introduce non-linearity and enhance robustness. Following the sampling strategy mentioned in Section  2.2 , we configured 252 true precipitation-flood events for training, 63 events for validation, and the remaining events as the test set. The comparison results are presented in Table  3 , which assesses the overall, channel, and non-channel RMSE, as well as the  R 2 superscript R 2 R^{2} italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  score.",
            "We strategically sampled 90 true events following the ratio described in Section  2.2  and configured 36 hyperparameter sets for our grid search. This involved combining six pairs of learning rate settings with six distinct stoppage epochs, increasing from 50 epochs to 300 epochs in increments of 50, and modeling on NVIDIAs RTX A6000 GPU. This meticulous grid search focused on generating records under the condition  duration  1 duration 1 \\text{duration}\\geq 1 duration  1 , targeting the potential simulation of global precipitation events. As shown in Fig.  3 , we identified the optimal checkpoint with a generator learning rate of  10  2 superscript 10 2 10^{-2} 10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT , a discriminator learning rate of  10  3 superscript 10 3 10^{-3} 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT , and early stopping at 250 epochs. Notably, the generator learning rate is set to be 10 times higher than that of the discriminator to prevent the generator from being overpowered in the early stages before it can effectively learn the underlying data patterns. Ultimately, both the generator and discriminator converged before reaching the 300-epoch limit. The optimal settings enabled a direct comparison of cumulative precipitation, peak precipitation, and duration between the training dataset and the 10,000,000 synthetic data points generated by CTGAN, as depicted in Fig.  4 . This configuration produced an average marginal distribution score of 0.802, demonstrating a high fidelity of the synthetic data, particularly for cumulative and peak precipitation when compared to the original dataset. As shown in Fig.  4 (c), we enforced a constraint in CTGAN that limits  duration  1 duration 1 \\text{duration}\\leq 1 duration  1 , since our focus is primarily on global precipitation event generation. Additionally, the CTGAN model maintains the flexibility to generate local precipitation events by allowing cumulative precipitation to drop to zero. Moving forward, the challenge remains to construct isolated precipitation-depth events from the synthetic point dataset that closely correlate with true events, thus advancing the goals of this research in generating probabilistic flood maps.",
            "Using the strategic filtering and sampling methods introduced in Section  2.5 , we generated 10,000 rainfall events through parallel computing. For each event, the global duration was sampled from the tailored pool. As the number of event simulations increases, the overall synthetic precipitation events tend to closely resemble true events. We used 20-nearest neighbors (20-NN) to fill the empty cells after stringent filtering, using averaged precipitation-based feature values. It is common to observe a pepper-salt appearance in cumulative and peak precipitation maps due to the independence of point data in CTGAN records generation. To address this, we applied 20-NN smoothing for synthetic precipitation events with duration of 2 hours or less, and 50-NN smoothing for longer duration, to enhance the realism of the synthetic events. From these smoothed cumulative and peak precipitation features, we derived heavy cumulative and peak precipitation ratios, resulting in 21 input features for the selected depth estimator, MaxFloodCast V2, to generate synthetic inundation depths and complete the precipitation-flood event. The event generation process took an average of 10 minutes to generate 100 rainfall events in parallel, consuming 100 GB of memory on an AMD EPYC 7702P 64-core processor."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Depth Estimator Comparison on true test dataset.",
        "table": "S3.T3.3",
        "footnotes": [
            "",
            "",
            ""
        ],
        "references": [
            "The cell-wise machine learning modeling, introduced in Section  2.3 , presents an innovative approach to feature engineering, yielding enhanced prediction performance compared to global models.",
            "We assessed the predictive performance of the depth estimators (in Section  3.1 ) using Root Mean Squared Error (RMSE) and R-squared ( R 2 superscript R 2 R^{2} italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) scores  [ 21 ]  to evaluate both accuracy and explainability. RMSE measures the average magnitude of the prediction errors, providing insight into the estimators precision, while  R 2 superscript R 2 R^{2} italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  indicates the proportion of variance in the observed data that is predictable from the input features, reflecting the models explanatory power. In addition, we conducted a comprehensive analysis of the aggregated performance of these models on both channel and non-channel cells. This distinction is crucial due to the varying hydrological and ground texture characteristics inherent to these environments, which influence the uncertainty and behavior of flood dynamics.",
            "With the establishment of the synthetic indices pool for each cell, we were enabled to generate global precipitation events across the study area. For each event generation, a specific duration was sampled randomly from the aggregated pool to set as the global rainfall duration, constraining the potential pool. Subsequently, each cell randomly sampled a synthetic record from its tailored pool to represent the entire cell mesh. In instances where some cells returned an empty record, we employed a K-Nearest Neighbors (KNN) approach  [ 25 ]  to generate records by averaging the attributes of surrounding neighbors, effectively filling any gaps. This strategic sampling allowed us to create isolated synthetic precipitation events from a large point cloud, transitioning from point-level to cell-level representation. To enhance the approximation of true precipitation events, we applied the KNN method as a smoother  [ 26 ]  to both cumulative and peak precipitation data. Additionally, we computed heavy precipitation ratios from these smoothed values to highlight interactions among various watersheds. Following this cell-level processing, we utilized the depth estimator with optimal predictive performance as identified in Section  2.3  to generate inundation depths from the aggregated synthetic features, thus completing the precipitation-flood event. After thousands of iterations of individual rainfall event generation, we built synthetic inundation depth distribution for each cell, enabling the estimation of the frequency of rainfall events that cause certain flood depth levels in different areas in producing the synthetic flood probability maps.",
            "where  p p \\mathbf{p} bold_p  and  q q \\mathbf{q} bold_q  are the normalized versions of  d t subscript d t \\mathbf{d_{t}} bold_d start_POSTSUBSCRIPT bold_t end_POSTSUBSCRIPT  and  d s  superscript subscript d s  \\mathbf{d_{s}^{\\prime}} bold_d start_POSTSUBSCRIPT bold_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , respectively. Besides the averaged synthetic depth distribution across the study region, we further explore the performance by aggregating the results into channel and non-channel levels, as described in Section  2.3 . This detailed analysis allows us to assess how well the synthetic data captures the unique characteristics of different regions in the study area.",
            "To understand the advancements of tree-based models and transformers in regression problems, specifically in the context of inundation depth estimation, we compared the predictive performance of two universal models, XGBoost Regressor and Regression Transformer, with one cell-wise model, MaxFloodCast V2. MaxFloodCast V2 utilizes an XGBoost-based architecture and incorporates heavy precipitation ratio features. We trained the Regression Transformer, configured with 4 layers in the Transformer encoder, a dimensionality of 128, 2,048 in the feed-forward layer, and 8 attention heads with 25% dropout, on 8 NVIDIA RTX A100 GPUs. The XGBoost Regressor was trained on an NVIDIA RTX A6000, while MaxFloodCast V2 was trained in parallel on an AMD EPYC 7702P 64-Core Processor, sharing the same hyperparameter settings as the XGBoost Regressor. The objective function minimized squared errors with a learning rate of 0.01. The XGBoost model built 1000 trees with a maximum depth of 5, incorporating L1 regularization to prevent overfitting and a subsample ratio of 0.3 to introduce non-linearity and enhance robustness. Following the sampling strategy mentioned in Section  2.2 , we configured 252 true precipitation-flood events for training, 63 events for validation, and the remaining events as the test set. The comparison results are presented in Table  3 , which assesses the overall, channel, and non-channel RMSE, as well as the  R 2 superscript R 2 R^{2} italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  score.",
            "We strategically sampled 90 true events following the ratio described in Section  2.2  and configured 36 hyperparameter sets for our grid search. This involved combining six pairs of learning rate settings with six distinct stoppage epochs, increasing from 50 epochs to 300 epochs in increments of 50, and modeling on NVIDIAs RTX A6000 GPU. This meticulous grid search focused on generating records under the condition  duration  1 duration 1 \\text{duration}\\geq 1 duration  1 , targeting the potential simulation of global precipitation events. As shown in Fig.  3 , we identified the optimal checkpoint with a generator learning rate of  10  2 superscript 10 2 10^{-2} 10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT , a discriminator learning rate of  10  3 superscript 10 3 10^{-3} 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT , and early stopping at 250 epochs. Notably, the generator learning rate is set to be 10 times higher than that of the discriminator to prevent the generator from being overpowered in the early stages before it can effectively learn the underlying data patterns. Ultimately, both the generator and discriminator converged before reaching the 300-epoch limit. The optimal settings enabled a direct comparison of cumulative precipitation, peak precipitation, and duration between the training dataset and the 10,000,000 synthetic data points generated by CTGAN, as depicted in Fig.  4 . This configuration produced an average marginal distribution score of 0.802, demonstrating a high fidelity of the synthetic data, particularly for cumulative and peak precipitation when compared to the original dataset. As shown in Fig.  4 (c), we enforced a constraint in CTGAN that limits  duration  1 duration 1 \\text{duration}\\leq 1 duration  1 , since our focus is primarily on global precipitation event generation. Additionally, the CTGAN model maintains the flexibility to generate local precipitation events by allowing cumulative precipitation to drop to zero. Moving forward, the challenge remains to construct isolated precipitation-depth events from the synthetic point dataset that closely correlate with true events, thus advancing the goals of this research in generating probabilistic flood maps."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Statistical comparison of sampled training depth distribution and downsampled synthetic depth distribution for key metrics across Overall, Channel, and Non-channel Cells.",
        "table": "S3.T4.1",
        "footnotes": [],
        "references": [
            "Where   i subscript  i \\mu_{i} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is the mean of a precipitation-based feature  x i subscript x i x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  from training dataset, and   i subscript  i \\sigma_{i} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  refers to its standard deviation.   1 subscript  1 \\theta_{1} italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and   2 subscript  2 \\theta_{2} italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  are two positive constants. Consequently, for each cell mesh, we established low (L), medium (M), and high (H) thresholds based on the distribution of precipitation-based features (cumulative precipitation, peak precipitation and duration) from the training dataset. These thresholds were then applied to the synthetic point dataset generated as described in section  2.4 . We conducted a comprehensive iteration over all synthetic point records, retaining the index of a point record to corresponding cell-level feature pool only if all its synthetic precipitation-based features consistently matched within the same LMH classification. This rigorous iterative screening ensured that each cells pool included indices of synthetic records that not only covered a range of precipitation levels but also met stringent quality criteria.",
            "We strategically sampled 90 true events following the ratio described in Section  2.2  and configured 36 hyperparameter sets for our grid search. This involved combining six pairs of learning rate settings with six distinct stoppage epochs, increasing from 50 epochs to 300 epochs in increments of 50, and modeling on NVIDIAs RTX A6000 GPU. This meticulous grid search focused on generating records under the condition  duration  1 duration 1 \\text{duration}\\geq 1 duration  1 , targeting the potential simulation of global precipitation events. As shown in Fig.  3 , we identified the optimal checkpoint with a generator learning rate of  10  2 superscript 10 2 10^{-2} 10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT , a discriminator learning rate of  10  3 superscript 10 3 10^{-3} 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT , and early stopping at 250 epochs. Notably, the generator learning rate is set to be 10 times higher than that of the discriminator to prevent the generator from being overpowered in the early stages before it can effectively learn the underlying data patterns. Ultimately, both the generator and discriminator converged before reaching the 300-epoch limit. The optimal settings enabled a direct comparison of cumulative precipitation, peak precipitation, and duration between the training dataset and the 10,000,000 synthetic data points generated by CTGAN, as depicted in Fig.  4 . This configuration produced an average marginal distribution score of 0.802, demonstrating a high fidelity of the synthetic data, particularly for cumulative and peak precipitation when compared to the original dataset. As shown in Fig.  4 (c), we enforced a constraint in CTGAN that limits  duration  1 duration 1 \\text{duration}\\leq 1 duration  1 , since our focus is primarily on global precipitation event generation. Additionally, the CTGAN model maintains the flexibility to generate local precipitation events by allowing cumulative precipitation to drop to zero. Moving forward, the challenge remains to construct isolated precipitation-depth events from the synthetic point dataset that closely correlate with true events, thus advancing the goals of this research in generating probabilistic flood maps.",
            "We generated 10,000 synthetic precipitation-flood events, providing each cell with a comprehensive synthetic depth distribution for direct comparison with the training dataset. To assess the performance of the synthetic events, we utilized four key metricsRMSE, Cosine Similarity, Correlation, and KL Divergencecomparing them against the limited number of sampled training events. To ensure a fair comparison, the synthetic events were downsampled to match the size of the training data. This downsampling process was repeated 50 times to minimize variability introduced by randomness. A detailed statistical comparison between the normalized versions of the training and synthetic datasets is provided in Table  4 , with the corresponding visualizations shown in Fig.  6 .",
            "In this study, we presented an innovative methodology,  Flood-Precip GAN , for generating high-resolution flood probability maps using synthetic precipitation-flood events. Our approach leveraged advanced machine learning techniques, including Conditional Generative Adversarial Networks (CTGAN)  [ 23 ]  as point location and precipitation-based features generator and XGBoost-based models MaxFloodCast  [ 20 ]  as synthetic depth estimator, to overcome the limitations of traditional flood modeling that rely heavily on historical data. To address the limitations in feature dimensionality found in existing CTGAN tools and the challenge of generating individual precipitation-flood events from a massive synthetic point cloud, we designed a robust and systematic workflow. This approach includes cell-level pool creation, strategic sampling, noise smoothing, depth synthesis, and the eventual formation of individual events. Each step in the process is carefully structured to avoid overfitting and ensure that the generated synthetic events accurately reflect real-world flood scenarios while maintaining computational efficiency and high fidelity to hydrological patterns. By generating 10,000 synthetic precipitation-flood events, we created a comprehensive dataset that captures a wide spectrum of potential flood scenarios, significantly improving the robustness of flood risk assessments. Based on the metrics between the training and synthetic events across overall, channel, and non-channel aspects shown on Table  4 , the overall Cosine Similarity, with a mean of 0.86 and a standard deviation of 0.05, indicates a strong alignment between the synthetic and training data. The RMSE values highlight a notable difference between channel and non-channel cells, reflecting the challenges in accurately modeling flood depth distribution in regions with dynamic flows and greater variability. These findings demonstrate the models ability to capture key trends and linear relationships, particularly in hydrologically complex regions such as channel cells, which are more susceptible to flooding. Additionally, the synthetic datas capacity to generalize beyond the training events highlights its potential for producing realistic flood simulations, which is critical for accurate risk assessment and effective mitigation planning."
        ]
    }
}