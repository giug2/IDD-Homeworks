{
    "id_table_1": {
        "caption": "Table 1:  The LLM performance in different scenarios",
        "table": "S4.T1.1",
        "footnotes": [],
        "references": [
            "RAG Baseline : The table  1  displays several RAG baselines, including scenarios where RAG is not used, direct feeding of the correct context to the large language model, the scenario where only a retriever is used in pipeline, and the scenario where a retriever is used in combination with Instruction-Aware Contextual Compressor for re-ranking."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Comparison of ROUGE-L scores for the Selective Context method using Baichuan-7B and GPT-2 models.",
        "table": "S4.T2.1",
        "footnotes": [],
        "references": [
            "We introduce Instruction-Aware Contextual Compression  [ 19 ] , an innovative approach for context compression that leverages both ranking and generation information.  In contrast to the instruction-agnostic context compression methods used previously, Instruction-Aware Contextual Compression is a method that relies on instructions to perform context compression. Depending on the specific instruction provided, the model produces different compression outcomes, as shown in Figure  2 , removing irrelevant portions of the context, ultimately achieving improved context compression results."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Comparing Instruction-Aware Contextual Compressor with different context retention ratio to the original context",
        "table": "S5.T3.1.1",
        "footnotes": [],
        "references": [
            "We introduce Instruction-Aware Contextual Compressor as a trainable module to implement the Instruction-Aware Contextual Compression method. Instruction-Aware Contextual Compressor adopts an encoder-decoder architecture, consisting of both an encoder and a decoder, as illustrated in Figure  3 .",
            "We initially compare the performance of Instruction-Aware Contextual Compressor with varying context retention ratios to the reranked original context, which utilizes the original context after reranking but no compression at all. All results are shown in table  3 , and the diff column represents the difference in performance compared to uncompressed text.",
            "As shown in the table  3 , at retention rate of 0.8, the performance loss is minimal, with Rouge-1 showing only a marginal decrease in the range of 0.003 to 0.008. This demonstrates a high level of consistency between answers provided in compressed contexts and those in original contexts. Surprisingly, the Rouge-2 and Rouge-L score with the generation method is even higher than the original text, which was unexpected. This indicates that our method successfully filtered unrelated or even noisy content, improving the LLMs performance."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Speed up after context compression",
        "table": "S5.T4.1",
        "footnotes": [],
        "references": [
            "In this section, we compare our method to Selective Context baseline, and the results are presented in Figure  4 .  Selective Context is a context compression method based on text self-information and represents the state-of-the-art as of the time of writing this paper. Comparing our method to Selective Context, which serves as a baseline, can effectively demonstrate the validity of our approach.  As shown in Figure  4 , our proposed method, Instruction-Aware Contextual Compressor, is even more effective compared to Selective Context. Both methods, whether purely ranking-based or generation-based, outperform Selective Context, and the lead becomes more significant as the retention ratio decreases. This indicates that our proposed method excels in selecting more informative content even when only a limited amount of information can be retained.",
            "We also measured the impact of context compression on the Large Language Model (LLM). As demonstrated in the table  4 , when the retention ratio is set to 0.5, the inference speed per token increases by a factor of 2.2, while memory usage decreases by 5.05%."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Re-ranking Performance of Instruction-Aware Contextual Compressor",
        "table": "S5.T5.1",
        "footnotes": [],
        "references": [
            "For context compression using generative information, intuitively, if the number of generation steps is too few, it might not have generated a complete response. Consequently, the effectiveness at this stage may be suboptimal. As the number of generation steps increases, the effectiveness of compression is expected to improve. To explore this, we conducted experiments with a fixed retention ratio of 0.5, testing a series of generation step values ranging from 4 to 64. The results, as shown in Figure  5 , indeed demonstrate that for generation-based context compression, the performance gradually improves with an increase in the number of steps. However, after reaching 32 steps, it reaches a plateau, indicating a diminishing marginal return with further increases in the number of generation steps.",
            "The ability to filter out irrelevant documents through re-ranking is also a crucial capability of Instruction-Aware Contextual Compression. Following a hierarchical design approach, a large number of initially retrieved documents are first re-ordered using Instruction-Aware Contextual Compressor to select the top-k documents, which are then further compressed. Therefore, we present the retrieval performance of Instruction-Aware Contextual Compressor on extensive datasets, measured by Recall@1, 5, and 10, as shown in  5 ."
        ]
    },
    "global_footnotes": []
}