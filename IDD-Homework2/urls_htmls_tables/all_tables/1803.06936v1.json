{
    "S4.T1": {
        "caption": "TABLE I: Overall question generation performance on the testing set.",
        "table": "<table id=\"S4.T1.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.1.1.1\" class=\"ltx_td ltx_border_r ltx_border_tt\"></td>\n<td id=\"S4.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S4.T1.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">CIDEr</span></td>\n<td id=\"S4.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T1.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">BLEU-4</span></td>\n<td id=\"S4.T1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T1.1.1.1.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">BLEU-3</span></td>\n<td id=\"S4.T1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T1.1.1.1.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">BLEU-2</span></td>\n<td id=\"S4.T1.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S4.T1.1.1.1.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">BLEU-1</span></td>\n<td id=\"S4.T1.1.1.1.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S4.T1.1.1.1.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">ROUGE-L</span></td>\n<td id=\"S4.T1.1.1.1.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S4.T1.1.1.1.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">METEOR</span></td>\n<td id=\"S4.T1.1.1.1.9\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T1.1.1.1.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">Acc@1</span></td>\n<td id=\"S4.T1.1.1.1.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S4.T1.1.1.1.10.1\" class=\"ltx_text\" style=\"font-size:80%;\">Acc@3</span></td>\n<td id=\"S4.T1.1.1.1.11\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_tt\"><span id=\"S4.T1.1.1.1.11.1\" class=\"ltx_text\" style=\"font-size:80%;\">Human*</span></td>\n</tr>\n<tr id=\"S4.T1.1.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S4.T1.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A</span></td>\n<td id=\"S4.T1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T1.1.2.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.952</span></td>\n<td id=\"S4.T1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.1.2.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.146</span></td>\n<td id=\"S4.T1.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.1.2.2.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.192</span></td>\n<td id=\"S4.T1.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.1.2.2.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.265</span></td>\n<td id=\"S4.T1.1.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T1.1.2.2.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.371</span></td>\n<td id=\"S4.T1.1.2.2.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T1.1.2.2.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.408</span></td>\n<td id=\"S4.T1.1.2.2.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T1.1.2.2.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.161</span></td>\n<td id=\"S4.T1.1.2.2.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.1.2.2.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">14.589</span></td>\n<td id=\"S4.T1.1.2.2.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T1.1.2.2.10.1\" class=\"ltx_text\" style=\"font-size:80%;\">28.795</span></td>\n<td id=\"S4.T1.1.2.2.11\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\"><span id=\"S4.T1.1.2.2.11.1\" class=\"ltx_text\" style=\"font-size:80%;\">2.00</span></td>\n</tr>\n<tr id=\"S4.T1.1.3.3\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S4.T1.1.3.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">I</span></td>\n<td id=\"S4.T1.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.1.3.3.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.652</span></td>\n<td id=\"S4.T1.1.3.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.1.3.3.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.086</span></td>\n<td id=\"S4.T1.1.3.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.1.3.3.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.121</span></td>\n<td id=\"S4.T1.1.3.3.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.1.3.3.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.179</span></td>\n<td id=\"S4.T1.1.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.1.3.3.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.280</span></td>\n<td id=\"S4.T1.1.3.3.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.1.3.3.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.310</span></td>\n<td id=\"S4.T1.1.3.3.8\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.1.3.3.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.117</span></td>\n<td id=\"S4.T1.1.3.3.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.1.3.3.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">13.012</span></td>\n<td id=\"S4.T1.1.3.3.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.1.3.3.10.1\" class=\"ltx_text\" style=\"font-size:80%;\">28.644</span></td>\n<td id=\"S4.T1.1.3.3.11\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T1.1.3.3.11.1\" class=\"ltx_text\" style=\"font-size:80%;\">2.10</span></td>\n</tr>\n<tr id=\"S4.T1.1.4.4\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S4.T1.1.4.4.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">I+AT</span></td>\n<td id=\"S4.T1.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.1.4.4.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.904</span></td>\n<td id=\"S4.T1.1.4.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.1.4.4.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.122</span></td>\n<td id=\"S4.T1.1.4.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.1.4.4.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.164</span></td>\n<td id=\"S4.T1.1.4.4.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.1.4.4.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.234</span></td>\n<td id=\"S4.T1.1.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.1.4.4.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.350</span></td>\n<td id=\"S4.T1.1.4.4.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.1.4.4.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.397</span></td>\n<td id=\"S4.T1.1.4.4.8\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.1.4.4.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.151</span></td>\n<td id=\"S4.T1.1.4.4.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.1.4.4.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">20.277</span></td>\n<td id=\"S4.T1.1.4.4.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.1.4.4.10.1\" class=\"ltx_text\" style=\"font-size:80%;\">36.134</span></td>\n<td id=\"S4.T1.1.4.4.11\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T1.1.4.4.11.1\" class=\"ltx_text\" style=\"font-size:80%;\">2.70</span></td>\n</tr>\n<tr id=\"S4.T1.1.5.5\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.5.5.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S4.T1.1.5.5.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">NN</span></td>\n<td id=\"S4.T1.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T1.1.5.5.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">1.372</span></td>\n<td id=\"S4.T1.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.1.5.5.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.175</span></td>\n<td id=\"S4.T1.1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.1.5.5.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.223</span></td>\n<td id=\"S4.T1.1.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.1.5.5.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.294</span></td>\n<td id=\"S4.T1.1.5.5.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T1.1.5.5.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.404</span></td>\n<td id=\"S4.T1.1.5.5.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T1.1.5.5.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.428</span></td>\n<td id=\"S4.T1.1.5.5.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T1.1.5.5.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.183</span></td>\n<td id=\"S4.T1.1.5.5.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.1.5.5.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">26.783</span></td>\n<td id=\"S4.T1.1.5.5.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T1.1.5.5.10.1\" class=\"ltx_text\" style=\"font-size:80%;\">48.755</span></td>\n<td id=\"S4.T1.1.5.5.11\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\"><span id=\"S4.T1.1.5.5.11.1\" class=\"ltx_text\" style=\"font-size:80%;\">3.11</span></td>\n</tr>\n<tr id=\"S4.T1.1.6.6\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.6.6.1\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S4.T1.1.6.6.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">SAT</span></td>\n<td id=\"S4.T1.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.1.6.6.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">1.533</span></td>\n<td id=\"S4.T1.1.6.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.1.6.6.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.192</span></td>\n<td id=\"S4.T1.1.6.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.1.6.6.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.241</span></td>\n<td id=\"S4.T1.1.6.6.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.1.6.6.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.311</span></td>\n<td id=\"S4.T1.1.6.6.6\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.1.6.6.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.417</span></td>\n<td id=\"S4.T1.1.6.6.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.1.6.6.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.456</span></td>\n<td id=\"S4.T1.1.6.6.8\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.1.6.6.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.195</span></td>\n<td id=\"S4.T1.1.6.6.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.1.6.6.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">29.722</span></td>\n<td id=\"S4.T1.1.6.6.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.1.6.6.10.1\" class=\"ltx_text\" style=\"font-size:80%;\">48.118</span></td>\n<td id=\"S4.T1.1.6.6.11\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T1.1.6.6.11.1\" class=\"ltx_text\" style=\"font-size:80%;\">3.30</span></td>\n</tr>\n<tr id=\"S4.T1.1.7.7\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.7.7.1\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S4.T1.1.7.7.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">VQG+VQA</span></td>\n<td id=\"S4.T1.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.1.7.7.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">1.110</span></td>\n<td id=\"S4.T1.1.7.7.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.1.7.7.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.147</span></td>\n<td id=\"S4.T1.1.7.7.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.1.7.7.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.193</span></td>\n<td id=\"S4.T1.1.7.7.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.1.7.7.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.261</span></td>\n<td id=\"S4.T1.1.7.7.6\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.1.7.7.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.371</span></td>\n<td id=\"S4.T1.1.7.7.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.1.7.7.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.396</span></td>\n<td id=\"S4.T1.1.7.7.8\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.1.7.7.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.165</span></td>\n<td id=\"S4.T1.1.7.7.9\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.1.7.7.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">16.529</span></td>\n<td id=\"S4.T1.1.7.7.10\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.1.7.7.10.1\" class=\"ltx_text\" style=\"font-size:80%;\">41.655</span></td>\n<td id=\"S4.T1.1.7.7.11\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S4.T1.1.7.7.11.1\" class=\"ltx_text\" style=\"font-size:80%;\">2.85</span></td>\n</tr>\n<tr id=\"S4.T1.1.8.8\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.8.8.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S4.T1.1.8.8.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Ours</span></td>\n<td id=\"S4.T1.1.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S4.T1.1.8.8.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">1.682</span></td>\n<td id=\"S4.T1.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S4.T1.1.8.8.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.205</span></td>\n<td id=\"S4.T1.1.8.8.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S4.T1.1.8.8.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.253</span></td>\n<td id=\"S4.T1.1.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S4.T1.1.8.8.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.320</span></td>\n<td id=\"S4.T1.1.8.8.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S4.T1.1.8.8.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.421</span></td>\n<td id=\"S4.T1.1.8.8.7\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S4.T1.1.8.8.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.466</span></td>\n<td id=\"S4.T1.1.8.8.8\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S4.T1.1.8.8.8.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.201</span></td>\n<td id=\"S4.T1.1.8.8.9\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S4.T1.1.8.8.9.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">30.814</span></td>\n<td id=\"S4.T1.1.8.8.10\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S4.T1.1.8.8.10.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">49.653</span></td>\n<td id=\"S4.T1.1.8.8.11\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S4.T1.1.8.8.11.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">3.37</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Overall  In the first experiment we report the overall iVQA performance on the test split. The results are shown in Table I with both the standard linguistic metrics, as well as our ranking accuracy metric. From the table, we can make the following observations: (i) Unlike VQA the margin between the no-image case (A), and the full model (Ours) is dramatic. The ranking accuracies are more than doubled, and the language metrics show similarly striking improvements. This demonstrates that unlike conventional VQA [1], the ‘V’ does matter in iVQA.\n(ii) The margin between the image + answer type (I+AT) and image-only (I) setting exists, but is not too significant. This shows that while it is a useful hint for an iVQA model to know the question type, it still really needs the actual semantic answer to generate the right questions. E.g., rather than just knowing that it was counting something (answer type), the model does need to know how many objects were counted (answer) in order to generate the right question specifying what object type needs to be counted – as there may be other objects that could be counted.\n(iii) The margins between the (I) and (I+AT) cases and the full model are also striking. This demonstrates that as a test of multi-modal intelligence, iVQA reassuringly requires both modalities in order to do well.\n(iv) VQG+VQA indeed performs better than the vanilla VQG model by making the generated question more answer conditional, but it is still weaker than the captioning adapted models (NN and SAT) or ours. The reason is that due to the language bias, it is difficult for the VQA model to identify the right question from multiple candidates where the same answer applies.\n(v) The captioning adapted models (NN and SAT) perform well, but are still inferior to the proposed model which is specifically designed for iVQA.",
            "Human Study  The human study is applied on a subset of 300 samples, and evaluates the models in a way that is fully robust to open-ended question generation. Results in Table I show that (i) our model performs the best among all competitors; (ii) the human study scores are highly correlated with the proposed ranking metric. Specifically, the Pearson correlation coefficient between manually labelled scores and the proposed acc@1 and acc@3 metrics are 0.917 and 0.981 respectively, while the best performing linguistic measure (CIDEr) only reaches 0.898. Thus our ranking metric is a reasonably accurate yet cost-effective alternative to the more expensive human evaluation."
        ]
    },
    "S4.T2": {
        "caption": "TABLE II: VQA vs iVQA in terms of bias-based gameability.",
        "table": "<table id=\"S4.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"></th>\n<th id=\"S4.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S4.T2.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">split</span></th>\n<th id=\"S4.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T2.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Prior</span></th>\n<th id=\"S4.T2.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T2.1.1.1.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">Language</span></th>\n<th id=\"S4.T2.1.1.1.5\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T2.1.1.1.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">Language+Visual</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.1.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S4.T2.1.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">iVQA (acc@1)</span></th>\n<th id=\"S4.T2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S4.T2.1.2.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">test</span></th>\n<td id=\"S4.T2.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.1.2.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">3.94</span></td>\n<td id=\"S4.T2.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T2.1.2.1.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">14.59</span></td>\n<td id=\"S4.T2.1.2.1.5\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\"><span id=\"S4.T2.1.2.1.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">28.44</span></td>\n</tr>\n<tr id=\"S4.T2.1.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span id=\"S4.T2.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">VQA (accuracy)</span></th>\n<th id=\"S4.T2.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span id=\"S4.T2.1.3.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">test-dev</span></th>\n<td id=\"S4.T2.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.1.3.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">29.66</span></td>\n<td id=\"S4.T2.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.1.3.2.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">48.76</span></td>\n<td id=\"S4.T2.1.3.2.5\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\"><span id=\"S4.T2.1.3.2.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">57.75</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Contrasting VQA and iVQA as benchmarks  Finally, we discuss iVQA’s interest as a benchmark compared to the conventional VQA. Two of the main kinds of bias that a VQA/iVQA model could use to cheat the benchmark are the output Prior bias (Ignore both inputs and predict only the most likely answer on VQA; use question frequency in iVQA), and Language bias (ignore the image and use only the input – question for VQA, answer for iVQA – to predict the output). A good multimodal intelligence benchmark should require understanding and mutual grounding of both modalities, and should be hard to game by exploiting those biases. To analyses these issues we compare performance on iVQA and VQA benchmarks using Prior-alone and Language-alone (LSTM Q for VQA and Answer only for iVQA) baselines versus the full multi-modal model in each case (DeeperLSTM+Norm I for VQA, and I+A model for iVQA).\nThe results in Table II show that for VQA the bias-based baselines approach the performance of a full multi-modal model much more closely than the corresponding baselines do for iVQA. This suggests that VQA is easier to ‘game’ (achieve an apparently high score without any image understanding or multimodal grounding), compared to iVQA. Thus we propose that iVQA makes a distinct and interesting benchmark for multimodal intelligence."
        ]
    },
    "S6.T3": {
        "caption": "TABLE III: Belief set compositions of different VQA models. Note that the VQA accuracy is evaluated on the test-dev split of VQA 1.0 in all experiments.",
        "table": "<table id=\"S6.T3.7.7\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T3.7.7.8.1\" class=\"ltx_tr\">\n<th id=\"S6.T3.7.7.8.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt\"></th>\n<td id=\"S6.T3.7.7.8.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S6.T3.7.7.8.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Vanilla</span></td>\n<td id=\"S6.T3.7.7.8.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S6.T3.7.7.8.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">N2NMN</span></td>\n<td id=\"S6.T3.7.7.8.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S6.T3.7.7.8.1.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">MLB-att</span></td>\n<td id=\"S6.T3.7.7.8.1.5\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_tt\"><span id=\"S6.T3.7.7.8.1.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">MLB2-att</span></td>\n</tr>\n<tr id=\"S6.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S6.T3.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">\n<span id=\"S6.T3.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Rephrase (</span><math id=\"S6.T3.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S6.T3.1.1.1.1.m1.1a\"><mo mathsize=\"80%\" stretchy=\"false\" id=\"S6.T3.1.1.1.1.m1.1.1\" xref=\"S6.T3.1.1.1.1.m1.1.1.cmml\">↑</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T3.1.1.1.1.m1.1b\"><ci id=\"S6.T3.1.1.1.1.m1.1.1.cmml\" xref=\"S6.T3.1.1.1.1.m1.1.1\">↑</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T3.1.1.1.1.m1.1c\">\\uparrow</annotation></semantics></math><span id=\"S6.T3.1.1.1.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">)</span>\n</th>\n<td id=\"S6.T3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S6.T3.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">7.8</span></td>\n<td id=\"S6.T3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S6.T3.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">7.6</span></td>\n<td id=\"S6.T3.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S6.T3.1.1.1.4.1\" class=\"ltx_text\" style=\"font-size:80%;\"></span><span id=\"S6.T3.1.1.1.4.2\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">11.6</span>\n</td>\n<td id=\"S6.T3.1.1.1.5\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\"><span id=\"S6.T3.1.1.1.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">11.2</span></td>\n</tr>\n<tr id=\"S6.T3.2.2.2\" class=\"ltx_tr\">\n<th id=\"S6.T3.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span id=\"S6.T3.2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Complementary (</span><math id=\"S6.T3.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S6.T3.2.2.2.1.m1.1a\"><mo mathsize=\"80%\" stretchy=\"false\" id=\"S6.T3.2.2.2.1.m1.1.1\" xref=\"S6.T3.2.2.2.1.m1.1.1.cmml\">↑</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T3.2.2.2.1.m1.1b\"><ci id=\"S6.T3.2.2.2.1.m1.1.1.cmml\" xref=\"S6.T3.2.2.2.1.m1.1.1\">↑</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T3.2.2.2.1.m1.1c\">\\uparrow</annotation></semantics></math><span id=\"S6.T3.2.2.2.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">)</span>\n</th>\n<td id=\"S6.T3.2.2.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T3.2.2.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">40.8</span></td>\n<td id=\"S6.T3.2.2.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T3.2.2.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">47.6</span></td>\n<td id=\"S6.T3.2.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S6.T3.2.2.2.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">47.2</span></td>\n<td id=\"S6.T3.2.2.2.5\" class=\"ltx_td ltx_nopad_r ltx_align_center\">\n<span id=\"S6.T3.2.2.2.5.1\" class=\"ltx_text\" style=\"font-size:80%;\"></span><span id=\"S6.T3.2.2.2.5.2\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">54.2</span>\n</td>\n</tr>\n<tr id=\"S6.T3.3.3.3\" class=\"ltx_tr\">\n<th id=\"S6.T3.3.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span id=\"S6.T3.3.3.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Adversarial (</span><math id=\"S6.T3.3.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S6.T3.3.3.3.1.m1.1a\"><mo mathsize=\"80%\" stretchy=\"false\" id=\"S6.T3.3.3.3.1.m1.1.1\" xref=\"S6.T3.3.3.3.1.m1.1.1.cmml\">↓</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T3.3.3.3.1.m1.1b\"><ci id=\"S6.T3.3.3.3.1.m1.1.1.cmml\" xref=\"S6.T3.3.3.3.1.m1.1.1\">↓</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T3.3.3.3.1.m1.1c\">\\downarrow</annotation></semantics></math><span id=\"S6.T3.3.3.3.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">)</span>\n</th>\n<td id=\"S6.T3.3.3.3.2\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T3.3.3.3.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">33.0</span></td>\n<td id=\"S6.T3.3.3.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T3.3.3.3.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">27.4</span></td>\n<td id=\"S6.T3.3.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S6.T3.3.3.3.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">24.0</span></td>\n<td id=\"S6.T3.3.3.3.5\" class=\"ltx_td ltx_nopad_r ltx_align_center\">\n<span id=\"S6.T3.3.3.3.5.1\" class=\"ltx_text\" style=\"font-size:80%;\"></span><span id=\"S6.T3.3.3.3.5.2\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">19.8</span>\n</td>\n</tr>\n<tr id=\"S6.T3.4.4.4\" class=\"ltx_tr\">\n<th id=\"S6.T3.4.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span id=\"S6.T3.4.4.4.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Irrelevant (</span><math id=\"S6.T3.4.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S6.T3.4.4.4.1.m1.1a\"><mo mathsize=\"80%\" stretchy=\"false\" id=\"S6.T3.4.4.4.1.m1.1.1\" xref=\"S6.T3.4.4.4.1.m1.1.1.cmml\">↓</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T3.4.4.4.1.m1.1b\"><ci id=\"S6.T3.4.4.4.1.m1.1.1.cmml\" xref=\"S6.T3.4.4.4.1.m1.1.1\">↓</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T3.4.4.4.1.m1.1c\">\\downarrow</annotation></semantics></math><span id=\"S6.T3.4.4.4.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">)</span>\n</th>\n<td id=\"S6.T3.4.4.4.2\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T3.4.4.4.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">18.4</span></td>\n<td id=\"S6.T3.4.4.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T3.4.4.4.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">17.4</span></td>\n<td id=\"S6.T3.4.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S6.T3.4.4.4.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">17.2</span></td>\n<td id=\"S6.T3.4.4.4.5\" class=\"ltx_td ltx_nopad_r ltx_align_center\">\n<span id=\"S6.T3.4.4.4.5.1\" class=\"ltx_text\" style=\"font-size:80%;\"></span><span id=\"S6.T3.4.4.4.5.2\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">14.8</span>\n</td>\n</tr>\n<tr id=\"S6.T3.5.5.5\" class=\"ltx_tr\">\n<th id=\"S6.T3.5.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">\n<span id=\"S6.T3.5.5.5.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Correct (</span><math id=\"S6.T3.5.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S6.T3.5.5.5.1.m1.1a\"><mo mathsize=\"80%\" stretchy=\"false\" id=\"S6.T3.5.5.5.1.m1.1.1\" xref=\"S6.T3.5.5.5.1.m1.1.1.cmml\">↑</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T3.5.5.5.1.m1.1b\"><ci id=\"S6.T3.5.5.5.1.m1.1.1.cmml\" xref=\"S6.T3.5.5.5.1.m1.1.1\">↑</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T3.5.5.5.1.m1.1c\">\\uparrow</annotation></semantics></math><span id=\"S6.T3.5.5.5.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">)</span>\n</th>\n<td id=\"S6.T3.5.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S6.T3.5.5.5.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">48.6</span></td>\n<td id=\"S6.T3.5.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S6.T3.5.5.5.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">55.2</span></td>\n<td id=\"S6.T3.5.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S6.T3.5.5.5.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">58.8</span></td>\n<td id=\"S6.T3.5.5.5.5\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">\n<span id=\"S6.T3.5.5.5.5.1\" class=\"ltx_text\" style=\"font-size:80%;\"></span><span id=\"S6.T3.5.5.5.5.2\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">65.4</span>\n</td>\n</tr>\n<tr id=\"S6.T3.6.6.6\" class=\"ltx_tr\">\n<th id=\"S6.T3.6.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">\n<span id=\"S6.T3.6.6.6.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Incorrect (</span><math id=\"S6.T3.6.6.6.1.m1.1\" class=\"ltx_Math\" alttext=\"\\downarrow\" display=\"inline\"><semantics id=\"S6.T3.6.6.6.1.m1.1a\"><mo mathsize=\"80%\" stretchy=\"false\" id=\"S6.T3.6.6.6.1.m1.1.1\" xref=\"S6.T3.6.6.6.1.m1.1.1.cmml\">↓</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T3.6.6.6.1.m1.1b\"><ci id=\"S6.T3.6.6.6.1.m1.1.1.cmml\" xref=\"S6.T3.6.6.6.1.m1.1.1\">↓</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T3.6.6.6.1.m1.1c\">\\downarrow</annotation></semantics></math><span id=\"S6.T3.6.6.6.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">)</span>\n</th>\n<td id=\"S6.T3.6.6.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T3.6.6.6.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">51.4</span></td>\n<td id=\"S6.T3.6.6.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T3.6.6.6.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">44.8</span></td>\n<td id=\"S6.T3.6.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S6.T3.6.6.6.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">41.2</span></td>\n<td id=\"S6.T3.6.6.6.5\" class=\"ltx_td ltx_nopad_r ltx_align_center\">\n<span id=\"S6.T3.6.6.6.5.1\" class=\"ltx_text\" style=\"font-size:80%;\"></span><span id=\"S6.T3.6.6.6.5.2\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">34.6</span>\n</td>\n</tr>\n<tr id=\"S6.T3.7.7.7\" class=\"ltx_tr\">\n<th id=\"S6.T3.7.7.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\">\n<span id=\"S6.T3.7.7.7.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">VQA accuracy (</span><math id=\"S6.T3.7.7.7.1.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S6.T3.7.7.7.1.m1.1a\"><mo mathsize=\"80%\" stretchy=\"false\" id=\"S6.T3.7.7.7.1.m1.1.1\" xref=\"S6.T3.7.7.7.1.m1.1.1.cmml\">↑</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T3.7.7.7.1.m1.1b\"><ci id=\"S6.T3.7.7.7.1.m1.1.1.cmml\" xref=\"S6.T3.7.7.7.1.m1.1.1\">↑</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T3.7.7.7.1.m1.1c\">\\uparrow</annotation></semantics></math><span id=\"S6.T3.7.7.7.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">)</span>\n</th>\n<td id=\"S6.T3.7.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S6.T3.7.7.7.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">55.30</span></td>\n<td id=\"S6.T3.7.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">\n<span id=\"S6.T3.7.7.7.3.1\" class=\"ltx_text\" style=\"font-size:80%;\"></span><span id=\"S6.T3.7.7.7.3.2\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">64.90</span>\n</td>\n<td id=\"S6.T3.7.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S6.T3.7.7.7.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">64.59</span></td>\n<td id=\"S6.T3.7.7.7.5\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S6.T3.7.7.7.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">62.19</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "The belief set compositions of different VQA models are summarised in Table III, from which we can make the following observations:\n(1) Complementary and adversarial beliefs are the most common ones in all VQA models. Irrelevant beliefs are ranked the 3rd,  indicating the importance of introducing a rejection mechanism into VQA models. Rephrased questions are the least common, indicating that our question generator extracts novel beliefs and does not just rely on perturbing known facts.\n(2) MLB-att with visual attention generalises better than the vanilla VQA model. It has more correct (complementary, rephrase) and fewer incorrect (adversarial, irrelevant) beliefs.\n(3) The N2NMN model with explicit reasoning performs similarly to the attention based MLB-att, and their percentages of correct beliefs are almost the same. However, there are some slight differences: The N2NMN model is better at generalising to complementary visual facts but less robust to adversarial beliefs.\n(4) We can fix a model (MLB-att) and compare the benefit of training on the more balanced VQA 2.0 dataset (MLB2-att vs. MLB-att). The percentage of correct beliefs increased by 6.6%, due to an increased proportion of complementary beliefs. This indicates that the VQA2.0 trained model recognises some new visual facts which it originally missed when trained on VQA 1.0; and to a decreased proportion of adversarial beliefs – indicating that the VQA2.0-trained model is harder to fool than the original model. This shows that the richer annotation in VQA2.0 is beneficial, however the large outstanding proportion (19.8%) of adversarial examples in the belief set indicate that the VQA2.0 dataset is still far from being adequate to train a model that does not suffer from many misconceptions. (5) Finally, we observe a difference in the standard metric (VQA accuracy) vs our metric (correct beliefs): The standard metric ranks N2NMN and MLB-att slightly above MLB2-att, while our metric ranks MLB2-att clearly above the others. We interpret this as the VQA2.0-trained model actually being better, but the VQA1.0 trained models appear to be better under the standard metric due to overfitting to the biases of this dataset.",
            "Based on the union of our collected adversarial examples for the four VQA models we evaluated, we release a small but challenging dataset for VQA which is designed to be a focused extra-challenging benchmark for future VQA studies. It contains 282 unique samples over 195 images, and 77, 74, 76, 55 examples are from the adversarial beliefs of Vanilla, N2NMN, MLB-att, and MLB2-att respectively. The correct question-answer pairs are annotated manually for these wrong beliefs for quantitative evaluation. Examples of the datasets can be found in the Supplementary Material.\nThe accuracy (correctly predicting the answer given an image and question) of the above VQA models as well as MCB-att666An attention model with multi-modal compact bilinear pooling as the fusion strategy [20]. https://github.com/akirafukui/vqa-mcb on the new dataset are shown in Table V. The results show that the evaluated VQA models (including MCB-att which was not used for belief set generation) all struggle on these extra-challenging examples. Comparing Table V with Table III, the accuracy drops as much as 40% (Vanilla, from 55.30% to 14.54%). This dataset can be used to complement existing VQA dataset to evaluate newly proposed VQA models to see that whether they are still making the same mistakes as previous models."
        ]
    },
    "S6.T5": {
        "caption": "TABLE V: VQA accuracy on the proposed extra-challenging VQA dataset, where ††\\dagger indicates the model is not contribute to the set of hard examples.",
        "table": "<table id=\"S6.T5.1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T5.1.1.1\" class=\"ltx_tr\">\n<th id=\"S6.T5.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S6.T5.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Models</span></th>\n<td id=\"S6.T5.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S6.T5.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Vanilla</span></td>\n<td id=\"S6.T5.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S6.T5.1.1.1.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">N2NMN2</span></td>\n<td id=\"S6.T5.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S6.T5.1.1.1.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">MLB-att</span></td>\n<td id=\"S6.T5.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S6.T5.1.1.1.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">MLB2-att</span></td>\n<td id=\"S6.T5.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_tt\">\n<span id=\"S6.T5.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">MCB-att</span><sup id=\"S6.T5.1.1.1.1.2\" class=\"ltx_sup\"><span id=\"S6.T5.1.1.1.1.2.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">†</span></sup>\n</td>\n</tr>\n<tr id=\"S6.T5.1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S6.T5.1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S6.T5.1.1.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">accuracy</span></th>\n<td id=\"S6.T5.1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S6.T5.1.1.2.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">14.54</span></td>\n<td id=\"S6.T5.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S6.T5.1.1.2.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">24.82</span></td>\n<td id=\"S6.T5.1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S6.T5.1.1.2.1.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">24.47</span></td>\n<td id=\"S6.T5.1.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S6.T5.1.1.2.1.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">45.39</span></td>\n<td id=\"S6.T5.1.1.2.1.6\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S6.T5.1.1.2.1.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">26.24</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Based on the union of our collected adversarial examples for the four VQA models we evaluated, we release a small but challenging dataset for VQA which is designed to be a focused extra-challenging benchmark for future VQA studies. It contains 282 unique samples over 195 images, and 77, 74, 76, 55 examples are from the adversarial beliefs of Vanilla, N2NMN, MLB-att, and MLB2-att respectively. The correct question-answer pairs are annotated manually for these wrong beliefs for quantitative evaluation. Examples of the datasets can be found in the Supplementary Material.\nThe accuracy (correctly predicting the answer given an image and question) of the above VQA models as well as MCB-att666An attention model with multi-modal compact bilinear pooling as the fusion strategy [20]. https://github.com/akirafukui/vqa-mcb on the new dataset are shown in Table V. The results show that the evaluated VQA models (including MCB-att which was not used for belief set generation) all struggle on these extra-challenging examples. Comparing Table V with Table III, the accuracy drops as much as 40% (Vanilla, from 55.30% to 14.54%). This dataset can be used to complement existing VQA dataset to evaluate newly proposed VQA models to see that whether they are still making the same mistakes as previous models."
        ]
    }
}