{
    "id_table_1": {
        "caption": "Table 1 :  DinoHMR vs. Dessie with DessiePIPE on unseen image datasets.",
        "table": "S3.E1",
        "footnotes": [],
        "references": [
            "We examine the performance of DinoHMR and Dessie when utilizing the contrastive loss  L D  F  L subscript L D F L L_{DFL} italic_L start_POSTSUBSCRIPT italic_D italic_F italic_L end_POSTSUBSCRIPT , in comparison to their supervised-learning variants employing  L gt subscript L gt L_{\\text{gt}} italic_L start_POSTSUBSCRIPT gt end_POSTSUBSCRIPT , as presented in Table  1 . Notably, DinoHMR and Dessie are trained without any real images. Our evaluation metrics include the Intersection Over Union (IoU) and the Percentage of Correct Keypoint (PCK) with a threshold of 0.10, applied to manually defined keypoints on the model across all unseen real-world datasets. For MagicPony dataset, we report PCK among the 17 predicted keypoints defined by ViTPose+. For Pascal and AnimalPose datasets, we follow the evaluation in  [ 67 ] , employing 16 keypoints as specified for Pascal. Dessie (w/o  L g  t subscript L g t L_{gt} italic_L start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT , w/  L D  F  L subscript L D F L L_{DFL} italic_L start_POSTSUBSCRIPT italic_D italic_F italic_L end_POSTSUBSCRIPT ) consistently outperforms other methods, supporting our hypothesis that a model promoting feature disentanglement achieves better generalization in zero-shot real image reconstructions."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  KeyPoint Evaluation.    \\dagger  : number taken from   [ 67 ] .",
        "table": "S4.T1.13",
        "footnotes": [
            ""
        ],
        "references": [
            "To overcome this challenge, we develop DessiePIPE. DessiePIPE allows for the creation of horse images with diverse combinations of camera viewpoints, horse appearances, and poses, utilizing Pytorch3D  [ 60 ]  and the hSMAL model as shown in Fig  2 . It differs from previous offline synthetic data generation, using limited textures and poses learned from 2D images [ 75 ] , or fixed shape  [ 52 ,  37 ] . A concurrent method is FoundationPose  [ 73 ] , which generates synthetic data using language and diffusion models for RGBD 3D rendering.",
            "To generate realistic horse appearances, we create a finite set containing 80 highly realistic UV texture maps following TEXTure  [ 62 ] , which generates texture maps for a given 3D shape with a text prompt by leveraging pretrained depth-to-image stable diffusion model  [ 63 ] . Specifically, the texture set is constructed with text prompts based on eight distinct horse species, including Bay Thoroughbred, Palomino Quarter Horse, Chestnut Morgan, Buckskin Tennessee Walker, White Arabian, Black Friesian, Dapple Gray Andalusian, Pinto Paint Horse. We refer the readers to  Supplementary Materials  for more details. For the horses shape, we obtain an infinite set by randomly sampling within the hSMAL shape space with a Gaussian distribution. Regarding the pose, we create a finite set composed of realistic horse poses extracted from PFERD  [ 39 ] , including daily motion poses (standing, walking, trotting, eating, bending neck) and advanced poses (sitting, rearing). Note that these poses also encapsulate the global rotations. Random image backgrounds come from the COCO dataset  [ 46 ] . With the described components, DessiePIPE randomly selects an item from each of the appearance, pose, and background sets to compose a horse image. This process is further exploited by DessiePIPE to generate pairs of images  ( I  1 , I  2 ) I 1 I 2 (I1,I2) ( italic_I 1 , italic_I 2 ) , where  I  1 I 1 I1 italic_I 1  and  I  2 I 2 I2 italic_I 2  are images that differ in one aspecteither model shape, pose, or global rotation. Fig  2  provides examples of how individual changes impact the rendered outcomes. Images are generated using Pytorch3D, each with a resolution of (256, 256). Ground-truth annotations for each image include: global rotation   G g  t subscript  subscript G g t \\theta_{G_{gt}} italic_ start_POSTSUBSCRIPT italic_G start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT , pose   J g  t subscript  subscript J g t \\theta_{J_{gt}} italic_ start_POSTSUBSCRIPT italic_J start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT , shape   g  t subscript  g t \\beta_{gt} italic_ start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT , global translation   g  t subscript  g t \\xi_{gt} italic_ start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT , silhouette  S g  t subscript S g t S_{gt} italic_S start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT , and landmark locations  K g  t subscript K g t K_{gt} italic_K start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT . We define 17 landmarks on the 3D model template, corresponding to the 2D animal keypoint labels in ViTPose+  [ 76 ] , and apply the Pytorch3D rasterizer to filter out and retain landmarks visible in the camera views.",
            "DinoHMR and Dessie both use DINO-ViTs8 as the backbone for the extractor  F F \\mathcal{F} caligraphic_F , with the last three layers unfrozen for adaptation to the 3D animal reconstruction task. In the decoder  D D \\mathcal{D} caligraphic_D , each module comprises two fully connected layers with dropout applied in between, followed by a fully connected layer to predict the residual of each parameter. The associated features and the corresponding parameters are concatenated as input to each module, which outputs residual to update the parameters for three iterations. Training spans up to 800 epochs, with a learning rate of  5  10  5 5 superscript 10 5 5\\times 10^{-5} 5  10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT . Model selection is based on achieving the lowest loss on the validation dataset. For the weights in the loss function, from Eq.  2  to Eq.  5 , we set   K = 0.001 subscript  K 0.001 \\omega_{K}=0.001 italic_ start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT = 0.001 ,   S = 0.0001 subscript  S 0.0001 \\omega_{S}=0.0001 italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT = 0.0001 ,   Prior  = 0.01 subscript superscript   Prior 0.01 \\omega^{\\beta}_{\\text{Prior}}=0.01 italic_ start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT Prior end_POSTSUBSCRIPT = 0.01 ,   Prior  = 0.01 subscript superscript   Prior 0.01 \\omega^{\\theta}_{\\text{Prior}}=0.01 italic_ start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT Prior end_POSTSUBSCRIPT = 0.01 , and   D = 0.02 subscript  D 0.02 \\omega_{D}=0.02 italic_ start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT = 0.02 . We apply color jittering to the images. The training process is conducted on two Nvidia A100 GPUs for approximately two days.",
            "We assess the performance of DinoHMR and Dessie against A-CSM  [ 35 ]  and Staths  [ 67 ] , with a constrained training dataset of 150 labeled images  [ 67 ] . Notably, Staths is an augmented A-CSM framework with additional web-sourced images for training. During the training, we construct batches to maintain an equal proportion of synthetic and real images, selecting model based on the performance of the synthetic validation set. Real images are excluded from computing  L D  F  L subscript L D F L L_{DFL} italic_L start_POSTSUBSCRIPT italic_D italic_F italic_L end_POSTSUBSCRIPT  and the silhouette loss  L S  I  L subscript L S I L L_{SIL} italic_L start_POSTSUBSCRIPT italic_S italic_I italic_L end_POSTSUBSCRIPT  to maintain consistency with  [ 67 ]  during training. The effectiveness is quantified on AnimalPose and Pascal datasets as shown in Table  2 , using the Area Under the Curve (AUC) metric to capture PCK performance across varying thresholds from 0.06 to 0.1. The results show that both DinoHMR and Dessie, even when fine-tuned with only the 150 images, outperform Staths, highlighting their effectiveness in utilizing limited real-world data resources."
        ]
    },
    "id_table_3": {
        "caption": "",
        "table": "S4.T3.st1.10.8",
        "footnotes": [
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Inspired by HMR  [ 25 ]  and DINO-related feature studies  [ 8 ,  71 ,  1 ,  74 ] , we propose two extractor-decoder frameworks that incorporate the DINO model: 1) DinoHMR, an HMR-like model, and 2) Dessie, a multi-stream structure with a modified contrastive loss  L D  F  L subscript L D F L L_{DFL} italic_L start_POSTSUBSCRIPT italic_D italic_F italic_L end_POSTSUBSCRIPT , for disentangled shape and pose estimation (Fig  3 ). Unlike HMR, which encodes image features with ResNet, both DinoHMR and Dessie exploit the capabilities of DINOs key features from the last layer.",
            "In DinoHMR, we employ an  extractor  with a Conv2D network to extract a single feature representation from the keys, and a  decoder  for predicting shape    \\beta italic_ , pose   J subscript  J \\theta_{J} italic_ start_POSTSUBSCRIPT italic_J end_POSTSUBSCRIPT , global rotation   G subscript  G \\theta_{G} italic_ start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT  and translation    \\xi italic_  parameters. In contrast, the Dessie  extractor   F ( . ) \\mathcal{F}(.) caligraphic_F ( . )  employs three Conv2D networks on the keys to isolate features   A   A subscript  A subscript  A \\gamma_{A}\\in\\Gamma_{A} italic_ start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT  roman_ start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT ,   P   P subscript  P subscript  P \\gamma_{P}\\in\\Gamma_{P} italic_ start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT  roman_ start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT , and   G   G subscript  G subscript  G \\gamma_{G}\\in\\Gamma_{G} italic_ start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT  roman_ start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT , corresponding to the feature spaces for subject appearance, subject pose, and global information, respectively, with   A ,  P ,  G  R 640 subscript  A subscript  P subscript  G superscript R 640 \\Gamma_{A},\\Gamma_{P},\\Gamma_{G}\\subseteq\\mathbb{R}^{640} roman_ start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT , roman_ start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT , roman_ start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT 640 end_POSTSUPERSCRIPT . The Dessie  decoder  is composed of three distinct submodules  M A subscript M A \\mathcal{M}_{A} caligraphic_M start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT ,  M P subscript M P \\mathcal{M}_{P} caligraphic_M start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT , and  M G subscript M G \\mathcal{M}_{G} caligraphic_M start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT , each processing a specific set of features extracted by the Dessie  extractor . As shown in Fig.  3 , the submodule  M A subscript M A \\mathcal{M}_{A} caligraphic_M start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT  is responsible for inferring the shape parameters    \\beta italic_  from the appearance features   A subscript  A \\gamma_{A} italic_ start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT . The submodule  M P subscript M P \\mathcal{M}_{P} caligraphic_M start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT  deduces the pose parameters,   J subscript  J \\theta_{J} italic_ start_POSTSUBSCRIPT italic_J end_POSTSUBSCRIPT  leveraging the pose features   P subscript  P \\gamma_{P} italic_ start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT . Lastly,  M G subscript M G \\mathcal{M}_{G} caligraphic_M start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT  estimates both the global rotation   G subscript  G \\theta_{G} italic_ start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT  and the global translation    \\xi italic_ , utilizing the global information features   G subscript  G \\gamma_{G} italic_ start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT . The entire framework is described by:",
            "We evaluate the quality of keypoint transfer  [ 35 ] , by generating source-target image pairs from the PASCAL validation list. For each source image, we project the visible vertices of the predicted mesh onto it and align each annotated 2D ground-truth keypoint with its closest vertex. This vertex is projected onto the target image to compute the error with the ground-truth target points, quantified by PCK@0.1. Table  3(a)  shows that Dessie outperforms the latest learning-based SOTAs. Remarkably, 3D Fauna is trained on a dataset larger than MagicPony.",
            "We further conduct a quantitative 3D evaluation on a subset of PFERD  [ 39 ]  by computing the Chamfer Distance (CD) after Procrustes alignment between the 3D GT and predicted results. As shown in Table  3(b) , Dessie outperforms other SOTAs."
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "S4.T3.st2.2.2",
        "footnotes": [],
        "references": [
            "Given that we train Dessie with DINO backbone unfrozen, we are interested in one question:  How does the 3D reconstruction learning affect the DINO feature space?  To answer this question, we utilize PCA to visualize the key features extracted from the DINO models last layer, and examine the self-similarity of these features, as in previous studies  [ 71 ] . Brighter areas represent higher component values, indicating regions of intense focus for reconstruction. This analysis uses two synthetic test images and two real images from the AnimalPose dataset, as shown in Fig  4 . The comparison reveals that while the vanilla DINO features capture the spatial layout and structure of both the subjects and environments, the features in Dessie shift to focus more toward the targeted reconstructed subjects, effectively minimizing background distractions."
        ]
    }
}