{
    "S4.T1.1.1": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T1.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r\" id=\"S4.T1.1.1.1.1.1\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.1.1.1.1\">Datasets</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T1.1.1.1.1.2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.1.1.2.1\">Ours+ARMA</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T1.1.1.1.1.3\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.1.1.3.1\">Ours+GCN</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T1.1.1.1.1.4\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.1.1.4.1\">MedSAM</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06114v1#bib.bib5\" title=\"\">5</a>]</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T1.1.1.1.1.5\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.1.1.5.1\">MinCutPool&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06114v1#bib.bib15\" title=\"\">15</a>]</cite></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T1.1.1.1.1.6\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.1.1.6.1\">GDISM<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06114v1#bib.bib20\" title=\"\">20</a>]</cite></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T1.1.1.1.1.7\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.1.1.7.1\">DSM</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06114v1#bib.bib17\" title=\"\">17</a>]</cite>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T1.1.1.1.1.8\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.1.1.8.1\">ViT-Kmeans</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S4.T1.1.1.2.1.1\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">KVASIR</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.1.1.2.1.2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">75.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.1.1.2.1.3\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.1.1.2.1.4\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">76.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.1.1.2.1.5\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">73.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.1.1.2.1.6\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">59.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.1.1.2.1.7\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">58.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.1.1.2.1.8\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">66.10</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.1.1.3.2.1\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">ISIC-2018</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.3.2.2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">68.72</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.3.2.3\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">73.94</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.3.2.4\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">61.36</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.3.2.5\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">72.31</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.3.2.6\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">52.53</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.3.2.7\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">72.20</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.3.2.8\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">68.60</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T1.1.1.4.3.1\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">CVC-ClinicDB</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.4.3.2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">67.93</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.4.3.3\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">64</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.4.3.4\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">71.53</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.4.3.5\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">62.10</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.4.3.6\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">59.22</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.4.3.7\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">60.48</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.4.3.8\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">62.80</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 1:  Average mIOU scores for medical image data",
        "footnotes": [
            "[5] \nJun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang.\n\n Segment anything in medical images.\n\n Nature Communications , 15(1):654, 2024.\n\n",
            "[15] \nFilippo Maria Bianchi, Daniele Grattarola, and Cesare Alippi.\n\n Spectral clustering with graph neural networks for graph pooling.\n\n In  International conference on machine learning , pages 874–883. PMLR, 2020.\n\n",
            "[20] \nMarco Trombini, David Solarna, Gabriele Moser, and Silvana Dellepiane.\n\n A goal-driven unsupervised image segmentation method combining graph-based processing and markov random fields.\n\n Pattern Recognition , 134:109082, 2023.\n\n",
            "[17] \nLuke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and Andrea Vedaldi.\n\n Deep spectral methods: A surprisingly strong baseline for unsupervised semantic segmentation and localization.\n\n In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8364–8375, 2022.\n\n"
        ],
        "references": [
            "As shown in Table 1, UnSeGArmaNet outperforms other unsupervised methods. Notably, it achieves comparable results to MedSAM on KVASIR [27] and ISIC-2018 [28] datasets, despite MedSAM being fine-tuned on a large pool of medical image datasets, including large colonoscopy datasets. However, MedSAM struggles with modality-imbalance issues, which may have contributed to its better performance on KVASIR and CVC-ClinicDB datasets, but significantly lower scores on ISIC-2018. This imbalance is likely due to MedSAM’s training on large polyp and small dermoscopy image-mask pairs.",
            "Our approach consists of two components: a pre-trained network and a Graph Arma Convolutional Neural Network (ARMAConv) with its optimization. We utilize DINO-ViT small as the pre-trained network, which is a widely adopted feature extractor known for providing robust and adaptable features across different data modalities [18]. However, as demonstrated through the results of ViT-Kmeans, (where ViT features are fed to vanilla k-means algorithm), these features alone are insufficient for producing high-quality segmentation results. This is evident from the coarse segmentation outcomes shown in Fig. 2 and Fig. 3. The idea of GCN integrated with ARMA convolutional filter is applied in MedSAM [5]. Although MedSAM is a generalized model for universal medical image segmentation, it is vulnerable to modality imbalance issues, as discussed in Section 4.2. Moreover, even if images from all the modalities are equally represented, it does not guarantee good accuracies across all the modalities. Furthermore, updating such a model is computationally expensive, making it a challenging task. While DINO-ViT generates useful global features, it often leads to coarse image segmentation, as depicted in Fig. 2 and Fig. 3. Whereas, the proposed model can effectively leverage the features to capture both local and global contexts in images from diverse modalities. While GDISM [20] effectively captures local contextual information (Fig.2 and Fig. 3), its underlying methodologies result in limited mean intersection over union (mIoU) performance as shown in Table 1."
        ]
    },
    "S4.T2.1.1": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T2.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r\" id=\"S4.T2.1.1.1.1.1\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.1.1.1\">Datasets</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T2.1.1.1.1.2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.1.2.1\">Ours+ARMA</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T2.1.1.1.1.3\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.1.3.1\">Ours+GCN</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T2.1.1.1.1.4\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.1.4.1\">MinCutPool<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06114v1#bib.bib15\" title=\"\">15</a>]</cite></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T2.1.1.1.1.5\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.1.5.1\">DSM<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text ltx_font_medium\" id=\"S4.T2.1.1.1.1.5.1.1.1\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06114v1#bib.bib17\" title=\"\">17</a><span class=\"ltx_text ltx_font_medium\" id=\"S4.T2.1.1.1.1.5.1.2.2\">]</span></cite></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S4.T2.1.1.1.1.6\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.1.6.1\">ViT-Kmeans</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S4.T2.1.1.2.1.1\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">ECSSD</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.2.1.2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">77.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.2.1.3\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">75.57</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.2.1.4\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">74.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.2.1.5\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">73.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.2.1.6\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">65.76</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.1.1.3.2.1\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">DUTS</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.3.2.2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">60.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.3.2.3\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">61.04</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.3.2.4\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">59.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.3.2.5\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">51.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.3.2.6\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">42.78</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S4.T2.1.1.4.3.1\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">CUB</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.4.3.2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">81.23</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.4.3.3\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">78.41</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.4.3.4\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">78.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.4.3.5\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">76.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.4.3.6\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">51.86</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 2:  Average mIOU scores for computer vision datasets",
        "footnotes": [
            "[15] \nFilippo Maria Bianchi, Daniele Grattarola, and Cesare Alippi.\n\n Spectral clustering with graph neural networks for graph pooling.\n\n In  International conference on machine learning , pages 874–883. PMLR, 2020.\n\n",
            "[17] \nLuke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and Andrea Vedaldi.\n\n Deep spectral methods: A surprisingly strong baseline for unsupervised semantic segmentation and localization.\n\n In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8364–8375, 2022.\n\n"
        ],
        "references": [
            "We evaluate UnSeGArmaNet on publicly available ECSSD, CUB, and DUTs vision datasets, comparing it to MinCutPool [15], DSM [17], and ViT-Kmeans. The results, summarized in Table 2, show that UnSeGArmaNet outperforms the other methods. Notably, UnSeGArmaNet and the compared methods all utilize ViT to extract high-level features. However, ViT-Kmeans, which applies k-means algorithm [37] to these features, achieves significantly lower scores than UnSeGArmaNet on all datasets. This highlights that high-level features from VIT alone are insufficient for effective segmentation. Our modularity-based UnSeGArmaNet framework surpasses the performance of  [15] and  [17], which employ graph-cut based spectral decomposition in their deep learning architectures."
        ]
    }
}