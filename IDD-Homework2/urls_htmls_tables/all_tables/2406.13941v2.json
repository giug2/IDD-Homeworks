{
    "S4.T1": {
        "caption": "Table 1. Workload Configurations",
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T1.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.1.2.1\">Category</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.1.3.1\">Workload</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.1.4.1\">Avg.Reduction</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.1.1.1\">\n<math alttext=\"\\#\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.1.1.1.1.m1.1\"><semantics id=\"S4.T1.1.1.1.1.m1.1a\"><mi id=\"S4.T1.1.1.1.1.m1.1.1\" mathvariant=\"normal\" xref=\"S4.T1.1.1.1.1.m1.1.1.cmml\">#</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.1.1.1.1.m1.1b\"><ci id=\"S4.T1.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T1.1.1.1.1.m1.1.1\">#</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.1.1.1.1.m1.1c\">\\#</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T1.1.1.1.1.m1.1d\">#</annotation></semantics></math><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.1.1.1\">Items</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.2.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.2.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T1.1.1.2.1.1.1\">Low Hot</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.2.1.2\">AmazonClothes(clo)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al<span class=\"ltx_text\">.</span>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.13941v2#bib.bib16\" title=\"\">2019</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.2.1.3\">52.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.2.1.4\">2,685,059</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.3.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.3.2.1\">AmazonHome(home)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ni et&#160;al<span class=\"ltx_text\">.</span>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.13941v2#bib.bib16\" title=\"\">2019</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.3.2.2\">67.56</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.3.2.3\">1,301,225</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.4.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.4.3.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T1.1.1.4.3.1.1\">Medium Hot</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.4.3.2\">MetaFBGEMM1(meta1)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(met, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.13941v2#bib.bib2\" title=\"\">2022</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.4.3.3\">107.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.4.3.4\">5,783,210</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.5.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.5.4.1\">MetaFBGEMM2(meta2)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(met, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.13941v2#bib.bib2\" title=\"\">2022</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.5.4.2\">188.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.5.4.3\">5,999,981</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.6.5\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\" id=\"S4.T1.1.1.6.5.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T1.1.1.6.5.1.1\">High Hot</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.6.5.2\">GoodReads(read)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wan et&#160;al<span class=\"ltx_text\">.</span>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.13941v2#bib.bib18\" title=\"\">2018</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.13941v2#bib.bib19\" title=\"\">2019</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.6.5.3\">245.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.6.5.4\">2,360,650</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.7.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T1.1.1.7.6.1\">GoodReads2(read2)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wan et&#160;al<span class=\"ltx_text\">.</span>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.13941v2#bib.bib18\" title=\"\">2018</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.13941v2#bib.bib19\" title=\"\">2019</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T1.1.1.7.6.2\">374.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T1.1.1.7.6.3\">2,360,650</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "Workloads. We adopt Meta\u2019s deep learning recommendation model (DLRM)\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Naumov et&#160;al<span class=\"ltx_text\">.</span>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.13941v2#bib.bib14\" title=\"\">2019</a>)</cite> with six real-world datasets\n(Naumov et\u00a0al., 2019) with six real-world datasets<sup class=\"ltx_note_mark\">1</sup>\n1<sup class=\"ltx_note_mark\">1</sup>\n11https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2\n<br class=\"ltx_break\"/>\nhttps://mengtingwan.github.io/data/goodreads.html, as shown in Table\u00a01. The datasets can be categorized into three groups, namely low hot, medium hot and high hot, according to the average reduction frequency of the dataset. In our experiments, we duplicate each dataset to form eight EMTs and each embedding vector has 32 dimensions. To measure inference performance, we conduct a sampling of 12,800 inferences in each set of experiments. The batch size is set to 64."
        ]
    },
    "S4.T2": {
        "caption": "Table 2. Specifics of evaluated hardware architectures",
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T2.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.2.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.2.1.1.1\">Implementation</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.2.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.2.1.2.1\">Architecture</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.2.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.2.1.3.1\">CPU core</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.2.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.2.1.4.1\">Memory</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.3.2.1\">DLRM-CPU&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Naumov et&#160;al<span class=\"ltx_text\">.</span>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.13941v2#bib.bib14\" title=\"\">2019</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.3.2.2\">Intel Xeon(R) Silver 4110(2.10GHz)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.3.2.3\">32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.3.2.4\">128GB</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.4.3.1\">DLRM-Hybrid&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Adnan et&#160;al<span class=\"ltx_text\">.</span>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.13941v2#bib.bib5\" title=\"\">2021</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.4.3.2\">Intel Xeon(R) Silver 4110(2.10GHz)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.4.3.3\">32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.4.3.4\">128GB</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.5.4.1\">FAE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Adnan et&#160;al<span class=\"ltx_text\">.</span>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.13941v2#bib.bib5\" title=\"\">2021</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.5.4.2\">NVIDIA GeForce GTX 1080 Ti</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S4.T2.1.1.5.4.3\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.5.4.4\">11GB</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.6.5.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T2.1.1.6.5.1.1\">UpDLRM</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.6.5.2\">Intel Xeon(R) Silver 4110(2.10GHz)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S4.T2.1.1.6.5.3\">32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.6.5.4\">128GB</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T2.1.1.1.1\">UPMEM DPU(350MHz) <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.1.1.1.1.m1.1\"><semantics id=\"S4.T2.1.1.1.1.m1.1a\"><mo id=\"S4.T2.1.1.1.1.m1.1.1\" xref=\"S4.T2.1.1.1.1.m1.1.1.cmml\">&#215;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.1.1.1.1.m1.1b\"><times id=\"S4.T2.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T2.1.1.1.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.1.1.1.1.m1.1c\">\\times</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T2.1.1.1.1.m1.1d\">&#215;</annotation></semantics></math> 256</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" id=\"S4.T2.1.1.1.2\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T2.1.1.1.3\">16GB</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "Comparisons. We compare UpDLRM with three other open-source DLRM implementations, as detailed in Table\u00a02.\nDLRM-CPU is the CPU-only implementation that adopts CPU for both EMT storage and parallel computations. DLRM-Hybrid and FAE\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Adnan et&#160;al<span class=\"ltx_text\">.</span>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.13941v2#bib.bib5\" title=\"\">2021</a>)</cite> are two implementations based on the CPU-GPU hybrid architecture. In the hybrid architecture, CPU is used to store EMTs and perform embedding lookups, while the GPU manages CTR computation. Embedding lookup results are communicated to the GPU via PCI-e. The FAE approach differs from DLRM-Hybrid in that it involves placing a subset of highly accessed item embedding vectors in a cache space, such as the GPU memory, to accelerate DLRM inference time.\nOur UpDLRM implementation is illustrated in Figure&#160;\n(Adnan et\u00a0al., 2021) are two implementations based on the CPU-GPU hybrid architecture. In the hybrid architecture, CPU is used to store EMTs and perform embedding lookups, while the GPU manages CTR computation. Embedding lookup results are communicated to the GPU via PCI-e. The FAE approach differs from DLRM-Hybrid in that it involves placing a subset of highly accessed item embedding vectors in a cache space, such as the GPU memory, to accelerate DLRM inference time.\nOur UpDLRM implementation is illustrated in Figure\u00a04, where cache-aware EMT partitioning is used in the pre-process stage. We adopt two UPMEM modules, totalling 256 DPUs. Each DPU employs 14 tasklets."
        ]
    }
}