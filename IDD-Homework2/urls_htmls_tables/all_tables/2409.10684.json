{
    "id_table_1": {
        "caption": "TABLE I :  Closed-set classification performances.",
        "table": "S5.T1.5.5",
        "footnotes": [],
        "references": [
            "Given some kind of text representation    \\tau italic_  and a composite model  T  (  ) T  \\mathcal{T}(\\cdot) caligraphic_T (  ) , TTMs techniques model the function  x = T  (  ) x T  \\mathbf{x}=\\mathcal{T}(\\tau) bold_x = caligraphic_T ( italic_ ) , where  x  R 1  N x superscript R 1 N \\mathbf{x}\\in\\mathbb{R}^{1\\times N} bold_x  blackboard_R start_POSTSUPERSCRIPT 1  italic_N end_POSTSUPERSCRIPT  is an audio waveform containing music corresponding to the textual description provided in    \\tau italic_ . Then, the text-to-music attribution problem, schematically shown in Fig.  1 , can be formally defined as follows. Given the time-discrete music signal  x x \\mathbf{x} bold_x  and a set of  I I I italic_I  TTM models  { T 0 , ... , T I  1 } subscript T 0 ... subscript T I 1 \\{\\mathcal{T}_{0},\\ldots,\\mathcal{T}_{I-1}\\} { caligraphic_T start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , ... , caligraphic_T start_POSTSUBSCRIPT italic_I - 1 end_POSTSUBSCRIPT } , the objective is to retrieve which generator  T i subscript T i \\mathcal{T}_{i} caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  has been used to generate  x x \\mathbf{x} bold_x . This is done by training a classifier that takes as input  x x \\mathbf{x} bold_x  and outputs the probabilities  p i , i = 0 , ... , I  1 formulae-sequence subscript p i i 0 ... I 1 p_{i},i=0,\\ldots,I-1 italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_i = 0 , ... , italic_I - 1  of  x x \\mathbf{x} bold_x  being generated using each of the known TTM models."
        ]
    },
    "id_table_2": {
        "caption": "TABLE II :  Open set (Threshold) classification performances.",
        "table": "S5.T2.1.1",
        "footnotes": [],
        "references": [
            "Despite closed-set classification on a single dataset is often considered a trivial task in forensic applications, it is worth investigating the performance of the tested methods in this scenario. Table  I  reports closed-set classification results in terms of balanced accuracy  ACC B subscript ACC B \\mathrm{ACC}_{B} roman_ACC start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT , Precision, Recall, and F1 Score. The left column of Fig.  2  shows the confusion matrix corresponding to M5, RawNet2 and ResNet18+Spec, respectively. In all metrics, the best performances are obtained by ResNet18 + Spec, while RawNet2 obtains slightly worse results than M5. From the inspection of the confusion matrices, we can see that ResNet18 slightly confounds TTM03 (AudioLDM2) with TTM05(Mustango), both diffusion-based models. M5 has slightly lower performances in detecting the ground-truth data, while RawNet2 struggles more to detect model TTM02 (MusicLDM).",
            "We show in Table  II  the performances when performing open set classification using the threshold approach and the corresponding confusion matrices in the second column of Fig.  2 .",
            "Results corresponding to the open set classification using the SVM approach are shown in Table  III , with the corresponding confusion matrices in the last column of Fig.  2 ."
        ]
    },
    "id_table_3": {
        "caption": "TABLE III :  Open set (SVM) classification performances.",
        "table": "S5.T3.1.1",
        "footnotes": [],
        "references": [
            "We further perform a small experiment to verify how much the impact of the temporal window length used as input to the models changes their performance. This is important, especially for the design of further datasets, i.e. do we need to create longer musical excerpts or not? We consider four window lengths, namely  10  s 10 s 10~{}\\mathrm{s} 10 roman_s ,  7.5  s 7.5 s 7.5~{}\\mathrm{s} 7.5 roman_s ,  5  s 5 s 5\\mathrm{s} 5 roman_s  and  2.5  s 2.5 s 2.5~{}\\mathrm{s} 2.5 roman_s  and report the results in terms of balanced accuracy in Fig.  3 . As we can see, the variations in accuracy are not extreme in all classification scenarios. M5 seems to have an accuracy boost passing from  7.5 7.5 7.5 7.5  to  10  s 10 s 10\\mathrm{s} 10 roman_s  window length for both closed set and open set (threshold) methods, ResNet18+Spec does not have major improvements, while a slight increase in accuracy is seen for RawNet2. Results in the case of Open set (SVM) show a less clear trend, but the impact of the window size does not seem to be relevant even in this case."
        ]
    }
}