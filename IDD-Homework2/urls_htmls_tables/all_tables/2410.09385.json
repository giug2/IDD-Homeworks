{
    "id_table_1": {
        "caption": "Table 1:  Characteristics of Datasets Used for Zero-Shot Evaluation of Mamba4Cast and baselines.",
        "table": "A5.EGx1",
        "footnotes": [],
        "references": [
            "Our proposed architecture, illustrated in Figure  1 , consists of four primary components:   (1)  Pre-processing : we scale the input series using a Min-Max Scaler and extract time features for positional embeddings. (2)  Embedding : we embed the scaled input values and their temporal information using convolutions with different dilations, ensuring a large receptive field for the representation used by future layers. For more details about data pre-processing and embedding, refer to Appendix  B . (3)  Encoder : comprises of Mamba2 blocks with LayerNorm to avoid noisy learning signals followed by another dilated convolution layer. (4)  Decoder : the final component is a linear projection layer that transforms the embedded token representations into point forecasts.",
            "The Mamba4Cast model is designed with approximately 27M parameters, positioning it between Chronos-Mini (20M) and Chronos-Small (46M) in size. As demonstrated in Figure  1 , Mamba4Cast is built on Mamba2  (Dao & Gu,  2024 )  with a state expansion factor (N) of 128 and a block expansion factor (E) of 2. It features 2 encoder layers following an input projection to an embedding dimension of 1024. The final layer of the encoder is defined similarly to the stacked convolution layer illustrated in Appendix  B  with the difference in the input channels being 1024 for the embedding size. We minimize the mean squared error over the prediction horizon using AdamW  (Loshchilov & Hutter,  2019 ) .",
            "We use 17 datasets from Chronos zero-shot benchmark while removing datasets with very small context and prediction length, datasets that are very large, and datasets with sub-hourly frequencies. We will extend to support those datasets in future work. We used GluonTS as an interface for these datasets to have a comparable evaluation pipeline to Chronos. The context length (input sequence length) was restricted to be at most 512, while the prediction length varied according to the evaluated dataset as shown in Table  1 .",
            "The ablation studies were conducted on the first 360K training rounds mentioned in Section  4.1 , as the subsequent 60K were later applied to our chosen setup for the baseline comparisons cited in Section  4.2 .",
            "As part of our evaluation, we tested the performance of our model on real-world time series datasets alongside the synthetic data. The primary metric used was the seasonal Mean Absolute Scaled Error (MASE), which scales the forecast error by the mean absolute error of a seasonal naive forecast on the training data. The evaluation of Mamba4Cast on real-world datasets demonstrates the models capability to generalize and perform well in diverse, real-world forecasting scenarios. Detailed evaluations per dataset can be found in Table  3 . We witnessed inconsistencies between the evaluations performed by AutoGluon in our setups and the ones reported in Chronos paper on datasets with daily frequency, specifically on \"Covid Deaths.\" This resulted in the large gap witnessed on ForecastPFNs results reported here, since the models MASE evaluations are sourced from the Chronos paper. The results reported for Mamba4Cast per dataset are evaluated with the best model trained according to the procedures in Section  4.1 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Ablation study on architectural changes, prior mixing ratios and the inference modes. The value reported is the geometric mean of MASE across all 17 datasets for each setup.",
        "table": "A3.T1.1",
        "footnotes": [],
        "references": [
            "The model is trained exclusively on synthetic data generated using two methods outlined in Section   3.3 . The data composition is 70% sampled from GP priors and 30% sampled from FPFN priors, leveraging the GP kernels flexibility in capturing diverse patterns. Training was conducted over 3 days on a single Nvidia RTX2080Ti GPU, for 360k training rounds consisting of 64 independently generated samples each. As stated in Appendix  A.2 , we continue training for another 60K rounds with a changed kernel composition and a learning rate of 1e-6.",
            "The results, as illustrated in Figures  2  and  3 , demonstrate that Mamba4Cast achieves competitive performance with Chronos-Base(200M) and surpasses other baselines. Notably, this performance is achieved without fine-tuning on real-world datasets. Figure  3  shows a critical difference diagram, visualizing the mean model rankings based on MASE (Mean Absolute Scaled Error) over the datasets. In this diagram, models are arranged from best ( left ) to worst ( right ), with statistically insignificant performance differences indicated by connecting horizontal lines (at a significance level of   = 0.05  0.05 \\alpha=0.05 italic_ = 0.05 ). Detailed information on the MASE metric and per-dataset results can be found in Appendix  E .",
            "The model sizes of the three setups listed in the corresponding section of Table  2  are 27M, 17M, and 15M, in the same order as in the table.",
            "We further test the impact of ensembling by averaging the forecasts generated at 5 different levels of dropout, from 0 to 0.5, of the input sequence. However, given the superior performance over longer and more inclusive contexts, demonstrated in Figure  4 , it follows that including a less accurate forecast can degrade performance in case Mamba4Cast is certain about its forecast as shown in Table  2 .",
            "The ablation studies were conducted on the first 360K training rounds mentioned in Section  4.1 , as the subsequent 60K were later applied to our chosen setup for the baseline comparisons cited in Section  4.2 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  MASE evaluations on all of the 17 datasets with the lower value the better. The best results per dataset are in bold and the second best results are underlined.",
        "table": "A4.T2.15.15",
        "footnotes": [],
        "references": [
            "The model is trained for 420K batches of size 64, using data sampled from the priors in Section  3.3 , via a parallelized data loader that ensures the same sample is not seen twice. We train on sequence lengths uniformly sampled between 30 and 512 and minimize the mean squared error over a prediction length uniformly sampled between 10 and 60 per batch. 50% of the time we train to predict a contiguous chunk from the middle of the prediction length to improve predictability over the sequence by reducing reliance on previous states and encouraging emphasis on temporal information. The learning rate is cosine annealed  (Loshchilov & Hutter,  2017 )  from 1e-5 to 1e-7 throughout the training.",
            "The model is trained exclusively on synthetic data generated using two methods outlined in Section   3.3 . The data composition is 70% sampled from GP priors and 30% sampled from FPFN priors, leveraging the GP kernels flexibility in capturing diverse patterns. Training was conducted over 3 days on a single Nvidia RTX2080Ti GPU, for 360k training rounds consisting of 64 independently generated samples each. As stated in Appendix  A.2 , we continue training for another 60K rounds with a changed kernel composition and a learning rate of 1e-6.",
            "The results, as illustrated in Figures  2  and  3 , demonstrate that Mamba4Cast achieves competitive performance with Chronos-Base(200M) and surpasses other baselines. Notably, this performance is achieved without fine-tuning on real-world datasets. Figure  3  shows a critical difference diagram, visualizing the mean model rankings based on MASE (Mean Absolute Scaled Error) over the datasets. In this diagram, models are arranged from best ( left ) to worst ( right ), with statistically insignificant performance differences indicated by connecting horizontal lines (at a significance level of   = 0.05  0.05 \\alpha=0.05 italic_ = 0.05 ). Detailed information on the MASE metric and per-dataset results can be found in Appendix  E .",
            "We adopted the prior generation process from  Dooley et al. ( 2023 )  that decomposes the time series into three components as outlined in Section  3.3 . The trend incorporates linear and exponential growth factors, while seasonal components capture periodic variations at multiple time scales (minutely, hourly, daily, weekly, and monthly), reflecting natural cycles in the data. Noise is modeled using a Weibull distribution to maintain a constant expected value. We introduced some modifications to the original procedure that are mentioned below.",
            "Prior Mixing Ratios:  Given the importance of the distribution of synthetic data, we conducted experiments to explore the impact of each of the two approaches mentioned in Section  3.3 . The ablation indicates the effectiveness of the GP prior over the FPFN prior, leading to our choice of a GP favoured mixture of data for training.",
            "As part of our evaluation, we tested the performance of our model on real-world time series datasets alongside the synthetic data. The primary metric used was the seasonal Mean Absolute Scaled Error (MASE), which scales the forecast error by the mean absolute error of a seasonal naive forecast on the training data. The evaluation of Mamba4Cast on real-world datasets demonstrates the models capability to generalize and perform well in diverse, real-world forecasting scenarios. Detailed evaluations per dataset can be found in Table  3 . We witnessed inconsistencies between the evaluations performed by AutoGluon in our setups and the ones reported in Chronos paper on datasets with daily frequency, specifically on \"Covid Deaths.\" This resulted in the large gap witnessed on ForecastPFNs results reported here, since the models MASE evaluations are sourced from the Chronos paper. The results reported for Mamba4Cast per dataset are evaluated with the best model trained according to the procedures in Section  4.1 ."
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "A5.T3.1.1",
        "footnotes": [],
        "references": [
            "We conduct a qualitative inspection of Mamba4Cast to evaluate its ability to extrapolate over the forecasting horizon. Figure  4  illustrates Mamba4Casts improvement with increasing context length and its ability to capture real-life patterns. We also visualize the models forecasting capability on additional real-world data in Appendix  E .",
            "We further test the impact of ensembling by averaging the forecasts generated at 5 different levels of dropout, from 0 to 0.5, of the input sequence. However, given the superior performance over longer and more inclusive contexts, demonstrated in Figure  4 , it follows that including a less accurate forecast can degrade performance in case Mamba4Cast is certain about its forecast as shown in Table  2 .",
            "The ablation studies were conducted on the first 360K training rounds mentioned in Section  4.1 , as the subsequent 60K were later applied to our chosen setup for the baseline comparisons cited in Section  4.2 .",
            "As part of our evaluation, we tested the performance of our model on real-world time series datasets alongside the synthetic data. The primary metric used was the seasonal Mean Absolute Scaled Error (MASE), which scales the forecast error by the mean absolute error of a seasonal naive forecast on the training data. The evaluation of Mamba4Cast on real-world datasets demonstrates the models capability to generalize and perform well in diverse, real-world forecasting scenarios. Detailed evaluations per dataset can be found in Table  3 . We witnessed inconsistencies between the evaluations performed by AutoGluon in our setups and the ones reported in Chronos paper on datasets with daily frequency, specifically on \"Covid Deaths.\" This resulted in the large gap witnessed on ForecastPFNs results reported here, since the models MASE evaluations are sourced from the Chronos paper. The results reported for Mamba4Cast per dataset are evaluated with the best model trained according to the procedures in Section  4.1 ."
        ]
    }
}