{
    "id_table_1": {
        "caption": "Table 1:  Evaluated models in RAD-Bench. For each scenario,  bold score  indicates the best open-weight model;  underlined score  marks the best model overall. We report instruct versions of the open-weight models.",
        "table": "S3.T1.1.1",
        "footnotes": [],
        "references": [
            "To address the aforementioned gap, we propose Retrieval Augmented Dialogue Benchmark (RAD-Bench), a benchmark designed to measure LLMs ability to follow user instructions in multi-turn dialogue scenarios and effectively recall and utilize retrieved context to enhance their responses. Specifically, as shown in Figure  1 , each benchmark sample consists of three-turn questions with accompanied retrieved context at each turn. We curate sample scenarios from real-world multi-turn dialogue data where continuous context application is essential for generation augmentation. These scenarios are grouped into two major evaluated abilities of LLMs:  Retrieval Synthesis  and  Retrieval Reasoning . We define Retrieval Synthesis the ability of LLMs to progressively integrate and utilize retrieved context for tasks such as summarization and article writing, enabling effective knowledge accumulation and synthesis. We define Retrieval Reasoning being whether LLMs can make reasonable inference when user intent changes or additional conditions are introduced across turns, utilizing context in each turn to refine and improve responses. For each evaluated abilities, we select three representative scenarios that exemplify multi-turn dialogues following retrievals.",
            "RAD-Bench aims to evaluate the capabilities of LLMs in multi-turn dialogues following retrievals, as exemplified by real world application such as ChatGPT  (OpenAI,  2023 ) , Perplexity  (Perplexity AI,  2024 ) , and MediaTek DaVinci  (MediaTek,  2024 ) . As shown in Figure  1 , each sample in the benchmark consists of three-turn questions with accompanied retrieved context to simulate the retrieval augmented dialogues. Responses to the turn questions by an LLM are evaluated by a reference-guided-judge, and a point-wise evaluation score for the LLM is reported.",
            "We employ an LLM as a scorer to assist the filtering of question candidates. For each scenario, we design customized prompts following scoring criteria to score each candidate. The criteria include Relevance, Progression, Clarity, Support, Knowledge Points, and Medium Complexity as shown in Figure  14 . The Support and Knowledge criteria prompt the scoring LLMs to examine whether the retrieved context from web search and RAG services contains relevant information for answering candidate questions. We scored candidates with BreeXe-8x7B, Llama3-70B, and Mixtral-8x22B. After conducting a human review of a subsampled set of scored candidates, we selected the scoring results from BreeXe-8x7B due to its preferable alignment with the established criteria. With the scored candidates of three-turn questions for each scenario, we then filtered out the top candidates and manually verified that the retrieved contexts contain informative and relevant information for answering the questions in each turn.",
            "We show scores of evaluated models in Table  1  and in Figure  2 . Overall speaking, the closed-source models, particularly  GPT-4o  with average of 8.72, consistently outperformed the open-source models across most scenarios. As for the open-source models,  Llama3.1-405B  and  Deepseek-v2  show strong performance with averages of 7.88 and 7.86, respectively. These two models stand out within the open-source category, though still trailing behind the top closed-source models."
        ]
    },
    "id_table_2": {
        "caption": "",
        "table": "A2.F21.2",
        "footnotes": [],
        "references": [
            "We show scores of evaluated models in Table  1  and in Figure  2 . Overall speaking, the closed-source models, particularly  GPT-4o  with average of 8.72, consistently outperformed the open-source models across most scenarios. As for the open-source models,  Llama3.1-405B  and  Deepseek-v2  show strong performance with averages of 7.88 and 7.86, respectively. These two models stand out within the open-source category, though still trailing behind the top closed-source models."
        ]
    },
    "id_table_3": {
        "caption": "",
        "table": "A2.F22.2",
        "footnotes": [],
        "references": [
            "Following  (Zheng et al.,  2023 ; Fu et al.,  2023 ; Liu et al.,  2023 ) , we utilize LLM-as-a-Judge and prompt the judge to evaluate chatbot responses to benchmark questions. The judge takes in chat history, retrieved context, and current turn question and response as inputs and provide a point-wise score to model response for each turn. We devise evaluation criteria for judge prompts, inspired by  (Fu et al.,  2023 ) . Each criterion is accompanied by tailored instructions to guide the LLMs evaluation. For Retrieval Synthesis, we assess Consistency, Informativeness, and Coherence, while for Retrieval Reasoning, we evaluate Accuracy, Consistency, and Coherence. These specific instructions enable the LLM to conduct more precise assessments of each aspect. We implement reference-guided judges  Zheng et al. ( 2023 )  with audited reference answers (Section  3.3.5 ) for each evaluated turn in News TLDR, Education, Academic Writing, Customer Support, and Finance scenarios. For the Travel Planning scenario, we apply this approach only to the last turn. We utilize chain-of-thought to prompt the judge to generate analysis based on the criteria and the reference answer prior to generating the final point-wise score. For further details of the judge prompts, see Appendix  A .",
            "With the collected source documents, candidates of three-turn questions for each scenarios are generated by a question generator as realized by an LLM. Output of the generator for News TLDR, Education, Finance, and Customer Support scenarios for each turn includes a question and a search query. The search queries are used for retrieving relevant context as discussed in Section  3.3.3 . As to Academic Writing and Travel Planning scenarios, outputs of the generator include only the questions. We craft step-by-step guidance as prompts to the generator for aligning the generated questions with the evaluated abilities. See Appendix  A  for details of the guidance and the prompts. We used multiple LLMs (BreeXe-8x7B, Llama3-70B, and Mixtral-8x22B) as the generator and varied the generation temperature for generating a diverse set of candidates.",
            "To study whether industry chat benchmark is sufficient to represent the performance of LLMs in applications requiring augmented generations, we compare the evaluation results of models in the benchmark to Elo scores of models from Chatbot Arena, an industry benchmark for assessing LLMs chat capability  (Chiang et al.,  2024 )  through anonymous human evaluations. We include models appearing in the Chatbot Arena for comparison. Results in Figure  3  shows that RAD-Bench is discriminative. Models exhibiting similar level of chat capability, such as  GPT-4o  vs  Llama3.1-405B ;  Llama3.1-70B  vs  Deepseek-v2 ;  Llama3.1-8B  vs  Mistral-Large , do not perform equally well when the models are applied to scenarios with dialogues from retrieval."
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "A2.F23.2",
        "footnotes": [],
        "references": [
            "We employ an LLM as a scorer to assist the filtering of question candidates. For each scenario, we design customized prompts following scoring criteria to score each candidate. The criteria include Relevance, Progression, Clarity, Support, Knowledge Points, and Medium Complexity as shown in Figure  14 . The Support and Knowledge criteria prompt the scoring LLMs to examine whether the retrieved context from web search and RAG services contains relevant information for answering candidate questions. We scored candidates with BreeXe-8x7B, Llama3-70B, and Mixtral-8x22B. After conducting a human review of a subsampled set of scored candidates, we selected the scoring results from BreeXe-8x7B due to its preferable alignment with the established criteria. With the scored candidates of three-turn questions for each scenario, we then filtered out the top candidates and manually verified that the retrieved contexts contain informative and relevant information for answering the questions in each turn."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "A2.F24.2",
        "footnotes": [],
        "references": [
            "Following  (Zheng et al.,  2023 ; Fu et al.,  2023 ; Liu et al.,  2023 ) , we utilize LLM-as-a-Judge and prompt the judge to evaluate chatbot responses to benchmark questions. The judge takes in chat history, retrieved context, and current turn question and response as inputs and provide a point-wise score to model response for each turn. We devise evaluation criteria for judge prompts, inspired by  (Fu et al.,  2023 ) . Each criterion is accompanied by tailored instructions to guide the LLMs evaluation. For Retrieval Synthesis, we assess Consistency, Informativeness, and Coherence, while for Retrieval Reasoning, we evaluate Accuracy, Consistency, and Coherence. These specific instructions enable the LLM to conduct more precise assessments of each aspect. We implement reference-guided judges  Zheng et al. ( 2023 )  with audited reference answers (Section  3.3.5 ) for each evaluated turn in News TLDR, Education, Academic Writing, Customer Support, and Finance scenarios. For the Travel Planning scenario, we apply this approach only to the last turn. We utilize chain-of-thought to prompt the judge to generate analysis based on the criteria and the reference answer prior to generating the final point-wise score. For further details of the judge prompts, see Appendix  A .",
            "To construct benchmark questions with auditable reference answers, we propose a data generation pipeline (Figure  5 ) that generates questions synthetically. This process involves deconstructing the knowledge points of an article into multiple-turn questions for Retrieval Synthesis and breaking down the joint conditions of solved tasks into multiple-turn questions for Retrieval Reasoning. We leverage LLMs both as question generators to create a pool of synthetic candidates and as question scorers to select the most suitable synthetic candidates for multi-turn dialogues from the retrievals."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "A2.F25.2",
        "footnotes": [],
        "references": [
            "Figure  6  illustrates the performance distribution of various model series across different scenarios as the model scale changes. It is evident that as the model size increases, there is a notable improvement in reasoning capabilities, with the most significant growth observed in the Travel Planning scenario. This observation aligns with findings of  Bai et al. ( 2024 )  and  Mondorf and Plank ( 2024 ) , which emphasize that as model scale increases, the models ability to reason, employ strategies, and interact becomes more pronounced."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "A2.F26.2",
        "footnotes": [],
        "references": [
            "To investigate the model performance across turns for different evaluated abilities, we calculate the average score for each dialogue turn as shown in Figure  7 . In Retrieval Synthesis, the average performance of models tends to improve at the second turn and declines at the third turn. Upon inspecting the evaluation by the judge, we observe that the score increase can be attributed to models following the instructions in second turn question to expand the breadth or deepen the depth of the augmented generation; the decline in the third turn is due to the challenge in synthesizing drafted article and paragraphs from the previous turns with the retrieved context provided in the third turn. As to Retrieval Reasoning, average scores of models notably decrease as turn number increases. We analyzed the model responses and conclude that models tend to fail to comply with the new conditions or constraints as provided in the second and third turn questions, resulting in failures to answer Retrieval Reasoning questions. We hypothesize that the chain-of-density framework  (Adams et al.,  2023 ) , due to its effectiveness in progressively extracting and synthesizing knowledge points, can enhance model performance in Retrieval Synthesis scenarios; that the self-discovery framework  (Zhou et al.,  2024 ) with its structured approach to reasoning strategiescan improve reasoning in the context of newly presented conditions during multi-turn questions following retrieval in Retrieval Reasoning. Investigations into these hypotheses are left for future work."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "A2.F27.2",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": [
        ""
    ]
}