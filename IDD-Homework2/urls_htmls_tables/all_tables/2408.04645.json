{
    "id_table_1": {
        "caption": "Table 1:  Hyperparameters selected for Fine-Tuning the LLaMA-2 model (13B parameters).",
        "table": "S2.T1.1",
        "footnotes": [],
        "references": [
            "One common extension for LLMs is to enrich the input to the system using external background data that relates to the given input and could potentially help in generating a response which is called Retrieval-Augmented Generation (RAG)  (Lewis et al.,  2020 ) .  The main idea of RAG, visualized in Fig.  1 , is to retrieve such related information from a provided database. The retrieved data is added to the prompt as additional context.  Lewis et al. ( 2020 )  have shown that a model, that is supported with RAG, can outperform state-of-the-art approaches in open-domain Question Answering. It was also shown that such a model can approach the efficacy of systems with domain-specific architectures in Fact Verification.",
            "For RAG, the AI Tutor application uses a PostgreSQL Database equipped with the PGVector plugin. This database configuration allows for the storage of original text chunks alongside their respective embedding vectors in a distinct vector column  4 4 4 PGVector:  https://github.com/pgvector/pgvector  (Accessed: 28.02.2024) . The embedding vectors are derived using an OpenAI embedding model  5 5 5 OpenAIEmbedding:  https://platform.openai.com/docs/guides/embeddings  (Accessed: 28.02.2024) . The vector storage can be used to retrieve useful information during the query stage. The process is illustrated above in Fig.  1 , where a search for a users query is started within the vector storage, retrieving the  k k k italic_k  most similar chunks for the given query (we experimented with different numbers of chunks and found good results with three chunks). These chunks are integrated as context into the prompt given to the Large Language Model (LLM).",
            "As one further step, LLaMA-2 was used in a fine-tuned version. The model was hosted and trained locally (we used a workstation equipped with a NVIDIA RTX A6000 with 48 GB of RAM on the graphics card). Therefore, we decided to use the mid-size LLaMA for all our experiments (with 13 billion parameters) as this still allowed to train and fine-tune the model when using quantization and LoRA  (Hu et al.,  2021 ) . For further details on training, see table  1 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Detailed overview of metrics for the different GPT-3.5 configurations, shown as a comparison between a full RAG including a system message as a prompt and simply using the question as an input. Best value is highlighted by green shading.",
        "table": "A1.T2.26.26",
        "footnotes": [],
        "references": [
            "As a first research question, we are interested in how well do different models perform and how do the different extension techniques affect the performance. Therefore, we evaluated these models on our test dataset. The different metrics compare the provided ground truth answer from the test set with the answer given by the model on the particular question (for detailed results see Appendix  A , table  2  and table  3 ). Given are the BLEU-4 and ROUGE similarity scores in Fig.  3 . As a first observation, for both generic LLM models (indicated by colors: GPT-3.5 in blue, LLaMA-2 in orange) extending the prompt always improved the response considerably.  Just adding a simple system message that instructs the model to act as a tutoring system showed a considerable improvement (Fig.  3 , lower part, comparison of GPT-3.5 with GPT-3.5 (System Message) in which a general message on the role and topic was provided to the model).  Retrieval-Augmented-Generation and prompt engineering show a further improvement and quite large positive effect."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Detailed overview of metrics for the different LLaMA-2 (13B parameters) configurations. Shown is the comparison: First, between the base model and fine-tuned versions. Further-more, for all models we compare directly asking the model the question or enriching the input using RAG and a system message. In case of the fine-tuned versions, we further filtered the output.",
        "table": "A1.T3.78.78",
        "footnotes": [],
        "references": [
            "As a first research question, we are interested in how well do different models perform and how do the different extension techniques affect the performance. Therefore, we evaluated these models on our test dataset. The different metrics compare the provided ground truth answer from the test set with the answer given by the model on the particular question (for detailed results see Appendix  A , table  2  and table  3 ). Given are the BLEU-4 and ROUGE similarity scores in Fig.  3 . As a first observation, for both generic LLM models (indicated by colors: GPT-3.5 in blue, LLaMA-2 in orange) extending the prompt always improved the response considerably.  Just adding a simple system message that instructs the model to act as a tutoring system showed a considerable improvement (Fig.  3 , lower part, comparison of GPT-3.5 with GPT-3.5 (System Message) in which a general message on the role and topic was provided to the model).  Retrieval-Augmented-Generation and prompt engineering show a further improvement and quite large positive effect."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Example pair of question together with the Ground Truth answer.",
        "table": "A4.T4.1",
        "footnotes": [],
        "references": [
            "For the METEOR score most models perform almost equally good (not shown in detail). Except for the base model with raw data and question only, which performs slightly worse. In BERTScore, we, first, observe as a similar pattern that applying RAG and using a specific prompt dramatically improves the measured similarity (Fig.  4 , comparing inside the specific modelsLLaMA-2 respectively GPT-3.5the versions with RAG and without RAG). Secondly, again similar reflecting the results from above, fine-tuning (showing in green) introduces a strong advantage compared to the original model (shown in orange). Again, integrating RAG after fine-tuning appears problematic. What is noteworthy from the BERTScore evaluation is that numbers appear overall more pronounced.",
            "Despite the limited scope of the human evaluation, some general trends were observed. Checkpoint  128000 128000 128000 128000  yielded the best results for the Fine-tuned LLaMA model configuration when used without applying RAG. In contrast, already checkpoint  2000 2000 2000 2000  performed best for the model when used with RAG (Fine-tuned LLaMA + RAG).  Example answers to a sample question are presented in the appendix (table  4  shows the ground truth and the input question; Table  5  compares the fine-tuned models response without using RAG for the two different checkpoints, i.e., early and late in training; table  6  compares the fine-tuned models response when additionally integrating RAG for the two different checkpoints). The performance of the checkpoints deteriorates when the configurations are switched."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Comparison of responses from different fine-tuned LLaMA-2 versions (after  2000 2000 2000 2000  and  128000 128000 128000 128000  steps), without using RAG.",
        "table": "A4.T5.5",
        "footnotes": [],
        "references": [
            "We observed (Fig.  5 ) that the traditionally computed similarity metrics as BLEU, ROUGE, and BERTScore (to a slightly lesser extend) are highly correlated. Interestingly, all these metrics showed a negative correlation with the token count. In our case, we found the assumption confirmed that shorter sentences are preferred by these metrics.  METEOR also showed a positivebut smallercorrelation with the metrics above. It appeared nearly uncorrelated with the token counter (only a very small negative correlation). GPTSimilarity (results are shown in the appendix, Fig.  7 ), as a measurement that prompts an LLM to evaluate similarity, did not show any large correlation with the more traditional similarity metrics, but surprisingly has a positive correlation with the token count, meaning in our evaluation longer answers were usually favored (this should be further analyzed).",
            "As a first observation, when adding RAG into any model (GPT-3.5 or LLaMA-2) this increased trustworthiness in the model (results are not shown in detail). This is to be expected as base modelswithout any extensionare not capable of providing references to back up their answers. Adding references appears to have a direct positive impact on trustworthiness.  For helpfulness we couldnt find a clear trend (e.g., GPT-3.5 already proves as a strong baseline in this respect which gives always answers that relate to the question). Overall, we are interested in how the human evaluation correlates with the computed metrics (Fig.  5 ). There is a good correlation of trustworthiness with BLEU, Rouge, and BERTScore (as a cautious reminder: the human evaluation is currently only restricted to feedback by two teaching assistants). Trustworthiness is also negatively correlated with the token count. This appears reasonable as the scale of trustworthiness rewards answers that are backed up by argument. In contrast, well known facts that might be known from training data are evaluated as neutral. There is no strong correlation for helpfulness with respect to the other metrics, only a small correlation with BERTScore. Trustworthiness and Helpfulness appear correlated.",
            "Despite the limited scope of the human evaluation, some general trends were observed. Checkpoint  128000 128000 128000 128000  yielded the best results for the Fine-tuned LLaMA model configuration when used without applying RAG. In contrast, already checkpoint  2000 2000 2000 2000  performed best for the model when used with RAG (Fine-tuned LLaMA + RAG).  Example answers to a sample question are presented in the appendix (table  4  shows the ground truth and the input question; Table  5  compares the fine-tuned models response without using RAG for the two different checkpoints, i.e., early and late in training; table  6  compares the fine-tuned models response when additionally integrating RAG for the two different checkpoints). The performance of the checkpoints deteriorates when the configurations are switched."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Comparison of responses from different fine-tuned LLaMA-2 versions (after  2000 2000 2000 2000  and  128000 128000 128000 128000  steps) when enriching the input using RAG and prompt engineering.",
        "table": "A4.T6.5",
        "footnotes": [],
        "references": [
            "Despite the limited scope of the human evaluation, some general trends were observed. Checkpoint  128000 128000 128000 128000  yielded the best results for the Fine-tuned LLaMA model configuration when used without applying RAG. In contrast, already checkpoint  2000 2000 2000 2000  performed best for the model when used with RAG (Fine-tuned LLaMA + RAG).  Example answers to a sample question are presented in the appendix (table  4  shows the ground truth and the input question; Table  5  compares the fine-tuned models response without using RAG for the two different checkpoints, i.e., early and late in training; table  6  compares the fine-tuned models response when additionally integrating RAG for the two different checkpoints). The performance of the checkpoints deteriorates when the configurations are switched.",
            "Secondly, we computed the different metrics for these two type of settings throughout the training process.  Interestingly, there was a strong correspondence between the human assessments and the performance metrics at specific training points (see Fig.  6 ). For the plain LLaMA-2 model without RAG, the scores (e.g., BLEU-4 and ROUGE, shown in blue in Fig.  6 ) improved consistently over time and continued to improve late in training. In contrast, with RAG integration (shown in orange), fine-tuning peaked early and then dropped considerably. The metrics indicate an optimal performance around  4000 4000 4000 4000  training steps, after which the systems performance deteriorated drastically. This aligns well with the human assessment, which chose the models responses after  2000 2000 2000 2000  training steps as optimal which is quite close and for which the model performed on a similar level when considering the metrics."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Human evaluation of the different LLM versions (evaluation was conducted by only two teaching assistants who assessed a randomly selected subset of the evaluation set). Each sentence of an answer was presented to human labelers together with context. The labelers were tasked to rate this along two dimensions: Trust and Helpfulness. Each dimension was given on a five step scale. Trust: Nonsense  False statement  General knowledge  Partially proven and shown  Proven (with proof / reference in answer); Helpfulness: Not helpful  Repetition  Unclear  Limited extent helpful  Helpful.",
        "table": "A5.T7.1.1",
        "footnotes": [],
        "references": [
            "It is currently assumed that such metrics by itself are not sufficient to analyze LLMs  (Maynez et al.,  2020 ) . Therefore, human evaluations are still used as a gold standard for assessing the capabilities. As a first step, we setup a web-based framework that present a question together with an answer to a user. The user is tasked with judging how helpful and how correct the given answer is. There is additional information provided for each question (the original slide for which we generated the question, plus a context of neighboring slides). This human evaluation has only been tested with two teaching assistants for the given robotics course which can be considered domain experts (detailed results are shown in the Appendix in table  7 ). We will run this evaluation in the upcoming semester inside the robotics course.",
            "We observed (Fig.  5 ) that the traditionally computed similarity metrics as BLEU, ROUGE, and BERTScore (to a slightly lesser extend) are highly correlated. Interestingly, all these metrics showed a negative correlation with the token count. In our case, we found the assumption confirmed that shorter sentences are preferred by these metrics.  METEOR also showed a positivebut smallercorrelation with the metrics above. It appeared nearly uncorrelated with the token counter (only a very small negative correlation). GPTSimilarity (results are shown in the appendix, Fig.  7 ), as a measurement that prompts an LLM to evaluate similarity, did not show any large correlation with the more traditional similarity metrics, but surprisingly has a positive correlation with the token count, meaning in our evaluation longer answers were usually favored (this should be further analyzed)."
        ]
    },
    "global_footnotes": [
        "You are now a lecture assistant that helps students to understand the lecture.  You are an expert about the lecture topic autonomic systems and mobile robots.  You will always bring the answer in the context of the lecture.",
        "You are a helpful, respectful, and honest teaching assistant for lecture material that will be provided to you.  Your current task is to answer student queries. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.  Please ensure that your responses are socially unbiased and positive in nature. If a question does not make sense,  or is not factually coherent, explain why instead of answering something incorrect.  If you dont know the answer to a question, please do not share false information. You only have access to the lecture material provided to you.  Lecture material includes slides, images, and transcripts of videos.  The only thing you can do is write text, nothing else! Never, ever repeat yourself.",
        "",
        "Vector storages:",
        "(Accessed: 28.02.2024)",
        "PGVector:",
        "(Accessed: 28.02.2024)",
        "OpenAIEmbedding:",
        "(Accessed: 28.02.2024)",
        "https://toloka.ai/blog/perplexity-case/"
    ]
}