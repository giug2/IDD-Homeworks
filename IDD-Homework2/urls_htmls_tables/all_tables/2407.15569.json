{
    "id_table_1": {
        "caption": "Table 1:  Evaluation in EM score",
        "table": "S4.T1.1",
        "footnotes": [],
        "references": [
            "The RAFT method combines retrieval augmented generation and supervised fine-tuning, as well as incorporating the idea of chain-of-thought. This is akin to training the model to compute results from relevant information before taking an exam. Consequently, during an open-book exam, the model can deduce correct answers more quickly and accurately using the reference materials.  In summary, the RAFT method has two key features.  First, in addition to the oracle documents, irrelevant distractor documents are also included in the reference documents to improve models robustness against irrelevant information retrieved during the retrieval process. Second, chain-of-thought style responses are used as the target text in the fine-tuning dataset rather than plain short answers to improve models reasoning capability. To be specific, each data in RAFT dataset contains a question ( Q Q Q italic_Q ), several distractor documents ( D k subscript D k D_{k} italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ), a oracle document containing the effective information to answer the question ( D  superscript D D^{*} italic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ), and a chain-of-thought style response ( A  superscript A A^{*} italic_A start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) generated from the oracle document ( D  superscript D D^{*} italic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ). Figure  1  shows overview of RAFT.",
            "We compared the performance of the models using the RAFT method and the baselines. Table  1  and Table  2  show the results for the EM score and F1 score respectively. In the HotpotQA [Oracle]  experiment group, only oracle documents were provided as references for the model in the RAG experiments. For all other groups, distractor documents were included alongside the reference documents in the RAG experiments."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Evaluation in F1 score",
        "table": "S4.T2.2",
        "footnotes": [],
        "references": [
            "Figure  2  shows our RAFT fine-tuning dataset construction process.  In order to make the datasets tailored to RAFT fine-tuning, we use two methods to process open-source datasets. When dealing with a dataset where a question corresponds to several reference documents (include oracle documents and distractor documents), we use the first method: For each question, we extract all the oracle documents from the questions corresponding documents, then randomly select a specified number of documents from the remaining corresponding documents as the distractor documents. When dealing with a dataset where a question corresponds to only one oracle document, we use the second method: For each question, we take its corresponding document as oracle document and randomly select a specified number of documents from  other  questions reference documents as the questions distractor documents. In this study, the dataset HotpotQA  [ 16 ]  was processed using the first method, while the datasets like PubMedQA  [ 17 ]  and DuReader_robust  [ 18 ]  were processed using the second method. In our RAFT experiments, we use four distractor documents for each question.",
            "We compared the performance of the models using the RAFT method and the baselines. Table  1  and Table  2  show the results for the EM score and F1 score respectively. In the HotpotQA [Oracle]  experiment group, only oracle documents were provided as references for the model in the RAG experiments. For all other groups, distractor documents were included alongside the reference documents in the RAG experiments.",
            "Since the yes/no QA of PubMedQA and QA of HotpotQA are both short-form, we also assessed the long-form QA in dataset PubMedQA. The experiment results are shown in Table  2  under the PubMedQA [long]  group. The results in F1 score of long-form QA indicate that RAFT method brought about a 13% performance improvement for long-answer questions over zero-shot prompting baseline. However, compared to the DSF+RAG baseline, the performance gain was less prominent than for the short-form QA. This is because the content of long answers is more focused on induction and summarization, rather than definitive results derived from reasoning, as is common with short answers. The study of long-form QA with chain-of-thought needs further exploration.",
            "We also conducted evaluation on DuReader_robust to assess the effectiveness of the RAFT method on the Chinese datasets. Since the questions in this dataset heavily rely on information from reference documents, the gain brought by the use of DSF is only 7.43% over the zero-shot prompting baseline (in Table  2  comparing the zero-shot and DSF+zero-shot rows in the DuReader group). In this case, the use of RAG to supplement reference documents with the question is more effective, which obtains a 12.59% performance gain over the zero-shot baseline. After RAFT fine-tuning, the models ability to extract and process information, as well as its reasoning capability can be significantly improved. It achieves 44.34% and 19.9% performance gain in F1 score over zero-shot prompting baseline and DSF+RAG baseline respectively. These results demonstrate that the RAFT method performs exceptionally well on both English and Chinese datasets."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Performance gains across different types of reasoning tasks by RAFT",
        "table": "S4.T3.3",
        "footnotes": [],
        "references": [
            "After selecting the oracle and distractor documents, we used GPT-3.5 with to generate chain-of-thought style response. We require the model to generate a chain-of-thought reasoning process based on the input question as well as its corresponding oracle documents. During this reasoning process, the model are prompted to cite the referenced content from the oracle documents and provide a final answer separately at the end. Figure  3  and figure  4  show our CoT answer generation process via GPT-3.5 in Chinese and English respectively.",
            "We evaluated the RAFT method separately on bridge-type QA and comparison-type QA in HotpotQA dataset, as shown in Table  3 . The results indicate that RAFT performs better on comparison-type questions. This is likely because comparison-type questions typically involve comparing features between two or more entities, which can rely on direct information retrieval and simple comparison operations. In contrast, bridge-type questions often require the model to extract relevant information from multiple documents, involving longer reasoning chains and multiple intermediate steps so it demands a higher level of understanding and reasoning ability from the model."
        ]
    },
    "global_footnotes": []
}