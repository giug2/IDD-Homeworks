{
    "S3.T1.5": {
        "caption": "AIC for different GAMM fittings for  . A smaller  AIC indicates better performance (n=45886)",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T1.5\">\n<tr class=\"ltx_tr\" id=\"S3.T1.5.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"S3.T1.5.1.2\"><span class=\"ltx_text\" id=\"S3.T1.5.1.2.1\" style=\"font-size:90%;\">factor (contribution)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.5.1.1\">\n<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.5.1.1.m1.1\"><semantics id=\"S3.T1.5.1.1.m1.1a\"><mi id=\"S3.T1.5.1.1.m1.1.1\" mathsize=\"90%\" mathvariant=\"normal\" xref=\"S3.T1.5.1.1.m1.1.1.cmml\">&#916;</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.5.1.1.m1.1b\"><ci id=\"S3.T1.5.1.1.m1.1.1.cmml\" xref=\"S3.T1.5.1.1.m1.1.1\">&#916;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.5.1.1.m1.1c\">\\Delta</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.5.1.1.m1.1d\">roman_&#916;</annotation></semantics></math><span class=\"ltx_text\" id=\"S3.T1.5.1.1.1\" style=\"font-size:90%;\">AIC</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.5.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" id=\"S3.T1.5.2.1\"><span class=\"ltx_text\" id=\"S3.T1.5.2.1.1\" style=\"font-size:90%;\">(m1- base model) contribution of textual similarity</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T1.5.2.2\"><span class=\"ltx_text\" id=\"S3.T1.5.2.2.1\" style=\"font-size:90%;\">848.87</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.5.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"S3.T1.5.3.1\"><span class=\"ltx_text\" id=\"S3.T1.5.3.1.1\" style=\"font-size:90%;\">(m2- base model) contribution of ML_eval</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.5.3.2\"><span class=\"ltx_text\" id=\"S3.T1.5.3.2.1\" style=\"font-size:90%;\">8785.82</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" id=\"S3.T1.5.4.1\"><span class=\"ltx_text\" id=\"S3.T1.5.4.1.1\" style=\"font-size:90%;\">(m3-base model) contribution of hter</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S3.T1.5.4.2\"><span class=\"ltx_text\" id=\"S3.T1.5.4.2.1\" style=\"font-size:90%;\">586.3</span></td>\n</tr>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "\nWe established three types of GAMM fittings for \nMLQE-PE\n, where “human score (mean)” is the independent variable, and other factors include “ML_eval”, “textual similarity”, “sd of human scores”. “human evaluator number”, “language pairs (langs)” are incorporated as random variables. We chose “human score (mean)” over “human score (z-mean)” as the independent variable primarily because “human score (mean)” closely follows a normal distribution. The GAMM setups are detailed below, and the results are presented in Table \n\n1\n\n.\n\n",
            "\nThe base model demonstrates the best performance when all factors are included. However, each of the remaining models includes only a subset of these factors. The Akaike Information Criterion (AIC) is used to represent the performance of this GAMM fitting. The base model has the lowest AIC. When the AIC of model \n\n\n\nm\n⁢\n1\n\n\n\n\n𝑚\n1\n\n\nm1\nitalic_m 1\n\n\n is subtracted from the AIC of the base model, the resulting \n\n\nΔ\n\nΔ\n\n\\Delta\nroman_Δ\n\n\nAIC indicates the contribution of “textual similarity”, as “m1” does not include “textual similarity” compared to the base model. A smaller \n\n\nΔ\n\nΔ\n\n\\Delta\nroman_Δ\n\n\nAIC also indicates better performance and greater contribution for a given factor. The results are shown in Table \n\n1\n\n.\n\n"
        ]
    },
    "S3.T2.5": {
        "caption": "AIC for different GAMM fittings for  . A smaller  AIC indicates better performance (n=14706)",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T2.5\">\n<tr class=\"ltx_tr\" id=\"S3.T2.5.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"S3.T2.5.1.2\"><span class=\"ltx_text\" id=\"S3.T2.5.1.2.1\" style=\"font-size:90%;\">factor (contribution)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T2.5.1.1\">\n<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.5.1.1.m1.1\"><semantics id=\"S3.T2.5.1.1.m1.1a\"><mi id=\"S3.T2.5.1.1.m1.1.1\" mathsize=\"90%\" mathvariant=\"normal\" xref=\"S3.T2.5.1.1.m1.1.1.cmml\">&#916;</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.T2.5.1.1.m1.1b\"><ci id=\"S3.T2.5.1.1.m1.1.1.cmml\" xref=\"S3.T2.5.1.1.m1.1.1\">&#916;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T2.5.1.1.m1.1c\">\\Delta</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T2.5.1.1.m1.1d\">roman_&#916;</annotation></semantics></math><span class=\"ltx_text\" id=\"S3.T2.5.1.1.1\" style=\"font-size:90%;\">AIC</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.5.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" id=\"S3.T2.5.2.1\"><span class=\"ltx_text\" id=\"S3.T2.5.2.1.1\" style=\"font-size:90%;\">(t1- base model) contribution of textual similarity</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T2.5.2.2\"><span class=\"ltx_text\" id=\"S3.T2.5.2.2.1\" style=\"font-size:90%;\">237.11</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.5.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"S3.T2.5.3.1\"><span class=\"ltx_text\" id=\"S3.T2.5.3.1.1\" style=\"font-size:90%;\">(t2- base model) contribution of trigram sentence probability</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T2.5.3.2\"><span class=\"ltx_text\" id=\"S3.T2.5.3.2.1\" style=\"font-size:90%;\">232.16</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" id=\"S3.T2.5.4.1\"><span class=\"ltx_text\" id=\"S3.T2.5.4.1.1\" style=\"font-size:90%;\">(t3-base model) contribution of hter</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S3.T2.5.4.2\"><span class=\"ltx_text\" id=\"S3.T2.5.4.2.1\" style=\"font-size:90%;\">424.29</span></td>\n</tr>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "\nNext, we used similar GAMM methods to explore how different metrics affect human scores for \nPreQuEL\n. In the \nPreQuEL\n dataset, “human score (z_mean)” is the dependent variable \n\n2\n\n22In this dataset, “human score (mean)” is not available.\n\n\n, and other factors include “n-gram sentence probability”, “language model score”, “hter”, and “textual similarity”. In this dataset, “language model score” (“lm_score”) has four values, so we treat it as a random factor, and “language pairs” (“langs”) are incorporated as a random variable. There are five types of “n-gram sentence probability”; however, we chose the optimal one, “trigram sentence probability”,\"for the GAMM fittings. The GAMM setups are detailed below, and the results are presented in Table \n\n2\n\n.\n\n"
        ]
    }
}