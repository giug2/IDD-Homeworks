{
    "S3.T1.5": {
        "caption": "AIC for different GAMM fittings for  . A smaller  AIC indicates better performance (n=45886)",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T1.5\">\n<tr class=\"ltx_tr\" id=\"S3.T1.5.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"S3.T1.5.1.2\"><span class=\"ltx_text\" id=\"S3.T1.5.1.2.1\" style=\"font-size:90%;\">factor (contribution)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.5.1.1\">\n<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.5.1.1.m1.1\"><semantics id=\"S3.T1.5.1.1.m1.1a\"><mi id=\"S3.T1.5.1.1.m1.1.1\" mathsize=\"90%\" mathvariant=\"normal\" xref=\"S3.T1.5.1.1.m1.1.1.cmml\">&#916;</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.5.1.1.m1.1b\"><ci id=\"S3.T1.5.1.1.m1.1.1.cmml\" xref=\"S3.T1.5.1.1.m1.1.1\">&#916;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.5.1.1.m1.1c\">\\Delta</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T1.5.1.1.m1.1d\">roman_&#916;</annotation></semantics></math><span class=\"ltx_text\" id=\"S3.T1.5.1.1.1\" style=\"font-size:90%;\">AIC</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.5.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" id=\"S3.T1.5.2.1\"><span class=\"ltx_text\" id=\"S3.T1.5.2.1.1\" style=\"font-size:90%;\">(m1- base model) contribution of textual similarity</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T1.5.2.2\"><span class=\"ltx_text\" id=\"S3.T1.5.2.2.1\" style=\"font-size:90%;\">848.87</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.5.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"S3.T1.5.3.1\"><span class=\"ltx_text\" id=\"S3.T1.5.3.1.1\" style=\"font-size:90%;\">(m2- base model) contribution of ML_eval</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.5.3.2\"><span class=\"ltx_text\" id=\"S3.T1.5.3.2.1\" style=\"font-size:90%;\">8785.82</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" id=\"S3.T1.5.4.1\"><span class=\"ltx_text\" id=\"S3.T1.5.4.1.1\" style=\"font-size:90%;\">(m3-base model) contribution of hter</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S3.T1.5.4.2\"><span class=\"ltx_text\" id=\"S3.T1.5.4.2.1\" style=\"font-size:90%;\">586.3</span></td>\n</tr>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "\nWe established three types of GAMM fittings for \nMLQE-PE\n, where â€œhuman score (mean)â€ is the independent variable, and other factors include â€œML_evalâ€, â€œtextual similarityâ€, â€œsd of human scoresâ€. â€œhuman evaluator numberâ€, â€œlanguage pairs (langs)â€ are incorporated as random variables. We chose â€œhuman score (mean)â€ over â€œhuman score (z-mean)â€ as the independent variable primarily because â€œhuman score (mean)â€ closely follows a normal distribution. The GAMM setups are detailed below, and the results are presented in Table \n\n1\n\n.\n\n",
            "\nThe base model demonstrates the best performance when all factors are included. However, each of the remaining models includes only a subset of these factors. The Akaike Information Criterion (AIC) is used to represent the performance of this GAMM fitting. The base model has the lowest AIC. When the AIC of model \n\n\n\nm\nâ¢\n1\n\n\n\n\nğ‘š\n1\n\n\nm1\nitalic_m 1\n\n\n is subtracted from the AIC of the base model, the resulting \n\n\nÎ”\n\nÎ”\n\n\\Delta\nroman_Î”\n\n\nAIC indicates the contribution of â€œtextual similarityâ€, as â€œm1â€ does not include â€œtextual similarityâ€ compared to the base model. A smaller \n\n\nÎ”\n\nÎ”\n\n\\Delta\nroman_Î”\n\n\nAIC also indicates better performance and greater contribution for a given factor. The results are shown in Table \n\n1\n\n.\n\n"
        ]
    },
    "S3.T2.5": {
        "caption": "AIC for different GAMM fittings for  . A smaller  AIC indicates better performance (n=14706)",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T2.5\">\n<tr class=\"ltx_tr\" id=\"S3.T2.5.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"S3.T2.5.1.2\"><span class=\"ltx_text\" id=\"S3.T2.5.1.2.1\" style=\"font-size:90%;\">factor (contribution)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T2.5.1.1\">\n<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.5.1.1.m1.1\"><semantics id=\"S3.T2.5.1.1.m1.1a\"><mi id=\"S3.T2.5.1.1.m1.1.1\" mathsize=\"90%\" mathvariant=\"normal\" xref=\"S3.T2.5.1.1.m1.1.1.cmml\">&#916;</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.T2.5.1.1.m1.1b\"><ci id=\"S3.T2.5.1.1.m1.1.1.cmml\" xref=\"S3.T2.5.1.1.m1.1.1\">&#916;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T2.5.1.1.m1.1c\">\\Delta</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T2.5.1.1.m1.1d\">roman_&#916;</annotation></semantics></math><span class=\"ltx_text\" id=\"S3.T2.5.1.1.1\" style=\"font-size:90%;\">AIC</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.5.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" id=\"S3.T2.5.2.1\"><span class=\"ltx_text\" id=\"S3.T2.5.2.1.1\" style=\"font-size:90%;\">(t1- base model) contribution of textual similarity</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S3.T2.5.2.2\"><span class=\"ltx_text\" id=\"S3.T2.5.2.2.1\" style=\"font-size:90%;\">237.11</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.5.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" id=\"S3.T2.5.3.1\"><span class=\"ltx_text\" id=\"S3.T2.5.3.1.1\" style=\"font-size:90%;\">(t2- base model) contribution of trigram sentence probability</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T2.5.3.2\"><span class=\"ltx_text\" id=\"S3.T2.5.3.2.1\" style=\"font-size:90%;\">232.16</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" id=\"S3.T2.5.4.1\"><span class=\"ltx_text\" id=\"S3.T2.5.4.1.1\" style=\"font-size:90%;\">(t3-base model) contribution of hter</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S3.T2.5.4.2\"><span class=\"ltx_text\" id=\"S3.T2.5.4.2.1\" style=\"font-size:90%;\">424.29</span></td>\n</tr>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "\nNext, we used similar GAMM methods to explore how different metrics affect human scores for \nPreQuEL\n. In the \nPreQuEL\n dataset, â€œhuman score (z_mean)â€ is the dependent variable \n\n2\n\n22In this dataset, â€œhuman score (mean)â€ is not available.\n\n\n, and other factors include â€œn-gram sentence probabilityâ€, â€œlanguage model scoreâ€, â€œhterâ€, and â€œtextual similarityâ€. In this dataset, â€œlanguage model scoreâ€ (â€œlm_scoreâ€) has four values, so we treat it as a random factor, and â€œlanguage pairsâ€ (â€œlangsâ€) are incorporated as a random variable. There are five types of â€œn-gram sentence probabilityâ€; however, we chose the optimal one, â€œtrigram sentence probabilityâ€,\"for the GAMM fittings. The GAMM setups are detailed below, and the results are presented in Table \n\n2\n\n.\n\n"
        ]
    }
}