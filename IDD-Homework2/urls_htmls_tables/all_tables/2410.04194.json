{
    "id_table_1": {
        "caption": "Table 1:  Autoformalization results for different settings. BM25 retriever is used to retrieve Top-3 most similar samples for retrieval augmented autoformalization. Greedy decoding is used in generation for reproducibility. Code-based denoising is applied to all outputs. The query used to retrieve relevant exemplars includes: ( T ): natural language textual description; ( ZS ): zero-shot autoformalization result from Mistral. The index used for knowledge base has the following options: ( T ): natural language textual description; ( I ): informalization of formal statement generated from Mistral; ( S ): formal statement. The setting with highest scores is highlighted in  bold .",
        "table": "S3.T1.1",
        "footnotes": [],
        "references": [
            "This work targets the overarching research question: how to systematically support the creation of consistent and coherent formal mathematical libraries from informal mathematical statements?. In order to address this task, we decompose this broader aim into the following research questions:  RQ1: To what extent contemporary LLMs are capable of formalizing specialized mathematical statements into formal representations for mathematical libraries?; RQ2: Which metrics can be used to assess the quality of the formalized outputs?; RQ3: Which mechanisms can be used to extend the autoformalization properties of LLMs to achieve better generative control and enhance terminological, syntactic and semantic consistency and coherence? . To address these research questions, we propose a novel framework (See Figure  1 ) that leverages LLMs with most-similar retrieval augmented generation (MS-RAG), denoising steps and iterative feedback-guided syntax error refinement cycles (Auto-SEF) to deliver a syntactically consistent and semantically coherent autoformalization.",
            "For MS-RAG, BM25  (Robertson et al.,  1994 )  is used as the primary ranking function to retrieve Top-k (k=3) most similar samples for exemplars (BM25 will concentrate a terminological similarity function). Different settings are contrasted for querying and indexing the reference KB. There are two choices for query: 1. natural language textual description; 2. description along with zero-shot autoformalization result from Mistral. The choices for indexing KB elements combine three content sources: 1. natural language textual description; 2. informalization of formal statements; 3. formal statements. For this specific analysis, we constrain the foundation model to Mistral. All results are reported in Table  1 .",
            "MS-RAG can improve autoformalization in mathematical libraries settings.   As shown in Table  1 , for the same type of LLMs, using retrieved examples rather than fixed examples leads to an improvement in both semantic similarity and syntactic correctness of the generated formal statements. This mechanism can lift the performance of smaller models: e.g. as a smaller model, Mistral (7B) with MS-RAG can outperform Mixtral (8  \\times  7B) with standard prompting across all metrics and is comparable to GPT-3.5 (175B) without MS-RAG according to some metrics such as RUBY.",
            "Augmenting the index with auto-informalization or the query with zero-shot auto-formalization does not lead to better retrieval.  Among all results in Table  1 , GPT-3.5 with textual description query and textual description index achieves highest scores in four metrics except BLEU-2. This query and index combination setting is also an optimal choice for Mistral, as this setting leads to highest scores in ChrF, RUBY and CBS and second highest scores in Pass. Incorporating zero-shot results from Mistral as queries generally yields worse results compared to its counterpart. This is probably caused by the low quality of zero-shot formalization results. The application of informalized descriptions during indexing also does not lead to a performance improvement.",
            "The example in Table  4.1  communicates the necessity of denoising. As shown in Table  4.1 , both 3-shot and MS-RAG results include an additional textual description in the final output which does not form a formal statement. PBD 1A changes \\<in> into :: which is another way of expressing   \\in   but this expression is not provided in its prompt, so this behaviour is highly likely to be an inherited bias. PBD 1B and 1C mitigate this behaviour but they also introduce other syntactic errors, such as the missing word of or the special character `.  Only PBD 1D maintains the validity of the formal statement because the similarity of the retrieved examples and are thus emphasized during generation.",
            "Pass and BLEU metrics should be jointly used to prevent evaluation bias.  Some zero-shot results in Table  1  lead to a high score on the Pass metric but lower scores on other metrics due to internal LLM style biases. According to Figure  4 , among metrics for semantic similarity, BLEU-2 has the strongest correlation with the Pass metric and hence can indicate syntactic correctness to some extent. We suggest considering both BLEU scores and Pass rate when comparing results."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  The effect of denoising on Mistral. The change of scores after applying CBD is recorded in round brackets. The setting with highest final scores is marked in  bold .",
        "table": "S4.T2.1",
        "footnotes": [],
        "references": [
            "We mentioned semantic correctness and consistency in Section  2  as the final desirable properties as an outcome of an autoformalization process, which can become too strict for evaluating autoformalization tasks with current LLMs. Therefore, in this work, we propose two distinct proxies to assess code correctness:  semantic similarity  and  syntactic correctness . Utilizing the ground truth as a reference, we measure semantic similarity using pairwise metrics, including BLEU  (Papineni et al.,  2002 ) , ChrF  (Popovic,  2015 ) , RUBY  (Tran et al.,  2019 ) , and CodeBERTScore (CBS)  (Zhou et al.,  2023 ) . The description of these metrics are provided in the  Appendix . For syntactic correctness, we use the Isabelle theorem prover to detect syntax errors in formal statements and use the  Pass  metric which represents the success rate at which the generated formal statement does not exhibit any syntax errors, as verified by the theorem prover. The integration between the transformer and Isabelle is done on a ToolFormer setting with the support of an Isabelle client 3 3 3 https://github.com/inpefess/isabelle-client   (Shminke,  2022 ) .",
            "In this section, we investigate the impact of denoising. We select the result of MS-RAG (Query: T, Index: T) to apply PBD with four prompts: ( 1A ) The prompt only contains instructions to remove explanations and proofs; ( 1B ) 1A adds an additional instruction for  stylistic alignment  to declare that the final output after refinement should maintain the same syntactic style; ( 1C ) Includes some fixed formal statement examples for the stylistic alignment instruction in 1B; ( 1D ) Changes the fixed examples in 1C to retrieved examples from MS-RAG. We record the results of Mistral in Table  2 .",
            "In this section, we mainly focus on answering the question on whether syntactic errors can be corrected by LLMs in coordination with symbolic solvers. This process is iteratively run for up to nine cycles. To better illustrate the changes, we plot the scores of each iteration on the Pass metric in Figure  2 .",
            "Iterative Auto-SEF improves syntactic correctness of the formalization results.  As shown in Figure  2 , both GPT-3.5 and Mistral can receive improvements from the iterative Auto-SEF method. This result demonstrates that Auto-SEF can indeed enable LLMs to fix some syntactic errors. The first iteration brings the largest increase (2.56% for Mistral, 4.38% for GPT-3.5) in pass rate. After that, the change becomes smoother and iterative improvements are limited to a small number of cycles."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  An example using Mistral shows that only MS-RAG and PBD 1D have no syntax errors of formalization.",
        "table": "S4.SS1.1.1",
        "footnotes": [],
        "references": [
            "Smaller LLM tends to trade-off semantic similarity for syntactic correctness when applying Auto-SEF.  We focus on BLEU-2 as a proxy for semantic similarity and illustrate the scores of each iteration in Figure  3 . The BLEU-2 scores for GPT-3.5 remain steady across different iterations, whereas for Mistral, the scores decrease in the first few iterations. Combining this result with the improvement in pass rate, we hypothesize that a trade-off occurs due to the comparatively lower capacity of the model to perform syntactic correction while controlling for semantic drifting during the Auto-SEF prompting."
        ]
    },
    "id_table_4": {
        "caption": "Figure 2:  Pass rate of each iteration with Auto-SEF. Iteration 0 is the start point before applying Auto-SEF.",
        "table": "A3.T4.1",
        "footnotes": [],
        "references": [
            "The example in Table  4.1  communicates the necessity of denoising. As shown in Table  4.1 , both 3-shot and MS-RAG results include an additional textual description in the final output which does not form a formal statement. PBD 1A changes \\<in> into :: which is another way of expressing   \\in   but this expression is not provided in its prompt, so this behaviour is highly likely to be an inherited bias. PBD 1B and 1C mitigate this behaviour but they also introduce other syntactic errors, such as the missing word of or the special character `.  Only PBD 1D maintains the validity of the formal statement because the similarity of the retrieved examples and are thus emphasized during generation.",
            "Evaluation metrics for autoformalization can disagree with each other  (Evtikhiev et al.,  2023 ) . We use all the results under CBD to calculate the Pearson product-moment correlation coefficients across the metrics, illustrating these coefficients with a heatmap in Figure  4 .",
            "Pass and BLEU metrics should be jointly used to prevent evaluation bias.  Some zero-shot results in Table  1  lead to a high score on the Pass metric but lower scores on other metrics due to internal LLM style biases. According to Figure  4 , among metrics for semantic similarity, BLEU-2 has the strongest correlation with the Pass metric and hence can indicate syntactic correctness to some extent. We suggest considering both BLEU scores and Pass rate when comparing results.",
            "We provide prompts for informalization, autoformalization, denoising, and Auto-SEF in Table  4 ,  5 ,  6 ,  7 , respectively."
        ]
    },
    "id_table_5": {
        "caption": "Figure 3:  BLEU-2 scores of each Auto-SEF iteration.",
        "table": "A3.T5.1",
        "footnotes": [],
        "references": [
            "We provide prompts for informalization, autoformalization, denoising, and Auto-SEF in Table  4 ,  5 ,  6 ,  7 , respectively."
        ]
    },
    "id_table_6": {
        "caption": "Figure 4:  Correlation coefficients between metrics.",
        "table": "A3.T6.1.1",
        "footnotes": [],
        "references": [
            "We provide prompts for informalization, autoformalization, denoising, and Auto-SEF in Table  4 ,  5 ,  6 ,  7 , respectively."
        ]
    },
    "id_table_7": {
        "caption": "Table 4:  Prompt for informalization.",
        "table": "A3.T7.1.1",
        "footnotes": [],
        "references": [
            "We provide prompts for informalization, autoformalization, denoising, and Auto-SEF in Table  4 ,  5 ,  6 ,  7 , respectively."
        ]
    },
    "id_table_8": {
        "caption": "Table 5:  Prompt for autoformalization.",
        "table": "A4.T8.1",
        "footnotes": [],
        "references": [
            "We provide the exact number of scores of denoising in Table  8  and Auto-SEF in Table  9 ."
        ]
    },
    "id_table_9": {
        "caption": "Table 6:  Prompts for informalization.",
        "table": "A4.T9.1",
        "footnotes": [],
        "references": [
            "We provide the exact number of scores of denoising in Table  8  and Auto-SEF in Table  9 ."
        ]
    },
    "global_footnotes": [
        "Code and datasets are available at"
    ]
}