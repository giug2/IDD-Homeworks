{
    "id_table_1": {
        "caption": "Table 1:  Overall Win-Rate (%) of different inference-time support. Flash and Pro refer to Gemini-1.5 Flash and Pro respectively. Unlike  SH ,  AG  can work for both MAB and CB. Refer to Section  4.1  for details.",
        "table": "S4.T1.2",
        "footnotes": [
            ""
        ],
        "references": [
            "Developing an LLM agent suited for in-context decision-making tasks also requires designing a robust textualization function   italic- \\phi italic_  that translates histories  H t  subscript superscript H  t H^{\\pi}_{t} italic_H start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  for the LLM to consume. The obvious baseline for   italic- \\phi italic_  is to simply record the  Raw History  ( RH ) from the environments as a list of (context, action, reward) tuples directly as the context. In this representation, the context length of    ( H t  ) italic- superscript subscript H t  \\phi(H_{t}^{\\pi}) italic_ ( italic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT )  grows linearly with  t t t italic_t , and  RH  contains all information. While  RH  is a general textualization function applicable to any task  T T {\\mathcal{T}} caligraphic_T , more advanced task-specific textualization functions may exist and yield better performance. For example,  Krishnamurthy et al. ( 2024 )  proposed a  Summarized History  function ( SH ) that compresses the history while still containing sufficient information for a given task  T T {\\mathcal{T}} caligraphic_T .  RH  and  SH  differ in how past interaction histories are represented to the LLM agent, as shown in Figure  A1 . At time step  t t t italic_t ,  RH  provides a complete list of past interactions as (Time  t  superscript t  t^{\\prime} italic_t start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , Action Name  a t  subscript a superscript t  a_{t^{\\prime}} italic_a start_POSTSUBSCRIPT italic_t start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT end_POSTSUBSCRIPT , Reward  r t  subscript r superscript t  r_{t^{\\prime}} italic_r start_POSTSUBSCRIPT italic_t start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ) for  t  = 0    t superscript t  0  t t^{\\prime}=0\\cdots t italic_t start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = 0  italic_t . In contrast,  SH  provides sufficient statistics of the past interactions. Specifically, under MAB,  SH  utilizes the empirical mean over each arm (i.e.,  E ^  [ r a ] ,  a  A ^ E delimited-[] superscript r a for-all a A \\hat{\\mathbb{E}}[r^{a}],\\forall a\\in{\\mathcal{A}} over^ start_ARG blackboard_E end_ARG [ italic_r start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT ] ,  italic_a  caligraphic_A ), the number of times each arm has been pulled up to time  t t t italic_t ,  N t  ( a ) subscript N t a N_{t}(a) italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_a ) , and the current horizon  t t t italic_t . In this paper, we consider good textualizations to be those that satisfy sufficiency, which we express using the following definition.",
            "The detailed configurations are shown in Appendix  A.1 . For the action space, we explore two different sizes:  K = 5 K 5 K=5 italic_K = 5  for a small action space and  K = 20 K 20 K=20 italic_K = 20  for a large action space. We also differentiate between two types of action descriptions:  Videos , represented as arbitrary two-letter combinations with no semantic meaning such as Video AA, and  Clothes , described using semantically meaningful phrases, such as Supreme Sylvan Sandals. Regarding reward distributions, we evaluate two types:  Bernoulli  and  Gaussian  Bandit. For Bernoulli, the reward  r  { 0 , 1 } r 0 1 r\\in\\{0,1\\} italic_r  { 0 , 1 }  is binary with  r a k  Bernoulli  ( p k ) similar-to superscript r subscript a k Bernoulli subscript p k r^{a_{k}}\\sim\\text{Bernoulli}(p_{k}) italic_r start_POSTSUPERSCRIPT italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  Bernoulli ( italic_p start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) , where  p k subscript p k p_{k} italic_p start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT  is the mean for the  k k k italic_k -th action. Following   Krishnamurthy et al. ( 2024 ) , the best-performing arm has  p k := 0.5 +  min / 2 assign subscript p k 0.5 subscript  2 p_{k}:=0.5+\\Delta_{\\min}/2 italic_p start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT := 0.5 + roman_ start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT / 2 , while the remaining arms have  p k = 0.5   min / 2 subscript p k 0.5 subscript  2 p_{k}=0.5-\\Delta_{\\min}/2 italic_p start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = 0.5 - roman_ start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT / 2 . The parameter   min subscript  \\Delta_{\\min} roman_ start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT  captures the exploration difficulty, with a larger gap (  min = 0.5 subscript  0.5 \\Delta_{\\min}=0.5 roman_ start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT = 0.5 ) indicating easy tasks and smaller gap (  min = 0.2 subscript  0.2 \\Delta_{\\min}=0.2 roman_ start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT = 0.2 ) representing hard tasks. For the Gaussian bandit, the rewards are continuous with  r a k  N  (  k ,  ) similar-to superscript r subscript a k N subscript  k  r^{a_{k}}\\sim{\\mathcal{N}}(\\mu_{k},\\sigma) italic_r start_POSTSUPERSCRIPT italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  caligraphic_N ( italic_ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_ ) . Here   k  N  ( 0 ,  ) similar-to subscript  k N 0  \\mu_{k}\\sim{\\mathcal{N}}(0,\\sigma) italic_ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT  caligraphic_N ( 0 , italic_ )  represents the mean for each action, and the variance    \\sigma italic_  captures the difficulty of exploration. Following  Sutton ( 2018 ) , we study both   = 1  1 \\sigma=1 italic_ = 1  and   = 3  3 \\sigma=3 italic_ = 3 .",
            "Motivated by the existence of optimal algorithms for bandits, we aim to leverage these algorithms to improve LLMs for exploration by: 1) incorporating algorithmic guidance during inference (Section  4.1 ), 2) teaching optimal exploration through algorithm distillation (Section  4.2 ). We show that smaller models trained using algorithm distillation can even outperform larger models, offering a promising way to efficiently explore with lower inference costs.",
            "In this section, we empirically evaluate LLMs in-context exploration capabilities, using BanditBench. We begin with introducing the setup, baselines and metrics in Section  4.3.1 . Following this, in Section  4.3.2 , we analyze the performance of inference-time guided support, in-context few-shot demonstration and oracle behavior fine-tuning across various experimental settings, as well as models of different sizes. Additionally, we perform extensive ablation studies on the impact of task difficulty, textual representation of the oracle trajectories, and inference-training representation alignment.",
            "We evaluate the in-context exploration capabilities of various LLMs, including Gemma-2B, Gemma-9B  (Team et al.,  2024 ) , Gemini 1.5 Flash, and Gemini 1.5 Pro  (Reid et al.,  2024 ) , on 16 MAB tasks (Table  A1 ) and 2 CB tasks. For MAB tasks, the interaction horizon ( T T T italic_T ) differs based on the size of the action space ( K K K italic_K ): we use  T = 1000 T 1000 T=1000 italic_T = 1000  for  K = 30 K 30 K=30 italic_K = 30  and  T = 200 T 200 T=200 italic_T = 200  for  K = 10 K 10 K=10 italic_K = 10 . All CB tasks use a constant horizon of  T = 200 T 200 T=200 italic_T = 200  steps. To ensure statistical significance of the results, we conduct 30 independent runs for each experimental setup.",
            "Figure  1  presents a comparative overview of in-context few-shot demonstration, oracle behavior fine-tuning, and inference-time algorithmic guidance performance across various model sizes and training configurations. Few-shot demonstrations exhibited contrasting effects on Gemini-1.5 Flash and Pro. While few-shot learning boosts the performance of Flash beyond the best inference-time setup, it surprisingly hurts Pros performance in both MAB and CB. Aligned with the observations in  Zheng et al. ( 2024 ) , our hypothesis is that few shot examples we manually crafted could disrupt the CoT structure in these larger models, which requires the few-shot examples to be carefully tuned in order to be helpful. Further analysis reveals the remarkable effectiveness of oracle behavior fine-tuning. It significantly outperforms both few-shot and baseline approaches in both MAB and CB across all model sizes, even larger ones. This robust improvement highlights the effectiveness of directly optimizing model parameters for the exploration task. Notably, the best fine-tuned Gemini-1.5 Flash model surpasses even the highest-performing Gemini-1.5 Pro model. The significant advantage of fine-tuning over few-shot learning and baseline performance highlights its potential as a key technique for enhancing LLM exploration capabilities.",
            "We examine how different inference-time support techniquesnamely  RH ,  SH , and  AG influence model performance, each utilizing distinct history textualization functions   italic- \\phi italic_ , as introduced in Section  2 . It is worth mentioning that in the MAB setup, both  SH  and  AG  significantly reduce context length compared to  RH , to  O  ( K ) O K O(K) italic_O ( italic_K )  instead of  O  ( t ) O t O(t) italic_O ( italic_t ) . As illustrated in Table  1 , leveraging inference-time support (i.e.,  SH  and  AG ) significantly enhances exploration performance across all models. This supports the intuition that effective in-context exploration requires more than memorizing input-output pairs; it demands reasoning to extract sufficient statistics from raw data and utilize them effectively for balancing exploration and exploitation. However, the exact benefit of incorporating UCB-style information in the MAB setup remains uncertain. We hypothesize that under MAB, the exploitation value and exploration bonus are straightforward transformations of the empirical mean and the number of times each arm has been pulled  N t  ( a ) subscript N t a N_{t}(a) italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_a ) , and that the LLM has the capacity to learn the functional form efficiently. In CB, we compare  AG  to  RH  and find a substantial improvement. This gap is particularly significant, as learning the exploitation value and exploration bonus in this scenario requires the model to implicitly solve ridge regression and determine the appropriate functional form of the high-probability confidence bound, making it a more complex reasoning task. The algorithmic guide approach can thus be seen as LLMs calling external tools to compute sufficient statistics required for optimal exploration.",
            "We have 16 configurations for the multi-armed bandit domain, shown in Table  A1 .",
            "In order to present the full information that was provided to LinUCB to LLM as well, we include the user preference vector in the prompt space, represented by a list of 5 floating point numbers. We additionally add descriptions to indicate that this is a user preference vector. We show our full prompt in Figure  A10 .",
            "CB,  K = 10 K 10 K=10 italic_K = 10 , Raw History ( RH ) (Figure  A10 )",
            "CB,  K = 10 K 10 K=10 italic_K = 10 , Raw History ( RH ) with Algorithm-Guided Support ( AG ) (Prompt Part 1 Figure  A11 , Prompt Part 2 Figure  A12 ).",
            "MAB, Benoulli Bandit, Video Action Description,  K = 5 K 5 K=5 italic_K = 5 , Raw History ( RH ), with Few-shot Demonstrations from Video Action Description,  K = 5 K 5 K=5 italic_K = 5 , Raw History ( RH ) (Figure  A13 )",
            "MAB, Benoulli Bandit, Video Action Description,  K = 5 K 5 K=5 italic_K = 5 , Raw History ( RH ), ith Few-shot Demonstrations from Clothes Action Description,  K = 5 K 5 K=5 italic_K = 5 , Raw History ( RH ) (Figure  A14 )"
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Overall Win-Rate (%) of different algorithm distillation methods. Flash and Pro refer to Gemini-1.5 Flash and Pro respectively. Best achieved performances are highlighted. In this table, the history textualization used in oracle trajectory and inference-time support are the same. We conduct a few ablations in Figure  4 .",
        "table": "S4.T2.2",
        "footnotes": [
            ""
        ],
        "references": [
            "We expect  good  agents to have  average  regret that converges to zero (i.e.  1 T  REG  T 0 superscript  T 1 T REG 0 \\frac{1}{T}\\text{REG}\\stackrel{{\\scriptstyle T}}{{\\rightarrow}}0 divide start_ARG 1 end_ARG start_ARG italic_T end_ARG REG start_RELOP SUPERSCRIPTOP start_ARG  end_ARG start_ARG italic_T end_ARG end_RELOP 0 ), demonstrating that they eventually learn to perform as good as the optimal policy. UCB and Thompson Sampling are two such examples with sublinear regret. Examples of cumulative regret curves are shown in Figure  2(c) .",
            "We present BanditBench, an extensive suite of MAB  (Slivkins et al.,  2019 )  and CB  (Li et al.,  2010 )  environments in  natural language  to benchmark the in-context exploration capabilities of LLMs. We show two examples in Figure  2 .",
            "For contextual bandit, at each round  t  [ T ] t delimited-[] T t\\in[T] italic_t  [ italic_T ] , the agent is presented with some contextual feature  x x x italic_x  (which may consist of both textual descriptions and numeric values) describing the state (and action). The LLM agent    \\pi italic_  chooses an action  a  A a A a\\in{\\mathcal{A}} italic_a  caligraphic_A , and then a reward  r  ( x , a ) r x a r(x,a) italic_r ( italic_x , italic_a )  is received, which depends on both the context and the chosen action. We design the semi-synthetic contextual bandit task based on the MovieLens dataset  (Harper and Konstan,  2015 ) , which consists of approximately 10,000 real users movie ratings. The goal of the agent is to recommend a personalized movie that a specific user is likely to enjoy. In particular, the observations  x x x italic_x  include user-specific features such as age, gender, occupation, and geographical location (county and state), as well as features of the movies. The action space is limited to the top- K K K italic_K  most-watched movies in the dataset, with  K = 10 K 10 K=10 italic_K = 10  for the easy setting and  K = 30 K 30 K=30 italic_K = 30  for the more challenging setting. To construct the ground-truth reward distribution, we perform low-rank approximation  (Koren et al.,  2009 )  on the user-movie rating matrix  P  R N  K P superscript R N K P\\in\\mathbb{R}^{N\\times K} italic_P  blackboard_R start_POSTSUPERSCRIPT italic_N  italic_K end_POSTSUPERSCRIPT , where  N N N italic_N  is the number of users. This is done by approximating  P P P italic_P  with  P ~ = U    V T ~ P U  superscript V T \\tilde{P}=U\\Sigma V^{T} over~ start_ARG italic_P end_ARG = italic_U roman_ italic_V start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT  using singular value decomposition (SVD), yielding a user embedding matrix  U  R N  d U superscript R N d U\\in\\mathbb{R}^{N\\times d} italic_U  blackboard_R start_POSTSUPERSCRIPT italic_N  italic_d end_POSTSUPERSCRIPT , a movie embedding matrix  V  R K  d V superscript R K d V\\in\\mathbb{R}^{K\\times d} italic_V  blackboard_R start_POSTSUPERSCRIPT italic_K  italic_d end_POSTSUPERSCRIPT , and a diagonal matrix    R d  d  superscript R d d \\Sigma\\in\\mathbb{R}^{d\\times d} roman_  blackboard_R start_POSTSUPERSCRIPT italic_d  italic_d end_POSTSUPERSCRIPT  of the top singular values. In our case, we set  d = 5 d 5 d=5 italic_d = 5  to be the dimension of the embeddings. The ground-truth reward for user  i i i italic_i  and movie  j j j italic_j  is then computed as  r i , j = u i T    v j subscript r i j superscript subscript u i T  subscript v j r_{i,j}=u_{i}^{T}\\Sigma v_{j} italic_r start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT = italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT roman_ italic_v start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT . At each time step, we provide textual contextual features alongside a 5-dimensional user preference vector  u i subscript u i u_{i} italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . The task can easily be scaled up to include more movies, i.e., a larger  K K K italic_K . Further details about the setup can be found in Appendix  A.2 .",
            "Motivated by the existence of optimal algorithms for bandits, we aim to leverage these algorithms to improve LLMs for exploration by: 1) incorporating algorithmic guidance during inference (Section  4.1 ), 2) teaching optimal exploration through algorithm distillation (Section  4.2 ). We show that smaller models trained using algorithm distillation can even outperform larger models, offering a promising way to efficiently explore with lower inference costs.",
            "We use UCB as the oracle algorithm to generate the trajectories. Following the notations defined in Section  2 , the trajectories are in the form of tuples of  (   ( H t UCB ) , a t UCB ) italic- subscript superscript H UCB t subscript superscript a UCB t (\\phi(H^{\\text{UCB}}_{t}),a^{\\text{UCB}}_{t}) ( italic_ ( italic_H start_POSTSUPERSCRIPT UCB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , italic_a start_POSTSUPERSCRIPT UCB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , where each tuple pairs the transformed representation of the history at time  t t t italic_t  and the action  a t UCB subscript superscript a UCB t a^{\\text{UCB}}_{t} italic_a start_POSTSUPERSCRIPT UCB end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  from UCB. For MAB, we create trajectories from reward distributions that  differ  from those used in evaluation. This assesses the LLMs ability to generalize across different bandit instances with the same underlying scenario but varying  action descriptions  and  action-reward mappings . We further control the data generation process by varying: (1).  Action Description : trajectories are generated from either \"Video\" or \"Clothes\" action descriptions; (2).  Difficulty : we control the reward gap in the Bernoulli bandit to create \"easy\" or \"hard\" instances; (3).  Trajectory Textualization : trajectories are represented either as  RH  or  AG . For CB, we use a fixed dataset and evaluate the LLMs performance on a held-out set of users. While these users are unseen during training, their profiles and preferences remain within the distribution of the training data. This evaluates the LLMs ability to leverage prior knowledge for effective exploration. In CB, we only vary the trajectory representation ( RH  or  AG ). In both MAB and CB, each trajectory consists of a sequence of exploration steps: 300 steps for MAB with  K = 5 K 5 K=5 italic_K = 5  arms, 1000 steps for MAB with  K = 20 K 20 K=20 italic_K = 20  arms, and 200 steps for CB. We generate 50 trajectories for 2 MAB domain configurations (the easiest and the hardest configuration) with 2 trajectory textualizations, and 200 trajectories for CB with 2 trajectory textualization . This results in 4 algorithm distillation datasets for MAB and 2 datasets for CB.",
            "In this section, we empirically evaluate LLMs in-context exploration capabilities, using BanditBench. We begin with introducing the setup, baselines and metrics in Section  4.3.1 . Following this, in Section  4.3.2 , we analyze the performance of inference-time guided support, in-context few-shot demonstration and oracle behavior fine-tuning across various experimental settings, as well as models of different sizes. Additionally, we perform extensive ablation studies on the impact of task difficulty, textual representation of the oracle trajectories, and inference-training representation alignment.",
            "We examine how different inference-time support techniquesnamely  RH ,  SH , and  AG influence model performance, each utilizing distinct history textualization functions   italic- \\phi italic_ , as introduced in Section  2 . It is worth mentioning that in the MAB setup, both  SH  and  AG  significantly reduce context length compared to  RH , to  O  ( K ) O K O(K) italic_O ( italic_K )  instead of  O  ( t ) O t O(t) italic_O ( italic_t ) . As illustrated in Table  1 , leveraging inference-time support (i.e.,  SH  and  AG ) significantly enhances exploration performance across all models. This supports the intuition that effective in-context exploration requires more than memorizing input-output pairs; it demands reasoning to extract sufficient statistics from raw data and utilize them effectively for balancing exploration and exploitation. However, the exact benefit of incorporating UCB-style information in the MAB setup remains uncertain. We hypothesize that under MAB, the exploitation value and exploration bonus are straightforward transformations of the empirical mean and the number of times each arm has been pulled  N t  ( a ) subscript N t a N_{t}(a) italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_a ) , and that the LLM has the capacity to learn the functional form efficiently. In CB, we compare  AG  to  RH  and find a substantial improvement. This gap is particularly significant, as learning the exploitation value and exploration bonus in this scenario requires the model to implicitly solve ridge regression and determine the appropriate functional form of the high-probability confidence bound, making it a more complex reasoning task. The algorithmic guide approach can thus be seen as LLMs calling external tools to compute sufficient statistics required for optimal exploration.",
            "In Table  A2 , we provide a detailed comparison between the exploitation values and exploration bonus used in both UCB and LinUCB.",
            "To verify that our functional form fits the data (empirical cumulative regret curve) well, we show a few figures of our fitted curve and actual data. In Figure  A2 , we show how the learned function  f  ( T ) f T f(T) italic_f ( italic_T )  fit the actual empirical cumulative regret curve.",
            "In Figure  A2 , it is interesting to see that the function we choose exhibit the behavior of pushing either    \\alpha italic_  or    \\beta italic_  to 0, if either of the two describes the trend better. We note that although the fit is not perfect, the MSE is relatively small compared to the data we are trying to fit. For a cumulative regret as large as 100 at some time step  T T T italic_T , our fitted function ccan still maintain an MSE of 0.22.",
            "CB,  K = 10 K 10 K=10 italic_K = 10 , Raw History ( RH ) with Algorithm-Guided Support ( AG ) (Prompt Part 1 Figure  A11 , Prompt Part 2 Figure  A12 )."
        ]
    },
    "id_table_3": {
        "caption": "Table A1:  Configuration of the MAB setup.",
        "table": "A1.EGx1",
        "footnotes": [],
        "references": [
            "In this section, we empirically evaluate LLMs in-context exploration capabilities, using BanditBench. We begin with introducing the setup, baselines and metrics in Section  4.3.1 . Following this, in Section  4.3.2 , we analyze the performance of inference-time guided support, in-context few-shot demonstration and oracle behavior fine-tuning across various experimental settings, as well as models of different sizes. Additionally, we perform extensive ablation studies on the impact of task difficulty, textual representation of the oracle trajectories, and inference-training representation alignment.",
            "We examine whether the choice of oracle trajectories used in both in-context demonstration and oracle behavior fine-tuning significantly affects the models performance during inference. To investigate this, we select trajectories from two extreme setups. The easiest setup involves  (Bernoulli, Video, Large   m  i  n subscript  m i n \\Delta_{min} roman_ start_POSTSUBSCRIPT italic_m italic_i italic_n end_POSTSUBSCRIPT ,  K = 5 K 5 K=5 italic_K = 5 ) , denoted as  D easy subscript D easy D_{\\text{easy}} italic_D start_POSTSUBSCRIPT easy end_POSTSUBSCRIPT , with  AG . Conversely, the hardest setup, denoted as  D hard subscript D hard D_{\\text{hard}} italic_D start_POSTSUBSCRIPT hard end_POSTSUBSCRIPT  utilizes  (Bernoulli, Clothes, Small   m  i  n subscript  m i n \\Delta_{min} roman_ start_POSTSUBSCRIPT italic_m italic_i italic_n end_POSTSUBSCRIPT ,  K = 20 K 20 K=20 italic_K = 20 ) , with  RH . Figure  3(a)  illustrates that the choice of oracle trajectories significantly impacts the models performance, with a surprising contrast between the two algorithm distillation methods. In-context demonstration achieves a higher win-rate when using  D easy subscript D easy D_{\\text{easy}} italic_D start_POSTSUBSCRIPT easy end_POSTSUBSCRIPT  as demonstration (50.2) compared to when using  D hard subscript D hard D_{\\text{hard}} italic_D start_POSTSUBSCRIPT hard end_POSTSUBSCRIPT  (43.0). This suggests that the limited examples provided in the demonstrations may be insufficient for the model to effectively utilize them under the higher complexity and subtle reward signals of the harder task. Conversely, fine-tuning exhibits the opposite trend, with a higher win-rate when trained on  D hard subscript D hard D_{\\text{hard}} italic_D start_POSTSUBSCRIPT hard end_POSTSUBSCRIPT  (65.6) compared to  D easy subscript D easy D_{\\text{easy}} italic_D start_POSTSUBSCRIPT easy end_POSTSUBSCRIPT  (54.5). This implies that fine-tuning, with its extensive training data, might be overfitting to the specific nuances of the training distribution, leading to poor generalization when faced with a different task structure.",
            "We further investigate the effect of textualization in the oracle trajectories. We consider two representations in MAB:  RH  and  SH . The results in Figure  3(b)  reveal a clear contrast in how these representations affect the two algorithm distillation methods. For in-context demonstration,  SH  leads to significantly better performance (50.2% win-rate) compared to  RH  (27.5% win-rate). This suggests that providing concise, informative summaries of optimal exploration behavior is more effective for few-shot learning than presenting the complete raw history. On the other hand, fine-tuning exhibits the opposite trend.  RH  has a substantially higher win-rate (65.6) compared to  SH  (28.3). This indicates that fine-tuning benefits from the richer information present in complete action-reward sequences, allowing it to learn more nuanced patterns of the optimal exploration strategy. These contrasting preferences for textual representation in oracle trajectories highlight the nuanced ways in which fine-tuning and few-shot learning interact with different types of information. Furthermore, in CB, we observe a significant impact of incorporating algorithm-guided ( AG ) information into the oracle trajectories for fine-tuning. Augmenting  RH  with  AG  details, including the exploitation value and exploration bonus, leads to a dramatic improvement in win-rate, rising from 28.6 to 89.3 in Figure  3(c) . This suggests that providing the LLM with explicit insights into the underlying decision-making process of the oracle algorithm (UCB, in this case), in addition to the complete action-reward sequence, significantly enhances its ability to learn and generalize the optimal exploration strategy in the CB environment.",
            "We additionally add the analysis for the MAB Gaussian bandit. The instance optimality gap   min subscript  \\Delta_{\\min} roman_ start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT  is characterized by the KL-divergence of the Gaussian reward distribution between the best arm and the second best arm. We show the result in Figure  A3 . The trend is somewhat similar to Bernoulli bandits, where smaller models perform much worse than larger models.",
            "MAB, Benoulli Bandit, Video Action Description,  K = 5 K 5 K=5 italic_K = 5 , Raw History ( RH ), with Few-shot Demonstrations from Video Action Description,  K = 5 K 5 K=5 italic_K = 5 , Raw History ( RH ) (Figure  A13 )"
        ]
    },
    "id_table_4": {
        "caption": "Table A2 :  Calculation for the value of each arm/item. The decision rule is  a  = arg  max a  V t  ( a , x ) superscript a subscript arg max a subscript V t a x a^{*}=\\operatorname*{arg\\,max}_{a}V_{t}(a,x) italic_a start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT italic_V start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_a , italic_x ) .",
        "table": "A1.T1.6",
        "footnotes": [],
        "references": [
            "Motivated by the existence of optimal algorithms for bandits, we aim to leverage these algorithms to improve LLMs for exploration by: 1) incorporating algorithmic guidance during inference (Section  4.1 ), 2) teaching optimal exploration through algorithm distillation (Section  4.2 ). We show that smaller models trained using algorithm distillation can even outperform larger models, offering a promising way to efficiently explore with lower inference costs.",
            "In this section, we empirically evaluate LLMs in-context exploration capabilities, using BanditBench. We begin with introducing the setup, baselines and metrics in Section  4.3.1 . Following this, in Section  4.3.2 , we analyze the performance of inference-time guided support, in-context few-shot demonstration and oracle behavior fine-tuning across various experimental settings, as well as models of different sizes. Additionally, we perform extensive ablation studies on the impact of task difficulty, textual representation of the oracle trajectories, and inference-training representation alignment.",
            "Our experiments also reveal an interesting interplay between the presence of algorithm-guided information ( AG ) in both the oracle  trajectories  and  inference . In the CB setting, providing  AG  during inference consistently boosts performance, regardless of whether  AG  was used in oracle trajectories. This is clearly demonstrated in Figure  4 , where the right column (with  AG  at inference time) exhibits higher win-rates than the corresponding left column across all training conditions. This suggests that the LLM can effectively leverage this information even if it wasnt explicitly trained on it, highlighting the inherent value of structured guidance for decision-making. Furthermore, we observe that incorporating  AG  into few-shot demonstrations improves exploration even when  AG  is absent during inference (e.g., Fewshot,  RH  3.6 to  RH  + AG  10.7). This indicates that exposing the LLM to  AG  in oracle trajectories, even in a limited capacity, can enhance its ability to extract relevant patterns from  RH . We hypothesize that  AG  helps the LLM learn to focus on the most informative aspects of the history, which generalizes even when  AG  is not provided during inference.",
            "MAB, Bernoulli Bandit,  K = 5 K 5 K=5 italic_K = 5 , Raw History ( RH ), Video Action Description (Figure  A4 ), Clothes Action Description (Figure  A5 )",
            "MAB, Benoulli Bandit, Video Action Description,  K = 5 K 5 K=5 italic_K = 5 , Raw History ( RH ), ith Few-shot Demonstrations from Clothes Action Description,  K = 5 K 5 K=5 italic_K = 5 , Raw History ( RH ) (Figure  A14 )"
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "A1.T2.2",
        "footnotes": [],
        "references": [
            "The three parameters   ,  ,  1   subscript  1 \\alpha,\\beta,\\lambda_{1} italic_ , italic_ , italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  in the equation are all positive real numbers.   2 subscript  2 \\lambda_{2} italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  is unconstrained.   min subscript  \\Delta_{\\min} roman_ start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT  captures the gap between the best and second best arm. This functional form provides intuitive interpretations for the underlying parameters. Specifically,  log  ( T ) T \\log(T) roman_log ( italic_T )  represents sublinear scaling of the regret, which is known to be achieved by only the best bandit algorithms (e.g. UCB and Thompson Sampling). The  T T T italic_T  scaling describes a linear growth or the inability of an agent to match the optimal policy    superscript  \\pi^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT . This means a strong algorithm should have    \\alpha italic_  as small as possible, and have   = 0  0 \\beta=0 italic_ = 0 . This functional form also allows us to see some growth behaviors in-between with both positive    \\alpha italic_  and    \\beta italic_ . We use the curve fit function in Scikit-learn  (Pedregosa et al.,  2011 )  to fit the cumulative regret curve of UCB and LLMs coupled with different methods (i.e., inference-time algorithm-guided support, in-context demonstration, and oracle behavior fine-tuning). The results of the fitted    \\alpha italic_  and    \\beta italic_  values are presented in Figure  5 . For the largest Pro models, applying effective inference-time support, such as  AG  and  SH  can achieve nearly sub-linear regret. More intriguingly, for Flash models, fine-tuning for oracle behavior significantly boosts performance, enabling them to attain sub-linear regret with a lower    \\alpha italic_ . In contrast, weaker models such as Gemma 2B and 9B appear to remain in the linear regret regime across nearly all methods.",
            "We perform the same analysis with the cumulative regret function on MAB in the Hard Difficulty setting. We can see that in Figure  5(b) , a lot fewer LLM models achieved   = 0  0 \\beta=0 italic_ = 0 , which means achieving the desirable logarithmic sublinear regret that algorithms like UCB and Thompson Sampling have.",
            "MAB, Bernoulli Bandit,  K = 5 K 5 K=5 italic_K = 5 , Raw History ( RH ), Video Action Description (Figure  A4 ), Clothes Action Description (Figure  A5 )"
        ]
    }
}