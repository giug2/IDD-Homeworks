{
    "id_table_1": {
        "caption": "Table 1:  Implementation Details for the safety LLM alignment with IPO",
        "table": "A6.EGx1",
        "footnotes": [],
        "references": [
            "In the asymptotic regime where  n ^    ^ n \\hat{n}\\to\\infty over^ start_ARG italic_n end_ARG    with  p n ^  0  p ^ n 0 \\frac{p}{\\hat{n}}\\to 0 divide start_ARG italic_p end_ARG start_ARG over^ start_ARG italic_n end_ARG end_ARG  0 , we can generate synthetic samples that follow asymptotically the exact same distribution as of the real ones, and therefore only label noise is relevant to the quality of the synthetic data. However, in the regime when both  n ^ , p    ^ n p \\hat{n},p\\to\\infty over^ start_ARG italic_n end_ARG , italic_p    with  p n ^   ^ > 0  p ^ n ^  0 \\frac{p}{\\hat{n}}\\to\\hat{\\eta}>0 divide start_ARG italic_p end_ARG start_ARG over^ start_ARG italic_n end_ARG end_ARG  over^ start_ARG italic_ end_ARG > 0 , while the estimation of    {\\bm{\\mu}} bold_italic_  with   ^ ^  \\hat{\\bm{\\mu}} over^ start_ARG bold_italic_ end_ARG  remains consistent, the estimation of the covariance is not. In fact, in this regime   C ^  I p   0  norm ^ C subscript I p 0 \\|\\hat{\\mathbf{C}}-{\\mathbf{I}}_{p}\\|\\not\\to 0  over^ start_ARG bold_C end_ARG - bold_I start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT   0  and the eigenvalues of  C ^ ^ C \\hat{\\mathbf{C}} over^ start_ARG bold_C end_ARG  spread in the vicinity of  1 1 1 1  which is described in the limit by the Marchenko-Pastur law  (Marchenko & Pastur,  1967 )  as depicted in Fig.  1 . Eventually, such inconsistency in estimating the second moment in high dimensions yields a distribution shift between synthetic and real data, which might cause a drop in performance when training a new model on synthetic data generated with   ^ ^  \\hat{\\bm{\\mu}} over^ start_ARG bold_italic_ end_ARG  and  C ^ ^ C \\hat{\\mathbf{C}} over^ start_ARG bold_C end_ARG . In the remainder, we describe precisely how the performance of a simple classifier is affected in this regime.",
            "We start by analyzing the general case of training on a mix of real and synthetic data. As we described in the previous section, the statistics of synthetic data are empirical estimates of the ones of real data. Under Assumption  4.1 , the estimation of    {\\bm{\\mu}} bold_italic_  with   ^ ^  \\hat{\\bm{\\mu}} over^ start_ARG bold_italic_ end_ARG  remains consistent, while the estimation of the underlying real data covariance (i.e.,  I p subscript I p {\\mathbf{I}}_{p} bold_I start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT  in our setting) with  C ^ ^ C \\hat{\\mathbf{C}} over^ start_ARG bold_C end_ARG  is inconsistent as we previously discussed. As a result, studying the theoretical performance of the classifier in equation  6  demands deploying tools from random matrix theory that refines the estimation of scalar quantities depending on large random matrices. In our case, the scalar quantity of interest corresponds to the models accuracy which depends on the random matrices  C ^ ^ C \\hat{\\mathbf{C}} over^ start_ARG bold_C end_ARG  and  XX  superscript XX top {\\mathbf{X}}{\\mathbf{X}}^{\\top} bold_XX start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  as per equation  6 .",
            "In our analysis of the classifiers theoretical performance, we found that the effect of high-dimension (and that of distribution shift between real and synthetic samples) is described by three scalar quantities  (  r  ,  s  ,  g  ) superscript subscript  r superscript subscript  s superscript subscript  g (\\delta_{r}^{*},\\delta_{s}^{*},\\delta_{g}^{*}) ( italic_ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , italic_ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , italic_ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT )  which are defined as the unique solution of the following fixed point system which is derived from Lemma  D.1  in the Appendix.",
            "where   =   ( 1   ) +     italic- 1    \\alpha=\\phi(1-\\varepsilon)+\\rho\\varepsilon italic_ = italic_ ( 1 - italic_ ) + italic_ italic_ . These quantities will be used subsequently in our results. Intuitively,   r  superscript subscript  r \\delta_{r}^{*} italic_ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  captures the contribution of real data,   s  superscript subscript  s \\delta_{s}^{*} italic_ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  corresponds to the contribution of synthetic data, and   g  superscript subscript  g \\delta_{g}^{*} italic_ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  corresponds to the influence of the generative model. In an infinite sample size regime where  n , m , n ^    n m ^ n n,m,\\hat{n}\\to\\infty italic_n , italic_m , over^ start_ARG italic_n end_ARG    while the dimension  p p p italic_p  is kept fixed,  (  r  ,  s  ,  g  ) = ( 0 , 0 , 0 ) superscript subscript  r superscript subscript  s superscript subscript  g 0 0 0 (\\delta_{r}^{*},\\delta_{s}^{*},\\delta_{g}^{*})=(0,0,0) ( italic_ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , italic_ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , italic_ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) = ( 0 , 0 , 0 )  as per Fig.  2 , while under Assumption  4.1  these quantities are non zero yielding a counterintuitive behavior in high-dimension. For convenience, we further define a set of scalar quantities that will prove usefull in the next result.",
            "Let  w w {\\bm{w}} bold_italic_w  be the Ridge classifier as defined in equation  6  and suppose that Assumption  4.1  holds. The decision function  w   x superscript w top x {\\bm{w}}^{\\top}{\\bm{x}} bold_italic_w start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT bold_italic_x , on some (real) test sample  x  C a x subscript C a {\\bm{x}}\\in{\\mathcal{C}}_{a} bold_italic_x  caligraphic_C start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT , with corresponding label  y = (  1 ) a y superscript 1 a y=(-1)^{a} italic_y = ( - 1 ) start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT  and independent of  X X {\\mathbf{X}} bold_X , satisfies",
            "Let  w s subscript w s {\\bm{w}}_{s} bold_italic_w start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  be the Ridge classifier described in equation  6  trained only on synthetic data with only label noise (i.e.,  C ^ = I p ^ C subscript I p \\hat{\\mathbf{C}}={\\mathbf{I}}_{p} over^ start_ARG bold_C end_ARG = bold_I start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ). Under Assumption  4.1 , the decision function  w s   x superscript subscript w s top x {\\bm{w}}_{s}^{\\top}{\\bm{x}} bold_italic_w start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT bold_italic_x  on a test sample  x  C a x subscript C a {\\bm{x}}\\in{\\mathcal{C}}_{a} bold_italic_x  caligraphic_C start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT  with corresponding label  y = (  1 ) a y superscript 1 a y=(-1)^{a} italic_y = ( - 1 ) start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT  and independent of  X X {\\mathbf{X}} bold_X , satisfies",
            "Let us state here the deterministic equivalent of the resolvent matrix  Q Q {\\mathbf{Q}} bold_Q  defined in the general models equation ( 6 ) for any general covariance matrix  C C {\\mathbf{C}} bold_C  and mean    =    +   subscript     superscript  perpendicular-to {\\bm{\\mu}}_{\\beta}=\\beta{\\bm{\\mu}}+{\\bm{\\mu}}^{\\perp} bold_italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT = italic_ bold_italic_ + bold_italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  that define the statistic of the synthetic data, as in equation  13 .",
            "Under the  4.1  assumptions listed above in the main paper, a deterministic equivalent for  Q  Q  (  ) Q Q  {\\mathbf{Q}}\\equiv{\\mathbf{Q}}(\\gamma) bold_Q  bold_Q ( italic_ ) , denoted  Q     Q \\bar{\\mathbf{Q}} over  start_ARG bold_Q end_ARG , is given by:",
            "Let:  M = ( I 2 + U   A  1  U )  1 M superscript subscript I 2 superscript U top superscript A 1 U 1 {\\mathbf{M}}=\\left({\\mathbf{I}}_{2}+{\\mathbf{U}}^{\\top}{\\mathbf{A}}^{-1}{% \\mathbf{U}}\\right)^{-1} bold_M = ( bold_I start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + bold_U start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT bold_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_U ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT , and denote by  M i , j subscript M i j M_{i,j} italic_M start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT  its coordinate in the  i th superscript i th i^{\\text{th}} italic_i start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT  row and  j th superscript j th j^{\\text{th}} italic_j start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT  column. We have that using the expression of  Q     Q \\bar{\\mathbf{Q}} over  start_ARG bold_Q end_ARG  in equation  12 :",
            "since     = O  ( N  1 ) norm  O superscript N 1 \\|{\\bm{\\mu}}\\|={\\mathcal{O}}(N^{-1})  bold_italic_  = caligraphic_O ( italic_N start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT )  by assumption  4.1 . The same applies for    subscript   {\\bm{\\mu}}_{\\beta} bold_italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT . Thus:",
            "Hence we have the desired result for    \\delta italic_  in the regime  N  1 much-greater-than N 1 N\\gg 1 italic_N  1  which we considered in our assumption  4.1 .  Similarly for   S subscript  S \\delta_{S} italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT , we have that:",
            "where    R  R \\beta\\in{\\mathbb{R}} italic_  blackboard_R  defines the alignment of the synthetic mean with the mean of real data, and    superscript  perpendicular-to {\\bm{\\mu}}^{\\perp} bold_italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is a vector orthogonal to    {\\bm{\\mu}} bold_italic_ .  Now we will analyze here the performance of the classifier given by equation ( 6 ), and prove a generalized theorem  B.1  in the paper.",
            "Let  w q subscript w q {\\bm{w}}_{q} bold_italic_w start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT  be the Mixed classifier as defined in equation  6  and suppose that Assumption  4.1  holds. The decision function  w q   x superscript subscript w q top x {\\bm{w}}_{q}^{\\top}{\\bm{x}} bold_italic_w start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT bold_italic_x , on some test sample  x  C a x subscript C a {\\bm{x}}\\in{\\mathcal{C}}_{a} bold_italic_x  caligraphic_C start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT  independent of  X X {\\mathbf{X}} bold_X , satisfies:",
            "Let  w q subscript w q {\\bm{w}}_{q} bold_italic_w start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT  be the Mixed classifier as defined in equation  6  and suppose that Assumption  4.1  holds. The decision function  w q   x superscript subscript w q top x {\\bm{w}}_{q}^{\\top}{\\bm{x}} bold_italic_w start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT bold_italic_x , on some test sample  x  C a x subscript C a {\\bm{x}}\\in{\\mathcal{C}}_{a} bold_italic_x  caligraphic_C start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT  independent of  X X {\\mathbf{X}} bold_X , satisfies:",
            "Now we will prove the deterministic equivalent given by lemma  D.1 .",
            "It only suffices to apply the expectation on  z i subscript z i {\\bm{z}}_{i} bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  to  m q subscript m q m_{q} italic_m start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT  obtained with the general model in theorem  B.1 . Hence:",
            "Using theorem  B.1 , we need to apply the expectation on  z z {\\bm{z}} bold_italic_z  to the following second order moment:",
            "As part of this experiment, we had to generate a synthetic QA Dataset. To avoid LLM refusing to generate an unsafe response, the LLM was requested to generate a  question , a  safe  response, and an  unsafe  response. Figure  10  shows the system prompt used to request from an LLM to generate QA.  < < < Topic > > >  is a placeholder referring to a particular risk topic, selected from the list of topics seen in Figure  11 , the section written in red. As discussed in the paper, the generated QA will be annotated by LLM, using the prompt presented in Figure  11 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Fine-tuning for  Llama3.1-8B-Instruct",
        "table": "A6.EGx2",
        "footnotes": [],
        "references": [
            "which control the pruner accuracy (as discussed in  (Feng et al.,  2024 ) ). As we mentioned previously, we suppose training on  n  n ^ n ^ n n\\geq\\hat{n} italic_n  over^ start_ARG italic_n end_ARG  real data, modeling a situation where new real samples are available with  n ^ ^ n \\hat{n} over^ start_ARG italic_n end_ARG  controlling the generative model quality in generating faithful synthetic features 1 1 1 Technically, our results hold irrespective of the statistical dependencies between the data used to train the generative model in equation  2  or the classifier in equation  6 . .",
            "where   =   ( 1   ) +     italic- 1    \\alpha=\\phi(1-\\varepsilon)+\\rho\\varepsilon italic_ = italic_ ( 1 - italic_ ) + italic_ italic_ . These quantities will be used subsequently in our results. Intuitively,   r  superscript subscript  r \\delta_{r}^{*} italic_ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  captures the contribution of real data,   s  superscript subscript  s \\delta_{s}^{*} italic_ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  corresponds to the contribution of synthetic data, and   g  superscript subscript  g \\delta_{g}^{*} italic_ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  corresponds to the influence of the generative model. In an infinite sample size regime where  n , m , n ^    n m ^ n n,m,\\hat{n}\\to\\infty italic_n , italic_m , over^ start_ARG italic_n end_ARG    while the dimension  p p p italic_p  is kept fixed,  (  r  ,  s  ,  g  ) = ( 0 , 0 , 0 ) superscript subscript  r superscript subscript  s superscript subscript  g 0 0 0 (\\delta_{r}^{*},\\delta_{s}^{*},\\delta_{g}^{*})=(0,0,0) ( italic_ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , italic_ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , italic_ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) = ( 0 , 0 , 0 )  as per Fig.  2 , while under Assumption  4.1  these quantities are non zero yielding a counterintuitive behavior in high-dimension. For convenience, we further define a set of scalar quantities that will prove usefull in the next result.",
            "Theorem  4.2  states that the decision function of the classifier in equation  6  is asymptotically equivalent to the thresholding of two monovariate Gaussian random variables with respective means    \\mu italic_  and     -\\mu - italic_  and standard deviation    \\nu italic_ , where the statistics    \\mu italic_  and    \\nu italic_  are expressed in terms of the scalar quantities defined above. Here,    \\mu italic_  represents the signal strength while    \\nu italic_  highlights the classifiers uncertainty or dispersion. To provide some insights into the implications of this theorem, we start by examining it in a low-dimensional regime where  p p p italic_p  is kept fixed while  n , m , n ^    n m ^ n n,m,\\hat{n}\\to\\infty italic_n , italic_m , over^ start_ARG italic_n end_ARG   . In this case, we have   ,  ^  0   ^  0 \\eta,\\hat{\\eta}\\to 0 italic_ , over^ start_ARG italic_ end_ARG  0  and   r  ,  s  ,  g   0  superscript subscript  r superscript subscript  s superscript subscript  g 0 \\delta_{r}^{*},\\delta_{s}^{*},\\delta_{g}^{*}\\to 0 italic_ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , italic_ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , italic_ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  0  which yields",
            "Fig.  3  depicts the empirical test accuracy and the theoretical prediction as per Theorem  4.2  when varying the proportion of synthetic data. As theoretically anticipated, adding synthetic data does not boost the classifiers performance unless it is verified accurately (oracle supervision versus weak supervision). Moreover, our results show the effect of the distribution shift which heavily affects performance in the case of weak supervision (Fig.  3  right).",
            "In this section, we study the fully synthetic setting which corresponds to training solely on synthetic data (i.e.  n = 0 n 0 n=0 italic_n = 0  in equation  6 ). For simplicity, we consider only label noise and ignore feature noise in the synthetic data. Essentially, this allows us to exhibit the smooth phase transition of the classifiers accuracy in terms of label noise, which extends the result of  Feng et al. ( 2024 ) . Specifically, we obtain the following corollary of theorem  4.2 .",
            "Corollary  4.3  provides an explicit formulation of Theorem  4.2  with synthetic data only and ignoring distribution shift (yielding an explicit expression of   s subscript  s \\delta_{s} italic_ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ). This setting provides a clearer interpretation of the effect of label noise since the classifiers performance is directly related to the quantity   =   ( 1   )      italic- 1    \\lambda=\\phi(1-\\varepsilon)-\\rho\\varepsilon italic_ = italic_ ( 1 - italic_ ) - italic_ italic_ . The breaking point of the classifiers performance occurs at   = 0  0 \\lambda=0 italic_ = 0 , which corresponds to the accuracy of random guessing, yielding to the critical value of label noise    = ( 1 +   )  1 superscript  superscript 1  italic- 1 \\varepsilon^{*}=(1+\\frac{\\rho}{\\phi})^{-1} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = ( 1 + divide start_ARG italic_ end_ARG start_ARG italic_ end_ARG ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT . This critical value is equivalent to the one obtained by  Feng et al. ( 2024 ) , however, we extend their result to the high-dimensional setting which exhibits a smoother phase transition as depicted in Fig.  4 . Essentially, the sharp phase transition of  Feng et al. ( 2024 )  is covered by our result by taking   s  0  subscript  s 0 \\eta_{s}\\to 0 italic_ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  0 . In this sense, the predicted smooth transition better mirrors real-world scenarios where finite sample sizes introduce gradual changes in performance rather than abrupt shifts. This makes our theoretical findings more applicable and reliable for practical scenarios.",
            "We use the Amazon Reviews datasets ( Blitzer et al. ( 2007 ) ) which include several binary classification tasks corresponding to positive versus negative reviews of  books ,  electronics  and  kitchen . We apply the standard scaler from  sklearn   (Pedregosa et al.,  2011 )  and estimate     norm  \\|{\\bm{\\mu}}\\|  bold_italic_   with the normalized data. The synthetic data is generated following the described generative scheme (see equation  2 ). We use the Ridge classifier in equation  6  for this data.",
            "A particular case of this lemma  A.2 , in the case of  k = 1 k 1 k=1 italic_k = 1 , is called  Sherman-Morisson s identity.",
            "And using Woodburys identity in lemma  A.2 , we get that:",
            "Let:  M = ( I 2 + U   A  1  U )  1 M superscript subscript I 2 superscript U top superscript A 1 U 1 {\\mathbf{M}}=\\left({\\mathbf{I}}_{2}+{\\mathbf{U}}^{\\top}{\\mathbf{A}}^{-1}{% \\mathbf{U}}\\right)^{-1} bold_M = ( bold_I start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + bold_U start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT bold_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_U ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT , and denote by  M i , j subscript M i j M_{i,j} italic_M start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT  its coordinate in the  i th superscript i th i^{\\text{th}} italic_i start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT  row and  j th superscript j th j^{\\text{th}} italic_j start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT  column. We have that using the expression of  Q     Q \\bar{\\mathbf{Q}} over  start_ARG bold_Q end_ARG  in equation  12 :",
            "And finally we use lemma  C.2  and the following identities to obtain the result:",
            "We will now quantify the performance of the classifier obtained through mixing some real data and synthetic data sampled according to the schema described in  2 . Hence, the matrix  Q     Q \\bar{\\mathbf{Q}} over  start_ARG bold_Q end_ARG , defined in lemma  A.4 , is no longer deterministic as we take the covariance matrix  C ^ = 1 n ^   i = 1 n ^ ( x i  y i   ^ )  ( x i  y i   ^ )  ^ C 1 ^ n superscript subscript i 1 ^ n subscript x i subscript y i ^  superscript subscript x i subscript y i ^  top \\hat{{\\mathbf{C}}}=\\frac{1}{\\hat{n}}\\sum_{i=1}^{\\hat{n}}({\\bm{x}}_{i}-y_{i}% \\hat{{\\bm{\\mu}}})({\\bm{x}}_{i}-y_{i}\\hat{{\\bm{\\mu}}})^{\\top} over^ start_ARG bold_C end_ARG = divide start_ARG 1 end_ARG start_ARG over^ start_ARG italic_n end_ARG end_ARG  start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT over^ start_ARG italic_n end_ARG end_POSTSUPERSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT over^ start_ARG bold_italic_ end_ARG ) ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT over^ start_ARG bold_italic_ end_ARG ) start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT . For simplicity, and without loss of generality, we consider  n ^ ^ n \\hat{n} over^ start_ARG italic_n end_ARG  Gaussian vectors  ( z i ) i = 1 n ^  N  ( 0 , I p ) similar-to superscript subscript subscript z i i 1 ^ n N 0 subscript I p ({\\bm{z}}_{i})_{i=1}^{\\hat{n}}\\sim{\\mathcal{N}}(0,{\\mathbf{I}}_{p}) ( bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT over^ start_ARG italic_n end_ARG end_POSTSUPERSCRIPT  caligraphic_N ( 0 , bold_I start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT )  that are independent of  ( x i ) i = 1 n ^ superscript subscript subscript x i i 1 ^ n ({\\bm{x}}_{i})_{i=1}^{\\hat{n}} ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT over^ start_ARG italic_n end_ARG end_POSTSUPERSCRIPT , and write:",
            "Since the covariance estimate in equation  2  is stochastic, the matrix  Q     Q \\bar{\\mathbf{Q}} over  start_ARG bold_Q end_ARG  is no longer deterministic when replacing  C C {\\mathbf{C}} bold_C  with  C ^ ^ C \\hat{{\\mathbf{C}}} over^ start_ARG bold_C end_ARG . Hence, we will give a further deterministic equivalent to  Q     Q \\bar{\\mathbf{Q}} over  start_ARG bold_Q end_ARG  in the following lemma.",
            "Then, using lemma  D.2  we have that the following identities stand for any linear form:",
            "which concludes the proof of the main theorem  4.2  of this paper."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Fine-tuning for  Gemma-2-2B-it",
        "table": "A6.EGx3",
        "footnotes": [],
        "references": [
            "Fig.  3  depicts the empirical test accuracy and the theoretical prediction as per Theorem  4.2  when varying the proportion of synthetic data. As theoretically anticipated, adding synthetic data does not boost the classifiers performance unless it is verified accurately (oracle supervision versus weak supervision). Moreover, our results show the effect of the distribution shift which heavily affects performance in the case of weak supervision (Fig.  3  right).",
            "Corollary  4.3  provides an explicit formulation of Theorem  4.2  with synthetic data only and ignoring distribution shift (yielding an explicit expression of   s subscript  s \\delta_{s} italic_ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ). This setting provides a clearer interpretation of the effect of label noise since the classifiers performance is directly related to the quantity   =   ( 1   )      italic- 1    \\lambda=\\phi(1-\\varepsilon)-\\rho\\varepsilon italic_ = italic_ ( 1 - italic_ ) - italic_ italic_ . The breaking point of the classifiers performance occurs at   = 0  0 \\lambda=0 italic_ = 0 , which corresponds to the accuracy of random guessing, yielding to the critical value of label noise    = ( 1 +   )  1 superscript  superscript 1  italic- 1 \\varepsilon^{*}=(1+\\frac{\\rho}{\\phi})^{-1} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = ( 1 + divide start_ARG italic_ end_ARG start_ARG italic_ end_ARG ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT . This critical value is equivalent to the one obtained by  Feng et al. ( 2024 ) , however, we extend their result to the high-dimensional setting which exhibits a smoother phase transition as depicted in Fig.  4 . Essentially, the sharp phase transition of  Feng et al. ( 2024 )  is covered by our result by taking   s  0  subscript  s 0 \\eta_{s}\\to 0 italic_ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  0 . In this sense, the predicted smooth transition better mirrors real-world scenarios where finite sample sizes introduce gradual changes in performance rather than abrupt shifts. This makes our theoretical findings more applicable and reliable for practical scenarios.",
            "Let us state here the deterministic equivalent of the resolvent matrix  Q Q {\\mathbf{Q}} bold_Q  defined in the general models equation ( 6 ) for any general covariance matrix  C C {\\mathbf{C}} bold_C  and mean    =    +   subscript     superscript  perpendicular-to {\\bm{\\mu}}_{\\beta}=\\beta{\\bm{\\mu}}+{\\bm{\\mu}}^{\\perp} bold_italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT = italic_ bold_italic_ + bold_italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  that define the statistic of the synthetic data, as in equation  13 .",
            "Then, using lemma  A.3 , we have that:",
            "We have that, using the same lemma  A.3 :",
            "Using the same lemma  A.3 :",
            "Thus, using lemma  A.3 :",
            "Using the first identity in Sherman-Morissons lemma  A.3 , we have that the expressions of  R 1 subscript R 1 {\\mathbf{R}}_{1} bold_R start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and  R 2 subscript R 2 {\\mathbf{R}}_{2} bold_R start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  are given by:",
            "By Sherman-Morissons lemma  A.3 , we have that:",
            "By Sherman-Morissons lemma  A.3 , we have that:",
            "and these two quantities are obtained using corollary  A.6  and lemma  D.3 , which after simplification are given by:"
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "A6.EGx4",
        "footnotes": [],
        "references": [
            "In the remainder of the paper we take  l l \\ell roman_l  to be the regularized squared loss as it allows us to obtain a closed-form solution for the optimization problem in equation  4 , hence, a more tractable analysis. Specifically, we take  l  ( x , y ; w ) = ( w   x  y ) 2 +    w  2 l x y w superscript superscript w top x y 2  superscript norm w 2 \\ell({\\bm{x}},y;{\\bm{w}})=({\\bm{w}}^{\\top}{\\bm{x}}-y)^{2}+\\gamma\\|{\\bm{w}}\\|^{2} roman_l ( bold_italic_x , italic_y ; bold_italic_w ) = ( bold_italic_w start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT bold_italic_x - italic_y ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_  bold_italic_w  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  where    0  0 \\gamma\\geq 0 italic_  0  is a regularisation parameter, which yields the following closed-form solution",
            "We start by analyzing the general case of training on a mix of real and synthetic data. As we described in the previous section, the statistics of synthetic data are empirical estimates of the ones of real data. Under Assumption  4.1 , the estimation of    {\\bm{\\mu}} bold_italic_  with   ^ ^  \\hat{\\bm{\\mu}} over^ start_ARG bold_italic_ end_ARG  remains consistent, while the estimation of the underlying real data covariance (i.e.,  I p subscript I p {\\mathbf{I}}_{p} bold_I start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT  in our setting) with  C ^ ^ C \\hat{\\mathbf{C}} over^ start_ARG bold_C end_ARG  is inconsistent as we previously discussed. As a result, studying the theoretical performance of the classifier in equation  6  demands deploying tools from random matrix theory that refines the estimation of scalar quantities depending on large random matrices. In our case, the scalar quantity of interest corresponds to the models accuracy which depends on the random matrices  C ^ ^ C \\hat{\\mathbf{C}} over^ start_ARG bold_C end_ARG  and  XX  superscript XX top {\\mathbf{X}}{\\mathbf{X}}^{\\top} bold_XX start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  as per equation  6 .",
            "where   =   ( 1   ) +     italic- 1    \\alpha=\\phi(1-\\varepsilon)+\\rho\\varepsilon italic_ = italic_ ( 1 - italic_ ) + italic_ italic_ . These quantities will be used subsequently in our results. Intuitively,   r  superscript subscript  r \\delta_{r}^{*} italic_ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  captures the contribution of real data,   s  superscript subscript  s \\delta_{s}^{*} italic_ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  corresponds to the contribution of synthetic data, and   g  superscript subscript  g \\delta_{g}^{*} italic_ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  corresponds to the influence of the generative model. In an infinite sample size regime where  n , m , n ^    n m ^ n n,m,\\hat{n}\\to\\infty italic_n , italic_m , over^ start_ARG italic_n end_ARG    while the dimension  p p p italic_p  is kept fixed,  (  r  ,  s  ,  g  ) = ( 0 , 0 , 0 ) superscript subscript  r superscript subscript  s superscript subscript  g 0 0 0 (\\delta_{r}^{*},\\delta_{s}^{*},\\delta_{g}^{*})=(0,0,0) ( italic_ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , italic_ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , italic_ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) = ( 0 , 0 , 0 )  as per Fig.  2 , while under Assumption  4.1  these quantities are non zero yielding a counterintuitive behavior in high-dimension. For convenience, we further define a set of scalar quantities that will prove usefull in the next result.",
            "Let  w w {\\bm{w}} bold_italic_w  be the Ridge classifier as defined in equation  6  and suppose that Assumption  4.1  holds. The decision function  w   x superscript w top x {\\bm{w}}^{\\top}{\\bm{x}} bold_italic_w start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT bold_italic_x , on some (real) test sample  x  C a x subscript C a {\\bm{x}}\\in{\\mathcal{C}}_{a} bold_italic_x  caligraphic_C start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT , with corresponding label  y = (  1 ) a y superscript 1 a y=(-1)^{a} italic_y = ( - 1 ) start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT  and independent of  X X {\\mathbf{X}} bold_X , satisfies",
            "Theorem  4.2  states that the decision function of the classifier in equation  6  is asymptotically equivalent to the thresholding of two monovariate Gaussian random variables with respective means    \\mu italic_  and     -\\mu - italic_  and standard deviation    \\nu italic_ , where the statistics    \\mu italic_  and    \\nu italic_  are expressed in terms of the scalar quantities defined above. Here,    \\mu italic_  represents the signal strength while    \\nu italic_  highlights the classifiers uncertainty or dispersion. To provide some insights into the implications of this theorem, we start by examining it in a low-dimensional regime where  p p p italic_p  is kept fixed while  n , m , n ^    n m ^ n n,m,\\hat{n}\\to\\infty italic_n , italic_m , over^ start_ARG italic_n end_ARG   . In this case, we have   ,  ^  0   ^  0 \\eta,\\hat{\\eta}\\to 0 italic_ , over^ start_ARG italic_ end_ARG  0  and   r  ,  s  ,  g   0  superscript subscript  r superscript subscript  s superscript subscript  g 0 \\delta_{r}^{*},\\delta_{s}^{*},\\delta_{g}^{*}\\to 0 italic_ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , italic_ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , italic_ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  0  which yields",
            "Fig.  3  depicts the empirical test accuracy and the theoretical prediction as per Theorem  4.2  when varying the proportion of synthetic data. As theoretically anticipated, adding synthetic data does not boost the classifiers performance unless it is verified accurately (oracle supervision versus weak supervision). Moreover, our results show the effect of the distribution shift which heavily affects performance in the case of weak supervision (Fig.  3  right).",
            "In this section, we study the fully synthetic setting which corresponds to training solely on synthetic data (i.e.  n = 0 n 0 n=0 italic_n = 0  in equation  6 ). For simplicity, we consider only label noise and ignore feature noise in the synthetic data. Essentially, this allows us to exhibit the smooth phase transition of the classifiers accuracy in terms of label noise, which extends the result of  Feng et al. ( 2024 ) . Specifically, we obtain the following corollary of theorem  4.2 .",
            "Let  w s subscript w s {\\bm{w}}_{s} bold_italic_w start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  be the Ridge classifier described in equation  6  trained only on synthetic data with only label noise (i.e.,  C ^ = I p ^ C subscript I p \\hat{\\mathbf{C}}={\\mathbf{I}}_{p} over^ start_ARG bold_C end_ARG = bold_I start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ). Under Assumption  4.1 , the decision function  w s   x superscript subscript w s top x {\\bm{w}}_{s}^{\\top}{\\bm{x}} bold_italic_w start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT bold_italic_x  on a test sample  x  C a x subscript C a {\\bm{x}}\\in{\\mathcal{C}}_{a} bold_italic_x  caligraphic_C start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT  with corresponding label  y = (  1 ) a y superscript 1 a y=(-1)^{a} italic_y = ( - 1 ) start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT  and independent of  X X {\\mathbf{X}} bold_X , satisfies",
            "Corollary  4.3  provides an explicit formulation of Theorem  4.2  with synthetic data only and ignoring distribution shift (yielding an explicit expression of   s subscript  s \\delta_{s} italic_ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ). This setting provides a clearer interpretation of the effect of label noise since the classifiers performance is directly related to the quantity   =   ( 1   )      italic- 1    \\lambda=\\phi(1-\\varepsilon)-\\rho\\varepsilon italic_ = italic_ ( 1 - italic_ ) - italic_ italic_ . The breaking point of the classifiers performance occurs at   = 0  0 \\lambda=0 italic_ = 0 , which corresponds to the accuracy of random guessing, yielding to the critical value of label noise    = ( 1 +   )  1 superscript  superscript 1  italic- 1 \\varepsilon^{*}=(1+\\frac{\\rho}{\\phi})^{-1} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = ( 1 + divide start_ARG italic_ end_ARG start_ARG italic_ end_ARG ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT . This critical value is equivalent to the one obtained by  Feng et al. ( 2024 ) , however, we extend their result to the high-dimensional setting which exhibits a smoother phase transition as depicted in Fig.  4 . Essentially, the sharp phase transition of  Feng et al. ( 2024 )  is covered by our result by taking   s  0  subscript  s 0 \\eta_{s}\\to 0 italic_ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  0 . In this sense, the predicted smooth transition better mirrors real-world scenarios where finite sample sizes introduce gradual changes in performance rather than abrupt shifts. This makes our theoretical findings more applicable and reliable for practical scenarios.",
            "Under the  4.1  assumptions listed above in the main paper, a deterministic equivalent for  Q  Q  (  ) Q Q  {\\mathbf{Q}}\\equiv{\\mathbf{Q}}(\\gamma) bold_Q  bold_Q ( italic_ ) , denoted  Q     Q \\bar{\\mathbf{Q}} over  start_ARG bold_Q end_ARG , is given by:",
            "Let us recall the expression of  Q     Q \\bar{\\mathbf{Q}} over  start_ARG bold_Q end_ARG  defined in lemma  A.4 :",
            "since     = O  ( N  1 ) norm  O superscript N 1 \\|{\\bm{\\mu}}\\|={\\mathcal{O}}(N^{-1})  bold_italic_  = caligraphic_O ( italic_N start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT )  by assumption  4.1 . The same applies for    subscript   {\\bm{\\mu}}_{\\beta} bold_italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT . Thus:",
            "Hence we have the desired result for    \\delta italic_  in the regime  N  1 much-greater-than N 1 N\\gg 1 italic_N  1  which we considered in our assumption  4.1 .  Similarly for   S subscript  S \\delta_{S} italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT , we have that:",
            "Let  w q subscript w q {\\bm{w}}_{q} bold_italic_w start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT  be the Mixed classifier as defined in equation  6  and suppose that Assumption  4.1  holds. The decision function  w q   x superscript subscript w q top x {\\bm{w}}_{q}^{\\top}{\\bm{x}} bold_italic_w start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT bold_italic_x , on some test sample  x  C a x subscript C a {\\bm{x}}\\in{\\mathcal{C}}_{a} bold_italic_x  caligraphic_C start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT  independent of  X X {\\mathbf{X}} bold_X , satisfies:",
            "We have that by lemma  A.4 :",
            "Let  w q subscript w q {\\bm{w}}_{q} bold_italic_w start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT  be the Mixed classifier as defined in equation  6  and suppose that Assumption  4.1  holds. The decision function  w q   x superscript subscript w q top x {\\bm{w}}_{q}^{\\top}{\\bm{x}} bold_italic_w start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT bold_italic_x , on some test sample  x  C a x subscript C a {\\bm{x}}\\in{\\mathcal{C}}_{a} bold_italic_x  caligraphic_C start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT  independent of  X X {\\mathbf{X}} bold_X , satisfies:",
            "We will now quantify the performance of the classifier obtained through mixing some real data and synthetic data sampled according to the schema described in  2 . Hence, the matrix  Q     Q \\bar{\\mathbf{Q}} over  start_ARG bold_Q end_ARG , defined in lemma  A.4 , is no longer deterministic as we take the covariance matrix  C ^ = 1 n ^   i = 1 n ^ ( x i  y i   ^ )  ( x i  y i   ^ )  ^ C 1 ^ n superscript subscript i 1 ^ n subscript x i subscript y i ^  superscript subscript x i subscript y i ^  top \\hat{{\\mathbf{C}}}=\\frac{1}{\\hat{n}}\\sum_{i=1}^{\\hat{n}}({\\bm{x}}_{i}-y_{i}% \\hat{{\\bm{\\mu}}})({\\bm{x}}_{i}-y_{i}\\hat{{\\bm{\\mu}}})^{\\top} over^ start_ARG bold_C end_ARG = divide start_ARG 1 end_ARG start_ARG over^ start_ARG italic_n end_ARG end_ARG  start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT over^ start_ARG italic_n end_ARG end_POSTSUPERSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT over^ start_ARG bold_italic_ end_ARG ) ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT over^ start_ARG bold_italic_ end_ARG ) start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT . For simplicity, and without loss of generality, we consider  n ^ ^ n \\hat{n} over^ start_ARG italic_n end_ARG  Gaussian vectors  ( z i ) i = 1 n ^  N  ( 0 , I p ) similar-to superscript subscript subscript z i i 1 ^ n N 0 subscript I p ({\\bm{z}}_{i})_{i=1}^{\\hat{n}}\\sim{\\mathcal{N}}(0,{\\mathbf{I}}_{p}) ( bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT over^ start_ARG italic_n end_ARG end_POSTSUPERSCRIPT  caligraphic_N ( 0 , bold_I start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT )  that are independent of  ( x i ) i = 1 n ^ superscript subscript subscript x i i 1 ^ n ({\\bm{x}}_{i})_{i=1}^{\\hat{n}} ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT over^ start_ARG italic_n end_ARG end_POSTSUPERSCRIPT , and write:",
            "The resolvent matrix to be considered in this setting is the one defined in lemma  A.4  but with  C ^ ^ C \\hat{{\\mathbf{C}}} over^ start_ARG bold_C end_ARG :",
            "which concludes the proof of the main theorem  4.2  of this paper."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "A6.EGx5",
        "footnotes": [],
        "references": [
            "We also conducted experiments on the MNIST ( LeCun & Cortes ( 2010 ) ) dataset to illustrate our theoretical insights, by training a simple neural network with one-hidden layer and ReLU activation function. Concerning the synthetic data, we used different values of  n ^ ^ n \\hat{n} over^ start_ARG italic_n end_ARG  to generate new samples in order to highlight the importance of the generation quality, and introduced a label noise    \\varepsilon italic_  to emphasize on the importance of the pruning. Figure  5  shows some examples of MNIST-like synthetic data that has been generated and used in our experiments.",
            "Using the above lemma  A.5 , we get that:",
            "We also have that the constants  a 1 , a 2 , b 1 subscript a 1 subscript a 2 subscript b 1 a_{1},a_{2},b_{1} italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and  b 2 subscript b 2 b_{2} italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  from lemma  A.5  become by taking their expectations on  z z {\\bm{z}} bold_italic_z :"
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "A6.EGx6",
        "footnotes": [],
        "references": [
            "which control the pruner accuracy (as discussed in  (Feng et al.,  2024 ) ). As we mentioned previously, we suppose training on  n  n ^ n ^ n n\\geq\\hat{n} italic_n  over^ start_ARG italic_n end_ARG  real data, modeling a situation where new real samples are available with  n ^ ^ n \\hat{n} over^ start_ARG italic_n end_ARG  controlling the generative model quality in generating faithful synthetic features 1 1 1 Technically, our results hold irrespective of the statistical dependencies between the data used to train the generative model in equation  2  or the classifier in equation  6 . .",
            "In this section, we present and discuss the main results obtained through the analysis of the classifier model defined in equation  6 . We start by specifying the supposed growth rate assumptions.",
            "We start by analyzing the general case of training on a mix of real and synthetic data. As we described in the previous section, the statistics of synthetic data are empirical estimates of the ones of real data. Under Assumption  4.1 , the estimation of    {\\bm{\\mu}} bold_italic_  with   ^ ^  \\hat{\\bm{\\mu}} over^ start_ARG bold_italic_ end_ARG  remains consistent, while the estimation of the underlying real data covariance (i.e.,  I p subscript I p {\\mathbf{I}}_{p} bold_I start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT  in our setting) with  C ^ ^ C \\hat{\\mathbf{C}} over^ start_ARG bold_C end_ARG  is inconsistent as we previously discussed. As a result, studying the theoretical performance of the classifier in equation  6  demands deploying tools from random matrix theory that refines the estimation of scalar quantities depending on large random matrices. In our case, the scalar quantity of interest corresponds to the models accuracy which depends on the random matrices  C ^ ^ C \\hat{\\mathbf{C}} over^ start_ARG bold_C end_ARG  and  XX  superscript XX top {\\mathbf{X}}{\\mathbf{X}}^{\\top} bold_XX start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  as per equation  6 .",
            "Let  w w {\\bm{w}} bold_italic_w  be the Ridge classifier as defined in equation  6  and suppose that Assumption  4.1  holds. The decision function  w   x superscript w top x {\\bm{w}}^{\\top}{\\bm{x}} bold_italic_w start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT bold_italic_x , on some (real) test sample  x  C a x subscript C a {\\bm{x}}\\in{\\mathcal{C}}_{a} bold_italic_x  caligraphic_C start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT , with corresponding label  y = (  1 ) a y superscript 1 a y=(-1)^{a} italic_y = ( - 1 ) start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT  and independent of  X X {\\mathbf{X}} bold_X , satisfies",
            "Theorem  4.2  states that the decision function of the classifier in equation  6  is asymptotically equivalent to the thresholding of two monovariate Gaussian random variables with respective means    \\mu italic_  and     -\\mu - italic_  and standard deviation    \\nu italic_ , where the statistics    \\mu italic_  and    \\nu italic_  are expressed in terms of the scalar quantities defined above. Here,    \\mu italic_  represents the signal strength while    \\nu italic_  highlights the classifiers uncertainty or dispersion. To provide some insights into the implications of this theorem, we start by examining it in a low-dimensional regime where  p p p italic_p  is kept fixed while  n , m , n ^    n m ^ n n,m,\\hat{n}\\to\\infty italic_n , italic_m , over^ start_ARG italic_n end_ARG   . In this case, we have   ,  ^  0   ^  0 \\eta,\\hat{\\eta}\\to 0 italic_ , over^ start_ARG italic_ end_ARG  0  and   r  ,  s  ,  g   0  superscript subscript  r superscript subscript  s superscript subscript  g 0 \\delta_{r}^{*},\\delta_{s}^{*},\\delta_{g}^{*}\\to 0 italic_ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , italic_ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , italic_ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  0  which yields",
            "In this section, we study the fully synthetic setting which corresponds to training solely on synthetic data (i.e.  n = 0 n 0 n=0 italic_n = 0  in equation  6 ). For simplicity, we consider only label noise and ignore feature noise in the synthetic data. Essentially, this allows us to exhibit the smooth phase transition of the classifiers accuracy in terms of label noise, which extends the result of  Feng et al. ( 2024 ) . Specifically, we obtain the following corollary of theorem  4.2 .",
            "Let  w s subscript w s {\\bm{w}}_{s} bold_italic_w start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  be the Ridge classifier described in equation  6  trained only on synthetic data with only label noise (i.e.,  C ^ = I p ^ C subscript I p \\hat{\\mathbf{C}}={\\mathbf{I}}_{p} over^ start_ARG bold_C end_ARG = bold_I start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ). Under Assumption  4.1 , the decision function  w s   x superscript subscript w s top x {\\bm{w}}_{s}^{\\top}{\\bm{x}} bold_italic_w start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT bold_italic_x  on a test sample  x  C a x subscript C a {\\bm{x}}\\in{\\mathcal{C}}_{a} bold_italic_x  caligraphic_C start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT  with corresponding label  y = (  1 ) a y superscript 1 a y=(-1)^{a} italic_y = ( - 1 ) start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT  and independent of  X X {\\mathbf{X}} bold_X , satisfies",
            "We use the Amazon Reviews datasets ( Blitzer et al. ( 2007 ) ) which include several binary classification tasks corresponding to positive versus negative reviews of  books ,  electronics  and  kitchen . We apply the standard scaler from  sklearn   (Pedregosa et al.,  2011 )  and estimate     norm  \\|{\\bm{\\mu}}\\|  bold_italic_   with the normalized data. The synthetic data is generated following the described generative scheme (see equation  2 ). We use the Ridge classifier in equation  6  for this data.",
            "Figures  6 ,  7  (left plot)  8  reflect the effect of label noise. Essentially, as theoretically anticipated, the trained models do not benefit from synthetic data unless it is accurately verified. Specifically, in the case of weak supervision, model performance drops significantly, and the improvement from using synthetic data is only visible with very high synthetic sample sizes. On the contrary, with strong supervision, we observe a monotonous performance boost as the proportion of synthetic data increases.",
            "Let us state here the deterministic equivalent of the resolvent matrix  Q Q {\\mathbf{Q}} bold_Q  defined in the general models equation ( 6 ) for any general covariance matrix  C C {\\mathbf{C}} bold_C  and mean    =    +   subscript     superscript  perpendicular-to {\\bm{\\mu}}_{\\beta}=\\beta{\\bm{\\mu}}+{\\bm{\\mu}}^{\\perp} bold_italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT = italic_ bold_italic_ + bold_italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  that define the statistic of the synthetic data, as in equation  13 .",
            "Let  Q Q {\\mathbf{Q}} bold_Q  be the resolvent matrix defined in equation ( 6 ). Denote by  Q  v i subscript Q subscript v i {\\mathbf{Q}}_{-{\\bm{v}}_{i}} bold_Q start_POSTSUBSCRIPT - bold_italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT  the resolvent matrix obtained from the dataset  V V {\\mathbf{V}} bold_V  by removing the  i t  h superscript i t h i^{th} italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT  sample  v i subscript v i {\\bm{v}}_{i} bold_italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , i.e:",
            "where    R  R \\beta\\in{\\mathbb{R}} italic_  blackboard_R  defines the alignment of the synthetic mean with the mean of real data, and    superscript  perpendicular-to {\\bm{\\mu}}^{\\perp} bold_italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is a vector orthogonal to    {\\bm{\\mu}} bold_italic_ .  Now we will analyze here the performance of the classifier given by equation ( 6 ), and prove a generalized theorem  B.1  in the paper.",
            "The performance of ( 6 ) are fully determined by the first two order moments:  E  [ w q   x ] E delimited-[] superscript subscript w q top x \\mathbb{E}[{\\bm{w}}_{q}^{\\top}{\\bm{x}}] blackboard_E [ bold_italic_w start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT bold_italic_x ]  and  E  [ ( w q   x ) 2 ] E delimited-[] superscript superscript subscript w q top x 2 \\mathbb{E}[({\\bm{w}}_{q}^{\\top}{\\bm{x}})^{2}] blackboard_E [ ( bold_italic_w start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT bold_italic_x ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] .",
            "Let  w q subscript w q {\\bm{w}}_{q} bold_italic_w start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT  be the Mixed classifier as defined in equation  6  and suppose that Assumption  4.1  holds. The decision function  w q   x superscript subscript w q top x {\\bm{w}}_{q}^{\\top}{\\bm{x}} bold_italic_w start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT bold_italic_x , on some test sample  x  C a x subscript C a {\\bm{x}}\\in{\\mathcal{C}}_{a} bold_italic_x  caligraphic_C start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT  independent of  X X {\\mathbf{X}} bold_X , satisfies:",
            "The performance of  w q subscript w q {\\bm{w}}_{q} bold_italic_w start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT  in ( 6 ) is fully determined by the first two order moments:  E  [ w q   x ] E delimited-[] superscript subscript w q top x \\mathbb{E}[{\\bm{w}}_{q}^{\\top}{\\bm{x}}] blackboard_E [ bold_italic_w start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT bold_italic_x ]  and  E  [ ( w q   x ) 2 ] E delimited-[] superscript superscript subscript w q top x 2 \\mathbb{E}[({\\bm{w}}_{q}^{\\top}{\\bm{x}})^{2}] blackboard_E [ ( bold_italic_w start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT bold_italic_x ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] .",
            "Let  w q subscript w q {\\bm{w}}_{q} bold_italic_w start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT  be the Mixed classifier as defined in equation  6  and suppose that Assumption  4.1  holds. The decision function  w q   x superscript subscript w q top x {\\bm{w}}_{q}^{\\top}{\\bm{x}} bold_italic_w start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT bold_italic_x , on some test sample  x  C a x subscript C a {\\bm{x}}\\in{\\mathcal{C}}_{a} bold_italic_x  caligraphic_C start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT  independent of  X X {\\mathbf{X}} bold_X , satisfies:",
            "and these two quantities are obtained using corollary  A.6  and lemma  D.3 , which after simplification are given by:"
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "A6.EGx7",
        "footnotes": [],
        "references": [
            "Figures  6 ,  7  (left plot)  8  reflect the effect of label noise. Essentially, as theoretically anticipated, the trained models do not benefit from synthetic data unless it is accurately verified. Specifically, in the case of weak supervision, model performance drops significantly, and the improvement from using synthetic data is only visible with very high synthetic sample sizes. On the contrary, with strong supervision, we observe a monotonous performance boost as the proportion of synthetic data increases.",
            "In this section, we discuss the experiments related to feature noise. In Fig.  7  (right), we depict the performance of a one-hidden layer MLP trained on a mix of real and synthetic MNIST data following our theoretical framework. As we can observe from the figure, the performance boost from synthetic data heavily depends on the generative model quality as predicted by our theoretical results. We further observe the same trend using LLMs as depicted in Fig.  9 , where we observe that the synthetic data generated by  Llama3.1-8B-Instruct  yields a better performance boost compared to  Gemma-2-2B-it  as we increase the amount of synthetic samples."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "A6.EGx8",
        "footnotes": [],
        "references": [
            "For the evaluation, we use the ALERT dataset 5 5 5 https://github.com/Babelscape/ALERT/blob/master/data/alert.jsonl  ( Tedeschi et al. ( 2024 ) ) to test the safety of responses of the finetuned model after being judged by  LLama-Guard-3-8B   (Dubey et al.,  2024 ) . As in  (Alami et al.,  2024 ) , we compute the safety score as the percentage of safe answers labeled by  Llama-Guard-3-8B . We report the results in figure  8  for strong supervision  (  ,  ) = ( 0.2 , 0.9 )  italic- 0.2 0.9 (\\rho,\\phi)=(0.2,0.9) ( italic_ , italic_ ) = ( 0.2 , 0.9 )  and weak supervision  (  ,  ) = ( 0.5 , 0.5 )  italic- 0.5 0.5 (\\rho,\\phi)=(0.5,0.5) ( italic_ , italic_ ) = ( 0.5 , 0.5 )  for both   = 0.1  0.1 \\varepsilon=0.1 italic_ = 0.1  and   = 0.5  0.5 \\varepsilon=0.5 italic_ = 0.5 .",
            "Figures  6 ,  7  (left plot)  8  reflect the effect of label noise. Essentially, as theoretically anticipated, the trained models do not benefit from synthetic data unless it is accurately verified. Specifically, in the case of weak supervision, model performance drops significantly, and the improvement from using synthetic data is only visible with very high synthetic sample sizes. On the contrary, with strong supervision, we observe a monotonous performance boost as the proportion of synthetic data increases."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "A6.EGx9",
        "footnotes": [],
        "references": [
            "In this section, we discuss the experiments related to feature noise. In Fig.  7  (right), we depict the performance of a one-hidden layer MLP trained on a mix of real and synthetic MNIST data following our theoretical framework. As we can observe from the figure, the performance boost from synthetic data heavily depends on the generative model quality as predicted by our theoretical results. We further observe the same trend using LLMs as depicted in Fig.  9 , where we observe that the synthetic data generated by  Llama3.1-8B-Instruct  yields a better performance boost compared to  Gemma-2-2B-it  as we increase the amount of synthetic samples.",
            "The parameters    \\delta italic_  and   S subscript  S \\delta_{S} italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  defined in equation  9 , are given by the following identities:",
            "The parameter    \\delta italic_  as defined in equation  9 , is given by the following identity:"
        ]
    },
    "id_table_10": {
        "caption": "",
        "table": "A6.EGx10",
        "footnotes": [],
        "references": [
            "As part of this experiment, we had to generate a synthetic QA Dataset. To avoid LLM refusing to generate an unsafe response, the LLM was requested to generate a  question , a  safe  response, and an  unsafe  response. Figure  10  shows the system prompt used to request from an LLM to generate QA.  < < < Topic > > >  is a placeholder referring to a particular risk topic, selected from the list of topics seen in Figure  11 , the section written in red. As discussed in the paper, the generated QA will be annotated by LLM, using the prompt presented in Figure  11 ."
        ]
    },
    "id_table_11": {
        "caption": "",
        "table": "A6.EGx11",
        "footnotes": [],
        "references": [
            "As part of this experiment, we had to generate a synthetic QA Dataset. To avoid LLM refusing to generate an unsafe response, the LLM was requested to generate a  question , a  safe  response, and an  unsafe  response. Figure  10  shows the system prompt used to request from an LLM to generate QA.  < < < Topic > > >  is a placeholder referring to a particular risk topic, selected from the list of topics seen in Figure  11 , the section written in red. As discussed in the paper, the generated QA will be annotated by LLM, using the prompt presented in Figure  11 ."
        ]
    },
    "id_table_12": {
        "caption": "",
        "table": "A6.EGx12",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "Let:  M = ( I 2 + U   A  1  U )  1 M superscript subscript I 2 superscript U top superscript A 1 U 1 {\\mathbf{M}}=\\left({\\mathbf{I}}_{2}+{\\mathbf{U}}^{\\top}{\\mathbf{A}}^{-1}{% \\mathbf{U}}\\right)^{-1} bold_M = ( bold_I start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + bold_U start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT bold_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_U ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT , and denote by  M i , j subscript M i j M_{i,j} italic_M start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT  its coordinate in the  i th superscript i th i^{\\text{th}} italic_i start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT  row and  j th superscript j th j^{\\text{th}} italic_j start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT  column. We have that using the expression of  Q     Q \\bar{\\mathbf{Q}} over  start_ARG bold_Q end_ARG  in equation  12 :"
        ]
    },
    "id_table_13": {
        "caption": "",
        "table": "A6.EGx13",
        "footnotes": [],
        "references": [
            "Let us state here the deterministic equivalent of the resolvent matrix  Q Q {\\mathbf{Q}} bold_Q  defined in the general models equation ( 6 ) for any general covariance matrix  C C {\\mathbf{C}} bold_C  and mean    =    +   subscript     superscript  perpendicular-to {\\bm{\\mu}}_{\\beta}=\\beta{\\bm{\\mu}}+{\\bm{\\mu}}^{\\perp} bold_italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT = italic_ bold_italic_ + bold_italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  that define the statistic of the synthetic data, as in equation  13 ."
        ]
    },
    "id_table_14": {
        "caption": "",
        "table": "A6.EGx14",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "",
        "table": "A6.EGx15",
        "footnotes": [],
        "references": []
    },
    "id_table_16": {
        "caption": "",
        "table": "A6.EGx16",
        "footnotes": [],
        "references": []
    },
    "id_table_17": {
        "caption": "",
        "table": "A6.EGx17",
        "footnotes": [],
        "references": []
    },
    "id_table_18": {
        "caption": "",
        "table": "A6.EGx18",
        "footnotes": [],
        "references": []
    },
    "id_table_19": {
        "caption": "",
        "table": "A6.EGx19",
        "footnotes": [],
        "references": []
    },
    "id_table_20": {
        "caption": "",
        "table": "A6.EGx20",
        "footnotes": [],
        "references": []
    },
    "id_table_21": {
        "caption": "",
        "table": "A6.EGx21",
        "footnotes": [],
        "references": []
    },
    "id_table_22": {
        "caption": "",
        "table": "A6.EGx22",
        "footnotes": [],
        "references": []
    },
    "id_table_23": {
        "caption": "",
        "table": "A6.EGx23",
        "footnotes": [],
        "references": []
    },
    "id_table_24": {
        "caption": "",
        "table": "A6.EGx24",
        "footnotes": [],
        "references": []
    },
    "id_table_25": {
        "caption": "",
        "table": "A6.EGx25",
        "footnotes": [],
        "references": []
    },
    "id_table_26": {
        "caption": "",
        "table": "A6.EGx26",
        "footnotes": [],
        "references": []
    },
    "id_table_27": {
        "caption": "",
        "table": "A6.EGx27",
        "footnotes": [],
        "references": []
    },
    "id_table_28": {
        "caption": "",
        "table": "A6.EGx28",
        "footnotes": [],
        "references": []
    },
    "id_table_29": {
        "caption": "",
        "table": "A6.EGx29",
        "footnotes": [],
        "references": []
    },
    "id_table_30": {
        "caption": "",
        "table": "A6.EGx30",
        "footnotes": [],
        "references": []
    },
    "id_table_31": {
        "caption": "",
        "table": "A6.EGx31",
        "footnotes": [],
        "references": []
    },
    "id_table_32": {
        "caption": "",
        "table": "A6.EGx32",
        "footnotes": [],
        "references": []
    },
    "id_table_33": {
        "caption": "",
        "table": "A6.EGx33",
        "footnotes": [],
        "references": []
    },
    "id_table_34": {
        "caption": "",
        "table": "A6.EGx34",
        "footnotes": [],
        "references": []
    },
    "id_table_35": {
        "caption": "",
        "table": "A6.EGx35",
        "footnotes": [],
        "references": []
    },
    "id_table_36": {
        "caption": "",
        "table": "A6.EGx36",
        "footnotes": [],
        "references": []
    },
    "id_table_37": {
        "caption": "",
        "table": "A6.EGx37",
        "footnotes": [],
        "references": []
    },
    "id_table_38": {
        "caption": "",
        "table": "A6.EGx38",
        "footnotes": [],
        "references": []
    },
    "id_table_39": {
        "caption": "",
        "table": "A6.EGx39",
        "footnotes": [],
        "references": []
    },
    "id_table_40": {
        "caption": "",
        "table": "A6.EGx40",
        "footnotes": [],
        "references": []
    },
    "id_table_41": {
        "caption": "",
        "table": "A6.EGx41",
        "footnotes": [],
        "references": []
    },
    "id_table_42": {
        "caption": "",
        "table": "A6.EGx42",
        "footnotes": [],
        "references": []
    },
    "id_table_43": {
        "caption": "",
        "table": "A6.EGx43",
        "footnotes": [],
        "references": []
    },
    "id_table_44": {
        "caption": "",
        "table": "A6.EGx44",
        "footnotes": [],
        "references": []
    },
    "id_table_45": {
        "caption": "",
        "table": "A6.EGx45",
        "footnotes": [],
        "references": []
    },
    "id_table_46": {
        "caption": "",
        "table": "A6.EGx46",
        "footnotes": [],
        "references": []
    },
    "id_table_47": {
        "caption": "",
        "table": "A6.EGx47",
        "footnotes": [],
        "references": []
    },
    "id_table_48": {
        "caption": "",
        "table": "A6.EGx48",
        "footnotes": [],
        "references": []
    },
    "id_table_49": {
        "caption": "",
        "table": "A6.EGx49",
        "footnotes": [],
        "references": []
    },
    "id_table_50": {
        "caption": "",
        "table": "A6.EGx50",
        "footnotes": [],
        "references": []
    },
    "id_table_51": {
        "caption": "",
        "table": "A6.EGx51",
        "footnotes": [],
        "references": []
    },
    "id_table_52": {
        "caption": "",
        "table": "A6.EGx52",
        "footnotes": [],
        "references": []
    },
    "id_table_53": {
        "caption": "",
        "table": "A6.EGx53",
        "footnotes": [],
        "references": []
    },
    "id_table_54": {
        "caption": "",
        "table": "A6.EGx54",
        "footnotes": [],
        "references": []
    },
    "id_table_55": {
        "caption": "",
        "table": "A6.EGx55",
        "footnotes": [],
        "references": []
    },
    "id_table_56": {
        "caption": "",
        "table": "A6.EGx56",
        "footnotes": [],
        "references": []
    },
    "id_table_57": {
        "caption": "",
        "table": "A6.EGx57",
        "footnotes": [],
        "references": []
    },
    "id_table_58": {
        "caption": "",
        "table": "A6.EGx58",
        "footnotes": [],
        "references": []
    },
    "id_table_59": {
        "caption": "",
        "table": "A6.EGx59",
        "footnotes": [],
        "references": []
    },
    "id_table_60": {
        "caption": "",
        "table": "A6.EGx60",
        "footnotes": [],
        "references": []
    },
    "id_table_61": {
        "caption": "",
        "table": "A6.EGx61",
        "footnotes": [],
        "references": []
    },
    "id_table_62": {
        "caption": "",
        "table": "A6.EGx62",
        "footnotes": [],
        "references": []
    },
    "id_table_63": {
        "caption": "",
        "table": "A6.EGx63",
        "footnotes": [],
        "references": []
    },
    "id_table_64": {
        "caption": "",
        "table": "A6.EGx64",
        "footnotes": [],
        "references": []
    },
    "id_table_65": {
        "caption": "",
        "table": "A6.EGx65",
        "footnotes": [],
        "references": []
    },
    "id_table_66": {
        "caption": "",
        "table": "A6.EGx66",
        "footnotes": [],
        "references": []
    },
    "id_table_67": {
        "caption": "",
        "table": "A6.EGx67",
        "footnotes": [],
        "references": []
    },
    "id_table_68": {
        "caption": "",
        "table": "A6.EGx68",
        "footnotes": [],
        "references": []
    },
    "id_table_69": {
        "caption": "",
        "table": "A6.EGx69",
        "footnotes": [],
        "references": []
    },
    "id_table_70": {
        "caption": "",
        "table": "A6.EGx70",
        "footnotes": [],
        "references": []
    },
    "id_table_71": {
        "caption": "",
        "table": "A6.EGx71",
        "footnotes": [],
        "references": []
    },
    "id_table_72": {
        "caption": "",
        "table": "A6.EGx72",
        "footnotes": [],
        "references": []
    },
    "id_table_73": {
        "caption": "",
        "table": "A6.EGx73",
        "footnotes": [],
        "references": []
    },
    "id_table_74": {
        "caption": "",
        "table": "A6.EGx74",
        "footnotes": [],
        "references": []
    },
    "id_table_75": {
        "caption": "",
        "table": "A6.EGx75",
        "footnotes": [],
        "references": []
    },
    "id_table_76": {
        "caption": "",
        "table": "A6.EGx76",
        "footnotes": [],
        "references": []
    },
    "id_table_77": {
        "caption": "",
        "table": "A6.EGx77",
        "footnotes": [],
        "references": []
    },
    "id_table_78": {
        "caption": "",
        "table": "A6.EGx78",
        "footnotes": [],
        "references": []
    },
    "id_table_79": {
        "caption": "",
        "table": "A6.EGx79",
        "footnotes": [],
        "references": []
    },
    "id_table_80": {
        "caption": "",
        "table": "A6.EGx80",
        "footnotes": [],
        "references": []
    },
    "id_table_81": {
        "caption": "",
        "table": "A6.EGx81",
        "footnotes": [],
        "references": []
    },
    "id_table_82": {
        "caption": "",
        "table": "A6.EGx82",
        "footnotes": [],
        "references": []
    },
    "id_table_83": {
        "caption": "",
        "table": "A6.EGx83",
        "footnotes": [],
        "references": []
    },
    "id_table_84": {
        "caption": "",
        "table": "A6.EGx84",
        "footnotes": [],
        "references": []
    },
    "id_table_85": {
        "caption": "",
        "table": "A6.EGx85",
        "footnotes": [],
        "references": []
    },
    "id_table_86": {
        "caption": "",
        "table": "A6.EGx86",
        "footnotes": [],
        "references": []
    },
    "id_table_87": {
        "caption": "",
        "table": "A6.EGx87",
        "footnotes": [],
        "references": []
    },
    "id_table_88": {
        "caption": "",
        "table": "A5.T1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_89": {
        "caption": "",
        "table": "A6.T2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_90": {
        "caption": "",
        "table": "A6.T3.1",
        "footnotes": [],
        "references": []
    }
}