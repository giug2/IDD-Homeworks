{
    "id_table_1": {
        "caption": "Table 1:  Objective evaluation comparison on  RainDrop  test set in terms of five no-reference IQA (NR-IQA) metrics. The best results are in  bold  and the second bests are with  underline .",
        "table": "S4.T1.6",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "The training process of WaveDiff is outlined in algorithm  1 , wherein the denoising process is illustrated in Fig.  1 . Specifically, the model comprises a generator and a discriminator, operating on images in the wavelet domain. Given the input  y y y italic_y , it is first transformed into  y 0 subscript y 0 y_{0} italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  through wavelet transformation and then fed into the model for training. The generator is used to predict the denoised output of  y t  1  superscript subscript y t 1  y_{t-1}^{\\prime} italic_y start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT . But instead of directly outputting  y t  1  superscript subscript y t 1  y_{t-1}^{\\prime} italic_y start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  in the denoising step, the generator first predicts the  y 0  superscript subscript y 0  y_{0}^{\\prime} italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  from the noise-corrupted version of the data  y t  q  ( y t  y 0 ) similar-to subscript y t q conditional subscript y t subscript y 0 y_{t}\\sim q(y_{t}\\mid y_{0}) italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  italic_q ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )  at time step t, and then using the posterior distribution:  y t  1   q  ( y t  1   y t , y 0  ) similar-to superscript subscript y t 1  q conditional superscript subscript y t 1  subscript y t superscript subscript y 0  y_{t-1}^{\\prime}\\sim q(y_{t-1}^{\\prime}\\mid y_{t},y_{0}^{\\prime}) italic_y start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  italic_q ( italic_y start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT )  to get the denoised output  y t  1  superscript subscript y t 1  y_{t-1}^{\\prime} italic_y start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT . So it can implicitly model the denoising distribution. The discriminator is used to determine the authenticity of the denoised outputs and optimizes both the generator and the discriminator in an adversarial training manner. Moreover, the introduction of wavelets enrich the frequency-aware at both image and feature levels, which will facilitate the perceptual quality of the generated images  [ 8 ] . Inspired by this model, we customize a wavelet diffusion model, built upon WaveDiff, to serve as the backbone of SemiDDM-Weather by modifying its inputs and loss functions to better adapt to our adverse weather removal task.",
            "This section detailedly describes the design of our SemiDDM-Weather. Section  3.1  outlines the proposed semi-supervised adverse weather removal framework, i.e., SemiDDM-Weather. How we customize a wavelet diffusion model to serve as the backbone of SemiDDM-Weather, is provided in Section  3.2 . The targeted design for improving the accuracy of pseudo-labels under the teacher-student based network is detailed in Section  3.3 . Training and inference details are presented in Section  3.4 .",
            "(2)  Objective Evaluation : Considering that only using subjective evaluation is not comprehensive, we also introduce objective evaluation to evaluate the performance of our method. Objective image quality assessment (IQA) methods can be categorized as full-reference, reduced-reference, and no-reference. Since it is difficult to obtain realistic GT images under adverse weather conditions, and our primary aim is to recover perceptually clearer images, GT images are no longer suitable as a reference. Consequently, the corresponding FR-IQA metrics, such as PSNR and SSIM, are not applicable. Instead, we adopt NR-IQA metrics for image quality evaluation. In this paper, five top-performance metrics are employed for objective evaluation, including PAQ2PIQ  [ 50 ] , DBCNN  [ 51 ] , CLIPIQA  [ 52 ] , MUSIQ-SPAQ  [ 37 ] , and NIQE  [ 53 ] . In Tables  1 ,  2 , and  3 , we compare our approach to other fully-supervised approaches, including weather-specific and all-in-one approaches, for these five NR-IQA metrics. It is clear that our method, which employs a semi-supervised learning framework trained with limited labeled data, significantly outperforms other methods overall, even over GT, affirming that the restoration results from our method are visually high-quality.",
            "To further validate the practicality of our method, we extend our evaluation to real-world data and include the current advanced methods, e.g., TransWeather, WGWS-Net, and  WeatherDiff 64 subscript WeatherDiff 64 \\text{WeatherDiff}_{64} WeatherDiff start_POSTSUBSCRIPT 64 end_POSTSUBSCRIPT , for comparison. Figs.  9  and   10  illustrate the deraining instance on the  IVIPC-DQA  test set and the desnowing instance on the  Snow100K-Real  test set, respectively. It is observed that our method not only achieves superior degradation removal but also enhances image clarity, confirming the superiority of our method over other existing methods, such as clearer lawn and sharper tree textures. Tables  4  and   5  show the objective evaluation results on the  IVIPC-DQA  test set and the  Snow100K-Real  test set, respectively. Similarly, it can be observed that our method achieves the best results."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Objective Evaluation comparison on  Outdoor-rain  test set in terms of five no-reference IQA (NR-IQA) metrics. The best results are in  bold  and the second bests are with  underline .",
        "table": "S4.T2.6",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "This section detailedly describes the design of our SemiDDM-Weather. Section  3.1  outlines the proposed semi-supervised adverse weather removal framework, i.e., SemiDDM-Weather. How we customize a wavelet diffusion model to serve as the backbone of SemiDDM-Weather, is provided in Section  3.2 . The targeted design for improving the accuracy of pseudo-labels under the teacher-student based network is detailed in Section  3.3 . Training and inference details are presented in Section  3.4 .",
            "To achieve the removal of various adverse weather conditions with limited labeled data, inspired by study in  [ 36 ] , we adopt a teacher-student network 1 1 1 In this paper, for differentiation, we use the subscripts  tn  and  sn  to indicate the teacher and student networks, respectively.  as the foundation for semi-supervised learning. As depicted in Fig.  2 , the teacher and student networks share the same structure network with a customized wavelet diffusion model as the backbone to effectively capture the complex many-to-one mapping distributions. The teacher network is responsible for generating pseudo-labels for unlabeled images, which are subsequently utilized to guide the training of the student network. Its weights   tn subscript  tn \\theta_{\\text{tn}} italic_ start_POSTSUBSCRIPT tn end_POSTSUBSCRIPT  are updated from the student network via EMA strategy, i.e.,",
            "As to evaluating whether the teacher networks current output is of the best quality, we employ two criteria. 1) An NR-IQA metric, MUSIQ  [ 37 ] , is introduced to evaluate the visual quality. The quality of the teacher networks current output is proportional to the score of MUSIQ, i.e., the better the quality, the higher the score. As shown in line 2 of Algorithm  2 ,  Q  ( x ) Q x \\textsc{Q}({x}) Q ( italic_x )  represents the MUSIQ score of input  x x x italic_x . 2) A consistency constraint is incorporated to evaluate the quality of teachers outputs. As shown in line 2 of Algorithm  2 , the  L 1 subscript L 1 \\mathcal{L}_{{1}} caligraphic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  distance between the outputs of the teacher and the student networks is required to be less than a predefined threshold    \\varphi italic_ . Based on extensive testing,    \\varphi italic_  is optimally set to 0.1 in our scheme. Note that the predictions of the teacher network were considered to be of the best quality only if they comply with these two criteria.",
            "(2)  Objective Evaluation : Considering that only using subjective evaluation is not comprehensive, we also introduce objective evaluation to evaluate the performance of our method. Objective image quality assessment (IQA) methods can be categorized as full-reference, reduced-reference, and no-reference. Since it is difficult to obtain realistic GT images under adverse weather conditions, and our primary aim is to recover perceptually clearer images, GT images are no longer suitable as a reference. Consequently, the corresponding FR-IQA metrics, such as PSNR and SSIM, are not applicable. Instead, we adopt NR-IQA metrics for image quality evaluation. In this paper, five top-performance metrics are employed for objective evaluation, including PAQ2PIQ  [ 50 ] , DBCNN  [ 51 ] , CLIPIQA  [ 52 ] , MUSIQ-SPAQ  [ 37 ] , and NIQE  [ 53 ] . In Tables  1 ,  2 , and  3 , we compare our approach to other fully-supervised approaches, including weather-specific and all-in-one approaches, for these five NR-IQA metrics. It is clear that our method, which employs a semi-supervised learning framework trained with limited labeled data, significantly outperforms other methods overall, even over GT, affirming that the restoration results from our method are visually high-quality."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Objective evaluation comparison on  Snow100K-2000  test set in terms of five no-reference IQA (NR-IQA) metrics. The best results are in  bold  and the second bests are with  underline .",
        "table": "S4.T3.6",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "This section detailedly describes the design of our SemiDDM-Weather. Section  3.1  outlines the proposed semi-supervised adverse weather removal framework, i.e., SemiDDM-Weather. How we customize a wavelet diffusion model to serve as the backbone of SemiDDM-Weather, is provided in Section  3.2 . The targeted design for improving the accuracy of pseudo-labels under the teacher-student based network is detailed in Section  3.3 . Training and inference details are presented in Section  3.4 .",
            "In the following, we will describe the model training and inference in detail. The training process is outlined in Algorithm  3 . The schematic diagram of labeled data training is shown in Fig.  3 . To be specific, given the degraded image  x  R 3  H  W x superscript R 3 H W x\\in\\mathbb{R}^{3\\times\\text{H}\\times\\text{W}} italic_x  blackboard_R start_POSTSUPERSCRIPT 3  H  W end_POSTSUPERSCRIPT  and its corresponding label  y  R 3  H  W y superscript R 3 H W y\\in\\mathbb{R}^{3\\times\\text{H}\\times\\text{W}} italic_y  blackboard_R start_POSTSUPERSCRIPT 3  H  W end_POSTSUPERSCRIPT , where  y y y italic_y  represents the GT if  x x x italic_x  is labeled data, otherwise the optimal outputs from the pseudo-label dynamic warehouse, we first crop them into  64  64 64 64 64\\times 64 64  64  patches, resulting  x c  R 3  64  64 superscript x c superscript R 3 64 64 x^{c}\\in\\mathbb{R}^{3\\times 64\\times 64} italic_x start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT 3  64  64 end_POSTSUPERSCRIPT  and  y c  R 3  64  64 superscript y c superscript R 3 64 64 y^{c}\\in\\mathbb{R}^{3\\times 64\\times 64} italic_y start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT 3  64  64 end_POSTSUPERSCRIPT . Note that the labeled data is randomly cropped while the unlabeled ones are center-cropped, due to the consideration of the alignment of the optimal outputs. Next, we decompose them into four wavelet subbands and then concatenate them into a single target in channel dimension, i.e.,  x 0  R 12  64  64 subscript x 0 superscript R 12 64 64 x_{0}\\in\\mathbb{R}^{12\\times 64\\times 64} italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT 12  64  64 end_POSTSUPERSCRIPT  and  y 0  R 12  64  64 subscript y 0 superscript R 12 64 64 y_{0}\\in\\mathbb{R}^{12\\times 64\\times 64} italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT 12  64  64 end_POSTSUPERSCRIPT , for the diffusion/denoising process. More concretely, referring to Fig.  3 , we inject Gaussian noise to  y 0 subscript y 0 y_{0} italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  (i.e., diffusion), obtaining  y t  1 subscript y t 1 y_{t-1} italic_y start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT  and  y t subscript y t y_{t} italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , as detailed in lines 7-8 of Algorithm  3 . Subsequently, the noise-corrupted result  y t subscript y t y_{t} italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and the degraded image  x 0 subscript x 0 x_{0} italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  are fed into the generator, resulting in the predicted  y 0  superscript subscript y 0  {y}_{0}^{\\prime} italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT . This predicted value is then used in the posterior distribution:  y t  1   q  ( y t  1   y t , y 0  ) similar-to superscript subscript y t 1  q conditional superscript subscript y t 1  subscript y t superscript subscript y 0  {y}_{t-1}^{\\prime}\\sim q({y}_{t-1}^{\\prime}\\mid y_{t},{y}_{0}^{\\prime}) italic_y start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  italic_q ( italic_y start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) , for denoising, yielding the denoised output  y t  1  superscript subscript y t 1  {y}_{t-1}^{\\prime} italic_y start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , as shown in lines 10-11 of Algorithm  3 .",
            "Referring to Eq. ( 3 ), apart from the original adversarial loss  log  D sn  ( y t  1  , y t , t ) subscript D sn superscript subscript y t 1  subscript y t t \\log D_{\\text{sn}}({y}_{t-1}^{\\prime},y_{t},t) roman_log italic_D start_POSTSUBSCRIPT sn end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t )  in WaveDiff, a auxiliary loss  L aux  ( y c  , y c ) subscript L aux superscript subscript y c  subscript y c \\mathcal{L}_{\\text{aux}}({y}_{c}^{\\prime},y_{c}) caligraphic_L start_POSTSUBSCRIPT aux end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , italic_y start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT )  comprises of  L rec subscript L rec \\mathcal{L}_{\\text{rec}} caligraphic_L start_POSTSUBSCRIPT rec end_POSTSUBSCRIPT  loss and  L perc subscript L perc \\mathcal{L}_{\\text{perc}} caligraphic_L start_POSTSUBSCRIPT perc end_POSTSUBSCRIPT  are further introduced. Specifically, the  L aux subscript L aux \\mathcal{L}_{\\text{aux}} caligraphic_L start_POSTSUBSCRIPT aux end_POSTSUBSCRIPT  is formulated as:",
            "As for the discriminator loss, as shown in line 16 of Algorithm  3 , it follows exactly the one of WaveDiff. The specific form of the discriminator loss is as follows:",
            "For unlabeled data training, the discriminator loss, as shown in line 23 of Algorithm 2, also follows the formulation used in WaveDiff, i.e., Eq. ( 8 ). For the generator loss, as shown in line 21 of Algorithm  3 , only the  L 1 subscript L 1 \\mathcal{L}_{\\text{1}} caligraphic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  loss and the adversarial loss are employed to train the generator, as it is considered that applying too many constraints to unlabeled data with pseudo-labels may mislead the training process. Besides, as aforementioned, simply using the  L 1 subscript L 1 \\mathcal{L}_{{1}} caligraphic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  distance as the consistency loss can easily overfit the student model to incorrect predictions, thus resulting in confirmation bias. To cope with this issue, we further replace it with a contrastive loss  L contr subscript L contr \\mathcal{L}_{\\text{contr}} caligraphic_L start_POSTSUBSCRIPT contr end_POSTSUBSCRIPT  in our scheme, i.e.,",
            "(2)  Objective Evaluation : Considering that only using subjective evaluation is not comprehensive, we also introduce objective evaluation to evaluate the performance of our method. Objective image quality assessment (IQA) methods can be categorized as full-reference, reduced-reference, and no-reference. Since it is difficult to obtain realistic GT images under adverse weather conditions, and our primary aim is to recover perceptually clearer images, GT images are no longer suitable as a reference. Consequently, the corresponding FR-IQA metrics, such as PSNR and SSIM, are not applicable. Instead, we adopt NR-IQA metrics for image quality evaluation. In this paper, five top-performance metrics are employed for objective evaluation, including PAQ2PIQ  [ 50 ] , DBCNN  [ 51 ] , CLIPIQA  [ 52 ] , MUSIQ-SPAQ  [ 37 ] , and NIQE  [ 53 ] . In Tables  1 ,  2 , and  3 , we compare our approach to other fully-supervised approaches, including weather-specific and all-in-one approaches, for these five NR-IQA metrics. It is clear that our method, which employs a semi-supervised learning framework trained with limited labeled data, significantly outperforms other methods overall, even over GT, affirming that the restoration results from our method are visually high-quality."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Objective evaluation comparison on  IVIPC-DQA  test set in terms of five no-reference IQA (NR-IQA) metrics. The best results are in  bold  and the second bests are with  underline .",
        "table": "S4.T4.6",
        "footnotes": [
            "",
            "",
            ""
        ],
        "references": [
            "This section detailedly describes the design of our SemiDDM-Weather. Section  3.1  outlines the proposed semi-supervised adverse weather removal framework, i.e., SemiDDM-Weather. How we customize a wavelet diffusion model to serve as the backbone of SemiDDM-Weather, is provided in Section  3.2 . The targeted design for improving the accuracy of pseudo-labels under the teacher-student based network is detailed in Section  3.3 . Training and inference details are presented in Section  3.4 .",
            "For model inference, we continue to use the crop-based data input strategy. Notably, there is a slight difference. Instead of randomly cropping, we crop the degraded image  x x x italic_x  into  K K K italic_K  overlapping patches of size 64  \\times  64 over dense regular grids with a spacing of 4 pixels, collected into a dictionary  P P \\mathcal{P} caligraphic_P . The dense regular grid cropping strategy is illustrated in Fig.  4 . These patches will be firstly transformed by wavelet and then input into the student network for denoising. After that, the inverse wavelet transform will be conducted on the denoised results to obtain the restoration of these patches. Finally, the restored patches will be reassembled to reconstruct the entire image. Note that, for the overlapping regions, an averaging method is applied to ensure visual consistency, effectively smoothing the edges of these areas to achieve a cohesive and natural image restoration outcome. The illustration of this process is referred to in Fig.  5 . As for the details of denoising process, we fed the variable  y t subscript y t y_{t} italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , the conditional input  x 0 subscript x 0 x_{0} italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , the latent variable  z  N  ( 0 , I ) similar-to z N 0 I z\\sim\\mathcal{N}(0,\\mathbf{I}) italic_z  caligraphic_N ( 0 , bold_I ) , and the timestep  t t t italic_t  into the generator of the student network to approximate the  y 0  = G sn  ( y t , x 0 , z , t ) superscript subscript y 0  subscript G sn subscript y t subscript x 0 z t y_{0}^{\\prime}=G_{\\text{sn}}(y_{t},x_{0},z,t) italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = italic_G start_POSTSUBSCRIPT sn end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_z , italic_t ) . According to the posterior distribution  q  ( y t  1   y t , y 0  ) q conditional superscript subscript y t 1  subscript y t superscript subscript y 0  q(y_{t-1}^{\\prime}\\mid y_{t},y_{0}^{\\prime}) italic_q ( italic_y start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) , then we can obtain the denoised output  q  ( y t  1  ) q superscript subscript y t 1  q(y_{t-1}^{\\prime}) italic_q ( italic_y start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) . This process is iteratively repeated until obtaining the final denoised output  q  ( y 0  ) q superscript subscript y 0  q(y_{0}^{\\prime}) italic_q ( italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) , as shown in Algorithm  4 , steps 7-10.",
            "To further validate the practicality of our method, we extend our evaluation to real-world data and include the current advanced methods, e.g., TransWeather, WGWS-Net, and  WeatherDiff 64 subscript WeatherDiff 64 \\text{WeatherDiff}_{64} WeatherDiff start_POSTSUBSCRIPT 64 end_POSTSUBSCRIPT , for comparison. Figs.  9  and   10  illustrate the deraining instance on the  IVIPC-DQA  test set and the desnowing instance on the  Snow100K-Real  test set, respectively. It is observed that our method not only achieves superior degradation removal but also enhances image clarity, confirming the superiority of our method over other existing methods, such as clearer lawn and sharper tree textures. Tables  4  and   5  show the objective evaluation results on the  IVIPC-DQA  test set and the  Snow100K-Real  test set, respectively. Similarly, it can be observed that our method achieves the best results."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Objective evaluation comparison on  Snow100K-Real  test set in terms of five no-reference IQA (NR-IQA) metrics. The best results are in  bold  and the second bests are with  underline .",
        "table": "S4.T5.6",
        "footnotes": [
            "",
            "",
            ""
        ],
        "references": [
            "For model inference, we continue to use the crop-based data input strategy. Notably, there is a slight difference. Instead of randomly cropping, we crop the degraded image  x x x italic_x  into  K K K italic_K  overlapping patches of size 64  \\times  64 over dense regular grids with a spacing of 4 pixels, collected into a dictionary  P P \\mathcal{P} caligraphic_P . The dense regular grid cropping strategy is illustrated in Fig.  4 . These patches will be firstly transformed by wavelet and then input into the student network for denoising. After that, the inverse wavelet transform will be conducted on the denoised results to obtain the restoration of these patches. Finally, the restored patches will be reassembled to reconstruct the entire image. Note that, for the overlapping regions, an averaging method is applied to ensure visual consistency, effectively smoothing the edges of these areas to achieve a cohesive and natural image restoration outcome. The illustration of this process is referred to in Fig.  5 . As for the details of denoising process, we fed the variable  y t subscript y t y_{t} italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , the conditional input  x 0 subscript x 0 x_{0} italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , the latent variable  z  N  ( 0 , I ) similar-to z N 0 I z\\sim\\mathcal{N}(0,\\mathbf{I}) italic_z  caligraphic_N ( 0 , bold_I ) , and the timestep  t t t italic_t  into the generator of the student network to approximate the  y 0  = G sn  ( y t , x 0 , z , t ) superscript subscript y 0  subscript G sn subscript y t subscript x 0 z t y_{0}^{\\prime}=G_{\\text{sn}}(y_{t},x_{0},z,t) italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = italic_G start_POSTSUBSCRIPT sn end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_z , italic_t ) . According to the posterior distribution  q  ( y t  1   y t , y 0  ) q conditional superscript subscript y t 1  subscript y t superscript subscript y 0  q(y_{t-1}^{\\prime}\\mid y_{t},y_{0}^{\\prime}) italic_q ( italic_y start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) , then we can obtain the denoised output  q  ( y t  1  ) q superscript subscript y t 1  q(y_{t-1}^{\\prime}) italic_q ( italic_y start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) . This process is iteratively repeated until obtaining the final denoised output  q  ( y 0  ) q superscript subscript y 0  q(y_{0}^{\\prime}) italic_q ( italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) , as shown in Algorithm  4 , steps 7-10.",
            "To further validate the practicality of our method, we extend our evaluation to real-world data and include the current advanced methods, e.g., TransWeather, WGWS-Net, and  WeatherDiff 64 subscript WeatherDiff 64 \\text{WeatherDiff}_{64} WeatherDiff start_POSTSUBSCRIPT 64 end_POSTSUBSCRIPT , for comparison. Figs.  9  and   10  illustrate the deraining instance on the  IVIPC-DQA  test set and the desnowing instance on the  Snow100K-Real  test set, respectively. It is observed that our method not only achieves superior degradation removal but also enhances image clarity, confirming the superiority of our method over other existing methods, such as clearer lawn and sharper tree textures. Tables  4  and   5  show the objective evaluation results on the  IVIPC-DQA  test set and the  Snow100K-Real  test set, respectively. Similarly, it can be observed that our method achieves the best results."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Performance comparison of lane detection using CLRNet-ResNet-34  [ 54 ]  and ADNet-ResNet-34  [ 55 ]  on  CULane  test set: our method versus TKL and WGWS-Net restoration methods. The best results are highlighted in bold.",
        "table": "S4.T6.6",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "(1)  Subjective Evaluation : Figs.  6 ,  7 , and  8  illustrate instances of test datasets, including  Raindrop ,  Outdoor-rain , and  Snow100K-2000 . It shows that our method not only effectively removes the degradations caused by adverse weather conditions such as raindrops, rain streaks, and snow through a unified (a.k.a., all-in-one) framework, but also enhances the image clarity, such as clearer vehicle markings, text, and lane lines, especially surpassing that of the GT, affirming our superior restoration quality. This improvement provides better image restoration for various downstream tasks. Furthermore, compared to other specific as well as unified frameworks, our approach trained with limited labeled data demonstrates greater practical value.",
            "In this part, we conduct further experiments to verify the practical effect of our method in enhancing the performance of downstream vision applications, such as lane detection. Since TransWeather fails on these downstream tasks, and  WeatherDiff 64 subscript WeatherDiff 64 \\text{WeatherDiff}_{64} WeatherDiff start_POSTSUBSCRIPT 64 end_POSTSUBSCRIPT  is quite time-consuming, only TKL  [ 7 ]  and WGWS-Net (all-in-one methods) are included for comparison. We selected models trained on the  CULane   [ 56 ]  dataset, including CLRNet-ResNet-34  [ 54 ] , and ADNet-ResNet-34  [ 55 ] , to verify the practical effect of our method. The related experimental results are shown in Table  6 . These results clearly demonstrate that, in mitigating performance degradation caused by adverse weather conditions in downstream tasks, our method is not only effective but even superior to fully-supervised methods."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Abalation study of SemiDDM-Weather on  Outdoor-Rain  test set. The best results are in  bold  and the second bests are with  underline .",
        "table": "S4.T7.5.5",
        "footnotes": [],
        "references": [
            "(1)  Subjective Evaluation : Figs.  6 ,  7 , and  8  illustrate instances of test datasets, including  Raindrop ,  Outdoor-rain , and  Snow100K-2000 . It shows that our method not only effectively removes the degradations caused by adverse weather conditions such as raindrops, rain streaks, and snow through a unified (a.k.a., all-in-one) framework, but also enhances the image clarity, such as clearer vehicle markings, text, and lane lines, especially surpassing that of the GT, affirming our superior restoration quality. This improvement provides better image restoration for various downstream tasks. Furthermore, compared to other specific as well as unified frameworks, our approach trained with limited labeled data demonstrates greater practical value.",
            "(1)  Auxiliary Loss of the Generator for Labeled Images Training : Referring to Table  7 , it can be seen that without the auxiliary loss, the performance of our proposed model consistently degrades significantly over all the five metrics. This verifies that our auxiliary loss design contributes to the overall framework.",
            "(2)  Pseudo-label Dynamic Warehouse for Unlabeled Images Training : To assess the impact of the pseudo-label dynamic warehouse on our method, we compare the difference in performance with and without the pseudo-label dynamic warehouse. The results in Table  7  show that the introduction of the pseudo-label dynamic warehouse has improved our performance in general, especially on the DBCNN metric.",
            "(3)  Contrastive Loss for Unlabeled Images Training : As can be seen from the results in Table  7 , removing the contrast loss leads to a decrease in overall performance, which proves that the introduced contrast loss is effective to some extent.",
            "(4)  Method for Constructing Pseudo-label Dynamic Warehouse : We added a consistency constraint to assess the quality of teachers network predictions. As can be seen in Table  7 , this constraint plays an important role in selecting optimal pseudo-labels.",
            "(5)  Contribution of Memory Replay Training Strategy : As can be seen in Table  7 , the memory replay training strategy has demonstrated significant effectiveness for our model training."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Model specifications of WeatherDiff 64  and our method including parameters (M) and FLOPs (GB) on a single GPU for one sample",
        "table": "S4.T8.2",
        "footnotes": [],
        "references": [
            "For unlabeled data training, the discriminator loss, as shown in line 23 of Algorithm 2, also follows the formulation used in WaveDiff, i.e., Eq. ( 8 ). For the generator loss, as shown in line 21 of Algorithm  3 , only the  L 1 subscript L 1 \\mathcal{L}_{\\text{1}} caligraphic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  loss and the adversarial loss are employed to train the generator, as it is considered that applying too many constraints to unlabeled data with pseudo-labels may mislead the training process. Besides, as aforementioned, simply using the  L 1 subscript L 1 \\mathcal{L}_{{1}} caligraphic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  distance as the consistency loss can easily overfit the student model to incorrect predictions, thus resulting in confirmation bias. To cope with this issue, we further replace it with a contrastive loss  L contr subscript L contr \\mathcal{L}_{\\text{contr}} caligraphic_L start_POSTSUBSCRIPT contr end_POSTSUBSCRIPT  in our scheme, i.e.,",
            "(1)  Subjective Evaluation : Figs.  6 ,  7 , and  8  illustrate instances of test datasets, including  Raindrop ,  Outdoor-rain , and  Snow100K-2000 . It shows that our method not only effectively removes the degradations caused by adverse weather conditions such as raindrops, rain streaks, and snow through a unified (a.k.a., all-in-one) framework, but also enhances the image clarity, such as clearer vehicle markings, text, and lane lines, especially surpassing that of the GT, affirming our superior restoration quality. This improvement provides better image restoration for various downstream tasks. Furthermore, compared to other specific as well as unified frameworks, our approach trained with limited labeled data demonstrates greater practical value.",
            "To comprehensively assess the performance, we compare the efficiency of our method with the other DDM-based all-in-one adverse weather removal method, WeatherDiff 64   [ 8 ] . The results of model parameters (Params.) and floating-point operations (FLOPs) are reported in Table  8 . It is evident that our method, which uses only half as much labeled data as WeatherDiff 64  yet achieves superior restoration performance, is also more efficient and consumes fewer computational resources."
        ]
    }
}