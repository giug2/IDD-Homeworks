{
    "id_table_1": {
        "caption": "Table 1:  Data curation strategies.  We enumerate the strategies we consider for curating datasets. Image quality is low for synthetic image generating methods and high otherwise, as synthetic methods can introduce noise in the image. We estimate class imbalance using our LT@500 (long-tailedness) and LS@5pct (left-skewedness) metrics, described in depth in Sec.  G . We report high cost when humans were paid to label images and low otherwise. Our ImageNet label error estimates are drawn from  [ 30 ] , and in the absence of other information, estimates for OpenImages are assumed to be similar. Syn img2img label error estimates are identical to ImageNet, as labels are inherited.The emb-txt2img and emb-img2img label error estimates were derived from experiments in  [ 11 ] , who computed a 10% rate of disagreement between ImageNet original labels and those generated by a text-based embedding search (the lower bound assumes that all human errors were corrected by txt2img labeling, the upper bound assumes the union of errors). For extended definitions of the abbrevations used in this table, please refer to Sec. 2. FS:= Fully Supervised; WS:= Weakly Supervised. We enumerate the strategies employed for curating datasets.",
        "table": "S3.T1.3.1",
        "footnotes": [
            "",
            "",
            ""
        ],
        "references": [
            "In this section, we formalize the problem of data curation and offer an overview of data curation strategies, as well as exemplary datasets for each curation strategy, in Tab.  1 .",
            "Overview.  Having defined our data curation strategies in Tab.  1  and constructed a benchmark for them, we turn to producing datasets using each of our strategies and comparing them with our baseline strategy of expert curation for ImageNet-train. Few distribution shifts of ImageNet train exist, and those that exist rarely document their curation process. To fill this need, we introduce  ImageNet++  , the largest and most diverse set of shifts of ImageNet-train to date.  ImageNet++  consists of ImageNet-train and 5 distinct training shifts, each one constructed using a strategy from Tab.  1 . The constituent shifts of  ImageNet++  are:",
            "No reduced-cost curation strategy improves on ImageNet.  We explore this surprising result further in Sec.  5.1 .",
            "Report their approaches to the problems of data curation (See Tab.  1 ).",
            "In Tab.  1 , we enumerate these strategies and describe their distinguishing attributes.   D  ( im, txt ) D im, txt D(\\texttt{im, txt}) italic_D ( im, txt )  represents the data distribution of the baseline strategy,  D   ( im, txt ) superscript D  im, txt D^{\\prime}(\\texttt{im, txt}) italic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ( im, txt )  the distribution of the shift strategy. In this context, a model is any learned representation over data, such as a diffusion model or an embedding search model.",
            "Embedding search  methods generate S via nearest neighbors search over an index of embeddings generated by a computer vision model (typically a vision-language classifier such as OpenAIs CLIP  [ 32 ] . One challlenge of such methods is that they introduce label noise into the dataset. Many authors in the literature have proposed to correct noisy labels via a family of models  M  ( y )  y c  o  r  r  e  c  t  M y subscript y c o r r e c t M(y)\\rightarrow y_{correct} italic_M ( italic_y )  italic_y start_POSTSUBSCRIPT italic_c italic_o italic_r italic_r italic_e italic_c italic_t end_POSTSUBSCRIPT  in the literature; we discuss some methods in Sec.  2.1 . Another property of embedding-based search methods is that they are difficult to scale reliably. For LA1000-img2img, we retrieved over 1.3 million images, but were only able to construct a dataset of around 0.8 million images from them after NSFW filtering, deduplication, and broken links in the embedding lookup table. When constructing LA1000-txt2img from LaionNet, we encountered a similar rate of failure.  [ 35 ]"
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Utility of curated datasets.  Embedding search outperforms other curation methods, including the human-labeled, but heavily imbalanced, OI1000 dataset. Size is not a reliable predictor of quality; the smallest dataset is also one of the strongest performing by most metrics. Fine-tuned and self-supervised learning performance do not strictly track base task accuracy or robust accuracy, underscoring the importance of holistic evaluations of dataset quality.",
        "table": "S5.T2.30.30",
        "footnotes": [],
        "references": [
            "Our goal in this paper is to bring the implicitly studied subject of data curation into sharper focus, broaden the scope of curation beyond data filtration, and introduce it as a topic of research in its own right. In Sec.  2 , we use rational choice theory to formalize any data curation strategy as a utility function, where an increment to the marginal cost produces an expected gain in utility. In Sec.  3 , we introduce  Select  , a benchmark that serves as a diverse measure of utility of data curation methods in the domain of image classification. In Sec.  4 , we introduce  ImageNet++  , which we leverage to produce a large-scale set of baselines for data curation, composed of 5 new training-data shifts of ImageNet-1K. Finally, in Sec.  5 , we compare our  ImageNet++  baselines on the  Select  benchmark and derive several useful insights. Specifically, our contributions are as follows.",
            "We introduce  ImageNet++  , the largest, most diverse set of distribution shifts for ImageNet-train to date  [ 10 ] . This serves as a rich source of data curation baselines on which we train over 130 models (Fig.  2 ).",
            "OOD Robustness.  We report several out-of-distribution robustness metrics, both synthetic and natural. Synthetic OOD-robustness shifts are generated using algorithms which transform existing real images in the validation set (e.g., synthetic image corruptions). Natural OOD-robustness shifts contain novel real images collected according to some heuristic, such as sketches of the class  [ 36 ]  or only collecting class examples with unusual context  [ 2 ] . For natural distribution shifts, we include  ImageNet-Sketch   [ 36 ] ,  ObjectNet   [ 2 ] ,  ImageNet-V2  - a replication of the original ImageNet test set,  ImageNet-R  [ 17 ]  - a 200-class subset of ImageNet-2012, highlighting renditions of everyday objects, and  ImageNet-A   [ 17 ]  a 200-class subset of ImageNet-2012 selected by misleading previous methods. For synthetic distribution shifts, we report  ImageNet-C  and  Stylized-ImageNet   [ 16 ,  14 ] . In Tab.  2 , we report the average over all natural distribution shifts as Avg. Nat. Rob., and the average over all synthetic distribution shifts as Avg. Syn. Rob..",
            "Pretraining and fine-tuning.  In order to holistically assess the quality of a data curation strategy it is important to include utility metrics that do not strictly track base accuracy. One such metric is treating the model trained on the shift as a pre-trained checkpoint, and evaluating it via a diverse regime of fine-tuning tasks. Inspired by the VTAB-1k benchmark introduced in  [ 39 ] , we assemble 11 such tasks. The details of the tasks and our implementation details for fine-tuning models can be found in Appendix Sec.  K . We report the results of such a regime in Tab.  2  as Avg. VTAB.",
            "Guiding self-supervised models.  All of the utility metrics we describe so far require first pretraining a model on the shift dataset. However, for rapid evaluation, it is also useful to consider metrics that estimate the utility of a curated dataset  without  training a model. For this, we turn to the field of self-supervised learning, in particular the DINO method introduced in  [ 6 ] . We evaluate a DINO model pretrained on ImageNet-train; note that the DINO pretraining method makes no use of the labels in the dataset, relying entirely on the images. We then evaluate the pretrained DINO model on the ImageNet-val test set using the method of  k k k italic_k NN classification described in  [ 6 ] . We evaluate the model multiple times, each time allowing a different quantity of samples-per-class (SPC). For the details of our implementation and the specific SPC values we consider, please refer to Appendix Sec.  I . We report the average over all SPC values in Tab.  2  as Avg. SSL.",
            "In this section, we evaluate our baseline strategy as well as our 5 shifts, training over 130 different models evaluating the utility of the shifts for different tasks. For further implementation details, please refer to Appendix Sec.  L . In Tab.  2 , we report our utility metrics for each strategy. Our key findings are as follows:",
            "Human curation does not always lead to more useful shifts . The OI1000 shift was expensive to curate because of its use of human annotators. However, for the majority of metrics we report, LA1000(img2img) performs better. We attribute this surprising result to label imbalance, and analyze it in greater depth in Sec.  5.2 .",
            "Experimental details.  In this section, we use the imbalance metrics introduced in Sec.  3.2  to analyze the performance of OI1000 models when the data is blended with IN1000 data to rebalance it. To control for the possible confounding factor of label set size, we conduct this experiment at |L|=100 as well as |L|=1000.",
            "Benchmark some of their training data against existing curation baselines in a standardized fashion, following Tab.  2 .",
            "Embedding search  methods generate S via nearest neighbors search over an index of embeddings generated by a computer vision model (typically a vision-language classifier such as OpenAIs CLIP  [ 32 ] . One challlenge of such methods is that they introduce label noise into the dataset. Many authors in the literature have proposed to correct noisy labels via a family of models  M  ( y )  y c  o  r  r  e  c  t  M y subscript y c o r r e c t M(y)\\rightarrow y_{correct} italic_M ( italic_y )  italic_y start_POSTSUBSCRIPT italic_c italic_o italic_r italic_r italic_e italic_c italic_t end_POSTSUBSCRIPT  in the literature; we discuss some methods in Sec.  2.1 . Another property of embedding-based search methods is that they are difficult to scale reliably. For LA1000-img2img, we retrieved over 1.3 million images, but were only able to construct a dataset of around 0.8 million images from them after NSFW filtering, deduplication, and broken links in the embedding lookup table. When constructing LA1000-txt2img from LaionNet, we encountered a similar rate of failure.  [ 35 ]"
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Analytic metrics for curated datasets.  We consider a range of analytic metrics to help interpret the differences we observe in shift utility. The definitions for the abbreviations used in this table can be found in Sec.  3.2 . For the purposes of readability, we put the quality metric headers in  red . LT@500 refers to long-tailedness at 500 samples per class, and LS@5pc refers refers to left-skewedness at  5 % percent 5 5\\% 5 % .",
        "table": "S5.T3.7.1",
        "footnotes": [
            ""
        ],
        "references": [
            "Our goal in this paper is to bring the implicitly studied subject of data curation into sharper focus, broaden the scope of curation beyond data filtration, and introduce it as a topic of research in its own right. In Sec.  2 , we use rational choice theory to formalize any data curation strategy as a utility function, where an increment to the marginal cost produces an expected gain in utility. In Sec.  3 , we introduce  Select  , a benchmark that serves as a diverse measure of utility of data curation methods in the domain of image classification. In Sec.  4 , we introduce  ImageNet++  , which we leverage to produce a large-scale set of baselines for data curation, composed of 5 new training-data shifts of ImageNet-1K. Finally, in Sec.  5 , we compare our  ImageNet++  baselines on the  Select  benchmark and derive several useful insights. Specifically, our contributions are as follows.",
            "Imbalance metrics.  Zipfians distribution suggests that the frequency of an event is inversely proportional to its rank in a frequency distribution. This type of distribution is observed in natural language, where a few words (like the and and) appear very frequently, while the majority are used much less often. Inspired by this law, we utilize two metrics for imbalance; one which estimates the effect of overrepresented classes, a phenomenon we term  left-skewedness ; and another estimating the impact of long-tail classes, which we refer to as  long-tailedness . For formal definitions of these terms, please refer to Sec.  G . For a visualization, please refer to Fig.  3 .",
            "Despite innumerable advances in the field, including new vision-language foundation models, realistic generative image models and massive web-scraped datasets, it is still not possible to recreate, much less improve on, ImageNet curation without human labeling. The best human labels outperform reduced cost methods on every utility metric. In order to gain a better intuition for why reduced-cost shifts fail to match the utility of the baseline, we evaluate our analytic metrics, listed in Tab.  3 .",
            "Experimental details.  In this section, we use the imbalance metrics introduced in Sec.  3.2  to analyze the performance of OI1000 models when the data is blended with IN1000 data to rebalance it. To control for the possible confounding factor of label set size, we conduct this experiment at |L|=100 as well as |L|=1000."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Under-represented classes trigger performance declines.  To better understand the importance of left-skewedness and long-tailedness measures,  identical models are trained on various blended combinations of ImageNet  (  green )  and OpenImages  (  red )  samples . Robustness generally correlates strongly with dataset size, unless the dataset is heavily left-skewed. Base accuracy, however, correlates most closely with long-tailedness. The classes with the longest tails ( k = 100 k 100 k=100 italic_k = 100 ) are responsible for most of the performance decrease. Sample sizes are rounded to the nearest 1,000 and percentiles to the nearest whole number for clarity. The percentage in the first column indicates the proportion of data from ImageNet, while the remainder is from OpenImages. Rows are sorted by base (Val) accuracy.",
        "table": "S5.T4.53.51",
        "footnotes": [],
        "references": [
            "Our goal in this paper is to bring the implicitly studied subject of data curation into sharper focus, broaden the scope of curation beyond data filtration, and introduce it as a topic of research in its own right. In Sec.  2 , we use rational choice theory to formalize any data curation strategy as a utility function, where an increment to the marginal cost produces an expected gain in utility. In Sec.  3 , we introduce  Select  , a benchmark that serves as a diverse measure of utility of data curation methods in the domain of image classification. In Sec.  4 , we introduce  ImageNet++  , which we leverage to produce a large-scale set of baselines for data curation, composed of 5 new training-data shifts of ImageNet-1K. Finally, in Sec.  5 , we compare our  ImageNet++  baselines on the  Select  benchmark and derive several useful insights. Specifically, our contributions are as follows.",
            "Results.  Tab.  4 , which includes information on the data source, dataset size, our principal indicators, validation accuracy, and average accuracy under shifts, indicates that the presence of long-tailed classes is driving the performance declines in OI1000. Our indicator for long-tailedness shows strong rank-order agreement with validation accuracy. In contrast, dataset size and left-skewedness demonstrate only weak agreement.",
            "The minimum number of samples below which a class accuracy begins to decline, however, is affected by the size of the label set and the size of the dataset. Comparing rows 8 and 9 in Tab.  4 , we see that rebalancing only the classes with 101-500 samples results in an a small gain of around 6% to accuracy, but the same change with 1000 classes (rows 13, 14) results in a much larger gain (over 25% base accuracy).",
            "If your work uses existing assets, did you cite the creators?  [Yes]  , see Sec.  4"
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Correlation predicts shift accuracy.  We find that when accuracy on the training dataset is strongly correlated with accuracy on the shift dataset,  and  base accuracy is high, shift performance improves.",
        "table": "A5.T5.5",
        "footnotes": [],
        "references": [
            "Our goal in this paper is to bring the implicitly studied subject of data curation into sharper focus, broaden the scope of curation beyond data filtration, and introduce it as a topic of research in its own right. In Sec.  2 , we use rational choice theory to formalize any data curation strategy as a utility function, where an increment to the marginal cost produces an expected gain in utility. In Sec.  3 , we introduce  Select  , a benchmark that serves as a diverse measure of utility of data curation methods in the domain of image classification. In Sec.  4 , we introduce  ImageNet++  , which we leverage to produce a large-scale set of baselines for data curation, composed of 5 new training-data shifts of ImageNet-1K. Finally, in Sec.  5 , we compare our  ImageNet++  baselines on the  Select  benchmark and derive several useful insights. Specifically, our contributions are as follows.",
            "No reduced-cost curation strategy improves on ImageNet.  We explore this surprising result further in Sec.  5.1 .",
            "Human curation does not always lead to more useful shifts . The OI1000 shift was expensive to curate because of its use of human annotators. However, for the majority of metrics we report, LA1000(img2img) performs better. We attribute this surprising result to label imbalance, and analyze it in greater depth in Sec.  5.2 .",
            "We select as our exemplary shift ImageNet-R  [ 17 ] , because performance on this shift varies widely and does not closely track base classifier accuracy. In Tab.  5 , we report the ratio of ImageNet-R accuracy to ImageNet-Val accuracy (R-Acc-Pct). The widely referenced probit-scaled linear fit hypothesis of  [ 27 ]  would predict that this metric should remain constant except at the extremes of ImageNet-Val; surprisingly, we see considerable variance in practice."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Datasets with IN1000 Val. Acc and Avg. Rob.  We compare the model performance on different datasets by training a ResNet-50 model and measuring the validation accuracy on ImageNet-Val and the average robustness on the shift datasets. Specifically, models trained on shift combination datasets of ImageNet-1000 and ImageNet++ (line 5, 6, 7) are with a class-balanced weighted loss. We find that the weighted loss boosts the performance when the dataset is class-imbalanced.",
        "table": "A6.T6.20.20",
        "footnotes": [],
        "references": [
            "How many instances are there in total?  See Table  6  for reference of our dataset."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  The performance of data curation methods for self-supervised guidance depends on the number of samples per class (SPC).  When only a few samples per class are provided, the highly imbalanced OI1000 is more competitive with IN1000, and SD1000(txt2img), whose prompts were highly consistent across samples, performs well. As SPC increases, the greater sample diversity of LA1000 and IN1000 allows their performance to scale better.",
        "table": "A9.T7.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_8": {
        "caption": "Table 8:  Shifts vs. Unseen Downstream Tasks.  We take the models trained on various dataset shifts introduced in this work and adapt them to 11 different datasets. The models are fine tuned for 1000 steps of SGD with a batch size of 64. The presented results are accuracy on the entire test set with 95% confidence intervals.",
        "table": "A11.T8.77.77",
        "footnotes": [],
        "references": [
            "We take the benchmarking process introduced in VTAB and rewrite it to be compatible with PyTorch models. In addition, we choose the following datasets for this new benchmark: Caltech256, SVHN, DTD, EuroSAT, Flowers102, Country211, FGVCAircraft, GTSRB, RenderedSST2, LFWPeople, and SUN397. We finetune each of the models introduced in this work for 1000 steps of SGD with a batch size of 64. We have an initial learning rate of 0.01 which is decreased by a factor of 10 every 300 steps of SGD. The momentum for SGD is 0.9. Results for our models and the original timm ResNet50 are present in Table  8 ."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Extended robustness results for SELECT.",
        "table": "A12.T9.3.1",
        "footnotes": [],
        "references": []
    }
}