{
    "id_table_1": {
        "caption": "Table 2:  Image quality metrics:  The videos generated by different methods under various conditions across three datasets are evaluated using three image quality metrics. The  best  and  second  results are highlighted. Note that EchoNet-Synthetic  (Reynaud et al.,  2024 )  did not report the SSIM metric and did not specify whether the results correspond to A2C or A4C views.",
        "table": "S4.SS1.12.12.12",
        "footnotes": [
            ""
        ],
        "references": [
            "ECHOPulse, as shown in Figure  1 , consists of three key components: (1) a video tokenization model that encodes videos into temporal token sequences, (2) a masked generative transformer that learns the latent alignment between video tokens and ECG signal, and (3) a progressive video generation pipeline that enables fast, unlimited video generation. The video tokenization and transformer components are trained separately.",
            "In our experiments, we conduct a quantitative comparison between the performance of the diffusion-based EchoNet-Synthetic model and our VQ-VAE-based model, primarily evaluated using MSE (Mean Squared Error)  (Gauss,  1809 )  and MAE (Mean Absolute Error)  (Qi et al.,  2020 )  on reconstruction results of different datasets. As shown in Table  4.1 .",
            "The configuration of the transformer model in Figure  1 b is as follows:"
        ]
    },
    "id_table_2": {
        "caption": "Table 3:  Clinical and time-inference metrics:  This table presents the EF (Ejection Fraction) differences between the generated videos and the target ones. The sampling time refers to the time required to generate 64 video frames. The  best  results are highlighted.",
        "table": "S4.T2.6.6",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "The key to generating videos from ECG signals lies in capturing the temporal correlation between the video and ECG data. Unlike text  (Touvron et al.,  2023 ) , ECG is a continuous temporal signal with variable sequence lengths. As shown in Figure  2 , each ECG phase should correspond to different stages of the ECHO. Thus, encoding the entire video into a single embedding risks disrupting temporal alignment between ECG and video, also complicating autoregressive video generation and reducing generalizability. To temporally align ECHO video with the various phases of the ECG and support autoregressive video generation, an effective approach is to discretize the video into tokens while splitting the ECG into patches and embedding them for cross-modal alignment. Drawing inspiration from ViViT  (Arnab et al.,  2021 )  and C-ViViT  (Villegas et al.,  2022 ) , we developed ECHOPulses video tokenization model to meet these specific requirements.",
            "Based on Table  2 , ECHOPulse outperforms other models across several key metrics. In the ECG-conditioned setting, it achieves the lowest FID scores of 15.50 (A2C) and 20.82 (A4C), indicating highly realistic video generation. Its FVD scores of 82.44 (A2C) and 107.40 (A4C) further highlight its ability to capture temporal dynamics. Additionally, ECHOPulses SSIM scores of 0.67 (A2C) and 0.66 (A4C) are higher than those of competing models, showcasing its superior video quality and structural coherence.",
            "To validate that the generated videos are synchronized with the ECG signals, we compared the cardiac phases between the generated and original videos, focusing on the end-diastole (ED) and end-systole (ES) phases. ED, which corresponds to the R wave in the ECG, represents the moment when the chamber area is at its largest. Similarly, ES corresponds to the T wave, when the chamber area is at its smallest. As shown in Figure  2 , the generated videos, conditioned on the input image and ECG, strictly follow the phase changes dictated by the ECG. A more quantitative evaluation is performed by assessing the LVEF, demonstrating the alignment of cardiac dynamics between the generated and original ECHO video."
        ]
    },
    "id_table_3": {
        "caption": "Table 4:  Video tokenizer hyperparameters",
        "table": "S4.T3.5.5",
        "footnotes": [
            ""
        ],
        "references": [
            "To evaluate our models performance on LVEF accuracy, we applied SAM2  (Ravi et al.,  2024 )  to the generated videos for direct segmentation of the endocardium, epicardium, and left atrium. After segmenting the cardiac chamber structures, we compared the LVEF between the original echocardiograms and the generated ones.  L  V  E  F = L  V  E  D  L  V  E  S L  V  E  D L V E F L V E D L V E S L V E D LVEF=\\frac{LVED-LVES}{LVED} italic_L italic_V italic_E italic_F = divide start_ARG italic_L italic_V italic_E italic_D - italic_L italic_V italic_E italic_S end_ARG start_ARG italic_L italic_V italic_E italic_D end_ARG . This comparison ensured that the generated videos were controlled solely by the ECG signalsintroducing temporal information without affecting the structural integrity of the heart images. As shown in Figure  3 .",
            "Quantitative results of  L  V  E  F L V E F LVEF italic_L italic_V italic_E italic_F  are provided in Table  3 , where our model outperforms the other  (Reynaud et al.,  2023 )  in terms of MAE and RMSE. Although the R-square value is slightly lower, this is understandable given the higher difficulty of our task. Additionally, ECHOPulse achieves these results with fewer parameters and requires less sampling time to generate higher-quality videos. This not only underscores the validity of using ECG as a conditioning signal but also demonstrates the clinical potential of our model."
        ]
    },
    "id_table_4": {
        "caption": "Table 5:  Video generation model hyperparameters",
        "table": "A1.T4.2",
        "footnotes": [],
        "references": [
            "In this experiment, the process is conducted in a distributed manner. Initially, we train the video tokenization model on the Webvim-10M dataset  (Bain et al.,  2021 ) , followed by fine-tuning on the CAMUS dataset  (Leclerc et al.,  2019b )  to adapt the tokenization for medical imaging tasks. With the introduction of LFQ, the codebook size is set to  2 13 superscript 2 13 2^{13} 2 start_POSTSUPERSCRIPT 13 end_POSTSUPERSCRIPT . All natural videos are resized to 128x128 resolution with 11 frames. The temporal patch size is set to 2, and the spatial patch size is set to 8, resulting in a token sequence with a length of 1536. More detailed information can be seen in Appendix Table  4  and Table  5 .",
            "In our experiments, we conduct a quantitative comparison between the performance of the diffusion-based EchoNet-Synthetic model and our VQ-VAE-based model, primarily evaluated using MSE (Mean Squared Error)  (Gauss,  1809 )  and MAE (Mean Absolute Error)  (Qi et al.,  2020 )  on reconstruction results of different datasets. As shown in Table  4.1 .",
            "Our private dataset consists of 94,078 ECHO videos, including both apical 2 view and apical 4 view. It contains ECHO recordings from both healthy individuals and patients with various cardiovascular diseases, with real-time ECG signals recorded for each video. All videos were anonymized, underwent random contrast enhancement, and were resized to 128x128 resolution. Additional video generation examples are shown in the Figure  4 , 5 , 6  and  7 ."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "A1.T5.1",
        "footnotes": [],
        "references": [
            "In this experiment, the process is conducted in a distributed manner. Initially, we train the video tokenization model on the Webvim-10M dataset  (Bain et al.,  2021 ) , followed by fine-tuning on the CAMUS dataset  (Leclerc et al.,  2019b )  to adapt the tokenization for medical imaging tasks. With the introduction of LFQ, the codebook size is set to  2 13 superscript 2 13 2^{13} 2 start_POSTSUPERSCRIPT 13 end_POSTSUPERSCRIPT . All natural videos are resized to 128x128 resolution with 11 frames. The temporal patch size is set to 2, and the spatial patch size is set to 8, resulting in a token sequence with a length of 1536. More detailed information can be seen in Appendix Table  4  and Table  5 .",
            "Our private dataset consists of 94,078 ECHO videos, including both apical 2 view and apical 4 view. It contains ECHO recordings from both healthy individuals and patients with various cardiovascular diseases, with real-time ECG signals recorded for each video. All videos were anonymized, underwent random contrast enhancement, and were resized to 128x128 resolution. Additional video generation examples are shown in the Figure  4 , 5 , 6  and  7 ."
        ]
    }
}