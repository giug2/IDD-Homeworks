{
    "id_table_1": {
        "caption": "Table 1:  Comparison of RTD and MH-RTD on Open Book QA.",
        "table": "S3.T1.1",
        "footnotes": [],
        "references": [
            "We categorize the common downstream tasks of language models into two types: language understanding and language generation. The former focuses on understanding the input information, based on the context and the information stored within the model, and then outputs the answer in the form of a few tokens, usually in a very simple form. The latter focuses on generating new sentences with complete semantics. We explored the potential of RTD compared to other methods on these two types of tasks. We first compared the effects of RTD and MH-RTD. As shown in Table  1 , we found that MH-RTD effectively enhances the capabilities of RTD. Therefore, we default to using the MH-RTD method in the following tests.",
            "We tested the language understanding capabilities of RTD on multiple benchmarks.  When testing, question without answer be shown to the LLM, then we will gather its baseline output by LLMs first output token and our RTD result through searching our reference datastore.    \\lambda italic_  is set to  1 1 1 1  in this task. How the reference datastore is generated can be fount at appendix  B.1 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  RTD on language understanding benches. Baseline refers to zero-shot performance. ICL exceeds MPT-7Bs  2048 2048 2048 2048  context window, with a 0 score result, recorded as failed in the table.",
        "table": "S4.T2.2",
        "footnotes": [],
        "references": [
            "In-Context Learning (ICL), as a category of methods that do not require parameter adjustments, is one of the mainstream methods for adapting models to downstream tasks. ICL embeds domain knowledge, question-answering patterns, etc., into prompts through few-shot learning  [ 6 ] , prompt engineering  [ 49 ] , and Retrieval-Augmented Generation (RAG)  [ 25 ]  methods, leveraging the learning ability of the model itself to provide better answers. As pointed out in Figure  2 , ICL focuses on the prompt stage. However, ICL significantly increases the length of the input, consequently increases the space occupied by the KV-Cache required for inference. Further, according to the Roofline model  [ 44 ] , this part of the KV-Cache cannot be parallelized through batch processing, making memory I/O throughput a system bottleneck, wasting hardware computing power, and increasing token generation time during the entire inference stage.",
            "The performance boost can be found both with or without ICL. Results are in table  2 . Besides testing scores, we also record the confused rate of baseline, the proportion of the questions that failed to be answered properly, including output irrelevant text or cant give a certain answer, in table  4 . Meanwhile RTD is designed to given the LLMs decision in a trustable and controllable way.  In comparison with fine-tuning methods in table  4 , we can notice that RTD can achieve approximate performance improvements as using PEFT methods like LoRA. Although it is still insufficient compared to full-parameter fine-tuning, the latter has a higher cost and has undergone knowledge injection (which is not considered in this part of the experiment). The dataset used for full-parameter fine-tuning is MMLU-Recall  [ 31 ;  32 ] , and the hyper-parameters of LoRA can be found in Appendix  D .",
            "Generative tasks are generally subjective and difficult to test. We constructed a benchmark based on Retrieval-Augmented Generation (RAG) and Open Book Question Answering  [ 29 ]  to test the potential of RTD in areas requires advance reasoning such as knowledge injection. Chain-of-Thought  [ 40 ]  is a method that encourage the model to provide a step-by-step analysis before giving the final answer, thereby enhancing the models capabilities. We compared the performance of the model when introducing references through the ICL method and the RTD method, to determine the effectiveness of the RTD method. The extra knowledge source was Wikipedia. The generation of the datastore can be found in Detailed in Appendix  B.2 . With the results of table  6 , it can be seen that RTD was indeed helpful in knowledge injection. Besides, the context length is shrunk by a lot, thus saves reasoning GPU time and memory consumption. A detailed exploration of why RAG score is lower than baseline can be find in Appendix  C ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Confused rate.",
        "table": "S4.T4.fig1.1",
        "footnotes": [],
        "references": [
            "However, traditional next token prediction does not support incorporating external information and therefore, we introduce reference trustable decoding where we build a bypass around the  LM_Head , showcased in Figure  3 , as the entrance of additional knowledge or guidance.",
            "where  D = ( C , Y ) D C Y \\mathcal{D}=(\\mathcal{C},\\mathcal{Y}) caligraphic_D = ( caligraphic_C , caligraphic_Y )  is the task dataset with input context set  C C \\mathcal{C} caligraphic_C  and label set  Y Y \\mathcal{Y} caligraphic_Y , and  | Y | Y |\\mathcal{Y}| | caligraphic_Y |  refers the number of possible labels. This process is depicted in Figure  3 . Its obvious that  the computational requirement is same as performing a forward pass to every content in the task dataset , which aligned with the minimal requirement of the inference stage, denotes the superiority of RTD as a gradient-free method.",
            "On the Multi-head RTD side, the memory cost remains the same as the regular RTD takes. The proof is same as the Section  3.4 . For instance, reference datastore and head-wise reference datastore with  20 , 480 20 480 20,480 20 , 480  entries with  d m = 4096 subscript d m 4096 d_{m}=4096 italic_d start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT = 4096 ,  n = 32 n 32 n=32 italic_n = 32  and  d h = 128 subscript d h 128 d_{h}=128 italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT = 128 , stored in  float32 float32 \\mathtt{float32} typewriter_float32 , takes  320  MB 320 MB 320\\mathtt{MB} 320 typewriter_MB  of memory and hard disk space.",
            "As MH-RTD splits long vectors into multiple smaller ones, it gives us the opportunity to cut time and memory cost by merging different heads together, or directly evict some of them.  If on average,  p p p italic_p  heads are merged into one head, then we expect a  1 p 1 p \\frac{1}{p} divide start_ARG 1 end_ARG start_ARG italic_p end_ARG  resource consumption. The time and memory improvement and corresponding performance impact can be find in the tuning Section  4.3 .",
            "Justification: Section  3  and section  4  reflects our main claim.",
            "Justification: Our main theoretical result is about efficiencies in section  3.4 , in which they were proved.",
            "Justification: Discussions of hyper-parameters of our methods can be found in section  4.3 ."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  RTD comparing with fine-tune methods.",
        "table": "S4.T4.fig2.1",
        "footnotes": [],
        "references": [
            "At each decoding round, given the input context  c c c italic_c , we first compute  h ( l ) = L  M  ( c ) superscript h l L M c h^{(l)}=\\mathcal{LM}(c) italic_h start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT = caligraphic_L caligraphic_M ( italic_c ) , which is the input to RTD and  LM_Head . Then we use a three stage approach to get the RTD output,  Fetch ,  Normalization , and  Aggregation , depicted in Figure  4 .",
            "On the Multi-head RTD side, the memory cost remains the same as the regular RTD takes. The proof is same as the Section  3.4 . For instance, reference datastore and head-wise reference datastore with  20 , 480 20 480 20,480 20 , 480  entries with  d m = 4096 subscript d m 4096 d_{m}=4096 italic_d start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT = 4096 ,  n = 32 n 32 n=32 italic_n = 32  and  d h = 128 subscript d h 128 d_{h}=128 italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT = 128 , stored in  float32 float32 \\mathtt{float32} typewriter_float32 , takes  320  MB 320 MB 320\\mathtt{MB} 320 typewriter_MB  of memory and hard disk space.",
            "As MH-RTD splits long vectors into multiple smaller ones, it gives us the opportunity to cut time and memory cost by merging different heads together, or directly evict some of them.  If on average,  p p p italic_p  heads are merged into one head, then we expect a  1 p 1 p \\frac{1}{p} divide start_ARG 1 end_ARG start_ARG italic_p end_ARG  resource consumption. The time and memory improvement and corresponding performance impact can be find in the tuning Section  4.3 .",
            "The performance boost can be found both with or without ICL. Results are in table  2 . Besides testing scores, we also record the confused rate of baseline, the proportion of the questions that failed to be answered properly, including output irrelevant text or cant give a certain answer, in table  4 . Meanwhile RTD is designed to given the LLMs decision in a trustable and controllable way.  In comparison with fine-tuning methods in table  4 , we can notice that RTD can achieve approximate performance improvements as using PEFT methods like LoRA. Although it is still insufficient compared to full-parameter fine-tuning, the latter has a higher cost and has undergone knowledge injection (which is not considered in this part of the experiment). The dataset used for full-parameter fine-tuning is MMLU-Recall  [ 31 ;  32 ] , and the hyper-parameters of LoRA can be found in Appendix  D .",
            "Justification: Section  3  and section  4  reflects our main claim.",
            "Justification: Our main theoretical result is about efficiencies in section  3.4 , in which they were proved.",
            "Justification: The detailed description of our experiments and hyper-parameters can be found in section  4  and appendix  D .",
            "Justification: Discussions of hyper-parameters of our methods can be found in section  4.3 .",
            "Justification: All information above are given in section  4  and with specific experiment focused on some of them.",
            "Justification: All models and datasets weve used have been cited properly in section  4 ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Comparison of RTD and RAG using Wikipedia on LLaMA2-7B-Chat.",
        "table": "S4.T6.fig1.1",
        "footnotes": [],
        "references": [
            "Large language models like LLaMA2-70B  [ 38 ]  or Mistral-7B  [ 1 ]  utilized MHA and GQA mechanism  [ 2 ] , implies the potential of splitting a large attention vector into smaller ones. So we adapt this method into our RTD process. We define  n h subscript n h n_{h} italic_n start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT  of the head count of the LM model, and  d h subscript d h d_{h} italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT  the dimension of the each attention head where  d m = n h  d h subscript d m  subscript n h subscript d h d_{m}=n_{h}\\cdot d_{h} italic_d start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT = italic_n start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT  italic_d start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT . with this in mind, we split the reference datastore into  n h subscript n h n_{h} italic_n start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT  sub-datastore by head. When decoding, we first split  h ( l ) superscript h l h^{(l)} italic_h start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT  in to heads, then query each sub-datastore and merge the result, showcased in Figure  5 . Mathematically,"
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  PPL of the fitted model on domain datasets.",
        "table": "S4.T6.fig2.1",
        "footnotes": [],
        "references": [
            "Generative tasks are generally subjective and difficult to test. We constructed a benchmark based on Retrieval-Augmented Generation (RAG) and Open Book Question Answering  [ 29 ]  to test the potential of RTD in areas requires advance reasoning such as knowledge injection. Chain-of-Thought  [ 40 ]  is a method that encourage the model to provide a step-by-step analysis before giving the final answer, thereby enhancing the models capabilities. We compared the performance of the model when introducing references through the ICL method and the RTD method, to determine the effectiveness of the RTD method. The extra knowledge source was Wikipedia. The generation of the datastore can be found in Detailed in Appendix  B.2 . With the results of table  6 , it can be seen that RTD was indeed helpful in knowledge injection. Besides, the context length is shrunk by a lot, thus saves reasoning GPU time and memory consumption. A detailed exploration of why RAG score is lower than baseline can be find in Appendix  C .",
            "To explore whether the RTD method can be used to modify the language style of the model, we designed a style transfer experiment. We used a moderately scaled and strongly styled dataset, Tiny-Shakespeare  [ tinyssp ;  tinyssp2 ] , and compared the perplexity (PPL) of the model on the test set after LoRA and RTD, to measure whether our method can help the model change the output style. The results in Table  6  prove that our RTD method can reduce the perplexity of the model, enabling the model to adapt to the style of different datasets.  The hyperparameters of LoRA are in Appendix  D .",
            "Although our method is quick and efficient, it still introduces several hyper-parameters. We hope to explore the relationship between these hyper-parameters and the final performance of RTD. We conducted a series of ablation experiments on LLaMA2-7B  [ 38 ]  and OBQA  [ 29 ]  to explore the impact of different hyper-parameters on performance and how to quickly determine the optimal hyper-parameters. The overall result can be found in Figure  6 .  If not tuned, we set  k = 1024 k 1024 k=1024 italic_k = 1024 ,  s L = 19 , 828 subscript s L 19 828 s_{\\mathcal{L}}=19,828 italic_s start_POSTSUBSCRIPT caligraphic_L end_POSTSUBSCRIPT = 19 , 828 ,   = 1  1 \\lambda=1 italic_ = 1  and  T = 750 T 750 T=750 italic_T = 750  by default.",
            "Depicted in Figure  6  (a), RTDs performance improves initially with increasing  s L subscript s L s_{\\mathcal{L}} italic_s start_POSTSUBSCRIPT caligraphic_L end_POSTSUBSCRIPT  but eventually maxed out and starts oscillating when  s L subscript s L s_{\\mathcal{L}} italic_s start_POSTSUBSCRIPT caligraphic_L end_POSTSUBSCRIPT  reaches  4096 4096 4096 4096 . Generally, a larger  s L subscript s L s_{\\mathcal{L}} italic_s start_POSTSUBSCRIPT caligraphic_L end_POSTSUBSCRIPT  gives a better performance, but it do get maxed out depends on the specific task.  Figure  6  (b) showcased us how RTDs performance consistently improves as  k k k italic_k  increases initially, but eventually reaches a plateau, similiar with the  s L subscript s L s_{\\mathcal{L}} italic_s start_POSTSUBSCRIPT caligraphic_L end_POSTSUBSCRIPT . To be denoted is that a larger  k k k italic_k  could harm efficiency.  Figure  6  (c) implies that RTD can only reach its best performance when  T T T italic_T  is large enough. Empirically, due to the characteristics of the exponential function, as long as the range of scaled distances  d  superscript d  d^{\\prime\\prime} italic_d start_POSTSUPERSCRIPT   end_POSTSUPERSCRIPT  is kept between  1 1 1 1 - 2 2 2 2 , a sufficiently good effect can be achieved.  In RTD,    \\lambda italic_  is an important variable, especially in generation tasks. However,    \\lambda italic_  does not require high precision, and the range is relatively limited, so a good enough effect can be achieved quickly through a few attempts. Empirically speaking,  0.4 0.4 0.4 0.4 - 0.7 0.7 0.7 0.7  is a suitable range for    \\lambda italic_ .    Previous studies indicated that by pruning the dimension of attention wont hurt  k k k italic_k nn algorithms performance  [ 13 ] . In the case of RTD, showcased in Figure  6  (d), it can be found that the performance wont drop with at least  1 4 1 4 \\frac{1}{4} divide start_ARG 1 end_ARG start_ARG 4 end_ARG  heads remained, and the generation speed was boosted as more heads are dropped.",
            "RAG methods shows a decline in performance in Table  6 . To explain this, we can further examine the average length of the tokenized sequences of the retrieved context, which is around  6200 6200 6200 6200 , showcased in Table  7 . This length will hardly increase any inference cost for the RTD method, due to the small  s L subscript s L s_{\\mathcal{L}} italic_s start_POSTSUBSCRIPT caligraphic_L end_POSTSUBSCRIPT , but it exceeds the pre-training sequence length of LLaMA2-7B-Chat, which is  4096 4096 4096 4096 . That is to say, the naive RAG method here will cause sequence length overflow, thereby significantly affecting performance. If the overflow happened, then the models ability is cut down significantly."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Average length by token in OBQA question answering process, split by sections.",
        "table": "A3.T7.1",
        "footnotes": [],
        "references": [
            "RAG methods shows a decline in performance in Table  6 . To explain this, we can further examine the average length of the tokenized sequences of the retrieved context, which is around  6200 6200 6200 6200 , showcased in Table  7 . This length will hardly increase any inference cost for the RTD method, due to the small  s L subscript s L s_{\\mathcal{L}} italic_s start_POSTSUBSCRIPT caligraphic_L end_POSTSUBSCRIPT , but it exceeds the pre-training sequence length of LLaMA2-7B-Chat, which is  4096 4096 4096 4096 . That is to say, the naive RAG method here will cause sequence length overflow, thereby significantly affecting performance. If the overflow happened, then the models ability is cut down significantly."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  LoRA Hyper-parameters",
        "table": "A4.T8.4",
        "footnotes": [],
        "references": [
            "See Table  8 . For LoRA tuning on MMLU, any question whoes tokenized length exceed 4096 was evicted from both training and testing. The maximum tokenized length of the Tiny-Shakespeare dataset is 900."
        ]
    },
    "global_footnotes": []
}