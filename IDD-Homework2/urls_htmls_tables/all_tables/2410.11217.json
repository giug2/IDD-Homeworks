{
    "id_table_1": {
        "caption": "Table 1:  Experiments on WebGLM-QA.",
        "table": "S3.T1.1.1",
        "footnotes": [],
        "references": [
            "The main results are summarized in Table  1 , 2 , 3 , and we have the key observations as follows.",
            "The experiment results as shown in Fig.  1 .  In the evaluation of citation recall and citation precision metrics, there is a significant difference in the judgment criteria between NLI model and GPT-3.5-turbo model. However, the evaluation results from both models show a noticeable positive correlation. From the perspective of evaluating the relative quality of citations, their evaluation results are consistent. In other words, responses with higher citation quality as measured by NLI model are also recognized by GPT-3.5-turbo model. This indicates that the improvement in citation quality brought about by our proposed method is not due to the NLI models preference. The improvements in citation quality brought by the refiner is genuine."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Experiments on ASQA.",
        "table": "S3.T2.1.1",
        "footnotes": [],
        "references": [
            "The main results are summarized in Table  1 , 2 , 3 , and we have the key observations as follows.",
            "The experiment results as shown in Fig.  2 . From the experiment results, it can be observed that Post-hoc method only works for answers generated by models that lack attribution capabilities. Once a model has good attribution capabilities, the Post-hoc method performs worse than Pre-hoc method in both Citation Recall and Citation Precision metrics. Moreover, due to the difficulty in setting a similarity threshold, the BLEU-based method performs significantly worse than the ROUGE-based method in Citation Precision metric."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Experiments on ELI5.",
        "table": "S3.T3.1.1",
        "footnotes": [],
        "references": [
            "The main results are summarized in Table  1 , 2 , 3 , and we have the key observations as follows."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Experiments on WebGLM-QA.",
        "table": "S4.T4.1.1",
        "footnotes": [],
        "references": [
            "In this section, we fine-tune a Mistral-7B model  [ 30 ]  to serve as the refiner. The main results are summarized in Table  4 , 5 , 6 , and we have the key observations as follows."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Experiments on ASQA.",
        "table": "S4.T5.1.1",
        "footnotes": [],
        "references": [
            "In this section, we fine-tune a Mistral-7B model  [ 30 ]  to serve as the refiner. The main results are summarized in Table  4 , 5 , 6 , and we have the key observations as follows."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Experiments on ELI5.",
        "table": "S4.T6.1.1",
        "footnotes": [],
        "references": [
            "In this section, we fine-tune a Mistral-7B model  [ 30 ]  to serve as the refiner. The main results are summarized in Table  4 , 5 , 6 , and we have the key observations as follows."
        ]
    },
    "global_footnotes": [
        "https://www.bing.com/new",
        "https://www.perplexity.ai"
    ]
}