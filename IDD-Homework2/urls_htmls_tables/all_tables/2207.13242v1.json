{
    "S4.T1": {
        "caption": "TABLE I: Table of OK-VQA dataset.",
        "table": "",
        "footnotes": "",
        "references": [
            "We use the OK-VQA dataset [1] which is a popular KVQA benchmark dataset. The dataset consists of a total of 14,031 images and 14,055 questions. The detailed dataset sizes for training and testing are shown in Table I. For the validation dataset, we also use 1/3 of the training dataset based on the number of questions."
        ]
    },
    "S4.T2": {
        "caption": "TABLE II: Table of MSCOCO dataset.",
        "table": "",
        "footnotes": "",
        "references": [
            "MSCOCO dataset [32] is used to pre-train baseline models that generate captions. The dataset size is shown in Table II. In addition, Att2in [33], BuDn [34], and Transformer [35] are selected as the baseline models for caption generation, which are the representative image captioning models, and are used to generate captions of the OK-VQA dataset."
        ]
    },
    "S4.T3": {
        "caption": "TABLE III: Table of pearson correlation with the uncertainty and the similarity. s‚Äãi‚Äãmc‚Äãa‚Äãpùë†ùëñsuperscriptùëöùëêùëéùëùsim^{cap} represents a caption similarity. u‚Äãna‚Äãlùë¢superscriptùëõùëéùëôun^{al} and u‚Äãne‚Äãpùë¢superscriptùëõùëíùëùun^{ep} represent aleatoric uncertainty and epistemic uncertainty, respectively.",
        "table": "",
        "footnotes": "",
        "references": [
            "Table III illustrates the pearson correlation between uncertainty and caption similarity. The caption similarity and aleatoric uncertainty have a negative correlation of -0.1907, and the correlation between similarity and epistemic uncertainty is -0.1653. The correlation between aleatoric uncertainty and epistemic uncertainty shows a positive correlation, with a value of 0.4518. The correlation analysis indicates that there are relations between caption similarity and uncertainty, as we expected. In Fig. 5, the distributions of (a) aleatoric uncertainty and (b) epistemic uncertainty are right-skewed, while in (c) caption similarity distribution presents a left-skewed shape. Since there are extreme values in distributions, we believe that the semantic inconsistency can be identified with the uncertainties of the caption and the caption similarity.",
            "An ablation study is performed with three values of caption similarity, aleatoric uncertainty, and epistemic uncertainty with the weights in Eq. (7). In Table V, the baseline model that makes use of both explicit and implicit knowledge shows an accuracy of 31.15%. When caption similarity is added, the accuracy increases by 0.4%. In addition, when aleatoric and epistemic uncertainty are added, respectively, it shows further improvement. Also, when the similarity and epistemic uncertainty are added, the accuracy increases by 0.49%. The best performance of 32.45% in accuracy is achieved when the caption similarity and the aleatoric uncertainty are concatenated. As shown in Table III, since the similarity and aleatoric uncertainty has a higher correlation than between the similarity and epistemic uncertainty, the concatenated model seems to provide the best performance. When the three values of caption similarity, aleatoric uncertainty, and epistemic uncertainty are used together, the accuracy is 31.19%, which is only slightly better than the baseline. These results indicate that the caption similarity captures the semantic inconsistency relatively well and when the value which has a high correlation with the similarity is given to the model, it can predict correct answers better."
        ]
    },
    "S4.T4": {
        "caption": "TABLE IV: Performances of image captioning with commonsense knowledge on the OK-VQA dataset.",
        "table": "",
        "footnotes": "",
        "references": [
            "Table IV shows the performances of the baseline model for caption generation with the OK-VQA dataset. When we compared the image caption performance of the Att2in, BuDn, and Transformer models with the OK-VQA dataset, overall, the Transformer model shows better performance than others, and our study uses the Transformer model for uncertainty modeling. Fig. 7 shows aleatoric uncertainty and epistemic uncertainty of the word in the generated caption, and the word for uncertain actions and unusual objects in the image shows higher uncertainty than the average uncertainty of the sentence."
        ]
    },
    "S4.T5": {
        "caption": "TABLE V: An ablation study of the external knowledge assimilation methods with the OK-VQA dataset.",
        "table": "",
        "footnotes": "",
        "references": [
            "An ablation study is performed with three values of caption similarity, aleatoric uncertainty, and epistemic uncertainty with the weights in Eq. (7). In Table V, the baseline model that makes use of both explicit and implicit knowledge shows an accuracy of 31.15%. When caption similarity is added, the accuracy increases by 0.4%. In addition, when aleatoric and epistemic uncertainty are added, respectively, it shows further improvement. Also, when the similarity and epistemic uncertainty are added, the accuracy increases by 0.49%. The best performance of 32.45% in accuracy is achieved when the caption similarity and the aleatoric uncertainty are concatenated. As shown in Table III, since the similarity and aleatoric uncertainty has a higher correlation than between the similarity and epistemic uncertainty, the concatenated model seems to provide the best performance. When the three values of caption similarity, aleatoric uncertainty, and epistemic uncertainty are used together, the accuracy is 31.19%, which is only slightly better than the baseline. These results indicate that the caption similarity captures the semantic inconsistency relatively well and when the value which has a high correlation with the similarity is given to the model, it can predict correct answers better."
        ]
    },
    "S4.T6": {
        "caption": "TABLE VI: Results with the OK-VQA dataset, comparing our work with the state-of-the-art approaches. * represents results from a re-implementation with the author‚Äôs code and parameter setting using three experiments.",
        "table": "",
        "footnotes": "",
        "references": [
            "We compare our proposed semantic inconsistency model with the following state-of-the-art approaches including those with pre-trained methods and a combination of graph-based and pre-trained methods: 1) BAN[9]: Bilinear attention network which uses co-attention module with question features and image features from pre-trained models; 2) BAN+AN[1]: The model incorporates the external knowledge into BAN by using ArticleNet; 3) BAN+KG-Aug[4]: The model incorporates knowledge graph augmented model into BAN by using late augmentation scheme; 4) MUTAN[10]: Multimodal tucker fusion network which focuses on image and textual features extracted from pre-trained models based on tucker decomposition; 5) MUTAN+AN[1]: Similarly with BAN+AN, this method also incorporates the external knowledge into MUTAN by using ArticleNet; 6) KA[3]: The model uses image features, question features, and concept graphs with the external knowledge; 7) KRISP[5]: The model integrates image-text representation extracted from the BERT-based model and graph information based on external knowledge. In general, the methods using the knowledge information show better performance. As shown in Table VI, the model with both explicit knowledge, implicit knowledge, and semantic inconsistency measure achieves the state-of-the-art performance."
        ]
    }
}