{
    "S3.T1": {
        "caption": "Table 1: Comparison with SOTA methods for compositional visual question answering. Accuracy (%) on three datasets is reported. Code: ✓✓\\checkmark indicates code generation methods. LLM: Model names of large language models.",
        "table": "<table id=\"S3.T1.8\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.8.7.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.8.7.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding-left:9.0pt;padding-right:9.0pt;\" rowspan=\"2\"><span id=\"S3.T1.8.7.1.1.1\" class=\"ltx_text\">Method</span></th>\n<th id=\"S3.T1.8.7.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding-left:9.0pt;padding-right:9.0pt;\" rowspan=\"2\"><span id=\"S3.T1.8.7.1.2.1\" class=\"ltx_text\">Code</span></th>\n<th id=\"S3.T1.8.7.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding-left:9.0pt;padding-right:9.0pt;\" rowspan=\"2\"><span id=\"S3.T1.8.7.1.3.1\" class=\"ltx_text\">LLM</span></th>\n<td id=\"S3.T1.8.7.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">GQA</td>\n<td id=\"S3.T1.8.7.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">GQA</td>\n<td id=\"S3.T1.8.7.1.6\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">VQAv2</td>\n<td id=\"S3.T1.8.7.1.7\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">NLVR2</td>\n</tr>\n<tr id=\"S3.T1.8.8.2\" class=\"ltx_tr\">\n<td id=\"S3.T1.8.8.2.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">val2000</td>\n<td id=\"S3.T1.8.8.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">testdev</td>\n<td id=\"S3.T1.8.8.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">val4000</td>\n<td id=\"S3.T1.8.8.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">test</td>\n</tr>\n<tr id=\"S3.T1.3.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.3.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">CodeVQA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">8</a>]</cite>\n</th>\n<th id=\"S3.T1.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\"><math id=\"S3.T1.3.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\checkmark\" display=\"inline\"><semantics id=\"S3.T1.3.1.1.m1.1a\"><mi mathvariant=\"normal\" id=\"S3.T1.3.1.1.m1.1.1\" xref=\"S3.T1.3.1.1.m1.1.1.cmml\">✓</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.3.1.1.m1.1b\"><ci id=\"S3.T1.3.1.1.m1.1.1.cmml\" xref=\"S3.T1.3.1.1.m1.1.1\">✓</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.3.1.1.m1.1c\">\\checkmark</annotation></semantics></math></th>\n<th id=\"S3.T1.3.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">StarCoder-Base</th>\n<td id=\"S3.T1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">46.5</td>\n<td id=\"S3.T1.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">41.0</td>\n<td id=\"S3.T1.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">56.4</td>\n<td id=\"S3.T1.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">53.5</td>\n</tr>\n<tr id=\"S3.T1.4.2\" class=\"ltx_tr\">\n<th id=\"S3.T1.4.2.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">PyramidCoder (Ours)</th>\n<th id=\"S3.T1.4.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:9.0pt;padding-right:9.0pt;\"><math id=\"S3.T1.4.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\checkmark\" display=\"inline\"><semantics id=\"S3.T1.4.2.1.m1.1a\"><mi mathvariant=\"normal\" id=\"S3.T1.4.2.1.m1.1.1\" xref=\"S3.T1.4.2.1.m1.1.1.cmml\">✓</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.4.2.1.m1.1b\"><ci id=\"S3.T1.4.2.1.m1.1.1.cmml\" xref=\"S3.T1.4.2.1.m1.1.1\">✓</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.4.2.1.m1.1c\">\\checkmark</annotation></semantics></math></th>\n<th id=\"S3.T1.4.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">StarCoder-Base</th>\n<td id=\"S3.T1.4.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:9.0pt;padding-right:9.0pt;\"><span id=\"S3.T1.4.2.4.1\" class=\"ltx_text ltx_font_bold\">48.2</span></td>\n<td id=\"S3.T1.4.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:9.0pt;padding-right:9.0pt;\"><span id=\"S3.T1.4.2.5.1\" class=\"ltx_text ltx_font_bold\">41.5</span></td>\n<td id=\"S3.T1.4.2.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:9.0pt;padding-right:9.0pt;\"><span id=\"S3.T1.4.2.6.1\" class=\"ltx_text ltx_font_bold\">57.8</span></td>\n<td id=\"S3.T1.4.2.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:9.0pt;padding-right:9.0pt;\"><span id=\"S3.T1.4.2.7.1\" class=\"ltx_text ltx_font_bold\">59.2</span></td>\n</tr>\n<tr id=\"S3.T1.5.3\" class=\"ltx_tr\">\n<th id=\"S3.T1.5.3.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">CodeVQA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">8</a>]</cite>\n</th>\n<th id=\"S3.T1.5.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\"><math id=\"S3.T1.5.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\checkmark\" display=\"inline\"><semantics id=\"S3.T1.5.3.1.m1.1a\"><mi mathvariant=\"normal\" id=\"S3.T1.5.3.1.m1.1.1\" xref=\"S3.T1.5.3.1.m1.1.1.cmml\">✓</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.5.3.1.m1.1b\"><ci id=\"S3.T1.5.3.1.m1.1.1.cmml\" xref=\"S3.T1.5.3.1.m1.1.1\">✓</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.5.3.1.m1.1c\">\\checkmark</annotation></semantics></math></th>\n<th id=\"S3.T1.5.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">CodeLlama-7b-Python</th>\n<td id=\"S3.T1.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">48.0</td>\n<td id=\"S3.T1.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">42.8</td>\n<td id=\"S3.T1.5.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">61.3</td>\n<td id=\"S3.T1.5.3.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">57.8</td>\n</tr>\n<tr id=\"S3.T1.6.4\" class=\"ltx_tr\">\n<th id=\"S3.T1.6.4.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">PyramidCoder (Ours)</th>\n<th id=\"S3.T1.6.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:9.0pt;padding-right:9.0pt;\"><math id=\"S3.T1.6.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\checkmark\" display=\"inline\"><semantics id=\"S3.T1.6.4.1.m1.1a\"><mi mathvariant=\"normal\" id=\"S3.T1.6.4.1.m1.1.1\" xref=\"S3.T1.6.4.1.m1.1.1.cmml\">✓</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.6.4.1.m1.1b\"><ci id=\"S3.T1.6.4.1.m1.1.1.cmml\" xref=\"S3.T1.6.4.1.m1.1.1\">✓</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.6.4.1.m1.1c\">\\checkmark</annotation></semantics></math></th>\n<th id=\"S3.T1.6.4.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">CodeLlama-7b-Python</th>\n<td id=\"S3.T1.6.4.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:9.0pt;padding-right:9.0pt;\"><span id=\"S3.T1.6.4.4.1\" class=\"ltx_text ltx_font_bold\">51.3</span></td>\n<td id=\"S3.T1.6.4.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:9.0pt;padding-right:9.0pt;\"><span id=\"S3.T1.6.4.5.1\" class=\"ltx_text ltx_font_bold\">43.4</span></td>\n<td id=\"S3.T1.6.4.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:9.0pt;padding-right:9.0pt;\"><span id=\"S3.T1.6.4.6.1\" class=\"ltx_text ltx_font_bold\">62.8</span></td>\n<td id=\"S3.T1.6.4.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:9.0pt;padding-right:9.0pt;\"><span id=\"S3.T1.6.4.7.1\" class=\"ltx_text ltx_font_bold\">60.7</span></td>\n</tr>\n<tr id=\"S3.T1.8.6\" class=\"ltx_tr\">\n<th id=\"S3.T1.8.6.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\"><span id=\"S3.T1.8.6.3.1\" class=\"ltx_text\" style=\"color:#808080;\">Few-shot PnP-VQA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib5\" title=\"\" class=\"ltx_ref\">5</a>]</cite></span></th>\n<th id=\"S3.T1.7.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\"><math id=\"S3.T1.7.5.1.m1.1\" class=\"ltx_Math\" alttext=\"-\" display=\"inline\"><semantics id=\"S3.T1.7.5.1.m1.1a\"><mo mathcolor=\"#808080\" id=\"S3.T1.7.5.1.m1.1.1\" xref=\"S3.T1.7.5.1.m1.1.1.cmml\">−</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.7.5.1.m1.1b\"><minus id=\"S3.T1.7.5.1.m1.1.1.cmml\" xref=\"S3.T1.7.5.1.m1.1.1\"></minus></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.7.5.1.m1.1c\">-</annotation></semantics></math></th>\n<th id=\"S3.T1.8.6.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\"><math id=\"S3.T1.8.6.2.m1.1\" class=\"ltx_Math\" alttext=\"-\" display=\"inline\"><semantics id=\"S3.T1.8.6.2.m1.1a\"><mo mathcolor=\"#808080\" id=\"S3.T1.8.6.2.m1.1.1\" xref=\"S3.T1.8.6.2.m1.1.1.cmml\">−</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.8.6.2.m1.1b\"><minus id=\"S3.T1.8.6.2.m1.1.1.cmml\" xref=\"S3.T1.8.6.2.m1.1.1\"></minus></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.8.6.2.m1.1c\">-</annotation></semantics></math></th>\n<td id=\"S3.T1.8.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\"><span id=\"S3.T1.8.6.4.1\" class=\"ltx_text\" style=\"color:#808080;\">55.1</span></td>\n<td id=\"S3.T1.8.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\"><span id=\"S3.T1.8.6.5.1\" class=\"ltx_text\" style=\"color:#808080;\">46.6</span></td>\n<td id=\"S3.T1.8.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\"><span id=\"S3.T1.8.6.6.1\" class=\"ltx_text\" style=\"color:#808080;\">66.8</span></td>\n<td id=\"S3.T1.8.6.7\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\"><span id=\"S3.T1.8.6.7.1\" class=\"ltx_text\" style=\"color:#808080;\">63.4</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "VQA performance.\nTable 1 lists the performance across three VQA datasets. Our baseline is the state-of-the-art CodeVQA model, which uses default IO prompting. The same settings and APIs are utilized for all experiments. Compared with the baseline, PyramidCoder demonstrates relative improvements of a minimum of 1.65 points on the GQA val2000 set, 0.5 points on the GQA test set, 1.4 points on the VQAv2 val4000 set, and 2.9 points on the NLVR2 test set. These results indicate the effectiveness of PyramidCoder, which maintains its performance across a variety of underlying LLMs and datasets.",
            "LLM.\nTable 3 shows the performance of PyramidCoder using different LLMs on the GQA val2000 dataset. Besides the results listed in Table 1, we also include results using GPT-3.5, a private general LLM, for code generation to measure its performance.\nThe results summarized in Table 3 demonstrate that PyramidCoder consistently improves performance across diverse LLMs.\nThis consistency highlights that PyramidCoder is independent of the employed LLM, as stronger LLMs usually lead to better performance. Remarkably, PyramidCoder exhibits versatility by performing well with both closed and open-source LLMs, as well as with models designed for general and code-specific tasks."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Comparison with state-of-the-art prompting methods.",
        "table": "<table id=\"S4.T2.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.3.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">Method</th>\n<th id=\"S4.T2.3.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">GQA</th>\n<th id=\"S4.T2.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">VQAv2</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.3.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.3.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">Input-Output (Default)</td>\n<td id=\"S4.T2.3.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">48.0</td>\n<td id=\"S4.T2.3.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">61.3</td>\n</tr>\n<tr id=\"S4.T2.3.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.3.3.2.1\" class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">Chain-of-Thought <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">9</a>]</cite>\n</td>\n<td id=\"S4.T2.3.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">48.3</td>\n<td id=\"S4.T2.3.3.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">61.0</td>\n</tr>\n<tr id=\"S4.T2.3.4.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.3.4.3.1\" class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">Tree of Thoughts <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">10</a>]</cite>\n</td>\n<td id=\"S4.T2.3.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">48.7</td>\n<td id=\"S4.T2.3.4.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">61.5</td>\n</tr>\n<tr id=\"S4.T2.3.5.4\" class=\"ltx_tr\">\n<td id=\"S4.T2.3.5.4.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">PyramidCoder (Ours)</td>\n<td id=\"S4.T2.3.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:9.0pt;padding-right:9.0pt;\"><span id=\"S4.T2.3.5.4.2.1\" class=\"ltx_text ltx_font_bold\">51.3</span></td>\n<td id=\"S4.T2.3.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:9.0pt;padding-right:9.0pt;\"><span id=\"S4.T2.3.5.4.3.1\" class=\"ltx_text ltx_font_bold\">62.8</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Prompting Methods.\nTable 2 provides a comparative analysis of our PyramidCoder against existing prompting methodologies, namely few-shot CoT and few-shot ToT with a creative writing setup. All methods employ identical in-context examples and few-shot configurations as described earlier. While the preceding prompting techniques show improvements in comparison to default IO prompting, they tend to generate homogeneous codes. In contrast, PyramidCoder surpasses these methods by producing diverse code candidates, which provides more substantial reasoning evidence."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Results using different LLMs on GQA val2000. IO is the default strategy used in CodeVQA.",
        "table": "<table id=\"S4.T3.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.3.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.3.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">LLM</th>\n<th id=\"S4.T3.3.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">IO</th>\n<th id=\"S4.T3.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">Ours</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.3.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.3.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">gpt3.5-turbo</th>\n<td id=\"S4.T3.3.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">46.6</td>\n<td id=\"S4.T3.3.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\"><span id=\"S4.T3.3.2.1.3.1\" class=\"ltx_text ltx_font_bold\">55.8</span></td>\n</tr>\n<tr id=\"S4.T3.3.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.3.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">StarCoder-Base</th>\n<td id=\"S4.T3.3.3.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">46.5</td>\n<td id=\"S4.T3.3.3.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:9.0pt;padding-right:9.0pt;\"><span id=\"S4.T3.3.3.2.3.1\" class=\"ltx_text ltx_font_bold\">48.2</span></td>\n</tr>\n<tr id=\"S4.T3.3.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T3.3.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">CodeLlama-7b</th>\n<td id=\"S4.T3.3.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">48.0</td>\n<td id=\"S4.T3.3.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:9.0pt;padding-right:9.0pt;\"><span id=\"S4.T3.3.4.3.3.1\" class=\"ltx_text ltx_font_bold\">51.3</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "LLM.\nTable 3 shows the performance of PyramidCoder using different LLMs on the GQA val2000 dataset. Besides the results listed in Table 1, we also include results using GPT-3.5, a private general LLM, for code generation to measure its performance.\nThe results summarized in Table 3 demonstrate that PyramidCoder consistently improves performance across diverse LLMs.\nThis consistency highlights that PyramidCoder is independent of the employed LLM, as stronger LLMs usually lead to better performance. Remarkably, PyramidCoder exhibits versatility by performing well with both closed and open-source LLMs, as well as with models designed for general and code-specific tasks."
        ]
    },
    "S4.T4": {
        "caption": "Table 4: Ablation study with respect to the three modules.",
        "table": "<table id=\"S4.T4.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T4.3.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.3.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">Method</th>\n<th id=\"S4.T4.3.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">GQA</th>\n<th id=\"S4.T4.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">VQAv2</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T4.3.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.3.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">PyramidCoder</th>\n<td id=\"S4.T4.3.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">51.3</td>\n<td id=\"S4.T4.3.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">62.8</td>\n</tr>\n<tr id=\"S4.T4.3.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T4.3.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">w/o query rephraser</th>\n<td id=\"S4.T4.3.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">47.8</td>\n<td id=\"S4.T4.3.3.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">61.0</td>\n</tr>\n<tr id=\"S4.T4.3.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T4.3.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">w/o code generator</th>\n<td id=\"S4.T4.3.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">49.4</td>\n<td id=\"S4.T4.3.4.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">61.0</td>\n</tr>\n<tr id=\"S4.T4.3.5.4\" class=\"ltx_tr\">\n<th id=\"S4.T4.3.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">w/o answer aggregator</th>\n<td id=\"S4.T4.3.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">51.0</td>\n<td id=\"S4.T4.3.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:9.0pt;padding-right:9.0pt;\">62.1</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Ablation study.\nThe ablation study is conducted on the sampled GQA and VQAv2 datasets, and the results are summarized in Table 4. The first row shows the performance of the complete PyramidCoder. To show the significance of each component, the query rephraser, code generator, and answer aggregator are individually omitted from the whole framework. The results clearly indicate that the exclusion of any of these modules leads to a noticeable decrease in the VQA accuracy across both datasets."
        ]
    }
}