{
    "id_table_1": {
        "caption": "Table 1:  Dataset statistics.  We collect data from visual question answering (VQA) datasets for training and evaluation and synthetic additional query-document pairs for training.  We apply filtering on VQA datasets to remove context-dependent queries that are not suitable for retrieval.",
        "table": "S3.T1.1.1",
        "footnotes": [],
        "references": [
            "To evaluate VisRAG on real-world multi-modal documents, we construct datasets from open-source visual question answering (VQA) datasets and synthetic query-document pairs derived from web-crawled PDFs.  In terms of retrieval, VisRAG-Ret exhibits superior performance in retrieving multi-modal documents.  It outperforms state-of-the-art text- and vision-centric retrievers and achieves better results than solely relying on its constituent vision encoder or language model under identical training conditions.  For generation, VisRAG-Gen surpasses traditional text-based generators with open-source VLMs.  With GPT-4o, capable of handling multiple images, VisRAG shows increasing performance gains with more retrieved documents, indicating the potential for improved multi-page reasoning in the future.  As depicted in Figure  1 , in a direct comparison of pipeline performances, VisRAG achieves a 39% relative improvement over TextRAG using MiniCPM-V 2.6 as the generator and a 25% relative improvement with GPT-4o as the generator, attributed to the cascade effect.  Further analysis reveals that VisRAG possesses better training data efficiency and generalization ability than baseline models, and demonstrates robustness across both text-centric and vision-centric documents.  VisRAG shows great promise in replacing TextRAG as the next-generation standard for RAG pipelines.",
            "In this section, we first recap the typical RAG pipeline (Sec.  3.1 ), then present our VisRAG framework (Sec.  3.2 ) and the construction of our training and evaluation data (Sec.  3.3 ).",
            "To effectively build and evaluate RAG pipelines on multi-modal documents, we construct our datasets using a combination of visual question answering (VQA) datasets and synthetic data.  The statistics of our constructed dataset are provided in Table  1 .",
            "We collect question-document pairs from a series of VQA datasets, targeting different document types: MP-DocVQA  (Tito et al.,  2023 )  for industrial documents, ArXivQA  (Li et al.,  2024b ) , ChartQA  (Masry et al.,  2022 ) , InfographicsVQA  (Mathew et al.,  2022 ) , and PlotQA  (Methani et al.,  2020 )  for various figure types, and SlideVQA  (Tanaka et al.,  2023 )  for presentation slides.  All datasets feature questions that can be answered using a single document (page), except for SlideVQA, which includes multi-hop questions requiring information from multiple pages.  We follow the original datasets train-test splits, except for MP-DocVQA and InfographicsVQA, where the validation split serves as our evaluation set.  Additionally, we enhance our training set by collecting openly available PDFs from online sources and generating queries using GPT-4o  (OpenAI,  2024 ) , with details presented in Appendix  A.1 .  We assemble the retrieval corpus by gathering the positive document associated with each query from the training and evaluation sets.",
            "Some queries extracted from VQA datasets are  context-dependent , which lack specificity to a certain entity.  For instance, the response to Where was  the conference  held? varies based on the contextual document.  Using such context-dependent queries in open retrieval tasks is ineffective because they lack strong document specificity.  To address this, we implement an additional filtering stage to remove these context-dependent questions, where we prompt llama-3-8b-instruct  (AI@Meta,  2024 )  with human-annotated in-context samples to generate the classification label.  Table  1  shows a substantial reduction in context-dependent questions across data sources.  The details of filtering are presented in Appendix  A.2 .",
            "Notebly, VisRAG achieves a higher rate of accurately retrieving documents than TextRAG, and demonstrates a significantly improved rate of correct answer generation from accurately retrieved documents.  The cumulative improvements in both retrieval and generation phases result in an overall accuracy increment from 22.1% to 42.7%.  Across the six evaluation datasets, VisRAG shows a 39% relative accuracy increment on average, as illustrated in Figure  1 .  The case study of VisRAG and TextRAG is presented in Appendix  F .",
            "We show two cases in Table  8  and Table  9 .  In both instances, we compare VisRAG with TextRAG, maintaining the same setup as described in the End-to-end Performance paragraph in Sec.  5.1 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Overall retrieval performance in MRR@10. The best retrieval performance in each group is marked in  bold , and the second best performance is  underlined .  ColPali is trained on its synthetic data and datasets marked with    \\dagger  .  Corresponding Recall@10 performance can be found in Table  6 .",
        "table": "S5.T2.5.3",
        "footnotes": [],
        "references": [
            "In this section, we first recap the typical RAG pipeline (Sec.  3.1 ), then present our VisRAG framework (Sec.  3.2 ) and the construction of our training and evaluation data (Sec.  3.3 ).",
            "As shown in Figure  2  (left), traditional RAG frameworks (TextRAG) typically utilize text-based units for retrieval and generation.  However, in real-world scenarios, data often appear in complex, multi-modal documents, requiring an additional parsing step to obtain text.  In this paper, we propose to use the  page  as the fundamental unit for retrieval and generation, which is directly processed by vision language models (VLMs) as an image without further processing during retrieval and generation.  In subsequent sections, we use the terms page and document interchangeably.",
            "In this section, we present  Vis ion-based  R etrieval- a ugmented  G eneration (VisRAG), as shown in Figure  2  (right).  In contrast to traditional RAG frameworks which use text segments for both retrieval and generation, VisRAG leverages the image of the document to preserve all information.",
            "Some queries extracted from VQA datasets are  context-dependent , which lack specificity to a certain entity.  For instance, the response to Where was  the conference  held? varies based on the contextual document.  Using such context-dependent queries in open retrieval tasks is ineffective because they lack strong document specificity.  To address this, we implement an additional filtering stage to remove these context-dependent questions, where we prompt llama-3-8b-instruct  (AI@Meta,  2024 )  with human-annotated in-context samples to generate the classification label.  Table  1  shows a substantial reduction in context-dependent questions across data sources.  The details of filtering are presented in Appendix  A.2 .",
            "VisRAG-Ret is fine-tuned using in-batch negatives  (Karpukhin et al.,  2020 )  for one epoch with a batch size of 128 on 8 NVIDIA A100 80GB GPUs.  The temperature parameter in Equation  2  is set to 0.02.  Baseline retrievers are fine-tuned with the same hyper-parameters, and textual baselines utilize extracted text data as document-side input.  The generation part does not use any fine-tuning; we directly use off-the-shelf LLMs/VLMs for generation.",
            "As shown in Table  2 (a)(b), VisRAG-Ret, trained on out-of-domain data, outperforms all off-the-shelf baselines, including both text and vision models.  It significantly outperforms both BM25 and bge-large, and surpasses NV-Embed-v2, a state-of-the-art text retrieval model with 7.85B parameters.  Note that bge-large and NV-Embed-v2 are trained on millions of query-doc pairs  (Xiao et al.,  2023 ; Lee et al.,  2024 ) , which are 10x more than our training data.  Although bge-large outperforms BM25 on benchmarks like MTEB  (Muennighoff et al.,  2023 ) , it fails on our datasets, indicating text-based embedding models trained on clean text struggle with texts parsed from real-world documents.",
            "When trained with the same data setup, as demonstrated in Table  2 (b)(c), VisRAG-Ret outperforms text models MiniCPM (OCR) & (Captioner) and the vision model SigLIP by a significant margin.  The advantage is more pronounced in the out-of-domain setting, where VisRAG-Ret achieves 15% and 22% gains over MiniCPM (OCR) and SigLIP, respectively, compared to 8% and 10% in the in-domain setting.  This indicates that VisRAG-Ret has better generalization capability compared to text- and vision-centric models.  Notably, despite utilizing the same VLM MiniCPM-V 2.0 for parsing, MiniCPM (Captioner) performs worse than VisRAG-Ret, indicating that directly encoding with VLMs works better than using VLMs for parsing.  This can be attributed to the inevitable information loss when multi-modality information is transcribed into text.",
            "(Hu et al.,  2024 )  is a large language model (LLM) with 2.4 billion non-embedding parameters, demonstrating capabilities comparable to much larger models, such as Llama2-7B  (Touvron et al.,  2023 )  and Gemma-7B  (Team et al.,  2024 ) . In this paper, we employ MiniCPM to construct the baseline text-based retriever (Table  2 ) and generator (Table  3 ).",
            "(Zhai et al.,  2023 )  is a CLIP-style multi-modal model designed to align text and vision representations. We utilize SigLIP-400m, released by Hugging Face 2 2 2 https://huggingface.co/HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit , which incorporates Flash Attention 2, increases maximum resolution to 980x980, and adopts the NaViT strategy to allow (a) variable resolution images and (b) aspect ratio preserved images. In this paper, SigLIP is used to develop the baseline vision-based retriever (Table  2 ).",
            "(OpenBMB,  2024a ; Yao et al.,  2024 )  is a vision-language model (VLM) with 2.8 billion non-embedding parameters, built upon SigLIP-400m and MiniCPM. It can process single images up to 1.8 million pixels (e.g., 1344x1344) at any aspect ratio. We use MiniCPM-V 2.0 to build VisRAG-Ret (Table  2 ) and VisRAG-Gen (Table  3 (b)), as well as the document parsing model."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Overall generation performance in accuracy (%).  Different generation models and methods utilize the same retriever, VisRAG.  Performance relative to Oracle (using the ground-truth document(s) for generation) is colored in  blue .",
        "table": "S5.T3.18.18",
        "footnotes": [],
        "references": [
            "In this section, we first recap the typical RAG pipeline (Sec.  3.1 ), then present our VisRAG framework (Sec.  3.2 ) and the construction of our training and evaluation data (Sec.  3.3 ).",
            "In this experiment, we apply a series of text- and vision-based generators and methods on top of the same retriever VisRAG-Ret to study their effectiveness in generating the answer given the query and retrieved documents.  Table  3  shows the performance of (a) text-based generation (TextRAG-Gen), (b) generation using the VLM MiniCPM-V 2.0 which only accepts a single image as input, and (c) generation using VLMs which accept multiple images as input.",
            "In this experiment, we study the effectiveness of the VisRAG  pipeline , by comparing it with the TextRAG pipeline.  We construct TextRAG using MiniCPM (OCR) and MiniCPM-V 2.6 (OCR) for retrieval and generation, respectively, and VisRAG using VisRAG-Ret for retrieval and MiniCPM-V 2.6 for generation.  The performance on InfographicsVQA is visually represented in Figure  3 .",
            "In this experiment, we assess the retrieval and generation performance of VisRAG and TextRAG defined in Figure  3 , as well as VisRAG (SigLIP), which replaces the retriever in VisRAG with SigLIP.  We report their performance across different data subsets by categorizing queries based on the lengths of their positive documents, measured by the number of tokens of the extracted text.  Documents with a higher volume of extracted text may prioritize textual information over visual content.  As illustrated in Figure  5 , queries in ArxivQA and InfographicsVQA are divided into equal-sized bins according to the lengths of their relevant documents.  For each bin, we calculate and plot the average performance differences between VisRAG and TextRAG, as well as between VisRAG (SigLIP) and TextRAG, to compare how each model performs relative to TextRAG.  We observe that, in general, the relative performance of VisRAG and VisRAG (SigLIP) improves as the length of the relevant document decreases.  This suggests that models with vision encoders can better understand documents that emphasize visual information.  However, VisRAG (SigLIP) consistently underperforms VisRAG across all data subsets and, in some cases, even performs worse than TextRAG.  In contrast, VisRAG consistently outperforms TextRAG, indicating that the underlying language model in VisRAG is crucial for better understanding the semantics conveyed through visual cues.",
            "As mentioned in Sec.  3.3 , a significant portion of queries in VQA datasets are context-dependent that are unsuitable for retrieval.  We prompt llama-3-8b-instruct  (AI@Meta,  2024 )  to filter out such queries using the prompt in Figure  7 , which includes human-annotated samples from DocVQA.",
            "(Hu et al.,  2024 )  is a large language model (LLM) with 2.4 billion non-embedding parameters, demonstrating capabilities comparable to much larger models, such as Llama2-7B  (Touvron et al.,  2023 )  and Gemma-7B  (Team et al.,  2024 ) . In this paper, we employ MiniCPM to construct the baseline text-based retriever (Table  2 ) and generator (Table  3 ).",
            "(OpenBMB,  2024a ; Yao et al.,  2024 )  is a vision-language model (VLM) with 2.8 billion non-embedding parameters, built upon SigLIP-400m and MiniCPM. It can process single images up to 1.8 million pixels (e.g., 1344x1344) at any aspect ratio. We use MiniCPM-V 2.0 to build VisRAG-Ret (Table  2 ) and VisRAG-Gen (Table  3 (b)), as well as the document parsing model.",
            "(OpenBMB,  2024b ; Yao et al.,  2024 )  is an upgrade of MiniCPM-V 2.0 and MiniCPM-Llama3-V 2.5  (Yao et al.,  2024 ) .  It is built upon SigLIP-400M and Qwen2-7B  (Yang et al.,  2024 )  with a total of 8.5B parameters, exihibiting a significant performance improvement over MiniCPM-Llama3-V 2.5  (Yao et al.,  2024 ) .  Different from previous models, MiniCPM-V 2.6 can accept multiple images as the input and perform multi-modal in-context learning.  It also demonstrates stronger OCR capabilities.  We use MiniCPM-V 2.6 to build VisRAG-Gen (Table  3 ) and a text-based generation baseline MiniCPM-V 2.6 (OCR) (Figure  3 , Figure  5 ).",
            "(OpenAI,  2024 )  is OpenAIs latest multi-modal model, capable of processing any combination of text, audio, image, and video inputs and generating outputs in text, audio, and image formats. We use GPT-4o to construct VisRAG-Gen (Table  3 ) and to synthesize training data."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Statistics of crawled documents. We prompt GPT-4o to generate queries on these documents.",
        "table": "A1.T4.1.1",
        "footnotes": [],
        "references": [
            "As retrieval acts as the bottleneck in an RAG pipeline, it is crucial to have an effective retrieval component to maintain optimal performance.  In this experiment, we study the training data efficiency of VisRAG-Ret by evaluating the performance of VisRAG-Ret trained under different amounts of synthetic training data, i.e. in the out-of-domain setting.  As shown in Figure  4 , when only trained on 20k q-d pairs, VisRAG can surpass bge-large (OCR).  After training on 150k pairs, it can further surpass NV-Embed-v2 (OCR), the SOTA 8B-sized text embedding model trained on millions of curated text pairs.  This highlights VisRAG-Rets high training data efficiency and strong generalization capability, as all models are evaluated out-of-domain.  When compared with MiniCPM (OCR), which uses extracted text for training, VisRAG-Ret consistently achieves a performance gain of about 17% and exhibits a more stable training process.  The results show VisRAG-Rets potential for further performance improvements by scaling up the training data.",
            "To augment the training dataset of VisRAG, we gather additional documents from the web and utilize GPT-4o to generate queries based on these documents.  The sources of the collected documents are listed in Table  4 .  The prompt employed is shown in Figure  6 ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Overall retrieval performance of different document parsing pipelines.",
        "table": "A2.T5.1.1",
        "footnotes": [],
        "references": [
            "In this experiment, we assess the retrieval and generation performance of VisRAG and TextRAG defined in Figure  3 , as well as VisRAG (SigLIP), which replaces the retriever in VisRAG with SigLIP.  We report their performance across different data subsets by categorizing queries based on the lengths of their positive documents, measured by the number of tokens of the extracted text.  Documents with a higher volume of extracted text may prioritize textual information over visual content.  As illustrated in Figure  5 , queries in ArxivQA and InfographicsVQA are divided into equal-sized bins according to the lengths of their relevant documents.  For each bin, we calculate and plot the average performance differences between VisRAG and TextRAG, as well as between VisRAG (SigLIP) and TextRAG, to compare how each model performs relative to TextRAG.  We observe that, in general, the relative performance of VisRAG and VisRAG (SigLIP) improves as the length of the relevant document decreases.  This suggests that models with vision encoders can better understand documents that emphasize visual information.  However, VisRAG (SigLIP) consistently underperforms VisRAG across all data subsets and, in some cases, even performs worse than TextRAG.  In contrast, VisRAG consistently outperforms TextRAG, indicating that the underlying language model in VisRAG is crucial for better understanding the semantics conveyed through visual cues.",
            "We run the aforementioned pipelines on our dataset to obtain text-based training and evaluation data, and fine-tune a MiniCPM retriever to assess performance.  The results are presented in Table  5 .  Methods based on PPOCR demonstrate significantly better performance compared to pytesseract, with adjacent merging and layout preserving yielding similar results.  Consequently, we opt to use the adjacent merging policy for our (OCR) runs.",
            "(OpenBMB,  2024b ; Yao et al.,  2024 )  is an upgrade of MiniCPM-V 2.0 and MiniCPM-Llama3-V 2.5  (Yao et al.,  2024 ) .  It is built upon SigLIP-400M and Qwen2-7B  (Yang et al.,  2024 )  with a total of 8.5B parameters, exihibiting a significant performance improvement over MiniCPM-Llama3-V 2.5  (Yao et al.,  2024 ) .  Different from previous models, MiniCPM-V 2.6 can accept multiple images as the input and perform multi-modal in-context learning.  It also demonstrates stronger OCR capabilities.  We use MiniCPM-V 2.6 to build VisRAG-Gen (Table  3 ) and a text-based generation baseline MiniCPM-V 2.6 (OCR) (Figure  3 , Figure  5 ).",
            "We show two cases in Table  8  and Table  9 .  In both instances, we compare VisRAG with TextRAG, maintaining the same setup as described in the End-to-end Performance paragraph in Sec.  5.1 ."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Overall retrieval performance in Recall@10.",
        "table": "A4.T6.1.1",
        "footnotes": [],
        "references": [
            "To augment the training dataset of VisRAG, we gather additional documents from the web and utilize GPT-4o to generate queries based on these documents.  The sources of the collected documents are listed in Table  4 .  The prompt employed is shown in Figure  6 .",
            "Table  6  presents the retrieval performance in Recall@10."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Prompt templates for generation. Others refers to all VQA datasets except ArxivQA.",
        "table": "A5.T7.1.1",
        "footnotes": [],
        "references": [
            "As mentioned in Sec.  3.3 , a significant portion of queries in VQA datasets are context-dependent that are unsuitable for retrieval.  We prompt llama-3-8b-instruct  (AI@Meta,  2024 )  to filter out such queries using the prompt in Figure  7 , which includes human-annotated samples from DocVQA.",
            "We present the prompts of VisRAG-Gen and TextRAG-Gen in Table  7 ."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Case study from DocVQA. In this case, VisRAG successfully retrieves the ground-truth document, while TextRAG fails, leading to VisRAGs correct generation and TextRAGs incorrect generation.",
        "table": "A6.T8.227.227",
        "footnotes": [],
        "references": [
            "To train this model, we collect data from two sources:  a) ALLaVA  (Chen et al.,  2024a )  (image, caption) pairs, and  b) VQA documents with descriptions generated by GPT-4V.  We use the prompt in Figure  8  to instruct GPT-4V to generate detailed descriptions of documents from DocVQA, ChartQA, SlideVQA, InfographicsVQA, TextVQA  (Singh et al.,  2019 ) , and ArxivQA.",
            "We show two cases in Table  8  and Table  9 .  In both instances, we compare VisRAG with TextRAG, maintaining the same setup as described in the End-to-end Performance paragraph in Sec.  5.1 ."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Case study from InfographicsVQA. In this case, both VisRAG and TextRAG successfully retrieve the correct document; however, only VisRAG effectively leverages the layout information, enabling accurate generation. In contrast, TextRAG suffers from information loss of the layout, resulting in incorrect responses.",
        "table": "A6.T9.46.46",
        "footnotes": [],
        "references": [
            "We show two cases in Table  8  and Table  9 .  In both instances, we compare VisRAG with TextRAG, maintaining the same setup as described in the End-to-end Performance paragraph in Sec.  5.1 ."
        ]
    },
    "global_footnotes": [
        "In many cases, the retriever uses language models smaller than 1B parameters, which may not be considered large, but we use the term LLM for simplicity.",
        "https://huggingface.co/HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit"
    ]
}