{
    "id_table_1": {
        "caption": "Table 1:  Dataset Statistics for all 16 Procgen tasks.",
        "table": "A2.T1.32",
        "footnotes": [],
        "references": [
            "We introduce Retrieval-Augmented Decision Transformer (RA-DT), which incorporates an external memory into the Decision Transformer  [Chen et al.,  2021 , DT]  architecture (see Figure  1 ).  Our external memory enables efficient storage and retrieval of past experiences, that are relevant for the current situation.  We achieve this by leveraging a vector index populated with sub-trajectories, in combination with maximum inner product search; akin to Retrieval-augmented Generation (RAG) in LLMs  [Khandelwal et al.,  2019 ; Lewis et al.,  2020 ; Borgeaud et al.,  2022 ] .  To encode retrieved sub-trajectories, RA-DT relies on a pre-trained embedding model, which can either be domain-specific, such as a DT trained on the same domain, or a domain-agnostic language model (LM) (see Section  3 ).  Subsequently, RA-DT uses cross-attention to leverage the retrieved sub-trajectories and predict the next action.  This way, RA-DT does not rely on a long context and can deal with sparse reward settings.",
            "Processing long sequences with DTs is computationally expensive due to the quadratic complexity of the Transformer architecture.  To address this challenge, we introduce RA-DT, which equips the DT with an external memory that relies on a vector index for retrieval.  Consequently, RA-DT consists of a parametric and a non-parametric component, reminiscent of complementary learning systems  [Mcclelland et al.,  1995 ; Kumaran et al.,  2016 ] .  The former is represented by the DT and learns to predict actions conditioned on the future return.  The latter is the retrieval component that searches for relevant experiences, similar to  Borgeaud et al. [ 2022 ]  (see Figure  1 ).",
            "where  s r  e  l = cossim  ( k , q ) subscript s r e l cossim k q s_{rel}=\\operatorname{cossim}(\\bm{k},\\bm{q}) italic_s start_POSTSUBSCRIPT italic_r italic_e italic_l end_POSTSUBSCRIPT = roman_cossim ( bold_italic_k , bold_italic_q )  and  s u subscript s u s_{\\text{u}} italic_s start_POSTSUBSCRIPT u end_POSTSUBSCRIPT  measures the utility of a retrieved sub-trajectory weighted by    \\alpha italic_ .  Note that we instantiate  s u  (  ,  ) subscript s u   s_{\\text{u}}(\\cdot,\\cdot) italic_s start_POSTSUBSCRIPT u end_POSTSUBSCRIPT (  ,  )  differently depending on whether the agent is in training or inference mode.  At  training  time, a pre-collected set of trajectories that contains multiple tasks is stored in the vector index (Figure  1 , left). Trajectories can be obtained from human demonstrations or RL agents.  Therefore, we encourage the agent to retrieve sub-trajectories of the same task. During training, we use:   s u  (  ret ,  in ) = 1  ( t  (  ret ) = t  (  in ) ) , subscript s u subscript  ret subscript  in 1 t subscript  ret t subscript  in s_{\\text{u}}(\\tau_{\\text{ret}},\\tau_{\\text{in}})=\\mathds{1}(\\mathrm{t}(\\tau_{%  \\text{ret}})=\\mathrm{t}(\\tau_{\\text{in}})), italic_s start_POSTSUBSCRIPT u end_POSTSUBSCRIPT ( italic_ start_POSTSUBSCRIPT ret end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT in end_POSTSUBSCRIPT ) = blackboard_1 ( roman_t ( italic_ start_POSTSUBSCRIPT ret end_POSTSUBSCRIPT ) = roman_t ( italic_ start_POSTSUBSCRIPT in end_POSTSUBSCRIPT ) ) ,   where  t  (  ) t  \\mathrm{t}(\\cdot) roman_t (  )  takes a sub-trajectory and returns its task index.",
            "During  inference , we evaluate the ICL capabilities of the agent.  Starting from an  empty  vector index, we store experiences of the agent while it interacts with the environment (see Figure  1 , right).  Thus, during inference, the agent can only retrieve experiences from the same task.  Therefore, we steer the agent to produce high reward behaviour on the new task by reweighting a retrieved sub-trajectory by the total return achieved over the episode it appears in, i.e.,  s u  (  ret ,  in ) =  i = 0 T r i subscript s u subscript  ret subscript  in superscript subscript i 0 T subscript r i s_{\\text{u}}(\\tau_{\\text{ret}},\\tau_{\\text{in}})=\\sum_{i=0}^{T}r_{i} italic_s start_POSTSUBSCRIPT u end_POSTSUBSCRIPT ( italic_ start_POSTSUBSCRIPT ret end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT in end_POSTSUBSCRIPT ) =  start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT .  We apply this reweighting to the retrieved experiences in  R R \\mathcal{R} caligraphic_R  and select the top- k k k italic_k  elements by:",
            "In Algorithm  1 , we show the pseudocode for in-context RL with RA-DT at  inference  time.  In addition, we show RA-DT at  training  time in Algorithm  2  of Appendix  C.4 .",
            "We evaluate the ICL abilities of RA-DT on grid-world environments used in prior works, namely Dark-Room (see Section  4.1 ), Dark Key-Door (Section  4.2 ), and MazeRunner (Section  4.3 )  [Laskin et al.,  2022 ; Lee et al.,  2022 ; Grigsby et al.,  2023 ] , with increasingly larger grid-sizes, resulting in longer episodes.  Moreover, we evaluate RA-DT on two robotic benchmarks (Meta-World and DMControl, Section  4.4 ) and procedurally-generated video games (Procgen, Section  4.5 ).",
            "Experiment Setup.  Dark-Room is commonly used in prior work on in-context RL  [Laskin et al.,  2022 ; Lee et al.,  2023 ] .  The agent is located in an empty room, observes only its x-y coordinates, and has to navigate to an invisible goal state ( | S | = 2 S 2 |\\mathcal{S}|=2 | caligraphic_S | = 2 ,  | A | = 5 A 5 |\\mathcal{A}|=5 | caligraphic_A | = 5 , see Figure  9 ).  A reward of +1 is obtained in every step the agent is located in the goal state.  Because of partial observability, it must leverage memory of previous episodes to find the goal.  We conduct experiments on three different grid sizes, namely 10  \\times  10, 20  \\times  20, and 40  \\times  20, and corresponding episode lengths of 100, 200 and 800, respectively.  We designate 80 and 20 randomly assigned goals as train and evaluation locations, respectively, as in  Lee et al. [ 2023 ] .  We use Proximal Policy Optimization (PPO)  [Schulman et al.,  2017 ]  to generate 100K transitions per goal for 10  \\times  10 and 20  \\times  20 grids and 200K for 40  \\times  20 (see Figure  7  for single task expert scores).  During evaluation, the agent interacts with the environment for 40 ICL trials, and we report the scores at the last evaluation step (100K).  We provide additional details on the environment, the generated data, and the training procedure in Appendix  B.1  and  C .",
            "Results.   In Figure  3 , we show the ICL performances on the 20 hold-out tasks for all considered methods on Dark-Room  (a) 10  \\times  10,  (b)  20  \\times  20, and  (c)  40  \\times  20.  In addition, we present the ICL curves on the training tasks and the learning curves across the entire training period in Figures  14  and  15  in Appendix  D.1 .  Overall, we observe that RA-DT attains the highest average rewards on all 3 grid-sizes at the end of the 40 ICL-trials.  On 10  \\times  10, RA-DT obtains near-optimal performance scores both with the domain-specific and domain-agnostic embedding model.  The vanilla DT does not exhibit any performance improvement across trials.  This indicates the improvement in performance for RA-DT can be attributed to the retrieval component.  Furthermore, RA-DT outperforms AD and DPT without keeping entire episodes in its context window.  Similarly, RA-DT outperforms all baselines on the 20  \\times  20 and 40  \\times  20 grids.  While RA-DT successfully improves in-context, the baselines exhibit only little learning progress over the ICL trials, especially for larger grid sizes.  However, the final performance scores for 20  \\times  20 and 40  \\times  20 are not optimal.  With increasing grid size, discovering the goal requires systematic exploration in combination with targeted exploitation.  Therefore, we conduct a qualitative analysis on the exploration behaviour of RA-DT.  We find that RA-DT develops strategies to imitate a given successful context (see Figure  16 ), and avoids low-reward routes given an unsuccessful context (see Figure  17 ).",
            "Experiment Setup.   In Dark Key-Door, the agent is located in a room with two invisible objects: a key and a door.  The agent has to pick up the invisible key, then navigate to the door.  Because of the presence of two key events, the task-space is combinatorial in the number of grid-cells ( 100 2 = 10000 superscript 100 2 10000 100^{2}=10000 100 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 10000  possible tasks for  10  10 10 10 10\\times 10 10  10 ) and is therefore considered more difficult.  A reward of +1 is obtained once for picking up the key and for every step the agent stands on the door grid-cell after it collected the key.  We retain the same experiment setup as in Section  4.1  and provide further details in Appendix  B.1  (also see Figure  8  for single-task expert scores).",
            "Experiment Setup.   Maze-Runner was introduced by  Grigsby et al. [ 2023 ]  and inspired by  Pasukonis et al. [ 2022 ] .  The agent is located in a procedurally-generated  15  15 15 15 15\\times 15 15  15  maze (see Figure  10 ), observes continuous Lidar-like depth representations of states, and has to navigate to one, two, or three goal locations in the correct order ( | S | = 6 S 6 |\\mathcal{S}|=6 | caligraphic_S | = 6 , | A | = 4 A 4 |\\mathcal{A}|=4 | caligraphic_A | = 4 ).  A reward of +1 is obtained when reaching a goal location.  Episodes last for a maximum of 400 steps, or terminate early if all goal locations have been visited.  Similar to Dark-Room, we use PPO to generate 100K environment interactions for 100 procedurally-generated mazes.  We train all methods on a multi-task dataset that comprises trajectories from 100 mazes, evaluate on 20 unseen mazes, and report performance over 30 ICL trials.  We give further details on the environment, the dataset, and the experiment setup in Appendix  B.2  and  D.2 .",
            "Experiment Setup.   Finally, we conduct experiments on Procgen  [Cobbe et al.,  2020 ] , a benchmark consisting  of 16 procedurally-generated video games, designed to test the generalization abilities of RL agents.  The procedural generation in Procgen is controlled by setting an environment seed, which results in visually diverse observations for the same underlying task (see  starpilot -example in Figure  12 ).  In Procgen, the agent receives image-based inputs ( | S | = S absent |\\mathcal{S}|= | caligraphic_S | = 3  \\times  64  \\times  64).  All 16 tasks share a discrete action space ( | A | = 15 A 15 |\\mathcal{A}|=15 | caligraphic_A | = 15 ).  Rewards are either dense or sparse depending on the environment.",
            "Challenges of Next-Action Prediction.   Most in-context RL methods learn from offline datasets via next-action prediction and causal sequence modelling objectives.  As such, they cannot learn to infer the utility of an action, and thus, distinguish between positive and negative examples. This can induce delusions, which lead to repetitions of suboptimal actions and copying behaviour  [Ortega et al.,  2021 ]  (see Figure  19  for examples on Dark-Room).  In contrast,  online  in-context RL methods have shown promising adaptation abilities  [Team et al.,  2023 ; Grigsby et al.,  2023 ; Lu et al.,  2024 ] .  Consequently, a potential remedy to this problem is to train a value function to learn the utility of an action, as is commonly done in offline RL  [Levine et al.,  2020 ] .",
            "MazeRunner was introduced by  [Grigsby et al.,  2023 ]  and inspired by the Memory Maze environment  [Pasukonis et al.,  2022 ] .  The agent is located in a 15  \\times  15 procedurally-generated maze and has to navigate to a sequence of one, two, or three goal locations in the right order (see Figure  10 ).  Similar to Dark-Room environments, MazeRunner is partially observable and exhibits sparse rewards.  The agent observes a Lidar-like 6-dimensional representation of the state that contains 4 continuous values that measure the distance from the agents location to the nearest wall, and the x-y coordinates of the agents position in the grid.  The action-space is 4-dimensional (up, down, left, right).  A reward of +1 is obtained when reaching the currently active goal state in the goal sequence.  Therefore, the total achievable reward is equal to the number of goal states.  Episodes last for a maximum of 400 steps or terminate early, if all goal locations have been reached.  After every episode, the agent (gray box in Figure  10 ) is reset to the origin location.  During evaluation, we allow for 30 ICL trials, which amounts to 12K environment steps in total.",
            "Source Algorithm performance.   We show the average learning curves over all 100 task-specific PPO agents in Figure  11 .  On average, the agents receive a reward of   1 absent 1 \\approx 1  1  over all mazes.  This average include environments with one, two or three goals.  We provide further dataset statistics for MazeRunner with the corresponding dataset release.",
            "The Procgen benchmark consists of 16 procedurally-generated video games and was designed to test the generalization abilities of RL agents  [Cobbe et al.,  2020 ] .  Unlike other environments considered in this work, Procgen environments emits  3  64  64 3 64 64 3\\times 64\\times 64 3  64  64  images as observations.  All 16 environments share a common action space of 15 discrete actions.  The procedural generation in Procgen is controlled by setting an environment seed.  The environments seed randomizes the background and colour of the environment, but retains the same game dynamics.  This results in visually diverse observations for the same underlying task, as illustrated in Figure  12  for three seeds on the game  starpilot .  The rewards in Procgen can be dense or sparse depending on the environment.",
            "Source Algorithm performance.   We show the individual learning curves for all tasks in Figure  13 , and the aggregate statistics over all 16 datasets in Table  1 .",
            "Context Length.   On grid-worlds, we use a context length  C C C italic_C  equivalent to two 2 episodes for AD, DPT and DT.  For example, on  40  20 40 20 40\\times 20 40  20  grids, this results in a sequence length of 6400 ( = 1600  4 absent 1600 4 =1600*4 = 1600  4  for state/action/reward/RTG) for the DT and a sequence length of 4800 for AD.  On Meta-World, DMControl and Procgen, we reduce the sequence context length to 50 steps for DT.  For RA-DT, we use a shorter context length of  C = 50 C 50 C=50 italic_C = 50  transitions across environments, except for  20  20 20 20 20\\times 20 20  20  and  40  20 40 20 40\\times 20 40  20  grids, where we increase the context length to 100.  We want to highlight, that the context length for RA-DT applies to both the input context and the retrieved context.  The retrieved context contains the past, and future context, as described in Section  3.2.1 .  Consequently, the effective context length of RA-DT is  C + 2  C C 2 C C+2*C italic_C + 2  italic_C  and is independent of the episode length.",
            "Constructing queries/keys/values.   Regardless of whether  g g g italic_g  is domain-specific or domain-agnostic, we obtain  C C C italic_C  embedded tokens after applying  g g g italic_g  to the input trajectory   i  n subscript  i n \\tau_{in} italic_ start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT .  Subsequently, we apply mean aggregation over the context length  C C C italic_C  to obtain the  d r subscript d r d_{r} italic_d start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT -dimensional query representation.  We experimented with aggregating over all tokens or only tokens of a particular modality (state/action/reward/RTG), and found aggregation over states-only to be most effective (see Appendix  E.4 ).  As described in Section  3.2.1 , we construct the key-value pairs in our retrieval index by embedding all sub-trajectories in the dataset  D D \\mathcal{D} caligraphic_D  using our embedding model  g g g italic_g ,  K  V = { ( g  (  i , t  C : t ) ,  i , t  C : t + C )  1  i  | D | } K V conditional-set g subscript  : i t C t subscript  : i t C t C 1 i D \\mathcal{K}\\times\\mathcal{V}=\\{(g(\\tau_{i,t-C:t}),\\tau_{i,t-C:t+C})\\mid 1\\leq i%  \\leq|\\mathcal{D}|\\} caligraphic_K  caligraphic_V = { ( italic_g ( italic_ start_POSTSUBSCRIPT italic_i , italic_t - italic_C : italic_t end_POSTSUBSCRIPT ) , italic_ start_POSTSUBSCRIPT italic_i , italic_t - italic_C : italic_t + italic_C end_POSTSUBSCRIPT )  1  italic_i  | caligraphic_D | } .  To avoid redundancy, in practice we construct  H / C H C H/C italic_H / italic_C  key-value pairs for a given trajectory    \\tau italic_  with episode length  H H H italic_H  and sub-sequence length  C C C italic_C , instead of constructing the key and values for every step  t  [ 1 , H ] t 1 H t\\in[1,H] italic_t  [ 1 , italic_H ] .  Note that the values, we store   i , t  C : t + C subscript  : i t C t C \\tau_{i,t-C:t+C} italic_ start_POSTSUBSCRIPT italic_i , italic_t - italic_C : italic_t + italic_C end_POSTSUBSCRIPT , contain both the sub-trajectory itself (  i , t  C : t subscript  : i t C t \\tau_{i,t-C:t} italic_ start_POSTSUBSCRIPT italic_i , italic_t - italic_C : italic_t end_POSTSUBSCRIPT ) and its continuation (  i , t : t + C subscript  : i t t C \\tau_{i,t:t+C} italic_ start_POSTSUBSCRIPT italic_i , italic_t : italic_t + italic_C end_POSTSUBSCRIPT ).  Similar to  Borgeaud et al. [ 2022 ] , we found this choice important for high performance in RA-DT, because it allows the model to observe how the trajectory may evolve if it predicts a certain action (given that the retrieved context is similar enough).",
            "Analogous to the ICL curves on the 20 evaluation tasks in Figure  3 , we present ICL curves on the 80 train tasks in Figure  14 .  In general, we observe a similar learning behaviour on the train tasks as on the evaluation tasks, with slightly higher scores on average.  Interestingly, the domain-agnostic variant of RA-DT slightly outperforms its domain-specific counterpart on the training tasks.",
            "In addition, we also show the learning curves on Dark-Room  10  10 10 10 10\\times 10 10  10  over the entire training phase in Figure  15 .  We evaluate after every 25K updates and observe a steady improvement in the average performances with every evaluation.",
            "What happens if an optimal trajectory is retrieved in context?    In Figure  16 , we showcase this example.  The goal location is located at grid cell (4,6).  The attention maps exhibit high attention scores for the state and the RTG at the end of the retrieved trajectory.  We also observe high attention scores for the state similar to the current state and the action selected in that state.  The agent initially imitates the actions in the context trajectory, but deviates further into the episode.  Once the agent reaches the goal state, the attention scores for states and RTGs at the end of the trajectory reduce considerably, because the agent need not pay attention to the retrieved context any more.",
            "What happens if a suboptimal trajectory is retrieved in Context?   Similarly, we show the corresponding example in Figure  17 .  The goal location is again in grid cell (4,6).  The retrieved context trajectory reaches the final state (9,5).  Similar to Figure  16 , the attention maps exhibit high attention scores for the last state and RTG for that state, as well as for a state at a similar timestep.  Previously, RA-DT imitated the action, but in this situation the agent  picks a different route, as the context trajectory does not lead to a successful outcome.",
            "State Visitations.  In Section  D.1.1 , we found that RA-DT learned to either copy or avoid behaviours given positive or negative context trajectories.  Therefore, we further analyse the exploration behaviour of RA-DT by visualizing the state-visitation frequencies on Dark-Room  10  10 10 10 10\\times 10 10  10  across the 40 ICL trials for three different goal locations:  ( 5 , 8 ) 5 8 (5,8) ( 5 , 8 ) ,  ( 5 , 1 ) 5 1 (5,1) ( 5 , 1 ) , and  ( 4 , 6 ) 4 6 (4,6) ( 4 , 6 )  (see Figure  18 ).  The agent visits nearly all states at least once at test time, as visualized in Figure  18  (a) and (b).  Once the agent finds the goal location, it starts to imitate and stops exploring, as illustrated in Figure  18  (c).",
            "Delusions in RA-DT.   Furthermore, we find that in some unsuccessful trials, the agent repeatedly performs the same suboptimal action sequences.   Ortega et al. [ 2021 ]  refer to such behaviour as delusions.  In Figure  19 , we illustrate two examples in which the agent suffers from delusions and does not recover until the end of the episode.",
            "In Figures  20  and  21 , we report the average performances at the end of the training (100K) for both the 100 train and 20 evaluation mazes, as well as the corresponding ICL curves, respectively.",
            "To better understand the effect of learning with retrieval, we presented a number of ablation studies on critical components in RA-DT (Section  4.6 ).  We conduct all ablations on Dark-Room  10  10 10 10 10\\times 10 10  10  and otherwise retain the same experiment design choices, as reported in Section  4.1 .",
            "In RA-DT, we aggregate the hidden states of an input trajectory using mean aggregation of state tokens over the context length  C C C italic_C  to obtain the  d r subscript d r d_{r} italic_d start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT -dimensional query representation.  It is, however, possible to use the hidden states of other tokens to construct the query.  Therefore, we provide empirical evidence for this design choice in Figure  31 a.  We compare aggregating states, rewards, actions, returns-to-gos, all tokens, or only using the very last hidden state.  Indeed, we find that aggregating state tokens gives the best results.",
            "Next, we investigate the effect of the placement of the cross-attention layers in RA-DT.  In Figure  31 b, we therefore vary the placement of cross-attention layers in RA-DT.  By default, we use cross-attention after every self-attention layer.  We find that other choice also provide good results.  While placing the cross-attention at bottom layers tends to be beneficial, placing them only upper level layers tends to hurt performance.",
            "As mentioned in Section  C.4 , we perform context retrieval after every  t t t italic_t  environment steps.  Here,  t t t italic_t  represents a trade-off between inference time and final performance.  For grid-worlds, we use  t = 1 t 1 t=1 italic_t = 1  by default.  To better understand the effect of this design choice, we conduct an ablation in which we vary  t t t italic_t  (see Figure  31 c).  Indeed, we find that higher values for  t t t italic_t  result in a slight decrease in performance, but faster inference."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Hyperparameters for RA-DT.",
        "table": "A3.T2.8",
        "footnotes": [],
        "references": [
            "where we normalize both scores to be in the range  [ 0 , 1 ] 0 1 [0,1] [ 0 , 1 ] , such that they contribute equally to the final weight.  Our reweighting mechanism is illustrated in Figure  2 .",
            "In Algorithm  1 , we show the pseudocode for in-context RL with RA-DT at  inference  time.  In addition, we show RA-DT at  training  time in Algorithm  2  of Appendix  C.4 .",
            "We evaluate the ICL abilities of RA-DT on grid-world environments used in prior works, namely Dark-Room (see Section  4.1 ), Dark Key-Door (Section  4.2 ), and MazeRunner (Section  4.3 )  [Laskin et al.,  2022 ; Lee et al.,  2022 ; Grigsby et al.,  2023 ] , with increasingly larger grid-sizes, resulting in longer episodes.  Moreover, we evaluate RA-DT on two robotic benchmarks (Meta-World and DMControl, Section  4.4 ) and procedurally-generated video games (Procgen, Section  4.5 ).",
            "Experiment Setup.   Maze-Runner was introduced by  Grigsby et al. [ 2023 ]  and inspired by  Pasukonis et al. [ 2022 ] .  The agent is located in a procedurally-generated  15  15 15 15 15\\times 15 15  15  maze (see Figure  10 ), observes continuous Lidar-like depth representations of states, and has to navigate to one, two, or three goal locations in the correct order ( | S | = 6 S 6 |\\mathcal{S}|=6 | caligraphic_S | = 6 , | A | = 4 A 4 |\\mathcal{A}|=4 | caligraphic_A | = 4 ).  A reward of +1 is obtained when reaching a goal location.  Episodes last for a maximum of 400 steps, or terminate early if all goal locations have been visited.  Similar to Dark-Room, we use PPO to generate 100K environment interactions for 100 procedurally-generated mazes.  We train all methods on a multi-task dataset that comprises trajectories from 100 mazes, evaluate on 20 unseen mazes, and report performance over 30 ICL trials.  We give further details on the environment, the dataset, and the experiment setup in Appendix  B.2  and  D.2 .",
            "Results.   We find that RA-DT considerably outperforms all baselines in terms of final performance (see Figure  5 ).  Surprisingly, RA-DT is the only method to improve over the course of the 30 ICL trials.  However, we observe a considerable performance gap between train mazes and test mazes (0.65 vs. 0.4 reward, see Figure  20 ), indicating that solving unseen mazes requires an enhanced ability to generalize and learn from previous trials.",
            "Results.   We present the learning curves and corresponding ICL curves for Meta-World and DMControl in Figure  22  and  23 , and Figures  24  and  25  in Appendix  D , respectively.  In addition, we provide the raw and data-normalized scores in Tables  3  and  4 , respectively.  On both benchmarks, we find that RA-DT attains considerably higher scores on unseen evaluation tasks, but slightly lower average scores across training tasks compared to DT.  However, these performance gains on evaluation tasks are not reflected in improved ICL performance.  In fact, we only observe slight in-context improvement on training tasks, but not on holdout tasks for any of the considered methods.",
            "Experiment Setup.   Finally, we conduct experiments on Procgen  [Cobbe et al.,  2020 ] , a benchmark consisting  of 16 procedurally-generated video games, designed to test the generalization abilities of RL agents.  The procedural generation in Procgen is controlled by setting an environment seed, which results in visually diverse observations for the same underlying task (see  starpilot -example in Figure  12 ).  In Procgen, the agent receives image-based inputs ( | S | = S absent |\\mathcal{S}|= | caligraphic_S | = 3  \\times  64  \\times  64).  All 16 tasks share a discrete action space ( | A | = 15 A 15 |\\mathcal{A}|=15 | caligraphic_A | = 15 ).  Rewards are either dense or sparse depending on the environment.",
            "Results.   Similar to our results on Meta-World and DMControl, we find that RA-DT improves average performance scores across all three settings compared to the baselines (see Figure  26  and Tables  5 ,  6 ,  7  in Appendix  D.5 ), but no method exhibits in-context improvement during evaluation (Figure  27 ).  We further discuss our negative results on Procgen, Meta-World, and DMControl in Section  5 .",
            "Reweighting Experiences.   RA-DT reweights a sub-trajectory by its  relevance  and  utility  score.  By default, we use task-based reweighting during training.  In Figure  28 , we compare against alternatives, such as reweighting by return.  Indeed, we find that task-based reweighting is critical for high performance, because it ensures that retrieved experiences are useful for predicting the next action.",
            "Sensitivity of Reweighting.  In addition, we conduct a sensitivity analysis on    \\alpha italic_  used in the reweighting mechanism (see Equation  4 ) that determines the influence of utility on the retrieval score.  In Figure  6 b, we find that RA-DT performs well for a range of values for    \\alpha italic_  used during training, but performance declines if no re-weighting is employed (  = 0  0 \\alpha=0 italic_ = 0 ).  We perform the same analysis for    \\alpha italic_  during evaluation in Figure  29 .",
            "Different LMs for domain-agnostic RA-DT.   Finally, we investigate how strongly domain-agnostic RA-DT is influenced by the choice of pre-trained LM for the embedding model.  We compare our default choice BERT against other smaller/larger LMs (see Figure  32 ).  We found that BERT performs best and performance decreases with smaller models.",
            "The Procgen benchmark consists of 16 procedurally-generated video games and was designed to test the generalization abilities of RL agents  [Cobbe et al.,  2020 ] .  Unlike other environments considered in this work, Procgen environments emits  3  64  64 3 64 64 3\\times 64\\times 64 3  64  64  images as observations.  All 16 environments share a common action space of 15 discrete actions.  The procedural generation in Procgen is controlled by setting an environment seed.  The environments seed randomizes the background and colour of the environment, but retains the same game dynamics.  This results in visually diverse observations for the same underlying task, as illustrated in Figure  12  for three seeds on the game  starpilot .  The rewards in Procgen can be dense or sparse depending on the environment.",
            "Context Length.   On grid-worlds, we use a context length  C C C italic_C  equivalent to two 2 episodes for AD, DPT and DT.  For example, on  40  20 40 20 40\\times 20 40  20  grids, this results in a sequence length of 6400 ( = 1600  4 absent 1600 4 =1600*4 = 1600  4  for state/action/reward/RTG) for the DT and a sequence length of 4800 for AD.  On Meta-World, DMControl and Procgen, we reduce the sequence context length to 50 steps for DT.  For RA-DT, we use a shorter context length of  C = 50 C 50 C=50 italic_C = 50  transitions across environments, except for  20  20 20 20 20\\times 20 20  20  and  40  20 40 20 40\\times 20 40  20  grids, where we increase the context length to 100.  We want to highlight, that the context length for RA-DT applies to both the input context and the retrieved context.  The retrieved context contains the past, and future context, as described in Section  3.2.1 .  Consequently, the effective context length of RA-DT is  C + 2  C C 2 C C+2*C italic_C + 2  italic_C  and is independent of the episode length.",
            "Embedding Model.   For the embedding model  g  (  ) g  g(\\cdot) italic_g (  ) , we either use a DT pre-trained on the same environment with the same hyperparameters as listed in Section  C , or a pre-trained and frozen LM.  For the pre-trained LM, we use  bert-base-uncased  from the  transformers  library by default.  BERT is an encoder-only LM with 110M parameters, vocabulary size  v = 30522 v 30522 v=30522 italic_v = 30522 , and embedding dimension of  d LM = 768 subscript d LM 768 d_{\\text{LM}}=768 italic_d start_POSTSUBSCRIPT LM end_POSTSUBSCRIPT = 768   [Devlin et al.,  2019 ] .  We apply FrozenHopfield with   = 10  10 \\beta=10 italic_ = 10  to state, action, reward and RTG tokens (see Equation  2 ).  To achieve this, we one-hot encode all discrete input tokens, such as actions in Dark-Room/MazeRunner/Procgen or states in Dark-Room, and rewards/RTGs in the sequence before applying the FH.  For other tokens, such as continuous states/actions as in Meta-World/DMControl, we directly apply the FH.  We evaluate other alternatives for the LM in Appendix  E .",
            "Constructing queries/keys/values.   Regardless of whether  g g g italic_g  is domain-specific or domain-agnostic, we obtain  C C C italic_C  embedded tokens after applying  g g g italic_g  to the input trajectory   i  n subscript  i n \\tau_{in} italic_ start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT .  Subsequently, we apply mean aggregation over the context length  C C C italic_C  to obtain the  d r subscript d r d_{r} italic_d start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT -dimensional query representation.  We experimented with aggregating over all tokens or only tokens of a particular modality (state/action/reward/RTG), and found aggregation over states-only to be most effective (see Appendix  E.4 ).  As described in Section  3.2.1 , we construct the key-value pairs in our retrieval index by embedding all sub-trajectories in the dataset  D D \\mathcal{D} caligraphic_D  using our embedding model  g g g italic_g ,  K  V = { ( g  (  i , t  C : t ) ,  i , t  C : t + C )  1  i  | D | } K V conditional-set g subscript  : i t C t subscript  : i t C t C 1 i D \\mathcal{K}\\times\\mathcal{V}=\\{(g(\\tau_{i,t-C:t}),\\tau_{i,t-C:t+C})\\mid 1\\leq i%  \\leq|\\mathcal{D}|\\} caligraphic_K  caligraphic_V = { ( italic_g ( italic_ start_POSTSUBSCRIPT italic_i , italic_t - italic_C : italic_t end_POSTSUBSCRIPT ) , italic_ start_POSTSUBSCRIPT italic_i , italic_t - italic_C : italic_t + italic_C end_POSTSUBSCRIPT )  1  italic_i  | caligraphic_D | } .  To avoid redundancy, in practice we construct  H / C H C H/C italic_H / italic_C  key-value pairs for a given trajectory    \\tau italic_  with episode length  H H H italic_H  and sub-sequence length  C C C italic_C , instead of constructing the key and values for every step  t  [ 1 , H ] t 1 H t\\in[1,H] italic_t  [ 1 , italic_H ] .  Note that the values, we store   i , t  C : t + C subscript  : i t C t C \\tau_{i,t-C:t+C} italic_ start_POSTSUBSCRIPT italic_i , italic_t - italic_C : italic_t + italic_C end_POSTSUBSCRIPT , contain both the sub-trajectory itself (  i , t  C : t subscript  : i t C t \\tau_{i,t-C:t} italic_ start_POSTSUBSCRIPT italic_i , italic_t - italic_C : italic_t end_POSTSUBSCRIPT ) and its continuation (  i , t : t + C subscript  : i t t C \\tau_{i,t:t+C} italic_ start_POSTSUBSCRIPT italic_i , italic_t : italic_t + italic_C end_POSTSUBSCRIPT ).  Similar to  Borgeaud et al. [ 2022 ] , we found this choice important for high performance in RA-DT, because it allows the model to observe how the trajectory may evolve if it predicts a certain action (given that the retrieved context is similar enough).",
            "Reweighting.   To implement the reweighting mechanism, as described in Section  3.2.3 , we first retrieve the top  l  k much-greater-than l k l\\gg k italic_l  italic_k  experiences and the select the top- k k k italic_k  experiences according to their reweighted scores.  We set  l = 50 l 50 l=50 italic_l = 50  in all our experiments.",
            "One simple strategy to mitigate this issue is  deduplication , i.e., to discard duplicate experiences before the training phase of RA-DT.  To achieve this, we first construct our index as described in Section  3.2 .  For every key  k  K k K \\mathbf{k}\\in\\mathbf{K} bold_k  bold_K , we retrieve the top- k k k italic_k  neighbours (excluding experiences from the same episode as  k k \\mathbf{k} bold_k ).  If the similarity score is above a cosine similarity of  0.98 0.98 0.98 0.98 , we discard the experience.  This substantially reduces the number of experiences in the index and speeds-up retrieval.",
            "Finally, we use the same RTG-conditioning strategy as the vanilla DT, as described in Appendix  C.2 .",
            "In Figures  20  and  21 , we report the average performances at the end of the training (100K) for both the 100 train and 20 evaluation mazes, as well as the corresponding ICL curves, respectively.",
            "While RA-DT outperforms competitors, we observe a considerable performance gap between train mazes and test mazes (0.65 vs. 0.4 reward, see Figure  20 ).  This indicates that RA-DT struggles to solve difficult, unseen mazes.  We believe that this gap is an artifact of the small pre-training distribution of 100 mazes, and be closed by increasing the number of pre-training mazes.  Furthermore, increasing the number of ICL trials may also enhance the performance.",
            "In Figures  22  and  23 , we show the training curves across the entire training period (200K steps), and the corresponding ICL curves at the end of training for both ML45 and ML5.",
            "In Figures  24  and  25 , we show the training curves across the entire training period (200K steps), and the corresponding ICL curves at the end of training for both DMC11 and DMC5.",
            "In Figures  26  and  27 , we show the training curves across the entire training period (200K steps), and the corresponding ICL curves at the end of training for PG12-Seen, PG12-Unseen, and PG4.  While we observe slightly better average performance of RA-DT compared to competitors, we do not find any in-context improvement.",
            "RA-DT constructs bursty sequences. .  Building on work by  Chan et al. [ 2022 ] ,  Raparthy et al. [ 2023 ]  identified trajectory burstiness as one important property for ICL to emerge on the Procgen benchmark.  A given sequence is considered bursty, if it contains at least two trajectories from the same seed (or level).  Consequently, the agent obtains relevant information that it can leverage to predict the next action.  Therefore, we follow  Raparthy et al. [ 2023 ]  and always provide a trajectory from the same seed in the context of AD and DPT.  Indeed, we observed that this improves performance, compared to not taking trajectory burstiness into account.  Interestingly, we found that RA-DT retrieves trajectories from the same or similar seeds (seed accuracy of  80 80 ~{}80 80 %), that is, RA-DT automatically constructs bursty sequences.  This intuitively makes sense, as retrieval directly searches for the most relevant experiences (see Section  3.2.3 ).  Therefore, for RA-DT, we do not provide additional information that indicates with which environment seed the trajectory was generated.",
            "Next, we evaluate how our reweighting mechanism affects the ICL abilities of RA-DT.  RA-DT reweights a sub-trajectory by its  relevance  and  utility  score (see Section  3.2 ).  During training, we set  s u  (  ret ) = 1 subscript s u subscript  ret 1 s_{u}(\\tau_{\\text{ret}})=1 italic_s start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ( italic_ start_POSTSUBSCRIPT ret end_POSTSUBSCRIPT ) = 1 , if the   ret subscript  ret \\tau_{\\text{ret}} italic_ start_POSTSUBSCRIPT ret end_POSTSUBSCRIPT  is from the same task as   in subscript  in \\tau_{\\text{in}} italic_ start_POSTSUBSCRIPT in end_POSTSUBSCRIPT , and 0 otherwise.  Instead of reweighting by task ID, alternatives are to reweight a   ret subscript  ret \\tau_{\\text{ret}} italic_ start_POSTSUBSCRIPT ret end_POSTSUBSCRIPT  by its return achieved or by its position in the training dataset.  When reweighting by position, we assign  s u  (  ret ) = 1 subscript s u subscript  ret 1 s_{u}(\\tau_{\\text{ret}})=1 italic_s start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ( italic_ start_POSTSUBSCRIPT ret end_POSTSUBSCRIPT ) = 1  if   ret subscript  ret \\tau_{\\text{ret}} italic_ start_POSTSUBSCRIPT ret end_POSTSUBSCRIPT  was generated before   in subscript  in \\tau_{\\text{in}} italic_ start_POSTSUBSCRIPT in end_POSTSUBSCRIPT  by the PPO agent that generated the data.  Reweighting by position makes it likely that RA-DT observes the improvement steps in its context.",
            "We find that task-based reweighting is essential for achieving the highest performance scores (see Figure  28 ).  Using no reweighting at all results in a considerable drop in ICL performance.  However, using retrieval with no task reweighting still compares favourably to uniform sampling across all tasks.  This result suggests that retrieval can play an important role in environments without a clear task separation or in scenarios where no task IDs are available.",
            "In addition, we conduct a sensitivity analysis on the    \\alpha italic_  parameter used in the re-weighting mechanism that determines how strongly the utility scores influences the final retrieval score.    = 1  1 \\alpha=1 italic_ = 1  is used both during training for task-based reweighing and during evaluation for return-based reweighting (see Section  3 ).  In Figure  29 , we vary    \\alpha italic_   (a)  during training, or  (b)  during evaluation, while keeping the other fixed.  We find that RA-DT perform well for a range of values, but performance declines if no re-weighting is employed (  = 0  0 \\alpha=0 italic_ = 0 ).",
            "We investigate how strongly the ICL performance of RA-DT is influenced by the pre-trained LM used in our domain-agnostic embedding model.  In Figure  32 , we compare our default choice BERT  [Devlin et al.,  2019 ]  against four alternative encoder and decoder backbones, namely RoBERTa  [Liu et al.,  2019 ] , DistilRoBERTa, DistilBERT  [Sanh et al.,  2019 ]  and DistilGPT2.  We find that RA-DT maintains decent performance across all pre-trained LMs, indicating robust retrieval performance across different LMs.  Generally, the non-distilled variants outperform their distilled counterparts.  Moreover, this experiment suggests a clear advantage of encoder-only models over the decoder-only LM, DistilGPT2.  This suggests that the encoder-only LMs are better able to capture the relations between tokens within the token sequence, which leads to more precise retrieval of sub-trajectories and higher down-stream performance."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Meta-World Evaluation Tasks.",
        "table": "A4.T3.1",
        "footnotes": [],
        "references": [
            "We introduce Retrieval-Augmented Decision Transformer (RA-DT), which incorporates an external memory into the Decision Transformer  [Chen et al.,  2021 , DT]  architecture (see Figure  1 ).  Our external memory enables efficient storage and retrieval of past experiences, that are relevant for the current situation.  We achieve this by leveraging a vector index populated with sub-trajectories, in combination with maximum inner product search; akin to Retrieval-augmented Generation (RAG) in LLMs  [Khandelwal et al.,  2019 ; Lewis et al.,  2020 ; Borgeaud et al.,  2022 ] .  To encode retrieved sub-trajectories, RA-DT relies on a pre-trained embedding model, which can either be domain-specific, such as a DT trained on the same domain, or a domain-agnostic language model (LM) (see Section  3 ).  Subsequently, RA-DT uses cross-attention to leverage the retrieved sub-trajectories and predict the next action.  This way, RA-DT does not rely on a long context and can deal with sparse reward settings.",
            "We evaluate the ICL abilities of RA-DT on grid-world environments used in prior works, namely Dark-Room (see Section  4.1 ), Dark Key-Door (Section  4.2 ), and MazeRunner (Section  4.3 )  [Laskin et al.,  2022 ; Lee et al.,  2022 ; Grigsby et al.,  2023 ] , with increasingly larger grid-sizes, resulting in longer episodes.  Moreover, we evaluate RA-DT on two robotic benchmarks (Meta-World and DMControl, Section  4.4 ) and procedurally-generated video games (Procgen, Section  4.5 ).",
            "Results.   In Figure  3 , we show the ICL performances on the 20 hold-out tasks for all considered methods on Dark-Room  (a) 10  \\times  10,  (b)  20  \\times  20, and  (c)  40  \\times  20.  In addition, we present the ICL curves on the training tasks and the learning curves across the entire training period in Figures  14  and  15  in Appendix  D.1 .  Overall, we observe that RA-DT attains the highest average rewards on all 3 grid-sizes at the end of the 40 ICL-trials.  On 10  \\times  10, RA-DT obtains near-optimal performance scores both with the domain-specific and domain-agnostic embedding model.  The vanilla DT does not exhibit any performance improvement across trials.  This indicates the improvement in performance for RA-DT can be attributed to the retrieval component.  Furthermore, RA-DT outperforms AD and DPT without keeping entire episodes in its context window.  Similarly, RA-DT outperforms all baselines on the 20  \\times  20 and 40  \\times  20 grids.  While RA-DT successfully improves in-context, the baselines exhibit only little learning progress over the ICL trials, especially for larger grid sizes.  However, the final performance scores for 20  \\times  20 and 40  \\times  20 are not optimal.  With increasing grid size, discovering the goal requires systematic exploration in combination with targeted exploitation.  Therefore, we conduct a qualitative analysis on the exploration behaviour of RA-DT.  We find that RA-DT develops strategies to imitate a given successful context (see Figure  16 ), and avoids low-reward routes given an unsuccessful context (see Figure  17 ).",
            "Experiment Setup.   Next, we evaluate RA-DT on two multi-task robotics benchmarks, Meta-World  [Yu et al.,  2020b ]  and DMControl  [Tassa et al.,  2018 ] .  States and actions in both benchmarks are multidimensional continuous vectors.  While the state and action space in Meta-World remain constant across all tasks ( | S | = 39 S 39 |\\mathcal{S}|=39 | caligraphic_S | = 39 ,  | A | = 6 A 6 |\\mathcal{A}|=6 | caligraphic_A | = 6 ), they vary considerably in DMControl ( 3  | S |  24 3 S 24 3\\leq|\\mathcal{S}|\\leq 24 3  | caligraphic_S |  24 ,  1  | A |  6 1 A 6 1\\leq|\\mathcal{A}|\\leq 6 1  | caligraphic_A |  6 ).  Episodes last for 200 and 1000 steps in Meta-World and DMControl, respectively.   We leverage the datasets released by  Schmied et al. [ 2023 ] .  For Meta-World, we pre-train a multi-task policy on 45 of the 50 tasks (ML45, 90M transitions in total) and evaluate on the 5 remaining tasks (ML5).  Similarly, on DMControl, we pre-train on 11 tasks (DMC11, 11M transitions in total) and evaluate on 5 unseen tasks (DMC5).  We provide further details on the environments, datasets, and experiment setup in Appendices  B.3  and  D.3 , and  B.4  and  D.4  for Meta-World and DMControl, respectively.",
            "Results.   We present the learning curves and corresponding ICL curves for Meta-World and DMControl in Figure  22  and  23 , and Figures  24  and  25  in Appendix  D , respectively.  In addition, we provide the raw and data-normalized scores in Tables  3  and  4 , respectively.  On both benchmarks, we find that RA-DT attains considerably higher scores on unseen evaluation tasks, but slightly lower average scores across training tasks compared to DT.  However, these performance gains on evaluation tasks are not reflected in improved ICL performance.  In fact, we only observe slight in-context improvement on training tasks, but not on holdout tasks for any of the considered methods.",
            "Effect of Retrieval Regularization.   We evaluate with three retrieval regularization strategies to mitigate the effect of copying the context: deduplication, similarity cut-off, and query dropout.  To evaluate their impact on ICL performance, we systematically removed each one from RA-DT (see Figure  30 ).  We found the combination of all three to be effective and add them to our pipeline.",
            "Different LMs for domain-agnostic RA-DT.   Finally, we investigate how strongly domain-agnostic RA-DT is influenced by the choice of pre-trained LM for the embedding model.  We compare our default choice BERT against other smaller/larger LMs (see Figure  32 ).  We found that BERT performs best and performance decreases with smaller models.",
            "Memory-Exploitation vs. Meta-learning Abilities.   Current  offline  in-context RL methods are predominantly evaluated on contextual bandits or grid-worlds, such as Dark-Room  [Laskin et al.,  2022 ; Lee et al.,  2023 ; Lin et al.,  2023 ; Sinii et al.,  2023 ; Huang et al.,  2024 ] , which can only be solved by leveraging the context.  However, it remains unclear to what extent the agent learns to learn in-context or simply copies from its context.  Further, in our experiments on fully-observable environments (MetaWorld, DMControl, and Procgen), we did not observe ICL behaviour (see Appendices  D.3 ,  D.4 ,  D.5 ).  Therefore, it is necessary that future research on in-context RL disentangles the effects of memory and meta-learning abilities, similar to memory and credit-assignment  [Ni et al.,  2024 ] . We believe our datasets facilitate future work in this direction.",
            "Source Algorithm performance.   We show the individual learning curves for all tasks in Figure  13 , and the aggregate statistics over all 16 datasets in Table  1 .",
            "Context Length.   On grid-worlds, we use a context length  C C C italic_C  equivalent to two 2 episodes for AD, DPT and DT.  For example, on  40  20 40 20 40\\times 20 40  20  grids, this results in a sequence length of 6400 ( = 1600  4 absent 1600 4 =1600*4 = 1600  4  for state/action/reward/RTG) for the DT and a sequence length of 4800 for AD.  On Meta-World, DMControl and Procgen, we reduce the sequence context length to 50 steps for DT.  For RA-DT, we use a shorter context length of  C = 50 C 50 C=50 italic_C = 50  transitions across environments, except for  20  20 20 20 20\\times 20 20  20  and  40  20 40 20 40\\times 20 40  20  grids, where we increase the context length to 100.  We want to highlight, that the context length for RA-DT applies to both the input context and the retrieved context.  The retrieved context contains the past, and future context, as described in Section  3.2.1 .  Consequently, the effective context length of RA-DT is  C + 2  C C 2 C C+2*C italic_C + 2  italic_C  and is independent of the episode length.",
            "Constructing queries/keys/values.   Regardless of whether  g g g italic_g  is domain-specific or domain-agnostic, we obtain  C C C italic_C  embedded tokens after applying  g g g italic_g  to the input trajectory   i  n subscript  i n \\tau_{in} italic_ start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT .  Subsequently, we apply mean aggregation over the context length  C C C italic_C  to obtain the  d r subscript d r d_{r} italic_d start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT -dimensional query representation.  We experimented with aggregating over all tokens or only tokens of a particular modality (state/action/reward/RTG), and found aggregation over states-only to be most effective (see Appendix  E.4 ).  As described in Section  3.2.1 , we construct the key-value pairs in our retrieval index by embedding all sub-trajectories in the dataset  D D \\mathcal{D} caligraphic_D  using our embedding model  g g g italic_g ,  K  V = { ( g  (  i , t  C : t ) ,  i , t  C : t + C )  1  i  | D | } K V conditional-set g subscript  : i t C t subscript  : i t C t C 1 i D \\mathcal{K}\\times\\mathcal{V}=\\{(g(\\tau_{i,t-C:t}),\\tau_{i,t-C:t+C})\\mid 1\\leq i%  \\leq|\\mathcal{D}|\\} caligraphic_K  caligraphic_V = { ( italic_g ( italic_ start_POSTSUBSCRIPT italic_i , italic_t - italic_C : italic_t end_POSTSUBSCRIPT ) , italic_ start_POSTSUBSCRIPT italic_i , italic_t - italic_C : italic_t + italic_C end_POSTSUBSCRIPT )  1  italic_i  | caligraphic_D | } .  To avoid redundancy, in practice we construct  H / C H C H/C italic_H / italic_C  key-value pairs for a given trajectory    \\tau italic_  with episode length  H H H italic_H  and sub-sequence length  C C C italic_C , instead of constructing the key and values for every step  t  [ 1 , H ] t 1 H t\\in[1,H] italic_t  [ 1 , italic_H ] .  Note that the values, we store   i , t  C : t + C subscript  : i t C t C \\tau_{i,t-C:t+C} italic_ start_POSTSUBSCRIPT italic_i , italic_t - italic_C : italic_t + italic_C end_POSTSUBSCRIPT , contain both the sub-trajectory itself (  i , t  C : t subscript  : i t C t \\tau_{i,t-C:t} italic_ start_POSTSUBSCRIPT italic_i , italic_t - italic_C : italic_t end_POSTSUBSCRIPT ) and its continuation (  i , t : t + C subscript  : i t t C \\tau_{i,t:t+C} italic_ start_POSTSUBSCRIPT italic_i , italic_t : italic_t + italic_C end_POSTSUBSCRIPT ).  Similar to  Borgeaud et al. [ 2022 ] , we found this choice important for high performance in RA-DT, because it allows the model to observe how the trajectory may evolve if it predicts a certain action (given that the retrieved context is similar enough).",
            "Reweighting.   To implement the reweighting mechanism, as described in Section  3.2.3 , we first retrieve the top  l  k much-greater-than l k l\\gg k italic_l  italic_k  experiences and the select the top- k k k italic_k  experiences according to their reweighted scores.  We set  l = 50 l 50 l=50 italic_l = 50  in all our experiments.",
            "One simple strategy to mitigate this issue is  deduplication , i.e., to discard duplicate experiences before the training phase of RA-DT.  To achieve this, we first construct our index as described in Section  3.2 .  For every key  k  K k K \\mathbf{k}\\in\\mathbf{K} bold_k  bold_K , we retrieve the top- k k k italic_k  neighbours (excluding experiences from the same episode as  k k \\mathbf{k} bold_k ).  If the similarity score is above a cosine similarity of  0.98 0.98 0.98 0.98 , we discard the experience.  This substantially reduces the number of experiences in the index and speeds-up retrieval.",
            "Analogous to the ICL curves on the 20 evaluation tasks in Figure  3 , we present ICL curves on the 80 train tasks in Figure  14 .  In general, we observe a similar learning behaviour on the train tasks as on the evaluation tasks, with slightly higher scores on average.  Interestingly, the domain-agnostic variant of RA-DT slightly outperforms its domain-specific counterpart on the training tasks.",
            "In Figures  22  and  23 , we show the training curves across the entire training period (200K steps), and the corresponding ICL curves at the end of training for both ML45 and ML5.",
            "In addition, we provide the average rewards and data-normalized scores in for the MT5 evaluation tasks in Table  3 .",
            "RA-DT constructs bursty sequences. .  Building on work by  Chan et al. [ 2022 ] ,  Raparthy et al. [ 2023 ]  identified trajectory burstiness as one important property for ICL to emerge on the Procgen benchmark.  A given sequence is considered bursty, if it contains at least two trajectories from the same seed (or level).  Consequently, the agent obtains relevant information that it can leverage to predict the next action.  Therefore, we follow  Raparthy et al. [ 2023 ]  and always provide a trajectory from the same seed in the context of AD and DPT.  Indeed, we observed that this improves performance, compared to not taking trajectory burstiness into account.  Interestingly, we found that RA-DT retrieves trajectories from the same or similar seeds (seed accuracy of  80 80 ~{}80 80 %), that is, RA-DT automatically constructs bursty sequences.  This intuitively makes sense, as retrieval directly searches for the most relevant experiences (see Section  3.2.3 ).  Therefore, for RA-DT, we do not provide additional information that indicates with which environment seed the trajectory was generated.",
            "Next, we evaluate how our reweighting mechanism affects the ICL abilities of RA-DT.  RA-DT reweights a sub-trajectory by its  relevance  and  utility  score (see Section  3.2 ).  During training, we set  s u  (  ret ) = 1 subscript s u subscript  ret 1 s_{u}(\\tau_{\\text{ret}})=1 italic_s start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ( italic_ start_POSTSUBSCRIPT ret end_POSTSUBSCRIPT ) = 1 , if the   ret subscript  ret \\tau_{\\text{ret}} italic_ start_POSTSUBSCRIPT ret end_POSTSUBSCRIPT  is from the same task as   in subscript  in \\tau_{\\text{in}} italic_ start_POSTSUBSCRIPT in end_POSTSUBSCRIPT , and 0 otherwise.  Instead of reweighting by task ID, alternatives are to reweight a   ret subscript  ret \\tau_{\\text{ret}} italic_ start_POSTSUBSCRIPT ret end_POSTSUBSCRIPT  by its return achieved or by its position in the training dataset.  When reweighting by position, we assign  s u  (  ret ) = 1 subscript s u subscript  ret 1 s_{u}(\\tau_{\\text{ret}})=1 italic_s start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ( italic_ start_POSTSUBSCRIPT ret end_POSTSUBSCRIPT ) = 1  if   ret subscript  ret \\tau_{\\text{ret}} italic_ start_POSTSUBSCRIPT ret end_POSTSUBSCRIPT  was generated before   in subscript  in \\tau_{\\text{in}} italic_ start_POSTSUBSCRIPT in end_POSTSUBSCRIPT  by the PPO agent that generated the data.  Reweighting by position makes it likely that RA-DT observes the improvement steps in its context.",
            "In addition, we conduct a sensitivity analysis on the    \\alpha italic_  parameter used in the re-weighting mechanism that determines how strongly the utility scores influences the final retrieval score.    = 1  1 \\alpha=1 italic_ = 1  is used both during training for task-based reweighing and during evaluation for return-based reweighting (see Section  3 ).  In Figure  29 , we vary    \\alpha italic_   (a)  during training, or  (b)  during evaluation, while keeping the other fixed.  We find that RA-DT perform well for a range of values, but performance declines if no re-weighting is employed (  = 0  0 \\alpha=0 italic_ = 0 ).",
            "Providing the agent with too similar trajectories, can encourage it to adopt copying behaviour instead of generating high-reward actions.  To mitigate this, we found it useful to regularize the retrieval using three strategies: deduplication, similarity cut-off, and query dropout.  To evaluate their individual impact on ICL performance, we systematically removed each one from RA-DT in Figure  30 .",
            "In RA-DT, we aggregate the hidden states of an input trajectory using mean aggregation of state tokens over the context length  C C C italic_C  to obtain the  d r subscript d r d_{r} italic_d start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT -dimensional query representation.  It is, however, possible to use the hidden states of other tokens to construct the query.  Therefore, we provide empirical evidence for this design choice in Figure  31 a.  We compare aggregating states, rewards, actions, returns-to-gos, all tokens, or only using the very last hidden state.  Indeed, we find that aggregating state tokens gives the best results.",
            "Next, we investigate the effect of the placement of the cross-attention layers in RA-DT.  In Figure  31 b, we therefore vary the placement of cross-attention layers in RA-DT.  By default, we use cross-attention after every self-attention layer.  We find that other choice also provide good results.  While placing the cross-attention at bottom layers tends to be beneficial, placing them only upper level layers tends to hurt performance.",
            "As mentioned in Section  C.4 , we perform context retrieval after every  t t t italic_t  environment steps.  Here,  t t t italic_t  represents a trade-off between inference time and final performance.  For grid-worlds, we use  t = 1 t 1 t=1 italic_t = 1  by default.  To better understand the effect of this design choice, we conduct an ablation in which we vary  t t t italic_t  (see Figure  31 c).  Indeed, we find that higher values for  t t t italic_t  result in a slight decrease in performance, but faster inference.",
            "We investigate how strongly the ICL performance of RA-DT is influenced by the pre-trained LM used in our domain-agnostic embedding model.  In Figure  32 , we compare our default choice BERT  [Devlin et al.,  2019 ]  against four alternative encoder and decoder backbones, namely RoBERTa  [Liu et al.,  2019 ] , DistilRoBERTa, DistilBERT  [Sanh et al.,  2019 ]  and DistilGPT2.  We find that RA-DT maintains decent performance across all pre-trained LMs, indicating robust retrieval performance across different LMs.  Generally, the non-distilled variants outperform their distilled counterparts.  Moreover, this experiment suggests a clear advantage of encoder-only models over the decoder-only LM, DistilGPT2.  This suggests that the encoder-only LMs are better able to capture the relations between tokens within the token sequence, which leads to more precise retrieval of sub-trajectories and higher down-stream performance.",
            "Finally, we investigate the effect of  K K K italic_K  on the performance of AD.   K K K italic_K  determines the number of episodes that have passed between the current and the context trajectory, which are provided to AD as the context.  Consequently,  K K K italic_K  specifies the extent of improvement observed between subsequent episodes.  By default, we use  K = 100 K 100 K=100 italic_K = 100  for our experiments on Dark-Room  10  10 10 10 10\\times 10 10  10 .  Therefore, we conduct an ablation study, in which we very  K K K italic_K  (see Figure  33 .  We find that too small values for  K K K italic_K  (e.g., 1 and 10) result in slow ICL behavior.  In contrast, too high values for  K K K italic_K  (e.g., 500) lead to fast initial progress but suboptimal performance in the long term.  Only  K = 100 K 100 K=100 italic_K = 100  leads to steady improvement across all interaction episodes.  Consequently, AD requires careful tuning of  K K K italic_K ."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  DMControl Eval Tasks.",
        "table": "A4.T4.48",
        "footnotes": [],
        "references": [
            "We aim at augmenting the DT with a vector index (external memory) that allows for retrieval of relevant experiences.  To this end, we build our vector index by leveraging an embedding model  g :   R d r : g maps-to  superscript R subscript d r g:\\tau\\mapsto\\mathbb{R}^{d_{r}} italic_g : italic_  blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  that takes a trajectory    \\tau italic_  and returns a vector of size  d r subscript d r d_{r} italic_d start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT .  Given a dataset  D D \\mathcal{D} caligraphic_D  of trajectories, we obtain a set of key-value pairs of our vector index by embedding all sub-trajectories   t  C : t  D subscript  : t C t D \\tau_{t-C:t}\\in\\mathcal{D} italic_ start_POSTSUBSCRIPT italic_t - italic_C : italic_t end_POSTSUBSCRIPT  caligraphic_D  via  g  (  ) g  g(\\cdot) italic_g (  )  to obtain  K  V = { ( g  (  i , t  C : t ) ,  i , t  C : t + C )  1  i  | D | } K V conditional-set g subscript  : i t C t subscript  : i t C t C 1 i D \\mathcal{K}\\times\\mathcal{V}=\\{(g(\\tau_{i,t-C:t}),\\tau_{i,t-C:t+C})\\mid 1\\leq i%  \\leq|\\mathcal{D}|\\} caligraphic_K  caligraphic_V = { ( italic_g ( italic_ start_POSTSUBSCRIPT italic_i , italic_t - italic_C : italic_t end_POSTSUBSCRIPT ) , italic_ start_POSTSUBSCRIPT italic_i , italic_t - italic_C : italic_t + italic_C end_POSTSUBSCRIPT )  1  italic_i  | caligraphic_D | } .  Note that values contain sub-trajectories ranging from  t  C t C t-C italic_t - italic_C  to  t + C t C t+C italic_t + italic_C , while keys use sub-trajectories  t  C : t : t C t t-C:t italic_t - italic_C : italic_t  for a fixed  C C C italic_C , where  t t t italic_t  goes over trajectory length in increments of  C C C italic_C  (see Appendix  C.4  for more details).  The reason for this choice is that during inference, the model does not have access to future states.",
            "Given an input sub-trajectory   in  D subscript  in D \\tau_{\\text{in}}\\in\\mathcal{D} italic_ start_POSTSUBSCRIPT in end_POSTSUBSCRIPT  caligraphic_D , we first construct a query  q = g  (  in ) q g subscript  in \\bm{q}=g(\\tau_{\\text{in}}) bold_italic_q = italic_g ( italic_ start_POSTSUBSCRIPT in end_POSTSUBSCRIPT ) , using our embedding model  g  (  ) g  g(\\cdot) italic_g (  )  (see Appendix  C.4  for details).  Then, we use maximum inner product search (MIPS) between  q q \\bm{q} bold_italic_q  and all keys  k  K k K \\bm{k}\\in\\mathcal{K} bold_italic_k  caligraphic_K  and select the corresponding top- l l l italic_l  sub-trajectories   ret  V subscript  ret V \\tau_{\\text{ret}}\\in\\mathcal{V} italic_ start_POSTSUBSCRIPT ret end_POSTSUBSCRIPT  caligraphic_V  by:",
            "where  cossim  ( q , k ) = q  k  q    k  cossim q k  q k norm q norm k \\operatorname{cossim}(\\bm{q},\\bm{k})=\\frac{\\mathbf{q}\\cdot\\mathbf{k}}{\\|%  \\mathbf{q}\\|\\|\\mathbf{k}\\|} roman_cossim ( bold_italic_q , bold_italic_k ) = divide start_ARG bold_q  bold_k end_ARG start_ARG  bold_q   bold_k  end_ARG   is the cosine similarity.  Consequently,  R R \\mathcal{R} caligraphic_R  contains the set of retrieved sub-trajectories and their keys.  Providing too similar experiences to the model may hinder learning  [Yasunaga et al.,  2023 ]  and we apply retrieval regularization during training (see Appendix  C.4 ).",
            "In Algorithm  1 , we show the pseudocode for in-context RL with RA-DT at  inference  time.  In addition, we show RA-DT at  training  time in Algorithm  2  of Appendix  C.4 .",
            "We evaluate the ICL abilities of RA-DT on grid-world environments used in prior works, namely Dark-Room (see Section  4.1 ), Dark Key-Door (Section  4.2 ), and MazeRunner (Section  4.3 )  [Laskin et al.,  2022 ; Lee et al.,  2022 ; Grigsby et al.,  2023 ] , with increasingly larger grid-sizes, resulting in longer episodes.  Moreover, we evaluate RA-DT on two robotic benchmarks (Meta-World and DMControl, Section  4.4 ) and procedurally-generated video games (Procgen, Section  4.5 ).",
            "Results.   In Figure  3 , we show the ICL performances on the 20 hold-out tasks for all considered methods on Dark-Room  (a) 10  \\times  10,  (b)  20  \\times  20, and  (c)  40  \\times  20.  In addition, we present the ICL curves on the training tasks and the learning curves across the entire training period in Figures  14  and  15  in Appendix  D.1 .  Overall, we observe that RA-DT attains the highest average rewards on all 3 grid-sizes at the end of the 40 ICL-trials.  On 10  \\times  10, RA-DT obtains near-optimal performance scores both with the domain-specific and domain-agnostic embedding model.  The vanilla DT does not exhibit any performance improvement across trials.  This indicates the improvement in performance for RA-DT can be attributed to the retrieval component.  Furthermore, RA-DT outperforms AD and DPT without keeping entire episodes in its context window.  Similarly, RA-DT outperforms all baselines on the 20  \\times  20 and 40  \\times  20 grids.  While RA-DT successfully improves in-context, the baselines exhibit only little learning progress over the ICL trials, especially for larger grid sizes.  However, the final performance scores for 20  \\times  20 and 40  \\times  20 are not optimal.  With increasing grid size, discovering the goal requires systematic exploration in combination with targeted exploitation.  Therefore, we conduct a qualitative analysis on the exploration behaviour of RA-DT.  We find that RA-DT develops strategies to imitate a given successful context (see Figure  16 ), and avoids low-reward routes given an unsuccessful context (see Figure  17 ).",
            "Experiment Setup.   In Dark Key-Door, the agent is located in a room with two invisible objects: a key and a door.  The agent has to pick up the invisible key, then navigate to the door.  Because of the presence of two key events, the task-space is combinatorial in the number of grid-cells ( 100 2 = 10000 superscript 100 2 10000 100^{2}=10000 100 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 10000  possible tasks for  10  10 10 10 10\\times 10 10  10 ) and is therefore considered more difficult.  A reward of +1 is obtained once for picking up the key and for every step the agent stands on the door grid-cell after it collected the key.  We retain the same experiment setup as in Section  4.1  and provide further details in Appendix  B.1  (also see Figure  8  for single-task expert scores).",
            "Results.   On  10  10 10 10 10\\times 10 10  10  and  20  20 20 20 20\\times 20 20  20 , RA-DT outperforms baselines, with the performance ranking remaining the same as on Dark-Room (see Figure  4 ).  Surprisingly, domain-agnostic RA-DT outperforms its domain-specific counterpart on  40  20 40 20 40\\times 20 40  20 , which demonstrates that the domain-agnostic embedding model is a promising alternative.  This result indicates that RA-DT can successfully handle environments with more than one key event, even with shorter observed context.",
            "Experiment Setup.   Next, we evaluate RA-DT on two multi-task robotics benchmarks, Meta-World  [Yu et al.,  2020b ]  and DMControl  [Tassa et al.,  2018 ] .  States and actions in both benchmarks are multidimensional continuous vectors.  While the state and action space in Meta-World remain constant across all tasks ( | S | = 39 S 39 |\\mathcal{S}|=39 | caligraphic_S | = 39 ,  | A | = 6 A 6 |\\mathcal{A}|=6 | caligraphic_A | = 6 ), they vary considerably in DMControl ( 3  | S |  24 3 S 24 3\\leq|\\mathcal{S}|\\leq 24 3  | caligraphic_S |  24 ,  1  | A |  6 1 A 6 1\\leq|\\mathcal{A}|\\leq 6 1  | caligraphic_A |  6 ).  Episodes last for 200 and 1000 steps in Meta-World and DMControl, respectively.   We leverage the datasets released by  Schmied et al. [ 2023 ] .  For Meta-World, we pre-train a multi-task policy on 45 of the 50 tasks (ML45, 90M transitions in total) and evaluate on the 5 remaining tasks (ML5).  Similarly, on DMControl, we pre-train on 11 tasks (DMC11, 11M transitions in total) and evaluate on 5 unseen tasks (DMC5).  We provide further details on the environments, datasets, and experiment setup in Appendices  B.3  and  D.3 , and  B.4  and  D.4  for Meta-World and DMControl, respectively.",
            "Results.   We present the learning curves and corresponding ICL curves for Meta-World and DMControl in Figure  22  and  23 , and Figures  24  and  25  in Appendix  D , respectively.  In addition, we provide the raw and data-normalized scores in Tables  3  and  4 , respectively.  On both benchmarks, we find that RA-DT attains considerably higher scores on unseen evaluation tasks, but slightly lower average scores across training tasks compared to DT.  However, these performance gains on evaluation tasks are not reflected in improved ICL performance.  In fact, we only observe slight in-context improvement on training tasks, but not on holdout tasks for any of the considered methods.",
            "Sensitivity of Reweighting.  In addition, we conduct a sensitivity analysis on    \\alpha italic_  used in the reweighting mechanism (see Equation  4 ) that determines the influence of utility on the retrieval score.  In Figure  6 b, we find that RA-DT performs well for a range of values for    \\alpha italic_  used during training, but performance declines if no re-weighting is employed (  = 0  0 \\alpha=0 italic_ = 0 ).  We perform the same analysis for    \\alpha italic_  during evaluation in Figure  29 .",
            "Memory-Exploitation vs. Meta-learning Abilities.   Current  offline  in-context RL methods are predominantly evaluated on contextual bandits or grid-worlds, such as Dark-Room  [Laskin et al.,  2022 ; Lee et al.,  2023 ; Lin et al.,  2023 ; Sinii et al.,  2023 ; Huang et al.,  2024 ] , which can only be solved by leveraging the context.  However, it remains unclear to what extent the agent learns to learn in-context or simply copies from its context.  Further, in our experiments on fully-observable environments (MetaWorld, DMControl, and Procgen), we did not observe ICL behaviour (see Appendices  D.3 ,  D.4 ,  D.5 ).  Therefore, it is necessary that future research on in-context RL disentangles the effects of memory and meta-learning abilities, similar to memory and credit-assignment  [Ni et al.,  2024 ] . We believe our datasets facilitate future work in this direction.",
            "Constructing queries/keys/values.   Regardless of whether  g g g italic_g  is domain-specific or domain-agnostic, we obtain  C C C italic_C  embedded tokens after applying  g g g italic_g  to the input trajectory   i  n subscript  i n \\tau_{in} italic_ start_POSTSUBSCRIPT italic_i italic_n end_POSTSUBSCRIPT .  Subsequently, we apply mean aggregation over the context length  C C C italic_C  to obtain the  d r subscript d r d_{r} italic_d start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT -dimensional query representation.  We experimented with aggregating over all tokens or only tokens of a particular modality (state/action/reward/RTG), and found aggregation over states-only to be most effective (see Appendix  E.4 ).  As described in Section  3.2.1 , we construct the key-value pairs in our retrieval index by embedding all sub-trajectories in the dataset  D D \\mathcal{D} caligraphic_D  using our embedding model  g g g italic_g ,  K  V = { ( g  (  i , t  C : t ) ,  i , t  C : t + C )  1  i  | D | } K V conditional-set g subscript  : i t C t subscript  : i t C t C 1 i D \\mathcal{K}\\times\\mathcal{V}=\\{(g(\\tau_{i,t-C:t}),\\tau_{i,t-C:t+C})\\mid 1\\leq i%  \\leq|\\mathcal{D}|\\} caligraphic_K  caligraphic_V = { ( italic_g ( italic_ start_POSTSUBSCRIPT italic_i , italic_t - italic_C : italic_t end_POSTSUBSCRIPT ) , italic_ start_POSTSUBSCRIPT italic_i , italic_t - italic_C : italic_t + italic_C end_POSTSUBSCRIPT )  1  italic_i  | caligraphic_D | } .  To avoid redundancy, in practice we construct  H / C H C H/C italic_H / italic_C  key-value pairs for a given trajectory    \\tau italic_  with episode length  H H H italic_H  and sub-sequence length  C C C italic_C , instead of constructing the key and values for every step  t  [ 1 , H ] t 1 H t\\in[1,H] italic_t  [ 1 , italic_H ] .  Note that the values, we store   i , t  C : t + C subscript  : i t C t C \\tau_{i,t-C:t+C} italic_ start_POSTSUBSCRIPT italic_i , italic_t - italic_C : italic_t + italic_C end_POSTSUBSCRIPT , contain both the sub-trajectory itself (  i , t  C : t subscript  : i t C t \\tau_{i,t-C:t} italic_ start_POSTSUBSCRIPT italic_i , italic_t - italic_C : italic_t end_POSTSUBSCRIPT ) and its continuation (  i , t : t + C subscript  : i t t C \\tau_{i,t:t+C} italic_ start_POSTSUBSCRIPT italic_i , italic_t : italic_t + italic_C end_POSTSUBSCRIPT ).  Similar to  Borgeaud et al. [ 2022 ] , we found this choice important for high performance in RA-DT, because it allows the model to observe how the trajectory may evolve if it predicts a certain action (given that the retrieved context is similar enough).",
            "Analogous to the ICL curves on the 20 evaluation tasks in Figure  3 , we present ICL curves on the 80 train tasks in Figure  14 .  In general, we observe a similar learning behaviour on the train tasks as on the evaluation tasks, with slightly higher scores on average.  Interestingly, the domain-agnostic variant of RA-DT slightly outperforms its domain-specific counterpart on the training tasks.",
            "In Figures  24  and  25 , we show the training curves across the entire training period (200K steps), and the corresponding ICL curves at the end of training for both DMC11 and DMC5.",
            "In addition, we show the average rewards obtained and corresponding data-normalized scores for all DMC5 evaluation tasks in Table  4 .",
            "To better understand the effect of learning with retrieval, we presented a number of ablation studies on critical components in RA-DT (Section  4.6 ).  We conduct all ablations on Dark-Room  10  10 10 10 10\\times 10 10  10  and otherwise retain the same experiment design choices, as reported in Section  4.1 .",
            "As mentioned in Section  C.4 , we perform context retrieval after every  t t t italic_t  environment steps.  Here,  t t t italic_t  represents a trade-off between inference time and final performance.  For grid-worlds, we use  t = 1 t 1 t=1 italic_t = 1  by default.  To better understand the effect of this design choice, we conduct an ablation in which we vary  t t t italic_t  (see Figure  31 c).  Indeed, we find that higher values for  t t t italic_t  result in a slight decrease in performance, but faster inference."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Procgen Train Tasks, Train Seeds.",
        "table": "A4.T5.104",
        "footnotes": [],
        "references": [
            "We evaluate the ICL abilities of RA-DT on grid-world environments used in prior works, namely Dark-Room (see Section  4.1 ), Dark Key-Door (Section  4.2 ), and MazeRunner (Section  4.3 )  [Laskin et al.,  2022 ; Lee et al.,  2022 ; Grigsby et al.,  2023 ] , with increasingly larger grid-sizes, resulting in longer episodes.  Moreover, we evaluate RA-DT on two robotic benchmarks (Meta-World and DMControl, Section  4.4 ) and procedurally-generated video games (Procgen, Section  4.5 ).",
            "Results.   In Figure  3 , we show the ICL performances on the 20 hold-out tasks for all considered methods on Dark-Room  (a) 10  \\times  10,  (b)  20  \\times  20, and  (c)  40  \\times  20.  In addition, we present the ICL curves on the training tasks and the learning curves across the entire training period in Figures  14  and  15  in Appendix  D.1 .  Overall, we observe that RA-DT attains the highest average rewards on all 3 grid-sizes at the end of the 40 ICL-trials.  On 10  \\times  10, RA-DT obtains near-optimal performance scores both with the domain-specific and domain-agnostic embedding model.  The vanilla DT does not exhibit any performance improvement across trials.  This indicates the improvement in performance for RA-DT can be attributed to the retrieval component.  Furthermore, RA-DT outperforms AD and DPT without keeping entire episodes in its context window.  Similarly, RA-DT outperforms all baselines on the 20  \\times  20 and 40  \\times  20 grids.  While RA-DT successfully improves in-context, the baselines exhibit only little learning progress over the ICL trials, especially for larger grid sizes.  However, the final performance scores for 20  \\times  20 and 40  \\times  20 are not optimal.  With increasing grid size, discovering the goal requires systematic exploration in combination with targeted exploitation.  Therefore, we conduct a qualitative analysis on the exploration behaviour of RA-DT.  We find that RA-DT develops strategies to imitate a given successful context (see Figure  16 ), and avoids low-reward routes given an unsuccessful context (see Figure  17 ).",
            "Results.   We find that RA-DT considerably outperforms all baselines in terms of final performance (see Figure  5 ).  Surprisingly, RA-DT is the only method to improve over the course of the 30 ICL trials.  However, we observe a considerable performance gap between train mazes and test mazes (0.65 vs. 0.4 reward, see Figure  20 ), indicating that solving unseen mazes requires an enhanced ability to generalize and learn from previous trials.",
            "Results.   We present the learning curves and corresponding ICL curves for Meta-World and DMControl in Figure  22  and  23 , and Figures  24  and  25  in Appendix  D , respectively.  In addition, we provide the raw and data-normalized scores in Tables  3  and  4 , respectively.  On both benchmarks, we find that RA-DT attains considerably higher scores on unseen evaluation tasks, but slightly lower average scores across training tasks compared to DT.  However, these performance gains on evaluation tasks are not reflected in improved ICL performance.  In fact, we only observe slight in-context improvement on training tasks, but not on holdout tasks for any of the considered methods.",
            "We follow  Raparthy et al. [ 2023 ]  and use 12 tasks for training (PG12) and 4 tasks for evaluation (PG4). First, we generate datasets by training task-specific PPO agents for 25M timesteps on 200 environment seeds per task in  easy  difficulty.  Then, we pre-train a multi-task policy on the PG12 datasets (24M transitions in total, 2M per task).  We leverage the procedural generation of Procgen and evaluate all models in three settings:  training tasks - seen  (PG12-Seen),  training tasks - unseen  (PG12-Unseen), and  evaluation tasks - unseen  (PG4).  Additional details on the generated datasets and our environment setup are available in Appendices  B.5  and  D.5 .",
            "Results.   Similar to our results on Meta-World and DMControl, we find that RA-DT improves average performance scores across all three settings compared to the baselines (see Figure  26  and Tables  5 ,  6 ,  7  in Appendix  D.5 ), but no method exhibits in-context improvement during evaluation (Figure  27 ).  We further discuss our negative results on Procgen, Meta-World, and DMControl in Section  5 .",
            "Memory-Exploitation vs. Meta-learning Abilities.   Current  offline  in-context RL methods are predominantly evaluated on contextual bandits or grid-worlds, such as Dark-Room  [Laskin et al.,  2022 ; Lee et al.,  2023 ; Lin et al.,  2023 ; Sinii et al.,  2023 ; Huang et al.,  2024 ] , which can only be solved by leveraging the context.  However, it remains unclear to what extent the agent learns to learn in-context or simply copies from its context.  Further, in our experiments on fully-observable environments (MetaWorld, DMControl, and Procgen), we did not observe ICL behaviour (see Appendices  D.3 ,  D.4 ,  D.5 ).  Therefore, it is necessary that future research on in-context RL disentangles the effects of memory and meta-learning abilities, similar to memory and credit-assignment  [Ni et al.,  2024 ] . We believe our datasets facilitate future work in this direction.",
            "Future Work.   Besides the general directions discussed in Section  5 , we highlight a number of concrete approaches to extend RA-DT.  While we focus on in-context improvement without relying on expert demonstrations, pre-filling the external memory with demonstrations may enable RA-DT to perform more complex tasks.  This may be particularly powerful for robotics applications, where expert demonstrations are easy to obtain.  Furthermore, end-to-end training of the retrieval component in RA-DT, similar to  [Izacard et al.,  2022 ] , may result in more precise context retrieval and enhanced down-stream performance.  Finally, we envision that modern recurrent architectures  [Bulatov et al.,  2022 ; Gu & Dao,  2023 ; Beck et al.,  2024 ]  as policy backbones may benefit RA-DT by maintaining hidden states across many episodes.",
            "In addition, we also show the learning curves on Dark-Room  10  10 10 10 10\\times 10 10  10  over the entire training phase in Figure  15 .  We evaluate after every 25K updates and observe a steady improvement in the average performances with every evaluation.",
            "In Figures  24  and  25 , we show the training curves across the entire training period (200K steps), and the corresponding ICL curves at the end of training for both DMC11 and DMC5."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Procgen Train Tasks, Evaluation Seeds.",
        "table": "A4.T6.104",
        "footnotes": [],
        "references": [
            "Results.   In Figure  3 , we show the ICL performances on the 20 hold-out tasks for all considered methods on Dark-Room  (a) 10  \\times  10,  (b)  20  \\times  20, and  (c)  40  \\times  20.  In addition, we present the ICL curves on the training tasks and the learning curves across the entire training period in Figures  14  and  15  in Appendix  D.1 .  Overall, we observe that RA-DT attains the highest average rewards on all 3 grid-sizes at the end of the 40 ICL-trials.  On 10  \\times  10, RA-DT obtains near-optimal performance scores both with the domain-specific and domain-agnostic embedding model.  The vanilla DT does not exhibit any performance improvement across trials.  This indicates the improvement in performance for RA-DT can be attributed to the retrieval component.  Furthermore, RA-DT outperforms AD and DPT without keeping entire episodes in its context window.  Similarly, RA-DT outperforms all baselines on the 20  \\times  20 and 40  \\times  20 grids.  While RA-DT successfully improves in-context, the baselines exhibit only little learning progress over the ICL trials, especially for larger grid sizes.  However, the final performance scores for 20  \\times  20 and 40  \\times  20 are not optimal.  With increasing grid size, discovering the goal requires systematic exploration in combination with targeted exploitation.  Therefore, we conduct a qualitative analysis on the exploration behaviour of RA-DT.  We find that RA-DT develops strategies to imitate a given successful context (see Figure  16 ), and avoids low-reward routes given an unsuccessful context (see Figure  17 ).",
            "Results.   Similar to our results on Meta-World and DMControl, we find that RA-DT improves average performance scores across all three settings compared to the baselines (see Figure  26  and Tables  5 ,  6 ,  7  in Appendix  D.5 ), but no method exhibits in-context improvement during evaluation (Figure  27 ).  We further discuss our negative results on Procgen, Meta-World, and DMControl in Section  5 .",
            "Retrieval outperforms sampling of experiences.   To investigate the effect of learning with retrieved context, we substitute retrieval with random sampling, either over all tasks, or from the same task (see Figure  6 a).  We find that training with retrieval outperforms both sampling variants, highlighting the benefit of training with retrieval to improve ICL abilities.  We hypothesise this is because retrieval constructs bursty sequences, which was found to be important for ICL  [Chan et al.,  2022 ] .",
            "Sensitivity of Reweighting.  In addition, we conduct a sensitivity analysis on    \\alpha italic_  used in the reweighting mechanism (see Equation  4 ) that determines the influence of utility on the retrieval score.  In Figure  6 b, we find that RA-DT performs well for a range of values for    \\alpha italic_  used during training, but performance declines if no re-weighting is employed (  = 0  0 \\alpha=0 italic_ = 0 ).  We perform the same analysis for    \\alpha italic_  during evaluation in Figure  29 .",
            "Vector Index.  We use Faiss  [Johnson et al.,  2019 ; Douze et al.,  2024 ]  to instantiate our vector index  I I \\mathcal{I} caligraphic_I .  This allows us to search our vector index in  O  ( log  M ) O M \\mathcal{O}(\\log M) caligraphic_O ( roman_log italic_M )  time using Hierarchical Navigable Small World (HNSW) graphs.  However, in practice we found it faster to use a Flat index on the GPU as provided by Faiss instead of using HNSW, because our retrieval datasets are small enough.  We use retrieval both during training and during inference.  It is, however, possible to pre-compute the retrieved trajectories for  D D \\mathcal{D} caligraphic_D  prior to the training phase to limit the computational demand of retrieval, as suggested by  Borgeaud et al. [ 2022 ] .  During evaluation, we can retrieve after every environment step or only after every  t t t italic_t  environment steps.  Here,  t t t italic_t  represents a trade-off between inference time and final performance.  We use  t = 1 t 1 t=1 italic_t = 1  for Dark-Room and Dark Key-Door, and  t = 25 t 25 t=25 italic_t = 25  for all other environments (see Appendix  E.6  for an ablation on this design choice).  For all environments, except for Meta-World and DMControl, we provide a single retrieved sub-trajectory in the agents context.  For Meta-World and DMControl, we found that providing more than one retrieved sub-trajectory benefits the agents performance.  Therefore, for these two environments, we retrieve the top-4 sub-trajectories, order them by return achieved in that trajectory, and provide their concatenation as retrieved context for RA-DT.",
            "What happens if an optimal trajectory is retrieved in context?    In Figure  16 , we showcase this example.  The goal location is located at grid cell (4,6).  The attention maps exhibit high attention scores for the state and the RTG at the end of the retrieved trajectory.  We also observe high attention scores for the state similar to the current state and the action selected in that state.  The agent initially imitates the actions in the context trajectory, but deviates further into the episode.  Once the agent reaches the goal state, the attention scores for states and RTGs at the end of the trajectory reduce considerably, because the agent need not pay attention to the retrieved context any more.",
            "What happens if a suboptimal trajectory is retrieved in Context?   Similarly, we show the corresponding example in Figure  17 .  The goal location is again in grid cell (4,6).  The retrieved context trajectory reaches the final state (9,5).  Similar to Figure  16 , the attention maps exhibit high attention scores for the last state and RTG for that state, as well as for a state at a similar timestep.  Previously, RA-DT imitated the action, but in this situation the agent  picks a different route, as the context trajectory does not lead to a successful outcome.",
            "In Figures  26  and  27 , we show the training curves across the entire training period (200K steps), and the corresponding ICL curves at the end of training for PG12-Seen, PG12-Unseen, and PG4.  While we observe slightly better average performance of RA-DT compared to competitors, we do not find any in-context improvement.",
            "To better understand the effect of learning with retrieval, we presented a number of ablation studies on critical components in RA-DT (Section  4.6 ).  We conduct all ablations on Dark-Room  10  10 10 10 10\\times 10 10  10  and otherwise retain the same experiment design choices, as reported in Section  4.1 .",
            "In Figure  6 a,  we show the ICL curves for training RA-DT with retrieved sub-trajectories, sub-trajectories sampled from the same task as the input sequence, and sub-trajectories sampled uniformly across all tasks.  We find that training with retrieval outperforms both sampling variants.  Uniform sampling results in poor ICL performance.  A reason for this, is that context trajectories from a different goal location, are not relevant for predicting actions in the current sequences.  As a result, the model ignores the given context during the training phase, and subsequently is unable to leverage it during inference.  In contrast, sampling sub-trajectories from the same task as the input sequence results in better ICL performance, as the model learns to make use of the context trajectories.  Nevertheless, using retrieval results in even better ICL performance, as sub-trajectories are not only relevant for the current task, but also similar to the current situation."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Procgen Eval Envs.",
        "table": "A4.T7.40",
        "footnotes": [],
        "references": [
            "Experiment Setup.  Dark-Room is commonly used in prior work on in-context RL  [Laskin et al.,  2022 ; Lee et al.,  2023 ] .  The agent is located in an empty room, observes only its x-y coordinates, and has to navigate to an invisible goal state ( | S | = 2 S 2 |\\mathcal{S}|=2 | caligraphic_S | = 2 ,  | A | = 5 A 5 |\\mathcal{A}|=5 | caligraphic_A | = 5 , see Figure  9 ).  A reward of +1 is obtained in every step the agent is located in the goal state.  Because of partial observability, it must leverage memory of previous episodes to find the goal.  We conduct experiments on three different grid sizes, namely 10  \\times  10, 20  \\times  20, and 40  \\times  20, and corresponding episode lengths of 100, 200 and 800, respectively.  We designate 80 and 20 randomly assigned goals as train and evaluation locations, respectively, as in  Lee et al. [ 2023 ] .  We use Proximal Policy Optimization (PPO)  [Schulman et al.,  2017 ]  to generate 100K transitions per goal for 10  \\times  10 and 20  \\times  20 grids and 200K for 40  \\times  20 (see Figure  7  for single task expert scores).  During evaluation, the agent interacts with the environment for 40 ICL trials, and we report the scores at the last evaluation step (100K).  We provide additional details on the environment, the generated data, and the training procedure in Appendix  B.1  and  C .",
            "Results.   In Figure  3 , we show the ICL performances on the 20 hold-out tasks for all considered methods on Dark-Room  (a) 10  \\times  10,  (b)  20  \\times  20, and  (c)  40  \\times  20.  In addition, we present the ICL curves on the training tasks and the learning curves across the entire training period in Figures  14  and  15  in Appendix  D.1 .  Overall, we observe that RA-DT attains the highest average rewards on all 3 grid-sizes at the end of the 40 ICL-trials.  On 10  \\times  10, RA-DT obtains near-optimal performance scores both with the domain-specific and domain-agnostic embedding model.  The vanilla DT does not exhibit any performance improvement across trials.  This indicates the improvement in performance for RA-DT can be attributed to the retrieval component.  Furthermore, RA-DT outperforms AD and DPT without keeping entire episodes in its context window.  Similarly, RA-DT outperforms all baselines on the 20  \\times  20 and 40  \\times  20 grids.  While RA-DT successfully improves in-context, the baselines exhibit only little learning progress over the ICL trials, especially for larger grid sizes.  However, the final performance scores for 20  \\times  20 and 40  \\times  20 are not optimal.  With increasing grid size, discovering the goal requires systematic exploration in combination with targeted exploitation.  Therefore, we conduct a qualitative analysis on the exploration behaviour of RA-DT.  We find that RA-DT develops strategies to imitate a given successful context (see Figure  16 ), and avoids low-reward routes given an unsuccessful context (see Figure  17 ).",
            "Results.   Similar to our results on Meta-World and DMControl, we find that RA-DT improves average performance scores across all three settings compared to the baselines (see Figure  26  and Tables  5 ,  6 ,  7  in Appendix  D.5 ), but no method exhibits in-context improvement during evaluation (Figure  27 ).  We further discuss our negative results on Procgen, Meta-World, and DMControl in Section  5 .",
            "Source Algorithm Performance.  We show average learning curves across all task-specific PPO agents on the 80 training tasks for all grid-sizes in Figures  7  and  8  for Dark-Room and Dark Key-Door, respectively.  For the  10  10 10 10 10\\times 10 10  10  grids, the average performance converges towards optimal performance.  However, on the larger grid sizes, the performances are below the optimum.  This is because it takes the agent longer to discover and collect successful episodes by initially random environment interaction as the grids become larger.",
            "What happens if a suboptimal trajectory is retrieved in Context?   Similarly, we show the corresponding example in Figure  17 .  The goal location is again in grid cell (4,6).  The retrieved context trajectory reaches the final state (9,5).  Similar to Figure  16 , the attention maps exhibit high attention scores for the last state and RTG for that state, as well as for a state at a similar timestep.  Previously, RA-DT imitated the action, but in this situation the agent  picks a different route, as the context trajectory does not lead to a successful outcome.",
            "In Figures  26  and  27 , we show the training curves across the entire training period (200K steps), and the corresponding ICL curves at the end of training for PG12-Seen, PG12-Unseen, and PG4.  While we observe slightly better average performance of RA-DT compared to competitors, we do not find any in-context improvement."
        ]
    },
    "global_footnotes": [
        "GitHub:"
    ]
}