{
    "PAPER'S NUMBER OF TABLES": 4,
    "S4.T1": {
        "caption": "TABLE I: Rotation and translation errors with various λ𝜆\\lambda and a various number of target views. We report the mean and standard deviation over 10 trials.",
        "table": "<table id=\"S4.T1.13.11\" class=\"ltx_tabular ltx_guessed_headers ltx_align_top\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.3.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.3.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r\"><math id=\"S4.T1.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\lambda\" display=\"inline\"><semantics id=\"S4.T1.3.1.1.1.m1.1a\"><mi id=\"S4.T1.3.1.1.1.m1.1.1\" xref=\"S4.T1.3.1.1.1.m1.1.1.cmml\">λ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.3.1.1.1.m1.1b\"><ci id=\"S4.T1.3.1.1.1.m1.1.1.cmml\" xref=\"S4.T1.3.1.1.1.m1.1.1\">𝜆</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.3.1.1.1.m1.1c\">\\lambda</annotation></semantics></math></th>\n<th id=\"S4.T1.3.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Rot. (deg)</th>\n<th id=\"S4.T1.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Trans. (m)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.5.3.3\" class=\"ltx_tr\">\n<th id=\"S4.T1.5.3.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">0.0</th>\n<td id=\"S4.T1.4.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\">24.3<math id=\"S4.T1.4.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T1.4.2.2.1.m1.1a\"><mo id=\"S4.T1.4.2.2.1.m1.1.1\" xref=\"S4.T1.4.2.2.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.4.2.2.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T1.4.2.2.1.m1.1.1.cmml\" xref=\"S4.T1.4.2.2.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.4.2.2.1.m1.1c\">\\pm</annotation></semantics></math>15.5</td>\n<td id=\"S4.T1.5.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">32.3<math id=\"S4.T1.5.3.3.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T1.5.3.3.2.m1.1a\"><mo id=\"S4.T1.5.3.3.2.m1.1.1\" xref=\"S4.T1.5.3.3.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.5.3.3.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T1.5.3.3.2.m1.1.1.cmml\" xref=\"S4.T1.5.3.3.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.5.3.3.2.m1.1c\">\\pm</annotation></semantics></math>18.7</td>\n</tr>\n<tr id=\"S4.T1.7.5.5\" class=\"ltx_tr\">\n<th id=\"S4.T1.7.5.5.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">0.25</th>\n<td id=\"S4.T1.6.4.4.1\" class=\"ltx_td ltx_align_center\">19.9<math id=\"S4.T1.6.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T1.6.4.4.1.m1.1a\"><mo id=\"S4.T1.6.4.4.1.m1.1.1\" xref=\"S4.T1.6.4.4.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.6.4.4.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T1.6.4.4.1.m1.1.1.cmml\" xref=\"S4.T1.6.4.4.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.6.4.4.1.m1.1c\">\\pm</annotation></semantics></math>16.7</td>\n<td id=\"S4.T1.7.5.5.2\" class=\"ltx_td ltx_align_center\">17.7<math id=\"S4.T1.7.5.5.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T1.7.5.5.2.m1.1a\"><mo id=\"S4.T1.7.5.5.2.m1.1.1\" xref=\"S4.T1.7.5.5.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.7.5.5.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T1.7.5.5.2.m1.1.1.cmml\" xref=\"S4.T1.7.5.5.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.7.5.5.2.m1.1c\">\\pm</annotation></semantics></math>11.2</td>\n</tr>\n<tr id=\"S4.T1.9.7.7\" class=\"ltx_tr\">\n<th id=\"S4.T1.9.7.7.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">0.5</th>\n<td id=\"S4.T1.8.6.6.1\" class=\"ltx_td ltx_align_center\">12.9<math id=\"S4.T1.8.6.6.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T1.8.6.6.1.m1.1a\"><mo id=\"S4.T1.8.6.6.1.m1.1.1\" xref=\"S4.T1.8.6.6.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.8.6.6.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T1.8.6.6.1.m1.1.1.cmml\" xref=\"S4.T1.8.6.6.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.8.6.6.1.m1.1c\">\\pm</annotation></semantics></math>16.3</td>\n<td id=\"S4.T1.9.7.7.2\" class=\"ltx_td ltx_align_center\">9.62<math id=\"S4.T1.9.7.7.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T1.9.7.7.2.m1.1a\"><mo id=\"S4.T1.9.7.7.2.m1.1.1\" xref=\"S4.T1.9.7.7.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.9.7.7.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T1.9.7.7.2.m1.1.1.cmml\" xref=\"S4.T1.9.7.7.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.9.7.7.2.m1.1c\">\\pm</annotation></semantics></math>11.7</td>\n</tr>\n<tr id=\"S4.T1.11.9.9\" class=\"ltx_tr\">\n<th id=\"S4.T1.11.9.9.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">0.75</th>\n<td id=\"S4.T1.10.8.8.1\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.10.8.8.1.1\" class=\"ltx_text ltx_font_bold\">0.53<math id=\"S4.T1.10.8.8.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T1.10.8.8.1.1.m1.1a\"><mo id=\"S4.T1.10.8.8.1.1.m1.1.1\" xref=\"S4.T1.10.8.8.1.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.10.8.8.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T1.10.8.8.1.1.m1.1.1.cmml\" xref=\"S4.T1.10.8.8.1.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.10.8.8.1.1.m1.1c\">\\pm</annotation></semantics></math>0.96</span></td>\n<td id=\"S4.T1.11.9.9.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.11.9.9.2.1\" class=\"ltx_text ltx_font_bold\">0.19<math id=\"S4.T1.11.9.9.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T1.11.9.9.2.1.m1.1a\"><mo id=\"S4.T1.11.9.9.2.1.m1.1.1\" xref=\"S4.T1.11.9.9.2.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.11.9.9.2.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T1.11.9.9.2.1.m1.1.1.cmml\" xref=\"S4.T1.11.9.9.2.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.11.9.9.2.1.m1.1c\">\\pm</annotation></semantics></math>0.33</span></td>\n</tr>\n<tr id=\"S4.T1.13.11.11\" class=\"ltx_tr\">\n<th id=\"S4.T1.13.11.11.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1.0</th>\n<td id=\"S4.T1.12.10.10.1\" class=\"ltx_td ltx_align_center\">0.92<math id=\"S4.T1.12.10.10.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T1.12.10.10.1.m1.1a\"><mo id=\"S4.T1.12.10.10.1.m1.1.1\" xref=\"S4.T1.12.10.10.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.12.10.10.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T1.12.10.10.1.m1.1.1.cmml\" xref=\"S4.T1.12.10.10.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.12.10.10.1.m1.1c\">\\pm</annotation></semantics></math>1.63</td>\n<td id=\"S4.T1.13.11.11.2\" class=\"ltx_td ltx_align_center\">0.43<math id=\"S4.T1.13.11.11.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T1.13.11.11.2.m1.1a\"><mo id=\"S4.T1.13.11.11.2.m1.1.1\" xref=\"S4.T1.13.11.11.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.13.11.11.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T1.13.11.11.2.m1.1.1.cmml\" xref=\"S4.T1.13.11.11.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.13.11.11.2.m1.1c\">\\pm</annotation></semantics></math>0.78</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "To evaluate our global pose alignment framework, we randomly select one local model as the global model.\nWe also randomly select another model as a local model from the models whose training data partially overlap the area modeled by the global model.\nAfter selecting the models, we compute a center position of the overlapped area as a translation vector, ",
                "t",
                "∗",
                "∈",
                "ℝ",
                "3",
                "superscript",
                "𝑡",
                "∗",
                "superscript",
                "ℝ",
                "3",
                "t^{\\ast}\\in\\mathbb{R}^{3}",
                ", and obtain rotation matrices, ",
                "{",
                "R",
                "j",
                "∗",
                "∈",
                "SO",
                "​",
                "(",
                "3",
                ")",
                "}",
                "j",
                "subscript",
                "subscript",
                "superscript",
                "𝑅",
                "∗",
                "𝑗",
                "SO",
                "3",
                "𝑗",
                "\\{R^{\\ast}_{j}\\in\\mathrm{SO}(3)\\}_{j}",
                ", from camera poses of the training data close to ",
                "t",
                "∗",
                "superscript",
                "𝑡",
                "∗",
                "t^{\\ast}",
                ".\nWe render the images at ",
                "{",
                "(",
                "t",
                "∗",
                ",",
                "R",
                "j",
                "∗",
                ")",
                "}",
                "j",
                "subscript",
                "superscript",
                "𝑡",
                "∗",
                "subscript",
                "superscript",
                "𝑅",
                "∗",
                "𝑗",
                "𝑗",
                "\\{(t^{\\ast},R^{\\ast}_{j})\\}_{j}",
                " as the target values through the global model.\nFinally, we randomly sample a translation vector, ",
                "t",
                "∈",
                "ℝ",
                "3",
                "𝑡",
                "superscript",
                "ℝ",
                "3",
                "t\\in\\mathbb{R}^{3}",
                ", and a rotation matrix, ",
                "R",
                "∈",
                "SO",
                "​",
                "(",
                "3",
                ")",
                "𝑅",
                "SO",
                "3",
                "R\\in\\mathrm{SO}(3)",
                ", which can be regarded as the sensor noises, and optimize them by solving eq. (",
                "5",
                ").\nThus, the optimal values will be ",
                "t",
                "=",
                "(",
                "0",
                ",",
                "0",
                ",",
                "0",
                ")",
                "𝑡",
                "0",
                "0",
                "0",
                "t=(0,0,0)",
                " and ",
                "R",
                "=",
                "I",
                "𝑅",
                "𝐼",
                "R=I",
                ", and we report the gap from them.\nNote that since we use the appearance vector, as described in ",
                "IV-A",
                ", we also optimize it in addition to the pose.\nFor optimization, we use the Adam optimizer ",
                "[",
                "37",
                "]",
                " with a batch size of 4096.\nWe set an initial learning rate to 5e-4 and decay it to 5e-5.",
                "As an ablation study, we show the rotation errors and translation errors with various ",
                "λ",
                "𝜆",
                "\\lambda",
                " in eq. (",
                "5",
                ") and a various number of target views in Tab. ",
                "I",
                ".\nWe randomly sample noises in a range of [-20 m, 20 m] and [-20",
                "∘",
                ", 20",
                "∘",
                "].",
                "As presented on the left-hand side of Tab. ",
                "I",
                ", RGB information is critical for the pose alignment, but the depth information helps marginally with error reduction.\nThe multiple target views also contribute to reducing errors, as depicted on the right-hand side of Tab. ",
                "I",
                ".",
                "We also show the evaluation results of our alignment framework with various magnitudes of initial errors in Fig. ",
                "3",
                ".\nWe randomly sample the pose noises and then align them.\nWe plot the results for 100 trials in Fig. ",
                "3",
                ".\nOur method can align the translation error up to 75 m, which is comparable to a typical GPS error.\nAlso, it can correct the rotation error that is larger than errors in a typical IMU.\nA few plots in Fig. ",
                "3",
                " indicate relatively large errors after alignment, but we can decrease the errors by increasing the number of particles and resampling rounds in the Monte Carlo-based optimization; in fact, our method can reduce errors even when the initial errors are larger than that of the failure cases.\nIn our method, the increase in computation time is acceptable because alignment will be performed offline and does not require real-time processing.",
                "Note that a few trials with various translation errors have failed, and the failure rate is 5%.\nIt is due to the repetitive pattern in the scene.\nAs shown in Fig. ",
                "4",
                ", there are grid patterns, and the alignment is failure when these patterns occupy the majority of the target image."
            ]
        ]
    },
    "S4.T1.13": {
        "caption": "",
        "table": "<table id=\"S4.T1.13.11\" class=\"ltx_tabular ltx_guessed_headers ltx_align_top\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.3.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.3.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r\"><math id=\"S4.T1.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\lambda\" display=\"inline\"><semantics id=\"S4.T1.3.1.1.1.m1.1a\"><mi id=\"S4.T1.3.1.1.1.m1.1.1\" xref=\"S4.T1.3.1.1.1.m1.1.1.cmml\">λ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.3.1.1.1.m1.1b\"><ci id=\"S4.T1.3.1.1.1.m1.1.1.cmml\" xref=\"S4.T1.3.1.1.1.m1.1.1\">𝜆</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.3.1.1.1.m1.1c\">\\lambda</annotation></semantics></math></th>\n<th id=\"S4.T1.3.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Rot. (deg)</th>\n<th id=\"S4.T1.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Trans. (m)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.5.3.3\" class=\"ltx_tr\">\n<th id=\"S4.T1.5.3.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">0.0</th>\n<td id=\"S4.T1.4.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\">24.3<math id=\"S4.T1.4.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T1.4.2.2.1.m1.1a\"><mo id=\"S4.T1.4.2.2.1.m1.1.1\" xref=\"S4.T1.4.2.2.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.4.2.2.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T1.4.2.2.1.m1.1.1.cmml\" xref=\"S4.T1.4.2.2.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.4.2.2.1.m1.1c\">\\pm</annotation></semantics></math>15.5</td>\n<td id=\"S4.T1.5.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">32.3<math id=\"S4.T1.5.3.3.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T1.5.3.3.2.m1.1a\"><mo id=\"S4.T1.5.3.3.2.m1.1.1\" xref=\"S4.T1.5.3.3.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.5.3.3.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T1.5.3.3.2.m1.1.1.cmml\" xref=\"S4.T1.5.3.3.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.5.3.3.2.m1.1c\">\\pm</annotation></semantics></math>18.7</td>\n</tr>\n<tr id=\"S4.T1.7.5.5\" class=\"ltx_tr\">\n<th id=\"S4.T1.7.5.5.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">0.25</th>\n<td id=\"S4.T1.6.4.4.1\" class=\"ltx_td ltx_align_center\">19.9<math id=\"S4.T1.6.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T1.6.4.4.1.m1.1a\"><mo id=\"S4.T1.6.4.4.1.m1.1.1\" xref=\"S4.T1.6.4.4.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.6.4.4.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T1.6.4.4.1.m1.1.1.cmml\" xref=\"S4.T1.6.4.4.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.6.4.4.1.m1.1c\">\\pm</annotation></semantics></math>16.7</td>\n<td id=\"S4.T1.7.5.5.2\" class=\"ltx_td ltx_align_center\">17.7<math id=\"S4.T1.7.5.5.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T1.7.5.5.2.m1.1a\"><mo id=\"S4.T1.7.5.5.2.m1.1.1\" xref=\"S4.T1.7.5.5.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.7.5.5.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T1.7.5.5.2.m1.1.1.cmml\" xref=\"S4.T1.7.5.5.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.7.5.5.2.m1.1c\">\\pm</annotation></semantics></math>11.2</td>\n</tr>\n<tr id=\"S4.T1.9.7.7\" class=\"ltx_tr\">\n<th id=\"S4.T1.9.7.7.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">0.5</th>\n<td id=\"S4.T1.8.6.6.1\" class=\"ltx_td ltx_align_center\">12.9<math id=\"S4.T1.8.6.6.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T1.8.6.6.1.m1.1a\"><mo id=\"S4.T1.8.6.6.1.m1.1.1\" xref=\"S4.T1.8.6.6.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.8.6.6.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T1.8.6.6.1.m1.1.1.cmml\" xref=\"S4.T1.8.6.6.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.8.6.6.1.m1.1c\">\\pm</annotation></semantics></math>16.3</td>\n<td id=\"S4.T1.9.7.7.2\" class=\"ltx_td ltx_align_center\">9.62<math id=\"S4.T1.9.7.7.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T1.9.7.7.2.m1.1a\"><mo id=\"S4.T1.9.7.7.2.m1.1.1\" xref=\"S4.T1.9.7.7.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.9.7.7.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T1.9.7.7.2.m1.1.1.cmml\" xref=\"S4.T1.9.7.7.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.9.7.7.2.m1.1c\">\\pm</annotation></semantics></math>11.7</td>\n</tr>\n<tr id=\"S4.T1.11.9.9\" class=\"ltx_tr\">\n<th id=\"S4.T1.11.9.9.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">0.75</th>\n<td id=\"S4.T1.10.8.8.1\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.10.8.8.1.1\" class=\"ltx_text ltx_font_bold\">0.53<math id=\"S4.T1.10.8.8.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T1.10.8.8.1.1.m1.1a\"><mo id=\"S4.T1.10.8.8.1.1.m1.1.1\" xref=\"S4.T1.10.8.8.1.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.10.8.8.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T1.10.8.8.1.1.m1.1.1.cmml\" xref=\"S4.T1.10.8.8.1.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.10.8.8.1.1.m1.1c\">\\pm</annotation></semantics></math>0.96</span></td>\n<td id=\"S4.T1.11.9.9.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.11.9.9.2.1\" class=\"ltx_text ltx_font_bold\">0.19<math id=\"S4.T1.11.9.9.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T1.11.9.9.2.1.m1.1a\"><mo id=\"S4.T1.11.9.9.2.1.m1.1.1\" xref=\"S4.T1.11.9.9.2.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.11.9.9.2.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T1.11.9.9.2.1.m1.1.1.cmml\" xref=\"S4.T1.11.9.9.2.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.11.9.9.2.1.m1.1c\">\\pm</annotation></semantics></math>0.33</span></td>\n</tr>\n<tr id=\"S4.T1.13.11.11\" class=\"ltx_tr\">\n<th id=\"S4.T1.13.11.11.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1.0</th>\n<td id=\"S4.T1.12.10.10.1\" class=\"ltx_td ltx_align_center\">0.92<math id=\"S4.T1.12.10.10.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T1.12.10.10.1.m1.1a\"><mo id=\"S4.T1.12.10.10.1.m1.1.1\" xref=\"S4.T1.12.10.10.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.12.10.10.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T1.12.10.10.1.m1.1.1.cmml\" xref=\"S4.T1.12.10.10.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.12.10.10.1.m1.1c\">\\pm</annotation></semantics></math>1.63</td>\n<td id=\"S4.T1.13.11.11.2\" class=\"ltx_td ltx_align_center\">0.43<math id=\"S4.T1.13.11.11.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T1.13.11.11.2.m1.1a\"><mo id=\"S4.T1.13.11.11.2.m1.1.1\" xref=\"S4.T1.13.11.11.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.13.11.11.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T1.13.11.11.2.m1.1.1.cmml\" xref=\"S4.T1.13.11.11.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.13.11.11.2.m1.1c\">\\pm</annotation></semantics></math>0.78</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Many FL approaches update models using a synchronous protocol, which needs to wait for all updates on clients before aggregation and delays training.\nThe existence of lagging devices (i.e., stragglers, stale workers) is inevitable due to device heterogeneity and network unreliability.",
            "where 𝐫t,Rsubscript𝐫𝑡𝑅\\mathbf{r}_{t,R} denotes the ray depending on the camera pose (t,R)𝑡𝑅(t,R), and D^gsubscript^𝐷𝑔\\hat{D}_{g} and D^^𝐷\\hat{D} denote the rendered depth through the global and local models, respectively, which are computed by replacing color values, cisubscript𝑐𝑖c_{i}, in eq. (2) with the sampled point position, sisubscript𝑠𝑖s_{i}, as in [3]; λ∈[0,1]𝜆01\\lambda\\in[0,1] is a weight parameter.\nAfter optimization, we align the global pose of the local model based on t^^𝑡\\hat{t} and R^^𝑅\\hat{R}.\nSince the rendering process can be differentiable with respect to t𝑡t and R𝑅R, we optimize them using the gradient method, as in the existing NeRF-based localization methods [2, 7, 13].\nIn our experiments, we used the same optimization pipeline as that in Lin et al. [13] to optimize the pose, which is a Monte Carlo-based method.\nNote that, unlike existing methods [2, 7, 13], we leverage multiple views and rendered depth to make optimization stable.\nWe assess the effectiveness of the multiple target views and the depth loss in the experiment section.",
            "We use InstantNGP [36] as the local model because it converges faster and has fewer floating-point number operations than the original NeRF architecture, which is suitable if the computational resources of the clients are limited.\nEach local model is trained for one epoch333One epoch corresponds to the iterations of #pixels / #batchsize. with a batch size of 8192.\nWe optimize the local models with Adam [37], whose hyperparameters are the same as those of InstantNGP[36], except for the learning rate; it is set to 5e-3 only for the hash encoding and 5e-4 for the other parameters.",
            "We also show the evaluation results of our alignment framework with various magnitudes of initial errors in Fig. 3.\nWe randomly sample the pose noises and then align them.\nWe plot the results for 100 trials in Fig. 3.\nOur method can align the translation error up to 75 m, which is comparable to a typical GPS error.\nAlso, it can correct the rotation error that is larger than errors in a typical IMU.\nA few plots in Fig. 3 indicate relatively large errors after alignment, but we can decrease the errors by increasing the number of particles and resampling rounds in the Monte Carlo-based optimization; in fact, our method can reduce errors even when the initial errors are larger than that of the failure cases.\nIn our method, the increase in computation time is acceptable because alignment will be performed offline and does not require real-time processing.",
            "The training time of distributed training and ours is much shorter than that of the baseline because of the distributed training protocol.\nSince the scale of Mill19 is relatively small compared to our envisioned scenario, the training time of the baseline is acceptable.\nHowever, if we scale it to an earth-scale, it is impossible to train a model with the standard training pipeline.\nIn addition to the reduction in training time, our method can alleviate bandwidth consumption because the size of the local model is 0.1 GB while that of the local data is up to 1 GB.\nThe rendering speed for the distributed training is slower than the others because the distributed training combines the outputs of the multiple models to render one image.\nRendering by ours is the fastest because it only samples cached outputs from the voxel grids for rendering, unlike the others that consist of hash encoding and MLP."
        ]
    },
    "S4.T1.21": {
        "caption": "",
        "table": "<table id=\"S4.T1.21.8\" class=\"ltx_tabular ltx_guessed_headers ltx_align_top\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.21.8.9.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.21.8.9.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r\">#views</th>\n<th id=\"S4.T1.21.8.9.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Rot. (deg)</th>\n<th id=\"S4.T1.21.8.9.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Trans. (m)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.15.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T1.15.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">1</th>\n<td id=\"S4.T1.14.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">0.34<math id=\"S4.T1.14.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T1.14.1.1.1.m1.1a\"><mo id=\"S4.T1.14.1.1.1.m1.1.1\" xref=\"S4.T1.14.1.1.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.14.1.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T1.14.1.1.1.m1.1.1.cmml\" xref=\"S4.T1.14.1.1.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.14.1.1.1.m1.1c\">\\pm</annotation></semantics></math>0.24</td>\n<td id=\"S4.T1.15.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.97<math id=\"S4.T1.15.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T1.15.2.2.2.m1.1a\"><mo id=\"S4.T1.15.2.2.2.m1.1.1\" xref=\"S4.T1.15.2.2.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.15.2.2.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T1.15.2.2.2.m1.1.1.cmml\" xref=\"S4.T1.15.2.2.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.15.2.2.2.m1.1c\">\\pm</annotation></semantics></math>0.68</td>\n</tr>\n<tr id=\"S4.T1.17.4.4\" class=\"ltx_tr\">\n<th id=\"S4.T1.17.4.4.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">2</th>\n<td id=\"S4.T1.16.3.3.1\" class=\"ltx_td ltx_align_center\">0.33<math id=\"S4.T1.16.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T1.16.3.3.1.m1.1a\"><mo id=\"S4.T1.16.3.3.1.m1.1.1\" xref=\"S4.T1.16.3.3.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.16.3.3.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T1.16.3.3.1.m1.1.1.cmml\" xref=\"S4.T1.16.3.3.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.16.3.3.1.m1.1c\">\\pm</annotation></semantics></math>0.24</td>\n<td id=\"S4.T1.17.4.4.2\" class=\"ltx_td ltx_align_center\">0.49<math id=\"S4.T1.17.4.4.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T1.17.4.4.2.m1.1a\"><mo id=\"S4.T1.17.4.4.2.m1.1.1\" xref=\"S4.T1.17.4.4.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.17.4.4.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T1.17.4.4.2.m1.1.1.cmml\" xref=\"S4.T1.17.4.4.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.17.4.4.2.m1.1c\">\\pm</annotation></semantics></math>1.18</td>\n</tr>\n<tr id=\"S4.T1.19.6.6\" class=\"ltx_tr\">\n<th id=\"S4.T1.19.6.6.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">4</th>\n<td id=\"S4.T1.18.5.5.1\" class=\"ltx_td ltx_align_center\">0.53<math id=\"S4.T1.18.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T1.18.5.5.1.m1.1a\"><mo id=\"S4.T1.18.5.5.1.m1.1.1\" xref=\"S4.T1.18.5.5.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.18.5.5.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T1.18.5.5.1.m1.1.1.cmml\" xref=\"S4.T1.18.5.5.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.18.5.5.1.m1.1c\">\\pm</annotation></semantics></math>0.96</td>\n<td id=\"S4.T1.19.6.6.2\" class=\"ltx_td ltx_align_center\">0.19<math id=\"S4.T1.19.6.6.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T1.19.6.6.2.m1.1a\"><mo id=\"S4.T1.19.6.6.2.m1.1.1\" xref=\"S4.T1.19.6.6.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.19.6.6.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T1.19.6.6.2.m1.1.1.cmml\" xref=\"S4.T1.19.6.6.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.19.6.6.2.m1.1c\">\\pm</annotation></semantics></math>0.33</td>\n</tr>\n<tr id=\"S4.T1.21.8.8\" class=\"ltx_tr\">\n<th id=\"S4.T1.21.8.8.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">8</th>\n<td id=\"S4.T1.20.7.7.1\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.20.7.7.1.1\" class=\"ltx_text ltx_font_bold\">0.12<math id=\"S4.T1.20.7.7.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T1.20.7.7.1.1.m1.1a\"><mo id=\"S4.T1.20.7.7.1.1.m1.1.1\" xref=\"S4.T1.20.7.7.1.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.20.7.7.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T1.20.7.7.1.1.m1.1.1.cmml\" xref=\"S4.T1.20.7.7.1.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.20.7.7.1.1.m1.1c\">\\pm</annotation></semantics></math>0.09</span></td>\n<td id=\"S4.T1.21.8.8.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.21.8.8.2.1\" class=\"ltx_text ltx_font_bold\">0.02<math id=\"S4.T1.21.8.8.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T1.21.8.8.2.1.m1.1a\"><mo id=\"S4.T1.21.8.8.2.1.m1.1.1\" xref=\"S4.T1.21.8.8.2.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.21.8.8.2.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T1.21.8.8.2.1.m1.1.1.cmml\" xref=\"S4.T1.21.8.8.2.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.21.8.8.2.1.m1.1c\">\\pm</annotation></semantics></math>0.01</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Many FL approaches update models using a synchronous protocol, which needs to wait for all updates on clients before aggregation and delays training.\nThe existence of lagging devices (i.e., stragglers, stale workers) is inevitable due to device heterogeneity and network unreliability.",
            "where 𝐫t,Rsubscript𝐫𝑡𝑅\\mathbf{r}_{t,R} denotes the ray depending on the camera pose (t,R)𝑡𝑅(t,R), and D^gsubscript^𝐷𝑔\\hat{D}_{g} and D^^𝐷\\hat{D} denote the rendered depth through the global and local models, respectively, which are computed by replacing color values, cisubscript𝑐𝑖c_{i}, in eq. (2) with the sampled point position, sisubscript𝑠𝑖s_{i}, as in [3]; λ∈[0,1]𝜆01\\lambda\\in[0,1] is a weight parameter.\nAfter optimization, we align the global pose of the local model based on t^^𝑡\\hat{t} and R^^𝑅\\hat{R}.\nSince the rendering process can be differentiable with respect to t𝑡t and R𝑅R, we optimize them using the gradient method, as in the existing NeRF-based localization methods [2, 7, 13].\nIn our experiments, we used the same optimization pipeline as that in Lin et al. [13] to optimize the pose, which is a Monte Carlo-based method.\nNote that, unlike existing methods [2, 7, 13], we leverage multiple views and rendered depth to make optimization stable.\nWe assess the effectiveness of the multiple target views and the depth loss in the experiment section.",
            "We use InstantNGP [36] as the local model because it converges faster and has fewer floating-point number operations than the original NeRF architecture, which is suitable if the computational resources of the clients are limited.\nEach local model is trained for one epoch333One epoch corresponds to the iterations of #pixels / #batchsize. with a batch size of 8192.\nWe optimize the local models with Adam [37], whose hyperparameters are the same as those of InstantNGP[36], except for the learning rate; it is set to 5e-3 only for the hash encoding and 5e-4 for the other parameters.",
            "We also show the evaluation results of our alignment framework with various magnitudes of initial errors in Fig. 3.\nWe randomly sample the pose noises and then align them.\nWe plot the results for 100 trials in Fig. 3.\nOur method can align the translation error up to 75 m, which is comparable to a typical GPS error.\nAlso, it can correct the rotation error that is larger than errors in a typical IMU.\nA few plots in Fig. 3 indicate relatively large errors after alignment, but we can decrease the errors by increasing the number of particles and resampling rounds in the Monte Carlo-based optimization; in fact, our method can reduce errors even when the initial errors are larger than that of the failure cases.\nIn our method, the increase in computation time is acceptable because alignment will be performed offline and does not require real-time processing.",
            "The training time of distributed training and ours is much shorter than that of the baseline because of the distributed training protocol.\nSince the scale of Mill19 is relatively small compared to our envisioned scenario, the training time of the baseline is acceptable.\nHowever, if we scale it to an earth-scale, it is impossible to train a model with the standard training pipeline.\nIn addition to the reduction in training time, our method can alleviate bandwidth consumption because the size of the local model is 0.1 GB while that of the local data is up to 1 GB.\nThe rendering speed for the distributed training is slower than the others because the distributed training combines the outputs of the multiple models to render one image.\nRendering by ours is the fastest because it only samples cached outputs from the voxel grids for rendering, unlike the others that consist of hash encoding and MLP."
        ]
    },
    "S4.T2": {
        "caption": "TABLE II: Performance for different training protocols on building and rubble scenes of Mill19.",
        "table": "<table id=\"S4.T2.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.4.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.4.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_border_r\"></th>\n<th id=\"S4.T2.4.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" colspan=\"2\">Building</th>\n<th id=\"S4.T2.4.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" colspan=\"2\">Rubble</th>\n<th id=\"S4.T2.4.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Rendering Speed</th>\n</tr>\n<tr id=\"S4.T2.4.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.4.2.2.1\" class=\"ltx_td ltx_th ltx_th_column ltx_border_r\"></th>\n<th id=\"S4.T2.4.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">PSNR</th>\n<th id=\"S4.T2.4.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">Training Time (h)</th>\n<th id=\"S4.T2.4.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">PSNR</th>\n<th id=\"S4.T2.4.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">Training Time (h)</th>\n<th id=\"S4.T2.4.2.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">(Pixel/Second)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.4.3.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.4.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Baseline</td>\n<td id=\"S4.T2.4.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">20.24</td>\n<td id=\"S4.T2.4.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">32.4</td>\n<td id=\"S4.T2.4.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">22.01</td>\n<td id=\"S4.T2.4.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">28.0</td>\n<td id=\"S4.T2.4.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">80.4K</td>\n</tr>\n<tr id=\"S4.T2.4.4.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.4.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Distributed Training</td>\n<td id=\"S4.T2.4.4.2.2\" class=\"ltx_td ltx_align_center\">18.43</td>\n<td id=\"S4.T2.4.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">0.75</td>\n<td id=\"S4.T2.4.4.2.4\" class=\"ltx_td ltx_align_center\">19.93</td>\n<td id=\"S4.T2.4.4.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\">0.67</td>\n<td id=\"S4.T2.4.4.2.6\" class=\"ltx_td ltx_align_center\">69.2K</td>\n</tr>\n<tr id=\"S4.T2.4.5.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.4.5.3.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Ours</td>\n<td id=\"S4.T2.4.5.3.2\" class=\"ltx_td ltx_align_center\">17.51</td>\n<td id=\"S4.T2.4.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">0.97</td>\n<td id=\"S4.T2.4.5.3.4\" class=\"ltx_td ltx_align_center\">20.12</td>\n<td id=\"S4.T2.4.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_r\">0.95</td>\n<td id=\"S4.T2.4.5.3.6\" class=\"ltx_td ltx_align_center\">342.6K</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "To evaluate the rendered image quality of our method, we compare the proposed training pipeline with two data-centralized training protocols, baseline and distributed training.\nThe baseline indicates training one model with all data, which corresponds to the ordinary NeRF training pipeline.\nThe distributed training indicates the Mega-NeRF’s training pipeline ",
                "[",
                "11",
                "]",
                "; namely, the scene is divided into grids, and then models are trained with data corresponding to each grid.\nWe divide the scene into 4",
                "×",
                "\\times",
                "4 grids for the distributed training; that is, 16 models are trained, and the number of data for training each model is approximately 150, which is the same as the expected number of data used to train each model of our pipeline.\nFor a fair comparison, we evaluate these training pipelines with the same model and the same hyperparameters as ours, except for the model size in the baseline.\nIn other words, unlike Mega-NeRF, our distributed training do not use appearance modeling, and the number of training iterations is less.\nNote that we assume that the global position is correctly aligned by our position alignment algorithm in this experiment.",
                "We show the evaluation results in Tab. ",
                "II",
                ".\nNote that since the models can be trained in parallel, the longest training time out of all local models is reported as the training time in the distributed learning and ours.",
                "The training time of distributed training and ours is much shorter than that of the baseline because of the distributed training protocol.\nSince the scale of Mill19 is relatively small compared to our envisioned scenario, the training time of the baseline is acceptable.\nHowever, if we scale it to an earth-scale, it is impossible to train a model with the standard training pipeline.\nIn addition to the reduction in training time, our method can alleviate bandwidth consumption because the size of the local model is 0.1 GB while that of the local data is up to 1 GB.\nThe rendering speed for the distributed training is slower than the others because the distributed training combines the outputs of the multiple models to render one image.\nRendering by ours is the fastest because it only samples cached outputs from the voxel grids for rendering, unlike the others that consist of hash encoding and MLP.",
                "The PSNR for our method is worse than that for the baseline for two reasons: each client trains a local model with a relatively small number of data, while the baseline trains the model with a sufficiently large number of data.\nBasically, NeRF requires a sufficiently large number of viewpoints to correctly represent the scene in 3D.\nHowever, some clients do not satisfy this requirement around the test view and the local models trained by such clients cannot represent the scene correctly, as shown in Fig. ",
                "4",
                ".\nConsequently, such local models degrade the global model performance.\nThis disadvantage is also found in distributed training; in fact, PSNR for distributed training is worse than that for the baseline.\nThe other reason is the quantization error in the caching process.\nWe cache the outputs of the local models on the voxel grid in the aggregation step, and such a quantization operation approximates the continuous function by the piece-wise linear function.\nThus, there are approximation errors, which cause the gap between PSNR for distributed training and ours, especially for the building scene that includes high-frequency components, as shown in Fig. ",
                "5",
                ".\nThis error can be reduced by increasing the cached grid size, but it requires larger memory budgets.\nImproving local model training and alleviating the quantization errors are possible directions for future work.",
                "The PSNR for ours on the rubble scene is better than that for distributed training.\nThe quantization errors in the rubble scene are smaller than those in the building scene because images in the rubble scenes are basically composed of low-frequency components.\nTherefore, in scenes where the effect of the errors is small, our training protocol would be better than distributed training in this experimental setting."
            ]
        ]
    }
}