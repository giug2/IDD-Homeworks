{
    "id_table_1": {
        "caption": "Table 1 :  Compression rates at three different quality settings for the five objects in the DiLiGenT-MV dataset. Compression rates are averaged over all  20 20 20 20  views for each object. Results are computed from ground truth normals as well as normals obtained from photometric stereo using  [ 24 ]  and  [ 20 ] .",
        "table": "Sx1.EGx1",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "In practice, many 3D reconstruction pipelines operate at high resolutions and switch to a sparser triangle mesh in the last processing step  [ 30 ,  29 ,  18 ] . However, this ignores the underlying problem: all steps up to the final mesh representation are still performed at full resolution, with obvious negative effects on the computational performance. In contrast to previous work  [ 15 ,  17 ] , we do not aim to speed up computations in the pixel domain. Instead, we introduce a flexible and locally adaptive triangle mesh  before  the normal integration and solve the problem at its origin, see  Fig.   1 .",
            "that is based on curvature and a user-defined approximation error   italic- \\epsilon italic_ , see  Fig.   4 b. However, there are two obstacles: The first issue is the edge length; lengths on screen are subject to foreshortening, while the optimal edge length is not. They cannot be simply compared without compensation ( Sec.   4.1 ). The second issue is that we require a vertex-based curvature measure to obtain the optimal lengths. In  Sec.   3.2 , we discussed how to compute pixel-wise curvatures from normal maps and we will present a solution for computing vertex-based curvatures in screen space in  Sec.   4.2 . Finally, we will discuss the optimization of vertex positions ( Sec.   4.3 ). For all these steps, we assume that the screen is already covered by a triangle mesh. In  Sec.   4.4 , we show how we obtain the initial triangulation of the screen and summarize our screen-space algorithm to obtain the final triangulation. An overview of our approach is given in  Fig.   5 .",
            "In the mesh-based normal integration, we have one unknown depth value per vertex. Depth values within the triangle are obtained through linear interpolation using barycentric coordinates. By linearity, the derivatives in  Eq.   14  are constant within each face  [ 4 ]  and so are the normals. The integral can then be split into a sum of integrals over faces which can be carried out explicitly. The result is a discretized energy that is quadratic in the vertex depths. Taking the derivative, the optimality condition is",
            "where  V f subscript V f \\mathcal{V}_{f} caligraphic_V start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT  are the three vertices of a face  f f f italic_f . Analogous to the pixel-based case, the linear system is underdetermined as  Eq.   15  only contains depth differences,  i.e .  adding a constant offset does not change  E Int subscript E Int E_{\\text{Int}} italic_E start_POSTSUBSCRIPT Int end_POSTSUBSCRIPT .",
            "Readers familiar with the topic of mesh processing will notice the similarity to the seminal cotangent weights for the vertex Laplacian  [ 27 ] . These weights ensure that our integration energy is independent of the triangulation. Generally, the cotan may diverge but as angles in isotropic meshes are close to  60  superscript 60 60^{\\circ} 60 start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , the cotans are well-behaved and yield numerically stable linear systems  [ 4 ] . To our surprise and to the best of our knowledge, this discretized version of  Eq.   14  and its derivation for triangle meshes, has not been published yet.",
            "Unlike previous work, our method is not bound to the image resolution. Instead, the mesh resolution is controlled by a user parameter   italic- \\epsilon italic_ . Therefore, we conduct all experiments at three quality settings: high ( 0.1  mm 0.1 mm 0.1\\,\\text{mm} 0.1 mm ), medium ( 0.3  mm 0.3 mm 0.3\\,\\text{mm} 0.3 mm ) and low ( 1  mm 1 mm 1\\,\\text{mm} 1 mm ), see  Figs.   1  and  8 . With the high setting, we aim for accurate reconstructions, ideally matching pixel-level methods. The low setting focuses on high compression rates, while the medium setting balances quality and compression.",
            "Our evaluation consists of three parts: First, we analyze the compression rates and runtime advantages of our meshing-first approach over pixel-based methods in  Sec.   7.1 . We continue by comparing the reconstruction error on samples of the DiLiGenT-MV dataset  [ 24 ]  in  Sec.   7.2 . Finally, we discuss various design decisions of our algorithm in an ablation study and investigate the reliability of our single user-parameter, see  Sec.   7.3 . The DiLiGenT-MV dataset contains  5 5 5 5  objects with  20 20 20 20  different views, leading to  100 100 100 100  normal maps of real data for the evaluation. It is an extensive and recent dataset but lacks high-resolution normal maps. We compensate for that by testing rendered normal maps of virtual objects with resolutions of up to  8192 2 superscript 8192 2 8192^{2} 8192 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . Additional results are found in the supplemental material.",
            "The core motivation for our meshing-first approach is to substantially reduce the number of free variables  prior  to the integration step. We test all three quality settings of our screen-space meshing algorithm on all  20 20 20 20  views of the DiLiGenT-MV dataset ( Tab.   1 ) with ground truth normals as well as estimated normals from the photometric stereo methods in  [ 24 ]  and  [ 20 ] . Despite the relatively small image size of the DiLiGenT-MV dataset, our mesh representation requires only 6-18% of free variables for high-resolution meshes. For some low-resolution meshes, our method requires only 1% the number of vertices compared to foreground pixels. The results are consistent across all test cases and robust even for less accurate normals. The image resolutions of the DiLiGenT-MV dataset are comparatively low (less than  1  MP 1 MP 1\\,\\text{MP} 1 MP ). To obtain resolutions ranging from  512 2 superscript 512 2 512^{2} 512 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  to  8192 2 superscript 8192 2 8192^{2} 8192 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  pixels, we render normal maps from three highly detailed meshes in Blender. In this application, we only use the high-resolution mesh setting. The resulting plots in  Fig.   6  suggest that the number of vertices grows sublinearly with the number of pixels, meaning that our compression factor increases at higher resolutions. For the female face, we obtain an impressive reduction by about  200 200 200 200  at  8192 2 superscript 8192 2 8192^{2} 8192 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  pixels and maintain runtimes of a few minutes instead of hours."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Average RMSE over all  20 20 20 20  views of the DiLiGenT-MV dataset for pixel-based integration  [ 7 ] ,  [ 34 ] ,  [ 20 ] , a combination of pixel-based integration  [ 7 ]  and subsequent remeshing  [ 12 ]  as well as mesh integration of uniform and our adaptive meshes. Errors are reported in mm.",
        "table": "Sx1.EGx2",
        "footnotes": [
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "that is based on curvature and a user-defined approximation error   italic- \\epsilon italic_ , see  Fig.   4 b. However, there are two obstacles: The first issue is the edge length; lengths on screen are subject to foreshortening, while the optimal edge length is not. They cannot be simply compared without compensation ( Sec.   4.1 ). The second issue is that we require a vertex-based curvature measure to obtain the optimal lengths. In  Sec.   3.2 , we discussed how to compute pixel-wise curvatures from normal maps and we will present a solution for computing vertex-based curvatures in screen space in  Sec.   4.2 . Finally, we will discuss the optimization of vertex positions ( Sec.   4.3 ). For all these steps, we assume that the screen is already covered by a triangle mesh. In  Sec.   4.4 , we show how we obtain the initial triangulation of the screen and summarize our screen-space algorithm to obtain the final triangulation. An overview of our approach is given in  Fig.   5 .",
            "Edges in screen-space are subject to foreshortening,  i.e .  appear shorter on screen than in 3D, see  Fig.   4(a) . This foreshortening is exactly described by the first fundamental form  I   I \\operatorname{\\vec{I}} start_OPFUNCTION over start_ARG roman_I end_ARG end_OPFUNCTION ,  Eq.   5 . We have shown in  Sec.   3.2 , how to derive the first fundamental form from normals. To apply this to the mesh setting, we use the pixel-wise normal map to obtain face normals",
            "Our evaluation consists of three parts: First, we analyze the compression rates and runtime advantages of our meshing-first approach over pixel-based methods in  Sec.   7.1 . We continue by comparing the reconstruction error on samples of the DiLiGenT-MV dataset  [ 24 ]  in  Sec.   7.2 . Finally, we discuss various design decisions of our algorithm in an ablation study and investigate the reliability of our single user-parameter, see  Sec.   7.3 . The DiLiGenT-MV dataset contains  5 5 5 5  objects with  20 20 20 20  different views, leading to  100 100 100 100  normal maps of real data for the evaluation. It is an extensive and recent dataset but lacks high-resolution normal maps. We compensate for that by testing rendered normal maps of virtual objects with resolutions of up to  8192 2 superscript 8192 2 8192^{2} 8192 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . Additional results are found in the supplemental material.",
            "We compare the reconstruction error of our adaptive mesh integration against different pixel-based approaches  [ 34 ,  7 ,  20 ] . Even at the highest mesh resolution, we require on average less than 10% of free variables. Given this level of compression, it is not surprising that pixel-based approaches perform slightly better, see  Tab.   2 . Still, discontinuities remain the main source of integration errors.",
            "Our mesh refinement algorithm consists of three iterative steps, all described and evaluated in previous work or textbooks  [ 3 ,  4 ] . The remaining design choice of the algorithm is our locally adaptive over a uniform triangulation, see  Sec.   7.2 . We create uniform meshes as a baseline by directly setting a constant optimal edge length such that the resulting mesh roughly matches the vertex count of its adaptive counterpart. To exclude possible side-effects of the surface integration, the 2D meshes are non-rigidly fitted to ground truth depth maps. Thus, the remaining RMSE quantifies the error introduced by the mesh representation alone. Although our locally adaptive triangulations always contain equal or fewer vertices than the uniform triangulations, they still achieve a lower alignment error in all cases, see  Sec.   7.2 ."
        ]
    },
    "id_table_3": {
        "caption": "",
        "table": "Sx1.EGx3",
        "footnotes": [],
        "references": [
            "Most state-of-the-art normal integration methods are so-called  variational methods , which find a depth map by minimizing an  L 2 superscript L 2 L^{2} italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  functional containing the difference between the actual depth-map gradients and the observed gradients,  e.g .  from photometric stereo. Broadly, there exist two types of variational methods: Either, functional analysis is used to derive a Poisson equation which is then discretized and solved  [ 21 ]  or the functional itself is discretized  [ 13 ] . Both cases lead to a linear system of equations. Methods using the more robust  L 1 superscript L 1 L^{1} italic_L start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT  norm have been proposed but are computationally more involved  [ 11 ] . More recently, authors have raised concerns about checkerboard artefacts  [ 38 ]  and the Gibbs phenomenon  [ 7 ]  ( Fig.   3 ) occurring in the discretized functional setting. Still, these problems are not innate to the variational approach:  [ 6 ]  and  [ 20 ]  independently proposed a functional using normals over gradients in the pixel-based integration setting. This functional avoids the artefacts mentioned above without resorting to larger stencils for the partial derivatives  [ 37 ]  or introducing additional variables  [ 7 ] . Our method starts with the same functional but discretizes it for general triangle meshes. An overview of variational methods can be found in  [ 28 ] .",
            "This modified Dirichlet energy has only recently been proposed in the context of discontinuity preserving normal integration  [ 6 ]  and depth-from-stereo  [ 20 ] . Using normals instead of depth map gradients overcomes the problem of distortions near sharp corners. This phenomenon, also known as the Gibbs phenomenon  [ 7 ] ,  Fig.   3 , is present in traditional variational approaches and is discussed in more depth within the supplementary material. Relying on discrete differential geometry, we derive our novel mesh-based discretization of the modified Dirichlet energy in  Sec.   5 .",
            "that is based on curvature and a user-defined approximation error   italic- \\epsilon italic_ , see  Fig.   4 b. However, there are two obstacles: The first issue is the edge length; lengths on screen are subject to foreshortening, while the optimal edge length is not. They cannot be simply compared without compensation ( Sec.   4.1 ). The second issue is that we require a vertex-based curvature measure to obtain the optimal lengths. In  Sec.   3.2 , we discussed how to compute pixel-wise curvatures from normal maps and we will present a solution for computing vertex-based curvatures in screen space in  Sec.   4.2 . Finally, we will discuss the optimization of vertex positions ( Sec.   4.3 ). For all these steps, we assume that the screen is already covered by a triangle mesh. In  Sec.   4.4 , we show how we obtain the initial triangulation of the screen and summarize our screen-space algorithm to obtain the final triangulation. An overview of our approach is given in  Fig.   5 .",
            "Edges in screen-space are subject to foreshortening,  i.e .  appear shorter on screen than in 3D, see  Fig.   4(a) . This foreshortening is exactly described by the first fundamental form  I   I \\operatorname{\\vec{I}} start_OPFUNCTION over start_ARG roman_I end_ARG end_OPFUNCTION ,  Eq.   5 . We have shown in  Sec.   3.2 , how to derive the first fundamental form from normals. To apply this to the mesh setting, we use the pixel-wise normal map to obtain face normals",
            "Our evaluation consists of three parts: First, we analyze the compression rates and runtime advantages of our meshing-first approach over pixel-based methods in  Sec.   7.1 . We continue by comparing the reconstruction error on samples of the DiLiGenT-MV dataset  [ 24 ]  in  Sec.   7.2 . Finally, we discuss various design decisions of our algorithm in an ablation study and investigate the reliability of our single user-parameter, see  Sec.   7.3 . The DiLiGenT-MV dataset contains  5 5 5 5  objects with  20 20 20 20  different views, leading to  100 100 100 100  normal maps of real data for the evaluation. It is an extensive and recent dataset but lacks high-resolution normal maps. We compensate for that by testing rendered normal maps of virtual objects with resolutions of up to  8192 2 superscript 8192 2 8192^{2} 8192 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . Additional results are found in the supplemental material."
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "Sx1.EGx4",
        "footnotes": [],
        "references": [
            "In this section, we will briefly summarize some fundamental concepts in differential geometry and the relation between surfaces, normals and curvatures. This knowledge will be required to derive our solutions for screen-space meshing in  Sec.   4  and mesh-based integration in  Sec.   5 .",
            "that is based on curvature and a user-defined approximation error   italic- \\epsilon italic_ , see  Fig.   4 b. However, there are two obstacles: The first issue is the edge length; lengths on screen are subject to foreshortening, while the optimal edge length is not. They cannot be simply compared without compensation ( Sec.   4.1 ). The second issue is that we require a vertex-based curvature measure to obtain the optimal lengths. In  Sec.   3.2 , we discussed how to compute pixel-wise curvatures from normal maps and we will present a solution for computing vertex-based curvatures in screen space in  Sec.   4.2 . Finally, we will discuss the optimization of vertex positions ( Sec.   4.3 ). For all these steps, we assume that the screen is already covered by a triangle mesh. In  Sec.   4.4 , we show how we obtain the initial triangulation of the screen and summarize our screen-space algorithm to obtain the final triangulation. An overview of our approach is given in  Fig.   5 .",
            "Edges in screen-space are subject to foreshortening,  i.e .  appear shorter on screen than in 3D, see  Fig.   4(a) . This foreshortening is exactly described by the first fundamental form  I   I \\operatorname{\\vec{I}} start_OPFUNCTION over start_ARG roman_I end_ARG end_OPFUNCTION ,  Eq.   5 . We have shown in  Sec.   3.2 , how to derive the first fundamental form from normals. To apply this to the mesh setting, we use the pixel-wise normal map to obtain face normals",
            "In the mesh-based normal integration, we have one unknown depth value per vertex. Depth values within the triangle are obtained through linear interpolation using barycentric coordinates. By linearity, the derivatives in  Eq.   14  are constant within each face  [ 4 ]  and so are the normals. The integral can then be split into a sum of integrals over faces which can be carried out explicitly. The result is a discretized energy that is quadratic in the vertex depths. Taking the derivative, the optimality condition is",
            "Readers familiar with the topic of mesh processing will notice the similarity to the seminal cotangent weights for the vertex Laplacian  [ 27 ] . These weights ensure that our integration energy is independent of the triangulation. Generally, the cotan may diverge but as angles in isotropic meshes are close to  60  superscript 60 60^{\\circ} 60 start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , the cotans are well-behaved and yield numerically stable linear systems  [ 4 ] . To our surprise and to the best of our knowledge, this discretized version of  Eq.   14  and its derivation for triangle meshes, has not been published yet."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "Sx1.EGx5",
        "footnotes": [],
        "references": [
            "In this section, we will briefly summarize some fundamental concepts in differential geometry and the relation between surfaces, normals and curvatures. This knowledge will be required to derive our solutions for screen-space meshing in  Sec.   4  and mesh-based integration in  Sec.   5 .",
            "This modified Dirichlet energy has only recently been proposed in the context of discontinuity preserving normal integration  [ 6 ]  and depth-from-stereo  [ 20 ] . Using normals instead of depth map gradients overcomes the problem of distortions near sharp corners. This phenomenon, also known as the Gibbs phenomenon  [ 7 ] ,  Fig.   3 , is present in traditional variational approaches and is discussed in more depth within the supplementary material. Relying on discrete differential geometry, we derive our novel mesh-based discretization of the modified Dirichlet energy in  Sec.   5 .",
            "To create a feature-adaptive mesh before the actual surface integration, we must reliably predict detailed surface areas and adjust the density of the triangle mesh accordingly. While mesh simplification algorithms like  [ 16 ]  achieve an impressive reduction in vertex number, their sole focus on geometric faithfulness often leads to skinny triangles. As we will see in  5 , small angles decrease the numerical stability of the mesh-based integration. In contrast, remeshing algorithms also take mesh quality into account and are much more suited for our use-case. That being said, existing remeshing methods operate on 3D surfaces and are not directly applicable to our case. Searching for a fast and robust method with only a few parameters, we identified the work by Dunyach  et al .   [ 12 ]  as the most suitable candidate. In their algorithm, all edge lengths are optimized towards an optimal edge length",
            "that is based on curvature and a user-defined approximation error   italic- \\epsilon italic_ , see  Fig.   4 b. However, there are two obstacles: The first issue is the edge length; lengths on screen are subject to foreshortening, while the optimal edge length is not. They cannot be simply compared without compensation ( Sec.   4.1 ). The second issue is that we require a vertex-based curvature measure to obtain the optimal lengths. In  Sec.   3.2 , we discussed how to compute pixel-wise curvatures from normal maps and we will present a solution for computing vertex-based curvatures in screen space in  Sec.   4.2 . Finally, we will discuss the optimization of vertex positions ( Sec.   4.3 ). For all these steps, we assume that the screen is already covered by a triangle mesh. In  Sec.   4.4 , we show how we obtain the initial triangulation of the screen and summarize our screen-space algorithm to obtain the final triangulation. An overview of our approach is given in  Fig.   5 .",
            "Edges in screen-space are subject to foreshortening,  i.e .  appear shorter on screen than in 3D, see  Fig.   4(a) . This foreshortening is exactly described by the first fundamental form  I   I \\operatorname{\\vec{I}} start_OPFUNCTION over start_ARG roman_I end_ARG end_OPFUNCTION ,  Eq.   5 . We have shown in  Sec.   3.2 , how to derive the first fundamental form from normals. To apply this to the mesh setting, we use the pixel-wise normal map to obtain face normals",
            "To obtain vertex curvatures, we start by calculating maximum absolute curvature   p ( max ) subscript superscript  max p \\kappa^{(\\text{max})}_{p} italic_ start_POSTSUPERSCRIPT ( max ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT  for each pixel by solving the eigenvalue problem in  Eq.   6 . For this, we require the first and second fundamental forms,  Eq.   5 . Depending on the projections, we use  Eqs.   7  and  8  respectively for the surface gradients and finite difference for gradients of the normal maps. We lift the pixel-wise curvatures to the mesh setting via",
            "After deriving all measurements for curvature and edge lengths in screen-space, we formulate our screen-based remeshing. We start with a uniform triangulation of the pixel grid, where each foreground pixel is split into two triangles. Following previous remeshing methods  [ 3 ,  12 ] , which are visualized in  Fig.   5 , the mesh is iteratively refined by performing the following steps:",
            "where  V f subscript V f \\mathcal{V}_{f} caligraphic_V start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT  are the three vertices of a face  f f f italic_f . Analogous to the pixel-based case, the linear system is underdetermined as  Eq.   15  only contains depth differences,  i.e .  adding a constant offset does not change  E Int subscript E Int E_{\\text{Int}} italic_E start_POSTSUBSCRIPT Int end_POSTSUBSCRIPT ."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "Sx1.EGx6",
        "footnotes": [],
        "references": [
            "To obtain vertex curvatures, we start by calculating maximum absolute curvature   p ( max ) subscript superscript  max p \\kappa^{(\\text{max})}_{p} italic_ start_POSTSUPERSCRIPT ( max ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT  for each pixel by solving the eigenvalue problem in  Eq.   6 . For this, we require the first and second fundamental forms,  Eq.   5 . Depending on the projections, we use  Eqs.   7  and  8  respectively for the surface gradients and finite difference for gradients of the normal maps. We lift the pixel-wise curvatures to the mesh setting via",
            "The core motivation for our meshing-first approach is to substantially reduce the number of free variables  prior  to the integration step. We test all three quality settings of our screen-space meshing algorithm on all  20 20 20 20  views of the DiLiGenT-MV dataset ( Tab.   1 ) with ground truth normals as well as estimated normals from the photometric stereo methods in  [ 24 ]  and  [ 20 ] . Despite the relatively small image size of the DiLiGenT-MV dataset, our mesh representation requires only 6-18% of free variables for high-resolution meshes. For some low-resolution meshes, our method requires only 1% the number of vertices compared to foreground pixels. The results are consistent across all test cases and robust even for less accurate normals. The image resolutions of the DiLiGenT-MV dataset are comparatively low (less than  1  MP 1 MP 1\\,\\text{MP} 1 MP ). To obtain resolutions ranging from  512 2 superscript 512 2 512^{2} 512 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  to  8192 2 superscript 8192 2 8192^{2} 8192 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  pixels, we render normal maps from three highly detailed meshes in Blender. In this application, we only use the high-resolution mesh setting. The resulting plots in  Fig.   6  suggest that the number of vertices grows sublinearly with the number of pixels, meaning that our compression factor increases at higher resolutions. For the female face, we obtain an impressive reduction by about  200 200 200 200  at  8192 2 superscript 8192 2 8192^{2} 8192 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  pixels and maintain runtimes of a few minutes instead of hours."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "Sx1.EGx7",
        "footnotes": [],
        "references": [
            "where  P f subscript P f \\mathcal{P}_{f} caligraphic_P start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT  contains all pixels in the image that are covered by triangle  f f f italic_f . As large triangles occur only in regions with low curvature, where normals vary slowly, the variance of  n  f subscript  n f \\vec{n}_{f} over start_ARG italic_n end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT  is small. Plugging this normal into  Eqs.   7  and  8  respectively, we obtain fundamental forms for each face. With these fundamental forms, the length of an edge  ( i , j )  E i j E (i,j)\\in\\mathcal{E} ( italic_i , italic_j )  caligraphic_E  is",
            "To obtain vertex curvatures, we start by calculating maximum absolute curvature   p ( max ) subscript superscript  max p \\kappa^{(\\text{max})}_{p} italic_ start_POSTSUPERSCRIPT ( max ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT  for each pixel by solving the eigenvalue problem in  Eq.   6 . For this, we require the first and second fundamental forms,  Eq.   5 . Depending on the projections, we use  Eqs.   7  and  8  respectively for the surface gradients and finite difference for gradients of the normal maps. We lift the pixel-wise curvatures to the mesh setting via",
            "Our evaluation consists of three parts: First, we analyze the compression rates and runtime advantages of our meshing-first approach over pixel-based methods in  Sec.   7.1 . We continue by comparing the reconstruction error on samples of the DiLiGenT-MV dataset  [ 24 ]  in  Sec.   7.2 . Finally, we discuss various design decisions of our algorithm in an ablation study and investigate the reliability of our single user-parameter, see  Sec.   7.3 . The DiLiGenT-MV dataset contains  5 5 5 5  objects with  20 20 20 20  different views, leading to  100 100 100 100  normal maps of real data for the evaluation. It is an extensive and recent dataset but lacks high-resolution normal maps. We compensate for that by testing rendered normal maps of virtual objects with resolutions of up to  8192 2 superscript 8192 2 8192^{2} 8192 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . Additional results are found in the supplemental material.",
            "Our mesh refinement algorithm consists of three iterative steps, all described and evaluated in previous work or textbooks  [ 3 ,  4 ] . The remaining design choice of the algorithm is our locally adaptive over a uniform triangulation, see  Sec.   7.2 . We create uniform meshes as a baseline by directly setting a constant optimal edge length such that the resulting mesh roughly matches the vertex count of its adaptive counterpart. To exclude possible side-effects of the surface integration, the 2D meshes are non-rigidly fitted to ground truth depth maps. Thus, the remaining RMSE quantifies the error introduced by the mesh representation alone. Although our locally adaptive triangulations always contain equal or fewer vertices than the uniform triangulations, they still achieve a lower alignment error in all cases, see  Sec.   7.2 ."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "Sx1.EGx8",
        "footnotes": [],
        "references": [
            "where  P f subscript P f \\mathcal{P}_{f} caligraphic_P start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT  contains all pixels in the image that are covered by triangle  f f f italic_f . As large triangles occur only in regions with low curvature, where normals vary slowly, the variance of  n  f subscript  n f \\vec{n}_{f} over start_ARG italic_n end_ARG start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT  is small. Plugging this normal into  Eqs.   7  and  8  respectively, we obtain fundamental forms for each face. With these fundamental forms, the length of an edge  ( i , j )  E i j E (i,j)\\in\\mathcal{E} ( italic_i , italic_j )  caligraphic_E  is",
            "To obtain vertex curvatures, we start by calculating maximum absolute curvature   p ( max ) subscript superscript  max p \\kappa^{(\\text{max})}_{p} italic_ start_POSTSUPERSCRIPT ( max ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT  for each pixel by solving the eigenvalue problem in  Eq.   6 . For this, we require the first and second fundamental forms,  Eq.   5 . Depending on the projections, we use  Eqs.   7  and  8  respectively for the surface gradients and finite difference for gradients of the normal maps. We lift the pixel-wise curvatures to the mesh setting via",
            "Unlike previous work, our method is not bound to the image resolution. Instead, the mesh resolution is controlled by a user parameter   italic- \\epsilon italic_ . Therefore, we conduct all experiments at three quality settings: high ( 0.1  mm 0.1 mm 0.1\\,\\text{mm} 0.1 mm ), medium ( 0.3  mm 0.3 mm 0.3\\,\\text{mm} 0.3 mm ) and low ( 1  mm 1 mm 1\\,\\text{mm} 1 mm ), see  Figs.   1  and  8 . With the high setting, we aim for accurate reconstructions, ideally matching pixel-level methods. The low setting focuses on high compression rates, while the medium setting balances quality and compression.",
            "Lastly, our method relies on a single user parameter to control the density of the mesh, namely   italic- \\epsilon italic_ , the permitted error of the surface. Throughout our evaluations, we consistently reported the results for three settings the parameter. In  Fig.   8 , we set    [ 0.1  mm , 3  mm ] italic- 0.1 mm 3 mm \\epsilon\\in[0.1\\,\\text{mm},3\\,\\text{mm}] italic_  [ 0.1 mm , 3 mm ]  and non-rigidly fit the 2D meshes to ground truth depth maps. The results show a strong correlation between the intended and achieved approximation error. After a dataset-dependent offset, the achieved error grows approximately linearly with   italic- \\epsilon italic_ . Additional results in the supplementary material suggest that this linear relation is still present after integration."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "Sx1.EGx9",
        "footnotes": [],
        "references": [
            "where  S v subscript S v \\mathcal{S}_{v} caligraphic_S start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT  is the star of  v v v italic_v , i.e. the union of all triangles touching  v v v italic_v . Taking the maximum ensures small triangles at sharp corners. Sticking to the work of Dunyach  et al . , we use  Eq.   9  to obtain optimal lengths  L v subscript L v L_{v} italic_L start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT  which are nominally vertex properties. The optimal length for an edge is simply the mean of these vertex properties."
        ]
    },
    "id_table_10": {
        "caption": "",
        "table": "Sx1.EGx10",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "",
        "table": "Sx1.EGx11",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "",
        "table": "Sx1.EGx12",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "",
        "table": "Sx1.EGx13",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "Sx1.EGx14",
        "footnotes": [],
        "references": [
            "In the mesh-based normal integration, we have one unknown depth value per vertex. Depth values within the triangle are obtained through linear interpolation using barycentric coordinates. By linearity, the derivatives in  Eq.   14  are constant within each face  [ 4 ]  and so are the normals. The integral can then be split into a sum of integrals over faces which can be carried out explicitly. The result is a discretized energy that is quadratic in the vertex depths. Taking the derivative, the optimality condition is",
            "Readers familiar with the topic of mesh processing will notice the similarity to the seminal cotangent weights for the vertex Laplacian  [ 27 ] . These weights ensure that our integration energy is independent of the triangulation. Generally, the cotan may diverge but as angles in isotropic meshes are close to  60  superscript 60 60^{\\circ} 60 start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , the cotans are well-behaved and yield numerically stable linear systems  [ 4 ] . To our surprise and to the best of our knowledge, this discretized version of  Eq.   14  and its derivation for triangle meshes, has not been published yet."
        ]
    },
    "id_table_15": {
        "caption": "",
        "table": "Sx1.EGx15",
        "footnotes": [],
        "references": [
            "where  V f subscript V f \\mathcal{V}_{f} caligraphic_V start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT  are the three vertices of a face  f f f italic_f . Analogous to the pixel-based case, the linear system is underdetermined as  Eq.   15  only contains depth differences,  i.e .  adding a constant offset does not change  E Int subscript E Int E_{\\text{Int}} italic_E start_POSTSUBSCRIPT Int end_POSTSUBSCRIPT ."
        ]
    },
    "id_table_16": {
        "caption": "",
        "table": "Sx1.EGx16",
        "footnotes": [],
        "references": []
    },
    "id_table_17": {
        "caption": "",
        "table": "S6.T1.5",
        "footnotes": [
            "",
            ""
        ],
        "references": []
    },
    "id_table_18": {
        "caption": "",
        "table": "S7.T2.112",
        "footnotes": [
            "",
            "",
            "",
            "",
            ""
        ],
        "references": []
    }
}