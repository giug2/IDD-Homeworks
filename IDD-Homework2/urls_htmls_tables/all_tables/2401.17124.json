{
    "PAPER'S NUMBER OF TABLES": 5,
    "S3.T1": {
        "caption": "Table 1: Average (3 trials) and standard deviation of the best test accuracies for generic/personalized models of various methods on CIFAR-10 with different non-IID settings. See also Remark 4.1.",
        "table": "",
        "footnotes": "\n\n\n\n\n\nMethods\nα=1𝛼1\\alpha=1\nα=0.5𝛼0.5\\alpha=0.5\nα=0.1𝛼0.1\\alpha=0.1\n\nGM\nPM\nGM\nPM\nGM\nPM\n\nFedAvg\n85.35 ±plus-or-minus\\pm 0.11\n(80.33 ±plus-or-minus\\pm 0.38)\n80.76 ±plus-or-minus\\pm 0.13\n(74.51 ±plus-or-minus\\pm 0.48)\n73.51 ±plus-or-minus\\pm 0.17\n(72.68 ±plus-or-minus\\pm 0.39)\n\nFedProx\n85.61 ±plus-or-minus\\pm 0.08\n(86.28 ±plus-or-minus\\pm 0.21)\n80.54 ±plus-or-minus\\pm 0.14\n(76.88 ±plus-or-minus\\pm 0.30)\n71.96 ±plus-or-minus\\pm 0.12\n(73.77 ±plus-or-minus\\pm 0.30)\n\nFedDyn\n86.03 ±plus-or-minus\\pm 0.13\n(85.33 ±plus-or-minus\\pm 0.19)\n80.88 ±plus-or-minus\\pm 0.18\n(78.93 ±plus-or-minus\\pm 0.25)\n73.62 ±plus-or-minus\\pm 0.14\n(74.25 ±plus-or-minus\\pm 0.58)\n\nFedGen\n86.17 ±plus-or-minus\\pm 0.32\n( 85.24 ±plus-or-minus\\pm 0.47)\n79.86 ±plus-or-minus\\pm 0.34\n(77.52 ±plus-or-minus\\pm 0.43)\n71.36 ±plus-or-minus\\pm 0.28\n(71.42 ±plus-or-minus\\pm 0.63)\n\nFedAvgM\n85.44 ±plus-or-minus\\pm 0.05\n(82.85 ±plus-or-minus\\pm 0.28)\n81.04 ±plus-or-minus\\pm 0.09\n(75.71 ±plus-or-minus\\pm 0.33)\n72.87 ±plus-or-minus\\pm 0.06\n(72.96 ±plus-or-minus\\pm 0.14)\n\npFedMe\n85.58 ±plus-or-minus\\pm 0.23\n88.17 ±plus-or-minus\\pm 0.17\n79.33 ±plus-or-minus\\pm 0.14\n84.66 ±plus-or-minus\\pm 0.17\n72.11 ±plus-or-minus\\pm 0.23\n81.18 ±plus-or-minus\\pm 0.15\n\nDitto\n85.34 ±plus-or-minus\\pm 0.10\n87.55 ±plus-or-minus\\pm 0.09\n80.70 ±plus-or-minus\\pm 0.13\n83.39 ±plus-or-minus\\pm 0.12\n73.45 ±plus-or-minus\\pm 0.18\n80.08 ±plus-or-minus\\pm 0.20\n\nFedRep\n(85.61 ±plus-or-minus\\pm 0.19)\n87.32 ±plus-or-minus\\pm 0.11\n(80.33 ±plus-or-minus\\pm0.23)\n84.10 ±plus-or-minus\\pm 0.10\n(73.50 ±plus-or-minus\\pm 0.24)\n79.74 ±plus-or-minus\\pm 0.31\n\nFedRoD\n86.02 ±plus-or-minus\\pm 0.12\n91.67 ±plus-or-minus\\pm 0.16\n81.31 ±plus-or-minus\\pm 0.15\n85.91 ±plus-or-minus\\pm 0.15\n74.64 ±plus-or-minus\\pm 0.07\n81.37 ±plus-or-minus\\pm 0.17\n\nFedBABU\n(85.67 ±plus-or-minus\\pm 0.24)\n91.34 ±plus-or-minus\\pm 0.19\n(79.57 ±plus-or-minus\\pm 0.23)\n83.22 ±plus-or-minus\\pm 0.33\n(73.88 ±plus-or-minus\\pm0.19)\n80.58 ±plus-or-minus\\pm 0.22\n\nOurs\n86.37 ±plus-or-minus\\pm 0.15\n92.25 ±plus-or-minus\\pm 0.21\n81.27 ±plus-or-minus\\pm 0.18\n86.59 ±plus-or-minus\\pm 0.17\n75.52 ±plus-or-minus\\pm 0.11\n82.69 ±plus-or-minus\\pm 0.16\n\n\n",
        "references": [
            [
                "Our proposed spectral co-distillation framework combined with our wait-free local training protocol, is given in ",
                "Algorithm",
                " ",
                "1",
                ".\nAs an overview, we begin every communication round ",
                "t",
                "𝑡",
                "t",
                " with the server broadcasting the global generic model ",
                "w",
                "G",
                "t",
                "−",
                "1",
                "superscript",
                "subscript",
                "𝑤",
                "G",
                "𝑡",
                "1",
                "w_{\\text{G}}^{t-1}",
                " to each client for local computation. Each client ",
                "i",
                "𝑖",
                "i",
                " would send back the updated generic model ",
                "w",
                "G",
                ",",
                "i",
                "t",
                "superscript",
                "subscript",
                "𝑤",
                "G",
                "𝑖",
                "𝑡",
                "w_{\\text{G},i}^{t}",
                " after ",
                "E",
                "G",
                "subscript",
                "𝐸",
                "G",
                "E_{\\text{G}}",
                " local computation steps for global model aggregation, then immediately start the personalized model training and continue until the global generic model ",
                "w",
                "G",
                "t",
                "superscript",
                "subscript",
                "𝑤",
                "G",
                "𝑡",
                "w_{\\text{G}}^{t}",
                " is received, which marks the start of the next communication round ",
                "t",
                "+",
                "1",
                "𝑡",
                "1",
                "t+1",
                ".",
                "Remark on convergence analysis.",
                "\nNote that the global loss function includes a weighted sum of the local loss functions and a regularizer. The regularizer is given in the form of the divergence function ",
                "𝔇",
                "𝔇",
                "\\mathfrak{D}",
                ", which is equivalent to KL divergence; cf. Sec. ",
                "3.2",
                ". As demonstrated in ",
                "[",
                "50",
                "]",
                ", the KL divergence usually exhibits convexity in terms of the model parameters. Consequently, since the model training undergoes (stochastic) gradient descent, it is possible to establish a convergence rate for the training of the global model (under the commonly employed assumption of smoothness of the local loss functions)."
            ]
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Average (3 trials) and standard deviation of the best test accuracies for generic/personalized models of various methods on CIFAR-100 with different non-IID settings. See also Remark 4.1.",
        "table": "",
        "footnotes": "\n\n\n\n\n\nMethods\nα=1𝛼1\\alpha=1\nα=0.1𝛼0.1\\alpha=0.1\n\nGM\nPM\nGM\nPM\n\nFedAvg\n48.37 ±plus-or-minus\\pm 0.22\n(52.64 ±plus-or-minus\\pm 0.48)\n38.61 ±plus-or-minus\\pm 0.27\n(39.27 ±plus-or-minus\\pm 0.42)\n\nFedProx\n47.33 ±plus-or-minus\\pm 0.15\n(53.85 ±plus-or-minus\\pm 0.33)\n39.55 ±plus-or-minus\\pm 0.18\n(41.33 ±plus-or-minus\\pm 0.38)\n\nFedDyn\n49.24 ±plus-or-minus\\pm 0.27\n(57.20 ±plus-or-minus\\pm 0.35)\n40.43 ±plus-or-minus\\pm 0.14\n(40.92 ±plus-or-minus\\pm 0.26)\n\nFedAvgM\n48.55 ±plus-or-minus\\pm 0.19\n(55.60 ±plus-or-minus\\pm 0.26)\n39.03 ±plus-or-minus\\pm 0.08\n(40.85 ±plus-or-minus\\pm 0.19)\n\npFedMe\n47.29 ±plus-or-minus\\pm 0.27\n61.52 ±plus-or-minus\\pm 0.25\n38.22 ±plus-or-minus\\pm 0.23\n45.88 ±plus-or-minus\\pm 0.32\n\nDitto\n48.37 ±plus-or-minus\\pm 0.25\n60.47 ±plus-or-minus\\pm 0.27\n39.61 ±plus-or-minus\\pm 0.19\n43.12 ±plus-or-minus\\pm 0.28\n\nFedRep\n(46.32 ±plus-or-minus\\pm 0.23)\n58.76 ±plus-or-minus\\pm 0.36\n(40.11±plus-or-minus\\pm 0.35)\n45.22 ±plus-or-minus\\pm 0.19\n\nFedRoD\n50.07 ±plus-or-minus\\pm 0.16\n62.51 ±plus-or-minus\\pm 0.15\n40.58 ±plus-or-minus\\pm 0.22\n45.99 ±plus-or-minus\\pm 0.14\n\nFedBABU\n(48.52 ±plus-or-minus\\pm 0.30)\n60.33 ±plus-or-minus\\pm 0.28\n(37.35 ±plus-or-minus\\pm 0.29)\n44.72 ±plus-or-minus\\pm 0.28\n\nOurs\n51.39 ±plus-or-minus\\pm 0.22\n63.15 ±plus-or-minus\\pm 0.16\n40.67 ±plus-or-minus\\pm 0.14\n46.82 ±plus-or-minus\\pm 0.23\n\n\n",
        "references": [
            [
                "Our proposed spectral co-distillation framework combined with our wait-free local training protocol, is given in ",
                "Algorithm",
                " ",
                "1",
                ".\nAs an overview, we begin every communication round ",
                "t",
                "𝑡",
                "t",
                " with the server broadcasting the global generic model ",
                "w",
                "G",
                "t",
                "−",
                "1",
                "superscript",
                "subscript",
                "𝑤",
                "G",
                "𝑡",
                "1",
                "w_{\\text{G}}^{t-1}",
                " to each client for local computation. Each client ",
                "i",
                "𝑖",
                "i",
                " would send back the updated generic model ",
                "w",
                "G",
                ",",
                "i",
                "t",
                "superscript",
                "subscript",
                "𝑤",
                "G",
                "𝑖",
                "𝑡",
                "w_{\\text{G},i}^{t}",
                " after ",
                "E",
                "G",
                "subscript",
                "𝐸",
                "G",
                "E_{\\text{G}}",
                " local computation steps for global model aggregation, then immediately start the personalized model training and continue until the global generic model ",
                "w",
                "G",
                "t",
                "superscript",
                "subscript",
                "𝑤",
                "G",
                "𝑡",
                "w_{\\text{G}}^{t}",
                " is received, which marks the start of the next communication round ",
                "t",
                "+",
                "1",
                "𝑡",
                "1",
                "t+1",
                ".",
                "Remark on convergence analysis.",
                "\nNote that the global loss function includes a weighted sum of the local loss functions and a regularizer. The regularizer is given in the form of the divergence function ",
                "𝔇",
                "𝔇",
                "\\mathfrak{D}",
                ", which is equivalent to KL divergence; cf. Sec. ",
                "3.2",
                ". As demonstrated in ",
                "[",
                "50",
                "]",
                ", the KL divergence usually exhibits convexity in terms of the model parameters. Consequently, since the model training undergoes (stochastic) gradient descent, it is possible to establish a convergence rate for the training of the global model (under the commonly employed assumption of smoothness of the local loss functions)."
            ]
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Average (3 trials) and standard deviation of the best test accuracies for generic/personalized models of various methods on iNaturalist-2017 with non-IID setting α=0.1𝛼0.1\\alpha=0.1. See also Remark 4.1.",
        "table": "",
        "footnotes": "\n\n\n\n\n\nMethods\nFedProx\nFedDyn\nDitto\nFedRep\nFedRoD\nFedBABU\nOurs\n\n\n\nGM\n39.46±plus-or-minus\\pm0.39\n39.35±plus-or-minus\\pm0.27\n39.33±plus-or-minus\\pm0.33\n39.81±plus-or-minus\\pm0.41\n40.16±plus-or-minus\\pm0.35\n39.23±plus-or-minus\\pm0.53\n41.75±plus-or-minus\\pm0.37\n\nPM\n41.58±plus-or-minus\\pm0.27\n40.99±plus-or-minus\\pm0.35\n41.88±plus-or-minus\\pm0.41\n42.07±plus-or-minus\\pm0.24\n44.54±plus-or-minus\\pm0.29\n42.36±plus-or-minus\\pm0.44\n45.87±plus-or-minus\\pm0.21\n\n\n",
        "references": [
            [
                "Datasets, DNN models, federated settings, and evaluation metrics.",
                " We evaluated our proposed PFL+ framework with ",
                "N",
                "𝑁",
                "N",
                " clients on CIFAR-10/100 ",
                "[",
                "51",
                "]",
                ", and iNaturalist-2017, using model architectures ResNet-18/34 ",
                "[",
                "52",
                "]",
                " and ResNet-50, respectively.\nFor the experiments on CIFAR-10 (resp. CIFAR-100), we used ",
                "N",
                "=",
                "100",
                "𝑁",
                "100",
                "N=100",
                " (resp. ",
                "N",
                "=",
                "50",
                "𝑁",
                "50",
                "N=50",
                ").\nFor experiments on iNaturalist-2017 ",
                "[",
                "53",
                "]",
                ", we used ",
                "N",
                "=",
                "20",
                "𝑁",
                "20",
                "N=20",
                ". For dataset partition, we used the symmetric Dirichlet distribution to emulate real-world heterogeneous data distributions ",
                "[",
                "9",
                ", ",
                "11",
                "]",
                ", where the heterogeneity is controlled by the concentration parameter ",
                "α",
                "𝛼",
                "\\alpha",
                ". (A smaller ",
                "α",
                "𝛼",
                "\\alpha",
                " indicates a higher degree of data heterogeneity.)\nFor evaluation, we used two performance metrics:",
                "Generic model evaluation: global test accuracy (same metric in conventional FL).",
                "Personalized model evaluation: weighted average of local test accuracies.",
                "For every client, the PM is evaluated on a local test set, whose underlying distribution is the same as that for the local training set.\nAll the experiments are implemented with a full client participation scheme. Further experiment details, results on partial client participation, and the computation overhead discussion are provided in the Appendix.",
                "For generic FL methods, personalized model (PM) accuracies are obtained by evaluating the generic model (GM) on local test sets. For PFL methods without GM training, GM accuracies are obtained by evaluating the averaged PM on the global test set.",
                "Baselines.",
                " We compared our proposed method with the following SOTA PFL methods: pFedMe ",
                "[",
                "17",
                "]",
                ", Ditto ",
                "[",
                "15",
                "]",
                ", FedRoD ",
                "[",
                "36",
                "]",
                ", FedRep ",
                "[",
                "24",
                "]",
                ", and FedBABU ",
                "[",
                "30",
                "]",
                ".\nMoreover, to have a fair performance evaluation of the generic models, we also include methods designed for conventional FL as baselines: FedAvg ",
                "[",
                "19",
                "]",
                ", FedProx ",
                "[",
                "10",
                "]",
                ", FedDyn ",
                "[",
                "11",
                "]",
                ",\nFedGen",
                "[",
                "54",
                "]",
                ", and FedAvgM ",
                "[",
                "9",
                "]",
                "."
            ]
        ]
    },
    "S4.T4": {
        "caption": "Table 4: Communication cost comparison of various methods for personalized model accuracies on CIFAR-10 to reach target accuracy (40%/80%) with non-IID setting α=0.1𝛼0.1\\alpha=0.1. The speedup factors are with respect to the performance of the corresponding methods without WF.",
        "table": "<table id=\"S4.T4.12.12\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T4.12.12.13.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.12.12.13.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\" rowspan=\"3\"><span id=\"S4.T4.12.12.13.1.1.1\" class=\"ltx_text\">Methods</span></th>\n<th id=\"S4.T4.12.12.13.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\" colspan=\"2\">3 epochs</th>\n<th id=\"S4.T4.12.12.13.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\" colspan=\"2\">5 epochs</th>\n</tr>\n<tr id=\"S4.T4.12.12.14.2\" class=\"ltx_tr\">\n<th id=\"S4.T4.12.12.14.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">40%</th>\n<th id=\"S4.T4.12.12.14.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">80%</th>\n<th id=\"S4.T4.12.12.14.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">40%</th>\n<th id=\"S4.T4.12.12.14.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">80%</th>\n</tr>\n<tr id=\"S4.T4.12.12.15.3\" class=\"ltx_tr\">\n<th id=\"S4.T4.12.12.15.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\" colspan=\"4\">Speedup</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T4.4.4.4\" class=\"ltx_tr\">\n<th id=\"S4.T4.4.4.4.5\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Ours (w/ WF)</th>\n<td id=\"S4.T4.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">1.82 <math id=\"S4.T4.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S4.T4.1.1.1.1.m1.1a\"><mo id=\"S4.T4.1.1.1.1.m1.1.1\" xref=\"S4.T4.1.1.1.1.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.1.1.1.1.m1.1b\"><times id=\"S4.T4.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T4.1.1.1.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.1.1.1.1.m1.1c\">\\times</annotation></semantics></math>\n</td>\n<td id=\"S4.T4.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">1.56 <math id=\"S4.T4.2.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S4.T4.2.2.2.2.m1.1a\"><mo id=\"S4.T4.2.2.2.2.m1.1.1\" xref=\"S4.T4.2.2.2.2.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.2.2.2.2.m1.1b\"><times id=\"S4.T4.2.2.2.2.m1.1.1.cmml\" xref=\"S4.T4.2.2.2.2.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.2.2.2.2.m1.1c\">\\times</annotation></semantics></math>\n</td>\n<td id=\"S4.T4.3.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">2.21<math id=\"S4.T4.3.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S4.T4.3.3.3.3.m1.1a\"><mo id=\"S4.T4.3.3.3.3.m1.1.1\" xref=\"S4.T4.3.3.3.3.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.3.3.3.3.m1.1b\"><times id=\"S4.T4.3.3.3.3.m1.1.1.cmml\" xref=\"S4.T4.3.3.3.3.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.3.3.3.3.m1.1c\">\\times</annotation></semantics></math>\n</td>\n<td id=\"S4.T4.4.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">1.85<math id=\"S4.T4.4.4.4.4.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S4.T4.4.4.4.4.m1.1a\"><mo id=\"S4.T4.4.4.4.4.m1.1.1\" xref=\"S4.T4.4.4.4.4.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.4.4.4.4.m1.1b\"><times id=\"S4.T4.4.4.4.4.m1.1.1.cmml\" xref=\"S4.T4.4.4.4.4.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.4.4.4.4.m1.1c\">\\times</annotation></semantics></math>\n</td>\n</tr>\n<tr id=\"S4.T4.8.8.8\" class=\"ltx_tr\">\n<th id=\"S4.T4.8.8.8.5\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Ditto w/ WF</th>\n<td id=\"S4.T4.5.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">1.97<math id=\"S4.T4.5.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S4.T4.5.5.5.1.m1.1a\"><mo id=\"S4.T4.5.5.5.1.m1.1.1\" xref=\"S4.T4.5.5.5.1.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.5.5.5.1.m1.1b\"><times id=\"S4.T4.5.5.5.1.m1.1.1.cmml\" xref=\"S4.T4.5.5.5.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.5.5.5.1.m1.1c\">\\times</annotation></semantics></math>\n</td>\n<td id=\"S4.T4.6.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">1.38 <math id=\"S4.T4.6.6.6.2.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S4.T4.6.6.6.2.m1.1a\"><mo id=\"S4.T4.6.6.6.2.m1.1.1\" xref=\"S4.T4.6.6.6.2.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.6.6.6.2.m1.1b\"><times id=\"S4.T4.6.6.6.2.m1.1.1.cmml\" xref=\"S4.T4.6.6.6.2.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.6.6.6.2.m1.1c\">\\times</annotation></semantics></math>\n</td>\n<td id=\"S4.T4.7.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">2.87<math id=\"S4.T4.7.7.7.3.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S4.T4.7.7.7.3.m1.1a\"><mo id=\"S4.T4.7.7.7.3.m1.1.1\" xref=\"S4.T4.7.7.7.3.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.7.7.7.3.m1.1b\"><times id=\"S4.T4.7.7.7.3.m1.1.1.cmml\" xref=\"S4.T4.7.7.7.3.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.7.7.7.3.m1.1c\">\\times</annotation></semantics></math>\n</td>\n<td id=\"S4.T4.8.8.8.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">1.93<math id=\"S4.T4.8.8.8.4.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S4.T4.8.8.8.4.m1.1a\"><mo id=\"S4.T4.8.8.8.4.m1.1.1\" xref=\"S4.T4.8.8.8.4.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.8.8.8.4.m1.1b\"><times id=\"S4.T4.8.8.8.4.m1.1.1.cmml\" xref=\"S4.T4.8.8.8.4.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.8.8.8.4.m1.1c\">\\times</annotation></semantics></math>\n</td>\n</tr>\n<tr id=\"S4.T4.12.12.12\" class=\"ltx_tr\">\n<th id=\"S4.T4.12.12.12.5\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">FedRoD w/ WF</th>\n<td id=\"S4.T4.9.9.9.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">1.75<math id=\"S4.T4.9.9.9.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S4.T4.9.9.9.1.m1.1a\"><mo id=\"S4.T4.9.9.9.1.m1.1.1\" xref=\"S4.T4.9.9.9.1.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.9.9.9.1.m1.1b\"><times id=\"S4.T4.9.9.9.1.m1.1.1.cmml\" xref=\"S4.T4.9.9.9.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.9.9.9.1.m1.1c\">\\times</annotation></semantics></math>\n</td>\n<td id=\"S4.T4.10.10.10.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">1.54 <math id=\"S4.T4.10.10.10.2.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S4.T4.10.10.10.2.m1.1a\"><mo id=\"S4.T4.10.10.10.2.m1.1.1\" xref=\"S4.T4.10.10.10.2.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.10.10.10.2.m1.1b\"><times id=\"S4.T4.10.10.10.2.m1.1.1.cmml\" xref=\"S4.T4.10.10.10.2.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.10.10.10.2.m1.1c\">\\times</annotation></semantics></math>\n</td>\n<td id=\"S4.T4.11.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">2.42<math id=\"S4.T4.11.11.11.3.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S4.T4.11.11.11.3.m1.1a\"><mo id=\"S4.T4.11.11.11.3.m1.1.1\" xref=\"S4.T4.11.11.11.3.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.11.11.11.3.m1.1b\"><times id=\"S4.T4.11.11.11.3.m1.1.1.cmml\" xref=\"S4.T4.11.11.11.3.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.11.11.11.3.m1.1c\">\\times</annotation></semantics></math>\n</td>\n<td id=\"S4.T4.12.12.12.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">2.19<math id=\"S4.T4.12.12.12.4.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S4.T4.12.12.12.4.m1.1a\"><mo id=\"S4.T4.12.12.12.4.m1.1.1\" xref=\"S4.T4.12.12.12.4.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.12.12.12.4.m1.1b\"><times id=\"S4.T4.12.12.12.4.m1.1.1.cmml\" xref=\"S4.T4.12.12.12.4.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.12.12.12.4.m1.1c\">\\times</annotation></semantics></math>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "We evaluated the generalizability of our proposed spectral co-distillation framework, as well as the communication cost performance of our wait-free training protocol for PFL+.",
                "Generalizability over heterogeneous settings.",
                "\nWe compared the best test accuracies with multiple baselines over the different levels of data heterogeneity, using the same system configuration. ",
                "Tab.",
                " ",
                "1",
                " and ",
                "Tab.",
                " ",
                "2",
                " give the main results on CIFAR-10 and CIFAR-100, respectively. In summary, our proposed framework achieves the best test accuracies across diverse heterogeneous data settings, outperforming all PFL and conventional FL baselines on both PM and GM test accuracies concurrently. We also investigated the performance on the real-world dataset iNaturalist2017 in ",
                "Tab.",
                " ",
                "3",
                ", where our proposed method also achieves the best GM/PM test accuracies.We attribute such consistent outperformance to the bi-directional co-distillation design.",
                "This demonstrates that: a) the spectral information of the generic model is useful for knowledge distillation during personalized model training; and b) using truncated spectral information of the personalized models could boost the performance of the generic model via careful spectrum truncation. (See Appendix for a sensitivity analysis of the truncation ratio ",
                "τ",
                "𝜏",
                "\\tau",
                " and other hyper-parameters.)",
                "Communication cost comparison.",
                "\nTo demonstrate the superiority of the wait-free training protocol (",
                "WF",
                "), we evaluated the communication cost performance of SOTA methods with/without the protocol on non-IID CIFAR-10 (",
                "α",
                "=",
                "0.1",
                "𝛼",
                "0.1",
                "\\alpha=0.1",
                "), in terms of the total runtime ",
                "ζ",
                "total",
                "subscript",
                "𝜁",
                "total",
                "\\zeta_{\\text{total}}",
                " for PM to reach the target test accuracy (40%/80%).\nA smaller ",
                "ζ",
                "total",
                "subscript",
                "𝜁",
                "total",
                "\\zeta_{\\text{total}}",
                " indicates higher communication efficiency.\nFor PFL methods that train generic and personalized models using the compute-and-wait local training protocol, we evaluated Ditto and FedRoD. We conduct experiments with different numbers of epochs for local PM training (3 or 5 epochs).\nAs shown in ",
                "Tab.",
                " ",
                "4",
                ", our proposed wait-free training protocol could significantly improve the efficiency of convergence time and has the potential to boost the time efficiency of PFL+ methods."
            ]
        ]
    },
    "S4.T5": {
        "caption": "Table 5: Ablation study results on non-IID CIFAR-10 (average and standard deviation of 3 trials). SCD-GM (resp. SCD-PM) represents the spectral distillation approaches adopted during the training of generic (resp. personalized) model.",
        "table": "",
        "footnotes": "\n\n\n\n\n\nMethod\nα=1𝛼1\\alpha=1\nα=0.1𝛼0.1\\alpha=0.1\n\nGM\nPM\nGM\nPM\n\nOurs\n86.37 ±plus-or-minus\\pm 0.15\n92.25 ±plus-or-minus\\pm 0.21\n75.52 ±plus-or-minus\\pm 0.11\n82.69 ±plus-or-minus\\pm 0.16\n\nOurs w/o SCD-GM\n85.35 ±plus-or-minus\\pm 0.11\n91.86 ±plus-or-minus\\pm 0.17\n73.51 ±plus-or-minus\\pm 0.17\n81.03 ±plus-or-minus\\pm 0.20\n\nOurs w/o SCD-PM\n82.74 ±plus-or-minus\\pm 0.39\n79.65 ±plus-or-minus\\pm 0.83\n68.96 ±plus-or-minus\\pm 0.47\n70.51 ±plus-or-minus\\pm 1.21\n\nOurs w/o Both\n85.35 ±plus-or-minus\\pm 0.11\n79.65 ±plus-or-minus\\pm 0.83\n73.51 ±plus-or-minus\\pm 0.17\n70.51 ±plus-or-minus\\pm 1.21\n\n\n",
        "references": [
            [
                "We evaluated the generalizability of our proposed spectral co-distillation framework, as well as the communication cost performance of our wait-free training protocol for PFL+.",
                "Generalizability over heterogeneous settings.",
                "\nWe compared the best test accuracies with multiple baselines over the different levels of data heterogeneity, using the same system configuration. ",
                "Tab.",
                " ",
                "1",
                " and ",
                "Tab.",
                " ",
                "2",
                " give the main results on CIFAR-10 and CIFAR-100, respectively. In summary, our proposed framework achieves the best test accuracies across diverse heterogeneous data settings, outperforming all PFL and conventional FL baselines on both PM and GM test accuracies concurrently. We also investigated the performance on the real-world dataset iNaturalist2017 in ",
                "Tab.",
                " ",
                "3",
                ", where our proposed method also achieves the best GM/PM test accuracies.We attribute such consistent outperformance to the bi-directional co-distillation design.",
                "This demonstrates that: a) the spectral information of the generic model is useful for knowledge distillation during personalized model training; and b) using truncated spectral information of the personalized models could boost the performance of the generic model via careful spectrum truncation. (See Appendix for a sensitivity analysis of the truncation ratio ",
                "τ",
                "𝜏",
                "\\tau",
                " and other hyper-parameters.)",
                "Communication cost comparison.",
                "\nTo demonstrate the superiority of the wait-free training protocol (",
                "WF",
                "), we evaluated the communication cost performance of SOTA methods with/without the protocol on non-IID CIFAR-10 (",
                "α",
                "=",
                "0.1",
                "𝛼",
                "0.1",
                "\\alpha=0.1",
                "), in terms of the total runtime ",
                "ζ",
                "total",
                "subscript",
                "𝜁",
                "total",
                "\\zeta_{\\text{total}}",
                " for PM to reach the target test accuracy (40%/80%).\nA smaller ",
                "ζ",
                "total",
                "subscript",
                "𝜁",
                "total",
                "\\zeta_{\\text{total}}",
                " indicates higher communication efficiency.\nFor PFL methods that train generic and personalized models using the compute-and-wait local training protocol, we evaluated Ditto and FedRoD. We conduct experiments with different numbers of epochs for local PM training (3 or 5 epochs).\nAs shown in ",
                "Tab.",
                " ",
                "4",
                ", our proposed wait-free training protocol could significantly improve the efficiency of convergence time and has the potential to boost the time efficiency of PFL+ methods."
            ]
        ]
    }
}