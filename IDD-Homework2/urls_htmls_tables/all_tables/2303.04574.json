{
    "PAPER'S NUMBER OF TABLES": 2,
    "S5.T1": {
        "caption": "Table 1: Hardware usage of DVFL",
        "table": "<table id=\"S5.T1.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.2.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.2.1.1.1\" class=\"ltx_td ltx_border_l ltx_border_r ltx_border_t\"></td>\n<td id=\"S5.T1.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">workers per party</td>\n<td id=\"S5.T1.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1 worker</td>\n<td id=\"S5.T1.2.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2 worker</td>\n<td id=\"S5.T1.2.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4 worker</td>\n<td id=\"S5.T1.2.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">8 worker</td>\n</tr>\n<tr id=\"S5.T1.2.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.2.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S5.T1.2.2.2.1.1\" class=\"ltx_text\">Machine A</span></td>\n<td id=\"S5.T1.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">peak CPU spike (%)</td>\n<td id=\"S5.T1.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.2.2.2.3.1\" class=\"ltx_text ltx_font_bold\">98.91</span></td>\n<td id=\"S5.T1.2.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.2.2.2.4.1\" class=\"ltx_text ltx_font_bold\">98.59</span></td>\n<td id=\"S5.T1.2.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">97.09</td>\n<td id=\"S5.T1.2.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">93.48</td>\n</tr>\n<tr id=\"S5.T1.2.3.3\" class=\"ltx_tr\">\n<td id=\"S5.T1.2.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">memory usage (G)</td>\n<td id=\"S5.T1.2.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.2.3.3.2.1\" class=\"ltx_text ltx_font_bold\">61.95</span></td>\n<td id=\"S5.T1.2.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">51.3</td>\n<td id=\"S5.T1.2.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">48</td>\n<td id=\"S5.T1.2.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">45.37</td>\n</tr>\n<tr id=\"S5.T1.2.4.4\" class=\"ltx_tr\">\n<td id=\"S5.T1.2.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Throughput</td>\n<td id=\"S5.T1.2.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1105</td>\n<td id=\"S5.T1.2.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1505</td>\n<td id=\"S5.T1.2.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1101</td>\n<td id=\"S5.T1.2.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">772</td>\n</tr>\n<tr id=\"S5.T1.2.5.5\" class=\"ltx_tr\">\n<td id=\"S5.T1.2.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S5.T1.2.5.5.1.1\" class=\"ltx_text\">Machine B</span></td>\n<td id=\"S5.T1.2.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">peak CPU spike (%)</td>\n<td id=\"S5.T1.2.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">60.39</td>\n<td id=\"S5.T1.2.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">39.48</td>\n<td id=\"S5.T1.2.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">27.91</td>\n<td id=\"S5.T1.2.5.5.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">15.98</td>\n</tr>\n<tr id=\"S5.T1.2.6.6\" class=\"ltx_tr\">\n<td id=\"S5.T1.2.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">memory usage (G)</td>\n<td id=\"S5.T1.2.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">18.78</td>\n<td id=\"S5.T1.2.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">16.82</td>\n<td id=\"S5.T1.2.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">15.83</td>\n<td id=\"S5.T1.2.6.6.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">14.82</td>\n</tr>\n<tr id=\"S5.T1.2.7.7\" class=\"ltx_tr\">\n<td id=\"S5.T1.2.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Throughput</td>\n<td id=\"S5.T1.2.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">809</td>\n<td id=\"S5.T1.2.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">324</td>\n<td id=\"S5.T1.2.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">248</td>\n<td id=\"S5.T1.2.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">175</td>\n</tr>\n<tr id=\"S5.T1.2.8.8\" class=\"ltx_tr\">\n<td id=\"S5.T1.2.8.8.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S5.T1.2.8.8.1.1\" class=\"ltx_text\">Machine C</span></td>\n<td id=\"S5.T1.2.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">peak CPU spike (%)</td>\n<td id=\"S5.T1.2.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Null</td>\n<td id=\"S5.T1.2.8.8.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.2.8.8.4.1\" class=\"ltx_text ltx_font_bold\">98.65</span></td>\n<td id=\"S5.T1.2.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">97.81</td>\n<td id=\"S5.T1.2.8.8.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">96.74</td>\n</tr>\n<tr id=\"S5.T1.2.9.9\" class=\"ltx_tr\">\n<td id=\"S5.T1.2.9.9.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">memory usage (G)</td>\n<td id=\"S5.T1.2.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Null</td>\n<td id=\"S5.T1.2.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">23.29</td>\n<td id=\"S5.T1.2.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">18</td>\n<td id=\"S5.T1.2.9.9.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">15.14</td>\n</tr>\n<tr id=\"S5.T1.2.10.10\" class=\"ltx_tr\">\n<td id=\"S5.T1.2.10.10.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Throughput</td>\n<td id=\"S5.T1.2.10.10.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Null</td>\n<td id=\"S5.T1.2.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1843</td>\n<td id=\"S5.T1.2.10.10.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1828</td>\n<td id=\"S5.T1.2.10.10.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1921</td>\n</tr>\n<tr id=\"S5.T1.2.11.11\" class=\"ltx_tr\">\n<td id=\"S5.T1.2.11.11.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S5.T1.2.11.11.1.1\" class=\"ltx_text\">Machine D</span></td>\n<td id=\"S5.T1.2.11.11.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">peak CPU spike (%)</td>\n<td id=\"S5.T1.2.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Null</td>\n<td id=\"S5.T1.2.11.11.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">34.8</td>\n<td id=\"S5.T1.2.11.11.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">18.37</td>\n<td id=\"S5.T1.2.11.11.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">17.03</td>\n</tr>\n<tr id=\"S5.T1.2.12.12\" class=\"ltx_tr\">\n<td id=\"S5.T1.2.12.12.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">memory usage (G)</td>\n<td id=\"S5.T1.2.12.12.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Null</td>\n<td id=\"S5.T1.2.12.12.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">12.15</td>\n<td id=\"S5.T1.2.12.12.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">11.53</td>\n<td id=\"S5.T1.2.12.12.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">10.5</td>\n</tr>\n<tr id=\"S5.T1.2.13.13\" class=\"ltx_tr\">\n<td id=\"S5.T1.2.13.13.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">Throughput</td>\n<td id=\"S5.T1.2.13.13.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">Null</td>\n<td id=\"S5.T1.2.13.13.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">400</td>\n<td id=\"S5.T1.2.13.13.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">262</td>\n<td id=\"S5.T1.2.13.13.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">165</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Besides drawing a similar conclusion as the previous experiment in the large-scale cluster, we also monitor the peak CPU spike, memory usage, and throughput (TPS) as shown in Table 1. In 1-worker per party case, Machine A handles Party A and Machine B handles Party P. In multi-worker cases, Machines A and B are responsible for Party A, while Machines C and D are responsible for Party P. We notice the CPU is overloaded when handling the magnitude of 108superscript10810^{8} dataset in 1-worker system. Even the number of workers increases, the peak CPU spikes decrease lethargically. However, the memory usage is also close to full-filled to handle Party A in the 1-worker system (61.95 G out of 64 G). But the memory usage is effectively reduced while the number of workers is increased. The throughput drops as expected when the overall I/O remains, excepting few outliers. Hence, the limitation of CPU in a cloud environment could be a common bottleneck of DVFL when dealing with big data."
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Execution time for training and inference. The time unit is second. “128” and “1024” represent the length of public key.",
        "table": "<table id=\"S5.T2.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T2.2.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.2.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Type</th>\n<th id=\"S5.T2.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Vanilla</th>\n<th id=\"S5.T2.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">HE (128)</th>\n<th id=\"S5.T2.2.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">HE (1024)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.2.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.2.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Training</th>\n<td id=\"S5.T2.2.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">89</td>\n<td id=\"S5.T2.2.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">878</td>\n<td id=\"S5.T2.2.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">19021</td>\n</tr>\n<tr id=\"S5.T2.2.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.2.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">Inference</th>\n<td id=\"S5.T2.2.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">72</td>\n<td id=\"S5.T2.2.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">73</td>\n<td id=\"S5.T2.2.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">74</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "The large-scale cluster environment is given by Hygon C86 7185 thirty-two 6000-core processors, 128 GB memory, four 6000-core Distributed Control Unit (DCU), 64-bit CentOS 7.6 Linux with x86_64 architecture. To run DVFL, Party A and Party P each contains ",
                "10",
                "6",
                "superscript",
                "10",
                "6",
                "10^{6}",
                " rows of data. To evaluate the system performance, we record and compare the time consumption of two parties running DVFL under 1, 2, 4, 8, 16, 32 workers respectively. As it shown in Figure ",
                "5",
                ", with the number of workers increase, the overall runtime drops while the total data processed per second increases. The execution time given by non-distributed system (1-worker per party) is 25865 seconds, while the execution time given by 32-worker per party is 2252 seconds. And the data processed per second is increased from 7732 rows (1-worker) to 88810 (32-worker). The assumption of the viability of DVFL is confirmed by noticing the tremendous execution time drop and the immense data processing capacity by the multi-worker system.",
                "We also observed the training time of distributed vertical FL decreases while worker nodes increases. Because of the inevitable communication cost, the throughput per worker of 1-worker-system is higher than a 32-worker-system. The training time dropped significantly when the number of workers increases from one to two (25865 seconds to 12056 second). With the workers continually increasing, the training time decreasing becomes inert. When the number of workers increases, the overall throughput of the distributed vertical FL is also augmented. However, because of the communication overhead, the increment becomes less significant when the number of workers becomes big enough.",
                "Additionally, we examined the execution time of the distributed PSI. In this experiment, we put ",
                "5",
                "×",
                "10",
                "8",
                "5",
                "superscript",
                "10",
                "8",
                "5\\times 10^{8}",
                " rows of data in Party A and ",
                "2",
                "×",
                "10",
                "7",
                "2",
                "superscript",
                "10",
                "7",
                "2\\times 10^{7}",
                " rows of data in Party P in the large-scale cluster environment. As shown in Figure ",
                "6",
                ", we obtain similar tendency in terms of the execution time and the throughput as that of DVFL. The execution time decreases from 2680 seconds to 593 seconds while the throughput increases from 186567 items per second to 843170 items per second.",
                "We carry out another experiment to verify the performance of DVFL on the Baidu AI Cloud environment. In this experiment, Party A holds 4 ",
                "×",
                "\\times",
                " ",
                "10",
                "8",
                "superscript",
                "10",
                "8",
                "10^{8}",
                " rows of data and Party P holds 2 ",
                "×",
                "\\times",
                " ",
                "10",
                "7",
                "superscript",
                "10",
                "7",
                "10^{7}",
                " rows of data. The environment is given by four 32-core, 64 GB memory, and 160 GB storage machines in version 7.4 centOS Linux. Figure ",
                "7",
                " shows the execution time of 1, 2, 4, 8 workers per party in DVFL on above datasets and environment.",
                "Besides drawing a similar conclusion as the previous experiment in the large-scale cluster, we also monitor the peak CPU spike, memory usage, and throughput (TPS) as shown in Table ",
                "1",
                ". In 1-worker per party case, Machine A handles Party A and Machine B handles Party P. In multi-worker cases, Machines A and B are responsible for Party A, while Machines C and D are responsible for Party P. We notice the CPU is overloaded when handling the magnitude of ",
                "10",
                "8",
                "superscript",
                "10",
                "8",
                "10^{8}",
                " dataset in 1-worker system. Even the number of workers increases, the peak CPU spikes decrease lethargically. However, the memory usage is also close to full-filled to handle Party A in the 1-worker system (61.95 G out of 64 G). But the memory usage is effectively reduced while the number of workers is increased. The throughput drops as expected when the overall I/O remains, excepting few outliers. Hence, the limitation of CPU in a cloud environment could be a common bottleneck of DVFL when dealing with big data.",
                "In order to analyze the overhead of Paillier HE, we carry out an experimentation to compare the training time with vanilla vertical FL and that with Paillier HE. We set the number of rounds to 10, the learning rate to 0.05 and the batch size to 16. We vary the length of the public key in Paillier HE. As shown in Table ",
                "2",
                ", the overhead of Paillier HE cannot be ignored. When we utilize the public key of 128 bits, the training is 8.9 times longer while the inference time is slightly longer (less than 1.4%). When the length becomes 1024, the overhead can be much more significant (213 times longer). In practice, we exploit the public key of 128 bits to balance the security and the efficiency."
            ]
        ]
    }
}