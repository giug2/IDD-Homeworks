{
    "S3.T1": {
        "caption": "Table 1: Ablation study on KITTI Eigen test split of the proposed unsupervised online stereo method. At the top we show fine-tuning without pre-training, in the middle part fine-tuning after standard batch pre-training and in the bottom part fine-tuning after meta-pre-training on Synthia dataset. Depth predictions are capped at 50 meters.",
        "table": null,
        "footnotes": [],
        "references": [
            "We report the evaluation scores obtained by the different methods, in Table 1.\nFirst, we observe that directly performing naive online learning without pretraining (Online Naive, w/o) does not lead to good performance. The scores obtained on the last 20%percent2020\\% frames are not better than the average scores over the whole videos showing that the model is not learning.\nSimilarly, moving to the models pretrained with Standard pretraining on Synthia, Online Naive only provides a very limited gain. A first proof that online meta-learning is beneficial is found in Online Meta-learning where we see a clear improvement in the last 20%percent2020\\% frames when applying meta-learning for online fine-tuning. Even better performance are obtained with OFDA that perform feature distribution alignment. These results show that handling domain shift between the source and the target distributions truly improve the quality of the estimated depth maps. A further improvement is obtained with our full meta-learning method, OMLA, that reaches the best performances in the Standard pretraining setting."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Analysis of the performance of our method on common stereo architectures, DispNet [26], MADNet [39] and Godard (ResNet) [11], and different pretraining datasets, Synthia [34] or SceneFlow [26]. Depth predictions are capped at 50 meters.",
        "table": null,
        "footnotes": [],
        "references": [
            "Analysis on Network Architectures and Datasets.\nIn order to further evaluate our approach, in Table 2 we report the performances of our method considering three different architectures: DispNet [26], MADNet [39] and Godard et al.  (ResNet encoder) [11]. DispNet and MADNet are two light-weight networks for stereo matching. We compare the performances obtained using these network architectures when we employ the baseline naive online learning approach and our full model referred to as ours. We report results when pretraining either on Synthia or SceneFlow Driving datasets.",
            "From Table 2, we see that our proposed method obtains significantly better performances independently of the architecture. Such results demonstrate that OMLA is effective even with smaller networks as DispNet or MADNet.\nConcerning the SceneFlow driving dataset, we observe that the models, both Naive and ours, obtain slightly poorer performance than when pretraining on Synthia. A possible explanation is that the SceneFlow driving dataset is smaller and less diverse than Synthia. Nonetheless, these experiments confirm again the excellent performance of our approach even on this small dataset."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Comparison with different offline methods. Only points with depth less than 50m are calculated.",
        "table": null,
        "footnotes": [],
        "references": [
            "The results are reported in Table 3. First, when the models are trained in an Offline setting on the KITTI training set, naive online learning does not improve significantly the performance.\nSecond, we observe that our online approach is competitive with the methods trained offline on the KITTI training set whereas our model did not see any real-world KITTI image. According to some metrics, our approach even outperforms the model trained offline. These observations clearly show the potential of our approach."
        ]
    },
    "A1.T4": {
        "caption": "Table 4: Unsupervised online monocular depth estimation results on Eigen test scenes in the Kitti dataset. Only points with depth less than 50m are calculated.",
        "table": null,
        "footnotes": [],
        "references": [
            "Although our method is meant for online stereo depth estimation, we also report experiment results in a monocular setting for further evaluation. More precisely, we employ the monocular network of [11] but still employed stereo pairs to compute the loss as in [11].\nThe results obtained in the monocular setting are reported in Table 4. As in the main paper, we also show the results averaged over the last 20% frames of each scene. We can observe that directly performing naive online learning without pretraining does not lead to good performances. We notice that the scores on the last 20%percent2020\\% frames are not better than the scores averaged over the whole video showing that the model is not learning.\nConcerning the online methods with pretraining, the results are well in-line with the stereo setting results reported in the main paper. We first observe that our meta-pretraining strategy improves consistently the performance for every strategy adopted on the target video. With meta-pretraining, the models can all obtain better results on the last 20% frames, which show again that meta-pretraining helps the model, not only to adapt faster, but also to perform better after observing many frames. Finally, using Online Feature Distribution Alignment (OFDA) and online meta-learning both improve the performances of online learning. Similarly to the stereo setting, OMLA (online meta-learning with OFDA) leads to the best results with both standard and meta pretraining."
        ]
    },
    "A2.T5": {
        "caption": "Table 5: Comparison with different offline methods. Only points with depth less than 50m are calculated.",
        "table": "<table id=\"A2.T5.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"A2.T5.1.1.1\" class=\"ltx_tr\">\n<th id=\"A2.T5.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt\">Method</th>\n<td id=\"A2.T5.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">d1-all</td>\n<td id=\"A2.T5.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">EPE</td>\n</tr>\n<tr id=\"A2.T5.1.2.2\" class=\"ltx_tr\">\n<th id=\"A2.T5.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t\">Godard <span id=\"A2.T5.1.2.2.1.1\" class=\"ltx_text ltx_font_italic\">et al.  <cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A2.T5.1.2.2.1.1.1.1\" class=\"ltx_text ltx_font_upright\">[</span><a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a><span id=\"A2.T5.1.2.2.1.1.2.2\" class=\"ltx_text ltx_font_upright\">]</span></cite></span> Offline</th>\n<td id=\"A2.T5.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">18.6883</td>\n<td id=\"A2.T5.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.9076</td>\n</tr>\n<tr id=\"A2.T5.1.3.3\" class=\"ltx_tr\">\n<th id=\"A2.T5.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr\">Godard <span id=\"A2.T5.1.3.3.1.1\" class=\"ltx_text ltx_font_italic\">et al.  <cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A2.T5.1.3.3.1.1.1.1\" class=\"ltx_text ltx_font_upright\">[</span><a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a><span id=\"A2.T5.1.3.3.1.1.2.2\" class=\"ltx_text ltx_font_upright\">]</span></cite></span> Offline + Online</th>\n<td id=\"A2.T5.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">19.3257</td>\n<td id=\"A2.T5.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">2.9803</td>\n</tr>\n<tr id=\"A2.T5.1.4.4\" class=\"ltx_tr\">\n<th id=\"A2.T5.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr\">Godard <span id=\"A2.T5.1.4.4.1.1\" class=\"ltx_text ltx_font_italic\">et al.  <cite class=\"ltx_cite ltx_citemacro_cite\"><span id=\"A2.T5.1.4.4.1.1.1.1\" class=\"ltx_text ltx_font_upright\">[</span><a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a><span id=\"A2.T5.1.4.4.1.1.2.2\" class=\"ltx_text ltx_font_upright\">]</span></cite></span> Naive</th>\n<td id=\"A2.T5.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">50.2587</td>\n<td id=\"A2.T5.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">5.2140</td>\n</tr>\n<tr id=\"A2.T5.1.5.5\" class=\"ltx_tr\">\n<th id=\"A2.T5.1.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr\">Godard <span id=\"A2.T5.1.5.5.1.1\" class=\"ltx_text ltx_font_italic\">et al.  </span>+ <em id=\"A2.T5.1.5.5.1.2\" class=\"ltx_emph ltx_font_italic\">OMLA</em>\n</th>\n<td id=\"A2.T5.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\">22.3525</td>\n<td id=\"A2.T5.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\">3.5820</td>\n</tr>\n<tr id=\"A2.T5.1.6.6\" class=\"ltx_tr\">\n<th id=\"A2.T5.1.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t\">MADNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">39</a>]</cite> Offline</th>\n<td id=\"A2.T5.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">17.2573</td>\n<td id=\"A2.T5.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.7544</td>\n</tr>\n<tr id=\"A2.T5.1.7.7\" class=\"ltx_tr\">\n<th id=\"A2.T5.1.7.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr\">MADNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">39</a>]</cite> Offline + Online</th>\n<td id=\"A2.T5.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_r\">17.1209</td>\n<td id=\"A2.T5.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\">2.7631</td>\n</tr>\n<tr id=\"A2.T5.1.8.8\" class=\"ltx_tr\">\n<th id=\"A2.T5.1.8.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr\">MADNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">39</a>]</cite> Naive</th>\n<td id=\"A2.T5.1.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_r\">46.9753</td>\n<td id=\"A2.T5.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r\">4.9866</td>\n</tr>\n<tr id=\"A2.T5.1.9.9\" class=\"ltx_tr\">\n<th id=\"A2.T5.1.9.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr\">MADNet + <em id=\"A2.T5.1.9.9.1.1\" class=\"ltx_emph ltx_font_italic\">OMLA</em>\n</th>\n<td id=\"A2.T5.1.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_r\">20.2215</td>\n<td id=\"A2.T5.1.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r\">3.2014</td>\n</tr>\n<tr id=\"A2.T5.1.10.10\" class=\"ltx_tr\">\n<th id=\"A2.T5.1.10.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t\">DispNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">26</a>]</cite> Offline</th>\n<td id=\"A2.T5.1.10.10.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">20.4301</td>\n<td id=\"A2.T5.1.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.9542</td>\n</tr>\n<tr id=\"A2.T5.1.11.11\" class=\"ltx_tr\">\n<th id=\"A2.T5.1.11.11.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr\">DispNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">26</a>]</cite> Offline + Online</th>\n<td id=\"A2.T5.1.11.11.2\" class=\"ltx_td ltx_align_center ltx_border_r\">20.1037</td>\n<td id=\"A2.T5.1.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_r\">2.9256</td>\n</tr>\n<tr id=\"A2.T5.1.12.12\" class=\"ltx_tr\">\n<th id=\"A2.T5.1.12.12.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr\">DispNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">26</a>]</cite> Naive</th>\n<td id=\"A2.T5.1.12.12.2\" class=\"ltx_td ltx_align_center ltx_border_r\">51.8796</td>\n<td id=\"A2.T5.1.12.12.3\" class=\"ltx_td ltx_align_center ltx_border_r\">3.0259</td>\n</tr>\n<tr id=\"A2.T5.1.13.13\" class=\"ltx_tr\">\n<th id=\"A2.T5.1.13.13.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_l ltx_border_rr\">DispNet + <em id=\"A2.T5.1.13.13.1.1\" class=\"ltx_emph ltx_font_italic\">OMLA</em>\n</th>\n<td id=\"A2.T5.1.13.13.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">25.3598</td>\n<td id=\"A2.T5.1.13.13.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">3.3746</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "We now evaluate our approach with different network architectures according to stereo matching metrics. We use D1-all and End point Error (EPE) to compare the different approaches [27]. Here, all the experiments are performed using the exact same protocol as in the main paper: all the online models are pretrained on Synthia dataset [34], and the offline models are pretrained on KITTI [27] Eigen training split [6].\nAs shown in table  5, the offline methods obtain better results according to both metrics, but the naive online fine-tuning cannot bring any significant contribution to the offline method.\nHere again, the naive online learning obtains poor results in all the metrics and we observe that both OMLA and meta-pretraining improve significantly the performances. Even though online models report slightly lower performances than offline models, these experiments clearly illustrate the interesting potential of the online learning setting."
        ]
    }
}