{
    "id_table_1": {
        "caption": "Table 1 :  Perplexity results of experiments for  Retro-li -on/-off with  Bert  and  SBert  embeddings averaged over three random seeds.",
        "table": "S3.T1.7",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": [
            "In retrieval augmented generation (RAG)  [ 26 ] , a language model is enhanced with a non-parametric memory as depicted in Figure  1 . During training and inference, for each input sequence, RAG searches this memory for the  k k k italic_k  most similar sequences (the so-called nearest neighbors) and uses them as additional input. RAG has been shown to help with under-fitted language models as the non-parametric memory increases with the number of training tokens  [ 15 ] . Moreover, it has been shown to reduce toxic language and hallucinations  [ 52 ]  through its use of retrieval. Furthermore, unlike in purely parametric models, it is straightforward and does not require extensive computing power to update the retrieval database of a RAG model to reflect updated information. Finally, in a question-answering context, the ability to provide sources for the answer helps combat misinformation. This provides users with the opportunity to double-check the responses themselves, aiding the models interpretability.",
            "Despite all the aforementioned functional advantages of RAG, in many cases, the retrieval database of non-parametric memory contains trillions of tokens to be searched. In fact, in  Retro   [ 2 ] , the scale begins at a retrieval database size of billions of tokens, which raises the question of search speed and optimization. Although similarity search acceleration libraries such as  Faiss   [ 19 ]  and ScaNN  [ 10 ]  can compare millions of vectors in milliseconds, they quickly become a bottleneck, especially for retrieval databases containing trillions of tokens  (e.g., see Section  3.3.1 )  . To address this critical bottleneck, we suggest three directions whose effectiveness is shown in our proposed  Retro-li .",
            "The neighbors consist of [N, C], each N containing 64 tokens and C containing another 64 tokens. The retrieval is based on N only, the key of this key-value pair, the value being C, which is the continuation of N. We retrieve them for each chunk. Equation  1  shows retrieval of 10 neighbors for each chunk in a sequence consisting of 16 chunks from the retrieval database (DB).",
            "For the first batch of experiments, we gradually increase the number of neighbors from 2 up to 10 and run the experiments for three random seeds. In Table  1  we report their average.  Bert  embeddings with 5 neighbors already outperforms  Retro-li -off. Moreover, changing the embedding model to  SBert  improves our performance significantly. This shows that not only does  SBert  find more semantically similar neighbors, but our model also correctly attends to them.",
            "The setting  Retro-li -on with  no neighbors  is meant to ascertain how much of the domain shift performance gain is simply due to improved language modeling capabilities, and not due to the retrieval itself. In settings with  Retro-li -on, we further have a model trained with a regularizer or without one.  In Table  3 , we only include the best-performing regularizer, a Gaussian regularizer with   t = 0.2 subscript  t 0.2 \\lambda_{t}=0.2 italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 0.2  (see Equation  2 ), which also reflects the signal-to-noise ratio observed in the modern IMC hardware (more on this in Section  3.3.1 ).",
            "We take the best  Retro-li -off checkpoint and keep training it for six random seeds, with and without retrieval, and with and without regularization. WikiText-103 is taken as the base dataset (dataset A, see above) for the taining. We train both  Retro-li -on and  Retro-li -off, in order to confirm that the improved performance is not due to the increased number of training tokens. As is evident in the rows for  Retro-li -on with ideal retrieval in Table  3 , when compared to Table  1  with training from scratch, this setup decreases WikiText-103-Validation set perplexity for both,  Retro-li -on and  Retro-li -off. The seemingly significant perplexity improvement of  Retro-li -off is due to there no longer being one particularly bad random seed, as this setup reduces the variance across seeds.",
            "Retro  was trained on MassiveText  [ 37 ]  which is unfortunately proprietary. Furthermore, The Pile  [ 6 ]  which is an open-source alternative to MassiveText and on which  Retro  was also trained for comparison purposes, was taken down in July of 2023 due to a DMCA (Digital Millenium Copyright Act) notice 3 3 3 https://academictorrents.com/details/ 0d366035664fdf51cfbe9f733953ba325776e667. . The  Nvidia   Retro  implementation training data was mostly based on The Pile  [ 45 ]  as well. This means that we could not use the same dataset as  Retro , which would make it harder for us to compare our performance to theirs. As an alternative, looking for datasets similar in data sources and size to The Pile, we decided on SlimPajama  [ 46 ]  which is a cleaned and de-duplicated version of RedPajama  [ 4 ] . De-duplication is especially important for textual training data, see  [ 25 ] . In Table  10  we compare The Pile to SlimPajama, where Pile-CC is comparable to CommonCrawl and C4  [ 38 ] . We see that although the data sources themselves are similar, the proportions are only somewhat comparable.",
            "For this set of experiments, we chose three neighbors and the  SBert  word embedding model, which gave us the best results so far. Looking at the results for WikiText-103-Validation, we cannot see an improvement of perplexity in any combination of noise and noise placement when averaged over the random seeds, see Tables  16  to  16 .",
            "The best combination for these experiments is setting   = 10  10 \\alpha=10 italic_ = 10  and adding noise to the neighbors only, see Table  13 . Adding noise to both, sequences and neighbors, see Table  16  gives worse results than adding noise only to the sequences. This suggests that the neighbors stabilize the negative effects of the noisy sequence. Furthermore, considering our best result comes from adding noise to the neighbors only, we can see that this NEFTune noise has a regularizing effect on the neighbors. This is likely due to the fact that our retrieval database is too small for our training set. In the original  Retro  the retrieval database consisted of trillions of tokens and only saw improvement once the size of the retrieval database was in the order of billions, see the Figure on page 1 of the  Retro  paper as well as the table on page 34.",
            "As we show in Table  17 , the checkpoints we trained with uniform noise and   = 10  10 \\alpha=10 italic_ = 10  do not handle approximation to the hardware platform at inference time any better than the checkpoints trained without noise. Thus, uniform regularization does not play a role in this case.",
            "It is clear in Table  17  and Figures  6  that not only is  Gaussian,   t = 0.2 subscript  t 0.2 \\lambda_{t}=0.2 italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 0.2  the best checkpoint overall it can handle large amounts of noise better than other regularizers or no regularizer. We see how for   i = 0.2 subscript  i 0.2 \\lambda_{i}=0.2 italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 0.2  and   i = 0.4 subscript  i 0.4 \\lambda_{i}=0.4 italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 0.4  the perplexity barely increases for any of the regularizers (None, Uniform, Gaussian). For   i = 1.0 subscript  i 1.0 \\lambda_{i}=1.0 italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1.0  which is the largest relative standard deviation in our setup this changes. Here clear patterns emerge. For instance,  Gaussian,   t = 0.4 subscript  t 0.4 \\lambda_{t}=0.4 italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 0.4  does consistently worst because it has the lowest signal-to-noise ratio of the regularizers shown here. It is most affected by   i = 1.0 subscript  i 1.0 \\lambda_{i}=1.0 italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1.0 . Moreover,  Gaussian,   t = 0.2 subscript  t 0.2 \\lambda_{t}=0.2 italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 0.2  does best for every dataset and noise level at inference time. It is least affected by   i = 1.0 subscript  i 1.0 \\lambda_{i}=1.0 italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1.0 . This is unsurprising, as we also observe that out of all the regularizers  Gaussian,   t = 0.2 subscript  t 0.2 \\lambda_{t}=0.2 italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 0.2  is least affected by the inference mode  no retrieval . Most of this models generalization performance comes from improved language modeling and not from the retrieved neighbors themselves.",
            "Before we tokenize with GPT-2, we normalize the text using the  Bert  normalizer. This is essential, as  Bert  and GPT treat neither special characters nor repeated characters the same way. Without normalization, the re-tokenized chunk might be longer than 512 tokens. The pseudo-code for this algorithm is in Algorithm  1  in detail.",
            "Optimizer and Batch Size    Retro  itself used AdamW  [ 29 ] , so Adam  [ 23 ]  with true weight decay (as opposed to Adam with L2 regularization), and a linearly increasing learning rate schedule with a batch size of 256 for their smallest model. Due to our memory restrictions, we can work with a batch size of 2 at most. Thus, using the same hyperparameters as Table 11 in  [ 2 ]  does not yield the same results in terms of convergence. As a consequence, we tried a variety of optimizers and decided on RAdam  [ 28 ] , which has the additional benefit of eliminating the need for us to tune the hyperparameters.",
            "The model outputs a  n  _  e  m  b  n  _  v  o  c  a  b n _ e m b n _ v o c a b n\\_emb\\times n\\_vocab italic_n _ italic_e italic_m italic_b  italic_n _ italic_v italic_o italic_c italic_a italic_b  matrix which is interpreted as a probability distribution over the vocabulary words. In greedy generation, we take the vocabulary word with the highest probability, but that can lead to significant degeneration, see Figure  13 . To generate more natural language, it can be beneficial to occasionally opt for something other than the most likely word.",
            "The SlimPajama dataset is by far the worst in terms of perplexity. Though this can easily be addressed with minimal fine-tuning (see Section  H.4 ), we are more interested in true  plug-and-play  performance, as it pertains to retrieval. The question becomes: Can retrieval alone help a language model generalize to unseen domains? In Section  3  we show that this is the case quantitatively, here we analyze the output qualitatively as well. Looking at Figure  13  and Figure  14  side-by-side, we can see the benefit of retrieval. The topic of this excerpt is a description of an HDMI cable. For  Retro-li -off the generated outputs are vaguely about technology whereas  Retro-li -ons output is much more on-topic with respect to actual HDMI cables."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Perplexity of the plug-and-play domain shift experiments with six random seeds for different regularizers:   t subscript  t \\lambda_{t} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  refers to the relative standard deviation of a Gaussian regularizer,    \\alpha italic_  refers to the NEFTune uniform regularizer, N/A refers to no regularizer.   i subscript  i \\lambda_{i} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  refers to the relative standard deviation of the additive noise that simulates different IMC hardware inducing noisy retrieval at inference time.",
        "table": "S3.T2.131",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": [
            "Our proposed  Retro-li  is a medium-sized parametric model based on  Retro  with a small-scale non-parametric database. In  Retro-li , the embedding model used for the neighbor search is a  Bert  based sentence similarity model called  SBert . The architecture diagram of  Retro-li  is shown in Figure  2 , where we enhanced GPT-2 attention blocks similarly to what has been down with  Retro -fitting to improve our language modeling performance without extensive re-training.  Retro -fitting as introduced in the  Retro  paper  [ 2 ] , refers to taking a language model not trained with retrieval and adding retrieval in the form of a frozen retrieval database and chunked cross-attention (CCA) blocks in order to incorporate the information from neighbor embeddings in the training sequences.",
            "In  Retro-li , we set out to work with small retrieval databases and almost no token overlap (see Appendix  A.2 ). Thus, we have to be judicious about which neighbors are returned. In this setup with smaller retrieval databases, the search space of sequences is not as densely populated, thus not finding the true closest sequence carries a bigger penalty.  Considering both ease of use and theoretical results, we choose to use  SBert   [ 41 ] , a  Bert -based sentence-similarity model, in  Retro-li .",
            "Taking inspiration from NEFTune  [ 18 ] , we aim to improve generalization through a word-embedding regularizer. We introduce a new regularization method by adding noise to the word embeddings of the non-parametric memory, in contrast to NEFTune which regularizes the input sequence, in  Retro-li  their retrieved neighbors are regularized (see Figure  2 ).",
            "We explore a variety of regularizers, some similar to NEFTune, some more similar to the intrinsic noise in the stochastic IMC hardware (see Section  3.2.3 ), ensuring that the signal-to-noise ratio from different approaches remains comparable.",
            "For our retrieval experiments we only freeze the GPT-2 layers as shown in Figure  2 . We explored un-freezing these layers as well, see Appendix  F . Both  Retro-li -off and  Retro-li -on are trained on the same data and, aside from the GPT-2 attention blocks and embeddings, from scratch.",
            "Preliminary ablations performed to determine the best type of noise, both in terms of magnitude and in terms of where the noise is added (so whether to add on the input embeddings or to the neighbor embeddings), showed that regularization works best when moderately added only to the neighbor embeddings. The full results are in Appendix  C . We also performed additional experiments to determine the best type of noise among the uniform and the Gaussian while ensuring a similar signal-to-noise ratio. Results are shown in Table  2 . A Gaussian with zero mean and   t = 0.2 subscript  t 0.2 \\lambda_{t}=0.2 italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 0.2  (the variance as described by Equation  2 ) improves our generalization performance better than the rest of the regularizers we have tried.",
            "The setting  Retro-li -on with  no neighbors  is meant to ascertain how much of the domain shift performance gain is simply due to improved language modeling capabilities, and not due to the retrieval itself. In settings with  Retro-li -on, we further have a model trained with a regularizer or without one.  In Table  3 , we only include the best-performing regularizer, a Gaussian regularizer with   t = 0.2 subscript  t 0.2 \\lambda_{t}=0.2 italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 0.2  (see Equation  2 ), which also reflects the signal-to-noise ratio observed in the modern IMC hardware (more on this in Section  3.3.1 ).",
            "One drawback of the IMC hardware however is low-precision and noisy similarity searches due to analog computations with non-idealities. We simulate the noise on such a hardware platform by a Gaussian distribution with zero mean and a certain standard deviation    \\sigma italic_ , described by Equation  2 . This additive noise at inference time is denoted by   i subscript  i \\lambda_{i} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  as opposed to   t subscript  t \\lambda_{t} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  which is used to describe the noise during training. For a more in-depth explanation of noise modeling on IMC-based hardware, please refer to  [ 39 ] . For instance, for a recent large-scale chip based on phase-change memory devices  [ 22 ] , this relative standard deviation is 0.2.",
            "Training with the regularizer does not decrease the performance drop upon adding noisy retrieval at inference time in all cases. For   i = 0.2 subscript  i 0.2 \\lambda_{i}=0.2 italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 0.2  and   i = 0.4 subscript  i 0.4 \\lambda_{i}=0.4 italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 0.4 , even when we did not train using a regularizer, our performance dropped about the same relative to ideal retrieval as if we had trained with a regularizer. On the other hand, for   i = 1.0 subscript  i 1.0 \\lambda_{i}=1.0 italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1.0 , a large relative standard deviation, this regularized training does make a difference. Where other models fail, the one trained with a Gaussian regularizer is barely affected. This can be seen more clearly in Table  2  where we also add the standard deviation error bar for the six random seeds.",
            "In this section, we provide additional details on the architecture introduced in Section  2 . Specifically, we elaborate on other changes we made to the  Retro  architecture and how we implemented them.",
            "This reduces the number of trainable parameters, making it faster to train. In the paper, instead of taking a pre-trained GPT-2 checkpoint, the authors built a GPT-2-like model with the appropriately changed parameters (see Table  20 )  . Then they trained it without retrieval before  Retro -fitting it. Due to our limited resources, we use the publicly available GPT-2 checkpoints. Consequently, we had to change the aforementioned hyperparameters. Moreover, we had to train additional feed-forward network parameters at the end of the attention layers. Using a pre-trained model leads to a significant improvement of perplexity on the language modeling task compared to training  Retro  from scratch and it decreases our model size by 30%.",
            "Considering the results in Table  22 , we observe that unfreezing GPT-2 and continuing to train from a previous checkpoint does improve our performance slightly for the  Retro-li -off case. It is clear, however, that this was due to there no longer being one significant outlier (random seed 42). So it is a stabilizing effect, rather than an overall improved training.",
            "Moving on to  Retro-li -on (2 neighbors,  SBert  embeddings) in Table  23 , the case is even clearer, namely unfreezing GPT-2 did not improve our language modeling capabilities. This is likely because GPT-2 was trained to a minima of language modeling, so when adding retrieval we only have to train the retrieval parameters. Adding chunked cross-attention changed the loss landscape, which is why unfreezing does not help."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Perplexity results for the language modeling and the plug-and-play domain shift experiments for different model inference settings, averaged over six random seeds. Models with various inference settings are evaluated:  Retro-li -off and  Retro-li -on (without neighbors, with ideal neighbors, and with noisy neighbors impacted by   i  { 0.2 , 0.4 , 1 } subscript  i 0.2 0.4 1 \\lambda_{i}\\in\\{0.2,0.4,1\\} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  { 0.2 , 0.4 , 1 } ). Models are trained without a regularizer (None) and with our best regularizer (Gaussian with   t = 0.2 subscript  t 0.2 \\lambda_{t}=0.2 italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 0.2 ).",
        "table": "S3.T2.7.1.5.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": [
            "Despite all the aforementioned functional advantages of RAG, in many cases, the retrieval database of non-parametric memory contains trillions of tokens to be searched. In fact, in  Retro   [ 2 ] , the scale begins at a retrieval database size of billions of tokens, which raises the question of search speed and optimization. Although similarity search acceleration libraries such as  Faiss   [ 19 ]  and ScaNN  [ 10 ]  can compare millions of vectors in milliseconds, they quickly become a bottleneck, especially for retrieval databases containing trillions of tokens  (e.g., see Section  3.3.1 )  . To address this critical bottleneck, we suggest three directions whose effectiveness is shown in our proposed  Retro-li .",
            "We explore a variety of regularizers, some similar to NEFTune, some more similar to the intrinsic noise in the stochastic IMC hardware (see Section  3.2.3 ), ensuring that the signal-to-noise ratio from different approaches remains comparable.",
            "Table  3  shows the impact of this best-performing regularizer (the Gaussian regularizer with   t = 0.2 subscript  t 0.2 \\lambda_{t}=0.2 italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 0.2 ) in both language modeling and domain shift.  In the WikiText-103 language modeling, the regularizer improves or at least does not impair the validation perplexity in various inference settings: having no neighbors, ideal retrieval, or different amounts of noise during inference (i.e.,   i  { 0.2 , 0.4 , 1 } subscript  i 0.2 0.4 1 \\lambda_{i}\\in\\{0.2,0.4,1\\} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  { 0.2 , 0.4 , 1 } ).  For the domain shift experiments, it exhibits more promising behavior and consistently shows benefits when different noisy scenarios are considered.  More details are provided in the next subsection.",
            "The setting  Retro-li -on with  no neighbors  is meant to ascertain how much of the domain shift performance gain is simply due to improved language modeling capabilities, and not due to the retrieval itself. In settings with  Retro-li -on, we further have a model trained with a regularizer or without one.  In Table  3 , we only include the best-performing regularizer, a Gaussian regularizer with   t = 0.2 subscript  t 0.2 \\lambda_{t}=0.2 italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 0.2  (see Equation  2 ), which also reflects the signal-to-noise ratio observed in the modern IMC hardware (more on this in Section  3.3.1 ).",
            "In the context of this work, a domain refers to the formality of the language. At one end, we have Wikipedia, which is highly formal text, and on the other end, we have SlimPajama, which consists mostly of text gathered through web crawling and thus contains predominantly informal, poorly or unstructured text, in a variety of languages and even quite a bit of code (see Appendix  A.3  for more details).",
            "We take the best  Retro-li -off checkpoint and keep training it for six random seeds, with and without retrieval, and with and without regularization. WikiText-103 is taken as the base dataset (dataset A, see above) for the taining. We train both  Retro-li -on and  Retro-li -off, in order to confirm that the improved performance is not due to the increased number of training tokens. As is evident in the rows for  Retro-li -on with ideal retrieval in Table  3 , when compared to Table  1  with training from scratch, this setup decreases WikiText-103-Validation set perplexity for both,  Retro-li -on and  Retro-li -off. The seemingly significant perplexity improvement of  Retro-li -off is due to there no longer being one particularly bad random seed, as this setup reduces the variance across seeds.",
            "As shown in Table  3 , even without a regularizer, we see across seeds and datasets that  Retro-li -on deals with the domain shift better than  Retro-li -off. Adding a regularizer to the retrieval improves this performance even further. In this table, the datasets are ordered by the size of the retrieval databases. Although retrieval and regularization help, the perplexity also keeps going up, despite our retrieval database size increasing as well. However, considering the improved percentage, where  Retro-li -off is our baseline, we can see that our  Retro-li -off model struggles with a large domain shift, where retrieval and regularization help the most.",
            "Adding Gaussian noise with a variety of relative standard deviations to our neighbor word embeddings at inference time gives us stable results, despite it not being seen during training (i.e., trained with   t = 0.2 subscript  t 0.2 \\lambda_{t}=0.2 italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 0.2  while tested with   i  { 0 , 0.2 , 0.4 , 1.0 } subscript  i 0 0.2 0.4 1.0 \\lambda_{i}\\in\\{0,0.2,0.4,1.0\\} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  { 0 , 0.2 , 0.4 , 1.0 }  ). This is evident in Table  3 , in the rows describing the setting  noisy retrieval  with no regularize (i.e.,  None ). Even for a relative standard deviation as large as 1.0, the performance never drops more than 1% compared to the ideal retrieval without any noisy neighbors. This suggests that the searches on the non-parametric memory can be moved to the IMC hardware without issue.",
            "As a starting point, we take the best checkpoints created by training both  Retro-li -off and  Retro-li -on models on the base WikiText-103 dataset as mentioned in Section  3.3 . For  Retro-li -on models we observe updating CCA layer parameters leads to slightly worse performance. Hence we update only the parameters in the feed-forward layers and the read-out layers for both model types during fine-tuning for a few epochs.",
            "After fine-tuning for a few epochs, the updated model checkpoints are used to run inference on the validation sets from the corresponding datasets. The validation training sequences and the validation retrieval database are prepared exactly the same way as in plug-and-play experiments explained in Section  3.3  such that the performance can be compared under equal settings.",
            "The results of fine-tuning experiments are presented in Table  9 . We evaluate on a subset of datasets used for the plug-and-play domain shift generalization experiments, namely, BBC-News, Reuters, CNN-DailyMail. At epoch=0, i.e., before any fine-tuning, the perplexities reported on the validation show that  Retro-li -on consistently dominates  Retro-li -off. This is in line with the plug-and-play results in Table  3 ; the slight deviations are due to the number of tokens considered in each sequence for the evaluation.",
            "In the  Retro  paper they checked if the training sequences have eight or more contiguous tokens in common with one of their neighbors to determine the degree of leakage. In our case, for the training and validation set, 4.76% and 5.1% of the ten nearest neighbors respectively have at least eight contiguous tokens in common with the sequence used to query the index. Evaluating such samples qualitatively, in Figure  3  we can show that this is hardly leakage in a sense that our model could exploit, as the neighbors come from different articles.",
            "The best combination for these experiments is setting   = 10  10 \\alpha=10 italic_ = 10  and adding noise to the neighbors only, see Table  13 . Adding noise to both, sequences and neighbors, see Table  16  gives worse results than adding noise only to the sequences. This suggests that the neighbors stabilize the negative effects of the noisy sequence. Furthermore, considering our best result comes from adding noise to the neighbors only, we can see that this NEFTune noise has a regularizing effect on the neighbors. This is likely due to the fact that our retrieval database is too small for our training set. In the original  Retro  the retrieval database consisted of trillions of tokens and only saw improvement once the size of the retrieval database was in the order of billions, see the Figure on page 1 of the  Retro  paper as well as the table on page 34.",
            "Moving on to  Retro-li -on (2 neighbors,  SBert  embeddings) in Table  23 , the case is even clearer, namely unfreezing GPT-2 did not improve our language modeling capabilities. This is likely because GPT-2 was trained to a minima of language modeling, so when adding retrieval we only have to train the retrieval parameters. Adding chunked cross-attention changed the loss landscape, which is why unfreezing does not help.",
            "The model outputs a  n  _  e  m  b  n  _  v  o  c  a  b n _ e m b n _ v o c a b n\\_emb\\times n\\_vocab italic_n _ italic_e italic_m italic_b  italic_n _ italic_v italic_o italic_c italic_a italic_b  matrix which is interpreted as a probability distribution over the vocabulary words. In greedy generation, we take the vocabulary word with the highest probability, but that can lead to significant degeneration, see Figure  13 . To generate more natural language, it can be beneficial to occasionally opt for something other than the most likely word.",
            "The SlimPajama dataset is by far the worst in terms of perplexity. Though this can easily be addressed with minimal fine-tuning (see Section  H.4 ), we are more interested in true  plug-and-play  performance, as it pertains to retrieval. The question becomes: Can retrieval alone help a language model generalize to unseen domains? In Section  3  we show that this is the case quantitatively, here we analyze the output qualitatively as well. Looking at Figure  13  and Figure  14  side-by-side, we can see the benefit of retrieval. The topic of this excerpt is a description of an HDMI cable. For  Retro-li -off the generated outputs are vaguely about technology whereas  Retro-li -ons output is much more on-topic with respect to actual HDMI cables."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  Size of validation data and retrieval database in thousands of tokens and database entries in thousands for datasets considered for domain shift experiments.",
        "table": "S3.T2.7.1.6.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": [
            "Table  4  lists the datasets used for domain shift experiments, which are ordered by the size of the retrieval databases. The datasets are chosen such that they are sourced from a variety of domains and have sufficiently distinctive characteristics to identify them as unique domains. The datasets are further explained with their differences highlighted in Appendix  A .",
            "Finally, we report the 1-gram Jaccard-Similarity  [ 33 ]  of the training sequences and their two nearest neighbors in Figure  4  and the ten nearest neighbors in Figure  5  as a way to determine leakage. As is evident in the histograms, for most neighbors we do not get a Jaccard-Similarity of more than 0.2, so limited leakage is assured.",
            "This is an extension of Section  4 . In the main body of the work, only the three most important papers are chosen in order to adhere to the page limit. The papers presented here and their concepts were also crucial for this work and for possible future avenues to explore.",
            "The SlimPajama dataset is by far the worst in terms of perplexity. Though this can easily be addressed with minimal fine-tuning (see Section  H.4 ), we are more interested in true  plug-and-play  performance, as it pertains to retrieval. The question becomes: Can retrieval alone help a language model generalize to unseen domains? In Section  3  we show that this is the case quantitatively, here we analyze the output qualitatively as well. Looking at Figure  13  and Figure  14  side-by-side, we can see the benefit of retrieval. The topic of this excerpt is a description of an HDMI cable. For  Retro-li -off the generated outputs are vaguely about technology whereas  Retro-li -ons output is much more on-topic with respect to actual HDMI cables."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 :  Average search time (ms) per dataset on a Tesla V100 GPU using  Faiss  index.",
        "table": "S3.T2.7.1.7.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": [
            "In Table  5 , we measure the time it takes for the search function call to  IndexIVF_search  to complete. This function is called on 16 chunks concurrently on a Tesla V100 GPU. More on these measurements including the search time distributions can be found in Appendix  G .",
            "Finally, we report the 1-gram Jaccard-Similarity  [ 33 ]  of the training sequences and their two nearest neighbors in Figure  4  and the ten nearest neighbors in Figure  5  as a way to determine leakage. As is evident in the histograms, for most neighbors we do not get a Jaccard-Similarity of more than 0.2, so limited leakage is assured.",
            "For illustration purposes, we measure the time it takes for the search function call to  IndexIVF_search  to complete and show the results in Table  5 . This function is called on 16 chunks concurrently on a Tesla V100 GPU."
        ]
    },
    "id_table_6": {
        "caption": "Table 6 :  Inference perplexity with and without the accurate noisy retrieval",
        "table": "S3.T2.7.1.8.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": [
            "The results of this study are presented in Table  6 . We use the best-trained models on the WikiText and BBC-News datasets and compare the inference results where in one case there is no noise on retrieved neighbors and in the other case the retrieval neighbors noise is modeled comprehensively using the Analog AI hardware kit. We notice that the noisy retrieval across both datasets drops only by a negligible margin (<0.003%) indicating the robustness of the  Retro-li  models trained with regularization towards noisy retrieval.",
            "For this set of experiments, we chose three neighbors and the  SBert  word embedding model, which gave us the best results so far. Looking at the results for WikiText-103-Validation, we cannot see an improvement of perplexity in any combination of noise and noise placement when averaged over the random seeds, see Tables  16  to  16 .",
            "The best combination for these experiments is setting   = 10  10 \\alpha=10 italic_ = 10  and adding noise to the neighbors only, see Table  13 . Adding noise to both, sequences and neighbors, see Table  16  gives worse results than adding noise only to the sequences. This suggests that the neighbors stabilize the negative effects of the noisy sequence. Furthermore, considering our best result comes from adding noise to the neighbors only, we can see that this NEFTune noise has a regularizing effect on the neighbors. This is likely due to the fact that our retrieval database is too small for our training set. In the original  Retro  the retrieval database consisted of trillions of tokens and only saw improvement once the size of the retrieval database was in the order of billions, see the Figure on page 1 of the  Retro  paper as well as the table on page 34.",
            "It is clear in Table  17  and Figures  6  that not only is  Gaussian,   t = 0.2 subscript  t 0.2 \\lambda_{t}=0.2 italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 0.2  the best checkpoint overall it can handle large amounts of noise better than other regularizers or no regularizer. We see how for   i = 0.2 subscript  i 0.2 \\lambda_{i}=0.2 italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 0.2  and   i = 0.4 subscript  i 0.4 \\lambda_{i}=0.4 italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 0.4  the perplexity barely increases for any of the regularizers (None, Uniform, Gaussian). For   i = 1.0 subscript  i 1.0 \\lambda_{i}=1.0 italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1.0  which is the largest relative standard deviation in our setup this changes. Here clear patterns emerge. For instance,  Gaussian,   t = 0.4 subscript  t 0.4 \\lambda_{t}=0.4 italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 0.4  does consistently worst because it has the lowest signal-to-noise ratio of the regularizers shown here. It is most affected by   i = 1.0 subscript  i 1.0 \\lambda_{i}=1.0 italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1.0 . Moreover,  Gaussian,   t = 0.2 subscript  t 0.2 \\lambda_{t}=0.2 italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 0.2  does best for every dataset and noise level at inference time. It is least affected by   i = 1.0 subscript  i 1.0 \\lambda_{i}=1.0 italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1.0 . This is unsurprising, as we also observe that out of all the regularizers  Gaussian,   t = 0.2 subscript  t 0.2 \\lambda_{t}=0.2 italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 0.2  is least affected by the inference mode  no retrieval . Most of this models generalization performance comes from improved language modeling and not from the retrieved neighbors themselves."
        ]
    },
    "id_table_7": {
        "caption": "Table 7 :  Retro-li -off generated text for a SlimPajama-6B-Validation sample with the lowest  Retro-li -off perplexity.",
        "table": "S3.T2.7.1.9.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": [
            "For  Retro-li -on and  Retro-li -off we pick their best-performing validation samples on SlimPajama-Validation, feed 75% of the context window into the model and generate the next chunk. In order to evaluate the similarity of the generated chunk to the real continuation, we embed it using  SBert  and report the similarity measure. We employed several generation modes of which we only present the best-performing ones from retrieval on and off in Tables  7  and  8  respectively, full results for this experiment can be found in Appendix  H .",
            "As we show in Table  17 , the checkpoints we trained with uniform noise and   = 10  10 \\alpha=10 italic_ = 10  do not handle approximation to the hardware platform at inference time any better than the checkpoints trained without noise. Thus, uniform regularization does not play a role in this case.",
            "It is clear in Table  17  and Figures  6  that not only is  Gaussian,   t = 0.2 subscript  t 0.2 \\lambda_{t}=0.2 italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 0.2  the best checkpoint overall it can handle large amounts of noise better than other regularizers or no regularizer. We see how for   i = 0.2 subscript  i 0.2 \\lambda_{i}=0.2 italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 0.2  and   i = 0.4 subscript  i 0.4 \\lambda_{i}=0.4 italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 0.4  the perplexity barely increases for any of the regularizers (None, Uniform, Gaussian). For   i = 1.0 subscript  i 1.0 \\lambda_{i}=1.0 italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1.0  which is the largest relative standard deviation in our setup this changes. Here clear patterns emerge. For instance,  Gaussian,   t = 0.4 subscript  t 0.4 \\lambda_{t}=0.4 italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 0.4  does consistently worst because it has the lowest signal-to-noise ratio of the regularizers shown here. It is most affected by   i = 1.0 subscript  i 1.0 \\lambda_{i}=1.0 italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1.0 . Moreover,  Gaussian,   t = 0.2 subscript  t 0.2 \\lambda_{t}=0.2 italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 0.2  does best for every dataset and noise level at inference time. It is least affected by   i = 1.0 subscript  i 1.0 \\lambda_{i}=1.0 italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1.0 . This is unsurprising, as we also observe that out of all the regularizers  Gaussian,   t = 0.2 subscript  t 0.2 \\lambda_{t}=0.2 italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 0.2  is least affected by the inference mode  no retrieval . Most of this models generalization performance comes from improved language modeling and not from the retrieved neighbors themselves.",
            "For later experiments, we use our trained  Retro-li -off with GPT-2 attention blocks as a checkpoint. This enables us to freeze all decoder layers except the CCA, just as in the original paper. Additionally, it decreases the number of trainable parameters even further from the complete  Retro-li  by 90.29%. The architecture is visualized in Diagram  7 ."
        ]
    },
    "id_table_8": {
        "caption": "Table 8 :  Retro-li -on generated text for a SlimPajama-6B-Validation sample with the lowest  Retro-li -on perplexity.",
        "table": "S3.T3.8",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": [
            "For  Retro-li -on and  Retro-li -off we pick their best-performing validation samples on SlimPajama-Validation, feed 75% of the context window into the model and generate the next chunk. In order to evaluate the similarity of the generated chunk to the real continuation, we embed it using  SBert  and report the similarity measure. We employed several generation modes of which we only present the best-performing ones from retrieval on and off in Tables  7  and  8  respectively, full results for this experiment can be found in Appendix  H ."
        ]
    },
    "id_table_9": {
        "caption": "Table 9 :  Perplexity results on the validation set for the domain shift experiments of three datasets with fine-tuning the feed-forward and the read-out layers for both  Retro-li -on (with the best regularization) and  Retro-li -off using different epochs (17). Epochs at which the lowest perplexities are achieved are highlighted in each row. The number of training tokens processed per epoch is shown in the # Train tokens column.",
        "table": "S3.T3.8.5.1.7.1.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": [
            "The results of fine-tuning experiments are presented in Table  9 . We evaluate on a subset of datasets used for the plug-and-play domain shift generalization experiments, namely, BBC-News, Reuters, CNN-DailyMail. At epoch=0, i.e., before any fine-tuning, the perplexities reported on the validation show that  Retro-li -on consistently dominates  Retro-li -off. This is in line with the plug-and-play results in Table  3 ; the slight deviations are due to the number of tokens considered in each sequence for the evaluation."
        ]
    },
    "id_table_10": {
        "caption": "Table 10 :  Dataset proportions SlimPajama and The Pile.",
        "table": "S3.T3.8.5.1.8.1.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": [
            "Retro  was trained on MassiveText  [ 37 ]  which is unfortunately proprietary. Furthermore, The Pile  [ 6 ]  which is an open-source alternative to MassiveText and on which  Retro  was also trained for comparison purposes, was taken down in July of 2023 due to a DMCA (Digital Millenium Copyright Act) notice 3 3 3 https://academictorrents.com/details/ 0d366035664fdf51cfbe9f733953ba325776e667. . The  Nvidia   Retro  implementation training data was mostly based on The Pile  [ 45 ]  as well. This means that we could not use the same dataset as  Retro , which would make it harder for us to compare our performance to theirs. As an alternative, looking for datasets similar in data sources and size to The Pile, we decided on SlimPajama  [ 46 ]  which is a cleaned and de-duplicated version of RedPajama  [ 4 ] . De-duplication is especially important for textual training data, see  [ 25 ] . In Table  10  we compare The Pile to SlimPajama, where Pile-CC is comparable to CommonCrawl and C4  [ 38 ] . We see that although the data sources themselves are similar, the proportions are only somewhat comparable."
        ]
    },
    "id_table_11": {
        "caption": "Table 11 :  Comparison of features among different dataset domains",
        "table": "S3.T3.8.5.1.9.1.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": [
            "Optimizer and Batch Size    Retro  itself used AdamW  [ 29 ] , so Adam  [ 23 ]  with true weight decay (as opposed to Adam with L2 regularization), and a linearly increasing learning rate schedule with a batch size of 256 for their smallest model. Due to our memory restrictions, we can work with a batch size of 2 at most. Thus, using the same hyperparameters as Table 11 in  [ 2 ]  does not yield the same results in terms of convergence. As a consequence, we tried a variety of optimizers and decided on RAdam  [ 28 ] , which has the additional benefit of eliminating the need for us to tune the hyperparameters."
        ]
    },
    "id_table_12": {
        "caption": "Table 12 :  Signal to noise ratio averaged over the WikiText-103-Validation word embeddings.",
        "table": "S3.T3.8.5.1.10.1.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_13": {
        "caption": "Table 13 :  Perplexity results of NEFTune noise with alpha 5, 10, 15 added to sequence, neighbors, and both in  Retro-li -on, three neighbors using  SBert  embeddings, averaged over three random seeds.",
        "table": "S3.T3.8.5.1.11.1.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": [
            "The best combination for these experiments is setting   = 10  10 \\alpha=10 italic_ = 10  and adding noise to the neighbors only, see Table  13 . Adding noise to both, sequences and neighbors, see Table  16  gives worse results than adding noise only to the sequences. This suggests that the neighbors stabilize the negative effects of the noisy sequence. Furthermore, considering our best result comes from adding noise to the neighbors only, we can see that this NEFTune noise has a regularizing effect on the neighbors. This is likely due to the fact that our retrieval database is too small for our training set. In the original  Retro  the retrieval database consisted of trillions of tokens and only saw improvement once the size of the retrieval database was in the order of billions, see the Figure on page 1 of the  Retro  paper as well as the table on page 34.",
            "The model outputs a  n  _  e  m  b  n  _  v  o  c  a  b n _ e m b n _ v o c a b n\\_emb\\times n\\_vocab italic_n _ italic_e italic_m italic_b  italic_n _ italic_v italic_o italic_c italic_a italic_b  matrix which is interpreted as a probability distribution over the vocabulary words. In greedy generation, we take the vocabulary word with the highest probability, but that can lead to significant degeneration, see Figure  13 . To generate more natural language, it can be beneficial to occasionally opt for something other than the most likely word.",
            "The SlimPajama dataset is by far the worst in terms of perplexity. Though this can easily be addressed with minimal fine-tuning (see Section  H.4 ), we are more interested in true  plug-and-play  performance, as it pertains to retrieval. The question becomes: Can retrieval alone help a language model generalize to unseen domains? In Section  3  we show that this is the case quantitatively, here we analyze the output qualitatively as well. Looking at Figure  13  and Figure  14  side-by-side, we can see the benefit of retrieval. The topic of this excerpt is a description of an HDMI cable. For  Retro-li -off the generated outputs are vaguely about technology whereas  Retro-li -ons output is much more on-topic with respect to actual HDMI cables."
        ]
    },
    "id_table_14": {
        "caption": "Table 14 :  Perplexity results of NEFTune noise added to the training sequence in  Retro-li -on, three neighbors,  SBert  embeddings.",
        "table": "S3.T4.4",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": [
            "The SlimPajama dataset is by far the worst in terms of perplexity. Though this can easily be addressed with minimal fine-tuning (see Section  H.4 ), we are more interested in true  plug-and-play  performance, as it pertains to retrieval. The question becomes: Can retrieval alone help a language model generalize to unseen domains? In Section  3  we show that this is the case quantitatively, here we analyze the output qualitatively as well. Looking at Figure  13  and Figure  14  side-by-side, we can see the benefit of retrieval. The topic of this excerpt is a description of an HDMI cable. For  Retro-li -off the generated outputs are vaguely about technology whereas  Retro-li -ons output is much more on-topic with respect to actual HDMI cables."
        ]
    },
    "id_table_15": {
        "caption": "(a)   Sliding window.",
        "table": "S3.T4.4.1.1.2.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_16": {
        "caption": "(b)   Full results.",
        "table": "S3.T4.4.1.1.3.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": [
            "For this set of experiments, we chose three neighbors and the  SBert  word embedding model, which gave us the best results so far. Looking at the results for WikiText-103-Validation, we cannot see an improvement of perplexity in any combination of noise and noise placement when averaged over the random seeds, see Tables  16  to  16 .",
            "The best combination for these experiments is setting   = 10  10 \\alpha=10 italic_ = 10  and adding noise to the neighbors only, see Table  13 . Adding noise to both, sequences and neighbors, see Table  16  gives worse results than adding noise only to the sequences. This suggests that the neighbors stabilize the negative effects of the noisy sequence. Furthermore, considering our best result comes from adding noise to the neighbors only, we can see that this NEFTune noise has a regularizing effect on the neighbors. This is likely due to the fact that our retrieval database is too small for our training set. In the original  Retro  the retrieval database consisted of trillions of tokens and only saw improvement once the size of the retrieval database was in the order of billions, see the Figure on page 1 of the  Retro  paper as well as the table on page 34."
        ]
    },
    "id_table_17": {
        "caption": "Table 15 :  NEFTune noise added to the neighbors in  Retro-li -on, three neighbors,  SBert  embeddings.",
        "table": "S3.T4.4.1.1.4.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": [
            "As we show in Table  17 , the checkpoints we trained with uniform noise and   = 10  10 \\alpha=10 italic_ = 10  do not handle approximation to the hardware platform at inference time any better than the checkpoints trained without noise. Thus, uniform regularization does not play a role in this case.",
            "It is clear in Table  17  and Figures  6  that not only is  Gaussian,   t = 0.2 subscript  t 0.2 \\lambda_{t}=0.2 italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 0.2  the best checkpoint overall it can handle large amounts of noise better than other regularizers or no regularizer. We see how for   i = 0.2 subscript  i 0.2 \\lambda_{i}=0.2 italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 0.2  and   i = 0.4 subscript  i 0.4 \\lambda_{i}=0.4 italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 0.4  the perplexity barely increases for any of the regularizers (None, Uniform, Gaussian). For   i = 1.0 subscript  i 1.0 \\lambda_{i}=1.0 italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1.0  which is the largest relative standard deviation in our setup this changes. Here clear patterns emerge. For instance,  Gaussian,   t = 0.4 subscript  t 0.4 \\lambda_{t}=0.4 italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 0.4  does consistently worst because it has the lowest signal-to-noise ratio of the regularizers shown here. It is most affected by   i = 1.0 subscript  i 1.0 \\lambda_{i}=1.0 italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1.0 . Moreover,  Gaussian,   t = 0.2 subscript  t 0.2 \\lambda_{t}=0.2 italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 0.2  does best for every dataset and noise level at inference time. It is least affected by   i = 1.0 subscript  i 1.0 \\lambda_{i}=1.0 italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1.0 . This is unsurprising, as we also observe that out of all the regularizers  Gaussian,   t = 0.2 subscript  t 0.2 \\lambda_{t}=0.2 italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 0.2  is least affected by the inference mode  no retrieval . Most of this models generalization performance comes from improved language modeling and not from the retrieved neighbors themselves."
        ]
    },
    "id_table_18": {
        "caption": "(a)   Sliding window.",
        "table": "S3.T5.5",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_19": {
        "caption": "(b)   Full results.",
        "table": "S3.T6.4",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_20": {
        "caption": "Table 16 :  NEFTune noise added to both training sequence and neighbors in  Retro-li -on, three neighbors,  SBert  embeddings.",
        "table": "S3.T7.6",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": [
            "This reduces the number of trainable parameters, making it faster to train. In the paper, instead of taking a pre-trained GPT-2 checkpoint, the authors built a GPT-2-like model with the appropriately changed parameters (see Table  20 )  . Then they trained it without retrieval before  Retro -fitting it. Due to our limited resources, we use the publicly available GPT-2 checkpoints. Consequently, we had to change the aforementioned hyperparameters. Moreover, we had to train additional feed-forward network parameters at the end of the attention layers. Using a pre-trained model leads to a significant improvement of perplexity on the language modeling task compared to training  Retro  from scratch and it decreases our model size by 30%."
        ]
    },
    "id_table_21": {
        "caption": "(a)   Sliding window.",
        "table": "S3.T8.6",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_22": {
        "caption": "(b)   Full results.",
        "table": "S3.T9.3",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": [
            "Considering the results in Table  22 , we observe that unfreezing GPT-2 and continuing to train from a previous checkpoint does improve our performance slightly for the  Retro-li -off case. It is clear, however, that this was due to there no longer being one significant outlier (random seed 42). So it is a stabilizing effect, rather than an overall improved training."
        ]
    },
    "id_table_23": {
        "caption": "Table 17 :  Perplexity results for the language modeling and the domain shift experiments, averaged over six random seeds. Models with various settings are evaluated:  Retro-li -off and  Retro-li -on (without neighbors, with ideal neighbors, and with noisy neighbors impacted by   i  { 0.2 , 0.4 , 1 } subscript  i 0.2 0.4 1 \\lambda_{i}\\in\\{0.2,0.4,1\\} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  { 0.2 , 0.4 , 1 } ).",
        "table": "A1.T10.4",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": [
            "Moving on to  Retro-li -on (2 neighbors,  SBert  embeddings) in Table  23 , the case is even clearer, namely unfreezing GPT-2 did not improve our language modeling capabilities. This is likely because GPT-2 was trained to a minima of language modeling, so when adding retrieval we only have to train the retrieval parameters. Adding chunked cross-attention changed the loss landscape, which is why unfreezing does not help."
        ]
    },
    "id_table_24": {
        "caption": "Table 18 :  Perplexity results of experiments for  Retro-li -on/-off with  Bert  embeddings on a variety of number of neighbors and random seeds.",
        "table": "A1.T11.4",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_25": {
        "caption": "(a)   Sliding window.",
        "table": "A1.T11.4.1.1.5.1.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_26": {
        "caption": "(b)   Full results.",
        "table": "A1.T11.4.1.1.6.1.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_27": {
        "caption": "Table 19 :  Perplexity results of experiments for  Retro-li -on/-off with  SBert  embeddings on a variety of number of neighbors and random seeds.",
        "table": "A1.T11.4.1.1.7.1.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_28": {
        "caption": "(a)   Sliding window.",
        "table": "A1.T11.4.1.1.8.1.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_29": {
        "caption": "(b)   Full results.",
        "table": "A1.T11.4.1.1.9.1.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_30": {
        "caption": "Table 20 :  Changes to the hyperparameters for  Retro -fitting.",
        "table": "A3.T12.5",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_31": {
        "caption": "Table 21 :  Input and output dimensions for the blocks in the  Retro  architecture.",
        "table": "A3.T13.6",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_32": {
        "caption": "Table 22 :  Perplexity results of  Retro-li -off with unfrozen GPT-2 backbone.",
        "table": "A3.T14.st1.4",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_33": {
        "caption": "(a)   Sliding window.",
        "table": "A3.T14.st1.4.1.1.1.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_34": {
        "caption": "(b)   Full results.",
        "table": "A3.T14.st2.4",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_35": {
        "caption": "Table 23 :  Perplexity results of  Retro-li -on with unfrozen GPT-2 backbone.",
        "table": "A3.T14.st2.4.1.1.1.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_36": {
        "caption": "(a)   Sliding window.",
        "table": "A3.T15.st1.4",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_37": {
        "caption": "(b)   Full results.",
        "table": "A3.T15.st1.4.1.1.1.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_38": {
        "caption": "Table 24 :  Results of fine-tuning on other domains.",
        "table": "A3.T15.st2.4",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_39": {
        "caption": "",
        "table": "A3.T15.st2.4.1.1.1.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_40": {
        "caption": "",
        "table": "A3.T16.st1.4",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_41": {
        "caption": "",
        "table": "A3.T16.st1.4.1.1.1.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_42": {
        "caption": "",
        "table": "A3.T16.st2.4",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_43": {
        "caption": "",
        "table": "A3.T16.st2.4.1.1.1.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_44": {
        "caption": "",
        "table": "A3.T17.20",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_45": {
        "caption": "",
        "table": "A3.T17.20.19.1.7.1.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_46": {
        "caption": "",
        "table": "A3.T17.20.19.1.8.1.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_47": {
        "caption": "",
        "table": "A3.T17.20.19.1.9.1.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_48": {
        "caption": "",
        "table": "A3.T17.20.19.1.10.1.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_49": {
        "caption": "",
        "table": "A3.T17.20.19.1.11.1.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_50": {
        "caption": "",
        "table": "A4.T18.st1.4",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_51": {
        "caption": "",
        "table": "A4.T18.st2.4",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_52": {
        "caption": "",
        "table": "A4.T19.st1.4",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_53": {
        "caption": "",
        "table": "A4.T19.st2.4",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_54": {
        "caption": "",
        "table": "A5.T20.5",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_55": {
        "caption": "",
        "table": "A5.T21.5",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_56": {
        "caption": "",
        "table": "A5.T21.5.2.2.1.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_57": {
        "caption": "",
        "table": "A5.T21.5.3.3.1.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_58": {
        "caption": "",
        "table": "A5.T21.5.4.4.1.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_59": {
        "caption": "",
        "table": "A5.T21.5.5.5.2.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_60": {
        "caption": "",
        "table": "A5.T21.5.8.8.1.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_61": {
        "caption": "",
        "table": "A5.T21.5.8.8.2.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_62": {
        "caption": "",
        "table": "A6.T22.st1.4",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_63": {
        "caption": "",
        "table": "A6.T22.st2.4",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_64": {
        "caption": "",
        "table": "A6.T23.st1.4",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_65": {
        "caption": "",
        "table": "A6.T23.st2.4",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_66": {
        "caption": "",
        "table": "A8.T24.4",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_67": {
        "caption": "",
        "table": "A8.T24.4.1.1.2.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_68": {
        "caption": "",
        "table": "A8.T24.4.1.1.3.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "id_table_69": {
        "caption": "",
        "table": "A8.T24.4.1.1.4.1",
        "footnotes": [
            "The vocabulary size of the original",
            "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size."
        ],
        "references": []
    },
    "global_footnotes": [
        "https://en.wikipedia.org/wiki/Wikipedia:Good_articles.",
        "https://en.wikipedia.org/wiki/ Wikipedia:Featured_article_criteria.",
        "https://academictorrents.com/details/ 0d366035664fdf51cfbe9f733953ba325776e667.",
        "https://huggingface.co/datasets/DKYoon/SlimPajama-6B.",
        "www.reddit.com.",
        "https://paperswithcode.com/sota/language-modelling-on-wikitext-103.",
        "The vocabulary size of the original",
        "is 128000, but our training data is much smaller thus we chose an appropriately smaller vocabulary size.",
        "Also called inverted lists.",
        "https://faiss.ai/cpp_api/struct/structfaiss_1_1IndexIVFPQ.html.",
        "https://github.com/facebookresearch/faiss/wiki/Faster-search."
    ]
}