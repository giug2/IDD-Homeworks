{
    "PAPER'S NUMBER OF TABLES": 1,
    "S4.T1": {
        "caption": "Table 1: Variational inference schemes encompassed by the PVI framework. (See next page.)\nSelected past work has been organized into four categories: global VI (PVI with M=1ğ‘€1M=1), fully local PVI (M=Nğ‘€ğ‘M=N), Power EP variants, and online VI. The citation to the work is provided along with the granularity of the method (global indicates M=1ğ‘€1M=1, fully local M=Nğ‘€ğ‘M=N, local implies general Mğ‘€M can be used).\nThe optimization used from the PVI perspective on this work is noted. Abbreviations used here are: Conjugate Gradient (CG) and Monte Carlo (MC). The model class that the scheme encompasses is noted (conjugate versus non-conjugate) along with the specific models that the scheme was tested on. Model abbreviations are: Non-linear State-space Model (NSSM), Non-linear Factor Analysis (NFA), Latent Dirichlet Allocation (LDA), Poisson Mixed Model (PMM), Heteroscedastic Linear Regression (HLR), Sparse Gaussian Processes (SGPs), Graphical Model (GM), Logistic Regression (LR), Beta-binomial (BB), Stochastic Volatility model (SV), Probit Regression (PR), Multinomial Regression (MR), Bayesian Neural Network (BNN), Gamma factor model (GFM), Poisson Gamma Matrix Factorization (PGMF), Mixture of Gaussians (MoG). Poisson Mixed Model (PMM), Heteroscedastic Linear Regression (HLR), Gaussian Latent Variable (GLV).\nIf the scheme proposed by the method has a name, this is noted in the final column. Abbreviations of the inference scheme are: Automatic Differentiation VI (ADVI), Incremental VI (IVI), Non-conjugate Variational Message Passing (NC-VMP), Simplified NC-VMP (SNC-VMP), Conjugate-Computation VI (CCVI), Power EP (PEP), Alpha-divergence PEP (ADPEP), Convergent Power EP (CPEP), Stochastic Power EP (SPEP), Variational Continual Learning (VCL), Bayesian Gradient Descent (BGD). ",
        "table": "",
        "footnotes": "",
        "references": [
            [
                "The convergent distributed Power EP approach of ",
                "Hasenclever etÂ al. (",
                "2017",
                ")",
                " recovers a version of PVI as ",
                "Î±",
                "â†’",
                "0",
                "â†’",
                "ğ›¼",
                "0",
                "\\alpha\\rightarrow 0",
                " with convergence guarantees. The PVI approach is also similar in spirit to ",
                "Gelman etÂ al. (",
                "2014",
                "); Hasenclever etÂ al. (",
                "2017",
                ")",
                " who use EP to split up data sets into small parts that are amenable to MCMC. Here we are using PVI to split up data sets so that they are amenable for optimization."
            ]
        ]
    }
}