{
    "id_table_1": {
        "caption": "Table 1.  The dataset statistics include the number of quadruples in the training, validation, and test sets, denoted as #Training, #Validation, and #Test, respectively.  N obs subscript N obs N_{\\text{obs}} italic_N start_POSTSUBSCRIPT obs end_POSTSUBSCRIPT  indicates the total snapshots of the temporal knowledge graph (tKG), with each snapshot capturing its state at a distinct time point.",
        "table": "S4.T1.31.31",
        "footnotes": [],
        "references": [
            "Knowledge Graphs (KGs) and their dynamic extension, Temporal Knowledge Graphs (tKGs), play a pivotal role in AI applications like recommendation engines and web searches by structuring data into graph-formatted databases. KGs utilize triples  ( s , r , o ) s r o (s,r,o) ( italic_s , italic_r , italic_o ) where  s s s italic_s  is the subject,  o o o italic_o  is the object, and  r r r italic_r  describes their relationto encode facts, while tKGs add a time element  ( t ) t (t) ( italic_t )  to represent time-validity of the fact, allowing them to capture how facts evolve over time. tKG reasoning, essential for deriving new insights, encompasses interpolationto fill in historical data gaps  (Jiang et al . ,  2016 ; Dasgupta et al . ,  2018 ; Goel et al . ,  2020a ; Wu et al . ,  2020 ; Lacroix et al . ,  2020 ; Garcia-Duran et al . ,  2018 ) and extrapolation, for future event prediction  (Jin et al . ,  2019 ; Trivedi et al . ,  2017 ) . Predicting future events in tKGs is difficult, it requires handling unseen time periods and entirely new entities, demanding advanced methods to navigate the ever-changing nature of relationships. Closed-source pretrained large language models(PLLMs) like OpenAI ChatGPT (Achiam et al . ,  2023 )  and Google Gemini (Team et al . ,  2023 )  show potential for predicting future events due to their extensive pre-training knowledge and reasoning abilities. However, these large-scale models face challenges like fact recall issues stemming from complex architectures, resulting in unreliable predictions (hallucinations), potential bias from training data, and inadvertent leveraging of future data during pretraining, causing data leakage for tKG forecasting. Detecting data leakage is challenging when opaque training datasets are used for pretraining proprietary LLMs. Ensuring predictions rely solely on legitimate predictive abilities without the undue influence from future data remains a critical challenge for trustworthy tKG forecasting. To address the limitations of existing methods, we present  sLA-tKGF , a small-scale language assistant for tKG forecasting based on Retrieval-Augmented Generation (RAG)  (Lewis et al . ,  2020 )  to ground predictions in historical context with source attribution and traceability. The framework employs a multi-layered stacked vanilla transformer architecture  (Vaswani et al . ,  2017 )  as its backbone language model and custom trained from scratch to avoid biases and data leakage inherent in pre-trained LLMs. The framework incorporates three key components: (i) retrieval of relevant historical knowledge from the tKGs. By retrieving historical facts based on context and semantic similarity to the query, we can infer causality and gain insights into temporal dynamics. Additionally, we employ text-embedding model and semantic similarity for query matching to filter out anachronistic/irrelevant information. (ii) utilizing PLLMs to analyze entity relationships prior to the target time to generate textual descriptions of historical entity relationships based on their internal knowledge acquired from vast pre-training text corpora. (iii) web scraping for up-to-date contextual information relevant to the query. We utilize advanced pre-processing techniques like sentence tokenization, temporal tagging, and date conversion for excluding future facts beyond the target time and retain appropriate scraped data. By incorporating information from diverse sources within a carefully crafted knowledge-augmented prompt, the framework generates factually accurate predictions grounded in historical context and ensures explainability. It minimizes the risk of bias, hallucinations, and avoids data leakage by pruning out-of-bound information. This Tabula Rasa approach of training the framework from scratch ensures a foundation of accountability and trustworthiness in tKG forecasting by ensuring predictions are truly based on past knowledge and patterns. Figure  1  provides an overview of the proposed approach. In summary, our proposed retrieval-augmented small-scale language model significantly improves tKG forecasting. It achieves this by dynamically accessing and leveraging external, continually evolving diverse data sources. This enhances the frameworks utility in real-world applications, enabling it to generate historically accurate and well-explained forecasts. Experiments on real-world benchmark datasets demonstrate the framework effectiveness.",
            "We evaluate our framework with the following benchmark datasets: ICEWS14 (Trivedi et al . ,  2017 ) , ICEWS18 (Boschee et al . ,  2015 ) , ICEWS05-15 (Garcia-Duran et al . ,  2018 ) , YAGO (Mahdisoltani et al . ,  2013 ) , and WIKI (Leblay and Chekol,  2018 ) . These diverse datasets, encapsulate real-world events in a quadruple format. The ICEWS datasets offers geo-coded events for systematic analysis and prediction relating to political violence, instability, and international relations from news reports over multi-decade time periods. For example, a quadruple fact  ( s , r , o , t ) s r o t (s,r,o,t) ( italic_s , italic_r , italic_o , italic_t )  such as (Barack Obama, visited, India, 2015-01-25). ICEWS14, ICEWS18, and ICEWS05-15 span events from 2014, january to october 2018, and 2005 to 2015, respectively. WIKI and YAGO, extracted from Wikipedia (Leblay and Chekol,  2018 )  and YAGO3-10 (Mahdisoltani et al . ,  2013 )  knowledge graphs (KGs), cover historical facts formatted as  ( s , p , o , [ T s , T e ] ) s p o subscript T s subscript T e (s,p,o,[T_{s},T_{e}]) ( italic_s , italic_p , italic_o , [ italic_T start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , italic_T start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ] ) , with  T s subscript T s T_{s} italic_T start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  and  T e subscript T e T_{e} italic_T start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT  marking start and end times. Our paper adopts the method from previous work (Jin et al . ,  2019 ) , for temporal fact discretization by breaking down multi-timestamp events into consecutive single events, improving accuracy by isolating each event. Table  1  summarizes the dataset statistics. We utilize a new dataset from the Armed Conflict Location & Event Data Project (ACLED) 1 1 1 https://data.humdata.org/organization/acled , detailing crisis situations globally. Our study targets hostility and aggression towards civilians in Cabo Delgado from January 1900 to March 2022, with the period from October 2021 to March 2022 as the test phase. Following the method in an earlier study (Jin et al . ,  2019 ) , we split datasets into training, validation, and test sets chronologically with an 80 % percent \\% % /10 % percent \\% % /10 % percent \\% %  ratio, except ICEWS14, split 50 % percent \\% % /50 % percent \\% %  due to no validation set. Splitting ensures training precedes validation and testing. Notably, many entities in the test sets are new compared to those in training and validation. Our research categorizes two types of tKG forecasting benchmarks: ICEWS datasets, featuring recurring short-lived events, e.g.,  Xi Jinping visited the United States three times in 2015 , and WIKI and YAGO datasets, capturing longer, non-recurring events, e.g.,  Yossi Benayoun played for Chelsea FC from 2011 to 2013 .",
            "In this section, we evaluate the impact of augmenting natural language questions(verbalized queries) with external knowledge from historical events sampled from tKGs on the  sLA-tKGF W/GPT-4  frameworks performance in tKG forecasting tasks. We explore various knowledge retrieval strategies for constructing knowledge-augmented prompts, focusing on optimizing the frameworks performance in tKG forecasting. Table  10  shows how different strategies affect the  sLA-tKGF W/GPT-4  frameworks performance across datasets (YAGO, WIKI, ICEWS14, ICEWS18, ACLED-CD22) for Single-Step and Multi-Step forecasting. The results demonstrate that selectively incorporating relevant prior knowledge significantly enhances tKG forecasting performance, surpassing baselines that either omit additional knowledge or use it indiscriminately.",
            "Our framework utilizes historical events or facts from time  ( t q  m ) subscript t q m (t_{q}-m) ( italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT - italic_m )  to  t q subscript t q t_{q} italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT  to predict missing entities in the query quadruple at  t q subscript t q t_{q} italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , where  m m m italic_m  represents the historical context window size, a configurable hyperparameter. We experimented with various lengths to evaluate the impact of historical context on the forecasting performance of the  sLA-tKGF W/GPT-4  framework for tKGs. According to Table  11 , leveraging a greater amount of past data to generate knowledge-augmented prompts for the small-scale language model enhances the accuracy of missing entity predictions, as evidenced by improved mean reciprocal rank (MRR) scores, albeit at the expense of higher computational demands. The small-scale language model within the  sLA-tKGF W/GPT-4  framework is constrained by the maximum input token sequence length. Although extending the historical window marginally increases MRR scores, it leads to the creation of longer and more complex prompts that are impractical for broad-scale application. To strike a balance between accuracy and computational efficiency, we selected a historical context window of 25 for our experiments. We varied the historical facts in the prompts to identify the optimal setup for generating accurate forecasts, with the goal of minimizing wall-clock time. Wall-clock time for  sLA-tKGF W/GPT-4  queriesthe time elapsed from query submission to responseis influenced by the size of the small-scale language model, the complexity of the query, and the available computational resources. While typically, small-scale language models return responses within seconds, more complex or extensive queries may require longer processing times.",
            "Many state-of-the-art techniques struggle with tKGs featuring irregular time intervals, unlike the  sLA-tKGF  framework, which effectively addresses this issue by leveraging knowledge-augmented prompting for small-scale language models in temporal KG forecasting. The framework excels in handling real-world complexities and data sparsity, capturing complex dynamics and causal relationships more accurately, thus offering a versatile and reliable solution for temporal KG forecasting. Experimental evaluations on the ICEWS05-15 _ _ \\_ _ continuous dataset, a subset created by sampling from the original ICEWS05-15 dataset to simulate non-periodic observations in continuous time with 1-4 units interval, support our claim. The  sLA-tKGF  framework, trained on this benchmark and assessed using the Mean Reciprocal Rank (MRR) metric, demonstrates strong performance, especially with the  sLA-tKGF W/GPT-4  configuration, on temporal KGs with irregular intervals. The dataset statistics for ICEWS05-15 _  continuous _ continuous \\_\\text{continuous} _ continuous  are presented in Table  12 . We trained the  sLA-tKGF  framework with various off-the-shelf Pretrained Large Language Models (PLLMs) on this new benchmark dataset and evaluated their performance using the Mean Reciprocal Rank (MRR) metric. As demonstrated in Table  13 , our results validate that the  sLA-tKGF W/GPT-4  framework, exhibits strong performance on tKG forecasting task with irregular time intervals.",
            "In this work, we introduce a zero-shot learning method to predict missing entities in query quadruples. Our approach includes: (a) Constructing a historical context for a query quadruple ( s q , p q , ? , t q subscript s q subscript p q ? subscript t q s_{q},p_{q},?,t_{q} italic_s start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , ? , italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) using historical facts from previous static KG snapshots  G t q  m : t q  1 subscript G : subscript t q m subscript t q 1 \\mathcal{G}_{t_{q}-m:t_{q}-1} caligraphic_G start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT - italic_m : italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT - 1 end_POSTSUBSCRIPT  to form a knowledge-infused augmented prompt. (b) Converting retrieved facts and the query into verbalized sentences, employing sentence embedding for knowledge distillation, and using semantic similarity to filter facts, thereby constructing augmented prompts. (c) Estimating the missing entity conditioned on the augmented prompt, following  o q  P  ( o | ( s q , r q , ? , t q ) , G ( t q  m , t q  1 ) ) similar-to subscript o q P conditional o subscript s q subscript r q ? subscript t q subscript G subscript t q m subscript t q 1 o_{q}\\sim P(o\\hskip 1.42262pt|\\hskip 1.42262pt(s_{q},r_{q},?,t_{q}),\\mathcal{G%  }_{(t_{q}-m,t_{q}-1)}) italic_o start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT  italic_P ( italic_o | ( italic_s start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , ? , italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) , caligraphic_G start_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT - italic_m , italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT - 1 ) end_POSTSUBSCRIPT ) . We evaluate the impact of  single-subject entity  and  subject entity-relation pair  historical facts on forecasting accuracy. The former involves only the subject entity  ( s q ) subscript s q (s_{q}) ( italic_s start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) , while the latter includes both the subject  ( s q ) subscript s q (s_{q}) ( italic_s start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT )  and the relation  ( r q ) subscript r q (r_{q}) ( italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) . We examine these effects across various benchmark KG datasets, analyzing the influence of different retrieved facts on the frameworks performance. We find that  single-subject entity  queries benefit from a broader range of historical facts, improving performance, while  subject entity-relation pair  queries yield a more targeted set of facts, potentially enhancing outcomes. Our findings, as indicated in Table  14 , demonstrate that the performance of the  sLA-tKGF W/GPT-4  framework varies depending on the dataset. The WIKI and ICEWS18 benchmarks exhibit improvements with entity-focused facts, whereas ICEWS14 performs better with pair-focused facts. Our study explores the impact of either  single-subject entity  or  subject-relation pair  facts, revealing that different datasets benefit from specific types of facts. This leads to more context-aware enhancements in the  sLA-tKGF W/GPT-4  framework.",
            "In the tKG forecasting task, unidirectional refers to when the subject entity ( s q subscript s q s_{q} italic_s start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) or subject-relation pair ( s q , r q subscript s q subscript r q s_{q},r_{q} italic_s start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) from historical facts matches their position in the query quadruple  ( s q , r q , ? , t q ) subscript s q subscript r q ? subscript t q (s_{q},r_{q},?,t_{q}) ( italic_s start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , ? , italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) . Bidirectional denotes cases where they can appear in any position. For bidirectional modeling, historical facts are transformed by swapping entities and inverting the relation, e.g.,  ( s , r , o , t ) s r o t (s,r,o,t) ( italic_s , italic_r , italic_o , italic_t )  becomes  ( o , r  1 , s , t ) o superscript r 1 s t (o,r^{-1},s,t) ( italic_o , italic_r start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT , italic_s , italic_t ) . We use PLLMs such as GPT-4 to obtain reciprocal relations, enhancing tKG forecasting by incorporating diverse historical contexts. For instance,  (Barack Obama, visit, India, 2015-01-25)  becomes its reciprocal  (India, visited by, Barack Obama, 2015-01-25) . Our study evaluates the directionalitys impact on the tKG forecasting, finding that bidirectional modeling slightly improves performance, notably on ICEWS datasets. The experimental results highlight the value of appropriate relation modeling for  sLA-tKGF W/GPT-4  in understanding context, offering modest performance boosts in tKG forecasting. Table  15  shows the experimental results.",
            "The  sLA-tKGF  framework leverages a blend of historical tKG data, current web-scraped information, and contextually relevant descriptions of past entity relationships generated by pre-trained language models (PLLMs) to construct knowledge-augmented prompts. These prompts query the small-scale language model to estimate forecasts. Designed to enhance both reliability and accountability, the framework offers a significant advancement over traditional forecasting methods. In this study, we examine the power law relationship between PLLM model size and performance on tKG forecasting using our  sLA-tKGF  framework. We explore the effect of PLLM model size on  sLA-tKGF  framework performance in zero-shot tKG forecasting through experiments with various PLLMs. Table  16  lists the language models used, including GPT-2  (Radford et al . ,  2019 ) , GPT-J  (Wang,  2021 ) , and GPT-NeoX  (Black et al . ,  2022 ) , all employing the GPT-2 BPE tokenizer  (Radford et al . ,  2019 )  with similar vocabulary sizes. Our findings, detailed in Table  18 , confirm that larger models yield better results, supporting the scaling laws in zero-shot learning. These models demonstrate enhanced linguistic comprehension, more complex architectures, and enhanced generalization capabilities.",
            "Our  sLA-tKGF-GPT-4  framework improves tKG forecasting accuracy by integrating PLLMs with small-scale language models. Hyperparameter tuning is challenging due to its complex dimensionality, computational demands, and dataset-specific requirements. We opt for random search over grid search or Bayesian optimization for efficient hyperparameter exploration, seeking the optimal configuration on benchmark datasets. The small-scale language models are trained on knowledge-augmented prompting for tKG forecasting, aiming to predict missing entities by minimizing cross-entropy loss. Hyperparameter optimization for  sLA-tKGF-GPT-4  focuses on batch size ( b  16 , 48 b 16 48 b\\in{16,48} italic_b  16 , 48 ), epochs ( e p  10 , 30 subscript e p 10 30 e_{p}\\in{10,30} italic_e start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT  10 , 30 ), and embedding dimension ( d  64 , 128 d 64 128 d\\in{64,128} italic_d  64 , 128 ). Table  18  shows tuning results on benchmark datasets, highlighting that the combination ( b = 48 , e p = 30 , d = 128 formulae-sequence b 48 formulae-sequence subscript e p 30 d 128 b=48,e_{p}=30,d=128 italic_b = 48 , italic_e start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = 30 , italic_d = 128 ) is most effective for tKG forecasting tasks.",
            "Top-P and temperature serve as hyperparameters in pre-trained large language models (PLLMs), such as GPT-4 and Google Gemini. These hyperparameters influence the predictability and variety of model outputs. The temperature parameter primarily affects output predictability. Lower temperature values result in more deterministic responses, while higher values enable greater variability. A temperature value of 1.0 maintains the original output probabilities. On the other hand, Top-P (nucleus) sampling dynamically chooses tokens based on their cumulative probability. This method strikes a balance between text generation diversity and coherence by adjusting the threshold. By fine-tuning these parameters, it is possible to strike an optimal balance between randomness and determinism in generated outputs. Regarding the  sLA-tKGF-GPT-4  variant, hyperparameter optimization studies indicate that Top-P ranges over  [ 0 , 1 ] 0 1 [0,1] [ 0 , 1 ] , and temperature falls within the range of  [ 0 , 3 ] 0 3 [0,3] [ 0 , 3 ] . As shown in Table  19 , top performance has been consistently observed when using the configuration  ( Top-P = 1 , Temp = 0 ) formulae-sequence Top-P 1 Temp 0 (\\text{Top-P}=1,\\text{Temp}=0) ( Top-P = 1 , Temp = 0 )  across multiple benchmark datasets, highlighting its broad applicability and efficacy."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.  The table shows the performance comparison of the proposed framework variants with various off-the-shelf PLLMs and baselines using the  Hits@K  metric for single-step and multi-step tKG forecasting tasks. The model demonstrating the best performance for each dataset is highlighted in  bold .",
        "table": "S4.T2.1.1",
        "footnotes": [],
        "references": [
            "By employing a Tabula Rasa approachstarting from a clean slatethe framework addresses the critical challenges of data leakage in predicting future events in tKGs.  Our orchestrated framework workflow utilizes LlamaIndex (Liu,  2022 )  for the development of PLLMs-powered applications. We utilize DuckDuckGos search engine for searching the web. We access PLLMs using a text-based high-level API through Language Model as a Service (LaMaaS,  (Sun et al . ,  2022 ) ). The framework is trained on tKG forecasting, aiming to minimize cross-entropy loss and predict missing entities in future events. Framework hyperparameters include a batch size ( b b b italic_b ) set at 48, the number of epochs ( e p subscript e p e_{p} italic_e start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) at 30, and the hidden/embedding dimension ( d d d italic_d ) at 128. The Adam optimizer ( (Kingma and Ba,  2014 ) ) is employed with an initial learning rate of  1  e  3 1 superscript e 3 1e^{-3} 1 italic_e start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT . A learning rate decay scheduler halves the learning rate if the validation loss doesnt improve over 5 epochs, and early stopping is implemented to prevent overfitting.  Four representative off-the-shelf PLLMs used are GPT-4, GPT-3.5-turbo, GPT-3.0-text-davinci-003, and Google Bard. Framework hyperparameters remain consistent across PLLMs and benchmark datasets, demonstrating versatility. The context window ( m m m italic_m ) is set to 25 for knowledge injection of historical events from evolving tKG into the prompt, and the quadruple retrieval hop is set to one. No limit is imposed on the number of facts retrieved for knowledge injection. Utilizing 8 V100 GPUs, each with 8 GB of GPU memory, ensures efficient training. Due to high computational costs, experiments are run three times, and averaged results from independent experimental runs are reported in Tables  2 ,  3 , and  4  for robust comparisons. Refer appendix for hyperparameter tuning results.",
            "Our study compared the proposed framework with various supervised learning methods for tKG forecasting. Table  2  demonstrates that  sLA-tKGF W/GPT-4 , outperformed baseline algorithms. We report baseline results from prior research  (Lee et al . ,  2023 ; Gastinger et al . ,  2022 )  for fair and consistent comparison. Tables  3  and  4  provide further comparisons using several popular baselines, with results reported from an earlier study (Han et al . ,  2021b ) . Our experimental results confirm the effectiveness of  sLA-tKGF  framework in constructing knowledge-augmented prompts, involving: (1) retrieving historical knowledge from tKGs, (2) incorporating web search results for current information, and (3) using pre-trained large language models for summarizing historical entity-relationships, to query the small-scale language model and generate accurate and interpretable forecasts.",
            "Many state-of-the-art techniques struggle with tKGs featuring irregular time intervals, unlike the  sLA-tKGF  framework, which effectively addresses this issue by leveraging knowledge-augmented prompting for small-scale language models in temporal KG forecasting. The framework excels in handling real-world complexities and data sparsity, capturing complex dynamics and causal relationships more accurately, thus offering a versatile and reliable solution for temporal KG forecasting. Experimental evaluations on the ICEWS05-15 _ _ \\_ _ continuous dataset, a subset created by sampling from the original ICEWS05-15 dataset to simulate non-periodic observations in continuous time with 1-4 units interval, support our claim. The  sLA-tKGF  framework, trained on this benchmark and assessed using the Mean Reciprocal Rank (MRR) metric, demonstrates strong performance, especially with the  sLA-tKGF W/GPT-4  configuration, on temporal KGs with irregular intervals. The dataset statistics for ICEWS05-15 _  continuous _ continuous \\_\\text{continuous} _ continuous  are presented in Table  12 . We trained the  sLA-tKGF  framework with various off-the-shelf Pretrained Large Language Models (PLLMs) on this new benchmark dataset and evaluated their performance using the Mean Reciprocal Rank (MRR) metric. As demonstrated in Table  13 , our results validate that the  sLA-tKGF W/GPT-4  framework, exhibits strong performance on tKG forecasting task with irregular time intervals."
        ]
    },
    "id_table_3": {
        "caption": "Table 3.  The table shows the tKG forecasting results on three benchmark datasets on multi-step tKG forecasting task. The evaluation metrics include MRR ( % percent \\% % ) and Hits@1/3/10 ( % percent \\% % ). The best model for each dataset is highlighted in  bold .",
        "table": "S4.T2.2.1.1",
        "footnotes": [],
        "references": [
            "By employing a Tabula Rasa approachstarting from a clean slatethe framework addresses the critical challenges of data leakage in predicting future events in tKGs.  Our orchestrated framework workflow utilizes LlamaIndex (Liu,  2022 )  for the development of PLLMs-powered applications. We utilize DuckDuckGos search engine for searching the web. We access PLLMs using a text-based high-level API through Language Model as a Service (LaMaaS,  (Sun et al . ,  2022 ) ). The framework is trained on tKG forecasting, aiming to minimize cross-entropy loss and predict missing entities in future events. Framework hyperparameters include a batch size ( b b b italic_b ) set at 48, the number of epochs ( e p subscript e p e_{p} italic_e start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) at 30, and the hidden/embedding dimension ( d d d italic_d ) at 128. The Adam optimizer ( (Kingma and Ba,  2014 ) ) is employed with an initial learning rate of  1  e  3 1 superscript e 3 1e^{-3} 1 italic_e start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT . A learning rate decay scheduler halves the learning rate if the validation loss doesnt improve over 5 epochs, and early stopping is implemented to prevent overfitting.  Four representative off-the-shelf PLLMs used are GPT-4, GPT-3.5-turbo, GPT-3.0-text-davinci-003, and Google Bard. Framework hyperparameters remain consistent across PLLMs and benchmark datasets, demonstrating versatility. The context window ( m m m italic_m ) is set to 25 for knowledge injection of historical events from evolving tKG into the prompt, and the quadruple retrieval hop is set to one. No limit is imposed on the number of facts retrieved for knowledge injection. Utilizing 8 V100 GPUs, each with 8 GB of GPU memory, ensures efficient training. Due to high computational costs, experiments are run three times, and averaged results from independent experimental runs are reported in Tables  2 ,  3 , and  4  for robust comparisons. Refer appendix for hyperparameter tuning results.",
            "Our study compared the proposed framework with various supervised learning methods for tKG forecasting. Table  2  demonstrates that  sLA-tKGF W/GPT-4 , outperformed baseline algorithms. We report baseline results from prior research  (Lee et al . ,  2023 ; Gastinger et al . ,  2022 )  for fair and consistent comparison. Tables  3  and  4  provide further comparisons using several popular baselines, with results reported from an earlier study (Han et al . ,  2021b ) . Our experimental results confirm the effectiveness of  sLA-tKGF  framework in constructing knowledge-augmented prompts, involving: (1) retrieving historical knowledge from tKGs, (2) incorporating web search results for current information, and (3) using pre-trained large language models for summarizing historical entity-relationships, to query the small-scale language model and generate accurate and interpretable forecasts.",
            "Many state-of-the-art techniques struggle with tKGs featuring irregular time intervals, unlike the  sLA-tKGF  framework, which effectively addresses this issue by leveraging knowledge-augmented prompting for small-scale language models in temporal KG forecasting. The framework excels in handling real-world complexities and data sparsity, capturing complex dynamics and causal relationships more accurately, thus offering a versatile and reliable solution for temporal KG forecasting. Experimental evaluations on the ICEWS05-15 _ _ \\_ _ continuous dataset, a subset created by sampling from the original ICEWS05-15 dataset to simulate non-periodic observations in continuous time with 1-4 units interval, support our claim. The  sLA-tKGF  framework, trained on this benchmark and assessed using the Mean Reciprocal Rank (MRR) metric, demonstrates strong performance, especially with the  sLA-tKGF W/GPT-4  configuration, on temporal KGs with irregular intervals. The dataset statistics for ICEWS05-15 _  continuous _ continuous \\_\\text{continuous} _ continuous  are presented in Table  12 . We trained the  sLA-tKGF  framework with various off-the-shelf Pretrained Large Language Models (PLLMs) on this new benchmark dataset and evaluated their performance using the Mean Reciprocal Rank (MRR) metric. As demonstrated in Table  13 , our results validate that the  sLA-tKGF W/GPT-4  framework, exhibits strong performance on tKG forecasting task with irregular time intervals."
        ]
    },
    "id_table_4": {
        "caption": "Table 4.  The table presents tKG forecasting results on two benchmark datasets on multi-step tKG forecasting task. The evaluation metrics are MRR ( % percent \\% % ) and Hits@1/3/10 ( % percent \\% % ). The best model for each dataset is highlighted in  bold .",
        "table": "S4.T3.5.1",
        "footnotes": [],
        "references": [
            "By employing a Tabula Rasa approachstarting from a clean slatethe framework addresses the critical challenges of data leakage in predicting future events in tKGs.  Our orchestrated framework workflow utilizes LlamaIndex (Liu,  2022 )  for the development of PLLMs-powered applications. We utilize DuckDuckGos search engine for searching the web. We access PLLMs using a text-based high-level API through Language Model as a Service (LaMaaS,  (Sun et al . ,  2022 ) ). The framework is trained on tKG forecasting, aiming to minimize cross-entropy loss and predict missing entities in future events. Framework hyperparameters include a batch size ( b b b italic_b ) set at 48, the number of epochs ( e p subscript e p e_{p} italic_e start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) at 30, and the hidden/embedding dimension ( d d d italic_d ) at 128. The Adam optimizer ( (Kingma and Ba,  2014 ) ) is employed with an initial learning rate of  1  e  3 1 superscript e 3 1e^{-3} 1 italic_e start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT . A learning rate decay scheduler halves the learning rate if the validation loss doesnt improve over 5 epochs, and early stopping is implemented to prevent overfitting.  Four representative off-the-shelf PLLMs used are GPT-4, GPT-3.5-turbo, GPT-3.0-text-davinci-003, and Google Bard. Framework hyperparameters remain consistent across PLLMs and benchmark datasets, demonstrating versatility. The context window ( m m m italic_m ) is set to 25 for knowledge injection of historical events from evolving tKG into the prompt, and the quadruple retrieval hop is set to one. No limit is imposed on the number of facts retrieved for knowledge injection. Utilizing 8 V100 GPUs, each with 8 GB of GPU memory, ensures efficient training. Due to high computational costs, experiments are run three times, and averaged results from independent experimental runs are reported in Tables  2 ,  3 , and  4  for robust comparisons. Refer appendix for hyperparameter tuning results.",
            "Our study compared the proposed framework with various supervised learning methods for tKG forecasting. Table  2  demonstrates that  sLA-tKGF W/GPT-4 , outperformed baseline algorithms. We report baseline results from prior research  (Lee et al . ,  2023 ; Gastinger et al . ,  2022 )  for fair and consistent comparison. Tables  3  and  4  provide further comparisons using several popular baselines, with results reported from an earlier study (Han et al . ,  2021b ) . Our experimental results confirm the effectiveness of  sLA-tKGF  framework in constructing knowledge-augmented prompts, involving: (1) retrieving historical knowledge from tKGs, (2) incorporating web search results for current information, and (3) using pre-trained large language models for summarizing historical entity-relationships, to query the small-scale language model and generate accurate and interpretable forecasts.",
            "In this work, we introduce a zero-shot learning method to predict missing entities in query quadruples. Our approach includes: (a) Constructing a historical context for a query quadruple ( s q , p q , ? , t q subscript s q subscript p q ? subscript t q s_{q},p_{q},?,t_{q} italic_s start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , ? , italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) using historical facts from previous static KG snapshots  G t q  m : t q  1 subscript G : subscript t q m subscript t q 1 \\mathcal{G}_{t_{q}-m:t_{q}-1} caligraphic_G start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT - italic_m : italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT - 1 end_POSTSUBSCRIPT  to form a knowledge-infused augmented prompt. (b) Converting retrieved facts and the query into verbalized sentences, employing sentence embedding for knowledge distillation, and using semantic similarity to filter facts, thereby constructing augmented prompts. (c) Estimating the missing entity conditioned on the augmented prompt, following  o q  P  ( o | ( s q , r q , ? , t q ) , G ( t q  m , t q  1 ) ) similar-to subscript o q P conditional o subscript s q subscript r q ? subscript t q subscript G subscript t q m subscript t q 1 o_{q}\\sim P(o\\hskip 1.42262pt|\\hskip 1.42262pt(s_{q},r_{q},?,t_{q}),\\mathcal{G%  }_{(t_{q}-m,t_{q}-1)}) italic_o start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT  italic_P ( italic_o | ( italic_s start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , ? , italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) , caligraphic_G start_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT - italic_m , italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT - 1 ) end_POSTSUBSCRIPT ) . We evaluate the impact of  single-subject entity  and  subject entity-relation pair  historical facts on forecasting accuracy. The former involves only the subject entity  ( s q ) subscript s q (s_{q}) ( italic_s start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) , while the latter includes both the subject  ( s q ) subscript s q (s_{q}) ( italic_s start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT )  and the relation  ( r q ) subscript r q (r_{q}) ( italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) . We examine these effects across various benchmark KG datasets, analyzing the influence of different retrieved facts on the frameworks performance. We find that  single-subject entity  queries benefit from a broader range of historical facts, improving performance, while  subject entity-relation pair  queries yield a more targeted set of facts, potentially enhancing outcomes. Our findings, as indicated in Table  14 , demonstrate that the performance of the  sLA-tKGF W/GPT-4  framework varies depending on the dataset. The WIKI and ICEWS18 benchmarks exhibit improvements with entity-focused facts, whereas ICEWS14 performs better with pair-focused facts. Our study explores the impact of either  single-subject entity  or  subject-relation pair  facts, revealing that different datasets benefit from specific types of facts. This leads to more context-aware enhancements in the  sLA-tKGF W/GPT-4  framework."
        ]
    },
    "id_table_5": {
        "caption": "Table 5.  The table presents the experimental results from the ablation study conducted on tKG forecasting using benchmark datasets. The results show that the original framework consistently outperformed its ablated variants across all datasets. The ablation study supports the hypothesis of constructing knowledge-augmented prompts to improve the performance of the framework for optimal tkG forecasting. Nevertheless, all the ablated variants demonstrated a performance drop compared to the original framework, emphasizing the importance of the components that were disabled.",
        "table": "S4.T4.5.1",
        "footnotes": [],
        "references": [
            "Table  5  shows the ablation study results. The ablated variants performance declines compared to the original framework (i.e., baseline for ablation study) and signifies the importance of the individual components. The study supports our approach of constructing knowledge-augmented prompts for querying the small-scale language model for improved forecasts.",
            "In the tKG forecasting task, unidirectional refers to when the subject entity ( s q subscript s q s_{q} italic_s start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) or subject-relation pair ( s q , r q subscript s q subscript r q s_{q},r_{q} italic_s start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) from historical facts matches their position in the query quadruple  ( s q , r q , ? , t q ) subscript s q subscript r q ? subscript t q (s_{q},r_{q},?,t_{q}) ( italic_s start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , ? , italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) . Bidirectional denotes cases where they can appear in any position. For bidirectional modeling, historical facts are transformed by swapping entities and inverting the relation, e.g.,  ( s , r , o , t ) s r o t (s,r,o,t) ( italic_s , italic_r , italic_o , italic_t )  becomes  ( o , r  1 , s , t ) o superscript r 1 s t (o,r^{-1},s,t) ( italic_o , italic_r start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT , italic_s , italic_t ) . We use PLLMs such as GPT-4 to obtain reciprocal relations, enhancing tKG forecasting by incorporating diverse historical contexts. For instance,  (Barack Obama, visit, India, 2015-01-25)  becomes its reciprocal  (India, visited by, Barack Obama, 2015-01-25) . Our study evaluates the directionalitys impact on the tKG forecasting, finding that bidirectional modeling slightly improves performance, notably on ICEWS datasets. The experimental results highlight the value of appropriate relation modeling for  sLA-tKGF W/GPT-4  in understanding context, offering modest performance boosts in tKG forecasting. Table  15  shows the experimental results."
        ]
    },
    "id_table_6": {
        "caption": "Table 6.  The table presents the results for the long-term tKG forecasting task on the ICEWS05-15 dataset across different forecast horizon(   T  T \\Delta T roman_ italic_T ) in terms of the Mean Reciprocal Rank(MRR) metric.",
        "table": "S4.T5.1.1",
        "footnotes": [],
        "references": [
            "Existing tKG forecasting research often overlooks insights from edge dissolution and formation. Our work considers the creation and dissolution of relationships (edges) between entities in tKGs as critical evolutionary factors for accurate tKG forecasting by capturing the essence of how relationships evolve and change over time. Nevertheless, the high dimensionality and dynamic complexity of tKGs pose challenges for both short and long-term event forecasting. Our approach uses historical context within an evolving tKG to improve both short-term and long-term forecasting accuracy. Short-term forecasts predict near-future changes, capturing the immediate evolution of events, while long-term forecasts anticipate broader trends that unfold over an extended period. Standard tKG forecasting relies on static KG snapshots, denoted as  G ( t  t m , t ) subscript G t subscript t m t \\mathcal{G}_{(t-t_{m},t)} caligraphic_G start_POSTSUBSCRIPT ( italic_t - italic_t start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT , italic_t ) end_POSTSUBSCRIPT , from a historical window  m m m italic_m  up until time  t t t italic_t , to predict events at time  t +   t t  t t+\\Delta t italic_t + roman_ italic_t  (e.g., one day). In real-world scenarios, the absence of graph information motivates the evaluation of forecasting techniques for making predictions about the distant future (   T    t  T  t \\Delta T\\geq\\Delta t roman_ italic_T  roman_ italic_t ). In the context of long-term tKG forecasting, the objective is to predict missing entities at a future time  t +   T t  T t+\\Delta T italic_t + roman_ italic_T , beginning with an initial forecast period from  t  t m t subscript t m t-t_{m} italic_t - italic_t start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT  to  t +   t t  t t+\\Delta t italic_t + roman_ italic_t . This initial forecast targets the short-term future, represented by    t  t \\Delta t roman_ italic_t . The forecasting process then extends into the final forecast period, from  t +   t t  t t+\\Delta t italic_t + roman_ italic_t  to  t +   T t  T t+\\Delta T italic_t + roman_ italic_T , to focus on long-term predictions, where    T  T \\Delta T roman_ italic_T  signifies the extended future. Experimental results on the ICEWS05-15 dataset, using  sLA-tKGF W/GPT-4 , for different    T  T \\Delta T roman_ italic_T  values are shown in Table  6 . As    T  T \\Delta T roman_ italic_T  increases (1 day to 8 days), performance declines in both single-step and multi-step approaches, indicating increased difficulty in predicting further into the future. This decline in performance is attributed from the assumption that dynamics at  t +   t t  t t+\\Delta t italic_t + roman_ italic_t  apply at  t +   T t  T t+\\Delta T italic_t + roman_ italic_T . With larger    T  T \\Delta T roman_ italic_T , this assumption falters due to data distributional shifts and the performance drops because of the lack of recent relevant historical facts in the augmented prompt for accurate predictions at  t +   T t  T t+\\Delta T italic_t + roman_ italic_T .",
            "The  sLA-tKGF  framework leverages a blend of historical tKG data, current web-scraped information, and contextually relevant descriptions of past entity relationships generated by pre-trained language models (PLLMs) to construct knowledge-augmented prompts. These prompts query the small-scale language model to estimate forecasts. Designed to enhance both reliability and accountability, the framework offers a significant advancement over traditional forecasting methods. In this study, we examine the power law relationship between PLLM model size and performance on tKG forecasting using our  sLA-tKGF  framework. We explore the effect of PLLM model size on  sLA-tKGF  framework performance in zero-shot tKG forecasting through experiments with various PLLMs. Table  16  lists the language models used, including GPT-2  (Radford et al . ,  2019 ) , GPT-J  (Wang,  2021 ) , and GPT-NeoX  (Black et al . ,  2022 ) , all employing the GPT-2 BPE tokenizer  (Radford et al . ,  2019 )  with similar vocabulary sizes. Our findings, detailed in Table  18 , confirm that larger models yield better results, supporting the scaling laws in zero-shot learning. These models demonstrate enhanced linguistic comprehension, more complex architectures, and enhanced generalization capabilities."
        ]
    },
    "id_table_7": {
        "caption": "Table 7.  The table presents the results for inductive future link prediction task on ICEWS05-15 dataset in terms of Mean Reciprocal Rank(MRR) metric.",
        "table": "S4.T5.2.1.1",
        "footnotes": [],
        "references": [
            "Since the framework hasnt encountered  new economic policy  during training, this serves as an instance of an unobserved entity. We call this evaluation, inductive link prediction analysis. We conducted future link prediction experiments on sets of inductive link prediction quadruples, and the results are shown in Table  7 . We evaluated our framework against robust baselines, using the ICEWS05-15 dataset. Despite not being explicitly designed for knowledge graph tasks like inductive link prediction, our proposed framework ( sLA-tKGF W/GPT-4 ) surprisingly outperforms all baselines across various metrics, as demonstrated in Table  7 . This success stems from our frameworks ability to leverage both the web search results and historical entity-relationships based descriptions generated by PLLMs using implicit pre-training knowledge. In essence, when applied to the test dataset, the framework utilizes patterns and information from its training data, augmented by knowledge-rich prompts at inference time, to successfully predict relationships between unseen entities within an evolving tKG."
        ]
    },
    "id_table_8": {
        "caption": "Table 8.  The table shows the performance of the  sLA-tKGF W/GPT-4  framework on tKG forecasting declines without timestamped (TS) facts. This suggests that  sLA-tKGF W/GPT-4  can leverage temporal information in historical facts to improve its performance on tKG forecasting tasks.",
        "table": "S4.T6.5.5.5",
        "footnotes": [],
        "references": [
            "The proposed framework,  sLA-tKGF , introduces a novel two-pronged approach to predict future events in tKGs by combining a clean-slate trained, small-scale language model with the Retrieval-Augmented Generation (RAG) technique. It emphasizes accuracy and bias-free predictions by leveraging historical tKG data, current web information, and contextually relevant historical entity relationship descriptions generated by PLLMs. We examined the performance of the  sLA-tKGF  framework by understanding temporal information in retrieved historical events from tKGs, comparing its performance in the tKG forecasting task with and without explicit timestamps in the knowledge-augmented prompt. We also investigated the impact of shuffling historical facts without time information on the performance of the  sLA-tKGF  framework. The results, shown in Table  8 , reveal that the  sLA-tKGF  frameworks performance worsens without temporal information. This decline is exacerbated by the random arrangement of events, as demonstrated in Table  9 . These outcomes underscore the frameworks dependence on chronological sequencing for accurate predictions, emphasizing the critical role of temporal information and sequence in the accuracy of the  sLA-tKGF W/GPT-4  framework for tKG forecasting. The consistent significance of temporal and sequence information across datasets (YAGO, WIKI, ICEWS14, ICEWS18, ACLED-CD22) reinforces the reliability and applicability of these findings.",
            "The  sLA-tKGF  framework leverages a blend of historical tKG data, current web-scraped information, and contextually relevant descriptions of past entity relationships generated by pre-trained language models (PLLMs) to construct knowledge-augmented prompts. These prompts query the small-scale language model to estimate forecasts. Designed to enhance both reliability and accountability, the framework offers a significant advancement over traditional forecasting methods. In this study, we examine the power law relationship between PLLM model size and performance on tKG forecasting using our  sLA-tKGF  framework. We explore the effect of PLLM model size on  sLA-tKGF  framework performance in zero-shot tKG forecasting through experiments with various PLLMs. Table  16  lists the language models used, including GPT-2  (Radford et al . ,  2019 ) , GPT-J  (Wang,  2021 ) , and GPT-NeoX  (Black et al . ,  2022 ) , all employing the GPT-2 BPE tokenizer  (Radford et al . ,  2019 )  with similar vocabulary sizes. Our findings, detailed in Table  18 , confirm that larger models yield better results, supporting the scaling laws in zero-shot learning. These models demonstrate enhanced linguistic comprehension, more complex architectures, and enhanced generalization capabilities.",
            "Our  sLA-tKGF-GPT-4  framework improves tKG forecasting accuracy by integrating PLLMs with small-scale language models. Hyperparameter tuning is challenging due to its complex dimensionality, computational demands, and dataset-specific requirements. We opt for random search over grid search or Bayesian optimization for efficient hyperparameter exploration, seeking the optimal configuration on benchmark datasets. The small-scale language models are trained on knowledge-augmented prompting for tKG forecasting, aiming to predict missing entities by minimizing cross-entropy loss. Hyperparameter optimization for  sLA-tKGF-GPT-4  focuses on batch size ( b  16 , 48 b 16 48 b\\in{16,48} italic_b  16 , 48 ), epochs ( e p  10 , 30 subscript e p 10 30 e_{p}\\in{10,30} italic_e start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT  10 , 30 ), and embedding dimension ( d  64 , 128 d 64 128 d\\in{64,128} italic_d  64 , 128 ). Table  18  shows tuning results on benchmark datasets, highlighting that the combination ( b = 48 , e p = 30 , d = 128 formulae-sequence b 48 formulae-sequence subscript e p 30 d 128 b=48,e_{p}=30,d=128 italic_b = 48 , italic_e start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = 30 , italic_d = 128 ) is most effective for tKG forecasting tasks."
        ]
    },
    "id_table_9": {
        "caption": "Table 9.  The table shows performance of  sLA-tKGF W/GPT-4  framework on tKG forecasting declines when the timestamp removed facts in the augmented prompt are randomly shuffled(RS). This suggests that  sLA-tKGF W/GPT-4  can leverage the chronological order of events to improve performance on tKG forecasting tasks.",
        "table": "S4.T6.10.5.5",
        "footnotes": [],
        "references": [
            "The proposed framework,  sLA-tKGF , introduces a novel two-pronged approach to predict future events in tKGs by combining a clean-slate trained, small-scale language model with the Retrieval-Augmented Generation (RAG) technique. It emphasizes accuracy and bias-free predictions by leveraging historical tKG data, current web information, and contextually relevant historical entity relationship descriptions generated by PLLMs. We examined the performance of the  sLA-tKGF  framework by understanding temporal information in retrieved historical events from tKGs, comparing its performance in the tKG forecasting task with and without explicit timestamps in the knowledge-augmented prompt. We also investigated the impact of shuffling historical facts without time information on the performance of the  sLA-tKGF  framework. The results, shown in Table  8 , reveal that the  sLA-tKGF  frameworks performance worsens without temporal information. This decline is exacerbated by the random arrangement of events, as demonstrated in Table  9 . These outcomes underscore the frameworks dependence on chronological sequencing for accurate predictions, emphasizing the critical role of temporal information and sequence in the accuracy of the  sLA-tKGF W/GPT-4  framework for tKG forecasting. The consistent significance of temporal and sequence information across datasets (YAGO, WIKI, ICEWS14, ICEWS18, ACLED-CD22) reinforces the reliability and applicability of these findings.",
            "Top-P and temperature serve as hyperparameters in pre-trained large language models (PLLMs), such as GPT-4 and Google Gemini. These hyperparameters influence the predictability and variety of model outputs. The temperature parameter primarily affects output predictability. Lower temperature values result in more deterministic responses, while higher values enable greater variability. A temperature value of 1.0 maintains the original output probabilities. On the other hand, Top-P (nucleus) sampling dynamically chooses tokens based on their cumulative probability. This method strikes a balance between text generation diversity and coherence by adjusting the threshold. By fine-tuning these parameters, it is possible to strike an optimal balance between randomness and determinism in generated outputs. Regarding the  sLA-tKGF-GPT-4  variant, hyperparameter optimization studies indicate that Top-P ranges over  [ 0 , 1 ] 0 1 [0,1] [ 0 , 1 ] , and temperature falls within the range of  [ 0 , 3 ] 0 3 [0,3] [ 0 , 3 ] . As shown in Table  19 , top performance has been consistently observed when using the configuration  ( Top-P = 1 , Temp = 0 ) formulae-sequence Top-P 1 Temp 0 (\\text{Top-P}=1,\\text{Temp}=0) ( Top-P = 1 , Temp = 0 )  across multiple benchmark datasets, highlighting its broad applicability and efficacy."
        ]
    },
    "id_table_10": {
        "caption": "Table 10.  The table displays the results of the study investigating the impact of various types of knowledge-infused augmented prompts on the performance of the  sLA-tKGF W/GPT-4  framework in tKG forecasting, assessed using benchmark datasets.",
        "table": "S4.T7.1.1.1",
        "footnotes": [],
        "references": [
            "In this section, we evaluate the impact of augmenting natural language questions(verbalized queries) with external knowledge from historical events sampled from tKGs on the  sLA-tKGF W/GPT-4  frameworks performance in tKG forecasting tasks. We explore various knowledge retrieval strategies for constructing knowledge-augmented prompts, focusing on optimizing the frameworks performance in tKG forecasting. Table  10  shows how different strategies affect the  sLA-tKGF W/GPT-4  frameworks performance across datasets (YAGO, WIKI, ICEWS14, ICEWS18, ACLED-CD22) for Single-Step and Multi-Step forecasting. The results demonstrate that selectively incorporating relevant prior knowledge significantly enhances tKG forecasting performance, surpassing baselines that either omit additional knowledge or use it indiscriminately."
        ]
    },
    "id_table_11": {
        "caption": "Table 11.  The table illustrates the effect of using different historical context window sizes on tKG forecasting performance across several benchmark datasets. Increasing the historical context length consistently improved performance, indicating that more historical data enables the  sLA-tKGF  framework with GPT-4 integration to learn more effectively. The results highlighted in bold represent the performance of our proposed framework compared to existing methods.",
        "table": "S4.T7.2.1",
        "footnotes": [],
        "references": [
            "Our framework utilizes historical events or facts from time  ( t q  m ) subscript t q m (t_{q}-m) ( italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT - italic_m )  to  t q subscript t q t_{q} italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT  to predict missing entities in the query quadruple at  t q subscript t q t_{q} italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , where  m m m italic_m  represents the historical context window size, a configurable hyperparameter. We experimented with various lengths to evaluate the impact of historical context on the forecasting performance of the  sLA-tKGF W/GPT-4  framework for tKGs. According to Table  11 , leveraging a greater amount of past data to generate knowledge-augmented prompts for the small-scale language model enhances the accuracy of missing entity predictions, as evidenced by improved mean reciprocal rank (MRR) scores, albeit at the expense of higher computational demands. The small-scale language model within the  sLA-tKGF W/GPT-4  framework is constrained by the maximum input token sequence length. Although extending the historical window marginally increases MRR scores, it leads to the creation of longer and more complex prompts that are impractical for broad-scale application. To strike a balance between accuracy and computational efficiency, we selected a historical context window of 25 for our experiments. We varied the historical facts in the prompts to identify the optimal setup for generating accurate forecasts, with the goal of minimizing wall-clock time. Wall-clock time for  sLA-tKGF W/GPT-4  queriesthe time elapsed from query submission to responseis influenced by the size of the small-scale language model, the complexity of the query, and the available computational resources. While typically, small-scale language models return responses within seconds, more complex or extensive queries may require longer processing times."
        ]
    },
    "id_table_12": {
        "caption": "Table 12.  The table displays the statistics of the new dataset. #Training, #Validation, #Test represent the number of quadruples in the training set, validation set, and test set, respectively.  N obs subscript N obs N_{\\text{obs}} italic_N start_POSTSUBSCRIPT obs end_POSTSUBSCRIPT  represents the total number of snapshots in the new benchmark tKG forecasting dataset, where each snapshot captures the state of the tKG at a specific point in time.",
        "table": "S6.T8.1.1",
        "footnotes": [],
        "references": [
            "Many state-of-the-art techniques struggle with tKGs featuring irregular time intervals, unlike the  sLA-tKGF  framework, which effectively addresses this issue by leveraging knowledge-augmented prompting for small-scale language models in temporal KG forecasting. The framework excels in handling real-world complexities and data sparsity, capturing complex dynamics and causal relationships more accurately, thus offering a versatile and reliable solution for temporal KG forecasting. Experimental evaluations on the ICEWS05-15 _ _ \\_ _ continuous dataset, a subset created by sampling from the original ICEWS05-15 dataset to simulate non-periodic observations in continuous time with 1-4 units interval, support our claim. The  sLA-tKGF  framework, trained on this benchmark and assessed using the Mean Reciprocal Rank (MRR) metric, demonstrates strong performance, especially with the  sLA-tKGF W/GPT-4  configuration, on temporal KGs with irregular intervals. The dataset statistics for ICEWS05-15 _  continuous _ continuous \\_\\text{continuous} _ continuous  are presented in Table  12 . We trained the  sLA-tKGF  framework with various off-the-shelf Pretrained Large Language Models (PLLMs) on this new benchmark dataset and evaluated their performance using the Mean Reciprocal Rank (MRR) metric. As demonstrated in Table  13 , our results validate that the  sLA-tKGF W/GPT-4  framework, exhibits strong performance on tKG forecasting task with irregular time intervals."
        ]
    },
    "id_table_13": {
        "caption": "Table 13.  The table shows the tKG forecasting results on Irregular temporal KGs.",
        "table": "S6.T8.2.1",
        "footnotes": [],
        "references": [
            "Many state-of-the-art techniques struggle with tKGs featuring irregular time intervals, unlike the  sLA-tKGF  framework, which effectively addresses this issue by leveraging knowledge-augmented prompting for small-scale language models in temporal KG forecasting. The framework excels in handling real-world complexities and data sparsity, capturing complex dynamics and causal relationships more accurately, thus offering a versatile and reliable solution for temporal KG forecasting. Experimental evaluations on the ICEWS05-15 _ _ \\_ _ continuous dataset, a subset created by sampling from the original ICEWS05-15 dataset to simulate non-periodic observations in continuous time with 1-4 units interval, support our claim. The  sLA-tKGF  framework, trained on this benchmark and assessed using the Mean Reciprocal Rank (MRR) metric, demonstrates strong performance, especially with the  sLA-tKGF W/GPT-4  configuration, on temporal KGs with irregular intervals. The dataset statistics for ICEWS05-15 _  continuous _ continuous \\_\\text{continuous} _ continuous  are presented in Table  12 . We trained the  sLA-tKGF  framework with various off-the-shelf Pretrained Large Language Models (PLLMs) on this new benchmark dataset and evaluated their performance using the Mean Reciprocal Rank (MRR) metric. As demonstrated in Table  13 , our results validate that the  sLA-tKGF W/GPT-4  framework, exhibits strong performance on tKG forecasting task with irregular time intervals."
        ]
    },
    "id_table_14": {
        "caption": "Table 14.  The table presents experimental results from a study on the impact of retrieved fact types on tKG forecasting performance. Two types of historical facts, Single-Subject Entity and Subject Entity-Relation Pair, were evaluated for their impact on forecasting accuracy. Results indicated that the performance of the  sLA-tKGF W/GPT-4  framework varies based on the dataset and the type of historical fact used. In essence, the research highlights the importance of including both historical fact types in the augmented prompts for optimal performance in the forecasting task across various datasets.",
        "table": "S6.T9.1.1",
        "footnotes": [],
        "references": [
            "In this work, we introduce a zero-shot learning method to predict missing entities in query quadruples. Our approach includes: (a) Constructing a historical context for a query quadruple ( s q , p q , ? , t q subscript s q subscript p q ? subscript t q s_{q},p_{q},?,t_{q} italic_s start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , ? , italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) using historical facts from previous static KG snapshots  G t q  m : t q  1 subscript G : subscript t q m subscript t q 1 \\mathcal{G}_{t_{q}-m:t_{q}-1} caligraphic_G start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT - italic_m : italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT - 1 end_POSTSUBSCRIPT  to form a knowledge-infused augmented prompt. (b) Converting retrieved facts and the query into verbalized sentences, employing sentence embedding for knowledge distillation, and using semantic similarity to filter facts, thereby constructing augmented prompts. (c) Estimating the missing entity conditioned on the augmented prompt, following  o q  P  ( o | ( s q , r q , ? , t q ) , G ( t q  m , t q  1 ) ) similar-to subscript o q P conditional o subscript s q subscript r q ? subscript t q subscript G subscript t q m subscript t q 1 o_{q}\\sim P(o\\hskip 1.42262pt|\\hskip 1.42262pt(s_{q},r_{q},?,t_{q}),\\mathcal{G%  }_{(t_{q}-m,t_{q}-1)}) italic_o start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT  italic_P ( italic_o | ( italic_s start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , ? , italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) , caligraphic_G start_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT - italic_m , italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT - 1 ) end_POSTSUBSCRIPT ) . We evaluate the impact of  single-subject entity  and  subject entity-relation pair  historical facts on forecasting accuracy. The former involves only the subject entity  ( s q ) subscript s q (s_{q}) ( italic_s start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) , while the latter includes both the subject  ( s q ) subscript s q (s_{q}) ( italic_s start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT )  and the relation  ( r q ) subscript r q (r_{q}) ( italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) . We examine these effects across various benchmark KG datasets, analyzing the influence of different retrieved facts on the frameworks performance. We find that  single-subject entity  queries benefit from a broader range of historical facts, improving performance, while  subject entity-relation pair  queries yield a more targeted set of facts, potentially enhancing outcomes. Our findings, as indicated in Table  14 , demonstrate that the performance of the  sLA-tKGF W/GPT-4  framework varies depending on the dataset. The WIKI and ICEWS18 benchmarks exhibit improvements with entity-focused facts, whereas ICEWS14 performs better with pair-focused facts. Our study explores the impact of either  single-subject entity  or  subject-relation pair  facts, revealing that different datasets benefit from specific types of facts. This leads to more context-aware enhancements in the  sLA-tKGF W/GPT-4  framework."
        ]
    },
    "id_table_15": {
        "caption": "Table 15.  The table presents the experimental results on the study of the impact of directionality in historical modeling on tKG forecasting performance. In the context of tKG forecasting, unidirectional and bidirectional indicate whether entities in historical facts align with the positions of the query quadruple. Unidirectional maintains alignment, while bidirectional allows entities to shift after transforming facts with inverse relations. The aim is to determine if incorporating both original historical facts and their inverse relations in the bidirectional modeling context can enhance the performance of tKG forecasting by offering a more varied historical context. The adoption of bidirectional relation modeling for tKG forecasting shows marginal improvements, with particularly notable gains evident in the ICEWS benchmark datasets.",
        "table": "S6.T9.2.1",
        "footnotes": [],
        "references": [
            "In the tKG forecasting task, unidirectional refers to when the subject entity ( s q subscript s q s_{q} italic_s start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) or subject-relation pair ( s q , r q subscript s q subscript r q s_{q},r_{q} italic_s start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) from historical facts matches their position in the query quadruple  ( s q , r q , ? , t q ) subscript s q subscript r q ? subscript t q (s_{q},r_{q},?,t_{q}) ( italic_s start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , ? , italic_t start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) . Bidirectional denotes cases where they can appear in any position. For bidirectional modeling, historical facts are transformed by swapping entities and inverting the relation, e.g.,  ( s , r , o , t ) s r o t (s,r,o,t) ( italic_s , italic_r , italic_o , italic_t )  becomes  ( o , r  1 , s , t ) o superscript r 1 s t (o,r^{-1},s,t) ( italic_o , italic_r start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT , italic_s , italic_t ) . We use PLLMs such as GPT-4 to obtain reciprocal relations, enhancing tKG forecasting by incorporating diverse historical contexts. For instance,  (Barack Obama, visit, India, 2015-01-25)  becomes its reciprocal  (India, visited by, Barack Obama, 2015-01-25) . Our study evaluates the directionalitys impact on the tKG forecasting, finding that bidirectional modeling slightly improves performance, notably on ICEWS datasets. The experimental results highlight the value of appropriate relation modeling for  sLA-tKGF W/GPT-4  in understanding context, offering modest performance boosts in tKG forecasting. Table  15  shows the experimental results."
        ]
    },
    "id_table_16": {
        "caption": "Table 16.  Overview of different GPT-based models by family and parameter count.",
        "table": "S6.T10.1.1.1",
        "footnotes": [],
        "references": [
            "The  sLA-tKGF  framework leverages a blend of historical tKG data, current web-scraped information, and contextually relevant descriptions of past entity relationships generated by pre-trained language models (PLLMs) to construct knowledge-augmented prompts. These prompts query the small-scale language model to estimate forecasts. Designed to enhance both reliability and accountability, the framework offers a significant advancement over traditional forecasting methods. In this study, we examine the power law relationship between PLLM model size and performance on tKG forecasting using our  sLA-tKGF  framework. We explore the effect of PLLM model size on  sLA-tKGF  framework performance in zero-shot tKG forecasting through experiments with various PLLMs. Table  16  lists the language models used, including GPT-2  (Radford et al . ,  2019 ) , GPT-J  (Wang,  2021 ) , and GPT-NeoX  (Black et al . ,  2022 ) , all employing the GPT-2 BPE tokenizer  (Radford et al . ,  2019 )  with similar vocabulary sizes. Our findings, detailed in Table  18 , confirm that larger models yield better results, supporting the scaling laws in zero-shot learning. These models demonstrate enhanced linguistic comprehension, more complex architectures, and enhanced generalization capabilities."
        ]
    },
    "id_table_17": {
        "caption": "Table 17.  The table presents the experimental results of the study on the impact of PLLM size on the tKG forecasting task.",
        "table": "S6.T10.2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_18": {
        "caption": "Table 18.  The table presents the hyperparameter tuning results conducted on the tKG forecasting task using benchmark datasets.",
        "table": "S6.T11.1.1",
        "footnotes": [],
        "references": [
            "The  sLA-tKGF  framework leverages a blend of historical tKG data, current web-scraped information, and contextually relevant descriptions of past entity relationships generated by pre-trained language models (PLLMs) to construct knowledge-augmented prompts. These prompts query the small-scale language model to estimate forecasts. Designed to enhance both reliability and accountability, the framework offers a significant advancement over traditional forecasting methods. In this study, we examine the power law relationship between PLLM model size and performance on tKG forecasting using our  sLA-tKGF  framework. We explore the effect of PLLM model size on  sLA-tKGF  framework performance in zero-shot tKG forecasting through experiments with various PLLMs. Table  16  lists the language models used, including GPT-2  (Radford et al . ,  2019 ) , GPT-J  (Wang,  2021 ) , and GPT-NeoX  (Black et al . ,  2022 ) , all employing the GPT-2 BPE tokenizer  (Radford et al . ,  2019 )  with similar vocabulary sizes. Our findings, detailed in Table  18 , confirm that larger models yield better results, supporting the scaling laws in zero-shot learning. These models demonstrate enhanced linguistic comprehension, more complex architectures, and enhanced generalization capabilities.",
            "Our  sLA-tKGF-GPT-4  framework improves tKG forecasting accuracy by integrating PLLMs with small-scale language models. Hyperparameter tuning is challenging due to its complex dimensionality, computational demands, and dataset-specific requirements. We opt for random search over grid search or Bayesian optimization for efficient hyperparameter exploration, seeking the optimal configuration on benchmark datasets. The small-scale language models are trained on knowledge-augmented prompting for tKG forecasting, aiming to predict missing entities by minimizing cross-entropy loss. Hyperparameter optimization for  sLA-tKGF-GPT-4  focuses on batch size ( b  16 , 48 b 16 48 b\\in{16,48} italic_b  16 , 48 ), epochs ( e p  10 , 30 subscript e p 10 30 e_{p}\\in{10,30} italic_e start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT  10 , 30 ), and embedding dimension ( d  64 , 128 d 64 128 d\\in{64,128} italic_d  64 , 128 ). Table  18  shows tuning results on benchmark datasets, highlighting that the combination ( b = 48 , e p = 30 , d = 128 formulae-sequence b 48 formulae-sequence subscript e p 30 d 128 b=48,e_{p}=30,d=128 italic_b = 48 , italic_e start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = 30 , italic_d = 128 ) is most effective for tKG forecasting tasks."
        ]
    },
    "id_table_19": {
        "caption": "Table 19.  The table shows the experimental results of hyperparameter optimization for both Top-P and temperature conducted on the tKG forecasting task using benchmark datasets, specifically for the  sLA-tKGF-GPT-4  variant of our framework.",
        "table": "S6.T11.2.1",
        "footnotes": [],
        "references": [
            "Top-P and temperature serve as hyperparameters in pre-trained large language models (PLLMs), such as GPT-4 and Google Gemini. These hyperparameters influence the predictability and variety of model outputs. The temperature parameter primarily affects output predictability. Lower temperature values result in more deterministic responses, while higher values enable greater variability. A temperature value of 1.0 maintains the original output probabilities. On the other hand, Top-P (nucleus) sampling dynamically chooses tokens based on their cumulative probability. This method strikes a balance between text generation diversity and coherence by adjusting the threshold. By fine-tuning these parameters, it is possible to strike an optimal balance between randomness and determinism in generated outputs. Regarding the  sLA-tKGF-GPT-4  variant, hyperparameter optimization studies indicate that Top-P ranges over  [ 0 , 1 ] 0 1 [0,1] [ 0 , 1 ] , and temperature falls within the range of  [ 0 , 3 ] 0 3 [0,3] [ 0 , 3 ] . As shown in Table  19 , top performance has been consistently observed when using the configuration  ( Top-P = 1 , Temp = 0 ) formulae-sequence Top-P 1 Temp 0 (\\text{Top-P}=1,\\text{Temp}=0) ( Top-P = 1 , Temp = 0 )  across multiple benchmark datasets, highlighting its broad applicability and efficacy."
        ]
    },
    "id_table_20": {
        "caption": "",
        "table": "S6.T12.8.8",
        "footnotes": [],
        "references": []
    },
    "id_table_21": {
        "caption": "",
        "table": "S6.T13.1.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_22": {
        "caption": "",
        "table": "S6.T13.2.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_23": {
        "caption": "",
        "table": "S6.T14.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_24": {
        "caption": "",
        "table": "S6.T14.2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_25": {
        "caption": "",
        "table": "S6.T15.2.2.2",
        "footnotes": [],
        "references": []
    },
    "id_table_26": {
        "caption": "",
        "table": "S6.T15.4.2.2",
        "footnotes": [],
        "references": []
    },
    "id_table_27": {
        "caption": "",
        "table": "S6.T16.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_28": {
        "caption": "",
        "table": "S6.T17.2.2.2",
        "footnotes": [],
        "references": []
    },
    "id_table_29": {
        "caption": "",
        "table": "S6.T17.4.2.2",
        "footnotes": [],
        "references": []
    },
    "id_table_30": {
        "caption": "",
        "table": "S6.T18.3.3.3",
        "footnotes": [],
        "references": []
    },
    "id_table_31": {
        "caption": "",
        "table": "S6.T18.6.3.3",
        "footnotes": [],
        "references": []
    },
    "id_table_32": {
        "caption": "",
        "table": "S6.T19.3.3.3",
        "footnotes": [],
        "references": []
    },
    "id_table_33": {
        "caption": "",
        "table": "S6.T19.6.3.3",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": []
}