{
    "PAPER'S NUMBER OF TABLES": 1,
    "S1.T1": {
        "caption": "TABLE I: Federated Transformed Learning Functionalities distributed across onboard, edge, and cloud capabilities on a wireless network.",
        "table": "<table id=\"S1.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S1.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S1.T1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Deep Transforms</span></th>\n<th id=\"S1.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Circular</span></th>\n<th id=\"S1.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Secure</span></th>\n<th id=\"S1.T1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">Tiny</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S1.T1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Cloud-side</span></td>\n<td id=\"S1.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Memory Surrogates <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">8</a>]</cite>, Memory Points <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">9</a>]</cite>\n</td>\n<td id=\"S1.T1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Statistical <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">10</a>]</cite>, Manifold Transform <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>]</cite>\n</td>\n<td id=\"S1.T1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">On-the-fly Decomposition</td>\n</tr>\n<tr id=\"S1.T1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.3.2.1.1\" class=\"ltx_text ltx_font_bold\">Edge-side</span></td>\n<td id=\"S1.T1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Temporal Difference <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">8</a>]</cite>, GP Kernel Update <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">9</a>]</cite>\n</td>\n<td id=\"S1.T1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Anomaly Detection <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">10</a>, <a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>]</cite>\n</td>\n<td id=\"S1.T1.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Federated Learning <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">7</a>]</cite>\n</td>\n</tr>\n<tr id=\"S1.T1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S1.T1.1.4.3.1.1\" class=\"ltx_text ltx_font_bold\">Machine-side</span></td>\n<td id=\"S1.T1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">DL Model Update</td>\n<td id=\"S1.T1.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">N/A</td>\n<td id=\"S1.T1.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">Post-Training Decomposition <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib5\" title=\"\" class=\"ltx_ref\">5</a>]</cite>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "As shown in Figure 1, this CST-AI ecosystem creates stringent demands on the wireless network because it requires different AI transformed functionalities to be distributed, and to have their data streams and QoS demands streamed across the network. Network slicing is an integral part of the current 3GPP R16 and largely depends on explicit and timely knowledge on the demand and network environment condition. This enables a range of optimisation solutions ranging from classic optimisation to DL. However, many of the AI modules on-board and at edge are not known and change dynamically. The Radio Resource Management (RRM) need to allocate resources, reacting on the millisecond order in mass autonomy cases [1]. This requires that a slicing is conducted in the absence of system state information while performance safeguards are kept across the run time trajectory. This leads to the need to create run-time slicing for AI demands. As shown in [13], directly solving an off-line slicing solution is not feasible and a DL approach with stability guarantees can enable the system to learn a safe slicing solution from both historical records and run-time observations. To facilitate the imagination of the reader, a range of transformed DL functionalities and their QoS requirements are given in Table I."
        ]
    }
}