{
    "PAPER'S NUMBER OF TABLES": 2,
    "S2.T1": {
        "caption": "TABLE I: Deep generative models for federated learning-aided AIGC",
        "table": "<table id=\"S2.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S2.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S2.T1.1.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\" style=\"padding:5pt 5.0pt;\">\n<span id=\"S2.T1.1.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.1.1.1.1.1\" class=\"ltx_p\" style=\"width:32.5pt;\"><span id=\"S2.T1.1.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Model</span></span>\n</span>\n</th>\n<th id=\"S2.T1.1.1.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\" style=\"padding:5pt 5.0pt;\">\n<span id=\"S2.T1.1.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.1.1.2.1.1\" class=\"ltx_p\" style=\"width:54.2pt;\"><span id=\"S2.T1.1.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Component</span></span>\n</span>\n</th>\n<th id=\"S2.T1.1.1.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\" style=\"padding:5pt 5.0pt;\">\n<span id=\"S2.T1.1.1.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.1.1.3.1.1\" class=\"ltx_p\" style=\"width:97.6pt;\"><span id=\"S2.T1.1.1.1.3.1.1.1\" class=\"ltx_text ltx_font_bold\">Feature</span></span>\n</span>\n</th>\n<th id=\"S2.T1.1.1.1.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\" style=\"padding:5pt 5.0pt;\">\n<span id=\"S2.T1.1.1.1.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.1.1.4.1.1\" class=\"ltx_p\" style=\"width:97.6pt;\"><span id=\"S2.T1.1.1.1.4.1.1.1\" class=\"ltx_text ltx_font_bold\">Comparison</span></span>\n</span>\n</th>\n<th id=\"S2.T1.1.1.1.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\" style=\"padding:5pt 5.0pt;\">\n<span id=\"S2.T1.1.1.1.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.1.1.5.1.1\" class=\"ltx_p\" style=\"width:97.6pt;\"><span id=\"S2.T1.1.1.1.5.1.1.1\" class=\"ltx_text ltx_font_bold\">Application</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S2.T1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.2.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding:5pt 5.0pt;\">\n<span id=\"S2.T1.1.2.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.2.1.1.1.1\" class=\"ltx_p\" style=\"width:32.5pt;\">GAN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">2</a>]</cite></span>\n</span>\n</td>\n<td id=\"S2.T1.1.2.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding:5pt 5.0pt;\">\n<span id=\"S2.T1.1.2.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.2.1.2.1.1\" class=\"ltx_p\" style=\"width:54.2pt;\">Generator and discriminator</span>\n</span>\n</td>\n<td id=\"S2.T1.1.2.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding:5pt 5.0pt;\">\n<span id=\"S2.T1.1.2.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.2.1.3.1.1\" class=\"ltx_p\" style=\"width:97.6pt;\">Two models are trained together in an adversarial manner in terms of a zero-sum non-cooperative game.</span>\n</span>\n</td>\n<td id=\"S2.T1.1.2.1.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding:5pt 5.0pt;\">\n<span id=\"S2.T1.1.2.1.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.2.1.4.1.1\" class=\"ltx_p\" style=\"width:97.6pt;\">GAN adopts unsupervised learning and is sensitive to hyperparameter settings.</span>\n</span>\n</td>\n<td id=\"S2.T1.1.2.1.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding:5pt 5.0pt;\">\n<span id=\"S2.T1.1.2.1.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.2.1.5.1.1\" class=\"ltx_p\" style=\"width:97.6pt;\">Image generation, restoration and manipulation, super resolution and 3D shape reconstruction.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.3.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding:5pt 5.0pt;\">\n<span id=\"S2.T1.1.3.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.3.2.1.1.1\" class=\"ltx_p\" style=\"width:32.5pt;\">VAE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib3\" title=\"\" class=\"ltx_ref\">3</a>]</cite></span>\n</span>\n</td>\n<td id=\"S2.T1.1.3.2.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding:5pt 5.0pt;\">\n<span id=\"S2.T1.1.3.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.3.2.2.1.1\" class=\"ltx_p\" style=\"width:54.2pt;\">Encoder-decoder model</span>\n</span>\n</td>\n<td id=\"S2.T1.1.3.2.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding:5pt 5.0pt;\">\n<span id=\"S2.T1.1.3.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.3.2.3.1.1\" class=\"ltx_p\" style=\"width:97.6pt;\">VAE tries to create latent representations of training data in a probabilistic manner.</span>\n</span>\n</td>\n<td id=\"S2.T1.1.3.2.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding:5pt 5.0pt;\">\n<span id=\"S2.T1.1.3.2.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.3.2.4.1.1\" class=\"ltx_p\" style=\"width:97.6pt;\">VAE utilizes supervised learning and is easier to train and generate more coherent data.</span>\n</span>\n</td>\n<td id=\"S2.T1.1.3.2.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding:5pt 5.0pt;\">\n<span id=\"S2.T1.1.3.2.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.3.2.5.1.1\" class=\"ltx_p\" style=\"width:97.6pt;\">Image denoising, image generation and audio synthesis.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.4.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding:5pt 5.0pt;\">\n<span id=\"S2.T1.1.4.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.4.3.1.1.1\" class=\"ltx_p\" style=\"width:32.5pt;\">Diffusion model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\">4</a>]</cite></span>\n</span>\n</td>\n<td id=\"S2.T1.1.4.3.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding:5pt 5.0pt;\">\n<span id=\"S2.T1.1.4.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.4.3.2.1.1\" class=\"ltx_p\" style=\"width:54.2pt;\">Likelihood-based model</span>\n</span>\n</td>\n<td id=\"S2.T1.1.4.3.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding:5pt 5.0pt;\">\n<span id=\"S2.T1.1.4.3.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.4.3.3.1.1\" class=\"ltx_p\" style=\"width:97.6pt;\">Add random noise to input data in the forward process and recover the data by removing the noise in the reverse process.</span>\n</span>\n</td>\n<td id=\"S2.T1.1.4.3.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding:5pt 5.0pt;\">\n<span id=\"S2.T1.1.4.3.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.4.3.4.1.1\" class=\"ltx_p\" style=\"width:97.6pt;\">Diffusion model has the better learning performance but requires more training data, computational cost and time.</span>\n</span>\n</td>\n<td id=\"S2.T1.1.4.3.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding:5pt 5.0pt;\">\n<span id=\"S2.T1.1.4.3.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.4.3.5.1.1\" class=\"ltx_p\" style=\"width:97.6pt;\">Computer version, natural language generation, and multi-modal generation.</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.1.5.4\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.5.4.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t\" style=\"padding:5pt 5.0pt;\">\n<span id=\"S2.T1.1.5.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.5.4.1.1.1\" class=\"ltx_p\" style=\"width:32.5pt;\">Transformer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib5\" title=\"\" class=\"ltx_ref\">5</a>]</cite></span>\n</span>\n</td>\n<td id=\"S2.T1.1.5.4.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t\" style=\"padding:5pt 5.0pt;\">\n<span id=\"S2.T1.1.5.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.5.4.2.1.1\" class=\"ltx_p\" style=\"width:54.2pt;\">Sequence-to-sequence encoder-decoder model</span>\n</span>\n</td>\n<td id=\"S2.T1.1.5.4.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t\" style=\"padding:5pt 5.0pt;\">\n<span id=\"S2.T1.1.5.4.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.5.4.3.1.1\" class=\"ltx_p\" style=\"width:97.6pt;\">There are self-attention layers in both the encoder and decoder and each layer performs a multi-head attention mechanism.</span>\n</span>\n</td>\n<td id=\"S2.T1.1.5.4.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t\" style=\"padding:5pt 5.0pt;\">\n<span id=\"S2.T1.1.5.4.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.5.4.4.1.1\" class=\"ltx_p\" style=\"width:97.6pt;\">Transformer enables multiple attention heads to work in parallel, and achieves an advantage in the scalability to large-scale datasets.</span>\n</span>\n</td>\n<td id=\"S2.T1.1.5.4.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t\" style=\"padding:5pt 5.0pt;\">\n<span id=\"S2.T1.1.5.4.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.1.5.4.5.1.1\" class=\"ltx_p\" style=\"width:97.6pt;\">Natural language processing and computer vision.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "As a key enabling technology for AIGC, deep generative model learns to generate predictions in the identical modality as the input data with one modality. The rapid development of deep generative models has witnessed several promising techniques and we pay attention to Generative Adversarial Network (GAN), Variational Autoencode (VAE), diffusion model and Transformer. We summarize technique details of the deep generative models and their comparisons in Table I."
        ]
    },
    "S2.T2": {
        "caption": "TABLE II: Comparison of different fine-tuning methods for federated learning-aided AIGC",
        "table": "<table id=\"S2.T2.8\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S2.T2.8.9.1\" class=\"ltx_tr\">\n<th id=\"S2.T2.8.9.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span id=\"S2.T2.8.9.1.1.1\" class=\"ltx_text\">Method</span></th>\n<th id=\"S2.T2.8.9.1.2\" class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\">\n<span id=\"S2.T2.8.9.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.8.9.1.2.1.1\" class=\"ltx_p\">Description</span>\n</span>\n</th>\n<th id=\"S2.T2.8.9.1.3\" class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\">\n<span id=\"S2.T2.8.9.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.8.9.1.3.1.1\" class=\"ltx_p\">Feature</span>\n</span>\n</th>\n<th id=\"S2.T2.8.9.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\">Combining with federated learning</th>\n</tr>\n<tr id=\"S2.T2.8.10.2\" class=\"ltx_tr\">\n<th id=\"S2.T2.8.10.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Pros</th>\n<th id=\"S2.T2.8.10.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Cons</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S2.T2.1.1\" class=\"ltx_tr\">\n<td id=\"S2.T2.1.1.2\" class=\"ltx_td ltx_align_left ltx_border_t\">Traditional</td>\n<td id=\"S2.T2.1.1.3\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S2.T2.1.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.1.1.3.1.1\" class=\"ltx_p\">Fine-tune the full or partial weights of a pre-trained model.</span>\n</span>\n</td>\n<td id=\"S2.T2.1.1.1\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S2.T2.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.1.1.1.1.1\" class=\"ltx_p\">Very large gradient size for model update (1<math id=\"S2.T2.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S2.T2.1.1.1.1.1.m1.1a\"><mo id=\"S2.T2.1.1.1.1.1.m1.1.1\" xref=\"S2.T2.1.1.1.1.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S2.T2.1.1.1.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S2.T2.1.1.1.1.1.m1.1.1.cmml\" xref=\"S2.T2.1.1.1.1.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T2.1.1.1.1.1.m1.1c\">\\sim</annotation></semantics></math>10GB)</span>\n</span>\n</td>\n<td id=\"S2.T2.1.1.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S2.T2.1.1.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.1.1.4.1.1\" class=\"ltx_p\">Simple and effective</span>\n</span>\n</td>\n<td id=\"S2.T2.1.1.5\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S2.T2.1.1.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.1.1.5.1.1\" class=\"ltx_p\">Likely causing over-fitting when learning with few training samples</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T2.3.3\" class=\"ltx_tr\">\n<td id=\"S2.T2.3.3.3\" class=\"ltx_td ltx_align_left ltx_border_t\">LoRA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib9\" title=\"\" class=\"ltx_ref\">9</a>]</cite>\n</td>\n<td id=\"S2.T2.3.3.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S2.T2.3.3.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.3.3.4.1.1\" class=\"ltx_p\">Add extra bypass layers with few parameters for low-rank adaption.</span>\n</span>\n</td>\n<td id=\"S2.T2.3.3.2\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S2.T2.3.3.2.2\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.3.3.2.2.2\" class=\"ltx_p\">Moderate gradient size (<math id=\"S2.T2.2.2.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim 100\" display=\"inline\"><semantics id=\"S2.T2.2.2.1.1.1.m1.1a\"><mrow id=\"S2.T2.2.2.1.1.1.m1.1.1\" xref=\"S2.T2.2.2.1.1.1.m1.1.1.cmml\"><mi id=\"S2.T2.2.2.1.1.1.m1.1.1.2\" xref=\"S2.T2.2.2.1.1.1.m1.1.1.2.cmml\"></mi><mo id=\"S2.T2.2.2.1.1.1.m1.1.1.1\" xref=\"S2.T2.2.2.1.1.1.m1.1.1.1.cmml\">∼</mo><mn id=\"S2.T2.2.2.1.1.1.m1.1.1.3\" xref=\"S2.T2.2.2.1.1.1.m1.1.1.3.cmml\">100</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T2.2.2.1.1.1.m1.1b\"><apply id=\"S2.T2.2.2.1.1.1.m1.1.1.cmml\" xref=\"S2.T2.2.2.1.1.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S2.T2.2.2.1.1.1.m1.1.1.1.cmml\" xref=\"S2.T2.2.2.1.1.1.m1.1.1.1\">similar-to</csymbol><csymbol cd=\"latexml\" id=\"S2.T2.2.2.1.1.1.m1.1.1.2.cmml\" xref=\"S2.T2.2.2.1.1.1.m1.1.1.2\">absent</csymbol><cn type=\"integer\" id=\"S2.T2.2.2.1.1.1.m1.1.1.3.cmml\" xref=\"S2.T2.2.2.1.1.1.m1.1.1.3\">100</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T2.2.2.1.1.1.m1.1c\">\\sim 100</annotation></semantics></math> MB), moderate training data (<math id=\"S2.T2.3.3.2.2.2.m2.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S2.T2.3.3.2.2.2.m2.1a\"><mo id=\"S2.T2.3.3.2.2.2.m2.1.1\" xref=\"S2.T2.3.3.2.2.2.m2.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S2.T2.3.3.2.2.2.m2.1b\"><csymbol cd=\"latexml\" id=\"S2.T2.3.3.2.2.2.m2.1.1.cmml\" xref=\"S2.T2.3.3.2.2.2.m2.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T2.3.3.2.2.2.m2.1c\">\\sim</annotation></semantics></math> 30 samples)</span>\n</span>\n</td>\n<td id=\"S2.T2.3.3.5\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S2.T2.3.3.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.3.3.5.1.1\" class=\"ltx_p\">Exploiting decomposition matrices for <span id=\"S2.T2.3.3.5.1.1.1\" class=\"ltx_text ltx_font_italic\">communication and data efficient</span> tuning</span>\n</span>\n</td>\n<td id=\"S2.T2.3.3.6\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S2.T2.3.3.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.3.3.6.1.1\" class=\"ltx_p\">Deteriorate the diversity of the generated content</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T2.5.5\" class=\"ltx_tr\">\n<td id=\"S2.T2.5.5.3\" class=\"ltx_td ltx_align_left ltx_border_t\">DreamBooth <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">10</a>]</cite>\n</td>\n<td id=\"S2.T2.5.5.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S2.T2.5.5.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.5.5.4.1.1\" class=\"ltx_p\">Create a personalized model for a specific subject under various contexts.</span>\n</span>\n</td>\n<td id=\"S2.T2.5.5.2\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S2.T2.5.5.2.2\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.5.5.2.2.2\" class=\"ltx_p\">Large gradient size (<math id=\"S2.T2.4.4.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim 2\" display=\"inline\"><semantics id=\"S2.T2.4.4.1.1.1.m1.1a\"><mrow id=\"S2.T2.4.4.1.1.1.m1.1.1\" xref=\"S2.T2.4.4.1.1.1.m1.1.1.cmml\"><mi id=\"S2.T2.4.4.1.1.1.m1.1.1.2\" xref=\"S2.T2.4.4.1.1.1.m1.1.1.2.cmml\"></mi><mo id=\"S2.T2.4.4.1.1.1.m1.1.1.1\" xref=\"S2.T2.4.4.1.1.1.m1.1.1.1.cmml\">∼</mo><mn id=\"S2.T2.4.4.1.1.1.m1.1.1.3\" xref=\"S2.T2.4.4.1.1.1.m1.1.1.3.cmml\">2</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T2.4.4.1.1.1.m1.1b\"><apply id=\"S2.T2.4.4.1.1.1.m1.1.1.cmml\" xref=\"S2.T2.4.4.1.1.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S2.T2.4.4.1.1.1.m1.1.1.1.cmml\" xref=\"S2.T2.4.4.1.1.1.m1.1.1.1\">similar-to</csymbol><csymbol cd=\"latexml\" id=\"S2.T2.4.4.1.1.1.m1.1.1.2.cmml\" xref=\"S2.T2.4.4.1.1.1.m1.1.1.2\">absent</csymbol><cn type=\"integer\" id=\"S2.T2.4.4.1.1.1.m1.1.1.3.cmml\" xref=\"S2.T2.4.4.1.1.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T2.4.4.1.1.1.m1.1c\">\\sim 2</annotation></semantics></math> GB), very little training data (<math id=\"S2.T2.5.5.2.2.2.m2.1\" class=\"ltx_Math\" alttext=\"3\\sim 5\" display=\"inline\"><semantics id=\"S2.T2.5.5.2.2.2.m2.1a\"><mrow id=\"S2.T2.5.5.2.2.2.m2.1.1\" xref=\"S2.T2.5.5.2.2.2.m2.1.1.cmml\"><mn id=\"S2.T2.5.5.2.2.2.m2.1.1.2\" xref=\"S2.T2.5.5.2.2.2.m2.1.1.2.cmml\">3</mn><mo id=\"S2.T2.5.5.2.2.2.m2.1.1.1\" xref=\"S2.T2.5.5.2.2.2.m2.1.1.1.cmml\">∼</mo><mn id=\"S2.T2.5.5.2.2.2.m2.1.1.3\" xref=\"S2.T2.5.5.2.2.2.m2.1.1.3.cmml\">5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T2.5.5.2.2.2.m2.1b\"><apply id=\"S2.T2.5.5.2.2.2.m2.1.1.cmml\" xref=\"S2.T2.5.5.2.2.2.m2.1.1\"><csymbol cd=\"latexml\" id=\"S2.T2.5.5.2.2.2.m2.1.1.1.cmml\" xref=\"S2.T2.5.5.2.2.2.m2.1.1.1\">similar-to</csymbol><cn type=\"integer\" id=\"S2.T2.5.5.2.2.2.m2.1.1.2.cmml\" xref=\"S2.T2.5.5.2.2.2.m2.1.1.2\">3</cn><cn type=\"integer\" id=\"S2.T2.5.5.2.2.2.m2.1.1.3.cmml\" xref=\"S2.T2.5.5.2.2.2.m2.1.1.3\">5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T2.5.5.2.2.2.m2.1c\">3\\sim 5</annotation></semantics></math> samples)</span>\n</span>\n</td>\n<td id=\"S2.T2.5.5.5\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S2.T2.5.5.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.5.5.5.1.1\" class=\"ltx_p\">Leverage the semantic prior for <span id=\"S2.T2.5.5.5.1.1.1\" class=\"ltx_text ltx_font_italic\">data-efficient</span> training</span>\n</span>\n</td>\n<td id=\"S2.T2.5.5.6\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S2.T2.5.5.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.5.5.6.1.1\" class=\"ltx_p\">Communication-intensive fine-tuning for each single subject</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T2.7.7\" class=\"ltx_tr\">\n<td id=\"S2.T2.7.7.3\" class=\"ltx_td ltx_align_left ltx_border_t\">Textual inversion <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>]</cite>\n</td>\n<td id=\"S2.T2.7.7.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S2.T2.7.7.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.7.7.4.1.1\" class=\"ltx_p\">Capture novel concepts for personalized image generation via embedding tuning.</span>\n</span>\n</td>\n<td id=\"S2.T2.7.7.2\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S2.T2.7.7.2.2\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.7.7.2.2.2\" class=\"ltx_p\">Small embedding vector (<math id=\"S2.T2.6.6.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim 1\" display=\"inline\"><semantics id=\"S2.T2.6.6.1.1.1.m1.1a\"><mrow id=\"S2.T2.6.6.1.1.1.m1.1.1\" xref=\"S2.T2.6.6.1.1.1.m1.1.1.cmml\"><mi id=\"S2.T2.6.6.1.1.1.m1.1.1.2\" xref=\"S2.T2.6.6.1.1.1.m1.1.1.2.cmml\"></mi><mo id=\"S2.T2.6.6.1.1.1.m1.1.1.1\" xref=\"S2.T2.6.6.1.1.1.m1.1.1.1.cmml\">∼</mo><mn id=\"S2.T2.6.6.1.1.1.m1.1.1.3\" xref=\"S2.T2.6.6.1.1.1.m1.1.1.3.cmml\">1</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T2.6.6.1.1.1.m1.1b\"><apply id=\"S2.T2.6.6.1.1.1.m1.1.1.cmml\" xref=\"S2.T2.6.6.1.1.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S2.T2.6.6.1.1.1.m1.1.1.1.cmml\" xref=\"S2.T2.6.6.1.1.1.m1.1.1.1\">similar-to</csymbol><csymbol cd=\"latexml\" id=\"S2.T2.6.6.1.1.1.m1.1.1.2.cmml\" xref=\"S2.T2.6.6.1.1.1.m1.1.1.2\">absent</csymbol><cn type=\"integer\" id=\"S2.T2.6.6.1.1.1.m1.1.1.3.cmml\" xref=\"S2.T2.6.6.1.1.1.m1.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T2.6.6.1.1.1.m1.1c\">\\sim 1</annotation></semantics></math> KB), very little training data (<math id=\"S2.T2.7.7.2.2.2.m2.1\" class=\"ltx_Math\" alttext=\"3\\sim 5\" display=\"inline\"><semantics id=\"S2.T2.7.7.2.2.2.m2.1a\"><mrow id=\"S2.T2.7.7.2.2.2.m2.1.1\" xref=\"S2.T2.7.7.2.2.2.m2.1.1.cmml\"><mn id=\"S2.T2.7.7.2.2.2.m2.1.1.2\" xref=\"S2.T2.7.7.2.2.2.m2.1.1.2.cmml\">3</mn><mo id=\"S2.T2.7.7.2.2.2.m2.1.1.1\" xref=\"S2.T2.7.7.2.2.2.m2.1.1.1.cmml\">∼</mo><mn id=\"S2.T2.7.7.2.2.2.m2.1.1.3\" xref=\"S2.T2.7.7.2.2.2.m2.1.1.3.cmml\">5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T2.7.7.2.2.2.m2.1b\"><apply id=\"S2.T2.7.7.2.2.2.m2.1.1.cmml\" xref=\"S2.T2.7.7.2.2.2.m2.1.1\"><csymbol cd=\"latexml\" id=\"S2.T2.7.7.2.2.2.m2.1.1.1.cmml\" xref=\"S2.T2.7.7.2.2.2.m2.1.1.1\">similar-to</csymbol><cn type=\"integer\" id=\"S2.T2.7.7.2.2.2.m2.1.1.2.cmml\" xref=\"S2.T2.7.7.2.2.2.m2.1.1.2\">3</cn><cn type=\"integer\" id=\"S2.T2.7.7.2.2.2.m2.1.1.3.cmml\" xref=\"S2.T2.7.7.2.2.2.m2.1.1.3\">5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T2.7.7.2.2.2.m2.1c\">3\\sim 5</annotation></semantics></math> samples)</span>\n</span>\n</td>\n<td id=\"S2.T2.7.7.5\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S2.T2.7.7.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.7.7.5.1.1\" class=\"ltx_p\">Manipulate the word embedding for <span id=\"S2.T2.7.7.5.1.1.1\" class=\"ltx_text ltx_font_italic\">communication and data efficient</span> fine-tuning</span>\n</span>\n</td>\n<td id=\"S2.T2.7.7.6\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S2.T2.7.7.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.7.7.6.1.1\" class=\"ltx_p\">Computation-intensive fine-tuning for each word embedding</span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T2.8.8\" class=\"ltx_tr\">\n<td id=\"S2.T2.8.8.2\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">ControlNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">12</a>]</cite>\n</td>\n<td id=\"S2.T2.8.8.3\" class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\">\n<span id=\"S2.T2.8.8.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.8.8.3.1.1\" class=\"ltx_p\">Introduce extra conditional input for controlling the generated content.</span>\n</span>\n</td>\n<td id=\"S2.T2.8.8.1\" class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\">\n<span id=\"S2.T2.8.8.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.8.8.1.1.1\" class=\"ltx_p\">Large gradient size (<math id=\"S2.T2.8.8.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim 1\" display=\"inline\"><semantics id=\"S2.T2.8.8.1.1.1.m1.1a\"><mrow id=\"S2.T2.8.8.1.1.1.m1.1.1\" xref=\"S2.T2.8.8.1.1.1.m1.1.1.cmml\"><mi id=\"S2.T2.8.8.1.1.1.m1.1.1.2\" xref=\"S2.T2.8.8.1.1.1.m1.1.1.2.cmml\"></mi><mo id=\"S2.T2.8.8.1.1.1.m1.1.1.1\" xref=\"S2.T2.8.8.1.1.1.m1.1.1.1.cmml\">∼</mo><mn id=\"S2.T2.8.8.1.1.1.m1.1.1.3\" xref=\"S2.T2.8.8.1.1.1.m1.1.1.3.cmml\">1</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T2.8.8.1.1.1.m1.1b\"><apply id=\"S2.T2.8.8.1.1.1.m1.1.1.cmml\" xref=\"S2.T2.8.8.1.1.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S2.T2.8.8.1.1.1.m1.1.1.1.cmml\" xref=\"S2.T2.8.8.1.1.1.m1.1.1.1\">similar-to</csymbol><csymbol cd=\"latexml\" id=\"S2.T2.8.8.1.1.1.m1.1.1.2.cmml\" xref=\"S2.T2.8.8.1.1.1.m1.1.1.2\">absent</csymbol><cn type=\"integer\" id=\"S2.T2.8.8.1.1.1.m1.1.1.3.cmml\" xref=\"S2.T2.8.8.1.1.1.m1.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T2.8.8.1.1.1.m1.1c\">\\sim 1</annotation></semantics></math> GB), extra inference cost</span>\n</span>\n</td>\n<td id=\"S2.T2.8.8.4\" class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\">\n<span id=\"S2.T2.8.8.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.8.8.4.1.1\" class=\"ltx_p\">Improve the <span id=\"S2.T2.8.8.4.1.1.1\" class=\"ltx_text ltx_font_italic\">scalability</span> of the based model from diverse data over edge networks</span>\n</span>\n</td>\n<td id=\"S2.T2.8.8.5\" class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\">\n<span id=\"S2.T2.8.8.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T2.8.8.5.1.1\" class=\"ltx_p\">Communication-intensive; incompatible with joint training of multiple controllers</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We specifically consider the diffusion-based text-to-image generative model and introduce baseline methods for AIGC fine-tuning. The traditional fine-tuning method is to directly fine-tune the full or partial weights of a pre-trained model. Many advanced fine-tuning methods have been presented to overcome the limitations of the traditional method. Low-rank adaptation (LoRA) fine-tuning is proposed by Microsoft to explore low-rank adaptation to facilitate the fine-tuning process in a storage and computation efficient manner [9]. In Section IV, we apply this method to fine-tune a stable diffusion model on a customized dataset. DreamBooth fine-tuning [10] is proposed by Google to bind a unique identifier with an interesting subject in a few (typically 3-5) input images, and embed the subject within the pre-trained diffusion model’s output domain to generate new images of the subject. Textual inversion fine-tuning [11] is proposed by NVIDIA to find new pseudo-words that capture high-level semantic information and fine-grained visual details in the textual embedding space of a frozen latent diffusion model. ControlNet [12] adds an adapter model to a frozen pre-trained diffusion model, and train the adapter model to let the diffusion model support additional input conditions. We have a brief introduction of the above fine-tuning methods and compare their gradient and training data sizes in the first three columns of Table II.",
            "According to the overview of AIGC, we realize that FL has begun to be applied for decentralizing the model training procedures regrading the AIGC processes. However, the integration of FL and AIGC is not straightforward. We take fine-tuning an AIGC model in a federated manner as an example. For the privacy protection, the federated fine-tuning methods are motivated but technical challenges remain unresolved. For example, the combination of FL and LoRA is helpful to exploit the decomposition matrices for achieving the communication and data-efficient fine-tuning while this may deteriorate the diversity of the generated content. We summarize the pros and cons of combining the fine-tuning methods with FL for AIGC in the last two columns of Table II."
        ]
    }
}