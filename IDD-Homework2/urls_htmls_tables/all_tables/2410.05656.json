{
    "id_table_1": {
        "caption": "Table 1 :  LLMs struggle to predict the next observation.  We show the decreasing accuracy of the LLM to predict the next observation with increasing task complexity. LLMs are unable to generate pixel observations, which are used in MetaWorld.",
        "table": "A1.T1.3.3",
        "footnotes": [],
        "references": [
            "However, as the predominant paradigm for training LLMs is not inherently aligned with the challenges of sequential decision-making problems, such as active exploration, it is not obvious how to best bridge their capabilities to tackle such challenges in a general manner. We study this problem through the lens of reinforcement learning  (RL,  Sutton & Barto ,  2018 ) , which formalizes how an agent interacts with an environment, receiving scalar rewards for each of its actions over a trajectory. We examine the capabilities of LLMs to solve RL tasks by comparing how they model policies 1) directly by generating action tokens, to 2) indirectly through a reward model derived from the LLM to be used within an RL algorithm. We perform a comprehensive evaluation on a diverse set of domains, including MiniWob  ( Liu et al. ,  2018 ) , NetHack  ( Kuttler et al. ,  2020 ) , and Wordle  ( Lokshtanov & Subercaseaux ,  2022 ) , and MetaWorld  ( Yu et al. ,  2019 ) . The environments we study present a variety of challenges, such as different action space granularities, observation modalities ranging from natural language to pixel data, and varying horizon lengths.",
            "We first consider the off-the-shelf capabilities of LLMs for decision-making without updating them through additional gradient updates coming from the RL task. We find that indirectly modeling policies by first extracting knowledge from LLMs in the form of a Bradley-Terry model  ( Bradley & Terry ,  1952 ;  Christiano et al. ,  2017 )  provides the best and most consistent performance across the environments we study. We empirically analyze the various benefits, and limitations, provided by this approach, showing that it improves on long-standing challenges in RL problems, such as credit assignment and exploration.",
            "Finally, while LLMs possess knowledge useful for many decision making tasks of interest, domains with complex or unfamiliar dynamics can significantly restrict their broader utility. We explore how fine-tuning an LLM with domain-specific data can bridge this knowledge gap and study the effect of this procedure on the LLMs previous knowledge, as measured through success on datasets like POPE  ( Yifan Li & Wen ,  2023 ) , GQA  ( Hudson & Manning ,  2019 ) , AI2D  ( Kembhavi et al. ,  2016 )  and MMMU  ( Yue et al. ,  2024 ) . Our investigation reveals that fine-tuning for indirect policy modeling mitigates catastrophic forgetting more effectively than direct policy modeling, offering a broadly applicable strategy for leveraging LLMs across diverse sequential decision-making tasks.",
            "An RL task can be defined through a Markov Decision Process  (MDP,  Puterman ,  2014 ) , which is composed of a state space  S S \\mathcal{S} caligraphic_S , an action space  A A \\mathcal{A} caligraphic_A , a transition function  p : S  A    ( S ) : p  S A  S p:\\mathcal{S}\\times\\mathcal{A}\\to\\Delta(\\mathcal{S}) italic_p : caligraphic_S  caligraphic_A  roman_ ( caligraphic_S )  which describes the forward dynamics of the system, a reward function  r : S  A  R : r  S A R r:\\mathcal{S}\\times\\mathcal{A}\\to\\mathbb{R} italic_r : caligraphic_S  caligraphic_A  blackboard_R  and a discount factor    [ 0 , 1 ]  0 1 \\gamma\\in[0,1] italic_  [ 0 , 1 ] . Since it is often the case that the state is only partially observable, we also assume the environment emits an observation  o t  p O : S    ( O ) : similar-to subscript o t subscript p O  S  O o_{t}\\sim p_{\\mathcal{O}}:\\mathcal{S}\\to\\Delta(\\mathcal{O}) italic_o start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  italic_p start_POSTSUBSCRIPT caligraphic_O end_POSTSUBSCRIPT : caligraphic_S  roman_ ( caligraphic_O )  from observation space  O O \\mathcal{O} caligraphic_O . A policy, or  actor , is a probability distribution   : S    ( A ) :   S  A \\pi:\\mathcal{S}\\to\\Delta(\\mathcal{A}) italic_ : caligraphic_S  roman_ ( caligraphic_A )  which describes the action to be taken at every step. The objective of a rational actor is to maximize the expected cumulative rewards over horizon  H > 0 H 0 H>0 italic_H > 0 ,",
            "As shown in Equation  1 , the goal of a decision making agent is to learn a high performing policy    \\pi italic_ . This can be done either by maximizing the expected cumulative rewards and directly modeling the policy parameters  ( Sutton et al. ,  1999 ;  Kakade & Langford ,  2002 ) . Equivalently, this can be done indirectly by first modeling the parameters of the value function and applying a greedy operator, such as in Q-Learning  ( Watkins & Dayan ,  1992 ) . A similar separation between direct and indirect approaches can be useful to study the capabilities of LLMs to model RL policies.",
            "The most straightforward way to obtain a policy using LLMs is for the LLM to generate tokens that will be directly interpreted as actions from the environment,  a  A a A a\\in\\mathcal{A} italic_a  caligraphic_A   ( Yao et al. ,  2022 ;  Shinn et al. ,  2023 ;  Kim et al. ,  2024 ) . To ensure the outputted actions adhere to the environments action set, the LLM output tokens can be projected back onto  A A \\mathcal{A} caligraphic_A  using projection operator  proj  (  , A ) proj  A \\text{proj}(\\cdot,\\mathcal{A}) proj (  , caligraphic_A )   (e.g., see  Huang et al. ,  2022 ;  Kim et al. ,  2024 , for examples of projection operators) . A variety of prompting techniques can be combined to increase the ability of the LLM to act, without task-specific fine-tuning, as a policy, which we detail in Section  2.1 . This direct policy method will be referred to in our experiments as LLM Policy.",
            "In direct policy modeling experiments (LLM Policy), we found combining all of the prompting techniques in Section  2.1  to work the best, while for indirect modeling methods through reward we relied only on chain-of-thought prompting. Additional details, such specific prompt details and ablations on these choices are presented in the Appendix  A.3 .",
            "where  P   [ o 1  o 2 ] = e r   ( o 1 ) e r   ( o 1 ) + e r   ( o 2 ) subscript P  delimited-[] succeeds subscript o 1 subscript o 2 superscript e subscript r  subscript o 1 superscript e subscript r  subscript o 1 superscript e subscript r  subscript o 2 P_{\\theta}[o_{1}\\succ o_{2}]=\\frac{e^{r_{\\theta}(o_{1})}}{e^{r_{\\theta}(o_{1})% }+e^{r_{\\theta}(o_{2})}} italic_P start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT [ italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  italic_o start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ] = divide start_ARG italic_e start_POSTSUPERSCRIPT italic_r start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPT end_ARG start_ARG italic_e start_POSTSUPERSCRIPT italic_r start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPT + italic_e start_POSTSUPERSCRIPT italic_r start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_o start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPT end_ARG  the probability of preferring an observation to another, referred to as the Bradley-Terry model for preference learning  ( Bradley & Terry ,  1952 ) . The minimization of this equation is commonly done through binary cross-entropy.",
            "We investigate four separate domains, where each domain aims to highlight a specific capability of LLMs: 1) MiniWob-Hard, a subset of hard tasks from the full MiniWob suite, tests web interaction in observation/action spaces close to natural language, 2) Wordle measures reasoning and planning capabilities, 3) NetHack presents the difficulty of exploring open-ended environments under partial observability, long horizons and procedural scenarios, and 4) MetaWorld assesses the ability to control low-level, high-frequency actions in continuous space. We provide a detailed description of each domain in Appendix  A.1 .",
            "We first present a comparison of the various indirect policy modeling approaches discussed in  Section   2.3 . In these experiments, the LLM generates a reward function which will be given to a RL agent for optimization, without access to any rewards coming from the environment. When learning policies through RL we do not perform any hyperparameter search and simply borrow the existing empirical setup for each domain, as detailed in Appendix  A.1 .",
            "In  Figure   1 , we present the performance across domains as measured by the average success rate on all domains, except for NetHack, where performance (the in-game score) is normalized by the highest recorded value. Results show that AI feedback is the only method that successfully crafts rewards across all environments and modalities    In Appendix  A.6 , we verify that AI feedback yields policies with performance on par with those optimized using human-designed environment rewards. . On easier domains such as MiniWob-Hard, which consists of short episodes and limited scope of variations, the Direct Scalar method performs nearly as well as AI feedback. However, the disparity between methods is much more pronounced on harder, open-ended tasks such as NetHack. Out of all the methods, Embedding-based leads to the lowest performance. Finally, the effectiveness of Reward as Code appears to be highly contingent on the availability of symbolic features for code processing. In Appendix  A.5 , we further examine the assumptionssuch as access to functional knowledge of the environmentunder which Reward as Code can achieve performance comparable to AI feedback",
            "In Figure  3 , we present the output of the AI feedback-based reward model over each timestep of an episode within a simple grid world environment. This task includes an agent, a key, a door, and a goal  ( Chevalier-Boisvert et al. ,  2023 ) . We notice that this reward model naturally captures the fact that picking up the key, as well as opening the locked door, are important steps towards the goal. By propagating credit over such key moments in a trajectory, the LLM effectively shortens the horizon over which the RL algorithm must assign credit through temporal difference learning  ( Sutton & Barto ,  2018 ) . This is manifested in Figure  3  where the agent learning through AI feedback reaches a high success rate in a fraction of the timesteps required by a similar agent learning from the environment feedback (which in this case is sparse reward of +1 for reaching the goal).",
            "In Figure  4 , we present the correlation between the reward model derived from AI feedback and the value function of an RL agent across various levels of policy optimality. We observe that AI feedback generates reward functions with a stronger correlation to value functions obtained later in the training process compared to those from earlier stages. Additionally, this correlation is higher than that observed with the environment reward. In the Wordle game, we generate, in code, a near-optimal policy and estimate its value function using Monte Carlo. We then compare it to the LLM-derived reward function find an almost perfect correlation. These findings suggest that the reward models derived from AI feedback inherently encode aspects of high-quality value functions, which, when used as rewards for the RL agent, can substantially simplify the credit assignment process. In Appendix  A.7 , we provide additional insights from the lens of heuristic-guided reinforcement learning  ( Cheng et al. ,  2021 ) .",
            "where  r A  I  F subscript r A I F r_{AIF} italic_r start_POSTSUBSCRIPT italic_A italic_I italic_F end_POSTSUBSCRIPT  is the reward model obtained from AI feedback,  N  ( o t ) N subscript o t N(o_{t}) italic_N ( italic_o start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  denotes the number of times a particular observation  o t subscript o t o_{t} italic_o start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  was seen in an episode, and    \\beta italic_  is a positive real-valued coefficient set to  3 3 3 3 . The counting term was added to encourage exploration  ( Henaff et al. ,  2022 ) , which is a key difficulty in NetHack. However, instantiating such a counting function proves difficult in many practical settings  ( Bellemare et al. ,  2016 ) . Given the flexibility of natural language, can we alleviate the need for such a term and integrate the notion of exploration in the prompt itself?",
            "We therefore fine-tune PaliGemma on image-caption pairs annotated by GPT-4o and trained the model to predict the caption for a given image.  Figure   6(a)  shows significant gains in downstream RL performance after only a few fine-tuning epochs and as few as approximately  100 100 100 100  image-caption pairs. Moreover,  Figure   6(a)  shows how this procedure only marginally decreases performance of the LLM on the standard multi-modal reasoning benchmarks, such as POPE  ( Yifan Li & Wen ,  2023 ) , GQA  ( Hudson & Manning ,  2019 ) , AI2D  ( Kembhavi et al. ,  2016 )  and MMMU  ( Yue et al. ,  2024 ) . Surprisingly, performance on the AI2D benchmark  improves  as the number of task-specific fine-tuning epochs increases.",
            "Some notable limitations and caveats exist. For example, interacting with LLMs through natural language requires experimenting with various prompting techniques and specifications. However, this flexibility also enables the shaping of reward functions to incorporate valuable strategies  ( Knox et al. ,  2013 ) , such as promoting exploration, which can further enhance the performance of RL agents.",
            "In our experiments, we investigate tasks from four different domains: MiniWob  ( Liu et al. ,  2018 ) , NetHack  ( Kuttler et al. ,  2020 ) , and Wordle  ( Lokshtanov & Subercaseaux ,  2022 ) , and MetaWorld  ( Yu et al. ,  2019 ) . The observation space for all these environments is text, except fro MetaWorld which consists of RGB pixels.",
            "We use the following prompt templates to query the agent for AI feedback, Scalar Reward and Reward as Code across various environments. For the Embedding-based approach, we use calculate the cosine similarity between the representation, provided by a BERT  ( Devlin et al. ,  2019 )  sentence encoder (specifically the same  paraphrase-MiniLM-L3-v2  model) when environments are text-based, and otherwise we use the CLIP encoder  ( Radford et al. ,  2021 ) . The similarity is measured between the current observation and the same goal description contained in the each of the following prompts given for the other baselines.",
            "We present the exact prompts used to query GPT-4o for each of the domains we have considered. These are presented through Prompt  13 ,  15 ,  14  and  16 .",
            "Additionally, in Figure  7 , we ablate the prompting techniques used in our direct policy modeling approach. Results show that a combination of all prompting techniques presened in Section  2.1  works best.",
            "Many of the above could theoretically be used to construct a policy, yet a full implementation is out of scope from this paper due to the lack of available code-bases to build upon and we do not seek to build new algorithms from scratch. However, in Figure  2(b)  we perform investigations into the capabilities of LLMs to perform Action Preference and State Preference. The results show that current LLMs struggle to achieve strong performance on any of these tasks. Additionally, in Table  1 , we report the accuracy with which LLMs directly predicts the next observation (Direct State Generation), providing a probe into their direct world modeling capabilities. Results show limited performance, except on MiniWob-Hard tasks, which are fully observable and encode deterministic transitions.",
            "While prior works have shown that rewards can be extracted from a language model  ( Brooks et al. ,  2024 ;  Klissarov et al. ,  2024 ) , it can be more generally thought of as encoding a heuristic function  h h h italic_h . The function  h h h italic_h  contains high-level, multi-step information about the MDP  M M M italic_M . To extract it, one can solve the re-shaped MDP  M ~ ~ M \\tilde{M} over~ start_ARG italic_M end_ARG  with  r ~  ( s t , a t ) = r  ( s t , a t ) + ( 1   )    E s t + 1 | s t , a t  [ h  ( s t + 1 ) ] ~ r subscript s t subscript a t r subscript s t subscript a t 1   subscript E conditional subscript s t 1 subscript s t subscript a t delimited-[] h subscript s t 1 \\tilde{r}(s_{t},a_{t})=r(s_{t},a_{t})+(1-\\lambda)\\gamma\\mathbb{E}_{s_{t+1}|s_{% t},a_{t}}[h(s_{t+1})] over~ start_ARG italic_r end_ARG ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = italic_r ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) + ( 1 - italic_ ) italic_ blackboard_E start_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT | italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ italic_h ( italic_s start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ) ]  and   ~ =    ~    \\tilde{\\gamma}=\\lambda\\gamma over~ start_ARG italic_ end_ARG = italic_ italic_  where    [ 0 , 1 ]  0 1 \\lambda\\in[0,1] italic_  [ 0 , 1 ]   Cheng et al.   ( 2021 ) . Solving  M ~ ~ M \\tilde{M} over~ start_ARG italic_M end_ARG  yields a policy    superscript  \\pi^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  that is also optimal in  M M M italic_M  - its value functions bias can be shown to converge to  V  superscript V V^{*} italic_V start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  in  M M M italic_M  as a function of   h  V    subscript norm h superscript V ||h-V^{*}||_{\\infty} | | italic_h - italic_V start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT | | start_POSTSUBSCRIPT  end_POSTSUBSCRIPT .",
            "In Figure  10 , we present a variation on the Wordle game where the color code has been altered, which we refer to as Eldrow (reverse Wordle). Under this transformation, the off-the-shelf model provides feedback that correlates very poorly with the optimal value function. When we measure the perplexity of the LLM on a natural language description of the new rule set of Eldrow (see Appendix  A ) we obtain a value of  6.97 6.97 6.97 6.97  which is higher than the one measured on the standard rule set of Wordle, with a value of  5.06 5.06 5.06 5.06 . Given that the difference in values is not very large, we leverage the simplest way for adapting the LLM: through in-context learning. As shown in Figure  10(b) , by providing hints in the prompt about the new rule set, the LLM adapts its preferences and generates a Bradley-Terry model that recovers the correlation values we witnessed in  4 .",
            "To test this, we query  Gemini-1.5 Pro   ( Team et al. ,  2023 )  with a context video containing 500 frames of an agent exploring the bottom-left room ( Figure   11 -left) and a single frame sampled uniformly at random from a query episode which covers in the top-right room, center and bottom of the maze ( Figure   11 -middle). We ask the LLM to identify novel query states, i.e. states which are not seen in the context episode. We then train a direct predictor (3-layer MLP) to estimate the probability of any state on the grid to be novel with respect to the context ( Figure   11 -right). The language model correctly identifies the top-right portion of the trajectory to be novel, knowledge which could then be used to construct an intrinsic reward function.",
            "Large language models (LLMs) require additional adaptation for general-use language tasks  ( Christiano et al. ,  2017 ;  Stiennon et al. ,  2020 ;  Ouyang et al. ,  2022 ;  Mialon et al. ,  2023 ) . Without additional context and/or fine-tuning, LLMs can generate misleading, harmful, or even nonsensical answers to queries or conversations with humans  ( Bai et al. ,  2022 ) . To modify their behavior, it is necessary to tune their prompts and/or fine-tune their outputs to ensure their output is desirable w.r.t. some set of linguistic tasks before deployment. This at least if not more true in embodied settings, where real-world actions can have physical consequences, and methodologies for modifying LLM behavior in embodied settings more-or-less align with efforts in the language space.",
            "Arguably the most common theme among techniques that modify LLM behavior in general is to change the prompt such that the distribution of LLM outputs better-fits a given desiderata on behavior. Prompt-engineering can greatly align or calibrate an LLM, pretrained or no, to desired beneficial behavior  ( Christiano et al. ,  2017 ;  Glaese et al. ,  2022 ;  Bai et al. ,  2022 ) , or even expose harmful or other unexpected behaviors. Chain-of-thought  (CoT,   Wei et al. ,  2022 )  is an in-context method to either few-shot or zero-shot  ( Kojima et al. ,  2022 )  adjust an LLMs outputs to generate more correct responses to question-and-answering tasks. Further modifications to the prompt such as providing feedback from an environment  ( Yao et al. ,  2022 ) , self-critique  ( Zelikman et al. ,  2022 ) , or self-reflection  ( Shinn et al. ,  2023 )  can improve LLM performance in language as well as tasks that have an environment. The biggest promise of in-context-based methods in RL is that somewhere within the large language models conditional distribution is the optimal policy for any given task  ( Brohan et al. ,  2023 ;  Szot et al. ,  2023 ) , an accurate world-explicit model  ( Lin et al. ,  2024 ) , and/or a useful reward-model  ( Klissarov et al. ,  2024 ) . However, it is at best speculative as LLMs are black box systems and prompt optimization is extremely difficult, and besides: systems built on this idea still must still overcome affordance mismatch  ( Ahn et al. ,  2022 )  and hallucinations  ( Zhang et al. ,  2024a )  to be useful for RL.",
            "Querying model for feedback Another hypothesis is that LLMs contain knowledge relevant to tasks, and this knowledge can be extracted  ( Xu et al. ,  2024 )  in a way to train a policy that has desirable behavior  ( Huang et al. ,  2022 ) . RL AI Feedback  (RLAIF  Bai et al. ,  2022 ;  Lee et al. ,  2023 )  is a scalable method akin to but without the practical issues that come paired with RL from Human Feedback  (RLHF  Christiano et al. ,  2017 ) , the goal of which is to fine-tune an existing LLM to be more specific, accurate, innocuous, etc. RLAIF trains a reward model on a dataset collected from an LLMs preferences given a dataset of language responses from an LLM and a given set of queries, and this reward model is used to train a policy using RL, for example using PPO. This process of extracting knowledge using preference data can also be directly used to train a policy without a reward model  ( Rafailov et al. ,  2024 ) ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  AI Feedback performs on par with Reward as Code, without proprioceptive observations or expert demonstrations. To match AI Feedback performance on Metaworld, Reward as Code requires GPT-4o level knowledge, augmented with either in-context expert demonstrations or proprioceptive observations.",
        "table": "A1.T2.7.7",
        "footnotes": [],
        "references": [
            "Large Language Models (LLMs) are generative models of natural language that can produce accurate general and domain-specific knowledge  ( Singhal et al. ,  2022 ;  Imani et al. ,  2023 ;  Manigrasso et al. ,  2024 ;  Liu et al. ,  2024a ) , reason over long textual contexts  ( Reid et al. ,  2024 ) , and generalize zero-shot  ( Kojima et al. ,  2022 ) . These capabilities suggest that LLMs might be well-suited for complex sequential decision-making problems, such as in embodied settings where an agent acts in an environment. Recent research has begun exploring this potential, investigating how LLMs can serve as sources of intrinsic motivation  ( Wang et al. ,  2024 ;  Klissarov et al. ,  2024 ) , demonstrating world modeling capabilities  ( Lin et al. ,  2024 ;  Liu et al. ,  2024b ) , and for acting and/or planning directly in an environment  ( Wang et al. ,  2023 ;  Padalkar et al. ,  2023 ;  Zhang et al. ,  2024b ) .",
            "However, as the predominant paradigm for training LLMs is not inherently aligned with the challenges of sequential decision-making problems, such as active exploration, it is not obvious how to best bridge their capabilities to tackle such challenges in a general manner. We study this problem through the lens of reinforcement learning  (RL,  Sutton & Barto ,  2018 ) , which formalizes how an agent interacts with an environment, receiving scalar rewards for each of its actions over a trajectory. We examine the capabilities of LLMs to solve RL tasks by comparing how they model policies 1) directly by generating action tokens, to 2) indirectly through a reward model derived from the LLM to be used within an RL algorithm. We perform a comprehensive evaluation on a diverse set of domains, including MiniWob  ( Liu et al. ,  2018 ) , NetHack  ( Kuttler et al. ,  2020 ) , and Wordle  ( Lokshtanov & Subercaseaux ,  2022 ) , and MetaWorld  ( Yu et al. ,  2019 ) . The environments we study present a variety of challenges, such as different action space granularities, observation modalities ranging from natural language to pixel data, and varying horizon lengths.",
            "We first consider the off-the-shelf capabilities of LLMs for decision-making without updating them through additional gradient updates coming from the RL task. We find that indirectly modeling policies by first extracting knowledge from LLMs in the form of a Bradley-Terry model  ( Bradley & Terry ,  1952 ;  Christiano et al. ,  2017 )  provides the best and most consistent performance across the environments we study. We empirically analyze the various benefits, and limitations, provided by this approach, showing that it improves on long-standing challenges in RL problems, such as credit assignment and exploration.",
            "Finally, while LLMs possess knowledge useful for many decision making tasks of interest, domains with complex or unfamiliar dynamics can significantly restrict their broader utility. We explore how fine-tuning an LLM with domain-specific data can bridge this knowledge gap and study the effect of this procedure on the LLMs previous knowledge, as measured through success on datasets like POPE  ( Yifan Li & Wen ,  2023 ) , GQA  ( Hudson & Manning ,  2019 ) , AI2D  ( Kembhavi et al. ,  2016 )  and MMMU  ( Yue et al. ,  2024 ) . Our investigation reveals that fine-tuning for indirect policy modeling mitigates catastrophic forgetting more effectively than direct policy modeling, offering a broadly applicable strategy for leveraging LLMs across diverse sequential decision-making tasks.",
            "An RL task can be defined through a Markov Decision Process  (MDP,  Puterman ,  2014 ) , which is composed of a state space  S S \\mathcal{S} caligraphic_S , an action space  A A \\mathcal{A} caligraphic_A , a transition function  p : S  A    ( S ) : p  S A  S p:\\mathcal{S}\\times\\mathcal{A}\\to\\Delta(\\mathcal{S}) italic_p : caligraphic_S  caligraphic_A  roman_ ( caligraphic_S )  which describes the forward dynamics of the system, a reward function  r : S  A  R : r  S A R r:\\mathcal{S}\\times\\mathcal{A}\\to\\mathbb{R} italic_r : caligraphic_S  caligraphic_A  blackboard_R  and a discount factor    [ 0 , 1 ]  0 1 \\gamma\\in[0,1] italic_  [ 0 , 1 ] . Since it is often the case that the state is only partially observable, we also assume the environment emits an observation  o t  p O : S    ( O ) : similar-to subscript o t subscript p O  S  O o_{t}\\sim p_{\\mathcal{O}}:\\mathcal{S}\\to\\Delta(\\mathcal{O}) italic_o start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  italic_p start_POSTSUBSCRIPT caligraphic_O end_POSTSUBSCRIPT : caligraphic_S  roman_ ( caligraphic_O )  from observation space  O O \\mathcal{O} caligraphic_O . A policy, or  actor , is a probability distribution   : S    ( A ) :   S  A \\pi:\\mathcal{S}\\to\\Delta(\\mathcal{A}) italic_ : caligraphic_S  roman_ ( caligraphic_A )  which describes the action to be taken at every step. The objective of a rational actor is to maximize the expected cumulative rewards over horizon  H > 0 H 0 H>0 italic_H > 0 ,",
            "In this section, we describe the inputs, or  prompts , to the LLM used in this work which allow to change the LLMs output distribution to be useful for solving RL tasks. All prompts in this work use 1) task specification using natural language as input to provide information about the MDP to the LLM as context and 2) episode history in order to address issues of partial-observability in some environments  (similar to the Act-only baseline prompt found in  Yao et al. ,  2022 ) . We additionally use the following set of techniques,",
            "Chain of Thought . By prompting the LLM to provide a step-by-step reasoning process for its output, rather than just the final answer, we can help surface its internal decision-making and improve the resulting performance  ( Wei et al. ,  2022 ) .",
            "In-Context Learning . To enhance the LLMs ability to solve the task, example solutions (e.g., from expert policies) are provided for in-context learning  ( Brown et al. ,  2020 ) , where solutions contain sequences of a combination of states, actions, and rewards.",
            "Self-Refinement . To further refine its output, the LLM is prompted to provide recursive criticism and improvement from its generated outputs. This general strategy knows many variants, such as feedback from an environment  ( Yao et al. ,  2022 ) , self-critique  ( Zelikman et al. ,  2022 ) , or self-reflection  ( Shinn et al. ,  2023 ) . In this work, we use Recursive Criticism and Improvement  (RCI,  Kim et al. ,  2024 )  for its state-of-the-art performance on web agent domains and general applicability. In its original form, the LLM is given a task description and generates a high-level plan. This plan is used along with the task description and current state to refine an action so that it is grounded in the current observation and the action space.",
            "As shown in Equation  1 , the goal of a decision making agent is to learn a high performing policy    \\pi italic_ . This can be done either by maximizing the expected cumulative rewards and directly modeling the policy parameters  ( Sutton et al. ,  1999 ;  Kakade & Langford ,  2002 ) . Equivalently, this can be done indirectly by first modeling the parameters of the value function and applying a greedy operator, such as in Q-Learning  ( Watkins & Dayan ,  1992 ) . A similar separation between direct and indirect approaches can be useful to study the capabilities of LLMs to model RL policies.",
            "The most straightforward way to obtain a policy using LLMs is for the LLM to generate tokens that will be directly interpreted as actions from the environment,  a  A a A a\\in\\mathcal{A} italic_a  caligraphic_A   ( Yao et al. ,  2022 ;  Shinn et al. ,  2023 ;  Kim et al. ,  2024 ) . To ensure the outputted actions adhere to the environments action set, the LLM output tokens can be projected back onto  A A \\mathcal{A} caligraphic_A  using projection operator  proj  (  , A ) proj  A \\text{proj}(\\cdot,\\mathcal{A}) proj (  , caligraphic_A )   (e.g., see  Huang et al. ,  2022 ;  Kim et al. ,  2024 , for examples of projection operators) . A variety of prompting techniques can be combined to increase the ability of the LLM to act, without task-specific fine-tuning, as a policy, which we detail in Section  2.1 . This direct policy method will be referred to in our experiments as LLM Policy.",
            "On the other hand, we can prompt the LLM to output tokens representing intermediate quantities that will then be used to learn a policy. For example, one can model the forward dynamics of the environment for planning  ( Liu et al. ,  2024b ) , or an affordance model for action selection  ( Mullen Jr & Manocha ,  2024 ) . In this work, we focus on the case where these intermediate quantities will be used to generate rewards  i.e., a reward model  which will then be maximized by an off-the-shelf RL policy. In Section  2.3 , we enumerate the different approaches for modeling reward functions with LLMs covered in our work.    It is important to note that there exists many more ways in which we could indirectly model the policy. In Appendix  A.4 , we present in detail these possibilities and, in  Figure   2(b) , provide initial investigations that showcase their potential and limitations.",
            "In direct policy modeling experiments (LLM Policy), we found combining all of the prompting techniques in Section  2.1  to work the best, while for indirect modeling methods through reward we relied only on chain-of-thought prompting. Additional details, such specific prompt details and ablations on these choices are presented in the Appendix  A.3 .",
            "AI Feedback    ( Lee et al. ,  2023 ;  Klissarov et al. ,  2024 ) . Ask the LLM to express a preference  y = { 1 , 2 ,  } y 1 2 y=\\{1,2,\\varnothing\\} italic_y = { 1 , 2 ,  }  between two observations,  o 1 subscript o 1 o_{1} italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and  o 2 subscript o 2 o_{2} italic_o start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , for the one showing the most progress towards a certain goal, or no preference if both observations are equally good. These labels can then be collected as a dataset of observation-preference tuples  D pref = { ( o 1 ( i ) , o 2 ( i ) , y ( i ) ) } i = 1 M subscript D pref superscript subscript superscript subscript o 1 i superscript subscript o 2 i superscript y i i 1 M \\mathcal{D}_{\\text{pref}}=\\{(o_{1}^{(i)},o_{2}^{(i)},y^{(i)})\\}_{i=1}^{M} caligraphic_D start_POSTSUBSCRIPT pref end_POSTSUBSCRIPT = { ( italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , italic_o start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , italic_y start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT , which are then used to train a reward function modeled as,",
            "where  P   [ o 1  o 2 ] = e r   ( o 1 ) e r   ( o 1 ) + e r   ( o 2 ) subscript P  delimited-[] succeeds subscript o 1 subscript o 2 superscript e subscript r  subscript o 1 superscript e subscript r  subscript o 1 superscript e subscript r  subscript o 2 P_{\\theta}[o_{1}\\succ o_{2}]=\\frac{e^{r_{\\theta}(o_{1})}}{e^{r_{\\theta}(o_{1})% }+e^{r_{\\theta}(o_{2})}} italic_P start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT [ italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  italic_o start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ] = divide start_ARG italic_e start_POSTSUPERSCRIPT italic_r start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPT end_ARG start_ARG italic_e start_POSTSUPERSCRIPT italic_r start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPT + italic_e start_POSTSUPERSCRIPT italic_r start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_o start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPT end_ARG  the probability of preferring an observation to another, referred to as the Bradley-Terry model for preference learning  ( Bradley & Terry ,  1952 ) . The minimization of this equation is commonly done through binary cross-entropy.",
            "Reward as Code   ( Yu et al. ,  2023 ;  Ma et al. ,  2023 ) . Prompt the LLM to write code that will take as input a subset of symbolic features from the environment observations and will produce a scalar output representing the reward. When symbolic features are not available, these are constructed as in  Venuto et al.   ( 2024 ) .",
            "Embedding-based   ( Rocamonde et al. ,  2023 ;  Du et al. ,  2023 ) . Instead of querying language tokens from the LLM, we can instead, for a given input, leverage the information encoded in its latent represention, or embeddings. These embeddings are used to calculate the cosine similarity with respect to the embeddings of natural language specification of a goal or a behaviour. The resulting similarity value is given as a reward to the agent.",
            "Additional details, such specific prompts, are presented in the Appendix  A.2 .",
            "Due to fundamentally different challenges between direct and indirect policy modeling approaches, conducting a fair comparison requires care. For example, using the LLM directly as a policy requires grounding its outputs in the action space defined by the environment  ( Ahn et al. ,  2022 ;  Huang et al. ,  2022 ) . As the action space can vary significantly between environments and attempting to solve this problem adds additional algorithm- or domain-specific complexities (e.g. by crafting skills, see  Ahn et al.   ( 2022 ) ;  Wang et al.   ( 2023 ) ), we fix our experimental setting to the following",
            "Direct policy modeling is done by querying the closed source GPT-4o model, whereas indirect policy modeling is done through the open source models of Llama 3  ( Dubey et al. ,  2024 ) , when environment observations consist of text, and PaliGemma  ( Beyer et al. ,  2024 ) , when environment observation consist of pixel images. All results are averaged over 10 seeds with error bars indicating the standard error.",
            "We first present a comparison of the various indirect policy modeling approaches discussed in  Section   2.3 . In these experiments, the LLM generates a reward function which will be given to a RL agent for optimization, without access to any rewards coming from the environment. When learning policies through RL we do not perform any hyperparameter search and simply borrow the existing empirical setup for each domain, as detailed in Appendix  A.1 .",
            "We now compare the direct policy modeling method, LLM Policy, to the best performing indirect modeling method, AI feedback, reporting performance across the same set of domains. Results in  Figure   2(a)  show that, despite the more complex prompting strategies and the use of a more capable closed source model, LLM Policy is unable to perform well in most environments, with the exception of MiniWob-Hard, where the performance is on-par with AI feedback.",
            "Results presented in  Figure   2(b)  show that the LLM performs relatively poorly on both of these tasks, indicating limited understanding of both the action space and the environment dynamics. This can potentially explain the limited performance of the LLM Policy approach on MiniWob-Hard, NetHack, and MetaWorld, while results on Wordle suggest that additional contributing factors are at play.",
            "AI feedback-based rewards depend on the prompt used to capture preferences. In the experiments conducted so far, these prompts were designed to elicit preferences by emphasizing states that contribute to task progress (see prompts Appendix  A.2 ). Additionally, a key aspect of our methodology involved presenting the LLM with observations sampled randomly within trajectories. This enabled querying preference for any observation in the environment, rather limiting the focus to final states - a distinction also known as process-based and outcome-based reward models  ( Uesato et al. ,  2023 ;  Lightman et al. ,  2023 ) . What are the resulting characteristics of the reward model under such choices?",
            "In Figure  3 , we present the output of the AI feedback-based reward model over each timestep of an episode within a simple grid world environment. This task includes an agent, a key, a door, and a goal  ( Chevalier-Boisvert et al. ,  2023 ) . We notice that this reward model naturally captures the fact that picking up the key, as well as opening the locked door, are important steps towards the goal. By propagating credit over such key moments in a trajectory, the LLM effectively shortens the horizon over which the RL algorithm must assign credit through temporal difference learning  ( Sutton & Barto ,  2018 ) . This is manifested in Figure  3  where the agent learning through AI feedback reaches a high success rate in a fraction of the timesteps required by a similar agent learning from the environment feedback (which in this case is sparse reward of +1 for reaching the goal).",
            "In Figure  4 , we present the correlation between the reward model derived from AI feedback and the value function of an RL agent across various levels of policy optimality. We observe that AI feedback generates reward functions with a stronger correlation to value functions obtained later in the training process compared to those from earlier stages. Additionally, this correlation is higher than that observed with the environment reward. In the Wordle game, we generate, in code, a near-optimal policy and estimate its value function using Monte Carlo. We then compare it to the LLM-derived reward function find an almost perfect correlation. These findings suggest that the reward models derived from AI feedback inherently encode aspects of high-quality value functions, which, when used as rewards for the RL agent, can substantially simplify the credit assignment process. In Appendix  A.7 , we provide additional insights from the lens of heuristic-guided reinforcement learning  ( Cheng et al. ,  2021 ) .",
            "Previously,  Klissarov et al.   ( 2024 )  employed AI feedback to design an effective reward function for an agent operating in the open-ended environment of NetHack. However, before applying this reward to the RL agent, the authors implemented the following transformation:",
            "where  r A  I  F subscript r A I F r_{AIF} italic_r start_POSTSUBSCRIPT italic_A italic_I italic_F end_POSTSUBSCRIPT  is the reward model obtained from AI feedback,  N  ( o t ) N subscript o t N(o_{t}) italic_N ( italic_o start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  denotes the number of times a particular observation  o t subscript o t o_{t} italic_o start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  was seen in an episode, and    \\beta italic_  is a positive real-valued coefficient set to  3 3 3 3 . The counting term was added to encourage exploration  ( Henaff et al. ,  2022 ) , which is a key difficulty in NetHack. However, instantiating such a counting function proves difficult in many practical settings  ( Bellemare et al. ,  2016 ) . Given the flexibility of natural language, can we alleviate the need for such a term and integrate the notion of exploration in the prompt itself?",
            "In  Figure   5 , we demonstrate that this is indeed possible, leading to performance comparable when using count-based exploration by directly modifying the prompt used for preference elicitation. Specifically, when querying the LLM for preferences, we present it with a pair of sequences of observations (rather than a single observation) which provides crucial context. The prompt was also modified to steer the LLM towards avoiding low entropy sequences, i.e. sequences with repetitions (see Appendix  A.2 ).",
            "Our findings reveal two potential failure modes: the offline nature of the preference elicitation method and the assumption of a Markovian reward model. Previous research has demonstrated that online preference querying can outperform offline methods when aligning LLMs  ( Bai et al. ,  2022 ;  Touvron et al. ,  2023 ) . In our experiments, offline elicitation led to a performance collapse, likely due to frequent RL policy updates during online learning. Additionally, assuming a Markov reward modelwhere the current observation fully determines the rewardcan lead to an equally poor performance, as complex tasks often require historical context beyond immediate observations (see Appendix  A.8  for a full breakdown).",
            "We therefore fine-tune PaliGemma on image-caption pairs annotated by GPT-4o and trained the model to predict the caption for a given image.  Figure   6(a)  shows significant gains in downstream RL performance after only a few fine-tuning epochs and as few as approximately  100 100 100 100  image-caption pairs. Moreover,  Figure   6(a)  shows how this procedure only marginally decreases performance of the LLM on the standard multi-modal reasoning benchmarks, such as POPE  ( Yifan Li & Wen ,  2023 ) , GQA  ( Hudson & Manning ,  2019 ) , AI2D  ( Kembhavi et al. ,  2016 )  and MMMU  ( Yue et al. ,  2024 ) . Surprisingly, performance on the AI2D benchmark  improves  as the number of task-specific fine-tuning epochs increases.",
            "We contrast these findings with  Figure   6(b) , where we fine-tune PaliGemma with behaviour cloning on expert data on the same MetaWorld task. Similarly to RT-2  ( Brohan et al. ,  2023 ) , we overwrite the least frequent tokens with residual VQ-VAE codebooks  ( Szot et al. ,  2024 ) . In this case, any significant increase of RL performance comes at the cost of catastrophically forgetting all previous knowledge. These results hint at an important trade-off: if preserving prior language reasoning knowledge is important, fine-tuning for AI feedback offers a viable approach. However, if maximizing downstream RL performance is the sole objective, directly fine-tuning for action selection can be more effective.",
            "Some notable limitations and caveats exist. For example, interacting with LLMs through natural language requires experimenting with various prompting techniques and specifications. However, this flexibility also enables the shaping of reward functions to incorporate valuable strategies  ( Knox et al. ,  2013 ) , such as promoting exploration, which can further enhance the performance of RL agents.",
            "In our experiments, we investigate tasks from four different domains: MiniWob  ( Liu et al. ,  2018 ) , NetHack  ( Kuttler et al. ,  2020 ) , and Wordle  ( Lokshtanov & Subercaseaux ,  2022 ) , and MetaWorld  ( Yu et al. ,  2019 ) . The observation space for all these environments is text, except fro MetaWorld which consists of RGB pixels.",
            "In the MiniWob domain, we sample the subset of the five tasks on which state-of-the-art results are low. Specifically, we carry experiments on:  click-tab-2-hard ,  click-checkboxes-soft ,  count-shape ,  tic-tac-toe  and  use-autocomplete . To learn RL policies from LLM-based rewards, we leverage the experimental setup of  Shaw et al.   ( 2023 ) . In NetHack, we use the same environment and the same algorithmic setup as in  Klissarov et al.   ( 2024 ) . In Wordle, we build on the code made available by  Snell et al.   ( 2022 )  and use their proposed subset of 200 words from the official list of the game. Finally, in MetaWorld we study the same subset of environments presented in  ( Wang et al. ,  2024 )  consisting of  drawer-open-v2 ,  soccer-v2  and  sweep-into-v2 . Across all experiments where RL policies are learned, we use the original hyperparameter values defined in the respective experimental setups we are building upon.",
            "We use the following prompt templates to query the agent for AI feedback, Scalar Reward and Reward as Code across various environments. For the Embedding-based approach, we use calculate the cosine similarity between the representation, provided by a BERT  ( Devlin et al. ,  2019 )  sentence encoder (specifically the same  paraphrase-MiniLM-L3-v2  model) when environments are text-based, and otherwise we use the CLIP encoder  ( Radford et al. ,  2021 ) . The similarity is measured between the current observation and the same goal description contained in the each of the following prompts given for the other baselines.",
            "Additionally, in Figure  7 , we ablate the prompting techniques used in our direct policy modeling approach. Results show that a combination of all prompting techniques presened in Section  2.1  works best.",
            "Many of the above could theoretically be used to construct a policy, yet a full implementation is out of scope from this paper due to the lack of available code-bases to build upon and we do not seek to build new algorithms from scratch. However, in Figure  2(b)  we perform investigations into the capabilities of LLMs to perform Action Preference and State Preference. The results show that current LLMs struggle to achieve strong performance on any of these tasks. Additionally, in Table  1 , we report the accuracy with which LLMs directly predicts the next observation (Direct State Generation), providing a probe into their direct world modeling capabilities. Results show limited performance, except on MiniWob-Hard tasks, which are fully observable and encode deterministic transitions.",
            "In Table  2 , we ablate the performance of the Reward as Code baseline across LLMs, observation spaces and additional assumptions. For pixel observations, we follow the methodology laid out in  ( Venuto et al. ,  2024 ) , whereas for proprioceptive observations we follow the one from  ( Yu et al. ,  2023 ) . Both methods heavily depend on access to a state-of-the-art, closed-source model to achieve performance comparable to that of AI Feedback, which uses the smaller, open-source model of Paligemma  ( Beyer et al. ,  2024 ) . Additionally, each method requires expert demonstrations or specialized domain knowledge to guide the reward design process. While these assumptions may be viable in certain situations, such as in a controlled simulation environment, they can present significant practical challenges in more general contexts. In contrast, AI Feedback operates by simply comparing observations and reasoning using a chain-of-thought approach.",
            "While prior works have shown that rewards can be extracted from a language model  ( Brooks et al. ,  2024 ;  Klissarov et al. ,  2024 ) , it can be more generally thought of as encoding a heuristic function  h h h italic_h . The function  h h h italic_h  contains high-level, multi-step information about the MDP  M M M italic_M . To extract it, one can solve the re-shaped MDP  M ~ ~ M \\tilde{M} over~ start_ARG italic_M end_ARG  with  r ~  ( s t , a t ) = r  ( s t , a t ) + ( 1   )    E s t + 1 | s t , a t  [ h  ( s t + 1 ) ] ~ r subscript s t subscript a t r subscript s t subscript a t 1   subscript E conditional subscript s t 1 subscript s t subscript a t delimited-[] h subscript s t 1 \\tilde{r}(s_{t},a_{t})=r(s_{t},a_{t})+(1-\\lambda)\\gamma\\mathbb{E}_{s_{t+1}|s_{% t},a_{t}}[h(s_{t+1})] over~ start_ARG italic_r end_ARG ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = italic_r ( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) + ( 1 - italic_ ) italic_ blackboard_E start_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT | italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ italic_h ( italic_s start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ) ]  and   ~ =    ~    \\tilde{\\gamma}=\\lambda\\gamma over~ start_ARG italic_ end_ARG = italic_ italic_  where    [ 0 , 1 ]  0 1 \\lambda\\in[0,1] italic_  [ 0 , 1 ]   Cheng et al.   ( 2021 ) . Solving  M ~ ~ M \\tilde{M} over~ start_ARG italic_M end_ARG  yields a policy    superscript  \\pi^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  that is also optimal in  M M M italic_M  - its value functions bias can be shown to converge to  V  superscript V V^{*} italic_V start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  in  M M M italic_M  as a function of   h  V    subscript norm h superscript V ||h-V^{*}||_{\\infty} | | italic_h - italic_V start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT | | start_POSTSUBSCRIPT  end_POSTSUBSCRIPT .",
            "Specifically, assume access to an initial dataset  D 0 subscript D 0 \\mathcal{D}_{0} caligraphic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , from which a heuristic  h h h italic_h  can be computed. In the reshaped MDP  M ~ ~ M \\tilde{M} over~ start_ARG italic_M end_ARG , one can learn a new policy    \\pi italic_  which optimizes  r ~ ~ r \\tilde{r} over~ start_ARG italic_r end_ARG  with    [ 0 , 1 ]  0 1 \\lambda\\in[0,1] italic_  [ 0 , 1 ] .  Equation   5  shows the performance difference lemma  Kakade & Langford   ( 2002 )  as a function of true and reshaped MDP quantities:",
            "where  c 1 , c 2 , c 3 subscript c 1 subscript c 2 subscript c 3 c_{1},c_{2},c_{3} italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT  are non-negative constants. Minimizing  L  (  , h ) L  h \\mathcal{L}(\\pi,h) caligraphic_L ( italic_ , italic_h )  with respect to    \\pi italic_  and  h h h italic_h  can be achieved by minimizing each individual term. In particular, the red term suggests that the heuristic  h h h italic_h  has to be updated on data from  D  superscript D  \\mathcal{D}^{\\pi} caligraphic_D start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  in order to not become \"stale\". This points out a shortcoming of existing LLM-as-critic algorithms, which sometimes fix  h h h italic_h  after distilling the language model knowledge into it  Klissarov et al.   ( 2024 )",
            "To test this, we query  Gemini-1.5 Pro   ( Team et al. ,  2023 )  with a context video containing 500 frames of an agent exploring the bottom-left room ( Figure   11 -left) and a single frame sampled uniformly at random from a query episode which covers in the top-right room, center and bottom of the maze ( Figure   11 -middle). We ask the LLM to identify novel query states, i.e. states which are not seen in the context episode. We then train a direct predictor (3-layer MLP) to estimate the probability of any state on the grid to be novel with respect to the context ( Figure   11 -right). The language model correctly identifies the top-right portion of the trajectory to be novel, knowledge which could then be used to construct an intrinsic reward function.",
            "Large language models (LLMs) require additional adaptation for general-use language tasks  ( Christiano et al. ,  2017 ;  Stiennon et al. ,  2020 ;  Ouyang et al. ,  2022 ;  Mialon et al. ,  2023 ) . Without additional context and/or fine-tuning, LLMs can generate misleading, harmful, or even nonsensical answers to queries or conversations with humans  ( Bai et al. ,  2022 ) . To modify their behavior, it is necessary to tune their prompts and/or fine-tune their outputs to ensure their output is desirable w.r.t. some set of linguistic tasks before deployment. This at least if not more true in embodied settings, where real-world actions can have physical consequences, and methodologies for modifying LLM behavior in embodied settings more-or-less align with efforts in the language space.",
            "Arguably the most common theme among techniques that modify LLM behavior in general is to change the prompt such that the distribution of LLM outputs better-fits a given desiderata on behavior. Prompt-engineering can greatly align or calibrate an LLM, pretrained or no, to desired beneficial behavior  ( Christiano et al. ,  2017 ;  Glaese et al. ,  2022 ;  Bai et al. ,  2022 ) , or even expose harmful or other unexpected behaviors. Chain-of-thought  (CoT,   Wei et al. ,  2022 )  is an in-context method to either few-shot or zero-shot  ( Kojima et al. ,  2022 )  adjust an LLMs outputs to generate more correct responses to question-and-answering tasks. Further modifications to the prompt such as providing feedback from an environment  ( Yao et al. ,  2022 ) , self-critique  ( Zelikman et al. ,  2022 ) , or self-reflection  ( Shinn et al. ,  2023 )  can improve LLM performance in language as well as tasks that have an environment. The biggest promise of in-context-based methods in RL is that somewhere within the large language models conditional distribution is the optimal policy for any given task  ( Brohan et al. ,  2023 ;  Szot et al. ,  2023 ) , an accurate world-explicit model  ( Lin et al. ,  2024 ) , and/or a useful reward-model  ( Klissarov et al. ,  2024 ) . However, it is at best speculative as LLMs are black box systems and prompt optimization is extremely difficult, and besides: systems built on this idea still must still overcome affordance mismatch  ( Ahn et al. ,  2022 )  and hallucinations  ( Zhang et al. ,  2024a )  to be useful for RL.",
            "Querying model for feedback Another hypothesis is that LLMs contain knowledge relevant to tasks, and this knowledge can be extracted  ( Xu et al. ,  2024 )  in a way to train a policy that has desirable behavior  ( Huang et al. ,  2022 ) . RL AI Feedback  (RLAIF  Bai et al. ,  2022 ;  Lee et al. ,  2023 )  is a scalable method akin to but without the practical issues that come paired with RL from Human Feedback  (RLHF  Christiano et al. ,  2017 ) , the goal of which is to fine-tune an existing LLM to be more specific, accurate, innocuous, etc. RLAIF trains a reward model on a dataset collected from an LLMs preferences given a dataset of language responses from an LLM and a given set of queries, and this reward model is used to train a policy using RL, for example using PPO. This process of extracting knowledge using preference data can also be directly used to train a policy without a reward model  ( Rafailov et al. ,  2024 ) ."
        ]
    }
}