{
    "Sx6.T1.3.1": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"Sx6.T1.3.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"Sx6.T1.3.1.1.1\" style=\"background-color:#D9D9D9;\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"Sx6.T1.3.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx6.T1.3.1.1.1.1.1\" style=\"background-color:#D9D9D9;\">Choice of MAT</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"Sx6.T1.3.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx6.T1.3.1.1.1.2.1\" style=\"background-color:#D9D9D9;\">Required data</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"Sx6.T1.3.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx6.T1.3.1.1.1.3.1\" style=\"background-color:#D9D9D9;\">Efficiency</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"Sx6.T1.3.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"Sx6.T1.3.1.1.1.4.1\" style=\"background-color:#D9D9D9;\">Performance (% mIoU)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"Sx6.T1.3.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"Sx6.T1.3.1.2.1.1\" style=\"background-color:#D9D9D9;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.2.1.1.1\" style=\"background-color:#D9D9D9;\">Training from scratch</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"Sx6.T1.3.1.2.1.2\" style=\"background-color:#CCFFCC;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.2.1.2.1\" style=\"background-color:#CCFFCC;\">Target data only</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"Sx6.T1.3.1.2.1.3\" style=\"background-color:#CCFFCC;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.2.1.3.1\" style=\"background-color:#CCFFCC;\">Efficient</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"Sx6.T1.3.1.2.1.4\" style=\"background-color:#CCFFCC;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.2.1.4.1\" style=\"background-color:#CCFFCC;\">25.43 (Bad performance)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx6.T1.3.1.3.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"Sx6.T1.3.1.3.2.1\" style=\"background-color:#D9D9D9;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.3.2.1.1\" style=\"background-color:#D9D9D9;\">Querying transformer in BLIP-2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.08083v1#bib.bib41\" title=\"\">41</a>]</cite></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"Sx6.T1.3.1.3.2.2\" style=\"background-color:#FFCCCC;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.3.2.2.1\" style=\"background-color:#FFCCCC;\">Source-target pair</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"Sx6.T1.3.1.3.2.3\" style=\"background-color:#FFCCCC;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.3.2.3.1\" style=\"background-color:#FFCCCC;\">NA (Not applicable)</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"Sx6.T1.3.1.3.2.4\" style=\"background-color:#FFCCCC;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.3.2.4.1\" style=\"background-color:#FFCCCC;\">NA (Not applicable)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx6.T1.3.1.4.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"Sx6.T1.3.1.4.3.1\" style=\"background-color:#D9D9D9;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.4.3.1.1\" style=\"background-color:#D9D9D9;\">Dimension alignment in ORCA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.08083v1#bib.bib30\" title=\"\">30</a>]</cite></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"Sx6.T1.3.1.4.3.2\" style=\"background-color:#FFCCCC;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.4.3.2.1\" style=\"background-color:#FFCCCC;\">Source-target pair</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"Sx6.T1.3.1.4.3.3\" style=\"background-color:#FFCCCC;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.4.3.3.1\" style=\"background-color:#FFCCCC;\">NA (Not applicable)</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"Sx6.T1.3.1.4.3.4\" style=\"background-color:#FFCCCC;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.4.3.4.1\" style=\"background-color:#FFCCCC;\">NA (Not applicable)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx6.T1.3.1.5.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"Sx6.T1.3.1.5.4.1\" style=\"background-color:#D9D9D9;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.5.4.1.1\" style=\"background-color:#D9D9D9;\">Randomly Initialized Patch Embedding&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.08083v1#bib.bib26\" title=\"\">26</a>]</cite></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"Sx6.T1.3.1.5.4.2\" style=\"background-color:#CCFFCC;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.5.4.2.1\" style=\"background-color:#CCFFCC;\">Target data only</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"Sx6.T1.3.1.5.4.3\" style=\"background-color:#CCFFCC;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.5.4.3.1\" style=\"background-color:#CCFFCC;\">Efficient</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"Sx6.T1.3.1.5.4.4\" style=\"background-color:#FFCCCC;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.5.4.4.1\" style=\"background-color:#FFCCCC;\">58.89 (Bad performance)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx6.T1.3.1.6.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"Sx6.T1.3.1.6.5.1\" style=\"background-color:#D9D9D9;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.6.5.1.1\" style=\"background-color:#D9D9D9;\">Linear Layer + Trainable Patch Embedding&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.08083v1#bib.bib40\" title=\"\">40</a>]</cite></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"Sx6.T1.3.1.6.5.2\" style=\"background-color:#CCFFCC;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.6.5.2.1\" style=\"background-color:#CCFFCC;\">Target data only</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"Sx6.T1.3.1.6.5.3\" style=\"background-color:#CCFFCC;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.6.5.3.1\" style=\"background-color:#CCFFCC;\">Efficient</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"Sx6.T1.3.1.6.5.4\" style=\"background-color:#FFCCCC;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.6.5.4.1\" style=\"background-color:#FFCCCC;\">63.96 (Moderate Performance)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx6.T1.3.1.7.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"Sx6.T1.3.1.7.6.1\" style=\"background-color:#D9D9D9;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.7.6.1.1\" style=\"background-color:#D9D9D9;\">Transformers + Trainable Patch Embedding&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.08083v1#bib.bib106\" title=\"\">106</a>]</cite></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"Sx6.T1.3.1.7.6.2\" style=\"background-color:#CCFFCC;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.7.6.2.1\" style=\"background-color:#CCFFCC;\">Target data only</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"Sx6.T1.3.1.7.6.3\" style=\"background-color:#FFCCCC;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.7.6.3.1\" style=\"background-color:#FFCCCC;\">More parameters</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"Sx6.T1.3.1.7.6.4\" style=\"background-color:#FFCCCC;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.7.6.4.1\" style=\"background-color:#FFCCCC;\">26.67 (Bad performance)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx6.T1.3.1.8.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"Sx6.T1.3.1.8.7.1\" style=\"background-color:#D9D9D9;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.8.7.1.1\" style=\"background-color:#D9D9D9;\">Transpose to Batch Dimension&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.08083v1#bib.bib13\" title=\"\">13</a>]</cite></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"Sx6.T1.3.1.8.7.2\" style=\"background-color:#CCFFCC;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.8.7.2.1\" style=\"background-color:#CCFFCC;\">Target data only</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"Sx6.T1.3.1.8.7.3\" style=\"background-color:#FFCCCC;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.8.7.3.1\" style=\"background-color:#FFCCCC;\">Much more FLOPS</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"Sx6.T1.3.1.8.7.4\" style=\"background-color:#CCFFCC;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.8.7.4.1\" style=\"background-color:#CCFFCC;\">70.90 (Good performance)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"Sx6.T1.3.1.9.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\" id=\"Sx6.T1.3.1.9.8.1\" style=\"background-color:#D9D9D9;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.9.8.1.1\" style=\"background-color:#D9D9D9;\">Ours</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\" id=\"Sx6.T1.3.1.9.8.2\" style=\"background-color:#CCFFCC;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.9.8.2.1\" style=\"background-color:#CCFFCC;\">Target data only</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\" id=\"Sx6.T1.3.1.9.8.3\" style=\"background-color:#CCFFCC;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.9.8.3.1\" style=\"background-color:#CCFFCC;\">Efficient</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\" id=\"Sx6.T1.3.1.9.8.4\" style=\"background-color:#CCFFCC;\"><span class=\"ltx_text\" id=\"Sx6.T1.3.1.9.8.4.1\" style=\"background-color:#CCFFCC;\">72.69 (Best performance)</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 1:  Exploring Modality-agnostic Transfer Layers.   Randomly initializing a patch embedding for each modality leads to worse results than any method that inherits the vision embedding layer. A simple  1 × 1 1 1 1\\times 1 1 × 1  convolution can improve the performance already. Interestingly, when the pretrained vision embedding layer is used, the performance would be better if we fixed the weights. All models are trained with Adapter finetuning strategy.",
        "footnotes": [
            "[41] \nLi, J., Li, D., Savarese, S. & Hoi, S.\n\n Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.\n\n In  International conference on machine learning , 19730–19742 (PMLR, 2023).\n\n",
            "[30] \nShen, J.  et al. \n Cross-modal fine-tuning: Align then refine.\n\n In  International Conference on Machine Learning , 31030–31056 (PMLR, 2023).\n\n",
            "[26] \nLu, K., Grover, A., Abbeel, P. & Mordatch, I.\n\n Frozen pretrained transformers as universal computation engines.\n\n In  Proceedings of the AAAI Conference on Artificial Intelligence , vol. 36, 7628–7636 (2022).\n\n",
            "[40] \nLiu, H., Li, C., Wu, Q. & Lee, Y. J.\n\n Visual instruction tuning.\n\n \\JournalTitle Advances in neural information processing systems   36  (2024).\n\n",
            "[106] \nZhang, J.  et al. \n Cmx: Cross-modal fusion for rgb-x semantic segmentation with transformers.\n\n \\JournalTitle IEEE Transactions on Intelligent Transportation Systems  (2023).\n\n",
            "[13] \nWu, J.  et al. \n Medical sam adapter: Adapting segment anything model for medical image segmentation.\n\n \\JournalTitle arXiv preprint arXiv:2304.12620  (2023).\n\n"
        ],
        "references": [
            "Table 1 shows different existing methods for solving the dimension misalignments in different tasks. While some representative works like ORCA [30] and BLIP-2 [41] propose methods for aligning dimensions, they require source-target paired data (e.g., image-text pair), which are not available under our setting. Hence, their modules are not applicable. There are some direct dimension alignment strategies adopted in priors works. We apply these methods in our SimMAT framework but they cannot achieve satisfying performance. (1) Randomly initialized MAT layer. We start by replacing the original patch embedding with a randomly initialized projection layer. This strategy is quite direct and naive, which has been commonly used in prior works [42, 43, 26]. Compared to training from scratch, this implementation fully utilizes the pretrained vision model weights. As a result, mIoU is improved from 25.43% to 58.89% compared to training from scratch, validating the potential of modality-agnostic transfer learning. However, the performance is significantly worse than our proposed strategy achieving 72.69% mIoU. (2) Inherited vision embedding with linear layer. Another common strategy to align different dimensions is using a linear layer (i.e.formulae-sequence𝑖𝑒i.e.italic_i . italic_e ., a fully connected layer). Many prior works adopt this strategy due to its simplicity and efficiency, including LLaVa [40]. With this simple strategy, we improve the mIoU score from 58.89% to 63.96% compared with the randomly initialized MAT layer. However, the performance is still worse than our 72.69%. We believe it is because the transformation of the linear layer is too simple to align two different image modalities, preventing it from achieving satisfying performance. (3) Transpose to batch dimension. Another strategy is to transpose the feature dimension to the batch dimension and process them separately, such as the method used in MedicalSAM [13]. Interestingly, we observe this implementation can achieve better performance than the prior two versions. Specifically, it gets 70.90% on our evaluated dataset, which is close to our results. However, it suffers from a practical resource problem: it is inefficient for some image modalities. Specifically, for single-channel images, the FLOPs of this method are similar to ours. However, it uses around 9×\\times× FLOPs compared with our MAT layer when using polarization images, which makes it quite challenging to train the models for many areas.",
            "We present a simple yet effective MAT layer for aligning new modalities. As shown in Table 1, our design achieves 72.69% mIoU, which is the best among different strategies. In addition, our MAT is also quite efficient. Different from these methods of building a complex module to bridge the modality gap, we find the two key factors to utilize the vision embedding layer for a new modality. First, the mapping from the novel modality to the pretrained RGB feature space is non-linear. Introducing a linear projection layer fails to achieve satisfactory performance due to limited mapping ability. Instead, SimMAT stacks convolutional layers with ReLU as an intermediate non-linear activation function. A similar conclusion is observed in contrastive learning [44, 45]: replacing the linear layer with MLP as the projection head can achieve better performance. Second, the receptive field is important for novel imaging modalities. It helps capture the cues from neighbor regions and benefit the pixel to learn more rich context; such observation has been well studied and verified in the RGB modality, which inspired us to enlarge the receptive field of SimMAT by setting the convolutional kernel size. Specifically, we stack n convolutional layers with k kernel size and dimension d, we set {n, k, d} to {2, 3, 64} in default since it achieves the best performance, more detailed experiment results are present in supplemental materials."
        ]
    },
    "S1.T2.1.1": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S1.T2.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S1.T2.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S1.T2.1.1.1.1.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Finetuning Strategies</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T2.1.1.1.1.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Trainable Parameters (M)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T2.1.1.2.2\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\" id=\"S1.T2.1.1.2.2.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"/>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T2.1.1.2.2.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">of Foundation Model</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T2.1.1.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S1.T2.1.1.3.3.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">LoRA</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T2.1.1.3.3.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">4.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T2.1.1.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S1.T2.1.1.4.4.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">MLP adapter</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T2.1.1.4.4.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">3.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T2.1.1.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S1.T2.1.1.5.5.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Prompt tuning</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T2.1.1.5.5.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">4.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T2.1.1.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\" id=\"S1.T2.1.1.6.6.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Full finetuning</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S1.T2.1.1.6.6.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">93.7</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 2:    The Number of Trainable Parameters in Foundation Model (SAM) with Different Finetuing Strategies.  Three parameter-efficient finetuning methods hold similar trainable parameters, which are much less than the trainable parameters of full finetuning strategies.",
        "footnotes": [],
        "references": [
            "We report the effect of different finetuning strategies on trainable parameters in Table 2. The foundation model SAM with ViT-B [84] as backbone contains 93.7M parameters from the image encoder, prompt encoder, and mask decoder.\nFull finetuning makes all parameters trainable.\nFor parameter-efficient tuning, we implement four typical methods including LoRA [31], MLP adapter [32], and prompt tuning [46]. Following He et al. [107], we balance their trainable parameters to achieve approximately 4% of full parameters for fair comparison."
        ]
    },
    "S1.T3.2.2.2": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S1.T3.2.2.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S1.T3.2.2.2.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S1.T3.2.2.2.3.1.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Config</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T3.2.2.2.3.1.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Value</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T3.2.2.2.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S1.T3.2.2.2.4.2.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">optimizer</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T3.2.2.2.4.2.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Adam</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T3.2.2.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S1.T3.2.2.2.2.3\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">optimizer momentum</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T3.2.2.2.2.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"\\beta_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T3.1.1.1.1.1.m1.1\"><semantics id=\"S1.T3.1.1.1.1.1.m1.1a\"><msub id=\"S1.T3.1.1.1.1.1.m1.1.1\" xref=\"S1.T3.1.1.1.1.1.m1.1.1.cmml\"><mi id=\"S1.T3.1.1.1.1.1.m1.1.1.2\" xref=\"S1.T3.1.1.1.1.1.m1.1.1.2.cmml\">&#946;</mi><mn id=\"S1.T3.1.1.1.1.1.m1.1.1.3\" xref=\"S1.T3.1.1.1.1.1.m1.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S1.T3.1.1.1.1.1.m1.1b\"><apply id=\"S1.T3.1.1.1.1.1.m1.1.1.cmml\" xref=\"S1.T3.1.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S1.T3.1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S1.T3.1.1.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S1.T3.1.1.1.1.1.m1.1.1.2.cmml\" xref=\"S1.T3.1.1.1.1.1.m1.1.1.2\">&#120573;</ci><cn id=\"S1.T3.1.1.1.1.1.m1.1.1.3.cmml\" type=\"integer\" xref=\"S1.T3.1.1.1.1.1.m1.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.T3.1.1.1.1.1.m1.1c\">\\beta_{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S1.T3.1.1.1.1.1.m1.1d\">italic_&#946; start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>,<math alttext=\"\\beta_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T3.2.2.2.2.2.m2.1\"><semantics id=\"S1.T3.2.2.2.2.2.m2.1a\"><msub id=\"S1.T3.2.2.2.2.2.m2.1.1\" xref=\"S1.T3.2.2.2.2.2.m2.1.1.cmml\"><mi id=\"S1.T3.2.2.2.2.2.m2.1.1.2\" xref=\"S1.T3.2.2.2.2.2.m2.1.1.2.cmml\">&#946;</mi><mn id=\"S1.T3.2.2.2.2.2.m2.1.1.3\" xref=\"S1.T3.2.2.2.2.2.m2.1.1.3.cmml\">2</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S1.T3.2.2.2.2.2.m2.1b\"><apply id=\"S1.T3.2.2.2.2.2.m2.1.1.cmml\" xref=\"S1.T3.2.2.2.2.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S1.T3.2.2.2.2.2.m2.1.1.1.cmml\" xref=\"S1.T3.2.2.2.2.2.m2.1.1\">subscript</csymbol><ci id=\"S1.T3.2.2.2.2.2.m2.1.1.2.cmml\" xref=\"S1.T3.2.2.2.2.2.m2.1.1.2\">&#120573;</ci><cn id=\"S1.T3.2.2.2.2.2.m2.1.1.3.cmml\" type=\"integer\" xref=\"S1.T3.2.2.2.2.2.m2.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.T3.2.2.2.2.2.m2.1c\">\\beta_{2}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S1.T3.2.2.2.2.2.m2.1d\">italic_&#946; start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>=0.9,0.999</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T3.2.2.2.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S1.T3.2.2.2.5.3.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">batch size</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T3.2.2.2.5.3.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T3.2.2.2.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S1.T3.2.2.2.6.4.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">epoch</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T3.2.2.2.6.4.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">50</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T3.2.2.2.7.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S1.T3.2.2.2.7.5.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">learning rate</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T3.2.2.2.7.5.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">{3e-6, 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3}</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T3.2.2.2.8.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S1.T3.2.2.2.8.6.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">learning rate schedule</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T3.2.2.2.8.6.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">step decay</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T3.2.2.2.9.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S1.T3.2.2.2.9.7.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">schedule step size</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T3.2.2.2.9.7.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">10 epoch</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T3.2.2.2.10.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"S1.T3.2.2.2.10.8.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">schedule gamma</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T3.2.2.2.10.8.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T3.2.2.2.11.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\" id=\"S1.T3.2.2.2.11.9.1\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">augmentation</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S1.T3.2.2.2.11.9.2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_text\" id=\"S1.T3.2.2.2.11.9.2.1\">Resize</span>(1024, 1024)</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 3:    The Training Setting for Our Experiments.  ",
        "footnotes": [],
        "references": [
            "The detailed training configuration is presented in Table 3.\nWe fix the training epoch to 50 and set the batch size as 4 regardless of the number of training samples in different modality datasets.\nWe sweep the learning rates from 3e-6 to 3e-3 and report the peak performance as the final result.\nThe input modality images are resized to (1024, 1024) to meet the requirements of SAM."
        ]
    },
    "S3.T4.1.1": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S3.T4.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T4.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"S3.T4.1.1.1.1.1\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.1.1.1.1\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row ltx_border_tt\" id=\"S3.T4.1.1.1.1.2\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.1.1.2.1\" style=\"font-size:90%;\">Params</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"S3.T4.1.1.1.1.3\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.1.1.3.1\" style=\"font-size:90%;\">Finetuning methods</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_tt\" id=\"S3.T4.1.1.1.1.4\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.1.1.4.1\" style=\"font-size:90%;\">RGB-T</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_tt\" id=\"S3.T4.1.1.1.1.5\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.1.1.5.1\" style=\"font-size:90%;\">RGB-D</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_tt\" id=\"S3.T4.1.1.1.1.6\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.1.1.6.1\" style=\"font-size:90%;\">RGB-HHA</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_tt\" id=\"S3.T4.1.1.1.1.7\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.1.1.7.1\" style=\"font-size:90%;\">RGB-NIR</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.1.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T4.1.1.2.2.1\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\">\n<span class=\"ltx_text\" id=\"S3.T4.1.1.2.2.1.1\" style=\"font-size:90%;\">CMX*&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S3.T4.1.1.2.2.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.08083v1#bib.bib106\" title=\"\">106</a><span class=\"ltx_text\" id=\"S3.T4.1.1.2.2.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S3.T4.1.1.2.2.2\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.2.2.2.1\" style=\"font-size:90%;\">403.8M</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T4.1.1.2.2.3\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.2.2.3.1\" style=\"font-size:90%;\">Full finetuning</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" id=\"S3.T4.1.1.2.2.4\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.2.2.4.1\" style=\"font-size:90%;\">44.91</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" id=\"S3.T4.1.1.2.2.5\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.2.2.5.1\" style=\"font-size:90%;\">36.41</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" id=\"S3.T4.1.1.2.2.6\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.2.2.6.1\" style=\"font-size:90%;\">37.33</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" id=\"S3.T4.1.1.2.2.7\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.2.2.7.1\" style=\"font-size:90%;\">34.75</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.1.1.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T4.1.1.3.3.1\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\">\n<span class=\"ltx_text\" id=\"S3.T4.1.1.3.3.1.1\" style=\"font-size:90%;\">ViPT*&#160;</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" id=\"S3.T4.1.1.3.3.1.2.1\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.08083v1#bib.bib33\" title=\"\">33</a><span class=\"ltx_text\" id=\"S3.T4.1.1.3.3.1.3.2\" style=\"font-size:90%;\">]</span></cite>\n</th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row\" id=\"S3.T4.1.1.3.3.2\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.3.3.2.1\" style=\"font-size:90%;\">94.5M</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row\" id=\"S3.T4.1.1.3.3.3\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.3.3.3.1\" style=\"font-size:90%;\">Prompt tuning</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" id=\"S3.T4.1.1.3.3.4\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.3.3.4.1\" style=\"font-size:90%;\">75.93</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" id=\"S3.T4.1.1.3.3.5\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.3.3.5.1\" style=\"font-size:90%;\">48.89</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" id=\"S3.T4.1.1.3.3.6\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.3.3.6.1\" style=\"font-size:90%;\">49.50</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" id=\"S3.T4.1.1.3.3.7\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.3.3.7.1\" style=\"font-size:90%;\">51.90</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.1.1.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T4.1.1.4.4.1\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.4.4.1.1\" style=\"font-size:90%;\">SimMAT</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S3.T4.1.1.4.4.2\" rowspan=\"3\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.4.4.2.1\" style=\"font-size:90%;\">94.4M</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T4.1.1.4.4.3\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.4.4.3.1\" style=\"font-size:90%;\">LoRA</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" id=\"S3.T4.1.1.4.4.4\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.4.4.4.1\" style=\"font-size:90%;\">84.52</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" id=\"S3.T4.1.1.4.4.5\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.4.4.5.1\" style=\"font-size:90%;\">57.56</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" id=\"S3.T4.1.1.4.4.6\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.4.4.6.1\" style=\"font-size:90%;\">56.44</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\" id=\"S3.T4.1.1.4.4.7\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T4.1.1.4.4.7.1\" style=\"font-size:90%;\">57.14</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.1.1.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T4.1.1.5.5.1\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.5.5.1.1\" style=\"font-size:90%;\">SimMAT</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row\" id=\"S3.T4.1.1.5.5.2\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.5.5.2.1\" style=\"font-size:90%;\">MLP Adapter</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" id=\"S3.T4.1.1.5.5.3\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T4.1.1.5.5.3.1\" style=\"font-size:90%;\">85.29</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" id=\"S3.T4.1.1.5.5.4\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T4.1.1.5.5.4.1\" style=\"font-size:90%;\">57.73</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" id=\"S3.T4.1.1.5.5.5\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T4.1.1.5.5.5.1\" style=\"font-size:90%;\">57.25</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\" id=\"S3.T4.1.1.5.5.6\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.5.5.6.1\" style=\"font-size:90%;\">55.81</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T4.1.1.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T4.1.1.6.6.1\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.6.6.1.1\" style=\"font-size:90%;\">SimMAT</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T4.1.1.6.6.2\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.6.6.2.1\" style=\"font-size:90%;\">Full finetuning</span></th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_bb\" id=\"S3.T4.1.1.6.6.3\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.6.6.3.1\" style=\"font-size:90%;\">82.68</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_bb\" id=\"S3.T4.1.1.6.6.4\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.6.6.4.1\" style=\"font-size:90%;\">56.96</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_bb\" id=\"S3.T4.1.1.6.6.5\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.6.6.5.1\" style=\"font-size:90%;\">57.17</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_bb\" id=\"S3.T4.1.1.6.6.6\" style=\"padding-top:0.9pt;padding-bottom:0.9pt;\"><span class=\"ltx_text\" id=\"S3.T4.1.1.6.6.6.1\" style=\"font-size:90%;\">56.37</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 4:  Comparison of SimMAT with Other Methods Tackling Pseudo New Modality (RGBX).  While with fewer parameters, SimMAT achieves better performance across four pseudo new modalities. Note that ViPT and CMX can tackle RGBX only. * means reproduced implementation in SAM.\n",
        "footnotes": [
            "[106] \nZhang, J.  et al. \n Cmx: Cross-modal fusion for rgb-x semantic segmentation with transformers.\n\n \\JournalTitle IEEE Transactions on Intelligent Transportation Systems  (2023).\n\n",
            "[33] \nZhu, J., Lai, S., Chen, X., Wang, D. & Lu, H.\n\n Visual prompt multi-modal tracking.\n\n In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 9516–9526 (2023).\n\n"
        ],
        "references": [
            "Besides, we compare our SimMAT to two SOTA methods with pseudo new modality (RGBX) input.\nViPT [33] introduce a modality-complementary prompter (MCP) block to fuse features from RGB and other modalities like thermal and depth.\nCMX [106] replicate the pretrained RGB encoder to tackle X modality, and place the proposed Feature Rectification Module (FRM) after each block to perform interaction of RGB features and X features. Note that these two baselines utilize the prior information about which channels are for RGB embedding while our framework does not utilize this information.\nWe reimplement the above two methods on SAM following their original finetuning methods and evaluate their performance on our benchmark.\nAs shown in Table 4, CMX [106] does not achieve satisfying performance on finetuning the foundation model SAM. We suspect the unsatisfying performance is caused by the noise introduced from FRM, which appended after each block deviates the features from its original distribution, making the learning difficult.\nWhile ViPT [33] can achieve reasonable performance, its performance lags behind SimMAT."
        ]
    }
}