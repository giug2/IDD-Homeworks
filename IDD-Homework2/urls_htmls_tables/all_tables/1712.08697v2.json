{
    "S3.T1": {
        "caption": "Table 1:  Size breakdown of HowMany-QA. Neither development or test included VG data.",
        "table": "<table id=\"S3.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Split</th>\n<th id=\"S3.T1.1.1.1.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">QA Pairs</th>\n<th id=\"S3.T1.1.1.1.3\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">Images</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">Train</th>\n<td id=\"S3.T1.1.2.1.2\" class=\"ltx_td ltx_align_right ltx_border_tt\">83,642</td>\n<td id=\"S3.T1.1.2.1.3\" class=\"ltx_td ltx_align_right ltx_border_tt\">31,932</td>\n</tr>\n<tr id=\"S3.T1.1.3.2\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">   <span id=\"S3.T1.1.3.2.1.1\" class=\"ltx_text ltx_font_italic\">from VQA 2.0</span>\n</th>\n<td id=\"S3.T1.1.3.2.2\" class=\"ltx_td ltx_align_right\">47,542</td>\n<td id=\"S3.T1.1.3.2.3\" class=\"ltx_td ltx_align_right\">31,932</td>\n</tr>\n<tr id=\"S3.T1.1.4.3\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">   <span id=\"S3.T1.1.4.3.1.1\" class=\"ltx_text ltx_font_italic\">from VG</span>\n</th>\n<td id=\"S3.T1.1.4.3.2\" class=\"ltx_td ltx_align_right\">36,100</td>\n<td id=\"S3.T1.1.4.3.3\" class=\"ltx_td ltx_align_right\">0</td>\n</tr>\n<tr id=\"S3.T1.1.5.4\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Dev.</th>\n<td id=\"S3.T1.1.5.4.2\" class=\"ltx_td ltx_align_right ltx_border_t\">17,714</td>\n<td id=\"S3.T1.1.5.4.3\" class=\"ltx_td ltx_align_right ltx_border_t\">13,119</td>\n</tr>\n<tr id=\"S3.T1.1.6.5\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">Test</th>\n<td id=\"S3.T1.1.6.5.2\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">5,000</td>\n<td id=\"S3.T1.1.6.5.3\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">2,483</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "As mentioned above, the HowMany-QA training data is augmented with available QA pairs from Visual Genome, which are selected using the same criteria.\nA breakdown of the size and composition of HowMany-QA is provided in Table  1.\nAll models compared in this work are trained and evaluated on HowMany-QA.\nTo facilitate future comparison to our work, we have made the training, development, and test question IDs available for download."
        ]
    },
    "S5.T2": {
        "caption": "Table 2: HowMany-QA test set performance. Values in parentheses apply to models trained without caption grounding.",
        "table": "<table id=\"S5.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.1.1.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt\">Model</th>\n<td id=\"S5.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">Accuracy</td>\n<td id=\"S5.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">RMSE</td>\n</tr>\n<tr id=\"S5.T2.1.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.2.2.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t\">Guess1</th>\n<td id=\"S5.T2.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">33.8</td>\n<td id=\"S5.T2.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">3.74</td>\n</tr>\n<tr id=\"S5.T2.1.3.3\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.3.3.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">LSTM</th>\n<td id=\"S5.T2.1.3.3.2\" class=\"ltx_td ltx_align_center\">36.8</td>\n<td id=\"S5.T2.1.3.3.3\" class=\"ltx_td ltx_align_center\">3.47</td>\n</tr>\n<tr id=\"S5.T2.1.4.4\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.4.4.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t\">SoftCount</th>\n<td id=\"S5.T2.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\">50.2 (49.2)</td>\n<td id=\"S5.T2.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S5.T2.1.4.4.3.1\" class=\"ltx_text ltx_font_bold\">2.37</span> (2.45)</td>\n</tr>\n<tr id=\"S5.T2.1.5.5\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.5.5.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">UpDown</th>\n<td id=\"S5.T2.1.5.5.2\" class=\"ltx_td ltx_align_center\">52.7 (51.5)</td>\n<td id=\"S5.T2.1.5.5.3\" class=\"ltx_td ltx_align_center\">2.64 (2.69)</td>\n</tr>\n<tr id=\"S5.T2.1.6.6\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.6.6.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb\">IRLC</th>\n<td id=\"S5.T2.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"S5.T2.1.6.6.2.1\" class=\"ltx_text ltx_font_bold\">57.7</span> (56.1)</td>\n<td id=\"S5.T2.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"S5.T2.1.6.6.3.1\" class=\"ltx_text ltx_font_bold\">2.37</span> (2.45)</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "IRLC achieves the highest overall accuracy and (with SoftCount) the lowest overall RMSE on the test set (Table 2).\nInterestingly, SoftCount clearly lags in accuracy but is competitive in RMSE, arguing that accuracy and RMSE are not redundant.\nWe observe this to result from the fact that IRLC is less prone to small errors and very slightly more prone to large errors (which disproportionately impact RMSE).\nHowever, whereas UpDown improves in accuracy at the cost of RMSE, IRLC is substantially more accurate without sacrificing overall RMSE."
        ]
    }
}