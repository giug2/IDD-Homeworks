{
    "PAPER'S NUMBER OF TABLES": 2,
    "S1.T1": {
        "caption": "TABLE I: One of our main results: Combining selection-based algorithms with aggregation-based algorithms, we compare the performance of FLASH on FEMNIST dataset. Upper: 30% non-IIDness, Lower: 15% label noise. Rows are client selection strategies, columns are aggregation strategies. We highlighted the best and second best one in each column. The average shows that our client selection strategy works better than other selection strategies when combined with a variety of aggregation methods.",
        "table": "",
        "footnotes": "30% non-IIDness\n\n\n\n\n\n\nFedAvg [23]\nFedProx [15]\nFedBiO [24]\nFedDF [25]\nFedNova [26]\nSCAFFOLD [27]\nRHFL [14]\nAverage\n\nRandom [23]\n41.9 ±plus-or-minus\\pm 2.4\n47.9 ±plus-or-minus\\pm 2.5\n50.4 ±plus-or-minus\\pm 2.3\n48.2 ±plus-or-minus\\pm 2.7\n51.4 ±plus-or-minus\\pm 2.3\n48.0 ±plus-or-minus\\pm 2.6\n53.2 ±plus-or-minus\\pm 2.1\n48.7\n\nOort [17]\n42.6 ±plus-or-minus\\pm 1.6\n44.9 ±plus-or-minus\\pm 1.8\n46.8 ±plus-or-minus\\pm 2.3\n51.9 ±plus-or-minus\\pm 1.7\n49.5 ±plus-or-minus\\pm 1.9\n54.8 ±plus-or-minus\\pm 2.1\n57.0 ±plus-or-minus\\pm 1.8\n49.6\n\nPyramidFL [18]\n48.5 ±plus-or-minus\\pm 2.1\n49.3 ±plus-or-minus\\pm 1.7\n50.3 ±plus-or-minus\\pm 1.4\n51.3 ±plus-or-minus\\pm 1.6\n52.1 ±plus-or-minus\\pm 1.8\n49.9 ±plus-or-minus\\pm 1.8\n50.6 ±plus-or-minus\\pm 1.5\n50.3\n\nRestless bandit [28]\n42.8 ±plus-or-minus\\pm 2.3\n44.9 ±plus-or-minus\\pm 3.1\n42.9 ±plus-or-minus\\pm 3.5\n40.6 ±plus-or-minus\\pm 2.7\n43.3 ±plus-or-minus\\pm 2.6\n41.7 ±plus-or-minus\\pm 2.7\n47.3 ±plus-or-minus\\pm 3.1\n43.4\n\nNeural bandit [29]\n50.8 ±plus-or-minus\\pm 2.1\n54.5 ±plus-or-minus\\pm 2.3\n60.1 ±plus-or-minus\\pm 2.3\n47.8 ±plus-or-minus\\pm 2.1\n49.9 ±plus-or-minus\\pm 1.6\n51.5 ±plus-or-minus\\pm 1.5\n51.7 ±plus-or-minus\\pm 1.6\n52.3\n\nFedCor [13]\n55.5 ±plus-or-minus\\pm 1.7\n53.2 ±plus-or-minus\\pm 1.7\n49.4 ±plus-or-minus\\pm 2.3\n61.5 ±plus-or-minus\\pm 2.4\n58.7 ±plus-or-minus\\pm 1.8\n57.8 ±plus-or-minus\\pm 1.8\n60.2 ±plus-or-minus\\pm 1.9\n56.6\n\nFEEL [16]\n54.1 ±plus-or-minus\\pm 1.5\n49.0 ±plus-or-minus\\pm 1.5\n47.7 ±plus-or-minus\\pm 1.8\n49.3 ±plus-or-minus\\pm 1.4\n53.3 ±plus-or-minus\\pm 1.9\n53.0 ±plus-or-minus\\pm 2.1\n51.3 ±plus-or-minus\\pm 1.4\n51.1\n\nFLASH (Ours)\n56.6 ±plus-or-minus\\pm 1.4\n58.0 ±plus-or-minus\\pm 1.6\n61.2 ±plus-or-minus\\pm 1.8\n61.8 ±plus-or-minus\\pm 1.5\n57.9 ±plus-or-minus\\pm 1.2\n57.2 ±plus-or-minus\\pm 1.4\n60.5 ±plus-or-minus\\pm 1.5\n59.0\n\n\n\n15% label noise\n\n\n\n\n\n\nFedAvg [23]\nFedProx [15]\nFedBiO [24]\nFedDF [25]\nFedNova [26]\nSCAFFOLD [27]\nRHFL [14]\nAverage\n\nRandom [23]\n38.9 ±plus-or-minus\\pm 2.2\n45.7 ±plus-or-minus\\pm 2.3\n49.8 ±plus-or-minus\\pm 2.5\n47.6 ±plus-or-minus\\pm 2.3\n50.9 ±plus-or-minus\\pm 2.6\n47.0 ±plus-or-minus\\pm 2.2\n50.6 ±plus-or-minus\\pm 2.1\n47.2\n\nOort [17]\n39.9 ±plus-or-minus\\pm 2.4\n42.3 ±plus-or-minus\\pm 2.1\n43.6 ±plus-or-minus\\pm 1.6\n48.8 ±plus-or-minus\\pm 2.0\n46.2 ±plus-or-minus\\pm 1.8\n53.7 ±plus-or-minus\\pm 1.9\n50.8 ±plus-or-minus\\pm 2.1\n46.4\n\nPyramidFL [18]\n44.4 ±plus-or-minus\\pm 1.9\n46.5 ±plus-or-minus\\pm 2.1\n47.1 ±plus-or-minus\\pm 1.8\n47.9 ±plus-or-minus\\pm 1.7\n50.0 ±plus-or-minus\\pm 2.3\n46.8 ±plus-or-minus\\pm 2.1\n48.3 ±plus-or-minus\\pm 2.7\n47.2\n\nRestless bandit [28]\n40.6 ±plus-or-minus\\pm 2.5\n41.3 ±plus-or-minus\\pm 2.2\n41.7 ±plus-or-minus\\pm 2.4\n38.3 ±plus-or-minus\\pm 2.6\n40.6 ±plus-or-minus\\pm 2.3\n37.6 ±plus-or-minus\\pm 2.7\n41.9 ±plus-or-minus\\pm 3.2\n40.3\n\nNeural bandit [29]\n47.6 ±plus-or-minus\\pm 2.1\n49.0 ±plus-or-minus\\pm 2.6\n46.7 ±plus-or-minus\\pm 2.6\n44.3 ±plus-or-minus\\pm 2.1\n52.2 ±plus-or-minus\\pm 2.2\n51.0 ±plus-or-minus\\pm 2.1\n48.8 ±plus-or-minus\\pm 3.4\n48.5\n\nFedCor [13]\n51.4 ±plus-or-minus\\pm 2.1\n54.8 ±plus-or-minus\\pm 1.5\n56.5 ±plus-or-minus\\pm 1.5\n59.9 ±plus-or-minus\\pm 1.9\n48.4 ±plus-or-minus\\pm 2.2\n47.8 ±plus-or-minus\\pm 2.0\n57.4 ±plus-or-minus\\pm 1.6\n53.7\n\nFEEL [16]\n50.4 ±plus-or-minus\\pm 1.5\n46.7 ±plus-or-minus\\pm 1.2\n43.2 ±plus-or-minus\\pm 1.4\n45.3 ±plus-or-minus\\pm 1.9\n51.0 ±plus-or-minus\\pm 1.4\n50.5 ±plus-or-minus\\pm 1.4\n48.9 ±plus-or-minus\\pm 1.7\n48.0\n\nFLASH (Ours)\n52.1 ±plus-or-minus\\pm 1.6\n58.3 ±plus-or-minus\\pm 1.3\n58.1 ±plus-or-minus\\pm 1.4\n58.8 ±plus-or-minus\\pm 1.6\n54.0 ±plus-or-minus\\pm 1.5\n53.2 ±plus-or-minus\\pm 1.6\n56.4 ±plus-or-minus\\pm 1.4\n55.8\n\n\n",
        "references": [
            [
                "Federated Learning (FL) is a distributed learning paradigm where multiple clients collaborate to train a model without exchanging raw data. Training is typically coordinated by a central server, who selects a subset of clients in each training round to update the (global) model ",
                "[",
                "1",
                ", ",
                "2",
                "]",
                ".\nThe selected clients train the model using their local data, and send their updates to the server, who then aggregates the local updates for the next training round. The process is repeated until convergence. In doing so, FL offers distinct advantages to conventional centralized learning paradigms, including enhanced privacy, reduced communication costs, and scalability ",
                "[",
                "3",
                "]",
                ". On the other hand, FL also presents several unique challenges due to its distributed nature, most importantly handling heterogeneity across clients, ensuring fairness and robustness, and balancing global model accuracy and privacy. Practical challenges surrounding FL have motivated a growing body of work to improve the efficiency of optimization ",
                "[",
                "3",
                ", ",
                "4",
                "]",
                ", communication ",
                "[",
                "5",
                ", ",
                "6",
                ", ",
                "7",
                "]",
                ", and resource allocation ",
                "[",
                "8",
                ", ",
                "9",
                "]",
                ", as well as to address heterogeneity across the clients ",
                "[",
                "10",
                ", ",
                "11",
                "]",
                ", including recent work ",
                "[",
                "12",
                ", ",
                "13",
                ", ",
                "14",
                "]",
                ".",
                "Client heterogeneity is a central challenge in FL which utilizes a diverse set of sources: Each client has unique characteristics in terms of its non-homogeneous label distribution ",
                "[",
                "13",
                ", ",
                "15",
                "]",
                ", unreliable label assignment ",
                "[",
                "14",
                "]",
                ", and latency ",
                "[",
                "16",
                ", ",
                "17",
                ", ",
                "18",
                "]",
                ". Heterogeneity degrades model accuracy ",
                "[",
                "19",
                ", ",
                "20",
                ", ",
                "16",
                "]",
                " and increases the time and resources required to achieve the desired performance ",
                "[",
                "21",
                ", ",
                "18",
                ", ",
                "17",
                "]",
                ". This drives the need for schemes that account for and handle the differences among the participating clients. Existing research suggests that informed client selection can ameliorate the bias introduced by heterogeneity and speed up convergence ",
                "[",
                "22",
                "]",
                ".",
                "Literature on FL heterogeneity can be divided into client aggregation methods ",
                "[",
                "15",
                ", ",
                "24",
                ", ",
                "25",
                ", ",
                "26",
                ", ",
                "27",
                ", ",
                "14",
                "]",
                ", and client selection methods ",
                "[",
                "17",
                ", ",
                "18",
                ", ",
                "28",
                ", ",
                "29",
                ", ",
                "13",
                ", ",
                "16",
                "]",
                ". Among the latter, existing methods can handle mostly one or at most two types of heterogeneities ",
                "[",
                "16",
                ", ",
                "18",
                ", ",
                "17",
                "]",
                ". Thus, while in real-world FL applications multiple heterogeneities coexist, existing client selection methods don’t account for diverse concurrent heterogeneities. There is also a need for client selection algorithms that can be seamlessly integrated with federated aggregation strategies with no loss in performance. These highlight a clear need for a new framework that can handle a range of heterogeneities while also being easily extensible. Motivated by this, we ask:",
                "Q:",
                " How can we select clients systematically under diverse and concurrent sources of heterogeneity to facilitate faster training and better accuracy? Can we combine the benefits of our client selection method with existing aggregation methods?",
                "Main contribution.",
                " Our algorithm ",
                "FLASH",
                " addresses this problem by explicitly modeling the heterogeneous nature of each client as a client context. Context is represented as a feature vector that summarizes a client’s characteristics/metadata, e.g., distributional heterogeneity, label noise, and straggler latency, and is defined precisely in Section ",
                "2.3",
                ". ",
                "FLASH",
                " utilizes context information to select clients based on Contextual Multi-Armed Bandits (CMAB): it treats each client as an arm and selects, in each round, the clients that yield the highest estimated improvement in the global optimization objective. Importantly, it can augment existing federated optimization schemes (as shown in Sec. ",
                "3.5",
                ") and leads to synergistic improvements. To summarize, ",
                "FLASH",
                " has three key features to enable FL under heterogeneous environments with substantial performance gains.\n",
                "∙",
                "∙",
                "\\bullet",
                " ",
                "Simultaneous and diverse heterogeneities:",
                " ",
                "FLASH",
                " addresses general heterogeneities across clients by explicitly modeling them as contextual variables. We consider latencies to the server, variations in label qualities, and distributional heterogeneity in data, and demonstrate ",
                "FLASH",
                "’s ",
                "best of all worlds",
                " performance in terms of latency and accuracy. We are not aware of other works that consider all these ",
                "simultaneous",
                " sources of heterogeneity - an important step towards practical FL algorithms. Our approach facilitates optimal trade-offs between diverse heterogeneities and is applicable to complex scenarios (e.g. clients that have low latency may not have good data quality and vice versa).\n\n",
                "∙",
                "∙",
                "\\bullet",
                " ",
                "Contextual and interpretable framework:",
                " ",
                "FLASH",
                " incorporates a novel CMAB framework that employs contextual features to represent client heterogeneity and to predict the contributions of individual clients to the global accuracy.\nExtensive ablation studies show that ",
                "FLASH",
                " is easy to interpret as it indeed automatically emphasizes the relevant contextual features as the nature and degree of heterogeneity changes (see ablation analysis in Section ",
                "3.5",
                ").\n\n",
                "∙",
                "∙",
                "\\bullet",
                " ",
                "Significant Performance Improvement.",
                " Through extensive experiments, we show that ",
                "FLASH",
                " outperforms existing client selection methods, with up to ",
                "10% improvement",
                " in accuracy over state-of-the-art baselines (see Figure ",
                "3",
                "(c)). Furthermore, when combined with various federated learning aggregation methods, ",
                "FLASH",
                " delivers the best average performance, thereby demonstrating its utility as a generalized client selection strategy (see Table ",
                "II",
                ")."
            ]
        ]
    },
    "S3.T2": {
        "caption": "TABLE II: This table is same as above however evaluations are on CIFAR dataset. ",
        "table": "",
        "footnotes": "30% non-IIDness\n\n\n\n\n\n\nFedAvg [23]\nFedProx [15]\nFedBiO [24]\nFedDF [25]\nFedNova [26]\nSCAFFOLD [27]\nRHFL [14]\nAverage\n\nRandom [23]\n57.1 ±plus-or-minus\\pm 2.4\n64.3 ±plus-or-minus\\pm 2.5\n65.2 ±plus-or-minus\\pm 2.7\n65.5 ±plus-or-minus\\pm 3.5\n67.8 ±plus-or-minus\\pm 3.2\n69.3 ±plus-or-minus\\pm 2.6\n64.7 ±plus-or-minus\\pm 2.7\n64.8\n\nOort [17]\n65.3 ±plus-or-minus\\pm 1.6\n66.5 ±plus-or-minus\\pm 1.4\n67.2 ±plus-or-minus\\pm 1.1\n70.1 ±plus-or-minus\\pm 1.2\n67.2 ±plus-or-minus\\pm 1.1\n69.0 ±plus-or-minus\\pm 1.4\n65.2 ±plus-or-minus\\pm 1.4\n67.2\n\nPyramidFL [18]\n67.1 ±plus-or-minus\\pm 1.1\n68.7 ±plus-or-minus\\pm 1.0\n69.1 ±plus-or-minus\\pm 1.3\n68.0 ±plus-or-minus\\pm 1.5\n69.3 ±plus-or-minus\\pm 1.3\n70.1 ±plus-or-minus\\pm 1.6\n66.5 ±plus-or-minus\\pm 1.2\n68.4\n\nRestless bandit [28]\n66.6 ±plus-or-minus\\pm 2.9\n63.2 ±plus-or-minus\\pm 2.2\n66.1 ±plus-or-minus\\pm 2.5\n67.6 ±plus-or-minus\\pm 2.4\n62.4 ±plus-or-minus\\pm 2.3\n63.2 ±plus-or-minus\\pm 1.6\n64.3 ±plus-or-minus\\pm 2.0\n64.7\n\nNeural bandit [29]\n69.3 ±plus-or-minus\\pm 2.6\n71.2 ±plus-or-minus\\pm 1.8\n69.3 ±plus-or-minus\\pm 2.3\n70.4 ±plus-or-minus\\pm 2.4\n69.0 ±plus-or-minus\\pm 2.1\n71.8 ±plus-or-minus\\pm 2.2\n65.9 ±plus-or-minus\\pm 2.5\n69.6\n\nFedCor [13]\n70.1 ±plus-or-minus\\pm 1.4\n73.2 ±plus-or-minus\\pm 1.5\n73.8 ±plus-or-minus\\pm 1.2\n69.5 ±plus-or-minus\\pm 1.6\n71.8 ±plus-or-minus\\pm 1.6\n72.8 ±plus-or-minus\\pm 1.4\n64.8 ±plus-or-minus\\pm 1.6\n70.8\n\nFEEL [16]\n65.5 ±plus-or-minus\\pm 1.1\n69.4 ±plus-or-minus\\pm 1.5\n67.3 ±plus-or-minus\\pm 1.6\n69.7 ±plus-or-minus\\pm 1.3\n70.4 ±plus-or-minus\\pm 1.4\n64.6 ±plus-or-minus\\pm 1.2\n65.3 ±plus-or-minus\\pm 1.5\n67.4\n\nFLASH (Ours)\n70.3 ±plus-or-minus\\pm 1.1\n71.6 ±plus-or-minus\\pm 1.3\n72.7 ±plus-or-minus\\pm 1.2\n72.2 ±plus-or-minus\\pm 1.1\n73.5 ±plus-or-minus\\pm 1.4\n73.7 ±plus-or-minus\\pm 1.3\n69.2 ±plus-or-minus\\pm 1.3\n71.8\n\n\n\n15% label noise\n\n\n\n\n\n\nFedAvg [23]\nFedProx [15]\nFedBiO [24]\nFedDF [25]\nFedNova [26]\nSCAFFOLD [27]\nRHFL [14]\nAverage\n\nRandom [23]\n57.6 ±plus-or-minus\\pm 2.7\n61.1 ±plus-or-minus\\pm 3.1\n61.3 ±plus-or-minus\\pm 3.4\n60.3 ±plus-or-minus\\pm 2.5\n59.6 ±plus-or-minus\\pm 3.1\n57.2 ±plus-or-minus\\pm 2.2\n68.3 ±plus-or-minus\\pm 2.4\n60.8\n\nOort [17]\n61.4 ±plus-or-minus\\pm 1.8\n66.8 ±plus-or-minus\\pm 1.4\n66.4 ±plus-or-minus\\pm 1.2\n67.6 ±plus-or-minus\\pm 1.3\n65.8 ±plus-or-minus\\pm 1.2\n66.3 ±plus-or-minus\\pm 1.5\n69.4 ±plus-or-minus\\pm 1.4\n66.3\n\nPyramidFL [18]\n63.9 ±plus-or-minus\\pm 1.2\n66.1 ±plus-or-minus\\pm 1.1\n65.7 ±plus-or-minus\\pm 1.2\n64.5 ±plus-or-minus\\pm 1.4\n65.7 ±plus-or-minus\\pm 1.4\n66.6 ±plus-or-minus\\pm 1.6\n70.0 ±plus-or-minus\\pm 1.3\n66.0\n\nRestless bandit [28]\n60.5 ±plus-or-minus\\pm 3.0\n60.7 ±plus-or-minus\\pm 2.6\n64.1 ±plus-or-minus\\pm 2.4\n58.9 ±plus-or-minus\\pm 2.3\n54.3 ±plus-or-minus\\pm 1.9\n60.2 ±plus-or-minus\\pm 1.6\n69.0 ±plus-or-minus\\pm 2.1\n61.1\n\nNeural bandit [29]\n63.6 ±plus-or-minus\\pm 2.4\n68.7 ±plus-or-minus\\pm 2.2\n66.8 ±plus-or-minus\\pm 2.4\n63.4 ±plus-or-minus\\pm 2.1\n67.6 ±plus-or-minus\\pm 2.0\n68.2 ±plus-or-minus\\pm 1.7\n71.4 ±plus-or-minus\\pm 2.5\n67.1\n\nFedCor [13]\n66.6 ±plus-or-minus\\pm 1.5\n71.4 ±plus-or-minus\\pm 1.3\n66.4 ±plus-or-minus\\pm 1.2\n63.6 ±plus-or-minus\\pm 1.8\n67.9 ±plus-or-minus\\pm 1.4\n70.3 ±plus-or-minus\\pm 1.7\n69.1 ±plus-or-minus\\pm 1.8\n67.9\n\nFEEL [16]\n62.5 ±plus-or-minus\\pm 1.4\n64.9 ±plus-or-minus\\pm 1.7\n59.2 ±plus-or-minus\\pm 1.5\n67.0 ±plus-or-minus\\pm 1.6\n65.5 ±plus-or-minus\\pm 2.1\n62.3 ±plus-or-minus\\pm 2.3\n71.5 ±plus-or-minus\\pm 1.5\n64.7\n\nFLASH (Ours)\n68.2 ±plus-or-minus\\pm 1.2\n69.0 ±plus-or-minus\\pm 1.4\n71.5 ±plus-or-minus\\pm 1.4\n72.2 ±plus-or-minus\\pm 1.2\n69.8 ±plus-or-minus\\pm 1.3\n71.9 ±plus-or-minus\\pm 1.4\n74.2 ±plus-or-minus\\pm 1.5\n70.9\n\n\n",
        "references": [
            "Main contribution. Our algorithm FLASH addresses this problem by explicitly modeling the heterogeneous nature of each client as a client context. Context is represented as a feature vector that summarizes a client’s characteristics/metadata, e.g., distributional heterogeneity, label noise, and straggler latency, and is defined precisely in Section 2.3. FLASH utilizes context information to select clients based on Contextual Multi-Armed Bandits (CMAB): it treats each client as an arm and selects, in each round, the clients that yield the highest estimated improvement in the global optimization objective. Importantly, it can augment existing federated optimization schemes (as shown in Sec. 3.5) and leads to synergistic improvements. To summarize, FLASH has three key features to enable FL under heterogeneous environments with substantial performance gains.\n∙∙\\bullet Simultaneous and diverse heterogeneities: FLASH addresses general heterogeneities across clients by explicitly modeling them as contextual variables. We consider latencies to the server, variations in label qualities, and distributional heterogeneity in data, and demonstrate FLASH’s best of all worlds performance in terms of latency and accuracy. We are not aware of other works that consider all these simultaneous sources of heterogeneity - an important step towards practical FL algorithms. Our approach facilitates optimal trade-offs between diverse heterogeneities and is applicable to complex scenarios (e.g. clients that have low latency may not have good data quality and vice versa).\n\n∙∙\\bullet Contextual and interpretable framework: FLASH incorporates a novel CMAB framework that employs contextual features to represent client heterogeneity and to predict the contributions of individual clients to the global accuracy.\nExtensive ablation studies show that FLASH is easy to interpret as it indeed automatically emphasizes the relevant contextual features as the nature and degree of heterogeneity changes (see ablation analysis in Section 3.5).\n\n∙∙\\bullet Significant Performance Improvement. Through extensive experiments, we show that FLASH outperforms existing client selection methods, with up to 10% improvement in accuracy over state-of-the-art baselines (see Figure 3(c)). Furthermore, when combined with various federated learning aggregation methods, FLASH delivers the best average performance, thereby demonstrating its utility as a generalized client selection strategy (see Table II)."
        ]
    }
}