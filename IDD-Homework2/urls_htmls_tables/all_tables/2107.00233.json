{
    "PAPER'S NUMBER OF TABLES": 14,
    "S3.SS2.tab1": {
        "caption": "",
        "table": "",
        "footnotes": "",
        "references": [
            "Although conceptually it provides an ideal learning environment for edge devices, the federated learning still has some practical challenges that prevent the widespread application of it (Li etÂ al., 2020a; Kairouz etÂ al., 2019).\nAmong such challenges, the one that we are interested in this paper is the heterogeneity of the data, as data is distributed non-iid across clients in many real-world settings; in other words, each local client data is not fairly drawn from identical underlying distribution. Since each client will learn from different data distributions, it becomes harder for the model to be trained efficiently, as reported in (McMahan etÂ al., 2017). While theoretical evidence on the convergence of FedAvg with non-iid case has recently been shown in (Li etÂ al., 2020c), efficient algorithms suitable for this setting have not yet been developed or systematically examined despite some efforts (Zhao etÂ al., 2018; Hsieh etÂ al., 2020).",
            "Additional concerns involve identification of data by detecting change in exchanged averaged data, in case of continual learning, which involves local data change across time. This issue is exacerbated as there is update of averaged data for every minute change on local data, which makes the client receiving ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} easier to infer the changed portion. One simple suggestion to alleviate this issue would be to only update ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} when there is enough change in local data across enough number of clients, so that such changes are not easily exploitable.",
            "While NaiveMix and FedMix is already superior to FedProx, they are parallel to FedProx modification and can be applied in conjunction with FedProx. We compare performances across FedProx variants of various Mixup algorithms in Table 1. FedMix outperforms vanilla FedProx for various datasets, although they do fall short of default version of FedMix used for the main experiment.",
            "While Mixup is usually performed for image classification tasks, it could be applied for language models. For language datasets, since Mixup cannot be performed on input, we perform Mixup on embeddings (for a detailed explanation of Mixup between hidden states, see Appendix E). When tested on Shakespeare dataset, FedMix and NaiveMix both show better performance than baseline algorithms (Table 2). Note that for this task, LocalMix has the lowest performance, and global Mixup does not result in the superior performance above federated algorithms as expected. We think Mixup does not provide performance boost for this specific task, but claim that MAFL algorithms still result in better performance compared to FedAvg.",
            "We also claim that FedMix is superior compared to other methods under various settings, in terms of varying number of clients (Nğ‘N) and varying number of local data per clients. We observe superior performance of FedMix compared to other algorithms for all settings (see Tables 4 and 4). We also vary the number of local epochs (Eğ¸E) between global updates, and still observe that FedMix outperforms other methods (see Appendix F).",
            "Since FedMix approximates loss function of global Mixup for fixed value of Î»â‰ª1much-less-thanğœ†1\\lambda\\ll 1, we can evaluate the efficiency of approximation by comparing between FedMix and a global Mixup scenario with fixed Î»ğœ†\\lambda value. Table 5 shows varying performance between global Mixup and FedMix under various values of Î»ğœ†\\lambda. As Î»ğœ†\\lambda increases, Mixup data reflects more of the features of external data, resulting in better performance in case of global Mixup. However, this also results in our approximation being much less accurate, and we indeed observe performance of FedMix decreasing instead. The result shows that the hyperparameter Î»ğœ†\\lambda should be chosen to balance between better Mixup and better approximation. However, it seems that high Î»ğœ†\\lambda results in significant decrease in both methods, probably due to external data (which is out-of-distribution for local distribution) being overrepresented during local update.",
            "We claim that our method is efficient when faced with non-iid federated settings. For example, our setting of CIFAR10 having only data from 2 classes per client is very non-iid, as in average a pair of clients share only roughly 20% of data distribution. We test settings for CIFAR10 where clients have data from greater number of classes, and while there is little difference for iid (10 class/client) setting, we observe that FedMix outperform other methods and suffer less from increased heterogeneity from highly non-iid settings (Table 7). In addition, we also observe less decline and better performance for MAFL-based algorithms, FedMix in particular, as we train less number of clients per round, reducing communication burden in cost of performance (Table 7).",
            "Local clients are trained by SGD optimizer with learning rate 0.01 and learning decay rate per round 0.999. We set local batch size as 10 for training. Specific hyperparameter setting for each dataset is explained in following Table 8. Throughout the experiment, Mksubscriptğ‘€ğ‘˜M_{k} is fixed to local clientâ€™s dataset size. Changes in these parameters are indicated, if made, are stated for all experiments. Note that we use a fixed small value of Î»ğœ†\\lambda for MAFL-based algorithms to show superior performance.",
            "In Table 9, we observe that if averaged data for MAFL is substituted for randomly generated noise or locally generated images, it does not show the level of performance FedMix is able to show. Thus, we claim that FedMix properly incorporates relevant information received.",
            "Previous works (McMahan etÂ al., 2017; Caldas etÂ al., 2019) show that number of local epochs, Eğ¸E, affects federated learning performance. We tested the effect of Eğ¸E on CIFAR10. In general, we showed that test performance increases as Eğ¸E increases. In addition, we observed that under various values of Eğ¸E, FedMix shows the best performance compared to other algorithms (see Table 10), being a close second after NaiveMix for E=10ğ¸10E=10. MAFL-based algorithms outperform existing algorithms for all values of Eğ¸E tested.",
            "We present the results in Table 12. While threshold does not hugely affect performance, we observe that a moderately small threshold level of 100 results in the best performance. We suggest that as the threshold level is heightened, there is less overfitting to clients with small size local data, but it also results in a decrease in the number of averaged data received by each client. We indeed find an appropriate value of threshold that maximizes performance.",
            "In case where there are a different number of data per client, the sensitivity of Î»ğœ†\\lambda could also be different compared to when all clients have the same number of data. Results in Table 13 show that there is little change in performance by change in Î»ğœ†\\lambda, especially compared to Table 5. In addition, an inspection of the performance of a global model on individual test data of clients does not reveal any noticeable pattern by the size of local data (see Table 13).",
            "Results show that the introduction of Gaussian noise does result in a decline in performance (Table 15),although the decline is very small. Interestingly as noise gets larger as Ïƒ=0.3ğœ0.3\\sigma=0.3, random noise provides an effect as data augmentation and results in a performance increase compared to Ïƒ=0ğœ0\\sigma=0. This experiment is in line with Appendix D. We conclude that introduction of noise in averaged data could provide us with a reasonable alternative to FedMix with large Mksubscriptğ‘€ğ‘˜M_{k}. While our method does not align directly with differential privacy, we leave as future work how FedMix could be smoothly combined with DP-related methods and how its privacy could be quantified in terms of differential privacy.",
            "Further averaging between entries of ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} practically provides an extension of the range of viable Mksubscriptğ‘€ğ‘˜M_{k} such that it exceeds nksubscriptğ‘›ğ‘˜n_{k}, in the sense that each averaged data is from multiple clientsâ€™ data. Such a process would also result in fewer data included in ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g}, so we tested effect of this procedure on model performance. Table 15 shows that for mğ‘šm-fold extra averaging, we even observe increase in performance, but it quickly declines as mğ‘šm gets too large. This method provides an improvement in privacy while even possibly resulting in better performance.",
            "We varied Mixup ratio Î»ğœ†\\lambda for NaiveMix as well. Results in Table 17 shows that NaiveMix also has an intermediate optimal value of Î»ğœ†\\lambda. The drop in performance for Î»=0.5ğœ†0.5\\lambda=0.5 is much more dramatic than for FedMix (see Table 5 for comparison with Global Mixup and FedMix). We think that NaiveMix loss also suffers as it gives more weight to the averaged data, especially for large Mksubscriptğ‘€ğ‘˜M_{k}."
        ]
    },
    "S4.T1": {
        "caption": "Table 1: Test accuracy after (target rounds) and number of rounds to reach (target test accuracy) on various datasets. Algorithms in conjunction with FedProx are compared separately (bottom). MAFL-based algorithms are marked in bold.",
        "table": "<table id=\"S4.T1.5.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.5.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.5.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.5.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Algorithm</span></th>\n<th id=\"S4.T1.5.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\"><span id=\"S4.T1.5.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">FEMNIST</span></th>\n<th id=\"S4.T1.5.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\"><span id=\"S4.T1.5.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">CIFAR10</span></th>\n<th id=\"S4.T1.5.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\"><span id=\"S4.T1.5.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">CIFAR100</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.5.1.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.5.1.2.1.1\" class=\"ltx_td\"></td>\n<th id=\"S4.T1.5.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">test acc. (200)</th>\n<th id=\"S4.T1.5.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">rounds (80%)</th>\n<th id=\"S4.T1.5.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">test acc. (500)</th>\n<th id=\"S4.T1.5.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">rounds (70%)</th>\n<th id=\"S4.T1.5.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">test acc. (500)</th>\n<th id=\"S4.T1.5.1.2.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">rounds (40%)</th>\n</tr>\n<tr id=\"S4.T1.5.1.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.5.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\">Global Mixup</td>\n<td id=\"S4.T1.5.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">88.2</td>\n<td id=\"S4.T1.5.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">8</td>\n<td id=\"S4.T1.5.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">88.2</td>\n<td id=\"S4.T1.5.1.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">85</td>\n<td id=\"S4.T1.5.1.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\">61.4</td>\n<td id=\"S4.T1.5.1.3.2.7\" class=\"ltx_td ltx_align_center ltx_border_t\">54</td>\n</tr>\n<tr id=\"S4.T1.5.1.4.3\" class=\"ltx_tr\">\n<td id=\"S4.T1.5.1.4.3.1\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.T1.5.1.4.3.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span>FedAvg</td>\n<td id=\"S4.T1.5.1.4.3.2\" class=\"ltx_td ltx_align_center\">85.3</td>\n<td id=\"S4.T1.5.1.4.3.3\" class=\"ltx_td ltx_align_center\">26</td>\n<td id=\"S4.T1.5.1.4.3.4\" class=\"ltx_td ltx_align_center\">73.8</td>\n<td id=\"S4.T1.5.1.4.3.5\" class=\"ltx_td ltx_align_center\">283</td>\n<td id=\"S4.T1.5.1.4.3.6\" class=\"ltx_td ltx_align_center\">50.4</td>\n<td id=\"S4.T1.5.1.4.3.7\" class=\"ltx_td ltx_align_center\">101</td>\n</tr>\n<tr id=\"S4.T1.5.1.5.4\" class=\"ltx_tr\">\n<td id=\"S4.T1.5.1.5.4.1\" class=\"ltx_td ltx_align_center\">LocalMix</td>\n<td id=\"S4.T1.5.1.5.4.2\" class=\"ltx_td ltx_align_center\">82.8</td>\n<td id=\"S4.T1.5.1.5.4.3\" class=\"ltx_td ltx_align_center\">28</td>\n<td id=\"S4.T1.5.1.5.4.4\" class=\"ltx_td ltx_align_center\">73.0</td>\n<td id=\"S4.T1.5.1.5.4.5\" class=\"ltx_td ltx_align_center\">267</td>\n<td id=\"S4.T1.5.1.5.4.6\" class=\"ltx_td ltx_align_center\">54.8</td>\n<td id=\"S4.T1.5.1.5.4.7\" class=\"ltx_td ltx_align_center\">91</td>\n</tr>\n<tr id=\"S4.T1.5.1.6.5\" class=\"ltx_tr\">\n<td id=\"S4.T1.5.1.6.5.1\" class=\"ltx_td ltx_align_center\">\n<span id=\"S4.T1.5.1.6.5.1.1\" class=\"ltx_ERROR undefined\">\\hdashline</span><span id=\"S4.T1.5.1.6.5.1.2\" class=\"ltx_text ltx_font_bold\">NaiveMix</span>\n</td>\n<td id=\"S4.T1.5.1.6.5.2\" class=\"ltx_td ltx_align_center\">85.9</td>\n<td id=\"S4.T1.5.1.6.5.3\" class=\"ltx_td ltx_align_center\">23</td>\n<td id=\"S4.T1.5.1.6.5.4\" class=\"ltx_td ltx_align_center\">77.4</td>\n<td id=\"S4.T1.5.1.6.5.5\" class=\"ltx_td ltx_align_center\">198</td>\n<td id=\"S4.T1.5.1.6.5.6\" class=\"ltx_td ltx_align_center\">53.8</td>\n<td id=\"S4.T1.5.1.6.5.7\" class=\"ltx_td ltx_align_center\">85</td>\n</tr>\n<tr id=\"S4.T1.5.1.7.6\" class=\"ltx_tr\">\n<td id=\"S4.T1.5.1.7.6.1\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.5.1.7.6.1.1\" class=\"ltx_text ltx_font_bold\">FedMix</span></td>\n<td id=\"S4.T1.5.1.7.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.5.1.7.6.2.1\" class=\"ltx_text ltx_font_bold\">86.5</span></td>\n<td id=\"S4.T1.5.1.7.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.5.1.7.6.3.1\" class=\"ltx_text ltx_font_bold\">18</span></td>\n<td id=\"S4.T1.5.1.7.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.5.1.7.6.4.1\" class=\"ltx_text ltx_font_bold\">81.2</span></td>\n<td id=\"S4.T1.5.1.7.6.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.5.1.7.6.5.1\" class=\"ltx_text ltx_font_bold\">162</span></td>\n<td id=\"S4.T1.5.1.7.6.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.5.1.7.6.6.1\" class=\"ltx_text ltx_font_bold\">56.7</span></td>\n<td id=\"S4.T1.5.1.7.6.7\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.5.1.7.6.7.1\" class=\"ltx_text ltx_font_bold\">34</span></td>\n</tr>\n<tr id=\"S4.T1.5.1.8.7\" class=\"ltx_tr\">\n<td id=\"S4.T1.5.1.8.7.1\" class=\"ltx_td ltx_align_center ltx_border_tt\">FedProx</td>\n<td id=\"S4.T1.5.1.8.7.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">84.6</td>\n<td id=\"S4.T1.5.1.8.7.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T1.5.1.8.7.3.1\" class=\"ltx_text ltx_font_bold\">29</span></td>\n<td id=\"S4.T1.5.1.8.7.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">77.3</td>\n<td id=\"S4.T1.5.1.8.7.5\" class=\"ltx_td ltx_align_center ltx_border_tt\">266</td>\n<td id=\"S4.T1.5.1.8.7.6\" class=\"ltx_td ltx_align_center ltx_border_tt\">51.2</td>\n<td id=\"S4.T1.5.1.8.7.7\" class=\"ltx_td ltx_align_center ltx_border_tt\">79</td>\n</tr>\n<tr id=\"S4.T1.5.1.9.8\" class=\"ltx_tr\">\n<td id=\"S4.T1.5.1.9.8.1\" class=\"ltx_td ltx_align_center\">FedProx + LocalMix</td>\n<td id=\"S4.T1.5.1.9.8.2\" class=\"ltx_td ltx_align_center\">84.1</td>\n<td id=\"S4.T1.5.1.9.8.3\" class=\"ltx_td ltx_align_center\">39</td>\n<td id=\"S4.T1.5.1.9.8.4\" class=\"ltx_td ltx_align_center\">74.1</td>\n<td id=\"S4.T1.5.1.9.8.5\" class=\"ltx_td ltx_align_center\">314</td>\n<td id=\"S4.T1.5.1.9.8.6\" class=\"ltx_td ltx_align_center\">54.0</td>\n<td id=\"S4.T1.5.1.9.8.7\" class=\"ltx_td ltx_align_center\">90</td>\n</tr>\n<tr id=\"S4.T1.5.1.10.9\" class=\"ltx_tr\">\n<td id=\"S4.T1.5.1.10.9.1\" class=\"ltx_td ltx_align_center\">FedProx + <span id=\"S4.T1.5.1.10.9.1.1\" class=\"ltx_text ltx_font_bold\">NaiveMix</span>\n</td>\n<td id=\"S4.T1.5.1.10.9.2\" class=\"ltx_td ltx_align_center\">85.7</td>\n<td id=\"S4.T1.5.1.10.9.3\" class=\"ltx_td ltx_align_center\">37</td>\n<td id=\"S4.T1.5.1.10.9.4\" class=\"ltx_td ltx_align_center\">76.7</td>\n<td id=\"S4.T1.5.1.10.9.5\" class=\"ltx_td ltx_align_center\">230</td>\n<td id=\"S4.T1.5.1.10.9.6\" class=\"ltx_td ltx_align_center\">53.1</td>\n<td id=\"S4.T1.5.1.10.9.7\" class=\"ltx_td ltx_align_center\">74</td>\n</tr>\n<tr id=\"S4.T1.5.1.11.10\" class=\"ltx_tr\">\n<td id=\"S4.T1.5.1.11.10.1\" class=\"ltx_td ltx_align_center ltx_border_b\">FedProx + <span id=\"S4.T1.5.1.11.10.1.1\" class=\"ltx_text ltx_font_bold\">FedMix</span>\n</td>\n<td id=\"S4.T1.5.1.11.10.2\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S4.T1.5.1.11.10.2.1\" class=\"ltx_text ltx_font_bold\">86.0</span></td>\n<td id=\"S4.T1.5.1.11.10.3\" class=\"ltx_td ltx_align_center ltx_border_b\">32</td>\n<td id=\"S4.T1.5.1.11.10.4\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S4.T1.5.1.11.10.4.1\" class=\"ltx_text ltx_font_bold\">78.9</span></td>\n<td id=\"S4.T1.5.1.11.10.5\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S4.T1.5.1.11.10.5.1\" class=\"ltx_text ltx_font_bold\">223</span></td>\n<td id=\"S4.T1.5.1.11.10.6\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S4.T1.5.1.11.10.6.1\" class=\"ltx_text ltx_font_bold\">54.5</span></td>\n<td id=\"S4.T1.5.1.11.10.7\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S4.T1.5.1.11.10.7.1\" class=\"ltx_text ltx_font_bold\">63</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "While NaiveMix and FedMix is already superior to FedProx, they are parallel to FedProx modification and can be applied in conjunction with FedProx. We compare performances across FedProx variants of various Mixup algorithms in Table 1. FedMix outperforms vanilla FedProx for various datasets, although they do fall short of default version of FedMix used for the main experiment.",
            "Previous works (McMahan etÂ al., 2017; Caldas etÂ al., 2019) show that number of local epochs, Eğ¸E, affects federated learning performance. We tested the effect of Eğ¸E on CIFAR10. In general, we showed that test performance increases as Eğ¸E increases. In addition, we observed that under various values of Eğ¸E, FedMix shows the best performance compared to other algorithms (see Table 10), being a close second after NaiveMix for E=10ğ¸10E=10. MAFL-based algorithms outperform existing algorithms for all values of Eğ¸E tested.",
            "We present the results in Table 12. While threshold does not hugely affect performance, we observe that a moderately small threshold level of 100 results in the best performance. We suggest that as the threshold level is heightened, there is less overfitting to clients with small size local data, but it also results in a decrease in the number of averaged data received by each client. We indeed find an appropriate value of threshold that maximizes performance.",
            "In case where there are a different number of data per client, the sensitivity of Î»ğœ†\\lambda could also be different compared to when all clients have the same number of data. Results in Table 13 show that there is little change in performance by change in Î»ğœ†\\lambda, especially compared to Table 5. In addition, an inspection of the performance of a global model on individual test data of clients does not reveal any noticeable pattern by the size of local data (see Table 13).",
            "Results show that the introduction of Gaussian noise does result in a decline in performance (Table 15),although the decline is very small. Interestingly as noise gets larger as Ïƒ=0.3ğœ0.3\\sigma=0.3, random noise provides an effect as data augmentation and results in a performance increase compared to Ïƒ=0ğœ0\\sigma=0. This experiment is in line with Appendix D. We conclude that introduction of noise in averaged data could provide us with a reasonable alternative to FedMix with large Mksubscriptğ‘€ğ‘˜M_{k}. While our method does not align directly with differential privacy, we leave as future work how FedMix could be smoothly combined with DP-related methods and how its privacy could be quantified in terms of differential privacy.",
            "Further averaging between entries of ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} practically provides an extension of the range of viable Mksubscriptğ‘€ğ‘˜M_{k} such that it exceeds nksubscriptğ‘›ğ‘˜n_{k}, in the sense that each averaged data is from multiple clientsâ€™ data. Such a process would also result in fewer data included in ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g}, so we tested effect of this procedure on model performance. Table 15 shows that for mğ‘šm-fold extra averaging, we even observe increase in performance, but it quickly declines as mğ‘šm gets too large. This method provides an improvement in privacy while even possibly resulting in better performance.",
            "We varied Mixup ratio Î»ğœ†\\lambda for NaiveMix as well. Results in Table 17 shows that NaiveMix also has an intermediate optimal value of Î»ğœ†\\lambda. The drop in performance for Î»=0.5ğœ†0.5\\lambda=0.5 is much more dramatic than for FedMix (see Table 5 for comparison with Global Mixup and FedMix). We think that NaiveMix loss also suffers as it gives more weight to the averaged data, especially for large Mksubscriptğ‘€ğ‘˜M_{k}."
        ]
    },
    "S4.T2": {
        "caption": "",
        "table": "<table id=\"S4.T2.fig1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.fig1.1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.fig1.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"S4.T2.fig1.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Algorithm</span></td>\n<td id=\"S4.T2.fig1.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\">Global Mixup</td>\n<td id=\"S4.T2.fig1.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\">FedAvg</td>\n<td id=\"S4.T2.fig1.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\">FedProx</td>\n<td id=\"S4.T2.fig1.1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\">LocalMix</td>\n<td id=\"S4.T2.fig1.1.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\">NaiveMix</td>\n<td id=\"S4.T2.fig1.1.1.1.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\">FedMix</td>\n</tr>\n<tr id=\"S4.T2.fig1.1.1.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.fig1.1.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\">Test Acc. (%)</td>\n<td id=\"S4.T2.fig1.1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\">54.4</td>\n<td id=\"S4.T2.fig1.1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\">54.7</td>\n<td id=\"S4.T2.fig1.1.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\">54.4</td>\n<td id=\"S4.T2.fig1.1.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\">53.7</td>\n<td id=\"S4.T2.fig1.1.1.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"S4.T2.fig1.1.1.2.2.6.1\" class=\"ltx_text ltx_font_bold\">56.9</span></td>\n<td id=\"S4.T2.fig1.1.1.2.2.7\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"S4.T2.fig1.1.1.2.2.7.1\" class=\"ltx_text ltx_font_bold\">56.9</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Although conceptually it provides an ideal learning environment for edge devices, the federated learning still has some practical challenges that prevent the widespread application of it (Li etÂ al., 2020a; Kairouz etÂ al., 2019).\nAmong such challenges, the one that we are interested in this paper is the heterogeneity of the data, as data is distributed non-iid across clients in many real-world settings; in other words, each local client data is not fairly drawn from identical underlying distribution. Since each client will learn from different data distributions, it becomes harder for the model to be trained efficiently, as reported in (McMahan etÂ al., 2017). While theoretical evidence on the convergence of FedAvg with non-iid case has recently been shown in (Li etÂ al., 2020c), efficient algorithms suitable for this setting have not yet been developed or systematically examined despite some efforts (Zhao etÂ al., 2018; Hsieh etÂ al., 2020).",
            "Additional concerns involve identification of data by detecting change in exchanged averaged data, in case of continual learning, which involves local data change across time. This issue is exacerbated as there is update of averaged data for every minute change on local data, which makes the client receiving ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} easier to infer the changed portion. One simple suggestion to alleviate this issue would be to only update ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} when there is enough change in local data across enough number of clients, so that such changes are not easily exploitable.",
            "While NaiveMix and FedMix is already superior to FedProx, they are parallel to FedProx modification and can be applied in conjunction with FedProx. We compare performances across FedProx variants of various Mixup algorithms in Table 1. FedMix outperforms vanilla FedProx for various datasets, although they do fall short of default version of FedMix used for the main experiment.",
            "While Mixup is usually performed for image classification tasks, it could be applied for language models. For language datasets, since Mixup cannot be performed on input, we perform Mixup on embeddings (for a detailed explanation of Mixup between hidden states, see Appendix E). When tested on Shakespeare dataset, FedMix and NaiveMix both show better performance than baseline algorithms (Table 2). Note that for this task, LocalMix has the lowest performance, and global Mixup does not result in the superior performance above federated algorithms as expected. We think Mixup does not provide performance boost for this specific task, but claim that MAFL algorithms still result in better performance compared to FedAvg.",
            "We also claim that FedMix is superior compared to other methods under various settings, in terms of varying number of clients (Nğ‘N) and varying number of local data per clients. We observe superior performance of FedMix compared to other algorithms for all settings (see Tables 4 and 4). We also vary the number of local epochs (Eğ¸E) between global updates, and still observe that FedMix outperforms other methods (see Appendix F).",
            "Since FedMix approximates loss function of global Mixup for fixed value of Î»â‰ª1much-less-thanğœ†1\\lambda\\ll 1, we can evaluate the efficiency of approximation by comparing between FedMix and a global Mixup scenario with fixed Î»ğœ†\\lambda value. Table 5 shows varying performance between global Mixup and FedMix under various values of Î»ğœ†\\lambda. As Î»ğœ†\\lambda increases, Mixup data reflects more of the features of external data, resulting in better performance in case of global Mixup. However, this also results in our approximation being much less accurate, and we indeed observe performance of FedMix decreasing instead. The result shows that the hyperparameter Î»ğœ†\\lambda should be chosen to balance between better Mixup and better approximation. However, it seems that high Î»ğœ†\\lambda results in significant decrease in both methods, probably due to external data (which is out-of-distribution for local distribution) being overrepresented during local update.",
            "We claim that our method is efficient when faced with non-iid federated settings. For example, our setting of CIFAR10 having only data from 2 classes per client is very non-iid, as in average a pair of clients share only roughly 20% of data distribution. We test settings for CIFAR10 where clients have data from greater number of classes, and while there is little difference for iid (10 class/client) setting, we observe that FedMix outperform other methods and suffer less from increased heterogeneity from highly non-iid settings (Table 7). In addition, we also observe less decline and better performance for MAFL-based algorithms, FedMix in particular, as we train less number of clients per round, reducing communication burden in cost of performance (Table 7).",
            "Local clients are trained by SGD optimizer with learning rate 0.01 and learning decay rate per round 0.999. We set local batch size as 10 for training. Specific hyperparameter setting for each dataset is explained in following Table 8. Throughout the experiment, Mksubscriptğ‘€ğ‘˜M_{k} is fixed to local clientâ€™s dataset size. Changes in these parameters are indicated, if made, are stated for all experiments. Note that we use a fixed small value of Î»ğœ†\\lambda for MAFL-based algorithms to show superior performance.",
            "In Table 9, we observe that if averaged data for MAFL is substituted for randomly generated noise or locally generated images, it does not show the level of performance FedMix is able to show. Thus, we claim that FedMix properly incorporates relevant information received.",
            "Previous works (McMahan etÂ al., 2017; Caldas etÂ al., 2019) show that number of local epochs, Eğ¸E, affects federated learning performance. We tested the effect of Eğ¸E on CIFAR10. In general, we showed that test performance increases as Eğ¸E increases. In addition, we observed that under various values of Eğ¸E, FedMix shows the best performance compared to other algorithms (see Table 10), being a close second after NaiveMix for E=10ğ¸10E=10. MAFL-based algorithms outperform existing algorithms for all values of Eğ¸E tested.",
            "We present the results in Table 12. While threshold does not hugely affect performance, we observe that a moderately small threshold level of 100 results in the best performance. We suggest that as the threshold level is heightened, there is less overfitting to clients with small size local data, but it also results in a decrease in the number of averaged data received by each client. We indeed find an appropriate value of threshold that maximizes performance.",
            "In case where there are a different number of data per client, the sensitivity of Î»ğœ†\\lambda could also be different compared to when all clients have the same number of data. Results in Table 13 show that there is little change in performance by change in Î»ğœ†\\lambda, especially compared to Table 5. In addition, an inspection of the performance of a global model on individual test data of clients does not reveal any noticeable pattern by the size of local data (see Table 13).",
            "Results show that the introduction of Gaussian noise does result in a decline in performance (Table 15),although the decline is very small. Interestingly as noise gets larger as Ïƒ=0.3ğœ0.3\\sigma=0.3, random noise provides an effect as data augmentation and results in a performance increase compared to Ïƒ=0ğœ0\\sigma=0. This experiment is in line with Appendix D. We conclude that introduction of noise in averaged data could provide us with a reasonable alternative to FedMix with large Mksubscriptğ‘€ğ‘˜M_{k}. While our method does not align directly with differential privacy, we leave as future work how FedMix could be smoothly combined with DP-related methods and how its privacy could be quantified in terms of differential privacy.",
            "Further averaging between entries of ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} practically provides an extension of the range of viable Mksubscriptğ‘€ğ‘˜M_{k} such that it exceeds nksubscriptğ‘›ğ‘˜n_{k}, in the sense that each averaged data is from multiple clientsâ€™ data. Such a process would also result in fewer data included in ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g}, so we tested effect of this procedure on model performance. Table 15 shows that for mğ‘šm-fold extra averaging, we even observe increase in performance, but it quickly declines as mğ‘šm gets too large. This method provides an improvement in privacy while even possibly resulting in better performance.",
            "We varied Mixup ratio Î»ğœ†\\lambda for NaiveMix as well. Results in Table 17 shows that NaiveMix also has an intermediate optimal value of Î»ğœ†\\lambda. The drop in performance for Î»=0.5ğœ†0.5\\lambda=0.5 is much more dramatic than for FedMix (see Table 5 for comparison with Global Mixup and FedMix). We think that NaiveMix loss also suffers as it gives more weight to the averaged data, especially for large Mksubscriptğ‘€ğ‘˜M_{k}."
        ]
    },
    "S4.T4": {
        "caption": "",
        "table": "<table id=\"S4.T4.3.3.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T4.3.3.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.3.3.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"S4.T4.3.3.1.1.1.1\" class=\"ltx_text ltx_font_bold\"># of Clients(<math id=\"S4.T4.3.3.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"N\" display=\"inline\"><semantics id=\"S4.T4.3.3.1.1.1.1.m1.1a\"><mi id=\"S4.T4.3.3.1.1.1.1.m1.1.1\" xref=\"S4.T4.3.3.1.1.1.1.m1.1.1.cmml\">N</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.3.3.1.1.1.1.m1.1b\"><ci id=\"S4.T4.3.3.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T4.3.3.1.1.1.1.m1.1.1\">ğ‘</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.3.3.1.1.1.1.m1.1c\">N</annotation></semantics></math>)</span></th>\n<th id=\"S4.T4.3.3.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"S4.T4.3.3.1.1.2.1\" class=\"ltx_text ltx_font_bold\">20</span></th>\n<th id=\"S4.T4.3.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"S4.T4.3.3.1.1.3.1\" class=\"ltx_text ltx_font_bold\">40</span></th>\n<th id=\"S4.T4.3.3.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"S4.T4.3.3.1.1.4.1\" class=\"ltx_text ltx_font_bold\">60</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T4.3.3.1.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.3.3.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">Global Mixup</th>\n<td id=\"S4.T4.3.3.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">86.3</td>\n<td id=\"S4.T4.3.3.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">89.2</td>\n<td id=\"S4.T4.3.3.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">88.2</td>\n</tr>\n<tr id=\"S4.T4.3.3.1.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T4.3.3.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">FedAvg</th>\n<td id=\"S4.T4.3.3.1.3.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">65.8</td>\n<td id=\"S4.T4.3.3.1.3.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">73.4</td>\n<td id=\"S4.T4.3.3.1.3.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">73.8</td>\n</tr>\n<tr id=\"S4.T4.3.3.1.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T4.3.3.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">LocalMix</th>\n<td id=\"S4.T4.3.3.1.4.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">46.9</td>\n<td id=\"S4.T4.3.3.1.4.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">71.4</td>\n<td id=\"S4.T4.3.3.1.4.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">73.0</td>\n</tr>\n<tr id=\"S4.T4.3.3.1.5.4\" class=\"ltx_tr\">\n<th id=\"S4.T4.3.3.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">NaiveMix</th>\n<td id=\"S4.T4.3.3.1.5.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">62.2</td>\n<td id=\"S4.T4.3.3.1.5.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">75.1</td>\n<td id=\"S4.T4.3.3.1.5.4.4\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">77.4</td>\n</tr>\n<tr id=\"S4.T4.3.3.1.6.5\" class=\"ltx_tr\">\n<th id=\"S4.T4.3.3.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\">FedMix</th>\n<td id=\"S4.T4.3.3.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"S4.T4.3.3.1.6.5.2.1\" class=\"ltx_text ltx_font_bold\">68.5</span></td>\n<td id=\"S4.T4.3.3.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"S4.T4.3.3.1.6.5.3.1\" class=\"ltx_text ltx_font_bold\">76.4</span></td>\n<td id=\"S4.T4.3.3.1.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"S4.T4.3.3.1.6.5.4.1\" class=\"ltx_text ltx_font_bold\">81.2</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Although conceptually it provides an ideal learning environment for edge devices, the federated learning still has some practical challenges that prevent the widespread application of it (Li etÂ al., 2020a; Kairouz etÂ al., 2019).\nAmong such challenges, the one that we are interested in this paper is the heterogeneity of the data, as data is distributed non-iid across clients in many real-world settings; in other words, each local client data is not fairly drawn from identical underlying distribution. Since each client will learn from different data distributions, it becomes harder for the model to be trained efficiently, as reported in (McMahan etÂ al., 2017). While theoretical evidence on the convergence of FedAvg with non-iid case has recently been shown in (Li etÂ al., 2020c), efficient algorithms suitable for this setting have not yet been developed or systematically examined despite some efforts (Zhao etÂ al., 2018; Hsieh etÂ al., 2020).",
            "Additional concerns involve identification of data by detecting change in exchanged averaged data, in case of continual learning, which involves local data change across time. This issue is exacerbated as there is update of averaged data for every minute change on local data, which makes the client receiving ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} easier to infer the changed portion. One simple suggestion to alleviate this issue would be to only update ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} when there is enough change in local data across enough number of clients, so that such changes are not easily exploitable.",
            "While NaiveMix and FedMix is already superior to FedProx, they are parallel to FedProx modification and can be applied in conjunction with FedProx. We compare performances across FedProx variants of various Mixup algorithms in Table 1. FedMix outperforms vanilla FedProx for various datasets, although they do fall short of default version of FedMix used for the main experiment.",
            "While Mixup is usually performed for image classification tasks, it could be applied for language models. For language datasets, since Mixup cannot be performed on input, we perform Mixup on embeddings (for a detailed explanation of Mixup between hidden states, see Appendix E). When tested on Shakespeare dataset, FedMix and NaiveMix both show better performance than baseline algorithms (Table 2). Note that for this task, LocalMix has the lowest performance, and global Mixup does not result in the superior performance above federated algorithms as expected. We think Mixup does not provide performance boost for this specific task, but claim that MAFL algorithms still result in better performance compared to FedAvg.",
            "We also claim that FedMix is superior compared to other methods under various settings, in terms of varying number of clients (Nğ‘N) and varying number of local data per clients. We observe superior performance of FedMix compared to other algorithms for all settings (see Tables 4 and 4). We also vary the number of local epochs (Eğ¸E) between global updates, and still observe that FedMix outperforms other methods (see Appendix F).",
            "Since FedMix approximates loss function of global Mixup for fixed value of Î»â‰ª1much-less-thanğœ†1\\lambda\\ll 1, we can evaluate the efficiency of approximation by comparing between FedMix and a global Mixup scenario with fixed Î»ğœ†\\lambda value. Table 5 shows varying performance between global Mixup and FedMix under various values of Î»ğœ†\\lambda. As Î»ğœ†\\lambda increases, Mixup data reflects more of the features of external data, resulting in better performance in case of global Mixup. However, this also results in our approximation being much less accurate, and we indeed observe performance of FedMix decreasing instead. The result shows that the hyperparameter Î»ğœ†\\lambda should be chosen to balance between better Mixup and better approximation. However, it seems that high Î»ğœ†\\lambda results in significant decrease in both methods, probably due to external data (which is out-of-distribution for local distribution) being overrepresented during local update.",
            "We claim that our method is efficient when faced with non-iid federated settings. For example, our setting of CIFAR10 having only data from 2 classes per client is very non-iid, as in average a pair of clients share only roughly 20% of data distribution. We test settings for CIFAR10 where clients have data from greater number of classes, and while there is little difference for iid (10 class/client) setting, we observe that FedMix outperform other methods and suffer less from increased heterogeneity from highly non-iid settings (Table 7). In addition, we also observe less decline and better performance for MAFL-based algorithms, FedMix in particular, as we train less number of clients per round, reducing communication burden in cost of performance (Table 7).",
            "Local clients are trained by SGD optimizer with learning rate 0.01 and learning decay rate per round 0.999. We set local batch size as 10 for training. Specific hyperparameter setting for each dataset is explained in following Table 8. Throughout the experiment, Mksubscriptğ‘€ğ‘˜M_{k} is fixed to local clientâ€™s dataset size. Changes in these parameters are indicated, if made, are stated for all experiments. Note that we use a fixed small value of Î»ğœ†\\lambda for MAFL-based algorithms to show superior performance.",
            "In Table 9, we observe that if averaged data for MAFL is substituted for randomly generated noise or locally generated images, it does not show the level of performance FedMix is able to show. Thus, we claim that FedMix properly incorporates relevant information received.",
            "Previous works (McMahan etÂ al., 2017; Caldas etÂ al., 2019) show that number of local epochs, Eğ¸E, affects federated learning performance. We tested the effect of Eğ¸E on CIFAR10. In general, we showed that test performance increases as Eğ¸E increases. In addition, we observed that under various values of Eğ¸E, FedMix shows the best performance compared to other algorithms (see Table 10), being a close second after NaiveMix for E=10ğ¸10E=10. MAFL-based algorithms outperform existing algorithms for all values of Eğ¸E tested.",
            "We present the results in Table 12. While threshold does not hugely affect performance, we observe that a moderately small threshold level of 100 results in the best performance. We suggest that as the threshold level is heightened, there is less overfitting to clients with small size local data, but it also results in a decrease in the number of averaged data received by each client. We indeed find an appropriate value of threshold that maximizes performance.",
            "In case where there are a different number of data per client, the sensitivity of Î»ğœ†\\lambda could also be different compared to when all clients have the same number of data. Results in Table 13 show that there is little change in performance by change in Î»ğœ†\\lambda, especially compared to Table 5. In addition, an inspection of the performance of a global model on individual test data of clients does not reveal any noticeable pattern by the size of local data (see Table 13).",
            "Results show that the introduction of Gaussian noise does result in a decline in performance (Table 15),although the decline is very small. Interestingly as noise gets larger as Ïƒ=0.3ğœ0.3\\sigma=0.3, random noise provides an effect as data augmentation and results in a performance increase compared to Ïƒ=0ğœ0\\sigma=0. This experiment is in line with Appendix D. We conclude that introduction of noise in averaged data could provide us with a reasonable alternative to FedMix with large Mksubscriptğ‘€ğ‘˜M_{k}. While our method does not align directly with differential privacy, we leave as future work how FedMix could be smoothly combined with DP-related methods and how its privacy could be quantified in terms of differential privacy.",
            "Further averaging between entries of ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} practically provides an extension of the range of viable Mksubscriptğ‘€ğ‘˜M_{k} such that it exceeds nksubscriptğ‘›ğ‘˜n_{k}, in the sense that each averaged data is from multiple clientsâ€™ data. Such a process would also result in fewer data included in ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g}, so we tested effect of this procedure on model performance. Table 15 shows that for mğ‘šm-fold extra averaging, we even observe increase in performance, but it quickly declines as mğ‘šm gets too large. This method provides an improvement in privacy while even possibly resulting in better performance.",
            "We varied Mixup ratio Î»ğœ†\\lambda for NaiveMix as well. Results in Table 17 shows that NaiveMix also has an intermediate optimal value of Î»ğœ†\\lambda. The drop in performance for Î»=0.5ğœ†0.5\\lambda=0.5 is much more dramatic than for FedMix (see Table 5 for comparison with Global Mixup and FedMix). We think that NaiveMix loss also suffers as it gives more weight to the averaged data, especially for large Mksubscriptğ‘€ğ‘˜M_{k}."
        ]
    },
    "S4.T5": {
        "caption": "Table 5: Test accuracy on CIFAR10, under varying mixup ratio Î»ğœ†\\lambda.",
        "table": "<table id=\"S4.T5.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T5.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T5.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding-top:-3.5pt;padding-bottom:-3.5pt;\"><math id=\"S4.T5.3.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\lambda\" display=\"inline\"><semantics id=\"S4.T5.3.1.1.m1.1a\"><mi id=\"S4.T5.3.1.1.m1.1.1\" xref=\"S4.T5.3.1.1.m1.1.1.cmml\">Î»</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.3.1.1.m1.1b\"><ci id=\"S4.T5.3.1.1.m1.1.1.cmml\" xref=\"S4.T5.3.1.1.m1.1.1\">ğœ†</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.3.1.1.m1.1c\">\\lambda</annotation></semantics></math></th>\n<th id=\"S4.T5.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:-3.5pt;padding-bottom:-3.5pt;\"><span id=\"S4.T5.3.1.2.1\" class=\"ltx_text ltx_font_bold\">0.05</span></th>\n<th id=\"S4.T5.3.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:-3.5pt;padding-bottom:-3.5pt;\"><span id=\"S4.T5.3.1.3.1\" class=\"ltx_text ltx_font_bold\">0.1</span></th>\n<th id=\"S4.T5.3.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:-3.5pt;padding-bottom:-3.5pt;\"><span id=\"S4.T5.3.1.4.1\" class=\"ltx_text ltx_font_bold\">0.2</span></th>\n<th id=\"S4.T5.3.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:-3.5pt;padding-bottom:-3.5pt;\"><span id=\"S4.T5.3.1.5.1\" class=\"ltx_text ltx_font_bold\">0.5</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T5.3.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T5.3.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:-3.5pt;padding-bottom:-3.5pt;\">Global Mixup</th>\n<td id=\"S4.T5.3.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-3.5pt;padding-bottom:-3.5pt;\">79.4</td>\n<td id=\"S4.T5.3.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-3.5pt;padding-bottom:-3.5pt;\">80.4</td>\n<td id=\"S4.T5.3.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-3.5pt;padding-bottom:-3.5pt;\">81.1</td>\n<td id=\"S4.T5.3.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-3.5pt;padding-bottom:-3.5pt;\">63.6</td>\n</tr>\n<tr id=\"S4.T5.3.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T5.3.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\" style=\"padding-top:-3.5pt;padding-bottom:-3.5pt;\">FedMix</th>\n<td id=\"S4.T5.3.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:-3.5pt;padding-bottom:-3.5pt;\">81.2</td>\n<td id=\"S4.T5.3.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:-3.5pt;padding-bottom:-3.5pt;\">80.5</td>\n<td id=\"S4.T5.3.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:-3.5pt;padding-bottom:-3.5pt;\">77.7</td>\n<td id=\"S4.T5.3.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:-3.5pt;padding-bottom:-3.5pt;\">67.1</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Since FedMix approximates loss function of global Mixup for fixed value of Î»â‰ª1much-less-thanğœ†1\\lambda\\ll 1, we can evaluate the efficiency of approximation by comparing between FedMix and a global Mixup scenario with fixed Î»ğœ†\\lambda value. Table 5 shows varying performance between global Mixup and FedMix under various values of Î»ğœ†\\lambda. As Î»ğœ†\\lambda increases, Mixup data reflects more of the features of external data, resulting in better performance in case of global Mixup. However, this also results in our approximation being much less accurate, and we indeed observe performance of FedMix decreasing instead. The result shows that the hyperparameter Î»ğœ†\\lambda should be chosen to balance between better Mixup and better approximation. However, it seems that high Î»ğœ†\\lambda results in significant decrease in both methods, probably due to external data (which is out-of-distribution for local distribution) being overrepresented during local update.",
            "In case where there are a different number of data per client, the sensitivity of Î»ğœ†\\lambda could also be different compared to when all clients have the same number of data. Results in Table 13 show that there is little change in performance by change in Î»ğœ†\\lambda, especially compared to Table 5. In addition, an inspection of the performance of a global model on individual test data of clients does not reveal any noticeable pattern by the size of local data (see Table 13).",
            "We varied Mixup ratio Î»ğœ†\\lambda for NaiveMix as well. Results in Table 17 shows that NaiveMix also has an intermediate optimal value of Î»ğœ†\\lambda. The drop in performance for Î»=0.5ğœ†0.5\\lambda=0.5 is much more dramatic than for FedMix (see Table 5 for comparison with Global Mixup and FedMix). We think that NaiveMix loss also suffers as it gives more weight to the averaged data, especially for large Mksubscriptğ‘€ğ‘˜M_{k}."
        ]
    },
    "S4.T7": {
        "caption": "",
        "table": "<table id=\"S4.T7.fig1.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_top\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T7.fig1.1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T7.fig1.1.1.1.1.1\" class=\"ltx_td ltx_border_t\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\"></td>\n<th id=\"S4.T7.fig1.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\" colspan=\"4\"><span id=\"S4.T7.fig1.1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">â€”â€”â€”class/clientâ€”â€”â€”</span></th>\n</tr>\n<tr id=\"S4.T7.fig1.1.1.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T7.fig1.1.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"S4.T7.fig1.1.1.2.2.1.1\" class=\"ltx_text ltx_font_bold\">Algorithm</span></th>\n<th id=\"S4.T7.fig1.1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"S4.T7.fig1.1.1.2.2.2.1\" class=\"ltx_text ltx_font_bold\">2</span></th>\n<th id=\"S4.T7.fig1.1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"S4.T7.fig1.1.1.2.2.3.1\" class=\"ltx_text ltx_font_bold\">3</span></th>\n<th id=\"S4.T7.fig1.1.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"S4.T7.fig1.1.1.2.2.4.1\" class=\"ltx_text ltx_font_bold\">5</span></th>\n<th id=\"S4.T7.fig1.1.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"S4.T7.fig1.1.1.2.2.5.1\" class=\"ltx_text ltx_font_bold\">10 (iid)</span></th>\n</tr>\n<tr id=\"S4.T7.fig1.1.1.3.3\" class=\"ltx_tr\">\n<td id=\"S4.T7.fig1.1.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">Global Mixup</td>\n<td id=\"S4.T7.fig1.1.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">88.2</td>\n<td id=\"S4.T7.fig1.1.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">90.7</td>\n<td id=\"S4.T7.fig1.1.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">90.9</td>\n<td id=\"S4.T7.fig1.1.1.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">91.4</td>\n</tr>\n<tr id=\"S4.T7.fig1.1.1.4.4\" class=\"ltx_tr\">\n<td id=\"S4.T7.fig1.1.1.4.4.1\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">FedAvg</td>\n<td id=\"S4.T7.fig1.1.1.4.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">73.8</td>\n<td id=\"S4.T7.fig1.1.1.4.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">84.2</td>\n<td id=\"S4.T7.fig1.1.1.4.4.4\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">86.8</td>\n<td id=\"S4.T7.fig1.1.1.4.4.5\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">89.3</td>\n</tr>\n<tr id=\"S4.T7.fig1.1.1.5.5\" class=\"ltx_tr\">\n<td id=\"S4.T7.fig1.1.1.5.5.1\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">Localmix</td>\n<td id=\"S4.T7.fig1.1.1.5.5.2\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">73</td>\n<td id=\"S4.T7.fig1.1.1.5.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">83.3</td>\n<td id=\"S4.T7.fig1.1.1.5.5.4\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">86.4</td>\n<td id=\"S4.T7.fig1.1.1.5.5.5\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">89.1</td>\n</tr>\n<tr id=\"S4.T7.fig1.1.1.6.6\" class=\"ltx_tr\">\n<td id=\"S4.T7.fig1.1.1.6.6.1\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">NaiveMix</td>\n<td id=\"S4.T7.fig1.1.1.6.6.2\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">77.4</td>\n<td id=\"S4.T7.fig1.1.1.6.6.3\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">84.5</td>\n<td id=\"S4.T7.fig1.1.1.6.6.4\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">87.7</td>\n<td id=\"S4.T7.fig1.1.1.6.6.5\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"S4.T7.fig1.1.1.6.6.5.1\" class=\"ltx_text ltx_font_bold\">89.4</span></td>\n</tr>\n<tr id=\"S4.T7.fig1.1.1.7.7\" class=\"ltx_tr\">\n<td id=\"S4.T7.fig1.1.1.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\">FedMix</td>\n<td id=\"S4.T7.fig1.1.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"S4.T7.fig1.1.1.7.7.2.1\" class=\"ltx_text ltx_font_bold\">81.2</span></td>\n<td id=\"S4.T7.fig1.1.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"S4.T7.fig1.1.1.7.7.3.1\" class=\"ltx_text ltx_font_bold\">85.1</span></td>\n<td id=\"S4.T7.fig1.1.1.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"S4.T7.fig1.1.1.7.7.4.1\" class=\"ltx_text ltx_font_bold\">87.9</span></td>\n<td id=\"S4.T7.fig1.1.1.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\">89.1</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Although conceptually it provides an ideal learning environment for edge devices, the federated learning still has some practical challenges that prevent the widespread application of it (Li etÂ al., 2020a; Kairouz etÂ al., 2019).\nAmong such challenges, the one that we are interested in this paper is the heterogeneity of the data, as data is distributed non-iid across clients in many real-world settings; in other words, each local client data is not fairly drawn from identical underlying distribution. Since each client will learn from different data distributions, it becomes harder for the model to be trained efficiently, as reported in (McMahan etÂ al., 2017). While theoretical evidence on the convergence of FedAvg with non-iid case has recently been shown in (Li etÂ al., 2020c), efficient algorithms suitable for this setting have not yet been developed or systematically examined despite some efforts (Zhao etÂ al., 2018; Hsieh etÂ al., 2020).",
            "Additional concerns involve identification of data by detecting change in exchanged averaged data, in case of continual learning, which involves local data change across time. This issue is exacerbated as there is update of averaged data for every minute change on local data, which makes the client receiving ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} easier to infer the changed portion. One simple suggestion to alleviate this issue would be to only update ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} when there is enough change in local data across enough number of clients, so that such changes are not easily exploitable.",
            "While NaiveMix and FedMix is already superior to FedProx, they are parallel to FedProx modification and can be applied in conjunction with FedProx. We compare performances across FedProx variants of various Mixup algorithms in Table 1. FedMix outperforms vanilla FedProx for various datasets, although they do fall short of default version of FedMix used for the main experiment.",
            "While Mixup is usually performed for image classification tasks, it could be applied for language models. For language datasets, since Mixup cannot be performed on input, we perform Mixup on embeddings (for a detailed explanation of Mixup between hidden states, see Appendix E). When tested on Shakespeare dataset, FedMix and NaiveMix both show better performance than baseline algorithms (Table 2). Note that for this task, LocalMix has the lowest performance, and global Mixup does not result in the superior performance above federated algorithms as expected. We think Mixup does not provide performance boost for this specific task, but claim that MAFL algorithms still result in better performance compared to FedAvg.",
            "We also claim that FedMix is superior compared to other methods under various settings, in terms of varying number of clients (Nğ‘N) and varying number of local data per clients. We observe superior performance of FedMix compared to other algorithms for all settings (see Tables 4 and 4). We also vary the number of local epochs (Eğ¸E) between global updates, and still observe that FedMix outperforms other methods (see Appendix F).",
            "Since FedMix approximates loss function of global Mixup for fixed value of Î»â‰ª1much-less-thanğœ†1\\lambda\\ll 1, we can evaluate the efficiency of approximation by comparing between FedMix and a global Mixup scenario with fixed Î»ğœ†\\lambda value. Table 5 shows varying performance between global Mixup and FedMix under various values of Î»ğœ†\\lambda. As Î»ğœ†\\lambda increases, Mixup data reflects more of the features of external data, resulting in better performance in case of global Mixup. However, this also results in our approximation being much less accurate, and we indeed observe performance of FedMix decreasing instead. The result shows that the hyperparameter Î»ğœ†\\lambda should be chosen to balance between better Mixup and better approximation. However, it seems that high Î»ğœ†\\lambda results in significant decrease in both methods, probably due to external data (which is out-of-distribution for local distribution) being overrepresented during local update.",
            "We claim that our method is efficient when faced with non-iid federated settings. For example, our setting of CIFAR10 having only data from 2 classes per client is very non-iid, as in average a pair of clients share only roughly 20% of data distribution. We test settings for CIFAR10 where clients have data from greater number of classes, and while there is little difference for iid (10 class/client) setting, we observe that FedMix outperform other methods and suffer less from increased heterogeneity from highly non-iid settings (Table 7). In addition, we also observe less decline and better performance for MAFL-based algorithms, FedMix in particular, as we train less number of clients per round, reducing communication burden in cost of performance (Table 7).",
            "Local clients are trained by SGD optimizer with learning rate 0.01 and learning decay rate per round 0.999. We set local batch size as 10 for training. Specific hyperparameter setting for each dataset is explained in following Table 8. Throughout the experiment, Mksubscriptğ‘€ğ‘˜M_{k} is fixed to local clientâ€™s dataset size. Changes in these parameters are indicated, if made, are stated for all experiments. Note that we use a fixed small value of Î»ğœ†\\lambda for MAFL-based algorithms to show superior performance.",
            "In Table 9, we observe that if averaged data for MAFL is substituted for randomly generated noise or locally generated images, it does not show the level of performance FedMix is able to show. Thus, we claim that FedMix properly incorporates relevant information received.",
            "Previous works (McMahan etÂ al., 2017; Caldas etÂ al., 2019) show that number of local epochs, Eğ¸E, affects federated learning performance. We tested the effect of Eğ¸E on CIFAR10. In general, we showed that test performance increases as Eğ¸E increases. In addition, we observed that under various values of Eğ¸E, FedMix shows the best performance compared to other algorithms (see Table 10), being a close second after NaiveMix for E=10ğ¸10E=10. MAFL-based algorithms outperform existing algorithms for all values of Eğ¸E tested.",
            "We present the results in Table 12. While threshold does not hugely affect performance, we observe that a moderately small threshold level of 100 results in the best performance. We suggest that as the threshold level is heightened, there is less overfitting to clients with small size local data, but it also results in a decrease in the number of averaged data received by each client. We indeed find an appropriate value of threshold that maximizes performance.",
            "In case where there are a different number of data per client, the sensitivity of Î»ğœ†\\lambda could also be different compared to when all clients have the same number of data. Results in Table 13 show that there is little change in performance by change in Î»ğœ†\\lambda, especially compared to Table 5. In addition, an inspection of the performance of a global model on individual test data of clients does not reveal any noticeable pattern by the size of local data (see Table 13).",
            "Results show that the introduction of Gaussian noise does result in a decline in performance (Table 15),although the decline is very small. Interestingly as noise gets larger as Ïƒ=0.3ğœ0.3\\sigma=0.3, random noise provides an effect as data augmentation and results in a performance increase compared to Ïƒ=0ğœ0\\sigma=0. This experiment is in line with Appendix D. We conclude that introduction of noise in averaged data could provide us with a reasonable alternative to FedMix with large Mksubscriptğ‘€ğ‘˜M_{k}. While our method does not align directly with differential privacy, we leave as future work how FedMix could be smoothly combined with DP-related methods and how its privacy could be quantified in terms of differential privacy.",
            "Further averaging between entries of ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} practically provides an extension of the range of viable Mksubscriptğ‘€ğ‘˜M_{k} such that it exceeds nksubscriptğ‘›ğ‘˜n_{k}, in the sense that each averaged data is from multiple clientsâ€™ data. Such a process would also result in fewer data included in ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g}, so we tested effect of this procedure on model performance. Table 15 shows that for mğ‘šm-fold extra averaging, we even observe increase in performance, but it quickly declines as mğ‘šm gets too large. This method provides an improvement in privacy while even possibly resulting in better performance.",
            "We varied Mixup ratio Î»ğœ†\\lambda for NaiveMix as well. Results in Table 17 shows that NaiveMix also has an intermediate optimal value of Î»ğœ†\\lambda. The drop in performance for Î»=0.5ğœ†0.5\\lambda=0.5 is much more dramatic than for FedMix (see Table 5 for comparison with Global Mixup and FedMix). We think that NaiveMix loss also suffers as it gives more weight to the averaged data, especially for large Mksubscriptğ‘€ğ‘˜M_{k}."
        ]
    },
    "A2.T8": {
        "caption": "Table 8: Hyperparameter settings for each dataset.",
        "table": "<table id=\"A2.T8.5\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"A2.T8.5.6.1\" class=\"ltx_tr\">\n<td id=\"A2.T8.5.6.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"A2.T8.5.6.1.1.1\" class=\"ltx_text ltx_font_bold\">dataset</span></td>\n<td id=\"A2.T8.5.6.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"A2.T8.5.6.1.2.1\" class=\"ltx_text ltx_font_bold\">FEMNIST</span></td>\n<td id=\"A2.T8.5.6.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"A2.T8.5.6.1.3.1\" class=\"ltx_text ltx_font_bold\">CIFAR10</span></td>\n<td id=\"A2.T8.5.6.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"A2.T8.5.6.1.4.1\" class=\"ltx_text ltx_font_bold\">CIFAR100</span></td>\n<td id=\"A2.T8.5.6.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"A2.T8.5.6.1.5.1\" class=\"ltx_text ltx_font_bold\">Shakespeare</span></td>\n</tr>\n<tr id=\"A2.T8.1.1\" class=\"ltx_tr\">\n<td id=\"A2.T8.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">local epochs (<math id=\"A2.T8.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"E\" display=\"inline\"><semantics id=\"A2.T8.1.1.1.m1.1a\"><mi id=\"A2.T8.1.1.1.m1.1.1\" xref=\"A2.T8.1.1.1.m1.1.1.cmml\">E</mi><annotation-xml encoding=\"MathML-Content\" id=\"A2.T8.1.1.1.m1.1b\"><ci id=\"A2.T8.1.1.1.m1.1.1.cmml\" xref=\"A2.T8.1.1.1.m1.1.1\">ğ¸</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T8.1.1.1.m1.1c\">E</annotation></semantics></math>)</td>\n<td id=\"A2.T8.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">10</td>\n<td id=\"A2.T8.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">2</td>\n<td id=\"A2.T8.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">10</td>\n<td id=\"A2.T8.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">2</td>\n</tr>\n<tr id=\"A2.T8.5.7.2\" class=\"ltx_tr\">\n<td id=\"A2.T8.5.7.2.1\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">local batch size</td>\n<td id=\"A2.T8.5.7.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">10</td>\n<td id=\"A2.T8.5.7.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">10</td>\n<td id=\"A2.T8.5.7.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">10</td>\n<td id=\"A2.T8.5.7.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">10</td>\n</tr>\n<tr id=\"A2.T8.5.8.3\" class=\"ltx_tr\">\n<td id=\"A2.T8.5.8.3.1\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">class per clients</td>\n<td id=\"A2.T8.5.8.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">-</td>\n<td id=\"A2.T8.5.8.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">2</td>\n<td id=\"A2.T8.5.8.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">20</td>\n<td id=\"A2.T8.5.8.3.5\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">-</td>\n</tr>\n<tr id=\"A2.T8.2.2\" class=\"ltx_tr\">\n<td id=\"A2.T8.2.2.1\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">fraction of Clients (<math id=\"A2.T8.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"K/N\" display=\"inline\"><semantics id=\"A2.T8.2.2.1.m1.1a\"><mrow id=\"A2.T8.2.2.1.m1.1.1\" xref=\"A2.T8.2.2.1.m1.1.1.cmml\"><mi id=\"A2.T8.2.2.1.m1.1.1.2\" xref=\"A2.T8.2.2.1.m1.1.1.2.cmml\">K</mi><mo id=\"A2.T8.2.2.1.m1.1.1.1\" xref=\"A2.T8.2.2.1.m1.1.1.1.cmml\">/</mo><mi id=\"A2.T8.2.2.1.m1.1.1.3\" xref=\"A2.T8.2.2.1.m1.1.1.3.cmml\">N</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A2.T8.2.2.1.m1.1b\"><apply id=\"A2.T8.2.2.1.m1.1.1.cmml\" xref=\"A2.T8.2.2.1.m1.1.1\"><divide id=\"A2.T8.2.2.1.m1.1.1.1.cmml\" xref=\"A2.T8.2.2.1.m1.1.1.1\"></divide><ci id=\"A2.T8.2.2.1.m1.1.1.2.cmml\" xref=\"A2.T8.2.2.1.m1.1.1.2\">ğ¾</ci><ci id=\"A2.T8.2.2.1.m1.1.1.3.cmml\" xref=\"A2.T8.2.2.1.m1.1.1.3\">ğ‘</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T8.2.2.1.m1.1c\">K/N</annotation></semantics></math>)</td>\n<td id=\"A2.T8.2.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">0.1</td>\n<td id=\"A2.T8.2.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">0.25</td>\n<td id=\"A2.T8.2.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">0.1</td>\n<td id=\"A2.T8.2.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">1</td>\n</tr>\n<tr id=\"A2.T8.5.9.4\" class=\"ltx_tr\">\n<td id=\"A2.T8.5.9.4.1\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">total dataset classes</td>\n<td id=\"A2.T8.5.9.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">62</td>\n<td id=\"A2.T8.5.9.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">10</td>\n<td id=\"A2.T8.5.9.4.4\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">100</td>\n<td id=\"A2.T8.5.9.4.5\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">84</td>\n</tr>\n<tr id=\"A2.T8.3.3\" class=\"ltx_tr\">\n<td id=\"A2.T8.3.3.1\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">\n<math id=\"A2.T8.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\lambda\" display=\"inline\"><semantics id=\"A2.T8.3.3.1.m1.1a\"><mi id=\"A2.T8.3.3.1.m1.1.1\" xref=\"A2.T8.3.3.1.m1.1.1.cmml\">Î»</mi><annotation-xml encoding=\"MathML-Content\" id=\"A2.T8.3.3.1.m1.1b\"><ci id=\"A2.T8.3.3.1.m1.1.1.cmml\" xref=\"A2.T8.3.3.1.m1.1.1\">ğœ†</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T8.3.3.1.m1.1c\">\\lambda</annotation></semantics></math> (for NaiveMix)</td>\n<td id=\"A2.T8.3.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">0.2</td>\n<td id=\"A2.T8.3.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">0.1</td>\n<td id=\"A2.T8.3.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">0.1</td>\n<td id=\"A2.T8.3.3.5\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">0.1</td>\n</tr>\n<tr id=\"A2.T8.4.4\" class=\"ltx_tr\">\n<td id=\"A2.T8.4.4.1\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">\n<math id=\"A2.T8.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\lambda\" display=\"inline\"><semantics id=\"A2.T8.4.4.1.m1.1a\"><mi id=\"A2.T8.4.4.1.m1.1.1\" xref=\"A2.T8.4.4.1.m1.1.1.cmml\">Î»</mi><annotation-xml encoding=\"MathML-Content\" id=\"A2.T8.4.4.1.m1.1b\"><ci id=\"A2.T8.4.4.1.m1.1.1.cmml\" xref=\"A2.T8.4.4.1.m1.1.1\">ğœ†</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T8.4.4.1.m1.1c\">\\lambda</annotation></semantics></math> (for FedMix)</td>\n<td id=\"A2.T8.4.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">0.2</td>\n<td id=\"A2.T8.4.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">0.05</td>\n<td id=\"A2.T8.4.4.4\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">0.1</td>\n<td id=\"A2.T8.4.4.5\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:1.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">0.1</td>\n</tr>\n<tr id=\"A2.T8.5.5\" class=\"ltx_tr\">\n<td id=\"A2.T8.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\">\n<math id=\"A2.T8.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"\\mu\" display=\"inline\"><semantics id=\"A2.T8.5.5.1.m1.1a\"><mi id=\"A2.T8.5.5.1.m1.1.1\" xref=\"A2.T8.5.5.1.m1.1.1.cmml\">Î¼</mi><annotation-xml encoding=\"MathML-Content\" id=\"A2.T8.5.5.1.m1.1b\"><ci id=\"A2.T8.5.5.1.m1.1.1.cmml\" xref=\"A2.T8.5.5.1.m1.1.1\">ğœ‡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T8.5.5.1.m1.1c\">\\mu</annotation></semantics></math> (for FedProx)</td>\n<td id=\"A2.T8.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\">0.1</td>\n<td id=\"A2.T8.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\">0.1</td>\n<td id=\"A2.T8.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\">0.01</td>\n<td id=\"A2.T8.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:-2.5pt;padding-bottom:-2.5pt;\">0.001</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Local clients are trained by SGD optimizer with learning rate 0.01 and learning decay rate per round 0.999. We set local batch size as 10 for training. Specific hyperparameter setting for each dataset is explained in following Table 8. Throughout the experiment, Mksubscriptğ‘€ğ‘˜M_{k} is fixed to local clientâ€™s dataset size. Changes in these parameters are indicated, if made, are stated for all experiments. Note that we use a fixed small value of Î»ğœ†\\lambda for MAFL-based algorithms to show superior performance."
        ]
    },
    "A4.T9": {
        "caption": "Table 9: Test accuracy after (target rounds) and number of rounds to reach (target test accuracy) on various datasets. We compare FedMix with baseline Mixup algorithms.",
        "table": "<table id=\"A4.T9.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A4.T9.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"A4.T9.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"A4.T9.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Algorithm</span></th>\n<th id=\"A4.T9.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\"><span id=\"A4.T9.1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">FEMNIST</span></th>\n<th id=\"A4.T9.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\"><span id=\"A4.T9.1.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">CIFAR10</span></th>\n<th id=\"A4.T9.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\"><span id=\"A4.T9.1.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">CIFAR100</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A4.T9.1.1.2.1\" class=\"ltx_tr\">\n<td id=\"A4.T9.1.1.2.1.1\" class=\"ltx_td\"></td>\n<th id=\"A4.T9.1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">test acc. (200)</th>\n<th id=\"A4.T9.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">rounds (80%)</th>\n<th id=\"A4.T9.1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">test acc. (500)</th>\n<th id=\"A4.T9.1.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">rounds (70%)</th>\n<th id=\"A4.T9.1.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">test acc. (500)</th>\n<th id=\"A4.T9.1.1.2.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">rounds (40%)</th>\n</tr>\n<tr id=\"A4.T9.1.1.3.2\" class=\"ltx_tr\">\n<td id=\"A4.T9.1.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A4.T9.1.1.3.2.1.1\" class=\"ltx_text ltx_font_bold\">NaiveMix</span></td>\n<td id=\"A4.T9.1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">85.9</td>\n<td id=\"A4.T9.1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">23</td>\n<td id=\"A4.T9.1.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">77.4</td>\n<td id=\"A4.T9.1.1.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">198</td>\n<td id=\"A4.T9.1.1.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\">53.8</td>\n<td id=\"A4.T9.1.1.3.2.7\" class=\"ltx_td ltx_align_center ltx_border_t\">85</td>\n</tr>\n<tr id=\"A4.T9.1.1.4.3\" class=\"ltx_tr\">\n<td id=\"A4.T9.1.1.4.3.1\" class=\"ltx_td ltx_align_center\">Mixup w/ random noise</td>\n<td id=\"A4.T9.1.1.4.3.2\" class=\"ltx_td ltx_align_center\">86.1</td>\n<td id=\"A4.T9.1.1.4.3.3\" class=\"ltx_td ltx_align_center\">23</td>\n<td id=\"A4.T9.1.1.4.3.4\" class=\"ltx_td ltx_align_center\">77.9</td>\n<td id=\"A4.T9.1.1.4.3.5\" class=\"ltx_td ltx_align_center\">201</td>\n<td id=\"A4.T9.1.1.4.3.6\" class=\"ltx_td ltx_align_center\">51.2</td>\n<td id=\"A4.T9.1.1.4.3.7\" class=\"ltx_td ltx_align_center\">105</td>\n</tr>\n<tr id=\"A4.T9.1.1.5.4\" class=\"ltx_tr\">\n<td id=\"A4.T9.1.1.5.4.1\" class=\"ltx_td ltx_align_center\">Mixup w/ local means</td>\n<td id=\"A4.T9.1.1.5.4.2\" class=\"ltx_td ltx_align_center\">85.5</td>\n<td id=\"A4.T9.1.1.5.4.3\" class=\"ltx_td ltx_align_center\">21</td>\n<td id=\"A4.T9.1.1.5.4.4\" class=\"ltx_td ltx_align_center\">73.5</td>\n<td id=\"A4.T9.1.1.5.4.5\" class=\"ltx_td ltx_align_center\">233</td>\n<td id=\"A4.T9.1.1.5.4.6\" class=\"ltx_td ltx_align_center\">51.0</td>\n<td id=\"A4.T9.1.1.5.4.7\" class=\"ltx_td ltx_align_center\">87</td>\n</tr>\n<tr id=\"A4.T9.1.1.6.5\" class=\"ltx_tr\">\n<td id=\"A4.T9.1.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"A4.T9.1.1.6.5.1.1\" class=\"ltx_text ltx_font_bold\">FedMix</span></td>\n<td id=\"A4.T9.1.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"A4.T9.1.1.6.5.2.1\" class=\"ltx_text ltx_font_bold\">86.5</span></td>\n<td id=\"A4.T9.1.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"A4.T9.1.1.6.5.3.1\" class=\"ltx_text ltx_font_bold\">18</span></td>\n<td id=\"A4.T9.1.1.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"A4.T9.1.1.6.5.4.1\" class=\"ltx_text ltx_font_bold\">81.2</span></td>\n<td id=\"A4.T9.1.1.6.5.5\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"A4.T9.1.1.6.5.5.1\" class=\"ltx_text ltx_font_bold\">162</span></td>\n<td id=\"A4.T9.1.1.6.5.6\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"A4.T9.1.1.6.5.6.1\" class=\"ltx_text ltx_font_bold\">56.7</span></td>\n<td id=\"A4.T9.1.1.6.5.7\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"A4.T9.1.1.6.5.7.1\" class=\"ltx_text ltx_font_bold\">34</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "In Table 9, we observe that if averaged data for MAFL is substituted for randomly generated noise or locally generated images, it does not show the level of performance FedMix is able to show. Thus, we claim that FedMix properly incorporates relevant information received."
        ]
    },
    "A6.T10": {
        "caption": "",
        "table": "<table id=\"A6.T10.3.3.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A6.T10.3.3.1.1\" class=\"ltx_tr\">\n<th id=\"A6.T10.3.3.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding-top:-3.5pt;padding-bottom:-3.5pt;\"><span id=\"A6.T10.3.3.1.1.1.1\" class=\"ltx_text ltx_font_bold\"># of Local Epochs (<math id=\"A6.T10.3.3.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"E\" display=\"inline\"><semantics id=\"A6.T10.3.3.1.1.1.1.m1.1a\"><mi id=\"A6.T10.3.3.1.1.1.1.m1.1.1\" xref=\"A6.T10.3.3.1.1.1.1.m1.1.1.cmml\">E</mi><annotation-xml encoding=\"MathML-Content\" id=\"A6.T10.3.3.1.1.1.1.m1.1b\"><ci id=\"A6.T10.3.3.1.1.1.1.m1.1.1.cmml\" xref=\"A6.T10.3.3.1.1.1.1.m1.1.1\">ğ¸</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A6.T10.3.3.1.1.1.1.m1.1c\">E</annotation></semantics></math>)</span></th>\n<th id=\"A6.T10.3.3.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:-3.5pt;padding-bottom:-3.5pt;\"><span id=\"A6.T10.3.3.1.1.2.1\" class=\"ltx_text ltx_font_bold\">1</span></th>\n<th id=\"A6.T10.3.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:-3.5pt;padding-bottom:-3.5pt;\"><span id=\"A6.T10.3.3.1.1.3.1\" class=\"ltx_text ltx_font_bold\">2</span></th>\n<th id=\"A6.T10.3.3.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:-3.5pt;padding-bottom:-3.5pt;\"><span id=\"A6.T10.3.3.1.1.4.1\" class=\"ltx_text ltx_font_bold\">5</span></th>\n<th id=\"A6.T10.3.3.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:-3.5pt;padding-bottom:-3.5pt;\"><span id=\"A6.T10.3.3.1.1.5.1\" class=\"ltx_text ltx_font_bold\">10</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A6.T10.3.3.1.2.1\" class=\"ltx_tr\">\n<th id=\"A6.T10.3.3.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-3.5pt;padding-bottom:-3.5pt;\">FedAvg</th>\n<td id=\"A6.T10.3.3.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-3.5pt;padding-bottom:-3.5pt;\">74.4</td>\n<td id=\"A6.T10.3.3.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-3.5pt;padding-bottom:-3.5pt;\">73.8</td>\n<td id=\"A6.T10.3.3.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-3.5pt;padding-bottom:-3.5pt;\">80.7</td>\n<td id=\"A6.T10.3.3.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-3.5pt;padding-bottom:-3.5pt;\">78.9</td>\n</tr>\n<tr id=\"A6.T10.3.3.1.3.2\" class=\"ltx_tr\">\n<th id=\"A6.T10.3.3.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-bottom:2.0pt;padding-top:-3.5pt;padding-bottom:-3.5pt;\">LocalMix</th>\n<td id=\"A6.T10.3.3.1.3.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-3.5pt;padding-bottom:-3.5pt;\">63.7</td>\n<td id=\"A6.T10.3.3.1.3.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-3.5pt;padding-bottom:-3.5pt;\">73.0</td>\n<td id=\"A6.T10.3.3.1.3.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-3.5pt;padding-bottom:-3.5pt;\">74.7</td>\n<td id=\"A6.T10.3.3.1.3.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-3.5pt;padding-bottom:-3.5pt;\">80.0</td>\n</tr>\n<tr id=\"A6.T10.3.3.1.4.3\" class=\"ltx_tr\">\n<th id=\"A6.T10.3.3.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-bottom:2.0pt;padding-top:-3.5pt;padding-bottom:-3.5pt;\">NaiveMix</th>\n<td id=\"A6.T10.3.3.1.4.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-3.5pt;padding-bottom:-3.5pt;\">72.0</td>\n<td id=\"A6.T10.3.3.1.4.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-3.5pt;padding-bottom:-3.5pt;\">77.4</td>\n<td id=\"A6.T10.3.3.1.4.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-3.5pt;padding-bottom:-3.5pt;\">81.0</td>\n<td id=\"A6.T10.3.3.1.4.3.5\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-3.5pt;padding-bottom:-3.5pt;\"><span id=\"A6.T10.3.3.1.4.3.5.1\" class=\"ltx_text ltx_font_bold\">82.6</span></td>\n</tr>\n<tr id=\"A6.T10.3.3.1.5.4\" class=\"ltx_tr\">\n<th id=\"A6.T10.3.3.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\" style=\"padding-top:-3.5pt;padding-bottom:-3.5pt;\">FedMix</th>\n<td id=\"A6.T10.3.3.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:-3.5pt;padding-bottom:-3.5pt;\"><span id=\"A6.T10.3.3.1.5.4.2.1\" class=\"ltx_text ltx_font_bold\">75.8</span></td>\n<td id=\"A6.T10.3.3.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:-3.5pt;padding-bottom:-3.5pt;\"><span id=\"A6.T10.3.3.1.5.4.3.1\" class=\"ltx_text ltx_font_bold\">81.2</span></td>\n<td id=\"A6.T10.3.3.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:-3.5pt;padding-bottom:-3.5pt;\"><span id=\"A6.T10.3.3.1.5.4.4.1\" class=\"ltx_text ltx_font_bold\">83.0</span></td>\n<td id=\"A6.T10.3.3.1.5.4.5\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:-3.5pt;padding-bottom:-3.5pt;\">82.5</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Although conceptually it provides an ideal learning environment for edge devices, the federated learning still has some practical challenges that prevent the widespread application of it (Li etÂ al., 2020a; Kairouz etÂ al., 2019).\nAmong such challenges, the one that we are interested in this paper is the heterogeneity of the data, as data is distributed non-iid across clients in many real-world settings; in other words, each local client data is not fairly drawn from identical underlying distribution. Since each client will learn from different data distributions, it becomes harder for the model to be trained efficiently, as reported in (McMahan etÂ al., 2017). While theoretical evidence on the convergence of FedAvg with non-iid case has recently been shown in (Li etÂ al., 2020c), efficient algorithms suitable for this setting have not yet been developed or systematically examined despite some efforts (Zhao etÂ al., 2018; Hsieh etÂ al., 2020).",
            "Additional concerns involve identification of data by detecting change in exchanged averaged data, in case of continual learning, which involves local data change across time. This issue is exacerbated as there is update of averaged data for every minute change on local data, which makes the client receiving ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} easier to infer the changed portion. One simple suggestion to alleviate this issue would be to only update ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} when there is enough change in local data across enough number of clients, so that such changes are not easily exploitable.",
            "While NaiveMix and FedMix is already superior to FedProx, they are parallel to FedProx modification and can be applied in conjunction with FedProx. We compare performances across FedProx variants of various Mixup algorithms in Table 1. FedMix outperforms vanilla FedProx for various datasets, although they do fall short of default version of FedMix used for the main experiment.",
            "While Mixup is usually performed for image classification tasks, it could be applied for language models. For language datasets, since Mixup cannot be performed on input, we perform Mixup on embeddings (for a detailed explanation of Mixup between hidden states, see Appendix E). When tested on Shakespeare dataset, FedMix and NaiveMix both show better performance than baseline algorithms (Table 2). Note that for this task, LocalMix has the lowest performance, and global Mixup does not result in the superior performance above federated algorithms as expected. We think Mixup does not provide performance boost for this specific task, but claim that MAFL algorithms still result in better performance compared to FedAvg.",
            "We also claim that FedMix is superior compared to other methods under various settings, in terms of varying number of clients (Nğ‘N) and varying number of local data per clients. We observe superior performance of FedMix compared to other algorithms for all settings (see Tables 4 and 4). We also vary the number of local epochs (Eğ¸E) between global updates, and still observe that FedMix outperforms other methods (see Appendix F).",
            "Since FedMix approximates loss function of global Mixup for fixed value of Î»â‰ª1much-less-thanğœ†1\\lambda\\ll 1, we can evaluate the efficiency of approximation by comparing between FedMix and a global Mixup scenario with fixed Î»ğœ†\\lambda value. Table 5 shows varying performance between global Mixup and FedMix under various values of Î»ğœ†\\lambda. As Î»ğœ†\\lambda increases, Mixup data reflects more of the features of external data, resulting in better performance in case of global Mixup. However, this also results in our approximation being much less accurate, and we indeed observe performance of FedMix decreasing instead. The result shows that the hyperparameter Î»ğœ†\\lambda should be chosen to balance between better Mixup and better approximation. However, it seems that high Î»ğœ†\\lambda results in significant decrease in both methods, probably due to external data (which is out-of-distribution for local distribution) being overrepresented during local update.",
            "We claim that our method is efficient when faced with non-iid federated settings. For example, our setting of CIFAR10 having only data from 2 classes per client is very non-iid, as in average a pair of clients share only roughly 20% of data distribution. We test settings for CIFAR10 where clients have data from greater number of classes, and while there is little difference for iid (10 class/client) setting, we observe that FedMix outperform other methods and suffer less from increased heterogeneity from highly non-iid settings (Table 7). In addition, we also observe less decline and better performance for MAFL-based algorithms, FedMix in particular, as we train less number of clients per round, reducing communication burden in cost of performance (Table 7).",
            "Local clients are trained by SGD optimizer with learning rate 0.01 and learning decay rate per round 0.999. We set local batch size as 10 for training. Specific hyperparameter setting for each dataset is explained in following Table 8. Throughout the experiment, Mksubscriptğ‘€ğ‘˜M_{k} is fixed to local clientâ€™s dataset size. Changes in these parameters are indicated, if made, are stated for all experiments. Note that we use a fixed small value of Î»ğœ†\\lambda for MAFL-based algorithms to show superior performance.",
            "In Table 9, we observe that if averaged data for MAFL is substituted for randomly generated noise or locally generated images, it does not show the level of performance FedMix is able to show. Thus, we claim that FedMix properly incorporates relevant information received.",
            "Previous works (McMahan etÂ al., 2017; Caldas etÂ al., 2019) show that number of local epochs, Eğ¸E, affects federated learning performance. We tested the effect of Eğ¸E on CIFAR10. In general, we showed that test performance increases as Eğ¸E increases. In addition, we observed that under various values of Eğ¸E, FedMix shows the best performance compared to other algorithms (see Table 10), being a close second after NaiveMix for E=10ğ¸10E=10. MAFL-based algorithms outperform existing algorithms for all values of Eğ¸E tested.",
            "We present the results in Table 12. While threshold does not hugely affect performance, we observe that a moderately small threshold level of 100 results in the best performance. We suggest that as the threshold level is heightened, there is less overfitting to clients with small size local data, but it also results in a decrease in the number of averaged data received by each client. We indeed find an appropriate value of threshold that maximizes performance.",
            "In case where there are a different number of data per client, the sensitivity of Î»ğœ†\\lambda could also be different compared to when all clients have the same number of data. Results in Table 13 show that there is little change in performance by change in Î»ğœ†\\lambda, especially compared to Table 5. In addition, an inspection of the performance of a global model on individual test data of clients does not reveal any noticeable pattern by the size of local data (see Table 13).",
            "Results show that the introduction of Gaussian noise does result in a decline in performance (Table 15),although the decline is very small. Interestingly as noise gets larger as Ïƒ=0.3ğœ0.3\\sigma=0.3, random noise provides an effect as data augmentation and results in a performance increase compared to Ïƒ=0ğœ0\\sigma=0. This experiment is in line with Appendix D. We conclude that introduction of noise in averaged data could provide us with a reasonable alternative to FedMix with large Mksubscriptğ‘€ğ‘˜M_{k}. While our method does not align directly with differential privacy, we leave as future work how FedMix could be smoothly combined with DP-related methods and how its privacy could be quantified in terms of differential privacy.",
            "Further averaging between entries of ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} practically provides an extension of the range of viable Mksubscriptğ‘€ğ‘˜M_{k} such that it exceeds nksubscriptğ‘›ğ‘˜n_{k}, in the sense that each averaged data is from multiple clientsâ€™ data. Such a process would also result in fewer data included in ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g}, so we tested effect of this procedure on model performance. Table 15 shows that for mğ‘šm-fold extra averaging, we even observe increase in performance, but it quickly declines as mğ‘šm gets too large. This method provides an improvement in privacy while even possibly resulting in better performance.",
            "We varied Mixup ratio Î»ğœ†\\lambda for NaiveMix as well. Results in Table 17 shows that NaiveMix also has an intermediate optimal value of Î»ğœ†\\lambda. The drop in performance for Î»=0.5ğœ†0.5\\lambda=0.5 is much more dramatic than for FedMix (see Table 5 for comparison with Global Mixup and FedMix). We think that NaiveMix loss also suffers as it gives more weight to the averaged data, especially for large Mksubscriptğ‘€ğ‘˜M_{k}."
        ]
    },
    "A8.T12": {
        "caption": "",
        "table": "<table id=\"A8.T12.4.4.2\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A8.T12.4.4.2.3.1\" class=\"ltx_tr\">\n<th id=\"A8.T12.4.4.2.3.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\"></th>\n<th id=\"A8.T12.4.4.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\" colspan=\"4\"><span id=\"A8.T12.4.4.2.3.1.2.1\" class=\"ltx_text ltx_font_bold\">â€”â€”â€”â€“thresholdâ€”â€”â€”â€“</span></th>\n</tr>\n<tr id=\"A8.T12.3.3.1.1\" class=\"ltx_tr\">\n<th id=\"A8.T12.3.3.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\"><math id=\"A8.T12.3.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\lambda\" display=\"inline\"><semantics id=\"A8.T12.3.3.1.1.1.m1.1a\"><mi id=\"A8.T12.3.3.1.1.1.m1.1.1\" xref=\"A8.T12.3.3.1.1.1.m1.1.1.cmml\">Î»</mi><annotation-xml encoding=\"MathML-Content\" id=\"A8.T12.3.3.1.1.1.m1.1b\"><ci id=\"A8.T12.3.3.1.1.1.m1.1.1.cmml\" xref=\"A8.T12.3.3.1.1.1.m1.1.1\">ğœ†</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A8.T12.3.3.1.1.1.m1.1c\">\\lambda</annotation></semantics></math></th>\n<th id=\"A8.T12.3.3.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"A8.T12.3.3.1.1.2.1\" class=\"ltx_text ltx_font_bold\">1</span></th>\n<th id=\"A8.T12.3.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"A8.T12.3.3.1.1.3.1\" class=\"ltx_text ltx_font_bold\">100</span></th>\n<th id=\"A8.T12.3.3.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"A8.T12.3.3.1.1.4.1\" class=\"ltx_text ltx_font_bold\">150</span></th>\n<th id=\"A8.T12.3.3.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"A8.T12.3.3.1.1.5.1\" class=\"ltx_text ltx_font_bold\">200</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A8.T12.4.4.2.4.1\" class=\"ltx_tr\">\n<th id=\"A8.T12.4.4.2.4.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">Test Acc. (%)</th>\n<td id=\"A8.T12.4.4.2.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">83.0</td>\n<td id=\"A8.T12.4.4.2.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"A8.T12.4.4.2.4.1.3.1\" class=\"ltx_text ltx_font_bold\">83.3</span></td>\n<td id=\"A8.T12.4.4.2.4.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">83.1</td>\n<td id=\"A8.T12.4.4.2.4.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">82.9</td>\n</tr>\n<tr id=\"A8.T12.4.4.2.2\" class=\"ltx_tr\">\n<th id=\"A8.T12.4.4.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\"><math id=\"A8.T12.4.4.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\lambda_{\\mathtt{optimal}}\" display=\"inline\"><semantics id=\"A8.T12.4.4.2.2.1.m1.1a\"><msub id=\"A8.T12.4.4.2.2.1.m1.1.1\" xref=\"A8.T12.4.4.2.2.1.m1.1.1.cmml\"><mi id=\"A8.T12.4.4.2.2.1.m1.1.1.2\" xref=\"A8.T12.4.4.2.2.1.m1.1.1.2.cmml\">Î»</mi><mi id=\"A8.T12.4.4.2.2.1.m1.1.1.3\" xref=\"A8.T12.4.4.2.2.1.m1.1.1.3.cmml\">ğš˜ğš™ğšğš’ğš–ğšŠğš•</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"A8.T12.4.4.2.2.1.m1.1b\"><apply id=\"A8.T12.4.4.2.2.1.m1.1.1.cmml\" xref=\"A8.T12.4.4.2.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A8.T12.4.4.2.2.1.m1.1.1.1.cmml\" xref=\"A8.T12.4.4.2.2.1.m1.1.1\">subscript</csymbol><ci id=\"A8.T12.4.4.2.2.1.m1.1.1.2.cmml\" xref=\"A8.T12.4.4.2.2.1.m1.1.1.2\">ğœ†</ci><ci id=\"A8.T12.4.4.2.2.1.m1.1.1.3.cmml\" xref=\"A8.T12.4.4.2.2.1.m1.1.1.3\">ğš˜ğš™ğšğš’ğš–ğšŠğš•</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A8.T12.4.4.2.2.1.m1.1c\">\\lambda_{\\mathtt{optimal}}</annotation></semantics></math></th>\n<td id=\"A8.T12.4.4.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">0.2</td>\n<td id=\"A8.T12.4.4.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">0.05</td>\n<td id=\"A8.T12.4.4.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">0.1</td>\n<td id=\"A8.T12.4.4.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">0.2</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Although conceptually it provides an ideal learning environment for edge devices, the federated learning still has some practical challenges that prevent the widespread application of it (Li etÂ al., 2020a; Kairouz etÂ al., 2019).\nAmong such challenges, the one that we are interested in this paper is the heterogeneity of the data, as data is distributed non-iid across clients in many real-world settings; in other words, each local client data is not fairly drawn from identical underlying distribution. Since each client will learn from different data distributions, it becomes harder for the model to be trained efficiently, as reported in (McMahan etÂ al., 2017). While theoretical evidence on the convergence of FedAvg with non-iid case has recently been shown in (Li etÂ al., 2020c), efficient algorithms suitable for this setting have not yet been developed or systematically examined despite some efforts (Zhao etÂ al., 2018; Hsieh etÂ al., 2020).",
            "Additional concerns involve identification of data by detecting change in exchanged averaged data, in case of continual learning, which involves local data change across time. This issue is exacerbated as there is update of averaged data for every minute change on local data, which makes the client receiving ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} easier to infer the changed portion. One simple suggestion to alleviate this issue would be to only update ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} when there is enough change in local data across enough number of clients, so that such changes are not easily exploitable.",
            "While NaiveMix and FedMix is already superior to FedProx, they are parallel to FedProx modification and can be applied in conjunction with FedProx. We compare performances across FedProx variants of various Mixup algorithms in Table 1. FedMix outperforms vanilla FedProx for various datasets, although they do fall short of default version of FedMix used for the main experiment.",
            "While Mixup is usually performed for image classification tasks, it could be applied for language models. For language datasets, since Mixup cannot be performed on input, we perform Mixup on embeddings (for a detailed explanation of Mixup between hidden states, see Appendix E). When tested on Shakespeare dataset, FedMix and NaiveMix both show better performance than baseline algorithms (Table 2). Note that for this task, LocalMix has the lowest performance, and global Mixup does not result in the superior performance above federated algorithms as expected. We think Mixup does not provide performance boost for this specific task, but claim that MAFL algorithms still result in better performance compared to FedAvg.",
            "We also claim that FedMix is superior compared to other methods under various settings, in terms of varying number of clients (Nğ‘N) and varying number of local data per clients. We observe superior performance of FedMix compared to other algorithms for all settings (see Tables 4 and 4). We also vary the number of local epochs (Eğ¸E) between global updates, and still observe that FedMix outperforms other methods (see Appendix F).",
            "Since FedMix approximates loss function of global Mixup for fixed value of Î»â‰ª1much-less-thanğœ†1\\lambda\\ll 1, we can evaluate the efficiency of approximation by comparing between FedMix and a global Mixup scenario with fixed Î»ğœ†\\lambda value. Table 5 shows varying performance between global Mixup and FedMix under various values of Î»ğœ†\\lambda. As Î»ğœ†\\lambda increases, Mixup data reflects more of the features of external data, resulting in better performance in case of global Mixup. However, this also results in our approximation being much less accurate, and we indeed observe performance of FedMix decreasing instead. The result shows that the hyperparameter Î»ğœ†\\lambda should be chosen to balance between better Mixup and better approximation. However, it seems that high Î»ğœ†\\lambda results in significant decrease in both methods, probably due to external data (which is out-of-distribution for local distribution) being overrepresented during local update.",
            "We claim that our method is efficient when faced with non-iid federated settings. For example, our setting of CIFAR10 having only data from 2 classes per client is very non-iid, as in average a pair of clients share only roughly 20% of data distribution. We test settings for CIFAR10 where clients have data from greater number of classes, and while there is little difference for iid (10 class/client) setting, we observe that FedMix outperform other methods and suffer less from increased heterogeneity from highly non-iid settings (Table 7). In addition, we also observe less decline and better performance for MAFL-based algorithms, FedMix in particular, as we train less number of clients per round, reducing communication burden in cost of performance (Table 7).",
            "Local clients are trained by SGD optimizer with learning rate 0.01 and learning decay rate per round 0.999. We set local batch size as 10 for training. Specific hyperparameter setting for each dataset is explained in following Table 8. Throughout the experiment, Mksubscriptğ‘€ğ‘˜M_{k} is fixed to local clientâ€™s dataset size. Changes in these parameters are indicated, if made, are stated for all experiments. Note that we use a fixed small value of Î»ğœ†\\lambda for MAFL-based algorithms to show superior performance.",
            "In Table 9, we observe that if averaged data for MAFL is substituted for randomly generated noise or locally generated images, it does not show the level of performance FedMix is able to show. Thus, we claim that FedMix properly incorporates relevant information received.",
            "Previous works (McMahan etÂ al., 2017; Caldas etÂ al., 2019) show that number of local epochs, Eğ¸E, affects federated learning performance. We tested the effect of Eğ¸E on CIFAR10. In general, we showed that test performance increases as Eğ¸E increases. In addition, we observed that under various values of Eğ¸E, FedMix shows the best performance compared to other algorithms (see Table 10), being a close second after NaiveMix for E=10ğ¸10E=10. MAFL-based algorithms outperform existing algorithms for all values of Eğ¸E tested.",
            "We present the results in Table 12. While threshold does not hugely affect performance, we observe that a moderately small threshold level of 100 results in the best performance. We suggest that as the threshold level is heightened, there is less overfitting to clients with small size local data, but it also results in a decrease in the number of averaged data received by each client. We indeed find an appropriate value of threshold that maximizes performance.",
            "In case where there are a different number of data per client, the sensitivity of Î»ğœ†\\lambda could also be different compared to when all clients have the same number of data. Results in Table 13 show that there is little change in performance by change in Î»ğœ†\\lambda, especially compared to Table 5. In addition, an inspection of the performance of a global model on individual test data of clients does not reveal any noticeable pattern by the size of local data (see Table 13).",
            "Results show that the introduction of Gaussian noise does result in a decline in performance (Table 15),although the decline is very small. Interestingly as noise gets larger as Ïƒ=0.3ğœ0.3\\sigma=0.3, random noise provides an effect as data augmentation and results in a performance increase compared to Ïƒ=0ğœ0\\sigma=0. This experiment is in line with Appendix D. We conclude that introduction of noise in averaged data could provide us with a reasonable alternative to FedMix with large Mksubscriptğ‘€ğ‘˜M_{k}. While our method does not align directly with differential privacy, we leave as future work how FedMix could be smoothly combined with DP-related methods and how its privacy could be quantified in terms of differential privacy.",
            "Further averaging between entries of ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} practically provides an extension of the range of viable Mksubscriptğ‘€ğ‘˜M_{k} such that it exceeds nksubscriptğ‘›ğ‘˜n_{k}, in the sense that each averaged data is from multiple clientsâ€™ data. Such a process would also result in fewer data included in ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g}, so we tested effect of this procedure on model performance. Table 15 shows that for mğ‘šm-fold extra averaging, we even observe increase in performance, but it quickly declines as mğ‘šm gets too large. This method provides an improvement in privacy while even possibly resulting in better performance.",
            "We varied Mixup ratio Î»ğœ†\\lambda for NaiveMix as well. Results in Table 17 shows that NaiveMix also has an intermediate optimal value of Î»ğœ†\\lambda. The drop in performance for Î»=0.5ğœ†0.5\\lambda=0.5 is much more dramatic than for FedMix (see Table 5 for comparison with Global Mixup and FedMix). We think that NaiveMix loss also suffers as it gives more weight to the averaged data, especially for large Mksubscriptğ‘€ğ‘˜M_{k}."
        ]
    },
    "A8.T13": {
        "caption": "",
        "table": "<table id=\"A8.T13.6.6.4\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A8.T13.5.5.3.3\" class=\"ltx_tr\">\n<th id=\"A8.T13.5.5.3.3.4\" class=\"ltx_td ltx_th ltx_th_row ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\"></th>\n<th id=\"A8.T13.3.3.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\" colspan=\"2\"><math id=\"A8.T13.3.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"n_{k}&lt;100\" display=\"inline\"><semantics id=\"A8.T13.3.3.1.1.1.m1.1a\"><mrow id=\"A8.T13.3.3.1.1.1.m1.1.1\" xref=\"A8.T13.3.3.1.1.1.m1.1.1.cmml\"><msub id=\"A8.T13.3.3.1.1.1.m1.1.1.2\" xref=\"A8.T13.3.3.1.1.1.m1.1.1.2.cmml\"><mi id=\"A8.T13.3.3.1.1.1.m1.1.1.2.2\" xref=\"A8.T13.3.3.1.1.1.m1.1.1.2.2.cmml\">n</mi><mi id=\"A8.T13.3.3.1.1.1.m1.1.1.2.3\" xref=\"A8.T13.3.3.1.1.1.m1.1.1.2.3.cmml\">k</mi></msub><mo id=\"A8.T13.3.3.1.1.1.m1.1.1.1\" xref=\"A8.T13.3.3.1.1.1.m1.1.1.1.cmml\">&lt;</mo><mn id=\"A8.T13.3.3.1.1.1.m1.1.1.3\" xref=\"A8.T13.3.3.1.1.1.m1.1.1.3.cmml\">100</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A8.T13.3.3.1.1.1.m1.1b\"><apply id=\"A8.T13.3.3.1.1.1.m1.1.1.cmml\" xref=\"A8.T13.3.3.1.1.1.m1.1.1\"><lt id=\"A8.T13.3.3.1.1.1.m1.1.1.1.cmml\" xref=\"A8.T13.3.3.1.1.1.m1.1.1.1\"></lt><apply id=\"A8.T13.3.3.1.1.1.m1.1.1.2.cmml\" xref=\"A8.T13.3.3.1.1.1.m1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"A8.T13.3.3.1.1.1.m1.1.1.2.1.cmml\" xref=\"A8.T13.3.3.1.1.1.m1.1.1.2\">subscript</csymbol><ci id=\"A8.T13.3.3.1.1.1.m1.1.1.2.2.cmml\" xref=\"A8.T13.3.3.1.1.1.m1.1.1.2.2\">ğ‘›</ci><ci id=\"A8.T13.3.3.1.1.1.m1.1.1.2.3.cmml\" xref=\"A8.T13.3.3.1.1.1.m1.1.1.2.3\">ğ‘˜</ci></apply><cn type=\"integer\" id=\"A8.T13.3.3.1.1.1.m1.1.1.3.cmml\" xref=\"A8.T13.3.3.1.1.1.m1.1.1.3\">100</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A8.T13.3.3.1.1.1.m1.1c\">n_{k}&lt;100</annotation></semantics></math></th>\n<th id=\"A8.T13.4.4.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\" colspan=\"2\"><math id=\"A8.T13.4.4.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"100\\leq n_{k}\\geq 199\" display=\"inline\"><semantics id=\"A8.T13.4.4.2.2.2.m1.1a\"><mrow id=\"A8.T13.4.4.2.2.2.m1.1.1\" xref=\"A8.T13.4.4.2.2.2.m1.1.1.cmml\"><mn id=\"A8.T13.4.4.2.2.2.m1.1.1.2\" xref=\"A8.T13.4.4.2.2.2.m1.1.1.2.cmml\">100</mn><mo id=\"A8.T13.4.4.2.2.2.m1.1.1.3\" xref=\"A8.T13.4.4.2.2.2.m1.1.1.3.cmml\">â‰¤</mo><msub id=\"A8.T13.4.4.2.2.2.m1.1.1.4\" xref=\"A8.T13.4.4.2.2.2.m1.1.1.4.cmml\"><mi id=\"A8.T13.4.4.2.2.2.m1.1.1.4.2\" xref=\"A8.T13.4.4.2.2.2.m1.1.1.4.2.cmml\">n</mi><mi id=\"A8.T13.4.4.2.2.2.m1.1.1.4.3\" xref=\"A8.T13.4.4.2.2.2.m1.1.1.4.3.cmml\">k</mi></msub><mo id=\"A8.T13.4.4.2.2.2.m1.1.1.5\" xref=\"A8.T13.4.4.2.2.2.m1.1.1.5.cmml\">â‰¥</mo><mn id=\"A8.T13.4.4.2.2.2.m1.1.1.6\" xref=\"A8.T13.4.4.2.2.2.m1.1.1.6.cmml\">199</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A8.T13.4.4.2.2.2.m1.1b\"><apply id=\"A8.T13.4.4.2.2.2.m1.1.1.cmml\" xref=\"A8.T13.4.4.2.2.2.m1.1.1\"><and id=\"A8.T13.4.4.2.2.2.m1.1.1a.cmml\" xref=\"A8.T13.4.4.2.2.2.m1.1.1\"></and><apply id=\"A8.T13.4.4.2.2.2.m1.1.1b.cmml\" xref=\"A8.T13.4.4.2.2.2.m1.1.1\"><leq id=\"A8.T13.4.4.2.2.2.m1.1.1.3.cmml\" xref=\"A8.T13.4.4.2.2.2.m1.1.1.3\"></leq><cn type=\"integer\" id=\"A8.T13.4.4.2.2.2.m1.1.1.2.cmml\" xref=\"A8.T13.4.4.2.2.2.m1.1.1.2\">100</cn><apply id=\"A8.T13.4.4.2.2.2.m1.1.1.4.cmml\" xref=\"A8.T13.4.4.2.2.2.m1.1.1.4\"><csymbol cd=\"ambiguous\" id=\"A8.T13.4.4.2.2.2.m1.1.1.4.1.cmml\" xref=\"A8.T13.4.4.2.2.2.m1.1.1.4\">subscript</csymbol><ci id=\"A8.T13.4.4.2.2.2.m1.1.1.4.2.cmml\" xref=\"A8.T13.4.4.2.2.2.m1.1.1.4.2\">ğ‘›</ci><ci id=\"A8.T13.4.4.2.2.2.m1.1.1.4.3.cmml\" xref=\"A8.T13.4.4.2.2.2.m1.1.1.4.3\">ğ‘˜</ci></apply></apply><apply id=\"A8.T13.4.4.2.2.2.m1.1.1c.cmml\" xref=\"A8.T13.4.4.2.2.2.m1.1.1\"><geq id=\"A8.T13.4.4.2.2.2.m1.1.1.5.cmml\" xref=\"A8.T13.4.4.2.2.2.m1.1.1.5\"></geq><share href=\"#A8.T13.4.4.2.2.2.m1.1.1.4.cmml\" id=\"A8.T13.4.4.2.2.2.m1.1.1d.cmml\" xref=\"A8.T13.4.4.2.2.2.m1.1.1\"></share><cn type=\"integer\" id=\"A8.T13.4.4.2.2.2.m1.1.1.6.cmml\" xref=\"A8.T13.4.4.2.2.2.m1.1.1.6\">199</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A8.T13.4.4.2.2.2.m1.1c\">100\\leq n_{k}\\geq 199</annotation></semantics></math></th>\n<th id=\"A8.T13.5.5.3.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\" colspan=\"2\"><math id=\"A8.T13.5.5.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"n_{k}&gt;199\" display=\"inline\"><semantics id=\"A8.T13.5.5.3.3.3.m1.1a\"><mrow id=\"A8.T13.5.5.3.3.3.m1.1.1\" xref=\"A8.T13.5.5.3.3.3.m1.1.1.cmml\"><msub id=\"A8.T13.5.5.3.3.3.m1.1.1.2\" xref=\"A8.T13.5.5.3.3.3.m1.1.1.2.cmml\"><mi id=\"A8.T13.5.5.3.3.3.m1.1.1.2.2\" xref=\"A8.T13.5.5.3.3.3.m1.1.1.2.2.cmml\">n</mi><mi id=\"A8.T13.5.5.3.3.3.m1.1.1.2.3\" xref=\"A8.T13.5.5.3.3.3.m1.1.1.2.3.cmml\">k</mi></msub><mo id=\"A8.T13.5.5.3.3.3.m1.1.1.1\" xref=\"A8.T13.5.5.3.3.3.m1.1.1.1.cmml\">&gt;</mo><mn id=\"A8.T13.5.5.3.3.3.m1.1.1.3\" xref=\"A8.T13.5.5.3.3.3.m1.1.1.3.cmml\">199</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A8.T13.5.5.3.3.3.m1.1b\"><apply id=\"A8.T13.5.5.3.3.3.m1.1.1.cmml\" xref=\"A8.T13.5.5.3.3.3.m1.1.1\"><gt id=\"A8.T13.5.5.3.3.3.m1.1.1.1.cmml\" xref=\"A8.T13.5.5.3.3.3.m1.1.1.1\"></gt><apply id=\"A8.T13.5.5.3.3.3.m1.1.1.2.cmml\" xref=\"A8.T13.5.5.3.3.3.m1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"A8.T13.5.5.3.3.3.m1.1.1.2.1.cmml\" xref=\"A8.T13.5.5.3.3.3.m1.1.1.2\">subscript</csymbol><ci id=\"A8.T13.5.5.3.3.3.m1.1.1.2.2.cmml\" xref=\"A8.T13.5.5.3.3.3.m1.1.1.2.2\">ğ‘›</ci><ci id=\"A8.T13.5.5.3.3.3.m1.1.1.2.3.cmml\" xref=\"A8.T13.5.5.3.3.3.m1.1.1.2.3\">ğ‘˜</ci></apply><cn type=\"integer\" id=\"A8.T13.5.5.3.3.3.m1.1.1.3.cmml\" xref=\"A8.T13.5.5.3.3.3.m1.1.1.3\">199</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A8.T13.5.5.3.3.3.m1.1c\">n_{k}&gt;199</annotation></semantics></math></th>\n</tr>\n<tr id=\"A8.T13.6.6.4.4\" class=\"ltx_tr\">\n<th id=\"A8.T13.6.6.4.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\"><math id=\"A8.T13.6.6.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\lambda\" display=\"inline\"><semantics id=\"A8.T13.6.6.4.4.1.m1.1a\"><mi id=\"A8.T13.6.6.4.4.1.m1.1.1\" xref=\"A8.T13.6.6.4.4.1.m1.1.1.cmml\">Î»</mi><annotation-xml encoding=\"MathML-Content\" id=\"A8.T13.6.6.4.4.1.m1.1b\"><ci id=\"A8.T13.6.6.4.4.1.m1.1.1.cmml\" xref=\"A8.T13.6.6.4.4.1.m1.1.1\">ğœ†</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A8.T13.6.6.4.4.1.m1.1c\">\\lambda</annotation></semantics></math></th>\n<th id=\"A8.T13.6.6.4.4.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\">mean</th>\n<th id=\"A8.T13.6.6.4.4.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\">std</th>\n<th id=\"A8.T13.6.6.4.4.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\">mean</th>\n<th id=\"A8.T13.6.6.4.4.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\">std</th>\n<th id=\"A8.T13.6.6.4.4.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\">mean</th>\n<th id=\"A8.T13.6.6.4.4.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\">std</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A8.T13.6.6.4.5.1\" class=\"ltx_tr\">\n<th id=\"A8.T13.6.6.4.5.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\">0.05</th>\n<td id=\"A8.T13.6.6.4.5.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\">85.8</td>\n<td id=\"A8.T13.6.6.4.5.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\">15.1</td>\n<td id=\"A8.T13.6.6.4.5.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\">77.6</td>\n<td id=\"A8.T13.6.6.4.5.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\">14.8</td>\n<td id=\"A8.T13.6.6.4.5.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\">85.4</td>\n<td id=\"A8.T13.6.6.4.5.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\">9.3</td>\n</tr>\n<tr id=\"A8.T13.6.6.4.6.2\" class=\"ltx_tr\">\n<th id=\"A8.T13.6.6.4.6.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\">0.1</th>\n<td id=\"A8.T13.6.6.4.6.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\">81.1</td>\n<td id=\"A8.T13.6.6.4.6.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\">17.4</td>\n<td id=\"A8.T13.6.6.4.6.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\">76.4</td>\n<td id=\"A8.T13.6.6.4.6.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\">14.1</td>\n<td id=\"A8.T13.6.6.4.6.2.6\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\">86.2</td>\n<td id=\"A8.T13.6.6.4.6.2.7\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\">9.5</td>\n</tr>\n<tr id=\"A8.T13.6.6.4.7.3\" class=\"ltx_tr\">\n<th id=\"A8.T13.6.6.4.7.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\">0.2</th>\n<td id=\"A8.T13.6.6.4.7.3.2\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\">83.9</td>\n<td id=\"A8.T13.6.6.4.7.3.3\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\">16.2</td>\n<td id=\"A8.T13.6.6.4.7.3.4\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\">77.6</td>\n<td id=\"A8.T13.6.6.4.7.3.5\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\">14.8</td>\n<td id=\"A8.T13.6.6.4.7.3.6\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\">85.8</td>\n<td id=\"A8.T13.6.6.4.7.3.7\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-bottom:2.0pt;padding-top:-4pt;padding-bottom:-4pt;\">10.0</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Although conceptually it provides an ideal learning environment for edge devices, the federated learning still has some practical challenges that prevent the widespread application of it (Li etÂ al., 2020a; Kairouz etÂ al., 2019).\nAmong such challenges, the one that we are interested in this paper is the heterogeneity of the data, as data is distributed non-iid across clients in many real-world settings; in other words, each local client data is not fairly drawn from identical underlying distribution. Since each client will learn from different data distributions, it becomes harder for the model to be trained efficiently, as reported in (McMahan etÂ al., 2017). While theoretical evidence on the convergence of FedAvg with non-iid case has recently been shown in (Li etÂ al., 2020c), efficient algorithms suitable for this setting have not yet been developed or systematically examined despite some efforts (Zhao etÂ al., 2018; Hsieh etÂ al., 2020).",
            "Additional concerns involve identification of data by detecting change in exchanged averaged data, in case of continual learning, which involves local data change across time. This issue is exacerbated as there is update of averaged data for every minute change on local data, which makes the client receiving ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} easier to infer the changed portion. One simple suggestion to alleviate this issue would be to only update ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} when there is enough change in local data across enough number of clients, so that such changes are not easily exploitable.",
            "While NaiveMix and FedMix is already superior to FedProx, they are parallel to FedProx modification and can be applied in conjunction with FedProx. We compare performances across FedProx variants of various Mixup algorithms in Table 1. FedMix outperforms vanilla FedProx for various datasets, although they do fall short of default version of FedMix used for the main experiment.",
            "While Mixup is usually performed for image classification tasks, it could be applied for language models. For language datasets, since Mixup cannot be performed on input, we perform Mixup on embeddings (for a detailed explanation of Mixup between hidden states, see Appendix E). When tested on Shakespeare dataset, FedMix and NaiveMix both show better performance than baseline algorithms (Table 2). Note that for this task, LocalMix has the lowest performance, and global Mixup does not result in the superior performance above federated algorithms as expected. We think Mixup does not provide performance boost for this specific task, but claim that MAFL algorithms still result in better performance compared to FedAvg.",
            "We also claim that FedMix is superior compared to other methods under various settings, in terms of varying number of clients (Nğ‘N) and varying number of local data per clients. We observe superior performance of FedMix compared to other algorithms for all settings (see Tables 4 and 4). We also vary the number of local epochs (Eğ¸E) between global updates, and still observe that FedMix outperforms other methods (see Appendix F).",
            "Since FedMix approximates loss function of global Mixup for fixed value of Î»â‰ª1much-less-thanğœ†1\\lambda\\ll 1, we can evaluate the efficiency of approximation by comparing between FedMix and a global Mixup scenario with fixed Î»ğœ†\\lambda value. Table 5 shows varying performance between global Mixup and FedMix under various values of Î»ğœ†\\lambda. As Î»ğœ†\\lambda increases, Mixup data reflects more of the features of external data, resulting in better performance in case of global Mixup. However, this also results in our approximation being much less accurate, and we indeed observe performance of FedMix decreasing instead. The result shows that the hyperparameter Î»ğœ†\\lambda should be chosen to balance between better Mixup and better approximation. However, it seems that high Î»ğœ†\\lambda results in significant decrease in both methods, probably due to external data (which is out-of-distribution for local distribution) being overrepresented during local update.",
            "We claim that our method is efficient when faced with non-iid federated settings. For example, our setting of CIFAR10 having only data from 2 classes per client is very non-iid, as in average a pair of clients share only roughly 20% of data distribution. We test settings for CIFAR10 where clients have data from greater number of classes, and while there is little difference for iid (10 class/client) setting, we observe that FedMix outperform other methods and suffer less from increased heterogeneity from highly non-iid settings (Table 7). In addition, we also observe less decline and better performance for MAFL-based algorithms, FedMix in particular, as we train less number of clients per round, reducing communication burden in cost of performance (Table 7).",
            "Local clients are trained by SGD optimizer with learning rate 0.01 and learning decay rate per round 0.999. We set local batch size as 10 for training. Specific hyperparameter setting for each dataset is explained in following Table 8. Throughout the experiment, Mksubscriptğ‘€ğ‘˜M_{k} is fixed to local clientâ€™s dataset size. Changes in these parameters are indicated, if made, are stated for all experiments. Note that we use a fixed small value of Î»ğœ†\\lambda for MAFL-based algorithms to show superior performance.",
            "In Table 9, we observe that if averaged data for MAFL is substituted for randomly generated noise or locally generated images, it does not show the level of performance FedMix is able to show. Thus, we claim that FedMix properly incorporates relevant information received.",
            "Previous works (McMahan etÂ al., 2017; Caldas etÂ al., 2019) show that number of local epochs, Eğ¸E, affects federated learning performance. We tested the effect of Eğ¸E on CIFAR10. In general, we showed that test performance increases as Eğ¸E increases. In addition, we observed that under various values of Eğ¸E, FedMix shows the best performance compared to other algorithms (see Table 10), being a close second after NaiveMix for E=10ğ¸10E=10. MAFL-based algorithms outperform existing algorithms for all values of Eğ¸E tested.",
            "We present the results in Table 12. While threshold does not hugely affect performance, we observe that a moderately small threshold level of 100 results in the best performance. We suggest that as the threshold level is heightened, there is less overfitting to clients with small size local data, but it also results in a decrease in the number of averaged data received by each client. We indeed find an appropriate value of threshold that maximizes performance.",
            "In case where there are a different number of data per client, the sensitivity of Î»ğœ†\\lambda could also be different compared to when all clients have the same number of data. Results in Table 13 show that there is little change in performance by change in Î»ğœ†\\lambda, especially compared to Table 5. In addition, an inspection of the performance of a global model on individual test data of clients does not reveal any noticeable pattern by the size of local data (see Table 13).",
            "Results show that the introduction of Gaussian noise does result in a decline in performance (Table 15),although the decline is very small. Interestingly as noise gets larger as Ïƒ=0.3ğœ0.3\\sigma=0.3, random noise provides an effect as data augmentation and results in a performance increase compared to Ïƒ=0ğœ0\\sigma=0. This experiment is in line with Appendix D. We conclude that introduction of noise in averaged data could provide us with a reasonable alternative to FedMix with large Mksubscriptğ‘€ğ‘˜M_{k}. While our method does not align directly with differential privacy, we leave as future work how FedMix could be smoothly combined with DP-related methods and how its privacy could be quantified in terms of differential privacy.",
            "Further averaging between entries of ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} practically provides an extension of the range of viable Mksubscriptğ‘€ğ‘˜M_{k} such that it exceeds nksubscriptğ‘›ğ‘˜n_{k}, in the sense that each averaged data is from multiple clientsâ€™ data. Such a process would also result in fewer data included in ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g}, so we tested effect of this procedure on model performance. Table 15 shows that for mğ‘šm-fold extra averaging, we even observe increase in performance, but it quickly declines as mğ‘šm gets too large. This method provides an improvement in privacy while even possibly resulting in better performance.",
            "We varied Mixup ratio Î»ğœ†\\lambda for NaiveMix as well. Results in Table 17 shows that NaiveMix also has an intermediate optimal value of Î»ğœ†\\lambda. The drop in performance for Î»=0.5ğœ†0.5\\lambda=0.5 is much more dramatic than for FedMix (see Table 5 for comparison with Global Mixup and FedMix). We think that NaiveMix loss also suffers as it gives more weight to the averaged data, especially for large Mksubscriptğ‘€ğ‘˜M_{k}."
        ]
    },
    "A10.T15": {
        "caption": "",
        "table": "<table id=\"A10.T15.6.6.4\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A10.T15.3.3.1.1\" class=\"ltx_tr\">\n<th id=\"A10.T15.3.3.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\"><math id=\"A10.T15.3.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sigma\" display=\"inline\"><semantics id=\"A10.T15.3.3.1.1.1.m1.1a\"><mi id=\"A10.T15.3.3.1.1.1.m1.1.1\" xref=\"A10.T15.3.3.1.1.1.m1.1.1.cmml\">Ïƒ</mi><annotation-xml encoding=\"MathML-Content\" id=\"A10.T15.3.3.1.1.1.m1.1b\"><ci id=\"A10.T15.3.3.1.1.1.m1.1.1.cmml\" xref=\"A10.T15.3.3.1.1.1.m1.1.1\">ğœ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A10.T15.3.3.1.1.1.m1.1c\">\\sigma</annotation></semantics></math></th>\n<th id=\"A10.T15.3.3.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"A10.T15.3.3.1.1.2.1\" class=\"ltx_text ltx_font_bold\">0</span></th>\n<th id=\"A10.T15.3.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"A10.T15.3.3.1.1.3.1\" class=\"ltx_text ltx_font_bold\">0.05</span></th>\n<th id=\"A10.T15.3.3.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"A10.T15.3.3.1.1.4.1\" class=\"ltx_text ltx_font_bold\">0.075</span></th>\n<th id=\"A10.T15.3.3.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"A10.T15.3.3.1.1.5.1\" class=\"ltx_text ltx_font_bold\">0.15</span></th>\n<th id=\"A10.T15.3.3.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"A10.T15.3.3.1.1.6.1\" class=\"ltx_text ltx_font_bold\">0.3</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A10.T15.4.4.2.2\" class=\"ltx_tr\">\n<th id=\"A10.T15.4.4.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\"><math id=\"A10.T15.4.4.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"M_{k}=5\" display=\"inline\"><semantics id=\"A10.T15.4.4.2.2.1.m1.1a\"><mrow id=\"A10.T15.4.4.2.2.1.m1.1.1\" xref=\"A10.T15.4.4.2.2.1.m1.1.1.cmml\"><msub id=\"A10.T15.4.4.2.2.1.m1.1.1.2\" xref=\"A10.T15.4.4.2.2.1.m1.1.1.2.cmml\"><mi id=\"A10.T15.4.4.2.2.1.m1.1.1.2.2\" xref=\"A10.T15.4.4.2.2.1.m1.1.1.2.2.cmml\">M</mi><mi id=\"A10.T15.4.4.2.2.1.m1.1.1.2.3\" xref=\"A10.T15.4.4.2.2.1.m1.1.1.2.3.cmml\">k</mi></msub><mo id=\"A10.T15.4.4.2.2.1.m1.1.1.1\" xref=\"A10.T15.4.4.2.2.1.m1.1.1.1.cmml\">=</mo><mn id=\"A10.T15.4.4.2.2.1.m1.1.1.3\" xref=\"A10.T15.4.4.2.2.1.m1.1.1.3.cmml\">5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A10.T15.4.4.2.2.1.m1.1b\"><apply id=\"A10.T15.4.4.2.2.1.m1.1.1.cmml\" xref=\"A10.T15.4.4.2.2.1.m1.1.1\"><eq id=\"A10.T15.4.4.2.2.1.m1.1.1.1.cmml\" xref=\"A10.T15.4.4.2.2.1.m1.1.1.1\"></eq><apply id=\"A10.T15.4.4.2.2.1.m1.1.1.2.cmml\" xref=\"A10.T15.4.4.2.2.1.m1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"A10.T15.4.4.2.2.1.m1.1.1.2.1.cmml\" xref=\"A10.T15.4.4.2.2.1.m1.1.1.2\">subscript</csymbol><ci id=\"A10.T15.4.4.2.2.1.m1.1.1.2.2.cmml\" xref=\"A10.T15.4.4.2.2.1.m1.1.1.2.2\">ğ‘€</ci><ci id=\"A10.T15.4.4.2.2.1.m1.1.1.2.3.cmml\" xref=\"A10.T15.4.4.2.2.1.m1.1.1.2.3\">ğ‘˜</ci></apply><cn type=\"integer\" id=\"A10.T15.4.4.2.2.1.m1.1.1.3.cmml\" xref=\"A10.T15.4.4.2.2.1.m1.1.1.3\">5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A10.T15.4.4.2.2.1.m1.1c\">M_{k}=5</annotation></semantics></math></th>\n<th id=\"A10.T15.4.4.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">81.4</th>\n<td id=\"A10.T15.4.4.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">80.1</td>\n<td id=\"A10.T15.4.4.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">81.1</td>\n<td id=\"A10.T15.4.4.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">78.7</td>\n<td id=\"A10.T15.4.4.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">81.5</td>\n</tr>\n<tr id=\"A10.T15.5.5.3.3\" class=\"ltx_tr\">\n<th id=\"A10.T15.5.5.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\"><math id=\"A10.T15.5.5.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"M_{k}=10\" display=\"inline\"><semantics id=\"A10.T15.5.5.3.3.1.m1.1a\"><mrow id=\"A10.T15.5.5.3.3.1.m1.1.1\" xref=\"A10.T15.5.5.3.3.1.m1.1.1.cmml\"><msub id=\"A10.T15.5.5.3.3.1.m1.1.1.2\" xref=\"A10.T15.5.5.3.3.1.m1.1.1.2.cmml\"><mi id=\"A10.T15.5.5.3.3.1.m1.1.1.2.2\" xref=\"A10.T15.5.5.3.3.1.m1.1.1.2.2.cmml\">M</mi><mi id=\"A10.T15.5.5.3.3.1.m1.1.1.2.3\" xref=\"A10.T15.5.5.3.3.1.m1.1.1.2.3.cmml\">k</mi></msub><mo id=\"A10.T15.5.5.3.3.1.m1.1.1.1\" xref=\"A10.T15.5.5.3.3.1.m1.1.1.1.cmml\">=</mo><mn id=\"A10.T15.5.5.3.3.1.m1.1.1.3\" xref=\"A10.T15.5.5.3.3.1.m1.1.1.3.cmml\">10</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A10.T15.5.5.3.3.1.m1.1b\"><apply id=\"A10.T15.5.5.3.3.1.m1.1.1.cmml\" xref=\"A10.T15.5.5.3.3.1.m1.1.1\"><eq id=\"A10.T15.5.5.3.3.1.m1.1.1.1.cmml\" xref=\"A10.T15.5.5.3.3.1.m1.1.1.1\"></eq><apply id=\"A10.T15.5.5.3.3.1.m1.1.1.2.cmml\" xref=\"A10.T15.5.5.3.3.1.m1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"A10.T15.5.5.3.3.1.m1.1.1.2.1.cmml\" xref=\"A10.T15.5.5.3.3.1.m1.1.1.2\">subscript</csymbol><ci id=\"A10.T15.5.5.3.3.1.m1.1.1.2.2.cmml\" xref=\"A10.T15.5.5.3.3.1.m1.1.1.2.2\">ğ‘€</ci><ci id=\"A10.T15.5.5.3.3.1.m1.1.1.2.3.cmml\" xref=\"A10.T15.5.5.3.3.1.m1.1.1.2.3\">ğ‘˜</ci></apply><cn type=\"integer\" id=\"A10.T15.5.5.3.3.1.m1.1.1.3.cmml\" xref=\"A10.T15.5.5.3.3.1.m1.1.1.3\">10</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A10.T15.5.5.3.3.1.m1.1c\">M_{k}=10</annotation></semantics></math></th>\n<th id=\"A10.T15.5.5.3.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">79.9</th>\n<td id=\"A10.T15.5.5.3.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">80.8</td>\n<td id=\"A10.T15.5.5.3.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">79.1</td>\n<td id=\"A10.T15.5.5.3.3.5\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">79.4</td>\n<td id=\"A10.T15.5.5.3.3.6\" class=\"ltx_td ltx_align_center\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">81.7</td>\n</tr>\n<tr id=\"A10.T15.6.6.4.4\" class=\"ltx_tr\">\n<th id=\"A10.T15.6.6.4.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\"><math id=\"A10.T15.6.6.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"M_{k}=20\" display=\"inline\"><semantics id=\"A10.T15.6.6.4.4.1.m1.1a\"><mrow id=\"A10.T15.6.6.4.4.1.m1.1.1\" xref=\"A10.T15.6.6.4.4.1.m1.1.1.cmml\"><msub id=\"A10.T15.6.6.4.4.1.m1.1.1.2\" xref=\"A10.T15.6.6.4.4.1.m1.1.1.2.cmml\"><mi id=\"A10.T15.6.6.4.4.1.m1.1.1.2.2\" xref=\"A10.T15.6.6.4.4.1.m1.1.1.2.2.cmml\">M</mi><mi id=\"A10.T15.6.6.4.4.1.m1.1.1.2.3\" xref=\"A10.T15.6.6.4.4.1.m1.1.1.2.3.cmml\">k</mi></msub><mo id=\"A10.T15.6.6.4.4.1.m1.1.1.1\" xref=\"A10.T15.6.6.4.4.1.m1.1.1.1.cmml\">=</mo><mn id=\"A10.T15.6.6.4.4.1.m1.1.1.3\" xref=\"A10.T15.6.6.4.4.1.m1.1.1.3.cmml\">20</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A10.T15.6.6.4.4.1.m1.1b\"><apply id=\"A10.T15.6.6.4.4.1.m1.1.1.cmml\" xref=\"A10.T15.6.6.4.4.1.m1.1.1\"><eq id=\"A10.T15.6.6.4.4.1.m1.1.1.1.cmml\" xref=\"A10.T15.6.6.4.4.1.m1.1.1.1\"></eq><apply id=\"A10.T15.6.6.4.4.1.m1.1.1.2.cmml\" xref=\"A10.T15.6.6.4.4.1.m1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"A10.T15.6.6.4.4.1.m1.1.1.2.1.cmml\" xref=\"A10.T15.6.6.4.4.1.m1.1.1.2\">subscript</csymbol><ci id=\"A10.T15.6.6.4.4.1.m1.1.1.2.2.cmml\" xref=\"A10.T15.6.6.4.4.1.m1.1.1.2.2\">ğ‘€</ci><ci id=\"A10.T15.6.6.4.4.1.m1.1.1.2.3.cmml\" xref=\"A10.T15.6.6.4.4.1.m1.1.1.2.3\">ğ‘˜</ci></apply><cn type=\"integer\" id=\"A10.T15.6.6.4.4.1.m1.1.1.3.cmml\" xref=\"A10.T15.6.6.4.4.1.m1.1.1.3\">20</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A10.T15.6.6.4.4.1.m1.1c\">M_{k}=20</annotation></semantics></math></th>\n<th id=\"A10.T15.6.6.4.4.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">80.4</th>\n<td id=\"A10.T15.6.6.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">80.5</td>\n<td id=\"A10.T15.6.6.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">79.7</td>\n<td id=\"A10.T15.6.6.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">80.7</td>\n<td id=\"A10.T15.6.6.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">81.0</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Although conceptually it provides an ideal learning environment for edge devices, the federated learning still has some practical challenges that prevent the widespread application of it (Li etÂ al., 2020a; Kairouz etÂ al., 2019).\nAmong such challenges, the one that we are interested in this paper is the heterogeneity of the data, as data is distributed non-iid across clients in many real-world settings; in other words, each local client data is not fairly drawn from identical underlying distribution. Since each client will learn from different data distributions, it becomes harder for the model to be trained efficiently, as reported in (McMahan etÂ al., 2017). While theoretical evidence on the convergence of FedAvg with non-iid case has recently been shown in (Li etÂ al., 2020c), efficient algorithms suitable for this setting have not yet been developed or systematically examined despite some efforts (Zhao etÂ al., 2018; Hsieh etÂ al., 2020).",
            "Additional concerns involve identification of data by detecting change in exchanged averaged data, in case of continual learning, which involves local data change across time. This issue is exacerbated as there is update of averaged data for every minute change on local data, which makes the client receiving ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} easier to infer the changed portion. One simple suggestion to alleviate this issue would be to only update ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} when there is enough change in local data across enough number of clients, so that such changes are not easily exploitable.",
            "While NaiveMix and FedMix is already superior to FedProx, they are parallel to FedProx modification and can be applied in conjunction with FedProx. We compare performances across FedProx variants of various Mixup algorithms in Table 1. FedMix outperforms vanilla FedProx for various datasets, although they do fall short of default version of FedMix used for the main experiment.",
            "While Mixup is usually performed for image classification tasks, it could be applied for language models. For language datasets, since Mixup cannot be performed on input, we perform Mixup on embeddings (for a detailed explanation of Mixup between hidden states, see Appendix E). When tested on Shakespeare dataset, FedMix and NaiveMix both show better performance than baseline algorithms (Table 2). Note that for this task, LocalMix has the lowest performance, and global Mixup does not result in the superior performance above federated algorithms as expected. We think Mixup does not provide performance boost for this specific task, but claim that MAFL algorithms still result in better performance compared to FedAvg.",
            "We also claim that FedMix is superior compared to other methods under various settings, in terms of varying number of clients (Nğ‘N) and varying number of local data per clients. We observe superior performance of FedMix compared to other algorithms for all settings (see Tables 4 and 4). We also vary the number of local epochs (Eğ¸E) between global updates, and still observe that FedMix outperforms other methods (see Appendix F).",
            "Since FedMix approximates loss function of global Mixup for fixed value of Î»â‰ª1much-less-thanğœ†1\\lambda\\ll 1, we can evaluate the efficiency of approximation by comparing between FedMix and a global Mixup scenario with fixed Î»ğœ†\\lambda value. Table 5 shows varying performance between global Mixup and FedMix under various values of Î»ğœ†\\lambda. As Î»ğœ†\\lambda increases, Mixup data reflects more of the features of external data, resulting in better performance in case of global Mixup. However, this also results in our approximation being much less accurate, and we indeed observe performance of FedMix decreasing instead. The result shows that the hyperparameter Î»ğœ†\\lambda should be chosen to balance between better Mixup and better approximation. However, it seems that high Î»ğœ†\\lambda results in significant decrease in both methods, probably due to external data (which is out-of-distribution for local distribution) being overrepresented during local update.",
            "We claim that our method is efficient when faced with non-iid federated settings. For example, our setting of CIFAR10 having only data from 2 classes per client is very non-iid, as in average a pair of clients share only roughly 20% of data distribution. We test settings for CIFAR10 where clients have data from greater number of classes, and while there is little difference for iid (10 class/client) setting, we observe that FedMix outperform other methods and suffer less from increased heterogeneity from highly non-iid settings (Table 7). In addition, we also observe less decline and better performance for MAFL-based algorithms, FedMix in particular, as we train less number of clients per round, reducing communication burden in cost of performance (Table 7).",
            "Local clients are trained by SGD optimizer with learning rate 0.01 and learning decay rate per round 0.999. We set local batch size as 10 for training. Specific hyperparameter setting for each dataset is explained in following Table 8. Throughout the experiment, Mksubscriptğ‘€ğ‘˜M_{k} is fixed to local clientâ€™s dataset size. Changes in these parameters are indicated, if made, are stated for all experiments. Note that we use a fixed small value of Î»ğœ†\\lambda for MAFL-based algorithms to show superior performance.",
            "In Table 9, we observe that if averaged data for MAFL is substituted for randomly generated noise or locally generated images, it does not show the level of performance FedMix is able to show. Thus, we claim that FedMix properly incorporates relevant information received.",
            "Previous works (McMahan etÂ al., 2017; Caldas etÂ al., 2019) show that number of local epochs, Eğ¸E, affects federated learning performance. We tested the effect of Eğ¸E on CIFAR10. In general, we showed that test performance increases as Eğ¸E increases. In addition, we observed that under various values of Eğ¸E, FedMix shows the best performance compared to other algorithms (see Table 10), being a close second after NaiveMix for E=10ğ¸10E=10. MAFL-based algorithms outperform existing algorithms for all values of Eğ¸E tested.",
            "We present the results in Table 12. While threshold does not hugely affect performance, we observe that a moderately small threshold level of 100 results in the best performance. We suggest that as the threshold level is heightened, there is less overfitting to clients with small size local data, but it also results in a decrease in the number of averaged data received by each client. We indeed find an appropriate value of threshold that maximizes performance.",
            "In case where there are a different number of data per client, the sensitivity of Î»ğœ†\\lambda could also be different compared to when all clients have the same number of data. Results in Table 13 show that there is little change in performance by change in Î»ğœ†\\lambda, especially compared to Table 5. In addition, an inspection of the performance of a global model on individual test data of clients does not reveal any noticeable pattern by the size of local data (see Table 13).",
            "Results show that the introduction of Gaussian noise does result in a decline in performance (Table 15),although the decline is very small. Interestingly as noise gets larger as Ïƒ=0.3ğœ0.3\\sigma=0.3, random noise provides an effect as data augmentation and results in a performance increase compared to Ïƒ=0ğœ0\\sigma=0. This experiment is in line with Appendix D. We conclude that introduction of noise in averaged data could provide us with a reasonable alternative to FedMix with large Mksubscriptğ‘€ğ‘˜M_{k}. While our method does not align directly with differential privacy, we leave as future work how FedMix could be smoothly combined with DP-related methods and how its privacy could be quantified in terms of differential privacy.",
            "Further averaging between entries of ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} practically provides an extension of the range of viable Mksubscriptğ‘€ğ‘˜M_{k} such that it exceeds nksubscriptğ‘›ğ‘˜n_{k}, in the sense that each averaged data is from multiple clientsâ€™ data. Such a process would also result in fewer data included in ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g}, so we tested effect of this procedure on model performance. Table 15 shows that for mğ‘šm-fold extra averaging, we even observe increase in performance, but it quickly declines as mğ‘šm gets too large. This method provides an improvement in privacy while even possibly resulting in better performance.",
            "We varied Mixup ratio Î»ğœ†\\lambda for NaiveMix as well. Results in Table 17 shows that NaiveMix also has an intermediate optimal value of Î»ğœ†\\lambda. The drop in performance for Î»=0.5ğœ†0.5\\lambda=0.5 is much more dramatic than for FedMix (see Table 5 for comparison with Global Mixup and FedMix). We think that NaiveMix loss also suffers as it gives more weight to the averaged data, especially for large Mksubscriptğ‘€ğ‘˜M_{k}."
        ]
    },
    "A10.T17": {
        "caption": "",
        "table": "<table id=\"A10.T17.2.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"A10.T17.2.3.1.1.1\" class=\"ltx_tr\">\n<td id=\"A10.T17.2.3.1.1.1.1\" class=\"ltx_td ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\"></td>\n<td id=\"A10.T17.2.3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">random split</td>\n<td id=\"A10.T17.2.3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">class split</td>\n</tr>\n<tr id=\"A10.T17.2.3.1.2.2\" class=\"ltx_tr\">\n<td id=\"A10.T17.2.3.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">FedMix</td>\n<td id=\"A10.T17.2.3.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\"><span id=\"A10.T17.2.3.1.2.2.2.1\" class=\"ltx_text ltx_font_bold\">81.2</span></td>\n<td id=\"A10.T17.2.3.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-bottom:2.0pt;padding-top:-2.5pt;padding-bottom:-2.5pt;\">78.8</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Although conceptually it provides an ideal learning environment for edge devices, the federated learning still has some practical challenges that prevent the widespread application of it (Li etÂ al., 2020a; Kairouz etÂ al., 2019).\nAmong such challenges, the one that we are interested in this paper is the heterogeneity of the data, as data is distributed non-iid across clients in many real-world settings; in other words, each local client data is not fairly drawn from identical underlying distribution. Since each client will learn from different data distributions, it becomes harder for the model to be trained efficiently, as reported in (McMahan etÂ al., 2017). While theoretical evidence on the convergence of FedAvg with non-iid case has recently been shown in (Li etÂ al., 2020c), efficient algorithms suitable for this setting have not yet been developed or systematically examined despite some efforts (Zhao etÂ al., 2018; Hsieh etÂ al., 2020).",
            "Additional concerns involve identification of data by detecting change in exchanged averaged data, in case of continual learning, which involves local data change across time. This issue is exacerbated as there is update of averaged data for every minute change on local data, which makes the client receiving ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} easier to infer the changed portion. One simple suggestion to alleviate this issue would be to only update ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} when there is enough change in local data across enough number of clients, so that such changes are not easily exploitable.",
            "While NaiveMix and FedMix is already superior to FedProx, they are parallel to FedProx modification and can be applied in conjunction with FedProx. We compare performances across FedProx variants of various Mixup algorithms in Table 1. FedMix outperforms vanilla FedProx for various datasets, although they do fall short of default version of FedMix used for the main experiment.",
            "While Mixup is usually performed for image classification tasks, it could be applied for language models. For language datasets, since Mixup cannot be performed on input, we perform Mixup on embeddings (for a detailed explanation of Mixup between hidden states, see Appendix E). When tested on Shakespeare dataset, FedMix and NaiveMix both show better performance than baseline algorithms (Table 2). Note that for this task, LocalMix has the lowest performance, and global Mixup does not result in the superior performance above federated algorithms as expected. We think Mixup does not provide performance boost for this specific task, but claim that MAFL algorithms still result in better performance compared to FedAvg.",
            "We also claim that FedMix is superior compared to other methods under various settings, in terms of varying number of clients (Nğ‘N) and varying number of local data per clients. We observe superior performance of FedMix compared to other algorithms for all settings (see Tables 4 and 4). We also vary the number of local epochs (Eğ¸E) between global updates, and still observe that FedMix outperforms other methods (see Appendix F).",
            "Since FedMix approximates loss function of global Mixup for fixed value of Î»â‰ª1much-less-thanğœ†1\\lambda\\ll 1, we can evaluate the efficiency of approximation by comparing between FedMix and a global Mixup scenario with fixed Î»ğœ†\\lambda value. Table 5 shows varying performance between global Mixup and FedMix under various values of Î»ğœ†\\lambda. As Î»ğœ†\\lambda increases, Mixup data reflects more of the features of external data, resulting in better performance in case of global Mixup. However, this also results in our approximation being much less accurate, and we indeed observe performance of FedMix decreasing instead. The result shows that the hyperparameter Î»ğœ†\\lambda should be chosen to balance between better Mixup and better approximation. However, it seems that high Î»ğœ†\\lambda results in significant decrease in both methods, probably due to external data (which is out-of-distribution for local distribution) being overrepresented during local update.",
            "We claim that our method is efficient when faced with non-iid federated settings. For example, our setting of CIFAR10 having only data from 2 classes per client is very non-iid, as in average a pair of clients share only roughly 20% of data distribution. We test settings for CIFAR10 where clients have data from greater number of classes, and while there is little difference for iid (10 class/client) setting, we observe that FedMix outperform other methods and suffer less from increased heterogeneity from highly non-iid settings (Table 7). In addition, we also observe less decline and better performance for MAFL-based algorithms, FedMix in particular, as we train less number of clients per round, reducing communication burden in cost of performance (Table 7).",
            "Local clients are trained by SGD optimizer with learning rate 0.01 and learning decay rate per round 0.999. We set local batch size as 10 for training. Specific hyperparameter setting for each dataset is explained in following Table 8. Throughout the experiment, Mksubscriptğ‘€ğ‘˜M_{k} is fixed to local clientâ€™s dataset size. Changes in these parameters are indicated, if made, are stated for all experiments. Note that we use a fixed small value of Î»ğœ†\\lambda for MAFL-based algorithms to show superior performance.",
            "In Table 9, we observe that if averaged data for MAFL is substituted for randomly generated noise or locally generated images, it does not show the level of performance FedMix is able to show. Thus, we claim that FedMix properly incorporates relevant information received.",
            "Previous works (McMahan etÂ al., 2017; Caldas etÂ al., 2019) show that number of local epochs, Eğ¸E, affects federated learning performance. We tested the effect of Eğ¸E on CIFAR10. In general, we showed that test performance increases as Eğ¸E increases. In addition, we observed that under various values of Eğ¸E, FedMix shows the best performance compared to other algorithms (see Table 10), being a close second after NaiveMix for E=10ğ¸10E=10. MAFL-based algorithms outperform existing algorithms for all values of Eğ¸E tested.",
            "We present the results in Table 12. While threshold does not hugely affect performance, we observe that a moderately small threshold level of 100 results in the best performance. We suggest that as the threshold level is heightened, there is less overfitting to clients with small size local data, but it also results in a decrease in the number of averaged data received by each client. We indeed find an appropriate value of threshold that maximizes performance.",
            "In case where there are a different number of data per client, the sensitivity of Î»ğœ†\\lambda could also be different compared to when all clients have the same number of data. Results in Table 13 show that there is little change in performance by change in Î»ğœ†\\lambda, especially compared to Table 5. In addition, an inspection of the performance of a global model on individual test data of clients does not reveal any noticeable pattern by the size of local data (see Table 13).",
            "Results show that the introduction of Gaussian noise does result in a decline in performance (Table 15),although the decline is very small. Interestingly as noise gets larger as Ïƒ=0.3ğœ0.3\\sigma=0.3, random noise provides an effect as data augmentation and results in a performance increase compared to Ïƒ=0ğœ0\\sigma=0. This experiment is in line with Appendix D. We conclude that introduction of noise in averaged data could provide us with a reasonable alternative to FedMix with large Mksubscriptğ‘€ğ‘˜M_{k}. While our method does not align directly with differential privacy, we leave as future work how FedMix could be smoothly combined with DP-related methods and how its privacy could be quantified in terms of differential privacy.",
            "Further averaging between entries of ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} practically provides an extension of the range of viable Mksubscriptğ‘€ğ‘˜M_{k} such that it exceeds nksubscriptğ‘›ğ‘˜n_{k}, in the sense that each averaged data is from multiple clientsâ€™ data. Such a process would also result in fewer data included in ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g}, so we tested effect of this procedure on model performance. Table 15 shows that for mğ‘šm-fold extra averaging, we even observe increase in performance, but it quickly declines as mğ‘šm gets too large. This method provides an improvement in privacy while even possibly resulting in better performance.",
            "We varied Mixup ratio Î»ğœ†\\lambda for NaiveMix as well. Results in Table 17 shows that NaiveMix also has an intermediate optimal value of Î»ğœ†\\lambda. The drop in performance for Î»=0.5ğœ†0.5\\lambda=0.5 is much more dramatic than for FedMix (see Table 5 for comparison with Global Mixup and FedMix). We think that NaiveMix loss also suffers as it gives more weight to the averaged data, especially for large Mksubscriptğ‘€ğ‘˜M_{k}."
        ]
    },
    "A10.T18": {
        "caption": "",
        "table": "<table id=\"A10.T18.1.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A10.T18.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"A10.T18.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding-top:-3pt;padding-bottom:-3pt;\"><math id=\"A10.T18.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha\" display=\"inline\"><semantics id=\"A10.T18.1.1.1.1.1.m1.1a\"><mi id=\"A10.T18.1.1.1.1.1.m1.1.1\" xref=\"A10.T18.1.1.1.1.1.m1.1.1.cmml\">Î±</mi><annotation-xml encoding=\"MathML-Content\" id=\"A10.T18.1.1.1.1.1.m1.1b\"><ci id=\"A10.T18.1.1.1.1.1.m1.1.1.cmml\" xref=\"A10.T18.1.1.1.1.1.m1.1.1\">ğ›¼</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A10.T18.1.1.1.1.1.m1.1c\">\\alpha</annotation></semantics></math></th>\n<th id=\"A10.T18.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:-3pt;padding-bottom:-3pt;\">FedAvg</th>\n<th id=\"A10.T18.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:-3pt;padding-bottom:-3pt;\">GlobalMix</th>\n<th id=\"A10.T18.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:-3pt;padding-bottom:-3pt;\">LocalMix</th>\n<th id=\"A10.T18.1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:-3pt;padding-bottom:-3pt;\">NaiveMix</th>\n<th id=\"A10.T18.1.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:-3pt;padding-bottom:-3pt;\">FedMix</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A10.T18.1.1.1.2.1\" class=\"ltx_tr\">\n<th id=\"A10.T18.1.1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:-3pt;padding-bottom:-3pt;\"><span id=\"A10.T18.1.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">0.2</span></th>\n<td id=\"A10.T18.1.1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-3pt;padding-bottom:-3pt;\">83.9</td>\n<td id=\"A10.T18.1.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-3pt;padding-bottom:-3pt;\">91.1</td>\n<td id=\"A10.T18.1.1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-3pt;padding-bottom:-3pt;\">84.0</td>\n<td id=\"A10.T18.1.1.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-3pt;padding-bottom:-3pt;\">85.0</td>\n<td id=\"A10.T18.1.1.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:-3pt;padding-bottom:-3pt;\"><span id=\"A10.T18.1.1.1.2.1.6.1\" class=\"ltx_text ltx_font_bold\">86.4</span></td>\n</tr>\n<tr id=\"A10.T18.1.1.1.3.2\" class=\"ltx_tr\">\n<th id=\"A10.T18.1.1.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\" style=\"padding-top:-3pt;padding-bottom:-3pt;\"><span id=\"A10.T18.1.1.1.3.2.1.1\" class=\"ltx_text ltx_font_bold\">0.5</span></th>\n<td id=\"A10.T18.1.1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:-3pt;padding-bottom:-3pt;\">87.6</td>\n<td id=\"A10.T18.1.1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:-3pt;padding-bottom:-3pt;\">91.1</td>\n<td id=\"A10.T18.1.1.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:-3pt;padding-bottom:-3pt;\">88.0</td>\n<td id=\"A10.T18.1.1.1.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:-3pt;padding-bottom:-3pt;\">88.2</td>\n<td id=\"A10.T18.1.1.1.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:-3pt;padding-bottom:-3pt;\"><span id=\"A10.T18.1.1.1.3.2.6.1\" class=\"ltx_text ltx_font_bold\">88.4</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Although conceptually it provides an ideal learning environment for edge devices, the federated learning still has some practical challenges that prevent the widespread application of it (Li etÂ al., 2020a; Kairouz etÂ al., 2019).\nAmong such challenges, the one that we are interested in this paper is the heterogeneity of the data, as data is distributed non-iid across clients in many real-world settings; in other words, each local client data is not fairly drawn from identical underlying distribution. Since each client will learn from different data distributions, it becomes harder for the model to be trained efficiently, as reported in (McMahan etÂ al., 2017). While theoretical evidence on the convergence of FedAvg with non-iid case has recently been shown in (Li etÂ al., 2020c), efficient algorithms suitable for this setting have not yet been developed or systematically examined despite some efforts (Zhao etÂ al., 2018; Hsieh etÂ al., 2020).",
            "Additional concerns involve identification of data by detecting change in exchanged averaged data, in case of continual learning, which involves local data change across time. This issue is exacerbated as there is update of averaged data for every minute change on local data, which makes the client receiving ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} easier to infer the changed portion. One simple suggestion to alleviate this issue would be to only update ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} when there is enough change in local data across enough number of clients, so that such changes are not easily exploitable.",
            "While NaiveMix and FedMix is already superior to FedProx, they are parallel to FedProx modification and can be applied in conjunction with FedProx. We compare performances across FedProx variants of various Mixup algorithms in Table 1. FedMix outperforms vanilla FedProx for various datasets, although they do fall short of default version of FedMix used for the main experiment.",
            "While Mixup is usually performed for image classification tasks, it could be applied for language models. For language datasets, since Mixup cannot be performed on input, we perform Mixup on embeddings (for a detailed explanation of Mixup between hidden states, see Appendix E). When tested on Shakespeare dataset, FedMix and NaiveMix both show better performance than baseline algorithms (Table 2). Note that for this task, LocalMix has the lowest performance, and global Mixup does not result in the superior performance above federated algorithms as expected. We think Mixup does not provide performance boost for this specific task, but claim that MAFL algorithms still result in better performance compared to FedAvg.",
            "We also claim that FedMix is superior compared to other methods under various settings, in terms of varying number of clients (Nğ‘N) and varying number of local data per clients. We observe superior performance of FedMix compared to other algorithms for all settings (see Tables 4 and 4). We also vary the number of local epochs (Eğ¸E) between global updates, and still observe that FedMix outperforms other methods (see Appendix F).",
            "Since FedMix approximates loss function of global Mixup for fixed value of Î»â‰ª1much-less-thanğœ†1\\lambda\\ll 1, we can evaluate the efficiency of approximation by comparing between FedMix and a global Mixup scenario with fixed Î»ğœ†\\lambda value. Table 5 shows varying performance between global Mixup and FedMix under various values of Î»ğœ†\\lambda. As Î»ğœ†\\lambda increases, Mixup data reflects more of the features of external data, resulting in better performance in case of global Mixup. However, this also results in our approximation being much less accurate, and we indeed observe performance of FedMix decreasing instead. The result shows that the hyperparameter Î»ğœ†\\lambda should be chosen to balance between better Mixup and better approximation. However, it seems that high Î»ğœ†\\lambda results in significant decrease in both methods, probably due to external data (which is out-of-distribution for local distribution) being overrepresented during local update.",
            "We claim that our method is efficient when faced with non-iid federated settings. For example, our setting of CIFAR10 having only data from 2 classes per client is very non-iid, as in average a pair of clients share only roughly 20% of data distribution. We test settings for CIFAR10 where clients have data from greater number of classes, and while there is little difference for iid (10 class/client) setting, we observe that FedMix outperform other methods and suffer less from increased heterogeneity from highly non-iid settings (Table 7). In addition, we also observe less decline and better performance for MAFL-based algorithms, FedMix in particular, as we train less number of clients per round, reducing communication burden in cost of performance (Table 7).",
            "Local clients are trained by SGD optimizer with learning rate 0.01 and learning decay rate per round 0.999. We set local batch size as 10 for training. Specific hyperparameter setting for each dataset is explained in following Table 8. Throughout the experiment, Mksubscriptğ‘€ğ‘˜M_{k} is fixed to local clientâ€™s dataset size. Changes in these parameters are indicated, if made, are stated for all experiments. Note that we use a fixed small value of Î»ğœ†\\lambda for MAFL-based algorithms to show superior performance.",
            "In Table 9, we observe that if averaged data for MAFL is substituted for randomly generated noise or locally generated images, it does not show the level of performance FedMix is able to show. Thus, we claim that FedMix properly incorporates relevant information received.",
            "Previous works (McMahan etÂ al., 2017; Caldas etÂ al., 2019) show that number of local epochs, Eğ¸E, affects federated learning performance. We tested the effect of Eğ¸E on CIFAR10. In general, we showed that test performance increases as Eğ¸E increases. In addition, we observed that under various values of Eğ¸E, FedMix shows the best performance compared to other algorithms (see Table 10), being a close second after NaiveMix for E=10ğ¸10E=10. MAFL-based algorithms outperform existing algorithms for all values of Eğ¸E tested.",
            "We present the results in Table 12. While threshold does not hugely affect performance, we observe that a moderately small threshold level of 100 results in the best performance. We suggest that as the threshold level is heightened, there is less overfitting to clients with small size local data, but it also results in a decrease in the number of averaged data received by each client. We indeed find an appropriate value of threshold that maximizes performance.",
            "In case where there are a different number of data per client, the sensitivity of Î»ğœ†\\lambda could also be different compared to when all clients have the same number of data. Results in Table 13 show that there is little change in performance by change in Î»ğœ†\\lambda, especially compared to Table 5. In addition, an inspection of the performance of a global model on individual test data of clients does not reveal any noticeable pattern by the size of local data (see Table 13).",
            "Results show that the introduction of Gaussian noise does result in a decline in performance (Table 15),although the decline is very small. Interestingly as noise gets larger as Ïƒ=0.3ğœ0.3\\sigma=0.3, random noise provides an effect as data augmentation and results in a performance increase compared to Ïƒ=0ğœ0\\sigma=0. This experiment is in line with Appendix D. We conclude that introduction of noise in averaged data could provide us with a reasonable alternative to FedMix with large Mksubscriptğ‘€ğ‘˜M_{k}. While our method does not align directly with differential privacy, we leave as future work how FedMix could be smoothly combined with DP-related methods and how its privacy could be quantified in terms of differential privacy.",
            "Further averaging between entries of ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g} practically provides an extension of the range of viable Mksubscriptğ‘€ğ‘˜M_{k} such that it exceeds nksubscriptğ‘›ğ‘˜n_{k}, in the sense that each averaged data is from multiple clientsâ€™ data. Such a process would also result in fewer data included in ğ‘¿g,ğ’€gsubscriptğ‘¿ğ‘”subscriptğ’€ğ‘”{\\bm{X}}_{g},{\\bm{Y}}_{g}, so we tested effect of this procedure on model performance. Table 15 shows that for mğ‘šm-fold extra averaging, we even observe increase in performance, but it quickly declines as mğ‘šm gets too large. This method provides an improvement in privacy while even possibly resulting in better performance.",
            "We varied Mixup ratio Î»ğœ†\\lambda for NaiveMix as well. Results in Table 17 shows that NaiveMix also has an intermediate optimal value of Î»ğœ†\\lambda. The drop in performance for Î»=0.5ğœ†0.5\\lambda=0.5 is much more dramatic than for FedMix (see Table 5 for comparison with Global Mixup and FedMix). We think that NaiveMix loss also suffers as it gives more weight to the averaged data, especially for large Mksubscriptğ‘€ğ‘˜M_{k}."
        ]
    }
}