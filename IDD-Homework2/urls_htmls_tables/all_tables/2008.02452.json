{
    "PAPER'S NUMBER OF TABLES": 2,
    "S5.T1": {
        "caption": "Table 1: System evaluation on the LibriSpeech task, for training offline seq2seq with attention model.",
        "table": "<table id=\"S5.T1.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T1.2.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.2.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\" colspan=\"3\"><span id=\"S5.T1.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">LibriSpeech Task</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.2.2.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.2.2.1.1\" class=\"ltx_td ltx_border_l ltx_border_r ltx_border_tt\"></td>\n<td id=\"S5.T1.2.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span id=\"S5.T1.2.2.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Training Scenario</span></td>\n<td id=\"S5.T1.2.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S5.T1.2.2.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">WER (%)</span></td>\n</tr>\n<tr id=\"S5.T1.2.3.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.2.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S5.T1.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Centralized</span></td>\n<td id=\"S5.T1.2.3.2.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S5.T1.2.3.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">SotA (lower bound)</span></td>\n<td id=\"S5.T1.2.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.2.3.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">4.00%</span></td>\n</tr>\n<tr id=\"S5.T1.2.4.3\" class=\"ltx_tr\">\n<td id=\"S5.T1.2.4.3.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span id=\"S5.T1.2.4.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Training on 1st 50% of LS(seed)</span></td>\n<td id=\"S5.T1.2.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S5.T1.2.4.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">5.66%</span></td>\n</tr>\n<tr id=\"S5.T1.2.5.4\" class=\"ltx_tr\">\n<td id=\"S5.T1.2.5.4.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S5.T1.2.5.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Online training on 2nd 50% of LS</span></td>\n<td id=\"S5.T1.2.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.2.5.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">4.61%</span></td>\n</tr>\n<tr id=\"S5.T1.2.6.5\" class=\"ltx_tr\">\n<td id=\"S5.T1.2.6.5.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"5\"><span id=\"S5.T1.2.6.5.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">FTL</span></td>\n<td id=\"S5.T1.2.6.5.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S5.T1.2.6.5.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">FedAvg</span></td>\n<td id=\"S5.T1.2.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.2.6.5.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">4.55%</span></td>\n</tr>\n<tr id=\"S5.T1.2.7.6\" class=\"ltx_tr\">\n<td id=\"S5.T1.2.7.6.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S5.T1.2.7.6.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Hier. Optim. (7 clients)</span></td>\n<td id=\"S5.T1.2.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.2.7.6.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">4.51%</span></td>\n</tr>\n<tr id=\"S5.T1.2.8.7\" class=\"ltx_tr\">\n<td id=\"S5.T1.2.8.7.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S5.T1.2.8.7.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Hier. Optim. (1.1k clients)</span></td>\n<td id=\"S5.T1.2.8.7.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.2.8.7.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">4.45%</span></td>\n</tr>\n<tr id=\"S5.T1.2.9.8\" class=\"ltx_tr\">\n<td id=\"S5.T1.2.9.8.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S5.T1.2.9.8.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">+ Softmax DGA</span></td>\n<td id=\"S5.T1.2.9.8.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.2.9.8.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">4.41%</span></td>\n</tr>\n<tr id=\"S5.T1.2.10.9\" class=\"ltx_tr\">\n<td id=\"S5.T1.2.10.9.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_tt\"><span id=\"S5.T1.2.10.9.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">+ RL DGA</span></td>\n<td id=\"S5.T1.2.10.9.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt\"><span id=\"S5.T1.2.10.9.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">4.40%</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "For the supervised experiments, the training is split into two parts of 460h each, with no overlapping speakers. The first part is used to train a seed model, without ever using this data again. Then, the 2nd part of the dataset is used to simulate online training, under the FL conditions. We will follow two different directions for the FL training process: first, the training set is split into 7 distinct parts, never reshuffling the data again (contrary to DT approaches). These data splits are random, with no overlapping speakers across them. The second direction is to segregate the data based on the ",
                "1100",
                "1100",
                "1100",
                " speaker labels. Each one, either ",
                "K",
                "=",
                "7",
                "𝐾",
                "7",
                "K=7",
                " or ",
                "K",
                "=",
                "1.1",
                "​",
                "k",
                "𝐾",
                "1.1",
                "𝑘",
                "K=1.1k",
                ", of the partitions, is assigned to a client. In the FTL framework, all clients are unaware of the rest of them – only the server “knows” which of the clients are used, randomly sampling which ones will be aggregated. The number of sampled clients ",
                "N",
                "𝑁",
                "N",
                " in our experiments varied from 25 to 400, with higher ",
                "N",
                "𝑁",
                "N",
                " being better but with small fluctuation in overall performance. Based on the compromise between communication overheads and memory usage, we henceforth set ",
                "N",
                "=",
                "100",
                "𝑁",
                "100",
                "N=100",
                ".",
                "The weighting approach for ",
                "α",
                "T",
                "j",
                "superscript",
                "subscript",
                "𝛼",
                "𝑇",
                "𝑗",
                "\\alpha_{T}^{{j}}",
                " is either based on the training loss, herein noted as “",
                "Softmax-weighting",
                ",” or inferred by the ",
                "R",
                "​",
                "L",
                "​",
                "(",
                "⋅",
                ")",
                "𝑅",
                "𝐿",
                "⋅",
                "RL(\\cdot)",
                " network, stated as “",
                "RL Weighting",
                ".” The ",
                "R",
                "​",
                "L",
                "𝑅",
                "𝐿",
                "RL",
                " network is a 5-layer DNN with ReLU activations, and a bottleneck layer (",
                "2",
                "n",
                "​",
                "d",
                "superscript",
                "2",
                "𝑛",
                "𝑑",
                "2^{nd}",
                " layer to last). The input layer size is ",
                "3",
                "​",
                "N",
                "3",
                "𝑁",
                "3N",
                ", and the output layer ",
                "N",
                "𝑁",
                "N",
                ". The reward policy is ",
                "+",
                "1",
                "1",
                "+1",
                " if the weights provide better CER value compared to the loss-based weights, ",
                "−",
                "1",
                "1",
                "-1",
                " in the opposite case, and ",
                "+",
                "0.1",
                "0.1",
                "+0.1",
                " when the performance of the two cases are similar. The network has a memory of the previous ",
                "1000",
                "1000",
                "1000",
                " instances, and it samples a mini-batch of ",
                "32",
                "32",
                "32",
                " instances for training per iteration.",
                "The top 3 rows in Table ",
                "1",
                " are with centralized training, with the lower bound in performance coming from the model trained on the entire dataset (offline training). The ",
                "4",
                "%",
                "percent",
                "4",
                "4\\%",
                " WER for this model appears inline with the literature. The ",
                "2",
                "n",
                "​",
                "d",
                "superscript",
                "2",
                "𝑛",
                "𝑑",
                "2^{nd}",
                " model (“online training” row) is based on the seed model initially trained on the 1st half of the data till convergence. The model is online trained with the ",
                "2",
                "n",
                "​",
                "d",
                "superscript",
                "2",
                "𝑛",
                "𝑑",
                "2^{nd}",
                " half of the data. In both steps, Horovod is utilized.",
                "Then, the second scenario simulating the FL condition is examined, where the seed model is further refined in an FL fashion by training on unseen data.\nDifferent strategies for model aggregation were investigated, such as model averaging (“FedAvg” row in the Table), or hierarchical optimization using optimizers such as Adam, LAMB, LAR, and SGD. For the FedAvg system, the model averaging is performed on the server, while the SGD optimizer is used for training on the client’s side. Combinations of the server/client optimizers were also investigated. The differences in performance of these combinations of optimizers were rather limited, and for the sake of space, are not further elaborated here. However, a state-less optimizer on the client-side is adopted as a standard, because the initial model is changing after each iteration",
                "4",
                "4",
                "4",
                "As discussed in Section ",
                "3",
                ", the server aggregates the client models and updates the seed (server) model. Then, this model is re-iterated to the clients.",
                " and therefore, keeping the state of the previous iteration/model as part of the local optimizer didn’t make much sense. Herein, the combination used for all the experiments is Adam/SGD for the server/client sides. SGD with momentum was also investigated without much difference in performance, though.",
                "The next experiment was to transition to the per-speaker partitioning of the data, i.e., ",
                "1100",
                "1100",
                "1100",
                " partitions and an equal number of clients. As mentioned above, ",
                "100",
                "100",
                "100",
                " clients per iteration are sampled out of the pool of ",
                "1100",
                "1100",
                "1100",
                " and finally aggregated. The transition from a homogeneous data split, i.e., the case of 7 partitions, to a more heterogeneous per-speaker partition, improved the overall model performance lightly. This can be explained by the additional diversity provided when aggregating such client models. However, due to this diversity, the convergence during training required additional iterations.",
                "The proposed DGA algorithm has addressed this issue by de-emphasizing the gradients from clients loosely modeled. The FedAvg system requires around 800 iterations for convergence. Such a system is too slow, and henceforth, it will not be considered as the state-of-the-art baseline. Our baseline system is based on hierarchical optimization but without DGA. Even though the overall performance was not impacted (this is only true for the case of the LS task), the overall convergence speed was improved by a factor of ",
                "1.5",
                "×",
                "~{}1.5\\times",
                " compared to the baseline system and ",
                "7",
                "×",
                "~{}7\\times",
                " compared to the FedAvg system. In more detail, approximately 384 iterations are required for the case of unweighted aggregation against only 224 for the SM_DGA. In the case of RL_DGA, the number of required iterations is even lower, decreased by an additional factor of ",
                "1.5",
                "×",
                "1.5\\times",
                ", requiring only 144 iterations. The variations in performance between different approaches are limited; however, the task is quite homogeneous. Further improvements in other in-house tasks, e.g., adaptation on presentation sessions, have also been realized.",
                "The convergence plots for the different weighting scenarios, i.e., uniform weights, Softmax- and RL-based weighting, for the LS task is shown in Figure ",
                "3",
                ".\nThe RL-based aggregation curve shows ripples (particularly wide at the beginning of the training process), but this is expected since the RL network starts from a random state and convergences later on. Further, the performance of the RL- and Softmax-based weighting schemes seem to converge after a few iterations. This is also expected since the rewards policy is based on the Softmax- performance. The RL network learns Softmax-based behavior after a while. However, it is consistently outperforming that system in terms of convergence speed. The LS dataset is quite homogeneous in terms of audio quality, so the proposed algorithm is not expected to perform vastly different in terms of WER performance. However, the RL-based system outperforms the uniform-weighted one in other tasks with improvements up to ",
                "11",
                "%",
                "percent",
                "11",
                "11\\%",
                " WERR."
            ]
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Hierarchical Unsupervised Training with Federated Session Adaptation.",
        "table": "<table id=\"S5.T2.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T2.2.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.2.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" colspan=\"4\"><span id=\"S5.T2.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Presentation-based Session Adaptation</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.2.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.2.2.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt\"></th>\n<th id=\"S5.T2.2.2.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S5.T2.2.2.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Training Scenario</span></th>\n<td id=\"S5.T2.2.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S5.T2.2.2.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">Adapt. Comp.</span></td>\n<td id=\"S5.T2.2.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S5.T2.2.2.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">WER (%)</span></td>\n</tr>\n<tr id=\"S5.T2.2.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.2.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"5\"><span id=\"S5.T2.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Centralized</span></th>\n<th id=\"S5.T2.2.3.2.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T2.2.3.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Baseline</span></th>\n<td id=\"S5.T2.2.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T2.2.3.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">None</span></td>\n<td id=\"S5.T2.2.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T2.2.3.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">6.86%</span></td>\n</tr>\n<tr id=\"S5.T2.2.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T2.2.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\"><span id=\"S5.T2.2.4.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Tenant Text</span></th>\n<td id=\"S5.T2.2.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S5.T2.2.4.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Encoder</span></td>\n<td id=\"S5.T2.2.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S5.T2.2.4.3.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">8.71%</span></td>\n</tr>\n<tr id=\"S5.T2.2.5.4\" class=\"ltx_tr\">\n<td id=\"S5.T2.2.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T2.2.5.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Decoder</span></td>\n<td id=\"S5.T2.2.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T2.2.5.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">6.41%</span></td>\n</tr>\n<tr id=\"S5.T2.2.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T2.2.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T2.2.6.5.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">+ PPT-audio + DGA</span></th>\n<td id=\"S5.T2.2.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T2.2.6.5.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Decoder</span></td>\n<td id=\"S5.T2.2.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T2.2.6.5.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">6.46%</span></td>\n</tr>\n<tr id=\"S5.T2.2.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T2.2.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T2.2.7.6.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">        + real-speech</span></th>\n<td id=\"S5.T2.2.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T2.2.7.6.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Decoder</span></td>\n<td id=\"S5.T2.2.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T2.2.7.6.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">6.29%</span></td>\n</tr>\n<tr id=\"S5.T2.2.8.7\" class=\"ltx_tr\">\n<th id=\"S5.T2.2.8.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_l ltx_border_r ltx_border_tt\"><span id=\"S5.T2.2.8.7.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">FTL</span></th>\n<th id=\"S5.T2.2.8.7.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_tt\"><span id=\"S5.T2.2.8.7.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">TTS- and real-speech</span></th>\n<td id=\"S5.T2.2.8.7.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_tt\"><span id=\"S5.T2.2.8.7.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">Decoder</span></td>\n<td id=\"S5.T2.2.8.7.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_tt\"><span id=\"S5.T2.2.8.7.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">5.51%</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In the case of unsupervised training, our approach is to adapt the seq2seq models hierarchically: first, adapt the seed model to organizational-level tenant data, and then perform session adaptation based on the FTL platform. The dataset used here consists of TTS data and a random sample of real speech already used for training the seq2seq model; the use of the real data prevents the model from overfitting to the TTS data. The test set is based on four presentations of about ",
                "40",
                "​",
                "m",
                "​",
                "i",
                "​",
                "n",
                "40",
                "𝑚",
                "𝑖",
                "𝑛",
                "40min",
                " each, with 1230 sentences, ",
                "12",
                "​",
                "k",
                "12",
                "𝑘",
                "12k",
                " words, and a vocabulary size of ",
                "2.5",
                "​",
                "k",
                "2.5",
                "𝑘",
                "2.5k",
                " words. The text data found in the presentation slides were also used to synthesize the speech data (using the TTS engine as described below) with a total size of ",
                "1.3",
                "​",
                "h",
                "1.3",
                "ℎ",
                "1.3h",
                ". As an initial model, the LAS model trained on ",
                "75",
                "​",
                "k",
                "75",
                "𝑘",
                "75k",
                " hours of data is used. The seed model is adapted with the TTS data generated from the tenant text data in a centralized way. Several scenarios for this adaptation step are investigated, performing subspace adaptation, i.e., changing the encoder, decoder or both, and including different sources of the TTS speech data: the tenant TTS data only, tenant and presentation slide TTS data, and the mixture of the TTS and real-speech data. The final adapted model matches the content of the presentation slides better without the exact transcript of the presentation speech.",
                "Once the seed model is adapted to the tenant text data, it is now used as the starting model for the second adaptations step. This second adaptation step is based on the FTL platform, where the 4 meetings and the new model are used for the final inference step.",
                "The new model is adapted iteratively to the presentation-related text using the TTS-based audio. The DGA step is also applied to weight the input gradients accordingly. However, we noticed that the model overfits very fast on the synthetic TTS data, with the overall performance steeply deteriorating. To address this issue, we have added real-speech data on the server-side training to regularize the process. The real-speech audio is randomly picked from the training set. Note here that this random subset of speech has already been used for training the initial LAS model – no need for held-out data. The addition of this set reduces the model drift significantly while improving the overall recognition performance. As mentioned above, this step resembles the “Naive Rehearsal” approach alleviating any Catastrophic Forgetting effect.",
                "The hierarchical approach shows an ",
                "20",
                "%",
                "percent",
                "20",
                "~{}20\\%",
                " WERR improvement over the original model performance. We have investigated TTS adaptation on the speakers’ voice (each of the 4 presentations is assumed to contain a single speaker), but no additional benefits in performance were found."
            ]
        ]
    }
}