{
    "id_table_1": {
        "caption": "Table 1:  Evaluation of  Source2Synth  on Multi-hop question answering.  The models shown are fine-tuned with 500 entries from HotPotQA (HotPotQA) and/or 1250 entries from the  Source2Synth  Synthetic Dataset (Synthetic Dataset). Using  Source2Synth  curated synthetic data in combination with HotPotQA (last row) works best.",
        "table": "S6.T1.1",
        "footnotes": [],
        "references": [
            "Source2Synth  consists of three stages:  Dataset Generation  and  Dataset Curation , followed by  Model Finetuning , see  Figure 1 . At the  Data Generation  stage, we start by selecting a data source (such as tables on the web, or related Wikipedia articles) to  ground  our synthetic data generation in realistic information for a specific task. Then, to generate a given example, our method first selects a seed topic to condition the generation - for example a specific entity in a Wikipedia article or a factual statement about a table. Given the seed topic, the method then generates the full example: the instruction (e.g., question), the reasoning chain to arrive at the answer (e.g., the steps of multi-hop question answering, or tool use) and the answer itself.",
            "The  Dataset Generation  process yields an augmented dataset grounded in real data. At the  Dataset Curation  step, we employ a model-based approach to automatically refine the dataset to enhance its quality, while avoiding the need for human supervision. In particular, we prune the newly-built dataset of all the entries that have been incorrectly crafted or that are deemed low quality. This is achieved by slicing the dataset in two and using one slice to fine-tune the LLM ( LLMSynth ). During curation,  LLMSynth  is then used to improve the quality of the second slice of the dataset using imputation plus a filtering step. After these steps, we obtain the final curated dataset (shown in purple in  Figure 1 ).",
            "We prompt an instruction-tuned language model to generate two questions: a question  Q 1 subscript Q 1 Q_{1} italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  based on  D 1 subscript D 1 D_{1} italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and whose answer is the selected entity  E E E italic_E , and a second question  Q 2 subscript Q 2 Q_{2} italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  based on  D 2 subscript D 2 D_{2} italic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  such that its main topic is  E E E italic_E . See Figures   16  and   17  for the exact prompts. For example, in Figure  2 ,  Q 1 = subscript Q 1 absent Q_{1}= italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT =  \"What was the spaceflight that first landed humans on the Moon?\", the hop is  E = E absent E= italic_E =  \"Apollo\" and  Q 2 = subscript Q 2 absent Q_{2}= italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT =  \"Who was the commander of Apollo 11?\". We then prompt the LLM to merge the two questions, in order to generate the final two-hop question  Q Q Q italic_Q  by using the entity as a conceptual link (hop). The exact prompt is given in  Figure 15 .",
            "We then prompt an instruction-tuned language model to generate a statement based on the table. This statement corresponds to our seed topic for the generation and is a pertinent interesting fact or set of observations in natural language that can be derived from the table. The prompt is given in  Figure 13 .",
            "We next generate an SQL-statement by zero-shot prompting the LLM: we provide the table and the seed (factual statement) as context, see  Figure 13  for the exact prompt. Given the produced SQL statement, it is then executed using the Python library  sqlite3 1 1 1 https://www.sqlite.org  to obtain an SQL answer formatted as a table. If the generated statement is invalid, we discard it and re-generate.",
            "LLMSynth  (Synthetic dataset only): training our model with 1250 synthetic examples from Slice 1 (see  Figure 1 ),  without  the data curation step.",
            "One-shot Table+SQL QA : the prompt includes an example containing the table and question, and an instruction suggesting that the model can leverage an SQL tool. We then execute the predicted SQL to obtain the answer. See  Figure 10  for the prompt.",
            "We report the experimental results in Table  1 . We include the baselines of the vanilla instruction-tuned LLM, a  fine-tuned LLM  using only the HPQA 500 examples from the train split (second row), and   LLMSynth  which only uses the uncurated synthetic data for fine-tuning (third row). All fine-tuned methods outperform the instruction-tuned model (first row). Using only synthetic data or only HotPotQA data for fine-tuning demonstrates worse performance than when combined, whether the synthetic data is curated (fifth row) or not as in   LLMSynth  (fourth row). Once we use the full  Source2Synth  pipeline to obtain the curated synthetic dataset for fine-tuning we see further performance improvements  LLMCurated  (fifth row) over not curating the data (fourth row)."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Analysis of MHQA bridge and comparison questions with respect to level of difficulty.  We evaluate models on 1k entries for each question type.  Source2Synth  was used to generated bridge question type data, hence LLM-Curated-1250 outperforms other models particularly for bridge questions.",
        "table": "S6.T2.1",
        "footnotes": [],
        "references": [
            "In multi-hop question answering (MHQA), we generate a dataset of multi-hop question-answer pairs, in addition to the reasoning chain that is used to answer the question, consisting of question decomposition into subquestions with answers, plus the entity that links them. See Figure  2  for an overview of the procedure and Figure  3  for an example response from the  Source2Synth  model.",
            "An MHQA seed topic corresponds to an entity  E E E italic_E  retrieved from  D 1 subscript D 1 D_{1} italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . The seed in MHQA doubles also as the hop in the multi-hop question  Q Q Q italic_Q  that we aim to generate, since  E E E italic_E  links the  n = 2 n 2 n=2 italic_n = 2  subquestions that compose  Q Q Q italic_Q . For example, in Figure  2 , we sample \"The Moon\" article at random, denoted by  D 1 subscript D 1 D_{1} italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , and the corresponding entity, denoted by  E E E italic_E , is \"Apollo 11\" (displayed in blue). Then, we pick \"Neil Armstrong\" as  D 2 subscript D 2 D_{2} italic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  from the pool of related articles, since it contains a paragraph where the entity \"Apollo 11\" is included.",
            "We prompt an instruction-tuned language model to generate two questions: a question  Q 1 subscript Q 1 Q_{1} italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  based on  D 1 subscript D 1 D_{1} italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and whose answer is the selected entity  E E E italic_E , and a second question  Q 2 subscript Q 2 Q_{2} italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  based on  D 2 subscript D 2 D_{2} italic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  such that its main topic is  E E E italic_E . See Figures   16  and   17  for the exact prompts. For example, in Figure  2 ,  Q 1 = subscript Q 1 absent Q_{1}= italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT =  \"What was the spaceflight that first landed humans on the Moon?\", the hop is  E = E absent E= italic_E =  \"Apollo\" and  Q 2 = subscript Q 2 absent Q_{2}= italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT =  \"Who was the commander of Apollo 11?\". We then prompt the LLM to merge the two questions, in order to generate the final two-hop question  Q Q Q italic_Q  by using the entity as a conceptual link (hop). The exact prompt is given in  Figure 15 .",
            "Analysis of performance on different question types and levels of difficulty  We study the capabilities of our model by analysing the performance of LLM-Curated-1250 with particular focus on the type and difficulty of the questions  namely hard/medium/easy bridge and comparison questions. We compare the performance of the base model, the model fine-tuned on HotPotQA, and   Source2Synth  according to the difficulty level, as provided by the HotPotQA dataset. We also subdivide the results according to the type of question (bridge vs. comparison). Results are given in  Table 2 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Tabular question answering.  Performance comparison on the WikiSQL evaluation dataset.",
        "table": "S6.T3.1",
        "footnotes": [],
        "references": [
            "In multi-hop question answering (MHQA), we generate a dataset of multi-hop question-answer pairs, in addition to the reasoning chain that is used to answer the question, consisting of question decomposition into subquestions with answers, plus the entity that links them. See Figure  2  for an overview of the procedure and Figure  3  for an example response from the  Source2Synth  model.",
            "We check if the predicted answer matches the answer in the synthetically generated example, and if after  k k k italic_k  tries the LLM has not supplied the correct answer we filter out the entry entirely. See  Figure 3  for an example of model inference.",
            "We then prompt an instruction-tuned language model to generate a statement based on the table. This statement corresponds to our seed topic for the generation and is a pertinent interesting fact or set of observations in natural language that can be derived from the table. The prompt is given in  Figure 13 .",
            "We next generate an SQL-statement by zero-shot prompting the LLM: we provide the table and the seed (factual statement) as context, see  Figure 13  for the exact prompt. Given the produced SQL statement, it is then executed using the Python library  sqlite3 1 1 1 https://www.sqlite.org  to obtain an SQL answer formatted as a table. If the generated statement is invalid, we discard it and re-generate.",
            "We report the experimental results for Tabular question answering in Table  3 . Firstly, they indicate that providing no context about the table when prompting the instruction-tuned StarChat language model has very poor performance (first row), with an EM metric of 0.25%. This is expected since the WikiSQL benchmark questions require information contained in the table, and the model does not have any other information to answer the question except for the general knowledge stored in its parameters. However, even if we pass the table as part of the prompt, the performance does not improve much. For example, passing in a zero-shot fashion (second row) only has an EM metric of 1.83%. This may be challenging for the model as the information in the table is presented in a form that is not easy for the LLM to naturally handle (i.e. a table rather than natural language). While passing an example of table usage in a one-shot fashion (third row) improves the soft-EM metric, the EM metric is still very low (2.03%). Hence, this is still very challenging for the model. Thirdly, the performance increases once we provide a one-shot example containing the relevant table and SQL query (fourth row), with an EM of 12.3%. The ability to use the SQL tool improves the performance markedly."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  MHQA prompts sweep.  Overview of the models accuracy across different prompt strategies.  Role  \"You are a QA-robot. Answer the following question:\". Model used: Llama-2-70B-Chat, Dataset: HotPotQA test.",
        "table": "A1.T4.1",
        "footnotes": [],
        "references": []
    }
}