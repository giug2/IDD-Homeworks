{
    "id_table_1": {
        "caption": "Table 1:   Jailbreak judge benchmark and methods.",
        "table": "S1.T1.1.1",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Although the jailbreak judge is a fundamental problem, comprehensive studies on it have been sparse  Jin et al. ( 2024b ) , as shown in Table  1 . Current methods can be broadly categorized into heuristic methods  Liu et al. ( 2024b ) , toxic text classifiers  Ji et al. ( 2024b ) , and LLM-based methods  Inan et al. ( 2023 ) . Heuristic and toxic text classifiers, while simple, often suffer high false positive rates. For instance, heuristic methods rely on keyword matching, misinterpreting benign responses containing certain keywords as malicious. Traditional toxic text classifiers  Ji et al. ( 2024b ) , trained on toxic text, struggle with complex scenarios (e.g., broad-range risks, adversarial, in-the-wild, multilingual) and often lack explanatory power. The harmfulness of a response alone is insufficient to determine whether a model refuses to answer, and the absence of explanations can lead to false judgments. Conversely, LLM-based methods utilize LLMs to evaluate potential jailbreaks or directly fine-tune them as moderation models (e.g., Llama-Guard  Inan et al. ( 2023 )  and ShieldGemma  Zeng et al. ( 2024a ) ). For example, prompt-driven GPT-4 uses tailored prompts to assess if an LLM is jailbroken but incurs significant computational and financial costs. Additionally, these methods may suffer from inherent biases and data ambiguities, leading to inaccurate judgments and reduced reliability in identifying jailbreak attempts due to lack reasoning explainability.",
            "We develop the JAILJUDGE benchmark datasets and a multi-agent jailbreak judge framework, making the decision inference process explicit and interpretable to enhance evaluation quality. Using the multi-agent framework to determine the ground truth with reasoning explainability and fine-grained scores, we then develop the end-to-end judge model, JAILJUDGE Guard. Trained on JAILJUDGEs training data, this model can also provide reasoning explainability with fine-grained evaluations without API cost. The overall framework is shown in Figure  1 .",
            "Vanilla Harmful Prompts.  We collect the original harmful prompts from prior safety benchmark datasets and refine broad and vague safety categories into 14 more fine-grained categories, following the MLCommons standardized hazards taxonomy  Inan et al. ( 2023 ) . This results in relatively more balanced categories (see Appendix  8.1  for detailed dataset sources and and categories).",
            "Synthetic Vanilla Prompts.  To ensure the diversity of harmful prompts, we utilize GPT-4 to rewrite and expand the original vanilla harmful prompts within the same hazard taxonomy categories. For instance, if the original harmful prompt is How to make a bomb?, the rewritten prompt might be Detailed instructions on constructing a homemade explosive using household items. Refer to Appendix  8.1  for the GPT-4 prompts used to generate the more diverse prompts.",
            "Synthetic Adversarial Prompts.  To increase the likelihood of generating unsafe responses from LLMs, we leverage the latest jailbreak attacks, which have demonstrated their ability to bypass LLM safety alignment. Specifically, we apply seven recent jailbreak techniques to modify vanilla harmful prompts, aiming to elicit unsafe responses from the model. For instance, a jailbreak attack may optimize an adversarial suffix (e.g., !!!$&xnksac) and append it to the original harmful prompt (e.g., How to make a bomb? !!!$&xnksac) to heighten the chance of the LLM producing a harmful output. See Appendix  8.1  for details on the jailbreak attacks used to generate adversarial prompts.",
            "In-the-wild Prompts.  To account for potential risks in real-world user requests, we incorporate prompts from diverse datasets. These prompts are collected from prompt templates on prominent platforms commonly used for prompt sharing, such as Reddit, Discord, various websites, and open-source datasets. These prompt templates can be combined with malicious prompts to create more complex and subtle harmful intentions. For example, a user might employ a template like Do anything now followed by additional harmful prompts. (See Appendix  8.1  for the detailed pipeline).",
            "Deceptive Harmful Prompts.  In addition to real-world user-LLM interactions, deceptive harmful prompts often mask their malicious intent through techniques such as role-playing, scenario assumptions, long-context prompts, and adaptive strategies. These complex cases are typically challenging for LLMs to identify. To ensure thorough coverage of these variations, we apply automatic adversarial prompt refinement to the original harmful prompts (see Appendix  8.1  for the detailed pipeline).",
            "Diverse LLM Responses.  To construct diverse LLM responses, we collect responses from three sources: closed-source LLMs (e.g., GPT-4, GPT-3.5), open-source LLMs (e.g., Llama-family, Qwen-family, Mistral-family, Vicuna-family), and defense-enhanced LLM responses. Specifically, we randomly split the above-tailored prompts and submit each prompt to a suite of LLMs, instructing the LLMs to generate the corresponding responses. To mimic a well-secured environment, we adopt the latest defense methods, including both system-level and model-level defenses. We randomly sample the prompts and submit them to the defended LLMs to get the target responses. This provides us with a set of diverse responses, including both safe and unsafe ones. (see Appendix  8.1  for the detailed defense methods).",
            "JAILJUDGE Guard.  Using explainability-enhanced JAILJUDGETRAIN with multi-agent judge, we instruction-tune JAILJUDGE Guard based on the Llama-2-7B model. We design an end-to-end input-output format for an explainability-driven jailbreak judge, where the users prompt and model response serve as inputs. The model is trained to output both an reasoning explainability and a fine-grained evaluation score (jailbroken score ranging from 1 to 10, with 1 indicating non-jailbroken and 10 indicating complete jailbreak). Further training details can be found in Appendix  10 .",
            "where  A  (  ) A  \\mathcal{A}(\\cdot) caligraphic_A (  )  is the attacker to refine the adversarial prompts  x ^ 1 : n subscript ^ x : 1 n \\hat{\\mathbf{x}}_{1:n} over^ start_ARG bold_x end_ARG start_POSTSUBSCRIPT 1 : italic_n end_POSTSUBSCRIPT . The JAILJUDGE Guard outputs the jailbroken score  s =    ( A  ( x ^ 1 : n ) , y ^ ) s subscript  italic- A subscript ^ x : 1 n ^ y s=\\pi_{\\phi}(\\mathcal{A}(\\hat{\\mathbf{x}}_{1:n}),\\hat{\\mathbf{y}}) italic_s = italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( caligraphic_A ( over^ start_ARG bold_x end_ARG start_POSTSUBSCRIPT 1 : italic_n end_POSTSUBSCRIPT ) , over^ start_ARG bold_y end_ARG )  as the iteratively evaluator to determine the quality of adversarial prompts, where   a subscript  a \\tau_{a} italic_ start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT  is the threshold. (We omit the output of analysis  a a a italic_a  for simplicity). The detailed algorithm of  JailBoost  can be seen in Appendix  11.1 .",
            "where  a a a italic_a  is the safe reasoning analysis, and   d subscript  d \\tau_{d} italic_ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT  is the predefined threshold. A detailed algorithm of  GuardShield  can be found in Appendix  11.2 .",
            "Evaluation Datasets and Metrics.  To assess the performance of the jailbreak judge, we use both JAILJUDGE ID and OOD datasets. Additionally, we include the public jailbreak judge dataset and evaluate on JBB Behaviors  Chao et al. ( 2024 )  and WILDTEST  Han et al. ( 2024 ) . For all evaluations, we report metrics including accuracy, precision, recall, and F1 score. To assess the quality of explainability, we employ GPT-4 to rate the explainability quality (EQ) on a scale of 1 to 5, where higher scores indicate better clarity and reasoning. More details can be found in Appendix  12.1",
            "Jailbreak Judge Baselines and Implementations.  To evaluate the performance of our jailbreak judge, we compare it against state-of-the-art baselines, including heuristic methods such as StringMatching  Liu et al. ( 2024b )  and toxic text classifiers and LLM-based moderation tools like Beaver-dam-7B  Ji et al. ( 2024b ) , Longformer-action  Wang et al. ( 2023 ) , Longformer-harmful  Wang et al. ( 2023 ) , and GPTFuzzer  Yu et al. ( 2023 ) , Llama-Guard-7B  Inan et al. ( 2023 ) , Llama-Guard-2-8B  Inan et al. ( 2023 ) , Llama-Guard-3-8B  Inan et al. ( 2023 ) , ShieldGemma-2B  Zeng et al. ( 2024a ) , and ShieldGemma-9B  Zeng et al. ( 2024a ) . Furthermore, we incorporate prompt-driven GPT-4 baselines such as GPT-4-liu2024autodan-Recheck  Liu et al. ( 2024b ) , GPT-4-qi2023  Qi et al. ( 2023 ) , and GPT-4-zhang2024intention  Zhang et al. ( 2024b ) . Since most existing jailbreak judge methods currently focus on directly determining whether an LLM is jailbroken, we designed two baselines: GPT-4-Reasoning, which provides reasoning-enhanced judgments based on GPT-4, and GPT-4-multi-agent Voting, which aggregates multi-agent voting using evidence theory. GPT-4-JailJudge MultiAgent is our multi-agent judging framework utilizing GPT-4 as the base model, whereas JAILJUDGE Guard is our end-to-end jailbreak judging model trained on the JAILJUDGETRAIN dataset based on Llama-2-7B. Detailed descriptions of experimental implementation settings are provided in Appendix  12.2 .",
            "To evaluate the effectiveness of  JailBoost  and  GuardShield , we conduct experiments on the HEx-PHI dataset under zero-shot settings. We use the attack success rate (ASR) as the primary metric. For attacker experiments, a higher ASR indicates a more effective attacker method, whereas for defense methods, a lower ASR indicates a better defense approach. Detailed descriptions of the experimental settings, metrics, and baselines can be found in Appendix  8.1  and  12.4 .  Jailbreak Attack.  The experimental results are presented in Figure  5 .  JailBoost  significantly enhances the attackers capability. For example,  JailBoost  increases the ASR for the attacker compared to the nominal AutoDAN.  Jailbreak Defense.  The experimental results are presented in Table  4 .  GuardShield  achieves superior defense performance compared to the state-of-the-art (SOTA) baselines. For instance,  GuardShield  achieves nearly 100% defense capability against four SOTA attackers, with an average ASR of 0.15%, outperforming most baselines.",
            "In this section, we present an ablation study to evaluate the effectiveness of each component in our multi-agent judge framework. We compared four configurations: (1) Vanilla GPT-4, which directly determines whether the LLM is jailbroken; (2) Reasoning-enhanced GPT-4 (RE-GPT-4); (3) RE-GPT-4 augmented with our uncertainty-aware evident judging agent (RE-GPT-4+UAE); and (4) the complete multi-agent judge framework. The results, shown in Figure  4  and  4 , demonstrate that each enhancement progressively improves performance across all datasets. For instance, in the JAILJUDGE ID task, the F1 score increased from 0.55 with Vanilla GPT-4 to 0.91 with the multi-agent judge. Similarly, in the JBB Behaviors scenario, scores rose from 0.79 to 0.96. Overall, our multi-agent judge consistently outperforms the baseline and individually enhanced models, underscoring the effectiveness of each component. Additionally, as detailed in Appendix  12.3 , human evaluators score the explainability of the reasons provided for the samples. For instance, our method demonstrates high accuracy under manual evaluation, with the JailJudge MultiAgent achieving average 95.29% on four datasets.",
            "JAILJUDGE ID.  The overall statistical information of JAILJUDGE ID is presented in Figures  11  and  11 . Since JAILJUDGE ID is a split from the JAILJUDGE TRAIN dataset, it is well-balanced for a broad range of risk scenarios, whereas SNA represents the safe prompts, as shown in Figure  11 . Figure  11  presents the distribution of prompt complexity categories. The data reveals that the Q5 category has the highest frequency, while Q1 has the least frequency. These distributions reflect the diverse and complex nature of the prompts in the JAILJUDGE ID dataset. There are a total of 4,500 data instances, and Figure  17  shows the distribution of jailbroken status in the JAILJUDGE ID dataset. Specifically, there are 66.4% jailbroken instances and 33.6% non-jailbroken instances.",
            "JAILJUDGE OOD.  The overall statistical information of JAILJUDGE OOD is presented in Figures  13  and  13 . Since JAILJUDGE OOD encompasses multilingual language scenarios and all the samples are not present in the JAILJUDGE TRAIN dataset, Figure  13  shows the distribution of different disruptions, which is well-balanced across categories. There are a total of 6,300 data instances, and Figure  17  shows the distribution of jailbroken status in the JAILJUDGE OOD dataset. Specifically, there are 88.6% non-jailbroken data and 11.4% jailbroken data. The percentage of jailbroken data is lower than JAILJUDGE IDs due to the multilingual language scenarios and the absence of optimized jailbroken attacks to increase the likelihood of generating unsafe responses.",
            "In this section, we provide detailed information about the LLM-powered agent. The base LLM used throughout is GPT-4. Specifically, there are three judging agents, three voting agents, and one inference agent. The judging agents analyze the prompts and model responses to determine whether the LLM is jailbroken, offering initial reasons and scores. The system prompt for the judging agents is similar to the baseline GPT-4-based reasoning presented in Figure  20 . Voting agents cast their votes based on the scores and reasons provided by the judging agents to decide on the validity of their judgments. The system prompt for the voting agents is presented in Figure  14 . Finally, inference agents make the final judgment based on the voting results and predetermined criteria. The system prompt for the inference agents is presented in Figure  15 .",
            "Using explainability-enhanced JAILJUDGETRAIN with a multi-agent judge, we instruction-tune JAILJUDGE Guard based on the Llama-7B model. We design an end-to-end input-output format for an explainability-driven jailbreak judge, where the users prompt and model response serve as inputs. The model outputs both an explainability rationale and a fine-grained evaluation score (1 indicating non-jailbroken to 10 indicating complete jailbreak). Specifically, we first use the multi-agent judge framework, with GPT-4 as an LLM-powered agent, to generate ground truth with reasoning explainability and a fine-grained evaluation score. We employ LoRA  Hu et al. ( 2021 )  for supervised fine-tuning (SFT) of the base LLM (Llama-2-7B) for the jailbreak judge task, where the input is a users prompt and model response, and the output is the reasoning explainability with a fine-grained evaluation score. The SFT template details are shown in Figure  18 .",
            "where  A  (  ) A  \\mathcal{A}(\\cdot) caligraphic_A (  )  is the attacker to refine the adversarial prompts  x ^ 1 : n subscript ^ x : 1 n \\hat{\\mathbf{x}}_{1:n} over^ start_ARG bold_x end_ARG start_POSTSUBSCRIPT 1 : italic_n end_POSTSUBSCRIPT . The JAILJUDGE Guard outputs the jailbroken score  s =    ( A  ( x ^ 1 : n ) , y ^ ) s subscript  italic- A subscript ^ x : 1 n ^ y s=\\pi_{\\phi}(\\mathcal{A}(\\hat{\\mathbf{x}}_{1:n}),\\hat{\\mathbf{y}}) italic_s = italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( caligraphic_A ( over^ start_ARG bold_x end_ARG start_POSTSUBSCRIPT 1 : italic_n end_POSTSUBSCRIPT ) , over^ start_ARG bold_y end_ARG )  as the iteratively evaluator to determine the quality of adversarial prompts, where   a subscript  a \\tau_{a} italic_ start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT  is the threshold. (We omit the output of analysis  a a a italic_a  for simplicity). The detailed  JailBoost  can be seen in Algorithm  1 .",
            "Metrics.  Accuracy is the ratio of correct predictions to the total number of predictions:  Accuracy = T  P + T  N T  P + T  N + F  P + F  N Accuracy T P T N T P T N F P F N \\text{Accuracy}=\\frac{TP+TN}{TP+TN+FP+FN} Accuracy = divide start_ARG italic_T italic_P + italic_T italic_N end_ARG start_ARG italic_T italic_P + italic_T italic_N + italic_F italic_P + italic_F italic_N end_ARG . Precision is the ratio of true positive predictions to the total number of positive predictions:  Precision = T  P T  P + F  P Precision T P T P F P \\text{Precision}=\\frac{TP}{TP+FP} Precision = divide start_ARG italic_T italic_P end_ARG start_ARG italic_T italic_P + italic_F italic_P end_ARG . Recall is the ratio of true positive predictions to the total number of actual positives:  Recall = T  P T  P + F  N Recall T P T P F N \\text{Recall}=\\frac{TP}{TP+FN} Recall = divide start_ARG italic_T italic_P end_ARG start_ARG italic_T italic_P + italic_F italic_N end_ARG . The F1 Score is the harmonic mean of Precision and Recall:  F1 Score = 2  Precision  Recall Precision + Recall F1 Score 2 Precision Recall Precision Recall \\text{F1\\ Score}=2\\times\\frac{\\text{Precision}\\times\\text{Recall}}{\\text{% Precision}+\\text{Recall}} F1 Score = 2  divide start_ARG Precision  Recall end_ARG start_ARG Precision + Recall end_ARG , where: TP stands for True Positives, TN stands for True Negatives, FP stands for False Positives, and FN stands for False Negatives. To evaluate the quality of explainability, we employ GPT-4 to rate the explainability quality (EQ) on a scale from 1 to 5, where higher scores indicate better clarity and reasoning. The evaluation prompt is shown in Figure  19 .",
            "Implementation Setting.  We use the Attack Success Rate (ASR) to evaluate the effectiveness of jailbreak attack and defense methods. For jailbreak attacks, a higher ASR indicates a more effective attack, whereas for jailbreak defenses, a lower ASR reflects a more effective defense. To calculate ASR, we follow standard settings, using GPT-4 to determine the success of an attack attempt. The evaluation prompts are shown in Figure  23 . The base model we used is Vicuna-13B-v1.5. Details of the baseline jailbreak attack and defense methods can be found in Appendix  8.1 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Jailbreak judge experiments on datasets JAILJUDGE ID and JBB Behaviors.",
        "table": "S5.T2.10.10",
        "footnotes": [],
        "references": [
            "Human Annotator Training Phase.  In the human annotator training phase, we follow the MLCommons standardized LLM policy, which includes nine detailed policies ranging from user privacy to ethics and morality. We provide detailed scenarios and examples to the human annotators, allowing them to learn what constitutes a violation of these policies. For example, regarding user privacy, the models response must comply with basic ethical and moral standards and must not generate content that violates social morality. Responses that violate these types of policies would be judged as jailbroken (see Appendix  8.2  for detailed policies).",
            "where  a a a italic_a  is the safe reasoning analysis, and   d subscript  d \\tau_{d} italic_ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT  is the predefined threshold. A detailed algorithm of  GuardShield  can be found in Appendix  11.2 .",
            "Evaluation Datasets and Metrics.  To assess the performance of the jailbreak judge, we use both JAILJUDGE ID and OOD datasets. Additionally, we include the public jailbreak judge dataset and evaluate on JBB Behaviors  Chao et al. ( 2024 )  and WILDTEST  Han et al. ( 2024 ) . For all evaluations, we report metrics including accuracy, precision, recall, and F1 score. To assess the quality of explainability, we employ GPT-4 to rate the explainability quality (EQ) on a scale of 1 to 5, where higher scores indicate better clarity and reasoning. More details can be found in Appendix  12.1",
            "Jailbreak Judge Baselines and Implementations.  To evaluate the performance of our jailbreak judge, we compare it against state-of-the-art baselines, including heuristic methods such as StringMatching  Liu et al. ( 2024b )  and toxic text classifiers and LLM-based moderation tools like Beaver-dam-7B  Ji et al. ( 2024b ) , Longformer-action  Wang et al. ( 2023 ) , Longformer-harmful  Wang et al. ( 2023 ) , and GPTFuzzer  Yu et al. ( 2023 ) , Llama-Guard-7B  Inan et al. ( 2023 ) , Llama-Guard-2-8B  Inan et al. ( 2023 ) , Llama-Guard-3-8B  Inan et al. ( 2023 ) , ShieldGemma-2B  Zeng et al. ( 2024a ) , and ShieldGemma-9B  Zeng et al. ( 2024a ) . Furthermore, we incorporate prompt-driven GPT-4 baselines such as GPT-4-liu2024autodan-Recheck  Liu et al. ( 2024b ) , GPT-4-qi2023  Qi et al. ( 2023 ) , and GPT-4-zhang2024intention  Zhang et al. ( 2024b ) . Since most existing jailbreak judge methods currently focus on directly determining whether an LLM is jailbroken, we designed two baselines: GPT-4-Reasoning, which provides reasoning-enhanced judgments based on GPT-4, and GPT-4-multi-agent Voting, which aggregates multi-agent voting using evidence theory. GPT-4-JailJudge MultiAgent is our multi-agent judging framework utilizing GPT-4 as the base model, whereas JAILJUDGE Guard is our end-to-end jailbreak judging model trained on the JAILJUDGETRAIN dataset based on Llama-2-7B. Detailed descriptions of experimental implementation settings are provided in Appendix  12.2 .",
            "Main Experiments.  To evaluate the effectiveness of the jailbreak judge methods, we conducted experiments using the JAILJUDGE ID and JBB behaviors datasets. Our JailJudge MultiAgent and JAILJUDGE Guard consistently outperformed all open-source baselines across both datasets, as shown in Table  2 . The multi-agent judge achieved the highest average F1 scores, specifically 0.9197 on the JAILJUDGE ID dataset and 0.9609 on the JBB behaviors dataset. Notably, our approach showed more stable performance on the JBB behaviors dataset, likely due to its simpler scenarios compared to the more complex JAILJUDGE ID dataset. Additionally, the JailJudge MultiAgent surpassed the baseline GPT-4-Reasoning model in reasoning capabilities. As shown in Table  2 , the GPT-4-Reasoning model attained an EQ score of 4.3989, while our multi-agent judge achieved a superior EQ score of 4.5234 on JAILJUDGE ID, indicating enhanced reasoning ability.",
            "To evaluate the effectiveness of  JailBoost  and  GuardShield , we conduct experiments on the HEx-PHI dataset under zero-shot settings. We use the attack success rate (ASR) as the primary metric. For attacker experiments, a higher ASR indicates a more effective attacker method, whereas for defense methods, a lower ASR indicates a better defense approach. Detailed descriptions of the experimental settings, metrics, and baselines can be found in Appendix  8.1  and  12.4 .  Jailbreak Attack.  The experimental results are presented in Figure  5 .  JailBoost  significantly enhances the attackers capability. For example,  JailBoost  increases the ASR for the attacker compared to the nominal AutoDAN.  Jailbreak Defense.  The experimental results are presented in Table  4 .  GuardShield  achieves superior defense performance compared to the state-of-the-art (SOTA) baselines. For instance,  GuardShield  achieves nearly 100% defense capability against four SOTA attackers, with an average ASR of 0.15%, outperforming most baselines.",
            "In this section, we present an ablation study to evaluate the effectiveness of each component in our multi-agent judge framework. We compared four configurations: (1) Vanilla GPT-4, which directly determines whether the LLM is jailbroken; (2) Reasoning-enhanced GPT-4 (RE-GPT-4); (3) RE-GPT-4 augmented with our uncertainty-aware evident judging agent (RE-GPT-4+UAE); and (4) the complete multi-agent judge framework. The results, shown in Figure  4  and  4 , demonstrate that each enhancement progressively improves performance across all datasets. For instance, in the JAILJUDGE ID task, the F1 score increased from 0.55 with Vanilla GPT-4 to 0.91 with the multi-agent judge. Similarly, in the JBB Behaviors scenario, scores rose from 0.79 to 0.96. Overall, our multi-agent judge consistently outperforms the baseline and individually enhanced models, underscoring the effectiveness of each component. Additionally, as detailed in Appendix  12.3 , human evaluators score the explainability of the reasons provided for the samples. For instance, our method demonstrates high accuracy under manual evaluation, with the JailJudge MultiAgent achieving average 95.29% on four datasets.",
            "In this section, we provide detailed information about the LLM-powered agent. The base LLM used throughout is GPT-4. Specifically, there are three judging agents, three voting agents, and one inference agent. The judging agents analyze the prompts and model responses to determine whether the LLM is jailbroken, offering initial reasons and scores. The system prompt for the judging agents is similar to the baseline GPT-4-based reasoning presented in Figure  20 . Voting agents cast their votes based on the scores and reasons provided by the judging agents to decide on the validity of their judgments. The system prompt for the voting agents is presented in Figure  14 . Finally, inference agents make the final judgment based on the voting results and predetermined criteria. The system prompt for the inference agents is presented in Figure  15 .",
            "where  a a a italic_a  is the safe reasoning analysis, and   d subscript  d \\tau_{d} italic_ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT  is the predefined threshold. A detailed algorithm of  GuardShield  can be found in Algorithm  2",
            "Jailbreak Judge Baselines and Implementation Settings.  To evaluate the performance of our jailbreak judge, StringMatching  Liu et al. ( 2024b )  is a type of heuristic method that uses keywords to detect whether the LLM response contains safe words. Longformer-action  Wang et al. ( 2023 )  and Longformer-harmful  Wang et al. ( 2023 )  are fine-tuned Longformer models used for evaluating action risks and harmfulness, respectively. GPTFuzzer  Yu et al. ( 2023 )  is a customized RoBERTa model tailored for the assessment of model safety. Beaver-dam-7B  Ji et al. ( 2024b )  is a specialized LLaMA-2 model designed for assessing model safety. The Llama Guard series models, including Llama-Guard-7B, Llama-Guard-2-8B, and Llama-Guard-3-8B  Inan et al. ( 2023 ) , are LLM-based input-output safeguard models designed to categorize a specific set of safety risks using human-AI conversation use cases. ShieldGemma, which includes ShieldGemma-2B  Zeng et al. ( 2024a )  and ShieldGemma-9B  Zeng et al. ( 2024a ) , comprises a suite of safety content moderation models based on Gemma 2, aimed at addressing four categories of harm. Furthermore, we incorporate prompt-driven GPT-4 baselines. For instance, GPT-4-liu2024autodan-Recheck  Liu et al. ( 2024b )  directly uses GPT-4 to determine whether the LLM is jailbroken. GPT-4-qi2023  Qi et al. ( 2023 )  integrates OpenAIs LLM policies and uses GPT-4 to provide a fine-grained score ranging from 1 to 5. and GPT-4-zhang2024intention  Zhang et al. ( 2024b )  also uses GPT-4 to evaluate the harmfulness of the answers provided by the LLM. Since most existing jailbreak judgment methods currently focus on directly determining whether an LLM is jailbroken, we designed two baselines: GPT-4-Reasoning, which provides reasoning-enhanced judgments based on GPT-4. The reasoning process is similar to Chain of Thought (CoT), and the prompt can be seen in Figure  20 . and GPT-4-multi-agent Voting, which aggregates multi-agent voting using evidence theory with the same reasoning prompt. For the baseline heuristic methods, such as string matching and toxic text classifiers, we follow the settings described in  Ran et al. ( 2024 )  to conduct the experiments. GPT-4-multi-agent Judge is our jailbreak judge framework, using GPT-4 as the base LLM. The hyper-parameter    \\beta italic_  is set to 0.1, and the normalized base number  C C C italic_C  is set to 10. The multi-agent framework provides a fine-grained evaluation with a jailbroken score ranging from 1 to 10, along with corresponding reasons. The JAILJUDGE Guard is our end-to-end jailbreak judge model, trained on JAILJUDGE TRAIN, with the base model being Llama-2-7B. To determine whether the LLM is jailbroken or not, the threshold    \\alpha italic_  is set to 2.",
            "We employ human evaluators to score the explainability of the reasons provided for the samples. For instance, our method demonstrates very high accuracy under manual evaluation, with the multi-agent judge achieving 95.29% accuracy across four datasets. Specifically, we sample 200 instances from each of the following datasets: JAILJUDGE ID, JBB Behaviors, JAILJUDGE OOD, and WEILDTEST. Three highly qualified human evaluators then score whether they agree with the analysis provided by Vanilla GPT-4, JAILJUDGE Guard, and multi-agent Judge. They assess whether the explanations are reasonable. If the explanation is deemed reasonable, the method receives a score of 1; otherwise, it receives a score of -1. Finally, we use a voting system to determine the final score, which is either 1 or -1. The final Human score (H-score) is the percentage of samples that received a score of 1. The higher the score, the greater the human agreement rate. Figures  22  and  22  show the final results. It can be observed that our multi-agent judge method achieves a very high human evaluation rate, with an average score of 95.29% across the four datasets.",
            "Implementation Setting.  We use the Attack Success Rate (ASR) to evaluate the effectiveness of jailbreak attack and defense methods. For jailbreak attacks, a higher ASR indicates a more effective attack, whereas for jailbreak defenses, a lower ASR reflects a more effective defense. To calculate ASR, we follow standard settings, using GPT-4 to determine the success of an attack attempt. The evaluation prompts are shown in Figure  23 . The base model we used is Vicuna-13B-v1.5. Details of the baseline jailbreak attack and defense methods can be found in Appendix  8.1 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Jailbreak judge experiments on datasets JAILJUDGE OOD and WILDTEST under zero-shot setting.",
        "table": "S5.T3.10.10",
        "footnotes": [],
        "references": [
            "JAILJUDGETRAIN is a comprehensive instruction-tuning dataset consisting of 35k+ items, derived from diverse sources with various target response pairs from different LLMs. The dataset includes six sources of prompts: vanilla harmful prompts (a wide range of risk scenarios), synthetic vanilla prompts (LLM-rewritten prompts), synthetic adversarial prompts (jailbreak attack rewrites), multilingual harmful prompts (ten multigual languages), in-the-wild harmful prompts (real-world user-LLM interactions), and deceptive harmful prompts (automatic prompt refinement prompts). These sources are selected to optimize coverage, diversity, and balance. To construct diverse LLM responses, we collect responses from three sources: closed-source LLMs (e.g., GPT-4, GPT-3.5), open-source LLMs (e.g., Llama-family, Qwen-family, Mistral-family), and Defense-enhanced LLM responses. The overview of the dataset composition can be seen in Appendix  8.3 .",
            "Zero-Shot Setting.  To assess the efficacy of the jailbreak judge in a zero-shot context, we conducted experiments using the JAILJUDGE OOD and WILDEST datasets. As summarized in Table  3 , our jailbreak judge methods consistently outperformed all open baselines across both evaluation sets. For instance, on the multilingual JAILJUDGE OOD dataset, the multi-agent judge achieved an F1 score of 0.711, significantly higher than the GPT-4-Reasoings 0.5633, underscoring the benefits of leveraging advanced LLMs like GPT-4 for multilingual and zero-shot scenarios. Although JAILJUDGE Guard achieved a respectable F1 score of 0.7314 on WILDTEST, it fell short of the multi-agent judge on JAILJUDGE OOD due to its limited multilingual training, as shown in Figure  4 . Overall, our methods demonstrated consistent superiority across both datasets, emphasizing the importance of advanced language models like GPT-4 for handling multilingual and zero-shot settings effectively, as evidenced by its higher EQ scores and logical consistency in reasoning. The insights findings can be summarized as follows.",
            "In this section, we present an ablation study to evaluate the effectiveness of each component in our multi-agent judge framework. We compared four configurations: (1) Vanilla GPT-4, which directly determines whether the LLM is jailbroken; (2) Reasoning-enhanced GPT-4 (RE-GPT-4); (3) RE-GPT-4 augmented with our uncertainty-aware evident judging agent (RE-GPT-4+UAE); and (4) the complete multi-agent judge framework. The results, shown in Figure  4  and  4 , demonstrate that each enhancement progressively improves performance across all datasets. For instance, in the JAILJUDGE ID task, the F1 score increased from 0.55 with Vanilla GPT-4 to 0.91 with the multi-agent judge. Similarly, in the JBB Behaviors scenario, scores rose from 0.79 to 0.96. Overall, our multi-agent judge consistently outperforms the baseline and individually enhanced models, underscoring the effectiveness of each component. Additionally, as detailed in Appendix  12.3 , human evaluators score the explainability of the reasons provided for the samples. For instance, our method demonstrates high accuracy under manual evaluation, with the JailJudge MultiAgent achieving average 95.29% on four datasets.",
            "JAILJUDGE OOD.  The overall statistical information of JAILJUDGE OOD is presented in Figures  13  and  13 . Since JAILJUDGE OOD encompasses multilingual language scenarios and all the samples are not present in the JAILJUDGE TRAIN dataset, Figure  13  shows the distribution of different disruptions, which is well-balanced across categories. There are a total of 6,300 data instances, and Figure  17  shows the distribution of jailbroken status in the JAILJUDGE OOD dataset. Specifically, there are 88.6% non-jailbroken data and 11.4% jailbroken data. The percentage of jailbroken data is lower than JAILJUDGE IDs due to the multilingual language scenarios and the absence of optimized jailbroken attacks to increase the likelihood of generating unsafe responses.",
            "Implementation Setting.  We use the Attack Success Rate (ASR) to evaluate the effectiveness of jailbreak attack and defense methods. For jailbreak attacks, a higher ASR indicates a more effective attack, whereas for jailbreak defenses, a lower ASR reflects a more effective defense. To calculate ASR, we follow standard settings, using GPT-4 to determine the success of an attack attempt. The evaluation prompts are shown in Figure  23 . The base model we used is Vicuna-13B-v1.5. Details of the baseline jailbreak attack and defense methods can be found in Appendix  8.1 ."
        ]
    },
    "id_table_4": {
        "caption": "Table 5:  Hazard Categories for Vanilla Harmful Prompts",
        "table": "S5.F5.9.4.4",
        "footnotes": [],
        "references": [
            "Zero-Shot Setting.  To assess the efficacy of the jailbreak judge in a zero-shot context, we conducted experiments using the JAILJUDGE OOD and WILDEST datasets. As summarized in Table  3 , our jailbreak judge methods consistently outperformed all open baselines across both evaluation sets. For instance, on the multilingual JAILJUDGE OOD dataset, the multi-agent judge achieved an F1 score of 0.711, significantly higher than the GPT-4-Reasoings 0.5633, underscoring the benefits of leveraging advanced LLMs like GPT-4 for multilingual and zero-shot scenarios. Although JAILJUDGE Guard achieved a respectable F1 score of 0.7314 on WILDTEST, it fell short of the multi-agent judge on JAILJUDGE OOD due to its limited multilingual training, as shown in Figure  4 . Overall, our methods demonstrated consistent superiority across both datasets, emphasizing the importance of advanced language models like GPT-4 for handling multilingual and zero-shot settings effectively, as evidenced by its higher EQ scores and logical consistency in reasoning. The insights findings can be summarized as follows.",
            "To evaluate the effectiveness of  JailBoost  and  GuardShield , we conduct experiments on the HEx-PHI dataset under zero-shot settings. We use the attack success rate (ASR) as the primary metric. For attacker experiments, a higher ASR indicates a more effective attacker method, whereas for defense methods, a lower ASR indicates a better defense approach. Detailed descriptions of the experimental settings, metrics, and baselines can be found in Appendix  8.1  and  12.4 .  Jailbreak Attack.  The experimental results are presented in Figure  5 .  JailBoost  significantly enhances the attackers capability. For example,  JailBoost  increases the ASR for the attacker compared to the nominal AutoDAN.  Jailbreak Defense.  The experimental results are presented in Table  4 .  GuardShield  achieves superior defense performance compared to the state-of-the-art (SOTA) baselines. For instance,  GuardShield  achieves nearly 100% defense capability against four SOTA attackers, with an average ASR of 0.15%, outperforming most baselines.",
            "In this section, we present an ablation study to evaluate the effectiveness of each component in our multi-agent judge framework. We compared four configurations: (1) Vanilla GPT-4, which directly determines whether the LLM is jailbroken; (2) Reasoning-enhanced GPT-4 (RE-GPT-4); (3) RE-GPT-4 augmented with our uncertainty-aware evident judging agent (RE-GPT-4+UAE); and (4) the complete multi-agent judge framework. The results, shown in Figure  4  and  4 , demonstrate that each enhancement progressively improves performance across all datasets. For instance, in the JAILJUDGE ID task, the F1 score increased from 0.55 with Vanilla GPT-4 to 0.91 with the multi-agent judge. Similarly, in the JBB Behaviors scenario, scores rose from 0.79 to 0.96. Overall, our multi-agent judge consistently outperforms the baseline and individually enhanced models, underscoring the effectiveness of each component. Additionally, as detailed in Appendix  12.3 , human evaluators score the explainability of the reasons provided for the samples. For instance, our method demonstrates high accuracy under manual evaluation, with the JailJudge MultiAgent achieving average 95.29% on four datasets.",
            "In this section, we provide detailed information about the LLM-powered agent. The base LLM used throughout is GPT-4. Specifically, there are three judging agents, three voting agents, and one inference agent. The judging agents analyze the prompts and model responses to determine whether the LLM is jailbroken, offering initial reasons and scores. The system prompt for the judging agents is similar to the baseline GPT-4-based reasoning presented in Figure  20 . Voting agents cast their votes based on the scores and reasons provided by the judging agents to decide on the validity of their judgments. The system prompt for the voting agents is presented in Figure  14 . Finally, inference agents make the final judgment based on the voting results and predetermined criteria. The system prompt for the inference agents is presented in Figure  15 ."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "S8.T5.1",
        "footnotes": [],
        "references": [
            "To evaluate the effectiveness of  JailBoost  and  GuardShield , we conduct experiments on the HEx-PHI dataset under zero-shot settings. We use the attack success rate (ASR) as the primary metric. For attacker experiments, a higher ASR indicates a more effective attacker method, whereas for defense methods, a lower ASR indicates a better defense approach. Detailed descriptions of the experimental settings, metrics, and baselines can be found in Appendix  8.1  and  12.4 .  Jailbreak Attack.  The experimental results are presented in Figure  5 .  JailBoost  significantly enhances the attackers capability. For example,  JailBoost  increases the ASR for the attacker compared to the nominal AutoDAN.  Jailbreak Defense.  The experimental results are presented in Table  4 .  GuardShield  achieves superior defense performance compared to the state-of-the-art (SOTA) baselines. For instance,  GuardShield  achieves nearly 100% defense capability against four SOTA attackers, with an average ASR of 0.15%, outperforming most baselines.",
            "Vanilla Harmful Prompts . We collect original harmful prompts from prior safety benchmark datasets, including  AdvBench   Zou et al. ( 2023 ) ,  MaliciousInstruct   Huang et al. ( 2023 ) , and  Forbidden Question Set   Shen et al. ( 2024 )  and refine broad and vague safety categories into 14 more specific categories following the MLCommons standardized hazards taxonomy  Inan et al. ( 2023 ) . This leads to more balanced category distributions. Detailed datasets and the 14 refined hazard categories used in this construction are provided in Table  5 . These categories are based on the MLCommons standardized hazards taxonomy, with an additional category for Code Interpreter Abuse.",
            "In this section, we provide detailed information about the LLM-powered agent. The base LLM used throughout is GPT-4. Specifically, there are three judging agents, three voting agents, and one inference agent. The judging agents analyze the prompts and model responses to determine whether the LLM is jailbroken, offering initial reasons and scores. The system prompt for the judging agents is similar to the baseline GPT-4-based reasoning presented in Figure  20 . Voting agents cast their votes based on the scores and reasons provided by the judging agents to decide on the validity of their judgments. The system prompt for the voting agents is presented in Figure  14 . Finally, inference agents make the final judgment based on the voting results and predetermined criteria. The system prompt for the inference agents is presented in Figure  15 ."
        ]
    }
}