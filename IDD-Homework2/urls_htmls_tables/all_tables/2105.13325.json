{
    "PAPER'S NUMBER OF TABLES": 3,
    "S4.T1": {
        "caption": "TABLE I: Summary of training scenarios, the forecasting models produced by each scenario and the privacy associated with each scenario.",
        "table": "<table id=\"S4.T1.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.2.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span id=\"S4.T1.2.3.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Training scenario</span></th>\n<th id=\"S4.T1.2.3.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.2.3.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Model</span></th>\n<th id=\"S4.T1.2.3.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.2.3.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">Private?</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.2.4.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.4.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span id=\"S4.T1.2.4.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Centralised</span></th>\n<td id=\"S4.T1.2.4.1.2\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S4.T1.2.4.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Single/Joint</span></td>\n<td id=\"S4.T1.2.4.1.3\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S4.T1.2.4.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">âœ—</span></td>\n</tr>\n<tr id=\"S4.T1.2.5.2\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.5.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S4.T1.2.5.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Localised</span></th>\n<td id=\"S4.T1.2.5.2.2\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.2.5.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Multiple/Specialised</span></td>\n<td id=\"S4.T1.2.5.2.3\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.2.5.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">âœ“</span></td>\n</tr>\n<tr id=\"S4.T1.2.6.3\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.6.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S4.T1.2.6.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">FL</span></th>\n<td id=\"S4.T1.2.6.3.2\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.2.6.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Single/Joint</span></td>\n<td id=\"S4.T1.2.6.3.3\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.2.6.3.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">âœ“ w.r.t. raw data</span></td>\n</tr>\n<tr id=\"S4.T1.2.7.4\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.7.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S4.T1.2.7.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">FL+HC</span></th>\n<td id=\"S4.T1.2.7.4.2\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.2.7.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Multiple/Specialised</span></td>\n<td id=\"S4.T1.2.7.4.3\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.2.7.4.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">âœ“ w.r.t. raw data</span></td>\n</tr>\n<tr id=\"S4.T1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span id=\"S4.T1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">FL </span><math id=\"S4.T1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\rightarrow\" display=\"inline\"><semantics id=\"S4.T1.1.1.1.m1.1a\"><mo mathsize=\"90%\" stretchy=\"false\" id=\"S4.T1.1.1.1.m1.1.1\" xref=\"S4.T1.1.1.1.m1.1.1.cmml\">â†’</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.1.1.1.m1.1b\"><ci id=\"S4.T1.1.1.1.m1.1.1.cmml\" xref=\"S4.T1.1.1.1.m1.1.1\">â†’</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.1.1.1.m1.1c\">\\rightarrow</annotation></semantics></math><span id=\"S4.T1.1.1.1.2\" class=\"ltx_text\" style=\"font-size:90%;\"> LFT</span>\n</th>\n<td id=\"S4.T1.1.1.2\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Multiple/Specialised</span></td>\n<td id=\"S4.T1.1.1.3\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">âœ“ w.r.t. raw data</span></td>\n</tr>\n<tr id=\"S4.T1.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\">\n<span id=\"S4.T1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">FL+HC </span><math id=\"S4.T1.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\rightarrow\" display=\"inline\"><semantics id=\"S4.T1.2.2.1.m1.1a\"><mo mathsize=\"90%\" stretchy=\"false\" id=\"S4.T1.2.2.1.m1.1.1\" xref=\"S4.T1.2.2.1.m1.1.1.cmml\">â†’</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.2.2.1.m1.1b\"><ci id=\"S4.T1.2.2.1.m1.1.1.cmml\" xref=\"S4.T1.2.2.1.m1.1.1\">â†’</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.2.2.1.m1.1c\">\\rightarrow</annotation></semantics></math><span id=\"S4.T1.2.2.1.2\" class=\"ltx_text\" style=\"font-size:90%;\"> LFT</span>\n</th>\n<td id=\"S4.T1.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_b\"><span id=\"S4.T1.2.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Multiple/Specialised</span></td>\n<td id=\"S4.T1.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_b\"><span id=\"S4.T1.2.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">âœ“ w.r.t. raw data</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In order to test the effectiveness of applying FL to load forecasting, we provide benchmarks against centralised learning, local-only learning and various FL training scenarios. These different training approaches are summarised in ",
                "TableÂ I",
                " and described diagrammatically in ",
                "FigureÂ 6",
                ".",
                "Firstly we developed a non-distributed, centralised learning approach that is most commonly applied where the privacy of data is not a major concern during training. This approach pools individual household datasets together and training is conducted in a single location. This approach provides a baseline for what a single, joint forecasting model can achieve in a non-private setting. In this scenario, the same network parameters are used by all households at the inference stage. Under this centralised approach we train models for 500 epochs with early stopping based on the lowest error achieved on the validation set.",
                "The most important benchmark we developed is a fully-private localised learning setting. All individual datasets remain private and unseen by other data owners under this scenario and the training procedure is isolated to each household. This approach results in unique forecasting models tailored to each household but cannot benefit from knowledge that could be embedded in data owned by other households. As per the centralised learning approach, training was conducted for a maximum of 500 epochs with early stopping.",
                "Any FL system needs to offer benefits above and beyond what can be achieved in the localised learning setting as this is already a fully private approach. In this paper we investigate how individual learners can benefit from patterns in energy usage from other households in the population.",
                "The goal of FL is the same as centralised learning - to learn a single, joint model that generalises well enough to provide accurate forecasts for all individual households. In FL however, the training data belonging to each household (or client in the parlance of FL) is not pooled as in centralised learning. Instead, the training data remains private to each local client. Whereas centralised learning seeks to optimise a global objective of the form: ",
                "min",
                "â¡",
                "f",
                "â€‹",
                "(",
                "w",
                ")",
                "ğ‘“",
                "ğ‘¤",
                "\\min f(w)",
                ", FL optimises an objective as the finite sum of local objectives taking the form: ",
                "min",
                "â¡",
                "1",
                "m",
                "â€‹",
                "âˆ‘",
                "i",
                "=",
                "1",
                "m",
                "f",
                "i",
                "â€‹",
                "(",
                "w",
                ")",
                "1",
                "ğ‘š",
                "subscript",
                "superscript",
                "ğ‘š",
                "ğ‘–",
                "1",
                "subscript",
                "ğ‘“",
                "ğ‘–",
                "ğ‘¤",
                "\\min\\frac{1}{m}\\sum^{m}_{i=1}f_{i}(w)",
                ".",
                "Training proceeds via communication rounds, beginning with an initialised model state ",
                "w",
                "t",
                "subscript",
                "ğ‘¤",
                "ğ‘¡",
                "w_{t}",
                " that is transmitted to a small set of clients ",
                "K",
                "ğ¾",
                "K",
                ". Each client ",
                "k",
                "âˆˆ",
                "K",
                "ğ‘˜",
                "ğ¾",
                "k\\in K",
                " computes an update ",
                "w",
                "t",
                "+",
                "1",
                "k",
                "subscript",
                "superscript",
                "ğ‘¤",
                "ğ‘˜",
                "ğ‘¡",
                "1",
                "w^{k}_{t+1}",
                " to the model state based on their dataset by optimising a local forecasting objective ",
                "f",
                "k",
                "â€‹",
                "(",
                "w",
                "t",
                ")",
                "subscript",
                "ğ‘“",
                "ğ‘˜",
                "subscript",
                "ğ‘¤",
                "ğ‘¡",
                "f_{k}(w_{t})",
                ". In practice this usually involves training just a few epochs on each client. Each client then transmits their update to a centralised server that aggregates the updates into a new model ",
                "w",
                "t",
                "+",
                "1",
                "subscript",
                "ğ‘¤",
                "ğ‘¡",
                "1",
                "w_{t+1}",
                ". For our experiments we apply federated averaging (FedAvg) ",
                "[",
                "6",
                "]",
                " as the FL algorithm for aggregating client updates. FedAvg aggregates incoming client model updates via a data weighted average such that:",
                "Here, ",
                "n",
                "k",
                "n",
                "subscript",
                "ğ‘›",
                "ğ‘˜",
                "ğ‘›",
                "\\frac{n_{k}}{n}",
                " represents the number of samples available to client ",
                "k",
                "ğ‘˜",
                "k",
                " compared to the total number of samples used for training in round ",
                "t",
                "ğ‘¡",
                "t",
                ", thus determining the data-weighted contribution of client ",
                "k",
                "ğ‘˜",
                "k",
                ". In the FL training scenario, model performance is affected by additional hyperparameters that we also test for:",
                "fraction of clients participating in each communication round: 0.1, 0.2 & 0.3",
                "Number of epochs of training on clients: 1, 3 & 5",
                "All FL training runs are capped at 500 communication rounds with early stopping based on the best average validation set performance across all clients.",
                "Models trained via FL have been shown to suffer under non-IID distributions. As we have explored, the individual household datasets exhibit differing data distributions due to the varied ways in which household occupants consume energy. As such, we investigate the use of a modification of FedAvg known as FL+HC ",
                "[",
                "21",
                "]",
                " (our previous work) that incorporates a clustering step into the FL protocol. In FL, the local objectives are expected to approximate the global objective, however if data among clients is distributed non-IID, this expectation over data available to client ",
                "D",
                "k",
                "subscript",
                "ğ·",
                "ğ‘˜",
                "D_{k}",
                " is not valid: ",
                "ğ”¼",
                "D",
                "k",
                "â€‹",
                "[",
                "f",
                "k",
                "â€‹",
                "(",
                "w",
                ")",
                "]",
                "â‰ ",
                "f",
                "â€‹",
                "(",
                "w",
                ")",
                "subscript",
                "ğ”¼",
                "subscript",
                "ğ·",
                "ğ‘˜",
                "delimited-[]",
                "subscript",
                "ğ‘“",
                "ğ‘˜",
                "ğ‘¤",
                "ğ‘“",
                "ğ‘¤",
                "\\mathbb{E}_{D_{k}}[f_{k}(w)]\\neq f(w)",
                ". In FL+HC, clients are assigned to a cluster ",
                "c",
                "âˆˆ",
                "C",
                "ğ‘",
                "ğ¶",
                "c\\in C",
                ", where the goal is to train a specialised model ",
                "f",
                "c",
                "subscript",
                "ğ‘“",
                "ğ‘",
                "f_{c}",
                " tailored to clients that share a similar data distribution. We use client updates as a proxy for client similarity in order to preserve the privacy of the raw training data. A hierarchical clustering algorithm is run at communication round ",
                "n",
                "ğ‘›",
                "n",
                " taking as input the weight updates from all clients. Clients that produce similar updates are clustered together and further training via FL proceeds for each cluster in isolation. For a good clustering under FL+HC, the expectation of local objectives (clients) assigned to a cluster ",
                "c",
                "ğ‘",
                "c",
                " approximates the cluster objective:",
                "FL+HC introduces more hyperparameters to control the clustering process which we test for:",
                "clustering distance threshold: 0.8, 1.4 & 2.0",
                "hierarchical clustering linkage mechanism: ward, average, complete & single",
                "number of rounds of FL prior to clustering step ",
                "n",
                "ğ‘›",
                "n",
                ": 3, 5 & 10",
                "To keep the number of permutations of experiments for the FL+HC scenario manageable, we fix the client fraction at 0.1, the number of epochs to 3 and exclusively use the Euclidean (L2) clustering distance metric. All FL+HC training runs are capped at 200 communication rounds with early stopping based on the best average validation set performance per cluster.",
                "Finally, we also test scenarios where the models produced by FL and FL+HC are further fine-tuned on the local clients for a small number of epochs (a process known as personalisation). These local fine-tuning (LFT) scenarios are termed FL ",
                "â†’",
                "â†’",
                "\\rightarrow",
                " LFT and FL+HC ",
                "â†’",
                "â†’",
                "\\rightarrow",
                " LFT. We test whether personalisation produces more accurate, highly specialised models for each household that also builds on the wisdom of the private data of other households. The fine-tuning step is limited to 25 epochs on all clients with early stopping based on the best validation set performance."
            ]
        ]
    },
    "S5.T2": {
        "caption": "TABLE II: Forecasting error (RMSE) values for the 6 datasets and 6 training scenarios. â€™Meanâ€™ and â€™Bestâ€™ columns show percentage difference compared to fully-private localised learning",
        "table": "<table id=\"S5.T2.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.2.3.1\" class=\"ltx_tr\">\n<td id=\"S5.T2.2.3.1.1\" class=\"ltx_td ltx_border_tt\"></td>\n<th id=\"S5.T2.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"6\"><span id=\"S5.T2.2.3.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Dataset</span></th>\n<th id=\"S5.T2.2.3.1.3\" class=\"ltx_td ltx_th ltx_th_column ltx_border_tt\"></th>\n<td id=\"S5.T2.2.3.1.4\" class=\"ltx_td ltx_border_tt\"></td>\n</tr>\n<tr id=\"S5.T2.2.4.2\" class=\"ltx_tr\">\n<td id=\"S5.T2.2.4.2.1\" class=\"ltx_td\"></td>\n<th id=\"S5.T2.2.4.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\"><span id=\"S5.T2.2.4.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">incl. weather</span></th>\n<th id=\"S5.T2.2.4.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\"><span id=\"S5.T2.2.4.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">not incl. weather</span></th>\n<th id=\"S5.T2.2.4.2.4\" class=\"ltx_td ltx_th ltx_th_column\"></th>\n<td id=\"S5.T2.2.4.2.5\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S5.T2.2.5.3\" class=\"ltx_tr\">\n<th id=\"S5.T2.2.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column\"><span id=\"S5.T2.2.5.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Training scenario</span></th>\n<th id=\"S5.T2.2.5.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T2.2.5.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">6 steps</span></th>\n<th id=\"S5.T2.2.5.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T2.2.5.3.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">12 steps</span></th>\n<th id=\"S5.T2.2.5.3.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T2.2.5.3.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">24 steps</span></th>\n<th id=\"S5.T2.2.5.3.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T2.2.5.3.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">6 steps</span></th>\n<th id=\"S5.T2.2.5.3.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T2.2.5.3.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">12 steps</span></th>\n<th id=\"S5.T2.2.5.3.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T2.2.5.3.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">24 steps</span></th>\n<th id=\"S5.T2.2.5.3.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span id=\"S5.T2.2.5.3.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">mean</span></th>\n<th id=\"S5.T2.2.5.3.9\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span id=\"S5.T2.2.5.3.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">best</span></th>\n</tr>\n<tr id=\"S5.T2.2.6.4\" class=\"ltx_tr\">\n<td id=\"S5.T2.2.6.4.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S5.T2.2.6.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Centralised</span></td>\n<td id=\"S5.T2.2.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T2.2.6.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0210</span></td>\n<td id=\"S5.T2.2.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T2.2.6.4.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0207</span></td>\n<td id=\"S5.T2.2.6.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T2.2.6.4.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0198</span></td>\n<td id=\"S5.T2.2.6.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T2.2.6.4.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0210</span></td>\n<td id=\"S5.T2.2.6.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T2.2.6.4.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0208</span></td>\n<td id=\"S5.T2.2.6.4.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T2.2.6.4.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0198</span></td>\n<td id=\"S5.T2.2.6.4.8\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S5.T2.2.6.4.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0205  (-4.8%)</span></td>\n<td id=\"S5.T2.2.6.4.9\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S5.T2.2.6.4.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0198  (-8.0%)</span></td>\n</tr>\n<tr id=\"S5.T2.2.7.5\" class=\"ltx_tr\">\n<td id=\"S5.T2.2.7.5.1\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T2.2.7.5.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Localised</span></td>\n<td id=\"S5.T2.2.7.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T2.2.7.5.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0198</span></td>\n<td id=\"S5.T2.2.7.5.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T2.2.7.5.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0183</span></td>\n<td id=\"S5.T2.2.7.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T2.2.7.5.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0201</span></td>\n<td id=\"S5.T2.2.7.5.5\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T2.2.7.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0201</span></td>\n<td id=\"S5.T2.2.7.5.6\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T2.2.7.5.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0188</span></td>\n<td id=\"S5.T2.2.7.5.7\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T2.2.7.5.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0202</span></td>\n<td id=\"S5.T2.2.7.5.8\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T2.2.7.5.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0196  (â€”)</span></td>\n<td id=\"S5.T2.2.7.5.9\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T2.2.7.5.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0183  (â€”)</span></td>\n</tr>\n<tr id=\"S5.T2.2.8.6\" class=\"ltx_tr\">\n<td id=\"S5.T2.2.8.6.1\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T2.2.8.6.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">FL</span></td>\n<td id=\"S5.T2.2.8.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T2.2.8.6.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0219</span></td>\n<td id=\"S5.T2.2.8.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T2.2.8.6.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0202</span></td>\n<td id=\"S5.T2.2.8.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T2.2.8.6.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0207</span></td>\n<td id=\"S5.T2.2.8.6.5\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T2.2.8.6.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0219</span></td>\n<td id=\"S5.T2.2.8.6.6\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T2.2.8.6.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0205</span></td>\n<td id=\"S5.T2.2.8.6.7\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T2.2.8.6.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0210</span></td>\n<td id=\"S5.T2.2.8.6.8\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T2.2.8.6.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0210  (-7.6%)</span></td>\n<td id=\"S5.T2.2.8.6.9\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T2.2.8.6.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0202  (-10.5%)</span></td>\n</tr>\n<tr id=\"S5.T2.2.9.7\" class=\"ltx_tr\">\n<td id=\"S5.T2.2.9.7.1\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T2.2.9.7.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">FL+HC</span></td>\n<td id=\"S5.T2.2.9.7.2\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T2.2.9.7.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0209</span></td>\n<td id=\"S5.T2.2.9.7.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T2.2.9.7.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0189</span></td>\n<td id=\"S5.T2.2.9.7.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T2.2.9.7.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0208</span></td>\n<td id=\"S5.T2.2.9.7.5\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T2.2.9.7.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0214</span></td>\n<td id=\"S5.T2.2.9.7.6\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T2.2.9.7.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0198</span></td>\n<td id=\"S5.T2.2.9.7.7\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T2.2.9.7.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0210</span></td>\n<td id=\"S5.T2.2.9.7.8\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T2.2.9.7.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0205  (-4.7%)</span></td>\n<td id=\"S5.T2.2.9.7.9\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T2.2.9.7.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0189  (-3.5%)</span></td>\n</tr>\n<tr id=\"S5.T2.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.1.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S5.T2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">FL </span><math id=\"S5.T2.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\rightarrow\" display=\"inline\"><semantics id=\"S5.T2.1.1.1.m1.1a\"><mo mathsize=\"90%\" stretchy=\"false\" id=\"S5.T2.1.1.1.m1.1.1\" xref=\"S5.T2.1.1.1.m1.1.1.cmml\">â†’</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.1.1.1.m1.1b\"><ci id=\"S5.T2.1.1.1.m1.1.1.cmml\" xref=\"S5.T2.1.1.1.m1.1.1\">â†’</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.1.1.1.m1.1c\">\\rightarrow</annotation></semantics></math><span id=\"S5.T2.1.1.1.2\" class=\"ltx_text\" style=\"font-size:90%;\"> LFT</span>\n</td>\n<td id=\"S5.T2.1.1.2\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T2.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0194</span></td>\n<td id=\"S5.T2.1.1.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T2.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0177</span></td>\n<td id=\"S5.T2.1.1.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T2.1.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0192</span></td>\n<td id=\"S5.T2.1.1.5\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T2.1.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0195</span></td>\n<td id=\"S5.T2.1.1.6\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T2.1.1.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.0174</span></td>\n<td id=\"S5.T2.1.1.7\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T2.1.1.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0192</span></td>\n<td id=\"S5.T2.1.1.8\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T2.1.1.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0187  (+4.3%)</span></td>\n<td id=\"S5.T2.1.1.9\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T2.1.1.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0174  (+4.9%)</span></td>\n</tr>\n<tr id=\"S5.T2.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T2.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">\n<span id=\"S5.T2.2.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">FL+HC </span><math id=\"S5.T2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\rightarrow\" display=\"inline\"><semantics id=\"S5.T2.2.2.1.m1.1a\"><mo mathsize=\"90%\" stretchy=\"false\" id=\"S5.T2.2.2.1.m1.1.1\" xref=\"S5.T2.2.2.1.m1.1.1.cmml\">â†’</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.2.2.1.m1.1b\"><ci id=\"S5.T2.2.2.1.m1.1.1.cmml\" xref=\"S5.T2.2.2.1.m1.1.1\">â†’</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.2.2.1.m1.1c\">\\rightarrow</annotation></semantics></math><span id=\"S5.T2.2.2.1.2\" class=\"ltx_text\" style=\"font-size:90%;\"> LFT</span>\n</td>\n<td id=\"S5.T2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T2.2.2.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.0193</span></td>\n<td id=\"S5.T2.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T2.2.2.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.0176</span></td>\n<td id=\"S5.T2.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T2.2.2.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.0192</span></td>\n<td id=\"S5.T2.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T2.2.2.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.0193</span></td>\n<td id=\"S5.T2.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T2.2.2.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0175</span></td>\n<td id=\"S5.T2.2.2.7\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T2.2.2.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.0192</span></td>\n<td id=\"S5.T2.2.2.8\" class=\"ltx_td ltx_align_left ltx_border_bb\"><span id=\"S5.T2.2.2.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0187  (+4.5%)</span></td>\n<td id=\"S5.T2.2.2.9\" class=\"ltx_td ltx_align_left ltx_border_bb\"><span id=\"S5.T2.2.2.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0175  (+4.5%)</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "To understand how the different training approaches perform compared to one another, we report the RMSE achieved on the test set for each of the 6 datasets, ",
                "ğ‘º",
                "K",
                "=",
                "6",
                ",",
                "+weather",
                "subscript",
                "ğ‘º",
                "ğ¾",
                "6",
                "+weather",
                "\\bm{S}_{K=6,\\text{+weather}}",
                ", ",
                "ğ‘º",
                "K",
                "=",
                "12",
                ",",
                "+weather",
                "subscript",
                "ğ‘º",
                "ğ¾",
                "12",
                "+weather",
                "\\bm{S}_{K=12,\\text{+weather}}",
                ", ",
                "ğ‘º",
                "K",
                "=",
                "24",
                ",",
                "+weather",
                "subscript",
                "ğ‘º",
                "ğ¾",
                "24",
                "+weather",
                "\\bm{S}_{K=24,\\text{+weather}}",
                ", ",
                "ğ‘º",
                "K",
                "=",
                "6",
                ",",
                "-weather",
                "subscript",
                "ğ‘º",
                "ğ¾",
                "6",
                "-weather",
                "\\bm{S}_{K=6,\\text{-weather}}",
                ", ",
                "ğ‘º",
                "K",
                "=",
                "12",
                ",",
                "-weather",
                "subscript",
                "ğ‘º",
                "ğ¾",
                "12",
                "-weather",
                "\\bm{S}_{K=12,\\text{-weather}}",
                " and ",
                "ğ‘º",
                "K",
                "=",
                "24",
                ",",
                "-weather",
                "subscript",
                "ğ‘º",
                "ğ¾",
                "24",
                "-weather",
                "\\bm{S}_{K=24,\\text{-weather}}",
                ". The RMSE reported is an average over all clients (in distributed approaches) or over all sequences (in the centralised approach).\nFor experiments that involve tuning hyperparameters (namely those that involve FL), we report the test set RMSE for the best performing model based on the lowest error on the validation set. We also report the mean RMSE and lowest RMSE over all datasets for each training approach along with a percentage difference to compare with the fully private localised approach. Model performance results are detailed in ",
                "TableÂ II",
                ".",
                "In the centralised approach, the training procedure has access to all sequences pooled from across the individual household datasets. Therefore model performance might be expected to be relatively high compared to the other approaches where there is much less data to learn from. Conversely, we show that average model performance in the centralised approach is actually 4.8% worse than the localised approach and the best centralised model is 8.0% worse than the best localised model. This is somewhat surprising given that the localised models only have access to ",
                "1",
                "/",
                "100",
                "1",
                "100",
                "1/100",
                " the amount of data. This implies that the centralised models (and possibly single, joint models in general) struggle to capture individual household behaviours in energy usage and/or suffer from trying to optimise for competing objectives. Larger models might allow for learning more individual behaviours but as data has to be gathered into a single location, the privacy risk to energy consumers is by far the highest in this training approach. The 24-step sequence (1 whole of day of prior readings) provides the model with the most information with which to make a prediction, resulting in the lowest RMSE in the centralised approach (followed by the 12-step, then 6-step).",
                "In the localised learning approach, a model for each household is trained in isolation using only the data available to that household. Model performance is exceptionally good in this approach and the simple LSTM architecture is sufficient to learn more nuanced energy demand behaviours unique to each household. This approach represents a fully private setting in that nothing is shared between households. Datasets formed around 12-step sequences result in models that significantly outperform 6-step and 24-step sequence datasets in this approach. We see a similar pattern for the remaining training approaches, suggesting that a 12-hour time window is optimal for local learners to most accurately predict future energy demand.",
                "In the FL approach, only a fraction of clients are selected for each round of training (and each client trains on its local data set in isolation for a small number of epochs). Additionally a single, joint model is being co-trained by these selected clients when model updates are aggregated. As such, we see that the RMSE for FL models suffers in the same way as centralised models when we compare to the localised approach. Additionally, as FL has been shown to perform sub-optimally in cases where the training data is non-IID as is the case with the individual household datasets, the RMSE suffers even more so than in the centralised learning approach. Compared with localised learning the average model performance in the FL approach was 7.6% worse with the best FL model significantly worse (10.5% higher RMSE) than the best localised learning model.",
                "The FL+HC approach produces specialised models for a number of clusters of clients that can more specifically tailor forecasts for groups of households that provide similar model updates (a proxy for similar underlying energy demand distributions across clients). As such, the average RMSE of clients is no longer tied to a single, joint model as in the FL training approach, but rather to a specialised cluster model. In the average case across the 6 datasets, FL+HC produces models 4.7% worse than the localised approach - comparable to centralised learning. However, the best model trained with FL+HC significantly outperforms models trained with FL or centralised learning but remains 3.5% worse than localised learning. Although the FL approaches (FL and FL+HC) do occasionally produce a slightly better model than the centralised training scenario, the mean RMSE across all datasets shows that on average FL performance is degraded compared to centralised learning, consistent with the findings of most previous FL literature.",
                "Although the base models trained with FL and FL+HC show a higher RMSE than those trained with fully private localised learning, we now show how the situation can be improved if we treat FL or FL+HC as a pre-training task to be followed by further fine-tuning on the local clients in isolation. In the FL ",
                "â†’",
                "â†’",
                "\\rightarrow",
                " LFT approach, we use each joint model trained under FL on each of the 6 datasets and perform a small amount of further training per client to produced highly specialised models. The trained parameters of the base models serve as a good initialisation point for rapid training on the clients which often converge within just a few epochs of fine-tuning. These personalised models exhibit a lower RMSE than the other approaches across all datasets. On average FL ",
                "â†’",
                "â†’",
                "\\rightarrow",
                " LFT produces models with a 4.5% lower RMSE than localised training with the best model performing 4.9% better than the best localised model. In FL+HC ",
                "â†’",
                "â†’",
                "\\rightarrow",
                " LFT approach, clients initialise their personalised models from the specialised model trained within the cluster each client belongs to. This approach produces similarly performing models (4.5% better than the average and best localised model). These personalisation approaches clearly show that local models can benefit by learning from the energy demand patterns of other users. FL allows for energy consumers to contribute to the shared learning task whilst retaining the privacy of their raw consumption data prior to privately fine-tuning their own models to produce more accurate forecasts.",
                "The datasets that included weather features (",
                "ğ‘º",
                "K",
                "=",
                "6",
                ",",
                "+weather",
                "subscript",
                "ğ‘º",
                "ğ¾",
                "6",
                "+weather",
                "\\bm{S}_{K=6,\\text{+weather}}",
                ", ",
                "ğ‘º",
                "K",
                "=",
                "12",
                ",",
                "+weather",
                "subscript",
                "ğ‘º",
                "ğ¾",
                "12",
                "+weather",
                "\\bm{S}_{K=12,\\text{+weather}}",
                " and ",
                "ğ‘º",
                "K",
                "=",
                "24",
                ",",
                "+weather",
                "subscript",
                "ğ‘º",
                "ğ¾",
                "24",
                "+weather",
                "\\bm{S}_{K=24,\\text{+weather}}",
                ") show a small improvement in model performance in almost all scenarios compared to datasets without such features. We would therefore recommend that an load forecasting system should make use of weather related features if possible as these indicators can help the model to make better predictions on the whole."
            ]
        ]
    },
    "S5.T3": {
        "caption": "TABLE III: Computational efficiency (measured in millions of samples required to train top performing models) for the 6 datasets and 6 training scenarios. â€™Meanâ€™ and â€™Bestâ€™ columns show savings in computation compared to fully-private localised learning.",
        "table": "<table id=\"S5.T3.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T3.2.3.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.2.3.1.1\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"S5.T3.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"6\"><span id=\"S5.T3.2.3.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Dataset</span></td>\n<td id=\"S5.T3.2.3.1.3\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"S5.T3.2.3.1.4\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"S5.T3.2.3.1.5\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"S5.T3.2.3.1.6\" class=\"ltx_td ltx_border_tt\"></td>\n</tr>\n<tr id=\"S5.T3.2.4.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.2.4.2.1\" class=\"ltx_td\"></td>\n<td id=\"S5.T3.2.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\"><span id=\"S5.T3.2.4.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">incl. weather</span></td>\n<td id=\"S5.T3.2.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\"><span id=\"S5.T3.2.4.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">not incl. weather</span></td>\n<td id=\"S5.T3.2.4.2.4\" class=\"ltx_td\"></td>\n<td id=\"S5.T3.2.4.2.5\" class=\"ltx_td\"></td>\n<td id=\"S5.T3.2.4.2.6\" class=\"ltx_td\"></td>\n<td id=\"S5.T3.2.4.2.7\" class=\"ltx_td\"></td>\n</tr>\n<tr id=\"S5.T3.2.5.3\" class=\"ltx_tr\">\n<td id=\"S5.T3.2.5.3.1\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T3.2.5.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Training scenario</span></td>\n<td id=\"S5.T3.2.5.3.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T3.2.5.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">6 steps</span></td>\n<td id=\"S5.T3.2.5.3.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T3.2.5.3.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">12 steps</span></td>\n<td id=\"S5.T3.2.5.3.4\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T3.2.5.3.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">24 steps</span></td>\n<td id=\"S5.T3.2.5.3.5\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T3.2.5.3.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">6 steps</span></td>\n<td id=\"S5.T3.2.5.3.6\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T3.2.5.3.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">12 steps</span></td>\n<td id=\"S5.T3.2.5.3.7\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T3.2.5.3.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">24 steps</span></td>\n<td id=\"S5.T3.2.5.3.8\" class=\"ltx_td ltx_align_center\" colspan=\"2\"><span id=\"S5.T3.2.5.3.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">mean</span></td>\n<td id=\"S5.T3.2.5.3.9\" class=\"ltx_td ltx_align_center\" colspan=\"2\"><span id=\"S5.T3.2.5.3.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">best*</span></td>\n</tr>\n<tr id=\"S5.T3.2.6.4\" class=\"ltx_tr\">\n<td id=\"S5.T3.2.6.4.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S5.T3.2.6.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Centralised</span></td>\n<td id=\"S5.T3.2.6.4.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T3.2.6.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">78.9</span></td>\n<td id=\"S5.T3.2.6.4.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T3.2.6.4.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">19.5</span></td>\n<td id=\"S5.T3.2.6.4.4\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T3.2.6.4.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">21.2</span></td>\n<td id=\"S5.T3.2.6.4.5\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T3.2.6.4.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">15.6</span></td>\n<td id=\"S5.T3.2.6.4.6\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T3.2.6.4.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">26.6</span></td>\n<td id=\"S5.T3.2.6.4.7\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T3.2.6.4.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">57.7</span></td>\n<td id=\"S5.T3.2.6.4.8\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T3.2.6.4.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">36.6</span></td>\n<td id=\"S5.T3.2.6.4.9\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S5.T3.2.6.4.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">(1.9x)</span></td>\n<td id=\"S5.T3.2.6.4.10\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T3.2.6.4.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">57.7</span></td>\n<td id=\"S5.T3.2.6.4.11\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S5.T3.2.6.4.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">(1.4x)</span></td>\n</tr>\n<tr id=\"S5.T3.2.7.5\" class=\"ltx_tr\">\n<td id=\"S5.T3.2.7.5.1\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T3.2.7.5.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Localised</span></td>\n<td id=\"S5.T3.2.7.5.2\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.7.5.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">71.5</span></td>\n<td id=\"S5.T3.2.7.5.3\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.7.5.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">79.5</span></td>\n<td id=\"S5.T3.2.7.5.4\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.7.5.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">52.7</span></td>\n<td id=\"S5.T3.2.7.5.5\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.7.5.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">94.9</span></td>\n<td id=\"S5.T3.2.7.5.6\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.7.5.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">67.1</span></td>\n<td id=\"S5.T3.2.7.5.7\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.7.5.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">60.5</span></td>\n<td id=\"S5.T3.2.7.5.8\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.7.5.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">71.0</span></td>\n<td id=\"S5.T3.2.7.5.9\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T3.2.7.5.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">(â€”)</span></td>\n<td id=\"S5.T3.2.7.5.10\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.7.5.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">79.5</span></td>\n<td id=\"S5.T3.2.7.5.11\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T3.2.7.5.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">(â€”)</span></td>\n</tr>\n<tr id=\"S5.T3.2.8.6\" class=\"ltx_tr\">\n<td id=\"S5.T3.2.8.6.1\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T3.2.8.6.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">FL</span></td>\n<td id=\"S5.T3.2.8.6.2\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.8.6.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">266.3</span></td>\n<td id=\"S5.T3.2.8.6.3\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.8.6.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">290.7</span></td>\n<td id=\"S5.T3.2.8.6.4\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.8.6.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">384.4</span></td>\n<td id=\"S5.T3.2.8.6.5\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.8.6.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">238.1</span></td>\n<td id=\"S5.T3.2.8.6.6\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.8.6.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">424.3</span></td>\n<td id=\"S5.T3.2.8.6.7\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.8.6.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">455.3</span></td>\n<td id=\"S5.T3.2.8.6.8\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.8.6.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">343.2</span></td>\n<td id=\"S5.T3.2.8.6.9\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T3.2.8.6.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">(0.2x)</span></td>\n<td id=\"S5.T3.2.8.6.10\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.8.6.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">290.7</span></td>\n<td id=\"S5.T3.2.8.6.11\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T3.2.8.6.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">(0.3x)</span></td>\n</tr>\n<tr id=\"S5.T3.2.9.7\" class=\"ltx_tr\">\n<td id=\"S5.T3.2.9.7.1\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T3.2.9.7.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">FL+HC</span></td>\n<td id=\"S5.T3.2.9.7.2\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.9.7.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">6.3</span></td>\n<td id=\"S5.T3.2.9.7.3\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.9.7.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">6.1</span></td>\n<td id=\"S5.T3.2.9.7.4\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.9.7.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">6.8</span></td>\n<td id=\"S5.T3.2.9.7.5\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.9.7.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">7.3</span></td>\n<td id=\"S5.T3.2.9.7.6\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.9.7.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">7.0</span></td>\n<td id=\"S5.T3.2.9.7.7\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.9.7.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.1</span></td>\n<td id=\"S5.T3.2.9.7.8\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.9.7.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">5.6</span></td>\n<td id=\"S5.T3.2.9.7.9\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T3.2.9.7.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">(12.7x)</span></td>\n<td id=\"S5.T3.2.9.7.10\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.9.7.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">6.1</span></td>\n<td id=\"S5.T3.2.9.7.11\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T3.2.9.7.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">(13.1x)</span></td>\n</tr>\n<tr id=\"S5.T3.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S5.T3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">FL </span><math id=\"S5.T3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\rightarrow\" display=\"inline\"><semantics id=\"S5.T3.1.1.1.m1.1a\"><mo mathsize=\"90%\" stretchy=\"false\" id=\"S5.T3.1.1.1.m1.1.1\" xref=\"S5.T3.1.1.1.m1.1.1.cmml\">â†’</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.1.1.1.m1.1b\"><ci id=\"S5.T3.1.1.1.m1.1.1.cmml\" xref=\"S5.T3.1.1.1.m1.1.1\">â†’</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.1.1.1.m1.1c\">\\rightarrow</annotation></semantics></math><span id=\"S5.T3.1.1.1.2\" class=\"ltx_text\" style=\"font-size:90%;\"> LFT</span>\n</td>\n<td id=\"S5.T3.1.1.2\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">268.4</span></td>\n<td id=\"S5.T3.1.1.3\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">292.5</span></td>\n<td id=\"S5.T3.1.1.4\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.1.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">388.0</span></td>\n<td id=\"S5.T3.1.1.5\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.1.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">239.2</span></td>\n<td id=\"S5.T3.1.1.6\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.1.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">429.6</span></td>\n<td id=\"S5.T3.1.1.7\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.1.1.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">458.2</span></td>\n<td id=\"S5.T3.1.1.8\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.1.1.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">346.0</span></td>\n<td id=\"S5.T3.1.1.9\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T3.1.1.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">(0.2x)</span></td>\n<td id=\"S5.T3.1.1.10\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.1.1.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">429.6</span></td>\n<td id=\"S5.T3.1.1.11\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T3.1.1.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">(0.2x)</span></td>\n</tr>\n<tr id=\"S5.T3.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.2.2.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S5.T3.2.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">FL+HC </span><math id=\"S5.T3.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\rightarrow\" display=\"inline\"><semantics id=\"S5.T3.2.2.1.m1.1a\"><mo mathsize=\"90%\" stretchy=\"false\" id=\"S5.T3.2.2.1.m1.1.1\" xref=\"S5.T3.2.2.1.m1.1.1.cmml\">â†’</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.2.2.1.m1.1b\"><ci id=\"S5.T3.2.2.1.m1.1.1.cmml\" xref=\"S5.T3.2.2.1.m1.1.1\">â†’</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.2.2.1.m1.1c\">\\rightarrow</annotation></semantics></math><span id=\"S5.T3.2.2.1.2\" class=\"ltx_text\" style=\"font-size:90%;\"> LFT</span>\n</td>\n<td id=\"S5.T3.2.2.2\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">7.2</span></td>\n<td id=\"S5.T3.2.2.3\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">6.9</span></td>\n<td id=\"S5.T3.2.2.4\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">7.9</span></td>\n<td id=\"S5.T3.2.2.5\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.2.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">8.0</span></td>\n<td id=\"S5.T3.2.2.6\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.2.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">8.7</span></td>\n<td id=\"S5.T3.2.2.7\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.2.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.2</span></td>\n<td id=\"S5.T3.2.2.8\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.2.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">6.7</span></td>\n<td id=\"S5.T3.2.2.9\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T3.2.2.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">(10.7x)</span></td>\n<td id=\"S5.T3.2.2.10\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.2.2.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">8.7</span></td>\n<td id=\"S5.T3.2.2.11\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T3.2.2.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">(9.1x)</span></td>\n</tr>\n<tr id=\"S5.T3.2.10.8\" class=\"ltx_tr\">\n<td id=\"S5.T3.2.10.8.1\" class=\"ltx_td ltx_align_left ltx_border_tt\" colspan=\"11\">\n<span id=\"S5.T3.2.10.8.1.1\" class=\"ltx_inline-block ltx_parbox ltx_align_middle\" style=\"width:426.8pt;\">\n<span id=\"S5.T3.2.10.8.1.1.1\" class=\"ltx_p\"><span id=\"S5.T3.2.10.8.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">* the best savings are calculated by comparing the best performing model for each training scenario to the best performing localised model, as reported in <a href=\"#S5.T2\" title=\"TABLE II â€£ V-A Forecasting performance â€£ V Results &amp; Discussion â€£ Federated Learning for Short-term Residential Load Forecasting\" class=\"ltx_ref ltx_refmacro_autoref\"><span class=\"ltx_text ltx_ref_tag\">TableÂ II</span></a>.</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "To understand how the different training approaches perform compared to one another, we report the RMSE achieved on the test set for each of the 6 datasets, ",
                "ğ‘º",
                "K",
                "=",
                "6",
                ",",
                "+weather",
                "subscript",
                "ğ‘º",
                "ğ¾",
                "6",
                "+weather",
                "\\bm{S}_{K=6,\\text{+weather}}",
                ", ",
                "ğ‘º",
                "K",
                "=",
                "12",
                ",",
                "+weather",
                "subscript",
                "ğ‘º",
                "ğ¾",
                "12",
                "+weather",
                "\\bm{S}_{K=12,\\text{+weather}}",
                ", ",
                "ğ‘º",
                "K",
                "=",
                "24",
                ",",
                "+weather",
                "subscript",
                "ğ‘º",
                "ğ¾",
                "24",
                "+weather",
                "\\bm{S}_{K=24,\\text{+weather}}",
                ", ",
                "ğ‘º",
                "K",
                "=",
                "6",
                ",",
                "-weather",
                "subscript",
                "ğ‘º",
                "ğ¾",
                "6",
                "-weather",
                "\\bm{S}_{K=6,\\text{-weather}}",
                ", ",
                "ğ‘º",
                "K",
                "=",
                "12",
                ",",
                "-weather",
                "subscript",
                "ğ‘º",
                "ğ¾",
                "12",
                "-weather",
                "\\bm{S}_{K=12,\\text{-weather}}",
                " and ",
                "ğ‘º",
                "K",
                "=",
                "24",
                ",",
                "-weather",
                "subscript",
                "ğ‘º",
                "ğ¾",
                "24",
                "-weather",
                "\\bm{S}_{K=24,\\text{-weather}}",
                ". The RMSE reported is an average over all clients (in distributed approaches) or over all sequences (in the centralised approach).\nFor experiments that involve tuning hyperparameters (namely those that involve FL), we report the test set RMSE for the best performing model based on the lowest error on the validation set. We also report the mean RMSE and lowest RMSE over all datasets for each training approach along with a percentage difference to compare with the fully private localised approach. Model performance results are detailed in ",
                "TableÂ II",
                ".",
                "In the centralised approach, the training procedure has access to all sequences pooled from across the individual household datasets. Therefore model performance might be expected to be relatively high compared to the other approaches where there is much less data to learn from. Conversely, we show that average model performance in the centralised approach is actually 4.8% worse than the localised approach and the best centralised model is 8.0% worse than the best localised model. This is somewhat surprising given that the localised models only have access to ",
                "1",
                "/",
                "100",
                "1",
                "100",
                "1/100",
                " the amount of data. This implies that the centralised models (and possibly single, joint models in general) struggle to capture individual household behaviours in energy usage and/or suffer from trying to optimise for competing objectives. Larger models might allow for learning more individual behaviours but as data has to be gathered into a single location, the privacy risk to energy consumers is by far the highest in this training approach. The 24-step sequence (1 whole of day of prior readings) provides the model with the most information with which to make a prediction, resulting in the lowest RMSE in the centralised approach (followed by the 12-step, then 6-step).",
                "In the localised learning approach, a model for each household is trained in isolation using only the data available to that household. Model performance is exceptionally good in this approach and the simple LSTM architecture is sufficient to learn more nuanced energy demand behaviours unique to each household. This approach represents a fully private setting in that nothing is shared between households. Datasets formed around 12-step sequences result in models that significantly outperform 6-step and 24-step sequence datasets in this approach. We see a similar pattern for the remaining training approaches, suggesting that a 12-hour time window is optimal for local learners to most accurately predict future energy demand.",
                "In the FL approach, only a fraction of clients are selected for each round of training (and each client trains on its local data set in isolation for a small number of epochs). Additionally a single, joint model is being co-trained by these selected clients when model updates are aggregated. As such, we see that the RMSE for FL models suffers in the same way as centralised models when we compare to the localised approach. Additionally, as FL has been shown to perform sub-optimally in cases where the training data is non-IID as is the case with the individual household datasets, the RMSE suffers even more so than in the centralised learning approach. Compared with localised learning the average model performance in the FL approach was 7.6% worse with the best FL model significantly worse (10.5% higher RMSE) than the best localised learning model.",
                "The FL+HC approach produces specialised models for a number of clusters of clients that can more specifically tailor forecasts for groups of households that provide similar model updates (a proxy for similar underlying energy demand distributions across clients). As such, the average RMSE of clients is no longer tied to a single, joint model as in the FL training approach, but rather to a specialised cluster model. In the average case across the 6 datasets, FL+HC produces models 4.7% worse than the localised approach - comparable to centralised learning. However, the best model trained with FL+HC significantly outperforms models trained with FL or centralised learning but remains 3.5% worse than localised learning. Although the FL approaches (FL and FL+HC) do occasionally produce a slightly better model than the centralised training scenario, the mean RMSE across all datasets shows that on average FL performance is degraded compared to centralised learning, consistent with the findings of most previous FL literature.",
                "Although the base models trained with FL and FL+HC show a higher RMSE than those trained with fully private localised learning, we now show how the situation can be improved if we treat FL or FL+HC as a pre-training task to be followed by further fine-tuning on the local clients in isolation. In the FL ",
                "â†’",
                "â†’",
                "\\rightarrow",
                " LFT approach, we use each joint model trained under FL on each of the 6 datasets and perform a small amount of further training per client to produced highly specialised models. The trained parameters of the base models serve as a good initialisation point for rapid training on the clients which often converge within just a few epochs of fine-tuning. These personalised models exhibit a lower RMSE than the other approaches across all datasets. On average FL ",
                "â†’",
                "â†’",
                "\\rightarrow",
                " LFT produces models with a 4.5% lower RMSE than localised training with the best model performing 4.9% better than the best localised model. In FL+HC ",
                "â†’",
                "â†’",
                "\\rightarrow",
                " LFT approach, clients initialise their personalised models from the specialised model trained within the cluster each client belongs to. This approach produces similarly performing models (4.5% better than the average and best localised model). These personalisation approaches clearly show that local models can benefit by learning from the energy demand patterns of other users. FL allows for energy consumers to contribute to the shared learning task whilst retaining the privacy of their raw consumption data prior to privately fine-tuning their own models to produce more accurate forecasts.",
                "The datasets that included weather features (",
                "ğ‘º",
                "K",
                "=",
                "6",
                ",",
                "+weather",
                "subscript",
                "ğ‘º",
                "ğ¾",
                "6",
                "+weather",
                "\\bm{S}_{K=6,\\text{+weather}}",
                ", ",
                "ğ‘º",
                "K",
                "=",
                "12",
                ",",
                "+weather",
                "subscript",
                "ğ‘º",
                "ğ¾",
                "12",
                "+weather",
                "\\bm{S}_{K=12,\\text{+weather}}",
                " and ",
                "ğ‘º",
                "K",
                "=",
                "24",
                ",",
                "+weather",
                "subscript",
                "ğ‘º",
                "ğ¾",
                "24",
                "+weather",
                "\\bm{S}_{K=24,\\text{+weather}}",
                ") show a small improvement in model performance in almost all scenarios compared to datasets without such features. We would therefore recommend that an load forecasting system should make use of weather related features if possible as these indicators can help the model to make better predictions on the whole."
            ]
        ]
    }
}