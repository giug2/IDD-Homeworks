{
    "id_table_1": {
        "caption": "Table 1:  Results of different models for sample data in Round 1 and 2. The cells with the X refers to tries where the LLM could not compute the task, and either the API was not returning any results over a long period of time or, in the case of  gemma-7b  the model was returning failure message. In bold, instead, we show the best performing results. In the table h1 and h5 refers to Hit@1 and Hit@5 metrics from the evaluation script.",
        "table": "S4.T1.3",
        "footnotes": [],
        "references": [
            "This year, the SemTab challenge introduced a new track: the  Metadata to KG  track. Participants of this task were asked to map table metadata to KGs without having access to the underlying data. This presents a unique challenge due to the limited available context, making traditional STI methods less applicable, as they typically rely on actual data for annotation. To better define this metadata-only task, we introduce the term  Column Vocabulary Association (CVA) . As further described in section  1.2 ,CVA involves annotating columns using solely KGs table metadata, without utilizing the underlying data. This approach is particularly relevant in scenarios where the data is confidential and cannot be accessed.",
            "The tables metadata file included information about 141 columns derived from different tables. For each column, the provided information included the column ID, column label, table ID, table name, and a list of the other column labels within the same table. The DBpedia properties file contained 2,881 properties. For each DBpedia property, the information included the property ID (the actual URI of the property in DBpedia), the property label, and the description. Below, we report examples of a table metadata entry (Listing  1 ) and of DBpedia property (Listing  2 ).",
            "Here we present the results from our initial analysis using different LLMs and temperature settings. We employed three models from OpenAI and four open-source models, testing them at five different temperatures, as detailed in the table below 1 . The table shows the average accuracy results for each model-temperature combination, evaluated using the evaluation script with the sample metadata and sample ground truth. Each query was run three times per model-temperature combination, and accuracy results were then averages. The numbers in bold correspond to the best-performing model-temperature combinations.  gpt-4o  outperformed other models in both Rounds 1 and 2, specifically at temperatures 0.5, 0.75, and 1.0. We observed that the LLMs did not perform very well in Round 2. In the discussion section 5  we explore possible reasons for this outcome.  Based on these preliminary results, for Round 1, we used  gpt-4o  at temperatures 0.5, 0.75, and 1.0 on the full metadata file for final analysis. For Round 2, we used  gpt-4o  only at temperatures 0.5 and 0.75."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Results of different embedding combination for sample data in Round 1 and 2. In bold we are showing the best performing results. In the table h1 and h5 refers to Hit@1 and Hit@5 metrics from the evaluation script.",
        "table": "S4.T2.1",
        "footnotes": [],
        "references": [
            "This year, the SemTab challenge introduced a new track: the  Metadata to KG  track. Participants of this task were asked to map table metadata to KGs without having access to the underlying data. This presents a unique challenge due to the limited available context, making traditional STI methods less applicable, as they typically rely on actual data for annotation. To better define this metadata-only task, we introduce the term  Column Vocabulary Association (CVA) . As further described in section  1.2 ,CVA involves annotating columns using solely KGs table metadata, without utilizing the underlying data. This approach is particularly relevant in scenarios where the data is confidential and cannot be accessed.",
            "The tables metadata file included information about 141 columns derived from different tables. For each column, the provided information included the column ID, column label, table ID, table name, and a list of the other column labels within the same table. The DBpedia properties file contained 2,881 properties. For each DBpedia property, the information included the property ID (the actual URI of the property in DBpedia), the property label, and the description. Below, we report examples of a table metadata entry (Listing  1 ) and of DBpedia property (Listing  2 ).",
            "Below we show the results from our initial analysis with SentenceBERT. Table  2  includes the possible combinations of information from the table metadata and the glossary, and the accuracy results for both Round 1 and 2, which we obtained by running the evaluation script against the ground truth for the sample metadata file. We used these results to find the best performing combinations, which were then applied to the full metadata file."
        ]
    },
    "global_footnotes": []
}