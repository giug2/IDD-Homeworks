{
    "A4.T1": {
        "caption": "Table 1: Architectures",
        "table": "<table id=\"A4.T1.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"A4.T1.4.1.1\" class=\"ltx_tr\">\n<th id=\"A4.T1.4.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\"><span id=\"A4.T1.4.1.1.1.1\" class=\"ltx_text ltx_font_bold\">RNN and LSTMs</span></th>\n<td id=\"A4.T1.4.1.1.2\" class=\"ltx_td ltx_align_left ltx_border_tt\"><span id=\"A4.T1.4.1.1.2.1\" class=\"ltx_text ltx_font_bold\">S</span></td>\n<td id=\"A4.T1.4.1.1.3\" class=\"ltx_td ltx_align_left ltx_border_tt\"><span id=\"A4.T1.4.1.1.3.1\" class=\"ltx_text ltx_font_bold\">M</span></td>\n<td id=\"A4.T1.4.1.1.4\" class=\"ltx_td ltx_align_left ltx_border_tt\"><span id=\"A4.T1.4.1.1.4.1\" class=\"ltx_text ltx_font_bold\">L</span></td>\n</tr>\n<tr id=\"A4.T1.4.2.2\" class=\"ltx_tr\">\n<th id=\"A4.T1.4.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">RNN Hidden size</th>\n<td id=\"A4.T1.4.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_t\">16</td>\n<td id=\"A4.T1.4.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_t\">32</td>\n<td id=\"A4.T1.4.2.2.4\" class=\"ltx_td ltx_align_left ltx_border_t\">128</td>\n</tr>\n<tr id=\"A4.T1.4.3.3\" class=\"ltx_tr\">\n<th id=\"A4.T1.4.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Number of RNN layers</th>\n<td id=\"A4.T1.4.3.3.2\" class=\"ltx_td ltx_align_left\">1</td>\n<td id=\"A4.T1.4.3.3.3\" class=\"ltx_td ltx_align_left\">2</td>\n<td id=\"A4.T1.4.3.3.4\" class=\"ltx_td ltx_align_left\">3</td>\n</tr>\n<tr id=\"A4.T1.4.4.4\" class=\"ltx_tr\">\n<th id=\"A4.T1.4.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MLP before RNN layers</th>\n<td id=\"A4.T1.4.4.4.2\" class=\"ltx_td ltx_align_left\">(16,)</td>\n<td id=\"A4.T1.4.4.4.3\" class=\"ltx_td ltx_align_left\">(32, 32)</td>\n<td id=\"A4.T1.4.4.4.4\" class=\"ltx_td ltx_align_left\">(128, 128, 128)</td>\n</tr>\n<tr id=\"A4.T1.4.5.5\" class=\"ltx_tr\">\n<th id=\"A4.T1.4.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">MLP after RNN layers</th>\n<td id=\"A4.T1.4.5.5.2\" class=\"ltx_td ltx_align_left\">(16,)</td>\n<td id=\"A4.T1.4.5.5.3\" class=\"ltx_td ltx_align_left\">(32, 32)</td>\n<td id=\"A4.T1.4.5.5.4\" class=\"ltx_td ltx_align_left\">(128, 128, 128)</td>\n</tr>\n<tr id=\"A4.T1.4.6.6\" class=\"ltx_tr\">\n<th id=\"A4.T1.4.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span id=\"A4.T1.4.6.6.1.1\" class=\"ltx_text ltx_font_bold\">Transformer SINCOS</span></th>\n<td id=\"A4.T1.4.6.6.2\" class=\"ltx_td ltx_border_t\"/>\n<td id=\"A4.T1.4.6.6.3\" class=\"ltx_td ltx_border_t\"/>\n<td id=\"A4.T1.4.6.6.4\" class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr id=\"A4.T1.4.7.7\" class=\"ltx_tr\">\n<th id=\"A4.T1.4.7.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Embedding dimension</th>\n<td id=\"A4.T1.4.7.7.2\" class=\"ltx_td ltx_align_left\">16</td>\n<td id=\"A4.T1.4.7.7.3\" class=\"ltx_td ltx_align_left\">64</td>\n<td id=\"A4.T1.4.7.7.4\" class=\"ltx_td ltx_align_left\">256</td>\n</tr>\n<tr id=\"A4.T1.4.8.8\" class=\"ltx_tr\">\n<th id=\"A4.T1.4.8.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Number of heads</th>\n<td id=\"A4.T1.4.8.8.2\" class=\"ltx_td ltx_align_left\">2</td>\n<td id=\"A4.T1.4.8.8.3\" class=\"ltx_td ltx_align_left\">4</td>\n<td id=\"A4.T1.4.8.8.4\" class=\"ltx_td ltx_align_left\">4</td>\n</tr>\n<tr id=\"A4.T1.4.9.9\" class=\"ltx_tr\">\n<th id=\"A4.T1.4.9.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Number of layers</th>\n<td id=\"A4.T1.4.9.9.2\" class=\"ltx_td ltx_align_left ltx_border_bb\">2</td>\n<td id=\"A4.T1.4.9.9.3\" class=\"ltx_td ltx_align_left ltx_border_bb\">4</td>\n<td id=\"A4.T1.4.9.9.4\" class=\"ltx_td ltx_align_left ltx_border_bb\">6</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "A vanilla multi-layer RNN (Elman, 1990) with hidden sizes and multi-layer perceptron (MLP) before and after the RNN layers as described in Table 1.",
            "A multi-layer RNN controller with hidden sizes and MLP exactly the same as the RNN and LSTMs on Table 1 with access to a differentiable stack (Joulin and Mikolov, 2015).\nThe controller can perform any linear combination of push, pop, and no-op on the stack of size according to Table 1, with action weights given by a softmax over a linear readout of the RNN output. Each cell of the stack contains a real vector of dimension 6 and the stack size is 64 for all (S, M and L) sizes.",
            "A multi-layer RNN controller with hidden sizes according to the Table 1 with access to a differentiable tape, inspired by the Baby-NTM architecture (Suzgun et al., 2019).\nThe controller can perform any linear combination of write-right, write-left, write-stay, jump-left, and jump-right on the tape, with action weights given by a softmax.\nThe actions correspond to: writing at the current position and moving to the right (write-right), writing at the current position and moving to the left (write-left), writing at the current position (write-stay), jumping ℓℓ\\ell steps to the right without writing (jump-right), where ℓℓ\\ell is the length of the input, and jumping ℓℓ\\ell steps to the left without writing (jump-left).\nAs in the Stack-RNN, each cell of the tape contains a real vector of dimension 6 and the tape size is 64 for all (S, M and L) sizes.",
            "A multi-layer LSTM (Hochreiter and Schmidhuber, 1997) of hidden sizes according to Table 1.",
            "A vanilla Transformer decoder (Vaswani et al., 2017). See Table 1 for the embedding dimension, number of heads and number of layers for each model size (S, M and L). Each layer is composed of an attention layer, two dense layers, and a layer normalization. We add a residual connections as in the original architecture (Vaswani et al., 2017).\nWe consider the standard sin/cos (Vaswani et al., 2017) positional encoding."
        ]
    },
    "A4.T2": {
        "caption": "Table 2: \nTable taken from (Deletang et al., 2022). Tasks with their level in the Chomsky hierarchy and example input/output pairs.\nThe ††\\dagger denotes permutation-invariant tasks; the ⋆⋆\\star denotes counting tasks; the ∘\\circ denotes tasks that require a nondeterministic controller; and the ×\\times denotes tasks that require superlinear running time in terms of the input length.",
        "table": null,
        "footnotes": [],
        "references": []
    },
    "A5.T3": {
        "caption": "Table 3: \nPre-trained BP program sampling probabilities\nInstead of sampling programs uniformly, we can sample them w.r.t. any\nprobability distribution Q𝑄Q that satisfies Theorem 9.\nWe initially sampled programs uniformly and filtered out ‘boring’ sequences.\nThen we trained Q𝑄Q via cross-entropy to mimic the distribution of ‘interesting’ sequences.\nWe used a 2nd-order Markov process as a model for Q𝑄Q.\nWhile uniform sampling resulted in only 0.02% interesting sequences,\nsampling from Q𝑄Q increased it to 2.5%, a 137-fold improvement.\nThe table on the left shows the 0th, 1st, and 2nd order Markov processes\nQ​(pt)𝑄subscript𝑝𝑡Q(p_{t}),\nQ​(pt|pt−1)𝑄conditionalsubscript𝑝𝑡subscript𝑝𝑡1Q(p_{t}|p_{t-1}), and\nQ​(pt|pt−2​pt−1)𝑄conditionalsubscript𝑝𝑡subscript𝑝𝑡2subscript𝑝𝑡1Q(p_{t}|p_{t-2}p_{t-1}) from which BP programs are sampled, for p⋅∈{<>+-[]{.}subscript𝑝⋅<>+-[]{.p_{\\cdot}\\in\\{\\texttt{<>+-[]\\{.}\\},\nbut where results for [ and { have been merged.\nEach row corresponds to a context (none or pt−1subscript𝑝𝑡1p_{t-1} or pt−2​pt−1subscript𝑝𝑡2subscript𝑝𝑡1p_{t-2}p_{t-1}).\nWe also included Q​(p1|p0:=_)𝑄assignconditionalsubscript𝑝1subscript𝑝0_Q(p_{1}|p_{0}\\!\\!:=\\!\\!\\texttt{\\_}) and Q​(p1|p−1​p0:=__)𝑄assignconditionalsubscript𝑝1subscript𝑝1subscript𝑝0__Q(p_{1}|p_{-1}p_{0}\\!\\!:=\\!\\!\\texttt{\\_\\_}).\nThe entries in each column correspond to the sampling probability of ptsubscript𝑝𝑡p_{t} in the corresponding row-context.\nTraining on interesting sequences has led to a non-uniform distribution Q𝑄Q. Universality is preserved for any k𝑘k-order Markov process, provided all transition probabilities are non-zero.\nThe probability Q​(.)𝑄.Q(\\texttt{.}) of outputting a symbol has nearly doubled from 0.14 to 0.27 on average,\nwhile the probability of loop brackets ([,]) reduced to 0.07 each on average.\nThe marginal probabilities Q​(<)≈Q​(>)≈Q​(+)≈Q​(-)≈1/7𝑄<𝑄>𝑄+𝑄-17Q(\\texttt{<})\\approx Q(\\texttt{>})\\approx Q(\\texttt{+})\\approx Q(\\texttt{-})\\approx 1/7 have not changed much,\nbut many of the conditional ones have.\nCertain combination of instructions are now blocked:\nFor instance +- and -+ and <> and >< have probability close to 00,\nsince they cancel each other and hence are redundant.\nSome triples such as ][- and <+ and >- and others are enhanced.\n\nCaveat: We did not have time to retrain our NN models on these newly generated sequences (experiments are still running).\nBut since the statistics is improved, we expect the results in Figures 4 and 5 to improve or at least not deteriorate.",
        "table": null,
        "footnotes": [],
        "references": [
            "For practical purposes, sampling from non-uniform (possibly learned) distribution over programs can be advantageous for efficiency. For our BrainPhoque language (that we use in our experiments later) it increases the yield of ‘interesting’ programs by a factor of 137 (see Appendix Table 3). Below we show this can be done without any concerns on losing universality."
        ]
    }
}