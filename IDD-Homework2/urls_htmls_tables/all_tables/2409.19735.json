{
    "id_table_1": {
        "caption": "Table 1:  As can be seen as the corruption function increases the CER, the text becomes increasingly illegible",
        "table": "S3.T1.1",
        "footnotes": [],
        "references": [
            "For clarity, Figure  1  shows the flow diagram of the project. The flow diagram has three types: Data, such as the line-aligned texts; Processes, such as corrupting the text; and Computational tools, such as  scrambledtext . Each element of Figure  1  will be described in detail in the rest of the method.",
            "The model is trained using the BLN600, SMH and CA datasets, which are used to try to create diversity in the observed errors. Each datasets OCR and ground truth texts are character aligned using the  genalog   [ gupte_lights_2021 ] . The texts are then sequentially loaded, and the conditional distributions are learned. Once the distributions have been learned, they are saved as a JSON file for use in the rest of the project. With the conditional probabilities learned, text can now be arbitrarily corrupted. Table  1  shows an example of text  [ lovelace_sketch_1842 ]  being corrupted from CER = 0 to CER = 0.5. Table  1  shows both the Target CER that the corruption function was aiming for and the Observed CER of the output sequence and drift from character insertions. The difference is due to the stochastic nature of the process.",
            "In relation to the discussion of PEFT in Section  1 , this paper will use LoRA  [ hu_lora_2021 ] , specifically Rank Stabilised LoRA  [ kalajdzievski_rank_2023 ]  which has been shown to produce better results than the original LoRA approach.",
            "A fundamental question when using synthetic data to fine-tune an LM to learn how to do CLOCR-C is how much error is necessary and how should the error be distributed? This paper explores the topic using three experiments: first, by simply uniformly increasing the CER across the dataset; second, by varying the relationship between CER and WER; and finally, by combining CER and WER levels within the dataset. A more detailed description of the parameters are shown in the bullet points below. Given the extreme levels of corruption shown for CER = 0.5 shown in Table  1 , having a CER level much beyond 0.4 produces almost complete nonsense; as such, this paper will focus on the lower end of the CER scale. The question this series of experiments seeks to answer is what corruption parameters provide the biggest improvement to model performance?",
            "All models with uniform synthetic CER reduced the WER on the NCSE dataset. The CER was minimised when the synthetic training data had a CER of 0.2, close to the median CER of 0.17 of the NCSE dataset, as well as the 0.05 model. Figure  3  shows the performance of the models trained on synthetic data from different CER-WER combinations. In Figure  3 , the performance of the baseline Llama3 model is shown with a solid red lines whilst the original NCSE dataset average is shown with a dashed red line. The figure shows that whilst all models reduced error compared to the baseline WER and most models beat the Baseline Llama3, reducing the CER relative to the NCSE dataset was much more difficult. Primarily, models trained with low levels of CER were more likely to reduce the overall CER; however, there was not such a clear pattern with the WER values. A key factor of OCR errors is that the range of errors can be large and have considerably different impact on the text (see Table 1  for examples). The mean CER of the top ten models was 0.14 and the WER was 0.28; which is a an error reduction percentage of 21% and 55% respectively relative to the NCSE test dataset, and an improvement of CER 55% and WER41% relative to the Baseline Llama model.",
            "One element of the process that could be improved to close the gap with the state of the art is the corruption function itself. The method described in Section  3.2.1  is very simple and may explain the relatively poor performance compared to the other datasets in the high corruption subset. Perhaps a more sophisticated or nuanced approach may produce more realistic errors, improving model performance across the corruption range."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  The prompt is populated by replacing the bold words in curly braces with appropriate descriptors. The descriptors can be found in Table  3",
        "table": "S3.T2.1",
        "footnotes": [
            ""
        ],
        "references": [
            "The rest of the paper is as follows, Section  2  introduces the dataset used in this paper, Section  3  describes the methods for creating the synthetic text and the experiments used to measure its effectiveness as training data, Section  4  reports the results of the experiments, Section  5  is the discussion interpreting the findings and their place relative to previous literature, Section  6  provides a list of heuristics for training CLOCR-C models, finally Section  7  provides the conclusions and considers future work",
            "Due to the issues with the existing methods, this paper develops ScrambledText, a simple, fast corruption process based on a Markov model specifically designed to simulate OCR corruption errors using a straightforward training process and can output text with an arbitrary amount of corruption. The Markov Model is shown in Figure  2 ; the model takes a single correct character  x x x italic_x  and outputs a sequence of characters  y y y italic_y  with a length greater than or equal to 0. Once in the model, each character has a learned conditional probability distribution of passing through the model unaltered, being deleted, substituted, and having a new character inserted after it. The insertion node of the model has a self-referential link, meaning that if a substitution or insertion takes place, the model can keep inserting characters with probability  P  ( I | x ) P conditional I x P(I|x) italic_P ( italic_I | italic_x ) .",
            "Equation  2  represents the probability distribution mapping only a single character to its output sequence, not the entire document. However, as this approach models all characters as independent, the joint probability of output sequence  Y Y Y italic_Y  for input sequence  X X X italic_X  can be represented as",
            "One element of the process that could be improved to close the gap with the state of the art is the corruption function itself. The method described in Section  3.2.1  is very simple and may explain the relatively poor performance compared to the other datasets in the high corruption subset. Perhaps a more sophisticated or nuanced approach may produce more realistic errors, improving model performance across the corruption range."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Variables and options used in the prompt template",
        "table": "S3.T3.1",
        "footnotes": [],
        "references": [
            "The rest of the paper is as follows, Section  2  introduces the dataset used in this paper, Section  3  describes the methods for creating the synthetic text and the experiments used to measure its effectiveness as training data, Section  4  reports the results of the experiments, Section  5  is the discussion interpreting the findings and their place relative to previous literature, Section  6  provides a list of heuristics for training CLOCR-C models, finally Section  7  provides the conclusions and considers future work",
            "The data is created by prompting a Language model (in this case GPT4o) to write a piece of text under a set of guidelines. The prompt is shown in Table  3 .",
            "The seven words inside curly braces, shown in Table  3 , are the variables used to create diversity between the texts. The seed prompt, shown as {text} in the prompt, was obtained by downloading the Wikipedia timeline of the 19th century and the timeline of British diplomacy in the 19th century; combining both lists created xx events to describe. These timelines provide a brief description of an actual 19th-century event, which could then be styled by the rest of the prompt. The variables used for prompt styling were chosen to be appropriate for the 19th century and are shown in table  3 ; there are 3888 possible combinations of text styling. The timeline and prompt variables were then sampled 11,000 times, and the text generated by GPT-4o. This resulted in a corpus of 11,000 texts with 3.5 million words. To create a more manageable training set, a random substring of text 200 tokens long is removed from each article; this is used to create a 10,000-observation training set where each observation is of consistent length. The remaining 1000 observations are split between a test an validation set, but are not used in this project. An example of a prompt an the resultant article are shown in Supplementary material Section 3, the synthetic data can be downloaded from  [ bourne_scrambled_2024 ] .",
            "All models with uniform synthetic CER reduced the WER on the NCSE dataset. The CER was minimised when the synthetic training data had a CER of 0.2, close to the median CER of 0.17 of the NCSE dataset, as well as the 0.05 model. Figure  3  shows the performance of the models trained on synthetic data from different CER-WER combinations. In Figure  3 , the performance of the baseline Llama3 model is shown with a solid red lines whilst the original NCSE dataset average is shown with a dashed red line. The figure shows that whilst all models reduced error compared to the baseline WER and most models beat the Baseline Llama3, reducing the CER relative to the NCSE dataset was much more difficult. Primarily, models trained with low levels of CER were more likely to reduce the overall CER; however, there was not such a clear pattern with the WER values. A key factor of OCR errors is that the range of errors can be large and have considerably different impact on the text (see Table 1  for examples). The mean CER of the top ten models was 0.14 and the WER was 0.28; which is a an error reduction percentage of 21% and 55% respectively relative to the NCSE test dataset, and an improvement of CER 55% and WER41% relative to the Baseline Llama model.",
            "One element of the process that could be improved to close the gap with the state of the art is the corruption function itself. The method described in Section  3.2.1  is very simple and may explain the relatively poor performance compared to the other datasets in the high corruption subset. Perhaps a more sophisticated or nuanced approach may produce more realistic errors, improving model performance across the corruption range."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Hyper-parameters used in the model.",
        "table": "S3.T4.1",
        "footnotes": [],
        "references": [
            "The rest of the paper is as follows, Section  2  introduces the dataset used in this paper, Section  3  describes the methods for creating the synthetic text and the experiments used to measure its effectiveness as training data, Section  4  reports the results of the experiments, Section  5  is the discussion interpreting the findings and their place relative to previous literature, Section  6  provides a list of heuristics for training CLOCR-C models, finally Section  7  provides the conclusions and considers future work",
            "The model will be trained using the Huggingface framework and the Unsloth  [ han_unsloth_2024 ]  implementation of Llama. Training will use the lightning.ai platform, the GPU will be a 24Gb Nvidia L4. Table  4  shows the most important hyper-parameters used in the training. The hyper-parameters were chosen to ensure that the model would not exceed the 24Gb RAM of the GPU; from this perspective, setting the context length was most important from experiments 1024 was chosen as it would be able to handle the tokens produced by all levels of corruption, detail on this choice can be seen in supplementary material Figure 1. After an appropriate context window was found, the batch size and LoRA parameters could be chosen. For a full list of hyper-parameters, see the code.",
            "The text was split at the median CER into high and low corruption subsets to explore the practical significance of the difference in overall CER on model performance. The High-low split was at CER = 0.17, shown in Figure  4 . What Figure  4  reveals is that there is a very large difference in the corruption, with the lower group having a median CER of 0.06 compared to the high corruption groups CER of 0.61. Models that perform well in the low corruption group all have very low target CER values, typically 0.05 or 0.1"
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  The models will be trained using different combinations of text length and token budget. The table shows the number of data observations for a given observation token length and total token combination.",
        "table": "S3.T5.1",
        "footnotes": [],
        "references": [
            "The rest of the paper is as follows, Section  2  introduces the dataset used in this paper, Section  3  describes the methods for creating the synthetic text and the experiments used to measure its effectiveness as training data, Section  4  reports the results of the experiments, Section  5  is the discussion interpreting the findings and their place relative to previous literature, Section  6  provides a list of heuristics for training CLOCR-C models, finally Section  7  provides the conclusions and considers future work",
            "Given that a CER-WER combination can be found that provides sufficient improvement in the test set, it is valuable to investigate whether other factors beyond the corruption type play a role in the performance of the LM at CLOCR-C. Two critical elements in the training of the LM are the length of the text supplied and the number of observations. This will be explored by modifying the total number of tokens in the training set and also the total number of tokens per observation. These two parameters will be varied according to Table  5 . This experiment will produce 36 token text length pairs, from which we will gain insight into the importance of text length and token volume. An example of the different token lengths is shown in supplementary material section 3.",
            "Given the results discussed in the previous section, the model trained with CER0.1 and WER = 0.2 was chosen as the base for the tokens-per-observations and tokens-in-training-set experiment. The results of the experiment can be seen in Figure  5  which shows the interaction between The two variables. Looking at the left panel of Figure  5  the relationship between number of tokens and CER is difficult to see, it appears that there is substantial noise in the results, with only 200 tokens per observation models showing any discernible trend. Such an observation contrasts with the right panel, which shows how CER changes with increased tokens per observation. Here, the CER reduces up to the maximum of 200 tokens per observation.",
            "Finally although this paper did not seek to optimise the performance the findings open the door for work exploring optimisation and model architecture comparison. This is made easier because, as is shown in Figure  5 , the model trained on observations of 200 tokens still beats the baseline when trained on only 512 examples, roughly 5% of the full dataset dramatically reducing training cost. Such work could then more concretely compare CLOCR-C with synthetic data against the state of the art."
        ]
    }
}