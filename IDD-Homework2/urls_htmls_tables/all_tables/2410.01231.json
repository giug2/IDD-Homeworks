{
    "id_table_1": {
        "caption": "Table 1.  PG index comparisons on Gist1M",
        "table": "S1.T1.1.1",
        "footnotes": [],
        "references": [
            "Although PG-based approaches have superior search performance, they still suffer from a significantly higher cost in terms of index construction than other methods.  This is primarily due to the necessity of identifying close neighbors for each point to establish the graph edges.  In the traditional applications of  k k k italic_k -ANN search, PG index is built offline and then responds to  k k k italic_k -ANN queries. Thus its construction cost is treated as a second-class performance indicator. However, in the emerging scenario, i.e., the RAG model training  (Guu et al . ,  2020 ; Izacard et al . ,  2023 ; Asai et al . ,  2023 ) , the PG index will be frequently built in an online manner, due to the tuning on the embedding model that transforms the source data such as text chunks into vectors. As a result, it is urgent to build PG efficiently while maintaining search performance.  To address this issue, several studies have been proposed to accelerate the index construction, such as DiskANN  (Subramanya et al . ,  2019 ) , RNN-Descent  (Ono and Matsui,  2023 ) , LSH-APG  (Zhao et al . ,  2023 )  and ParlayANN  (Manohar et al . ,  2024 ) . However,  as shown in Table  1 ,  their search performance remains noticeably inferior to NSG  (Fu et al . ,  2019 ) , where NSG is one of SOTA methods that do not prioritize index construction.  In Table  1  on  Gist1M Gist1M \\mathsf{Gist1M} sansserif_Gist1M , the second column shows the time of index construction and the next four columns depict queries per second (QPS) at different recall levels (higher is better). Hence, they sacrifice the search performance to expedite the index construction, which violates the primary goal of  k k k italic_k -ANN search.",
            "To extend the greedy routing for  k k k italic_k -ANN search, beam search (i.e., the best-first search) is thus proposed.  Specifically, as shown in Algorithm  1 , the search process starts from an entering point  e  p e p ep italic_e italic_p  and puts it in a sorted array  p  o  o  l p o o l pool italic_p italic_o italic_o italic_l  of nodes, which is maintained to store the currently found  L L L italic_L -closest neighbors (Lines 1-2). Then, it iteratively extracts the closest but unexpanded neighbor  u u u italic_u  from  p  o  o  l p o o l pool italic_p italic_o italic_o italic_l  (Line 4) and expands  u u u italic_u  to refine  p  o  o  l p o o l pool italic_p italic_o italic_o italic_l , until the termination condition is satisfied (Line 3).  In each iteration, expanding  u u u italic_u  for  q q q italic_q  is shown in Lines 5-7, where each neighbor  v  N G  ( u ) v subscript N G u v\\in N_{G}(u) italic_v  italic_N start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_u )  is treated as a  k k k italic_k -ANN candidate of  q q q italic_q  (Line 5) and further verified by an expensive distance computation (Line 6) to refine  p  o  o  l p o o l pool italic_p italic_o italic_o italic_l  (Line 7). At the end of each iteration (Line 8), the algorithm finds the closest but unexpanded vertex in  p  o  o  l p o o l pool italic_p italic_o italic_o italic_l  as the next one to be expanded. It terminates when the first  L L L italic_L  vertices in  p  o  o  l p o o l pool italic_p italic_o italic_o italic_l  have been expanded (Line 3).",
            "In a nutshell, as shown in Figure  1(a) , the construction of NSG involves three phases:  initialization initialization \\mathsf{initialization} sansserif_initialization ,  search search \\mathsf{search} sansserif_search  and  refinement refinement \\mathsf{refinement} sansserif_refinement . The  initialization initialization \\mathsf{initialization} sansserif_initialization  phase focuses on constructing an approximate KNNG, while  search search \\mathsf{search} sansserif_search  phase aims to improve the quality of the KNNG, i.e., enhancing the accuracy of neighbors ( k k k italic_k -ANN) for each node in the graph. Finally, the  refinement refinement \\mathsf{refinement} sansserif_refinement  phase incorporates  prune prune \\mathsf{prune} sansserif_prune  and  connect connect \\mathsf{connect} sansserif_connect  operations to reduce the node out-degree and enhance the graph connectivity respectively.  We present the details of NSG construction process in Algorithm  2 .",
            "Search Phase:   As in Lines 3-4 of Algorithm  1 , for each  u  D u D u\\in D italic_u  italic_D , the  search search \\mathsf{search} sansserif_search  phase performs  k k k italic_k -ANN search on  G k 0 subscript G subscript k 0 G_{k_{0}} italic_G start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT  in order to obtain the candidate set  C  ( u ) C u C(u) italic_C ( italic_u )  for  refinement refinement \\mathsf{refinement} sansserif_refinement . Notably, each  k k k italic_k -ANN search starts from the entering point  e  p e p ep italic_e italic_p  which is the closest point in  D D D italic_D  to the centroid of  D D D italic_D  (Line 2).",
            "As illustrated in Figure  1(b) , the construction of HNSW involves two phases for each inserted point:  search search \\mathsf{search} sansserif_search  and  refinement refinement \\mathsf{refinement} sansserif_refinement . The details are presented in Algorithm  4 .  HNSW begins by initializing the graph with a single point (Lines 1-2). For each remaining point, HNSW randomly determines its highest layer  l l l italic_l  using an exponentially decaying probability distribution (Line 4). Like NSG, the  search search \\mathsf{search} sansserif_search  phase focuses on finding a candidate neighbor set for each node  u u u italic_u . It starts the search from the top layer down to layer  l + 1 l 1 l+1 italic_l + 1  via greedy routing (Lines 8-9) and performs  k k k italic_k -ANN search on each lower layer to obtain the candidate neighbor set for  refinement refinement \\mathsf{refinement} sansserif_refinement  (Lines 10-12).  Next, for each lower layer (from  l l l italic_l  to  0 0 ), HNSW applies the RNG pruning strategy (Algorithm  3 ) to prune the neighbors obtained from the  search search \\mathsf{search} sansserif_search  phase. Like NSG, HNSW adds undirectional edges between the inserted node and its selected neighbors to enhance connectivity, while limiting the out-degree of each node to a specific number  M M M italic_M  (Lines 16-17). However, there is no  connect connect \\mathsf{connect} sansserif_connect  operation in the  refinement refinement \\mathsf{refinement} sansserif_refinement  phase of HNSW.",
            "In this section, we focus on addressing the issue identified in the last section regarding the RNG construction. At a high level, we propose replacing the  search-before-refinement  scheme (Figure  1(a) ) with a  refinement-before-search  scheme (Figure  1(c) ) in RNG construction.  To enhance the efficiency of acquiring high-quality  k k k italic_k -CNA results, we introduce a novel pruning strategy,    \\alpha italic_ -pruning for neighbor selection in  refinement refinement \\mathsf{refinement} sansserif_refinement  (Section  4.1 ). Then, we theoretically analyze our proposed scheme to demonstrate its efficacy (Section  4.2 ).",
            "As shown in Figure  6 ,  w w w italic_w  and  v v v italic_v  are out-neighbors of  u u u italic_u  in KNNG, and the RNG pruning strategy applied leads to the pruning of edge  ( u , v ) u v (u,v) ( italic_u , italic_v )  by  w w w italic_w . Consider a scenario where there exists a query point  q q q italic_q  such that  d  i  s  t  ( u , q ) < d  i  s  t  ( v , q ) < d  i  s  t  ( w , q ) d i s t u q d i s t v q d i s t w q dist(u,q)<dist(v,q)<dist(w,q) italic_d italic_i italic_s italic_t ( italic_u , italic_q ) < italic_d italic_i italic_s italic_t ( italic_v , italic_q ) < italic_d italic_i italic_s italic_t ( italic_w , italic_q ) .  During beam search on KNNG,  v v v italic_v  is found when  u u u italic_u  is included in  p  o  o  l p o o l pool italic_p italic_o italic_o italic_l  as defined in Algorithm  1 . However, after pruning,  v v v italic_v  may no longer be found even if  u u u italic_u  is included in  p  o  o  l p o o l pool italic_p italic_o italic_o italic_l . This is because  w w w italic_w  might not be successfully inserted in  p  o  o  l p o o l pool italic_p italic_o italic_o italic_l  due to its longer distance.  Thus, in such cases, the  k k k italic_k -CNA quality on NSG is inferior to that on KNNG.",
            "According to Theorem  4.3 , a node with a higher rank appearing in the search path on  G ^ ^ G \\hat{G} over^ start_ARG italic_G end_ARG  indicates that we miss a closer neighbor in the  k k k italic_k -CNA results, i.e.,  k k k italic_k -CNA quality loss compared with the search on  G G G italic_G .  Such a loss is caused by the    \\alpha italic_ -pruning operations on  G G G italic_G  for the sake of  k k k italic_k -CNA efficiency.  Being a widely-used pruning strategy, we have demonstrated in Example  4.1  that RNG pruning results in higher ranks in the path, leading to quality loss. Next, our focus shifts to analyzing our newly proposed    \\alpha italic_ -pruning strategy.",
            "In this section, we provide comprehensive details of our new construction framework for RNG and NSWG.  Combining the approaches outlined in the previous section, we begin by presenting an optimized  k k k italic_k -CNA approach in Section  5.1 , and then we present our new construction methods for RNG on top of our optimized  k k k italic_k -CNA approach, as in Section  5.2 .  Further, we enhance the NSWG construction by combining a layer-by-layer insertion strategy with the RNG construction framework in Section  5.3 .  In Section  5.4 , optimization techniques are introduced to enhance the efficiency of RNG construction framework.  Lastly, we consolidate all the methods discussed and present a streamlined and effective framework that can be applied to the construction of other PG methods, as in Section  5.5 .",
            "P2K : from    \\alpha italic_ -PG to KNNG.   This process entails answering a  k k k italic_k -ANN query on the current    \\alpha italic_ -PG for each  u  D u D u\\in D italic_u  italic_D . The optimization efforts primarily avoid repeated distance computations. It is evident that multiple iterations of  P2K  involve repeated distance computations, as each point  u u u italic_u  is inquired multiple times, leading to redundant verification of its similar points. This happens during node expansions, as depicted in Lines 5-6 of Algorithm  1 , when conducting  k k k italic_k -ANN search for node  u u u italic_u .",
            "The pipeline of our framework is summarized in Figure  1(c) . In the beginning, we obtain a KNNG through the  initialization initialization \\mathsf{initialization} sansserif_initialization  phase and then enter an iterative loop to continuously enhance the  k k k italic_k -CNA quality until it satisfies the quality examination or achieves the required number of iterations. During each iteration, we first obtain an intermediate graph index from the current  k k k italic_k -CNA results via a new pruning strategy for neighbor selection in the  refinement refinement \\mathsf{refinement} sansserif_refinement  phase. Subsequently, we perform a beam search for each node in the  search search \\mathsf{search} sansserif_search  phase to enhance the  k k k italic_k -CNA quality. The optimization techniques, i.e.,  K2P K2P \\mathsf{K2P} sansserif_K2P  and  P2K P2K \\mathsf{P2K} sansserif_P2K , further accelerate the  refinement refinement \\mathsf{refinement} sansserif_refinement  and  search search \\mathsf{search} sansserif_search  phases respectively.",
            "The comparison of search performance and building time are presented in Figures  10  and  12 , respectively. First, we can find that FastNSG exhibits comparable search performance to OriNSG in Figure  10 , while significantly reducing construction costs as shown in Figure  12 . Specifically, FastNSG achieves speedups of 1.2x, 5.1x, 5.6x, 3.2x, 2.3x and 1.2x over OriNSG on the Sift1M, Gist1M, Msong, Crawl, Glove and Deep1M datasets, respectively. Second, FastHNSW demonstrates significantly improved search performance compared to OriHNSW, primarily due to obtaining more accurate  k k k italic_k -CNA results. Note that FastHNSW finds candidates for each point on the entire dataset in each layer, while OriHNSW only on a subset (the existing nodes in the graph during insertion).  Compared with OriHNSW, FastHNSW accelerates construction by 2.4x, 4.6x, 3.0x, 3.7x, 1.6x and 2.1x speedups on the six datasets, respectively. In summary, our methods excel in construction efficiency while achieving comparable or even superior search performance.",
            "Exp.4: comparisons between existing approaches with ours.   In this part, we compare our methods FastNSG and FastHNSW with other four recently proposed SOTA PG methods which focus on the index construction, i.e., DiskANN  (Subramanya et al . ,  2019 ) , LSH-APG  (Zhao et al . ,  2023 ) , RNN-Descent  (Ono and Matsui,  2023 )  and ParlayANN  (Manohar et al . ,  2024 ) . We show the comparisons of search performance in Figure  11  and that of building cost in Figure  12 .  Overall, the results show that our methods achieve much less construction cost while obtaining better search performance.",
            "Exp.5: extension of our framework on other SOTA PG approaches.   In this part, we apply our framework to other two SOTA PG methods, i.e.,    \\tau italic_ -MNG (another SOTA RNG method) and NSW (another SOTA NSWG method). As shown in Figure  13 , our method Fast   \\tau italic_ -MNG achieves comparable or even better search performance compared with Ori   \\tau italic_ -MNG, while FastNSW significantly achieves better search performance than OriNSW. Moreover, as in Figure  15 , Fast   \\tau italic_ -MNG achieves construction speedups of 1.2x, 16.4x, 5.7x, 3.3x, 2.3x, 3.1x and 2.1x over Ori   \\tau italic_ -MNG on the six datasets respectively, while FastNSW obtains speedups of 4.6x, 3,1x, 3,6x, 1,6x, 2,1x and 2.1x respectively. Overall, our framework could be successfully applied to other SOTA PG methods, with superior construction efficiency and comparable search performance.",
            "Exp.6: effects of optimization techniques.   In this part, we present the effects of our optimization techniques for  FastNSG , i.e., reducing the repeated distance computations of  P2K  in consecutive iterations (referred to as  Opt1 ) and reducing the repeated distance and angle computations of  K2P  in consecutive iterations (referred to as  Opt2 ). Notably, these techniques do not affect the final graph index, and thus our focus is solely on the construction cost. We evaluate four methods: FN0 without any optimization, FN1 with only  Opt1 , FN2 with only  Opt2  and FN with both optimizations. The results are shown in Figure  15 , where the time cost of each method is represented as the speedup over FN0. Overall, each optimization technique leads to a significant acceleration of  FastNSG . Besides, all optimizations are mutually independent, and thus FN shows the lowest construction cost. Moreover, the speedups achieved by FN show a noticeable increase in construction speedup as the data dimensions rise. This is because both optimizations effectively reduce the burden associated with repeated distance and angle computations.",
            "Exp.7: scalability of our proposed methods.   In this section, we assess the scalability of our methods using large data from Sift50M. As depicted in Figure  17 , our Fast* approach consistently accelerates over the original Ori* methods as dataset sizes grow. Notably, the speedup in FastNSG further amplifies with expanding dataset sizes, underscoring the exceptional scalability of our approach. This demonstrates our RNG construction framework scales well as the data size rises.",
            "Exp.8: KNNG recall estimation via random sampling.   In this experiment, we study the impact of the parameter    \\varepsilon italic_  on KNNG recall estimation (refer to Theorem  5.1 ). The results on the Sift1M dataset are shown in Figure  17 , where the dotted line represents the exact value of recall@500 (the average recall of each node in KNNG ).  We find a reduction in running time as the value of    \\varepsilon italic_  increases. While the fluctuation range (error) of recall estimation expands with higher    \\varepsilon italic_  values, setting    \\varepsilon italic_  to  0.6 0.6 0.6 0.6  provides a reasonable estimate of recall swiftly, enabling a quick evaluation of KNNG quality."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.  Summary of notations",
        "table": "S2.T2.26.26",
        "footnotes": [],
        "references": [
            "The paper is organized as follows. We provide the preliminaries in Section  2 . In Section  3 , we revisit the existing PG methods and identify their construction issues. In Section  4 , we propose the refinement-before-search scheme as the basis of our construction framework for RNG and NSWG as shown in Section  5 .  We present our experimental studies in Section  6 .  Furthermore, we discuss the related works in Section  7  and conclude our work in Section  8 .",
            "k k k italic_k -ANN Search:   Let  D  R d D superscript R d D\\subset\\mathbb{R}^{d} italic_D  blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT  be a high-dimensional dataset consisting of  n n n italic_n   d d d italic_d -dimensional points. We denote the L2 norm (i.e., Euclidean distance) between two points  u , v  R d u v superscript R d u,v\\in\\mathbb{R}^{d} italic_u , italic_v  blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT  as  d  i  s  t  ( u , v ) d i s t u v dist(u,v) italic_d italic_i italic_s italic_t ( italic_u , italic_v ) .  Given a dataset  D D D italic_D  and a query point  q  R d q superscript R d q\\in\\mathbb{R}^{d} italic_q  blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ,  the  k k k italic_k -approximate nearest neighbor ( k k k italic_k -ANN)  search aims to find the top- k k k italic_k  points in  D D D italic_D  with the minimum distances from the query  q q q italic_q . This paper focuses on the in-memory solutions, which assume that  D D D italic_D  and the corresponding index can be hosted in the memory  (Malkov et al . ,  2014 ; Malkov and Yashunin,  2018 ; Fu et al . ,  2019 ; Li et al . ,  2019 ; Fu et al . ,  2022 ; Peng et al . ,  2023 ) .  Table  2  summarizes the notations.",
            "In a nutshell, as shown in Figure  1(a) , the construction of NSG involves three phases:  initialization initialization \\mathsf{initialization} sansserif_initialization ,  search search \\mathsf{search} sansserif_search  and  refinement refinement \\mathsf{refinement} sansserif_refinement . The  initialization initialization \\mathsf{initialization} sansserif_initialization  phase focuses on constructing an approximate KNNG, while  search search \\mathsf{search} sansserif_search  phase aims to improve the quality of the KNNG, i.e., enhancing the accuracy of neighbors ( k k k italic_k -ANN) for each node in the graph. Finally, the  refinement refinement \\mathsf{refinement} sansserif_refinement  phase incorporates  prune prune \\mathsf{prune} sansserif_prune  and  connect connect \\mathsf{connect} sansserif_connect  operations to reduce the node out-degree and enhance the graph connectivity respectively.  We present the details of NSG construction process in Algorithm  2 .",
            "Initialization Phase:   An initial KNNG  G k 0 subscript G subscript k 0 G_{k_{0}} italic_G start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT  where each node has  k 0 subscript k 0 k_{0} italic_k start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  neighbors is built in  initialization initialization \\mathsf{initialization} sansserif_initialization  phase by the SOTA method called KGraph  (Dong et al . ,  2011 ) , as shown in Line 1 of Algorithm  2 . The purpose of this phase is to build an index for  search search \\mathsf{search} sansserif_search  phase.",
            "The Importance of  k k k italic_k -CNA Quality:   For the PG derived from  k k k italic_k -CNA results by  refinement refinement \\mathsf{refinement} sansserif_refinement  phase, there are two aspects of its search performance, i.e., efficiency by queries per second (QPS) and accuracy by  R  e  c  a  l  l  @  10 R e c a l l @ 10 Recall@10 italic_R italic_e italic_c italic_a italic_l italic_l @ 10  of returned  k k k italic_k -ANN. As depicted in Figure  2 , each curve represents the search performance of a derived PG with a distinct recall of  k k k italic_k -CNA results.  The results clearly indicate that the  k k k italic_k -CNA quality significantly impacts the search performance of the derived PG. Specifically, the recall of  k k k italic_k -CNA results exhibits a positive effect on the search performance of PG. Such an effect could also observed on other PGs such as DPG and    \\tau italic_ -MNG. Notably, the  k k k italic_k -CNA quality of HNSW is limited to 0.5 due to its incremental node-by-node insertion strategy of index construction.",
            "NSWG Construction Issue:    NSWG methods suffer from a poor  k k k italic_k -CNA quality in construction , caused by its building strategy of incremental node-by-node insertions: utilizes the current graph index with only a part of nodes to conduct  k k k italic_k -ANN search for  k k k italic_k -CNA results. Hence, the expected value of  k k k italic_k -CNA quality in NSWG is only  0.5 0.5 0.5 0.5  even if all the  k k k italic_k -ANN queries are answered correctly.  Since achieving exact correctness in  k k k italic_k -ANN queries is not feasible, the average recall in practice is upper bounded by  0.5 0.5 0.5 0.5 , as demonstrated in Figure  2(b) .",
            "In this section, we focus on addressing the issue identified in the last section regarding the RNG construction. At a high level, we propose replacing the  search-before-refinement  scheme (Figure  1(a) ) with a  refinement-before-search  scheme (Figure  1(c) ) in RNG construction.  To enhance the efficiency of acquiring high-quality  k k k italic_k -CNA results, we introduce a novel pruning strategy,    \\alpha italic_ -pruning for neighbor selection in  refinement refinement \\mathsf{refinement} sansserif_refinement  (Section  4.1 ). Then, we theoretically analyze our proposed scheme to demonstrate its efficacy (Section  4.2 ).",
            "In this section, we provide comprehensive details of our new construction framework for RNG and NSWG.  Combining the approaches outlined in the previous section, we begin by presenting an optimized  k k k italic_k -CNA approach in Section  5.1 , and then we present our new construction methods for RNG on top of our optimized  k k k italic_k -CNA approach, as in Section  5.2 .  Further, we enhance the NSWG construction by combining a layer-by-layer insertion strategy with the RNG construction framework in Section  5.3 .  In Section  5.4 , optimization techniques are introduced to enhance the efficiency of RNG construction framework.  Lastly, we consolidate all the methods discussed and present a streamlined and effective framework that can be applied to the construction of other PG methods, as in Section  5.5 .",
            "Compared with the RNG construction, our method can be seen as a reversal of  search search \\mathsf{search} sansserif_search  and  refinement refinement \\mathsf{refinement} sansserif_refinement .  However, OptKCNA is more efficient due to the smaller node out-degrees and keeps the  k k k italic_k -CNA quality with a high probability as our analysis in Section  4.2 .",
            "Our global construction of HNSW is built on top of a layer-by-layer insertion strategy in a top-down manner.  To be specific, we first determine the layers for each node before the actual layer insertion and thus we obtain the whole set of nodes in each layer. Afterward, we employ our RNG construction framework in Section  5.2  to build a graph index for each layer. In this way, each layer of HNSW is built on top of  k k k italic_k -CNA results w.r.t the whole set of nodes in each layer instead of only a subset. Hence, the  k k k italic_k -CNA quality of each layer will significantly exceed 0.5 (i.e., the upper bound in the original HNSW). Note that each layer of HNSW is actually an RNG index, since it takes the RNG pruning strategy to prune the  k k k italic_k -CNA results only without the connectivity enhancement via DFS. Subsequently, we insert the RNG index of the layer into HNSW according to the layer-by-layer insertion strategy.",
            "The comparison of search performance and building time are presented in Figures  10  and  12 , respectively. First, we can find that FastNSG exhibits comparable search performance to OriNSG in Figure  10 , while significantly reducing construction costs as shown in Figure  12 . Specifically, FastNSG achieves speedups of 1.2x, 5.1x, 5.6x, 3.2x, 2.3x and 1.2x over OriNSG on the Sift1M, Gist1M, Msong, Crawl, Glove and Deep1M datasets, respectively. Second, FastHNSW demonstrates significantly improved search performance compared to OriHNSW, primarily due to obtaining more accurate  k k k italic_k -CNA results. Note that FastHNSW finds candidates for each point on the entire dataset in each layer, while OriHNSW only on a subset (the existing nodes in the graph during insertion).  Compared with OriHNSW, FastHNSW accelerates construction by 2.4x, 4.6x, 3.0x, 3.7x, 1.6x and 2.1x speedups on the six datasets, respectively. In summary, our methods excel in construction efficiency while achieving comparable or even superior search performance.",
            "Exp.4: comparisons between existing approaches with ours.   In this part, we compare our methods FastNSG and FastHNSW with other four recently proposed SOTA PG methods which focus on the index construction, i.e., DiskANN  (Subramanya et al . ,  2019 ) , LSH-APG  (Zhao et al . ,  2023 ) , RNN-Descent  (Ono and Matsui,  2023 )  and ParlayANN  (Manohar et al . ,  2024 ) . We show the comparisons of search performance in Figure  11  and that of building cost in Figure  12 .  Overall, the results show that our methods achieve much less construction cost while obtaining better search performance."
        ]
    },
    "id_table_3": {
        "caption": "Table 3.  Data statistics",
        "table": "S6.T3.1",
        "footnotes": [],
        "references": [
            "The paper is organized as follows. We provide the preliminaries in Section  2 . In Section  3 , we revisit the existing PG methods and identify their construction issues. In Section  4 , we propose the refinement-before-search scheme as the basis of our construction framework for RNG and NSWG as shown in Section  5 .  We present our experimental studies in Section  6 .  Furthermore, we discuss the related works in Section  7  and conclude our work in Section  8 .",
            "Refinement Phase:   This phase removes redundant neighbors in the set  C  ( u ) C u C(u) italic_C ( italic_u )  via a pruning strategy (Algorithm  3 ) to obtain  N G  ( u ) subscript N G u N_{G}(u) italic_N start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_u )  with the constraint  | N G  ( u ) |  M subscript N G u M |N_{G}(u)|\\leq M | italic_N start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_u ) |  italic_M , where  M M M italic_M  is a specific threshold (Lines 5-6). To improve the graph connectivity, unidirectional edges are added between  u u u italic_u  and each  v  N G  ( u ) v subscript N G u v\\in N_{G}(u) italic_v  italic_N start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_u ) , which might trigger an extra pruning process in order to limit the out-degree of  u u u italic_u  (Lines 7-8).  Finally, a depth-first search (DFS) is employed to identify any remaining connected components in  G G G italic_G , and additional edges are then added to connect them together (Lines 10-11).",
            "The widely used pruning process focuses on eliminating the longest edge within each possible triangle formed by the points in the dataset.  For simplicity, we call this strategy as  RNG pruning .  Specifically, if edge  ( u , v ) u v (u,v) ( italic_u , italic_v )  exists in the NSG only if  v v v italic_v  is not dominated by any neighbor  w w w italic_w  of  u u u italic_u , i.e., there is no edge  ( u , w ) u w (u,w) ( italic_u , italic_w )  such that  d  i  s  t  ( u , w ) < d  i  s  t  ( u , v ) d i s t u w d i s t u v dist(u,w)<dist(u,v) italic_d italic_i italic_s italic_t ( italic_u , italic_w ) < italic_d italic_i italic_s italic_t ( italic_u , italic_v )  and  d  i  s  t  ( v , w ) < d  i  s  t  ( u , v ) d i s t v w d i s t u v dist(v,w)<dist(u,v) italic_d italic_i italic_s italic_t ( italic_v , italic_w ) < italic_d italic_i italic_s italic_t ( italic_u , italic_v ) . In the practical version of NSG, this pruning process for each node  u u u italic_u  is modified in two aspects, i.e., (1) the out-neighbors of each  u u u italic_u  are only picked from the close neighbor set  C  ( u ) C u C(u) italic_C ( italic_u )  and (2) each node has at most  M M M italic_M  out-neighbors. The first modification improves the construction efficiency, while the second accelerates  k k k italic_k -ANN search in NSG.  The details of are presented in Algorithm  3 . Each candidate neighbor  v  C  ( u ) v C u v\\in C(u) italic_v  italic_C ( italic_u )  is checked individually, in ascending order of  d  i  s  t  ( u , v ) d i s t u v dist(u,v) italic_d italic_i italic_s italic_t ( italic_u , italic_v )  (Line 2).  v v v italic_v  is selected as a neighbor of  u u u italic_u  only if it is not dominated by any existing neighbors (Lines 3-8), and the process terminates once  M M M italic_M  neighbors have been selected (Line 9).",
            "As to other RNG methods such as DPG  (Li et al . ,  2019 )  and    \\tau italic_ -MNG  (Peng et al . ,  2023 ) , we can build them by only replacing the RNG pruning strategy (Algorithm  3 ) with their own ones. Hence, the mainstream RNG methods follow the same framework of index construction.",
            "As illustrated in Figure  1(b) , the construction of HNSW involves two phases for each inserted point:  search search \\mathsf{search} sansserif_search  and  refinement refinement \\mathsf{refinement} sansserif_refinement . The details are presented in Algorithm  4 .  HNSW begins by initializing the graph with a single point (Lines 1-2). For each remaining point, HNSW randomly determines its highest layer  l l l italic_l  using an exponentially decaying probability distribution (Line 4). Like NSG, the  search search \\mathsf{search} sansserif_search  phase focuses on finding a candidate neighbor set for each node  u u u italic_u . It starts the search from the top layer down to layer  l + 1 l 1 l+1 italic_l + 1  via greedy routing (Lines 8-9) and performs  k k k italic_k -ANN search on each lower layer to obtain the candidate neighbor set for  refinement refinement \\mathsf{refinement} sansserif_refinement  (Lines 10-12).  Next, for each lower layer (from  l l l italic_l  to  0 0 ), HNSW applies the RNG pruning strategy (Algorithm  3 ) to prune the neighbors obtained from the  search search \\mathsf{search} sansserif_search  phase. Like NSG, HNSW adds undirectional edges between the inserted node and its selected neighbors to enhance connectivity, while limiting the out-degree of each node to a specific number  M M M italic_M  (Lines 16-17). However, there is no  connect connect \\mathsf{connect} sansserif_connect  operation in the  refinement refinement \\mathsf{refinement} sansserif_refinement  phase of HNSW.",
            "RNG Construction Issue:    RNG methods suffer from inefficiency in obtaining  k k k italic_k -CNA results , caused by  search search \\mathsf{search} sansserif_search  phase that takes the initial KNNG generated by  initialization initialization \\mathsf{initialization} sansserif_initialization  phase as the graph index for accuracy improvement.  As demonstrated in previous experimental studies  (Li et al . ,  2019 ; Wang et al . ,  2021a ) , with similar (e.g. tens of ) average out-degrees, KNNG is more prone to local optima than RNG and NSWG due to directional edges and weak connectivity. To address this, RNG equips the initial KNNG with a pretty large (e.g. hundreds of) out-degree, which enhances the  k k k italic_k -CNA quality but leads to an inefficiency issue. As depicted in Figure  3(a) , the search on KNNG to enhance the  k k k italic_k -CNA quality constitutes a significant portion of the overall cost of building the representative RNG method (i.e., NSG), which even surpasses that taken for  initialization initialization \\mathsf{initialization} sansserif_initialization  on the  Gist1M Gist1M \\mathsf{Gist1M} sansserif_Gist1M  dataset. A similar phenomenon could be found in other RNG methods such as    \\tau italic_ -MNG.",
            "RNG Pruning Issue:   In Section  3 , we discuss the RNG pruning strategy used in the  refinement refinement \\mathsf{refinement} sansserif_refinement  as shown in Figure  6 , the edge  ( u , v ) u v (u,v) ( italic_u , italic_v )  will be pruned if there exists edge  ( u , w ) u w (u,w) ( italic_u , italic_w )  such that  d  i  s  t  ( u , w ) < d  i  s  t  ( u , v ) d i s t u w d i s t u v dist(u,w)<dist(u,v) italic_d italic_i italic_s italic_t ( italic_u , italic_w ) < italic_d italic_i italic_s italic_t ( italic_u , italic_v )  and  d  i  s  t  ( v , w ) < d  i  s  t  ( u , v ) d i s t v w d i s t u v dist(v,w)<dist(u,v) italic_d italic_i italic_s italic_t ( italic_v , italic_w ) < italic_d italic_i italic_s italic_t ( italic_u , italic_v ) .  However, as shown in the following example, it is primarily not designed for  k k k italic_k -CNA.",
            "According to Theorem  4.3 , a node with a higher rank appearing in the search path on  G ^ ^ G \\hat{G} over^ start_ARG italic_G end_ARG  indicates that we miss a closer neighbor in the  k k k italic_k -CNA results, i.e.,  k k k italic_k -CNA quality loss compared with the search on  G G G italic_G .  Such a loss is caused by the    \\alpha italic_ -pruning operations on  G G G italic_G  for the sake of  k k k italic_k -CNA efficiency.  Being a widely-used pruning strategy, we have demonstrated in Example  4.1  that RNG pruning results in higher ranks in the path, leading to quality loss. Next, our focus shifts to analyzing our newly proposed    \\alpha italic_ -pruning strategy.",
            "In this section, we provide comprehensive details of our new construction framework for RNG and NSWG.  Combining the approaches outlined in the previous section, we begin by presenting an optimized  k k k italic_k -CNA approach in Section  5.1 , and then we present our new construction methods for RNG on top of our optimized  k k k italic_k -CNA approach, as in Section  5.2 .  Further, we enhance the NSWG construction by combining a layer-by-layer insertion strategy with the RNG construction framework in Section  5.3 .  In Section  5.4 , optimization techniques are introduced to enhance the efficiency of RNG construction framework.  Lastly, we consolidate all the methods discussed and present a streamlined and effective framework that can be applied to the construction of other PG methods, as in Section  5.5 .",
            "Datasets:   We use 6 public datasets with diverse sizes and dimensions. These datasets encompass a wide range of applications, including image ( Sift1M 1 1 1 http://corpus-texmex.irisa.fr/ ,  Deep1M Deep1M \\mathsf{Deep1M} sansserif_Deep1M   2 2 2 https://disk.yandex.ru/d/11eDCm7Dsn9GA/  and  Gist1M 1 ), audio ( Msong 3 3 3 http://www.ifs.tuwien.ac.at/mir/msd/ ) and text( Crawl 4 4 4 https://commoncrawl.org/  and  Glove Glove \\mathsf{Glove} sansserif_Glove   5 5 5 https://nlp.stanford.edu/projects/glove/ ).  The statistics of those data sets are summarised in Table  3 , where  #queries  denotes the number of queries and  dim.  denotes the dimensions of datasets.  The query workloads of the datasets are given in the datasets.  Besides, we take several random samples of distinct sizes from Sift50M 1  dataset to test the scalability of our methods.",
            "Exp.5: extension of our framework on other SOTA PG approaches.   In this part, we apply our framework to other two SOTA PG methods, i.e.,    \\tau italic_ -MNG (another SOTA RNG method) and NSW (another SOTA NSWG method). As shown in Figure  13 , our method Fast   \\tau italic_ -MNG achieves comparable or even better search performance compared with Ori   \\tau italic_ -MNG, while FastNSW significantly achieves better search performance than OriNSW. Moreover, as in Figure  15 , Fast   \\tau italic_ -MNG achieves construction speedups of 1.2x, 16.4x, 5.7x, 3.3x, 2.3x, 3.1x and 2.1x over Ori   \\tau italic_ -MNG on the six datasets respectively, while FastNSW obtains speedups of 4.6x, 3,1x, 3,6x, 1,6x, 2,1x and 2.1x respectively. Overall, our framework could be successfully applied to other SOTA PG methods, with superior construction efficiency and comparable search performance."
        ]
    },
    "global_footnotes": [
        "Shuo Yang and Jiadong Xie are the joint first authors.",
        "Yingfan Liu is the corresponding author.",
        "This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit",
        "to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing",
        ". Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment.",
        "Proceedings of the VLDB Endowment, Vol. 18, No. 1 ISSN 2150-8097.",
        ""
    ]
}