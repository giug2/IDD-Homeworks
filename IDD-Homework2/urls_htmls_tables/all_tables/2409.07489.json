{
    "id_table_1": {
        "caption": "Table 1:  Comparison of RAGent to the existing automated policy generation frameworks. S - Subject, A - Action, R - Resource, P - Purpose, C - Condition",
        "table": "S2.T1.1",
        "footnotes": [],
        "references": [
            "We show that incorporating organization-specific information to generate access control policies through Retrieval Augmented Generation (RAG)  [ 39 ]  helps improve the reliability of the access control policy generation process (Section  3  and Appendix  A.4.1 ).  Furthermore, it enables the reliable adaptation of RAGent to any new domains without costly training of ML models.",
            "This paper addresses those research gaps by introducing RAGent, a novel retrieval-based access control policy generation framework, leveraging transformer-based LMs and their impressive language understanding capabilities  [ 74 ] .  As shown in Table  1 , RAGent stands out from existing policy generation frameworks in several key ways.  Firstly, RAGent accurately generates access control policies from high-level requirement specifications.  It adeptly manages complex access requirements, including those with intricate components like purposes and conditions, and also with ACRs containing different access decisions.  Furthermore, RAGent performs RAG incorporating organization-specific information like users and resources in policy generation to produce reliable policies that match the organizations authorization system (i.e., domain adaptation).  Additionally, it goes a step further by automatically verifying the generated policies and offering feedback in case of errors.  This feedback is first used to iteratively refine the generated policy automatically and also manually by the system administrator if the automatic refinement fails.",
            "The main objective of RAGent, is to improve the reliability of the access control policy generation process.  It utilizes transformer-based LMs to generate access control policies in six steps, as shown in Figure  1 .",
            "According to Figure  1 , the input to RAGent is the organizations high-level requirement specification document (i.e., input document) written by a security expert in NL.  In this research, we assume that the input document does not contain any requirement conflicts, similar to previous research  [ 50 ,  80 ,  52 ,  70 ] .  First, RAGent pre-processes the sentences of the input document.  Then, RAGent classifies the pre-processed sentences to identify NLACPs in the second step.  Once an NLACP is identified, in the third step, RAGent retrieves organization-specific information relevant to translating the identified NLACP into an access control policy.  This information will be combined with the NLACP and fed to the policy generation module in the fourth step to generate its access control policy containing ACRs with five policy components (i.e., subjects, actions, resources, purposes, and conditions) and their rule decisions (i.e., allow or deny) through RAG.  RAGent generates the access control policy as a structured representation of the NLACP  [ 16 ] .  By translating the NLACP into a structured representation rather than utilizing a standard access control language, RAGent becomes universally applicable.  It enables any organization, regardless of the language utilized in their authorization system, to use RAGent, and seamlessly transform the generated representation into their specific access control language.  Then, RAGent post-processes the generated policy to ensure that it only contains policy components that align with the authorization system.",
            "After generating the access control policy, it is verified in the fifth step using a novel policy verification technique to decide whether or not it is correct.  If the generated access control policy is verified as correct, it can be applied to the authorization system.  On the other hand, if the policy is verified as incorrect, the verifier provides the reason (i.e., error type) for it being incorrect as feedback.  In that case, as shown in Figure  1 , the feedback is used to address the identified error(s) iteratively and generate the correct refined policy within  n n n italic_n  iterations.  If the policy is still incorrect even after  n n n italic_n  rounds, it will be sent to the administrator with feedback  for the manual refinement.  This helps administrators easily find erroneous ACRs and refine them before adding them to the authorization system.",
            "However, we cannot use BERT directly to classify sentences as NLACPs, as it is pre-trained to predict the masked token of the input sentence (i.e., Masked Language Modeling) and to predict the next sentence (i.e., Next Sentence Prediction)  [ 20 ] .  Therefore, we improve the BERT LM by adding a feed-forward network (FFN) containing a single linear layer on top of the final hidden state of BERT corresponding to the  [ C  L  S ] delimited-[] C L S [CLS] [ italic_C italic_L italic_S ]  token that provides an aggregate representation of the input sequence  [ 20 ] .  Then, we fine-tune BERT with the access control policy dataset (Section  4.1 ) containing NLACPs and non-NLACPs, minimizing the cross-entropy loss  [ 20 ] .  It allows BERT to learn how to identify NLACPs from non-NLACPs accurately  [ 80 ,  27 ] .",
            "Therefore, to facilitate dense retrieval, first, RAGent generates embedding vectors for subjects, actions, resources, purposes, and conditions of the organization.  To this end, it utilizes an open-source lightweight embedding model  [ 37 ]  that shows superior performance in retrieval according to the Huggingface MTEB leaderboard  [ 47 ]  at the time of writing.  The embedding vectors are then stored in separate specialized databases (one for each information type), called vector databases  [ 26 ]  as shown in Figure  1 .  It helps optimize the retrieval of the stored information through algorithms like Product Quantization  [ 65 ] .  After preparing the vector databases, RAGent generates the embedding vector for the identified NLACP using the same embedding model as shown in Figure  1 .  Then, it compares the NLACPs embedding vector with embedding vectors of the information from vector databases using cosine similarity  [ 24 ,  47 ]  to retrieve the most similar  k k k italic_k  entities from each database  [ 21 ] .  These  5  k 5 k 5k 5 italic_k  entities are used in Step 4 to help generate the access control policy for an NLACP.",
            "After identifying the NLACPs of the high-level requirement specification documents (Step 2) with the relevant information (Step 3), RAGent translates them into access control policies through  structured information extraction   [ 16 ] , as shown in Figure  1 .  In other words, RAGent extracts information from an NLACP according to a structure/hierarchy (e.g., JSON) delineating an access control policy.  It represents underlying ACRs of the NLACP, access decisions, and policy components (i.e., subject, actions, resource, purpose, and condition) of each rule  [ 72 ,  49 ] .  For example, consider an NLACP,  The doctor can write prescriptions, but the nurse cannot. .  Given that NLACP, the RAGent generates the access control policy according to a structure as shown in Figure  2 , maintaining the relationships between policy components within each ACR  [ 16 ]  (We removed the empty purpose and condition fields of each rule for clarity).  This can easily be transformed into any standard access control language, such as XACML  [ 11 ] , when applying it to the authorization system.",
            "In this research, we use the  instruct  version of LLaMa 3 8B, which was created by fine-tuning the LLaMa 3 8B model using the technique called Reinforcement Learning from Human Feedback (RLHF)  [ 58 ]  to follow general user instructions  [ 2 ] .  We further fine-tune it using the access control policy dataset containing NLACPs annotated according to their ACRs (Section  4.1 ) through Parameter Efficient Fine Tuning (PEFT) with low-rank adapters (LoRA)  [ 31 ] .  While this helps the LLM to be specifically adapted to the access control policy generation domain, it also helps overcome catastrophic forgetting of LM fine-tuning as it only updates a small number of extra parameters of the LM  [ 31 ] .  In the fine-tuning process, we minimize the negative log-likelihood of the generated access control policy  [ 66 ] .  Once the LLM is fine-tuned, RAGent uses it to generate access control policies from NLACPs.",
            "As we described, we fine-tune LMs utilized in RAGent using domain-related datasets to improve the reliability of the process  [ 52 ] .  This section presents those datasets, namely the  access control policy dataset  (Section  4.1 ) used to train and evaluate the NLACP identification and policy generation modules,  access control policy verification dataset  (Section  4.2 ) used to train and evaluate the verifier, and  access control policy refinement dataset  (Section  4.3 ) used to train the policy generation module to refine incorrectly generated policies.",
            "Therefore, to begin with,  we cleaned the datasets (e.g., deduplication) and updated their annotations with the access decisions, purposes, and conditions according to the previous research  [ 84 ,  11 ] .  Then, we utilized two popular synthetic data generation techniques to generate more diverse access control policies in two steps to better adapt LMs for access control policy generation while minimizing the data scarcity of the domain  [ 52 ] .  They are (1) data augmentation  [ 80 ]  and (2) LLM-based synthetic data generation  [ 63 ,  43 ] .  More information about the synthetic data generation and annotation can be found in Appendix  A.1 .",
            "The statistics of the resultant dataset in terms of the number of non-NLACP and NLACP sentences and the number of ACRs are shown in Table  4.1 .  As shown in Table  4.1 ,  Cleaned document-folds  indicate the information about data from five real-world policy documents introduced by Slankas et al.  [ 72 ]  after cleaning and updating annotations.  Following the previous research  [ 80 ,  50 ,  52 ,  72 ] , each fold is specifically used to evaluate our framework, RAGent, after training it with the rest of the data  [ 6 ] .  For example, to evaluate RAGent using data from the dataset CC, the framework is trained using the rest of the folds (i.e., CACP, IBM, T2P, and ACRE), and the synthetic data  [ 6 ] .  As a result, even if the fine-tuning involves synthetic data, RAGent will only be evaluated using real-world access control requirements.  On the other hand, the  Overall  dataset is used to train RAGent and evaluate its overall performance with a diverse set of NLACPs.  It was created by combining all the sentences from document folds as well as the synthetic data and randomly splitting it into train (80%), validation (10%), and test (10%) sets.",
            "The access control policy verification dataset is used to train and evaluate the access control policy verifier discussed in Section  3.5 .  Therefore, the verification dataset should contain sequence pairs representing NLACP and its correct access control policy (i.e., negative sequence pairs) as well as pairs containing NLACP and its incorrect access control policy (i.e., positive sequence pairs).  Since we have the entity annotations for each NLACP from the access control policy dataset (Table  4.1 ), we already have the correct access control policies for the NLACPs to create the negative sequence pairs.  Therefore, to generate incorrect policies for each NLACP, we augment the correct policies by manipulating their properties according to each error type mentioned in Section  3.5 .",
            "The access control refinement dataset is used to teach the access control policy generation module how to refine an incorrect policy identified by the access control policy verification step.  Therefore, each training example of this dataset should contain the NLACP, its incorrectly generated access control policy, the error type of the incorrectly generated policy, and the correct access control policy of the NLACP as the label.  All the mentioned components of a training example can be derived from the previously described access control policy dataset (Section  4.1 ) and the access control policy verification dataset (Section  4.2 ).  For instance, the NLACP and its correct access control policy can be retrieved from the access control policy dataset.  Also, the incorrect access control policy for the same NLACP and the error type can be retrieved from the access control policy verification dataset.  The structure of a training example that combines all the mentioned components is shown in Appendix  A.2 .",
            "Finally, the dataset is combined with the access control policy dataset (Section  4.1 ) and used to fine-tune the access control policy generation module.",
            "After training the components of RAGent, as described in Section  3 , we evaluate their reliability using the F1 score (i.e., the harmonic mean of the precision and recall  [ 41 ] ).  Particularly, we evaluate the RAGents NLACP identification (Section  5.2.1 ) and access control policy generation (with iterative refinement) (Section  5.2.2 ) performance on each document-fold and the test set of the  Overall  dataset (Table  4.1 ).  Moreover, we evaluate the RAGents reliability in access control policy verification (Section  5.2.3 ) using the access control policy verification dataset (Section  4.2 ).",
            "We evaluate the performance of RAGent in identifying NLACPs using both document folds and the  Overall  datasets introduced in Section  4.1 .  The obtained evaluation results compared with the existing access control policy generation frameworks in terms of F1-score are shown in Table  3 . According to Table  3 , RAGent achieves an average document fold F1 score of 87.9%, outperforming the existing frameworks in almost all the document folds.  Moreover, it attains the F1 score of 91.9% on the test set of the  Overall  dataset.",
            "To evaluate the verification performance of the RAGents verifier, we utilized the  Overall  dataset shown in Table  4.1 , as it contains different types of NLACPs from different domains (e.g., healthcare, education, etc.).  After creating the verification dataset based on the  Overall  dataset according to Section  4.2 , we first divided it into train (80%), test (10%) and validation sets (10%) randomly  [ 44 ] .  Then, we train the verifier using the training set, select the best model/checkpoint based on the validation set, and finally, evaluate the verifiers performance in the test set.  The obtained evaluation results are shown in Table  6  in terms of average accuracy and F1 scores.",
            "According to Section  5.2.1 ,  5.2.2 , and  5.2.3 , the evaluation of RAGent on five real-world datasets show that it outperforms the current state-of-the-art, significantly improving the access control policy generation reliability.  However, one might argue that these results may not clearly capture RAGents performance in real-world scenarios, as the datasets were already processed by the original authors of the datasets  [ 72 ] .  Therefore, we also analyzed the performance of RAGent, in a real-world policy generation scenario and reported our findings in Appendix  A.3 .",
            "We propose a novel retrieval-based access control policy generation framework, RAGent as shown in Figure  1 , innovatively adapting pre-trained open-source LMs to the access control policy generation domain.  To the best of our knowledge, this is the first framework that utilizes LMs in identifying NLACPs, translating them into access control policies through RAG, and finally, verifying and refining the generated policies automatically using our novel iterative verification-refinement mechanism.  Our evaluation suggests that RAGent outperforms all the existing state-of-the-art policy generation frameworks (by 3.2% in NLACP identification and by 39.1% in access control policy component extraction),  significantly improving the reliability of access control policy generation  [ 51 ,  80 ,  70 ,  72 ,  50 ,  52 ] .  In this section, we discuss those evaluation results to understand why RAGent performs better than existing frameworks and why it is vital in access control policy generation.",
            "Furthermore, including domain/organization-specific information as a non-parametric memory aids RAGent to generate access control policies that match the pre-defined entities like users and resources, as shown in Figure  3 .  Utilizing such information provides RAGent contextual awareness necessary for generating policies accurately in unknown domains  [ 21 ,  26 ] , which is lacking in existing frameworks.  As a result, RAGent produces correct access control policies compared to the current state-of-the-art, especially when evaluated using out-of-domain data from document-folds, achieving superior F1 scores as shown in Table  4 .  Also, unlike in existing frameworks, this removes the need to train RAGent repetitively with the organizations confidential access requirements to adapt it to specific contexts.  The significance of incorporating organization-specific information is further evaluated in our ablation study reported in Appendix  A.4.1 .",
            "Even if the organization-specific entities are not provided, our ablation study reported in Appendix  A.4.1  shows that RAGent still generates access control policies with significantly high reliability compared to the existing frameworks  [ 80 ,  50 ,  52 ,  72 ] .  This proves that RAGent can also be used to develop the initial set of access control policies of an organization reliably, where there are no pre-defined entities  [ 52 ] .  However, in that case, the existing state-of-the-art frameworks  [ 80 ,  52 ] , often extract policy components for actions that are not related to the current access control policy  [ 51 ,  52 ,  50 ,  80 ] , leading to low reliability.  For example, consider the NLACP from CACP dataset,  The organisation may use email addresses to answer inquiries.  that has on ACR with the subject  organisation , action  use , resource  email address , and purpose  answer inquiries   [ 84 ,  11 ] .  However, when that NLACP is given to any existing policy generation framework, it will identify two ACRs as  The organisation may use email addresses  and  The organisation answer inquiries .  Consequently, the extracted components from the ACR with the action  answer  will be considered as false positives  [ 84 ] .  Even though Xia et al. used a domain dictionary  [ 81 ]  to store such unrelated actions  [ 80 ]  and filter out unwanted ACRs, they are not flexible enough to handle all such cases.  For instance, if the action  answer  is included in that dictionary to filter out the unwanted ACR detected from the aforementioned NLACP, it might reject a valid ACR with the action  answer  in another access control domain.  Therefore, those false positives are harder to avoid, resulting in low precision, leading to significantly lower F1 scores  [ 52 ]  compared to RAGent 2 2 2 We implemented the existing frameworks like Xia et al.s framework  [ 80 ]  as mentioned in Section  5.2.2 , assuming that all the information about their implementations, such as the entire domain dictionary, is reported in the paper.  However, the authors may have reported only a  portion  of the dictionary, partly affecting the reliability of Xia et al.s framework. .",
            "Secondly, we adapt the LLMs by fine-tuning them to identify NLACPs, generate access control policies, and verify them (Section  4 ).  To this end, we only use real-world yet publicly available datasets introduced by Slankas et al.  [ 72 ] , ensuring that they do not contain any sensitive information like email addresses, phone numbers, etc.  We also generate synthetic data and use them only during the fine-tuning stage of RAGent, as mentioned in Section  4.1 .  After the fine-tuning process, we evaluate RAGent using the public real-world dataset mentioned above to assess its performance in identifying real-world NLACPs and generating access control policies from them.  We further showcase RAGents performance in real-world policy generation scenarios by generating access control policies from high-level requirement specifications of the HotCRP.com conference management website as reported in Appendix  A.3 .",
            "As we mentioned in Section  4.1 , we used data augmentation and LLM-based synthetic data generation techniques to improve the original dataset introduced by Slankas et al.  [ 72 ] .  In the data augmentation step, we utilized BERT embeddings to substitute words of the sentences randomly and back translation  [ 80 ] .  For example, by substituting random words of the sentence  The doctor can read patients records.  using BERT embeddings  [ 20 ] , we can generate a sentence like  ER doctors can read patients medical history. .  Similarly, in back translation, we translated the original NLACP to German and translated it back to English using  [ 53 ] .  By doing so, we attempted to introduce variations to the sentences while keeping the sentence structure intact, helping LMs to improve their generalization.",
            "After preparing the access control policy dataset (Section  4.1 ), we convert each annotated sentence into chat messages (i.e., training examples) with three roles:  system  which provides instructions to perform a particular task (Line 1-5),  user  who provides the input (i.e., user query containing the NLACP and relevant information that can be used to translate the NLACP) to the task (Line 7-10), and  assistant  who provides the answer to the user query according to the system instructions (Line 12-14).  Finally, we applied the following LLaMa 3 chat template to each training example during the training phase of the RAGents access control policy generation module as advised by  [ 3 ] .",
            "To begin with, we copied the entire page that describes privacy policies and reformatted it according to the markdown format, for ease of processing and visualization.  Provided that reformatted document, RAGent first conducted pre-processing through techniques such as separating paragraphs, coreference resolutions, sentence segmentation, and tokenization, as we described in Section  3.1 .  After the pre-processing, RAGent correctly identified NLACPs in the HotCRP privacy policy webpage with an F1 score of 95%, showcasing its ability of NLACP identification even in a new domain.  Once a sentence is identified as an NLACP, RAGent translated it into its structured representation as described in Section  5.2.2 .  When generating policies, first, we did not provide the entities like subjects and resources relevant for policy generation through information retrieval.  Consequently, we found that RAGent sometimes does not identify ambiguous subjects like  hotcrp.com  and identify partial resources like  submission  instead of  submission artifact .  However, once we provided entities as relevant information to generate policies, RAGent successfully identified such ambiguous subjects and generated policies that match the pre-defined entities like resources.  This shows the importance of providing the relevant information for policy generation (other than the NLACP), further validating our results obtained in the ablation study (Appendix  A.4.1 ).  Furthermore, we found that RAGent uncovers complex hidden ACRs within NLACPs and generates access control policies accurately.  For instance, given the NLACP,  Demographic data is stored in user global profiles, and can only be modified by users (never by site managers). , it generates the structured representation as,",
            "Notably, RAGent identified that the  site manager  cannot  modify demographic data  even though it is not mentioned in the NLACP clearly.  Consequently, RAGent achieves the F1 score of 87.3% in extracting policy components from HotCRP privacy policies 3 3 3 To get the F1 scores in NLACP identification and policy generation, we conducted the same annotation process mentioned in Appendix  A.1  to annotate the sentences from HotCRP privacy policy .  In contrast, once we provided the same NLACP to the SRL-based Xia et al.s framework  [ 80 ] , it only identified the subject as  by users by site managers  and the resource as  demographic data  for the action  modify .  While it did not correctly extract either subject (i.e.,  user  or  site manager ), it cannot identify that the permission for the  site manager  to  modify  the  demographic data  is  denied .",
            "In RAGent, we utilize organization-specific entities like subjects and resources not only as an input to generate access control policies from NLACPs but also to post-process the generated policies in Step  3.4.1 .  According to Figure  3 , the reliability of RAGent drops considerably on all five document-folds when the organization-specific information is not provided to generate access control policies (as an input) through information retrieval (Section  3.3 ).  It highlights the importance of organization-specific information when generating the correct enforceable access control policies, despite being the reliability still significantly higher than the current state-of-the-art frameworks (i.e., Narouei et al.  [ 49 ,  52 ,  50 ]  and Xia et al.  [ 80 ] ) even without that information.",
            "As shown in Figure  3 , the effect of not providing organization-specific information to generate policies from NLACPs varies across different datasets.  While it causes an absolute performance drop of 17.7%, 15.3%, and 10.5% for CC, CACP, and IBM, respectively, the drop is smaller for T2P (6.5%) and ACRE (4.7%).  The main reason for the relatively smaller drop is that both T2P and ACRE datasets were derived from the same iTrust dataset  [ 72 ] .  In other words, while T2P and ACRE contain different NLACPs, they often address similar entities like LHCP, HCP, and Patient.  Therefore, as mentioned in Section  4.1 , in the document-fold evaluation of T2P (ACRE), we train the model with the other folds, including ACRE (T2P).  As a result, the policy generation module can learn some percentage of entities of the T2P (ACRE) dataset through the ACRE (T2P) dataset during its training stage, reducing the necessity of a non-parametric memory (i.e., information about pre-defined entities).  On the other hand, since the origins of IBM, CC, and Collected are different, they seem to benefit significantly from their domain-specific information (10.5% minimum improvement in F1 score), as indicated by their higher performance drops when that information is not provided to generate policies.",
            "Furthermore, Figure  3  shows that the policy generation reliability of RAGent is slightly reduced if the generated policies are not post-processed using the organization-specific information (Section  3.4.1 ).  While that reduction is relatively higher for ACRE (4%), it is lower for CC (1%), IBM (1.3%), T2P (2.3%), and CACP (2.5%).  Nonetheless, it shows that even though the organization-specific information is provided as input to the policy generation module to generate policies through RAG, post-processing the generated policies (with the same information) can further help improve the reliability of the process."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Access control policy dataset containing data from domains like    conference management,    healthcare,    education, and    the combined dataset from Xiao et al.  [ 80 ] .",
        "table": "S4.SS1.13.13",
        "footnotes": [],
        "references": [
            "We introduce a novel policy verification-refinement mechanism, improving the reliability of the process even further. It automatically verifies the generated policy and refines it iteratively using feedback, if it is generated incorrectly (Section  3  and Appendix  A.4.2 ).",
            "After identifying the NLACPs of the high-level requirement specification documents (Step 2) with the relevant information (Step 3), RAGent translates them into access control policies through  structured information extraction   [ 16 ] , as shown in Figure  1 .  In other words, RAGent extracts information from an NLACP according to a structure/hierarchy (e.g., JSON) delineating an access control policy.  It represents underlying ACRs of the NLACP, access decisions, and policy components (i.e., subject, actions, resource, purpose, and condition) of each rule  [ 72 ,  49 ] .  For example, consider an NLACP,  The doctor can write prescriptions, but the nurse cannot. .  Given that NLACP, the RAGent generates the access control policy according to a structure as shown in Figure  2 , maintaining the relationships between policy components within each ACR  [ 16 ]  (We removed the empty purpose and condition fields of each rule for clarity).  This can easily be transformed into any standard access control language, such as XACML  [ 11 ] , when applying it to the authorization system.",
            "To generate access control policies from NLACPs, previous research has often used general-purpose SRL models, leading to several limitations like extracting unwanted entities  [ 50 ,  52 ]  and not being able to extract purposes and conditions (Section  2 ).  Therefore, to avoid such limitations, in RAGent, we utilize domain-adapted Large LMs (LLMs), which have been proven effective in structured information extraction from input texts  [ 16 ] .  They show high efficacy in tasks that require a combination of a natural and domain-specific language, as well as an understanding of specialized terminology after adapting them to a specific domain  [ 62 ,  52 ] .  As a result, domain-adapted LLMs have been successfully used in similar tasks to access control policy generation, such as code generation from NL descriptions  [ 62 ,  54 ,  85 ,  2 ]  and structured information extraction from scientific documents  [ 16 ] .",
            "To utilize BART for access control policy verification, first, we add an FFN with two linear layers on top of the final layer hidden states of the last decoder token,  [eos]  of BART  [ 66 ] , to output the classification results.  Then, we fine-tune it using the policy verification dataset (Section  4.2 ) by minimizing the cross-entropy loss between the classifier output and the ground truth label  [ 66 ] .  After training the verifier, at the inference time, it outputs the class that represents the correct access control policy if the NLACP and the generated policy match.  Otherwise, it outputs a reason for the policy being incorrect (i.e., the error type) as feedback, which will then be used in Step 6 to refine the incorrect policy before adding it to the authorization system.",
            "After the verifier identifies the error category (if the generated policy is incorrect), it will be converted into an instruction (i.e.,  refinement instruction  shown in Appendix  A.2 ) that asks the policy generation module to correct the error and re-output the refined policy.  RAGent repeats this automatic verification-generation process at most  n n n italic_n  times until the verification result changes into correct (i.e., the correct policy for the given NLACP is generated).  If the verification result still indicates the refined policy is incorrect even after  n n n italic_n  iterations, the final refined policy and the error category are sent to the administrator as feedback.  It will help the administrator to identify the error of the incorrect policy and refine it manually before adding it to the authorization system  [ 82 ] .",
            "As we described, we fine-tune LMs utilized in RAGent using domain-related datasets to improve the reliability of the process  [ 52 ] .  This section presents those datasets, namely the  access control policy dataset  (Section  4.1 ) used to train and evaluate the NLACP identification and policy generation modules,  access control policy verification dataset  (Section  4.2 ) used to train and evaluate the verifier, and  access control policy refinement dataset  (Section  4.3 ) used to train the policy generation module to refine incorrectly generated policies.",
            "The access control refinement dataset is used to teach the access control policy generation module how to refine an incorrect policy identified by the access control policy verification step.  Therefore, each training example of this dataset should contain the NLACP, its incorrectly generated access control policy, the error type of the incorrectly generated policy, and the correct access control policy of the NLACP as the label.  All the mentioned components of a training example can be derived from the previously described access control policy dataset (Section  4.1 ) and the access control policy verification dataset (Section  4.2 ).  For instance, the NLACP and its correct access control policy can be retrieved from the access control policy dataset.  Also, the incorrect access control policy for the same NLACP and the error type can be retrieved from the access control policy verification dataset.  The structure of a training example that combines all the mentioned components is shown in Appendix  A.2 .",
            "After training the components of RAGent, as described in Section  3 , we evaluate their reliability using the F1 score (i.e., the harmonic mean of the precision and recall  [ 41 ] ).  Particularly, we evaluate the RAGents NLACP identification (Section  5.2.1 ) and access control policy generation (with iterative refinement) (Section  5.2.2 ) performance on each document-fold and the test set of the  Overall  dataset (Table  4.1 ).  Moreover, we evaluate the RAGents reliability in access control policy verification (Section  5.2.3 ) using the access control policy verification dataset (Section  4.2 ).",
            "To evaluate the verification performance of the RAGents verifier, we utilized the  Overall  dataset shown in Table  4.1 , as it contains different types of NLACPs from different domains (e.g., healthcare, education, etc.).  After creating the verification dataset based on the  Overall  dataset according to Section  4.2 , we first divided it into train (80%), test (10%) and validation sets (10%) randomly  [ 44 ] .  Then, we train the verifier using the training set, select the best model/checkpoint based on the validation set, and finally, evaluate the verifiers performance in the test set.  The obtained evaluation results are shown in Table  6  in terms of average accuracy and F1 scores.",
            "According to Section  5.2.1 ,  5.2.2 , and  5.2.3 , the evaluation of RAGent on five real-world datasets show that it outperforms the current state-of-the-art, significantly improving the access control policy generation reliability.  However, one might argue that these results may not clearly capture RAGents performance in real-world scenarios, as the datasets were already processed by the original authors of the datasets  [ 72 ] .  Therefore, we also analyzed the performance of RAGent, in a real-world policy generation scenario and reported our findings in Appendix  A.3 .",
            "Access Control Policy Generation.   As we reported in Section  5.2.2 , RAGent outperforms all the existing frameworks in access control policy generation significantly  [ 80 ,  50 ,  52 ,  49 ] .  RAGent excels in extracting not only basic components like subjects, actions, and resources but also purposes and conditions when generating policies, achieving an average F1 score of 80.7%.  It shows that even if components with complex linguistic forms like purposes and conditions  [ 11 ]  are involved in policy generation, RAGent performs with high reliability, which cannot be achieved in any existing framework.  As a result, in contrast to existing frameworks, RAGent allows organizations to generate access control policies that ensure access to resources is granted or denied based on legitimate reasons and contextual factors, thereby enhancing security and compliance.",
            "Even if the organization-specific entities are not provided, our ablation study reported in Appendix  A.4.1  shows that RAGent still generates access control policies with significantly high reliability compared to the existing frameworks  [ 80 ,  50 ,  52 ,  72 ] .  This proves that RAGent can also be used to develop the initial set of access control policies of an organization reliably, where there are no pre-defined entities  [ 52 ] .  However, in that case, the existing state-of-the-art frameworks  [ 80 ,  52 ] , often extract policy components for actions that are not related to the current access control policy  [ 51 ,  52 ,  50 ,  80 ] , leading to low reliability.  For example, consider the NLACP from CACP dataset,  The organisation may use email addresses to answer inquiries.  that has on ACR with the subject  organisation , action  use , resource  email address , and purpose  answer inquiries   [ 84 ,  11 ] .  However, when that NLACP is given to any existing policy generation framework, it will identify two ACRs as  The organisation may use email addresses  and  The organisation answer inquiries .  Consequently, the extracted components from the ACR with the action  answer  will be considered as false positives  [ 84 ] .  Even though Xia et al. used a domain dictionary  [ 81 ]  to store such unrelated actions  [ 80 ]  and filter out unwanted ACRs, they are not flexible enough to handle all such cases.  For instance, if the action  answer  is included in that dictionary to filter out the unwanted ACR detected from the aforementioned NLACP, it might reject a valid ACR with the action  answer  in another access control domain.  Therefore, those false positives are harder to avoid, resulting in low precision, leading to significantly lower F1 scores  [ 52 ]  compared to RAGent 2 2 2 We implemented the existing frameworks like Xia et al.s framework  [ 80 ]  as mentioned in Section  5.2.2 , assuming that all the information about their implementations, such as the entire domain dictionary, is reported in the paper.  However, the authors may have reported only a  portion  of the dictionary, partly affecting the reliability of Xia et al.s framework. .",
            "Nonetheless, as we explained in Section  3.6 , the ML/NLP-based policy generation framework may still generate incorrect policies due to reasons like complexities and ambiguities of high-level requirement specification (e.g., ambiguous policy components, grammatically incorrect NLACPs)  [ 50 ,  34 ,  33 ,  19 ] .  Therefore, as reported in our ablation study (Appendix  A.4.2 ), by iteratively identifying and refining such errors automatically, RAGent improves the reliability of access control policy generation even further compared to existing frameworks.",
            "Access Control Policy Verification.   In order to refine the generated policies, incorrect policies, as well as the error category of the incorrect policies, should be identified correctly by the access control policy verifier.  That is because RAGent should only refine the incorrect policies based on the specific error category  [ 35 ] .  As reported in Section  5.2.3 , RAGents BART-based verifier fulfills the mentioned requirements by identifying incorrect policies with an F1 score of 99% and error categories with an F1 score of more than 90%.  Therefore, the correct and necessary information required to refine an incorrect policy will be provided to the policy generation module, leading to the correct policy after refinement (assuming the policy generation module refines the policy correctly).",
            "Finally, since the existing ML-based automated policy generation approaches are far from 100% accurate  [ 19 ,  82 ,  80 ,  50 ,  52 ] , the chance of applying incorrect policies generated by those frameworks to the authorization system is high.  Consequently, these erroneous policies may cause access control failures, leading to data breaches  [ 59 ] .  Therefore, RAGent attempts to avoid applying such incorrectly generated policies by LMs, using a novel automated iterative verification-refinement mechanism (Section  3.6 ).  This mechanism involves identifying an incorrectly generated policy and re-generating the correct policy by iteratively addressing the identified error using LMs.  If the automatic iterative refinement fails to correct incorrect policies, we also incorporate system administrators as a fail-safe to refine the incorrect policies manually before adding them to the authorization system.  As a result, as we show in our ablation study (Appendix  A.2 ), our proposed verification-refinement mechanism of RAGent significantly improves the reliability of the fully automated policy generation process, proving that it is a promising approach for reducing access control failures due to incorrect policies.",
            "In Section  5.2 , we demonstrated the higher reliability of our framework, RAGent, compared to the existing frameworks  [ 80 ,  52 ,  72 ,  81 ] , using the datasets described in Section  4 .  The sentences of those datasets were extracted from high-level requirement specification documents using a pre-defined set of grammar by Slankas et al.  [ 70 ] .  However, Slankas et al. further refined those sentences to align with the techniques used by their policy generation framework  [ 70 ] .  For example, they replaced shorthand and removed text in the sentences of the datasets that their dependency parser would not recognize when extracting ACRs  [ 70 ] .  Such processing techniques have made the datasets somewhat  artificial .  Consequently, one might argue that the results reported in Section  5.2  based on those datasets may not accurately reflect the performance of RAGent in a real-world setting.  Therefore, to address this concern, we applied RAGent to the content from the HotCRP conference management websites privacy policy webpage  [ 30 ]  containing 39 English sentences.  We opted for  privacy policies  instead of  access control requirements  due to (1) the unavailability of confidential access control requirements of any organization and (2) the similarities between privacy and access control requirements, such as policy components  [ 84 ,  11 ] .",
            "To begin with, we copied the entire page that describes privacy policies and reformatted it according to the markdown format, for ease of processing and visualization.  Provided that reformatted document, RAGent first conducted pre-processing through techniques such as separating paragraphs, coreference resolutions, sentence segmentation, and tokenization, as we described in Section  3.1 .  After the pre-processing, RAGent correctly identified NLACPs in the HotCRP privacy policy webpage with an F1 score of 95%, showcasing its ability of NLACP identification even in a new domain.  Once a sentence is identified as an NLACP, RAGent translated it into its structured representation as described in Section  5.2.2 .  When generating policies, first, we did not provide the entities like subjects and resources relevant for policy generation through information retrieval.  Consequently, we found that RAGent sometimes does not identify ambiguous subjects like  hotcrp.com  and identify partial resources like  submission  instead of  submission artifact .  However, once we provided entities as relevant information to generate policies, RAGent successfully identified such ambiguous subjects and generated policies that match the pre-defined entities like resources.  This shows the importance of providing the relevant information for policy generation (other than the NLACP), further validating our results obtained in the ablation study (Appendix  A.4.1 ).  Furthermore, we found that RAGent uncovers complex hidden ACRs within NLACPs and generates access control policies accurately.  For instance, given the NLACP,  Demographic data is stored in user global profiles, and can only be modified by users (never by site managers). , it generates the structured representation as,"
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  The NLACP identification performance of RAGent in terms of F1 score compared with the existing access control policy generation frameworks. N/R indicates that the results were not reported in the respective paper.",
        "table": "S5.T3.1",
        "footnotes": [],
        "references": [
            "We introduce RAGent, a novel  R etrieval-based  A ccess control policy  Gen era t ion framework to identify and translate NLACPs into access control policies using transformer-based LMs.  Utilizing small open-source LMs compared to proprietary LMs, RAGent enables its efficient and local deployment within the organization, ensuring the confidentiality of the organizational access control requirements  [ 16 ]  (Section  3 ).",
            "We show that incorporating organization-specific information to generate access control policies through Retrieval Augmented Generation (RAG)  [ 39 ]  helps improve the reliability of the access control policy generation process (Section  3  and Appendix  A.4.1 ).  Furthermore, it enables the reliable adaptation of RAGent to any new domains without costly training of ML models.",
            "We introduce a novel policy verification-refinement mechanism, improving the reliability of the process even further. It automatically verifies the generated policy and refines it iteratively using feedback, if it is generated incorrectly (Section  3  and Appendix  A.4.2 ).",
            "By applying RAGent to a real-world policy generation scenario, we further highlight its ability to deal with complex access requirements involving multiple and hidden/implicit Access Control Rules (ACRs), which cannot be achieved using existing frameworks (Appendix  A.3 ).",
            "As we described, we fine-tune LMs utilized in RAGent using domain-related datasets to improve the reliability of the process  [ 52 ] .  This section presents those datasets, namely the  access control policy dataset  (Section  4.1 ) used to train and evaluate the NLACP identification and policy generation modules,  access control policy verification dataset  (Section  4.2 ) used to train and evaluate the verifier, and  access control policy refinement dataset  (Section  4.3 ) used to train the policy generation module to refine incorrectly generated policies.",
            "The access control policy verification dataset is used to train and evaluate the access control policy verifier discussed in Section  3.5 .  Therefore, the verification dataset should contain sequence pairs representing NLACP and its correct access control policy (i.e., negative sequence pairs) as well as pairs containing NLACP and its incorrect access control policy (i.e., positive sequence pairs).  Since we have the entity annotations for each NLACP from the access control policy dataset (Table  4.1 ), we already have the correct access control policies for the NLACPs to create the negative sequence pairs.  Therefore, to generate incorrect policies for each NLACP, we augment the correct policies by manipulating their properties according to each error type mentioned in Section  3.5 .",
            "After training the components of RAGent, as described in Section  3 , we evaluate their reliability using the F1 score (i.e., the harmonic mean of the precision and recall  [ 41 ] ).  Particularly, we evaluate the RAGents NLACP identification (Section  5.2.1 ) and access control policy generation (with iterative refinement) (Section  5.2.2 ) performance on each document-fold and the test set of the  Overall  dataset (Table  4.1 ).  Moreover, we evaluate the RAGents reliability in access control policy verification (Section  5.2.3 ) using the access control policy verification dataset (Section  4.2 ).",
            "We evaluate the performance of RAGent in identifying NLACPs using both document folds and the  Overall  datasets introduced in Section  4.1 .  The obtained evaluation results compared with the existing access control policy generation frameworks in terms of F1-score are shown in Table  3 . According to Table  3 , RAGent achieves an average document fold F1 score of 87.9%, outperforming the existing frameworks in almost all the document folds.  Moreover, it attains the F1 score of 91.9% on the test set of the  Overall  dataset.",
            "We evaluate the access control policy generation performance of RAGent in terms of  access control policy component extraction  and  access control rule generation .   Access control policy component extraction  focuses on extracting individual policy components for each action (i.e., subject, resource, purpose, and condition) of an NLACP  [ 80 ,  50 ,  52 ,  27 ,  70 ,  72 ] .  On the other hand,  access control rule generation  focuses on translating the ACRs of an NLACP into a structured representation, as mentioned in Section  3.4 .",
            "According to Section  5.2.1 ,  5.2.2 , and  5.2.3 , the evaluation of RAGent on five real-world datasets show that it outperforms the current state-of-the-art, significantly improving the access control policy generation reliability.  However, one might argue that these results may not clearly capture RAGents performance in real-world scenarios, as the datasets were already processed by the original authors of the datasets  [ 72 ] .  Therefore, we also analyzed the performance of RAGent, in a real-world policy generation scenario and reported our findings in Appendix  A.3 .",
            "NLACP Identification.   The superior results attained by RAGent in NLACP identification (average F1 score of 87.9%) according to Table  3  suggest that it identifies access requirements buried in high-level requirement specification documents better than the existing frameworks.  Therefore, compared to the existing frameworks, the probability of access control failures due to missing policies will be reduced  [ 70 ,  71 ,  72 ,  50 ,  52 ,  49 ,  80 ,  81 ] .  This improvement is mainly due to the capability of RAGents BERT-based NLACP identification module to learn language representations  [ 20 ] .  As a bi-directional LM, BERT attends tokens from both sides of a given token with its self-attention mechanism  [ 20 ] , learning to incorporate context from each word of the sentences when classifying them  [ 60 ] .  Consequently, BERT can identify NLACPs more accurately compared to other traditional ML techniques used in existing frameworks, such as SVM and k-NN  [ 72 ,  70 ,  69 ,  71 ,  49 ] , improving the reliability of RAGent.",
            "Furthermore, including domain/organization-specific information as a non-parametric memory aids RAGent to generate access control policies that match the pre-defined entities like users and resources, as shown in Figure  3 .  Utilizing such information provides RAGent contextual awareness necessary for generating policies accurately in unknown domains  [ 21 ,  26 ] , which is lacking in existing frameworks.  As a result, RAGent produces correct access control policies compared to the current state-of-the-art, especially when evaluated using out-of-domain data from document-folds, achieving superior F1 scores as shown in Table  4 .  Also, unlike in existing frameworks, this removes the need to train RAGent repetitively with the organizations confidential access requirements to adapt it to specific contexts.  The significance of incorporating organization-specific information is further evaluated in our ablation study reported in Appendix  A.4.1 .",
            "Nonetheless, as we explained in Section  3.6 , the ML/NLP-based policy generation framework may still generate incorrect policies due to reasons like complexities and ambiguities of high-level requirement specification (e.g., ambiguous policy components, grammatically incorrect NLACPs)  [ 50 ,  34 ,  33 ,  19 ] .  Therefore, as reported in our ablation study (Appendix  A.4.2 ), by iteratively identifying and refining such errors automatically, RAGent improves the reliability of access control policy generation even further compared to existing frameworks.",
            "Access Control Policy Verification.   In order to refine the generated policies, incorrect policies, as well as the error category of the incorrect policies, should be identified correctly by the access control policy verifier.  That is because RAGent should only refine the incorrect policies based on the specific error category  [ 35 ] .  As reported in Section  5.2.3 , RAGents BART-based verifier fulfills the mentioned requirements by identifying incorrect policies with an F1 score of 99% and error categories with an F1 score of more than 90%.  Therefore, the correct and necessary information required to refine an incorrect policy will be provided to the policy generation module, leading to the correct policy after refinement (assuming the policy generation module refines the policy correctly).",
            "Secondly, we adapt the LLMs by fine-tuning them to identify NLACPs, generate access control policies, and verify them (Section  4 ).  To this end, we only use real-world yet publicly available datasets introduced by Slankas et al.  [ 72 ] , ensuring that they do not contain any sensitive information like email addresses, phone numbers, etc.  We also generate synthetic data and use them only during the fine-tuning stage of RAGent, as mentioned in Section  4.1 .  After the fine-tuning process, we evaluate RAGent using the public real-world dataset mentioned above to assess its performance in identifying real-world NLACPs and generating access control policies from them.  We further showcase RAGents performance in real-world policy generation scenarios by generating access control policies from high-level requirement specifications of the HotCRP.com conference management website as reported in Appendix  A.3 .",
            "Finally, since the existing ML-based automated policy generation approaches are far from 100% accurate  [ 19 ,  82 ,  80 ,  50 ,  52 ] , the chance of applying incorrect policies generated by those frameworks to the authorization system is high.  Consequently, these erroneous policies may cause access control failures, leading to data breaches  [ 59 ] .  Therefore, RAGent attempts to avoid applying such incorrectly generated policies by LMs, using a novel automated iterative verification-refinement mechanism (Section  3.6 ).  This mechanism involves identifying an incorrectly generated policy and re-generating the correct policy by iteratively addressing the identified error using LMs.  If the automatic iterative refinement fails to correct incorrect policies, we also incorporate system administrators as a fail-safe to refine the incorrect policies manually before adding them to the authorization system.  As a result, as we show in our ablation study (Appendix  A.2 ), our proposed verification-refinement mechanism of RAGent significantly improves the reliability of the fully automated policy generation process, proving that it is a promising approach for reducing access control failures due to incorrect policies.",
            "After generating the synthetic data, we manually reviewed each sentence and its annotation to accept, reject, or refine them if necessary.  It is important to emphasize that we only use such a commercialized black-box LLM to generate  synthetic  data for fine-tuning our LMs.  We do not use it for the policy generation (Section  3.4 ) that deals with organizations confidential real-world access control requirements as it requires sending them to a private entity that controls GPT-4 exclusively, making the organization prone to data leakages  [ 73 ,  16 ]  (Section  3.4 ).",
            "As described in Section  4.3 , the complete structure of a training example in the access control policy refinement dataset can be shown as follows.  Similar to training examples from the access control policy dataset, we use the same chat message template to create training examples.  Finally, we applied the LLaMa 3 chat template to each training example (i.e., chat messages) during the training phase of the RAGents access control policy generation module.",
            "To begin with, we copied the entire page that describes privacy policies and reformatted it according to the markdown format, for ease of processing and visualization.  Provided that reformatted document, RAGent first conducted pre-processing through techniques such as separating paragraphs, coreference resolutions, sentence segmentation, and tokenization, as we described in Section  3.1 .  After the pre-processing, RAGent correctly identified NLACPs in the HotCRP privacy policy webpage with an F1 score of 95%, showcasing its ability of NLACP identification even in a new domain.  Once a sentence is identified as an NLACP, RAGent translated it into its structured representation as described in Section  5.2.2 .  When generating policies, first, we did not provide the entities like subjects and resources relevant for policy generation through information retrieval.  Consequently, we found that RAGent sometimes does not identify ambiguous subjects like  hotcrp.com  and identify partial resources like  submission  instead of  submission artifact .  However, once we provided entities as relevant information to generate policies, RAGent successfully identified such ambiguous subjects and generated policies that match the pre-defined entities like resources.  This shows the importance of providing the relevant information for policy generation (other than the NLACP), further validating our results obtained in the ablation study (Appendix  A.4.1 ).  Furthermore, we found that RAGent uncovers complex hidden ACRs within NLACPs and generates access control policies accurately.  For instance, given the NLACP,  Demographic data is stored in user global profiles, and can only be modified by users (never by site managers). , it generates the structured representation as,",
            "In RAGent, we utilize organization-specific entities like subjects and resources not only as an input to generate access control policies from NLACPs but also to post-process the generated policies in Step  3.4.1 .  According to Figure  3 , the reliability of RAGent drops considerably on all five document-folds when the organization-specific information is not provided to generate access control policies (as an input) through information retrieval (Section  3.3 ).  It highlights the importance of organization-specific information when generating the correct enforceable access control policies, despite being the reliability still significantly higher than the current state-of-the-art frameworks (i.e., Narouei et al.  [ 49 ,  52 ,  50 ]  and Xia et al.  [ 80 ] ) even without that information.",
            "As shown in Figure  3 , the effect of not providing organization-specific information to generate policies from NLACPs varies across different datasets.  While it causes an absolute performance drop of 17.7%, 15.3%, and 10.5% for CC, CACP, and IBM, respectively, the drop is smaller for T2P (6.5%) and ACRE (4.7%).  The main reason for the relatively smaller drop is that both T2P and ACRE datasets were derived from the same iTrust dataset  [ 72 ] .  In other words, while T2P and ACRE contain different NLACPs, they often address similar entities like LHCP, HCP, and Patient.  Therefore, as mentioned in Section  4.1 , in the document-fold evaluation of T2P (ACRE), we train the model with the other folds, including ACRE (T2P).  As a result, the policy generation module can learn some percentage of entities of the T2P (ACRE) dataset through the ACRE (T2P) dataset during its training stage, reducing the necessity of a non-parametric memory (i.e., information about pre-defined entities).  On the other hand, since the origins of IBM, CC, and Collected are different, they seem to benefit significantly from their domain-specific information (10.5% minimum improvement in F1 score), as indicated by their higher performance drops when that information is not provided to generate policies.",
            "Furthermore, Figure  3  shows that the policy generation reliability of RAGent is slightly reduced if the generated policies are not post-processed using the organization-specific information (Section  3.4.1 ).  While that reduction is relatively higher for ACRE (4%), it is lower for CC (1%), IBM (1.3%), T2P (2.3%), and CACP (2.5%).  Nonetheless, it shows that even though the organization-specific information is provided as input to the policy generation module to generate policies through RAG, post-processing the generated policies (with the same information) can further help improve the reliability of the process.",
            "After the policies are generated from Step 4 and verified in Step 5, they are refined based on the verification result automatically, as explained in Section  3 .  The effect of this iterative refinement mechanism on the policy generation performance of RAGent can be seen in Figure  4 .  According to Figure  4 , RAGent achieved the highest ACR generation reliability improvement of 3.8% for CC and the lowest improvement (0.2%) for IBM through iterative refinement.  Upon close inspection of the generations from RAGent, we found that, in contrast to the other document folds, CC contains many ambiguous NLACPs, making it harder for RAGent to generate policies accurately, avoiding hallucinations.  For instance, most NLACPs from CC do not contain required components like the subject (e.g.,  In step 2, the full paper must be submitted by uploading it. ).  Even if there is a subject, it is often the website name  CyberChair  without mentioning the exact entity like  site manager , compared to the subjects like  doctor, professor, nurse  from other document folds.  Consequently, RAGent has to depend on the verification and iterative refinement process to generate the correct policy frequently, leading to a significant accuracy drop if RAGent does not refine the policies."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  The access control policy component extraction performance of RAGent in terms of the F1 score compared with the existing frameworks.    Extracts subjects, actions, and resources.    Extracts access decisions, subjects, actions, resources, purposes, and conditions.    With iterative refinement.",
        "table": "S5.T4.10",
        "footnotes": [],
        "references": [
            "We show that incorporating organization-specific information to generate access control policies through Retrieval Augmented Generation (RAG)  [ 39 ]  helps improve the reliability of the access control policy generation process (Section  3  and Appendix  A.4.1 ).  Furthermore, it enables the reliable adaptation of RAGent to any new domains without costly training of ML models.",
            "We introduce a novel policy verification-refinement mechanism, improving the reliability of the process even further. It automatically verifies the generated policy and refines it iteratively using feedback, if it is generated incorrectly (Section  3  and Appendix  A.4.2 ).",
            "We develop and release three annotated datasets by addressing the data scarcity in the access control policy generation domain (Section  4 ).",
            "However, we cannot use BERT directly to classify sentences as NLACPs, as it is pre-trained to predict the masked token of the input sentence (i.e., Masked Language Modeling) and to predict the next sentence (i.e., Next Sentence Prediction)  [ 20 ] .  Therefore, we improve the BERT LM by adding a feed-forward network (FFN) containing a single linear layer on top of the final hidden state of BERT corresponding to the  [ C  L  S ] delimited-[] C L S [CLS] [ italic_C italic_L italic_S ]  token that provides an aggregate representation of the input sequence  [ 20 ] .  Then, we fine-tune BERT with the access control policy dataset (Section  4.1 ) containing NLACPs and non-NLACPs, minimizing the cross-entropy loss  [ 20 ] .  It allows BERT to learn how to identify NLACPs from non-NLACPs accurately  [ 80 ,  27 ] .",
            "In this research, we use the  instruct  version of LLaMa 3 8B, which was created by fine-tuning the LLaMa 3 8B model using the technique called Reinforcement Learning from Human Feedback (RLHF)  [ 58 ]  to follow general user instructions  [ 2 ] .  We further fine-tune it using the access control policy dataset containing NLACPs annotated according to their ACRs (Section  4.1 ) through Parameter Efficient Fine Tuning (PEFT) with low-rank adapters (LoRA)  [ 31 ] .  While this helps the LLM to be specifically adapted to the access control policy generation domain, it also helps overcome catastrophic forgetting of LM fine-tuning as it only updates a small number of extra parameters of the LM  [ 31 ] .  In the fine-tuning process, we minimize the negative log-likelihood of the generated access control policy  [ 66 ] .  Once the LLM is fine-tuned, RAGent uses it to generate access control policies from NLACPs.",
            "To utilize BART for access control policy verification, first, we add an FFN with two linear layers on top of the final layer hidden states of the last decoder token,  [eos]  of BART  [ 66 ] , to output the classification results.  Then, we fine-tune it using the policy verification dataset (Section  4.2 ) by minimizing the cross-entropy loss between the classifier output and the ground truth label  [ 66 ] .  After training the verifier, at the inference time, it outputs the class that represents the correct access control policy if the NLACP and the generated policy match.  Otherwise, it outputs a reason for the policy being incorrect (i.e., the error type) as feedback, which will then be used in Step 6 to refine the incorrect policy before adding it to the authorization system.",
            "As we described, we fine-tune LMs utilized in RAGent using domain-related datasets to improve the reliability of the process  [ 52 ] .  This section presents those datasets, namely the  access control policy dataset  (Section  4.1 ) used to train and evaluate the NLACP identification and policy generation modules,  access control policy verification dataset  (Section  4.2 ) used to train and evaluate the verifier, and  access control policy refinement dataset  (Section  4.3 ) used to train the policy generation module to refine incorrectly generated policies.",
            "The statistics of the resultant dataset in terms of the number of non-NLACP and NLACP sentences and the number of ACRs are shown in Table  4.1 .  As shown in Table  4.1 ,  Cleaned document-folds  indicate the information about data from five real-world policy documents introduced by Slankas et al.  [ 72 ]  after cleaning and updating annotations.  Following the previous research  [ 80 ,  50 ,  52 ,  72 ] , each fold is specifically used to evaluate our framework, RAGent, after training it with the rest of the data  [ 6 ] .  For example, to evaluate RAGent using data from the dataset CC, the framework is trained using the rest of the folds (i.e., CACP, IBM, T2P, and ACRE), and the synthetic data  [ 6 ] .  As a result, even if the fine-tuning involves synthetic data, RAGent will only be evaluated using real-world access control requirements.  On the other hand, the  Overall  dataset is used to train RAGent and evaluate its overall performance with a diverse set of NLACPs.  It was created by combining all the sentences from document folds as well as the synthetic data and randomly splitting it into train (80%), validation (10%), and test (10%) sets.",
            "The access control policy verification dataset is used to train and evaluate the access control policy verifier discussed in Section  3.5 .  Therefore, the verification dataset should contain sequence pairs representing NLACP and its correct access control policy (i.e., negative sequence pairs) as well as pairs containing NLACP and its incorrect access control policy (i.e., positive sequence pairs).  Since we have the entity annotations for each NLACP from the access control policy dataset (Table  4.1 ), we already have the correct access control policies for the NLACPs to create the negative sequence pairs.  Therefore, to generate incorrect policies for each NLACP, we augment the correct policies by manipulating their properties according to each error type mentioned in Section  3.5 .",
            "The access control refinement dataset is used to teach the access control policy generation module how to refine an incorrect policy identified by the access control policy verification step.  Therefore, each training example of this dataset should contain the NLACP, its incorrectly generated access control policy, the error type of the incorrectly generated policy, and the correct access control policy of the NLACP as the label.  All the mentioned components of a training example can be derived from the previously described access control policy dataset (Section  4.1 ) and the access control policy verification dataset (Section  4.2 ).  For instance, the NLACP and its correct access control policy can be retrieved from the access control policy dataset.  Also, the incorrect access control policy for the same NLACP and the error type can be retrieved from the access control policy verification dataset.  The structure of a training example that combines all the mentioned components is shown in Appendix  A.2 .",
            "Finally, the dataset is combined with the access control policy dataset (Section  4.1 ) and used to fine-tune the access control policy generation module.",
            "After training the components of RAGent, as described in Section  3 , we evaluate their reliability using the F1 score (i.e., the harmonic mean of the precision and recall  [ 41 ] ).  Particularly, we evaluate the RAGents NLACP identification (Section  5.2.1 ) and access control policy generation (with iterative refinement) (Section  5.2.2 ) performance on each document-fold and the test set of the  Overall  dataset (Table  4.1 ).  Moreover, we evaluate the RAGents reliability in access control policy verification (Section  5.2.3 ) using the access control policy verification dataset (Section  4.2 ).",
            "We evaluate the performance of RAGent in identifying NLACPs using both document folds and the  Overall  datasets introduced in Section  4.1 .  The obtained evaluation results compared with the existing access control policy generation frameworks in terms of F1-score are shown in Table  3 . According to Table  3 , RAGent achieves an average document fold F1 score of 87.9%, outperforming the existing frameworks in almost all the document folds.  Moreover, it attains the F1 score of 91.9% on the test set of the  Overall  dataset.",
            "We evaluate the access control policy generation performance of RAGent in terms of  access control policy component extraction  and  access control rule generation .   Access control policy component extraction  focuses on extracting individual policy components for each action (i.e., subject, resource, purpose, and condition) of an NLACP  [ 80 ,  50 ,  52 ,  27 ,  70 ,  72 ] .  On the other hand,  access control rule generation  focuses on translating the ACRs of an NLACP into a structured representation, as mentioned in Section  3.4 .",
            "The obtained results for access control policy component extraction in terms of F1 score are reported in Table  4  under two settings of RAGent: SAR and DSARCP.  In the SAR setting, we only consider subjects (S) and resources (R) for a given action (A) to calculate the F1 score, enabling the comparison with prior research  [ 80 ,  72 ,  50 ,  52 ,  70 ] .  In the DSARCP setting, we include access decisions (D), purposes (P), and conditions (C) in addition to subjects and resources when computing the F1 score for each action of NLACPs.",
            "However, we cannot directly compare the performance of RAGent in extracting policy components with results reported in previous research.  That is because, when developing our document folds, we updated the entity annotations by correcting the errors made by Slankas et al. (Section  4 ), even if the NLACPs and sentence annotations remained the same.  Therefore, we first implement the ACR extraction modules of two most reliable policy generation frameworks  [ 80 ,  52 ]  that provide sufficient details to implement them from scratch without a significant effort 1 1 1 We attempted to contact the authors regarding the implementation of their frameworks. Unfortunately, we did not receive any replies. .  Notably, since both the considered ACR extraction modules utilize general-purpose SRL models without any domain adaptation, we do not need to train them to extract policy components  [ 80 ,  52 ,  50 ] .  That being the case, once implemented, we evaluate the performance of those modules on our updated dataset and compare them with RAGent as shown in Table  4 .",
            "As shown in Table  4 , RAGent achieves an average document-fold F1 score of 81.5% in the SAR setting, outperforming all the existing policy generation frameworks significantly by at least 39.9%.  Even when considering access decisions and complex components such as purposes and conditions  [ 11 ]  of each ACR, RAGent still surpasses the existing frameworks by at least 39.1% in the DSARCP setting, achieving an average F1 score of 80.7%.  Furthermore, RAGent achieves the F1 score of 85.2% on the  Overall  dataset.",
            "To evaluate the verification performance of the RAGents verifier, we utilized the  Overall  dataset shown in Table  4.1 , as it contains different types of NLACPs from different domains (e.g., healthcare, education, etc.).  After creating the verification dataset based on the  Overall  dataset according to Section  4.2 , we first divided it into train (80%), test (10%) and validation sets (10%) randomly  [ 44 ] .  Then, we train the verifier using the training set, select the best model/checkpoint based on the validation set, and finally, evaluate the verifiers performance in the test set.  The obtained evaluation results are shown in Table  6  in terms of average accuracy and F1 scores.",
            "Furthermore, including domain/organization-specific information as a non-parametric memory aids RAGent to generate access control policies that match the pre-defined entities like users and resources, as shown in Figure  3 .  Utilizing such information provides RAGent contextual awareness necessary for generating policies accurately in unknown domains  [ 21 ,  26 ] , which is lacking in existing frameworks.  As a result, RAGent produces correct access control policies compared to the current state-of-the-art, especially when evaluated using out-of-domain data from document-folds, achieving superior F1 scores as shown in Table  4 .  Also, unlike in existing frameworks, this removes the need to train RAGent repetitively with the organizations confidential access requirements to adapt it to specific contexts.  The significance of incorporating organization-specific information is further evaluated in our ablation study reported in Appendix  A.4.1 .",
            "Even if the organization-specific entities are not provided, our ablation study reported in Appendix  A.4.1  shows that RAGent still generates access control policies with significantly high reliability compared to the existing frameworks  [ 80 ,  50 ,  52 ,  72 ] .  This proves that RAGent can also be used to develop the initial set of access control policies of an organization reliably, where there are no pre-defined entities  [ 52 ] .  However, in that case, the existing state-of-the-art frameworks  [ 80 ,  52 ] , often extract policy components for actions that are not related to the current access control policy  [ 51 ,  52 ,  50 ,  80 ] , leading to low reliability.  For example, consider the NLACP from CACP dataset,  The organisation may use email addresses to answer inquiries.  that has on ACR with the subject  organisation , action  use , resource  email address , and purpose  answer inquiries   [ 84 ,  11 ] .  However, when that NLACP is given to any existing policy generation framework, it will identify two ACRs as  The organisation may use email addresses  and  The organisation answer inquiries .  Consequently, the extracted components from the ACR with the action  answer  will be considered as false positives  [ 84 ] .  Even though Xia et al. used a domain dictionary  [ 81 ]  to store such unrelated actions  [ 80 ]  and filter out unwanted ACRs, they are not flexible enough to handle all such cases.  For instance, if the action  answer  is included in that dictionary to filter out the unwanted ACR detected from the aforementioned NLACP, it might reject a valid ACR with the action  answer  in another access control domain.  Therefore, those false positives are harder to avoid, resulting in low precision, leading to significantly lower F1 scores  [ 52 ]  compared to RAGent 2 2 2 We implemented the existing frameworks like Xia et al.s framework  [ 80 ]  as mentioned in Section  5.2.2 , assuming that all the information about their implementations, such as the entire domain dictionary, is reported in the paper.  However, the authors may have reported only a  portion  of the dictionary, partly affecting the reliability of Xia et al.s framework. .",
            "Nonetheless, as we explained in Section  3.6 , the ML/NLP-based policy generation framework may still generate incorrect policies due to reasons like complexities and ambiguities of high-level requirement specification (e.g., ambiguous policy components, grammatically incorrect NLACPs)  [ 50 ,  34 ,  33 ,  19 ] .  Therefore, as reported in our ablation study (Appendix  A.4.2 ), by iteratively identifying and refining such errors automatically, RAGent improves the reliability of access control policy generation even further compared to existing frameworks.",
            "Secondly, we adapt the LLMs by fine-tuning them to identify NLACPs, generate access control policies, and verify them (Section  4 ).  To this end, we only use real-world yet publicly available datasets introduced by Slankas et al.  [ 72 ] , ensuring that they do not contain any sensitive information like email addresses, phone numbers, etc.  We also generate synthetic data and use them only during the fine-tuning stage of RAGent, as mentioned in Section  4.1 .  After the fine-tuning process, we evaluate RAGent using the public real-world dataset mentioned above to assess its performance in identifying real-world NLACPs and generating access control policies from them.  We further showcase RAGents performance in real-world policy generation scenarios by generating access control policies from high-level requirement specifications of the HotCRP.com conference management website as reported in Appendix  A.3 .",
            "As we mentioned in Section  4.1 , we used data augmentation and LLM-based synthetic data generation techniques to improve the original dataset introduced by Slankas et al.  [ 72 ] .  In the data augmentation step, we utilized BERT embeddings to substitute words of the sentences randomly and back translation  [ 80 ] .  For example, by substituting random words of the sentence  The doctor can read patients records.  using BERT embeddings  [ 20 ] , we can generate a sentence like  ER doctors can read patients medical history. .  Similarly, in back translation, we translated the original NLACP to German and translated it back to English using  [ 53 ] .  By doing so, we attempted to introduce variations to the sentences while keeping the sentence structure intact, helping LMs to improve their generalization.",
            "After generating the synthetic data, we manually reviewed each sentence and its annotation to accept, reject, or refine them if necessary.  It is important to emphasize that we only use such a commercialized black-box LLM to generate  synthetic  data for fine-tuning our LMs.  We do not use it for the policy generation (Section  3.4 ) that deals with organizations confidential real-world access control requirements as it requires sending them to a private entity that controls GPT-4 exclusively, making the organization prone to data leakages  [ 73 ,  16 ]  (Section  3.4 ).",
            "After preparing the access control policy dataset (Section  4.1 ), we convert each annotated sentence into chat messages (i.e., training examples) with three roles:  system  which provides instructions to perform a particular task (Line 1-5),  user  who provides the input (i.e., user query containing the NLACP and relevant information that can be used to translate the NLACP) to the task (Line 7-10), and  assistant  who provides the answer to the user query according to the system instructions (Line 12-14).  Finally, we applied the following LLaMa 3 chat template to each training example during the training phase of the RAGents access control policy generation module as advised by  [ 3 ] .",
            "As described in Section  4.3 , the complete structure of a training example in the access control policy refinement dataset can be shown as follows.  Similar to training examples from the access control policy dataset, we use the same chat message template to create training examples.  Finally, we applied the LLaMa 3 chat template to each training example (i.e., chat messages) during the training phase of the RAGents access control policy generation module.",
            "In Section  5.2 , we demonstrated the higher reliability of our framework, RAGent, compared to the existing frameworks  [ 80 ,  52 ,  72 ,  81 ] , using the datasets described in Section  4 .  The sentences of those datasets were extracted from high-level requirement specification documents using a pre-defined set of grammar by Slankas et al.  [ 70 ] .  However, Slankas et al. further refined those sentences to align with the techniques used by their policy generation framework  [ 70 ] .  For example, they replaced shorthand and removed text in the sentences of the datasets that their dependency parser would not recognize when extracting ACRs  [ 70 ] .  Such processing techniques have made the datasets somewhat  artificial .  Consequently, one might argue that the results reported in Section  5.2  based on those datasets may not accurately reflect the performance of RAGent in a real-world setting.  Therefore, to address this concern, we applied RAGent to the content from the HotCRP conference management websites privacy policy webpage  [ 30 ]  containing 39 English sentences.  We opted for  privacy policies  instead of  access control requirements  due to (1) the unavailability of confidential access control requirements of any organization and (2) the similarities between privacy and access control requirements, such as policy components  [ 84 ,  11 ] .",
            "To begin with, we copied the entire page that describes privacy policies and reformatted it according to the markdown format, for ease of processing and visualization.  Provided that reformatted document, RAGent first conducted pre-processing through techniques such as separating paragraphs, coreference resolutions, sentence segmentation, and tokenization, as we described in Section  3.1 .  After the pre-processing, RAGent correctly identified NLACPs in the HotCRP privacy policy webpage with an F1 score of 95%, showcasing its ability of NLACP identification even in a new domain.  Once a sentence is identified as an NLACP, RAGent translated it into its structured representation as described in Section  5.2.2 .  When generating policies, first, we did not provide the entities like subjects and resources relevant for policy generation through information retrieval.  Consequently, we found that RAGent sometimes does not identify ambiguous subjects like  hotcrp.com  and identify partial resources like  submission  instead of  submission artifact .  However, once we provided entities as relevant information to generate policies, RAGent successfully identified such ambiguous subjects and generated policies that match the pre-defined entities like resources.  This shows the importance of providing the relevant information for policy generation (other than the NLACP), further validating our results obtained in the ablation study (Appendix  A.4.1 ).  Furthermore, we found that RAGent uncovers complex hidden ACRs within NLACPs and generates access control policies accurately.  For instance, given the NLACP,  Demographic data is stored in user global profiles, and can only be modified by users (never by site managers). , it generates the structured representation as,",
            "In RAGent, we utilize organization-specific entities like subjects and resources not only as an input to generate access control policies from NLACPs but also to post-process the generated policies in Step  3.4.1 .  According to Figure  3 , the reliability of RAGent drops considerably on all five document-folds when the organization-specific information is not provided to generate access control policies (as an input) through information retrieval (Section  3.3 ).  It highlights the importance of organization-specific information when generating the correct enforceable access control policies, despite being the reliability still significantly higher than the current state-of-the-art frameworks (i.e., Narouei et al.  [ 49 ,  52 ,  50 ]  and Xia et al.  [ 80 ] ) even without that information.",
            "As shown in Figure  3 , the effect of not providing organization-specific information to generate policies from NLACPs varies across different datasets.  While it causes an absolute performance drop of 17.7%, 15.3%, and 10.5% for CC, CACP, and IBM, respectively, the drop is smaller for T2P (6.5%) and ACRE (4.7%).  The main reason for the relatively smaller drop is that both T2P and ACRE datasets were derived from the same iTrust dataset  [ 72 ] .  In other words, while T2P and ACRE contain different NLACPs, they often address similar entities like LHCP, HCP, and Patient.  Therefore, as mentioned in Section  4.1 , in the document-fold evaluation of T2P (ACRE), we train the model with the other folds, including ACRE (T2P).  As a result, the policy generation module can learn some percentage of entities of the T2P (ACRE) dataset through the ACRE (T2P) dataset during its training stage, reducing the necessity of a non-parametric memory (i.e., information about pre-defined entities).  On the other hand, since the origins of IBM, CC, and Collected are different, they seem to benefit significantly from their domain-specific information (10.5% minimum improvement in F1 score), as indicated by their higher performance drops when that information is not provided to generate policies.",
            "Furthermore, Figure  3  shows that the policy generation reliability of RAGent is slightly reduced if the generated policies are not post-processed using the organization-specific information (Section  3.4.1 ).  While that reduction is relatively higher for ACRE (4%), it is lower for CC (1%), IBM (1.3%), T2P (2.3%), and CACP (2.5%).  Nonetheless, it shows that even though the organization-specific information is provided as input to the policy generation module to generate policies through RAG, post-processing the generated policies (with the same information) can further help improve the reliability of the process.",
            "After the policies are generated from Step 4 and verified in Step 5, they are refined based on the verification result automatically, as explained in Section  3 .  The effect of this iterative refinement mechanism on the policy generation performance of RAGent can be seen in Figure  4 .  According to Figure  4 , RAGent achieved the highest ACR generation reliability improvement of 3.8% for CC and the lowest improvement (0.2%) for IBM through iterative refinement.  Upon close inspection of the generations from RAGent, we found that, in contrast to the other document folds, CC contains many ambiguous NLACPs, making it harder for RAGent to generate policies accurately, avoiding hallucinations.  For instance, most NLACPs from CC do not contain required components like the subject (e.g.,  In step 2, the full paper must be submitted by uploading it. ).  Even if there is a subject, it is often the website name  CyberChair  without mentioning the exact entity like  site manager , compared to the subjects like  doctor, professor, nurse  from other document folds.  Consequently, RAGent has to depend on the verification and iterative refinement process to generate the correct policy frequently, leading to a significant accuracy drop if RAGent does not refine the policies.",
            "Figure  4  further shows that the iterative refinement also improves the RAGents reliability by   3 % absent percent 3 \\approx 3\\%  3 %  when it comes to the  Overall  dataset.  The above findings suggest that the reliability of the access control policy generation can further be improved through the RAGents novel automatic iterative refinement mechanism, as it reduces the hallucinations of deep learning models like LLMs  [ 35 ,  68 ] ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  The access control rule generation performance of RAGent in terms of F1 score, based on its ability to generate correct ACRs from NLACPs.",
        "table": "S5.T5.1",
        "footnotes": [],
        "references": [
            "We extensively evaluate RAGent through real-world high-level requirement specifications  [ 72 ,  81 ] , demonstrating that it identifies NLACPs with a state-of-the-art average F1 score of 87.9%.  Furthermore, in contrast to existing frameworks, RAGent translates NLACPs into granular access control policies extracting five policy components (i.e., subjects, actions, resources, purpose, and conditions) with an average state-of-the-art F1 score of 80.7%, which is 39.1% higher than the current state-of-the-art  [ 80 ]  (Section  5 ).",
            "The access control policy verification dataset is used to train and evaluate the access control policy verifier discussed in Section  3.5 .  Therefore, the verification dataset should contain sequence pairs representing NLACP and its correct access control policy (i.e., negative sequence pairs) as well as pairs containing NLACP and its incorrect access control policy (i.e., positive sequence pairs).  Since we have the entity annotations for each NLACP from the access control policy dataset (Table  4.1 ), we already have the correct access control policies for the NLACPs to create the negative sequence pairs.  Therefore, to generate incorrect policies for each NLACP, we augment the correct policies by manipulating their properties according to each error type mentioned in Section  3.5 .",
            "After training the components of RAGent, as described in Section  3 , we evaluate their reliability using the F1 score (i.e., the harmonic mean of the precision and recall  [ 41 ] ).  Particularly, we evaluate the RAGents NLACP identification (Section  5.2.1 ) and access control policy generation (with iterative refinement) (Section  5.2.2 ) performance on each document-fold and the test set of the  Overall  dataset (Table  4.1 ).  Moreover, we evaluate the RAGents reliability in access control policy verification (Section  5.2.3 ) using the access control policy verification dataset (Section  4.2 ).",
            "Nevertheless, the policy generation frameworks ability to extract individual policy components does not accurately evaluate its performance in generating reliable access control rules from NLACPs.  For example, consider an NLACP  The doctor can read records. .  Suppose the access control policy generation framework was able to correctly identify the subject as  doctor  but fails to identify the resource as  records  for the action  read .  Then, according to previous research  [ 80 ,  50 ,  52 ] , there is one true positive (i.e., the subject), no false positives, and one false negative (i.e., for predicting that the  records  is not a policy component).  As a result, the F1 score would be 0.67, indicating that the generated ACR is  partially  correct.  However, in reality, the F1 score should be 0, as the generated ACR is incorrect due to the absence of the correct resource, which may result in access control failures.  Therefore, we further evaluate the performance of RAGent in terms of its ability to extract/generate ACRs correctly rather than extracting individual components.  The result from Table  5  achieved through the mentioned evaluation technique shows that RAGent is able to generate ACRs with an average F1 score of 78.3% across all the document folds and with an F1 score of 80.6% over the  Overall  dataset.",
            "According to Section  5.2.1 ,  5.2.2 , and  5.2.3 , the evaluation of RAGent on five real-world datasets show that it outperforms the current state-of-the-art, significantly improving the access control policy generation reliability.  However, one might argue that these results may not clearly capture RAGents performance in real-world scenarios, as the datasets were already processed by the original authors of the datasets  [ 72 ] .  Therefore, we also analyzed the performance of RAGent, in a real-world policy generation scenario and reported our findings in Appendix  A.3 .",
            "Access Control Policy Generation.   As we reported in Section  5.2.2 , RAGent outperforms all the existing frameworks in access control policy generation significantly  [ 80 ,  50 ,  52 ,  49 ] .  RAGent excels in extracting not only basic components like subjects, actions, and resources but also purposes and conditions when generating policies, achieving an average F1 score of 80.7%.  It shows that even if components with complex linguistic forms like purposes and conditions  [ 11 ]  are involved in policy generation, RAGent performs with high reliability, which cannot be achieved in any existing framework.  As a result, in contrast to existing frameworks, RAGent allows organizations to generate access control policies that ensure access to resources is granted or denied based on legitimate reasons and contextual factors, thereby enhancing security and compliance.",
            "Even if the organization-specific entities are not provided, our ablation study reported in Appendix  A.4.1  shows that RAGent still generates access control policies with significantly high reliability compared to the existing frameworks  [ 80 ,  50 ,  52 ,  72 ] .  This proves that RAGent can also be used to develop the initial set of access control policies of an organization reliably, where there are no pre-defined entities  [ 52 ] .  However, in that case, the existing state-of-the-art frameworks  [ 80 ,  52 ] , often extract policy components for actions that are not related to the current access control policy  [ 51 ,  52 ,  50 ,  80 ] , leading to low reliability.  For example, consider the NLACP from CACP dataset,  The organisation may use email addresses to answer inquiries.  that has on ACR with the subject  organisation , action  use , resource  email address , and purpose  answer inquiries   [ 84 ,  11 ] .  However, when that NLACP is given to any existing policy generation framework, it will identify two ACRs as  The organisation may use email addresses  and  The organisation answer inquiries .  Consequently, the extracted components from the ACR with the action  answer  will be considered as false positives  [ 84 ] .  Even though Xia et al. used a domain dictionary  [ 81 ]  to store such unrelated actions  [ 80 ]  and filter out unwanted ACRs, they are not flexible enough to handle all such cases.  For instance, if the action  answer  is included in that dictionary to filter out the unwanted ACR detected from the aforementioned NLACP, it might reject a valid ACR with the action  answer  in another access control domain.  Therefore, those false positives are harder to avoid, resulting in low precision, leading to significantly lower F1 scores  [ 52 ]  compared to RAGent 2 2 2 We implemented the existing frameworks like Xia et al.s framework  [ 80 ]  as mentioned in Section  5.2.2 , assuming that all the information about their implementations, such as the entire domain dictionary, is reported in the paper.  However, the authors may have reported only a  portion  of the dictionary, partly affecting the reliability of Xia et al.s framework. .",
            "Access Control Policy Verification.   In order to refine the generated policies, incorrect policies, as well as the error category of the incorrect policies, should be identified correctly by the access control policy verifier.  That is because RAGent should only refine the incorrect policies based on the specific error category  [ 35 ] .  As reported in Section  5.2.3 , RAGents BART-based verifier fulfills the mentioned requirements by identifying incorrect policies with an F1 score of 99% and error categories with an F1 score of more than 90%.  Therefore, the correct and necessary information required to refine an incorrect policy will be provided to the policy generation module, leading to the correct policy after refinement (assuming the policy generation module refines the policy correctly).",
            "First, when choosing the LMs/LLMs for our research, we focused on small open-source LMs rather than utilizing powerful proprietary LLMs like GPT-4.  That is because closed-source GPT-4-like LMs are exclusively controlled by a private entity  [ 16 ] .  In that case, the policy generation frameworks based on such LLMs would need to send the confidential NLACPs to a different organization that controls the LLM for generating access control policies, posing security concerns like data leakages  [ 79 ,  73 ,  16 ] .  Furthermore, the entity controlling the LM may change it at any time under their discretion, affecting the reliability of policy generation or revoke access to the LM altogether  [ 16 ] .  In contrast, by utilizing small, open-source LMs, RAGent avoids those negative outcomes while improving the reliability of access control policy generation as we show in Section  5 .  It allows the efficient deployment of RAGent within the organization even in low resource environments, providing the control of RAGent to the organization.  Therefore, RAGent does not send an organizations confidential access control requirements to a separate entity, preserving its confidentiality.",
            "In Section  5.2 , we demonstrated the higher reliability of our framework, RAGent, compared to the existing frameworks  [ 80 ,  52 ,  72 ,  81 ] , using the datasets described in Section  4 .  The sentences of those datasets were extracted from high-level requirement specification documents using a pre-defined set of grammar by Slankas et al.  [ 70 ] .  However, Slankas et al. further refined those sentences to align with the techniques used by their policy generation framework  [ 70 ] .  For example, they replaced shorthand and removed text in the sentences of the datasets that their dependency parser would not recognize when extracting ACRs  [ 70 ] .  Such processing techniques have made the datasets somewhat  artificial .  Consequently, one might argue that the results reported in Section  5.2  based on those datasets may not accurately reflect the performance of RAGent in a real-world setting.  Therefore, to address this concern, we applied RAGent to the content from the HotCRP conference management websites privacy policy webpage  [ 30 ]  containing 39 English sentences.  We opted for  privacy policies  instead of  access control requirements  due to (1) the unavailability of confidential access control requirements of any organization and (2) the similarities between privacy and access control requirements, such as policy components  [ 84 ,  11 ] .",
            "To begin with, we copied the entire page that describes privacy policies and reformatted it according to the markdown format, for ease of processing and visualization.  Provided that reformatted document, RAGent first conducted pre-processing through techniques such as separating paragraphs, coreference resolutions, sentence segmentation, and tokenization, as we described in Section  3.1 .  After the pre-processing, RAGent correctly identified NLACPs in the HotCRP privacy policy webpage with an F1 score of 95%, showcasing its ability of NLACP identification even in a new domain.  Once a sentence is identified as an NLACP, RAGent translated it into its structured representation as described in Section  5.2.2 .  When generating policies, first, we did not provide the entities like subjects and resources relevant for policy generation through information retrieval.  Consequently, we found that RAGent sometimes does not identify ambiguous subjects like  hotcrp.com  and identify partial resources like  submission  instead of  submission artifact .  However, once we provided entities as relevant information to generate policies, RAGent successfully identified such ambiguous subjects and generated policies that match the pre-defined entities like resources.  This shows the importance of providing the relevant information for policy generation (other than the NLACP), further validating our results obtained in the ablation study (Appendix  A.4.1 ).  Furthermore, we found that RAGent uncovers complex hidden ACRs within NLACPs and generates access control policies accurately.  For instance, given the NLACP,  Demographic data is stored in user global profiles, and can only be modified by users (never by site managers). , it generates the structured representation as,",
            "We perform an ablation study for RAGent and compare the results with the same existing frameworks we used in Section  5 ."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  The performance of the verifier in identifying correct and incorrect policies. ACR, D, S, A, R, C, and P denote access control rule, decision, subject, action, resource, condition, and purpose, respectively.",
        "table": "S5.T6.8",
        "footnotes": [],
        "references": [
            "To evaluate the verification performance of the RAGents verifier, we utilized the  Overall  dataset shown in Table  4.1 , as it contains different types of NLACPs from different domains (e.g., healthcare, education, etc.).  After creating the verification dataset based on the  Overall  dataset according to Section  4.2 , we first divided it into train (80%), test (10%) and validation sets (10%) randomly  [ 44 ] .  Then, we train the verifier using the training set, select the best model/checkpoint based on the validation set, and finally, evaluate the verifiers performance in the test set.  The obtained evaluation results are shown in Table  6  in terms of average accuracy and F1 scores.",
            "According to Table  6 , RAGents verifier is able to identify incorrect and correct access control policies with F1 scores of 99% and 83%, respectively, achieving an overall macro F1 score of 91%.  It further shows that the verifier identifies each error type with an F1 score of more than 90%.",
            "Nonetheless, as we explained in Section  3.6 , the ML/NLP-based policy generation framework may still generate incorrect policies due to reasons like complexities and ambiguities of high-level requirement specification (e.g., ambiguous policy components, grammatically incorrect NLACPs)  [ 50 ,  34 ,  33 ,  19 ] .  Therefore, as reported in our ablation study (Appendix  A.4.2 ), by iteratively identifying and refining such errors automatically, RAGent improves the reliability of access control policy generation even further compared to existing frameworks.",
            "However, as we found out, the verifier sometimes misidentifies correct access control policies as incorrect (i.e., false positives), often saying that the generated policy is missing an ACR.  Consequently, the F1 score for identifying missing ACR becomes relatively lower than F1 scores for identifying other error categories due to low precision (Table  6 ).  In such cases, interestingly, even though the refinement instruction asks the policy generation module to address the error  missing ACR  (i.e., incorrect verification result), the policy generation module keeps outputting the correct policy generated in the first place without any modification.  That is because the policy generation module does not address an error that is unavailable in the generated policy.  It suggests that, even if the BART-based verifier sometimes misclassified the policies, the policy generation module based on LLaMa 3 (a more advanced LLM compared to BART) rectifies the effect of that misclassification.",
            "Finally, since the existing ML-based automated policy generation approaches are far from 100% accurate  [ 19 ,  82 ,  80 ,  50 ,  52 ] , the chance of applying incorrect policies generated by those frameworks to the authorization system is high.  Consequently, these erroneous policies may cause access control failures, leading to data breaches  [ 59 ] .  Therefore, RAGent attempts to avoid applying such incorrectly generated policies by LMs, using a novel automated iterative verification-refinement mechanism (Section  3.6 ).  This mechanism involves identifying an incorrectly generated policy and re-generating the correct policy by iteratively addressing the identified error using LMs.  If the automatic iterative refinement fails to correct incorrect policies, we also incorporate system administrators as a fail-safe to refine the incorrect policies manually before adding them to the authorization system.  As a result, as we show in our ablation study (Appendix  A.2 ), our proposed verification-refinement mechanism of RAGent significantly improves the reliability of the fully automated policy generation process, proving that it is a promising approach for reducing access control failures due to incorrect policies."
        ]
    },
    "global_footnotes": [
        "We attempted to contact the authors regarding the implementation of their frameworks. Unfortunately, we did not receive any replies.",
        "We implemented the existing frameworks like Xia et al.s framework",
        "as mentioned in Section",
        ", assuming that all the information about their implementations, such as the entire domain dictionary, is reported in the paper.  However, the authors may have reported only a",
        "of the dictionary, partly affecting the reliability of Xia et al.s framework.",
        "To get the F1 scores in NLACP identification and policy generation, we conducted the same annotation process mentioned in Appendix",
        "to annotate the sentences from HotCRP privacy policy"
    ]
}