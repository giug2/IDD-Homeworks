{
    "id_table_1": {
        "caption": "Table 1:  Ablation study of isometric learning for  \\acs ARCH dataset and  \\ac I-FABP protein dynamics datasets for  graph matching loss  (GM) and  stability regularization  (Stability). In both cases we choose  M d  = R subscript M superscript d  R \\mathcal{M}_{d^{\\prime}}=\\mathbb{R} caligraphic_M start_POSTSUBSCRIPT italic_d start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT end_POSTSUBSCRIPT = blackboard_R . We report invertibility (   \\downarrow  ), low-dimensionality (   \\downarrow  ) and isometry (   \\downarrow  ). The distance  (  ,  ) D superscript   D (\\cdot,\\cdot)^{\\mathcal{D}} (  ,  ) start_POSTSUPERSCRIPT caligraphic_D end_POSTSUPERSCRIPT  we preserve on the latent manifold is a locally Euclidean distance based on Isomap  Tenenbaum et al. ( 2000 ) .",
        "table": "A5.EGx1",
        "footnotes": [
            "",
            "",
            ""
        ],
        "references": [
            "Result.   Table 1  demonstrates that incorporating both the graph matching loss and stability regularization improves the invertibility and isometry metrics across both datasets, with the combined approach yielding both a low   i  n  v subscript  i n v \\varepsilon_{inv} italic_ start_POSTSUBSCRIPT italic_i italic_n italic_v end_POSTSUBSCRIPT  and   i  s  o subscript  i s o \\varepsilon_{iso} italic_ start_POSTSUBSCRIPT italic_i italic_s italic_o end_POSTSUBSCRIPT  values, indicating enhanced model performance in preserving the geometry of the data in the synthetic dataset as well ass the more noisy and high dimensional simulated dataset."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  \\Ac RMSE (   \\downarrow  ) of  100 100 100 100  random Isomap geodesics for various (latent) interpolation methods.",
        "table": "S5.T1.34",
        "footnotes": [],
        "references": [
            "A visual example of pullback geometry is given in  Figure 6 . Pullback geometry allows us to remetrize all of space  R d superscript R d \\mathbb{R}^{d} blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , including the data manifold  D  R d D superscript R d \\mathcal{D}\\subset\\mathbb{R}^{d} caligraphic_D  blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , through the pullback metric. We can use it to define geometric mappings on  ( R d , (  ,  )  ) superscript R d superscript    \\big{(}\\mathbb{R}^{d},(\\cdot,\\cdot)^{\\varphi}\\big{)} ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , (  ,  ) start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ) , including geodesics (see  Equation 23 ), through geometric mappings on the latent manifold  M M \\mathcal{M} caligraphic_M . Next, we summarize work on Riemannian Auto-Encoders, that leverage pullback geometry to create an interpolatable latent manifold."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Evaluation of generative model performance across dimensionality of (latent) (sub)manifold (   \\downarrow  ), number of model parameters, denoted by # pars (   \\downarrow  ), and  1 1 1 1 - \\acs NN accuracy ( 1 1 1 1 - \\acs NN  0.5  absent 0.5 \\rightarrow 0.5  0.5 ). The  1 1 1 1 - \\acs NN metric measures the generative quality, with values closer to  0.5 0.5 0.5 0.5  indicating better performance.",
        "table": "S5.T2.22",
        "footnotes": [],
        "references": [
            "where       \\|\\cdot\\|_{\\varphi_{\\bm{\\theta}}}    start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT bold_italic_ end_POSTSUBSCRIPT end_POSTSUBSCRIPT  is the norm induced by the pullback metric  (  ,  )   superscript   subscript   (\\cdot,\\cdot)^{\\varphi_{\\bm{\\theta}}} (  ,  ) start_POSTSUPERSCRIPT italic_ start_POSTSUBSCRIPT bold_italic_ end_POSTSUBSCRIPT end_POSTSUPERSCRIPT . By applying  Equation 3 , we can reformulate the  \\ac PFM objective in terms of manifold mappings on  M M \\mathcal{M} caligraphic_M ,",
            "We evaluate the effectiveness of our proposed method  \\ac PFM for generation on data manifolds  D D \\mathcal{D} caligraphic_D . We train two  \\acp PFM, one using the latent manifold  M M \\mathcal{M} caligraphic_M  and one using the lower dimensional latent submanifold  M d  subscript M superscript d  \\mathcal{M}_{d^{\\prime}} caligraphic_M start_POSTSUBSCRIPT italic_d start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT end_POSTSUBSCRIPT , named  \\ac PFM and  d  superscript d  d^{\\prime} italic_d start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT - \\acs PFM respectively. Additionally, we train a  \\ac CFM model on the raw data as a comparison. A visual example of the learned generative flows over time for the  \\acs ARCH dataset can be viewed in  Figure 3 .",
            "Result.   Figure 3  we see that the learned isometry to the latent manifold  M M \\mathcal{M} caligraphic_M  acts as a strong manifold prior, capturing the manifold structure at the start of the  \\ac CNF trajectory ( t = 0.0 t 0.0 t=0.0 italic_t = 0.0 ). Additionally, the learned isometry to the latent submanifold  M d  subscript M superscript d  \\mathcal{M}_{d^{\\prime}} caligraphic_M start_POSTSUBSCRIPT italic_d start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT end_POSTSUBSCRIPT  captures the noiseless manifold revealing the underlying manifold used to generate the data. Through this strong (noiseless) manifold prior, we see that both  \\ac PFM and  1 1 1 1 - \\acs PFM approximate the distribution on the manifold earlier in the trajectory and better.",
            "Results.   Table 3  highlights the effectiveness of the  1 1 1 1 - \\acs PFM model in generative tasks. The  1 1 1 1 - \\acs PFM model leverages the lower-dimensional isometric latent manifold  M d  subscript M superscript d  \\mathcal{M}_{d^{\\prime}} caligraphic_M start_POSTSUBSCRIPT italic_d start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT end_POSTSUBSCRIPT , significantly reducing the number of parameters required. Training the  1 1 1 1 - \\acs PFM is significantly faster due to the reduction in parameters and the dimensionality of the training samples. The  1 1 1 1 - \\acs NN accuracy for  1 1 1 1 - \\acs PFM approaches the ideal  0.5 0.5 0.5 0.5  across all datasets, indicating that this model better captures the underlying distribution on the data manifold compared to CFM and  \\ac PFM.",
            "A visual example of pullback geometry is given in  Figure 6 . Pullback geometry allows us to remetrize all of space  R d superscript R d \\mathbb{R}^{d} blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , including the data manifold  D  R d D superscript R d \\mathcal{D}\\subset\\mathbb{R}^{d} caligraphic_D  blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , through the pullback metric. We can use it to define geometric mappings on  ( R d , (  ,  )  ) superscript R d superscript    \\big{(}\\mathbb{R}^{d},(\\cdot,\\cdot)^{\\varphi}\\big{)} ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , (  ,  ) start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ) , including geodesics (see  Equation 23 ), through geometric mappings on the latent manifold  M M \\mathcal{M} caligraphic_M . Next, we summarize work on Riemannian Auto-Encoders, that leverage pullback geometry to create an interpolatable latent manifold.",
            "The final  \\ac CFM loss under this  \\ac OT formulation is derived by substituting the above vector field and flow into the general  \\ac CFM objective ( Equation 34 ) and reparameterizing  p t  ( x | x 1 ) subscript p t conditional x subscript x 1 p_{t}(\\bm{x}|\\bm{x}_{1}) italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x | bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT )  in terms of  x 0 subscript x 0 \\bm{x}_{0} bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT . This yields the following objective function:",
            "In the absence of such closed-form solutions, existing methods tackle these difficulties by either learning a metric that constrains the generative trajectory to align with the data support  Kapusniak et al. ( 2024 )  or by assuming a metric with easily computable geodesics on the data manifold  Chen & Lipman ( 2024 ) . However, learning a metric can be problematic as it may lead to overfitting or fail to capture the true geometry of the data, particularly when the data manifold is complex or poorly understood. On the other hand, assuming a simple metric with computable geodesics can oversimplify the problem, resulting in models that inadequately represent the underlying data structure. To overcome these challenges, we introduce Pullback Flow Matching in  section 3 ."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Unique protein sequences generated via analogue generation on the latent manifold  M M \\mathcal{M} caligraphic_M  and its submanifold  M d  subscript M superscript d  \\mathcal{M}_{d^{\\prime}} caligraphic_M start_POSTSUBSCRIPT italic_d start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT end_POSTSUBSCRIPT  at various temperatures (   \\tau italic_ ). The table presents the total sequences generated (Total), those already in the dataset (In Data), and the number of novel sequences (Novel). We perform a  \\ac KS test at a  5 % percent 5 5\\% 5 %  significance level to compare novel sequences with their base points. Non-significant  \\ac KS values are shown as X/Y, where X is the number of non-significant properties and Y is the total properties tested.",
        "table": "S5.T3.49",
        "footnotes": [],
        "references": [
            "The goal of isometric learning is to learn an interpolatable latent (sub)manifold of the data manifold with closed-form manifold mappings. To evaluate whether interpolation on the latent (sub)manifold accurately reflects interpolation on the data manifold, we conduct an interpolation experiment using the synthetic  \\acs ARCH dataset, as well as the molecular dynamics datasets of  \\ac AK ( n = 100 n 100 n=100 italic_n = 100 ,  d = 214  3 d 214 3 d=214\\times 3 italic_d = 214  3 ) and  \\ac I-FABP. In both cases we choose  M d  = R subscript M superscript d  R \\mathcal{M}_{d^{\\prime}}=\\mathbb{R} caligraphic_M start_POSTSUBSCRIPT italic_d start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT end_POSTSUBSCRIPT = blackboard_R , see  Appendix C  for guidance on latent manifold and metric selection. We approximate the metric on the data manifold  (  ,  ) D superscript   D (\\cdot,\\cdot)^{\\mathcal{D}} (  ,  ) start_POSTSUPERSCRIPT caligraphic_D end_POSTSUPERSCRIPT  through the length of Isomaps geodesics  Tenenbaum et al. ( 2000 ) , see  Figure 4  for an example. We compare the accuracy of  100 100 100 100  randomly selected geodesics between points in the test set for  \\acp VAE  Kingma & Welling ( 2013 ) ,    limit-from  \\beta- italic_ - VAEs  Higgins et al. ( 2017 ) ,  (  ,  ) M superscript   M (\\cdot,\\cdot)^{\\mathcal{M}} (  ,  ) start_POSTSUPERSCRIPT caligraphic_M end_POSTSUPERSCRIPT -interpolation and  (  ,  ) M d  superscript   subscript M superscript d  (\\cdot,\\cdot)^{\\mathcal{M}_{d^{\\prime}}} (  ,  ) start_POSTSUPERSCRIPT caligraphic_M start_POSTSUBSCRIPT italic_d start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_POSTSUPERSCRIPT -interpolation.",
            "Results.  The application of our designed latent manifold facilitated the generation of diverse novel protein sequences, demonstrating the effectiveness of the analogue generation methodology. As illustrated in Table  4 , increasing the temperature parameter,    \\tau italic_ , directly influenced the diversity of generated sequences. At lower temperatures (   0.1  0.1 \\tau\\leq 0.1 italic_  0.1 ), many unique sequences emerged while maintaining similarity to their base points, as indicated by non-significant KS test values. Conversely, higher temperatures (  > 0.1  0.1 \\tau>0.1 italic_ > 0.1 ) resulted in a significant increase in novel sequences, accompanied by significant KS values suggesting greater divergence from base sequences. This observation supports the hypothesis that novel sequences generated close to the base points are structurally similar, highlighting the effectiveness of isometric learning in structuring the latent space. Overall, our results indicate that temperature manipulation can strategically balance novelty and similarity, paving the way for innovative applications in protein engineering.",
            "The final  \\ac CFM loss under this  \\ac OT formulation is derived by substituting the above vector field and flow into the general  \\ac CFM objective ( Equation 34 ) and reparameterizing  p t  ( x | x 1 ) subscript p t conditional x subscript x 1 p_{t}(\\bm{x}|\\bm{x}_{1}) italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x | bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT )  in terms of  x 0 subscript x 0 \\bm{x}_{0} bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT . This yields the following objective function:",
            "There are several options when selecting the metric on the data manifold  (  ,  ) D superscript   D (\\cdot,\\cdot)^{\\mathcal{D}} (  ,  ) start_POSTSUPERSCRIPT caligraphic_D end_POSTSUPERSCRIPT . One can choose for example a locally euclidean approximation through Isomap  Tenenbaum et al. ( 2000 )  or a more noise-robust geodesic approximation  Little et al. ( 2022 ) . One can also design a metric to create a latent space  3 3 3 In this text we refer to the latent space as the concept in machine and representation learning, technically its a latent manifold endowed with a Riemannian metric, not a vector space.  structured based on properties of the data one cares about, we show how in  subsection 5.4 . In this work, we focus on using a proper metric and defer the exploration of learning with pseudo-metrics to future research.",
            "An example of the dataset can be found in  Figure 4 ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Hyperparameter settings for ablation study of  \\ac RAE on the  \\ac ARCH,  \\ac AK and  \\ac I-FABP datasets.",
        "table": "A5.EGx2",
        "footnotes": [],
        "references": [
            "Here,    N  ( 0 , I ) similar-to  N 0 I \\bm{\\varepsilon}\\sim\\mathcal{N}(\\bm{0},\\bm{I}) bold_italic_  caligraphic_N ( bold_0 , bold_italic_I )  and  d R d    ( x i , x  ) subscript superscript d subscript   superscript R d subscript x i subscript x  \\bm{d}^{\\varphi_{\\bm{\\theta}}}_{\\mathbb{R}^{d}}(\\bm{x}_{i},\\bm{x}_{\\cdot}) bold_italic_d start_POSTSUPERSCRIPT italic_ start_POSTSUBSCRIPT bold_italic_ end_POSTSUBSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_italic_x start_POSTSUBSCRIPT  end_POSTSUBSCRIPT )  and  d i ,  subscript d i  \\bm{d}_{i,\\cdot} bold_italic_d start_POSTSUBSCRIPT italic_i ,  end_POSTSUBSCRIPT  denote the columns of the distance matrices induces by  (  ,  )  superscript    (\\cdot,\\cdot)^{\\varphi} (  ,  ) start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  and  (  ,  ) D superscript   D (\\cdot,\\cdot)^{\\mathcal{D}} (  ,  ) start_POSTSUPERSCRIPT caligraphic_D end_POSTSUPERSCRIPT . The benefit of this formulation is that it only requires approximating geodesic distances  d i , j subscript d i j d_{i,j} italic_d start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT  on the data manifold  D D \\mathcal{D} caligraphic_D , without needing to calculate or differentiate the metric tensor. In  section 5 , we demonstrate the effectiveness of the new regularization terms through an ablation study on synthetic and high-dimensional simulated protein dynamics trajectories.",
            "Latent interpolation experiments, illustrated in  Figure 5 , further demonstrate the potential of our approach. By interpolating between sequences with contrasting properties, we revealed a smooth transition of characteristics within the latent space, reinforcing our methods capability to fine-tune specific protein attributes. This smooth transition indicates that our latent manifold can be effectively navigated to explore a continuum of properties such as hydrophobicity, hydrophobic moment, charge, and isoelectric point, which are essential for determining protein solubility, stability, and interaction behavior. This capability allows for the targeted design of protein sequences that could be optimized for specific biochemical contexts, potentially enhancing their performance in applications like enzyme catalysis or therapeutic development. In summary, the efficacy of our designed latent manifold not only expands the repertoire of available protein sequences but also ensures retention of biologically relevant properties, positioning this approach as a valuable tool for precision in protein engineering.",
            "There are several options when selecting the metric on the data manifold  (  ,  ) D superscript   D (\\cdot,\\cdot)^{\\mathcal{D}} (  ,  ) start_POSTSUPERSCRIPT caligraphic_D end_POSTSUPERSCRIPT . One can choose for example a locally euclidean approximation through Isomap  Tenenbaum et al. ( 2000 )  or a more noise-robust geodesic approximation  Little et al. ( 2022 ) . One can also design a metric to create a latent space  3 3 3 In this text we refer to the latent space as the concept in machine and representation learning, technically its a latent manifold endowed with a Riemannian metric, not a vector space.  structured based on properties of the data one cares about, we show how in  subsection 5.4 . In this work, we focus on using a proper metric and defer the exploration of learning with pseudo-metrics to future research.",
            "We explain the training procedure and hyperparameter settings for each of the experiments in  section 5  in further detail for reproducibility. In all experiments the datasets where split into train and test sets. We apply early stopping and present the model with the lowest average loss on the test data.",
            "For details of hyperparameter settings for the ablation study see  Table 5 ."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Hyperparameter settings for interpolation experiments for  (  ,  ) M superscript   M (\\cdot,\\cdot)^{\\mathcal{M}} (  ,  ) start_POSTSUPERSCRIPT caligraphic_M end_POSTSUPERSCRIPT - and  (  ,  ) M d  superscript   subscript M superscript d  (\\cdot,\\cdot)^{\\mathcal{M}_{d^{\\prime}}} (  ,  ) start_POSTSUPERSCRIPT caligraphic_M start_POSTSUBSCRIPT italic_d start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_POSTSUPERSCRIPT -interpolation on the  \\ac ARCH,  \\ac AK and  \\ac I-FABP datasets.",
        "table": "A5.EGx3",
        "footnotes": [],
        "references": [
            "A visual example of pullback geometry is given in  Figure 6 . Pullback geometry allows us to remetrize all of space  R d superscript R d \\mathbb{R}^{d} blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , including the data manifold  D  R d D superscript R d \\mathcal{D}\\subset\\mathbb{R}^{d} caligraphic_D  blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , through the pullback metric. We can use it to define geometric mappings on  ( R d , (  ,  )  ) superscript R d superscript    \\big{(}\\mathbb{R}^{d},(\\cdot,\\cdot)^{\\varphi}\\big{)} ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , (  ,  ) start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ) , including geodesics (see  Equation 23 ), through geometric mappings on the latent manifold  M M \\mathcal{M} caligraphic_M . Next, we summarize work on Riemannian Auto-Encoders, that leverage pullback geometry to create an interpolatable latent manifold.",
            "For details of hyperparameter settings for the interpolation experiments of  (  ,  ) M superscript   M (\\cdot,\\cdot)^{\\mathcal{M}} (  ,  ) start_POSTSUPERSCRIPT caligraphic_M end_POSTSUPERSCRIPT - and  (  ,  ) M d  superscript   subscript M superscript d  (\\cdot,\\cdot)^{\\mathcal{M}_{d^{\\prime}}} (  ,  ) start_POSTSUPERSCRIPT caligraphic_M start_POSTSUBSCRIPT italic_d start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_POSTSUPERSCRIPT -interpolation see  Table 6  and for the (   \\beta italic_ -) \\acsp VAE see  Table 7 ."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Hyperparameter settings for interpolation experiments for (   \\beta italic_ -) \\acs VAE on the  \\ac ARCH dataset.  \\Acp VAE have   = 1.0  1.0 \\beta=1.0 italic_ = 1.0 ,    \\beta italic_ - \\acsp VAE have   = 10.0  10.0 \\beta=10.0 italic_ = 10.0 .",
        "table": "S5.T4.11",
        "footnotes": [],
        "references": [
            "We consider the time-normalized open-to-close transition of  \\ac AK. This is a dataset from coarse-grained molecular dynamics simulations consisting of  n = 102 n 102 n=102 italic_n = 102  conformations of  214 214 214 214  amino-acids in  3  D 3 D 3D 3 italic_D , samples of the trajectory can be found in  Figure 7 .",
            "For details of hyperparameter settings for the interpolation experiments of  (  ,  ) M superscript   M (\\cdot,\\cdot)^{\\mathcal{M}} (  ,  ) start_POSTSUPERSCRIPT caligraphic_M end_POSTSUPERSCRIPT - and  (  ,  ) M d  superscript   subscript M superscript d  (\\cdot,\\cdot)^{\\mathcal{M}_{d^{\\prime}}} (  ,  ) start_POSTSUPERSCRIPT caligraphic_M start_POSTSUBSCRIPT italic_d start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_POSTSUPERSCRIPT -interpolation see  Table 6  and for the (   \\beta italic_ -) \\acsp VAE see  Table 7 ."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Hyperparameter settings for  \\ac CFM,  \\ac PFM and  d  superscript d  d^{\\prime} italic_d start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT - \\acs PFM for generation experiments. The same isometry    subscript   \\varphi_{\\bm{\\theta}} italic_ start_POSTSUBSCRIPT bold_italic_ end_POSTSUBSCRIPT  of the interpolation experiments is used for the  \\ac PFM and  d  superscript d  d^{\\prime} italic_d start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT - \\acs PFM.",
        "table": "A5.EGx4",
        "footnotes": [],
        "references": []
    },
    "id_table_9": {
        "caption": "Table 9:  Hyperparameter settings for protein design experiments of the  \\acp RAE on the  \\ac GRAMPA dataset.",
        "table": "A5.EGx5",
        "footnotes": [],
        "references": [
            "In  Table 9  one can find the settings for training the isometry on the  \\ac GRAMPA dataset for the protein sequence design experiments. Specific hyperparameter worth mentioning is the embedding dimensions, we use an embedding layer from the  Flax  library to embed the discrete sequences into a continuous space and use a sign-cosine positional embedding, to embed the location in the sequence of the amino acids in the data."
        ]
    },
    "id_table_10": {
        "caption": "",
        "table": "A5.T5.51",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "",
        "table": "A5.T6.59",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "",
        "table": "A5.T7.43",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "",
        "table": "A5.T8.63.57",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "A5.T9.21",
        "footnotes": [],
        "references": []
    }
}