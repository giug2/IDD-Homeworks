{
    "id_table_1": {
        "caption": "Table 2:  Hyperparameters for different models.",
        "table": "S5.T1.1",
        "footnotes": [],
        "references": [
            "We begin by pointing out a theoretical connection between the RLHF post-training approach outlined in the previous section and general preference modeling. One of the key observations of the DPO approach  (Rafailov et al.,  2023 )  is that the reward modeling objective in Eq.  1  is under-constrained, which can create significant optimization challenges for the RL problem in Eq.  3   (Wu et al.,  2023 ; Ahmadian et al.,  2024 )  (in fact, theoretically, it can have arbitrarily low signal-to-noise ratio). To alleviate this issue, prior works  (Stiennon et al.,  2022 ; Ouyang et al.,  2022 )  use a baseline reward from on a fixed reference distribution:",
            "Here the reference  y ref subscript y ref y_{\\text{ref}} italic_y start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT  is a human completion from an SFT dataset or a sample from the base SFT model. However, notice that by inverting Eq.  1  we have:",
            "The  core contribution  of our work is that we replace the Bradley-Terry reward modelling approach with a strictly more general preference modelling objective  p  ( y w  y l | x ) p succeeds subscript y w conditional subscript y l x p(y_{w}\\succ y_{l}|x) italic_p ( italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT  italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT | italic_x )  that does not assume a point-wise reward estimate, a special model architecture, or a specific preference distribution parameterization as in Eq.  1 . That is, we assume a standard preference dataset and model the preference distribution  p   ( y w  y l | x ) subscript p italic- succeeds subscript y w conditional subscript y l x p_{\\phi}(y_{w}\\succ y_{l}|x) italic_p start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT  italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT | italic_x )  using an LLM, without any additional assumptions. Notice that this formulation is fully general as we can extract preference probabilities either from the likelihoods of the LLM output or from majority voting counts. This can be used in the standard pipeline with PPO  (Stiennon et al.,  2022 ; Ouyang et al.,  2022 )  in Eq.  5  using the reward formulation in Eq.  6 . Alternatively, if we sample preferences from the above model, we can also utilize an iterative or online *PO optimization manner following  Munos et al. ( 2024 )  or  Calandriello et al. ( 2024 ) .",
            "Our prompts are based on the standard MT-Bench prompt  (Zheng et al.,  2023b ) , and can be found in Appendix  A . We use the LLM judge as a prior and further train it to align with the ground-truth dataset judgements. We begin with the preference dataset  D = { x ( i ) , y 1 ( i ) , y 2 ( i ) , I ( i ) } i = 1 N D superscript subscript superscript x i superscript subscript y 1 i superscript subscript y 2 i superscript I i i 1 N \\mathcal{D}=\\left\\{x^{(i)},y_{1}^{(i)},y_{2}^{(i)},I^{(i)}\\right\\}_{i=1}^{N} caligraphic_D = { italic_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , italic_I start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT  as outlined in Section  2.1.2 , however we consider unranked answers  y 1 ( i ) , y 2 ( i ) superscript subscript y 1 i superscript subscript y 2 i y_{1}^{(i)},y_{2}^{(i)} italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT  and the corresponding winning choice  I ( i ) superscript I i I^{(i)} italic_I start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT . We design several training techniques for the generative reward model   subscript  italic- \\pi_{\\phi} italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT .",
            "CoT-GenRM-STaR:  Finally we consider an approach where the model self-bootstraps intermediate reasoning using a STaR approach as outlined in Section  2.2.1 . We will also consider two loss objectives herefollowing the filtering strategy described in the above section, we use the standard SFT loss similar to Eq.  8  on reasoning chains that yield the correct judgement. We denote models trained with this objective as STaR-SFT.",
            "We show additional evaluations in Table  1  using different models to bootstrap reasoning on the UltraFeedback domain. We see similar results to the above when using rationales generated by a a strong GPT-4 model, which significantly under-performs the standard STaR methods, however this is alleviated with additional on-policy training. Finally we see that bootstrapping reasoning with samples from the stronger LLaMa 3.1 70B Instruct model from the same family slightly improves performance on UltraFeedback, but leads to somewhat worse generalization to RewardBench. Our results indicate that on-policy training of the critic models can meaningfully impact performance.",
            "The concept of using language models for providing feedback, also known as constitutional AI or RLAIF  Bai et al. ( 2022b )  has gained significant traction in recent years.  Zheng et al. ( 2023b )  further popularized the paradigm of LLM evaluation (LLM-as-a-judge), demonstrating that strong language models can effectively perform judgments with chain-of-thought (CoT) reasoning that approximate human evaluation. Building on this,  Kim et al. ( 2024 )  proposed Prometheus, employing supervised fine-tuning from a powerful model to provide CoT reasoning and scoring, demonstrating strong evaluation performance from a smaller open model. In the current work we show that zero-shot LLM evaluations may not fully align with human feedback and significant improvements in accuracy can be gained from additional fine-tuning. Moreover, in our proposed approach of combining RLAIF with STaR-based tuning we do not require ground-truth reasoning or supervision from a stronger model. Concurrently with this work,  Zhang et al. ( 2024 )  presented Generative Verifiers, training CoT-GenRM with an SFT objective to act as a verifier for mathematical reasoning. They find similiar observations to our experiments in Sections  5.2  and  5.4 . However, one notable exception is that unlike our approach they rely on access to full reference solutions at training time. Another concurrent work in this direction is  Ankner et al. ( 2024 ) , which combines CoT reasoning generation with Bradley-Terry reward modelling. They present empirical findings on the benefits of reasoning, similar to the ones we show in Section  5.1 , although crucially, they rely on a separate strong model to provide rationales and require the Bradley-Terry reward model hybrid architecture, while we use fully self-bootstrapped rationales in a full language modelling setup without the need for any additional architecture overhang."
        ]
    },
    "id_table_2": {
        "caption": "Table 3:  STaR method evaluation results throughout training iterations.",
        "table": "A1.T2.2",
        "footnotes": [],
        "references": [
            "In our proposed framework we begin with an LLM    subscript  italic- \\pi_{\\phi} italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  acting as a zero-shot judge in an RLAIF setting, as outlined in Section  2.3 . That is, given a task  x x x italic_x  and two responses  y 1 subscript y 1 y_{1} italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and  y 2 subscript y 2 y_{2} italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , we directly prompt the model    subscript  italic- \\pi_{\\phi} italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  to provide an answer indicator token  I I I italic_I  indicating a preference over the answers. We consider two variants of our approach:",
            "Our prompts are based on the standard MT-Bench prompt  (Zheng et al.,  2023b ) , and can be found in Appendix  A . We use the LLM judge as a prior and further train it to align with the ground-truth dataset judgements. We begin with the preference dataset  D = { x ( i ) , y 1 ( i ) , y 2 ( i ) , I ( i ) } i = 1 N D superscript subscript superscript x i superscript subscript y 1 i superscript subscript y 2 i superscript I i i 1 N \\mathcal{D}=\\left\\{x^{(i)},y_{1}^{(i)},y_{2}^{(i)},I^{(i)}\\right\\}_{i=1}^{N} caligraphic_D = { italic_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , italic_I start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT  as outlined in Section  2.1.2 , however we consider unranked answers  y 1 ( i ) , y 2 ( i ) superscript subscript y 1 i superscript subscript y 2 i y_{1}^{(i)},y_{2}^{(i)} italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT  and the corresponding winning choice  I ( i ) superscript I i I^{(i)} italic_I start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT . We design several training techniques for the generative reward model   subscript  italic- \\pi_{\\phi} italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT .",
            "we refer to this as a post-rationalization approach, similar to the approach described in Section  2.2.2 .",
            "CoT-GenRM-STaR:  Finally we consider an approach where the model self-bootstraps intermediate reasoning using a STaR approach as outlined in Section  2.2.1 . We will also consider two loss objectives herefollowing the filtering strategy described in the above section, we use the standard SFT loss similar to Eq.  8  on reasoning chains that yield the correct judgement. We denote models trained with this objective as STaR-SFT.",
            "We show our first main set of results in Fig.  2 . All models are trained on the UltraFeedback dataset and evaluated on a held-out split of in-domain data as well as on RewardBench.",
            "We first evaluate the zero-shot performance of the LLaMa 3.1 8B Instruct model as an evaluator using both CoT prompting with self-consistency (LLM-as-a-judge in Figure  2 ) and acting as a classifier directly outputting the response ranking (GenRM (base) in  2 ). We see that using that prompting the model to reason over the answers significantly boosts performance from 52.25% to 67.75% on the UltraFeedback evaluation dataset and from 60.60% to 75.18% accuracy on RewardBench.",
            "Based on the prior experiments we observe that the STaR-DPO model significantly outperforms the base LLM-as-a-judge, but also the GenRM model which does not use reasoning on held out tasks in RewardBench. One major variable is how to generate the reasoning chains used for training. In the experiments described so far rationales were sampled following a STaR-based approach using the same base model. We also evaluate an approach sampling rationales from a post-rationalization model  r  q   ( r | x , y 1 , y 2 , I ) similar-to r subscript q italic- conditional r x subscript y 1 subscript y 2 I r\\sim q_{\\phi}(r|x,y_{1},y_{2},I) italic_r  italic_q start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_r | italic_x , italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_I ) , which are then used for SFT training using Eq.  8 , we refer to this as STaR-Rationalizer. Results from this approach on UltraFeedback are shown in Fig.  2  and for UltraInteract in Fig.  4 . Interestingly we observe that the STaR-Rationalizer model matches the performance of the STaR-DPO model on both datasets, significantly out-performing the STaR-SFT approach, which uses the same training objective. However, we see that this model struggles to generalize to the RewardBench tasks, under-performing not only the STaR-DPO model, but the base LLM as well on both datasets. This is an interesting empirical phenomenon that warrants further study, but we hypothesize that the core issue is that the training rationales from the post-rationalization model are off-policy for the base model, creating a distribution mismatch during training. While the model is able to learn those rationales on the training distribution it fails on more novel tasks where it generates rationales different from those seen during training.",
            "The concept of using language models for providing feedback, also known as constitutional AI or RLAIF  Bai et al. ( 2022b )  has gained significant traction in recent years.  Zheng et al. ( 2023b )  further popularized the paradigm of LLM evaluation (LLM-as-a-judge), demonstrating that strong language models can effectively perform judgments with chain-of-thought (CoT) reasoning that approximate human evaluation. Building on this,  Kim et al. ( 2024 )  proposed Prometheus, employing supervised fine-tuning from a powerful model to provide CoT reasoning and scoring, demonstrating strong evaluation performance from a smaller open model. In the current work we show that zero-shot LLM evaluations may not fully align with human feedback and significant improvements in accuracy can be gained from additional fine-tuning. Moreover, in our proposed approach of combining RLAIF with STaR-based tuning we do not require ground-truth reasoning or supervision from a stronger model. Concurrently with this work,  Zhang et al. ( 2024 )  presented Generative Verifiers, training CoT-GenRM with an SFT objective to act as a verifier for mathematical reasoning. They find similiar observations to our experiments in Sections  5.2  and  5.4 . However, one notable exception is that unlike our approach they rely on access to full reference solutions at training time. Another concurrent work in this direction is  Ankner et al. ( 2024 ) , which combines CoT reasoning generation with Bradley-Terry reward modelling. They present empirical findings on the benefits of reasoning, similar to the ones we show in Section  5.1 , although crucially, they rely on a separate strong model to provide rationales and require the Bradley-Terry reward model hybrid architecture, while we use fully self-bootstrapped rationales in a full language modelling setup without the need for any additional architecture overhang.",
            "Models use the same hyperparameters across each dataset. Table  2  contains our hyperparameter choices for each training method."
        ]
    },
    "id_table_3": {
        "caption": "",
        "table": "A1.T2.1.1.1.1",
        "footnotes": [],
        "references": [
            "We begin by pointing out a theoretical connection between the RLHF post-training approach outlined in the previous section and general preference modeling. One of the key observations of the DPO approach  (Rafailov et al.,  2023 )  is that the reward modeling objective in Eq.  1  is under-constrained, which can create significant optimization challenges for the RL problem in Eq.  3   (Wu et al.,  2023 ; Ahmadian et al.,  2024 )  (in fact, theoretically, it can have arbitrarily low signal-to-noise ratio). To alleviate this issue, prior works  (Stiennon et al.,  2022 ; Ouyang et al.,  2022 )  use a baseline reward from on a fixed reference distribution:",
            "In our proposed framework we begin with an LLM    subscript  italic- \\pi_{\\phi} italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  acting as a zero-shot judge in an RLAIF setting, as outlined in Section  2.3 . That is, given a task  x x x italic_x  and two responses  y 1 subscript y 1 y_{1} italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and  y 2 subscript y 2 y_{2} italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , we directly prompt the model    subscript  italic- \\pi_{\\phi} italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  to provide an answer indicator token  I I I italic_I  indicating a preference over the answers. We consider two variants of our approach:",
            "When we evaluate these models on out-of-distribution tasks (RewardBench) we see that STaR-DPO achieves the strongest result, over both the base model prior (81.9% versus 77.8%) and trained RM with the GenRM having the stronger performance at 78.9%. The STaR-DPO model outperforms or matches baselines across all RewardBench categories. On the other hand, the STaR-SFT model still does not substantially differ from the base model. The GenRM model outperforms the Bradley-Terry RM and PairRM with performance comparable to STaR-DPO across Chat, Chat (Hard) and Reasoning. However, one notable observation is that reasoning-based approaches show significantly stronger performance on the Safety category with the STaR-DPO model achieving 91.0% accuracy versus the best performing PairRM model, which achieves accuracy of 81.8%. Figure  3  demonstrates an interesting case where LLM-as-a-judge fails to accurately judge a general assistant task but the STaR-DPO model suceeds. The STaR-DPO model response recognizes that Assistant As response lacks depth and detail, but correctly recognizes that Assistant Bs response does not follow the instruction and prefers Assistant As response. To the contrary, the LLM-as-a-judge response repeatedly emphasizes how Assistant Bs response is more helpful and detailed, even though the user requests a short list of 2 animals, incorrectly preferring Assistant Bs response."
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "A1.T2.2.2.5.1",
        "footnotes": [],
        "references": [
            "One limitation of bootstrapping rationale generation is that the model cannot improve on examples it initially fails to solve. To address this issue, STaR introduces  rationalization , a backward reasoning process. For prompts where the model generates an incorrect rationale and answer, the correct answer is provided to the model as a hint. The model then generates a new rationale based on the correct answer, reasoning backward to generate a post-rationale. In technical terms there is a rationalization model  q q q italic_q  which generates rationales  r ^ ( i )  q  ( r | x ( i ) , y ( i ) ) similar-to superscript ^ r i q conditional r superscript x i superscript y i \\hat{r}^{(i)}\\sim q(r|x^{(i)},y^{(i)}) over^ start_ARG italic_r end_ARG start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT  italic_q ( italic_r | italic_x start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , italic_y start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT )  to justify the final answer. This synthetic data is in turn used in the STaR objective in Eq.  4 . We will also use this approach to evaluate the effect of the quality of the bootstrapped reasoning chains, using samples from rationalization models  q q q italic_q  with different capabilities.",
            "We also evaluate performance specifically on reasoning tasks by training models on the UltraInteract dataset, which specifically focuses on challenging evaluations of reasoning chains. The experiments in this section focus on  meta -reasoning, i.e. the capability to reason about reasoning steps. Figure  4  shows that the STaR-DPO model outperforms STaR-SFT on the UltraInteract evaluation dataset and achieves a significant improvement of 90.2% versus 68.8% for the base model. This is slightly lower than the explicit RM models, which all achieve comparable performance around 94%.",
            "Based on the prior experiments we observe that the STaR-DPO model significantly outperforms the base LLM-as-a-judge, but also the GenRM model which does not use reasoning on held out tasks in RewardBench. One major variable is how to generate the reasoning chains used for training. In the experiments described so far rationales were sampled following a STaR-based approach using the same base model. We also evaluate an approach sampling rationales from a post-rationalization model  r  q   ( r | x , y 1 , y 2 , I ) similar-to r subscript q italic- conditional r x subscript y 1 subscript y 2 I r\\sim q_{\\phi}(r|x,y_{1},y_{2},I) italic_r  italic_q start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_r | italic_x , italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_I ) , which are then used for SFT training using Eq.  8 , we refer to this as STaR-Rationalizer. Results from this approach on UltraFeedback are shown in Fig.  2  and for UltraInteract in Fig.  4 . Interestingly we observe that the STaR-Rationalizer model matches the performance of the STaR-DPO model on both datasets, significantly out-performing the STaR-SFT approach, which uses the same training objective. However, we see that this model struggles to generalize to the RewardBench tasks, under-performing not only the STaR-DPO model, but the base LLM as well on both datasets. This is an interesting empirical phenomenon that warrants further study, but we hypothesize that the core issue is that the training rationales from the post-rationalization model are off-policy for the base model, creating a distribution mismatch during training. While the model is able to learn those rationales on the training distribution it fails on more novel tasks where it generates rationales different from those seen during training.",
            "The concept of using language models for providing feedback, also known as constitutional AI or RLAIF  Bai et al. ( 2022b )  has gained significant traction in recent years.  Zheng et al. ( 2023b )  further popularized the paradigm of LLM evaluation (LLM-as-a-judge), demonstrating that strong language models can effectively perform judgments with chain-of-thought (CoT) reasoning that approximate human evaluation. Building on this,  Kim et al. ( 2024 )  proposed Prometheus, employing supervised fine-tuning from a powerful model to provide CoT reasoning and scoring, demonstrating strong evaluation performance from a smaller open model. In the current work we show that zero-shot LLM evaluations may not fully align with human feedback and significant improvements in accuracy can be gained from additional fine-tuning. Moreover, in our proposed approach of combining RLAIF with STaR-based tuning we do not require ground-truth reasoning or supervision from a stronger model. Concurrently with this work,  Zhang et al. ( 2024 )  presented Generative Verifiers, training CoT-GenRM with an SFT objective to act as a verifier for mathematical reasoning. They find similiar observations to our experiments in Sections  5.2  and  5.4 . However, one notable exception is that unlike our approach they rely on access to full reference solutions at training time. Another concurrent work in this direction is  Ankner et al. ( 2024 ) , which combines CoT reasoning generation with Bradley-Terry reward modelling. They present empirical findings on the benefits of reasoning, similar to the ones we show in Section  5.1 , although crucially, they rely on a separate strong model to provide rationales and require the Bradley-Terry reward model hybrid architecture, while we use fully self-bootstrapped rationales in a full language modelling setup without the need for any additional architecture overhang."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "A1.T2.2.2.6.1",
        "footnotes": [],
        "references": [
            "The  core contribution  of our work is that we replace the Bradley-Terry reward modelling approach with a strictly more general preference modelling objective  p  ( y w  y l | x ) p succeeds subscript y w conditional subscript y l x p(y_{w}\\succ y_{l}|x) italic_p ( italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT  italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT | italic_x )  that does not assume a point-wise reward estimate, a special model architecture, or a specific preference distribution parameterization as in Eq.  1 . That is, we assume a standard preference dataset and model the preference distribution  p   ( y w  y l | x ) subscript p italic- succeeds subscript y w conditional subscript y l x p_{\\phi}(y_{w}\\succ y_{l}|x) italic_p start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT  italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT | italic_x )  using an LLM, without any additional assumptions. Notice that this formulation is fully general as we can extract preference probabilities either from the likelihoods of the LLM output or from majority voting counts. This can be used in the standard pipeline with PPO  (Stiennon et al.,  2022 ; Ouyang et al.,  2022 )  in Eq.  5  using the reward formulation in Eq.  6 . Alternatively, if we sample preferences from the above model, we can also utilize an iterative or online *PO optimization manner following  Munos et al. ( 2024 )  or  Calandriello et al. ( 2024 ) .",
            "In addition to the previously discussed benefits, using LLMs as reward models also allows us to utilize additional inference time compute to improve performance. Indeed our results from the previous sections show that using COT prompting to induce reasoning in the evaluator model can significantly improve performance on new prompts and tasks over the base GenRM approach (which does not use CoT). In this section we show further results on accuracy using self-consistency and majority voting. Our results are shown in Fig.  5 . We see that majority voting at 32 improves performance consistently and adds 1.6% accuracy on the UltraFeedback Dataset and 3.8% on RewardBench in that case. On UltraInteract majority voting improves performance by 4.6% and 4.9% on RewardBench. This indicates that System 2 types of reasoning approaches can significantly improve the accuracy of the critic model. We believe using models with strong reasoning to provide feedback and evaluation to other models is a promising direction.",
            "The concept of using language models for providing feedback, also known as constitutional AI or RLAIF  Bai et al. ( 2022b )  has gained significant traction in recent years.  Zheng et al. ( 2023b )  further popularized the paradigm of LLM evaluation (LLM-as-a-judge), demonstrating that strong language models can effectively perform judgments with chain-of-thought (CoT) reasoning that approximate human evaluation. Building on this,  Kim et al. ( 2024 )  proposed Prometheus, employing supervised fine-tuning from a powerful model to provide CoT reasoning and scoring, demonstrating strong evaluation performance from a smaller open model. In the current work we show that zero-shot LLM evaluations may not fully align with human feedback and significant improvements in accuracy can be gained from additional fine-tuning. Moreover, in our proposed approach of combining RLAIF with STaR-based tuning we do not require ground-truth reasoning or supervision from a stronger model. Concurrently with this work,  Zhang et al. ( 2024 )  presented Generative Verifiers, training CoT-GenRM with an SFT objective to act as a verifier for mathematical reasoning. They find similiar observations to our experiments in Sections  5.2  and  5.4 . However, one notable exception is that unlike our approach they rely on access to full reference solutions at training time. Another concurrent work in this direction is  Ankner et al. ( 2024 ) , which combines CoT reasoning generation with Bradley-Terry reward modelling. They present empirical findings on the benefits of reasoning, similar to the ones we show in Section  5.1 , although crucially, they rely on a separate strong model to provide rationales and require the Bradley-Terry reward model hybrid architecture, while we use fully self-bootstrapped rationales in a full language modelling setup without the need for any additional architecture overhang."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "A2.T3.1",
        "footnotes": [],
        "references": [
            "The  core contribution  of our work is that we replace the Bradley-Terry reward modelling approach with a strictly more general preference modelling objective  p  ( y w  y l | x ) p succeeds subscript y w conditional subscript y l x p(y_{w}\\succ y_{l}|x) italic_p ( italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT  italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT | italic_x )  that does not assume a point-wise reward estimate, a special model architecture, or a specific preference distribution parameterization as in Eq.  1 . That is, we assume a standard preference dataset and model the preference distribution  p   ( y w  y l | x ) subscript p italic- succeeds subscript y w conditional subscript y l x p_{\\phi}(y_{w}\\succ y_{l}|x) italic_p start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT  italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT | italic_x )  using an LLM, without any additional assumptions. Notice that this formulation is fully general as we can extract preference probabilities either from the likelihoods of the LLM output or from majority voting counts. This can be used in the standard pipeline with PPO  (Stiennon et al.,  2022 ; Ouyang et al.,  2022 )  in Eq.  5  using the reward formulation in Eq.  6 . Alternatively, if we sample preferences from the above model, we can also utilize an iterative or online *PO optimization manner following  Munos et al. ( 2024 )  or  Calandriello et al. ( 2024 ) .",
            "Our prompts follow  Zheng et al. ( 2023a )  with the following differences. For our LLM-as-a-judge and STaR (Figure  6 ), we remove the tie. For GenRM (Figure  8 ), we remove the brackets, and remove the explanation request in the system prompt. For Rationalizer (Figure  7 ), we remove the output instructions from the system prompt, and add a prompt to explain why A or B is better."
        ]
    }
}