{
    "id_table_1": {
        "caption": "Table 1:  Number of final examples for each LLM. The difference between LLMs is due to their different outputs going through the framework.",
        "table": "S4.T1.3.3",
        "footnotes": [],
        "references": [
            "Inspired by  Zhao et al. ( 2024 ) , we use the consistency of answers to different paraphrases of the same question  Q Q Q italic_Q  to measure the LLMs memory strength  S Q subscript S Q S_{Q} italic_S start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT  for the knowledge  K Q subscript K Q K_{Q} italic_K start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT  associated with the question. This method is motivated by the intuition that if an LLM does not have a strong memory of a question, it often produces varying answers when presented with different paraphrases but semantically equivalent questions, as shown in Table  8  in Appendix. In contrast, it can produce consistent answers if the LLM has a strong memory of a question. The process involves two key steps: First, several paraphrased versions of the original question are generated with ChatGPT 1 1 1 https://platform.openai.com/docs/models/gpt-3-5-turbo, the specific version is 0125. , and the answers to those paraphrased questions are clustered (Section  3.3.1 ). Then, memory strength  S Q subscript S Q S_{Q} italic_S start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT  is calculated using answer consistency (Section  3.3.2 ).",
            "For the popQA dataset, both MA and CMA are obtained following the method described in  Xie et al. ( 2024 ) . For the MA of the NQ dataset, we also use a closed-book approach, similar to  Xie et al. ( 2024 ) . While, the process for generating CMA differs.  Unlike the popQA dataset, the NQ dataset does not provide relation types for the questions or offer sets of subject and object entities for substitution.  To address this issue, we propose an approach using an LLM to substitute entities in MA to generate CMA. First, we identify which wh- question type 3 3 3 which refers to what, when, where, who, whom, which, whose, why, and how.  the question belongs to using string matching. Then, based on the question type, we determine the type of entity to be replaced in the MA. Finally, we use an LLM to make the substitution. For example, in Figure  1 , the question how many episodes... is of the type how_many, so the entity to be replaced in the MA there are 23 episodes... should be a  NUMBER . We let ChatGPT perform the substitution with an alternative entity. The prompt used is shown in Table  9  (index 5). We have the detailed description for generating CMA in Appendex  B.1 .",
            "CMA filter . As noted in Section  3.3 , LLMs can produce multiple MAs for the same question. To ensure the CMA conflicts with MAs, we require that the CMA is different from any of the answers  { A 1 ,  , A n } subscript A 1  subscript A n \\{A_{1},\\cdots,A_{n}\\} { italic_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,  , italic_A start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }  generated in Section  3.3.1 , so the alternative entity should not appear in MAs. For the popQA dataset, the alternative entity is known. For NQ dataset, we first identify the alternative entity in the CMA by comparing the MA and CMA, and then check if this entity appears in any of the MAs  { A 1 ,  , A n } subscript A 1  subscript A n \\{A_{1},\\cdots,A_{n}\\} { italic_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,  , italic_A start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } . We filter out data samples whose CMA does not conflict with MAs.",
            "Direct evidence  is a semantically equivalent statement of the CMA, providing the clearest support for the claim made by the CMA. We generate the direct evidence by paraphrasing the CMA with ChatGPT, following the prompt shown in Table  9  (index 6). For example, in Figure  1 , the CMA there are 15 episodes in Chicago Fire season 4 is paraphrased to season 4 of Chicago Fire consists of a total of 15 episodes. These two statements are semantically equivalent.",
            "Indirect evidence  differs from direct evidence by adding extra details that provide a more thorough description of the subject related to the CMA. This additional information makes the evidence more comprehensive and might be more persuasive. For example, in Figure  1 , the indirect evidence includes details not found in the counter answer, such as the title of the first episode and its release date, along with the fact that there are 15 episodes in total. The prompt to generate indirect evidence is shown in Table  9  (index 7).",
            "Table  1  presents the final number of instances used for evaluation. We observe a slight difference in the quantities of questions with direct and indirect evidence since it is easier for ChatGPT to generate direct evidence that meets our requirements. The specific number of instances at each step in evidence generation is detailed in Table  7  in the Appendix. Due to the quantity difference between direct evidence and indirect evidence, we divide the styles of evidence into two groups: Group 1 includes direct evidence and direct + paraphrase evidence. Group 2 includes indirect evidence and direct + indirect evidence. Each group has different Direct Evidence results serving as baselines.",
            "Identity Question Type:  We first build a typing tree using rules to categorize questions. Figure  5  illustrates the typing tree, which consists of a two-layer structure. In the typing process, we first determine if a question begins with one of the following words: what, when, where, which, who, why, or how. If it does, the question is categorized accordingly; if not, it is classified as other. However, this approach can still group different types of questions together. To address this, we use a second layer to refine the typing by analyzing two specific words in the question. For example, the question shown in Figure  1  falls into the how_many category."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Results of LLM Receptiveness to Different Evidence Styles Across NQ and popQA Datasets.  The table presents the MA ratio ( R m subscript R m R_{m} italic_R start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ), CMA ratio ( R c subscript R c R_{c} italic_R start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ), and uncertain answer ratio ( R u subscript R u R_{u} italic_R start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ) for various evidence styles across four models. All the ratios are in %.",
        "table": "S4.T2.12.12",
        "footnotes": [],
        "references": [
            "Inspired by  Zhao et al. ( 2024 ) , we use the consistency of answers to different paraphrases of the same question  Q Q Q italic_Q  to measure the LLMs memory strength  S Q subscript S Q S_{Q} italic_S start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT  for the knowledge  K Q subscript K Q K_{Q} italic_K start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT  associated with the question. This method is motivated by the intuition that if an LLM does not have a strong memory of a question, it often produces varying answers when presented with different paraphrases but semantically equivalent questions, as shown in Table  8  in Appendix. In contrast, it can produce consistent answers if the LLM has a strong memory of a question. The process involves two key steps: First, several paraphrased versions of the original question are generated with ChatGPT 1 1 1 https://platform.openai.com/docs/models/gpt-3-5-turbo, the specific version is 0125. , and the answers to those paraphrased questions are clustered (Section  3.3.1 ). Then, memory strength  S Q subscript S Q S_{Q} italic_S start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT  is calculated using answer consistency (Section  3.3.2 ).",
            "Next, LLMs answer the paraphrased questions  { P 1 ,  , P n } Q subscript subscript P 1  subscript P n Q \\{P_{1},\\cdots,P_{n}\\}_{Q} { italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,  , italic_P start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT  in a closed-book setting. We denote the answers as  { A 1 ,  , A n } Q subscript subscript A 1  subscript A n Q \\{A_{1},\\cdots,A_{n}\\}_{Q} { italic_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,  , italic_A start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT . The answers are grouped into several clusters based on their consistency. The clustering is done by checking answers incrementally. If an answer matches any answer within an existing cluster, this answer is added to this cluster; if not, a new cluster is created with this answer. We use an LLM 2  to determine whether two answers are consistent. The prompt used for this answer inconsistency detection is shown in Table  9  (index 3). We denote the clusters for question  Q Q Q italic_Q  as  { c 1 ,  , c m } Q subscript subscript c 1  subscript c m Q \\{c_{1},\\cdots,c_{m}\\}_{Q} { italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,  , italic_c start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT .",
            "In this study, we aim to investigate two key research questions. 1) Whether memory strength has an impact on the context faithfulness of LLMs. 2) If the style of evidence affects the context faithfulness of LLMs. These research questions are explored in Section  4.2  and  4.3 , respectively. We also provide additional studies in Appendix  A , which includes a study about the impact of option order and a case study.",
            "We first illustrate the distributions of memory strength across the popQA and NQ datasets for LLaMA2.7B, LLaMA2.70B, ChatGPT, and GPT-4, respectively (shown in Figure  2 ). From the analysis, two key insights can be drawn.",
            "Table  2  shows the results of different evidence styles. We can make the following observations and conclusions.",
            "The higher  R m subscript R m R_{m} italic_R start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT  of LLaMA2.7B may be attributed to its weakness of reasoning ability .  From Table 2, we observe that on the NQ dataset, the  R m subscript R m R_{m} italic_R start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT  of LLaMA2.7B is higher than that of LLaMA2.70B. This phenomenon is counter-intuitive, as a weaker memory strength in LLMs corresponds to a lower  R m subscript R m R_{m} italic_R start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT  for the other three LLMs (LLaMA2.70B, ChatGPT, and GPT-4). To investigate the cause of this issue, we prompt LLaMA2.7B to provide a rationale alongside the answer. We discover that LLaMA2.7B seems to exhibit reasoning errors. Below is an example that shows this phenomenon."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Results of LLM Receptiveness to Different Evidence Styles Across NQ and popQA Datasets.  The table presents the MA ratio ( R m subscript R m R_{m} italic_R start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ), CMA ratio ( R c subscript R c R_{c} italic_R start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ), and UCT ratio ( R u subscript R u R_{u} italic_R start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ) for Direct Evidence and Direct + Paraphrase Evidence with CMA first and MA first scenarios. All the ratios are in %.",
        "table": "A1.T3.12.12",
        "footnotes": [],
        "references": [
            "Inspired by  Zhao et al. ( 2024 ) , we use the consistency of answers to different paraphrases of the same question  Q Q Q italic_Q  to measure the LLMs memory strength  S Q subscript S Q S_{Q} italic_S start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT  for the knowledge  K Q subscript K Q K_{Q} italic_K start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT  associated with the question. This method is motivated by the intuition that if an LLM does not have a strong memory of a question, it often produces varying answers when presented with different paraphrases but semantically equivalent questions, as shown in Table  8  in Appendix. In contrast, it can produce consistent answers if the LLM has a strong memory of a question. The process involves two key steps: First, several paraphrased versions of the original question are generated with ChatGPT 1 1 1 https://platform.openai.com/docs/models/gpt-3-5-turbo, the specific version is 0125. , and the answers to those paraphrased questions are clustered (Section  3.3.1 ). Then, memory strength  S Q subscript S Q S_{Q} italic_S start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT  is calculated using answer consistency (Section  3.3.2 ).",
            "CMA filter . As noted in Section  3.3 , LLMs can produce multiple MAs for the same question. To ensure the CMA conflicts with MAs, we require that the CMA is different from any of the answers  { A 1 ,  , A n } subscript A 1  subscript A n \\{A_{1},\\cdots,A_{n}\\} { italic_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,  , italic_A start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }  generated in Section  3.3.1 , so the alternative entity should not appear in MAs. For the popQA dataset, the alternative entity is known. For NQ dataset, we first identify the alternative entity in the CMA by comparing the MA and CMA, and then check if this entity appears in any of the MAs  { A 1 ,  , A n } subscript A 1  subscript A n \\{A_{1},\\cdots,A_{n}\\} { italic_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,  , italic_A start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } . We filter out data samples whose CMA does not conflict with MAs.",
            "In this study, we aim to investigate two key research questions. 1) Whether memory strength has an impact on the context faithfulness of LLMs. 2) If the style of evidence affects the context faithfulness of LLMs. These research questions are explored in Section  4.2  and  4.3 , respectively. We also provide additional studies in Appendix  A , which includes a study about the impact of option order and a case study.",
            "To demonstrate the relationship between context-faithfulness and memory strength, we categorize the questions in each dataset into four groups according to memory strength for each LLM. The four groups are low, mid-low, mid-high, and high, corresponding to the memory strength intervals [-2, -1], (-1, -0.5], (-0.5, -0.25] and [-0.25, 0], respectively. We use the direct evidence. The results are shown in Figure  3 6 6 6 We put results for other evidence styles in Appendix, Figure  6 ,  7 ,  8 . The conclusion is consistent. . Figure  3  (a)(b) shows the ratios of questions with MA, CMA, and UCT answers for the NQ and popQA datasets, respectively. Note that different LLMs may have different memory strengths to the same question. Therefore, the count of questions and the specific questions within the same group can vary across different LLMs. To illustrate this, we present the count of questions in each group (low, mid-low, mid-high, and high) in Figure  3  (c)(d) for popQA and NQ datasets, respectively. We can draw the following conclusions.",
            "There is a clear positive correlation between memory strength and MA ratio . From Figure  3  (a)(b), it is obvious that as memory strength increases, the ratio of MA (red) increases significantly, while the proportion of CMA (blue) decreases. This trend is consistent across both datasets, but it is more obvious in the NQ dataset, especially for larger models such as GPT-4 and ChatGPT.",
            "In general, the LLaMA models are more context-faithful than the GPT models . Comparing the LLaMA models (7B and 70B) with the GPT models (ChatGPT and GPT-4) in Figure  3  (a)(b), the LLaMA models have a lower MA ratio and a higher CMA ratio. Even within groups of the same memory strength (low or high), the LLaMA models demonstrate more context-faithful performance.",
            "To further demonstrate the effect of the order of options on  R m subscript R m R_{m} italic_R start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT , we compare the performance of experiments with CMA first and MA first under two evidence styles: direct evidence with one sentence and direct + paraphrase with three sentences. The results are presented in Table  3 . The results show that, for different evidence styles,  R m subscript R m R_{m} italic_R start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT  is consistently higher in the CMA first compared to the MA first. Comparing the results under the CMA first, the  R m subscript R m R_{m} italic_R start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT  of direct + paraphrase with three sentences is significantly lower than that with direct evidence with one sentence. This demonstrates that paraphrasing direct evidence is an effective method for decreasing  R m subscript R m R_{m} italic_R start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT . Our conclusion remains unchanged."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Distribution of Question Types and their Counts",
        "table": "A2.T4.1.1",
        "footnotes": [],
        "references": [
            "To ensure the reliability of indirect evidence, the indirect evidence should entail the CMA, and the additional information introduced by the evidence should not entail the MA. Otherwise, the indirect evidence can support both the MA and CMA. The NLI model 4  is used to verify that indirect evidence entailed with CMA and neutral or contradictory with MA.",
            "In this study, we aim to investigate two key research questions. 1) Whether memory strength has an impact on the context faithfulness of LLMs. 2) If the style of evidence affects the context faithfulness of LLMs. These research questions are explored in Section  4.2  and  4.3 , respectively. We also provide additional studies in Appendix  A , which includes a study about the impact of option order and a case study.",
            "To test the effect of the order of options on  R m subscript R m R_{m} italic_R start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT , we conduct an experiment with one sentence direct evidence by changing the order of options (MA option and CMA option). We define the scenario where the CMA option is presented first in the prompt as CMA first, and the scenario where the MA option is presented first as MA first 8 8 8 All previous evaluations are under MA first conditions. . Figure  4  shows the results."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Question Types and Their Corresponding Key Terms",
        "table": "A2.T5.1.1",
        "footnotes": [],
        "references": [
            "Identity Question Type:  We first build a typing tree using rules to categorize questions. Figure  5  illustrates the typing tree, which consists of a two-layer structure. In the typing process, we first determine if a question begins with one of the following words: what, when, where, which, who, why, or how. If it does, the question is categorized accordingly; if not, it is classified as other. However, this approach can still group different types of questions together. To address this, we use a second layer to refine the typing by analyzing two specific words in the question. For example, the question shown in Figure  1  falls into the how_many category.",
            "Determine Entity Type in MA to Change:  After categorizing the questions, we determine the entity type in MA needs to be replaced. To achieve this, we give each type of question an entity type, and many questions can share the same entity type. For example, both when and what year ask for a time. So a time entity in MA should be substituted. The final set of entity types is summarized in Table  5 . We do not process questions starting with what, which or how due to the lack of a unified entity type for these questions. Table  6  shows the statistics of the unprocessed questions."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Summary of Excluded Question Types in Memory Answer and Counter Answer Generation.  The table lists question types that were excluded from processing due to either the difficulty in identifying a unified entity type (how, what, which) or poor question quality (other).",
        "table": "A2.T6.1.1",
        "footnotes": [],
        "references": [
            "To demonstrate the relationship between context-faithfulness and memory strength, we categorize the questions in each dataset into four groups according to memory strength for each LLM. The four groups are low, mid-low, mid-high, and high, corresponding to the memory strength intervals [-2, -1], (-1, -0.5], (-0.5, -0.25] and [-0.25, 0], respectively. We use the direct evidence. The results are shown in Figure  3 6 6 6 We put results for other evidence styles in Appendix, Figure  6 ,  7 ,  8 . The conclusion is consistent. . Figure  3  (a)(b) shows the ratios of questions with MA, CMA, and UCT answers for the NQ and popQA datasets, respectively. Note that different LLMs may have different memory strengths to the same question. Therefore, the count of questions and the specific questions within the same group can vary across different LLMs. To illustrate this, we present the count of questions in each group (low, mid-low, mid-high, and high) in Figure  3  (c)(d) for popQA and NQ datasets, respectively. We can draw the following conclusions.",
            "Determine Entity Type in MA to Change:  After categorizing the questions, we determine the entity type in MA needs to be replaced. To achieve this, we give each type of question an entity type, and many questions can share the same entity type. For example, both when and what year ask for a time. So a time entity in MA should be substituted. The final set of entity types is summarized in Table  5 . We do not process questions starting with what, which or how due to the lack of a unified entity type for these questions. Table  6  shows the statistics of the unprocessed questions.",
            "To demonstrate the relationship between context-faithfulness and memory strength with other evidence styles, we categorize the questions in each dataset into four groups according to the memory strength intervals [-2, -1], (-1, -0.5], (-0.5, -0.25] and [-0.25, 0], The evidence styles are direct + paraphrase evidence with two sentences and indirect evidence with two sentences. Figures  6 , 7  show the result. The figures show that there is a clear positive correlation between memory strength and MA ratio for both evidence styles, which implies that this positive correlation between memory strength and MA ratio is general."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  The dataset scale at each step across popQA and NQ dataset. intersection of 2&3 sentence evidence is the count for indirect evidence.",
        "table": "A2.T7.1.1",
        "footnotes": [],
        "references": [
            "To demonstrate the relationship between context-faithfulness and memory strength, we categorize the questions in each dataset into four groups according to memory strength for each LLM. The four groups are low, mid-low, mid-high, and high, corresponding to the memory strength intervals [-2, -1], (-1, -0.5], (-0.5, -0.25] and [-0.25, 0], respectively. We use the direct evidence. The results are shown in Figure  3 6 6 6 We put results for other evidence styles in Appendix, Figure  6 ,  7 ,  8 . The conclusion is consistent. . Figure  3  (a)(b) shows the ratios of questions with MA, CMA, and UCT answers for the NQ and popQA datasets, respectively. Note that different LLMs may have different memory strengths to the same question. Therefore, the count of questions and the specific questions within the same group can vary across different LLMs. To illustrate this, we present the count of questions in each group (low, mid-low, mid-high, and high) in Figure  3  (c)(d) for popQA and NQ datasets, respectively. We can draw the following conclusions.",
            "Table  1  presents the final number of instances used for evaluation. We observe a slight difference in the quantities of questions with direct and indirect evidence since it is easier for ChatGPT to generate direct evidence that meets our requirements. The specific number of instances at each step in evidence generation is detailed in Table  7  in the Appendix. Due to the quantity difference between direct evidence and indirect evidence, we divide the styles of evidence into two groups: Group 1 includes direct evidence and direct + paraphrase evidence. Group 2 includes indirect evidence and direct + indirect evidence. Each group has different Direct Evidence results serving as baselines.",
            "For the popQA dataset, we use the dataset from  Xie et al. ( 2024 )  by randomly selecting 1,000 questions from the data intersection of the conflicts generated by LLaMA2.7B, LLaMA2.70B, ChatGPT, and GPT-4. We use the MA and CMA from  Xie et al. ( 2024 )  and only generate direct evidence and indirect evidence using our framework. For the NQ dataset, we use the test set from  Longpre et al. ( 2021 ) , which consists of 1,667 unique questions. The MA, CMA, and evidence are all generated with our framework. The dataset scale at each step is presented in Table  7 .",
            "To demonstrate the relationship between context-faithfulness and memory strength with other evidence styles, we categorize the questions in each dataset into four groups according to the memory strength intervals [-2, -1], (-1, -0.5], (-0.5, -0.25] and [-0.25, 0], The evidence styles are direct + paraphrase evidence with two sentences and indirect evidence with two sentences. Figures  6 , 7  show the result. The figures show that there is a clear positive correlation between memory strength and MA ratio for both evidence styles, which implies that this positive correlation between memory strength and MA ratio is general."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Origin Question, Paraphrased Questions, and Corresponding Answers",
        "table": "A2.T8.1.1",
        "footnotes": [],
        "references": [
            "Inspired by  Zhao et al. ( 2024 ) , we use the consistency of answers to different paraphrases of the same question  Q Q Q italic_Q  to measure the LLMs memory strength  S Q subscript S Q S_{Q} italic_S start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT  for the knowledge  K Q subscript K Q K_{Q} italic_K start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT  associated with the question. This method is motivated by the intuition that if an LLM does not have a strong memory of a question, it often produces varying answers when presented with different paraphrases but semantically equivalent questions, as shown in Table  8  in Appendix. In contrast, it can produce consistent answers if the LLM has a strong memory of a question. The process involves two key steps: First, several paraphrased versions of the original question are generated with ChatGPT 1 1 1 https://platform.openai.com/docs/models/gpt-3-5-turbo, the specific version is 0125. , and the answers to those paraphrased questions are clustered (Section  3.3.1 ). Then, memory strength  S Q subscript S Q S_{Q} italic_S start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT  is calculated using answer consistency (Section  3.3.2 ).",
            "To demonstrate the relationship between context-faithfulness and memory strength, we categorize the questions in each dataset into four groups according to memory strength for each LLM. The four groups are low, mid-low, mid-high, and high, corresponding to the memory strength intervals [-2, -1], (-1, -0.5], (-0.5, -0.25] and [-0.25, 0], respectively. We use the direct evidence. The results are shown in Figure  3 6 6 6 We put results for other evidence styles in Appendix, Figure  6 ,  7 ,  8 . The conclusion is consistent. . Figure  3  (a)(b) shows the ratios of questions with MA, CMA, and UCT answers for the NQ and popQA datasets, respectively. Note that different LLMs may have different memory strengths to the same question. Therefore, the count of questions and the specific questions within the same group can vary across different LLMs. To illustrate this, we present the count of questions in each group (low, mid-low, mid-high, and high) in Figure  3  (c)(d) for popQA and NQ datasets, respectively. We can draw the following conclusions.",
            "To demonstrate the relationship between context-faithfulness and memory strength with CMA first scenario, we show MA, CMA, and UCT ratios with direct evidence with one sentence under CMA first scenario in Figure  8 . The positive correlation between memory strength and MA ratio stays unchanged."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Prompts for LLMs in this paper. [PLACEHOLDER] is the corresponding input.",
        "table": "A3.T9.1.1",
        "footnotes": [],
        "references": [
            "The prompt used for paraphrasing the question is provided in Tabel  9  (index 1) in the Appendix.  For each question  Q Q Q italic_Q , we generate  n n n italic_n  paraphrases  { P 1 ,  , P n } Q subscript subscript P 1  subscript P n Q \\{P_{1},\\cdots,P_{n}\\}_{Q} { italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,  , italic_P start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT . For the NQ dataset, we paraphrase the question in each data sample directly. For the popQA dataset, we paraphrase the question template for each relation type since all questions of the same relation type share the same question template. To ensure the paraphrased questions are proper to use, we check if two paraphrased questions are semantically equivalent with an LLM 2 2 2 https://huggingface.co/meta-llama/Meta-Llama-3.1-8B . The prompt for this semantic equivalence detection is provided in Table  9  (index 2). For any paraphrase that is deemed unequivalent, we ask the LLM to re-generate it until a satisfactory version is produced.",
            "Next, LLMs answer the paraphrased questions  { P 1 ,  , P n } Q subscript subscript P 1  subscript P n Q \\{P_{1},\\cdots,P_{n}\\}_{Q} { italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,  , italic_P start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT  in a closed-book setting. We denote the answers as  { A 1 ,  , A n } Q subscript subscript A 1  subscript A n Q \\{A_{1},\\cdots,A_{n}\\}_{Q} { italic_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,  , italic_A start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT . The answers are grouped into several clusters based on their consistency. The clustering is done by checking answers incrementally. If an answer matches any answer within an existing cluster, this answer is added to this cluster; if not, a new cluster is created with this answer. We use an LLM 2  to determine whether two answers are consistent. The prompt used for this answer inconsistency detection is shown in Table  9  (index 3). We denote the clusters for question  Q Q Q italic_Q  as  { c 1 ,  , c m } Q subscript subscript c 1  subscript c m Q \\{c_{1},\\cdots,c_{m}\\}_{Q} { italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,  , italic_c start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT .",
            "For the popQA dataset, both MA and CMA are obtained following the method described in  Xie et al. ( 2024 ) . For the MA of the NQ dataset, we also use a closed-book approach, similar to  Xie et al. ( 2024 ) . While, the process for generating CMA differs.  Unlike the popQA dataset, the NQ dataset does not provide relation types for the questions or offer sets of subject and object entities for substitution.  To address this issue, we propose an approach using an LLM to substitute entities in MA to generate CMA. First, we identify which wh- question type 3 3 3 which refers to what, when, where, who, whom, which, whose, why, and how.  the question belongs to using string matching. Then, based on the question type, we determine the type of entity to be replaced in the MA. Finally, we use an LLM to make the substitution. For example, in Figure  1 , the question how many episodes... is of the type how_many, so the entity to be replaced in the MA there are 23 episodes... should be a  NUMBER . We let ChatGPT perform the substitution with an alternative entity. The prompt used is shown in Table  9  (index 5). We have the detailed description for generating CMA in Appendex  B.1 .",
            "Direct evidence  is a semantically equivalent statement of the CMA, providing the clearest support for the claim made by the CMA. We generate the direct evidence by paraphrasing the CMA with ChatGPT, following the prompt shown in Table  9  (index 6). For example, in Figure  1 , the CMA there are 15 episodes in Chicago Fire season 4 is paraphrased to season 4 of Chicago Fire consists of a total of 15 episodes. These two statements are semantically equivalent.",
            "Indirect evidence  differs from direct evidence by adding extra details that provide a more thorough description of the subject related to the CMA. This additional information makes the evidence more comprehensive and might be more persuasive. For example, in Figure  1 , the indirect evidence includes details not found in the counter answer, such as the title of the first episode and its release date, along with the fact that there are 15 episodes in total. The prompt to generate indirect evidence is shown in Table  9  (index 7).",
            "Evidence Styles.   We formulate four types of evidence styles:   1)  Direct Evidence. This is the most straightforward form of evidence and serves as our baseline. To assess the impact of evidence length, we also create versions where the direct evidence is repeated twice and three times for comparison.   2)  Direct Evidence Combined with Paraphrases of CMA (Direct + Paraphrase). To examine the effect of evidence phrasing and expression, we combine the direct evidence with one paraphrase of the CMA to form a two-sentence evidence and with two paraphrases to form a three-sentence evidence.   3)  Indirect Evidence. We generate indirect evidence consisting of two sentences and three sentences, respectively 7 7 7 We regulate the length of the generated evidence to control the influence of evidence length. The prompts used are detailed in Table  9  (index 7) in Appendix. .   4)  Direct Evidence Combined with Indirect Evidence (Direct + Indirect). We combine the direct evidence with the first sentence of the two-sentence indirect evidence to form a two-sentence evidence and with both sentences to form a three-sentence evidence.",
            "Generate CMA with LLMs:  Instead of manually editing the MA, we leverage the LLMs ability to generate CMA by providing it with a carefully designed prompt, which is shown in Table  9  (index 5). This prompt instructs the LLM to replace the entity with a certain type in the MA (from Step 2) with an alternative, ensuring the generated CMA differs from the MA.",
            "In Table  9 , we present a detailed list of all the prompts used throughout this study."
        ]
    },
    "global_footnotes": [
        "https://platform.openai.com/docs/models/gpt-3-5-turbo, the specific version is 0125.",
        "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B",
        "which refers to what, when, where, who, whom, which, whose, why, and how.",
        "https://huggingface.co/microsoft/deberta-v2-xxlarge-mnli",
        "shows that answer consistency between free-form and multi-choice are 94%, 96% and 92% for ChatGPT, GPT-4 and LLaMA2.7B, respectively.",
        "We put results for other evidence styles in Appendix, Figure",
        ",",
        ",",
        ". The conclusion is consistent.",
        "We regulate the length of the generated evidence to control the influence of evidence length. The prompts used are detailed in Table",
        "(index 7) in Appendix.",
        "All previous evaluations are under MA first conditions.",
        "https://huggingface.co/microsoft/deberta-v2-xxlarge-mnli"
    ]
}