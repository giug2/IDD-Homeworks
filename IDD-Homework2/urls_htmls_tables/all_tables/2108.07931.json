{
    "PAPER'S NUMBER OF TABLES": 2,
    "S4.T1": {
        "caption": "Table 1: MovieLens 1M Dataset Statistics. â€˜Eâ€™ is short for â€˜Examplesâ€™ and â€˜Uâ€™ is short for â€˜Usersâ€™.",
        "table": "<table id=\"S4.T1.1\" class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"></th>\n<th id=\"S4.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Num</th>\n</tr>\n<tr id=\"S4.T1.1.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\">Ratings</th>\n<th id=\"S4.T1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">1,000,209</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.1.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Users</th>\n<td id=\"S4.T1.1.3.1.2\" class=\"ltx_td ltx_align_center\">6040</td>\n</tr>\n<tr id=\"S4.T1.1.4.2\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Movies</th>\n<td id=\"S4.T1.1.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">3952</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Dataset:  As shown in Table 1, the MovieLens 1M dataset contains approximately 1 million ratings from 6040 users on 3952 movies. Examples are created by taking moving \"windows\" of the movie sequence (sorted by timestamps) for each user, resulting in context inputs containing ten movie IDs and item inputs representing one next movie ID. For centralized training, examples are randomly shuffled across all users and split to train and test datasets. The train dataset has 894,752 examples, and the test dataset has 99,417 examples. We refer to these as centralized datasets later in this section. For federated training, all examples are grouped by user, forming a natural data partitioning across clients. The train and test examples are split by user ids, resulting in 4832 train, 603 validation, and 605 test users. We refer to these as federated datasets. We sample 100 clients for each training round."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: An overall recall comparison between centralized, improved centralized, and federated models. The performance drop is calculated as (Rcâˆ’Rf)/Rcsubscriptğ‘…ğ‘subscriptğ‘…ğ‘“subscriptğ‘…ğ‘(R_{c}-R_{f})/R_{c}, where Rcsubscriptğ‘…ğ‘R_{c} is the centralized global recall, and Rfsubscriptğ‘…ğ‘“R_{f} is the federated global recall.",
        "table": "<table id=\"S4.T2.7\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.7.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.7.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span id=\"S4.T2.7.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.1.1.1.1.1\" class=\"ltx_p\" style=\"width:13.0pt;\"></span>\n</span>\n</th>\n<th id=\"S4.T2.7.1.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span id=\"S4.T2.7.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.1.1.2.1.1\" class=\"ltx_p\" style=\"width:30.4pt;\">Centralized</span>\n</span>\n</th>\n<th id=\"S4.T2.7.1.1.3\" class=\"ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\">Improved Centralized</th>\n<th id=\"S4.T2.7.1.1.4\" class=\"ltx_td ltx_align_top ltx_th ltx_th_column ltx_border_tt\"></th>\n<th id=\"S4.T2.7.1.1.5\" class=\"ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\">Federated</th>\n<th id=\"S4.T2.7.1.1.6\" class=\"ltx_td ltx_align_top ltx_th ltx_th_column ltx_border_tt\"></th>\n<th id=\"S4.T2.7.1.1.7\" class=\"ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\">Performance Drop</th>\n</tr>\n<tr id=\"S4.T2.7.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.7.2.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span id=\"S4.T2.7.2.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.2.2.1.1.1\" class=\"ltx_p\" style=\"width:13.0pt;\"></span>\n</span>\n</th>\n<th id=\"S4.T2.7.2.2.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span id=\"S4.T2.7.2.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.2.2.2.1.1\" class=\"ltx_p\" style=\"width:30.4pt;\">BS</span>\n</span>\n</th>\n<th id=\"S4.T2.7.2.2.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span id=\"S4.T2.7.2.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.2.2.3.1.1\" class=\"ltx_p\" style=\"width:13.0pt;\">BS+S</span>\n</span>\n</th>\n<th id=\"S4.T2.7.2.2.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span id=\"S4.T2.7.2.2.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.2.2.4.1.1\" class=\"ltx_p\" style=\"width:13.0pt;\">H+S</span>\n</span>\n</th>\n<th id=\"S4.T2.7.2.2.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span id=\"S4.T2.7.2.2.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.2.2.5.1.1\" class=\"ltx_p\" style=\"width:13.0pt;\">GS</span>\n</span>\n</th>\n<th id=\"S4.T2.7.2.2.6\" class=\"ltx_td ltx_align_top ltx_th ltx_th_column\"></th>\n<th id=\"S4.T2.7.2.2.7\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span id=\"S4.T2.7.2.2.7.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.2.2.7.1.1\" class=\"ltx_p\" style=\"width:13.0pt;\">BS</span>\n</span>\n</th>\n<th id=\"S4.T2.7.2.2.8\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span id=\"S4.T2.7.2.2.8.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.2.2.8.1.1\" class=\"ltx_p\" style=\"width:13.0pt;\">BS+S</span>\n</span>\n</th>\n<th id=\"S4.T2.7.2.2.9\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span id=\"S4.T2.7.2.2.9.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.2.2.9.1.1\" class=\"ltx_p\" style=\"width:13.0pt;\">H+S</span>\n</span>\n</th>\n<th id=\"S4.T2.7.2.2.10\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span id=\"S4.T2.7.2.2.10.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.2.2.10.1.1\" class=\"ltx_p\" style=\"width:20.2pt;\">GS</span>\n</span>\n</th>\n<th id=\"S4.T2.7.2.2.11\" class=\"ltx_td ltx_align_top ltx_th ltx_th_column\"></th>\n<th id=\"S4.T2.7.2.2.12\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span id=\"S4.T2.7.2.2.12.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.2.2.12.1.1\" class=\"ltx_p\" style=\"width:16.6pt;\">BS</span>\n</span>\n</th>\n<th id=\"S4.T2.7.2.2.13\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span id=\"S4.T2.7.2.2.13.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.2.2.13.1.1\" class=\"ltx_p\" style=\"width:16.6pt;\">BS+S</span>\n</span>\n</th>\n<th id=\"S4.T2.7.2.2.14\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span id=\"S4.T2.7.2.2.14.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.2.2.14.1.1\" class=\"ltx_p\" style=\"width:16.6pt;\">H+S</span>\n</span>\n</th>\n<th id=\"S4.T2.7.2.2.15\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span id=\"S4.T2.7.2.2.15.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.2.2.15.1.1\" class=\"ltx_p\" style=\"width:16.6pt;\">GS</span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.7.3.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.7.3.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S4.T2.7.3.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.3.1.1.1.1\" class=\"ltx_p\" style=\"width:13.0pt;\">R@1</span>\n</span>\n</td>\n<td id=\"S4.T2.7.3.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S4.T2.7.3.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.3.1.2.1.1\" class=\"ltx_p\" style=\"width:30.4pt;\">1.02</span>\n</span>\n</td>\n<td id=\"S4.T2.7.3.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S4.T2.7.3.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.3.1.3.1.1\" class=\"ltx_p\" style=\"width:13.0pt;\">1.21</span>\n</span>\n</td>\n<td id=\"S4.T2.7.3.1.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S4.T2.7.3.1.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.3.1.4.1.1\" class=\"ltx_p\" style=\"width:13.0pt;\"><span id=\"S4.T2.7.3.1.4.1.1.1\" class=\"ltx_text ltx_font_bold\">1.27</span></span>\n</span>\n</td>\n<td id=\"S4.T2.7.3.1.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S4.T2.7.3.1.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.3.1.5.1.1\" class=\"ltx_p\" style=\"width:13.0pt;\">1.24</span>\n</span>\n</td>\n<td id=\"S4.T2.7.3.1.6\" class=\"ltx_td ltx_align_top ltx_border_t\"></td>\n<td id=\"S4.T2.7.3.1.7\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S4.T2.7.3.1.7.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.3.1.7.1.1\" class=\"ltx_p\" style=\"width:13.0pt;\">0.58</span>\n</span>\n</td>\n<td id=\"S4.T2.7.3.1.8\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S4.T2.7.3.1.8.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.3.1.8.1.1\" class=\"ltx_p\" style=\"width:13.0pt;\">0.88</span>\n</span>\n</td>\n<td id=\"S4.T2.7.3.1.9\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S4.T2.7.3.1.9.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.3.1.9.1.1\" class=\"ltx_p\" style=\"width:13.0pt;\">1.17</span>\n</span>\n</td>\n<td id=\"S4.T2.7.3.1.10\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S4.T2.7.3.1.10.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.3.1.10.1.1\" class=\"ltx_p\" style=\"width:20.2pt;\"><span id=\"S4.T2.7.3.1.10.1.1.1\" class=\"ltx_text ltx_font_bold\">1.21</span></span>\n</span>\n</td>\n<td id=\"S4.T2.7.3.1.11\" class=\"ltx_td ltx_align_top ltx_border_t\"></td>\n<td id=\"S4.T2.7.3.1.12\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S4.T2.7.3.1.12.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.3.1.12.1.1\" class=\"ltx_p\" style=\"width:16.6pt;\">43.14%</span>\n</span>\n</td>\n<td id=\"S4.T2.7.3.1.13\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S4.T2.7.3.1.13.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.3.1.13.1.1\" class=\"ltx_p\" style=\"width:16.6pt;\">27.27%</span>\n</span>\n</td>\n<td id=\"S4.T2.7.3.1.14\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S4.T2.7.3.1.14.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.3.1.14.1.1\" class=\"ltx_p\" style=\"width:16.6pt;\">7.87%</span>\n</span>\n</td>\n<td id=\"S4.T2.7.3.1.15\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S4.T2.7.3.1.15.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.3.1.15.1.1\" class=\"ltx_p\" style=\"width:16.6pt;\"><span id=\"S4.T2.7.3.1.15.1.1.1\" class=\"ltx_text ltx_font_bold\">2.42</span>%</span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T2.7.4.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.7.4.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"S4.T2.7.4.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.4.2.1.1.1\" class=\"ltx_p\" style=\"width:13.0pt;\">R@5</span>\n</span>\n</td>\n<td id=\"S4.T2.7.4.2.2\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"S4.T2.7.4.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.4.2.2.1.1\" class=\"ltx_p\" style=\"width:30.4pt;\">4.2</span>\n</span>\n</td>\n<td id=\"S4.T2.7.4.2.3\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"S4.T2.7.4.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.4.2.3.1.1\" class=\"ltx_p\" style=\"width:13.0pt;\">5.15</span>\n</span>\n</td>\n<td id=\"S4.T2.7.4.2.4\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"S4.T2.7.4.2.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.4.2.4.1.1\" class=\"ltx_p\" style=\"width:13.0pt;\"><span id=\"S4.T2.7.4.2.4.1.1.1\" class=\"ltx_text ltx_font_bold\">6.08</span></span>\n</span>\n</td>\n<td id=\"S4.T2.7.4.2.5\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"S4.T2.7.4.2.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.4.2.5.1.1\" class=\"ltx_p\" style=\"width:13.0pt;\">5.34</span>\n</span>\n</td>\n<td id=\"S4.T2.7.4.2.6\" class=\"ltx_td ltx_align_top\"></td>\n<td id=\"S4.T2.7.4.2.7\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"S4.T2.7.4.2.7.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.4.2.7.1.1\" class=\"ltx_p\" style=\"width:13.0pt;\">2.88</span>\n</span>\n</td>\n<td id=\"S4.T2.7.4.2.8\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"S4.T2.7.4.2.8.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.4.2.8.1.1\" class=\"ltx_p\" style=\"width:13.0pt;\">4.01</span>\n</span>\n</td>\n<td id=\"S4.T2.7.4.2.9\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"S4.T2.7.4.2.9.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.4.2.9.1.1\" class=\"ltx_p\" style=\"width:13.0pt;\"><span id=\"S4.T2.7.4.2.9.1.1.1\" class=\"ltx_text ltx_font_bold\">5.56</span></span>\n</span>\n</td>\n<td id=\"S4.T2.7.4.2.10\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"S4.T2.7.4.2.10.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.4.2.10.1.1\" class=\"ltx_p\" style=\"width:20.2pt;\">5.25</span>\n</span>\n</td>\n<td id=\"S4.T2.7.4.2.11\" class=\"ltx_td ltx_align_top\"></td>\n<td id=\"S4.T2.7.4.2.12\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"S4.T2.7.4.2.12.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.4.2.12.1.1\" class=\"ltx_p\" style=\"width:16.6pt;\">31.43%</span>\n</span>\n</td>\n<td id=\"S4.T2.7.4.2.13\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"S4.T2.7.4.2.13.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.4.2.13.1.1\" class=\"ltx_p\" style=\"width:16.6pt;\">22.14%</span>\n</span>\n</td>\n<td id=\"S4.T2.7.4.2.14\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"S4.T2.7.4.2.14.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.4.2.14.1.1\" class=\"ltx_p\" style=\"width:16.6pt;\">8.55%</span>\n</span>\n</td>\n<td id=\"S4.T2.7.4.2.15\" class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span id=\"S4.T2.7.4.2.15.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.4.2.15.1.1\" class=\"ltx_p\" style=\"width:16.6pt;\"><span id=\"S4.T2.7.4.2.15.1.1.1\" class=\"ltx_text ltx_font_bold\">1.69</span>%</span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T2.7.5.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.7.5.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span id=\"S4.T2.7.5.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.5.3.1.1.1\" class=\"ltx_p\" style=\"width:13.0pt;\">R@10</span>\n</span>\n</td>\n<td id=\"S4.T2.7.5.3.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span id=\"S4.T2.7.5.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.5.3.2.1.1\" class=\"ltx_p\" style=\"width:30.4pt;\">7.42</span>\n</span>\n</td>\n<td id=\"S4.T2.7.5.3.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span id=\"S4.T2.7.5.3.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.5.3.3.1.1\" class=\"ltx_p\" style=\"width:13.0pt;\">8.93</span>\n</span>\n</td>\n<td id=\"S4.T2.7.5.3.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span id=\"S4.T2.7.5.3.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.5.3.4.1.1\" class=\"ltx_p\" style=\"width:13.0pt;\"><span id=\"S4.T2.7.5.3.4.1.1.1\" class=\"ltx_text ltx_font_bold\">11.15</span></span>\n</span>\n</td>\n<td id=\"S4.T2.7.5.3.5\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span id=\"S4.T2.7.5.3.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.5.3.5.1.1\" class=\"ltx_p\" style=\"width:13.0pt;\">9.46</span>\n</span>\n</td>\n<td id=\"S4.T2.7.5.3.6\" class=\"ltx_td ltx_align_top ltx_border_bb\"></td>\n<td id=\"S4.T2.7.5.3.7\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span id=\"S4.T2.7.5.3.7.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.5.3.7.1.1\" class=\"ltx_p\" style=\"width:13.0pt;\">5.4</span>\n</span>\n</td>\n<td id=\"S4.T2.7.5.3.8\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span id=\"S4.T2.7.5.3.8.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.5.3.8.1.1\" class=\"ltx_p\" style=\"width:13.0pt;\">7.29</span>\n</span>\n</td>\n<td id=\"S4.T2.7.5.3.9\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span id=\"S4.T2.7.5.3.9.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.5.3.9.1.1\" class=\"ltx_p\" style=\"width:13.0pt;\"><span id=\"S4.T2.7.5.3.9.1.1.1\" class=\"ltx_text ltx_font_bold\">10.43</span></span>\n</span>\n</td>\n<td id=\"S4.T2.7.5.3.10\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span id=\"S4.T2.7.5.3.10.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.5.3.10.1.1\" class=\"ltx_p\" style=\"width:20.2pt;\">9.41</span>\n</span>\n</td>\n<td id=\"S4.T2.7.5.3.11\" class=\"ltx_td ltx_align_top ltx_border_bb\"></td>\n<td id=\"S4.T2.7.5.3.12\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span id=\"S4.T2.7.5.3.12.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.5.3.12.1.1\" class=\"ltx_p\" style=\"width:16.6pt;\">27.22%</span>\n</span>\n</td>\n<td id=\"S4.T2.7.5.3.13\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span id=\"S4.T2.7.5.3.13.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.5.3.13.1.1\" class=\"ltx_p\" style=\"width:16.6pt;\">18.37%</span>\n</span>\n</td>\n<td id=\"S4.T2.7.5.3.14\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span id=\"S4.T2.7.5.3.14.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.5.3.14.1.1\" class=\"ltx_p\" style=\"width:16.6pt;\">6.46%</span>\n</span>\n</td>\n<td id=\"S4.T2.7.5.3.15\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span id=\"S4.T2.7.5.3.15.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.7.5.3.15.1.1\" class=\"ltx_p\" style=\"width:16.6pt;\"><span id=\"S4.T2.7.5.3.15.1.1.1\" class=\"ltx_text ltx_font_bold\">0.53</span>%</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "This section studies the model performance with four different loss functions: batch softmax (BS), batch softmax with spreadout regularizer (BS+S), and two batch-insensitive losses (see ",
                "Section",
                "Â ",
                "3",
                "): hinge loss with spreadout (H+S) and global softmax (GS). All the federated models are trained with ",
                "FedAvg",
                ". We compare recall calculated globally across all items, which has no dependence on examples in a batch, enabling fair comparison.",
                "Batch Softmax (BS): ",
                "\nBatch softmax is the standard loss function for training a deep retrieval model. It is calculated with in-batch negatives as described in ",
                "Section",
                "Â ",
                "2",
                ". ",
                "Figure",
                "Â ",
                "5",
                "(a) shows a large gap between centralized and federated global recalls, similar to the batch recall results in ",
                "Section",
                "Â ",
                "4.2",
                ".",
                "Batch Softmax + Spreadout (BS+S): ",
                "\n",
                "Figure",
                "Â ",
                "5",
                "(b) presents results of training with batch softmax combined with spreadout regularization. With spreadout regularizer, the recall values of the federated model almost match those of the batch softmax centralized model, which is trained without spreadout regularizer. However, spreadout regularizer also improves centralized training. The federated model still performs significantly worse compared to the improved centralized model.",
                "The results indicate that spreadout regularization itself is not enough to solve the issue with the non-IID negatives. Although spreadout regularizer pushes embeddings of unrelated pairs farther apart, batch softmax loss still depends on in-batch negatives and can still lead to worse model quality. Therefore, we need a loss function less affected by the training data distribution, motivating batch-insensitive losses.",
                "Batch-Insensitive Losses (H+S and GS): ",
                "\n",
                "Figure",
                "Â ",
                "5",
                "(c) and ",
                "Figure",
                "Â ",
                "5",
                "(d) show the training results with the two types of batch-insensitive losses. With the combination of hinge loss and spreadout regularizer, both the improved centralized model and the federated model perform much better than the baseline model (",
                "Figure",
                "Â ",
                "5",
                "(d)). Also, the gap between improved centralized and federated models is much smaller than with batch softmax. With global softmax (",
                "Figure",
                "Â ",
                "5",
                "(c)), the federated model performs almost the same as the improved centralized model, and both perform significantly better than the batch softmax centralized model. Both of the results indicate that batch-insensitive loss alleviates the performance degradation caused by non-IID negatives effectively.",
                "We caution that applying global softmax may not be appropriate in all settings. In these experiments, we use all the items in the movie vocabulary as the negatives. In practice, when dealing with items with large or unbounded vocabulary size, we may need other strategies as global softmax is computationally expensive.",
                "Overall Comparison: ",
                "\n",
                "Table",
                "Â ",
                "2",
                " gives an overall performance comparison between centralized, improved centralized, and federated models under different losses.",
                "Training with batch-insensitive losses (H+S, GS) achieves the highest recall for both centralized and federated models. In particular, hinge loss with spreadout regularizer appears to perform slightly better than global softmax in terms of absolute recall, but both perform significantly better than batch-sensitive losses (BS, BS+S).",
                "We also observe that global softmax incurs the smallest performance gap between centralized and federated training. Hinge loss with spreadout regularizer has the next smallest performance gap, and batch-sensitive techniques have a more significant performance gap as expected. We expect that the remaining performance drop between centralized and federated models results from client drift (clients are still taking multiple local steps). This suggests that combining batch-insensitive losses with approaches to address client drift may be a promising future direction."
            ]
        ]
    }
}