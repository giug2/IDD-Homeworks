{
    "id_table_1": {
        "caption": "Table 1 :  Classification accuracies on BloodMNIST and Retina dataset compared with the state-of-the-art methods. We reported  FedImpres  results using random, CIFAR-10 and medical unlabeled data of the same modality data as initialization. Although medical initialization performs overall better than CIFAR-10 and random, we still outperform baselines in most of the settings.",
        "table": "S3.T1.1.1",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Heterogeneity happens due to 1) label imbalance  i.e. , various disease populations in different medical centers, and 2) domain shift,  i.e. , various data acquisition settings in medical devices. Studies have been carried out to mitigate each of the mentioned heterogeneities independently. However, based on our experiments in Fig.  1 , we show that both of these cases lead to a common issue called catastrophic forgetting, which has been usually overlooked in previous works. In FL, catastrophic forgetting  [ 5 ]  occurs when a model overwrites past aggregated knowledge with local data. As shown in Fig.  1 , when observing a specific client during local training, the local models accuracy on the other local datasets degrades since the server models past aggregated knowledge is overwritten by the local heterogeneous data. In this work, we focus on solving the catastrophic forgetting issue in FL caused by label imbalance and domain shift.",
            "As described in the introduction (Sec.  1 ), catastrophic forgetting during local training is one of the primary problems in heterogeneous FL. To develop a robust FL algorithm suitable for heterogeneous data, we need to address two fundamental challenges: 1) How to alleviate catastrophic forgetting in local training? This can be achieved by utilizing a united synthetic data as a regularizer in local client training to penalize catastrophic forgetting; 2) How to generate this synthetic dataset? We can synthesize data using the server model to capture a genuine federated impression for local training. The overall paradigm of our method is shown in Fig.  2 . In the following sections, we will provide a detailed description of our proposed paradigm.",
            "The results are illustrated in Table  1 . Although medical initialization has the best results, we show that even with CIFAR-10 and noise initialization, we outperform SOTA in most experiments, and this proves the effectiveness of the synthesis step regardless of the initialization. In all of the experiments  FedImpres  improves FedAvg by a large margin. This can be particularly observed when the level of heterogeneity is higher with   = 0.005  0.005 \\alpha=0.005 italic_ = 0.005  and the Retina dataset. Although FedProx was designed to have smoother local training by adding a penalty for divergence from the server model, this is harmful to severe heterogeneity due to a shortage in learning local data. Compared to VHL and FedVSS, we surpass them by virtue of our adaptive and unified synthesis data approach among clients, correspondingly. Although, FedCurv achieves close results to our method on Retina dataset, its performance degrades when facing label shift on the BloodMnist dataset. FedReg does not perform well on both datasets since its not designed for architectures with batch normalization."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Classification accuracies reported on the Retina dataset comparing synthesizing with  FedImpres  (CE loss) (Eq.  2 ) vs. vanilla CE loss. In both cases, we initialize the synthesis step with random noise.",
        "table": "S3.T2.5",
        "footnotes": [
            ""
        ],
        "references": [
            "As described in the introduction (Sec.  1 ), catastrophic forgetting during local training is one of the primary problems in heterogeneous FL. To develop a robust FL algorithm suitable for heterogeneous data, we need to address two fundamental challenges: 1) How to alleviate catastrophic forgetting in local training? This can be achieved by utilizing a united synthetic data as a regularizer in local client training to penalize catastrophic forgetting; 2) How to generate this synthetic dataset? We can synthesize data using the server model to capture a genuine federated impression for local training. The overall paradigm of our method is shown in Fig.  2 . In the following sections, we will provide a detailed description of our proposed paradigm.",
            "where  y ^ i subscript ^ y i \\hat{y}_{i} over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is initialized with the prediction of the server model when given  v i subscript v i v_{i} italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . Since optimizing Eq.  2  is computationally expensive, according to  [ 28 ] , we solve the relaxed version of the optimization problem imposing the equality constraint on   G r subscript superscript italic- r G \\phi^{r}_{G} italic_ start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT  only",
            "where    \\Lambda roman_  is the lagrangian dual variable matrix for the equality constraint in Eq. ( 2 ) and    \\rho italic_  is the penalty hyperparameter. According to  [ 28 ] , we solve it approximately using an alternating direction method of multipliers (ADMM)  [ 4 ] . After synthesizing this data as the federated impression, we pass it to all clients for local training. Note that we dont need any additional private data information to generate the synthetic dataset compared to general FL methods like  [ 18 ] .",
            "To assess the effect of our data synthesis algorithm, we consider another synthetic data generation variant adopted by our proposed method and study its performance on the Retina dataset, as it is a real-life dataset and has both label imbalance and domain shift. For this, we omit the constraint of globalizing data synthesized to distribution seized by the server model in Eq. ( 2 ) and optimize only with CE loss. For both methods, we use random noise to initialize data synthesis to omit any initialization bias. As shown in Table  2 , the proposed  FedImpres  approach surpasses its other variant, showing the effectiveness of its data synthesis algorithm for data generation."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Number of data points of each class for each client.",
        "table": "Pt0.A0.T3.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_4": {
        "caption": "Table 4 :   The architecture of the benchmark experiment includes specific parameters for each layer type. For Conv2D layers, we list the input and output dimensions, kernel size, stride, and padding. For MaxPool2D layers, we list the kernel size and stride. For FC layers, we list the input and output dimensions. For BN layers, we list the channel dimension. Right and left architecture is used for BloodMNIST and Retina, respectively.",
        "table": "Pt0.A0.T4.tab1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_5": {
        "caption": "Table 5 :   Hyper-parameters used for training.",
        "table": "Pt0.A0.T4.tab2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_6": {
        "caption": "",
        "table": "Pt0.A0.T5.3",
        "footnotes": [],
        "references": []
    }
}