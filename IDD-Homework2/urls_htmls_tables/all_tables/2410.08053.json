{
    "id_table_1": {
        "caption": "Table 1:  Examples from the dataset after our aggregation operations, with a hate speech label and a list of target identity groups mentioned or referred to in the text.",
        "table": "S2.T1.1.1",
        "footnotes": [],
        "references": [
            "In the freely available version of the MHS dataset 1 1 1 https://huggingface.co/datasets/ucberkeley-dlab/measuring-hate-speech  we find annotations for seven target identity groups:  race ,  religion ,  origin ,  gender ,  sexuality ,  age , and  disability . Their distribution in the data can be seen in Figure  1 , which shows how the most widely studied targets of hate speech,  race  and  gender , are also the most widely represented in the MHS corpus, while some targets such as  age ,  disability , or  religion  are less frequent.",
            "All HateCheck test cases mention a specific target identity, to allow the exploration of unintended biases against different target groups. However, the target groups used in HateCheck do not fully overlap with the target identity groups in the MHS corpus (Figure  1 ). The target identities that are present in HateCheck are: women (which would fall under  gender  in MHS), trans people ( gender  in MHS), gay people ( sexuality  in MHS), black people ( race  in MHS), disabled people ( disability  in MHS), Muslims ( religion  in MHS) and immigrants ( origin  in MHS). The  age  category is present in MHS corpus and entirely missing in HateCheck."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Templates used for fine-tuning and prompting generative models during the generation step.",
        "table": "S4.T2.2.2",
        "footnotes": [],
        "references": [
            "The paper is structured as follows: in Section  2  we summarize past works on data augmentation aimed at detecting hate speech. In Section  3  we introduce the Measuring Hate Speech Corpus, which is used in all the experiments. In Section  4  we detail the methodology adopted in the experiments, in particular how target identify is modeled, the differences between finetuning and few-shot prompting and the prompting layout. In Section  5  we describe the experimental setup and the baselines. In Section  6  we discuss the experimental results comparing the classification performance of the different setups, while in Section  7  we perform a qualitative analysis by manually annotating generated examples and exploring the weaknesses of our models with the HateCheck test suite  (Rottger et al.,  2021 ) . We finally discuss the limitations of our study in Section  Limitations  and summarize our findings and the potential impact of this work in Section  8 .",
            "However,  Kennedy et al. ( 2020b )  also include a binary hate speech label in the questionnaires to be assigned by annotators. They additionally conduct a comparison between the continuous score and the binary hate speech score, finding that while the continuous measure can better capture the extremity of hate speech, the two are moderately correlated. Given the scope of our work, we use the binary labels instead of the continuous hate speech scores in our experiments, in order to frame the task as classification rather than regression and to be able to test our models on out-of-distribution data (see Section  7.2 ).",
            "In our experiments, we implement a data augmentation pipeline inspired by  Anaby-Tavor et al. ( 2020 ) , which has also been used in other work  (Wullach et al.,  2021 ; Casula and Tonelli,  2023 ) . The pipeline is displayed in Figure  2 . Starting from a small set of  Gold data  from the MHS corpus, a  Generator  is employed to augment them by generating synthetic examples with the corresponding label (hateful or not) using either  finetuning  or  few-shot prompting  (see Section  4.2 ). Since the labels associated with the  Generated data  may not be accurate, given that generative models cannot always preserve the desired labels  (Kumar et al.,  2020 ) , a subsequent filtering step is used in order to maximize the chances of label correctness. In order to create a model for filtering generated texts, the same gold data is used also to fine-tune a binary classifier that assigns a hateful/non hateful label to the generated data. We keep only the synthetic examples where there is a match between the label assigned during generation and by the  Classification model  to filter out examples that are likely to have an inconsistent labels. The  Filtered synthetic data  is then used to train a hate speech classifier that we evaluate for the task performance in general and then on specific hate targets.",
            "For finetuning, we follow an approach similar to that of  Anaby-Tavor et al. ( 2020 ) , in which a generative LLM is fine-tuned on annotated sequences that are concatenated with labels. At generation time, the desired label information is fed into the model, and the model is expected to generate a sequence belonging to the specified class. We discuss the details of the formatting of the label information in Section  4.2.3 .",
            "As regards formatting, we aim at using the same type of prompting layout across experiments. We choose to use prompting sequences in natural language, given that they have been found to lead to generally more realistic generated examples  (Casula and Tonelli,  2023 ) . In order to find prompts in natural language that could be leveraged by our models, we consulted the FLAN corpus  (Wei et al.,  2022 ) , which is part of the finetuning data of both FLAN-T5 and OPT-IML. Among the instruction templates, we find one of the CommonGen templates  (Lin et al.,  2020 )  to fit with our aims:  Write a sentence about the following things: [concepts], [target] . We reformulate it to obtain a prompting sequence that reflects our application, and can be exploited by instruction-finetuned models: Write a [  \\emptyset  /  hateful ] social media post [  \\emptyset  / about  t ], where  t t t italic_t  is a target identity. Table  2  presents the sequences and prompts used for training and prompting our models.",
            "For all experiments, following previous work on data augmentation  (Anaby-Tavor et al.,  2020 ; Casula and Tonelli,  2023 ) , we simulate a setup in which we have a small amount of gold data available prior to augmentation (see Figure  2 ). We randomly select 1,000 gold examples from the MHS corpus, as we deem it a realistically small dataset size for a hate speech detection corpus, after looking at the hate speech dataset review by  Vidgen and Derczynski ( 2020 ) . Our goal is to create a larger dataset out of the starting 1,000 examples. Given that the natural size of the Measuring HS dataset is 35k examples, we aim for 30k new annotated examples (equally split between the labels) to use in augmentation, which will result in a 31k example dataset for each setup.",
            "We test the models trained on generation and EDA-augmented data (see setup in Section  6.2 ) on HateCheck targets, reporting the performance on a per-target basis in terms of hateful-class F 1 . The results are reported in Table  6 , divided by the target identity categories used in HateCheck. We also report, as baselines, the results of the classification model using no augmentation and EDA alone."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  DeBERTa results (macro-F 1  and hate-class F 1 ) with generative DA, averaged over 5 runs  stdev , overall and by target ( Ge nder,  Ra ce,  Or igin,  Se xuality,  Re ligion,  Di sability, and  Ag e). Statistical significance is calculated against the  no augmentation  baseline.    \\star  : highly statistically significant (  = 0.2  0.2 \\tau=0.2 italic_ = 0.2 ),    \\diamond  : statistically significant (  = 0.5  0.5 \\tau=0.5 italic_ = 0.5 ).  n(h)  = number of  hateful  synthetic examples preserved after filtering.",
        "table": "S5.T3.272.272",
        "footnotes": [],
        "references": [
            "The paper is structured as follows: in Section  2  we summarize past works on data augmentation aimed at detecting hate speech. In Section  3  we introduce the Measuring Hate Speech Corpus, which is used in all the experiments. In Section  4  we detail the methodology adopted in the experiments, in particular how target identify is modeled, the differences between finetuning and few-shot prompting and the prompting layout. In Section  5  we describe the experimental setup and the baselines. In Section  6  we discuss the experimental results comparing the classification performance of the different setups, while in Section  7  we perform a qualitative analysis by manually annotating generated examples and exploring the weaknesses of our models with the HateCheck test suite  (Rottger et al.,  2021 ) . We finally discuss the limitations of our study in Section  Limitations  and summarize our findings and the potential impact of this work in Section  8 .",
            "Since our research questions revolve around the impact that hate target information can have on data augmentation, finetuning and few-shot demonstration-based prompting are further tested in two variants: with and without mentioning the  target identity information . Our hypothesis is that the inclusion of this kind of information might help in generating more varied data with regards to identity group mentions for both hateful and non-hateful messages. By generating target-specific examples also for the non-hateful class, we ideally aim at implicitly contrasting identity term bias. In order to do this, we encode target identity information into the prompts given to the models in various ways. The values used for target are the identity group names in the MHS dataset, reported in Section  3 .",
            "For finetuning, we follow an approach similar to that of  Anaby-Tavor et al. ( 2020 ) , in which a generative LLM is fine-tuned on annotated sequences that are concatenated with labels. At generation time, the desired label information is fed into the model, and the model is expected to generate a sequence belonging to the specified class. We discuss the details of the formatting of the label information in Section  4.2.3 .",
            "Given that our focus is on different targets of hate, we aim at investigating the impact of their representation in the data on model performance. Specifically, since their distribution in the MHS corpus is highly imbalanced, as seen in Section  3 , we hypothesize that their representation might influence the performance of models. Because of this, we choose to equally augment each target identity category (gender, race, origin, sexuality, religion, disability, and age). Indeed, for models that rely on target identity information, out of the 50k generated instances for each class, we generate 1/7 for each target identity category (7,140 newly generated sequences for each target identity group).",
            "We report in Table  3  the results of our experiments using generative DA and compared with EDA and the two baselines described above. The classification performance is evaluated globally in terms of macro-F 1  and minority (hate) class F 1  and for each target identity category as hate class F 1 , so that the impact of synthetic data can be examined on a per-target basis.",
            "The impact of finetuning vs. few-shot prompting seems model-dependent, with differences across models also regarding the impact of target information. For all but OPT-IML, finetuning approaches tend to favor the inclusion of target information, albeit with relatively minor differences. Interestingly, the amount of synthetic examples labeled as  hateful  (reported in Table  3  as  n(h) ) that pass filtering does not appear to strongly impact the performance of models trained on synthetic data, indicating that potentially even just a few hundred synthetic examples can positively impact generalization.This could also indicate that even just the addition of non-hateful synthetic examples can help models to generalize.",
            "An overview of the manual annotations is reported in Table  5 . In most cases, the addition of target information results in more realistic texts and, in general, more accurate label assignment by the generation model. However, this is not directly associated with the augmented data improving model performance when used for training. For instance, the setting that yields the best results with data generated by T5 (0.798 M-F 1  and 0.700 Hate-F 1 , see Table  3 ) is the one with few-shot prompting without target information, whose generated sentences are deemed as never realistic by the human annotators. On the other hand, the worst classification setting is obtained with examples generated by OPT using finetuning and no target information (0.774 M-F 1  and 0.652 Hate-F 1 ), which led the model to generate nonsensical texts."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  DeBERTa results of generative DA + EDA overall and by target, averaged over 5 runs  stdev . Statistical significance is calculated against the results obtained with EDA.    \\diamond  : statistically significant against EDA alone (  = 0.5  0.5 \\tau=0.5 italic_ = 0.5 ).",
        "table": "S6.T4.165.165",
        "footnotes": [],
        "references": [
            "The paper is structured as follows: in Section  2  we summarize past works on data augmentation aimed at detecting hate speech. In Section  3  we introduce the Measuring Hate Speech Corpus, which is used in all the experiments. In Section  4  we detail the methodology adopted in the experiments, in particular how target identify is modeled, the differences between finetuning and few-shot prompting and the prompting layout. In Section  5  we describe the experimental setup and the baselines. In Section  6  we discuss the experimental results comparing the classification performance of the different setups, while in Section  7  we perform a qualitative analysis by manually annotating generated examples and exploring the weaknesses of our models with the HateCheck test suite  (Rottger et al.,  2021 ) . We finally discuss the limitations of our study in Section  Limitations  and summarize our findings and the potential impact of this work in Section  8 .",
            "In our experiments, we implement a data augmentation pipeline inspired by  Anaby-Tavor et al. ( 2020 ) , which has also been used in other work  (Wullach et al.,  2021 ; Casula and Tonelli,  2023 ) . The pipeline is displayed in Figure  2 . Starting from a small set of  Gold data  from the MHS corpus, a  Generator  is employed to augment them by generating synthetic examples with the corresponding label (hateful or not) using either  finetuning  or  few-shot prompting  (see Section  4.2 ). Since the labels associated with the  Generated data  may not be accurate, given that generative models cannot always preserve the desired labels  (Kumar et al.,  2020 ) , a subsequent filtering step is used in order to maximize the chances of label correctness. In order to create a model for filtering generated texts, the same gold data is used also to fine-tune a binary classifier that assigns a hateful/non hateful label to the generated data. We keep only the synthetic examples where there is a match between the label assigned during generation and by the  Classification model  to filter out examples that are likely to have an inconsistent labels. The  Filtered synthetic data  is then used to train a hate speech classifier that we evaluate for the task performance in general and then on specific hate targets.",
            "For finetuning, we follow an approach similar to that of  Anaby-Tavor et al. ( 2020 ) , in which a generative LLM is fine-tuned on annotated sequences that are concatenated with labels. At generation time, the desired label information is fed into the model, and the model is expected to generate a sequence belonging to the specified class. We discuss the details of the formatting of the label information in Section  4.2.3 .",
            "Since models trained on EDA-augmented data outperform models trained only on generation-augmented sequences, we also experiment with the mixture of the two methods, with 15k synthetic examples created using each of them. In Table  4  we report the results of these experiments.",
            "We perform a second qualitative analysis using the HateCheck test suite  (Rottger et al.,  2021 ) , a collection of functional testing examples that enable targeted diagnostic insights of hate speech detection models. We focus on the models trained with augmented data using generative DA + EDA for this analysis (Table  4 ), since they yield the best classification performance. Again, each generative model + EDA is used in four settings to generate new data: with finetuning or few-shot prompting, each one with or without target information."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Generated texts labeled as correct by human annotators in terms of labels, target categories, and realism. N/A refers to cases in which all of the generated texts were nonsensical (0% realistic), with impossible assignment of labels or categories. We also report the model performance from Table  3  in terms of Macro-F 1  and Hate F 1 , in order to make comparisons between model performance and manual annotation results easier.",
        "table": "S7.T5.2.2",
        "footnotes": [
            ""
        ],
        "references": [
            "The paper is structured as follows: in Section  2  we summarize past works on data augmentation aimed at detecting hate speech. In Section  3  we introduce the Measuring Hate Speech Corpus, which is used in all the experiments. In Section  4  we detail the methodology adopted in the experiments, in particular how target identify is modeled, the differences between finetuning and few-shot prompting and the prompting layout. In Section  5  we describe the experimental setup and the baselines. In Section  6  we discuss the experimental results comparing the classification performance of the different setups, while in Section  7  we perform a qualitative analysis by manually annotating generated examples and exploring the weaknesses of our models with the HateCheck test suite  (Rottger et al.,  2021 ) . We finally discuss the limitations of our study in Section  Limitations  and summarize our findings and the potential impact of this work in Section  8 .",
            "An overview of the manual annotations is reported in Table  5 . In most cases, the addition of target information results in more realistic texts and, in general, more accurate label assignment by the generation model. However, this is not directly associated with the augmented data improving model performance when used for training. For instance, the setting that yields the best results with data generated by T5 (0.798 M-F 1  and 0.700 Hate-F 1 , see Table  3 ) is the one with few-shot prompting without target information, whose generated sentences are deemed as never realistic by the human annotators. On the other hand, the worst classification setting is obtained with examples generated by OPT using finetuning and no target information (0.774 M-F 1  and 0.652 Hate-F 1 ), which led the model to generate nonsensical texts."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  DeBERTa results on HateCheck (hate-F 1 ) by target identity, averaged across 5 runs.  p.  is an abbreviation for  people . Statistical significance is calculated against the results obtained with EDA.    \\diamond  : statistically significant (  = 0.5  0.5 \\tau=0.5 italic_ = 0.5 ).",
        "table": "S7.T6.126.126",
        "footnotes": [],
        "references": [
            "The paper is structured as follows: in Section  2  we summarize past works on data augmentation aimed at detecting hate speech. In Section  3  we introduce the Measuring Hate Speech Corpus, which is used in all the experiments. In Section  4  we detail the methodology adopted in the experiments, in particular how target identify is modeled, the differences between finetuning and few-shot prompting and the prompting layout. In Section  5  we describe the experimental setup and the baselines. In Section  6  we discuss the experimental results comparing the classification performance of the different setups, while in Section  7  we perform a qualitative analysis by manually annotating generated examples and exploring the weaknesses of our models with the HateCheck test suite  (Rottger et al.,  2021 ) . We finally discuss the limitations of our study in Section  Limitations  and summarize our findings and the potential impact of this work in Section  8 .",
            "finetuned on the initial 1,000 gold examples. 5 5 5 We replicate all the experiments also with RoBERTa Large  (Liu et al.,  2019 ) . The results are in line with those obtained using DeBERTa, therefore they are not reported in this paper.  We only preserve the examples for which the classifier label assignment matches the desired label that was in the model input at generation time, in line with previous work that used this kind of filtering for synthetic sequences. If less than 15k generated sequences for a given label pass filtering, we preserve the examples that did pass filtering, and proceed with the rest of the pipeline. In some setups, this means that we end up with fewer synthetic examples than initially expected. We discuss this more in depth in Section  6 .",
            "We test the models trained on generation and EDA-augmented data (see setup in Section  6.2 ) on HateCheck targets, reporting the performance on a per-target basis in terms of hateful-class F 1 . The results are reported in Table  6 , divided by the target identity categories used in HateCheck. We also report, as baselines, the results of the classification model using no augmentation and EDA alone."
        ]
    }
}