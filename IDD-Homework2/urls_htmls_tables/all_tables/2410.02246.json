{
    "id_table_1": {
        "caption": "Table 1:  Fairness and utility performances of private generative models with and without PFGuard on synthetic data, evaluated on MNIST with subgroup bias under   = 10  10 \\varepsilon{=}10 italic_ = 10 . Blue and red arrows indicate positive and negative changes, respectively. Lower values are better across all metrics.",
        "table": "A6.EGx1",
        "footnotes": [],
        "references": [
            "However, we contend that naively combining developed techniques for privacy and fairness can lead to a  worse privacy-fairness-utility tradeoff , where utility is a models ability to generate realistic synthetic data. We first illustrate how  privacy and fairness can conflict  in Fig.  1 . Given the data samples  M 1 subscript M 1 M_{1} italic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,  M 2 subscript M 2 M_{2} italic_M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ,  M 3 subscript M 3 M_{3} italic_M start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , and  m 1 subscript m 1 m_{1} italic_m start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  where  M M M italic_M  and  m m m italic_m  denote the majority and minority data groups, respectively, DP and fairness techniques play a tug-of-war regarding the use of minority data point  m 1 subscript m 1 m_{1} italic_m start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ; DP techniques  limit  its use to prevent privacy risks such as memorization, while fairness techniques  increase  its use to promote more balanced learning w.r.t. groups given the biased data. As a result, fairness techniques may undermine privacy by overusing  m 1 subscript m 1 m_{1} italic_m start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , while DP techniques may undermine fairness by limiting  m 1 subscript m 1 m_{1} italic_m start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT s usage. Moreover, combining different techniques can  introduce new technical constraints , reducing the effectiveness of original methods. For instance, the fair preprocessing technique used by  Xu et al. ( 2021 )  hinders the utility of the DP generative model by requiring data binarization, which incurs significant information loss on high-dimensional data such as images  restricting their overall framework only applicable to low-dimensional structural data.",
            "We can enforce DP in an algorithm in two steps  (Dwork et al.,  2014 ) . First, we bound its sensitivity (Def.  2.2 ), which captures the maximum influence of an individual sample. Second, we add random noise that is proportional to the sensitivity. A common way to ensure DP is to use a Gaussian mechanism  (Dwork et al.,  2014 )  (Thm.  2.1 ), which utilizes Gaussian random noise with a scale proportional to  l 2 subscript l 2 l_{2} italic_l start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT -sensitivity. Two datasets  D D D italic_D ,  D  superscript D  D^{\\prime} italic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  are adjacent if they differ by a single sample.",
            "In this section, we examine the practical challenges of integrating privacy-only and fairness-only techniques to train both private and fair generative models. Based on Fig.  1 s intuition on how privacy and fairness conflict, we analyze how existing approaches for DP generative models and fair generative models can technically conflict with each other.",
            "DP-SGD  is a standard DP technique  Chen et al. ( 2020 )  for converting non-DP algorithms to DP algorithms by modifying traditional stochastic gradient descent (SGD). Compared to SGD, DP-SGD 1) applies  gradient clipping  to limit the individual data points contribution, where  g  ( x ) g x \\bm{g}({\\mathbf{x}}) bold_italic_g ( bold_x )  is clipped to  g  ( x ) / max  ( 1 ,  g  ( x )  2 / C ) g x 1 subscript norm g x 2 C \\bm{g}({\\mathbf{x}})/\\max(1,||\\bm{g}({\\mathbf{x}})||_{2}/C) bold_italic_g ( bold_x ) / roman_max ( 1 , | | bold_italic_g ( bold_x ) | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT / italic_C )  (the sensitivity becomes the clipping threshold  C C C italic_C ), and 2) uses a  Gaussian mechanism  (Thm.  2.1 ) to add sufficient noise to ensure DP.",
            "We can extend reweighting for fairness to also satisfy DP using DP-SGD, but finding the clipping threshold  C C C italic_C  that balances fairness and utility can be challenging. Note that if we perform DP-SGD and then reweighting, privacy breach may occur by amplifying sample gradients beyond  C C C italic_C , invalidating the sensitivity derived from gradient clipping. We thus consider performing reweighting and then DP-SGD, which at least guarantees DP for reweighting-based fair generative models. However, the clipping now undoes the fairness adjustments where reweighted gradients  g  ( x )  h  ( x )  g x h x \\bm{g}({\\mathbf{x}})\\cdot h({\\mathbf{x}}) bold_italic_g ( bold_x )  italic_h ( bold_x )  are clipped to  g  ( x )  h  ( x ) / max  ( 1 ,  g  ( x )  h  ( x )  2 / C )  g x h x 1 subscript norm  g x h x 2 C \\bm{g}({\\mathbf{x}})\\cdot h({\\mathbf{x}})/\\max(1,||\\bm{g}({\\mathbf{x}})\\cdot h% ({\\mathbf{x}})||_{2}/C) bold_italic_g ( bold_x )  italic_h ( bold_x ) / roman_max ( 1 , | | bold_italic_g ( bold_x )  italic_h ( bold_x ) | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT / italic_C ) , and Eq.  1  does not hold if  C  g  ( x ) C g x C\\leq\\bm{g}({\\mathbf{x}}) italic_C  bold_italic_g ( bold_x ) . Here one solution is to use a larger  C C C italic_C  such that  C  g  ( x ) C g x C\\geq\\bm{g}({\\mathbf{x}}) italic_C  bold_italic_g ( bold_x ) . However, increasing  C C C italic_C  also increases the noise required for DP, which reduces utility (Fig.  3 ). As a result, selecting a  C C C italic_C  that balances fairness and utility may necessitate extensive hyperparameter tuning  (Bu et al.,  2024 ) , complicating the systematic integration of fairness into DP generative models.",
            "We now propose PFGuard, the  first  generative framework that simultaneously achieves statistical fairness and DP on high-dimensional data, such as images. As shown in Fig.  2 , PFGuard balances privacy-fairness conflicts between fair and private training stages using an ensemble of teacher models as a key component. In Sec.  4.1 , we first explain the  fair training stage , which trains a fair teacher ensemble. In Sec.  4.2 , we then explain the  private training stage , which transfers the knowledge of this teacher ensemble to the generator with DP guarantees  ultimately training a generator that is both fair and private. In Sec.  4.3 , we lastly discuss how PFGuards integrated design offers advantages in terms of fairness, utility, and privacy compared to the naive approaches discussed in Sec.  3 .",
            "We ensure DP by privatizing  knowledge transfer  from a teacher ensemble to the generator. Although the sampling technique in Sec.  4.1  transfers more balanced knowledge,  privacy risks  can be also transferred due to privacy-fairness conflicts. For example, if certain data samples are resampled repeatedly during teacher training, privacy risks like memorization can occur in the teacher models and be transferred to the generator. We thus privatize the knowledge transfer with DP techniques to provide strict DP guarantees in the generator. Note that only the generator needs to have privacy as it is the one that is released publicly to produce synthetic data.",
            "where  n j  ( x ) subscript n j x n_{j}({\\mathbf{x}}) italic_n start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( bold_x )  denotes the vote count for the  j j j italic_j -th class (i.e.,  n j  ( x ) = | { i : T i  ( x ) = j } | subscript n j x conditional-set i subscript T i x j n_{j}({\\mathbf{x}})=|\\{i:T_{i}({\\mathbf{x}}){=}j\\}| italic_n start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( bold_x ) = | { italic_i : italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_x ) = italic_j } | ), and  N  ( 0 ,  2 ) N 0 superscript  2 \\mathcal{N}(0,\\sigma^{2}) caligraphic_N ( 0 , italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )  denotes random Gaussian noise. Here, the  l 2 subscript l 2 l_{2} italic_l start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT -sensitivity (Def.  2.2 ) is  2 2 \\sqrt{2} square-root start_ARG 2 end_ARG , as a single data point affects at most one teacher, increasing the vote counts by 1 for one class and decreasing the count by 1 for another class (see a more detailed analysis in Sec.  B.2 ). Consequently, the GNMax aggregator satisfies  (  ,  )   (\\varepsilon,\\delta) ( italic_ , italic_ ) -DP for    8  ln  ( 1.25 /  ) /   8 1.25   \\sigma\\geq\\sqrt{8\\ln(1.25/\\delta)}/\\varepsilon italic_  square-root start_ARG 8 roman_ln ( 1.25 / italic_ ) end_ARG / italic_  based on the Gaussian mechanism (Thm.  2.1 ).",
            "PFGuard can be easily integrated with existing PTEL-based generative models by simply modifying the minibatch sampling process as described in Sec.  4.1 . PTEL has been widely adopted to train generators with privatized teacher output to ensure DP  (Jordon et al.,  2018 ; Chen et al.,  2020 ; Long et al.,  2021 ; Wang et al.,  2021a ) . Although the exact sensitivity values of these PTEL-based generative models vary depending on what teacher knowledge is aggregated (e.g., votes on class labels  (Jordon et al.,  2018 )  or gradient directions  (Wang et al.,  2021a ) ), PFGuard preserves any sensitivity as long as the PTEL enforce data disjointness; even with fair sampling, a single data point still affects only one teacher. PFGuard is thus compatible with various PTEL-based generative models, enhancing fairness while preserving DP guarantees  see Sec.  C.1  for the full algorithm.",
            "Datasets      We evaluate PFGuard on three image datasets: 1)  MNIST   (LeCun et al.,  1998 )  and  FashionMNIST   (Xiao et al.,  2017 )  for various analyses and baseline comparisons, and 2)  CelebA   (Liu et al.,  2015 )  to observe performance in real-world scenarios more closely related to privacy and fairness concerns. Here, MNIST contains handwritten digit images, FashionMNIST contains clothing item images, and CelebA contains facial images. While MNIST and FashionMNIST are simplistic and less reflective of real-world biases, they enable reliable fairness analyses on top of high-performing DP generative models on these datasets, making them widely adopted in recent studies addressing the privacy-fairness intersections  (Bagdasaryan et al.,  2019 ; Farrand et al.,  2020 ; Ganev et al.,  2022 ) . For CelebA, we resize the images to 32   \\times   32   \\times   3 (i.e.,  CelebA(S) ) and to 64   \\times   64   \\times   3 (i.e.,  CelebA(L) ) following the conventions in the DP generative model literature  (Long et al.,  2021 ; Wang et al.,  2021a ; Cao et al.,  2021 ) . More dataset details are in Sec.  D.1 .",
            "Bias Settings      We create various bias settings across classes and subgroups, focusing on four scenarios: 1)  binary class bias , which is a basic scenario often addressed in DP generative models, and 2)  multi-class bias ,  subgroup bias , and  unknown subgroup bias , which are more challenging scenarios typically addressed in fairness techniques, but not in DP generative models. We observe that DP generative models mostly perform poorly in these challenging scenarios, especially with complex datasets like CelebA, so we use MNIST for more reliable analyses. While recent privacy-fairness studies on MNIST mostly focus on class bias  (Bagdasaryan et al.,  2019 ; Farrand et al.,  2020 ) , we additionally analyze subgroup bias using image rotation for more fine-grained fairness analyses and to support prominent fairness metrics like equalized odds  (Hardt et al.,  2016 ) . In all experiments, we denote  Y = 0 Y 0 Y=0 italic_Y = 0  as the minority class and  S = 0 S 0 S=0 italic_S = 0  as the minority group. More details on bias levels (e.g., size ratios between majority and minority data) and bias creation are in Sec.  D.1 .",
            "Privacy.  We use privacy budget    \\varepsilon italic_  for DP (Def.  2.1 ), which is preserved in both synthetic data and downstream tasks due to the post-processing property of DP (see more details in Sec.  B.1 ).",
            "Table  1  shows the fairness and utility performances on synthetic data. Private generative models generally produce synthetic data with better overall image quality, but exhibit high group size disparity. In contrast, PFGuard significantly improves fairness by balancing group size and groupwise image quality, with a slight decrease in overall image quality.",
            "More Analyses  We provide more experiments in Sec.  E , including a comparison of computation time (Sec.  E.1 ), results on different datasets such as FashionMNIST (Sec.  E.2 ), and employing an additional normalization technique to further enhance the overall image quality (Sec.  E.3 ).",
            "All datasets, methodologies, and experimental setups utilized in our study are described in detail in the supplementary material. More specifically, we provide a description of the proposed algorithm in Sec.  C.1 , details of datasets and preprocessing in Sec.  D.1 , and implementation details including hyperparameters in Sec.  D.2  to ensure reproducibility.",
            "Classification and generation settings often concentrate on  different DP notions  or rely on  different assumptions , which hinders simple extensions of techniques between them. In the classification setting, Differential Privacy  w.r.t. sensitive attribute   (Jagielski et al.,  2019 )  is commonly addressed  (Jagielski et al.,  2019 ; Mozannar et al.,  2020 ; Tran et al.,  2021b ;  2022 ; Lowy et al.,  2023 ) , which considers the demographic group attribute as the only private information. This DP notion requires less DP noise compared to a more general notion of DP (Def.  2.1 ), which protects  all  input features, and enables a better privacy-utility tradeoff for DP classifiers. However, in the generative setting, a general notion of DP is mostly addressed, as presumed non-private features may in fact encode private information (e.g., pixel values in a facial image). Therefore, simply extending classification techniques to generative settings can be challenging, as it necessitates rigorous mathematical proofs for corresponding DP notions and may add a large DP noise when adapting to a general DP notion. Moreover, classification techniques can rely on convex objective functions  (Tran et al.,  2021a ) , but the assumption of convexity does not usually hold in generative models  (Goodfellow et al.,  2014 ) .",
            "Continuing from Sec.  5 , Sec.  5.1 , Sec.  5.3 , and Sec.  5.4 , we provide more experimental results.",
            "Continuing from Sec.  5.2  and Sec.  5.4 , we show full results with standard deviation. Table  9  and Table  10  shows the full results of Table  4 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Fairness and utility performances of private generative models with and without PFGuard on downstream tasks, evaluated on MNIST with subgroup bias under   = 10  10 \\varepsilon{=}10 italic_ = 10 . Blue and red arrows indicate positive and negative changes, respectively.",
        "table": "A6.EGx2",
        "footnotes": [],
        "references": [
            "Therefore, we design a generative framework that  simultaneously  addresses fairness and privacy while achieving utility for high-dimensional synthetic data generation. To this end, we propose PFGuard: a generative framework with  P rivacy and  F airness Safe guard s. As illustrated in Fig.  2 , the key component is an  ensemble of intermediate teacher models , which balances privacy-fairness conflicts between fair training and private training stages. In the  fair training  stage, we design a new sampling technique to train fair teachers, which provides a theoretical convergence guarantee to the fair generator. In the  private training  stage, we employ the Private Teacher Ensemble Learning (PTEL) approach  (Papernot et al.,  2016 ;  2018 ) , which aggregates each teachers knowledge with random DP noise (e.g., noisy voting), to privatize the knowledge transfer to the generator. As a result, PFGuard provides a unified solution to train both fair and private generative models by transferring the teachers fair knowledge in a privacy-preserving manner.",
            "Compared to simple sequential approaches, PFGuard is carefully designed to address privacy-fairness conflicts. Recall that fairness techniques can incur privacy breaches by overusing minority data; in contrast, PFGuard  prevents privacy breaches  by decoupling fairness and privacy with intermediate teacher models. Although fair sampling can still compromise privacy in teacher models by potentially overusing minority data, PFGuard ensures privacy in the generator  our target model  by training it solely with the privatized teacher output, as shown in Fig.  2 . Also, recall that privacy techniques can lead to fairness cancellation by suppressing the use of minority data; in contrast, PFGuard  avoids fairness cancellation  through teacher-level privacy bounding using PTEL approaches. Compared to sample-level privacy bounding methods like gradient clipping  (Abadi et al.,  2016 ) , teacher-level bounding leaves room for teachers to effectively learn balanced knowledge via fair training. As a result, PFGuard provides strict DP guarantees for the generator and better preserves fairness compared to the combination of fairness-only and privacy-only techniques  see more analyses in Sec. 3.",
            "Moreover, PFGuard is compatible with a wide range of existing private generative models and preserves their utility. PTEL is widely adopted in private generative models as it provides prominent privacy-utility tradeoff  (Jordon et al.,  2018 ; Chen et al.,  2020 ; Long et al.,  2021 ; Wang et al.,  2021a ) . PFGuard can extend any of these models with a fair training stage as shown in Fig.  2 , which requires a  simple modification in the minibatch sampling process . Since additional fair sampling does not require additional training complexity compared to say adding a loss term for fairness, PFGuard preserves the privacy-utility tradeoff of PTEL as well while improving fairness. We also provide guidelines to control the fairness-privacy-utility tradeoff  see more details in Sec. 4.",
            "We can enforce DP in an algorithm in two steps  (Dwork et al.,  2014 ) . First, we bound its sensitivity (Def.  2.2 ), which captures the maximum influence of an individual sample. Second, we add random noise that is proportional to the sensitivity. A common way to ensure DP is to use a Gaussian mechanism  (Dwork et al.,  2014 )  (Thm.  2.1 ), which utilizes Gaussian random noise with a scale proportional to  l 2 subscript l 2 l_{2} italic_l start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT -sensitivity. Two datasets  D D D italic_D ,  D  superscript D  D^{\\prime} italic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  are adjacent if they differ by a single sample.",
            "Ensuring  fairness  in DP generative models can significantly  increase sensitivity  (Def.  2.2 ), leading to invalid DP guarantees. Sensitivity, which measures a data samples maximum impact on an algorithm, is crucial in DP generative models because the noise amount required for DP guarantees is often proportional to this sensitivity value. However, integrating fairness techniques in DP generative models can invalidate their sensitivity analyses by adjusting model outputs for fairness purposes. Examples include amplifying the impact of certain data samples to balance model training across groups  (Choi et al.,  2020 )  and directly feeding data attributes such as class labels or sensitive attributes (e.g., race, gender) to a generator for more balanced synthetic data  (Xu et al.,  2018 ; Sattigeri et al.,  2019 ; Yu et al.,  2020 ) , which can cause large fluctuations in the generator output with any variation in these data attributes. As a result, fairness techniques can end up increasing sensitivity and require more noise to maintain the same privacy level, compromising the original DP guarantees unless modifying DP techniques to add more noise. However, this modification is also not straightforward as assessing the increased sensitivity by fairness techniques can be challenging  (Tran et al.,  2021b ) .",
            "DP-SGD  is a standard DP technique  Chen et al. ( 2020 )  for converting non-DP algorithms to DP algorithms by modifying traditional stochastic gradient descent (SGD). Compared to SGD, DP-SGD 1) applies  gradient clipping  to limit the individual data points contribution, where  g  ( x ) g x \\bm{g}({\\mathbf{x}}) bold_italic_g ( bold_x )  is clipped to  g  ( x ) / max  ( 1 ,  g  ( x )  2 / C ) g x 1 subscript norm g x 2 C \\bm{g}({\\mathbf{x}})/\\max(1,||\\bm{g}({\\mathbf{x}})||_{2}/C) bold_italic_g ( bold_x ) / roman_max ( 1 , | | bold_italic_g ( bold_x ) | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT / italic_C )  (the sensitivity becomes the clipping threshold  C C C italic_C ), and 2) uses a  Gaussian mechanism  (Thm.  2.1 ) to add sufficient noise to ensure DP.",
            "We now propose PFGuard, the  first  generative framework that simultaneously achieves statistical fairness and DP on high-dimensional data, such as images. As shown in Fig.  2 , PFGuard balances privacy-fairness conflicts between fair and private training stages using an ensemble of teacher models as a key component. In Sec.  4.1 , we first explain the  fair training stage , which trains a fair teacher ensemble. In Sec.  4.2 , we then explain the  private training stage , which transfers the knowledge of this teacher ensemble to the generator with DP guarantees  ultimately training a generator that is both fair and private. In Sec.  4.3 , we lastly discuss how PFGuards integrated design offers advantages in terms of fairness, utility, and privacy compared to the naive approaches discussed in Sec.  3 .",
            "where  n j  ( x ) subscript n j x n_{j}({\\mathbf{x}}) italic_n start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( bold_x )  denotes the vote count for the  j j j italic_j -th class (i.e.,  n j  ( x ) = | { i : T i  ( x ) = j } | subscript n j x conditional-set i subscript T i x j n_{j}({\\mathbf{x}})=|\\{i:T_{i}({\\mathbf{x}}){=}j\\}| italic_n start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( bold_x ) = | { italic_i : italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_x ) = italic_j } | ), and  N  ( 0 ,  2 ) N 0 superscript  2 \\mathcal{N}(0,\\sigma^{2}) caligraphic_N ( 0 , italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )  denotes random Gaussian noise. Here, the  l 2 subscript l 2 l_{2} italic_l start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT -sensitivity (Def.  2.2 ) is  2 2 \\sqrt{2} square-root start_ARG 2 end_ARG , as a single data point affects at most one teacher, increasing the vote counts by 1 for one class and decreasing the count by 1 for another class (see a more detailed analysis in Sec.  B.2 ). Consequently, the GNMax aggregator satisfies  (  ,  )   (\\varepsilon,\\delta) ( italic_ , italic_ ) -DP for    8  ln  ( 1.25 /  ) /   8 1.25   \\sigma\\geq\\sqrt{8\\ln(1.25/\\delta)}/\\varepsilon italic_  square-root start_ARG 8 roman_ln ( 1.25 / italic_ ) end_ARG / italic_  based on the Gaussian mechanism (Thm.  2.1 ).",
            "We provide guidelines on how to set the number of teachers  n T subscript n T n_{T} italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  for PFGuard, which affects the privacy-fairness-utility tradeoff. While  n T subscript n T n_{T} italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  is typically tuned through experiments in PTEL approaches  (Long et al.,  2021 ; Wang et al.,  2021a ) , we need a different way to set  n T subscript n T n_{T} italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  to additionally consider fairness. Note that a large  n T subscript n T n_{T} italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  would result in a diverse ensemble that can generalize better, but also lead to a teacher receiving a data subset that is too small for training. We thus suggest  n T subscript n T n_{T} italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  to be at most   | D |  min s  S  p bias  ( s )  D subscript s S subscript p bias s \\lfloor|D|\\min_{s\\in\\mathcal{S}}p_{\\text{bias}}(s)\\rfloor  | italic_D | roman_min start_POSTSUBSCRIPT italic_s  caligraphic_S end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT bias end_POSTSUBSCRIPT ( italic_s )   where      \\lfloor\\cdot\\rfloor     denotes the floor function. This mathematical upper bound guarantees that each teacher  probabilistically  gets at least one sample of the smallest minority data group. In Sec.  5.3 , we demonstrate how this bound helps avoid compromising fairness. We also discuss how to set  n T subscript n T n_{T} italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  when sensitive attribute labels are unavailable in Sec.  C.2 .",
            "Privacy.  We use privacy budget    \\varepsilon italic_  for DP (Def.  2.1 ), which is preserved in both synthetic data and downstream tasks due to the post-processing property of DP (see more details in Sec.  B.1 ).",
            "Baselines  We compare PFGuard with three types of baselines: 1) privacy-only and fairness-only approaches for data generation, 2) simple combinations of these methods, and 3) recent privacy-fairness classification methods applicable to data generation. For 1) and 2), we use three state-of-the-art DP generative models  GS-WGAN  (Chen et al.,  2020 ) , G-PATE  (Long et al.,  2021 )  and DataLens  (Wang et al.,  2021a )   and a widely-adopted fair reweighting method  (Choi et al.,  2020 ) . For 3), we extend DP-SGD-F  (Xu et al.,  2020 )  and DPSGD-Global-Adapt  (Esipova et al.,  2022 ) , which are fair variants of DP-SGD  (Abadi et al.,  2016 ) . Specifically, we replace the DP-SGD used in GS-WGAN with these fairness-enhanced variants. We faithfully implement all baseline methods with their official codes and reported hyperparameters. More details on baseline methods are in Sec.  D.2 .",
            "Table  2  shows the fairness and utility performances on downstream tasks. Compared to the synthetic data analysis, PFGuard enhances not only fairness, but also overall utility, especially for CNN models. We suspect that the increased overall utility results from the improved fairness in the input synthetic data, promoting more balanced learning among groups.",
            "We compare our privacy-fairness-utility performance with naive combinations of prior approaches. We evaluate performance under two bias settings: 1) subgroup bias and 2) unknown subgroup bias. Table  5.2  shows the results, which aligns with our privacy-fairness counteraction analysis in Sec.  3 . On the one hand, fairness-only reweighting approaches compromise privacy due to the increased iterations from modifying the loss function for fair training (i.e., the more a model uses the data, the weaker privacy it provides). On the other hand, privacy-fairness classification techniques maintain the original privacy guarantees, but significantly degrade utility and fairness, resulting in lower image quality and size disparities across groups. We further discuss this fairness-utility degradation in Sec.  A , where we suspect that directly changing gradient clipping thresholds for fairness may severely affect utility when used in generative settings. In contrast, PFGuard is the only method that successfully achieves both privacy and fairness and preserves the closest utility to the original models.",
            "More Analyses  We provide more experiments in Sec.  E , including a comparison of computation time (Sec.  E.1 ), results on different datasets such as FashionMNIST (Sec.  E.2 ), and employing an additional normalization technique to further enhance the overall image quality (Sec.  E.3 ).",
            "All datasets, methodologies, and experimental setups utilized in our study are described in detail in the supplementary material. More specifically, we provide a description of the proposed algorithm in Sec.  C.1 , details of datasets and preprocessing in Sec.  D.1 , and implementation details including hyperparameters in Sec.  D.2  to ensure reproducibility.",
            "Classification and generation settings often concentrate on  different DP notions  or rely on  different assumptions , which hinders simple extensions of techniques between them. In the classification setting, Differential Privacy  w.r.t. sensitive attribute   (Jagielski et al.,  2019 )  is commonly addressed  (Jagielski et al.,  2019 ; Mozannar et al.,  2020 ; Tran et al.,  2021b ;  2022 ; Lowy et al.,  2023 ) , which considers the demographic group attribute as the only private information. This DP notion requires less DP noise compared to a more general notion of DP (Def.  2.1 ), which protects  all  input features, and enables a better privacy-utility tradeoff for DP classifiers. However, in the generative setting, a general notion of DP is mostly addressed, as presumed non-private features may in fact encode private information (e.g., pixel values in a facial image). Therefore, simply extending classification techniques to generative settings can be challenging, as it necessitates rigorous mathematical proofs for corresponding DP notions and may add a large DP noise when adapting to a general DP notion. Moreover, classification techniques can rely on convex objective functions  (Tran et al.,  2021a ) , but the assumption of convexity does not usually hold in generative models  (Goodfellow et al.,  2014 ) .",
            "Continuing from Sec.  2  and Sec.  4.2 , we provide more details on differential privcy (DP).",
            "Continuing from Sec.  2 , we detail the post-processing property of DP. Let  G G G italic_G  be an  (  ,  )   (\\varepsilon,\\delta) ( italic_ , italic_ ) -DP generator, which produces the synthetic data from input random noise  z  Z z Z {\\mathbf{z}}\\in\\mathcal{Z} bold_z  caligraphic_Z . Then, the synthetic dataset  D ~ = G  ( z ) ~ D G z \\tilde{D}=G({\\mathbf{z}}) over~ start_ARG italic_D end_ARG = italic_G ( bold_z )  is also satisfies  (  ,  )   (\\varepsilon,\\delta) ( italic_ , italic_ ) -DP due to the post-processing theorem, as the random noise  z z {\\mathbf{z}} bold_z  is independent of the private dataset  D D D italic_D , which is used to train the DP generator.",
            "Continuing from Sec.  4.2 , we echo the sensitivity analysis of GNMax aggregator provided by  Papernot et al. ( 2018 )  for readers convenience.",
            "Since a single training data point only affects at most one teacher due to data disjointness, changing one data sample will at most change the votes by 1 for two classes, where we denote here as classes  i i i italic_i  and  j j j italic_j . Given the two adjacent datasets  D , D  D superscript D  \\mathcal{D},\\mathcal{D}^{\\prime} caligraphic_D , caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  which differ by a single data point, let the aggregated vote counts are  n = ( n 1 , ... , n c ) n subscript n 1 ... subscript n c \\mathbf{n}=(n_{1},\\dots,n_{c}) bold_n = ( italic_n start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_n start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT )  and  n  = ( n 1  , ... , n c  ) superscript n  superscript subscript n 1  ... superscript subscript n c  \\mathbf{n}^{\\prime}=(n_{1}^{\\prime},\\dots,n_{c}^{\\prime}) bold_n start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = ( italic_n start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , ... , italic_n start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) , respectively. The  l 2 subscript l 2 l_{2} italic_l start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT -sensitivity (Def.  2.2 ) can be derived as follows:",
            "Continuing from Sec.  4.2 , we provide more details on the PFGuard framework.",
            "Continuing from Sec.  4.2 , we provide the pseudocode to describe the full training algorithm when PFGuard is integrated on top of a PTEL-based generative model.",
            "Continuing from Sec.  4.2 , we discuss how to extend the proposed upper bound on the number of teachers (i.e.,   | D |  min s  S  p bias  ( s )  D subscript s S subscript p bias s \\lfloor|D|\\min_{s\\in\\mathcal{S}}p_{\\text{bias}}(s)\\rfloor  | italic_D | roman_min start_POSTSUBSCRIPT italic_s  caligraphic_S end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT bias end_POSTSUBSCRIPT ( italic_s )  ) in settings where the label of sensitive attribute  s s {\\mathbf{s}} bold_s  is unavailable. The proposed upper bound does not rely on full knowledge of  p bias subscript p bias p_{\\text{bias}} italic_p start_POSTSUBSCRIPT bias end_POSTSUBSCRIPT , but the distribution w.r.t. sensitive attributes. Thus, given the training data, we can estimate the subgroup distributions using traditional techniques like K-means clustering  (Macqueen,  1967 )  or random subset labeling  (Forestier & Wemmert,  2016 ) . We note that these estimations can be effective, but may introduce some errors or additional overhead, such as increased computational time.",
            "Continuing from Sec.  5 , we provide more details on experiment settings. In all experiments, we use PyTorch and perform experiments using NVIDIA Quadro RTX 8000 GPUs. Also, we repeat all experiments 10 times and report the mean and standard deviation of the top 3 results. The reason we report the top-3 results is to favor the simple privacy-fairness baselines (e.g., Reweighting in Table  5.2 ), which tend to fail frequently. We compare their best performances with PFGuard.",
            "Continuing from Sec.  5.2  and Sec.  5.4 , we show full results with standard deviation. Table  9  and Table  10  shows the full results of Table  4 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:   Comparison of privacy-fairness-utility performance on MNIST under   = 10  10 \\varepsilon{=}10 italic_ = 10 , using GS-WGAN as the base DP generator (see Sec.  D.2  for more details). The first three rows represent upper bound performances for vanilla, DP-only, and fair-only models. Evaluations cover both subgroup bias and unknown subgroup bias, where no S indicates whether the method is applicable without group labels. perc denotes the proportion of public data used compared to the training data size. - indicates no samples are generated. Lower values are better across all metrics, and we boldface the best results in each subgroup bias and unknown subgroup bias settings.",
        "table": "A6.EGx3",
        "footnotes": [
            ""
        ],
        "references": [
            "We can extend reweighting for fairness to also satisfy DP using DP-SGD, but finding the clipping threshold  C C C italic_C  that balances fairness and utility can be challenging. Note that if we perform DP-SGD and then reweighting, privacy breach may occur by amplifying sample gradients beyond  C C C italic_C , invalidating the sensitivity derived from gradient clipping. We thus consider performing reweighting and then DP-SGD, which at least guarantees DP for reweighting-based fair generative models. However, the clipping now undoes the fairness adjustments where reweighted gradients  g  ( x )  h  ( x )  g x h x \\bm{g}({\\mathbf{x}})\\cdot h({\\mathbf{x}}) bold_italic_g ( bold_x )  italic_h ( bold_x )  are clipped to  g  ( x )  h  ( x ) / max  ( 1 ,  g  ( x )  h  ( x )  2 / C )  g x h x 1 subscript norm  g x h x 2 C \\bm{g}({\\mathbf{x}})\\cdot h({\\mathbf{x}})/\\max(1,||\\bm{g}({\\mathbf{x}})\\cdot h% ({\\mathbf{x}})||_{2}/C) bold_italic_g ( bold_x )  italic_h ( bold_x ) / roman_max ( 1 , | | bold_italic_g ( bold_x )  italic_h ( bold_x ) | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT / italic_C ) , and Eq.  1  does not hold if  C  g  ( x ) C g x C\\leq\\bm{g}({\\mathbf{x}}) italic_C  bold_italic_g ( bold_x ) . Here one solution is to use a larger  C C C italic_C  such that  C  g  ( x ) C g x C\\geq\\bm{g}({\\mathbf{x}}) italic_C  bold_italic_g ( bold_x ) . However, increasing  C C C italic_C  also increases the noise required for DP, which reduces utility (Fig.  3 ). As a result, selecting a  C C C italic_C  that balances fairness and utility may necessitate extensive hyperparameter tuning  (Bu et al.,  2024 ) , complicating the systematic integration of fairness into DP generative models.",
            "We now propose PFGuard, the  first  generative framework that simultaneously achieves statistical fairness and DP on high-dimensional data, such as images. As shown in Fig.  2 , PFGuard balances privacy-fairness conflicts between fair and private training stages using an ensemble of teacher models as a key component. In Sec.  4.1 , we first explain the  fair training stage , which trains a fair teacher ensemble. In Sec.  4.2 , we then explain the  private training stage , which transfers the knowledge of this teacher ensemble to the generator with DP guarantees  ultimately training a generator that is both fair and private. In Sec.  4.3 , we lastly discuss how PFGuards integrated design offers advantages in terms of fairness, utility, and privacy compared to the naive approaches discussed in Sec.  3 .",
            "Our fair sampling technique is also extensible to private settings where the label of sensitive attribute  s s {\\mathbf{s}} bold_s  is  unavailable , for example due to privacy regulations  (Jagielski et al.,  2019 ; Mozannar et al.,  2020 ; Tran et al.,  2022 ) . In such settings, we can employ a binary classification approach to estimate  h  ( x ) h x h({\\mathbf{x}}) italic_h ( bold_x )  like  Choi et al. ( 2020 ) . While their work focuses on  non-private settings  and assumes an unbiased public reference data on the order of 10%100% of  | D | D |D| | italic_D |  for the estimation, this assumption can be unrealistic in private domains due to the lack of public data. Our empirical study in Sec.  5.3  shows that we can achieve fairness with only 110% of the data, leveraging the ensemble learning of  multiple-teacher structure , which can further reduce the estimation error. Note that in this extension, the convergence guarantee may not hold, as  D i  p bias similar-to subscript D i subscript p bias D_{i}\\sim p_{\\text{bias}} italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  italic_p start_POSTSUBSCRIPT bias end_POSTSUBSCRIPT  in step (2) might not be true in practice if the dataset is randomly partitioned without considering sensitive attribute labels.",
            "We provide guidelines on how to set the number of teachers  n T subscript n T n_{T} italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  for PFGuard, which affects the privacy-fairness-utility tradeoff. While  n T subscript n T n_{T} italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  is typically tuned through experiments in PTEL approaches  (Long et al.,  2021 ; Wang et al.,  2021a ) , we need a different way to set  n T subscript n T n_{T} italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  to additionally consider fairness. Note that a large  n T subscript n T n_{T} italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  would result in a diverse ensemble that can generalize better, but also lead to a teacher receiving a data subset that is too small for training. We thus suggest  n T subscript n T n_{T} italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  to be at most   | D |  min s  S  p bias  ( s )  D subscript s S subscript p bias s \\lfloor|D|\\min_{s\\in\\mathcal{S}}p_{\\text{bias}}(s)\\rfloor  | italic_D | roman_min start_POSTSUBSCRIPT italic_s  caligraphic_S end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT bias end_POSTSUBSCRIPT ( italic_s )   where      \\lfloor\\cdot\\rfloor     denotes the floor function. This mathematical upper bound guarantees that each teacher  probabilistically  gets at least one sample of the smallest minority data group. In Sec.  5.3 , we demonstrate how this bound helps avoid compromising fairness. We also discuss how to set  n T subscript n T n_{T} italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  when sensitive attribute labels are unavailable in Sec.  C.2 .",
            "We discuss how PFGuard overcomes the challenges of naive approaches discussed in Sec.  3 .",
            "PFGuard can sidestep  privacy breaches  and  fairness cancellation  arising from privacy-fairness conflicts. Applying fairness-only techniques to existing DP generators can compromise DP guarantees and require complex sensitivity assessments; in contrast, PFGuard automatically preserves DP guarantees of any PTEL-based DP generators through data disjointness, eliminating the need of such assessments. Privacy-only techniques often use sample-level bounding, which  directly  limits an individual samples influence (e.g., gradient clipping discussed in Sec.  3 ) and can lead to fairness cancellation by suppressing the use of minority data. In contrast, PFGuard uses  indirect  privacy bounding, in the sense that we limit the knowledge transfer of teacher models in order to limit individual samples influence. Since there are no DP constraints during the teacher learning, the teacher models can effectively learn balanced knowledge across data groups.",
            "We compare our privacy-fairness-utility performance with naive combinations of prior approaches. We evaluate performance under two bias settings: 1) subgroup bias and 2) unknown subgroup bias. Table  5.2  shows the results, which aligns with our privacy-fairness counteraction analysis in Sec.  3 . On the one hand, fairness-only reweighting approaches compromise privacy due to the increased iterations from modifying the loss function for fair training (i.e., the more a model uses the data, the weaker privacy it provides). On the other hand, privacy-fairness classification techniques maintain the original privacy guarantees, but significantly degrade utility and fairness, resulting in lower image quality and size disparities across groups. We further discuss this fairness-utility degradation in Sec.  A , where we suspect that directly changing gradient clipping thresholds for fairness may severely affect utility when used in generative settings. In contrast, PFGuard is the only method that successfully achieves both privacy and fairness and preserves the closest utility to the original models.",
            "More Analyses  We provide more experiments in Sec.  E , including a comparison of computation time (Sec.  E.1 ), results on different datasets such as FashionMNIST (Sec.  E.2 ), and employing an additional normalization technique to further enhance the overall image quality (Sec.  E.3 ).",
            "Continuing from Sec.  3 , we provide more details of potential challenges when one tries to extend fairness-privacy classification techniques  (Jagielski et al.,  2019 ; Mozannar et al.,  2020 ; Tran et al.,  2021b ;  2022 ; Lowy et al.,  2023 )  to generative settings due to the fundamentally different goals.",
            "Reweighting . As outlined in Sec.  3 , the reweighting approach modifies the loss term of a generative model to achieve fairness. We use likelihood ratio computed in each bias setting as the reweighting factor and only modifies the loss term of a discriminator, following their approach. When computing likelihood ratio, we directly compute the value as in Eq.  5  using sensitive group labels for subgroup bias setting; we estimate the value using binary classification approach, implemented in their official Github code for unknown subgroup setting. We note that a public reference dataset is required in the estimation process to effectively train a binary classifier.",
            "Continuing from Sec.  5 , Sec.  5.1 , Sec.  5.3 , and Sec.  5.4 , we provide more experimental results.",
            "Continuing from Sec.  5.3 , we compare the computational time when integrating PFGuard with existing PTEL-based generative models. Table shows that PFGuard incurs minimal overhead in computational time ( < 4 % absent percent 4 <4\\% < 4 % ), due to the simple modification in minibatch sampling for fairness.",
            "Continuing from Sec.  5.3 , we show the results of the analysis in synthetic data (Table  6 ) and downstream tasks (Table  7  ) evaluated on FashionMNIST. Compared to the results evaluated on MNIST, private generative models often generate more imbalanced synthetic data w.r.t. sensitive groups and exhibit lower overall image quality. In comparison, PFGuard consistently improves both fairness and overall utility in most cases, similar to the results observed in the MNIST evaluation.",
            "Continuing from Sec.  5.3 , we investigate the impact of the normalization factor on the overall image quality of PFGuard. While we use a traditional normalization factor  N 1 =  i h  ( x i ) subscript N 1 subscript i h subscript x i N_{1}=\\sum_{i}h(x_{i}) italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT =  start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_h ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  for  w  ( x i )  h  ( x i ) proportional-to w subscript x i h subscript x i w(x_{i})\\propto h(x_{i}) italic_w ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  italic_h ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  in our SIR-based sampling algorithm, we can employ additional normalization techniques to boost the performance. For example, we can use  N 2 =  i h  ( x i ) / N  i subscript N 2 subscript i h subscript x i subscript N i N_{2}=\\sum_{i}h(x_{i})/N_{-i} italic_N start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT =  start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_h ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) / italic_N start_POSTSUBSCRIPT - italic_i end_POSTSUBSCRIPT  for  w  ( x i )  h  ( x i ) / N  i proportional-to w subscript x i h subscript x i subscript N i w(x_{i})\\propto h(x_{i})/N_{-i} italic_w ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  italic_h ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) / italic_N start_POSTSUBSCRIPT - italic_i end_POSTSUBSCRIPT , where  N  i =  i h  ( x i )  h  ( x i ) subscript N i subscript i h subscript x i h subscript x i N_{-i}=\\sum_{i}h(x_{i})-h(x_{i}) italic_N start_POSTSUBSCRIPT - italic_i end_POSTSUBSCRIPT =  start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_h ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - italic_h ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) , which is known to help faster convergence of SIR algorithms to the target distribution  p bal subscript p bal p_{\\text{bal}} italic_p start_POSTSUBSCRIPT bal end_POSTSUBSCRIPT   (Skare et al.,  2003 ) ."
        ]
    },
    "id_table_4": {
        "caption": "Figure 4:  Fairness performances when varying bias levels (   \\gamma italic_ ) given a fixed number of teachers, evaluated on MNIST with multi-class bias. We downsize the class 8 to    \\gamma italic_  times smaller than the other classes to make it the minority class and use GS-WGAN as the baseline model.",
        "table": "A6.EGx4",
        "footnotes": [],
        "references": [
            "We now propose PFGuard, the  first  generative framework that simultaneously achieves statistical fairness and DP on high-dimensional data, such as images. As shown in Fig.  2 , PFGuard balances privacy-fairness conflicts between fair and private training stages using an ensemble of teacher models as a key component. In Sec.  4.1 , we first explain the  fair training stage , which trains a fair teacher ensemble. In Sec.  4.2 , we then explain the  private training stage , which transfers the knowledge of this teacher ensemble to the generator with DP guarantees  ultimately training a generator that is both fair and private. In Sec.  4.3 , we lastly discuss how PFGuards integrated design offers advantages in terms of fairness, utility, and privacy compared to the naive approaches discussed in Sec.  3 .",
            "We ensure DP by privatizing  knowledge transfer  from a teacher ensemble to the generator. Although the sampling technique in Sec.  4.1  transfers more balanced knowledge,  privacy risks  can be also transferred due to privacy-fairness conflicts. For example, if certain data samples are resampled repeatedly during teacher training, privacy risks like memorization can occur in the teacher models and be transferred to the generator. We thus privatize the knowledge transfer with DP techniques to provide strict DP guarantees in the generator. Note that only the generator needs to have privacy as it is the one that is released publicly to produce synthetic data.",
            "PFGuard can be easily integrated with existing PTEL-based generative models by simply modifying the minibatch sampling process as described in Sec.  4.1 . PTEL has been widely adopted to train generators with privatized teacher output to ensure DP  (Jordon et al.,  2018 ; Chen et al.,  2020 ; Long et al.,  2021 ; Wang et al.,  2021a ) . Although the exact sensitivity values of these PTEL-based generative models vary depending on what teacher knowledge is aggregated (e.g., votes on class labels  (Jordon et al.,  2018 )  or gradient directions  (Wang et al.,  2021a ) ), PFGuard preserves any sensitivity as long as the PTEL enforce data disjointness; even with fair sampling, a single data point still affects only one teacher. PFGuard is thus compatible with various PTEL-based generative models, enhancing fairness while preserving DP guarantees  see Sec.  C.1  for the full algorithm.",
            "Table  4  shows the fairness and utility performances under these challenging conditions. We observe that DP generative models often exhibit extreme accuracy disparities even with a simplistic class bias setting, achieving over 90% accuracy for the majority class while achieving accuracy below 25% for the minority class. PFGuard consistently enhances the minority class performance and reduces accuracy disparity, while there is still room for improvements. Our results underscore the importance of tackling both privacy and fairness in future studies, encouraging more research in this critical area.",
            "Continuing from Sec.  2  and Sec.  4.2 , we provide more details on differential privcy (DP).",
            "Continuing from Sec.  4.2 , we echo the sensitivity analysis of GNMax aggregator provided by  Papernot et al. ( 2018 )  for readers convenience.",
            "Continuing from Sec.  4.2 , we provide more details on the PFGuard framework.",
            "Continuing from Sec.  4.2 , we provide the pseudocode to describe the full training algorithm when PFGuard is integrated on top of a PTEL-based generative model.",
            "Continuing from Sec.  4.2 , we discuss how to extend the proposed upper bound on the number of teachers (i.e.,   | D |  min s  S  p bias  ( s )  D subscript s S subscript p bias s \\lfloor|D|\\min_{s\\in\\mathcal{S}}p_{\\text{bias}}(s)\\rfloor  | italic_D | roman_min start_POSTSUBSCRIPT italic_s  caligraphic_S end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT bias end_POSTSUBSCRIPT ( italic_s )  ) in settings where the label of sensitive attribute  s s {\\mathbf{s}} bold_s  is unavailable. The proposed upper bound does not rely on full knowledge of  p bias subscript p bias p_{\\text{bias}} italic_p start_POSTSUBSCRIPT bias end_POSTSUBSCRIPT , but the distribution w.r.t. sensitive attributes. Thus, given the training data, we can estimate the subgroup distributions using traditional techniques like K-means clustering  (Macqueen,  1967 )  or random subset labeling  (Forestier & Wemmert,  2016 ) . We note that these estimations can be effective, but may introduce some errors or additional overhead, such as increased computational time.",
            "We create binary class bias using gender attributes, where we set female and male images as  Y = 1 Y 1 Y=1 italic_Y = 1  and  Y = 0 Y 0 Y=0 italic_Y = 0 , respectively. As discussed in the main text, DP generative models often show low performance on CelebA in challenging bias scenarios like multi-class bias, which can hinder the reliability of fairness analyses (e.g., a random generator achieves perfect fairness by outputting random images regardless of data groups). Notably, we show that DP generative models can produce highly biased synthetic data even in this simple binary class bias setting (Table  4 ).",
            "Continuing from Sec.  5 , Sec.  5.1 , Sec.  5.3 , and Sec.  5.4 , we provide more experimental results.",
            "Continuing from Sec.  5.2  and Sec.  5.4 , we show full results with standard deviation. Table  9  and Table  10  shows the full results of Table  4 ."
        ]
    },
    "id_table_5": {
        "caption": "Figure 5:  Fairness and utility performances for varying reference dataset size ratio compared to the training dataset size, evaluated on MNIST with unknown subgroup bias under   = 10  10 \\varepsilon{=}10 italic_ = 10 . Lower values are better across all metrics used to evaluate fairness and utility.",
        "table": "A6.EGx5",
        "footnotes": [],
        "references": [
            "Our study is the first to reveal that fairness and privacy techniques can counteract each other. We also demonstrate how both can be compromised  see empirical results in Sec.  5 .",
            "Our fair sampling technique is also extensible to private settings where the label of sensitive attribute  s s {\\mathbf{s}} bold_s  is  unavailable , for example due to privacy regulations  (Jagielski et al.,  2019 ; Mozannar et al.,  2020 ; Tran et al.,  2022 ) . In such settings, we can employ a binary classification approach to estimate  h  ( x ) h x h({\\mathbf{x}}) italic_h ( bold_x )  like  Choi et al. ( 2020 ) . While their work focuses on  non-private settings  and assumes an unbiased public reference data on the order of 10%100% of  | D | D |D| | italic_D |  for the estimation, this assumption can be unrealistic in private domains due to the lack of public data. Our empirical study in Sec.  5.3  shows that we can achieve fairness with only 110% of the data, leveraging the ensemble learning of  multiple-teacher structure , which can further reduce the estimation error. Note that in this extension, the convergence guarantee may not hold, as  D i  p bias similar-to subscript D i subscript p bias D_{i}\\sim p_{\\text{bias}} italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  italic_p start_POSTSUBSCRIPT bias end_POSTSUBSCRIPT  in step (2) might not be true in practice if the dataset is randomly partitioned without considering sensitive attribute labels.",
            "We provide guidelines on how to set the number of teachers  n T subscript n T n_{T} italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  for PFGuard, which affects the privacy-fairness-utility tradeoff. While  n T subscript n T n_{T} italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  is typically tuned through experiments in PTEL approaches  (Long et al.,  2021 ; Wang et al.,  2021a ) , we need a different way to set  n T subscript n T n_{T} italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  to additionally consider fairness. Note that a large  n T subscript n T n_{T} italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  would result in a diverse ensemble that can generalize better, but also lead to a teacher receiving a data subset that is too small for training. We thus suggest  n T subscript n T n_{T} italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  to be at most   | D |  min s  S  p bias  ( s )  D subscript s S subscript p bias s \\lfloor|D|\\min_{s\\in\\mathcal{S}}p_{\\text{bias}}(s)\\rfloor  | italic_D | roman_min start_POSTSUBSCRIPT italic_s  caligraphic_S end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT bias end_POSTSUBSCRIPT ( italic_s )   where      \\lfloor\\cdot\\rfloor     denotes the floor function. This mathematical upper bound guarantees that each teacher  probabilistically  gets at least one sample of the smallest minority data group. In Sec.  5.3 , we demonstrate how this bound helps avoid compromising fairness. We also discuss how to set  n T subscript n T n_{T} italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  when sensitive attribute labels are unavailable in Sec.  C.2 .",
            "The fair training of PFGuard adds minimal training complexity, preserving the utility for the subsequent private training stage. The proposed sampling technique requires a simple modification in the minibatch sampling process for fairness, avoiding the need for additional fairness loss terms  (Sattigeri et al.,  2019 ; Yu et al.,  2020 )  or auxiliary classifiers  (Tan et al.,  2020 ; Um & Suh,  2023 )  typically employed in fairness-only techniques. In Sec.  5 , we also show that PFGuard incurs negligible overhead in computation time when integrated with existing PTEL-based generative models.",
            "We compare our privacy-fairness-utility performance with naive combinations of prior approaches. We evaluate performance under two bias settings: 1) subgroup bias and 2) unknown subgroup bias. Table  5.2  shows the results, which aligns with our privacy-fairness counteraction analysis in Sec.  3 . On the one hand, fairness-only reweighting approaches compromise privacy due to the increased iterations from modifying the loss function for fair training (i.e., the more a model uses the data, the weaker privacy it provides). On the other hand, privacy-fairness classification techniques maintain the original privacy guarantees, but significantly degrade utility and fairness, resulting in lower image quality and size disparities across groups. We further discuss this fairness-utility degradation in Sec.  A , where we suspect that directly changing gradient clipping thresholds for fairness may severely affect utility when used in generative settings. In contrast, PFGuard is the only method that successfully achieves both privacy and fairness and preserves the closest utility to the original models.",
            "We validate the proposed theoretical upper bound on the number of teachers for fairness, which depends on the bias level of the training data. To effectively simulate scenarios where a teacher receives only a small subset of minority data, we evaluate PFGuard in a multi-class bias setting, downsizing the minority class (i.e., class 8 for MNIST) by a factor of    \\gamma italic_ . Given that MNIST has fewer than 6,000 samples for class 8, our proposed upper bound is    5  5 \\gamma{\\leq}5 italic_  5  if we fix the number of teachers to 1,000. Fig.  5  shows that exceeding   = 5  5 \\gamma{=}5 italic_ = 5  leads to a noticeable decline in accuracy for the minority class, which is consistent with our theoretical results. It is noteworthy that even with the decline, PFGuard shows higher accuracy than the privacy-only baseline, which shows a consistent decrease in accuracy for the minority as    \\gamma italic_  increases.",
            "We explore the influence of the reference dataset size when PFGuard is extended to unknown subgroup bias setting. Fig.  5  shows PFGuard achieves comparable fairness even with a small reference dataset size, while showing a slight decline in the overall utility.",
            "Continuing from Sec.  5 , we provide more details on experiment settings. In all experiments, we use PyTorch and perform experiments using NVIDIA Quadro RTX 8000 GPUs. Also, we repeat all experiments 10 times and report the mean and standard deviation of the top 3 results. The reason we report the top-3 results is to favor the simple privacy-fairness baselines (e.g., Reweighting in Table  5.2 ), which tend to fail frequently. We compare their best performances with PFGuard.",
            "Continuing from Sec.  5 , we provide more details on datasets. We use three datasets: MNIST  (LeCun et al.,  1998 ) , FashionMNIST  (Xiao et al.,  2017 ) , and CelebA  (Liu et al.,  2015 ) .  MNIST and FashionMNIST  contain grayscale images with 28 x 28 pixels and 10 classes. Both datasets have 60,000 training examples and 10,000 testing examples.  CelebA  contains 202,599 celebrity face images. We use the official preprocessed version with face alignment and follow the official training and testing partition  (Liu et al.,  2015 ) . Note that we are using image datasets instead of the traditional smaller tabular benchmarks for fairness because our goal is to make PFGuard work on higher dimensional data such as images.",
            "Continuing from Sec.  5 , we provide more details on baseline approaches used in our experiments.",
            "Reweighting . As outlined in Sec.  3 , the reweighting approach modifies the loss term of a generative model to achieve fairness. We use likelihood ratio computed in each bias setting as the reweighting factor and only modifies the loss term of a discriminator, following their approach. When computing likelihood ratio, we directly compute the value as in Eq.  5  using sensitive group labels for subgroup bias setting; we estimate the value using binary classification approach, implemented in their official Github code for unknown subgroup setting. We note that a public reference dataset is required in the estimation process to effectively train a binary classifier.",
            "Continuing from Sec.  5 , Sec.  5.1 , Sec.  5.3 , and Sec.  5.4 , we provide more experimental results.",
            "Continuing from Sec.  5.3 , we compare the computational time when integrating PFGuard with existing PTEL-based generative models. Table shows that PFGuard incurs minimal overhead in computational time ( < 4 % absent percent 4 <4\\% < 4 % ), due to the simple modification in minibatch sampling for fairness.",
            "Continuing from Sec.  5.3 , we show the results of the analysis in synthetic data (Table  6 ) and downstream tasks (Table  7  ) evaluated on FashionMNIST. Compared to the results evaluated on MNIST, private generative models often generate more imbalanced synthetic data w.r.t. sensitive groups and exhibit lower overall image quality. In comparison, PFGuard consistently improves both fairness and overall utility in most cases, similar to the results observed in the MNIST evaluation.",
            "Continuing from Sec.  5.3 , we investigate the impact of the normalization factor on the overall image quality of PFGuard. While we use a traditional normalization factor  N 1 =  i h  ( x i ) subscript N 1 subscript i h subscript x i N_{1}=\\sum_{i}h(x_{i}) italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT =  start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_h ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  for  w  ( x i )  h  ( x i ) proportional-to w subscript x i h subscript x i w(x_{i})\\propto h(x_{i}) italic_w ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  italic_h ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  in our SIR-based sampling algorithm, we can employ additional normalization techniques to boost the performance. For example, we can use  N 2 =  i h  ( x i ) / N  i subscript N 2 subscript i h subscript x i subscript N i N_{2}=\\sum_{i}h(x_{i})/N_{-i} italic_N start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT =  start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_h ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) / italic_N start_POSTSUBSCRIPT - italic_i end_POSTSUBSCRIPT  for  w  ( x i )  h  ( x i ) / N  i proportional-to w subscript x i h subscript x i subscript N i w(x_{i})\\propto h(x_{i})/N_{-i} italic_w ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  italic_h ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) / italic_N start_POSTSUBSCRIPT - italic_i end_POSTSUBSCRIPT , where  N  i =  i h  ( x i )  h  ( x i ) subscript N i subscript i h subscript x i h subscript x i N_{-i}=\\sum_{i}h(x_{i})-h(x_{i}) italic_N start_POSTSUBSCRIPT - italic_i end_POSTSUBSCRIPT =  start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_h ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - italic_h ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) , which is known to help faster convergence of SIR algorithms to the target distribution  p bal subscript p bal p_{\\text{bal}} italic_p start_POSTSUBSCRIPT bal end_POSTSUBSCRIPT   (Skare et al.,  2003 ) .",
            "Continuing from Sec.  5.2  and Sec.  5.4 , we show full results with standard deviation. Table  9  and Table  10  shows the full results of Table  4 ."
        ]
    },
    "id_table_6": {
        "caption": "Table 4:  Fairness and utility performances of private generative models with and without PFGuard on downstream tasks, evaluated on CelebA with binary class bias under   = 1  1 \\varepsilon{=}1 italic_ = 1 . GS-WGAN is excluded due to lower image quality in this setting. Blue and red arrows indicate positive and negative changes, respectively. We provide the full table with standard deviations in Sec.  E.4 .",
        "table": "S5.T1.56.54",
        "footnotes": [
            ""
        ],
        "references": [
            "Continuing from Sec.  5.3 , we show the results of the analysis in synthetic data (Table  6 ) and downstream tasks (Table  7  ) evaluated on FashionMNIST. Compared to the results evaluated on MNIST, private generative models often generate more imbalanced synthetic data w.r.t. sensitive groups and exhibit lower overall image quality. In comparison, PFGuard consistently improves both fairness and overall utility in most cases, similar to the results observed in the MNIST evaluation.",
            "Continuing from Sec.  6 , we present more related work."
        ]
    },
    "id_table_7": {
        "caption": "Algorithm 1    Integrating PFGuard with PTEL-based generative models",
        "table": "S5.T2.61.59",
        "footnotes": [],
        "references": [
            "Continuing from Sec.  5.3 , we show the results of the analysis in synthetic data (Table  6 ) and downstream tasks (Table  7  ) evaluated on FashionMNIST. Compared to the results evaluated on MNIST, private generative models often generate more imbalanced synthetic data w.r.t. sensitive groups and exhibit lower overall image quality. In comparison, PFGuard consistently improves both fairness and overall utility in most cases, similar to the results observed in the MNIST evaluation."
        ]
    },
    "id_table_8": {
        "caption": "Table 5:  Comparison of computational time of private generative models with and without PFGuard.",
        "table": "S5.SS2.8.8.6",
        "footnotes": [],
        "references": [
            "Table  8  shows the comparison of image quality using the MNIST dataset, measuring image quality with FID, where a lower value is better. While both  N 1 subscript N 1 N_{1} italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and  N 2 subscript N 2 N_{2} italic_N start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  demonstrate comparable performance when using a large batch size, the performance gap becomes more evident as the batch size decreases. This empirical evidence shows that the performance of PFGuard can be further improved by additionally employing various normalization techniques."
        ]
    },
    "id_table_9": {
        "caption": "Table 6:  Fairness and utility performances of private generative models with and without PFGuard on synthetic data, evaluated on FashionMNIST with subgroup bias under   = 10  10 \\varepsilon=10 italic_ = 10 . Blue and red arrows indicate positive and negative changes, respectively. Lower values are better across all metrics.",
        "table": "S5.T4.22.20",
        "footnotes": [],
        "references": [
            "Continuing from Sec.  5.2  and Sec.  5.4 , we show full results with standard deviation. Table  9  and Table  10  shows the full results of Table  4 ."
        ]
    },
    "id_table_10": {
        "caption": "Table 7:  Fairness and utility performances of private generative models with and without PFGuard on downstream tasks, evaluated on FashionMNIST with subgroup bias under   = 10  10 \\varepsilon=10 italic_ = 10 . Blue and red arrows indicate positive and negative changes, respectively.",
        "table": "A6.EGx6",
        "footnotes": [],
        "references": [
            "Continuing from Sec.  5.2  and Sec.  5.4 , we show full results with standard deviation. Table  9  and Table  10  shows the full results of Table  4 ."
        ]
    },
    "id_table_11": {
        "caption": "Table 8:  Influence of normalization factor on MNIST with corresponding DP guarantees (   \\varepsilon italic_ ) to different batch sizes. GS-WGAN is used as the base DP generator.",
        "table": "A5.T5.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "Table 9:  Full results of fairness and utility performances of private generative models with and without PFGuard on downstream tasks, evaluated on CelebA(S) with binary class bias under    \\varepsilon italic_  = 1. GS-WGAN is excluded due to lower image quality in this setting. Blue and red arrows indicate positive and negative changes, respectively.",
        "table": "A5.T6.56.54",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "Table 10:  Full results of fairness and utility performances of private generative models with and without PFGuard on downstream tasks, evaluated on CelebA(L) with binary class bias under    \\varepsilon italic_  = 1. GS-WGAN is excluded due to lower image quality in this setting. Blue and red arrows indicate positive and negative changes, respectively.",
        "table": "A5.T7.62.60",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "A5.T8.7.5",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "",
        "table": "A5.T9.28.26",
        "footnotes": [],
        "references": []
    },
    "id_table_16": {
        "caption": "",
        "table": "A5.T10.28.26",
        "footnotes": [],
        "references": []
    }
}