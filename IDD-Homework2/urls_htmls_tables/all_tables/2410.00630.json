{
    "id_table_1": {
        "caption": "Table 1.  Quantitative evaluation on the Multiface dataset  (Wuu et al . ,  2022 ) .",
        "table": "S1.F2.4.4",
        "footnotes": [
            ""
        ],
        "references": [
            "Lifting very sparse 2D views onto a 3D reconstruction requires a strong face prior, crafted using diverse data captured across the human population. However, large real datasets are expensive and challenging to collect and typically suffer from low resolution, sampling limitations, and biases in their coverage of diversity and detail of facial geometry and appearance, under varied viewpoints and lighting. Here, our key insight is that such prior can be built from synthetic data alone and fine-tuned on a few real-world images to bridge the synthetic-real domain gap, generalizing robustly to challenging portraits captured in the wild, as shown in Figs.  1  and  9 .",
            "This section starts with background information (Sec.  3.1 ), details the training of synthetic prior (Sec.  3.2 ) and then finetuning from three input views (Sec.  3.3 ).",
            "We train a prior model to capture the distribution of human heads with arbitrary facial expressions. As Preface  (Buehler et al . ,  2023 ) , our prior model is implemented as a conditional Neural Radiance Field (NeRF)  (Mildenhall et al . ,  2020 ; Rebain et al . ,  2022 )  with a Mip-NeRF 360 backbone  (Barron et al . ,  2022 ) , Fig.  4 . Yet, we observe that the simple Preface auto-decoder  (Rebain et al . ,  2022 ; Bojanowski et al . ,  2018 )  cannot achieve high-quality fitting results on expressive faces (see the ablation in Sec.  4.3 ). We hypothesize that the distribution of expressive faces is much more difficult to model and disentangle than the distribution of neutral faces. To address this limitation, we decompose the latent space of our prior model into three latent spaces: a predefined identity space  B B \\mathcal{B} caligraphic_B , a predefined expression space    \\Psi roman_ , and a learned latent space  W W \\mathcal{W} caligraphic_W . The identity and expression spaces come from a linear 3D Morphable Model (3DMM) in the style of  (Wood et al . ,  2021 ) . The latent codes in these two spaces are known  a priori  and represent the face shape for the arbitrary identity and expression in each synthetic training image. These codes are also frozen and do not change during training. The latent space  W W \\mathcal{W} caligraphic_W  represents characteristics that are not modeled by the 3DMM like hair, beard, clothing, glasses, appearance, lighting, etc., and is learned while training the auto-decoder as in Preface  (Buehler et al . ,  2023 ) . Considering this model, we adapt Eq.  1  to obtain:",
            "We use our low-resolution synthetic prior model to enable high-resolution novel view synthesis of real expressive faces from few input images. We first describe how we obtain the conditional inputs and camera parameters in Sec.  3.3.1  and the subsequent fine-tuning of our model in Sec.  3.3.2 .",
            "Performance on casual in-the-wild captures is inherently difficult to evaluate quantitatively due to the lack of proper validation views. Often, the captured subject can hardly remain completely still. Therefore, we quantitatively evaluate on the Multiface  (Wuu et al . ,  2022 )  studio dataset on nine scenes by randomly selecting three subjects with three expressions per subject. For each test subject, we use one frontal and two side views as input for training, as shown in Fig.  7 . We remove the background by multiplying a foreground alpha matte estimated by  (Pandey et al . ,  2021 ) . We hold out from 26 to 29 validation images per subject, where the camera viewing direction is located in the frontal hemisphere (see details in the supplementary material). As evaluation metrics, we measure photo-metric distance and image similarity via PSNR, SSIM, and LPIPS  (Zhang et al . ,  2018 ) , as summarized in Tbl.  1 . Note that photometric reconstruction metrics like PSNR and SSIM and perceptual metrics like LPIPS are at odds with each other  (Blau and Michaeli,  2018 ) . We handle this tradeoff by optimizing perceptual quality in the warm-up (Eq.  6 ) and reconstruction quality in the fine-tuning (Eq.  7 ).",
            "As shown in Tbl.  1 , our new method provides better modeling fidelity across all three metrics over the set of validation views. In particular, compared to the state-of-the-art Preface  (Buehler et al . ,  2023 ) , we achieve the following improvement on the computed metrics: 6% PSNR, 2.4% SSIM, and 5% LPIPS. Visually, Fig.  7  shows that the novel views generated by our fine-tuned model more closely resemble the facial shape and appearance, including eye and mouth details, of the example (ground-truth) validation view.",
            "To demonstrate the robustness of our method in the wild, we captured subjects with a handheld DSLR camera (Canon EOS 4000D) and mobile phones (Pixel 7 and Pixel 8 Pro). We capture between one and three images per subject in various outdoor and indoor environments, including very challenging lighting conditions. These diverse in-the-wild results are shown in Fig.  1 , Fig.  8 , Fig.  9 , and in the supplementary video and HTML page. The inlays in Fig.  1  show the high level of modeled detail on the lips and on the individual hair strands and eyelashes. Fig.  8  compares with the best performing related work  (Buehler et al . ,  2023 ) , which struggles with the mouth region. Of particular note is also the tongue-out expression in the third row of Fig.  9 . Note that our synthetic training data contains tongues but the tongues never stick out of the mouth.",
            "We also consider the more challenging single input image scenario, see Fig.  10 . Our method achieves high-fidelity synthesis of the frontal face even for stylized and painted faces (right) but quickly degrades for side views. This behavior is expected, as our model is only trained on low-resolution synthetic images and has never seen high-resolution views from the side of a face. Please see the supp. HTML page for more examples.",
            "Additional qualitative comparisons to previous work are shown in Fig.  7 . Our method outperforms the other baseline methods in two main ways. First, our approach captures fine identity-specific details like teeth outlines (middle row) and stubble (bottom row), while previous methods degenerate into blur or noise. Second, the overall face shape is more accurate, as evidenced by the eyeball in the top row and the cheek silhouette in the middle row. Our synthetic face prior encourages the reconstruction to remain faithful to its learned understanding of faces,  e.g. , that corneas should bulge outwards, not be flat. These results and the metrics in Tbl.  1  demonstrate that our new model and fine-tuning method outperforms previous work both qualitatively and quantitatively.",
            "While our method can reconstruct 3D faces from even just one view, we notice that the quality degrades at side views in this extremely challenging case (see Fig.  10 ) due to the lack of observation. Accessories like glasses frames and earrings may not be reconstructed perfectly and for extreme expressions, the mouth interior might not be fully consistent across all viewpoints (see the supp. HTML page). Furthermore, our method assumes that faces are non-occluded in the input images. This can cause the camera estimation during the 3DMM fitting (Sec.  3.3.1 ) to fail. These limitations could be potentially addressed by leveraging large generative models to hallucinate unobserved regions, which is an interesting future direction to explore. Another direction of future work could explore more efficient backbones to increase pre-training and fine-tuning efficiency, and potentially enable facial animation.",
            "This supplementary document provides more details about the synthetic dataset (Sec.  A.1 ), the models and method (Sec.  A.2 ), experiments (Sec.  A.4 ), and supplementary results (Sec.  A.5 ).",
            "We quantitatively compare and ablate on a subset of the Multiface dataset  (Wuu et al . ,  2022 ) . Metrics are computed on three expressions from three identitiesnine scenes in total. For each scene, we select three views for training: One frontal and two side views. For evaluation, we compute metrics on all holdout views where the cameras are not located on the back of the head. Please see Fig.  11  for a visualization."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.  We ablate the performance for different variants of the prior model. We pre-train a) on a smaller training set with fewer subjects, b) for a shorter number of steps, c) with different configurations of our synthetic rendering pipeline, and d) include real data. The  full  prior model is trained on 1,500 subjects with 13 expressions each for 1 Mio. steps on synthetic renderings with all available accessories in a single environment map. Please see the supp. mat. for details of the rendering pipeline.",
        "table": "A1.EGx1",
        "footnotes": [],
        "references": [
            "This section starts with background information (Sec.  3.1 ), details the training of synthetic prior (Sec.  3.2 ) and then finetuning from three input views (Sec.  3.3 ).",
            "We train this prior on synthetic data alone  it never sees a real face. While it would be feasible to train a prior model on real data (see our ablation in Tbl.  2 ), we chose synthetic over real for multiple reasons. Real datasets exhibit limited diversity. Most face datasets feature monocular frontal views only, with few expressions other than smiles. Some multi-view, multi-expression datasets exist  (Wuu et al . ,  2022 ; Kirschstein et al . ,  2023 ; Zhu et al . ,  2023 ) , but consist of relatively few individuals due to the complexity and expense of running a capture studio. Further, subjects must adhere to wardrobe restrictions: glasses are forbidden and hair must be tucked away. A prior trained on such data will not generalize well to expressive faces captured in the wild. Besides, the logistics of capturing large-scale real data is extremely expensive, time and energy-consuming, and cumbersome. Instead, synthetics guarantee us a wide range of identity, expression, and appearance diversity, at orders of magnitude lower cost and effort. In addition, synthetics provide perfect ground truth annotations: each render is accompanied by 3DMM latent codes   ,    \\boldsymbol{\\beta},\\boldsymbol{\\psi} bold_italic_ , bold_italic_ .",
            "We use our low-resolution synthetic prior model to enable high-resolution novel view synthesis of real expressive faces from few input images. We first describe how we obtain the conditional inputs and camera parameters in Sec.  3.3.1  and the subsequent fine-tuning of our model in Sec.  3.3.2 .",
            "We conduct extensive ablation studies of the prior model. Table   2  lists metrics after fine-tuning to three inputs at 2K resolution. Please see the supp. PDF for more ablations and the HTML page for visuals.",
            "We compare prior models without any pre-training (a.i) with models pre-trained on a single subject (a.ii), on 15 subjects (a.iii), and on 1,500 subjects (a.iv). Note that each subject is rendered with 13 expressions and 30 views per expression (as described in Sec.  3.2 ). Hence, the model trained on 1 subject (a.ii) sees  1  13  30 = 390  1 13 30 390 1\\cdot 13\\cdot 30=390 1  13  30 = 390  images in total. Without pre-training (a.i), the reconstruction completely fails. Pre-training on a single subject (a.ii) leads to a noisy face geometry (see the surface normals in the supp. HTML page). The performance improves when more subjects are added (a.iii and a.iv). We conclude that pre-training is necessary and more data improves the reconstruction quality.",
            "This supplementary document provides more details about the synthetic dataset (Sec.  A.1 ), the models and method (Sec.  A.2 ), experiments (Sec.  A.4 ), and supplementary results (Sec.  A.5 )."
        ]
    },
    "id_table_3": {
        "caption": "Table 3.  Number of Input Views. Our method can produce good-quality frontal views from a single frontal image. However, the quality suffers from side views, where the input image doesnt provide any information. Please see the supplementary HTML page for visuals.",
        "table": "A1.EGx2",
        "footnotes": [],
        "references": [
            "To overcome reconstruction ambiguities, our method uses a pre-trained volumetric face model as prior, trained on a large dataset of synthetic faces with a variety of identities, expressions, and viewpoints rendered in a single environment. At inference time, our method first fits the coefficients of our prior model to a small set of real input images. It then further fine-tunes the model weights and effectively performs domain adaptation during the few-shot reconstruction process, see Fig.  3 . While the prior model is trained only once on a large collection of synthetic face images, the inference-time optimization is performed on a per-subject basis from as few as three ( e.g. , smartphone) images captured in-the-wild (see Fig.  9 ).",
            "This section starts with background information (Sec.  3.1 ), details the training of synthetic prior (Sec.  3.2 ) and then finetuning from three input views (Sec.  3.3 ).",
            "We train a prior model to capture the distribution of human heads with arbitrary facial expressions. As Preface  (Buehler et al . ,  2023 ) , our prior model is implemented as a conditional Neural Radiance Field (NeRF)  (Mildenhall et al . ,  2020 ; Rebain et al . ,  2022 )  with a Mip-NeRF 360 backbone  (Barron et al . ,  2022 ) , Fig.  4 . Yet, we observe that the simple Preface auto-decoder  (Rebain et al . ,  2022 ; Bojanowski et al . ,  2018 )  cannot achieve high-quality fitting results on expressive faces (see the ablation in Sec.  4.3 ). We hypothesize that the distribution of expressive faces is much more difficult to model and disentangle than the distribution of neutral faces. To address this limitation, we decompose the latent space of our prior model into three latent spaces: a predefined identity space  B B \\mathcal{B} caligraphic_B , a predefined expression space    \\Psi roman_ , and a learned latent space  W W \\mathcal{W} caligraphic_W . The identity and expression spaces come from a linear 3D Morphable Model (3DMM) in the style of  (Wood et al . ,  2021 ) . The latent codes in these two spaces are known  a priori  and represent the face shape for the arbitrary identity and expression in each synthetic training image. These codes are also frozen and do not change during training. The latent space  W W \\mathcal{W} caligraphic_W  represents characteristics that are not modeled by the 3DMM like hair, beard, clothing, glasses, appearance, lighting, etc., and is learned while training the auto-decoder as in Preface  (Buehler et al . ,  2023 ) . Considering this model, we adapt Eq.  1  to obtain:",
            "Each training iteration randomly samples rays  r r \\bf r bold_r  from a subset of all identities and expressions. A ray is rendered into a pixel color as given by Eq.  3 . We optimize the network parameters   p subscript  p \\bf\\theta_{\\text{p}} italic_ start_POSTSUBSCRIPT p end_POSTSUBSCRIPT  and  N  per-identity latent codes  w 1 . . N \\mathbf{w}_{1..\\text{N}} bold_w start_POSTSUBSCRIPT 1 . . N end_POSTSUBSCRIPT  while keeping the 3DMM expression and identity codes    \\boldsymbol{\\beta} bold_italic_  and    \\boldsymbol{\\psi} bold_italic_  frozen:",
            "We use our low-resolution synthetic prior model to enable high-resolution novel view synthesis of real expressive faces from few input images. We first describe how we obtain the conditional inputs and camera parameters in Sec.  3.3.1  and the subsequent fine-tuning of our model in Sec.  3.3.2 .",
            "Figure  3  gives an overview of the 3DMM fitting. During inference, the first step is to recover camera and 3DMM parameters from un-calibrated input images. We follow the approach of previous work   (Wood et al . ,  2022 )  and fit to dense 2D landmarks (see Fig.  6 ). We first predict 599 2D probabilistic landmarks. Each landmark corresponds to a vertex in our 3DMM and is predicted as a 2D isotropic Gaussian with expected 2D location    \\mu italic_  and scalar uncertainty    \\sigma italic_ . Next, we minimize an energy  E  (  ; L ) E  L E(\\boldsymbol{\\Phi};L) italic_E ( bold_ ; italic_L ) , where  L L L italic_L  denotes the landmarks and    \\boldsymbol{\\Phi} bold_  all the optimized 3DMM parameters including identity, expressions, joint rotations, and global translation, and intrinsic and extrinsic camera parameters, if unknown.",
            "We compare prior models without any pre-training (a.i) with models pre-trained on a single subject (a.ii), on 15 subjects (a.iii), and on 1,500 subjects (a.iv). Note that each subject is rendered with 13 expressions and 30 views per expression (as described in Sec.  3.2 ). Hence, the model trained on 1 subject (a.ii) sees  1  13  30 = 390  1 13 30 390 1\\cdot 13\\cdot 30=390 1  13  30 = 390  images in total. Without pre-training (a.i), the reconstruction completely fails. Pre-training on a single subject (a.ii) leads to a noisy face geometry (see the surface normals in the supp. HTML page). The performance improves when more subjects are added (a.iii and a.iv). We conclude that pre-training is necessary and more data improves the reconstruction quality.",
            "While our method can reconstruct 3D faces from even just one view, we notice that the quality degrades at side views in this extremely challenging case (see Fig.  10 ) due to the lack of observation. Accessories like glasses frames and earrings may not be reconstructed perfectly and for extreme expressions, the mouth interior might not be fully consistent across all viewpoints (see the supp. HTML page). Furthermore, our method assumes that faces are non-occluded in the input images. This can cause the camera estimation during the 3DMM fitting (Sec.  3.3.1 ) to fail. These limitations could be potentially addressed by leveraging large generative models to hallucinate unobserved regions, which is an interesting future direction to explore. Another direction of future work could explore more efficient backbones to increase pre-training and fine-tuning efficiency, and potentially enable facial animation.",
            "We train the prior model on images of 1,500 synthetic identities, rendering each with 13 expressions and 30 views. In total, we train the full prior model for 1 Mio. steps on 64 TPUs with a batch size of  131 , 072 131 072 131,072 131 , 072  rays per step ( 256 256 256 256  identities   \\times    8 8 8 8  view   \\times    64 64 64 64  pixels), which takes about 10 days. However, we observe that the model already reaches near convergence after 105,000 steps. In particular, fine-tuning that uses the model trained for 105,000 steps achieves very similar photo-metric quality as fine-tuning the model trained for 1 Mio. steps, please see our ablation study in Sec.  4.3  and the supp. HTML page for visuals.",
            "We extend our ablations from the main paper with metrics computed on different variants of our prior model in Tables  3  and  4 . The metrics are computed on the Multiface dataset  (Wuu et al . ,  2022 ) , as described in the main paper.",
            "We ablate fine-tuning results when a different number of views are available in Tbl.  3 . We find that our method produces pleasing front-view faces even for a single input view. However, the quality quickly degrades for side views, where the input views do not provide any signal. Table  4  ablates a prior model trained without background removal. Keeping the background (i) yields more floaters during model fitting."
        ]
    },
    "id_table_4": {
        "caption": "Table 4.   Additional ablations. Training a prior model with a background leads to more floating artifacts during fine-tuning.",
        "table": "A1.EGx3",
        "footnotes": [],
        "references": [
            "We train a prior model to capture the distribution of human heads with arbitrary facial expressions. As Preface  (Buehler et al . ,  2023 ) , our prior model is implemented as a conditional Neural Radiance Field (NeRF)  (Mildenhall et al . ,  2020 ; Rebain et al . ,  2022 )  with a Mip-NeRF 360 backbone  (Barron et al . ,  2022 ) , Fig.  4 . Yet, we observe that the simple Preface auto-decoder  (Rebain et al . ,  2022 ; Bojanowski et al . ,  2018 )  cannot achieve high-quality fitting results on expressive faces (see the ablation in Sec.  4.3 ). We hypothesize that the distribution of expressive faces is much more difficult to model and disentangle than the distribution of neutral faces. To address this limitation, we decompose the latent space of our prior model into three latent spaces: a predefined identity space  B B \\mathcal{B} caligraphic_B , a predefined expression space    \\Psi roman_ , and a learned latent space  W W \\mathcal{W} caligraphic_W . The identity and expression spaces come from a linear 3D Morphable Model (3DMM) in the style of  (Wood et al . ,  2021 ) . The latent codes in these two spaces are known  a priori  and represent the face shape for the arbitrary identity and expression in each synthetic training image. These codes are also frozen and do not change during training. The latent space  W W \\mathcal{W} caligraphic_W  represents characteristics that are not modeled by the 3DMM like hair, beard, clothing, glasses, appearance, lighting, etc., and is learned while training the auto-decoder as in Preface  (Buehler et al . ,  2023 ) . Considering this model, we adapt Eq.  1  to obtain:",
            "This supplementary document provides more details about the synthetic dataset (Sec.  A.1 ), the models and method (Sec.  A.2 ), experiments (Sec.  A.4 ), and supplementary results (Sec.  A.5 ).",
            "We train the prior model on images of 1,500 synthetic identities, rendering each with 13 expressions and 30 views. In total, we train the full prior model for 1 Mio. steps on 64 TPUs with a batch size of  131 , 072 131 072 131,072 131 , 072  rays per step ( 256 256 256 256  identities   \\times    8 8 8 8  view   \\times    64 64 64 64  pixels), which takes about 10 days. However, we observe that the model already reaches near convergence after 105,000 steps. In particular, fine-tuning that uses the model trained for 105,000 steps achieves very similar photo-metric quality as fine-tuning the model trained for 1 Mio. steps, please see our ablation study in Sec.  4.3  and the supp. HTML page for visuals.",
            "We extend our ablations from the main paper with metrics computed on different variants of our prior model in Tables  3  and  4 . The metrics are computed on the Multiface dataset  (Wuu et al . ,  2022 ) , as described in the main paper.",
            "We ablate fine-tuning results when a different number of views are available in Tbl.  3 . We find that our method produces pleasing front-view faces even for a single input view. However, the quality quickly degrades for side views, where the input views do not provide any signal. Table  4  ablates a prior model trained without background removal. Keeping the background (i) yields more floaters during model fitting."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "A1.EGx4",
        "footnotes": [],
        "references": [
            "We synthesize facial training data as in   Wood et al .  ( 2021 ) . We first generate the 3D face geometry by sampling the identity and expression spaces of the 3DMM. We then make these faces look realistic by applying physically based skin materials, attaching strand-based hairstyles, and dressing them up with clothes and glasses from our digital wardrobe. The scene is rendered with environment lighting using Cycles, a physically-based ray tracer ( www.cycles-renderer.org ). Examples are shown in Fig.  5  and on the supplementary HTML page. To help disentangle identity from expression, we sample 13 different random expressions for each random identity. Each expression is then rendered from 30 random viewpoints around the head. All faces are rendered under the same lighting condition, which was chosen to minimize shadows on the face.",
            "This supplementary document provides more details about the synthetic dataset (Sec.  A.1 ), the models and method (Sec.  A.2 ), experiments (Sec.  A.4 ), and supplementary results (Sec.  A.5 )."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "A1.EGx5",
        "footnotes": [],
        "references": [
            "Figure  3  gives an overview of the 3DMM fitting. During inference, the first step is to recover camera and 3DMM parameters from un-calibrated input images. We follow the approach of previous work   (Wood et al . ,  2022 )  and fit to dense 2D landmarks (see Fig.  6 ). We first predict 599 2D probabilistic landmarks. Each landmark corresponds to a vertex in our 3DMM and is predicted as a 2D isotropic Gaussian with expected 2D location    \\mu italic_  and scalar uncertainty    \\sigma italic_ . Next, we minimize an energy  E  (  ; L ) E  L E(\\boldsymbol{\\Phi};L) italic_E ( bold_ ; italic_L ) , where  L L L italic_L  denotes the landmarks and    \\boldsymbol{\\Phi} bold_  all the optimized 3DMM parameters including identity, expressions, joint rotations, and global translation, and intrinsic and extrinsic camera parameters, if unknown.",
            "Performance on casual in-the-wild captures is inherently difficult to evaluate quantitatively due to the lack of proper validation views. Often, the captured subject can hardly remain completely still. Therefore, we quantitatively evaluate on the Multiface  (Wuu et al . ,  2022 )  studio dataset on nine scenes by randomly selecting three subjects with three expressions per subject. For each test subject, we use one frontal and two side views as input for training, as shown in Fig.  7 . We remove the background by multiplying a foreground alpha matte estimated by  (Pandey et al . ,  2021 ) . We hold out from 26 to 29 validation images per subject, where the camera viewing direction is located in the frontal hemisphere (see details in the supplementary material). As evaluation metrics, we measure photo-metric distance and image similarity via PSNR, SSIM, and LPIPS  (Zhang et al . ,  2018 ) , as summarized in Tbl.  1 . Note that photometric reconstruction metrics like PSNR and SSIM and perceptual metrics like LPIPS are at odds with each other  (Blau and Michaeli,  2018 ) . We handle this tradeoff by optimizing perceptual quality in the warm-up (Eq.  6 ) and reconstruction quality in the fine-tuning (Eq.  7 )."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "S4.T1.3.3",
        "footnotes": [
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "RegNeRF  (Niemeyer et al . ,  2022 )  employs a smoothness regularization term on the expected depth derived from the predicted density, in conjunction with a patch-based regularizer on appearance which together bias the model towards a 3D consistent solution. FreeNeRF  (Yang et al . ,  2023 )  proposes a gradual, coarse-to-fine training scheme to prevent overfitting caused by high-frequency, position encoding components early in the fit. Despite their promising results on in-the-wild images, these methods cannot yet provide high-quality reconstructions from few-shot inputs, as we show in Fig.  7 .",
            "Our method also extends Preface  (Buehler et al . ,  2023 ) , a method for novel view synthesis of neutral faces given sparse inputs. Besides the position  x  and view direction  d , Preface also takes a learned latent code  w  as input. The latent code represents the identity and is optimized while training the model as an auto-decoder  (Bojanowski et al . ,  2018 ) . During inference, Preface first projects the sparse input images into its latent space, by optimizing one identity code  w . Then, it fine-tunes all model parameters under regularization constraints on the predicted normals and the view weights. While Preface excels at high-resolution novel view synthesis of neutral faces, it struggles in the presence of strong, idiosyncratic expressions (Fig.  7 ). In the following, we address this limitation while building an improved prior from synthetic images alone.",
            "Performance on casual in-the-wild captures is inherently difficult to evaluate quantitatively due to the lack of proper validation views. Often, the captured subject can hardly remain completely still. Therefore, we quantitatively evaluate on the Multiface  (Wuu et al . ,  2022 )  studio dataset on nine scenes by randomly selecting three subjects with three expressions per subject. For each test subject, we use one frontal and two side views as input for training, as shown in Fig.  7 . We remove the background by multiplying a foreground alpha matte estimated by  (Pandey et al . ,  2021 ) . We hold out from 26 to 29 validation images per subject, where the camera viewing direction is located in the frontal hemisphere (see details in the supplementary material). As evaluation metrics, we measure photo-metric distance and image similarity via PSNR, SSIM, and LPIPS  (Zhang et al . ,  2018 ) , as summarized in Tbl.  1 . Note that photometric reconstruction metrics like PSNR and SSIM and perceptual metrics like LPIPS are at odds with each other  (Blau and Michaeli,  2018 ) . We handle this tradeoff by optimizing perceptual quality in the warm-up (Eq.  6 ) and reconstruction quality in the fine-tuning (Eq.  7 ).",
            "As shown in Tbl.  1 , our new method provides better modeling fidelity across all three metrics over the set of validation views. In particular, compared to the state-of-the-art Preface  (Buehler et al . ,  2023 ) , we achieve the following improvement on the computed metrics: 6% PSNR, 2.4% SSIM, and 5% LPIPS. Visually, Fig.  7  shows that the novel views generated by our fine-tuned model more closely resemble the facial shape and appearance, including eye and mouth details, of the example (ground-truth) validation view.",
            "Additional qualitative comparisons to previous work are shown in Fig.  7 . Our method outperforms the other baseline methods in two main ways. First, our approach captures fine identity-specific details like teeth outlines (middle row) and stubble (bottom row), while previous methods degenerate into blur or noise. Second, the overall face shape is more accurate, as evidenced by the eyeball in the top row and the cheek silhouette in the middle row. Our synthetic face prior encourages the reconstruction to remain faithful to its learned understanding of faces,  e.g. , that corneas should bulge outwards, not be flat. These results and the metrics in Tbl.  1  demonstrate that our new model and fine-tuning method outperforms previous work both qualitatively and quantitatively."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "S4.T2.3.3",
        "footnotes": [],
        "references": [
            "To demonstrate the robustness of our method in the wild, we captured subjects with a handheld DSLR camera (Canon EOS 4000D) and mobile phones (Pixel 7 and Pixel 8 Pro). We capture between one and three images per subject in various outdoor and indoor environments, including very challenging lighting conditions. These diverse in-the-wild results are shown in Fig.  1 , Fig.  8 , Fig.  9 , and in the supplementary video and HTML page. The inlays in Fig.  1  show the high level of modeled detail on the lips and on the individual hair strands and eyelashes. Fig.  8  compares with the best performing related work  (Buehler et al . ,  2023 ) , which struggles with the mouth region. Of particular note is also the tongue-out expression in the third row of Fig.  9 . Note that our synthetic training data contains tongues but the tongues never stick out of the mouth."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "S5.F7.25.25",
        "footnotes": [],
        "references": [
            "Lifting very sparse 2D views onto a 3D reconstruction requires a strong face prior, crafted using diverse data captured across the human population. However, large real datasets are expensive and challenging to collect and typically suffer from low resolution, sampling limitations, and biases in their coverage of diversity and detail of facial geometry and appearance, under varied viewpoints and lighting. Here, our key insight is that such prior can be built from synthetic data alone and fine-tuned on a few real-world images to bridge the synthetic-real domain gap, generalizing robustly to challenging portraits captured in the wild, as shown in Figs.  1  and  9 .",
            "To overcome reconstruction ambiguities, our method uses a pre-trained volumetric face model as prior, trained on a large dataset of synthetic faces with a variety of identities, expressions, and viewpoints rendered in a single environment. At inference time, our method first fits the coefficients of our prior model to a small set of real input images. It then further fine-tunes the model weights and effectively performs domain adaptation during the few-shot reconstruction process, see Fig.  3 . While the prior model is trained only once on a large collection of synthetic face images, the inference-time optimization is performed on a per-subject basis from as few as three ( e.g. , smartphone) images captured in-the-wild (see Fig.  9 ).",
            "To demonstrate the robustness of our method in the wild, we captured subjects with a handheld DSLR camera (Canon EOS 4000D) and mobile phones (Pixel 7 and Pixel 8 Pro). We capture between one and three images per subject in various outdoor and indoor environments, including very challenging lighting conditions. These diverse in-the-wild results are shown in Fig.  1 , Fig.  8 , Fig.  9 , and in the supplementary video and HTML page. The inlays in Fig.  1  show the high level of modeled detail on the lips and on the individual hair strands and eyelashes. Fig.  8  compares with the best performing related work  (Buehler et al . ,  2023 ) , which struggles with the mouth region. Of particular note is also the tongue-out expression in the third row of Fig.  9 . Note that our synthetic training data contains tongues but the tongues never stick out of the mouth."
        ]
    },
    "id_table_10": {
        "caption": "",
        "table": "S5.F8.10.10",
        "footnotes": [],
        "references": [
            "We also consider the more challenging single input image scenario, see Fig.  10 . Our method achieves high-fidelity synthesis of the frontal face even for stylized and painted faces (right) but quickly degrades for side views. This behavior is expected, as our model is only trained on low-resolution synthetic images and has never seen high-resolution views from the side of a face. Please see the supp. HTML page for more examples.",
            "While our method can reconstruct 3D faces from even just one view, we notice that the quality degrades at side views in this extremely challenging case (see Fig.  10 ) due to the lack of observation. Accessories like glasses frames and earrings may not be reconstructed perfectly and for extreme expressions, the mouth interior might not be fully consistent across all viewpoints (see the supp. HTML page). Furthermore, our method assumes that faces are non-occluded in the input images. This can cause the camera estimation during the 3DMM fitting (Sec.  3.3.1 ) to fail. These limitations could be potentially addressed by leveraging large generative models to hallucinate unobserved regions, which is an interesting future direction to explore. Another direction of future work could explore more efficient backbones to increase pre-training and fine-tuning efficiency, and potentially enable facial animation."
        ]
    },
    "id_table_11": {
        "caption": "",
        "table": "S5.F9.40.40",
        "footnotes": [],
        "references": [
            "We quantitatively compare and ablate on a subset of the Multiface dataset  (Wuu et al . ,  2022 ) . Metrics are computed on three expressions from three identitiesnine scenes in total. For each scene, we select three views for training: One frontal and two side views. For evaluation, we compute metrics on all holdout views where the cameras are not located on the back of the head. Please see Fig.  11  for a visualization."
        ]
    },
    "id_table_12": {
        "caption": "",
        "table": "S5.F10.16.16",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "",
        "table": "A1.T3.3.3",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "A1.T4.3.3",
        "footnotes": [],
        "references": []
    }
}