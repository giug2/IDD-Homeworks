{
    "S1.T1": {
        "caption": "Table 1: Comparison between Meta-Album and other large-scale or (meta-) datasets",
        "table": null,
        "footnotes": [],
        "references": [
            "We compare Meta-Album with previous benchmarks/datasets in Table 1, and provide further details in Appendix G.\nMeta-Album covers a variety of domains, including ecology, manufacturing, textures, object classification, and character recognition, as well as a variety of scales: microscopic, macroscopic (human scale), or distant (remote sensing). While mostly re-purposing public datasets from heterogeneous sources to maximally vary recording conditions, we also introduce a few fresh datasets in OCR and ecology domains. Meta-Album comprises 3 different versions, Micro ⊂\\subset Mini ⊂\\subset Extended: Micro includes 20 classes and 40 images per class for ease of running sample code, Mini retains all original classes but also includes only 40 examples per class, while Extended includes all classes and examples."
        ]
    },
    "S2.T2": {
        "caption": "Table 2: Meta-Album: Datasets summary (Mini versions)",
        "table": null,
        "footnotes": [],
        "references": [
            "The last criterion was needed to ensure the success of our challenges, since tasks that are too easy or too hard do not allow us to separate challenge participants.\nFor the purpose of designing Meta-Album, we defined a “domain” according to four characteristics: (1) application domain; (2) pattern recognition problem (texture or object classification); (3) scale: micro, human scale, or distant; (4) input channels.\nWe ended up with 10 domains (see Table 2): Large animals, small animals, plants, plant diseases, microscopy, remote sensing, vehicles, manufacturing, human actions, and optical character recognition (OCR). Data sources were very varied, and mostly came from internet searches, but we also produced our own optical character recognition datasets and obtained novel donated data.",
            "The initial release of Meta-Album consists of 3 datasets for each of the 10 domains. Each dataset has 3 versions controlling the size, as explained in Section 1.2.\nAll datasets are annotated with class labels and other meta-data.\nAll 30 datasets were chosen after careful and critical analysis during the data preparation and quality control steps as described in Appendix C.\nTable 2 provides statistics on the various versions; Figure 1\nshows sample images from each dataset. More details about datasets and their meta-data are listed in Appendix A. License information for all datasets can be found in Appendix B. Meta-Album datasets are being used in the NeurIPS Cross-domain meta-learning Challenge 2022.\nThe first 30 datasets are available on OpenML [71], and later in spring 2023, 10 more datasets will be released, followed by other releases as our challenge program unfolds. Details about how to access Meta-Album datasets, contribute to the open meta-dataset, prepare new datasets with quality control, and submit these datasets for inclusion in Meta-Album can be found on the Meta-Album Website.\nThis web page will also inform on software updates and revisions or new releases of our meta-dataset."
        ]
    },
    "S3.T3": {
        "caption": "Table 3: Datasets used in the NeurIPS 2021 MetaDL challenge [13].",
        "table": "<table id=\"S3.T3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S3.T3.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T3.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\">Phase</td>\n<td id=\"S3.T3.1.1.2\" class=\"ltx_td ltx_align_justify ltx_border_tt\">\n<span id=\"S3.T3.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.1.2.1.1\" class=\"ltx_p\">Datasets according to Table <a href=\"#S2.T2\" title=\"Table 2 ‣ 2.3 Data preparation ‣ 2 Meta-Album design and initial release ‣ Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T3.1.2\" class=\"ltx_tr\">\n<td id=\"S3.T3.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Feedback Phase</td>\n<td id=\"S3.T3.1.2.2\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T3.1.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.2.2.1.1\" class=\"ltx_p\">SM_AM.PLK,\nMDN.MLD,\nMNF.TEX_DTD,\nREM_SEN.RSICB,\nOCR.MD_MIX</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T3.1.3\" class=\"ltx_tr\">\n<td id=\"S3.T3.1.3.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">Final test phase</td>\n<td id=\"S3.T3.1.3.2\" class=\"ltx_td ltx_align_justify ltx_border_bb\">\n<span id=\"S3.T3.1.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.3.2.1.1\" class=\"ltx_p\">SM_AM.INS,\nPLT_DIS.PLT_VIL,\nMNF.TEX,\nREM_SEN.RESISC,\nOCR.MD_5_BIS</span>\n</span>\n</td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "The first motivational use of Meta-Album has been the NeurIPS 2021 MetaDL challenge [13]. This was a meta-learning challenge with code submission, aiming at evaluating few-shot learning methods in the within domain setting, as described in Section 3.1. The evaluation was carried out with 600 tasks in the 5-way 5-shot setting, using a subset of Meta-Album (see Table 3)."
        ]
    },
    "A2.T4": {
        "caption": "Table 4: Meta-Album: Datasets license information",
        "table": null,
        "footnotes": [],
        "references": []
    },
    "A5.T5": {
        "caption": "Table 5: Full performance results for 5-way 1-shot image classification on all datasets. The 95% confidence intervals are computed at per-task level over 3 runs per dataset with 600 tasks per run (total tasks: 3×600=1 800360018003\\times 600=1\\,800).",
        "table": null,
        "footnotes": [],
        "references": [
            "In this section, we show additional results for the within domain few-shot learning experiments that were presented in Section 3.2.\nTable 5, Table 6, Table 7, and Table 8 display the average accuracy per technique and dataset in the [1, 5, 10, 20]-shot settings, respectively.\nNote that the 1-shot performance of matching networks and prototypical networks is not the same as they use different distance measures. More specifically, matching networks use cosine similarity while prototypical networks use squared euclidean distance."
        ]
    },
    "A5.T6": {
        "caption": "Table 6: Full performance results for 5-way 5-shot image classification on all datasets. The 95% confidence intervals are computed at per-task level over 3 runs per dataset with 600 tasks per run (total tasks: 3×600=1 800360018003\\times 600=1\\,800).",
        "table": null,
        "footnotes": [],
        "references": [
            "In this section, we show additional results for the within domain few-shot learning experiments that were presented in Section 3.2.\nTable 5, Table 6, Table 7, and Table 8 display the average accuracy per technique and dataset in the [1, 5, 10, 20]-shot settings, respectively.\nNote that the 1-shot performance of matching networks and prototypical networks is not the same as they use different distance measures. More specifically, matching networks use cosine similarity while prototypical networks use squared euclidean distance."
        ]
    },
    "A5.T7": {
        "caption": "Table 7: Full performance results for 5-way 10-shot image classification on all datasets. The 95% confidence intervals are computed at per-task level over 3 runs per dataset with 600 tasks per run (total tasks: 3×600=1 800360018003\\times 600=1\\,800).",
        "table": null,
        "footnotes": [],
        "references": [
            "In this section, we show additional results for the within domain few-shot learning experiments that were presented in Section 3.2.\nTable 5, Table 6, Table 7, and Table 8 display the average accuracy per technique and dataset in the [1, 5, 10, 20]-shot settings, respectively.\nNote that the 1-shot performance of matching networks and prototypical networks is not the same as they use different distance measures. More specifically, matching networks use cosine similarity while prototypical networks use squared euclidean distance."
        ]
    },
    "A5.T8": {
        "caption": "Table 8: Full performance results for 5-way 20-shot image classification on all data sets. The 95% confidence intervals are computed at per-task level over 3 runs per dataset with 600 tasks per run (total tasks: 3×600=1 800360018003\\times 600=1\\,800).",
        "table": null,
        "footnotes": [],
        "references": [
            "In this section, we show additional results for the within domain few-shot learning experiments that were presented in Section 3.2.\nTable 5, Table 6, Table 7, and Table 8 display the average accuracy per technique and dataset in the [1, 5, 10, 20]-shot settings, respectively.\nNote that the 1-shot performance of matching networks and prototypical networks is not the same as they use different distance measures. More specifically, matching networks use cosine similarity while prototypical networks use squared euclidean distance."
        ]
    },
    "A5.T9": {
        "caption": "Table 9: The average rankings over all data sets and running times for all techniques on 5-way [1, 5, 10, 20]-shot image classification. The 95% confidence intervals are computed at dataset level over 30 datasets.",
        "table": null,
        "footnotes": [],
        "references": [
            "Table 9 displays the average ranks and running times per technique in different within domain settings.\nThe rankings follow roughly the same pattern as in the right subplot of Figure 11.\nNote that the confidence intervals for the average ranks are larger than for the average accuracy as the former is computed over the 30 datasets, while the latter is computed at per-task level over all datasets.\nFigure 11 displays the average rank per method and average accuracy for within domain 5-way [1, 5, 10, 20]-shot classification."
        ]
    },
    "A6.T10": {
        "caption": "Table 10: Full performance results for cross-domain 5-way 1-shot image classification on all datasets. The 95% confidence intervals are computed at per-task level over 3 runs per dataset with 600 tasks per run (total tasks: 3×600=1 800360018003\\times 600=1\\,800).",
        "table": null,
        "footnotes": [],
        "references": [
            "Here, we show additional results for the cross-domain few-shot learning experiments from Section 3.2. Table 10, Table 11, Table 12, and Table 13 display the average accuracy per technique and dataset in the 5-way [1, 5, 10, 20]-shot settings. Moreover, Figure 12 displays the average rank per method and average accuracy for the same settings."
        ]
    },
    "A6.T11": {
        "caption": "Table 11: Full performance results for cross-domain 5-way 5-shot image classification on all datasets. The 95% confidence intervals are computed at per-task level over 3 runs per dataset with 600 tasks per run (total tasks: 3×600=1 800360018003\\times 600=1\\,800).",
        "table": null,
        "footnotes": [],
        "references": [
            "Here, we show additional results for the cross-domain few-shot learning experiments from Section 3.2. Table 10, Table 11, Table 12, and Table 13 display the average accuracy per technique and dataset in the 5-way [1, 5, 10, 20]-shot settings. Moreover, Figure 12 displays the average rank per method and average accuracy for the same settings."
        ]
    },
    "A6.T12": {
        "caption": "Table 12: Full performance results for cross-domain 5-way 10-shot image classification on all datasets. The 95% confidence intervals are computed at per-task level over 3 runs per dataset with 600 tasks per run (total tasks: 3×600=1 800360018003\\times 600=1\\,800).",
        "table": null,
        "footnotes": [],
        "references": [
            "Here, we show additional results for the cross-domain few-shot learning experiments from Section 3.2. Table 10, Table 11, Table 12, and Table 13 display the average accuracy per technique and dataset in the 5-way [1, 5, 10, 20]-shot settings. Moreover, Figure 12 displays the average rank per method and average accuracy for the same settings."
        ]
    },
    "A6.T13": {
        "caption": "Table 13: Full performance results for cross-domain 5-way 20-shot image classification on all datasets. The 95% confidence intervals are computed at per-task level over 3 runs per dataset with 600 tasks per run (total tasks: 3×600=1 800360018003\\times 600=1\\,800).",
        "table": null,
        "footnotes": [],
        "references": [
            "Here, we show additional results for the cross-domain few-shot learning experiments from Section 3.2. Table 10, Table 11, Table 12, and Table 13 display the average accuracy per technique and dataset in the 5-way [1, 5, 10, 20]-shot settings. Moreover, Figure 12 displays the average rank per method and average accuracy for the same settings."
        ]
    },
    "A6.T14": {
        "caption": "Table 14: Full performance results for cross-domain any-way any-shot image classification on all datasets. The 95% confidence intervals are computed at per-task level over 3 runs per dataset with 600 tasks per run (total tasks: 3×600=1 800360018003\\times 600=1\\,800).",
        "table": null,
        "footnotes": [],
        "references": [
            "Table 14 displays the average accuracy per technique and dataset in the any-way any-shot setting. Table 15 and\nTable 16 display the results of the any-way any-shot setting but grouped by N𝑁N-way and k𝑘k-shot, respectively. Figure 13 shows a performance comparison of the different techniques when trained on each of the evaluated settings and tested on the novel any-way any-shot setting. From these results, we can observe that most of the techniques are not benefited from training with a variable number of shots; only MAML shows a performance improvement by this approach."
        ]
    },
    "A6.T15": {
        "caption": "Table 15: Full performance results for cross-domain any-way any-shot image classification per number of ways. The 95% confidence intervals are computed at per-task level using the number of task shown in the table.",
        "table": null,
        "footnotes": [],
        "references": [
            "Table 14 displays the average accuracy per technique and dataset in the any-way any-shot setting. Table 15 and\nTable 16 display the results of the any-way any-shot setting but grouped by N𝑁N-way and k𝑘k-shot, respectively. Figure 13 shows a performance comparison of the different techniques when trained on each of the evaluated settings and tested on the novel any-way any-shot setting. From these results, we can observe that most of the techniques are not benefited from training with a variable number of shots; only MAML shows a performance improvement by this approach."
        ]
    },
    "A6.T16": {
        "caption": "Table 16: Full performance results for cross-domain any-way any-shot image classification per number of shots. The 95% confidence intervals are computed at per-task level using the number of task shown in the table.",
        "table": null,
        "footnotes": [],
        "references": [
            "Table 14 displays the average accuracy per technique and dataset in the any-way any-shot setting. Table 15 and\nTable 16 display the results of the any-way any-shot setting but grouped by N𝑁N-way and k𝑘k-shot, respectively. Figure 13 shows a performance comparison of the different techniques when trained on each of the evaluated settings and tested on the novel any-way any-shot setting. From these results, we can observe that most of the techniques are not benefited from training with a variable number of shots; only MAML shows a performance improvement by this approach."
        ]
    },
    "A6.T17": {
        "caption": "Table 17: The running times for all techniques on all cross-domain settings over all datasets. The number of tasks in each phase is: 540 000 during meta-training, 129 600 during meta-validation, and 54 000 during meta-testing.",
        "table": null,
        "footnotes": [],
        "references": [
            "Table 17 displays the running times per technique in the different cross-domain settings. This time is separated in meta-training, meta-validation and meta-testing. Note that the meta-training and meta-validation time of TrainFromScratch is 0 because this baseline learns every task starting from a random initialization at meta-test time."
        ]
    },
    "A7.T18": {
        "caption": "Table 18: Feature comparison between Meta-Album and other large-scale or (meta-)datasets",
        "table": null,
        "footnotes": [],
        "references": [
            "We compare the Meta-Album meta-dataset with other (meta-)datasets in\nTable 18. Each row shows a (meta-)dataset while each column shows features. The last three rows show versions of the newly proposed meta-dataset, i.e., Meta-Album.\nThe table describes the following features, divided in quantitative and qualitative features:"
        ]
    }
}