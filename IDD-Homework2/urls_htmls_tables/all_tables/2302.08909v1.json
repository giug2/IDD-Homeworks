{
    "S1.T1": {
        "caption": "Table 1: Comparison between Meta-Album and other large-scale or (meta-) datasets",
        "table": null,
        "footnotes": [],
        "references": [
            "We compare Meta-Album with previous benchmarks/datasets in TableÂ 1, and provide further details in AppendixÂ G.\nMeta-Album covers a variety of domains, including ecology, manufacturing, textures, object classification, and character recognition, as well as a variety of scales: microscopic, macroscopic (human scale), or distant (remote sensing). While mostly re-purposing public datasets from heterogeneous sources to maximally vary recording conditions, we also introduce a few fresh datasets in OCR and ecology domains. Meta-Album comprises 3 different versions, Micro âŠ‚\\subset Mini âŠ‚\\subset Extended: Micro includes 20 classes and 40 images per class for ease of running sample code, Mini retains all original classes but also includes only 40 examples per class, while Extended includes all classes and examples."
        ]
    },
    "S2.T2": {
        "caption": "Table 2: Meta-Album: Datasets summary (Mini versions)",
        "table": null,
        "footnotes": [],
        "references": [
            "The last criterion was needed to ensure the success of our challenges, since tasks that are too easy or too hard do not allow us to separate challenge participants.\nFor the purpose of designing Meta-Album, we defined a â€œdomainâ€ according to four characteristics: (1)Â application domain; (2)Â pattern recognition problem (texture or object classification); (3)Â scale: micro, human scale, or distant; (4)Â input channels.\nWe ended up with 10 domains (see TableÂ 2): Large animals, small animals, plants, plant diseases, microscopy, remote sensing, vehicles, manufacturing, human actions, and optical character recognition (OCR). Data sources were very varied, and mostly came from internet searches, but we also produced our own optical character recognition datasets and obtained novel donated data.",
            "The initial release of Meta-Album consists of 3 datasets for each of the 10 domains. Each dataset has 3 versions controlling the size, as explained in SectionÂ 1.2.\nAll datasets are annotated with class labels and other meta-data.\nAll 30 datasets were chosen after careful and critical analysis during the data preparation and quality control steps as described in AppendixÂ C.\nTableÂ 2 provides statistics on the various versions; FigureÂ 1\nshows sample images from each dataset. More details about datasets and their meta-data are listed in AppendixÂ A. License information for all datasets can be found in AppendixÂ B. Meta-Album datasets are being used in the NeurIPS Cross-domain meta-learning Challenge 2022.\nThe first 30 datasets are available on OpenMLÂ [71], and later in spring 2023, 10 more datasets will be released, followed by other releases as our challenge program unfolds. Details about how to access Meta-Album datasets, contribute to the open meta-dataset, prepare new datasets with quality control, and submit these datasets for inclusion in Meta-Album can be found on the Meta-Album Website.\nThis web page will also inform on software updates and revisions or new releases of our meta-dataset."
        ]
    },
    "S3.T3": {
        "caption": "Table 3: Datasets used in the NeurIPS 2021 MetaDL challengeÂ [13].",
        "table": "<table id=\"S3.T3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S3.T3.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T3.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\">Phase</td>\n<td id=\"S3.T3.1.1.2\" class=\"ltx_td ltx_align_justify ltx_border_tt\">\n<span id=\"S3.T3.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.1.2.1.1\" class=\"ltx_p\">Datasets according to TableÂ <a href=\"#S2.T2\" title=\"Table 2 â€£ 2.3 Data preparation â€£ 2 Meta-Album design and initial release â€£ Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">2</span></a></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T3.1.2\" class=\"ltx_tr\">\n<td id=\"S3.T3.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Feedback Phase</td>\n<td id=\"S3.T3.1.2.2\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T3.1.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.2.2.1.1\" class=\"ltx_p\">SM_AM.PLK,\nMDN.MLD,\nMNF.TEX_DTD,\nREM_SEN.RSICB,\nOCR.MD_MIX</span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T3.1.3\" class=\"ltx_tr\">\n<td id=\"S3.T3.1.3.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">Final test phase</td>\n<td id=\"S3.T3.1.3.2\" class=\"ltx_td ltx_align_justify ltx_border_bb\">\n<span id=\"S3.T3.1.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.3.2.1.1\" class=\"ltx_p\">SM_AM.INS,\nPLT_DIS.PLT_VIL,\nMNF.TEX,\nREM_SEN.RESISC,\nOCR.MD_5_BIS</span>\n</span>\n</td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "The first motivational use of Meta-Album has been the NeurIPS 2021 MetaDL challengeÂ [13]. This was a meta-learning challenge with code submission, aiming at evaluating few-shot learning methods in the within domain setting, as described in SectionÂ 3.1. The evaluation was carried out with 600 tasks in the 5-way 5-shot setting, using a subset of Meta-Album (see TableÂ 3)."
        ]
    },
    "A2.T4": {
        "caption": "Table 4: Meta-Album: Datasets license information",
        "table": null,
        "footnotes": [],
        "references": []
    },
    "A5.T5": {
        "caption": "Table 5: Full performance results for 5-way 1-shot image classification on all datasets. The 95% confidence intervals are computed at per-task level over 3 runs per dataset with 600 tasks per run (total tasks: 3Ã—600=1â€‰800360018003\\times 600=1\\,800).",
        "table": null,
        "footnotes": [],
        "references": [
            "In this section, we show additional results for the within domain few-shot learning experiments that were presented in Section 3.2.\nTableÂ 5, TableÂ 6, TableÂ 7, and TableÂ 8 display the average accuracy per technique and dataset in the [1, 5, 10, 20]-shot settings, respectively.\nNote that the 1-shot performance of matching networks and prototypical networks is not the same as they use different distance measures. More specifically, matching networks use cosine similarity while prototypical networks use squared euclidean distance."
        ]
    },
    "A5.T6": {
        "caption": "Table 6: Full performance results for 5-way 5-shot image classification on all datasets. The 95% confidence intervals are computed at per-task level over 3 runs per dataset with 600 tasks per run (total tasks: 3Ã—600=1â€‰800360018003\\times 600=1\\,800).",
        "table": null,
        "footnotes": [],
        "references": [
            "In this section, we show additional results for the within domain few-shot learning experiments that were presented in Section 3.2.\nTableÂ 5, TableÂ 6, TableÂ 7, and TableÂ 8 display the average accuracy per technique and dataset in the [1, 5, 10, 20]-shot settings, respectively.\nNote that the 1-shot performance of matching networks and prototypical networks is not the same as they use different distance measures. More specifically, matching networks use cosine similarity while prototypical networks use squared euclidean distance."
        ]
    },
    "A5.T7": {
        "caption": "Table 7: Full performance results for 5-way 10-shot image classification on all datasets. The 95% confidence intervals are computed at per-task level over 3 runs per dataset with 600 tasks per run (total tasks: 3Ã—600=1â€‰800360018003\\times 600=1\\,800).",
        "table": null,
        "footnotes": [],
        "references": [
            "In this section, we show additional results for the within domain few-shot learning experiments that were presented in Section 3.2.\nTableÂ 5, TableÂ 6, TableÂ 7, and TableÂ 8 display the average accuracy per technique and dataset in the [1, 5, 10, 20]-shot settings, respectively.\nNote that the 1-shot performance of matching networks and prototypical networks is not the same as they use different distance measures. More specifically, matching networks use cosine similarity while prototypical networks use squared euclidean distance."
        ]
    },
    "A5.T8": {
        "caption": "Table 8: Full performance results for 5-way 20-shot image classification on all data sets. The 95% confidence intervals are computed at per-task level over 3 runs per dataset with 600 tasks per run (total tasks: 3Ã—600=1â€‰800360018003\\times 600=1\\,800).",
        "table": null,
        "footnotes": [],
        "references": [
            "In this section, we show additional results for the within domain few-shot learning experiments that were presented in Section 3.2.\nTableÂ 5, TableÂ 6, TableÂ 7, and TableÂ 8 display the average accuracy per technique and dataset in the [1, 5, 10, 20]-shot settings, respectively.\nNote that the 1-shot performance of matching networks and prototypical networks is not the same as they use different distance measures. More specifically, matching networks use cosine similarity while prototypical networks use squared euclidean distance."
        ]
    },
    "A5.T9": {
        "caption": "Table 9: The average rankings over all data sets and running times for all techniques on 5-way [1, 5, 10, 20]-shot image classification. The 95% confidence intervals are computed at dataset level over 30 datasets.",
        "table": null,
        "footnotes": [],
        "references": [
            "TableÂ 9 displays the average ranks and running times per technique in different within domain settings.\nThe rankings follow roughly the same pattern as in the right subplot of FigureÂ 11.\nNote that the confidence intervals for the average ranks are larger than for the average accuracy as the former is computed over the 30 datasets, while the latter is computed at per-task level over all datasets.\nFigureÂ 11 displays the average rank per method and average accuracy for within domain 5-way [1, 5, 10, 20]-shot classification."
        ]
    },
    "A6.T10": {
        "caption": "Table 10: Full performance results for cross-domain 5-way 1-shot image classification on all datasets. The 95% confidence intervals are computed at per-task level over 3 runs per dataset with 600 tasks per run (total tasks: 3Ã—600=1â€‰800360018003\\times 600=1\\,800).",
        "table": null,
        "footnotes": [],
        "references": [
            "Here, we show additional results for the cross-domain few-shot learning experiments from Section 3.2. TableÂ 10, TableÂ 11, TableÂ 12, and TableÂ 13 display the average accuracy per technique and dataset in the 5-way [1, 5, 10, 20]-shot settings. Moreover, FigureÂ 12 displays the average rank per method and average accuracy for the same settings."
        ]
    },
    "A6.T11": {
        "caption": "Table 11: Full performance results for cross-domain 5-way 5-shot image classification on all datasets. The 95% confidence intervals are computed at per-task level over 3 runs per dataset with 600 tasks per run (total tasks: 3Ã—600=1â€‰800360018003\\times 600=1\\,800).",
        "table": null,
        "footnotes": [],
        "references": [
            "Here, we show additional results for the cross-domain few-shot learning experiments from Section 3.2. TableÂ 10, TableÂ 11, TableÂ 12, and TableÂ 13 display the average accuracy per technique and dataset in the 5-way [1, 5, 10, 20]-shot settings. Moreover, FigureÂ 12 displays the average rank per method and average accuracy for the same settings."
        ]
    },
    "A6.T12": {
        "caption": "Table 12: Full performance results for cross-domain 5-way 10-shot image classification on all datasets. The 95% confidence intervals are computed at per-task level over 3 runs per dataset with 600 tasks per run (total tasks: 3Ã—600=1â€‰800360018003\\times 600=1\\,800).",
        "table": null,
        "footnotes": [],
        "references": [
            "Here, we show additional results for the cross-domain few-shot learning experiments from Section 3.2. TableÂ 10, TableÂ 11, TableÂ 12, and TableÂ 13 display the average accuracy per technique and dataset in the 5-way [1, 5, 10, 20]-shot settings. Moreover, FigureÂ 12 displays the average rank per method and average accuracy for the same settings."
        ]
    },
    "A6.T13": {
        "caption": "Table 13: Full performance results for cross-domain 5-way 20-shot image classification on all datasets. The 95% confidence intervals are computed at per-task level over 3 runs per dataset with 600 tasks per run (total tasks: 3Ã—600=1â€‰800360018003\\times 600=1\\,800).",
        "table": null,
        "footnotes": [],
        "references": [
            "Here, we show additional results for the cross-domain few-shot learning experiments from Section 3.2. TableÂ 10, TableÂ 11, TableÂ 12, and TableÂ 13 display the average accuracy per technique and dataset in the 5-way [1, 5, 10, 20]-shot settings. Moreover, FigureÂ 12 displays the average rank per method and average accuracy for the same settings."
        ]
    },
    "A6.T14": {
        "caption": "Table 14: Full performance results for cross-domain any-way any-shot image classification on all datasets. The 95% confidence intervals are computed at per-task level over 3 runs per dataset with 600 tasks per run (total tasks: 3Ã—600=1â€‰800360018003\\times 600=1\\,800).",
        "table": null,
        "footnotes": [],
        "references": [
            "TableÂ 14 displays the average accuracy per technique and dataset in the any-way any-shot setting. TableÂ 15 and\nTableÂ 16 display the results of the any-way any-shot setting but grouped by Nğ‘N-way and kğ‘˜k-shot, respectively. FigureÂ 13 shows a performance comparison of the different techniques when trained on each of the evaluated settings and tested on the novel any-way any-shot setting. From these results, we can observe that most of the techniques are not benefited from training with a variable number of shots; only MAML shows a performance improvement by this approach."
        ]
    },
    "A6.T15": {
        "caption": "Table 15: Full performance results for cross-domain any-way any-shot image classification per number of ways. The 95% confidence intervals are computed at per-task level using the number of task shown in the table.",
        "table": null,
        "footnotes": [],
        "references": [
            "TableÂ 14 displays the average accuracy per technique and dataset in the any-way any-shot setting. TableÂ 15 and\nTableÂ 16 display the results of the any-way any-shot setting but grouped by Nğ‘N-way and kğ‘˜k-shot, respectively. FigureÂ 13 shows a performance comparison of the different techniques when trained on each of the evaluated settings and tested on the novel any-way any-shot setting. From these results, we can observe that most of the techniques are not benefited from training with a variable number of shots; only MAML shows a performance improvement by this approach."
        ]
    },
    "A6.T16": {
        "caption": "Table 16: Full performance results for cross-domain any-way any-shot image classification per number of shots. The 95% confidence intervals are computed at per-task level using the number of task shown in the table.",
        "table": null,
        "footnotes": [],
        "references": [
            "TableÂ 14 displays the average accuracy per technique and dataset in the any-way any-shot setting. TableÂ 15 and\nTableÂ 16 display the results of the any-way any-shot setting but grouped by Nğ‘N-way and kğ‘˜k-shot, respectively. FigureÂ 13 shows a performance comparison of the different techniques when trained on each of the evaluated settings and tested on the novel any-way any-shot setting. From these results, we can observe that most of the techniques are not benefited from training with a variable number of shots; only MAML shows a performance improvement by this approach."
        ]
    },
    "A6.T17": {
        "caption": "Table 17: The running times for all techniques on all cross-domain settings over all datasets. The number of tasks in each phase is: 540â€‰000 during meta-training, 129â€‰600 during meta-validation, and 54â€‰000 during meta-testing.",
        "table": null,
        "footnotes": [],
        "references": [
            "TableÂ 17 displays the running times per technique in the different cross-domain settings. This time is separated in meta-training, meta-validation and meta-testing. Note that the meta-training and meta-validation time of TrainFromScratch is 0 because this baseline learns every task starting from a random initialization at meta-test time."
        ]
    },
    "A7.T18": {
        "caption": "Table 18: Feature comparison between Meta-Album and other large-scale or (meta-)datasets",
        "table": null,
        "footnotes": [],
        "references": [
            "We compare the Meta-Album meta-dataset with other (meta-)datasets in\nTableÂ 18. Each row shows a (meta-)dataset while each column shows features. The last three rows show versions of the newly proposed meta-dataset, i.e.,Â Meta-Album.\nThe table describes the following features, divided in quantitative and qualitative features:"
        ]
    }
}