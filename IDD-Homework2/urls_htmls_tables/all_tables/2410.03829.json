{
    "id_table_1": {
        "caption": "Table 1:  Statistics on our dataset, including total dataset size, the number of crowd-sourced checkworthy samples, label distribution for  MisLC , and special labels from legal annotations. Each sample can have one ground truth label ( MisLC ,  Non-MisLC , or  Unclear ).",
        "table": "S3.T1.1.1",
        "footnotes": [],
        "references": [
            "As illustrated in Figure  1 , we advocate a two-stage data curation process. First, non-legal crowd-sourced annotators discover checkworthy misinformation samples that arouse their suspicion. Second, legal experts annotate the samples and decide relevant legal issues.",
            "We first want to utilize a laypersons ability to identify misinformation. This component does not require legal expertise, but builds the dataset on which legal practitioners can operate.  We sampled social media data from  (Chen and Ferrara,  2023 ) , a large public domain dataset with Twitter data (tweets) regarding the  Russia-Ukraine conflict . We choose the Russia-Ukraine conflict as a recent event with a significant amount of misinformation, and is extensively studied in previous works  (Alyukov et al.,  2023 ; Tracey et al.,  2022 ) . For more details on data processing and data samples, please refer to Appendix  B.1 .",
            "We performed a secondary adversarial data filtering step to ensure the data is sufficiently consistent. Compared to previous works  (Sakaguchi et al.,  2021 ) , we replaced cross-entropy loss with KL divergence over the annotation distribution to model annotator disagreement. We score each sample by its training loss as defined in Equation  2  and Algorithm  1 . We perform this filtering three times with  k = 1000 k 1000 k=1000 italic_k = 1000  and retain a set of 711 samples that is consistently kept in each trial. The filtering process biases the label distribution to Checkworthy samples, as shown in Table  1 . This complements our intended pipeline where a sample is flagged by laypeople and further investigated by legal annotators, and indicates strongly Checkworthy samples are likely more consistent than ambiguous agreement. Further details are discussed in Appendix  B.4 .",
            "As shown in Figure  2 , the most relevant legal issues for our data to be Freedom of Expression, followed by closely by Defamation. Next, there are Election laws, the criminal offenses of Cyberbullying and Public Mischief, and Hate Speech. Our label distribution is summarized in Table  1 . While checkworthiness had a positive rate of 91.4% (650), only 13.1% (93 samples) of the dataset has some possible legal violation for misinformation. Additionally, there were a substantial number of  Unclear  samples (11.0%, or 78). These are samples with unclear context or implications that annotators felt could not be fact-checked, e.g. we all know what he did. In the context of our formal definition, this implies the evidence  E i subscript E i E_{i} italic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is non-existent, or  | E i | = 0 subscript E i 0 |E_{i}|=0 | italic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | = 0 . Examining the samples that were checkworthy but not a legal violation, there are a few recurring themes:",
            "Along with our dataset, we present a comprehensive set of baselines evaluating the performance of state-of-the-art LLMs on detecting misinformation with legal consequences.  We examine a wide range of both proprietary and open-source LLMs:   GPT-4o 5 5 5 https://openai.com/index/hello-gpt-4o/ ,  GPT-3.5-turbo   Ouyang et al. ( 2022 ) ,  Llama2-(7b, 13b, 70b)   Touvron et al. ( 2023 ) ,  Llama3-(8b, 70b) 6 6 6 https://ai.meta.com/blog/meta-llama-3/ ,  Mistral-7b   Jiang et al. ( 2023a ) , and   Solar-10b   Kim et al. ( 2023 ) . We choose Llama 2 and 3 to isolate the effect of model size, since these suites of models are trained with the same method at various parameter counts. We compare this suite to three open-source models trained on various combinations of fine-tuning, instruction tuning, and Proximal Policy Optimization (PPO) or Direct Policy Optimization (DPO). The  Solar-10b  we test combines two checkpoints of Solar  (Kim et al.,  2023 ) : Solar-Instruct, trained with instruction tuning, and OrcaDPO. Please refer to Appendix  C.1  for further details on the models and Appendix  C  for additional experimental details, including hyperparameters and prompt templates.",
            "LLMs have a significant amount of world knowledge, but our task of misinformation with legal consequences relies on legal material that likely does not exist in their pre-training data. As discussed in Section  3.1 , our ground truth labels are not just determined by the input text  t i subscript t i \\mathbf{t}_{i} bold_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , but also the relevant legal issues  L i subscript L i L_{i} italic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and evidence  E i subscript E i \\mathbf{E}_{i} bold_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . We use RAG to introduce knowledge from our legal literature, as well as to retrieve potential evidence via web search, in order for the model to receive the same information as our legal annotators.",
            "To align language models to our legal issues, we build a database using the full text of the documents compiled in Section  3.1 .  We collect 27 documents with an average length of   \\approx   24,000 tokens and the maximum being   \\approx   96,000 tokens. Having such long documents in the database might cause a few problems: (i) the text chunks are significantly longer than the context window of some LLMs,  and  (ii) most parts of the text chunk are irrelevant to the query. To this end, we perform a process to split the database into small, yet coherent, text chunks. Please refer to Appendix  B.2  for further processing steps.",
            "Older language models, especially the Llama 2 series, show high error rates (ER), i.e., failing to provide an expected keyword for 20-60% of the answers. Upon inspecting the generations, we find they often  refuse to answer the prompt  despite our prompt instructing otherwise. The balance between LLMs generating refusals and following instructions is constantly shifting in the field of AI Alignment (particularly red-teaming), so this might be an intentional shift in LLMs, but it might cause some concern in high-stakes domains. We also perform experiments without this constraint, allowing the model to generate freely and performing more extensive post-processing for evaluation. While the error rate decreases, the trends in performance are inconsistent. Please refer to Appendix  D.1  for further discussion on these additional experiments.",
            "Our task is heavily reliant on external data, evidence  E i subscript E i E_{i} italic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and legal issues  L i subscript L i L_{i} italic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , so a language model should be able to effectively retrieve and parse relevant knowledge. We retrieve from two sources: the legal resources used to create our definition, as described in Section  3.1 , as well as web search. Similar to the above  no-retrieval  setting, the models that have the best general domain performance benefit the most from retrieval. In particular,  GPT-4o  is the only model with a significant increase in performance (+ 9.0 9.0 9.0 9.0  Bin-f1) compared to other models. In the smaller models, combining the two sources  hinders  performance. Compared to the  no-retrieval  setting,  Mistral-7b  has a decrease in performance (- 11.2 11.2 11.2 11.2  Bin-f1). Its 3-way classification performance remains constant, due to the models improved performance on the  Unclear  class.",
            "While retrieval is important due to the broad range of knowledge required to detect and classify misinformation, we also examine the effectiveness of the models when directly given the legal issues  L i subscript L i L_{i} italic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and evidence  E i subscript E i E_{i} italic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . We present two ablations with the FLARE pipeline:  Random-legal , where we retrieve a random document from the legal dataset as a lower bound, and an  Oracle  setting as an upper bound. In the oracle setting, we provide the  definition  of the ground truth legal issues  L i subscript L i L_{i} italic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  as shown in Table  11 . If there are no legal issues, we perform retrieval as per our normal pipeline. We also consider the ground truth evidence  E i subscript E i E_{i} italic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , where we download the sources provided by legal annotators as HTML files, extract the first 500 characters of text, and concatenate all sources as the retrieved document.  We present results with  GPT-4o , our best-performing model, as well as  Llama3-70b  (in Appendix  D.3 ).",
            "Please refer to Table  10  for crowd-sourced annotation instructions. We first chose workers on Mechanical Turk through a prescreening process. We sampled 100 tweets and collected a set of annotations from two researchers given the instructions in Table  10 . The researchers labels had a fourth option of ambiguous  that is, these samples appeared to be too subjective to indicate good understanding of the workers performance. This ambiguous label is automatically assigned where the researchers disagreed, or if one researcher preemptively assigns a sample as ambiguous. Then, we scored all workers with the researcher annotations as a ground truth. A worker needed to have a 70% agreement with researcher annotations, excluding ambiguous samples, and they needed to have completed at least 10 HITs in the prescreening to be considered for further annotation. Among the annotators that met all requirements, two of them only labelled ambiguous samples  for them, we sent a secondary test to obtain a fair assessment. We compensated the workers at $0.18 per HIT.",
            "Please refer to Tables  11  for a comprehensive list of legal issues considered in our annotations."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Summary of our results across nine autoregressive LLMs, open- and closed-source, organized by different classes of model size. Bin-f1 refers to the f1 score in the binary classification setting, where we only consider label 2 ( MisLC ) as the positive class. Ma-f1 and Mi-f1 are the macro- and micro-f1 for the 3-way classification task, where label 1 and 2 ( MisLC ,  Unclear ) are both positive classes.    \\uparrow   indicates higher is better,    \\downarrow   indicates lower is better.",
        "table": "S4.T2.20.20",
        "footnotes": [],
        "references": [
            "The coarse-grained label of  MisLC  is  y i  { 0 , 1 , 2 } subscript y i 0 1 2 y_{i}\\in\\{0,1,2\\} italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  { 0 , 1 , 2 } , where  2 2 2 2  represents  Misinformation with Legal Consequence  ( MisLC ),  1 1 1 1  denotes  Unclear , and  0 0  denotes the negative class, not  MisLC  ( Non-MisLC ).   Unclear  is reserved for cases that are impossible to determine a classification when there is insufficient context to make the decision. This label is crucial because in real-life applications, we need to separate them for further legal processing, including collecting more evidence. The details will be further discussed in Section  3.2 . The  MisLC  evaluation is organized in two settings: (1) a binary task, with  MisLC  as the only positive class of interest, and the other two as negative, and (2) a 3-way classification task, where  MisLC  and  Unclear  are separate positive classes.",
            "We performed a secondary adversarial data filtering step to ensure the data is sufficiently consistent. Compared to previous works  (Sakaguchi et al.,  2021 ) , we replaced cross-entropy loss with KL divergence over the annotation distribution to model annotator disagreement. We score each sample by its training loss as defined in Equation  2  and Algorithm  1 . We perform this filtering three times with  k = 1000 k 1000 k=1000 italic_k = 1000  and retain a set of 711 samples that is consistently kept in each trial. The filtering process biases the label distribution to Checkworthy samples, as shown in Table  1 . This complements our intended pipeline where a sample is flagged by laypeople and further investigated by legal annotators, and indicates strongly Checkworthy samples are likely more consistent than ambiguous agreement. Further details are discussed in Appendix  B.4 .",
            "As shown in Figure  2 , the most relevant legal issues for our data to be Freedom of Expression, followed by closely by Defamation. Next, there are Election laws, the criminal offenses of Cyberbullying and Public Mischief, and Hate Speech. Our label distribution is summarized in Table  1 . While checkworthiness had a positive rate of 91.4% (650), only 13.1% (93 samples) of the dataset has some possible legal violation for misinformation. Additionally, there were a substantial number of  Unclear  samples (11.0%, or 78). These are samples with unclear context or implications that annotators felt could not be fact-checked, e.g. we all know what he did. In the context of our formal definition, this implies the evidence  E i subscript E i E_{i} italic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is non-existent, or  | E i | = 0 subscript E i 0 |E_{i}|=0 | italic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | = 0 . Examining the samples that were checkworthy but not a legal violation, there are a few recurring themes:",
            "The models are first prompted to classify misinformation based on  t i subscript t i \\mathbf{t}_{i} bold_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  without any external knowledge, purely based on their understanding of misinformation along with some evidence  E i subscript E i E_{i} italic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  potentially available in their parametric knowledge. Intuitively, this should be equivalent to the crowd-sourced annotators, and  we do not expect good performance.   Our prompt template can be found in Appendix  C.2 .  We ask the model to only output one of three keywords:  misinformation  for  MisLC ,  factual  for  not MisLC , or  unsure  for  Unclear . Then, we search the generated text for one of these keywords. If none of these keywords are present, we count the generation as an  error , and report Error Rate (ER) for each model. Errors are converted into a  Not MisLC  prediction, i.e. label 0. We also report Binary F1 (Bin-f1) as performance in the binary task setting, and Macro- and Micro-f1 (Ma-f1, Mi-f1) for 3-way classification.",
            "To align language models to our legal issues, we build a database using the full text of the documents compiled in Section  3.1 .  We collect 27 documents with an average length of   \\approx   24,000 tokens and the maximum being   \\approx   96,000 tokens. Having such long documents in the database might cause a few problems: (i) the text chunks are significantly longer than the context window of some LLMs,  and  (ii) most parts of the text chunk are irrelevant to the query. To this end, we perform a process to split the database into small, yet coherent, text chunks. Please refer to Appendix  B.2  for further processing steps.",
            "Our results are summarized in Table  2 . We also provide reference performances in Table  3 , where All label 2 refers to the performance where every prediction is  MisLC , All label 1 refers to the performance where every prediction is  Unclear , and Mean Expert Performance refers to the average human expert performance obtained from Section  3.2 . Bin-f1 refers to the f1 score in the binary classification setting, where we only consider label 2 as the positive class. Ma-f1 and Mi-f1 represent the macro- and micro-f1 for the 3-way classification task, where label 1 and 2 are separate positive classes, as defined earlier in this paper. Overall, the experiments show that the  MisLC  task is challenging for current large language models, even when augmented with retrieval, and they do not achieve human performance. This finding emphasizes the need to develop sophisticated methods to solve our  MisLC  task.",
            "Some models are not responsive to the retrieval methods combined with our task. For example, the  Llama 3  series predict only the  Unclear  class with the IC-RALM retrieval method, scoring  0.0 0.0 0.0 0.0  points on Bin-f1. We hypothesize this is due to the frequency of retrieval in  IC-RALM  compared to  FLARE ;  FLARE  chooses when to retrieve adaptively based on the minimum model perplexity in a generated sentence. This indicates that retrieving too often can harm performance  even in general domain tasks, FLAREs dynamic retrieval is found to perform better than static methods  Jiang et al. ( 2023b ) . We perform additional experiments to explore this hypothesis in Appendix  D.2 , and we urge further study in this direction.",
            "We use embeddings from RoBERTa-Large, and train the linear classifier with a KL divergence loss objective as shown in Equation  2 . Since this is a different task from binary classification, we do not set a fixed    \\tau italic_   instead, we take    \\tau italic_  to be the mean loss over the entire dataset, which we indicate with    subscript   \\tau_{\\mu} italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT . During our filtering process, we find    subscript   \\tau_{\\mu} italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  to be approximately 0.1 across three rounds."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Point-of-reference values for our binary and 3-way classification settings. Random class is a classifier where we sample predictions from a random distribution. The random class performance is taken over 100 runs.   plus-or-minus \\pm   indicates standard deviation.",
        "table": "S5.T3.9.9",
        "footnotes": [],
        "references": [
            "The coarse-grained label of  MisLC  is  y i  { 0 , 1 , 2 } subscript y i 0 1 2 y_{i}\\in\\{0,1,2\\} italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  { 0 , 1 , 2 } , where  2 2 2 2  represents  Misinformation with Legal Consequence  ( MisLC ),  1 1 1 1  denotes  Unclear , and  0 0  denotes the negative class, not  MisLC  ( Non-MisLC ).   Unclear  is reserved for cases that are impossible to determine a classification when there is insufficient context to make the decision. This label is crucial because in real-life applications, we need to separate them for further legal processing, including collecting more evidence. The details will be further discussed in Section  3.2 . The  MisLC  evaluation is organized in two settings: (1) a binary task, with  MisLC  as the only positive class of interest, and the other two as negative, and (2) a 3-way classification task, where  MisLC  and  Unclear  are separate positive classes.",
            "We collected crowd-sourced annotations on this data for  checkworthiness,  in order to filter samples that are likely to contain legal consequences. The crowd-sourced workers could choose between three labels: Checkworthy, Not Checkworthy, or Invalid/No Claim. We sourced our definition of checkworthiness from  (Das et al.,  2023 ) . Additionally, we incorporated indicators of disinformation from an official bulletin released by the Government of Canada.  4 4 4 https://www.canada.ca/en/campaign/online-disinformation.html  To ensure data quality, we conducted a pre-screening test with a pool of 100 samples using the same instructions as the main task. This pool was labelled by two members of the research team given the annotation instructions. Details of annotator instructions and the prescreening procedure are contained within Appendix  B.3 . After this screening process, we obtained a pool of eleven Turk workers for annotations. We randomly sample an additional 1,500 tweets from the 4,000 that we had collected, and provided these to our Turk workers in batches of 500 over one month.",
            "We also calculate an approximation of human performance on our task as an upper bound. First, we assign a random number to each annotator and retrieve all of their individual annotations. For statistical significance, we only retain experts that performed more than 50 annotations. Next, treating their annotations as predictions, we calculate their performance against the majority vote label. We include the mean expert performance for reference in Table  3 .",
            "LLMs have a significant amount of world knowledge, but our task of misinformation with legal consequences relies on legal material that likely does not exist in their pre-training data. As discussed in Section  3.1 , our ground truth labels are not just determined by the input text  t i subscript t i \\mathbf{t}_{i} bold_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , but also the relevant legal issues  L i subscript L i L_{i} italic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and evidence  E i subscript E i \\mathbf{E}_{i} bold_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . We use RAG to introduce knowledge from our legal literature, as well as to retrieve potential evidence via web search, in order for the model to receive the same information as our legal annotators.",
            "In-Context RALM ( IC-RALM )  Ram et al. ( 2023 )  uses the given input  w < i subscript w absent i w_{<i} italic_w start_POSTSUBSCRIPT < italic_i end_POSTSUBSCRIPT  as a query to retrieve a document, and prepends the document to the prompt to generate the output. In this approach, the retrieval is triggered at fixed generation intervals, or retrieval strides    \\delta italic_ . To avoid information dilution with long queries, the query is limited to the last  l l \\ell roman_l  tokens of the  w i subscript w i w_{i} italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . More formally, IC-RALM is formulated in Equation  3 , where  q j  , l = w  . j  l + 1 , ... , w  . j superscript subscript q j  l subscript w formulae-sequence  j l 1 ... subscript w formulae-sequence  j q_{j}^{\\delta,\\ell}=w_{\\delta.j-\\ell+1},...,w_{\\delta.j} italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ , roman_l end_POSTSUPERSCRIPT = italic_w start_POSTSUBSCRIPT italic_ . italic_j - roman_l + 1 end_POSTSUBSCRIPT , ... , italic_w start_POSTSUBSCRIPT italic_ . italic_j end_POSTSUBSCRIPT  and  [ a ; b ] a b [a;b] [ italic_a ; italic_b ]  denotes the concatenation of strings a and b.",
            "FLARE   Jiang et al. ( 2023b )  generates a temporary sentence  s ^ ^ s \\hat{s} over^ start_ARG italic_s end_ARG , where  p  ( s ^ ) =  i = 1 m p  ( w i | w < i ) p ^ s superscript subscript product i 1 m p conditional subscript w i subscript w absent i p(\\hat{s})=\\prod_{i=1}^{m}p\\left(w_{i}|w_{<i}\\right) italic_p ( over^ start_ARG italic_s end_ARG ) =  start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT italic_p ( italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_w start_POSTSUBSCRIPT < italic_i end_POSTSUBSCRIPT ) , and then chooses whether to regenerate the sentence with retrieval based on model confidence, i.e. the minimum token probability in the sentence. This is formulated in Equation  4 , where    \\theta italic_  is the threshold parameter.  Moreover, FLARE formulates the regenerated sentence  s  superscript s  s^{\\prime} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  as   p  ( s  ) =  i = 1 m p  ( w i | [ R C  ( qry  ( w < i ) ) ; w < i ] ) p superscript s  superscript subscript product i 1 m p conditional subscript w i subscript R C qry subscript w absent i subscript w absent i p(s^{\\prime})=\\prod_{i=1}^{m}p\\left(w_{i}|\\left[\\mathcal{R}_{\\mathcal{C}}(%  \\mathrm{qry}(w_{<i}));w_{<i}\\right]\\right) italic_p ( italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) =  start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT italic_p ( italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | [ caligraphic_R start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT ( roman_qry ( italic_w start_POSTSUBSCRIPT < italic_i end_POSTSUBSCRIPT ) ) ; italic_w start_POSTSUBSCRIPT < italic_i end_POSTSUBSCRIPT ] ) . The query formulation function  qry  (  ) qry  \\mathrm{qry}(\\cdot) roman_qry (  )  generates retrieval queries based on the lowest confidence token spans and by masking low confidence tokens. We adapt their implementation to share the same BM25 indexing and retrieval as IC-RALM. Please refer to Appendix  C.3  for further implementation details.",
            "To align language models to our legal issues, we build a database using the full text of the documents compiled in Section  3.1 .  We collect 27 documents with an average length of   \\approx   24,000 tokens and the maximum being   \\approx   96,000 tokens. Having such long documents in the database might cause a few problems: (i) the text chunks are significantly longer than the context window of some LLMs,  and  (ii) most parts of the text chunk are irrelevant to the query. To this end, we perform a process to split the database into small, yet coherent, text chunks. Please refer to Appendix  B.2  for further processing steps.",
            "Our results are summarized in Table  2 . We also provide reference performances in Table  3 , where All label 2 refers to the performance where every prediction is  MisLC , All label 1 refers to the performance where every prediction is  Unclear , and Mean Expert Performance refers to the average human expert performance obtained from Section  3.2 . Bin-f1 refers to the f1 score in the binary classification setting, where we only consider label 2 as the positive class. Ma-f1 and Mi-f1 represent the macro- and micro-f1 for the 3-way classification task, where label 1 and 2 are separate positive classes, as defined earlier in this paper. Overall, the experiments show that the  MisLC  task is challenging for current large language models, even when augmented with retrieval, and they do not achieve human performance. This finding emphasizes the need to develop sophisticated methods to solve our  MisLC  task.",
            "Our task is heavily reliant on external data, evidence  E i subscript E i E_{i} italic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and legal issues  L i subscript L i L_{i} italic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , so a language model should be able to effectively retrieve and parse relevant knowledge. We retrieve from two sources: the legal resources used to create our definition, as described in Section  3.1 , as well as web search. Similar to the above  no-retrieval  setting, the models that have the best general domain performance benefit the most from retrieval. In particular,  GPT-4o  is the only model with a significant increase in performance (+ 9.0 9.0 9.0 9.0  Bin-f1) compared to other models. In the smaller models, combining the two sources  hinders  performance. Compared to the  no-retrieval  setting,  Mistral-7b  has a decrease in performance (- 11.2 11.2 11.2 11.2  Bin-f1). Its 3-way classification performance remains constant, due to the models improved performance on the  Unclear  class.",
            "The label distributions of the best and worst models are shown in Figure  3 , and other models can be found in Figure  6 . The worst-performing model  Llama2-13b  predicts the majority of our samples as  MisLC , i.e. they tended to over-predict the positive class. Of the models tested,  GPT-4o  had the most consistent label balance across all experimental settings. It is important to note the positive class is relatively rare in our dataset; we provide reference values in Table  3 , but the reported performances is also heavily dependent on the distribution of predicted labels for each model.",
            "While retrieval is important due to the broad range of knowledge required to detect and classify misinformation, we also examine the effectiveness of the models when directly given the legal issues  L i subscript L i L_{i} italic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and evidence  E i subscript E i E_{i} italic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . We present two ablations with the FLARE pipeline:  Random-legal , where we retrieve a random document from the legal dataset as a lower bound, and an  Oracle  setting as an upper bound. In the oracle setting, we provide the  definition  of the ground truth legal issues  L i subscript L i L_{i} italic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  as shown in Table  11 . If there are no legal issues, we perform retrieval as per our normal pipeline. We also consider the ground truth evidence  E i subscript E i E_{i} italic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , where we download the sources provided by legal annotators as HTML files, extract the first 500 characters of text, and concatenate all sources as the retrieved document.  We present results with  GPT-4o , our best-performing model, as well as  Llama3-70b  (in Appendix  D.3 )."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Summary of our ablations with  GPT-4o  using FLARE pipeline.",
        "table": "S5.T4.6.6",
        "footnotes": [],
        "references": [
            "We performed a secondary adversarial data filtering step to ensure the data is sufficiently consistent. Compared to previous works  (Sakaguchi et al.,  2021 ) , we replaced cross-entropy loss with KL divergence over the annotation distribution to model annotator disagreement. We score each sample by its training loss as defined in Equation  2  and Algorithm  1 . We perform this filtering three times with  k = 1000 k 1000 k=1000 italic_k = 1000  and retain a set of 711 samples that is consistently kept in each trial. The filtering process biases the label distribution to Checkworthy samples, as shown in Table  1 . This complements our intended pipeline where a sample is flagged by laypeople and further investigated by legal annotators, and indicates strongly Checkworthy samples are likely more consistent than ambiguous agreement. Further details are discussed in Appendix  B.4 .",
            "FLARE   Jiang et al. ( 2023b )  generates a temporary sentence  s ^ ^ s \\hat{s} over^ start_ARG italic_s end_ARG , where  p  ( s ^ ) =  i = 1 m p  ( w i | w < i ) p ^ s superscript subscript product i 1 m p conditional subscript w i subscript w absent i p(\\hat{s})=\\prod_{i=1}^{m}p\\left(w_{i}|w_{<i}\\right) italic_p ( over^ start_ARG italic_s end_ARG ) =  start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT italic_p ( italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_w start_POSTSUBSCRIPT < italic_i end_POSTSUBSCRIPT ) , and then chooses whether to regenerate the sentence with retrieval based on model confidence, i.e. the minimum token probability in the sentence. This is formulated in Equation  4 , where    \\theta italic_  is the threshold parameter.  Moreover, FLARE formulates the regenerated sentence  s  superscript s  s^{\\prime} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  as   p  ( s  ) =  i = 1 m p  ( w i | [ R C  ( qry  ( w < i ) ) ; w < i ] ) p superscript s  superscript subscript product i 1 m p conditional subscript w i subscript R C qry subscript w absent i subscript w absent i p(s^{\\prime})=\\prod_{i=1}^{m}p\\left(w_{i}|\\left[\\mathcal{R}_{\\mathcal{C}}(%  \\mathrm{qry}(w_{<i}));w_{<i}\\right]\\right) italic_p ( italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) =  start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT italic_p ( italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | [ caligraphic_R start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT ( roman_qry ( italic_w start_POSTSUBSCRIPT < italic_i end_POSTSUBSCRIPT ) ) ; italic_w start_POSTSUBSCRIPT < italic_i end_POSTSUBSCRIPT ] ) . The query formulation function  qry  (  ) qry  \\mathrm{qry}(\\cdot) roman_qry (  )  generates retrieval queries based on the lowest confidence token spans and by masking low confidence tokens. We adapt their implementation to share the same BM25 indexing and retrieval as IC-RALM. Please refer to Appendix  C.3  for further implementation details.",
            "As shown in Table  4 , the random document does not confuse the model, with performance increasing consistently by approximately 2 points f1 across all metrics. The oracle setting demonstrate improvement when only performing web search. We observe a decrease in performance when utilizing the ground truth definitions of our legal issues. This indicates the context afforded by the legal resources benefits model performance more than just a definition, but the retrieval algorithm does not necessarily choose the most relevant documents.",
            "Please refer to Figure  4  for an illustration of the two architectures. We take the implementation of IC-RALM directly from their Github, 17 17 17 https://github.com/ai21labs/in-context-ralm  and take most of FLAREs original implementation 18 18 18 https://github.com/jzbjyb/flare  except for the generation. We use ChatGPT for query generation in FLARE  we also tested query generation using the same model (i.e. generating queries with  Llama3-70b , retrieving, and then generating the final answer with  Llama3-70b ) and found performance comparable."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Cherry-picked samples from our dataset comparing crowd-sourced labels of Checkworthiness to our expert annotations of  MisLC .",
        "table": "A2.T5.1.1",
        "footnotes": [],
        "references": [
            "Please refer to Table  5  for example social media posts from our dataset.",
            "For the IC-RALM experiments, we set the  stride  parameter to the  s = 4 s 4 s=4 italic_s = 4  tokens that was used in most of  Ram et al. ( 2023 )  experiments, as it keeps a balance between performance and efficiency. This parameter is the frequency of which the retrieval is triggered. In FLARE experiments, we set the    \\beta italic_  (the confidence threshold for query generation) value to be  0.4 0.4 0.4 0.4  and did a grid search for    \\theta italic_  (the confidence threshold for triggering retrieval) with  100 100 100 100  samples of our dataset to find the best-performing value. We found performance scales consistently with    \\theta italic_ , as shown in Figure  5 , and we choose   = 0.5  0.5 \\theta=0.5 italic_ = 0.5  to balance performance with throughput."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Prompt template used in our experiments. We use a simple sentence to indicate the context of our retrieved document [doc] and/or web search results [snippets], and a keyword Claim to indicate the target input text within the prompt. In the main results, we also add some instructions to constrain the output to a single keyword.",
        "table": "A3.T6.1.1",
        "footnotes": [],
        "references": [
            "The label distributions of the best and worst models are shown in Figure  3 , and other models can be found in Figure  6 . The worst-performing model  Llama2-13b  predicts the majority of our samples as  MisLC , i.e. they tended to over-predict the positive class. Of the models tested,  GPT-4o  had the most consistent label balance across all experimental settings. It is important to note the positive class is relatively rare in our dataset; we provide reference values in Table  3 , but the reported performances is also heavily dependent on the distribution of predicted labels for each model.",
            "Please refer to Table  6  for our prompting format. While individual model prompts might vary based on their specific template formatting requirements, the core text is held constant throughout all of our experiments.",
            "In addition to the stricter prompting instructions reported in the main body, we also evaluate the models without constraining the outputs  i.e. simply asking for a classification, as shown in Table  6 . We evaluate the generations by searching the entire generated text for the keywords factual or misinformation. We first check for the keywords in quotes ( ), as that is the format given in the prompt, and then we check for all other mentions if quotes do not exist. If a models generated text contains both of these keywords, we count this as an  unclear  prediction. For any generation without either keyword, we first filter over all model responses to analyse the responses. Many of these answers are non-answers, such as As an AI language model, I am unable to provide a response. is a non-answer, or an error in the generation. We report the Error Rate (ER) alongside macro- and micro-f1 score."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  A comparison of the RALM retrieval method with FLARE set to retrieve at every possible step (i.e.   = 1  1 \\theta=1 italic_ = 1 ). We conducted experiments for all models but only present results for these three to illustrate the effect of retrieval.",
        "table": "A4.T7.14.14",
        "footnotes": [],
        "references": [
            "We conducted an ablation with FLARE where we always performed retrieval on the legal dataset (i.e. set    \\theta italic_  to 1) and observed similar performance as RALM across all models, summarized in Table  7 . While we conducted experiments with all of the models, we only present three key results. First,  Llama3-8b  had a Bin-f1 score of 0.0 with the IC-RALM retrieval method. However, FLARE even at the highest retrieval level does not exhibit this behaviour."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Summary of our ablations with  Llama3-70b  using FLARE pipeline.",
        "table": "A4.T8.6.6",
        "footnotes": [],
        "references": [
            "Please refer to Table  8  for results with  Llama3-70b . As shown, the trend is similar to what was observed with  GPT-4o ."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Summary of the unconstrained results across seven LLMs, open- and closed-source, organized by model size. Since the size of ChatGPT is unknown, we present it at the top.",
        "table": "A4.T9.20.20",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": [
        "Our code and data are available at",
        "for replicability.",
        "Based on general performance from the Hugging Face Open LLM leaderboard",
        "R. v. Zundel, [1992] 2 S.C.R. 731",
        "Irwin Toy Ltd. v. Quebec (Attorney General), [1989] 1 SCR 927, 58 DLR (4th) 577.",
        "Canadian Constitution Foundation v. Canada (Attorney General), 2021 ONSC 1224"
    ]
}