{
    "S5.T1": {
        "caption": "Table 1. EchoGuide metrics across participants in user study, including correlation with dense 1-second clipping (measured as mean BERT F1 score across all sessions per participant) vs ActSonic correlation with 1=second clipping, and % reduction in activity record using active acoustics vs 1-second clipping",
        "table": "<table class=\"ltx_tabular ltx_figure_panel ltx_guessed_headers ltx_align_middle\" id=\"S5.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T1.1.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_r\" id=\"S5.T1.1.1.1.1\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r\" id=\"S5.T1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.1.1.1.2.1\">EchoGuide vs Dense</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r\" id=\"S5.T1.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.1.1.1.3.1\">ActSonic vs Dense</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column\" id=\"S5.T1.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.1.1.1.4.1\">% Reduction</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T1.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T1.1.2.1.1\">P01</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T1.1.2.1.2\">0.888</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T1.1.2.1.3\">0.854</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T1.1.2.1.4\">95.1%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.3.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.1.3.2.1\">P02</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.1.3.2.2\">0.897</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.1.3.2.3\">0.823</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.1.3.2.4\">53.4%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.4.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.1.4.3.1\">P03</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.1.4.3.2\">0.873</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.1.4.3.3\">0.866</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.1.4.3.4\">83.7%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.5.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.1.5.4.1\">P04</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.1.5.4.2\">0.888</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.1.5.4.3\">0.862</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.1.5.4.4\">95.5%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.6.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.1.6.5.1\">P05</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.1.6.5.2\">0.906</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.1.6.5.3\">0.774</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.1.6.5.4\">55.1%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.7.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.1.7.6.1\">P06</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.1.7.6.2\">0.890</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.1.7.6.3\">0.805</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.1.7.6.4\">34.7%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.8.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.1.8.7.1\">P07</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.1.8.7.2\">0.882</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.1.8.7.3\">0.785</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.1.8.7.4\">40%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.9.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.1.9.8.1\">P08</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.1.9.8.2\">0.897</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.1.9.8.3\">0.853</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.1.9.8.4\">95.9%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.10.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.1.10.9.1\">P09</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.1.10.9.2\">0.907</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.1.10.9.3\">0.833</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.1.10.9.4\">67.5%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [
            "."
        ],
        "references": [
            "We report per-participant metrics in Table 1. We find a relatively large reduction (avg 68%, max 95.9%, min 34.7%) in activity records using active acoustic sensing with relevant domain actions, though reductions are uneven due to the uneven distribution of eating activities (e.g. P06 spent most of the session eating, resulting in a low reduction of the activity record). We found a higher alignment score by combining both ultrasonic and video modalities to capture and record activities when compared to only using the cheaper ultrasonic modality (0.892 avg for EchoGuide vs 0.828 avg for ActSonic, with low alignment values primarily due to a lack of relevant details within the corresponding activity documents, preventing the LLM from giving a detailed response). Notably, these high correlations and significant % reductions are achieved without fine-tuning either the ultrasonic activity clipper or the visual captioning model on new videos, resulting in \u201csession-independent/user-independent\u201d performance metrics. In addition, these results are collected on user study data that is primarily centered around eating activities: if extended to longer \u201ceveryday recordings\u201d where eating is comparatively sparse, future iterations of this system could achieve much higher record reduction metrics.",
            "Improving overall model flexibility to new situations While Sec 5 and Table 1 show promising results for EchoGuide usage (combining video and ultrasonic modalities) across two distinct domains and procedural styles in everyday activities, further improvements can be made to enhance overall system performance. Collecting and fine-tuning on a larger base dataset of ultrasonic captures of activities can enable more robust, user-independent detection of human body motion, while leveraging steadily more powerful large multimodal models can enable more robust and generalizable video captions that encode more domain-specific or estoeric information."
        ]
    },
    "S6.T2": {
        "caption": "Table 2. Results of manual evaluation of EchoGuide + GPT4o given 1fps vs 0.5 fps sampling of frames from ActSonic-defined clips (based on zero-shot accuracy). Notation is defined as (X/Y) where X=accuracy at 1fps and Y=accuracy at 0.5fps",
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S6.T2.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S6.T2.1.1.1\">\n<td class=\"ltx_td ltx_border_l ltx_border_r\" id=\"S6.T2.1.1.1.1\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S6.T2.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.1.1.1.2.1\">Food type (1fps/0.5fps)</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S6.T2.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.1.1.1.3.1\">Utensil type</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.T2.1.1.1.4.1\">Container type</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.1.2.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" id=\"S6.T2.1.2.2.1\">P01</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S6.T2.1.2.2.2\">0/0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S6.T2.1.2.2.3\">0/0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T2.1.2.2.4\">1/0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.1.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\" id=\"S6.T2.1.3.3.1\">P02</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S6.T2.1.3.3.2\">1/1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S6.T2.1.3.3.3\">1/1</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.3.3.4\">1/1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.1.4.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\" id=\"S6.T2.1.4.4.1\">P03</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S6.T2.1.4.4.2\">0/0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S6.T2.1.4.4.3\">1/0</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.4.4.4\">1/0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.1.5.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\" id=\"S6.T2.1.5.5.1\">P04</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S6.T2.1.5.5.2\">0/0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S6.T2.1.5.5.3\">0/0</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.5.5.4\">0/0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.1.6.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\" id=\"S6.T2.1.6.6.1\">P05</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S6.T2.1.6.6.2\">0/0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S6.T2.1.6.6.3\">1/0</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.6.6.4\">0/0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.1.7.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\" id=\"S6.T2.1.7.7.1\">P06</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S6.T2.1.7.7.2\">1/1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S6.T2.1.7.7.3\">1/1</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.7.7.4\">1/1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.1.8.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\" id=\"S6.T2.1.8.8.1\">P07</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S6.T2.1.8.8.2\">1/0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S6.T2.1.8.8.3\">1/1</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.8.8.4\">1/1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.1.9.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\" id=\"S6.T2.1.9.9.1\">P08</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S6.T2.1.9.9.2\">1/1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S6.T2.1.9.9.3\">1/1</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.9.9.4\">1/1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T2.1.10.10\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\" id=\"S6.T2.1.10.10.1\">P09</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S6.T2.1.10.10.2\">0/0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S6.T2.1.10.10.3\">1/1</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T2.1.10.10.4\">1/1</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "EchoGuide, however, focuses not only on providing general summaries via activity records of an individual\u2019s day from video and wearable sensor data, but also on answering targeted questions about these summaries by leveraging the image-text pertaining of large multimodal language models. We evaluate this method via manual review and annotation of the system\u2019s answers to three eating questions (\u201cWhat did C eat/drink? What utensils did C use while eating/drinking? What container did C eat or drink out of?\u201d) when configured to use GPT4o <cite class=\"ltx_cite ltx_citemacro_citep\">(GPT, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2406.10750v2#bib.bib2\" title=\"\">[n.&#8201;d.]</a>)</cite> to caption images sampled at two varying FPS levels (1fps and 0.5fps) from clips proposed by ActSonic, and report accuracy metrics showing whether EchoGuide extracts correct values for these questions as compared to manually-determined &#8221;ground truth&#8221; (taken by watching the reference video and determining which item is present): we&#8217;ve shown accuracy values given 1fps and 0.5fps sampling in Table \n(GPT, [n.\u2009d.]) to caption images sampled at two varying FPS levels (1fps and 0.5fps) from clips proposed by ActSonic, and report accuracy metrics showing whether EchoGuide extracts correct values for these questions as compared to manually-determined \u201dground truth\u201d (taken by watching the reference video and determining which item is present): we\u2019ve shown accuracy values given 1fps and 0.5fps sampling in Table 2. Accuracy values are defined as a 0/1 binary: 0 represents responses that do not overlap with the ground truth, while 1 represents responses that do completely overlap with the ground truth."
        ]
    }
}