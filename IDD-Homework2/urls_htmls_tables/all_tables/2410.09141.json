{
    "id_table_1": {
        "caption": "Table 1:  Performance comparison of  ACER  and baselines on Natural Question (NQ), TriviaQA (TQA), and NarrativeQA. The best performance on each dataset is boldfaced.",
        "table": "S4.T1.1",
        "footnotes": [],
        "references": [
            "In this paper, to alleviate this problem and provide an intermediate solution, we propose a new approach which  A utomates  C ontext  E xtension via  R etrieval ( ACER ;  Figure 1 ). Overall,  ACER  is a two-stage method. We start with synthesizing  imperfect  data by combining retrieval with an LM  excels in short context . In the subsequent stage, we will fine-tune a large LM to bootstrap over this data.  ACER  will start with some pairs of question and its long context, while no labeling is required. In the synthesis pipeline, the long context is broken into chunks and a retrieval model will score and rank the chunks. A small set of top-ranked chunks will be fed into the short-context LM to produce answer  with chain-of-thought  (CoT;  Wei et al. ( 2022 ) ) reasoning. Then, in the fine-tuning stage, we train the model using the context, question, and the CoT. On the other hand, the retrieval-based data synthesis process will be hidden from the model. We desire that the deep model will learn a generic long-context understanding function by fitting over the full context and the CoT reasoning. We hypothesize that the model may discover a better latent function that transcends the original ranking mechanism used in the first stage. This also shares some spirit with LM pre-training. Whereas typically pre-training bootstrap from existing human data, due to the apparent data scarcity,  ACER  will bootstrap from synthetic long-context data generated using retrieval.",
            "In this section, we will give an overview of  ACER . The  ACER  method consists of two major stages: 1) automatic data synthesis, and 2) self training, as illustrated in  Figure 1 .  In this chapter, we will describe how each of these stages work.",
            "In  Table 1 , we show the performance of the compared systems as well as  ACER  on the evaluation datasets. We see a general trend that  ACER  outperforms the compared systems with decent margins especially on the novel long-context RAG tasks. We observe a general trend that the closed data model performing better than models trained on open data. Specifically, the Together-Llama-2 model, which is based on the previous generation Llama model, significantly underperforms all other model. This is likely due to two facts that the model essentially stems from an earlier generation, and its shorter context requires it to do more extrapolation."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Performance of  ACER  and baselines with different base retrievers.",
        "table": "S5.T2.1",
        "footnotes": [],
        "references": [
            "We show the prompt of the ranker model in  Figure 2 . We instruct the model to read the question and context chunk (referred to as passage in the prompt) and  think step-by-step  to decide the helpfulness of the passage for answering the question. We formulate it as a multiple choice question where the model is instructed to choose from 5 options from the most a) providing exact answer to e) not related.",
            "The previously discussed  ACER  pipeline for RAG tasks adopts a supervisedly trained dense retriever. In certain situations where even retrieval data is scarce, obtaining such a retriever may not even be possible. People instead need to fall back to the classical BM25 retrievers. In this section, we consider such a situation by running  ACER  and some of the baseline systems with BM25. This means the top-100 candidates will be different and of lower quality. In  Table 2 , we show the results of this BM25 setup compared with the original dense setup. While using BM25, all systems take a hit in performance because of lower-quality candidates, the general performance order of the systems remain the same. Our  ACER  method still outperforms the other systems by decent margins. In fact, when looking at the the absolute numbers, we can observe that  ACER  with BM25 attains close or even better performance than best-performing baseline systems. The results suggest that  ACER  is relatively agnostic to the retriever used and attains good performance to warm-start a system without requiring extra supervision."
        ]
    },
    "global_footnotes": []
}