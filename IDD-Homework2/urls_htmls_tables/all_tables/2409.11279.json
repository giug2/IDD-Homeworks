{
    "id_table_1": {
        "caption": "Table 1.  Comparison of Different Method on ALFRED. Star (*) stands for using step-by-step instruction instead of goal instruction. G.T. means ground-truth action. In the Dataset column, Full indicates the utilization of the entire training dataset, while Part indicates sampling from a subset of the training dataset. P-RAG (Self-Iter.) represents the iterative updates of P-RAG on the same group of task datasets.",
        "table": "S4.T1.3",
        "footnotes": [],
        "references": [
            "To address the issue of understanding linguistic instructions and imbuing knowledge about specific tasks and situations, we propose a new framework called  Progressive Retrieval Augmented Generation  (P-RAG) for embodied everyday tasks. It is based on LLM, and designs progressive retrieval to assist in generating actions with specific contextual knowledge iteratively, as shown in Fig.  1 . The progressive mechanism can iteratively increase the success rate similar to learning-based approach, without involving any training steps. With the text understanding ability of LLM, P-RAG combines both the advantages of both learning-based approaches and pretrained LLM for planning approaches. Indeed, previous works like LLM-planner  (Song et al . ,  2023 )  have also employed retrieval augmented generation with ground few-shot to enhance the agents knowledge of specific environments and situations. In comparison, P-RAG has improvements in the following aspects: 1) Instead of using ground-truth action list as few-shot samples, P-RAG utilizes data generated through straightforward interactions with the environment, which is more general for real scenes. 2) In the query, we not only consider searching for trajectory information related to similar tasks but also take into account trajectory information corresponding to similar situations, which provides more valuable context for LLM.",
            "To accomplish embodied everyday task, we propose a new progressive retrieval augmented planning framework named P-RAG, which integrates better task-specific knowledge progressively into the prompt of large language models (LLMs) for planning. The framework of P-RAG is shown in Fig.  1 . During each interaction episode, the trajectory of agent is collected to iteratively update a dynamic database, which is retrieved to provide task-relevant knowledge from the previous completed interaction.",
            "As depicted in Algorithm  1 , P-RAG initializes an empty database and initiates successive iterations. At this stage, P-RAG makes decisions solely based on observations of the environment and the generic prior knowledge of LLM. After the first iteration, the historical information from the previous iteration is collected and stored in the database. The database is progressively updated when the agent trajectories of new round are completed. In each subsequent iteration, P-RAG utilizes the collaborative similarity retrieval of scene graph and task name to identify similar tasks and scenes across different tasks. It then provides this information to LLM to facilitate more informed decision-making. The progressive approach is widely employed in learning-based solutions, yet its application in the RAG (Retrieval-Augmented Generation) framework for planning remains a novel paradigm. In scenarios where ground truth is not available, the progressive approach offers a mode of gradual performance improvement compared to the direct method. Moreover, it allows for the accumulation of environment-specific knowledge for the agent through historical trajectories.",
            "As illustrated in Table  1 , we categorize different methods into two groups based on experimental settings. One group including HiTUT  (Zhang and Chai,  2021 ) , HLSM  (Blukis et al . ,  2022 ) , E.T.  (Pashevich et al . ,  2021 )  and M-TRACK  (Song et al . ,  2022 )  utilizes the full train set with 21,023 instruction and trajectory pairs to derive performance results. Another group including HLSM  (Blukis et al . ,  2022 ) , LLM-Planer  (Song et al . ,  2023 )  and Saycan  (Ahn et al . ,  2022 )  utilizes a smaller subset of training data. Note that within the experimental setup, certain methods denoted with a star (*) utilize step-by-step instructions rather than goal instructions, such as Saycan  (Ahn et al . ,  2022 )  and E.T.  (Pashevich et al . ,  2021 ) . The performance comparison in the table reveals that our approach denoted as P-RAG (Ours) outperforms existing state-of-the-art methods of utilizing few training dataset on both Valid Seen and Valid Unseen dataset. These results demonstrate the effectiveness of our method, which develops the progressive retrieval augmentation to assist the large language model to obtain task-relevant information. Compared to methods trained on the full dataset, P-RAG maintains high performance even under conditions where only approximately 1/200 of the training data is utilized. These results prove that our method possesses better generalizability than these training methods, which usually overfit the training datasets and achieve better performance on the Val Seen dataset than Val Unseen dataset.",
            "Besides the standard setting of our method, we also construct a self-iteration variant denoted as P-RAG (Self-Iter.). It constructs progressive iteration over the test dataset, which means the retrieval database is constructed by the trajectories on test dataset. It directly improves the success rates of testing tasks, since the agent obtains better task-relevant experiences about testing tasks by progressive retrieval. From Table 1, with self-iteration on the Valid Unseen dataset, our method denoted as P-RAG (Self-Iter.) can even outperform all the methods, both with and without using the entire training dataset, which further verifies the effectiveness of the proposed progressive retrieval augmented planning framework.",
            "For the former, in the last row of Table  1 , P-RAG is demonstrated to be effective through iterations on both Valid Unseen and Valid Seen. Through iterations, P-RAG outperforms nearly all the methods, including step-by-step instruction and training on the full dataset, in performance on Valid Unseen without training and with a few number of samples. Similarly, in Table  2 , conducting two sets of experiments with LLM as GPT-4 and GPT-3.5 also demonstrates the effectiveness of the progressive method."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.  Comparison of Retrieval Augmented Models on MINI-BEHAVIOR. GPT-3.5 and GPT-4 represent the results of each as the baseline LLM planner. P-RAG-3.5 and P-RAG-4 represent the results of setting the LLM in P-RAG as GPT-3.5 and GPT-4, respectively.",
        "table": "S4.T2.1",
        "footnotes": [],
        "references": [
            "Specifically, the detailed pipeline in each episode is shown in Fig.  2 . First, the agent is provided with four types of information including natural language instruction, observations, the action space, and the retrieval results from database. Then, this information is input to LLM for planning, which outputs a series of actions for solving the embodied every task. After that, the environment responds to the receiving actions and converts to the new state, which provides agent new observations and reward. These new trajectory information for agent will be used to update the database, which stores the timely agent experience. In the next interaction episode, this experiential knowledge in the database can be retrieved based on agent information, returning task-relevant knowledge to the LLM for better planning. This pipeline can be iteratively improved, progressively enhancing planning ability by incorporating more successful trajectory information. The detailed components of this pipeline will be elaborated in the following subsections.",
            "The four components of information mentioned above in section  3.2  will be integrated into a single coherent prompt and provided as input to the LLM. Fig.  2  part b illustrates the main workflow of this section, including the LLM, error check, action filter, and decomposition components.  For the first component, we choose GPT-4 and GPT-3.5, renowned for their widespread utilization across various generation tasks, to serve as LLM in P-RAG. The second part involves error checking, where the input consists of the text output from the LLM. It employs regular expression matching and compares the generated actions with those in the action space to ensure adherence to fixed formatting and validity. In the case of invalid actions or non-compliant formatting, a new requirement will be raised for the LLM. Otherwise, the output from the LLM will be passed on to the next component. The third component is the action filter, which extracts the necessary strings from the text generated by the LLM using regular expression matching, providing high-level actions. The final component is decomposition, which translates the high-level actions obtained from the previous component into low-level actions to be executed in the environment. For example, the navigation high-level action is decomposed using the Fast Marching Method (FMM)  (Sethian,  1999 ) .",
            "We also evaluate the performance of P-RAG on MINI-BEHAVIOR which lacks ground-truth actions annotation, and therefore can be used to evaluate our method and compared methods in weak supervision setting. As shown in Table  2 , we evaluate through three evaluation metrics: Total success rate (SR), which represents the success average by episodes; Task success rate (SR), indicating success average by tasks; Success weighted by Path Length (SPL), evaluated according to the following formula:",
            "For the former, in the last row of Table  1 , P-RAG is demonstrated to be effective through iterations on both Valid Unseen and Valid Seen. Through iterations, P-RAG outperforms nearly all the methods, including step-by-step instruction and training on the full dataset, in performance on Valid Unseen without training and with a few number of samples. Similarly, in Table  2 , conducting two sets of experiments with LLM as GPT-4 and GPT-3.5 also demonstrates the effectiveness of the progressive method."
        ]
    },
    "id_table_3": {
        "caption": "Table 3.   Success Rate with the Iteration Number on ALFRED.",
        "table": "S4.T3.1",
        "footnotes": [],
        "references": [
            "Initially, during the interaction with the task environment, the current tasks goal instruction will be provided, such as Move a chilled mug to the coffee maker. During the interaction between P-RAG and the environment, before the agent provides an action, it will first return an observation image to depict the current observed state. We convert the observation image into a scene graph format, which is easier for LLMs to process. We extract the instance labels of all objects from the image and detect the relative relationships between each object, such as on the top of, inside of, and so on. In detail, we slightly adjust the extraction of the scene graph for different interaction environments. For ALFRED, we utilize tools from ALFWORLD, which include interactive TextWorld environments  (Cote et al . ,  2019 )  that mirror embodied worlds in the ALFRED dataset  (Shridhar et al . ,  2020a ) , to extract instance-level labels of objects observed in the image directly in front of the robot in the environment. After obtaining the instance labels, we first identify the landmarks among them and designate them as key nodes in the scene graph. Next, we organize these labels into a list format, with the key node being placed as the first node in the list for subsequent encoding steps. For the MINI-BEHAVIOR environment, which utilizes gym-minigrid, we sequentially compare the relative relationships between each object by function API in the environment to obtain a relationship matrix with form of each cell in matrix like object_1/object_2/relationship: True/False. After obtaining the relationship matrix, to reduce the length of the context input to the LLM, P-RAG retains only those pairs that are true. The action space is also a crucial component, and the action space provided to the LLM consists of high-level actions. Subsequent steps in section  3.3  will involve instruction decomposition, where high-level instructions are converted into low-level actions to be provided to the environment.",
            "The four components of information mentioned above in section  3.2  will be integrated into a single coherent prompt and provided as input to the LLM. Fig.  2  part b illustrates the main workflow of this section, including the LLM, error check, action filter, and decomposition components.  For the first component, we choose GPT-4 and GPT-3.5, renowned for their widespread utilization across various generation tasks, to serve as LLM in P-RAG. The second part involves error checking, where the input consists of the text output from the LLM. It employs regular expression matching and compares the generated actions with those in the action space to ensure adherence to fixed formatting and validity. In the case of invalid actions or non-compliant formatting, a new requirement will be raised for the LLM. Otherwise, the output from the LLM will be passed on to the next component. The third component is the action filter, which extracts the necessary strings from the text generated by the LLM using regular expression matching, providing high-level actions. The final component is decomposition, which translates the high-level actions obtained from the previous component into low-level actions to be executed in the environment. For example, the navigation high-level action is decomposed using the Fast Marching Method (FMM)  (Sethian,  1999 ) .",
            "During each round of data update, the database will retain four key pieces of information for each task interaction sequence: the goal instruction, scene graph, historical data, and completion state. The details of database construction and retrieval pipeline is illustrated in Fig.  3 . When an agent interacts with the environment during a task, it first receives the environments goal instruction  I g subscript I g I_{g} italic_I start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT  and observation  O t subscript O t O_{t} italic_O start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . Then it encodes with MiniLM  (Wang et al . ,  2020 )  both of them with formula as",
            "Progressive Iteration . We conduct additional experiments on P-RAG regarding self-revolution to validate its progressive effect. Compared to the previous approach of iterating on the training dataset, we directly conduct progressive retrieval iteration on the same dataset, which we call self-iteration. In particular, we choose to conduct self-iteration on the Valid Seen dataset and training dataset (where there may be overlap between the Valid Seen and Train datasets), respectively. From Table  3 , there is an improvement after progressive iteration in both task datasets. P-RAG not only demonstrates the advantages of performance in standard experimental setting as shown in the first row Train100, but also achieves performance improvement through progressive iteration on the Valid Unseen dataset as shown in the second row in Table  3 ."
        ]
    },
    "id_table_4": {
        "caption": "Table 4.  Relation Between Iteration on ALFRED. N-th Iter. represents the completion status of tasks after the n-th iteration of P-RAG. For the row labeled Next m-th. each cell represents the comparison between the (n+m)-th iteration and the n-th iteration. The value in each cell represents the proportion of True/False occurrences of done in its column compared to the total occurrences of True/False in its corresponding rows iterations.",
        "table": "S4.T4.1",
        "footnotes": [],
        "references": [
            "We compare the success rate of P-RAG relative to its corresponding baseline with LLM selecting either GPT-4 or GPT-3.5. From the results in the table, we can observe that P-RAG demonstrates significant improvements of 1.7% and 2.5% compared to the baselines of GPT-4 and GPT-3.5, respectively. Although simple and lightweight, MINI-BEHAVIOR presents a formidable challenge for popular Reinforcement Learning algorithms, particularly in the absence of a dense reward signal. The vanilla PPO algorithm is only able to achieve a valid success rate (approximately 8%) after training 1e6 steps and evaluating on a single task (not work in other tasks) within MINI-BEHAVIOR  (Jin et al . ,  2023 ) . In contrast, P-RAG achieves 16.7% Total SR requiring iterations of no more than 6 eposides and accomplish 5 tasks (compare to single task of vanilla PPO algorithm), demonstrating its effectiveness across different environments and its few-shot property. We select one group of task trajectories as visualization result, as shown in Fig.  4 . The visualization of trajectory demonstrates the decision-making processes of P-RAG and the GPT-4 baseline for the task water houseplants. From the visualization results, it can be observed that P-RAG is able to make judgments with more task-specific knowledge by referring to historical trajectory information.",
            "Correlation Between Iteration . We also continue to study the correlation between successfully completed tasks during the iterative process of P-RAG. Table  4  displays the correlations between tasks completed at different iteration stages. The data values in the table are derived from iterations 0, 1, 2, and 3 in the ALFRED dataset. Each pair of True and False corresponds to successful and failed tasks in a single iteration. The value within each cell indicates the ratio of occurrences of done compared to the total occurrences of True/False in the corresponding rows iteration. Considering the value located in the top-left corner of the table, this value signifies the proportion of successful tasks from the initial iteration that remain successful after the first subsequent iteration.",
            "By analyzing the data in Table  4 , it is evident that P-RAG predominantly achieves success in subsequent iterations based on the success of the previous iteration. Additionally, it demonstrates the capability to succeed in tasks that were unsuccessful in the previous iteration. This highlights how P-RAG not only effectively maintains performance from the previous iteration but also leverages information from unsuccessful trajectories to achieve new successes. This underscores the sources of performance enhancement in P-RAG."
        ]
    },
    "id_table_5": {
        "caption": "Table 5.  Ablation with Scene Graph on ALFRED Valid Unseen. The first 3 columns represent P-RAG with 3 rounds of self-iterations on Valid Unseen dataset. The following two columns indicate testing on Valid Unseen after iterations on Train100.",
        "table": "S4.T5.1",
        "footnotes": [],
        "references": [
            "We also conduct in-depth experiments on performance saturation, by enhancing retrieval through updating historical information from the previous round after encoding it into the database. Subsequently, we update the database based on the current rounds historical trajectory, iterating multiple times until saturation is achieved, as shown in Fig.  5 . From Fig.  5 , P-RAG achieves significant performance improvement through iteration. In the ALFRED Valid Unseen dataset, the success rate is improved from 7.05% at the beginning to 27.4% after 5 rounds of iteration. Similarly, in the ALFRED Train 100 dataset, the success rate increases from 5% at the beginning to 11% after 3 rounds of iteration. Both of them all eventually reach performance saturation. From the curves, it can be observed that the curve of success rate increase in each iteration of P-RAG until convergence, indicating that it is approaching the performance limit of LLM as a Planner on the testing tasks.",
            "To further evaluate the effectiveness of the progressive mechanism, we conduct extra ablation experiments on the ALFRED Valid Unseen dataset, as shown in Table  5 . The interpretation of the data in the table is as following. The GPT-4 column represents the results by using the GPT-4 baseline for testing. The P-RAG (Self-Iter.) colunm represents the results obtained through testing with 1, 2 and 3 iterations, respectively. The P-RAG column represents the results obtained by iteratively using task name and scene graph joint retrieval on the Train100 dataset and then testing on the Valid Unseen dataset. The P-RAG w/o SG column represents the results obtained by solely using task name for retrieval, iteratively on the Train100 and then testing on the Valid Unseen dataset. From the table, it is evident that compared to not utilizing scene graph for joint retrieval, P-RAG exhibits significant improvements. This demonstrates the effectiveness of the progressive mechanism and the necessity of incorporating scene graph and task name retrieval."
        ]
    },
    "global_footnotes": []
}