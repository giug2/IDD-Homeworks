{
    "id_table_1": {
        "caption": "Table 1:  Results(%) of overall performance on all experimental datasets.  The  Grey Area  represents different categories of baselines for comparison.  Bold  means the highest values within the results of different baselines with GPT-3.5 as the built-in model. Meanwhile, the symbol  denotes the increase in value compared to Vanilla. The output from Llama3-8b contains a substantial amount of English text; therefore, we do not report its results on the Chinese dataset CRUD, marked as -.",
        "table": "Sx3.T1.1",
        "footnotes": [],
        "references": [
            "In this section, we provide the task definition and an overview of our method   Adaptive-Note  (in Figure  1 ), and then delve into the details of each component. More details and all prompt templates are in Appendix B.",
            "In Table  1 , we conduct an in-depth comparison of our approach with several existing ARAG methods, including Flare, Self-RAG, and ReAct. Results indicate that our method consistently achieves the highest performance in both Multi-hop QA and Long-form QA tasks, outperforming the other methods by a considerable margin,  which highlights its superiority, effectiveness, and robustness.  To explain these trends, we conduct an in-depth analysis of the limitations of these baselines and the reasons behind our success.   First , ReAct leverages the LLMs internal knowledge to determine whether further retrieval actions are necessary. However, LLMs may be overly confident  (Zhou, Jurafsky, and Hashimoto  2023 )  in their own responses, which can interfere with retrieval decisions and lead to ignoring partial knowledge.  In contrast, our method employs a greedy strategy to thoroughly gather information first, before deciding whether to integrate new and useful information into the existing knowledge. This allows our method to maximize the extraction of knowledge from the corpus, thereby markedly enhancing the accuracy of responses.   Second , Self-RAG heavily relies on multiple self-reflection processes to assess the importance of each retrieved passage, which may miss valuable information if the reflections are inaccurate.  In contrast, our method allows the LLM to carefully review and robustly integrate relevant knowledge from each passage into its memory.  Moreover, our method does not limit the smallest unit of judgment to individual passages; it can evaluate text of any length or form.",
            "In Table  A1  and Table  A2 , we provide specific values and additional results of the in-depth comparison study under a fair top- k k k italic_k  setting across all datasets.  The results suggest that our method outperforms Vanilla RAG under the fair top-k setting across all datasets and metrics. This aligns with the trends observed in Figure  2 , further demonstrating the superiority, effectiveness, and robustness of our Adaptive-Note.  Especially on the 2WikiMQA dataset, we outperform Vanilla RAG (top-9) by +11.1% in F1-score (f1) and +5.4% in Exact Match (em)."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Results (%) of ablation study.",
        "table": "Sx5.T2.1",
        "footnotes": [],
        "references": [
            "Under the same raw top- k k k italic_k  setting, ARAG methods tend to use more retrieved passages than single-step methods.  Unfortunately, while we can set the top- k k k italic_k  value for each retrieval step, we cannot control the total number of retrieved passages due to the retrieval uncertainty in ARAG.  To ensure a fairer performance comparison, we bridge this gap.  In our experiments, we calculate the average number of deduplicated passages per sample during all adaptive steps, termed the fair top- k k k italic_k .  We then apply this fair top- k k k italic_k  to Vanilla RAG as the top- k k k italic_k  for a single retrieval.  Figure  2  shows the overall performance of Vanilla RAG under the fair top- k k k italic_k  setting. Additionally,  we interpolate Vanillas results for intermediate top- k k k italic_k  values between the raw and fair top- k k k italic_k .  The results suggest a similar trend across all datasets. As the value of top- k k k italic_k  increases, Vanilla RAGs performance improves but remains significantly lower than our method.  These findings further underline the superiority of our method.",
            "In Table  2 , Ours w/o AMR represents that our method uses only the Iterative Information Collector (IIC) module. Meanwhile, Ours w/o IIC&AMR means that our method does not use either the IIC or the Adaptive Memory Reviewer (AMR), which effectively equivalent to the Vanilla RAG we aforementioned.  The results suggest that adding the IIC to the Vanilla RAG improves performance across all datasets, confirming its effectiveness. The modules gains are most pronounced on multi-hop datasets.  This makes sense since multi-hop tasks rely on previous key information to determine what to explore in the next step. Our note-based iterative updates generate new queries using past knowledge, which closely mirrors the multi-hop QA process.  For ASQA, the gain likely stems from broad information exploration and integration.",
            "In Table  A1  and Table  A2 , we provide specific values and additional results of the in-depth comparison study under a fair top- k k k italic_k  setting across all datasets.  The results suggest that our method outperforms Vanilla RAG under the fair top-k setting across all datasets and metrics. This aligns with the trends observed in Figure  2 , further demonstrating the superiority, effectiveness, and robustness of our Adaptive-Note.  Especially on the 2WikiMQA dataset, we outperform Vanilla RAG (top-9) by +11.1% in F1-score (f1) and +5.4% in Exact Match (em)."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Adaptive-Note with different parameters in AMR.",
        "table": "Sx5.T3.1",
        "footnotes": [],
        "references": [
            "In Table  3 , with IU fixed at 1, the performance on multi-hop QA datasets improves as CI increases, reaching its peak when CI is 3.  However, ASQA already reaches its best performance when CI is 2, likely due to the need for deeper exploration in multi-hop QA.  Thus, we recommend using a smaller IU to potentially reduce costs while achieving similar gains as higher IU values.",
            "We present all the prompts used in our method in Table  A3  and Table  A4 . In Table  A3 , we detail the prompt of each stage of two core components, Iterative Information Collector (IIC) and Adaptive Memory Reviewer (AMR).  It is worth noting that since the prompts in our core components are consistent across all datasets, we present only the English versions. In our experiments, the Chinese prompts used for the CRUD dataset convey the same meanings as the provided English prompts.  Specifically, at the note initialization stage, {query} represents the original query  q q q italic_q , and {refs} represents the retrieved  k k k italic_k  passages obtained by feeding the original query  q q q italic_q  into the retriever.  At the iterative information collection stage, {query} still means the original query  q q q italic_q , and {note} actually represents the content of the optimal memory  M o  p  t subscript M o p t M_{opt} italic_M start_POSTSUBSCRIPT italic_o italic_p italic_t end_POSTSUBSCRIPT .  Additionally, as mentioned in the main text, LLMs tend to ask similar questions again if previous questions were not well resolved. We introduce the already-asked questions list {query_log} to avoid this issue.  At the note-updating stage, {query} still refers to the  q q q italic_q , while {refs} refers to new retrieved  k k k italic_k  passages according to the new set of queries, and the {note} means the  M o  p  t subscript M o p t M_{opt} italic_M start_POSTSUBSCRIPT italic_o italic_p italic_t end_POSTSUBSCRIPT . Then, we obtain the current state note  N t subscript N t N_{t} italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .  In the Tigger1 of the AMR stage, {query} represents the original query  q q q italic_q . The {best_note} and {new_note} represent  M o  p  t subscript M o p t M_{opt} italic_M start_POSTSUBSCRIPT italic_o italic_p italic_t end_POSTSUBSCRIPT  and  N t subscript N t N_{t} italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Adaptive-Note with different  top- k k \\bm{k} bold_italic_k .",
        "table": "Sx5.T4.1",
        "footnotes": [],
        "references": [
            "In Table  4 , we compare our method with the Vanilla RAG across various top- k k k italic_k  settings.  Our method consistently outperforms Vanilla RAG under the same top- k k k italic_k  conditions, confirming its robustness in achieving stable gains regardless of the number of retrieved passages.  Furthermore, in the majority of cases, our systems performance improves as the number of retrieved passages (top- k k k italic_k ) per step increases.",
            "We present all the prompts used in our method in Table  A3  and Table  A4 . In Table  A3 , we detail the prompt of each stage of two core components, Iterative Information Collector (IIC) and Adaptive Memory Reviewer (AMR).  It is worth noting that since the prompts in our core components are consistent across all datasets, we present only the English versions. In our experiments, the Chinese prompts used for the CRUD dataset convey the same meanings as the provided English prompts.  Specifically, at the note initialization stage, {query} represents the original query  q q q italic_q , and {refs} represents the retrieved  k k k italic_k  passages obtained by feeding the original query  q q q italic_q  into the retriever.  At the iterative information collection stage, {query} still means the original query  q q q italic_q , and {note} actually represents the content of the optimal memory  M o  p  t subscript M o p t M_{opt} italic_M start_POSTSUBSCRIPT italic_o italic_p italic_t end_POSTSUBSCRIPT .  Additionally, as mentioned in the main text, LLMs tend to ask similar questions again if previous questions were not well resolved. We introduce the already-asked questions list {query_log} to avoid this issue.  At the note-updating stage, {query} still refers to the  q q q italic_q , while {refs} refers to new retrieved  k k k italic_k  passages according to the new set of queries, and the {note} means the  M o  p  t subscript M o p t M_{opt} italic_M start_POSTSUBSCRIPT italic_o italic_p italic_t end_POSTSUBSCRIPT . Then, we obtain the current state note  N t subscript N t N_{t} italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .  In the Tigger1 of the AMR stage, {query} represents the original query  q q q italic_q . The {best_note} and {new_note} represent  M o  p  t subscript M o p t M_{opt} italic_M start_POSTSUBSCRIPT italic_o italic_p italic_t end_POSTSUBSCRIPT  and  N t subscript N t N_{t} italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .",
            "In Table  A4 ,  we detail the prompts of the task-oriented generator on different datasets.  Since each dataset has different output styles and preferences, we follow the prompts provided by the authors in the original papers of the datasets during the generation stage.  The {query} and {note} correspond to the original query  q q q italic_q  and the content in the optimal memory  M o  p  t subscript M o p t M_{opt} italic_M start_POSTSUBSCRIPT italic_o italic_p italic_t end_POSTSUBSCRIPT  after the iteration, respectively.  Specifically, in ASQA, since our work does not involve outputting citations, we discard this function from the author-provided prompt to more clearly assess the capabilities of our method. Furthermore, we use the aligned generator prompt across all baselines in our experiments, ensuring the fairness of our results.  For CRUD, we provide English translations of the Chinese prompts where we remove the exemplars for few-shot ICL from the original paper and use a zero-shot setting. We align this setting across all baselines in our experiments.  For multi-hop QA, we select the basic prompt. Since they tend to produce relatively concise outputs, we provide brief guidance on the output style."
        ]
    },
    "id_table_5": {
        "caption": "(a)  Results of ASQA.",
        "table": "A4.T1.st1.1",
        "footnotes": [],
        "references": [
            "Table  A5  shows a question-answering example from HotpotQA. We present the specific reasoning process of Adaptive-Note and compare its answers with those of other baselines. In this example, we observe that other methods fail to answer the question because they do not uncover the number  5024 . In contrast, the Adaptive-Note can explore the question more deeply and adaptively decide whether further retrieval is needed to update the note from the view of knowledge growth. Moreover, the note updating process remains stable, meaning that the incorporation of new information does not lead to the loss of existing valuable information or cause confusion."
        ]
    },
    "id_table_6": {
        "caption": "(b)  Results of CRUD.",
        "table": "A4.T1.st2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_7": {
        "caption": "Table A1:  Results (%) of an in-depth comparison under a fair top- k k k italic_k  across long-form QA datasets.",
        "table": "A4.T2.st1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_8": {
        "caption": "(a)  Results of HotpotQA.",
        "table": "A4.T2.st2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_9": {
        "caption": "(b)  Results of 2WikiMQA.",
        "table": "A4.T2.st3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_10": {
        "caption": "(c)  Results of MuSiQue.",
        "table": "A4.T3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "Table A2:  Results (%) of an in-depth comparison under a fair top- k k k italic_k  across multi-hop QA datasets.",
        "table": "A4.T4.1",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "Table A3:  All prompts of IIC and AMR.",
        "table": "A4.T5.3",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": []
}