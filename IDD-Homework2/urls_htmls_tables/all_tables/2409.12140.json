{
    "id_table_1": {
        "caption": "",
        "table": "S1.T1.1.1",
        "footnotes": [],
        "references": [
            "The key component of the MoRAG framework is the retrieval of diverse and semantically aligned motion samples from the database based on the given input text query. Existing text-to-motion retrieval methods  [ 23 ,  33 ,  5 ]  typically retrieve full-body motion samples directly from the database. However, these approaches overlook the fact that actions are frequently characterized by localized dynamics, often involving only small subsets of joint groups, such as the hands (e.g., eating) or legs (e.g., sitting).  [ 31 ,  14 ]  This results in two significant issues: (i) limited generalizability and (ii) lack of diversity in the retrieved samples. (See Table  1  and Fig.  6  (a)) This is due to the limited availability of text-motion annotated datasets. Although BABEL  [ 24 ]  and HumanML3D  [ 12 ]  provide detailed text annotations for the large-scale motion capture collection AMASS  [ 21 ] , they are still insufficient to generalize across the language space. However, the AMASS dataset contains extensive low-level body parts information that holds the potential to generalize across a significantly broader language space.",
            "First, we describe the dataset (Sec.  4.1 ) and implementation details (Sec. 4.2 ) used in our experiments. Following that, we analyze the effectiveness of the MoRAG framework, comparing it to previous research works by providing an analysis of both retrieval and generation results. (Sec. 4.3 )."
        ]
    },
    "id_table_2": {
        "caption": "",
        "table": "S3.SS3.53.53.53",
        "footnotes": [],
        "references": [
            "Based on the RAG concept, ReMoDiffuse [ 37 ] , adopts a hybrid retrieval approach using motion length and CLIP [ 26 ] -based text-to-text similarity, which does not incorporate any motion-specific information, which can result in inaccurate retrievals (Fig.  2 ) and motion generation. (Fig.  6  (b))",
            "Fig.  3  illustrates our multi-fusion retrieval-augmented human motion generation framework,  MoRAG , which aims to enhance the performance of diffusion based motion generation model by leveraging additional motion information constructed using part-specific motion retrieval. Given an input text description  text , we generate  N N N italic_N  diverse, semantically coherent human motion sequences  { H 1 , ... , H n , ... , H N } superscript H 1 ... superscript H n ... superscript H N \\{H^{1},\\dots,H^{n},\\dots,H^{N}\\} { italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , ... , italic_H start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , ... , italic_H start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT } . We prompt the input text description to an LLM to generate motion descriptions specific to the Torso, Hands, and Legs (Sec. 3.2 ). These descriptions are used for the retrieval of part-specific full-body motion sequences from pre-computed part-specific motion databases (Sec. 3.3 ). The retrieved motion sequences are then fused to construct full-body motion sequences, which serve as additional knowledge for the diffusion model (Sec. 3.4 ). This methodology enhances the models ability to effectively handle both typical and complex/unseen input conditions (Sec. 3.5 ).",
            "First, we describe the dataset (Sec.  4.1 ) and implementation details (Sec. 4.2 ) used in our experiments. Following that, we analyze the effectiveness of the MoRAG framework, comparing it to previous research works by providing an analysis of both retrieval and generation results. (Sec. 4.3 ).",
            "Table.  2  summarizes the results of using our framework in comparison with the existing diffusion-based motion generation models. Incorporating part-specific motion retrieval models as additional knowledge in the motion generation pipeline shows an improvement over Diversity, Multi-Modal Distance, and MultiModality metrics. As observed in MUGL [ 15 ] , quality scores such as FID based on feature representations often fail to capture the key action dynamics of the motion sequences. We empirically observed that these scores correlate poorly with the visual quality of motion generations."
        ]
    },
    "global_footnotes": []
}