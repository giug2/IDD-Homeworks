{
    "id_table_1": {
        "caption": "Table 1:  Comparison with SOTA dataset distillation baselines on CIFAR-10/100. Unless otherwise specified, we use the same network architecture for distillation and validation. Following the settings in their original paper, DC  [ 48 ] , DM  [ 47 ] , CAFE  [ 37 ] , MTT  [ 1 ] , and TESLA  [ 2 ]  use ConvNet-128 ( small model ). For SRe2L  [ 41 ] , ResNet-18 ( large model ) is used for synthesis and validation.",
        "table": "Ax1.EGx1",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Individually synthesizing each data instance can efficiently parallelize optimization tasks, thereby flexibly managing GPU memory usage and computational overhead. However, this approach may present challenges in ensuring the representativeness and diversity of each instance. If each instance is synthesized in isolation, there may be a risk of missing the holistic view of the data characteristics, which is crucial for the training of generalized neural networks. Intuitively,  SRe2L  might expect that random initialization of synthetic data would provide sufficient diversity to prevent homogeneity in the synthetic dataset. Nevertheless, our analysis, as demonstrated in  Figure 1 , reveals that this initialization contributes only marginally to diversity. Conversely, the Batch Normalization (BN) loss  [ 40 ]  in SRe2L plays the practical role in enhancing diversity of the distilled dataset.",
            "where  L BN subscript L BN {\\mathcal{L}}_{\\mathrm{BN}} caligraphic_L start_POSTSUBSCRIPT roman_BN end_POSTSUBSCRIPT  denotes the BN loss, and    \\lambda italic_  is the coefficient of  L BN subscript L BN {\\mathcal{L}}_{\\mathrm{BN}} caligraphic_L start_POSTSUBSCRIPT roman_BN end_POSTSUBSCRIPT . The detailed definition of  L BN subscript L BN {\\mathcal{L}}_{\\mathrm{BN}} caligraphic_L start_POSTSUBSCRIPT roman_BN end_POSTSUBSCRIPT  can be found in  subsection 3.1 . Minimizing the BN loss  L BN subscript L BN {\\mathcal{L}}_{\\mathrm{BN}} caligraphic_L start_POSTSUBSCRIPT roman_BN end_POSTSUBSCRIPT  significantly enhances the performance of  SRe2L , which is designed to ensure that  S S {\\mathcal{S}} caligraphic_S  aligns with the same normalization distribution as  T T {\\mathcal{T}} caligraphic_T . However, we argue that another essential but overlooked aspect of the BN loss  L BN subscript L BN {\\mathcal{L}}_{\\mathrm{BN}} caligraphic_L start_POSTSUBSCRIPT roman_BN end_POSTSUBSCRIPT  is its role in introducing diversity to  S S {\\mathcal{S}} caligraphic_S , which also greatly benefits the final performance. In the following section, we will analyze this issue in greater detail.",
            "Diversity in the synthetic dataset  S S {\\mathcal{S}} caligraphic_S  is essential for effective use of the limited distillation budget. This section reveals that the BN loss, referenced in  Equation 3 , enhances  S S {\\mathcal{S}} caligraphic_S s diversity. However, the suboptimal setting of BN loss limits this diversity. To overcome this, we propose a dynamic adjustment mechanism for the weight parameters of  f  T subscript f subscript  T f_{\\theta_{{\\mathcal{T}}}} italic_f start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT , enhancing diversity during synthesis. Finally, we detail our algorithm and theoretically demonstrate its effectiveness. The pseudocode of our proposed DWA can be found in  Algorithm   1 .",
            "However, simply increasing    \\lambda italic_  contributes marginally to enhancing the diversity of  S S {\\mathcal{S}} caligraphic_S . This is because a greater    \\lambda italic_  will also emphasize the regularization term    L mean  subscript L mean \\lambda{\\mathcal{L}}_{\\mathrm{mean}} italic_ caligraphic_L start_POSTSUBSCRIPT roman_mean end_POSTSUBSCRIPT , which contradicts the emphasis on    L var  subscript L var \\lambda{\\mathcal{L}}_{\\mathrm{var}} italic_ caligraphic_L start_POSTSUBSCRIPT roman_var end_POSTSUBSCRIPT . We provide a detailed analysis in the Appendix  A.1 . As a result, we propose using a decoupled coefficient,   var subscript  var \\lambda_{\\mathrm{var}} italic_ start_POSTSUBSCRIPT roman_var end_POSTSUBSCRIPT , to enhance the diversity of  S S {\\mathcal{S}} caligraphic_S .",
            "Additionally, the synthetic data instances are optimized individually to approximate the representative data instance  X c subscript X c {\\bm{X}}_{c} bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT . However, the gaussian initialization  N  ( 0 , 1 ) N 0 1 {\\mathcal{N}}(0,1) caligraphic_N ( 0 , 1 )  in pixel space does not distribute uniformly around  X c subscript X c {\\bm{X}}_{c} bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT  in latent space, making the converged synthetic data instances to cluster in a crowed area in latent space, as dedicated in  Figure 1 . To address this, we propose initializing with real instances from  T T {\\mathcal{T}} caligraphic_T  inspired by  MTT   [ 1 ] , ensuring a uniform projection when synthesizing  S S {\\mathcal{S}} caligraphic_S .",
            "where   T subscript  T \\theta_{{\\mathcal{T}}} italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT  contains informative features of  T T {\\mathcal{T}} caligraphic_T  because it achieves minimized training loss over  T T {\\mathcal{T}} caligraphic_T . We demonstrate that     ~ ~   \\widetilde{\\Delta\\theta} over~ start_ARG roman_ italic_ end_ARG , obtained from  Equation 12 , decreases the training loss computed over  T  B T B {\\mathcal{T}}\\setminus{\\mathbb{B}} caligraphic_T  blackboard_B , which, in fact, highlights the features of  T  B T B {\\mathcal{T}}\\setminus{\\mathbb{B}} caligraphic_T  blackboard_B . By applying a first-order Taylor expansion, we obtain:",
            "where  0 0 \\mathbf{0} bold_0  is the tensor of zeros with the same dimension as   T subscript  T \\theta_{\\mathcal{T}} italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT . Substitute it back into  Equation 14 , we have:",
            "L B  ( f  T +    ~ ) subscript L B subscript f subscript  T ~   L_{{\\mathbb{B}}}(f_{\\theta_{{\\mathcal{T}}}+\\widetilde{\\Delta\\theta}}) italic_L start_POSTSUBSCRIPT blackboard_B end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT + over~ start_ARG roman_ italic_ end_ARG end_POSTSUBSCRIPT )  will clearly be greater than  L B  ( f  T ) subscript L B subscript f subscript  T L_{{\\mathbb{B}}}(f_{\\theta_{{\\mathcal{T}}}}) italic_L start_POSTSUBSCRIPT blackboard_B end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) , as indicated by  Equation 12 . Thus, we demonstrate that the directed     ~ ~   \\widetilde{\\Delta\\theta} over~ start_ARG roman_ italic_ end_ARG  results in less noise and improved performance. In summary, after resolving     ~ ~   \\widetilde{\\Delta\\theta} over~ start_ARG roman_ italic_ end_ARG  as in  Equation 12 , our proposed method synthesizes data instance  s i subscript s i {\\bm{s}}_{i} bold_italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  by solving:",
            "Solving     ~ ~   \\widetilde{\\Delta\\theta} over~ start_ARG roman_ italic_ end_ARG .  Before we conduct our experiments, we propose to use a gradient descent approach to solve     ~ ~   \\widetilde{\\Delta\\theta} over~ start_ARG roman_ italic_ end_ARG  in  Equation 12 . There are two coefficients,  K K K italic_K  and    \\rho italic_ , used in the gradient descent approach.  K K K italic_K  represents the number of steps, and    \\rho italic_  normalizes the magnitude of the directed weight adjustment. The details for solving     ~ ~   \\widetilde{\\Delta\\theta} over~ start_ARG roman_ italic_ end_ARG  can be found in Line  8  of  Algorithm   1 .",
            "Experiment Setting.  Unless otherwise specified, we default to using ResNet-18 as the backbone for distillation. For ImageNet-1K, we use the pre-trained model provided by Torchvision while for CIFAR-10/100 and Tiny-ImageNet, we modify the original architecture under the suggestion in  [ 10 ] . More detailed hyper-parameter settings can be found in  Section   A.2.1 .",
            "CIFAR-10/100.  As shown in  Table 1 , our DWA exhibits superior performance compared to conventional dataset distillation methods, particularly evident on CIFAR-100 with a larger distillation budget. For instance, our DWA yields over a 10% performance enhancement compared to MTT  [ 1 ]  with  ipc = 50 ipc 50 \\texttt{ipc}=50 ipc = 50 . Leveraging a more robust distillation backbone like ResNet-18, our approach surpasses the SOTA method SRe2L  [ 41 ]  across all considered settings. Specifically, we achieve more than 5% and 8% accuracy improvement on CIFAR-10 and CIFAR-100, respectively.",
            "Decoupled  L var subscript L var {\\mathcal{L}}_{\\mathrm{var}} caligraphic_L start_POSTSUBSCRIPT roman_var end_POSTSUBSCRIPT  coefficient.  We first test our hypothesis, as outlined in  Section   3.1 , positing that strengthening  L mean subscript L mean {\\mathcal{L}}_{\\mathrm{mean}} caligraphic_L start_POSTSUBSCRIPT roman_mean end_POSTSUBSCRIPT  conflicts with the emphasis on  L var subscript L var {\\mathcal{L}}_{\\mathrm{var}} caligraphic_L start_POSTSUBSCRIPT roman_var end_POSTSUBSCRIPT , which is critical for ensuring diversity in synthetic datasets. Therefore, we compare the synthetic dataset distilled with an emphasis on  L BN subscript L BN {\\mathcal{L}}_{\\mathrm{BN}} caligraphic_L start_POSTSUBSCRIPT roman_BN end_POSTSUBSCRIPT  (which strengthens both  L mean subscript L mean {\\mathcal{L}}_{\\mathrm{mean}} caligraphic_L start_POSTSUBSCRIPT roman_mean end_POSTSUBSCRIPT  and  L var subscript L var {\\mathcal{L}}_{\\mathrm{var}} caligraphic_L start_POSTSUBSCRIPT roman_var end_POSTSUBSCRIPT ) against one that emphasizes  L var subscript L var {\\mathcal{L}}_{\\mathrm{var}} caligraphic_L start_POSTSUBSCRIPT roman_var end_POSTSUBSCRIPT  alone. As depicted in  Figure 4 , focusing solely on  L var subscript L var {\\mathcal{L}}_{\\mathrm{var}} caligraphic_L start_POSTSUBSCRIPT roman_var end_POSTSUBSCRIPT  outperforms the combined emphasis on   BN subscript  BN \\lambda_{\\mathrm{BN}} italic_ start_POSTSUBSCRIPT roman_BN end_POSTSUBSCRIPT  in both SRe2L  [ 41 ]  and our proposed Directed Weight Adjustment (DWA). These experimental results verify our hypothesis in  Section   3.1 , indicating the optimal value of the decoupled coefficient  L var subscript L var {\\mathcal{L}}_{\\mathrm{var}} caligraphic_L start_POSTSUBSCRIPT roman_var end_POSTSUBSCRIPT  is 11.",
            "Substitute  subsection A.1  and  subsection A.1  back into  Equation 17 ,"
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Comparison with SOTA dataset distillation baselines on Tiny-ImageNet and ImageNet-1K. Unless otherwise specified, we use the same network architecture for distillation and validation. Following the settings in their original paper, MTT  [ 1 ] , and TESLA  [ 2 ]  use ConvNet-128 ( small model ). For SRe2L  [ 41 ] , ResNet-18 ( large model ) is used for synthesis, and the distilled dataset is evaluated on ResNet-18, 50, and 101.    {\\dagger}   indicates MTT is performed on a 10-class subset of the full ImageNet-1K dataset.",
        "table": "Ax1.EGx2",
        "footnotes": [
            "",
            "",
            ""
        ],
        "references": [
            "In the previous section, we highlighted the often overlooked aspect of the BN loss in introducing diversity to  S S {\\mathcal{S}} caligraphic_S , which was also verified through experiments in  Section   4.2 . Building upon this, we propose to introduce randomness into   T subscript  T \\theta_{\\mathcal{T}} italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT  to further enhance  S S {\\mathcal{S}} caligraphic_S s diversity, as it is the only remaining factor affecting  Var  ( S ) Var S \\mathrm{Var}({\\mathcal{S}}) roman_Var ( caligraphic_S ) , as shown in  Equation 7 .",
            "Intuitively,    x  x \\Delta{\\bm{x}} roman_ bold_italic_x  must compensate for the    subscript   \\nabla_{\\theta}  start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  incurred by introducing the random perturbation      N  ( 0 ,   2 ) similar-to   N 0 subscript superscript  2  \\Delta\\theta\\sim{\\mathcal{N}}(0,\\sigma^{2}_{\\theta}) roman_ italic_  caligraphic_N ( 0 , italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT )  on   T subscript  T \\theta_{\\mathcal{T}} italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT . By  subsection 3.2 ,  Var  (   x )  Var  (    ) =   2 proportional-to Var  x Var   subscript superscript  2  \\mathrm{Var}(\\Delta{\\bm{x}})\\propto\\mathrm{Var}(\\Delta\\theta)=\\sigma^{2}_{\\theta} roman_Var ( roman_ bold_italic_x )  roman_Var ( roman_ italic_ ) = italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT , then:",
            "where    \\beta italic_  is determined by    x [   l ( f  T , x c ) ]  1   2 l ( f  T , x c ) -\\nabla_{{\\bm{x}}}[\\nabla_{\\theta}\\ell(f_{\\theta_{{\\mathcal{T}}}},{\\bm{x}}^{c}% )]^{-1}\\nabla^{2}_{\\theta}\\ell(f_{\\theta_{{\\mathcal{T}}}},{\\bm{x}}^{c}) -  start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [  start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT roman_l ( italic_f start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT , bold_italic_x start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) ] start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT roman_l ( italic_f start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT , bold_italic_x start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) , as shown in  subsection 3.2 . Therefore, the variance of the new synthetic dataset  S  superscript S  {\\mathcal{S}}^{\\prime} caligraphic_S start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is greater than that of  S S {\\mathcal{S}} caligraphic_S  without perturbing   T subscript  T \\theta_{\\mathcal{T}} italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT .",
            "where   T subscript  T \\theta_{{\\mathcal{T}}} italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT  contains informative features of  T T {\\mathcal{T}} caligraphic_T  because it achieves minimized training loss over  T T {\\mathcal{T}} caligraphic_T . We demonstrate that     ~ ~   \\widetilde{\\Delta\\theta} over~ start_ARG roman_ italic_ end_ARG , obtained from  Equation 12 , decreases the training loss computed over  T  B T B {\\mathcal{T}}\\setminus{\\mathbb{B}} caligraphic_T  blackboard_B , which, in fact, highlights the features of  T  B T B {\\mathcal{T}}\\setminus{\\mathbb{B}} caligraphic_T  blackboard_B . By applying a first-order Taylor expansion, we obtain:",
            "L B  ( f  T +    ~ ) subscript L B subscript f subscript  T ~   L_{{\\mathbb{B}}}(f_{\\theta_{{\\mathcal{T}}}+\\widetilde{\\Delta\\theta}}) italic_L start_POSTSUBSCRIPT blackboard_B end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT + over~ start_ARG roman_ italic_ end_ARG end_POSTSUBSCRIPT )  will clearly be greater than  L B  ( f  T ) subscript L B subscript f subscript  T L_{{\\mathbb{B}}}(f_{\\theta_{{\\mathcal{T}}}}) italic_L start_POSTSUBSCRIPT blackboard_B end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) , as indicated by  Equation 12 . Thus, we demonstrate that the directed     ~ ~   \\widetilde{\\Delta\\theta} over~ start_ARG roman_ italic_ end_ARG  results in less noise and improved performance. In summary, after resolving     ~ ~   \\widetilde{\\Delta\\theta} over~ start_ARG roman_ italic_ end_ARG  as in  Equation 12 , our proposed method synthesizes data instance  s i subscript s i {\\bm{s}}_{i} bold_italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  by solving:",
            "Solving     ~ ~   \\widetilde{\\Delta\\theta} over~ start_ARG roman_ italic_ end_ARG .  Before we conduct our experiments, we propose to use a gradient descent approach to solve     ~ ~   \\widetilde{\\Delta\\theta} over~ start_ARG roman_ italic_ end_ARG  in  Equation 12 . There are two coefficients,  K K K italic_K  and    \\rho italic_ , used in the gradient descent approach.  K K K italic_K  represents the number of steps, and    \\rho italic_  normalizes the magnitude of the directed weight adjustment. The details for solving     ~ ~   \\widetilde{\\Delta\\theta} over~ start_ARG roman_ italic_ end_ARG  can be found in Line  8  of  Algorithm   1 .",
            "Experiment Setting.  Unless otherwise specified, we default to using ResNet-18 as the backbone for distillation. For ImageNet-1K, we use the pre-trained model provided by Torchvision while for CIFAR-10/100 and Tiny-ImageNet, we modify the original architecture under the suggestion in  [ 10 ] . More detailed hyper-parameter settings can be found in  Section   A.2.1 .",
            "Tiny-ImageNet & ImageNet-1K : Compared with CIFAR-10/100, ImageNet datasets are more closely reflective of real-world scenarios.  Table 2  lists the related results. Due to the limited scalability capacity of conventional distillation paradigm, only a few methods have conducted evaluation on ImageNet datasets. Here we provide a comprehensive comparison with SRe2L  [ 41 ] , which has been validated as the most effective one for distilling large-scale dataset. It is obvious that our method significantly outperforms SRe2L on all  ipc  settings and validation models. For instance, our DWA surpasses SRe2L by 16.6% when  ipc = 10 ipc 10 \\texttt{ipc}=10 ipc = 10  on ImageNet-1K using ResNet-18.  Figure 2  further provides the visualization results, the enhanced diversity is the key driver behind the substantial performance improvement.",
            "We also employ the normalized feature distance as a metric to comprehensively evaluate our emphasis. This metric measures the mutual feature distances between instances, as defined in  Section   A.2.2 . By randomly selecting 10 classes from CIFAR-100, we calculate the normalized feature distances between synthetic datasets emphasized by the decoupled  L var subscript L var {\\mathcal{L}}_{\\mathrm{var}} caligraphic_L start_POSTSUBSCRIPT roman_var end_POSTSUBSCRIPT  and the coupled  L BN subscript L BN {\\mathcal{L}}_{\\mathrm{BN}} caligraphic_L start_POSTSUBSCRIPT roman_BN end_POSTSUBSCRIPT . The findings, illustrated in  Figure 3 , validate our hypothesis from a different perspective.",
            "Visualization.  We visualized the synthetic dataset for the goldfish class in the ImageNet-1K  [ 3 ]  dataset, as shown in  Figure 2 . The synthetic data instances generated by SRe2L  [ 41 ]  tend to capture similar features, resulting in a lack of diversity. In contrast, our DWA method produces a more varied and diverse synthetic dataset."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  An ablation study of DWA was conducted using various network architectures. The synthetic dataset was distilled by ResNet-18 from the CIFAR-100 dataset. We use  to denote the distilled dataset without weight adjustment,    \\bigcirc   to denote the distilled dataset with random weight adjustment, and  to represent Directed Weight Adjustment (DWA).",
        "table": "Ax1.EGx3",
        "footnotes": [],
        "references": [
            "where  L BN subscript L BN {\\mathcal{L}}_{\\mathrm{BN}} caligraphic_L start_POSTSUBSCRIPT roman_BN end_POSTSUBSCRIPT  denotes the BN loss, and    \\lambda italic_  is the coefficient of  L BN subscript L BN {\\mathcal{L}}_{\\mathrm{BN}} caligraphic_L start_POSTSUBSCRIPT roman_BN end_POSTSUBSCRIPT . The detailed definition of  L BN subscript L BN {\\mathcal{L}}_{\\mathrm{BN}} caligraphic_L start_POSTSUBSCRIPT roman_BN end_POSTSUBSCRIPT  can be found in  subsection 3.1 . Minimizing the BN loss  L BN subscript L BN {\\mathcal{L}}_{\\mathrm{BN}} caligraphic_L start_POSTSUBSCRIPT roman_BN end_POSTSUBSCRIPT  significantly enhances the performance of  SRe2L , which is designed to ensure that  S S {\\mathcal{S}} caligraphic_S  aligns with the same normalization distribution as  T T {\\mathcal{T}} caligraphic_T . However, we argue that another essential but overlooked aspect of the BN loss  L BN subscript L BN {\\mathcal{L}}_{\\mathrm{BN}} caligraphic_L start_POSTSUBSCRIPT roman_BN end_POSTSUBSCRIPT  is its role in introducing diversity to  S S {\\mathcal{S}} caligraphic_S , which also greatly benefits the final performance. In the following section, we will analyze this issue in greater detail.",
            "Diversity in the synthetic dataset  S S {\\mathcal{S}} caligraphic_S  is essential for effective use of the limited distillation budget. This section reveals that the BN loss, referenced in  Equation 3 , enhances  S S {\\mathcal{S}} caligraphic_S s diversity. However, the suboptimal setting of BN loss limits this diversity. To overcome this, we propose a dynamic adjustment mechanism for the weight parameters of  f  T subscript f subscript  T f_{\\theta_{{\\mathcal{T}}}} italic_f start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT , enhancing diversity during synthesis. Finally, we detail our algorithm and theoretically demonstrate its effectiveness. The pseudocode of our proposed DWA can be found in  Algorithm   1 .",
            "where  X c subscript X c {\\bm{X}}_{c} bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT  can be regarded as an optimal solution to  Equation 3  when the variance regularization term  L var subscript L var {\\mathcal{L}}_{\\mathrm{var}} caligraphic_L start_POSTSUBSCRIPT roman_var end_POSTSUBSCRIPT  is not considered,  i.e. ,",
            "Let  x c  = X c  (   L mean ,  T ) subscript superscript x c subscript X c  subscript L mean subscript  T {\\bm{x}}^{*}_{c}={\\bm{X}}_{c}(\\lambda{\\mathcal{L}}_{\\mathrm{mean}},\\theta_{{% \\mathcal{T}}}) bold_italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT = bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_ caligraphic_L start_POSTSUBSCRIPT roman_mean end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT )  to be the original optimal solution to  Equation 3 . We aim to solve the adjusted optimal solution  x c = X c  (   L mean ,  T +    ) = x c  +   x subscript x c subscript X c  subscript L mean subscript  T   subscript superscript x c  x {\\bm{x}}_{c}={\\bm{X}}_{c}(\\lambda{\\mathcal{L}}_{\\mathrm{mean}},\\theta_{{% \\mathcal{T}}}+\\Delta\\theta)={\\bm{x}}^{*}_{c}+\\Delta{\\bm{x}} bold_italic_x start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT = bold_italic_X start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_ caligraphic_L start_POSTSUBSCRIPT roman_mean end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT + roman_ italic_ ) = bold_italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT + roman_ bold_italic_x , where   T subscript  T \\theta_{\\mathcal{T}} italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT  is randomly perturbed by       \\Delta\\theta roman_ italic_ , and      N  ( 0 ,   2 ) similar-to   N 0 subscript superscript  2  \\Delta\\theta\\sim{\\mathcal{N}}(0,\\sigma^{2}_{\\theta}) roman_ italic_  caligraphic_N ( 0 , italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ) . Consequently, we have:",
            "Intuitively,    x  x \\Delta{\\bm{x}} roman_ bold_italic_x  must compensate for the    subscript   \\nabla_{\\theta}  start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  incurred by introducing the random perturbation      N  ( 0 ,   2 ) similar-to   N 0 subscript superscript  2  \\Delta\\theta\\sim{\\mathcal{N}}(0,\\sigma^{2}_{\\theta}) roman_ italic_  caligraphic_N ( 0 , italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT )  on   T subscript  T \\theta_{\\mathcal{T}} italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT . By  subsection 3.2 ,  Var  (   x )  Var  (    ) =   2 proportional-to Var  x Var   subscript superscript  2  \\mathrm{Var}(\\Delta{\\bm{x}})\\propto\\mathrm{Var}(\\Delta\\theta)=\\sigma^{2}_{\\theta} roman_Var ( roman_ bold_italic_x )  roman_Var ( roman_ italic_ ) = italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT , then:",
            "where    \\beta italic_  is determined by    x [   l ( f  T , x c ) ]  1   2 l ( f  T , x c ) -\\nabla_{{\\bm{x}}}[\\nabla_{\\theta}\\ell(f_{\\theta_{{\\mathcal{T}}}},{\\bm{x}}^{c}% )]^{-1}\\nabla^{2}_{\\theta}\\ell(f_{\\theta_{{\\mathcal{T}}}},{\\bm{x}}^{c}) -  start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [  start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT roman_l ( italic_f start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT , bold_italic_x start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) ] start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT roman_l ( italic_f start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT , bold_italic_x start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) , as shown in  subsection 3.2 . Therefore, the variance of the new synthetic dataset  S  superscript S  {\\mathcal{S}}^{\\prime} caligraphic_S start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is greater than that of  S S {\\mathcal{S}} caligraphic_S  without perturbing   T subscript  T \\theta_{\\mathcal{T}} italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT .",
            "Decoupled  L var subscript L var {\\mathcal{L}}_{\\mathrm{var}} caligraphic_L start_POSTSUBSCRIPT roman_var end_POSTSUBSCRIPT  coefficient.  We first test our hypothesis, as outlined in  Section   3.1 , positing that strengthening  L mean subscript L mean {\\mathcal{L}}_{\\mathrm{mean}} caligraphic_L start_POSTSUBSCRIPT roman_mean end_POSTSUBSCRIPT  conflicts with the emphasis on  L var subscript L var {\\mathcal{L}}_{\\mathrm{var}} caligraphic_L start_POSTSUBSCRIPT roman_var end_POSTSUBSCRIPT , which is critical for ensuring diversity in synthetic datasets. Therefore, we compare the synthetic dataset distilled with an emphasis on  L BN subscript L BN {\\mathcal{L}}_{\\mathrm{BN}} caligraphic_L start_POSTSUBSCRIPT roman_BN end_POSTSUBSCRIPT  (which strengthens both  L mean subscript L mean {\\mathcal{L}}_{\\mathrm{mean}} caligraphic_L start_POSTSUBSCRIPT roman_mean end_POSTSUBSCRIPT  and  L var subscript L var {\\mathcal{L}}_{\\mathrm{var}} caligraphic_L start_POSTSUBSCRIPT roman_var end_POSTSUBSCRIPT ) against one that emphasizes  L var subscript L var {\\mathcal{L}}_{\\mathrm{var}} caligraphic_L start_POSTSUBSCRIPT roman_var end_POSTSUBSCRIPT  alone. As depicted in  Figure 4 , focusing solely on  L var subscript L var {\\mathcal{L}}_{\\mathrm{var}} caligraphic_L start_POSTSUBSCRIPT roman_var end_POSTSUBSCRIPT  outperforms the combined emphasis on   BN subscript  BN \\lambda_{\\mathrm{BN}} italic_ start_POSTSUBSCRIPT roman_BN end_POSTSUBSCRIPT  in both SRe2L  [ 41 ]  and our proposed Directed Weight Adjustment (DWA). These experimental results verify our hypothesis in  Section   3.1 , indicating the optimal value of the decoupled coefficient  L var subscript L var {\\mathcal{L}}_{\\mathrm{var}} caligraphic_L start_POSTSUBSCRIPT roman_var end_POSTSUBSCRIPT  is 11.",
            "We also employ the normalized feature distance as a metric to comprehensively evaluate our emphasis. This metric measures the mutual feature distances between instances, as defined in  Section   A.2.2 . By randomly selecting 10 classes from CIFAR-100, we calculate the normalized feature distances between synthetic datasets emphasized by the decoupled  L var subscript L var {\\mathcal{L}}_{\\mathrm{var}} caligraphic_L start_POSTSUBSCRIPT roman_var end_POSTSUBSCRIPT  and the coupled  L BN subscript L BN {\\mathcal{L}}_{\\mathrm{BN}} caligraphic_L start_POSTSUBSCRIPT roman_BN end_POSTSUBSCRIPT . The findings, illustrated in  Figure 3 , validate our hypothesis from a different perspective.",
            "Directed weight adjustment.  We clarify the necessity of restricting the direction of weight adjustment in  Section   3.3 . To test its effectiveness, we apply a random       \\Delta\\theta roman_ italic_ , sampled from a Gaussian Distribution, to   T subscript  T \\theta_{\\mathcal{T}} italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT . As shown in  Table 3 , we assess synthetic datasets derived from three scenarios: no weight adjustment, random weight adjustment, and our directed weight adjustment (DWA) method, using the CIFAR-100 dataset. The results, examined across various architectures, underscore the importance of directing weight adjustments in distillation processes. Notably, we observe performance degradation in the synthetic dataset optimized with random weight adjustment at  ipc = 10  compared to those without weight adjustment. This decline is primarily due to the minimal performance gains achieved by introducing diversity in scenarios with higher  ipc . This issue becomes significant as more synthetic instances are individually optimized, where the noise caused by undirected weight adjustments becomes the primary factor in degrading overall performance.",
            "In  Figure 3 , we use feature distance  D f  e  a subscript D f e a {\\mathcal{D}}_{fea} caligraphic_D start_POSTSUBSCRIPT italic_f italic_e italic_a end_POSTSUBSCRIPT  to measure then diversity of distilled dataset. The following is how the class-wise feature distance is calculated,"
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Cross architecture performance of distilled dataset of CIFAR-100 using ResNet-18 and ConvNet-128.",
        "table": "Ax1.EGx4",
        "footnotes": [],
        "references": [
            "In the previous section, we highlighted the often overlooked aspect of the BN loss in introducing diversity to  S S {\\mathcal{S}} caligraphic_S , which was also verified through experiments in  Section   4.2 . Building upon this, we propose to introduce randomness into   T subscript  T \\theta_{\\mathcal{T}} italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT  to further enhance  S S {\\mathcal{S}} caligraphic_S s diversity, as it is the only remaining factor affecting  Var  ( S ) Var S \\mathrm{Var}({\\mathcal{S}}) roman_Var ( caligraphic_S ) , as shown in  Equation 7 .",
            "where  0 0 \\mathbf{0} bold_0  is the tensor of zeros with the same dimension as   T subscript  T \\theta_{\\mathcal{T}} italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT . Substitute it back into  Equation 14 , we have:",
            "Decoupled  L var subscript L var {\\mathcal{L}}_{\\mathrm{var}} caligraphic_L start_POSTSUBSCRIPT roman_var end_POSTSUBSCRIPT  coefficient.  We first test our hypothesis, as outlined in  Section   3.1 , positing that strengthening  L mean subscript L mean {\\mathcal{L}}_{\\mathrm{mean}} caligraphic_L start_POSTSUBSCRIPT roman_mean end_POSTSUBSCRIPT  conflicts with the emphasis on  L var subscript L var {\\mathcal{L}}_{\\mathrm{var}} caligraphic_L start_POSTSUBSCRIPT roman_var end_POSTSUBSCRIPT , which is critical for ensuring diversity in synthetic datasets. Therefore, we compare the synthetic dataset distilled with an emphasis on  L BN subscript L BN {\\mathcal{L}}_{\\mathrm{BN}} caligraphic_L start_POSTSUBSCRIPT roman_BN end_POSTSUBSCRIPT  (which strengthens both  L mean subscript L mean {\\mathcal{L}}_{\\mathrm{mean}} caligraphic_L start_POSTSUBSCRIPT roman_mean end_POSTSUBSCRIPT  and  L var subscript L var {\\mathcal{L}}_{\\mathrm{var}} caligraphic_L start_POSTSUBSCRIPT roman_var end_POSTSUBSCRIPT ) against one that emphasizes  L var subscript L var {\\mathcal{L}}_{\\mathrm{var}} caligraphic_L start_POSTSUBSCRIPT roman_var end_POSTSUBSCRIPT  alone. As depicted in  Figure 4 , focusing solely on  L var subscript L var {\\mathcal{L}}_{\\mathrm{var}} caligraphic_L start_POSTSUBSCRIPT roman_var end_POSTSUBSCRIPT  outperforms the combined emphasis on   BN subscript  BN \\lambda_{\\mathrm{BN}} italic_ start_POSTSUBSCRIPT roman_BN end_POSTSUBSCRIPT  in both SRe2L  [ 41 ]  and our proposed Directed Weight Adjustment (DWA). These experimental results verify our hypothesis in  Section   3.1 , indicating the optimal value of the decoupled coefficient  L var subscript L var {\\mathcal{L}}_{\\mathrm{var}} caligraphic_L start_POSTSUBSCRIPT roman_var end_POSTSUBSCRIPT  is 11.",
            "Cross-Architecture Generalization.  The generalizability across different architectures is a key feature for assessing the effectiveness of the distilled dataset. In this section, we evaluate the surrogate dataset condensed by different backbones (ResNet-18 and ConvNet-128) on various architectures including MobileNetV2  [ 30 ] , ShuffleNetV2  [ 24 ] , EfficientNet-B0  [ 34 ] , and VGGNet-16  [ 32 ] . The experimental results are reported in  Table 5  and  Table 4 . It is evident that our DWA synthetized dataset can effectively generalize across various architectures. Notably, on ShuffleNetV2, EfficientNet-B0, and ConvNet-128, three unknown architectures during the data synthesis phase, our method demonstrates prominence classification performance with the accuracy 41.7%, 40.7%, and 37.0%, outperforming the latest SOTA SRe2L  [ 41 ]  by a margin of 14.2%, 15.8%, and 17.6%, respectively."
        ]
    },
    "id_table_5": {
        "caption": "Table 6:  Hyper-parameter settings for CIFAR-10/100.",
        "table": "Ax1.EGx5",
        "footnotes": [],
        "references": [
            "Synthesizing  S S {\\mathcal{S}} caligraphic_S .  A series of previous works mentioned in  Section   5  have introduced various methods to synthesize  S S {\\mathcal{S}} caligraphic_S . Specifically,  SRe2L   [ 41 ]  proposes an efficient and effective synthesizing method, which optimizes each synthetic instance  s i subscript s i {\\bm{s}}_{i} bold_italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  by solving the following minimization problem 1 1 1 In the actual optimization process, operations occur within the pixel space using the entire network  h  T subscript h subscript  T h_{\\theta_{{\\mathcal{T}}}} italic_h start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT . However, as we discuss the optimization in the latent space, we only consider solutions within this space. Thus, we transform the solution in latent space back into pixel space as  s ~ = g  T  1  ( s ) ~ s superscript subscript g subscript  T 1 s \\tilde{{\\bm{s}}}=g_{\\theta_{{\\mathcal{T}}}}^{-1}({\\bm{s}}) over~ start_ARG bold_italic_s end_ARG = italic_g start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_s ) . :",
            "Parameters Study on  K K K italic_K  and    \\rho italic_ .  Apart from direction, the number of steps  K K K italic_K  and magnitude    \\rho italic_  of perturbation also influence the distillation process.  Figure 5  illustrates the grid search for these two hyper-parameters and demonstrates the positive impact of perturbation, which is achieved effortlessly, requiring no meticulous manual parameter tuning. In our experiments, we set  K = 12 K 12 K=12 italic_K = 12  and   = 15  e  3  15 superscript e 3 \\rho=15e^{-3} italic_ = 15 italic_e start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT  for all the datasets. Readers can adjust these hyper-parameters according to their specific circumstances (different datasets and networks) to obtain better results.",
            "Cross-Architecture Generalization.  The generalizability across different architectures is a key feature for assessing the effectiveness of the distilled dataset. In this section, we evaluate the surrogate dataset condensed by different backbones (ResNet-18 and ConvNet-128) on various architectures including MobileNetV2  [ 30 ] , ShuffleNetV2  [ 24 ] , EfficientNet-B0  [ 34 ] , and VGGNet-16  [ 32 ] . The experimental results are reported in  Table 5  and  Table 4 . It is evident that our DWA synthetized dataset can effectively generalize across various architectures. Notably, on ShuffleNetV2, EfficientNet-B0, and ConvNet-128, three unknown architectures during the data synthesis phase, our method demonstrates prominence classification performance with the accuracy 41.7%, 40.7%, and 37.0%, outperforming the latest SOTA SRe2L  [ 41 ]  by a margin of 14.2%, 15.8%, and 17.6%, respectively."
        ]
    },
    "id_table_6": {
        "caption": "Table 7:  Hyper-parameter settings for Tiny-ImageNet.",
        "table": "Ax1.EGx6",
        "footnotes": [],
        "references": [
            "Table 6 ,  Table 7 , and  Table 8  list the hyper-parameter settings of our method on experimental datasets. We maintain consistency with SRe2L for a fair comparison."
        ]
    },
    "id_table_7": {
        "caption": "Table 8:  Hyper-parameter settings for ImageNet-1K.",
        "table": "Ax1.EGx7",
        "footnotes": [],
        "references": [
            "In the previous section, we highlighted the often overlooked aspect of the BN loss in introducing diversity to  S S {\\mathcal{S}} caligraphic_S , which was also verified through experiments in  Section   4.2 . Building upon this, we propose to introduce randomness into   T subscript  T \\theta_{\\mathcal{T}} italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT  to further enhance  S S {\\mathcal{S}} caligraphic_S s diversity, as it is the only remaining factor affecting  Var  ( S ) Var S \\mathrm{Var}({\\mathcal{S}}) roman_Var ( caligraphic_S ) , as shown in  Equation 7 .",
            "Substitute  subsection A.1  and  subsection A.1  back into  Equation 17 ,",
            "Table 6 ,  Table 7 , and  Table 8  list the hyper-parameter settings of our method on experimental datasets. We maintain consistency with SRe2L for a fair comparison."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "Ax1.EGx8",
        "footnotes": [],
        "references": [
            "To satisfy  Equation 8 , we have:",
            "Solving     ~ ~   \\widetilde{\\Delta\\theta} over~ start_ARG roman_ italic_ end_ARG .  Before we conduct our experiments, we propose to use a gradient descent approach to solve     ~ ~   \\widetilde{\\Delta\\theta} over~ start_ARG roman_ italic_ end_ARG  in  Equation 12 . There are two coefficients,  K K K italic_K  and    \\rho italic_ , used in the gradient descent approach.  K K K italic_K  represents the number of steps, and    \\rho italic_  normalizes the magnitude of the directed weight adjustment. The details for solving     ~ ~   \\widetilde{\\Delta\\theta} over~ start_ARG roman_ italic_ end_ARG  can be found in Line  8  of  Algorithm   1 .",
            "Table 6 ,  Table 7 , and  Table 8  list the hyper-parameter settings of our method on experimental datasets. We maintain consistency with SRe2L for a fair comparison."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "Ax1.EGx9",
        "footnotes": [],
        "references": []
    },
    "id_table_10": {
        "caption": "",
        "table": "S4.T1.34",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": []
    },
    "id_table_11": {
        "caption": "",
        "table": "S4.T2.41",
        "footnotes": [
            "",
            "",
            "",
            ""
        ],
        "references": []
    },
    "id_table_12": {
        "caption": "",
        "table": "S4.T3.36",
        "footnotes": [],
        "references": [
            "where   T subscript  T \\theta_{{\\mathcal{T}}} italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT  contains informative features of  T T {\\mathcal{T}} caligraphic_T  because it achieves minimized training loss over  T T {\\mathcal{T}} caligraphic_T . We demonstrate that     ~ ~   \\widetilde{\\Delta\\theta} over~ start_ARG roman_ italic_ end_ARG , obtained from  Equation 12 , decreases the training loss computed over  T  B T B {\\mathcal{T}}\\setminus{\\mathbb{B}} caligraphic_T  blackboard_B , which, in fact, highlights the features of  T  B T B {\\mathcal{T}}\\setminus{\\mathbb{B}} caligraphic_T  blackboard_B . By applying a first-order Taylor expansion, we obtain:",
            "L B  ( f  T +    ~ ) subscript L B subscript f subscript  T ~   L_{{\\mathbb{B}}}(f_{\\theta_{{\\mathcal{T}}}+\\widetilde{\\Delta\\theta}}) italic_L start_POSTSUBSCRIPT blackboard_B end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT + over~ start_ARG roman_ italic_ end_ARG end_POSTSUBSCRIPT )  will clearly be greater than  L B  ( f  T ) subscript L B subscript f subscript  T L_{{\\mathbb{B}}}(f_{\\theta_{{\\mathcal{T}}}}) italic_L start_POSTSUBSCRIPT blackboard_B end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) , as indicated by  Equation 12 . Thus, we demonstrate that the directed     ~ ~   \\widetilde{\\Delta\\theta} over~ start_ARG roman_ italic_ end_ARG  results in less noise and improved performance. In summary, after resolving     ~ ~   \\widetilde{\\Delta\\theta} over~ start_ARG roman_ italic_ end_ARG  as in  Equation 12 , our proposed method synthesizes data instance  s i subscript s i {\\bm{s}}_{i} bold_italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  by solving:",
            "Solving     ~ ~   \\widetilde{\\Delta\\theta} over~ start_ARG roman_ italic_ end_ARG .  Before we conduct our experiments, we propose to use a gradient descent approach to solve     ~ ~   \\widetilde{\\Delta\\theta} over~ start_ARG roman_ italic_ end_ARG  in  Equation 12 . There are two coefficients,  K K K italic_K  and    \\rho italic_ , used in the gradient descent approach.  K K K italic_K  represents the number of steps, and    \\rho italic_  normalizes the magnitude of the directed weight adjustment. The details for solving     ~ ~   \\widetilde{\\Delta\\theta} over~ start_ARG roman_ italic_ end_ARG  can be found in Line  8  of  Algorithm   1 ."
        ]
    },
    "id_table_13": {
        "caption": "",
        "table": "S4.T4.47",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "S5.T5.13",
        "footnotes": [],
        "references": [
            "where  0 0 \\mathbf{0} bold_0  is the tensor of zeros with the same dimension as   T subscript  T \\theta_{\\mathcal{T}} italic_ start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT . Substitute it back into  Equation 14 , we have:"
        ]
    },
    "id_table_15": {
        "caption": "",
        "table": "Ax1.EGx10",
        "footnotes": [],
        "references": []
    },
    "id_table_16": {
        "caption": "",
        "table": "Ax1.EGx11",
        "footnotes": [],
        "references": []
    },
    "id_table_17": {
        "caption": "",
        "table": "Ax1.EGx12",
        "footnotes": [],
        "references": [
            "Substitute  subsection A.1  and  subsection A.1  back into  Equation 17 ,"
        ]
    },
    "id_table_18": {
        "caption": "",
        "table": "A1.T6.4",
        "footnotes": [],
        "references": []
    },
    "id_table_19": {
        "caption": "",
        "table": "A1.T6.4.9.5.4.1",
        "footnotes": [],
        "references": []
    },
    "id_table_20": {
        "caption": "",
        "table": "A1.T7.4",
        "footnotes": [],
        "references": []
    },
    "id_table_21": {
        "caption": "",
        "table": "A1.T7.4.9.5.2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_22": {
        "caption": "",
        "table": "A1.T7.4.9.5.4.1",
        "footnotes": [],
        "references": []
    },
    "id_table_23": {
        "caption": "",
        "table": "A1.T8.4",
        "footnotes": [],
        "references": []
    },
    "id_table_24": {
        "caption": "",
        "table": "A1.T8.4.9.5.2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_25": {
        "caption": "",
        "table": "A1.T8.4.9.5.4.1",
        "footnotes": [],
        "references": []
    }
}