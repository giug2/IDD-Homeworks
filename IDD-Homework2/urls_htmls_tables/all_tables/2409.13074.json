{
    "id_table_1": {
        "caption": "",
        "table": "A1.EGx1",
        "footnotes": [],
        "references": [
            "There is however one fundamental snag in the reasoning above. The aforementioned ODE or SDE involves the score function at  different noise levels , i.e. we would need access to   log  p t z , w  subscript superscript p z w t \\nabla\\log p^{z,w}_{t}  roman_log italic_p start_POSTSUPERSCRIPT italic_z , italic_w end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , where we use the notation  q t subscript q t q_{t} italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  to denote the distribution given by running a certain noising process (see Section  2.1 ) for time  t t t italic_t  starting from a distribution  q q q italic_q . Here  p t z , w subscript superscript p z w t p^{z,w}_{t} italic_p start_POSTSUPERSCRIPT italic_z , italic_w end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  means we tilt before adding noise, that is, we take  q = p z , w q superscript p z w q=p^{z,w} italic_q = italic_p start_POSTSUPERSCRIPT italic_z , italic_w end_POSTSUPERSCRIPT  and then apply noise to  q q q italic_q . Unfortunately, as soon as  t > 0 t 0 t>0 italic_t > 0 , the analogue of Eq. ( 1 ) no longer holds, i.e.",
            "To see clearly that diffusion guidance is not simply sampling from the tilted distribution, consider the following simple setting. Suppose that there are only two classes  z =  1 z 1 z=-1 italic_z = - 1  and  z = + 1 z 1 z=+1 italic_z = + 1 , and that the corresponding conditional distributions  p (   z =  1 ) p(\\cdot\\mid z=-1) italic_p (   italic_z = - 1 )  and  p (   z = + 1 ) p(\\cdot\\mid z=+1) italic_p (   italic_z = + 1 )  have disjoint supports    ,  + subscript  subscript  \\Omega_{-},\\Omega_{+} roman_ start_POSTSUBSCRIPT - end_POSTSUBSCRIPT , roman_ start_POSTSUBSCRIPT + end_POSTSUBSCRIPT . In this case, the conditional likelihood is simply given by  p  ( z = i  x )  1  [ x   i ] proportional-to p z conditional i x 1 delimited-[] x subscript  i p(z=i\\mid x)\\propto\\mathds{1}[x\\in\\Omega_{i}] italic_p ( italic_z = italic_i  italic_x )  blackboard_1 [ italic_x  roman_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ] . In particular, the conditional likelihood is binary-valued, which implies that for any  w > 0 w 0 w>0 italic_w > 0 , the tilted distribution  p z , w superscript p z w p^{z,w} italic_p start_POSTSUPERSCRIPT italic_z , italic_w end_POSTSUPERSCRIPT  is exactly the same! On the other hand, as Figure  1  shows, increasing  w w w italic_w  changes the distribution of generated samples to concentrate towards the edge of the support of the guided class.",
            "Consider a data distribution  p = 1 2  p ( 1 ) + 1 2  p (  1 ) p 1 2 superscript p 1 1 2 superscript p 1 p=\\frac{1}{2}p^{(1)}+\\frac{1}{2}p^{(-1)} italic_p = divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_p start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT + divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_p start_POSTSUPERSCRIPT ( - 1 ) end_POSTSUPERSCRIPT  where  p ( 1 ) , p (  1 ) superscript p 1 superscript p 1 p^{(1)},p^{(-1)} italic_p start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT , italic_p start_POSTSUPERSCRIPT ( - 1 ) end_POSTSUPERSCRIPT  are    \\beta italic_ -bounded and supported on disjoint intervals  [  1 ,  2 ] subscript  1 subscript  2 [\\alpha_{1},\\alpha_{2}] [ italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ]  and  [   2 ,   1 ] subscript  2 subscript  1 [-\\alpha_{2},-\\alpha_{1}] [ - italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , - italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ]  respectively (see Assumption  1 ). Suppose that one runs the probability flow ODE with guidance parameter  w w w italic_w  which is larger than some absolute constant. Then with probability  1  e    ( w ) 1 superscript e  w 1-e^{-\\Omega(w)} 1 - italic_e start_POSTSUPERSCRIPT - roman_ ( italic_w ) end_POSTSUPERSCRIPT , the resulting sample lies in the interval",
            "Theorems  1  and  2  illustrate that diffusion guidance results in a strong bias towards points in the support of one conditional distribution which are far from points in the support of the other.",
            "Next, we show how to leverage ideas in the proof of Theorem  1  to explain this degradation. Concretely, we give a simple example where a small perturbation to the score estimate at the tails of the data distribution is enough to take the sampling trajectory given by diffusion guidance far away from the trajectory predicted by Theorem  1 :",
            "Given  0 <  < 1 0 italic- 1 0<\\epsilon<1 0 < italic_ < 1 , assume  w   ~  ( log  log  ( 1 /  ) ) w ~  1 italic- w\\geq\\widetilde{\\Omega}(\\sqrt{\\log\\log(1/\\epsilon)}) italic_w  over~ start_ARG roman_ end_ARG ( square-root start_ARG roman_log roman_log ( 1 / italic_ ) end_ARG ) , where the hidden constant factor is sufficiently large. There exist densities  p ( 1 ) , p (  1 ) superscript p 1 superscript p 1 p^{(1)},p^{(-1)} italic_p start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT , italic_p start_POSTSUPERSCRIPT ( - 1 ) end_POSTSUPERSCRIPT  satisfying the assumptions of Theorem  1 , as well as functions  s t ( 1 ) , s t subscript superscript s 1 t subscript s t s^{(1)}_{t},s_{t} italic_s start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  satisfying",
            "In other words, for any level of score estimation error, if one takes the guidance parameter  w w w italic_w  to be too large, the sampler will end up going off the support of the data distribution  p p p italic_p . Roughly speaking, the idea derives from the proof of Theorem  1 . As we will see, one key feature of the guided ODE in the setting of Theorem  1  is that the trajectory first  swings past the edges of the support of  p p p italic_p  and into the tails of the noised data distribution  p t subscript p t p_{t} italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  before returning. As a result, errors in score estimation at these tails can move the sampling process away from the intended trajectory and thus prevent the trajectory from ever returning to the support of  p p p italic_p , leading to corrupted outputs. We stress that this phenomenon is not an issue of numerical precision: Theorem  3  applies even if one runs diffusion guidance with infinite precision.",
            "Taking inspiration from our theory, we posit that the optimal choice of guidance (from the perspective of sample quality) for compactly supported distributions that approximately satisfy the assumptions of our theory is the largest possible  w w w italic_w  for which the resulting trajectory does not exhibit this behavior of swinging away from the support of the data distribution and returning. Specifically, we propose a rule of thumb for selecting the guidance strength based on looking at a certain  monotonicity  property of the trajectory and experimentally validate this rule of thumb in both synthetic settings and on image classification datasets. Additionally, for compactly supported distributions that fall outside the scope of Theorem  1 , we propose an alternative heuristic based on the ideas of Theorem  3 : we should choose the guidance strength as large as possible while still ensuring that final samples are contained within the distribution support. See Section  6  for details.",
            "Instead of studying summary statistics of the generated output, we instead give a fine-grained analysis of where exactly the trajectory ends up at different times in the reverse process. While we do not directly study classification confidence, note that in the setting of our main result, Theorem  1 , for mixtures of compactly supported product distributions, the statement that classification confidence increases is uninformative because, as mentioned previously, the conditional likelihood for  any point  in the support of the target class is  1 1 1 1 . The dynamics that we elucidate in our results can be thought of as a more geometric notion of classification confidence. As for diversity, implicit in our Theorems  1  and  2  are quantitative bounds on how the diversity decreases as  w w w italic_w  increases.",
            "Having formalized the probability flow ODE with guidance, we now provide a high-level overview of our proofs by presenting general intuition for the effect of guidance. The behavior of the guided ODE in the setting of mixtures of compactly supported distributions is the richest, so we focus on illustrating the proof of Theorem  1 . In that setting, roughly speaking, we will show that there are three distinct regimes for the evolution of the guided ODE, depending on how the posterior probabilites  p t  ( z =  1 | x  ( t ) ) subscript p t z conditional 1 x t p_{t}(z=-1|x(t)) italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z = - 1 | italic_x ( italic_t ) )  and  p t  ( z = 1 | x  ( t ) ) subscript p t z conditional 1 x t p_{t}(z=1|x(t)) italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z = 1 | italic_x ( italic_t ) )  relate to each other.",
            "Under Assumption  1  we show that by running the guided ODE starting from Gaussian initialization, we sample from a distribution concentrated at the edge of  p ( 1 ) superscript p 1 p^{(1)} italic_p start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT . In particular, we prove the following result:",
            "Assume  ln  ( w ) 16  w   1   2  w 16 w subscript  1 subscript  2  \\frac{\\ln(w)}{16\\sqrt{w}}\\leq\\frac{\\alpha_{1}\\alpha_{2}}{\\beta} divide start_ARG roman_ln ( italic_w ) end_ARG start_ARG 16 square-root start_ARG italic_w end_ARG end_ARG  divide start_ARG italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG italic_ end_ARG ,  ln  ( w ) ln  ln  ( w )  1024  (  2  1  1 ) 2 w w 1024 superscript subscript  2 subscript  1 1 2 \\frac{\\ln(w)}{\\ln\\ln(w)}\\geq 1024\\left({\\frac{\\alpha_{2}}{\\alpha_{1}\\wedge 1}}% \\right)^{2} divide start_ARG roman_ln ( italic_w ) end_ARG start_ARG roman_ln roman_ln ( italic_w ) end_ARG  1024 ( divide start_ARG italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  1 end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . Then running the guided ODE with  T  2  log  2 T 2 2 T\\geq 2\\log 2 italic_T  2 roman_log 2  and parameter  w w w italic_w  for the mixture model  p p p italic_p  defined in ( 4 ) under Assumption  1 , with probability at least  1  exp  (  w   1 2 512 /  2 ) 1 w superscript subscript  1 2 superscript 512 2 1-\\exp(-\\frac{w\\alpha_{1}^{2}}{512^{/}2}) 1 - roman_exp ( - divide start_ARG italic_w italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 512 start_POSTSUPERSCRIPT / end_POSTSUPERSCRIPT 2 end_ARG ) , the resulting sample lies in the interval",
            "Note that combining ( 7 ) and ( 10 ), we can write",
            "We begin by sketching our argument in greater detail. First note that the second term in ( 11 ), namely",
            "Therefore, for the second term in Equation ( 11 ) we have",
            "Plugging this into Equation ( 11 ) and using the fact that  x  ( t )   1  w 2  missing / missing x t subscript  1 w 2 missing missing x(t)\\leq\\frac{\\alpha_{1}\\sqrt{w}}{2missing/missing} italic_x ( italic_t )  divide start_ARG italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT square-root start_ARG italic_w end_ARG end_ARG start_ARG 2 roman_missing / roman_missing end_ARG  and  a t  1 subscript a t 1 a_{t}\\leq 1 italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  1 , we get",
            "Next, we show that starting from  x ~  ( s 0 )     ( w ) ~ x subscript s 0  w \\tilde{x}(s_{0})\\geq-\\Theta(\\sqrt{w}) over~ start_ARG italic_x end_ARG ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )  - roman_ ( square-root start_ARG italic_w end_ARG ) , the particle  x ~  ( s ) ~ x s \\tilde{x}(s) over~ start_ARG italic_x end_ARG ( italic_s )  reaches at least    ( ln  (  ) )   \\Theta(\\ln(\\omega)) roman_ ( roman_ln ( italic_ ) )  before time 1. This results from the strong acceleration force toward the right in this phase, coming from the dominance of the aforementioned second term in ( 11 ).",
            "where recall  s ( 0 ) = e  T superscript s 0 superscript e T s^{(0)}=e^{-T} italic_s start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT = italic_e start_POSTSUPERSCRIPT - italic_T end_POSTSUPERSCRIPT  is the initial time for the ODE ( 12 ).",
            "Therefore, combining with Equation ( 19 ),",
            "where the last inequality is due to the fact that  ln  ( w )  16  c 1 2 w 16 superscript subscript c 1 2 \\ln(w)\\geq 16c_{1}^{2} roman_ln ( italic_w )  16 italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . Combining this with Equation ( 17 ), we can bound the first term in Equation ( 16 ):",
            "On the other hand, for the second term in Equation ( 16 ), we can upper bound the numerator as",
            "Combining this with Equation ( 20 ), we obtain the following upper bound for the second term in Equation ( 16 ):",
            "From Equation ( 12 ), as long as  x ~  ( s )  0 ~ x s 0 \\tilde{x}(s)\\geq 0 over~ start_ARG italic_x end_ARG ( italic_s )  0 , we have",
            "Next, we show that when the process gets close to the end, if the particle is on the right side of both of the intervals, then the first term in Equation ( 11 ) will dominate the second term and the particle is most likely to converge to some point in the interval  (  1 ,  2 ) subscript  1 subscript  2 (\\alpha_{1},\\alpha_{2}) ( italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) .",
            "On the other hand, for the first term in Equation ( 12 ), as long as  x ~  ( s )  1 2  s 0  1   2 ~ x s 1 2 subscript s 0 1 subscript  2 \\tilde{x}(s)\\geq\\frac{1}{2s_{0}-1}\\alpha_{2} over~ start_ARG italic_x end_ARG ( italic_s )  divide start_ARG 1 end_ARG start_ARG 2 italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - 1 end_ARG italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , we get",
            "First comparing Equations ( 12 ) and ( 28 ), since the second term in the RHS of the ODE in ( 12 ) is always positive, from ODE comparison theorem we get  B  ( s )  x ~  ( s 0 ) + A  ( s ) B s ~ x subscript s 0 A s B(s)\\geq\\tilde{x}(s_{0})+A(s) italic_B ( italic_s )  over~ start_ARG italic_x end_ARG ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) + italic_A ( italic_s ) .",
            "On the other hand, note that for any time  s 2  s 1  subscript s 2 subscript superscript s  1 s_{2}\\geq s^{\\prime}_{1} italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  before  x ~  ( s ) ~ x s \\tilde{x}(s) over~ start_ARG italic_x end_ARG ( italic_s )  reaches   1 +  2 2 subscript  1 subscript  2 2 \\frac{\\alpha_{1}+\\alpha_{2}}{2} divide start_ARG italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG  (if it ever reaches that value), the first term in ( 12 ) is always non-positive. Hence, we have",
            "Note that the inequality ( 31 ) is true even when  x ~  ( s 0 )  1 2  s 0  1   2 ~ x subscript s 0 1 2 subscript s 0 1 subscript  2 \\tilde{x}(s_{0})\\leq\\frac{1}{2s_{0}-1}\\alpha_{2} over~ start_ARG italic_x end_ARG ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )  divide start_ARG 1 end_ARG start_ARG 2 italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - 1 end_ARG italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  as the right hand side is positive and the left hand side is negative in ( 31 ) in this case. Hence, overall we showed that for any time  s 2  s ~ 1  s 0 subscript s 2 subscript ~ s 1 subscript s 0 s_{2}\\geq\\tilde{s}_{1}\\vee s_{0} italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , as long as  x ~ ~ x \\tilde{x} over~ start_ARG italic_x end_ARG  has not reached   1 +  2 2 subscript  1 subscript  2 2 \\frac{\\alpha_{1}+\\alpha_{2}}{2} divide start_ARG italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG , we have ( 31 ).",
            "Finally combining the upper bounds on the first and second terms in RHS of the ODE in ( 12 ) that we derived in Equations ( 24 ) and ( 30 ):",
            "where the last inequality follows from   x  2  w x 2 w -x\\leq 2w - italic_x  2 italic_w . Combining Equations ( 41 ) and ( 45 ), it is enough to show the following to prove  x ~  ( 1 ) > 0 ~ x 1 0 \\tilde{x}(1)>0 over~ start_ARG italic_x end_ARG ( 1 ) > 0 :",
            "Given  0 <  < 1 0 italic- 1 0<\\epsilon<1 0 < italic_ < 1 , assume  w   ~  ( log  log  ( 1 /  ) ) w ~  1 italic- w\\geq\\widetilde{\\Omega}(\\sqrt{\\log\\log(1/\\epsilon)}) italic_w  over~ start_ARG roman_ end_ARG ( square-root start_ARG roman_log roman_log ( 1 / italic_ ) end_ARG ) , where the hidden constant factor is sufficiently large. There exist densities  p ( 1 ) , p (  1 ) superscript p 1 superscript p 1 p^{(1)},p^{(-1)} italic_p start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT , italic_p start_POSTSUPERSCRIPT ( - 1 ) end_POSTSUPERSCRIPT  satisfying Assumption  1 , as well as functions  s t ( 1 ) , s t subscript superscript s 1 t subscript s t s^{(1)}_{t},s_{t} italic_s start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  satisfying",
            "We can take   1 = 1 subscript  1 1 \\alpha_{1}=1 italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 1 ,   2 = 2 subscript  2 2 \\alpha_{2}=2 italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 2 , and   = 1  1 \\beta=1 italic_ = 1  in the notation of the previous section. Note that  ln  ( w ) 16  w  1 w 16 w 1 \\frac{\\ln(w)}{16\\sqrt{w}}\\leq 1 divide start_ARG roman_ln ( italic_w ) end_ARG start_ARG 16 square-root start_ARG italic_w end_ARG end_ARG  1 , and  p p p italic_p  clearly satisfies Assumption  1 , so the hypotheses of Theorem  4  hold. We can then run through the proof of the Theorem unchanged up to and including Lemma  3 . There, instead of arguing that because we are moving at speed  w   1  = w w subscript  1  w \\frac{\\sqrt{w}\\alpha_{1}}{\\beta}=\\sqrt{w} divide start_ARG square-root start_ARG italic_w end_ARG italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG start_ARG italic_ end_ARG = square-root start_ARG italic_w end_ARG  for time  1 1 1 1  under the reparametrization of time  s s s italic_s , we only move at this speed for time  3 / 4 3 4 3/4 3 / 4 , thus reaching  w / 4 w 4 \\sqrt{w}/4 square-root start_ARG italic_w end_ARG / 4  at some point in this time interval. Because  w / 4  ln  ( w ) / ( 16   2 ) = ln  ( w ) / 32 w 4 w 16 subscript  2 w 32 \\sqrt{w}/4\\geq\\ln(w)/(16\\alpha_{2})=\\ln(w)/32 square-root start_ARG italic_w end_ARG / 4  roman_ln ( italic_w ) / ( 16 italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) = roman_ln ( italic_w ) / 32  for any  w  0 w 0 w\\geq 0 italic_w  0 , the ODE must enter the interval  [ ln  ( w ) / 32 ,  ) w 32 [\\ln(w)/32,\\infty) [ roman_ln ( italic_w ) / 32 ,  ) .",
            "By Lemma  10 , if we take  R  ln  ( w ) / 32  R w 32 R\\triangleq\\ln(w)/32 italic_R  roman_ln ( italic_w ) / 32 , then with probability  1  e  w / 8 1 superscript e w 8 1-e^{-w/8} 1 - italic_e start_POSTSUPERSCRIPT - italic_w / 8 end_POSTSUPERSCRIPT , the guided ODE will reach the (boundary of the) interval  [ R ,  ) R [R,\\infty) [ italic_R ,  )  at least time  t = ln  ( 3 / 4 ) t 3 4 t=\\ln(3/4) italic_t = roman_ln ( 3 / 4 )  before the end of the reverse process and then remain frozen at  R R R italic_R , as claimed.",
            "By Corollary  1 , the score error incurred by  s t ( 1 ) subscript superscript s 1 t s^{(1)}_{t} italic_s start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and  s t subscript s t s_{t} italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  is  1 6  ln  ( w )  ( 1 / w ) ln  ( w ) / 2048  1 6 w superscript 1 w w 2048 \\frac{1}{6}\\ln(w)\\cdot(1/w)^{\\ln(w)/2048} divide start_ARG 1 end_ARG start_ARG 6 end_ARG roman_ln ( italic_w )  ( 1 / italic_w ) start_POSTSUPERSCRIPT roman_ln ( italic_w ) / 2048 end_POSTSUPERSCRIPT , so as long as  w =  ~  ( log  log  ( 1 /  ) ) w ~  1 italic- w=\\widetilde{\\Omega}(\\sqrt{\\log\\log(1/\\epsilon)}) italic_w = over~ start_ARG roman_ end_ARG ( square-root start_ARG roman_log roman_log ( 1 / italic_ ) end_ARG ) , this is at most   italic- \\epsilon italic_  and the claim follows.",
            "Here we empirically verify the guidance dynamics predicted by Theorems  1  and  2 . All experiments in this section were conducted on a single A5000 GPU. We use Jax  [ 2 ]  for the experiments in Section  6.1  and PyTorch  [ 28 ]  for all other experiments.",
            "We first revisit the distribution used in Figure  1  (mixture of uniforms), and then consider the case of mixture of Gaussians in Appendix  A.1 . The distribution in Figure  1  is constructed by taking  p ( z ) superscript p z p^{(z)} italic_p start_POSTSUPERSCRIPT ( italic_z ) end_POSTSUPERSCRIPT  to be  Uniform  ( [  1 / 2 , 1 / 2 ]  [  1 / 2 , 1 / 2 ] ) Uniform 1 2 1 2 1 2 1 2 \\mathsf{Uniform}([-1/2,1/2]\\times[-1/2,1/2]) sansserif_Uniform ( [ - 1 / 2 , 1 / 2 ]  [ - 1 / 2 , 1 / 2 ] )  shifted by  2  z 2 z 2z 2 italic_z  in the  x x x italic_x -coordinate, and is a simple example that satisfies the assumptions of Theorem  1 . We demonstrated in Figure  1  how sampling with a larger guidance parameter yields a distribution of samples that is more concentrated than the true conditional distribution of the data. We now examine the ODE dynamics that produced these samples and compare them to the dynamics predicted by our theory.",
            "We generate samples using guidance by numerically solving the guided probability flow ODE ( 7 ) using the Dormand-Prince method  [ 14 ]  as implemented in JAX  [ 2 ] . For solving, we use  1000 1000 1000 1000  evaluation steps and take  T = 10 T 10 T=10 italic_T = 10 , which we is sufficiently large based on the stipulations of Theorem  1 . For obtaining the unconditional and conditional scores necessary for the ODE, it is straightforward to write down exact expressions for this case (which consist of integrals that we can numerically approximate). However, we estimate the scores using a more general approach that can be effectively applied to any mixture distribution for which we can sample both conditionally and unconditionally from.",
            "Both ( 61 ) and ( 63 ) follow from rewriting the convolutions as expectations and then using dominated convergence to pass the gradient into the expectations. Using the above, we can compute the scores by standard Monte-Carlo.",
            "Theorem  1  suggests that as we increase the guidance parameter  w w w italic_w , the ODE dynamics will push samples farther and farther in the direction of the guided class support before ultimately pulling them back to the support if necessary (i.e.  w w w italic_w  is large). As we show in Theorem  3 , this behavior can be undesirable if the sampling trajectory moves too far away from the desired class support, as it can amplify score estimation errors and lead to issues in the fidelity of the final produced samples.",
            "However, typical image datasets used in the diffusion literature such as ImageNet  [ 11 ]  are known to not be linearly separable, and therefore cannot fall under the exact conditions of Theorem  1 . That being said, simpler image datasets are known to be close to linearly separable - in particular, MNIST.",
            "We thus consider using the classifier-free guidance formulation (which corresponds to the second equality in ( 1 )) of  [ 20 ]  to conditionally sample from MNIST with guidance. We use the open-source classifier-free guidance implementation of  [ 29 ]  designed for MNIST.",
            "Although MNIST is perhaps the simplest generative image modeling testbed, it still presents a significant increase in complexity from the synthetic setting. Firstly, compared to the experiments of Section  6.1  and the setting of our theory, we are no longer considering only two classes. Furthermore, there is no guarantee that the class supports are well-separated, or even disjoint. Even more worrying, we do not have access to approximations of the true score functions of the conditional distributions that are guaranteed to be close as in ( 61 ) and ( 63 ); we have to instead learn a model-based score.",
            "We address the multi-class issue by using the standard one-vs-all reduction. In particular, we fix a single class as the positive class  y = + 1 y 1 y=+1 italic_y = + 1 , and then let the union of all other classes represent the negative class  y =  1 y 1 y=-1 italic_y = - 1 . We note that after this reduction, the distribution is close to linearly separable, and we are thus at least close in spirit to maintaining the separation from Theorem  1  under an appropriate basis.",
            "To obtain a projection direction for the sampling dynamics analogous to what was done in Section  6.1 , we generate 100 samples from the positive and negative classes using a guidance of  w = 0 w 0 w=0 italic_w = 0 , to approximate sampling from the conditional distributions. We then let the projection direction be the difference between the two sample means. For sampling, we use DDPM  [ 19 ]  with 400 time steps and a linear noise schedule, and we found that training the guidance model of  [ 29 ]  for 40 epochs was sufficient to generate high quality samples.",
            "Although we previously mentioned that experiments on more complicated datasets such as ImageNet are outside the scope of Theorem  1 , we show in this section that it is still possible to make qualitative guidance recommendations in such settings based on the ideas of Theorem  3 . The idea is that as we scale the guidance parameter  w w w italic_w  to be large, we start to obtain samples that are no longer within the original data distribution support due to amplification of score/precision errors.",
            "To conduct experiments on ImageNet, we use the  classifier-guided  ImageNet models available from  [ 13 ] . This is due to the fact that there are no classifier-free guidance models available from  [ 20 ] . The classifier-guidance formulation corresponds to the first equality in ( 1 ). To be consistent with the notation in  [ 13 ]  and to also clearly distinguish the classifier-guided setting from the classifier-free setting of Section  6.2 , we will use  s = 1 + w s 1 w s=1+w italic_s = 1 + italic_w  throughout the experiments in this section.",
            "For sampling with guidance, we use the  unconditional  diffusion model of  [ 13 ]  with the  256  256 256 256 256\\times 256 256  256  ImageNet classifier also released by  [ 13 ] . Note here that  [ 13 ]  combined diffusion guidance with their conditional model for their best results, but this does not fall in to the formulation of ( 1 ) and so we use the unconditional model. We use DDIM with 25 steps for the guidance samples as well.",
            "Figure  4  shows the final produced samples alongside the mean projected trajectories for an arbitrarily fixed positive class as in the experiments of Section  6.2 . We see that even for extreme guidance scales  s = 25 s 25 s=25 italic_s = 25  the previously observed non-monotonicity phenomenon in the projected trajectories no longer occurs. We suspect this can largely be attributed to the fact that the class supports are no longer close to separated, and as a result the direction corresponding to the difference in sample means is no longer a direction for which we can expect the dynamics of Theorem  1  (in fact, we can expect that there is no such direction along which these dynamics occur since the data is not linearly separable even after reducing to two classes). However, we note that as we increase the guidance strength, the final sample correlation along this mean difference direction continues to increase, more akin to the result of Theorem  2 .",
            "Here we consider a 2-D version of the mixture distribution of Theorem  2 , i.e.  p ( 1 ) = N  ( 1 , 1 )  N  ( 0 , 1 ) superscript p 1 tensor-product N 1 1 N 0 1 p^{(1)}=N(1,1)\\otimes N(0,1) italic_p start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT = italic_N ( 1 , 1 )  italic_N ( 0 , 1 )  and  p (  1 ) = N  (  1 , 1 )  N  ( 0 , 1 ) superscript p 1 tensor-product N 1 1 N 0 1 p^{(-1)}=N(-1,1)\\otimes N(0,1) italic_p start_POSTSUPERSCRIPT ( - 1 ) end_POSTSUPERSCRIPT = italic_N ( - 1 , 1 )  italic_N ( 0 , 1 ) . We follow the exact same experimental setup as Section  6.1  and generate 500 samples from the conditional distribution  p  ( x  z = + 1 ) p conditional x z 1 p(x\\mid z=+1) italic_p ( italic_x  italic_z = + 1 )  with varying levels of guidance.",
            "In Figures  6  to  14  we collect the MNIST experiments considering every other possible one-vs-all reduction. As mentioned in Section  6.2 , they have near-identical behavior to the experiments of Figure  3 .",
            "Figures  15  to  18  show the results of repeating the experimental setup of Section  6.3  for different choices of the positive class, and also illustrate limitations of this experimental setup in the context of ImageNet.",
            "In Figures  15  to  17 , we see approximately the same behavior as in  4 . Namely, guidance values for which we have support error lead to distorted samples. Similar to Figure  4 , the choice  s = 5 s 5 s=5 italic_s = 5  works well in Figure  15 . However, for Figures  16  and  17 , we see that we have non-zero support error even for  s = 5 s 5 s=5 italic_s = 5 . In these latter two cases, we expect the qualitatively best choice of guidance to be somewhere between  s = 1 s 1 s=1 italic_s = 1  and  s = 5 s 5 s=5 italic_s = 5 .",
            "Figure  18  demonstrates some limitations of our experimental setup for classes which have high levels of noise/variance. Indeed, we see that for the basketball class the support error is not even monotonically increasing with the guidance parameter, and that sample quality is poor across guidance levels. Furthermore, the projected trajectories become progressively more negative as opposed to positive, indicating that the direction between the means of the positive class samples and the negative class samples is likely useless in this case. Despite these various issues, there appears to still be some positive correlation between support error and sample distortion."
        ]
    },
    "id_table_2": {
        "caption": "",
        "table": "A1.EGx2",
        "footnotes": [],
        "references": [
            "There is however one fundamental snag in the reasoning above. The aforementioned ODE or SDE involves the score function at  different noise levels , i.e. we would need access to   log  p t z , w  subscript superscript p z w t \\nabla\\log p^{z,w}_{t}  roman_log italic_p start_POSTSUPERSCRIPT italic_z , italic_w end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , where we use the notation  q t subscript q t q_{t} italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  to denote the distribution given by running a certain noising process (see Section  2.1 ) for time  t t t italic_t  starting from a distribution  q q q italic_q . Here  p t z , w subscript superscript p z w t p^{z,w}_{t} italic_p start_POSTSUPERSCRIPT italic_z , italic_w end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  means we tilt before adding noise, that is, we take  q = p z , w q superscript p z w q=p^{z,w} italic_q = italic_p start_POSTSUPERSCRIPT italic_z , italic_w end_POSTSUPERSCRIPT  and then apply noise to  q q q italic_q . Unfortunately, as soon as  t > 0 t 0 t>0 italic_t > 0 , the analogue of Eq. ( 1 ) no longer holds, i.e.",
            "In other words, the operation of applying noise to  p p p italic_p  and the operation of tilting it in the direction of the conditional likelihood  do not commute . Nevertheless, in practice it is standard to use the right-hand side of Eq. ( 2 ) as an approximation  [ 13 ,  20 ] . Sampling using this approximation is called diffusion guidance.",
            "Theorems  1  and  2  illustrate that diffusion guidance results in a strong bias towards points in the support of one conditional distribution which are far from points in the support of the other.",
            "It has been observed previously  [ 15 ,  21 ]  that the score of the tilted distribution convolved with noise is different from what is used in diffusion guidance, i.e. Eq. ( 2 ). These works conclude informally that as a result, diffusion guidance should not be sampling from the tilted distribution. In contrast, our work gives rigorous justification for this and provides a fine-grained analysis of the behavior of diffusion guidance on simple toy examples, shedding new light on several key features of the dynamics of guidance.",
            "Instead of studying summary statistics of the generated output, we instead give a fine-grained analysis of where exactly the trajectory ends up at different times in the reverse process. While we do not directly study classification confidence, note that in the setting of our main result, Theorem  1 , for mixtures of compactly supported product distributions, the statement that classification confidence increases is uninformative because, as mentioned previously, the conditional likelihood for  any point  in the support of the target class is  1 1 1 1 . The dynamics that we elucidate in our results can be thought of as a more geometric notion of classification confidence. As for diversity, implicit in our Theorems  1  and  2  are quantitative bounds on how the diversity decreases as  w w w italic_w  increases.",
            "Assuming  ln  ( w )  2  w   1 missing / missing w subscript  2 w subscript  1 missing missing \\frac{\\ln(w)}{\\alpha_{2}}\\leq\\frac{\\sqrt{w}\\alpha_{1}}{missing/missing} divide start_ARG roman_ln ( italic_w ) end_ARG start_ARG italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG  divide start_ARG square-root start_ARG italic_w end_ARG italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG start_ARG roman_missing / roman_missing end_ARG ,  T  2  ln  2 T 2 2 T\\geq 2\\ln 2 italic_T  2 roman_ln 2 , under the conditions of Lemma  2 , given that  x ~  ( s ( 0 ) )   w   1 16  missing / missing ~ x superscript s 0 w subscript  1 16 missing missing \\tilde{x}(s^{(0)})\\geq-\\frac{\\sqrt{w}\\alpha_{1}}{16missing/missing} over~ start_ARG italic_x end_ARG ( italic_s start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT )  - divide start_ARG square-root start_ARG italic_w end_ARG italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG start_ARG 16 roman_missing / roman_missing end_ARG , there exists a time  s 0 subscript s 0 s_{0} italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  such that",
            "where recall  s ( 0 ) = e  T superscript s 0 superscript e T s^{(0)}=e^{-T} italic_s start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT = italic_e start_POSTSUPERSCRIPT - italic_T end_POSTSUPERSCRIPT  is the initial time for the ODE ( 12 ).",
            "Note that as long as  x ~  ( s ) < ln  ( w ) / ( 16   2 ) ~ x s w 16 subscript  2 \\tilde{x}(s)<\\ln(w)/(16\\alpha_{2}) over~ start_ARG italic_x end_ARG ( italic_s ) < roman_ln ( italic_w ) / ( 16 italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) , Lemma  2  gives",
            "which means starting from  x ~  ( s ( 0 ) )   w   1 16  missing / missing ~ x superscript s 0 w subscript  1 16 missing missing \\tilde{x}(s^{(0)})\\geq-\\frac{\\sqrt{w}\\alpha_{1}}{16missing/missing} over~ start_ARG italic_x end_ARG ( italic_s start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT )  - divide start_ARG square-root start_ARG italic_w end_ARG italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG start_ARG 16 roman_missing / roman_missing end_ARG  we definitely reach  w   1 16  missing / missing w subscript  1 16 missing missing \\frac{\\sqrt{w}\\alpha_{1}}{16missing/missing} divide start_ARG square-root start_ARG italic_w end_ARG italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG start_ARG 16 roman_missing / roman_missing end_ARG  by time  s = 1 2 s 1 2 s=\\frac{1}{2} italic_s = divide start_ARG 1 end_ARG start_ARG 2 end_ARG  if we continue with the same speed. (Note that the condition  a t 2  1 2 superscript subscript a t 2 1 2 a_{t}^{2}\\leq\\frac{1}{2} italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  divide start_ARG 1 end_ARG start_ARG 2 end_ARG  of Lemma  2  is satisfied up to time  s = 1 2 s 1 2 s=\\frac{1}{2} italic_s = divide start_ARG 1 end_ARG start_ARG 2 end_ARG .) But since  ln  ( w ) 16   2  w   1 4  missing / missing w 16 subscript  2 w subscript  1 4 missing missing \\frac{\\ln(w)}{16\\alpha_{2}}\\leq\\frac{\\sqrt{w}\\alpha_{1}}{4missing/missing} divide start_ARG roman_ln ( italic_w ) end_ARG start_ARG 16 italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG  divide start_ARG square-root start_ARG italic_w end_ARG italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG start_ARG 4 roman_missing / roman_missing end_ARG , then we definitely have to pass the point  ln  ( w ) 16   2 w 16 subscript  2 \\frac{\\ln(w)}{16\\alpha_{2}} divide start_ARG roman_ln ( italic_w ) end_ARG start_ARG 16 italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG .",
            "Plugging this back into ( 22 )",
            "Combining this with Equation ( 20 ), we obtain the following upper bound for the second term in Equation ( 16 ):",
            "From Equation ( 12 ), as long as  x ~  ( s )  0 ~ x s 0 \\tilde{x}(s)\\geq 0 over~ start_ARG italic_x end_ARG ( italic_s )  0 , we have",
            "If we denote by  s 1 subscript s 1 s_{1} italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  the first time  s  s 0 s subscript s 0 s\\geq s_{0} italic_s  italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  when  x  ( s ) = a t  ( s )   2 = s   2 x s subscript a t s subscript  2 s subscript  2 x(s)=a_{t(s)}\\alpha_{2}=s\\alpha_{2} italic_x ( italic_s ) = italic_a start_POSTSUBSCRIPT italic_t ( italic_s ) end_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = italic_s italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , then  s 1 subscript s 1 s_{1} italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  is certainly larger than the first time that  x  ( s ) =  2 x s subscript  2 x(s)=\\alpha_{2} italic_x ( italic_s ) = italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  as for all  t t t italic_t ,  a t  1 subscript a t 1 a_{t}\\leq 1 italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  1 . Let  s 2 subscript s 2 s_{2} italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  denote the first time  s  s 0 s subscript s 0 s\\geq s_{0} italic_s  italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  such that  x  ( s ) =  2 x s subscript  2 x(s)=\\alpha_{2} italic_x ( italic_s ) = italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT . From Equation ( 23 ),",
            "On the other hand, for the first term in Equation ( 12 ), as long as  x ~  ( s )  1 2  s 0  1   2 ~ x s 1 2 subscript s 0 1 subscript  2 \\tilde{x}(s)\\geq\\frac{1}{2s_{0}-1}\\alpha_{2} over~ start_ARG italic_x end_ARG ( italic_s )  divide start_ARG 1 end_ARG start_ARG 2 italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - 1 end_ARG italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , we get",
            "Note that using ( 25 ) and ( 27 ), we have",
            "First comparing Equations ( 12 ) and ( 28 ), since the second term in the RHS of the ODE in ( 12 ) is always positive, from ODE comparison theorem we get  B  ( s )  x ~  ( s 0 ) + A  ( s ) B s ~ x subscript s 0 A s B(s)\\geq\\tilde{x}(s_{0})+A(s) italic_B ( italic_s )  over~ start_ARG italic_x end_ARG ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) + italic_A ( italic_s ) .",
            "But we can solve the ODE in ( 27 ):",
            "Note that inequality ( 29 ) is valid more generally up to time  s 1  subscript superscript s  1 s^{\\prime}_{1} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , when  B  ( s ) B s B(s) italic_B ( italic_s )  reaches  1 2  s 0  1   2   2 = 2  ( 1  s 0 ) 1  2  ( 1  s 0 )   2 1 2 subscript s 0 1 subscript  2 subscript  2 2 1 subscript s 0 1 2 1 subscript s 0 subscript  2 \\frac{1}{2s_{0}-1}\\alpha_{2}-\\alpha_{2}=\\frac{2\\left({1-s_{0}}\\right)}{1-2(1-s% _{0})}\\alpha_{2} divide start_ARG 1 end_ARG start_ARG 2 italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - 1 end_ARG italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = divide start_ARG 2 ( 1 - italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_ARG start_ARG 1 - 2 ( 1 - italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_ARG italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT . Now we solve for the value of  s ~ 1 subscript ~ s 1 \\tilde{s}_{1} over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  when  d  ( s ~ 1 ) d subscript ~ s 1 d(\\tilde{s}_{1}) italic_d ( over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT )  reaches  2  ( 1  s 0 ) 1  2  ( 1  s 0 )   2 2 1 subscript s 0 1 2 1 subscript s 0 subscript  2 \\frac{2\\left({1-s_{0}}\\right)}{1-2(1-s_{0})}\\alpha_{2} divide start_ARG 2 ( 1 - italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_ARG start_ARG 1 - 2 ( 1 - italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_ARG italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , which upper bounds  s 1  subscript superscript s  1 s^{\\prime}_{1} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  due to ( 29 ):",
            "and from definition for this choice of  s ~ 1 subscript ~ s 1 \\tilde{s}_{1} over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  we get from Equation ( 29 )",
            "On the other hand, note that for any time  s 2  s 1  subscript s 2 subscript superscript s  1 s_{2}\\geq s^{\\prime}_{1} italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  before  x ~  ( s ) ~ x s \\tilde{x}(s) over~ start_ARG italic_x end_ARG ( italic_s )  reaches   1 +  2 2 subscript  1 subscript  2 2 \\frac{\\alpha_{1}+\\alpha_{2}}{2} divide start_ARG italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG  (if it ever reaches that value), the first term in ( 12 ) is always non-positive. Hence, we have",
            "Finally combining the upper bounds on the first and second terms in RHS of the ODE in ( 12 ) that we derived in Equations ( 24 ) and ( 30 ):",
            "Now take  s ~ 0  s 0  3 4 subscript ~ s 0 subscript s 0 3 4 \\tilde{s}_{0}\\geq s_{0}\\vee\\frac{3}{4} over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  divide start_ARG 3 end_ARG start_ARG 4 end_ARG . Then we can use Lemma  6  with  s 0 = s ~ 0 subscript s 0 subscript ~ s 0 s_{0}=\\tilde{s}_{0} italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  because   2  ( 1  32 ln  ( w ) )   1 +  2 2 subscript  2 1 32 w subscript  1 subscript  2 2 \\alpha_{2}\\left({1-\\frac{32}{\\sqrt{\\ln(w)}}}\\right)\\geq\\frac{\\alpha_{1}+\\alpha% _{2}}{2} italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( 1 - divide start_ARG 32 end_ARG start_ARG square-root start_ARG roman_ln ( italic_w ) end_ARG end_ARG )  divide start_ARG italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG  so its condition is satisfied from ( 32 ); then, Lemma  6  implies that there exists a time  s 1 subscript s 1 s_{1} italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  such that for all  s  [ s 1 , 1 ] s subscript s 1 1 s\\in[s_{1},1] italic_s  [ italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , 1 ] :",
            "Note that  s ~ 0 subscript ~ s 0 \\tilde{s}_{0} over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  can be picked any value in the interval  ( s 0  3 4 , 1 ) subscript s 0 3 4 1 (s_{0}\\vee\\frac{3}{4},1) ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  divide start_ARG 3 end_ARG start_ARG 4 end_ARG , 1 ) . Therefore, picking  s ~ 0  1  subscript ~ s 0 1 \\tilde{s}_{0}\\rightarrow 1 over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  1 , Equations ( 32 ) and ( 33 ) show that with probability at least  1  e  w   1 2 8   2 1 superscript e w superscript subscript  1 2 8 superscript  2 1-e^{-\\frac{w\\alpha_{1}^{2}}{8\\beta^{2}}} 1 - italic_e start_POSTSUPERSCRIPT - divide start_ARG italic_w italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 8 italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG end_POSTSUPERSCRIPT ,  x ~  ( s ) ~ x s \\tilde{x}(s) over~ start_ARG italic_x end_ARG ( italic_s )  converges to the interval  (  2  ( 1  32 ln  ( w ) ) ,  2 ) subscript  2 1 32 w subscript  2 \\left({\\alpha_{2}\\left({1-\\frac{32}{\\sqrt{\\ln(w)}}}\\right),\\alpha_{2}}\\right) ( italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( 1 - divide start_ARG 32 end_ARG start_ARG square-root start_ARG roman_ln ( italic_w ) end_ARG end_ARG ) , italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) .",
            "and let its roots be  s 0  < s 1  subscript superscript s 0 subscript superscript s 1 s^{*}_{0}<s^{*}_{1} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT < italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . First note that  Q  ( 0 ) = c > 0 Q 0 c 0 Q(0)=c>0 italic_Q ( 0 ) = italic_c > 0  and  Q  ( s 0 ) = s 0  x ~  ( s 0 ) + c  0 Q subscript s 0 subscript s 0 ~ x subscript s 0 c 0 Q(s_{0})=s_{0}\\tilde{x}(s_{0})+c\\leq 0 italic_Q ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT over~ start_ARG italic_x end_ARG ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) + italic_c  0 , hence  Q Q Q italic_Q  has a root in the interval  [ 0 , s 0 ] 0 subscript s 0 [0,s_{0}] [ 0 , italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ]  and  s 0   s 0 subscript superscript s 0 subscript s 0 s^{*}_{0}\\leq s_{0} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT . On the other hand,  s 1 > s 0 subscript s 1 subscript s 0 s_{1}>s_{0} italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , so ( 42 ) implies that  s 1 subscript s 1 s_{1} italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  should greater or equal to the larger root  s 1  subscript superscript s 1 s^{*}_{1} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . Now we lower bound  s 1  subscript superscript s 1 s^{*}_{1} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . Defining  G = x ~  ( s 0 )  ( 2  w + 1 )  s 0 2  ( 2  w + 1 ) G ~ x subscript s 0 2 w 1 subscript s 0 2 2 w 1 G=\\frac{\\tilde{x}(s_{0})-(2w+1)s_{0}}{2(2w+1)} italic_G = divide start_ARG over~ start_ARG italic_x end_ARG ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) - ( 2 italic_w + 1 ) italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG start_ARG 2 ( 2 italic_w + 1 ) end_ARG , completing the square for ( 43 ) gives",
            "If the guided ODE is run with the score estimates in Eq. ( 52 ), then as soon as the process enters the interval  [ R ,  ) R [R,\\infty) [ italic_R ,  ) , it no longer moves. The reason is that the within this interval, the guided ODE is given by",
            "Here we empirically verify the guidance dynamics predicted by Theorems  1  and  2 . All experiments in this section were conducted on a single A5000 GPU. We use Jax  [ 2 ]  for the experiments in Section  6.1  and PyTorch  [ 28 ]  for all other experiments.",
            "To verify this, we plot the mean of the projected ODE trajectories for increasing guidance parameter values alongside the final produced samples from each trajectory in Figure  2 . Since large choices of the guidance parameter lead to some trajectories diverging due to numerical instability/score approximation errors, we visualize only the samples and trajectories that were good in that they produced final samples constrained within the guided class support (and we indicate this proportion on the plots). The results show that the projected trajectories do indeed follow the predicted dynamics, with larger choices of  w w w italic_w  leading to a pronounced pullback towards the end of the trajectories.",
            "Figure  3  shows the mean projected sampling trajectories alongside the final produced samples for the same choices of guidance parameters used in Figure  2  and the positive class fixed to be the digit 0. We observe the same phenomenon as before: after the guidance parameter  w w w italic_w  is taken to be sufficiently large, there is a pullback effect in the projected sampling dynamics. Furthermore, once again as before we note that the qualitatively best choice of  w w w italic_w  (again  w = 3 w 3 w=3 italic_w = 3 ) is the largest choice for which we can preserve monotonicity of the projected sampling dynamics. These results are not sensitive to the choice of positive class; we show similar plots for every other choice of positive class (i.e. all the non-zero digits) in Appendix  A.2 . Interestingly, for almost any choice of positive class used in the reduction, the qualitatively optimal choice of guidance amongst the values we consider remains roughly the same.",
            "To conduct experiments on ImageNet, we use the  classifier-guided  ImageNet models available from  [ 13 ] . This is due to the fact that there are no classifier-free guidance models available from  [ 20 ] . The classifier-guidance formulation corresponds to the first equality in ( 1 ). To be consistent with the notation in  [ 13 ]  and to also clearly distinguish the classifier-guided setting from the classifier-free setting of Section  6.2 , we will use  s = 1 + w s 1 w s=1+w italic_s = 1 + italic_w  throughout the experiments in this section.",
            "First, we illustrate that the behavior exhibited in Figure  3  no longer holds when running diffusion with guidance on ImageNet, at least using the same experimental setup as before. To parallel the experiments of Section  6.2 , we use the  256  256 256 256 256\\times 256 256  256   conditional  diffusion model released by  [ 13 ]  to generate samples from a fixed ImageNet class (corresponding to  y = + 1 y 1 y=+1 italic_y = + 1  as before), and then use the same model to generate samples from all other classes (corresponding to  y =  1 y 1 y=-1 italic_y = - 1 ). We generate 50 samples from the positive and negative classes (due to the cost of sampling at this resolution and the overhead of storing the entire sampling trajectories), and then compute the normalized direction between the two sample means as before. 2 2 2 We should note that, in contrast to the MNIST experiments, this choice of direction is very noisy in the ImageNet setting. However, we use the same setup as before both for consistency and to show that Theorem  3  can be applied even in this imperfect experimental setting.  For sampling, we use DDIM  [ 33 ]  with 25 steps, once again because storing the entire sampling trajectories using DDPM with a large number of steps is prohibitive.",
            "Figure  4  shows the final produced samples alongside the mean projected trajectories for an arbitrarily fixed positive class as in the experiments of Section  6.2 . We see that even for extreme guidance scales  s = 25 s 25 s=25 italic_s = 25  the previously observed non-monotonicity phenomenon in the projected trajectories no longer occurs. We suspect this can largely be attributed to the fact that the class supports are no longer close to separated, and as a result the direction corresponding to the difference in sample means is no longer a direction for which we can expect the dynamics of Theorem  1  (in fact, we can expect that there is no such direction along which these dynamics occur since the data is not linearly separable even after reducing to two classes). However, we note that as we increase the guidance strength, the final sample correlation along this mean difference direction continues to increase, more akin to the result of Theorem  2 .",
            "Here we consider a 2-D version of the mixture distribution of Theorem  2 , i.e.  p ( 1 ) = N  ( 1 , 1 )  N  ( 0 , 1 ) superscript p 1 tensor-product N 1 1 N 0 1 p^{(1)}=N(1,1)\\otimes N(0,1) italic_p start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT = italic_N ( 1 , 1 )  italic_N ( 0 , 1 )  and  p (  1 ) = N  (  1 , 1 )  N  ( 0 , 1 ) superscript p 1 tensor-product N 1 1 N 0 1 p^{(-1)}=N(-1,1)\\otimes N(0,1) italic_p start_POSTSUPERSCRIPT ( - 1 ) end_POSTSUPERSCRIPT = italic_N ( - 1 , 1 )  italic_N ( 0 , 1 ) . We follow the exact same experimental setup as Section  6.1  and generate 500 samples from the conditional distribution  p  ( x  z = + 1 ) p conditional x z 1 p(x\\mid z=+1) italic_p ( italic_x  italic_z = + 1 )  with varying levels of guidance.",
            "We once again plot the mean of the probability flow ODE trajectories (projected on to the  x x x italic_x -coordinate) for increasing guidance parameter values alongside the final produced samples from each trajectory in Figure  5 . The figure is analogous to Figure  2 , except for larger choices of the guidance parameter  w w w italic_w  and the fact that the proportion of good samples here is the proportion of samples that did not result in NaNs (since we are no longer in the compact support setting).",
            "We use larger  w w w italic_w  values to better illustrate the behavior predicted in Theorem  2 . As can be seen from Figure  5  (a), we produce more samples with larger (positive)  x x x italic_x -coordinates as we increase  w w w italic_w . However, we also get significantly more numerical instability, and as a result the mean trajectory plot in Figure  5  (b) is much less meaningful than it was in Figure  2 .",
            "In Figures  6  to  14  we collect the MNIST experiments considering every other possible one-vs-all reduction. As mentioned in Section  6.2 , they have near-identical behavior to the experiments of Figure  3 ."
        ]
    },
    "id_table_3": {
        "caption": "",
        "table": "A1.EGx3",
        "footnotes": [],
        "references": [
            "In other words, for any level of score estimation error, if one takes the guidance parameter  w w w italic_w  to be too large, the sampler will end up going off the support of the data distribution  p p p italic_p . Roughly speaking, the idea derives from the proof of Theorem  1 . As we will see, one key feature of the guided ODE in the setting of Theorem  1  is that the trajectory first  swings past the edges of the support of  p p p italic_p  and into the tails of the noised data distribution  p t subscript p t p_{t} italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  before returning. As a result, errors in score estimation at these tails can move the sampling process away from the intended trajectory and thus prevent the trajectory from ever returning to the support of  p p p italic_p , leading to corrupted outputs. We stress that this phenomenon is not an issue of numerical precision: Theorem  3  applies even if one runs diffusion guidance with infinite precision.",
            "Taking inspiration from our theory, we posit that the optimal choice of guidance (from the perspective of sample quality) for compactly supported distributions that approximately satisfy the assumptions of our theory is the largest possible  w w w italic_w  for which the resulting trajectory does not exhibit this behavior of swinging away from the support of the data distribution and returning. Specifically, we propose a rule of thumb for selecting the guidance strength based on looking at a certain  monotonicity  property of the trajectory and experimentally validate this rule of thumb in both synthetic settings and on image classification datasets. Additionally, for compactly supported distributions that fall outside the scope of Theorem  1 , we propose an alternative heuristic based on the ideas of Theorem  3 : we should choose the guidance strength as large as possible while still ensuring that final samples are contained within the distribution support. See Section  6  for details.",
            "is a positive dominant term (compared to the first term) unless we have  q t (  1 ) = O  ( 1 / w ) subscript superscript q 1 t O 1 w q^{(-1)}_{t}=O(1/w) italic_q start_POSTSUPERSCRIPT ( - 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_O ( 1 / italic_w ) , which happens only when  x  ( t ) =   ( ln  ( w ) ) x t  w x(t)=\\Omega(\\ln(w)) italic_x ( italic_t ) = roman_ ( roman_ln ( italic_w ) ) . First, we show that (1) starting from a high probability region for  x  ( 0 ) x 0 x(0) italic_x ( 0 ) , the particle will get to  x  ( t )    ( ln  ( w ) ) x t  w x(t)\\geq\\Omega(\\ln(w)) italic_x ( italic_t )  roman_ ( roman_ln ( italic_w ) )  at some time  t t t italic_t , using the dominance of the second term (Lemma  3 ). Then, in the case where  x  ( t 0 ) =   ( ln  ( w ) ) x subscript t 0  w x(t_{0})=\\Omega(\\ln(w)) italic_x ( italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = roman_ ( roman_ln ( italic_w ) )  for some time  t 0 subscript t 0 t_{0} italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , (2) we show a lower bound on the time that it takes for  x  ( t ) x t x(t) italic_x ( italic_t )  to get back to the proximity of the origin and then an upper bound on how much it can move inside the support (Lemma  5 ). Finally in Lemma  6  we show that  x  ( t ) x t x(t) italic_x ( italic_t )  does converge to the interval, provided the event of Lemma  3  holds. Building upon these Lemmas, we prove the final result in Theorem  4 .",
            "Finally combining this with our upper bound for the first part in Equation ( 30 ) completes the proof.",
            "If we denote by  s 1 subscript s 1 s_{1} italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  the first time  s  s 0 s subscript s 0 s\\geq s_{0} italic_s  italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  when  x  ( s ) = a t  ( s )   2 = s   2 x s subscript a t s subscript  2 s subscript  2 x(s)=a_{t(s)}\\alpha_{2}=s\\alpha_{2} italic_x ( italic_s ) = italic_a start_POSTSUBSCRIPT italic_t ( italic_s ) end_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = italic_s italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , then  s 1 subscript s 1 s_{1} italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  is certainly larger than the first time that  x  ( s ) =  2 x s subscript  2 x(s)=\\alpha_{2} italic_x ( italic_s ) = italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  as for all  t t t italic_t ,  a t  1 subscript a t 1 a_{t}\\leq 1 italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  1 . Let  s 2 subscript s 2 s_{2} italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  denote the first time  s  s 0 s subscript s 0 s\\geq s_{0} italic_s  italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  such that  x  ( s ) =  2 x s subscript  2 x(s)=\\alpha_{2} italic_x ( italic_s ) = italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT . From Equation ( 23 ),",
            "Note that the inequality ( 31 ) is true even when  x ~  ( s 0 )  1 2  s 0  1   2 ~ x subscript s 0 1 2 subscript s 0 1 subscript  2 \\tilde{x}(s_{0})\\leq\\frac{1}{2s_{0}-1}\\alpha_{2} over~ start_ARG italic_x end_ARG ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )  divide start_ARG 1 end_ARG start_ARG 2 italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - 1 end_ARG italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  as the right hand side is positive and the left hand side is negative in ( 31 ) in this case. Hence, overall we showed that for any time  s 2  s ~ 1  s 0 subscript s 2 subscript ~ s 1 subscript s 0 s_{2}\\geq\\tilde{s}_{1}\\vee s_{0} italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , as long as  x ~ ~ x \\tilde{x} over~ start_ARG italic_x end_ARG  has not reached   1 +  2 2 subscript  1 subscript  2 2 \\frac{\\alpha_{1}+\\alpha_{2}}{2} divide start_ARG italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG , we have ( 31 ).",
            "Finally combining the upper bounds on the first and second terms in RHS of the ODE in ( 12 ) that we derived in Equations ( 24 ) and ( 30 ):",
            "Then, from Lemma  3 , we get that for some time  s 0  1 subscript s 0 1 s_{0}\\leq 1 italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  1 ,",
            "Now take  s ~ 0  s 0  3 4 subscript ~ s 0 subscript s 0 3 4 \\tilde{s}_{0}\\geq s_{0}\\vee\\frac{3}{4} over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  divide start_ARG 3 end_ARG start_ARG 4 end_ARG . Then we can use Lemma  6  with  s 0 = s ~ 0 subscript s 0 subscript ~ s 0 s_{0}=\\tilde{s}_{0} italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  because   2  ( 1  32 ln  ( w ) )   1 +  2 2 subscript  2 1 32 w subscript  1 subscript  2 2 \\alpha_{2}\\left({1-\\frac{32}{\\sqrt{\\ln(w)}}}\\right)\\geq\\frac{\\alpha_{1}+\\alpha% _{2}}{2} italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( 1 - divide start_ARG 32 end_ARG start_ARG square-root start_ARG roman_ln ( italic_w ) end_ARG end_ARG )  divide start_ARG italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG  so its condition is satisfied from ( 32 ); then, Lemma  6  implies that there exists a time  s 1 subscript s 1 s_{1} italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  such that for all  s  [ s 1 , 1 ] s subscript s 1 1 s\\in[s_{1},1] italic_s  [ italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , 1 ] :",
            "Note that  s ~ 0 subscript ~ s 0 \\tilde{s}_{0} over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  can be picked any value in the interval  ( s 0  3 4 , 1 ) subscript s 0 3 4 1 (s_{0}\\vee\\frac{3}{4},1) ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  divide start_ARG 3 end_ARG start_ARG 4 end_ARG , 1 ) . Therefore, picking  s ~ 0  1  subscript ~ s 0 1 \\tilde{s}_{0}\\rightarrow 1 over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  1 , Equations ( 32 ) and ( 33 ) show that with probability at least  1  e  w   1 2 8   2 1 superscript e w superscript subscript  1 2 8 superscript  2 1-e^{-\\frac{w\\alpha_{1}^{2}}{8\\beta^{2}}} 1 - italic_e start_POSTSUPERSCRIPT - divide start_ARG italic_w italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 8 italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG end_POSTSUPERSCRIPT ,  x ~  ( s ) ~ x s \\tilde{x}(s) over~ start_ARG italic_x end_ARG ( italic_s )  converges to the interval  (  2  ( 1  32 ln  ( w ) ) ,  2 ) subscript  2 1 32 w subscript  2 \\left({\\alpha_{2}\\left({1-\\frac{32}{\\sqrt{\\ln(w)}}}\\right),\\alpha_{2}}\\right) ( italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( 1 - divide start_ARG 32 end_ARG start_ARG square-root start_ARG roman_ln ( italic_w ) end_ARG end_ARG ) , italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) .",
            "We begin by performing some preliminary calculations to simplify the guided ODE, culminating in the simplified expression in Equation ( 37 ) below.",
            "where  x  N  ( 0 , 1 ) similar-to x N 0 1 x\\sim\\mathcal{N}(0,1) italic_x  caligraphic_N ( 0 , 1 ) . Below, we study the behavior of the ODE in ( 37 ) for different initial conditions  x ~  ( 0 ) = x ~ x 0 x \\tilde{x}(0)=x over~ start_ARG italic_x end_ARG ( 0 ) = italic_x .",
            "Before proceeding with the proof, we note that up to the    ( ln  w )  w \\Theta(\\ln w) roman_ ( roman_ln italic_w )  term, this analysis is nearly tight in the regime of large  w w w italic_w . The reason is that if  x ~  ( 0 ) <  2  w  1 ~ x 0 2 w 1 \\tilde{x}(0)<-2w-1 over~ start_ARG italic_x end_ARG ( 0 ) < - 2 italic_w - 1 , then because the velocity in Equation ( 37 ) is upper bounded by  2  w + 1 2 w 1 2w+1 2 italic_w + 1  at all times, the particle will remain negative at all times.",
            "First, note that the right-hand side of Equation ( 37 ) is lower bounded by  1 1 1 1 , so if  x ~  ( 0 )  0 ~ x 0 0 \\tilde{x}(0)\\geq 0 over~ start_ARG italic_x end_ARG ( 0 )  0 , then  x ~  ( 1 )  x ~  ( 0 ) + w  0 ~ x 1 ~ x 0 w 0 \\tilde{x}(1)\\geq\\tilde{x}(0)+w\\geq 0 over~ start_ARG italic_x end_ARG ( 1 )  over~ start_ARG italic_x end_ARG ( 0 ) + italic_w  0 . More generally, this implies that as soon as the particle becomes positive, it continues to move to the right at rate lower bounded by  1 1 1 1 .",
            "In the following we upper bound  s 1 subscript s 1 s_{1} italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . Defining   italic- \\epsilon italic_  by   / w = 1 + tanh  (  c ) italic- w 1 c \\epsilon/w=1+\\tanh(-c) italic_ / italic_w = 1 + roman_tanh ( - italic_c ) , we see from the definition of ODE ( 37 ) that for all  s 0  s  s 1 subscript s 0 s subscript s 1 s_{0}\\leq s\\leq s_{1} italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  italic_s  italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT :",
            "On the other hand, note that from ( 35 ),  x ~   ( s )  ( 2  w + 1 ) superscript ~ x  s 2 w 1 \\tilde{x}^{\\prime}(s)\\leq(2w+1) over~ start_ARG italic_x end_ARG start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ( italic_s )  ( 2 italic_w + 1 ) , we have  x ~  ( s )  x ~  ( s 0 ) + ( 2  w + 1 )  ( s  s 0 ) ~ x s ~ x subscript s 0 2 w 1 s subscript s 0 \\tilde{x}(s)\\leq\\tilde{x}(s_{0})+(2w+1)(s-s_{0}) over~ start_ARG italic_x end_ARG ( italic_s )  over~ start_ARG italic_x end_ARG ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) + ( 2 italic_w + 1 ) ( italic_s - italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) . In particular, from the definition of  s 1 subscript s 1 s_{1} italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,",
            "and let its roots be  s 0  < s 1  subscript superscript s 0 subscript superscript s 1 s^{*}_{0}<s^{*}_{1} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT < italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . First note that  Q  ( 0 ) = c > 0 Q 0 c 0 Q(0)=c>0 italic_Q ( 0 ) = italic_c > 0  and  Q  ( s 0 ) = s 0  x ~  ( s 0 ) + c  0 Q subscript s 0 subscript s 0 ~ x subscript s 0 c 0 Q(s_{0})=s_{0}\\tilde{x}(s_{0})+c\\leq 0 italic_Q ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT over~ start_ARG italic_x end_ARG ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) + italic_c  0 , hence  Q Q Q italic_Q  has a root in the interval  [ 0 , s 0 ] 0 subscript s 0 [0,s_{0}] [ 0 , italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ]  and  s 0   s 0 subscript superscript s 0 subscript s 0 s^{*}_{0}\\leq s_{0} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT . On the other hand,  s 1 > s 0 subscript s 1 subscript s 0 s_{1}>s_{0} italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , so ( 42 ) implies that  s 1 subscript s 1 s_{1} italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  should greater or equal to the larger root  s 1  subscript superscript s 1 s^{*}_{1} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . Now we lower bound  s 1  subscript superscript s 1 s^{*}_{1} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . Defining  G = x ~  ( s 0 )  ( 2  w + 1 )  s 0 2  ( 2  w + 1 ) G ~ x subscript s 0 2 w 1 subscript s 0 2 2 w 1 G=\\frac{\\tilde{x}(s_{0})-(2w+1)s_{0}}{2(2w+1)} italic_G = divide start_ARG over~ start_ARG italic_x end_ARG ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) - ( 2 italic_w + 1 ) italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG start_ARG 2 ( 2 italic_w + 1 ) end_ARG , completing the square for ( 43 ) gives",
            "Therefore, from ( 38 ),",
            "We now draw upon the first stage of the analysis in Section  3  to prove the following:",
            "We can take   1 = 1 subscript  1 1 \\alpha_{1}=1 italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 1 ,   2 = 2 subscript  2 2 \\alpha_{2}=2 italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 2 , and   = 1  1 \\beta=1 italic_ = 1  in the notation of the previous section. Note that  ln  ( w ) 16  w  1 w 16 w 1 \\frac{\\ln(w)}{16\\sqrt{w}}\\leq 1 divide start_ARG roman_ln ( italic_w ) end_ARG start_ARG 16 square-root start_ARG italic_w end_ARG end_ARG  1 , and  p p p italic_p  clearly satisfies Assumption  1 , so the hypotheses of Theorem  4  hold. We can then run through the proof of the Theorem unchanged up to and including Lemma  3 . There, instead of arguing that because we are moving at speed  w   1  = w w subscript  1  w \\frac{\\sqrt{w}\\alpha_{1}}{\\beta}=\\sqrt{w} divide start_ARG square-root start_ARG italic_w end_ARG italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG start_ARG italic_ end_ARG = square-root start_ARG italic_w end_ARG  for time  1 1 1 1  under the reparametrization of time  s s s italic_s , we only move at this speed for time  3 / 4 3 4 3/4 3 / 4 , thus reaching  w / 4 w 4 \\sqrt{w}/4 square-root start_ARG italic_w end_ARG / 4  at some point in this time interval. Because  w / 4  ln  ( w ) / ( 16   2 ) = ln  ( w ) / 32 w 4 w 16 subscript  2 w 32 \\sqrt{w}/4\\geq\\ln(w)/(16\\alpha_{2})=\\ln(w)/32 square-root start_ARG italic_w end_ARG / 4  roman_ln ( italic_w ) / ( 16 italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) = roman_ln ( italic_w ) / 32  for any  w  0 w 0 w\\geq 0 italic_w  0 , the ODE must enter the interval  [ ln  ( w ) / 32 ,  ) w 32 [\\ln(w)/32,\\infty) [ roman_ln ( italic_w ) / 32 ,  ) .",
            "Both ( 61 ) and ( 63 ) follow from rewriting the convolutions as expectations and then using dominated convergence to pass the gradient into the expectations. Using the above, we can compute the scores by standard Monte-Carlo.",
            "Theorem  1  suggests that as we increase the guidance parameter  w w w italic_w , the ODE dynamics will push samples farther and farther in the direction of the guided class support before ultimately pulling them back to the support if necessary (i.e.  w w w italic_w  is large). As we show in Theorem  3 , this behavior can be undesirable if the sampling trajectory moves too far away from the desired class support, as it can amplify score estimation errors and lead to issues in the fidelity of the final produced samples.",
            "Although MNIST is perhaps the simplest generative image modeling testbed, it still presents a significant increase in complexity from the synthetic setting. Firstly, compared to the experiments of Section  6.1  and the setting of our theory, we are no longer considering only two classes. Furthermore, there is no guarantee that the class supports are well-separated, or even disjoint. Even more worrying, we do not have access to approximations of the true score functions of the conditional distributions that are guaranteed to be close as in ( 61 ) and ( 63 ); we have to instead learn a model-based score.",
            "Figure  3  shows the mean projected sampling trajectories alongside the final produced samples for the same choices of guidance parameters used in Figure  2  and the positive class fixed to be the digit 0. We observe the same phenomenon as before: after the guidance parameter  w w w italic_w  is taken to be sufficiently large, there is a pullback effect in the projected sampling dynamics. Furthermore, once again as before we note that the qualitatively best choice of  w w w italic_w  (again  w = 3 w 3 w=3 italic_w = 3 ) is the largest choice for which we can preserve monotonicity of the projected sampling dynamics. These results are not sensitive to the choice of positive class; we show similar plots for every other choice of positive class (i.e. all the non-zero digits) in Appendix  A.2 . Interestingly, for almost any choice of positive class used in the reduction, the qualitatively optimal choice of guidance amongst the values we consider remains roughly the same.",
            "Although we previously mentioned that experiments on more complicated datasets such as ImageNet are outside the scope of Theorem  1 , we show in this section that it is still possible to make qualitative guidance recommendations in such settings based on the ideas of Theorem  3 . The idea is that as we scale the guidance parameter  w w w italic_w  to be large, we start to obtain samples that are no longer within the original data distribution support due to amplification of score/precision errors.",
            "First, we illustrate that the behavior exhibited in Figure  3  no longer holds when running diffusion with guidance on ImageNet, at least using the same experimental setup as before. To parallel the experiments of Section  6.2 , we use the  256  256 256 256 256\\times 256 256  256   conditional  diffusion model released by  [ 13 ]  to generate samples from a fixed ImageNet class (corresponding to  y = + 1 y 1 y=+1 italic_y = + 1  as before), and then use the same model to generate samples from all other classes (corresponding to  y =  1 y 1 y=-1 italic_y = - 1 ). We generate 50 samples from the positive and negative classes (due to the cost of sampling at this resolution and the overhead of storing the entire sampling trajectories), and then compute the normalized direction between the two sample means as before. 2 2 2 We should note that, in contrast to the MNIST experiments, this choice of direction is very noisy in the ImageNet setting. However, we use the same setup as before both for consistency and to show that Theorem  3  can be applied even in this imperfect experimental setting.  For sampling, we use DDIM  [ 33 ]  with 25 steps, once again because storing the entire sampling trajectories using DDPM with a large number of steps is prohibitive.",
            "In Figures  6  to  14  we collect the MNIST experiments considering every other possible one-vs-all reduction. As mentioned in Section  6.2 , they have near-identical behavior to the experiments of Figure  3 .",
            "Figures  15  to  18  show the results of repeating the experimental setup of Section  6.3  for different choices of the positive class, and also illustrate limitations of this experimental setup in the context of ImageNet."
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "A1.EGx4",
        "footnotes": [],
        "references": [
            "Assume  ln  ( w ) 16  w   1   2  w 16 w subscript  1 subscript  2  \\frac{\\ln(w)}{16\\sqrt{w}}\\leq\\frac{\\alpha_{1}\\alpha_{2}}{\\beta} divide start_ARG roman_ln ( italic_w ) end_ARG start_ARG 16 square-root start_ARG italic_w end_ARG end_ARG  divide start_ARG italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG italic_ end_ARG ,  ln  ( w ) ln  ln  ( w )  1024  (  2  1  1 ) 2 w w 1024 superscript subscript  2 subscript  1 1 2 \\frac{\\ln(w)}{\\ln\\ln(w)}\\geq 1024\\left({\\frac{\\alpha_{2}}{\\alpha_{1}\\wedge 1}}% \\right)^{2} divide start_ARG roman_ln ( italic_w ) end_ARG start_ARG roman_ln roman_ln ( italic_w ) end_ARG  1024 ( divide start_ARG italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  1 end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . Then running the guided ODE with  T  2  log  2 T 2 2 T\\geq 2\\log 2 italic_T  2 roman_log 2  and parameter  w w w italic_w  for the mixture model  p p p italic_p  defined in ( 4 ) under Assumption  1 , with probability at least  1  exp  (  w   1 2 512 /  2 ) 1 w superscript subscript  1 2 superscript 512 2 1-\\exp(-\\frac{w\\alpha_{1}^{2}}{512^{/}2}) 1 - roman_exp ( - divide start_ARG italic_w italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 512 start_POSTSUPERSCRIPT / end_POSTSUPERSCRIPT 2 end_ARG ) , the resulting sample lies in the interval",
            "The formal proof of Theorem  4  which combines all the pieces that we present in this section comes at the end of the section. At a high level, we show that the particle goes through two major phases: first, if we have the condition that the initial point satisfies  x  ( 0 )     (  w ) x 0  w x(0)\\geq-\\Theta(-\\sqrt{w}) italic_x ( 0 )  - roman_ ( - square-root start_ARG italic_w end_ARG ) , then it goes towards the support of  p ( 1 ) superscript p 1 p^{(1)} italic_p start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT  and  swings past it , ending up at position    ( w )  w \\Theta(w) roman_ ( italic_w ) . Then, in the second phase, it comes back to the support of  p ( 1 ) superscript p 1 p^{(1)} italic_p start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT , but by the time it reaches the rightmost edge of the support, it is already close to the end of the process; in particular, we show that it cannot move too much on the support of  p ( 1 ) superscript p 1 p^{(1)} italic_p start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT  once it arrives there. Hence the particle gets stuck near the edge of the support. We formalize this intuition below.",
            "Before proceeding with the proof of Theorem  4 , we present a key algebraic manipulation of the probability flow ODE in ( 7 ) which enables us to analyze it more easily:",
            "is a positive dominant term (compared to the first term) unless we have  q t (  1 ) = O  ( 1 / w ) subscript superscript q 1 t O 1 w q^{(-1)}_{t}=O(1/w) italic_q start_POSTSUPERSCRIPT ( - 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_O ( 1 / italic_w ) , which happens only when  x  ( t ) =   ( ln  ( w ) ) x t  w x(t)=\\Omega(\\ln(w)) italic_x ( italic_t ) = roman_ ( roman_ln ( italic_w ) ) . First, we show that (1) starting from a high probability region for  x  ( 0 ) x 0 x(0) italic_x ( 0 ) , the particle will get to  x  ( t )    ( ln  ( w ) ) x t  w x(t)\\geq\\Omega(\\ln(w)) italic_x ( italic_t )  roman_ ( roman_ln ( italic_w ) )  at some time  t t t italic_t , using the dominance of the second term (Lemma  3 ). Then, in the case where  x  ( t 0 ) =   ( ln  ( w ) ) x subscript t 0  w x(t_{0})=\\Omega(\\ln(w)) italic_x ( italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = roman_ ( roman_ln ( italic_w ) )  for some time  t 0 subscript t 0 t_{0} italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , (2) we show a lower bound on the time that it takes for  x  ( t ) x t x(t) italic_x ( italic_t )  to get back to the proximity of the origin and then an upper bound on how much it can move inside the support (Lemma  5 ). Finally in Lemma  6  we show that  x  ( t ) x t x(t) italic_x ( italic_t )  does converge to the interval, provided the event of Lemma  3  holds. Building upon these Lemmas, we prove the final result in Theorem  4 .",
            "But once we reach the interval  ( a t   1 , a t   2 ) subscript a t subscript  1 subscript a t subscript  2 (a_{t}\\alpha_{1},a_{t}\\alpha_{2}) ( italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) , we can lower bound  x   ( s ) superscript x  s x^{\\prime}(s) italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ( italic_s )  using Lemma  4  In particular, note that  b t  ( s 1 ) 2 = 1  s 1 2  32   2 2 ln  ( w ) superscript subscript b t subscript s 1 2 1 superscript subscript s 1 2 32 superscript subscript  2 2 w b_{t(s_{1})}^{2}=1-s_{1}^{2}\\leq\\frac{32\\alpha_{2}^{2}}{\\ln(w)} italic_b start_POSTSUBSCRIPT italic_t ( italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 1 - italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  divide start_ARG 32 italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG roman_ln ( italic_w ) end_ARG . Hence, we can use Lemma  4  with  c 1 = 8 subscript c 1 8 c_{1}=8 italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 8 , and given that  ln  ( w ) ln  ln  ( w )  1024  (  2  1  1 ) 2 w w 1024 superscript subscript  2 subscript  1 1 2 \\frac{\\ln(w)}{\\ln\\ln(w)}\\geq 1024\\left({\\frac{\\alpha_{2}}{\\alpha_{1}\\wedge 1}}% \\right)^{2} divide start_ARG roman_ln ( italic_w ) end_ARG start_ARG roman_ln roman_ln ( italic_w ) end_ARG  1024 ( divide start_ARG italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  1 end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  the conditions of Lemma  4  are satisfied, so we get",
            "Finally combining the upper bounds on the first and second terms in RHS of the ODE in ( 12 ) that we derived in Equations ( 24 ) and ( 30 ):",
            "Next, combining all the pieces, we prove Theorem  4 .",
            "and let its roots be  s 0  < s 1  subscript superscript s 0 subscript superscript s 1 s^{*}_{0}<s^{*}_{1} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT < italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . First note that  Q  ( 0 ) = c > 0 Q 0 c 0 Q(0)=c>0 italic_Q ( 0 ) = italic_c > 0  and  Q  ( s 0 ) = s 0  x ~  ( s 0 ) + c  0 Q subscript s 0 subscript s 0 ~ x subscript s 0 c 0 Q(s_{0})=s_{0}\\tilde{x}(s_{0})+c\\leq 0 italic_Q ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT over~ start_ARG italic_x end_ARG ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) + italic_c  0 , hence  Q Q Q italic_Q  has a root in the interval  [ 0 , s 0 ] 0 subscript s 0 [0,s_{0}] [ 0 , italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ]  and  s 0   s 0 subscript superscript s 0 subscript s 0 s^{*}_{0}\\leq s_{0} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT . On the other hand,  s 1 > s 0 subscript s 1 subscript s 0 s_{1}>s_{0} italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , so ( 42 ) implies that  s 1 subscript s 1 s_{1} italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  should greater or equal to the larger root  s 1  subscript superscript s 1 s^{*}_{1} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . Now we lower bound  s 1  subscript superscript s 1 s^{*}_{1} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . Defining  G = x ~  ( s 0 )  ( 2  w + 1 )  s 0 2  ( 2  w + 1 ) G ~ x subscript s 0 2 w 1 subscript s 0 2 2 w 1 G=\\frac{\\tilde{x}(s_{0})-(2w+1)s_{0}}{2(2w+1)} italic_G = divide start_ARG over~ start_ARG italic_x end_ARG ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) - ( 2 italic_w + 1 ) italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG start_ARG 2 ( 2 italic_w + 1 ) end_ARG , completing the square for ( 43 ) gives",
            "Note that for the last inequality in ( 44 ) to hold, we need to show",
            "But from  x   w x w x\\leq-w italic_x  - italic_w , we get  (  x ~  ( s 0 )  ( 2  w + 1 )  s 0 ) 2  ( w  2  c ) 2 superscript ~ x subscript s 0 2 w 1 subscript s 0 2 superscript w 2 c 2 \\left({-\\tilde{x}(s_{0})-(2w+1)s_{0}}\\right)^{2}\\geq(w-2c)^{2} ( - over~ start_ARG italic_x end_ARG ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) - ( 2 italic_w + 1 ) italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  ( italic_w - 2 italic_c ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , which follows from the assumption  w  18  c + 2 w 18 c 2 w\\geq 18c+2 italic_w  18 italic_c + 2 . Now based on ( 44 ) and using  x ~  ( s 0 )   w + 2  c ~ x subscript s 0 w 2 c \\tilde{x}(s_{0})\\leq-w+2c over~ start_ARG italic_x end_ARG ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )  - italic_w + 2 italic_c  and  s 0 = c / ( w + 1 )  c subscript s 0 c w 1 c s_{0}=c/(w+1)\\leq c italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = italic_c / ( italic_w + 1 )  italic_c , we have for the larger root  s 1  subscript superscript s 1 s^{*}_{1} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT :",
            "where the last inequality follows from   x  2  w x 2 w -x\\leq 2w - italic_x  2 italic_w . Combining Equations ( 41 ) and ( 45 ), it is enough to show the following to prove  x ~  ( 1 ) > 0 ~ x 1 0 \\tilde{x}(1)>0 over~ start_ARG italic_x end_ARG ( 1 ) > 0 :",
            "The first bound follows from the fact that the numerator of the expression in Eq. ( 47 ) is upper bounded by  2  e  t 2 superscript e t 2e^{-t} 2 italic_e start_POSTSUPERSCRIPT - italic_t end_POSTSUPERSCRIPT  times the denominator.",
            "We can take   1 = 1 subscript  1 1 \\alpha_{1}=1 italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 1 ,   2 = 2 subscript  2 2 \\alpha_{2}=2 italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 2 , and   = 1  1 \\beta=1 italic_ = 1  in the notation of the previous section. Note that  ln  ( w ) 16  w  1 w 16 w 1 \\frac{\\ln(w)}{16\\sqrt{w}}\\leq 1 divide start_ARG roman_ln ( italic_w ) end_ARG start_ARG 16 square-root start_ARG italic_w end_ARG end_ARG  1 , and  p p p italic_p  clearly satisfies Assumption  1 , so the hypotheses of Theorem  4  hold. We can then run through the proof of the Theorem unchanged up to and including Lemma  3 . There, instead of arguing that because we are moving at speed  w   1  = w w subscript  1  w \\frac{\\sqrt{w}\\alpha_{1}}{\\beta}=\\sqrt{w} divide start_ARG square-root start_ARG italic_w end_ARG italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG start_ARG italic_ end_ARG = square-root start_ARG italic_w end_ARG  for time  1 1 1 1  under the reparametrization of time  s s s italic_s , we only move at this speed for time  3 / 4 3 4 3/4 3 / 4 , thus reaching  w / 4 w 4 \\sqrt{w}/4 square-root start_ARG italic_w end_ARG / 4  at some point in this time interval. Because  w / 4  ln  ( w ) / ( 16   2 ) = ln  ( w ) / 32 w 4 w 16 subscript  2 w 32 \\sqrt{w}/4\\geq\\ln(w)/(16\\alpha_{2})=\\ln(w)/32 square-root start_ARG italic_w end_ARG / 4  roman_ln ( italic_w ) / ( 16 italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) = roman_ln ( italic_w ) / 32  for any  w  0 w 0 w\\geq 0 italic_w  0 , the ODE must enter the interval  [ ln  ( w ) / 32 ,  ) w 32 [\\ln(w)/32,\\infty) [ roman_ln ( italic_w ) / 32 ,  ) .",
            "Figure  4  shows the final produced samples alongside the mean projected trajectories for an arbitrarily fixed positive class as in the experiments of Section  6.2 . We see that even for extreme guidance scales  s = 25 s 25 s=25 italic_s = 25  the previously observed non-monotonicity phenomenon in the projected trajectories no longer occurs. We suspect this can largely be attributed to the fact that the class supports are no longer close to separated, and as a result the direction corresponding to the difference in sample means is no longer a direction for which we can expect the dynamics of Theorem  1  (in fact, we can expect that there is no such direction along which these dynamics occur since the data is not linearly separable even after reducing to two classes). However, we note that as we increase the guidance strength, the final sample correlation along this mean difference direction continues to increase, more akin to the result of Theorem  2 .",
            "In tandem with this increasing correlation, we also observe an increase in the mean support error of the final samples, which is overlain on to the trajectory plots in Figure  4 . This error is computed by taking the mean absolute deviation of every dimension of the final produced samples from the range of valid RGB values  [ 0 , 255 ] 0 255 [0,255] [ 0 , 255 ] ; dimensions that are outside of this range are truncated so as to form valid images. We find that, at least qualitatively, the largest guidance value ( s = 5 s 5 s=5 italic_s = 5 ) for which we have no support error seems to perform the best, as taking guidance values larger seems to introduce various visual idiosyncrasies and taking guidance small leads to insufficient concentration on the correct class (as we are guiding an unconditional diffusion model).",
            "In Figures  6  to  14  we collect the MNIST experiments considering every other possible one-vs-all reduction. As mentioned in Section  6.2 , they have near-identical behavior to the experiments of Figure  3 .",
            "In Figures  15  to  17 , we see approximately the same behavior as in  4 . Namely, guidance values for which we have support error lead to distorted samples. Similar to Figure  4 , the choice  s = 5 s 5 s=5 italic_s = 5  works well in Figure  15 . However, for Figures  16  and  17 , we see that we have non-zero support error even for  s = 5 s 5 s=5 italic_s = 5 . In these latter two cases, we expect the qualitatively best choice of guidance to be somewhere between  s = 1 s 1 s=1 italic_s = 1  and  s = 5 s 5 s=5 italic_s = 5 ."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "A1.EGx5",
        "footnotes": [],
        "references": [
            "Our goal is to understand the effect of introducing  guidance  into the probability flow ODE ( 5 ). Given guidance parameter  w w w italic_w , the resulting guided ODE is given by",
            "Note that when  w = 0 w 0 w=0 italic_w = 0 , this is identical to the vanilla probability flow ODE in Eq. ( 5 ) for  z = 1 z 1 z=1 italic_z = 1 . When  w =  1 w 1 w=-1 italic_w = - 1 , then this is identical to the probability flow ODE for the  unconditional distribution   p p p italic_p . Our goal in this work is to understand the behavior of the guided ODE for general  w w w italic_w , especially large  w w w italic_w . In particular, as noted at the outset, the guided score term  ( w + 1 )   log  p t  ( x  ( t ) | z )  w   log  p t  ( x  ( t ) ) w 1  subscript p t conditional x t z w  subscript p t x t (w+1)\\nabla\\log p_{t}(x(t)|z)-w\\nabla\\log p_{t}(x(t)) ( italic_w + 1 )  roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_x ( italic_t ) | italic_z ) - italic_w  roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_x ( italic_t ) )  in Eq. ( 7 ) does not correspond to the score function of the tilted distribution convolved with noise, so it is not  a priori  clear what the distribution over the final iterate  x  ( T ) x T x(T) italic_x ( italic_T )  actually is.",
            "is a positive dominant term (compared to the first term) unless we have  q t (  1 ) = O  ( 1 / w ) subscript superscript q 1 t O 1 w q^{(-1)}_{t}=O(1/w) italic_q start_POSTSUPERSCRIPT ( - 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_O ( 1 / italic_w ) , which happens only when  x  ( t ) =   ( ln  ( w ) ) x t  w x(t)=\\Omega(\\ln(w)) italic_x ( italic_t ) = roman_ ( roman_ln ( italic_w ) ) . First, we show that (1) starting from a high probability region for  x  ( 0 ) x 0 x(0) italic_x ( 0 ) , the particle will get to  x  ( t )    ( ln  ( w ) ) x t  w x(t)\\geq\\Omega(\\ln(w)) italic_x ( italic_t )  roman_ ( roman_ln ( italic_w ) )  at some time  t t t italic_t , using the dominance of the second term (Lemma  3 ). Then, in the case where  x  ( t 0 ) =   ( ln  ( w ) ) x subscript t 0  w x(t_{0})=\\Omega(\\ln(w)) italic_x ( italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = roman_ ( roman_ln ( italic_w ) )  for some time  t 0 subscript t 0 t_{0} italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , (2) we show a lower bound on the time that it takes for  x  ( t ) x t x(t) italic_x ( italic_t )  to get back to the proximity of the origin and then an upper bound on how much it can move inside the support (Lemma  5 ). Finally in Lemma  6  we show that  x  ( t ) x t x(t) italic_x ( italic_t )  does converge to the interval, provided the event of Lemma  3  holds. Building upon these Lemmas, we prove the final result in Theorem  4 .",
            "Note that using ( 25 ) and ( 27 ), we have",
            "Let  s 0 subscript s 0 s_{0} italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  be the minimum such time. Now plugging this into Lemma  5  then implies for all  s  s 0 s subscript s 0 s\\geq s_{0} italic_s  italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT",
            "The second bound in Theorem  5  gives a qualitatively stronger guarantee at the cost of a weaker high-probability bound: not only is the overwhelming majority of the output distribution concentrated on positive values, but most of that mass is concentrated on values at least    ( w )  w \\Omega(\\sqrt{w}) roman_ ( square-root start_ARG italic_w end_ARG ) .",
            "The proof of Theorem  5  is based on two key results which break down the behavior of the guided ODE dynamics into two cases depending on where the initialization lies.",
            "First, Lemma  7  below controls how far  x ~  ( 1 ) ~ x 1 \\tilde{x}(1) over~ start_ARG italic_x end_ARG ( 1 )  moves to the right when the initialization  x ~  ( 0 ) = x ~ x 0 x \\tilde{x}(0)=x over~ start_ARG italic_x end_ARG ( 0 ) = italic_x  is in the interval  [  2  w +   ( ln  w ) , 0 ] 2 w  w 0 [-2w+\\Theta(\\ln w),0] [ - 2 italic_w + roman_ ( roman_ln italic_w ) , 0 ] . This gives rise to the first bound in Theorem  5 . Lemma  8  below handles the case when the initialization is in the interval  [    ( w ) , 0 ]  w 0 [-\\Theta(\\sqrt{w}),0] [ - roman_ ( square-root start_ARG italic_w end_ARG ) , 0 ] , giving rise to the second bound in Theorem  5 . In the first case, we show that the movement of  x ~  ( t ) ~ x t \\tilde{x}(t) over~ start_ARG italic_x end_ARG ( italic_t )  can be as large as    ( w )  w \\Theta(w) roman_ ( italic_w )  while in the second lemma we show movement of order    ( w )  w \\Theta(\\sqrt{w}) roman_ ( square-root start_ARG italic_w end_ARG ) .",
            "On the other hand, note that from ( 35 ),  x ~   ( s )  ( 2  w + 1 ) superscript ~ x  s 2 w 1 \\tilde{x}^{\\prime}(s)\\leq(2w+1) over~ start_ARG italic_x end_ARG start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ( italic_s )  ( 2 italic_w + 1 ) , we have  x ~  ( s )  x ~  ( s 0 ) + ( 2  w + 1 )  ( s  s 0 ) ~ x s ~ x subscript s 0 2 w 1 s subscript s 0 \\tilde{x}(s)\\leq\\tilde{x}(s_{0})+(2w+1)(s-s_{0}) over~ start_ARG italic_x end_ARG ( italic_s )  over~ start_ARG italic_x end_ARG ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) + ( 2 italic_w + 1 ) ( italic_s - italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) . In particular, from the definition of  s 1 subscript s 1 s_{1} italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,",
            "where the last inequality follows from   x  2  w x 2 w -x\\leq 2w - italic_x  2 italic_w . Combining Equations ( 41 ) and ( 45 ), it is enough to show the following to prove  x ~  ( 1 ) > 0 ~ x 1 0 \\tilde{x}(1)>0 over~ start_ARG italic_x end_ARG ( 1 ) > 0 :",
            "If the guided ODE is run with the score estimates in Eq. ( 52 ), then as soon as the process enters the interval  [ R ,  ) R [R,\\infty) [ italic_R ,  ) , it no longer moves. The reason is that the within this interval, the guided ODE is given by",
            "We once again plot the mean of the probability flow ODE trajectories (projected on to the  x x x italic_x -coordinate) for increasing guidance parameter values alongside the final produced samples from each trajectory in Figure  5 . The figure is analogous to Figure  2 , except for larger choices of the guidance parameter  w w w italic_w  and the fact that the proportion of good samples here is the proportion of samples that did not result in NaNs (since we are no longer in the compact support setting).",
            "We use larger  w w w italic_w  values to better illustrate the behavior predicted in Theorem  2 . As can be seen from Figure  5  (a), we produce more samples with larger (positive)  x x x italic_x -coordinates as we increase  w w w italic_w . However, we also get significantly more numerical instability, and as a result the mean trajectory plot in Figure  5  (b) is much less meaningful than it was in Figure  2 .",
            "Figures  15  to  18  show the results of repeating the experimental setup of Section  6.3  for different choices of the positive class, and also illustrate limitations of this experimental setup in the context of ImageNet.",
            "In Figures  15  to  17 , we see approximately the same behavior as in  4 . Namely, guidance values for which we have support error lead to distorted samples. Similar to Figure  4 , the choice  s = 5 s 5 s=5 italic_s = 5  works well in Figure  15 . However, for Figures  16  and  17 , we see that we have non-zero support error even for  s = 5 s 5 s=5 italic_s = 5 . In these latter two cases, we expect the qualitatively best choice of guidance to be somewhere between  s = 1 s 1 s=1 italic_s = 1  and  s = 5 s 5 s=5 italic_s = 5 ."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "A1.EGx6",
        "footnotes": [],
        "references": [
            "Taking inspiration from our theory, we posit that the optimal choice of guidance (from the perspective of sample quality) for compactly supported distributions that approximately satisfy the assumptions of our theory is the largest possible  w w w italic_w  for which the resulting trajectory does not exhibit this behavior of swinging away from the support of the data distribution and returning. Specifically, we propose a rule of thumb for selecting the guidance strength based on looking at a certain  monotonicity  property of the trajectory and experimentally validate this rule of thumb in both synthetic settings and on image classification datasets. Additionally, for compactly supported distributions that fall outside the scope of Theorem  1 , we propose an alternative heuristic based on the ideas of Theorem  3 : we should choose the guidance strength as large as possible while still ensuring that final samples are contained within the distribution support. See Section  6  for details.",
            "is a positive dominant term (compared to the first term) unless we have  q t (  1 ) = O  ( 1 / w ) subscript superscript q 1 t O 1 w q^{(-1)}_{t}=O(1/w) italic_q start_POSTSUPERSCRIPT ( - 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_O ( 1 / italic_w ) , which happens only when  x  ( t ) =   ( ln  ( w ) ) x t  w x(t)=\\Omega(\\ln(w)) italic_x ( italic_t ) = roman_ ( roman_ln ( italic_w ) ) . First, we show that (1) starting from a high probability region for  x  ( 0 ) x 0 x(0) italic_x ( 0 ) , the particle will get to  x  ( t )    ( ln  ( w ) ) x t  w x(t)\\geq\\Omega(\\ln(w)) italic_x ( italic_t )  roman_ ( roman_ln ( italic_w ) )  at some time  t t t italic_t , using the dominance of the second term (Lemma  3 ). Then, in the case where  x  ( t 0 ) =   ( ln  ( w ) ) x subscript t 0  w x(t_{0})=\\Omega(\\ln(w)) italic_x ( italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = roman_ ( roman_ln ( italic_w ) )  for some time  t 0 subscript t 0 t_{0} italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , (2) we show a lower bound on the time that it takes for  x  ( t ) x t x(t) italic_x ( italic_t )  to get back to the proximity of the origin and then an upper bound on how much it can move inside the support (Lemma  5 ). Finally in Lemma  6  we show that  x  ( t ) x t x(t) italic_x ( italic_t )  does converge to the interval, provided the event of Lemma  3  holds. Building upon these Lemmas, we prove the final result in Theorem  4 .",
            "where the last inequality is due to the fact that  ln  ( w )  16  c 1 2 w 16 superscript subscript c 1 2 \\ln(w)\\geq 16c_{1}^{2} roman_ln ( italic_w )  16 italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . Combining this with Equation ( 17 ), we can bound the first term in Equation ( 16 ):",
            "On the other hand, for the second term in Equation ( 16 ), we can upper bound the numerator as",
            "Combining this with Equation ( 20 ), we obtain the following upper bound for the second term in Equation ( 16 ):",
            "Now take  s ~ 0  s 0  3 4 subscript ~ s 0 subscript s 0 3 4 \\tilde{s}_{0}\\geq s_{0}\\vee\\frac{3}{4} over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  divide start_ARG 3 end_ARG start_ARG 4 end_ARG . Then we can use Lemma  6  with  s 0 = s ~ 0 subscript s 0 subscript ~ s 0 s_{0}=\\tilde{s}_{0} italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  because   2  ( 1  32 ln  ( w ) )   1 +  2 2 subscript  2 1 32 w subscript  1 subscript  2 2 \\alpha_{2}\\left({1-\\frac{32}{\\sqrt{\\ln(w)}}}\\right)\\geq\\frac{\\alpha_{1}+\\alpha% _{2}}{2} italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( 1 - divide start_ARG 32 end_ARG start_ARG square-root start_ARG roman_ln ( italic_w ) end_ARG end_ARG )  divide start_ARG italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG  so its condition is satisfied from ( 32 ); then, Lemma  6  implies that there exists a time  s 1 subscript s 1 s_{1} italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  such that for all  s  [ s 1 , 1 ] s subscript s 1 1 s\\in[s_{1},1] italic_s  [ italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , 1 ] :",
            "Here we empirically verify the guidance dynamics predicted by Theorems  1  and  2 . All experiments in this section were conducted on a single A5000 GPU. We use Jax  [ 2 ]  for the experiments in Section  6.1  and PyTorch  [ 28 ]  for all other experiments.",
            "Both ( 61 ) and ( 63 ) follow from rewriting the convolutions as expectations and then using dominated convergence to pass the gradient into the expectations. Using the above, we can compute the scores by standard Monte-Carlo.",
            "Although MNIST is perhaps the simplest generative image modeling testbed, it still presents a significant increase in complexity from the synthetic setting. Firstly, compared to the experiments of Section  6.1  and the setting of our theory, we are no longer considering only two classes. Furthermore, there is no guarantee that the class supports are well-separated, or even disjoint. Even more worrying, we do not have access to approximations of the true score functions of the conditional distributions that are guaranteed to be close as in ( 61 ) and ( 63 ); we have to instead learn a model-based score.",
            "To obtain a projection direction for the sampling dynamics analogous to what was done in Section  6.1 , we generate 100 samples from the positive and negative classes using a guidance of  w = 0 w 0 w=0 italic_w = 0 , to approximate sampling from the conditional distributions. We then let the projection direction be the difference between the two sample means. For sampling, we use DDPM  [ 19 ]  with 400 time steps and a linear noise schedule, and we found that training the guidance model of  [ 29 ]  for 40 epochs was sufficient to generate high quality samples.",
            "To conduct experiments on ImageNet, we use the  classifier-guided  ImageNet models available from  [ 13 ] . This is due to the fact that there are no classifier-free guidance models available from  [ 20 ] . The classifier-guidance formulation corresponds to the first equality in ( 1 ). To be consistent with the notation in  [ 13 ]  and to also clearly distinguish the classifier-guided setting from the classifier-free setting of Section  6.2 , we will use  s = 1 + w s 1 w s=1+w italic_s = 1 + italic_w  throughout the experiments in this section.",
            "First, we illustrate that the behavior exhibited in Figure  3  no longer holds when running diffusion with guidance on ImageNet, at least using the same experimental setup as before. To parallel the experiments of Section  6.2 , we use the  256  256 256 256 256\\times 256 256  256   conditional  diffusion model released by  [ 13 ]  to generate samples from a fixed ImageNet class (corresponding to  y = + 1 y 1 y=+1 italic_y = + 1  as before), and then use the same model to generate samples from all other classes (corresponding to  y =  1 y 1 y=-1 italic_y = - 1 ). We generate 50 samples from the positive and negative classes (due to the cost of sampling at this resolution and the overhead of storing the entire sampling trajectories), and then compute the normalized direction between the two sample means as before. 2 2 2 We should note that, in contrast to the MNIST experiments, this choice of direction is very noisy in the ImageNet setting. However, we use the same setup as before both for consistency and to show that Theorem  3  can be applied even in this imperfect experimental setting.  For sampling, we use DDIM  [ 33 ]  with 25 steps, once again because storing the entire sampling trajectories using DDPM with a large number of steps is prohibitive.",
            "Figure  4  shows the final produced samples alongside the mean projected trajectories for an arbitrarily fixed positive class as in the experiments of Section  6.2 . We see that even for extreme guidance scales  s = 25 s 25 s=25 italic_s = 25  the previously observed non-monotonicity phenomenon in the projected trajectories no longer occurs. We suspect this can largely be attributed to the fact that the class supports are no longer close to separated, and as a result the direction corresponding to the difference in sample means is no longer a direction for which we can expect the dynamics of Theorem  1  (in fact, we can expect that there is no such direction along which these dynamics occur since the data is not linearly separable even after reducing to two classes). However, we note that as we increase the guidance strength, the final sample correlation along this mean difference direction continues to increase, more akin to the result of Theorem  2 .",
            "Here we consider a 2-D version of the mixture distribution of Theorem  2 , i.e.  p ( 1 ) = N  ( 1 , 1 )  N  ( 0 , 1 ) superscript p 1 tensor-product N 1 1 N 0 1 p^{(1)}=N(1,1)\\otimes N(0,1) italic_p start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT = italic_N ( 1 , 1 )  italic_N ( 0 , 1 )  and  p (  1 ) = N  (  1 , 1 )  N  ( 0 , 1 ) superscript p 1 tensor-product N 1 1 N 0 1 p^{(-1)}=N(-1,1)\\otimes N(0,1) italic_p start_POSTSUPERSCRIPT ( - 1 ) end_POSTSUPERSCRIPT = italic_N ( - 1 , 1 )  italic_N ( 0 , 1 ) . We follow the exact same experimental setup as Section  6.1  and generate 500 samples from the conditional distribution  p  ( x  z = + 1 ) p conditional x z 1 p(x\\mid z=+1) italic_p ( italic_x  italic_z = + 1 )  with varying levels of guidance.",
            "In Figures  6  to  14  we collect the MNIST experiments considering every other possible one-vs-all reduction. As mentioned in Section  6.2 , they have near-identical behavior to the experiments of Figure  3 .",
            "Figures  15  to  18  show the results of repeating the experimental setup of Section  6.3  for different choices of the positive class, and also illustrate limitations of this experimental setup in the context of ImageNet.",
            "In Figures  15  to  17 , we see approximately the same behavior as in  4 . Namely, guidance values for which we have support error lead to distorted samples. Similar to Figure  4 , the choice  s = 5 s 5 s=5 italic_s = 5  works well in Figure  15 . However, for Figures  16  and  17 , we see that we have non-zero support error even for  s = 5 s 5 s=5 italic_s = 5 . In these latter two cases, we expect the qualitatively best choice of guidance to be somewhere between  s = 1 s 1 s=1 italic_s = 1  and  s = 5 s 5 s=5 italic_s = 5 ."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "A1.EGx7",
        "footnotes": [],
        "references": [
            "Note that when  w = 0 w 0 w=0 italic_w = 0 , this is identical to the vanilla probability flow ODE in Eq. ( 5 ) for  z = 1 z 1 z=1 italic_z = 1 . When  w =  1 w 1 w=-1 italic_w = - 1 , then this is identical to the probability flow ODE for the  unconditional distribution   p p p italic_p . Our goal in this work is to understand the behavior of the guided ODE for general  w w w italic_w , especially large  w w w italic_w . In particular, as noted at the outset, the guided score term  ( w + 1 )   log  p t  ( x  ( t ) | z )  w   log  p t  ( x  ( t ) ) w 1  subscript p t conditional x t z w  subscript p t x t (w+1)\\nabla\\log p_{t}(x(t)|z)-w\\nabla\\log p_{t}(x(t)) ( italic_w + 1 )  roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_x ( italic_t ) | italic_z ) - italic_w  roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_x ( italic_t ) )  in Eq. ( 7 ) does not correspond to the score function of the tilted distribution convolved with noise, so it is not  a priori  clear what the distribution over the final iterate  x  ( T ) x T x(T) italic_x ( italic_T )  actually is.",
            "First, when the posterior probability  p t  ( z =  1 | x  ( t ) ) subscript p t z conditional 1 x t p_{t}(z=-1|x(t)) italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z = - 1 | italic_x ( italic_t ) )  is much larger than  p t  ( z = 1 | x  ( t ) ) subscript p t z conditional 1 x t p_{t}(z=1|x(t)) italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z = 1 | italic_x ( italic_t ) ) , then the score function of the convolved mixture model is dominated by the score of  p (  1 ) superscript p 1 p^{(-1)} italic_p start_POSTSUPERSCRIPT ( - 1 ) end_POSTSUPERSCRIPT  convolved with the appropriate Gaussian; in particular, the guided score term in Eq. ( 7 ) is almost",
            "The second case is when the posterior probabilities  p t  ( z = 1 | x  ( t ) ) subscript p t z conditional 1 x t p_{t}(z=1|x(t)) italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z = 1 | italic_x ( italic_t ) )  and  p t  ( z =  1 | x  ( t ) ) subscript p t z conditional 1 x t p_{t}(z=-1|x(t)) italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_z = - 1 | italic_x ( italic_t ) )  are approximately equal. In this case, the score of the convolved mixture is almost zero since the influences from  p ( 1 ) superscript p 1 p^{(1)} italic_p start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT  and  p (  1 ) superscript p 1 p^{(-1)} italic_p start_POSTSUPERSCRIPT ( - 1 ) end_POSTSUPERSCRIPT  cancel each other out. Hence, the guided score term in ( 7 ) roughly becomes",
            "Before proceeding with the proof of Theorem  4 , we present a key algebraic manipulation of the probability flow ODE in ( 7 ) which enables us to analyze it more easily:",
            "Note that combining ( 7 ) and ( 10 ), we can write",
            "where the last inequality is due to the fact that  ln  ( w )  16  c 1 2 w 16 superscript subscript c 1 2 \\ln(w)\\geq 16c_{1}^{2} roman_ln ( italic_w )  16 italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . Combining this with Equation ( 17 ), we can bound the first term in Equation ( 16 ):",
            "Note that using ( 25 ) and ( 27 ), we have",
            "But we can solve the ODE in ( 27 ):",
            "For  w w w italic_w  larger than some absolute constant (see Lemmas  7  and  8 ), running the guided ODE with parameter  w w w italic_w  for the mixture  1 2  N  ( 1 , 1 ) + 1 2  N  (  1 , 1 ) 1 2 N 1 1 1 2 N 1 1 \\frac{1}{2}\\mathcal{N}(1,1)+\\frac{1}{2}\\mathcal{N}(-1,1) divide start_ARG 1 end_ARG start_ARG 2 end_ARG caligraphic_N ( 1 , 1 ) + divide start_ARG 1 end_ARG start_ARG 2 end_ARG caligraphic_N ( - 1 , 1 )  results in a sample  x ~  ( 1 ) ~ x 1 \\tilde{x}(1) over~ start_ARG italic_x end_ARG ( 1 )  for which",
            "We begin by performing some preliminary calculations to simplify the guided ODE, culminating in the simplified expression in Equation ( 37 ) below.",
            "where  x  ( 0 )  N  ( 0 , I ) similar-to x 0 N 0 I x(0)\\sim N(0,I) italic_x ( 0 )  italic_N ( 0 , italic_I )  and  t t t italic_t  varies in  [ 0 , T ] 0 T [0,T] [ 0 , italic_T ] . For simplicity we assume   =  0 = 1  subscript  0 1 \\mu=\\sigma_{0}=1 italic_ = italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 1 . Then, writing the guidance equation for this mixture in the form ( 7 ):",
            "where  x  N  ( 0 , 1 ) similar-to x N 0 1 x\\sim\\mathcal{N}(0,1) italic_x  caligraphic_N ( 0 , 1 ) . Below, we study the behavior of the ODE in ( 37 ) for different initial conditions  x ~  ( 0 ) = x ~ x 0 x \\tilde{x}(0)=x over~ start_ARG italic_x end_ARG ( 0 ) = italic_x .",
            "First, Lemma  7  below controls how far  x ~  ( 1 ) ~ x 1 \\tilde{x}(1) over~ start_ARG italic_x end_ARG ( 1 )  moves to the right when the initialization  x ~  ( 0 ) = x ~ x 0 x \\tilde{x}(0)=x over~ start_ARG italic_x end_ARG ( 0 ) = italic_x  is in the interval  [  2  w +   ( ln  w ) , 0 ] 2 w  w 0 [-2w+\\Theta(\\ln w),0] [ - 2 italic_w + roman_ ( roman_ln italic_w ) , 0 ] . This gives rise to the first bound in Theorem  5 . Lemma  8  below handles the case when the initialization is in the interval  [    ( w ) , 0 ]  w 0 [-\\Theta(\\sqrt{w}),0] [ - roman_ ( square-root start_ARG italic_w end_ARG ) , 0 ] , giving rise to the second bound in Theorem  5 . In the first case, we show that the movement of  x ~  ( t ) ~ x t \\tilde{x}(t) over~ start_ARG italic_x end_ARG ( italic_t )  can be as large as    ( w )  w \\Theta(w) roman_ ( italic_w )  while in the second lemma we show movement of order    ( w )  w \\Theta(\\sqrt{w}) roman_ ( square-root start_ARG italic_w end_ARG ) .",
            "Before proceeding with the proof, we note that up to the    ( ln  w )  w \\Theta(\\ln w) roman_ ( roman_ln italic_w )  term, this analysis is nearly tight in the regime of large  w w w italic_w . The reason is that if  x ~  ( 0 ) <  2  w  1 ~ x 0 2 w 1 \\tilde{x}(0)<-2w-1 over~ start_ARG italic_x end_ARG ( 0 ) < - 2 italic_w - 1 , then because the velocity in Equation ( 37 ) is upper bounded by  2  w + 1 2 w 1 2w+1 2 italic_w + 1  at all times, the particle will remain negative at all times.",
            "First, note that the right-hand side of Equation ( 37 ) is lower bounded by  1 1 1 1 , so if  x ~  ( 0 )  0 ~ x 0 0 \\tilde{x}(0)\\geq 0 over~ start_ARG italic_x end_ARG ( 0 )  0 , then  x ~  ( 1 )  x ~  ( 0 ) + w  0 ~ x 1 ~ x 0 w 0 \\tilde{x}(1)\\geq\\tilde{x}(0)+w\\geq 0 over~ start_ARG italic_x end_ARG ( 1 )  over~ start_ARG italic_x end_ARG ( 0 ) + italic_w  0 . More generally, this implies that as soon as the particle becomes positive, it continues to move to the right at rate lower bounded by  1 1 1 1 .",
            "In the following we upper bound  s 1 subscript s 1 s_{1} italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . Defining   italic- \\epsilon italic_  by   / w = 1 + tanh  (  c ) italic- w 1 c \\epsilon/w=1+\\tanh(-c) italic_ / italic_w = 1 + roman_tanh ( - italic_c ) , we see from the definition of ODE ( 37 ) that for all  s 0  s  s 1 subscript s 0 s subscript s 1 s_{0}\\leq s\\leq s_{1} italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  italic_s  italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT :",
            "The first inequality follows from Lemma  7  as for the standard normal we have  P  ( x   2  w + ln  ( w ) )  e    ( w 2 ) P x 2 w w superscript e  superscript w 2 \\mathbb{P}(x\\leq-2w+\\ln(w))\\leq e^{-\\Theta(w^{2})} blackboard_P ( italic_x  - 2 italic_w + roman_ln ( italic_w ) )  italic_e start_POSTSUPERSCRIPT - roman_ ( italic_w start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_POSTSUPERSCRIPT . The second inequality follows from Lemma  8 , as  P  ( x   w + 1 / 4 )  e    ( w ) P x w 1 4 superscript e  w \\mathbb{P}(x\\leq-\\sqrt{w+1}/4)\\leq e^{-\\Theta(w)} blackboard_P ( italic_x  - square-root start_ARG italic_w + 1 end_ARG / 4 )  italic_e start_POSTSUPERSCRIPT - roman_ ( italic_w ) end_POSTSUPERSCRIPT .",
            "The first bound follows from the fact that the numerator of the expression in Eq. ( 47 ) is upper bounded by  2  e  t 2 superscript e t 2e^{-t} 2 italic_e start_POSTSUPERSCRIPT - italic_t end_POSTSUPERSCRIPT  times the denominator.",
            "We generate samples using guidance by numerically solving the guided probability flow ODE ( 7 ) using the Dormand-Prince method  [ 14 ]  as implemented in JAX  [ 2 ] . For solving, we use  1000 1000 1000 1000  evaluation steps and take  T = 10 T 10 T=10 italic_T = 10 , which we is sufficiently large based on the stipulations of Theorem  1 . For obtaining the unconditional and conditional scores necessary for the ODE, it is straightforward to write down exact expressions for this case (which consist of integrals that we can numerically approximate). However, we estimate the scores using a more general approach that can be effectively applied to any mixture distribution for which we can sample both conditionally and unconditionally from.",
            "In Figures  15  to  17 , we see approximately the same behavior as in  4 . Namely, guidance values for which we have support error lead to distorted samples. Similar to Figure  4 , the choice  s = 5 s 5 s=5 italic_s = 5  works well in Figure  15 . However, for Figures  16  and  17 , we see that we have non-zero support error even for  s = 5 s 5 s=5 italic_s = 5 . In these latter two cases, we expect the qualitatively best choice of guidance to be somewhere between  s = 1 s 1 s=1 italic_s = 1  and  s = 5 s 5 s=5 italic_s = 5 ."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "A1.EGx8",
        "footnotes": [],
        "references": [
            "First comparing Equations ( 12 ) and ( 28 ), since the second term in the RHS of the ODE in ( 12 ) is always positive, from ODE comparison theorem we get  B  ( s )  x ~  ( s 0 ) + A  ( s ) B s ~ x subscript s 0 A s B(s)\\geq\\tilde{x}(s_{0})+A(s) italic_B ( italic_s )  over~ start_ARG italic_x end_ARG ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) + italic_A ( italic_s ) .",
            "For  w w w italic_w  larger than some absolute constant (see Lemmas  7  and  8 ), running the guided ODE with parameter  w w w italic_w  for the mixture  1 2  N  ( 1 , 1 ) + 1 2  N  (  1 , 1 ) 1 2 N 1 1 1 2 N 1 1 \\frac{1}{2}\\mathcal{N}(1,1)+\\frac{1}{2}\\mathcal{N}(-1,1) divide start_ARG 1 end_ARG start_ARG 2 end_ARG caligraphic_N ( 1 , 1 ) + divide start_ARG 1 end_ARG start_ARG 2 end_ARG caligraphic_N ( - 1 , 1 )  results in a sample  x ~  ( 1 ) ~ x 1 \\tilde{x}(1) over~ start_ARG italic_x end_ARG ( 1 )  for which",
            "First, Lemma  7  below controls how far  x ~  ( 1 ) ~ x 1 \\tilde{x}(1) over~ start_ARG italic_x end_ARG ( 1 )  moves to the right when the initialization  x ~  ( 0 ) = x ~ x 0 x \\tilde{x}(0)=x over~ start_ARG italic_x end_ARG ( 0 ) = italic_x  is in the interval  [  2  w +   ( ln  w ) , 0 ] 2 w  w 0 [-2w+\\Theta(\\ln w),0] [ - 2 italic_w + roman_ ( roman_ln italic_w ) , 0 ] . This gives rise to the first bound in Theorem  5 . Lemma  8  below handles the case when the initialization is in the interval  [    ( w ) , 0 ]  w 0 [-\\Theta(\\sqrt{w}),0] [ - roman_ ( square-root start_ARG italic_w end_ARG ) , 0 ] , giving rise to the second bound in Theorem  5 . In the first case, we show that the movement of  x ~  ( t ) ~ x t \\tilde{x}(t) over~ start_ARG italic_x end_ARG ( italic_t )  can be as large as    ( w )  w \\Theta(w) roman_ ( italic_w )  while in the second lemma we show movement of order    ( w )  w \\Theta(\\sqrt{w}) roman_ ( square-root start_ARG italic_w end_ARG ) .",
            "Therefore, from ( 38 ),",
            "The first inequality follows from Lemma  7  as for the standard normal we have  P  ( x   2  w + ln  ( w ) )  e    ( w 2 ) P x 2 w w superscript e  superscript w 2 \\mathbb{P}(x\\leq-2w+\\ln(w))\\leq e^{-\\Theta(w^{2})} blackboard_P ( italic_x  - 2 italic_w + roman_ln ( italic_w ) )  italic_e start_POSTSUPERSCRIPT - roman_ ( italic_w start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_POSTSUPERSCRIPT . The second inequality follows from Lemma  8 , as  P  ( x   w + 1 / 4 )  e    ( w ) P x w 1 4 superscript e  w \\mathbb{P}(x\\leq-\\sqrt{w+1}/4)\\leq e^{-\\Theta(w)} blackboard_P ( italic_x  - square-root start_ARG italic_w + 1 end_ARG / 4 )  italic_e start_POSTSUPERSCRIPT - roman_ ( italic_w ) end_POSTSUPERSCRIPT .",
            "Figures  15  to  18  show the results of repeating the experimental setup of Section  6.3  for different choices of the positive class, and also illustrate limitations of this experimental setup in the context of ImageNet.",
            "Figure  18  demonstrates some limitations of our experimental setup for classes which have high levels of noise/variance. Indeed, we see that for the basketball class the support error is not even monotonically increasing with the guidance parameter, and that sample quality is poor across guidance levels. Furthermore, the projected trajectories become progressively more negative as opposed to positive, indicating that the direction between the means of the positive class samples and the negative class samples is likely useless in this case. Despite these various issues, there appears to still be some positive correlation between support error and sample distortion."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "A1.EGx9",
        "footnotes": [],
        "references": [
            "Therefore, combining with Equation ( 19 ),",
            "Note that inequality ( 29 ) is valid more generally up to time  s 1  subscript superscript s  1 s^{\\prime}_{1} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , when  B  ( s ) B s B(s) italic_B ( italic_s )  reaches  1 2  s 0  1   2   2 = 2  ( 1  s 0 ) 1  2  ( 1  s 0 )   2 1 2 subscript s 0 1 subscript  2 subscript  2 2 1 subscript s 0 1 2 1 subscript s 0 subscript  2 \\frac{1}{2s_{0}-1}\\alpha_{2}-\\alpha_{2}=\\frac{2\\left({1-s_{0}}\\right)}{1-2(1-s% _{0})}\\alpha_{2} divide start_ARG 1 end_ARG start_ARG 2 italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - 1 end_ARG italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = divide start_ARG 2 ( 1 - italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_ARG start_ARG 1 - 2 ( 1 - italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_ARG italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT . Now we solve for the value of  s ~ 1 subscript ~ s 1 \\tilde{s}_{1} over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  when  d  ( s ~ 1 ) d subscript ~ s 1 d(\\tilde{s}_{1}) italic_d ( over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT )  reaches  2  ( 1  s 0 ) 1  2  ( 1  s 0 )   2 2 1 subscript s 0 1 2 1 subscript s 0 subscript  2 \\frac{2\\left({1-s_{0}}\\right)}{1-2(1-s_{0})}\\alpha_{2} divide start_ARG 2 ( 1 - italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_ARG start_ARG 1 - 2 ( 1 - italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_ARG italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , which upper bounds  s 1  subscript superscript s  1 s^{\\prime}_{1} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  due to ( 29 ):",
            "and from definition for this choice of  s ~ 1 subscript ~ s 1 \\tilde{s}_{1} over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  we get from Equation ( 29 )"
        ]
    },
    "id_table_10": {
        "caption": "",
        "table": "A1.EGx10",
        "footnotes": [],
        "references": [
            "Note that combining ( 7 ) and ( 10 ), we can write",
            "By Lemma  10 , if we take  R  ln  ( w ) / 32  R w 32 R\\triangleq\\ln(w)/32 italic_R  roman_ln ( italic_w ) / 32 , then with probability  1  e  w / 8 1 superscript e w 8 1-e^{-w/8} 1 - italic_e start_POSTSUPERSCRIPT - italic_w / 8 end_POSTSUPERSCRIPT , the guided ODE will reach the (boundary of the) interval  [ R ,  ) R [R,\\infty) [ italic_R ,  )  at least time  t = ln  ( 3 / 4 ) t 3 4 t=\\ln(3/4) italic_t = roman_ln ( 3 / 4 )  before the end of the reverse process and then remain frozen at  R R R italic_R , as claimed."
        ]
    },
    "id_table_11": {
        "caption": "",
        "table": "A1.EGx11",
        "footnotes": [],
        "references": [
            "We begin by sketching our argument in greater detail. First note that the second term in ( 11 ), namely",
            "Therefore, for the second term in Equation ( 11 ) we have",
            "Plugging this into Equation ( 11 ) and using the fact that  x  ( t )   1  w 2  missing / missing x t subscript  1 w 2 missing missing x(t)\\leq\\frac{\\alpha_{1}\\sqrt{w}}{2missing/missing} italic_x ( italic_t )  divide start_ARG italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT square-root start_ARG italic_w end_ARG end_ARG start_ARG 2 roman_missing / roman_missing end_ARG  and  a t  1 subscript a t 1 a_{t}\\leq 1 italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  1 , we get",
            "Next, we show that starting from  x ~  ( s 0 )     ( w ) ~ x subscript s 0  w \\tilde{x}(s_{0})\\geq-\\Theta(\\sqrt{w}) over~ start_ARG italic_x end_ARG ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )  - roman_ ( square-root start_ARG italic_w end_ARG ) , the particle  x ~  ( s ) ~ x s \\tilde{x}(s) over~ start_ARG italic_x end_ARG ( italic_s )  reaches at least    ( ln  (  ) )   \\Theta(\\ln(\\omega)) roman_ ( roman_ln ( italic_ ) )  before time 1. This results from the strong acceleration force toward the right in this phase, coming from the dominance of the aforementioned second term in ( 11 ).",
            "Next, we show that when the process gets close to the end, if the particle is on the right side of both of the intervals, then the first term in Equation ( 11 ) will dominate the second term and the particle is most likely to converge to some point in the interval  (  1 ,  2 ) subscript  1 subscript  2 (\\alpha_{1},\\alpha_{2}) ( italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) ."
        ]
    },
    "id_table_12": {
        "caption": "",
        "table": "A1.EGx12",
        "footnotes": [],
        "references": [
            "where recall  s ( 0 ) = e  T superscript s 0 superscript e T s^{(0)}=e^{-T} italic_s start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT = italic_e start_POSTSUPERSCRIPT - italic_T end_POSTSUPERSCRIPT  is the initial time for the ODE ( 12 ).",
            "From Equation ( 12 ), as long as  x ~  ( s )  0 ~ x s 0 \\tilde{x}(s)\\geq 0 over~ start_ARG italic_x end_ARG ( italic_s )  0 , we have",
            "On the other hand, for the first term in Equation ( 12 ), as long as  x ~  ( s )  1 2  s 0  1   2 ~ x s 1 2 subscript s 0 1 subscript  2 \\tilde{x}(s)\\geq\\frac{1}{2s_{0}-1}\\alpha_{2} over~ start_ARG italic_x end_ARG ( italic_s )  divide start_ARG 1 end_ARG start_ARG 2 italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - 1 end_ARG italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , we get",
            "First comparing Equations ( 12 ) and ( 28 ), since the second term in the RHS of the ODE in ( 12 ) is always positive, from ODE comparison theorem we get  B  ( s )  x ~  ( s 0 ) + A  ( s ) B s ~ x subscript s 0 A s B(s)\\geq\\tilde{x}(s_{0})+A(s) italic_B ( italic_s )  over~ start_ARG italic_x end_ARG ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) + italic_A ( italic_s ) .",
            "On the other hand, note that for any time  s 2  s 1  subscript s 2 subscript superscript s  1 s_{2}\\geq s^{\\prime}_{1} italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  before  x ~  ( s ) ~ x s \\tilde{x}(s) over~ start_ARG italic_x end_ARG ( italic_s )  reaches   1 +  2 2 subscript  1 subscript  2 2 \\frac{\\alpha_{1}+\\alpha_{2}}{2} divide start_ARG italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG  (if it ever reaches that value), the first term in ( 12 ) is always non-positive. Hence, we have",
            "Finally combining the upper bounds on the first and second terms in RHS of the ODE in ( 12 ) that we derived in Equations ( 24 ) and ( 30 ):"
        ]
    },
    "id_table_13": {
        "caption": "",
        "table": "A1.EGx13",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "A1.EGx14",
        "footnotes": [],
        "references": [
            "In Figures  6  to  14  we collect the MNIST experiments considering every other possible one-vs-all reduction. As mentioned in Section  6.2 , they have near-identical behavior to the experiments of Figure  3 ."
        ]
    },
    "id_table_15": {
        "caption": "",
        "table": "A1.EGx15",
        "footnotes": [],
        "references": [
            "Figures  15  to  18  show the results of repeating the experimental setup of Section  6.3  for different choices of the positive class, and also illustrate limitations of this experimental setup in the context of ImageNet.",
            "In Figures  15  to  17 , we see approximately the same behavior as in  4 . Namely, guidance values for which we have support error lead to distorted samples. Similar to Figure  4 , the choice  s = 5 s 5 s=5 italic_s = 5  works well in Figure  15 . However, for Figures  16  and  17 , we see that we have non-zero support error even for  s = 5 s 5 s=5 italic_s = 5 . In these latter two cases, we expect the qualitatively best choice of guidance to be somewhere between  s = 1 s 1 s=1 italic_s = 1  and  s = 5 s 5 s=5 italic_s = 5 ."
        ]
    },
    "id_table_16": {
        "caption": "",
        "table": "A1.EGx16",
        "footnotes": [],
        "references": [
            "where the last inequality is due to the fact that  ln  ( w )  16  c 1 2 w 16 superscript subscript c 1 2 \\ln(w)\\geq 16c_{1}^{2} roman_ln ( italic_w )  16 italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . Combining this with Equation ( 17 ), we can bound the first term in Equation ( 16 ):",
            "On the other hand, for the second term in Equation ( 16 ), we can upper bound the numerator as",
            "Combining this with Equation ( 20 ), we obtain the following upper bound for the second term in Equation ( 16 ):",
            "In Figures  15  to  17 , we see approximately the same behavior as in  4 . Namely, guidance values for which we have support error lead to distorted samples. Similar to Figure  4 , the choice  s = 5 s 5 s=5 italic_s = 5  works well in Figure  15 . However, for Figures  16  and  17 , we see that we have non-zero support error even for  s = 5 s 5 s=5 italic_s = 5 . In these latter two cases, we expect the qualitatively best choice of guidance to be somewhere between  s = 1 s 1 s=1 italic_s = 1  and  s = 5 s 5 s=5 italic_s = 5 ."
        ]
    },
    "id_table_17": {
        "caption": "",
        "table": "A1.EGx17",
        "footnotes": [],
        "references": [
            "where the last inequality is due to the fact that  ln  ( w )  16  c 1 2 w 16 superscript subscript c 1 2 \\ln(w)\\geq 16c_{1}^{2} roman_ln ( italic_w )  16 italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . Combining this with Equation ( 17 ), we can bound the first term in Equation ( 16 ):",
            "In Figures  15  to  17 , we see approximately the same behavior as in  4 . Namely, guidance values for which we have support error lead to distorted samples. Similar to Figure  4 , the choice  s = 5 s 5 s=5 italic_s = 5  works well in Figure  15 . However, for Figures  16  and  17 , we see that we have non-zero support error even for  s = 5 s 5 s=5 italic_s = 5 . In these latter two cases, we expect the qualitatively best choice of guidance to be somewhere between  s = 1 s 1 s=1 italic_s = 1  and  s = 5 s 5 s=5 italic_s = 5 ."
        ]
    },
    "id_table_18": {
        "caption": "",
        "table": "A1.EGx18",
        "footnotes": [],
        "references": [
            "Figures  15  to  18  show the results of repeating the experimental setup of Section  6.3  for different choices of the positive class, and also illustrate limitations of this experimental setup in the context of ImageNet.",
            "Figure  18  demonstrates some limitations of our experimental setup for classes which have high levels of noise/variance. Indeed, we see that for the basketball class the support error is not even monotonically increasing with the guidance parameter, and that sample quality is poor across guidance levels. Furthermore, the projected trajectories become progressively more negative as opposed to positive, indicating that the direction between the means of the positive class samples and the negative class samples is likely useless in this case. Despite these various issues, there appears to still be some positive correlation between support error and sample distortion."
        ]
    },
    "id_table_19": {
        "caption": "",
        "table": "A1.EGx19",
        "footnotes": [],
        "references": [
            "Therefore, combining with Equation ( 19 ),"
        ]
    },
    "id_table_20": {
        "caption": "",
        "table": "A1.EGx20",
        "footnotes": [],
        "references": [
            "Combining this with Equation ( 20 ), we obtain the following upper bound for the second term in Equation ( 16 ):"
        ]
    },
    "id_table_21": {
        "caption": "",
        "table": "A1.EGx21",
        "footnotes": [],
        "references": []
    },
    "id_table_22": {
        "caption": "",
        "table": "A1.EGx22",
        "footnotes": [],
        "references": [
            "Plugging this back into ( 22 )"
        ]
    },
    "id_table_23": {
        "caption": "",
        "table": "A1.EGx23",
        "footnotes": [],
        "references": [
            "If we denote by  s 1 subscript s 1 s_{1} italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  the first time  s  s 0 s subscript s 0 s\\geq s_{0} italic_s  italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  when  x  ( s ) = a t  ( s )   2 = s   2 x s subscript a t s subscript  2 s subscript  2 x(s)=a_{t(s)}\\alpha_{2}=s\\alpha_{2} italic_x ( italic_s ) = italic_a start_POSTSUBSCRIPT italic_t ( italic_s ) end_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = italic_s italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , then  s 1 subscript s 1 s_{1} italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  is certainly larger than the first time that  x  ( s ) =  2 x s subscript  2 x(s)=\\alpha_{2} italic_x ( italic_s ) = italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  as for all  t t t italic_t ,  a t  1 subscript a t 1 a_{t}\\leq 1 italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  1 . Let  s 2 subscript s 2 s_{2} italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  denote the first time  s  s 0 s subscript s 0 s\\geq s_{0} italic_s  italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  such that  x  ( s ) =  2 x s subscript  2 x(s)=\\alpha_{2} italic_x ( italic_s ) = italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT . From Equation ( 23 ),"
        ]
    },
    "id_table_24": {
        "caption": "",
        "table": "A1.EGx24",
        "footnotes": [],
        "references": [
            "Finally combining the upper bounds on the first and second terms in RHS of the ODE in ( 12 ) that we derived in Equations ( 24 ) and ( 30 ):"
        ]
    },
    "id_table_25": {
        "caption": "",
        "table": "A1.EGx25",
        "footnotes": [],
        "references": [
            "Note that using ( 25 ) and ( 27 ), we have"
        ]
    },
    "id_table_26": {
        "caption": "",
        "table": "A1.EGx26",
        "footnotes": [],
        "references": []
    },
    "id_table_27": {
        "caption": "",
        "table": "A1.EGx27",
        "footnotes": [],
        "references": [
            "Note that using ( 25 ) and ( 27 ), we have",
            "But we can solve the ODE in ( 27 ):"
        ]
    },
    "id_table_28": {
        "caption": "",
        "table": "A1.EGx28",
        "footnotes": [],
        "references": [
            "First comparing Equations ( 12 ) and ( 28 ), since the second term in the RHS of the ODE in ( 12 ) is always positive, from ODE comparison theorem we get  B  ( s )  x ~  ( s 0 ) + A  ( s ) B s ~ x subscript s 0 A s B(s)\\geq\\tilde{x}(s_{0})+A(s) italic_B ( italic_s )  over~ start_ARG italic_x end_ARG ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) + italic_A ( italic_s ) ."
        ]
    },
    "id_table_29": {
        "caption": "",
        "table": "A1.EGx29",
        "footnotes": [],
        "references": [
            "Note that inequality ( 29 ) is valid more generally up to time  s 1  subscript superscript s  1 s^{\\prime}_{1} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , when  B  ( s ) B s B(s) italic_B ( italic_s )  reaches  1 2  s 0  1   2   2 = 2  ( 1  s 0 ) 1  2  ( 1  s 0 )   2 1 2 subscript s 0 1 subscript  2 subscript  2 2 1 subscript s 0 1 2 1 subscript s 0 subscript  2 \\frac{1}{2s_{0}-1}\\alpha_{2}-\\alpha_{2}=\\frac{2\\left({1-s_{0}}\\right)}{1-2(1-s% _{0})}\\alpha_{2} divide start_ARG 1 end_ARG start_ARG 2 italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - 1 end_ARG italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = divide start_ARG 2 ( 1 - italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_ARG start_ARG 1 - 2 ( 1 - italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_ARG italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT . Now we solve for the value of  s ~ 1 subscript ~ s 1 \\tilde{s}_{1} over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  when  d  ( s ~ 1 ) d subscript ~ s 1 d(\\tilde{s}_{1}) italic_d ( over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT )  reaches  2  ( 1  s 0 ) 1  2  ( 1  s 0 )   2 2 1 subscript s 0 1 2 1 subscript s 0 subscript  2 \\frac{2\\left({1-s_{0}}\\right)}{1-2(1-s_{0})}\\alpha_{2} divide start_ARG 2 ( 1 - italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_ARG start_ARG 1 - 2 ( 1 - italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) end_ARG italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , which upper bounds  s 1  subscript superscript s  1 s^{\\prime}_{1} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  due to ( 29 ):",
            "and from definition for this choice of  s ~ 1 subscript ~ s 1 \\tilde{s}_{1} over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  we get from Equation ( 29 )"
        ]
    },
    "id_table_30": {
        "caption": "",
        "table": "A1.EGx30",
        "footnotes": [],
        "references": [
            "Finally combining this with our upper bound for the first part in Equation ( 30 ) completes the proof.",
            "Finally combining the upper bounds on the first and second terms in RHS of the ODE in ( 12 ) that we derived in Equations ( 24 ) and ( 30 ):"
        ]
    },
    "id_table_31": {
        "caption": "",
        "table": "A1.EGx31",
        "footnotes": [],
        "references": [
            "Note that the inequality ( 31 ) is true even when  x ~  ( s 0 )  1 2  s 0  1   2 ~ x subscript s 0 1 2 subscript s 0 1 subscript  2 \\tilde{x}(s_{0})\\leq\\frac{1}{2s_{0}-1}\\alpha_{2} over~ start_ARG italic_x end_ARG ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )  divide start_ARG 1 end_ARG start_ARG 2 italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - 1 end_ARG italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  as the right hand side is positive and the left hand side is negative in ( 31 ) in this case. Hence, overall we showed that for any time  s 2  s ~ 1  s 0 subscript s 2 subscript ~ s 1 subscript s 0 s_{2}\\geq\\tilde{s}_{1}\\vee s_{0} italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , as long as  x ~ ~ x \\tilde{x} over~ start_ARG italic_x end_ARG  has not reached   1 +  2 2 subscript  1 subscript  2 2 \\frac{\\alpha_{1}+\\alpha_{2}}{2} divide start_ARG italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG , we have ( 31 )."
        ]
    },
    "id_table_32": {
        "caption": "",
        "table": "A1.EGx32",
        "footnotes": [],
        "references": [
            "Now take  s ~ 0  s 0  3 4 subscript ~ s 0 subscript s 0 3 4 \\tilde{s}_{0}\\geq s_{0}\\vee\\frac{3}{4} over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  divide start_ARG 3 end_ARG start_ARG 4 end_ARG . Then we can use Lemma  6  with  s 0 = s ~ 0 subscript s 0 subscript ~ s 0 s_{0}=\\tilde{s}_{0} italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  because   2  ( 1  32 ln  ( w ) )   1 +  2 2 subscript  2 1 32 w subscript  1 subscript  2 2 \\alpha_{2}\\left({1-\\frac{32}{\\sqrt{\\ln(w)}}}\\right)\\geq\\frac{\\alpha_{1}+\\alpha% _{2}}{2} italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( 1 - divide start_ARG 32 end_ARG start_ARG square-root start_ARG roman_ln ( italic_w ) end_ARG end_ARG )  divide start_ARG italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG  so its condition is satisfied from ( 32 ); then, Lemma  6  implies that there exists a time  s 1 subscript s 1 s_{1} italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  such that for all  s  [ s 1 , 1 ] s subscript s 1 1 s\\in[s_{1},1] italic_s  [ italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , 1 ] :",
            "Note that  s ~ 0 subscript ~ s 0 \\tilde{s}_{0} over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  can be picked any value in the interval  ( s 0  3 4 , 1 ) subscript s 0 3 4 1 (s_{0}\\vee\\frac{3}{4},1) ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  divide start_ARG 3 end_ARG start_ARG 4 end_ARG , 1 ) . Therefore, picking  s ~ 0  1  subscript ~ s 0 1 \\tilde{s}_{0}\\rightarrow 1 over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  1 , Equations ( 32 ) and ( 33 ) show that with probability at least  1  e  w   1 2 8   2 1 superscript e w superscript subscript  1 2 8 superscript  2 1-e^{-\\frac{w\\alpha_{1}^{2}}{8\\beta^{2}}} 1 - italic_e start_POSTSUPERSCRIPT - divide start_ARG italic_w italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 8 italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG end_POSTSUPERSCRIPT ,  x ~  ( s ) ~ x s \\tilde{x}(s) over~ start_ARG italic_x end_ARG ( italic_s )  converges to the interval  (  2  ( 1  32 ln  ( w ) ) ,  2 ) subscript  2 1 32 w subscript  2 \\left({\\alpha_{2}\\left({1-\\frac{32}{\\sqrt{\\ln(w)}}}\\right),\\alpha_{2}}\\right) ( italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( 1 - divide start_ARG 32 end_ARG start_ARG square-root start_ARG roman_ln ( italic_w ) end_ARG end_ARG ) , italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) ."
        ]
    },
    "id_table_33": {
        "caption": "",
        "table": "A1.EGx33",
        "footnotes": [],
        "references": [
            "Note that  s ~ 0 subscript ~ s 0 \\tilde{s}_{0} over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  can be picked any value in the interval  ( s 0  3 4 , 1 ) subscript s 0 3 4 1 (s_{0}\\vee\\frac{3}{4},1) ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  divide start_ARG 3 end_ARG start_ARG 4 end_ARG , 1 ) . Therefore, picking  s ~ 0  1  subscript ~ s 0 1 \\tilde{s}_{0}\\rightarrow 1 over~ start_ARG italic_s end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  1 , Equations ( 32 ) and ( 33 ) show that with probability at least  1  e  w   1 2 8   2 1 superscript e w superscript subscript  1 2 8 superscript  2 1-e^{-\\frac{w\\alpha_{1}^{2}}{8\\beta^{2}}} 1 - italic_e start_POSTSUPERSCRIPT - divide start_ARG italic_w italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 8 italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG end_POSTSUPERSCRIPT ,  x ~  ( s ) ~ x s \\tilde{x}(s) over~ start_ARG italic_x end_ARG ( italic_s )  converges to the interval  (  2  ( 1  32 ln  ( w ) ) ,  2 ) subscript  2 1 32 w subscript  2 \\left({\\alpha_{2}\\left({1-\\frac{32}{\\sqrt{\\ln(w)}}}\\right),\\alpha_{2}}\\right) ( italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( 1 - divide start_ARG 32 end_ARG start_ARG square-root start_ARG roman_ln ( italic_w ) end_ARG end_ARG ) , italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) ."
        ]
    },
    "id_table_34": {
        "caption": "",
        "table": "A1.EGx34",
        "footnotes": [],
        "references": []
    },
    "id_table_35": {
        "caption": "",
        "table": "A1.EGx35",
        "footnotes": [],
        "references": [
            "On the other hand, note that from ( 35 ),  x ~   ( s )  ( 2  w + 1 ) superscript ~ x  s 2 w 1 \\tilde{x}^{\\prime}(s)\\leq(2w+1) over~ start_ARG italic_x end_ARG start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ( italic_s )  ( 2 italic_w + 1 ) , we have  x ~  ( s )  x ~  ( s 0 ) + ( 2  w + 1 )  ( s  s 0 ) ~ x s ~ x subscript s 0 2 w 1 s subscript s 0 \\tilde{x}(s)\\leq\\tilde{x}(s_{0})+(2w+1)(s-s_{0}) over~ start_ARG italic_x end_ARG ( italic_s )  over~ start_ARG italic_x end_ARG ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) + ( 2 italic_w + 1 ) ( italic_s - italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) . In particular, from the definition of  s 1 subscript s 1 s_{1} italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,"
        ]
    },
    "id_table_36": {
        "caption": "",
        "table": "A1.EGx36",
        "footnotes": [],
        "references": []
    },
    "id_table_37": {
        "caption": "",
        "table": "A1.EGx37",
        "footnotes": [],
        "references": [
            "We begin by performing some preliminary calculations to simplify the guided ODE, culminating in the simplified expression in Equation ( 37 ) below.",
            "where  x  N  ( 0 , 1 ) similar-to x N 0 1 x\\sim\\mathcal{N}(0,1) italic_x  caligraphic_N ( 0 , 1 ) . Below, we study the behavior of the ODE in ( 37 ) for different initial conditions  x ~  ( 0 ) = x ~ x 0 x \\tilde{x}(0)=x over~ start_ARG italic_x end_ARG ( 0 ) = italic_x .",
            "Before proceeding with the proof, we note that up to the    ( ln  w )  w \\Theta(\\ln w) roman_ ( roman_ln italic_w )  term, this analysis is nearly tight in the regime of large  w w w italic_w . The reason is that if  x ~  ( 0 ) <  2  w  1 ~ x 0 2 w 1 \\tilde{x}(0)<-2w-1 over~ start_ARG italic_x end_ARG ( 0 ) < - 2 italic_w - 1 , then because the velocity in Equation ( 37 ) is upper bounded by  2  w + 1 2 w 1 2w+1 2 italic_w + 1  at all times, the particle will remain negative at all times.",
            "First, note that the right-hand side of Equation ( 37 ) is lower bounded by  1 1 1 1 , so if  x ~  ( 0 )  0 ~ x 0 0 \\tilde{x}(0)\\geq 0 over~ start_ARG italic_x end_ARG ( 0 )  0 , then  x ~  ( 1 )  x ~  ( 0 ) + w  0 ~ x 1 ~ x 0 w 0 \\tilde{x}(1)\\geq\\tilde{x}(0)+w\\geq 0 over~ start_ARG italic_x end_ARG ( 1 )  over~ start_ARG italic_x end_ARG ( 0 ) + italic_w  0 . More generally, this implies that as soon as the particle becomes positive, it continues to move to the right at rate lower bounded by  1 1 1 1 .",
            "In the following we upper bound  s 1 subscript s 1 s_{1} italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . Defining   italic- \\epsilon italic_  by   / w = 1 + tanh  (  c ) italic- w 1 c \\epsilon/w=1+\\tanh(-c) italic_ / italic_w = 1 + roman_tanh ( - italic_c ) , we see from the definition of ODE ( 37 ) that for all  s 0  s  s 1 subscript s 0 s subscript s 1 s_{0}\\leq s\\leq s_{1} italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  italic_s  italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT :"
        ]
    },
    "id_table_38": {
        "caption": "",
        "table": "S4.E38",
        "footnotes": [],
        "references": [
            "Therefore, from ( 38 ),"
        ]
    },
    "id_table_39": {
        "caption": "",
        "table": "A1.EGx38",
        "footnotes": [],
        "references": []
    },
    "id_table_40": {
        "caption": "",
        "table": "A1.EGx39",
        "footnotes": [],
        "references": []
    },
    "id_table_41": {
        "caption": "",
        "table": "A1.EGx40",
        "footnotes": [],
        "references": [
            "where the last inequality follows from   x  2  w x 2 w -x\\leq 2w - italic_x  2 italic_w . Combining Equations ( 41 ) and ( 45 ), it is enough to show the following to prove  x ~  ( 1 ) > 0 ~ x 1 0 \\tilde{x}(1)>0 over~ start_ARG italic_x end_ARG ( 1 ) > 0 :"
        ]
    },
    "id_table_42": {
        "caption": "",
        "table": "A1.EGx41",
        "footnotes": [],
        "references": [
            "and let its roots be  s 0  < s 1  subscript superscript s 0 subscript superscript s 1 s^{*}_{0}<s^{*}_{1} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT < italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . First note that  Q  ( 0 ) = c > 0 Q 0 c 0 Q(0)=c>0 italic_Q ( 0 ) = italic_c > 0  and  Q  ( s 0 ) = s 0  x ~  ( s 0 ) + c  0 Q subscript s 0 subscript s 0 ~ x subscript s 0 c 0 Q(s_{0})=s_{0}\\tilde{x}(s_{0})+c\\leq 0 italic_Q ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT over~ start_ARG italic_x end_ARG ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) + italic_c  0 , hence  Q Q Q italic_Q  has a root in the interval  [ 0 , s 0 ] 0 subscript s 0 [0,s_{0}] [ 0 , italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ]  and  s 0   s 0 subscript superscript s 0 subscript s 0 s^{*}_{0}\\leq s_{0} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT . On the other hand,  s 1 > s 0 subscript s 1 subscript s 0 s_{1}>s_{0} italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , so ( 42 ) implies that  s 1 subscript s 1 s_{1} italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  should greater or equal to the larger root  s 1  subscript superscript s 1 s^{*}_{1} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . Now we lower bound  s 1  subscript superscript s 1 s^{*}_{1} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . Defining  G = x ~  ( s 0 )  ( 2  w + 1 )  s 0 2  ( 2  w + 1 ) G ~ x subscript s 0 2 w 1 subscript s 0 2 2 w 1 G=\\frac{\\tilde{x}(s_{0})-(2w+1)s_{0}}{2(2w+1)} italic_G = divide start_ARG over~ start_ARG italic_x end_ARG ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) - ( 2 italic_w + 1 ) italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG start_ARG 2 ( 2 italic_w + 1 ) end_ARG , completing the square for ( 43 ) gives"
        ]
    },
    "id_table_43": {
        "caption": "",
        "table": "A1.EGx42",
        "footnotes": [],
        "references": [
            "and let its roots be  s 0  < s 1  subscript superscript s 0 subscript superscript s 1 s^{*}_{0}<s^{*}_{1} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT < italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . First note that  Q  ( 0 ) = c > 0 Q 0 c 0 Q(0)=c>0 italic_Q ( 0 ) = italic_c > 0  and  Q  ( s 0 ) = s 0  x ~  ( s 0 ) + c  0 Q subscript s 0 subscript s 0 ~ x subscript s 0 c 0 Q(s_{0})=s_{0}\\tilde{x}(s_{0})+c\\leq 0 italic_Q ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT over~ start_ARG italic_x end_ARG ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) + italic_c  0 , hence  Q Q Q italic_Q  has a root in the interval  [ 0 , s 0 ] 0 subscript s 0 [0,s_{0}] [ 0 , italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ]  and  s 0   s 0 subscript superscript s 0 subscript s 0 s^{*}_{0}\\leq s_{0} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT . On the other hand,  s 1 > s 0 subscript s 1 subscript s 0 s_{1}>s_{0} italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , so ( 42 ) implies that  s 1 subscript s 1 s_{1} italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  should greater or equal to the larger root  s 1  subscript superscript s 1 s^{*}_{1} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . Now we lower bound  s 1  subscript superscript s 1 s^{*}_{1} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . Defining  G = x ~  ( s 0 )  ( 2  w + 1 )  s 0 2  ( 2  w + 1 ) G ~ x subscript s 0 2 w 1 subscript s 0 2 2 w 1 G=\\frac{\\tilde{x}(s_{0})-(2w+1)s_{0}}{2(2w+1)} italic_G = divide start_ARG over~ start_ARG italic_x end_ARG ( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) - ( 2 italic_w + 1 ) italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG start_ARG 2 ( 2 italic_w + 1 ) end_ARG , completing the square for ( 43 ) gives"
        ]
    }
}