{
    "id_table_1": {
        "caption": "Table 1 :  SDICE  index using F-ratio and EMD for intra (  i  n  t  r  a subscript  i n t r a \\gamma_{intra} italic_ start_POSTSUBSCRIPT italic_i italic_n italic_t italic_r italic_a end_POSTSUBSCRIPT ) and inter (  i  n  t  e  r subscript  i n t e r \\gamma_{inter} italic_ start_POSTSUBSCRIPT italic_i italic_n italic_t italic_e italic_r end_POSTSUBSCRIPT ) cases, along with the influence of sample size (a) and prompt type (b) on the  SDICE  index. Additional results on FairFace dataset  [ Karkkainen and Joo(2021) ]  are provided in the supplementary material.",
        "table": "S3.T1.16.12",
        "footnotes": [
            ""
        ],
        "references": [
            "In this work, we propose a novel approach for diversity quantification of synthetic datasets. Given a sufficiently-diverse reference dataset of real images and a synthetic dataset, it is possible to analyze whether the variations in the synthetic dataset match or exceed those observed in the reference dataset, as shown in Figure  1 . Specifically, we characterize the observed variations in a synthetic dataset by analyzing the similarity distributions between images of the same class (intra-class) and images from different classes (inter-class). We assume that the similarity scores are computed based on a contrastive encoder, which is pre-trained to be invariant under different affine/photometric transformations of the same image. We hypothesize that benchmarking of intra- and inter-class synthetic similarity distributions against their counterparts based on a reference dataset is a good proxy for diversity. Based on this hypothesis, we make the following contributions:",
            "Firstly, we observe that  G t  r  a  n  s r superscript subscript G t r a n s r \\mathcal{G}_{trans}^{r} caligraphic_G start_POSTSUBSCRIPT italic_t italic_r italic_a italic_n italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT  consistently hovers close to 1.0 (see Figure  4 ), which is expected because the feature extractor and similarity computation are designed to ignore differences between an image and its transformed version. However, intra-class variations depict a greater range in  D r superscript D r \\mathcal{D}^{r} caligraphic_D start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT  than in  D s superscript D s \\mathcal{D}^{s} caligraphic_D start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT  for CXRs, whereas inter-class variations present comparable extents across both distributions, as shown in Figure  4 . To quantitatively assess these observations, we computed the  SDICE  index using both F-ratio and EMD for each case under study. As detailed in Table  1 , the   i  n  t  r  a subscript  i n t r a \\gamma_{intra} italic_ start_POSTSUBSCRIPT italic_i italic_n italic_t italic_r italic_a end_POSTSUBSCRIPT  for CXRs is significantly lower, indicating a lack of intra-class diversity compared to those of ImageNet. An analysis using different sample sizes and prompts is also presented in Table  1  and discussed in detail in section  3.3 .",
            "Table  1 (a) outlines how   intra subscript  intra \\gamma_{\\text{intra}} italic_ start_POSTSUBSCRIPT intra end_POSTSUBSCRIPT  values evolve as we increase sample sizes from  n n n italic_n  to  2  n 2 n 2n 2 italic_n , and further to  4  n 4 n 4n 4 italic_n . This progression reveals that   intra subscript  intra \\gamma_{\\text{intra}} italic_ start_POSTSUBSCRIPT intra end_POSTSUBSCRIPT , or the measure of diversity within classes, tends to rise with larger sample sets. Observed in both the MIMIC and ImageNet datasets, this trend suggests that expanding the dataset by introducing a wider variety of examples within each class enhances the overall diversity. The initial sample size was 350 for both MIMIC-CXR and ImageNet datasets, meaning 25 images per class. We found that a balanced sample size yielded better results in terms of diversity assessment compared to imbalanced samples.",
            "Table  1 (b) shows that the complexity of prompts affects the diversity of the generated images. In the case of CXR images, less detailed prompts, such as  P 1 subscript P 1 P_{1} italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , appear to encourage a wider diversity, perhaps due to the generative model having broader interpretative freedom. For ImageNet, descriptive prompts such as  P 3 subscript P 3 P_{3} italic_P start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT  lead to more diverse outputs, which implies that the detailed nature of these prompts provides useful guidance to the model, enabling it to capture the extensive variability inherent across ImageNets classes. This suggests that the level of detail in prompts should be carefully considered to match the desired diversity of the dataset being synthesized.",
            "One of the key factors impacting the proposed diversity assessment framework is the feature extractor  F F \\mathcal{F} caligraphic_F . We examined the performance of three feature extractors having the same architecture (ResNet-50), but trained in different ways. We consider: (i) ResNet-50 model pre-trained on the ImageNet dataset, (ii) ResNet-50 model that is pre-trained on the ImageNet dataset and fine-tuned on MIMIC-CXR, and (iii) ResNet-50 model that is pre-trained using self-supervised contrastive learning on CXR images. In all the three cases, the cosine similarities are calculated for the transformed, intra-class, and inter-class scenarios, and the corresponding box plots are shown in Figure  11 .",
            "A closer analysis of Figure  11  indicates that the contrastive learning encoder is the best choice for assessing diversity. This is because it gives similarity scores closest to 1 (compared to the other feature extractors) for the transformed case, which is the expected behavior. Notably, in the transformed case, the similarity scores are closest to 1 compared to the other encoders. This observation indicates that the contrastive learning encoder excels in preserving and understanding image representations, emphasizing its effectiveness in our diversity evaluation framework.",
            "We explore the significance of guidance scale and its influence on the quality of synthetic images generated by Stable Diffusion and Roentgen models for both MIMIC-CXR and ImageNet datasets. The guidance scale acts as a control parameter, influencing the fidelity of synthetic images which holds significance in the context of image generation models, serving as a guiding force in the synthesis process. In a technical sense, the guidance scale regulates the contribution of the guidance signal during the optimization process of the generative model. Its significance lies in guiding the generation process to strike a balance between realism and diversity. A carefully chosen guidance scale can enhance the quality and relevance of generated images to the target dataset. Density plots presented in Figure  12  visually assess the distribution of real and synthetic images across three scenarios: transformed, intra-class, and inter-class. They are accompanied by sets of synthetic samples corresponding to different values of the guidance scale.",
            "From Figure  13  it is obvious that all the synthetic images generated for the Atelectasis class look very similar, confirming their lower diversity. On the other hand, the synthetic images of the Fracture class exhibit reasonable diversity, which is correctly captured by the proposed  SDICE  index. Similarly, all the synthetic minibus images exhibit close similarity in color and shape unlike the corresponding real images, which justifies why they have a low  SDICE  index.",
            "Catastrophic forgetting is a common problem in machine learning that occurs when a model forgets what it has previously learned when it is trained on new data. This is often the case when a model is fine-tuned on a specific domain, such as chest x-rays, and then trained on a different domain. In the case of the RoentGen model, the model was fine-tuned on chest x-rays to improve the overall performance. However, this fine-tuning process caused the model to forget what it had previously learned, and its performance suffered when it was used on new, unseen data outside of the chest x-ray domain. The authors noted that as the model was introduced to new images, its weights rapidly changed, leading to knowledge collapse. We experimented with this by passing different prompts such as \"A Dog under a Tree\", \"Cat on a Chair\", \"Table Chair\" and \"Dense Forest\" through the model, We noticed that the model still generated different chest x-rays. Figure  14  shows the result on different prompts. We run this as a replication experiment and suspect that the RoentGen Model is susceptible to overfitting and may lack diversity or even memorize the training data."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :   i  n  t  r  a subscript  i n t r a \\gamma_{intra} italic_ start_POSTSUBSCRIPT italic_i italic_n italic_t italic_r italic_a end_POSTSUBSCRIPT  values where Atelectasis in  MIMIC-CXR  exhibits the least diversity, while Fracture demonstrates the highest diversity. In  ImageNet , minibus class has the least diversity, and fireboat stands out as the most diverse class.",
        "table": "S3.T2.2.2.2",
        "footnotes": [],
        "references": [
            "The key intuition underlying the proposed  SDICE  index is that a synthetic dataset can be considered to have good diversity if the variations in this dataset closely follow or exceed the variations observed in a reference dataset containing sufficiently-diverse real images. Figure  2  provides an overview of the architecture of our proposed  SDICE  index. However, two main challenges need to be overcome to determine if two datasets (synthetic and real) have similar variations. 1) The variations in a dataset can be caused due to many reasons such as image noise and within and between class differences, and it is essential to capture these variations individually. 2) A good metric is required to capture pair-wise similarities between the images.",
            "Further investigation of   i  n  t  r  a subscript  i n t r a \\gamma_{intra} italic_ start_POSTSUBSCRIPT italic_i italic_n italic_t italic_r italic_a end_POSTSUBSCRIPT  was done by measuring the diversity within individual classes of both datasets as shown in Figure  5  and detailed in Table  2 . We observe that several classes in the MIMIC-CXR synthetic dataset do not have the same range of diversity as its real counterpart. We observe poor diversity in classes with niche domain-specific names (such as Atelectasis and Enlarged Cardiomegaly) as opposed to more general ones (Pneumonia and Fracture). We hypothesize that the generative model  [ Chambon et al.(2022)Chambon, Bluethgen, Delbrouck, Van der Sluijs, Poacin, Chaves, Abraham, Purohit, Langlotz, and Chaudhari ]  possibly fails to capture the true variations within the esoteric classes due to limited training.",
            "We explore the significance of guidance scale and its influence on the quality of synthetic images generated by Stable Diffusion and Roentgen models for both MIMIC-CXR and ImageNet datasets. The guidance scale acts as a control parameter, influencing the fidelity of synthetic images which holds significance in the context of image generation models, serving as a guiding force in the synthesis process. In a technical sense, the guidance scale regulates the contribution of the guidance signal during the optimization process of the generative model. Its significance lies in guiding the generation process to strike a balance between realism and diversity. A carefully chosen guidance scale can enhance the quality and relevance of generated images to the target dataset. Density plots presented in Figure  12  visually assess the distribution of real and synthetic images across three scenarios: transformed, intra-class, and inter-class. They are accompanied by sets of synthetic samples corresponding to different values of the guidance scale."
        ]
    },
    "id_table_3": {
        "caption": "Figure 6 :  Progression of   i  n  t  e  r subscript  i n t e r \\gamma_{inter} italic_ start_POSTSUBSCRIPT italic_i italic_n italic_t italic_e italic_r end_POSTSUBSCRIPT  with increasing class inclusion.  This illustrates the increase in diversity within MIMIC-CXR and ImageNet as more classes are added, leveling off to indicate a maximum inter-class diversity threshold.",
        "table": "A1.T3.5.1",
        "footnotes": [],
        "references": [
            "Firstly, we observe that  G t  r  a  n  s r superscript subscript G t r a n s r \\mathcal{G}_{trans}^{r} caligraphic_G start_POSTSUBSCRIPT italic_t italic_r italic_a italic_n italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT  consistently hovers close to 1.0 (see Figure  4 ), which is expected because the feature extractor and similarity computation are designed to ignore differences between an image and its transformed version. However, intra-class variations depict a greater range in  D r superscript D r \\mathcal{D}^{r} caligraphic_D start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT  than in  D s superscript D s \\mathcal{D}^{s} caligraphic_D start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT  for CXRs, whereas inter-class variations present comparable extents across both distributions, as shown in Figure  4 . To quantitatively assess these observations, we computed the  SDICE  index using both F-ratio and EMD for each case under study. As detailed in Table  1 , the   i  n  t  r  a subscript  i n t r a \\gamma_{intra} italic_ start_POSTSUBSCRIPT italic_i italic_n italic_t italic_r italic_a end_POSTSUBSCRIPT  for CXRs is significantly lower, indicating a lack of intra-class diversity compared to those of ImageNet. An analysis using different sample sizes and prompts is also presented in Table  1  and discussed in detail in section  3.3 .",
            "A further analysis was conducted to highlight the variation obtained by the  SDICE  index as compared to the SSIM and FID scores (Supp: Table  3 ). Our analysis shows mean FID scores hover around 0.0082 and 0.0099 for intra and inter-class distributions, respectively. The FID score shows poor resolution as compared to the  SDICE  index, as the latter benefits from a domain-specific contrastive encoder. Similarly, SSIM values also fail to provide a clear separation between intra and inter-class diversity with mean SSIM scores of 0.68 and 0.60, respectively. The  SDICE  index effectively highlights the contrast in diversity across intra and inter-class categories, providing clear insights that FID and SSIM metrics may overlook. This highlights the benefits of  SDICE  index in domain-specific dataset analysis.",
            "From Figure  13  it is obvious that all the synthetic images generated for the Atelectasis class look very similar, confirming their lower diversity. On the other hand, the synthetic images of the Fracture class exhibit reasonable diversity, which is correctly captured by the proposed  SDICE  index. Similarly, all the synthetic minibus images exhibit close similarity in color and shape unlike the corresponding real images, which justifies why they have a low  SDICE  index."
        ]
    },
    "id_table_4": {
        "caption": "Figure 7 :  SDICE  Index Variation with   m  i  n subscript  m i n \\gamma_{min} italic_ start_POSTSUBSCRIPT italic_m italic_i italic_n end_POSTSUBSCRIPT  in MIMIC-CXR and ImageNet.  This displays a marked decrease in MIMIC-CXRs intra-class diversity with increasing   m  i  n subscript  m i n \\gamma_{min} italic_ start_POSTSUBSCRIPT italic_m italic_i italic_n end_POSTSUBSCRIPT , in contrast to ImageNets consistent inter-class diversity.",
        "table": "A1.T4.6.1",
        "footnotes": [],
        "references": [
            "Firstly, we observe that  G t  r  a  n  s r superscript subscript G t r a n s r \\mathcal{G}_{trans}^{r} caligraphic_G start_POSTSUBSCRIPT italic_t italic_r italic_a italic_n italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT  consistently hovers close to 1.0 (see Figure  4 ), which is expected because the feature extractor and similarity computation are designed to ignore differences between an image and its transformed version. However, intra-class variations depict a greater range in  D r superscript D r \\mathcal{D}^{r} caligraphic_D start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT  than in  D s superscript D s \\mathcal{D}^{s} caligraphic_D start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT  for CXRs, whereas inter-class variations present comparable extents across both distributions, as shown in Figure  4 . To quantitatively assess these observations, we computed the  SDICE  index using both F-ratio and EMD for each case under study. As detailed in Table  1 , the   i  n  t  r  a subscript  i n t r a \\gamma_{intra} italic_ start_POSTSUBSCRIPT italic_i italic_n italic_t italic_r italic_a end_POSTSUBSCRIPT  for CXRs is significantly lower, indicating a lack of intra-class diversity compared to those of ImageNet. An analysis using different sample sizes and prompts is also presented in Table  1  and discussed in detail in section  3.3 .",
            "Catastrophic forgetting is a common problem in machine learning that occurs when a model forgets what it has previously learned when it is trained on new data. This is often the case when a model is fine-tuned on a specific domain, such as chest x-rays, and then trained on a different domain. In the case of the RoentGen model, the model was fine-tuned on chest x-rays to improve the overall performance. However, this fine-tuning process caused the model to forget what it had previously learned, and its performance suffered when it was used on new, unseen data outside of the chest x-ray domain. The authors noted that as the model was introduced to new images, its weights rapidly changed, leading to knowledge collapse. We experimented with this by passing different prompts such as \"A Dog under a Tree\", \"Cat on a Chair\", \"Table Chair\" and \"Dense Forest\" through the model, We noticed that the model still generated different chest x-rays. Figure  14  shows the result on different prompts. We run this as a replication experiment and suspect that the RoentGen Model is susceptible to overfitting and may lack diversity or even memorize the training data."
        ]
    },
    "id_table_5": {
        "caption": "Table 3 :  Analysis of SSIM and FID Scores. This table shows SSIM and FID scores, highlighting their relative ineffectiveness in distinguishing between intra and inter-class diversity, as opposed to the  SDICE  index, which provides a clearer distinction of intra and inter-class diversity in both datasets.",
        "table": "A1.T5.7.5",
        "footnotes": [],
        "references": [
            "Further investigation of   i  n  t  r  a subscript  i n t r a \\gamma_{intra} italic_ start_POSTSUBSCRIPT italic_i italic_n italic_t italic_r italic_a end_POSTSUBSCRIPT  was done by measuring the diversity within individual classes of both datasets as shown in Figure  5  and detailed in Table  2 . We observe that several classes in the MIMIC-CXR synthetic dataset do not have the same range of diversity as its real counterpart. We observe poor diversity in classes with niche domain-specific names (such as Atelectasis and Enlarged Cardiomegaly) as opposed to more general ones (Pneumonia and Fracture). We hypothesize that the generative model  [ Chambon et al.(2022)Chambon, Bluethgen, Delbrouck, Van der Sluijs, Poacin, Chaves, Abraham, Purohit, Langlotz, and Chaudhari ]  possibly fails to capture the true variations within the esoteric classes due to limited training.",
            "The proposed  SDICE  index employs the F-ratio as a primary distance measure between two probability distributions ( G 0 subscript G 0 \\mathcal{G}_{0} caligraphic_G start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  and  G 1 subscript G 1 \\mathcal{G}_{1} caligraphic_G start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) as defined in Eq.  6 . To explore the robustness and sensitivity of the  SDICE  index, we also consider an alternative distance measure based on the Euclidean distance  [ Virtanen et al.(2023)Virtanen, Gommers, Oliphant, et al. ] . Table  5  illustrates the impact of these distance metrics on the  SDICE  index    \\gamma italic_  across different datasets.",
            "We conduct a visual assessment of images from the MIMIC-CXR and ImageNet datasets as well as their corresponding synthetic counterparts generated using the Roentgen and Stable Diffusion models, respectively. Specifically, we analyze the synthetic images corresponding to the most and least diverse classes identified using the proposed  SDICE  index. From Figure  5 , it can be observed that the Atelectasis is identified as the least diverse class, while Fracture is the most diverse class in CXR generation. Similarly, for natural image generation, minibus is the class with the least diversity and fireboat is the most diverse class identified using the  SDICE  index."
        ]
    }
}