{
    "id_table_1": {
        "caption": "",
        "table": "Sx8.tab1.1.1",
        "footnotes": [],
        "references": [
            "Evaluation Data    To determine an LMs ability in the medical domain, we evaluate the model on the MultiMedQA, a multi-dataset of medical questions  [ 12 ] . The MultiMedQA is composed of 8 individual datasets ranging from USMLE-style questions (MedQA) to College Biology (MMLU College Biology) and is outlined in Figure 1A. We choose to evaluate on these datasets due to the expert level of medical reasoning and knowledge required for USMLE-style questions, and to test the models ability against the range of medical tasks with the other datasets. Testing on the PubMedQA also demonstrates MedMobiles ability to perform on research-related medical inquiries. These results are displayed in Supplemental Table 1.",
            "Supplemental Table 1.   Evaluation results across the MultiMedQA, for phi-3-mini, MedMobile, UltraMedical 8B, and Flan-Palm. Scores for UltraMedical 8B and Flan-Palm are sourced from literature  [ 1 ,  10 ] ."
        ]
    },
    "global_footnotes": []
}