{
    "id_table_1": {
        "caption": "Table 1 :  Allowable semantic types when identifying mentions of the target.",
        "table": "S3.T1.2",
        "footnotes": [],
        "references": [
            "In this paper, we aim to provide a better understanding of the effects of some of these early decisions on the performance of information retrieval for the clinical domain. This pipeline is illustrated in Figure  1 . We provide an ablation study of recent models and varying embedding pooling methods on three extractive tasks and examine the reproducibility on two data sources: the publicly available MIMIC-III  [ 9 ]  dataset and a private EHR dataset.",
            "Due to the frequent use of acronyms and the numerous ways of expressing the same medical concept, we needed to employ a fuzzy matching technique to find these mentions. We first employed QuickUMLS to identify UMLS concepts within the text as potential matches, restricting by appropriate semantic types (Table  1 ). In the case of the primary diagnosis, we calculated the cosine similarity between the BioLORD-2023  [ 17 ]  embedding of the known diagnosis and that of each of the UMLS entity spans. If the similarity was >= 0.6, this was considered a positive match. This method enabled us to correctly identify occurrences such as  left knee OA  as a mention of the known primary diagnosis of  osteoarthritis of the left knee ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Statistics about the six datasets. \"Relevant chunks\" are those that contain at least one occurrence of the target information, such as the primary diagnosis.",
        "table": "S3.T1.2.2.1.2.1",
        "footnotes": [],
        "references": [
            "For computational practicality, we limited our consideration of the UW dataset to encounters of five days or less in length of stay. Even with this restriction, the encounters included were comprised of 5,245 to 63,376 tokens each, highlighting the importance of retrieval solutions for the clinical domain. We described the dataset statistics further in Table  2 . There was some variance between the UW data and MIMIC-III in the prevalence of relevant note chunks that contain the target information. Additionally, MIMIC-III typically consisted of fewer tokens than the UW data.",
            "We found that performance was very sensitive to the phrasing of the query, potentially even dropping it below baseline. For instance, using  primary diagnosis  with Llama3-8B-Instruct and mean pooling, the average precision was 27.44. Simply changing the query to  patients primary diagnosis  drastically improved retrieval to 36.68. In Figures  2  and  3  we present box plots to illustrate the distribution of scores for each model and task. We observed that the UW dataset experiences more variability compared to MIMIC, regardless of model or task."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Each models best embedding pooling methods for the 256-token-maximum note chunks. If multiple methods are listed, we did not find a significant ( p <0.05) difference between them.",
        "table": "S3.T2.2",
        "footnotes": [],
        "references": [
            "In Table  3 , we present our findings on the best  note  embedding pooling strategies for each model, across all queries and tasks. These results were largely consistent between the datasets. Through the same testing method, we found that the query pooling method has an insignificant effect on performance for most models.",
            "We found that performance was very sensitive to the phrasing of the query, potentially even dropping it below baseline. For instance, using  primary diagnosis  with Llama3-8B-Instruct and mean pooling, the average precision was 27.44. Simply changing the query to  patients primary diagnosis  drastically improved retrieval to 36.68. In Figures  2  and  3  we present box plots to illustrate the distribution of scores for each model and task. We observed that the UW dataset experiences more variability compared to MIMIC, regardless of model or task."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  Mean average precision [95% CI] for the models across the queries.",
        "table": "S4.T3.2",
        "footnotes": [],
        "references": [
            "In Table  4 , we present the mean average precision for the different models, using only the best note pooling strategy for each model, across the various query/query pooling configurations. It should be noted that due to the prevalence of the target information being different between datasets, these scores should not be directly interpreted as whether models perform better on one type of data than the other. With MIMIC-III having a higher prevalence of relevant information for all three tasks, higher scores on that dataset are expected."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 :  Queries used for retrieving the primary diagnosis.",
        "table": "S4.T4.2",
        "footnotes": [],
        "references": [
            "For each model, we used between 4 and 7 different phrasings of the query per task (see Tables  5 ,  6 , and  7  in the Appendix), constructed to be simple and intuitive and without system prompting or extensive tuning in order to provide a generalizable statistical approximation of using these configurations in new use cases.",
            "We present the lists of queries used for the diagnosis (Table  5 ), procedures (Table  6 ), and antibiotics (Table  7 ) tasks. The queries that begin with Instruct:[...], Given [...], and Represent [...] were only used for SFR-Embedding-Mistral, LLM2Vec, and BGE (respectively). These models were trained to use these formats, although they are not strictly necessary."
        ]
    },
    "id_table_6": {
        "caption": "Table 6 :  Queries used for retrieving the invasive and surgical procedures.",
        "table": "S7.T5.2",
        "footnotes": [],
        "references": [
            "For each model, we used between 4 and 7 different phrasings of the query per task (see Tables  5 ,  6 , and  7  in the Appendix), constructed to be simple and intuitive and without system prompting or extensive tuning in order to provide a generalizable statistical approximation of using these configurations in new use cases.",
            "We present the lists of queries used for the diagnosis (Table  5 ), procedures (Table  6 ), and antibiotics (Table  7 ) tasks. The queries that begin with Instruct:[...], Given [...], and Represent [...] were only used for SFR-Embedding-Mistral, LLM2Vec, and BGE (respectively). These models were trained to use these formats, although they are not strictly necessary."
        ]
    },
    "id_table_7": {
        "caption": "Table 7 :  Queries used for retrieving mentions of antibiotics.",
        "table": "S7.T5.2.6.6.1.1",
        "footnotes": [],
        "references": [
            "For each model, we used between 4 and 7 different phrasings of the query per task (see Tables  5 ,  6 , and  7  in the Appendix), constructed to be simple and intuitive and without system prompting or extensive tuning in order to provide a generalizable statistical approximation of using these configurations in new use cases.",
            "We present the lists of queries used for the diagnosis (Table  5 ), procedures (Table  6 ), and antibiotics (Table  7 ) tasks. The queries that begin with Instruct:[...], Given [...], and Represent [...] were only used for SFR-Embedding-Mistral, LLM2Vec, and BGE (respectively). These models were trained to use these formats, although they are not strictly necessary."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "S7.T5.2.7.7.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_9": {
        "caption": "",
        "table": "S7.T5.2.8.8.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_10": {
        "caption": "",
        "table": "S7.T5.2.9.9.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "",
        "table": "S7.T5.2.10.10.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "",
        "table": "S7.T5.2.11.11.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "",
        "table": "S7.T6.2",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "S7.T6.2.5.5.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "",
        "table": "S7.T6.2.6.6.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_16": {
        "caption": "",
        "table": "S7.T6.2.7.7.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_17": {
        "caption": "",
        "table": "S7.T6.2.8.8.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_18": {
        "caption": "",
        "table": "S7.T6.2.9.9.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_19": {
        "caption": "",
        "table": "S7.T6.2.10.10.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_20": {
        "caption": "",
        "table": "S7.T7.2",
        "footnotes": [],
        "references": []
    },
    "id_table_21": {
        "caption": "",
        "table": "S7.T7.2.5.5.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_22": {
        "caption": "",
        "table": "S7.T7.2.6.6.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_23": {
        "caption": "",
        "table": "S7.T7.2.7.7.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_24": {
        "caption": "",
        "table": "S7.T7.2.8.8.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_25": {
        "caption": "",
        "table": "S7.T7.2.9.9.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_26": {
        "caption": "",
        "table": "S7.T7.2.10.10.1.1",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": []
}