{
    "S5.T1.1": {
        "caption": [],
        "table": "<table id=\"S5.T1.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S5.T1.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Learning Rate Range</span></td>\n<td id=\"S5.T1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Batch Size Range</span></td>\n</tr>\n<tr id=\"S5.T1.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.2.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">1e-4, 1e-5, 1e-6, 1e-7</td>\n<td id=\"S5.T1.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">50, 20, 5</td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "The model’s training strategy involves dynamic parameter updates aimed at guiding the model towards convergence from local minima to as close as the global minima. The optimizer chosen for this task was ADAM optimizer defined by [21], and mean square error as a loss function. By employing a learning rate schedule batch sizes, and 202020 epochs per combination, the model initially grasps the data’s general features. Subsequently, through fine-tuning the learning rate and reduction of batch size, the model delves deeper into the feature space, capturing more intricate details. Figure 7 illustrates the training loss progression over time for the Binocular model during the dynamic parameter update process. Table 1 provides a summary of the ranges utilized for model training."
        ]
    }
}