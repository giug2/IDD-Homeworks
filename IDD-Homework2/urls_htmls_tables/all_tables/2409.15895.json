{
    "id_table_1": {
        "caption": "Table 1 .  Dataset Statistics. Code, NL, and Relevant refer to the average lengths after tokenization by CodeGPT. Relevant indicates the code retrieved by the retriever. Code tokens indicates the maximum length we set for the output generated by the refactorer.",
        "table": "S4.T1.4",
        "footnotes": [],
        "references": [
            "To address the aforementioned issues, we propose the  RRG  ( R etrieval,  R efactor,  G eneration) framework. We inserted a new module, a code refactorer, between the retriever and the generator. Fig.  1  shows the difference between our RRG and conventional RACG. This module processes the retrieved code, providing the generator with more efficient and model-friendly contextual information.",
            "The statistical information of the datasets and the code tokens for different dataset settings are shown in Table  1 . It is worth noting that we trained the entire framework using only the sampled datasets without generating new data.",
            "To investigate RQ1, we utilize two distinct datasets to simulate various scenarios. The ConCode dataset is characterized by relatively short code snippets with high similarity, introducing significant noise to the generator. In the RAG approach, all retrieved information is directly provided to the generator, whereas in the RRG framework, the generator solely refers to the output generated by the refactorer. Conversely, the CodeSearchNet dataset comprises longer and more intricate code, frequently exceeding the contextual window prescribed by the model. This complexity poses a challenge for the generator to produce precise outputs. Hence, we restrict the output size of both the retriever in RAG and the refactorer in RRG to a uniform length which is shown as code tokens in Table  1 . The objective of these experiments is to affirm the efficacy of the RRG framework in enhancing the RAG model under diverse conditions."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 .  Comparing the performance of different generators in RRG and RAG frameworks. The results are based on ConCode and CodeSearchNet. The refactorer is based on CodeT5-small. The retriever employs a dual-stage retrieval that incorporates UAE and BM25.",
        "table": "S5.T2.4",
        "footnotes": [],
        "references": [
            "We present our overall architecture and the two-phase training scheme in Fig.  2 . Given query  q q q italic_q , the retriever will use a retrieval algorithm to fetch the top-K most relevant codes to  q q q italic_q  from external knowledge databases. Subsequently, the code refactorer then converts the retrieved code into concise and model-friendly code. After concatenating the refactored code with  q q q italic_q  using special tokens, the combined input is then fed to the generator to generate targeted code.",
            "To answer the RQ1, we evaluate the effectiveness of different generator models in code generation under various frameworks, and the experimental results are presented in Table  2 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 .  The effectiveness of various retrievers and how the generator performs with each of them. The generator is based on CodeGPT-java.",
        "table": "S5.T3.4",
        "footnotes": [],
        "references": [
            "ii) Preference gap.  From a human perspective, documents containing correct answers are the most helpful in RACG  ( rag,  ) . Therefore, the retriever is designed to retrieve the most relevant code to better assist the generator. However, our experiments reveal that code snippets more closely related to the reference answers do not necessarily enhance the generators effectiveness, as shown by the results presented in Table  3 . The model may prefer a code snippet with a clear code structure, low repetition, and novel knowledge not already incorporated into its parameterized knowledge during the pre-training process  ( devlin2018bert,  ) . The difference between the golden code snippet preferred by the retriever and the generator is termed the preference gap. The existence of this preference gap renders the current RACG framework inadequate. Some studies  ( shi2023replug,  ;  radit,  )  have attempted to align them by jointly fine-tuning them or utilizing feedback signals from the generator. However, some LLMs accessed through APIs  ( tian2023chatgpt,  )  cannot be fine-tuned in conjunction with the retriever, nor do they provide logits to compute certain feedback signals. Additionally, changes in the semantic representation of text following the retriever parameter tuning necessitate updating the external database index, which consumes substantial resources in practice.",
            "In Equation  3 , the policy loss function of the PPO algorithm is presented. This is a clipped objective function that improves the training stability of the policy by limiting the changes made to the policy during each training epoch.",
            "To answer RQ2, we compare the improvements of RRG on generator performance under different retriever settings, including sparse retrieval BM25 and two semantic retrieval models. Table  3  shows the retrieval results of these retrievers in naive RAG and their generator output. Fig.  3  shows the improvement of the generator by the RRG framework.",
            "Overall, as shown in Fig.  3 , the RRG framework can bridge the preference gap between the retriever and the generator by inserting a refactorer between them. On the ConCode dataset, RRG can improve the EM metric by up to 16.86%, BLEU by up to 12.81%, and CodeBLEU by up to 6.14%. On the CSN-java dataset, it can improve EM by up to 4.30%, BLEU by 6.94%, and CodeBLEU by 9.22%. This demonstrates the robustness of RRG in the face of different retrievers.  However, on the CSN-java dataset, we notice a decrease in the EM metric when using BM25 as the retriever, and the other two metrics do not improve as significantly as with other retrievers. We deem that this is caused by the preference between the retriever and the generator. As shown in Table  3 , on the CSN-java dataset, the BM25 algorithm retrieves the relevant code that can best guide the generator. The preference of the two is relatively the same. If we then introduce refactoring for the alignment, the opposite effect may be triggered. For other retrievers, the gap between the two is large, and the use of refactoring can significantly improve the final performance of the generator.",
            "To answer the RQ4, we investigate the impact of the Preference-aware Tuning phase on the effectiveness of RRG. We conducted an ablation study. The experimental results are presented in Table  4  and Fig.  3 . Overall, removing the preference-aware tuning phase resulted in poorer performance of RRG across different models, datasets, and code tokens configurations. This decline is attributed to the fact that after the SFT phase, the output of the refactorer is aligned with human preferences without considering the preferences of the generator. In contrast, when the Preference-aware Tuning is incorporated during training, the refactorers output is directly aligned with the generators preferences, helping RRG generate more effective code tokens."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 .  Ablation Study on ConCode and CodeSearchNet-java datasets. The retriever employs a dual-stage retrieval that incorporates UAE and BM25.",
        "table": "S5.T4.4",
        "footnotes": [],
        "references": [
            "To answer RQ3, in this set of experiments, we investigated the effect of different code token settings on the code generation performance of RRG. Specifically, we performed a comparative analysis by limiting the maximum number of output tokens of refactorer. The experimental results are shown in Fig.  4 . It is demonstrated that the length of code tokens has a significant impact on the code generation performance because different numbers of code tokens represent the amount of information passed to the generator. This characteristic directly affects the performance of the model in generating code.  Notably, we observe that the results show a trend similar to a normal distribution. When the number of code tokens is set low, the amount of information passed to the model is low, leading to relatively poor results. As the number of tokens increases, the performance gradually improves. However, it is worth noting that when the number of tokens reaches a certain threshold, too much information may lead to a redundancy phenomenon, which in turn reduces the code generation performance.  In the ConCode dataset, the effect of different information lengths on the model performance is significant, with a 12.02 difference between the extremes of accuracy. In the CodeSearchNet-java dataset, this difference reaches 30%. This suggests that the relationship between the amount of information and generation effectiveness needs to be carefully weighed in practical applications.",
            "To answer the RQ4, we investigate the impact of the Preference-aware Tuning phase on the effectiveness of RRG. We conducted an ablation study. The experimental results are presented in Table  4  and Fig.  3 . Overall, removing the preference-aware tuning phase resulted in poorer performance of RRG across different models, datasets, and code tokens configurations. This decline is attributed to the fact that after the SFT phase, the output of the refactorer is aligned with human preferences without considering the preferences of the generator. In contrast, when the Preference-aware Tuning is incorporated during training, the refactorers output is directly aligned with the generators preferences, helping RRG generate more effective code tokens."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 .  Generalization experiment. The original combinationsin the table mean where the frozen refactorer was trained. And we directly applied it to other combinations on the ConCode.",
        "table": "S5.T5.4",
        "footnotes": [],
        "references": [
            "We froze the trained refactorer and experimented with various retriever and generator combinations to evaluate the performance of RRG in code generation. The original combinations listed in the table  5  represent the configurations where the refactorer was initially trained. Experimental results indicate that even when frozen, the refactorer aligns well with diverse retriever and generator combinations, demonstrating strong generalization capabilities.",
            "We illustrate an example of RRGs effectiveness in Fig.  5 . When the relevant code provided by the retriever contains redundant information, the generator may be misled and produce incorrect code. The generated code closely resembles the retrieved code and is heavily influenced by the top-1 code. However, when using refactored code, the generator can produce correct code. Due to certain model preferences, such as a tendency to favor certain token locations or repetitions, the generator may focus more on the irrelevant tokens, resulting in incorrect output. In contrast, the refactored code retains the tokens that mislead the generator in the raw but is shorter and contains less redundant information. This higher-quality context allows the generator to produce correct answers more easily and avoid being misled. We show an example of preferences between different models in Fig.  6 . Language models are based on GPT2, while code models are based on CodeGPT. We found that the refactorer provides different codes for different generators. For GPT2, the  f  i  l  t  e  r f i l t e r filter italic_f italic_i italic_l italic_t italic_e italic_r  token plays a significant role in the refactored code, while CodeGPT is  e  l  e  m  e  n  t  s e l e m e n t s elements italic_e italic_l italic_e italic_m italic_e italic_n italic_t italic_s  token and  d  e  c  i  s  i  o  n d e c i s i o n decision italic_d italic_e italic_c italic_i italic_s italic_i italic_o italic_n  token. We interpret this phenomenon as a result of inherent preference differences among various LLMs. Due to different internal knowledge, generating a piece of code requires different codes to be provided."
        ]
    },
    "global_footnotes": []
}