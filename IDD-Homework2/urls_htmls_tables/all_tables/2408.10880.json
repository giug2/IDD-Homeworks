{
    "Sx5.T1.1": {
        "caption": [],
        "table": "<table id=\"Sx5.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"Sx5.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"Sx5.T1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">T.B.</th>\n<th id=\"Sx5.T1.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\">P.B.</th>\n<th id=\"Sx5.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Params</th>\n<th id=\"Sx5.T1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\">FPS</th>\n<th id=\"Sx5.T1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">mAP(↑)</th>\n<th id=\"Sx5.T1.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">NDS(↑)</th>\n<th id=\"Sx5.T1.1.1.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">mATE(↓)</th>\n<th id=\"Sx5.T1.1.1.1.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">mASE(↓)</th>\n<th id=\"Sx5.T1.1.1.1.9\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">mAOE(↓)</th>\n<th id=\"Sx5.T1.1.1.1.10\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">mAVE(↓)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"Sx5.T1.1.2.1\" class=\"ltx_tr\">\n<th id=\"Sx5.T1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">CLIP Text Encoder</th>\n<th id=\"Sx5.T1.1.2.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">PointPillars</th>\n<th id=\"Sx5.T1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">52M</th>\n<th id=\"Sx5.T1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">23</th>\n<td id=\"Sx5.T1.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.3975</td>\n<td id=\"Sx5.T1.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">0.5528</td>\n<td id=\"Sx5.T1.1.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">0.3342</td>\n<td id=\"Sx5.T1.1.2.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\">0.2671</td>\n<td id=\"Sx5.T1.1.2.1.9\" class=\"ltx_td ltx_align_center ltx_border_t\">0.2911</td>\n<td id=\"Sx5.T1.1.2.1.10\" class=\"ltx_td ltx_align_center ltx_border_t\">0.2735</td>\n</tr>\n<tr id=\"Sx5.T1.1.3.2\" class=\"ltx_tr\">\n<th id=\"Sx5.T1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">CLIP Text Encoder</th>\n<th id=\"Sx5.T1.1.3.2.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SSN</th>\n<th id=\"Sx5.T1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">130M</th>\n<th id=\"Sx5.T1.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">8</th>\n<td id=\"Sx5.T1.1.3.2.5\" class=\"ltx_td ltx_align_center\">0.4413</td>\n<td id=\"Sx5.T1.1.3.2.6\" class=\"ltx_td ltx_align_center\">0.5717</td>\n<td id=\"Sx5.T1.1.3.2.7\" class=\"ltx_td ltx_align_center\">0.2916</td>\n<td id=\"Sx5.T1.1.3.2.8\" class=\"ltx_td ltx_align_center\">0.2008</td>\n<td id=\"Sx5.T1.1.3.2.9\" class=\"ltx_td ltx_align_center\">0.2529</td>\n<td id=\"Sx5.T1.1.3.2.10\" class=\"ltx_td ltx_align_center\">0.2457</td>\n</tr>\n<tr id=\"Sx5.T1.1.4.3\" class=\"ltx_tr\">\n<th id=\"Sx5.T1.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">CLIP Text Encoder</th>\n<th id=\"Sx5.T1.1.4.3.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Centerformer</th>\n<th id=\"Sx5.T1.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">177M</th>\n<th id=\"Sx5.T1.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">5</th>\n<td id=\"Sx5.T1.1.4.3.5\" class=\"ltx_td ltx_align_center\">0.4622</td>\n<td id=\"Sx5.T1.1.4.3.6\" class=\"ltx_td ltx_align_center\">0.6149</td>\n<td id=\"Sx5.T1.1.4.3.7\" class=\"ltx_td ltx_align_center\">0.2872</td>\n<td id=\"Sx5.T1.1.4.3.8\" class=\"ltx_td ltx_align_center\">0.2289</td>\n<td id=\"Sx5.T1.1.4.3.9\" class=\"ltx_td ltx_align_center\">0.2437</td>\n<td id=\"Sx5.T1.1.4.3.10\" class=\"ltx_td ltx_align_center\">0.2418</td>\n</tr>\n<tr id=\"Sx5.T1.1.5.4\" class=\"ltx_tr\">\n<th id=\"Sx5.T1.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">BERT-base</th>\n<th id=\"Sx5.T1.1.5.4.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">OpenSECOND</th>\n<th id=\"Sx5.T1.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">122M</th>\n<th id=\"Sx5.T1.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">9</th>\n<td id=\"Sx5.T1.1.5.4.5\" class=\"ltx_td ltx_align_center\">0.4171</td>\n<td id=\"Sx5.T1.1.5.4.6\" class=\"ltx_td ltx_align_center\">0.5862</td>\n<td id=\"Sx5.T1.1.5.4.7\" class=\"ltx_td ltx_align_center\">0.3118</td>\n<td id=\"Sx5.T1.1.5.4.8\" class=\"ltx_td ltx_align_center\">0.2589</td>\n<td id=\"Sx5.T1.1.5.4.9\" class=\"ltx_td ltx_align_center\">0.2795</td>\n<td id=\"Sx5.T1.1.5.4.10\" class=\"ltx_td ltx_align_center\">0.2519</td>\n</tr>\n<tr id=\"Sx5.T1.1.6.5\" class=\"ltx_tr\">\n<th id=\"Sx5.T1.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">CLIP Text Encoder</th>\n<th id=\"Sx5.T1.1.6.5.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">OpenSECOND</th>\n<th id=\"Sx5.T1.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\">76M</th>\n<th id=\"Sx5.T1.1.6.5.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\">14</th>\n<td id=\"Sx5.T1.1.6.5.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.4455</td>\n<td id=\"Sx5.T1.1.6.5.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.5924</td>\n<td id=\"Sx5.T1.1.6.5.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.2920</td>\n<td id=\"Sx5.T1.1.6.5.8\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.2573</td>\n<td id=\"Sx5.T1.1.6.5.9\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.2881</td>\n<td id=\"Sx5.T1.1.6.5.10\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.2675</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Our detection accuracy is comparable to that of specialized 3D detection models, despite not employing additional training techniques to enhance performance. This is intentional, as our primary objective is to develop a general open vocabulary model capable of seamlessly integrating new textual information. The test results on the Nuscenes-T dataset are presented in Table. 1.",
            "To evaluate the scalability and versatility of our framework, we conducted ablation experiments focusing on both the text backbone and the point cloud backbone. We compared the CLIP Text Encoder and BERT-base for the text backbone, with both models kept frozen during the tests. For the point cloud backbone, we evaluated PointPillars, SSN (Zhu et al. 2020), CenterFormer (Zhou et al. 2022), and our proposed OpenSECOND. These comparisons included the number of parameters, inference time, and key performance metrics. The results, shown in Table 1, highlight the trade-off between mean average precision (mAP) and inference speed. The ”Params” column refers to the total number of parameters across the entire model, including the backbone, fusion module, and heads."
        ]
    }
}{
    "Sx5.T2.1": {
        "caption": [
            "trained on Lyft Level 5 dataset"
        ],
        "table": "<table id=\"Sx5.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"Sx5.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"Sx5.T2.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\">Model</th>\n<th id=\"Sx5.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">car</th>\n<th id=\"Sx5.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">bus</th>\n<th id=\"Sx5.T2.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">truck</th>\n<th id=\"Sx5.T2.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">bi.</th>\n<th id=\"Sx5.T2.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">pe.</th>\n<th id=\"Sx5.T2.1.1.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">motor.</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"Sx5.T2.1.2.1\" class=\"ltx_tr\">\n<th id=\"Sx5.T2.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">P.P.</th>\n<td id=\"Sx5.T2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.422</td>\n<td id=\"Sx5.T2.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.177</td>\n<td id=\"Sx5.T2.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.176</td>\n<td id=\"Sx5.T2.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.099</td>\n<td id=\"Sx5.T2.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">0.066</td>\n<td id=\"Sx5.T2.1.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">0.049</td>\n</tr>\n<tr id=\"Sx5.T2.1.3.2\" class=\"ltx_tr\">\n<th id=\"Sx5.T2.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">Ours</th>\n<td id=\"Sx5.T2.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.279</td>\n<td id=\"Sx5.T2.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.106</td>\n<td id=\"Sx5.T2.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.089</td>\n<td id=\"Sx5.T2.1.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.047</td>\n<td id=\"Sx5.T2.1.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.043</td>\n<td id=\"Sx5.T2.1.3.2.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.018</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "To evaluate zero-shot performance, Open3DWorld was run on the Lyft Level 5 dataset. Results 2 indicated that the fusion model, leveraging BEV features and text features, can align theoretically to achieve classification.As we all know, in the field of point cloud object detection, when we use a model trained on one dataset to test on another dataset, the model will crash.Our method does not require training and can achieve preliminary results on Lyft Level 5 dataset, which shows that after the text features and BEV features are aligned, the generalization is much better than the traditional detection model."
        ]
    }
}