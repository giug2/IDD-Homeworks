{
    "id_table_1": {
        "caption": "Table 1 :  LPIPS  metrics for the five transformations employed in training the classification model.",
        "table": "S3.T1.4",
        "footnotes": [],
        "references": [
            "Towards the final step of controlled augmentation generation, existing medical imaging research for developing transformations in the GAN latent space predominantly rely on classification models. While these models have been instrumental in driving advances in image synthesis and manipulation, they come with significant drawbacks, as mentioned in   Section   1 . To address these concerns, we explore factorizing the latent space of the generator model as an alternative approach to extract semantics in an unsupervised manner. Our proposed pipeline significantly reduces the dependency on scarce and costly labeled data. This unsupervised approach is inherently more scalable, as it can leverage vast amounts of unlabeled data, which is more readily available, thus facilitating the training of models on a broader spectrum of semantic variations.",
            "The proposed transformation development pipeline (see  Fig.   1 ) consists of four stages that perform sequential tasks to achieve the goal of unsupervised semantic extraction. The end product is an unsupervised technique for transformation development, enabling the semi-automatic extraction of extensive semantic transformations reflected within a dataset and corresponding domain.",
            "We compared the predictive performance of a skin lesion classification model trained on data augmented with synthetic images with a baseline. As baseline scenario, we trained on the original HAM10000  [ 36 ]  dermatoscopy training dataset split and evaluated the performance of the model using the original test dataset split. HAM10000 includes 10,015 high-quality dermatoscopic images in the training set and 1,512 image in the test set. Each image is labeled with one specific diagnosis (see  Section   2.2.1  above). Class label distribution is highly imbalanced in training and test set, with NV being over-represented with a share of 60 % respectively 67 %, while the other six classes share the remaining fraction to varying degrees.",
            "We calculated the  LPIPS  metrics for the five transformations used in augmenting the training datasets (see  Tab.   1 .)  LPIPS  score ranges between 0 to 1 where a lower  LPIPS  score denotes higher perceptual similarity.",
            "The confusion matrices in Fig.  6  reveal a significant improvement of the models ability to correctly classify samples from class MEL. Whereas the baseline model only classified 54% of true Melanoma samples correctly, the model trained on (filtered) synthetic samples correctly labeled 67% of these samples. As the baseline model misclassified many true MEL test samples as BKL, which is particularly dangerous as this is a benign class, we further analyze the prediction behavior for this set of test samples. Specifically, we apply  Concept Relevance Propagation (CRP)   [ 3 ]  to compute concept-based explanations for individual predictions.  CRP  disentangles local explanations into concept-specific explanations. The concepts are defined by individual neurons in a chosen layer (e.g., last Conv layer) and their relevance scores can be computed with backpropagation-based explainers, for instance  Layer-wise Relevance Propagation (LRP)   [ 7 ] . The concepts can be visualized in a human-understandable manner by a set of representative samples from a reference dataset, e.g., the training data. Fig.  7  shows  CRP  explanations for a test sample misclassified as BKL by the baseline model ( left ) but classified correctly by the model augmented with synthetic data ( right ). While the baseline model is distracted by surroundings (e.g., concept 242), the augmented model uses features easier to interpret and more related to the task, such as the border of the mole (concept 274). We include Figure  B.1  in the appendices, which shows explanations of the MEL class for the baseline model. These explanations reveal that the baseline model is not capable of detecting any interesting features which indicate membership to the MEL class, as opposed to the augmented model. Furthermore, to understand the global prediction (sub-)strategies employed by the model, we compute  Prototypical Concept-based eXplanations (PCX)   [ 16 ]  for class MEL. These explanations can be found in Appendix  C .",
            "CRP requires preselecting an output neuron to explain the network decision to. Whichever output we choose, the heatmaps will show how relevant each part of the input is for this output. Figure  B.1  shows explanations for the wrong classification class. Here we provide explanations for the ground truth class. They indicate that the baseline model is incapable of finding any supporting evidence for the MEL class, contrary to the augmented model.",
            "Specifically,  PCX  clusters latent relevances, for instance obtained with  LRP , for samples from one class (here: MEL), followed by a cluster analysis, e.g., with Gaussian Mixture Models. This produces clusters of samples for which the model uses similar prediction strategies. Each cluster can be represented with prototypical samples and viewed as distribution over concepts. These concepts can further be visualized using  CRP . Figures  D.1  and  D.2  portray global explanations for the MEL class, for the models trained on the vanilla and synthetically augmented datasets, respectively. Specifically, columns show prototypes per cluster and rows represent concepts visualized with CRP. The values in the matrix indicate how much a concept is used by a prototype. Note that each sub-strategy can be considered as a distribution over concepts. The weight of each concept per substrategy is visualized as percentage in the matrix. While the baseline model heavily relies on distractor concepts, such as concepts 624 and 180, focusing on hair and skin markers, the model augmented with synthetic data uses clean and, to the best of our knowledge, clinically meaningful features.",
            "Figures  D.1  and  D.2  portray global explanations for the MEL class, for the models trained on the vanilla and synthetically augmented datasets, respectively. The columns in the figures correspond to different substrategies from the classification model, as discovered by a Gaussian Mixture Model trained on latent relevance scores. Subtrategies are visualized with exemplary prototypes from the training dataset. The rows correspond to different concepts on a preselected layer (here: activations  after  the last transition block) and show CRP-style concept representatives. Note, that each sub-strategy can be considered as distribution over concepts. The weight of each concept per substrategy is visualized as percentage in the matrix. While the baseline model heavily relies on distractor concepts, such as concepts 624 and 180, focusing on hair and skin markers, the model augmented with synthetic data uses clean and, to the best of our knowledge, clinically meaningful features."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Classification performance improvement of the synthetically augmented models in comparison to the baseline model (trained with real training data only), measured with weighted multi-class accuracy.",
        "table": "S3.T2.4",
        "footnotes": [],
        "references": [
            "The overall augmentation pipeline, detailed in  Section   2.2 , progresses step-by-step from a set of original images to the final outputs, incorporating semantic variations based on semi-automatically extracted features into the original images. First, we apply closed-form factorization to identify meaningful and orthogonal latent semantic directions within the latent space. Next, we utilize the GAN inversion function to map real images into the latent space accurately. Finally, using the semantic directions extracted through factorization, we produce new variants of the original images based on the identified semantics. This approach allows for the exploration of a broad range of semantic variations without the need for labeled data, ensuring that the synthetic outputs closely resemble the original inputs.",
            "The models performance was evaluated using the Frechet Inception Distance (FID)  [ 20 ] , which yielded a score of approximately 3.7, indicating a high level of conformance in distributional similarity between the generated and real images.  Fig.   2  showcases samples of synthetically generated dermatoscopic skin images, demonstrating the photorealism achieved by our trained model.",
            "We compared the predictive performance of a skin lesion classification model trained on data augmented with synthetic images with a baseline. As baseline scenario, we trained on the original HAM10000  [ 36 ]  dermatoscopy training dataset split and evaluated the performance of the model using the original test dataset split. HAM10000 includes 10,015 high-quality dermatoscopic images in the training set and 1,512 image in the test set. Each image is labeled with one specific diagnosis (see  Section   2.2.1  above). Class label distribution is highly imbalanced in training and test set, with NV being over-represented with a share of 60 % respectively 67 %, while the other six classes share the remaining fraction to varying degrees.",
            "We evaluated the model performance on the 1,512 images (512 x 512 pixels) of the original HAM10000 test split, which were neither transformed nor seen during training, using balanced multi-class accuracy. First, we compared our results with the existing benchmark [ 1 ]  (Task 3: Lesion Diagnosis) in the ISIC2018 challenge. Our best performing model was based on the DenseNet169 architecture, synthetically augmented with 6000 additional synthetic images (60 % of the original training dataset), achieved a balanced accuracy of 0.856 (see  Table   2 ). Comparing with other models evaluated in the challenge  [ 1 ] , we ranked 3rd on the evaluation metrics, with only the two ensemble based methods achieving a higher average balanced accuracy of 0.885 (Top 10 Models Averaged)  [ 29 ]  and 0.856 (Large Ensemble with heavy multi-cropping and loss weighting)  [ 17 ]  respectively. Our model even surpasses the larger DenseNet201 on rank 4 in the challenge with 0.815 (densenet submitted by Li and Li  [ 1 ] ).",
            "Then, we compared the performance of the synthetically augmented classification models with the baseline model. In  Tab.   2 , we can observe that all synthetically augmented models outperform the baseline model by 1.9% to 3.5%. However, the performance does not always increase with the number of synthetic images added to the original dataset and seems to plateau after adding 6,000 images to the original training dataset. We also observe that the filtered method helps to increase the performance gain only to a certain point. This suggests that more research is needed to understand the nature of the synthetic images that increase and/or decrease the models performance.",
            "Specifically,  PCX  clusters latent relevances, for instance obtained with  LRP , for samples from one class (here: MEL), followed by a cluster analysis, e.g., with Gaussian Mixture Models. This produces clusters of samples for which the model uses similar prediction strategies. Each cluster can be represented with prototypical samples and viewed as distribution over concepts. These concepts can further be visualized using  CRP . Figures  D.1  and  D.2  portray global explanations for the MEL class, for the models trained on the vanilla and synthetically augmented datasets, respectively. Specifically, columns show prototypes per cluster and rows represent concepts visualized with CRP. The values in the matrix indicate how much a concept is used by a prototype. Note that each sub-strategy can be considered as a distribution over concepts. The weight of each concept per substrategy is visualized as percentage in the matrix. While the baseline model heavily relies on distractor concepts, such as concepts 624 and 180, focusing on hair and skin markers, the model augmented with synthetic data uses clean and, to the best of our knowledge, clinically meaningful features.",
            "Figures  D.1  and  D.2  portray global explanations for the MEL class, for the models trained on the vanilla and synthetically augmented datasets, respectively. The columns in the figures correspond to different substrategies from the classification model, as discovered by a Gaussian Mixture Model trained on latent relevance scores. Subtrategies are visualized with exemplary prototypes from the training dataset. The rows correspond to different concepts on a preselected layer (here: activations  after  the last transition block) and show CRP-style concept representatives. Note, that each sub-strategy can be considered as distribution over concepts. The weight of each concept per substrategy is visualized as percentage in the matrix. While the baseline model heavily relies on distractor concepts, such as concepts 624 and 180, focusing on hair and skin markers, the model augmented with synthetic data uses clean and, to the best of our knowledge, clinically meaningful features."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  AUC ROC per class for baseline, and un/filtered models (best in bold font).",
        "table": "S3.T3.4",
        "footnotes": [],
        "references": [
            "Fig.   3  showcases the inversion results of several original images. The faithful reconstruction achieved through the synergy between the encoder and HyperStyle components demonstrates the success of the inversion process in capturing intricate semantic properties and detail from the original images.",
            "From here on we will report only on the DenseNet169, as it consistently outperforms the smaller DenseNet121.  Fig.   5  shows the recall results for each diagnostic class for the un/filtered synthetically augmented models in comparison to the baseline model. Synthetically augmented (un/filtered) models show a better recall for the classes MEL, BCC, and DF, when considering all sizes of augmented data. Note that all three are underrepresented classes, which shows our synthetic augmentations are fit to counteract class imbalances. The synthetically augmented models have better recall performance for all classes, except for BKL and VASC, when only considering the SA-6k and SA-6k-filter.  Fig.   6  shows the confusion matrices for the baseline, un/filtered synthetically augmented models (using 6000 augmented images).  Tab.   3  shows the area under the curve of the receiver operating characteristic (AUC ROC) for each class for the same models. The table demonstrates that both optimized models (SA-6k and SA-6k-filter) outperform the baseline model in almost all classes. Furthermore, the average AUC ROC for SA-6k is 0.945, higher than the baselines 0.924, indicating a general performance boost. Meanwhile, the average AUC ROC for SA-6k-filter is the highest at 0.947, suggesting it is the most effective model overall. This indicates that the additional enhancements and filtering techniques applied in SA-6k-filter lead to the most reliable and accurate model for distinguishing between different types of skin lesions."
        ]
    }
}