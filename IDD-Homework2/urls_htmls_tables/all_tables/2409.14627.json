{
    "S4.T1.6": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T1.6\">\n<tr class=\"ltx_tr\" id=\"S4.T1.6.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T1.6.4.5\">Object Prior</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.6.4.6\">AP</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.3.1.1\">AR<sub class=\"ltx_sub\" id=\"S4.T1.3.1.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S4.T1.3.1.1.1.1\">100</span></sub>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.4.2.2\">F<sub class=\"ltx_sub\" id=\"S4.T1.4.2.2.1\">1</sub>\n</td>\n<td class=\"ltx_td ltx_border_r ltx_border_tt\" id=\"S4.T1.6.4.7\"/>\n<td class=\"ltx_td ltx_border_tt\" id=\"S4.T1.6.4.8\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T1.6.4.9\">Object Prior</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.6.4.10\">AP</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.5.3.3\">AR<sub class=\"ltx_sub\" id=\"S4.T1.5.3.3.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S4.T1.5.3.3.1.1\">100</span></sub>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S4.T1.6.4.4\">F<sub class=\"ltx_sub\" id=\"S4.T1.6.4.4.1\">1</sub>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.6.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.6.5.1\">SOS + <span class=\"ltx_text ltx_font_italic\" id=\"S4.T1.6.5.1.1\">Grid</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.6.5.2\">3.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.6.5.3\">36.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.6.5.4\">6.9</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" id=\"S4.T1.6.5.5\"/>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T1.6.5.6\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.6.5.7\">SOS + <span class=\"ltx_text ltx_font_italic\" id=\"S4.T1.6.5.7.1\">DeepGaze</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.6.5.8\">5.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.6.5.9\">35.9</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S4.T1.6.5.10\">9.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.6.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.6.6.1\">SOS + <span class=\"ltx_text ltx_font_italic\" id=\"S4.T1.6.6.1.1\">Dist</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.6.6.2\">3.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.6.6.3\">27.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.6.6.4\">6.0</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S4.T1.6.6.5\"/>\n<td class=\"ltx_td\" id=\"S4.T1.6.6.6\"/>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.6.6.7\">SOS + <span class=\"ltx_text ltx_font_italic\" id=\"S4.T1.6.6.7.1\">CAM</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.6.6.8\">5.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.6.6.9\">36.7</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T1.6.6.10\">9.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.6.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.6.7.1\">SOS + <span class=\"ltx_text ltx_font_italic\" id=\"S4.T1.6.7.1.1\">Spx</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.6.7.2\">5.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.6.7.3\">34.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.6.7.4\">9.6</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S4.T1.6.7.5\"/>\n<td class=\"ltx_td\" id=\"S4.T1.6.7.6\"/>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.6.7.7\">SOS + <span class=\"ltx_text ltx_font_italic\" id=\"S4.T1.6.7.7.1\">DINO</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.6.7.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.6.7.8.1\">8.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.6.7.9\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.6.7.9.1\">38.1</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T1.6.7.10\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.6.7.10.1\">14.4</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.6.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.6.8.1\">SOS + <span class=\"ltx_text ltx_font_italic\" id=\"S4.T1.6.8.1.1\">Contour</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.6.8.2\">5.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.6.8.3\">36.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.6.8.4\">9.7</td>\n<td class=\"ltx_td ltx_border_r\" id=\"S4.T1.6.8.5\"/>\n<td class=\"ltx_td\" id=\"S4.T1.6.8.6\"/>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.6.8.7\">SOS + <span class=\"ltx_text ltx_font_italic\" id=\"S4.T1.6.8.7.1\">U-Net</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.6.8.8\">7.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.6.8.9\">37.3</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T1.6.8.10\">12.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.6.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T1.6.9.1\">SOS + <span class=\"ltx_text ltx_font_italic\" id=\"S4.T1.6.9.1.1\">VOCUS2</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.6.9.2\">6.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.6.9.3\">37.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.6.9.4\">10.5</td>\n<td class=\"ltx_td ltx_border_bb ltx_border_r\" id=\"S4.T1.6.9.5\"/>\n<td class=\"ltx_td ltx_border_bb\" id=\"S4.T1.6.9.6\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T1.6.9.7\">SOS + <span class=\"ltx_text ltx_font_italic\" id=\"S4.T1.6.9.7.1\">GT</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.6.9.8\">18.1*</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.6.9.9\">42.5*</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"S4.T1.6.9.10\">25.4*</td>\n</tr>\n</table>\n\n",
        "caption": "Table 1 :  Results of our SOS with various object priors in the cross-category COCO  (VOC) → COCO → (VOC) COCO \\text{(VOC)}\\rightarrow\\text{COCO} (VOC) → COCO  (non-VOC) setting. *: Uses ground truth of unknown classes.",
        "footnotes": [],
        "references": [
            "Table 1 shows the results of our object prior study. First, all priors outperform the baselines Grid and Dist in terms of F1. Both baselines exhibit a lower AP compared to all other object priors, reflecting the missing focus on objects. Hence, simply applying SAM (Grid) is not suitable to segment only objects, as several stuff regions are segmented as well. Second, most priors exhibit a similar performance in terms of all measures. For instance, learning-based CAM and DeepGaze do not outperform simple priors (Spx, Contour, VOCUS2). Only the priors U-Net and DINO perform substantially better, with DINO producing the best result. Comparing DINO to GT reveals that DINO is able to recall almost the same amount of objects, but exhibits a lower precision, as expected."
        ]
    },
    "S5.T3.8.8": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S5.T3.8.8\">\n<tr class=\"ltx_tr\" id=\"S5.T3.6.6.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T3.6.6.2.3\">System</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T3.6.6.2.4\">AP</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T3.5.5.1.1\">AR<sub class=\"ltx_sub\" id=\"S5.T3.5.5.1.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T3.5.5.1.1.1.1\">100</span></sub>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S5.T3.6.6.2.2\">F<sub class=\"ltx_sub\" id=\"S5.T3.6.6.2.2.1\">1</sub>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.8.8.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.8.8.5.1\">Mask R-CNN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.14627v1#bib.bib19\" title=\"\">19</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.8.8.5.2\">1.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.8.8.5.3\">8.2</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T3.8.8.5.4\">1.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.7.7.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.7.7.3.1\">SAM<sup class=\"ltx_sup\" id=\"S5.T3.7.7.3.1.1\">&#8224;</sup>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.14627v1#bib.bib27\" title=\"\">27</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.7.7.3.2\">3.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.7.7.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.7.7.3.3.1\">48.1</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S5.T3.7.7.3.4\">6.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.8.8.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.8.8.6.1\">OLN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.14627v1#bib.bib26\" title=\"\">26</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.8.8.6.2\">4.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.8.8.6.3\">28.4</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T3.8.8.6.4\">7.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.8.8.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.8.8.7.1\">LDET&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.14627v1#bib.bib46\" title=\"\">46</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.8.8.7.2\">4.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.8.8.7.3\">24.8</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S5.T3.8.8.7.4\">7.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.8.8.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.8.8.8.1\">GGN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.14627v1#bib.bib49\" title=\"\">49</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.8.8.8.2\">4.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.8.8.8.3\">28.3</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S5.T3.8.8.8.4\">8.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.8.8.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.8.8.9.1\">SWORD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.14627v1#bib.bib54\" title=\"\">54</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.8.8.9.2\">4.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.8.8.9.3\">30.2</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S5.T3.8.8.9.4\">8.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.8.8.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.8.8.10.1\">UDOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.14627v1#bib.bib24\" title=\"\">24</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.8.8.10.2\">2.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.8.8.10.3\">34.3</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S5.T3.8.8.10.4\">5.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.8.8.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S5.T3.8.8.4.1\">SOS<sup class=\"ltx_sup\" id=\"S5.T3.8.8.4.1.1\">&#8224;</sup> (ours)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T3.8.8.4.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.8.8.4.2.1\">8.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T3.8.8.4.3\">39.3</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T3.8.8.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.8.8.4.4.1\">14.5</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 2 :  Results of two baselines and various OWIS methods in the COCO  (VOC) → COCO → (VOC) COCO \\text{(VOC)}\\rightarrow\\text{COCO} (VOC) → COCO  (non-VOC) setting.  † : uses automatically annotated SA-1B dataset.",
        "footnotes": [
            "[19] \nHe, K., Gkioxari, G., Dollár, P., Girshick, R.: Mask R-CNN. In:\nInternational Conference on Computer Vision (2017)\n\n",
            "[27] \nKirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao,\nT., Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. In:\nInternational Conference on Computer Vision (2023)\n\n",
            "[26] \nKim, D., Lin, T.Y., Angelova, A., Kweon, I.S., Kuo, W.: Learning open-world\nobject proposals without learning to classify. IEEE Robotics and Automation\nLetters  7 (2) (2022)\n\n",
            "[46] \nSaito, K., Hu, P., Darrell, T., Saenko, K.: Learning to detect every thing in\nan open world. In: European Conference on Computer Vision (2022)\n\n",
            "[49] \nWang, W., Feiszli, M., Wang, H., Malik, J., Tran, D.: Open-world instance\nsegmentation: Exploiting pseudo ground truth from learned pairwise affinity.\nIn: Conference on Computer Vision and Pattern Recognition (2022)\n\n",
            "[54] \nWu, J., Jiang, Y., Yan, B., Lu, H., Yuan, Z., Luo, P.: Exploring transformers\nfor open-world instance segmentation. In: International Conference on\nComputer Vision (2023)\n\n",
            "[24] \nKalluri, T., Wang, W., Wang, H., Chandraker, M., Torresani, L., Tran, D.:\nOpen-world instance segmentation: Top-down learning with bottom-up\nsupervision. arXiv preprint arXiv:2303.05503 (2023)\n\n",
            "[19] \nHe, K., Gkioxari, G., Dollár, P., Girshick, R.: Mask R-CNN. In:\nInternational Conference on Computer Vision (2017)\n\n",
            "[27] \nKirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao,\nT., Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. In:\nInternational Conference on Computer Vision (2023)\n\n",
            "[46] \nSaito, K., Hu, P., Darrell, T., Saenko, K.: Learning to detect every thing in\nan open world. In: European Conference on Computer Vision (2022)\n\n",
            "[49] \nWang, W., Feiszli, M., Wang, H., Malik, J., Tran, D.: Open-world instance\nsegmentation: Exploiting pseudo ground truth from learned pairwise affinity.\nIn: Conference on Computer Vision and Pattern Recognition (2022)\n\n",
            "[56] \nXue, X., Yu, D., Liu, L., Liu, Y., Li, Y., Yuan, Z., Song, P., Shou, M.Z.:\nSingle-stage open-world instance segmentation with cross-task consistency\nregularization. arXiv preprint arXiv:2208.09023 (2022)\n\n",
            "[48] \nWang, C., Wang, G., Zhang, Q., Guo, P., Liu, W., Wang, X.: OpenInst: A simple\nquery-based method for open-world instance segmentation. arXiv preprint\narXiv:2303.15859 (2023)\n\n",
            "[24] \nKalluri, T., Wang, W., Wang, H., Chandraker, M., Torresani, L., Tran, D.:\nOpen-world instance segmentation: Top-down learning with bottom-up\nsupervision. arXiv preprint arXiv:2303.05503 (2023)\n\n"
        ],
        "references": [
            "Table 3 presents the results of various OWIS methods on the cross-category setup COCO (VOC)→COCO→(VOC)COCO\\text{(VOC)}\\rightarrow\\text{COCO}(VOC) → COCO (non-VOC). Our SOS clearly outperforms all OWIS methods in AR100, AP, and F1. Specifically, SOS outperforms the second-best system in terms of F1, GGN [49], by 6.1. While some of the improvement comes from a better recall (+38.9% compared to GGN), the results are driven by a much-improved precision (+81.6% compared to GGN). This reflects the high quality of the pseudo annotations in SOS on this cross-category setting. Moreover, there is a clear improvement of SOS over Mask R-CNN (+12.7 in F1). Compared to original SAM, the DINO-based SOS substantially improves the precision by focusing on objects, leading to an improved F1 score (+7.8).",
            "We evaluate the cross-dataset generalization capabilities of OWIS methods, starting with COCO→LVIS→COCOLVIS\\text{COCO}\\rightarrow\\text{LVIS}COCO → LVIS. The results in Tab. 3 show a trend similar to the cross-category results. SOS outperforms all OWIS methods across all measures with an improvement of 2.8 in F1 compared to second-best methods GGN [49] and LDET [46]. Similar to the previous results, the improvement is based on gains in both recall and precision, however, the relative improvements in AP and AR100 are more similar to each other. Overall, our high-quality pseudo annotations in SOS lead to new state-of-the-art results, which indicate strong generalization to unknown object classes outside COCO."
        ]
    },
    "S5.T3.16.8": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S5.T3.16.8\">\n<tr class=\"ltx_tr\" id=\"S5.T3.14.6.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T3.14.6.2.3\">System</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T3.14.6.2.4\">AP</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T3.13.5.1.1\">AR<sub class=\"ltx_sub\" id=\"S5.T3.13.5.1.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T3.13.5.1.1.1.1\">100</span></sub>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S5.T3.14.6.2.2\">F<sub class=\"ltx_sub\" id=\"S5.T3.14.6.2.2.1\">1</sub>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.16.8.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.16.8.5.1\">Mask R-CNN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.14627v1#bib.bib19\" title=\"\">19</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.16.8.5.2\">7.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.16.8.5.3\">23.6</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T3.16.8.5.4\">11.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.15.7.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.15.7.3.1\">SAM<sup class=\"ltx_sup\" id=\"S5.T3.15.7.3.1.1\">&#8224;</sup>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.14627v1#bib.bib27\" title=\"\">27</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.15.7.3.2\">6.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.15.7.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.15.7.3.3.1\">45.1</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S5.T3.15.7.3.4\">11.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.16.8.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.16.8.6.1\">LDET&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.14627v1#bib.bib46\" title=\"\">46</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.16.8.6.2\">6.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.16.8.6.3\">24.8</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T3.16.8.6.4\">10.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.16.8.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.16.8.7.1\">GGN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.14627v1#bib.bib49\" title=\"\">49</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.16.8.7.2\">6.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.16.8.7.3\">27.0</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S5.T3.16.8.7.4\">10.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.16.8.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.16.8.8.1\">SOIS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.14627v1#bib.bib56\" title=\"\">56</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.16.8.8.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.16.8.8.3\">25.2</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S5.T3.16.8.8.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.16.8.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.16.8.9.1\">OpenInst&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.14627v1#bib.bib48\" title=\"\">48</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.16.8.9.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.16.8.9.3\">29.3</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S5.T3.16.8.9.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.16.8.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.16.8.10.1\">UDOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.14627v1#bib.bib24\" title=\"\">24</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.16.8.10.2\">3.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.16.8.10.3\">24.9</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S5.T3.16.8.10.4\">6.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.16.8.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S5.T3.16.8.4.1\">SOS<sup class=\"ltx_sup\" id=\"S5.T3.16.8.4.1.1\">&#8224;</sup> (ours)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T3.16.8.4.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.16.8.4.2.1\">8.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T3.16.8.4.3\">33.3</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T3.16.8.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.16.8.4.4.1\">13.3</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 3 :  Results of two baselines and various OWIS methods in the  COCO → LVIS → COCO LVIS \\text{COCO}\\rightarrow\\text{LVIS} COCO → LVIS  setting.  † : uses automatically annotated SA-1B dataset.",
        "footnotes": [
            "[19] \nHe, K., Gkioxari, G., Dollár, P., Girshick, R.: Mask R-CNN. In:\nInternational Conference on Computer Vision (2017)\n\n",
            "[27] \nKirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao,\nT., Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. In:\nInternational Conference on Computer Vision (2023)\n\n",
            "[26] \nKim, D., Lin, T.Y., Angelova, A., Kweon, I.S., Kuo, W.: Learning open-world\nobject proposals without learning to classify. IEEE Robotics and Automation\nLetters  7 (2) (2022)\n\n",
            "[46] \nSaito, K., Hu, P., Darrell, T., Saenko, K.: Learning to detect every thing in\nan open world. In: European Conference on Computer Vision (2022)\n\n",
            "[49] \nWang, W., Feiszli, M., Wang, H., Malik, J., Tran, D.: Open-world instance\nsegmentation: Exploiting pseudo ground truth from learned pairwise affinity.\nIn: Conference on Computer Vision and Pattern Recognition (2022)\n\n",
            "[54] \nWu, J., Jiang, Y., Yan, B., Lu, H., Yuan, Z., Luo, P.: Exploring transformers\nfor open-world instance segmentation. In: International Conference on\nComputer Vision (2023)\n\n",
            "[24] \nKalluri, T., Wang, W., Wang, H., Chandraker, M., Torresani, L., Tran, D.:\nOpen-world instance segmentation: Top-down learning with bottom-up\nsupervision. arXiv preprint arXiv:2303.05503 (2023)\n\n",
            "[19] \nHe, K., Gkioxari, G., Dollár, P., Girshick, R.: Mask R-CNN. In:\nInternational Conference on Computer Vision (2017)\n\n",
            "[27] \nKirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao,\nT., Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. In:\nInternational Conference on Computer Vision (2023)\n\n",
            "[46] \nSaito, K., Hu, P., Darrell, T., Saenko, K.: Learning to detect every thing in\nan open world. In: European Conference on Computer Vision (2022)\n\n",
            "[49] \nWang, W., Feiszli, M., Wang, H., Malik, J., Tran, D.: Open-world instance\nsegmentation: Exploiting pseudo ground truth from learned pairwise affinity.\nIn: Conference on Computer Vision and Pattern Recognition (2022)\n\n",
            "[56] \nXue, X., Yu, D., Liu, L., Liu, Y., Li, Y., Yuan, Z., Song, P., Shou, M.Z.:\nSingle-stage open-world instance segmentation with cross-task consistency\nregularization. arXiv preprint arXiv:2208.09023 (2022)\n\n",
            "[48] \nWang, C., Wang, G., Zhang, Q., Guo, P., Liu, W., Wang, X.: OpenInst: A simple\nquery-based method for open-world instance segmentation. arXiv preprint\narXiv:2303.15859 (2023)\n\n",
            "[24] \nKalluri, T., Wang, W., Wang, H., Chandraker, M., Torresani, L., Tran, D.:\nOpen-world instance segmentation: Top-down learning with bottom-up\nsupervision. arXiv preprint arXiv:2303.05503 (2023)\n\n"
        ],
        "references": [
            "Table 3 presents the results of various OWIS methods on the cross-category setup COCO (VOC)→COCO→(VOC)COCO\\text{(VOC)}\\rightarrow\\text{COCO}(VOC) → COCO (non-VOC). Our SOS clearly outperforms all OWIS methods in AR100, AP, and F1. Specifically, SOS outperforms the second-best system in terms of F1, GGN [49], by 6.1. While some of the improvement comes from a better recall (+38.9% compared to GGN), the results are driven by a much-improved precision (+81.6% compared to GGN). This reflects the high quality of the pseudo annotations in SOS on this cross-category setting. Moreover, there is a clear improvement of SOS over Mask R-CNN (+12.7 in F1). Compared to original SAM, the DINO-based SOS substantially improves the precision by focusing on objects, leading to an improved F1 score (+7.8).",
            "We evaluate the cross-dataset generalization capabilities of OWIS methods, starting with COCO→LVIS→COCOLVIS\\text{COCO}\\rightarrow\\text{LVIS}COCO → LVIS. The results in Tab. 3 show a trend similar to the cross-category results. SOS outperforms all OWIS methods across all measures with an improvement of 2.8 in F1 compared to second-best methods GGN [49] and LDET [46]. Similar to the previous results, the improvement is based on gains in both recall and precision, however, the relative improvements in AP and AR100 are more similar to each other. Overall, our high-quality pseudo annotations in SOS lead to new state-of-the-art results, which indicate strong generalization to unknown object classes outside COCO."
        ]
    },
    "S5.T5.7.7": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S5.T5.7.7\">\n<tr class=\"ltx_tr\" id=\"S5.T5.6.6.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T5.6.6.2.3\">System</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T5.6.6.2.4\">AP</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T5.5.5.1.1\">AR<sub class=\"ltx_sub\" id=\"S5.T5.5.5.1.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T5.5.5.1.1.1.1\">100</span></sub>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S5.T5.6.6.2.2\">F<sub class=\"ltx_sub\" id=\"S5.T5.6.6.2.2.1\">1</sub>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.7.7.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T5.7.7.4.1\">Mask R-CNN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.14627v1#bib.bib19\" title=\"\">19</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.7.7.4.2\">6.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.7.7.4.3\">11.9</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T5.7.7.4.4\">8.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.7.7.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T5.7.7.5.1\">OLN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.14627v1#bib.bib26\" title=\"\">26</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.7.7.5.2\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.7.7.5.3\">20.4</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T5.7.7.5.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.7.7.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T5.7.7.6.1\">LDET&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.14627v1#bib.bib46\" title=\"\">46</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.7.7.6.2\">9.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.7.7.6.3\">18.5</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S5.T5.7.7.6.4\">12.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.7.7.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T5.7.7.7.1\">GGN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.14627v1#bib.bib49\" title=\"\">49</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.7.7.7.2\">9.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.7.7.7.3\">21.0</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S5.T5.7.7.7.4\">13.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.7.7.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T5.7.7.8.1\">UDOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.14627v1#bib.bib24\" title=\"\">24</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.7.7.8.2\">7.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.7.7.8.3\">22.9</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S5.T5.7.7.8.4\">11.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.7.7.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S5.T5.7.7.3.1\">SOS<sup class=\"ltx_sup\" id=\"S5.T5.7.7.3.1.1\">&#8224;</sup> (ours)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T5.7.7.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.7.7.3.2.1\">12.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T5.7.7.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.7.7.3.3.1\">26.5</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T5.7.7.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.7.7.3.4.1\">17.0</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 4 :  Results of Mask R-CNN baseline and various OWIS methods in the  COCO → ADE20k → COCO ADE20k \\text{COCO}\\rightarrow\\text{ADE20k} COCO → ADE20k  setting.  † : uses automatically annotated SA-1B dataset.",
        "footnotes": [
            "[19] \nHe, K., Gkioxari, G., Dollár, P., Girshick, R.: Mask R-CNN. In:\nInternational Conference on Computer Vision (2017)\n\n",
            "[26] \nKim, D., Lin, T.Y., Angelova, A., Kweon, I.S., Kuo, W.: Learning open-world\nobject proposals without learning to classify. IEEE Robotics and Automation\nLetters  7 (2) (2022)\n\n",
            "[46] \nSaito, K., Hu, P., Darrell, T., Saenko, K.: Learning to detect every thing in\nan open world. In: European Conference on Computer Vision (2022)\n\n",
            "[49] \nWang, W., Feiszli, M., Wang, H., Malik, J., Tran, D.: Open-world instance\nsegmentation: Exploiting pseudo ground truth from learned pairwise affinity.\nIn: Conference on Computer Vision and Pattern Recognition (2022)\n\n",
            "[24] \nKalluri, T., Wang, W., Wang, H., Chandraker, M., Torresani, L., Tran, D.:\nOpen-world instance segmentation: Top-down learning with bottom-up\nsupervision. arXiv preprint arXiv:2303.05503 (2023)\n\n",
            "[19] \nHe, K., Gkioxari, G., Dollár, P., Girshick, R.: Mask R-CNN. In:\nInternational Conference on Computer Vision (2017)\n\n",
            "[27] \nKirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao,\nT., Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. In:\nInternational Conference on Computer Vision (2023)\n\n",
            "[26] \nKim, D., Lin, T.Y., Angelova, A., Kweon, I.S., Kuo, W.: Learning open-world\nobject proposals without learning to classify. IEEE Robotics and Automation\nLetters  7 (2) (2022)\n\n",
            "[46] \nSaito, K., Hu, P., Darrell, T., Saenko, K.: Learning to detect every thing in\nan open world. In: European Conference on Computer Vision (2022)\n\n",
            "[49] \nWang, W., Feiszli, M., Wang, H., Malik, J., Tran, D.: Open-world instance\nsegmentation: Exploiting pseudo ground truth from learned pairwise affinity.\nIn: Conference on Computer Vision and Pattern Recognition (2022)\n\n",
            "[24] \nKalluri, T., Wang, W., Wang, H., Chandraker, M., Torresani, L., Tran, D.:\nOpen-world instance segmentation: Top-down learning with bottom-up\nsupervision. arXiv preprint arXiv:2303.05503 (2023)\n\n"
        ],
        "references": [
            "The results for COCO→ADE20k→COCOADE20k\\text{COCO}\\rightarrow\\text{ADE20k}COCO → ADE20k in Tab. 5, which include labeled object parts of ADE20k, show that SOS again outperforms all other OWIS methods across all measures. The gain over the second-best method in F1, GGN [49], is 3.7. Similar to COCO→LVIS→COCOLVIS\\text{COCO}\\rightarrow\\text{LVIS}COCO → LVIS, the relative gains in AR100 and AP are equally distributed. These results indicate that object parts do not degrade the results of SOS. This is in line with SOS’s pseudo annotations in Fig. 3, covering entire objects and selected object parts.",
            "Finally, Tab. 5 presents results for COCO→UVO→COCOUVO\\text{COCO}\\rightarrow\\text{UVO}COCO → UVO. SOS still outperforms Mask R-CNN, SAM and recent UDOS [24] in F1. Outperforming SAM implies that SOS does not exploit the limited taxonomy of annotations in COCO (only 60 object classes in test), but also improves the segmentation of objects outside the COCO object classes on the exhaustively labeled UVO dataset. However, the results of SOS are below LDET [46] in AP and F1. We attribute this to several classes in UVO not available in ImageNet, which was used to learn DINO object prior. Hence, DINO will miss such objects in the COCO dataset during training, and SOS might miss them during testing. This could be mitigated using more diverse unlabeled images for training DINO."
        ]
    },
    "S5.T5.15.8": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S5.T5.15.8\">\n<tr class=\"ltx_tr\" id=\"S5.T5.13.6.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T5.13.6.2.3\">System</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T5.13.6.2.4\">AP</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T5.12.5.1.1\">AR<sub class=\"ltx_sub\" id=\"S5.T5.12.5.1.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T5.12.5.1.1.1.1\">100</span></sub>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S5.T5.13.6.2.2\">F<sub class=\"ltx_sub\" id=\"S5.T5.13.6.2.2.1\">1</sub>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.15.8.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T5.15.8.5.1\">Mask R-CNN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.14627v1#bib.bib19\" title=\"\">19</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.15.8.5.2\">20.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.15.8.5.3\">36.7</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T5.15.8.5.4\">26.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.14.7.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T5.14.7.3.1\">SAM<sup class=\"ltx_sup\" id=\"S5.T5.14.7.3.1.1\">&#8224;</sup>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.14627v1#bib.bib27\" title=\"\">27</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.14.7.3.2\">11.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.14.7.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.14.7.3.3.1\">50.1</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S5.T5.14.7.3.4\">18.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.15.8.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T5.15.8.6.1\">OLN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.14627v1#bib.bib26\" title=\"\">26</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.15.8.6.2\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.15.8.6.3\">41.4</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T5.15.8.6.4\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.15.8.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T5.15.8.7.1\">LDET&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.14627v1#bib.bib46\" title=\"\">46</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.15.8.7.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.15.8.7.2.1\">22.0</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.15.8.7.3\">40.4</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S5.T5.15.8.7.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T5.15.8.7.4.1\">28.5</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.15.8.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T5.15.8.8.1\">GGN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.14627v1#bib.bib49\" title=\"\">49</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.15.8.8.2\">20.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.15.8.8.3\">43.4</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S5.T5.15.8.8.4\">27.7</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.15.8.9\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T5.15.8.9.1\">UDOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.14627v1#bib.bib24\" title=\"\">24</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.15.8.9.2\">10.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.15.8.9.3\">43.1</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S5.T5.15.8.9.4\">17.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.15.8.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S5.T5.15.8.4.1\">SOS<sup class=\"ltx_sup\" id=\"S5.T5.15.8.4.1.1\">&#8224;</sup> (ours)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T5.15.8.4.2\">20.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T5.15.8.4.3\">42.3</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T5.15.8.4.4\">28.0</td>\n</tr>\n</table>\n\n",
        "caption": "Table 5 :  Results of two baselines and various OWIS methods in the  COCO → UVO → COCO UVO \\text{COCO}\\rightarrow\\text{UVO} COCO → UVO  setting.  † : uses automatically annotated SA-1B dataset.",
        "footnotes": [
            "[19] \nHe, K., Gkioxari, G., Dollár, P., Girshick, R.: Mask R-CNN. In:\nInternational Conference on Computer Vision (2017)\n\n",
            "[26] \nKim, D., Lin, T.Y., Angelova, A., Kweon, I.S., Kuo, W.: Learning open-world\nobject proposals without learning to classify. IEEE Robotics and Automation\nLetters  7 (2) (2022)\n\n",
            "[46] \nSaito, K., Hu, P., Darrell, T., Saenko, K.: Learning to detect every thing in\nan open world. In: European Conference on Computer Vision (2022)\n\n",
            "[49] \nWang, W., Feiszli, M., Wang, H., Malik, J., Tran, D.: Open-world instance\nsegmentation: Exploiting pseudo ground truth from learned pairwise affinity.\nIn: Conference on Computer Vision and Pattern Recognition (2022)\n\n",
            "[24] \nKalluri, T., Wang, W., Wang, H., Chandraker, M., Torresani, L., Tran, D.:\nOpen-world instance segmentation: Top-down learning with bottom-up\nsupervision. arXiv preprint arXiv:2303.05503 (2023)\n\n",
            "[19] \nHe, K., Gkioxari, G., Dollár, P., Girshick, R.: Mask R-CNN. In:\nInternational Conference on Computer Vision (2017)\n\n",
            "[27] \nKirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao,\nT., Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. In:\nInternational Conference on Computer Vision (2023)\n\n",
            "[26] \nKim, D., Lin, T.Y., Angelova, A., Kweon, I.S., Kuo, W.: Learning open-world\nobject proposals without learning to classify. IEEE Robotics and Automation\nLetters  7 (2) (2022)\n\n",
            "[46] \nSaito, K., Hu, P., Darrell, T., Saenko, K.: Learning to detect every thing in\nan open world. In: European Conference on Computer Vision (2022)\n\n",
            "[49] \nWang, W., Feiszli, M., Wang, H., Malik, J., Tran, D.: Open-world instance\nsegmentation: Exploiting pseudo ground truth from learned pairwise affinity.\nIn: Conference on Computer Vision and Pattern Recognition (2022)\n\n",
            "[24] \nKalluri, T., Wang, W., Wang, H., Chandraker, M., Torresani, L., Tran, D.:\nOpen-world instance segmentation: Top-down learning with bottom-up\nsupervision. arXiv preprint arXiv:2303.05503 (2023)\n\n"
        ],
        "references": [
            "The results for COCO→ADE20k→COCOADE20k\\text{COCO}\\rightarrow\\text{ADE20k}COCO → ADE20k in Tab. 5, which include labeled object parts of ADE20k, show that SOS again outperforms all other OWIS methods across all measures. The gain over the second-best method in F1, GGN [49], is 3.7. Similar to COCO→LVIS→COCOLVIS\\text{COCO}\\rightarrow\\text{LVIS}COCO → LVIS, the relative gains in AR100 and AP are equally distributed. These results indicate that object parts do not degrade the results of SOS. This is in line with SOS’s pseudo annotations in Fig. 3, covering entire objects and selected object parts.",
            "Finally, Tab. 5 presents results for COCO→UVO→COCOUVO\\text{COCO}\\rightarrow\\text{UVO}COCO → UVO. SOS still outperforms Mask R-CNN, SAM and recent UDOS [24] in F1. Outperforming SAM implies that SOS does not exploit the limited taxonomy of annotations in COCO (only 60 object classes in test), but also improves the segmentation of objects outside the COCO object classes on the exhaustively labeled UVO dataset. However, the results of SOS are below LDET [46] in AP and F1. We attribute this to several classes in UVO not available in ImageNet, which was used to learn DINO object prior. Hence, DINO will miss such objects in the COCO dataset during training, and SOS might miss them during testing. This could be mitigated using more diverse unlabeled images for training DINO."
        ]
    },
    "S5.T6.6.6": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S5.T6.6.6\">\n<tr class=\"ltx_tr\" id=\"S5.T6.3.3.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T6.3.3.1.2\">System</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.3.3.1.3\">Prec.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.3.3.1.4\">Rec.</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S5.T6.3.3.1.1\">F<sub class=\"ltx_sub\" id=\"S5.T6.3.3.1.1.1\">1</sub>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.4.4.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T6.4.4.2.1\">GGN<sub class=\"ltx_sub\" id=\"S5.T6.4.4.2.1.1\">3</sub>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.14627v1#bib.bib49\" title=\"\">49</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.4.4.2.2\">7.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.4.4.2.3\">12.1</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T6.4.4.2.4\">9.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.5.5.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T6.5.5.3.1\">SOS<math alttext=\"{}_{3}^{\\dagger}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T6.5.5.3.1.m1.1\"><semantics id=\"S5.T6.5.5.3.1.m1.1a\"><mmultiscripts id=\"S5.T6.5.5.3.1.m1.1.1\" xref=\"S5.T6.5.5.3.1.m1.1.1.cmml\"><mi id=\"S5.T6.5.5.3.1.m1.1.1.2.2\" xref=\"S5.T6.5.5.3.1.m1.1.1.2.2.cmml\"/><mprescripts id=\"S5.T6.5.5.3.1.m1.1.1a\" xref=\"S5.T6.5.5.3.1.m1.1.1.cmml\"/><mrow id=\"S5.T6.5.5.3.1.m1.1.1b\" xref=\"S5.T6.5.5.3.1.m1.1.1.cmml\"/><mo id=\"S5.T6.5.5.3.1.m1.1.1.3\" xref=\"S5.T6.5.5.3.1.m1.1.1.3.cmml\">&#8224;</mo><mn id=\"S5.T6.5.5.3.1.m1.1.1.2.3\" xref=\"S5.T6.5.5.3.1.m1.1.1.2.3.cmml\">3</mn><mrow id=\"S5.T6.5.5.3.1.m1.1.1c\" xref=\"S5.T6.5.5.3.1.m1.1.1.cmml\"/></mmultiscripts><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.5.5.3.1.m1.1b\"><apply id=\"S5.T6.5.5.3.1.m1.1.1.cmml\" xref=\"S5.T6.5.5.3.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T6.5.5.3.1.m1.1.1.1.cmml\" xref=\"S5.T6.5.5.3.1.m1.1.1\">superscript</csymbol><apply id=\"S5.T6.5.5.3.1.m1.1.1.2.cmml\" xref=\"S5.T6.5.5.3.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T6.5.5.3.1.m1.1.1.2.1.cmml\" xref=\"S5.T6.5.5.3.1.m1.1.1\">subscript</csymbol><csymbol cd=\"latexml\" id=\"S5.T6.5.5.3.1.m1.1.1.2.2.cmml\" xref=\"S5.T6.5.5.3.1.m1.1.1.2.2\">absent</csymbol><cn id=\"S5.T6.5.5.3.1.m1.1.1.2.3.cmml\" type=\"integer\" xref=\"S5.T6.5.5.3.1.m1.1.1.2.3\">3</cn></apply><ci id=\"S5.T6.5.5.3.1.m1.1.1.3.cmml\" xref=\"S5.T6.5.5.3.1.m1.1.1.3\">&#8224;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.5.5.3.1.m1.1c\">{}_{3}^{\\dagger}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T6.5.5.3.1.m1.1d\">start_FLOATSUBSCRIPT 3 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT &#8224; end_POSTSUPERSCRIPT</annotation></semantics></math> (ours)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.5.5.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.5.5.3.2.1\">19.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.5.5.3.3\">26.4</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T6.5.5.3.4\">22.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.6.6.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T6.6.6.4.1\">SOS<math alttext=\"{}_{10}^{\\dagger}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T6.6.6.4.1.m1.1\"><semantics id=\"S5.T6.6.6.4.1.m1.1a\"><mmultiscripts id=\"S5.T6.6.6.4.1.m1.1.1\" xref=\"S5.T6.6.6.4.1.m1.1.1.cmml\"><mi id=\"S5.T6.6.6.4.1.m1.1.1.2.2\" xref=\"S5.T6.6.6.4.1.m1.1.1.2.2.cmml\"/><mprescripts id=\"S5.T6.6.6.4.1.m1.1.1a\" xref=\"S5.T6.6.6.4.1.m1.1.1.cmml\"/><mrow id=\"S5.T6.6.6.4.1.m1.1.1b\" xref=\"S5.T6.6.6.4.1.m1.1.1.cmml\"/><mo id=\"S5.T6.6.6.4.1.m1.1.1.3\" xref=\"S5.T6.6.6.4.1.m1.1.1.3.cmml\">&#8224;</mo><mn id=\"S5.T6.6.6.4.1.m1.1.1.2.3\" xref=\"S5.T6.6.6.4.1.m1.1.1.2.3.cmml\">10</mn><mrow id=\"S5.T6.6.6.4.1.m1.1.1c\" xref=\"S5.T6.6.6.4.1.m1.1.1.cmml\"/></mmultiscripts><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.6.6.4.1.m1.1b\"><apply id=\"S5.T6.6.6.4.1.m1.1.1.cmml\" xref=\"S5.T6.6.6.4.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T6.6.6.4.1.m1.1.1.1.cmml\" xref=\"S5.T6.6.6.4.1.m1.1.1\">superscript</csymbol><apply id=\"S5.T6.6.6.4.1.m1.1.1.2.cmml\" xref=\"S5.T6.6.6.4.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T6.6.6.4.1.m1.1.1.2.1.cmml\" xref=\"S5.T6.6.6.4.1.m1.1.1\">subscript</csymbol><csymbol cd=\"latexml\" id=\"S5.T6.6.6.4.1.m1.1.1.2.2.cmml\" xref=\"S5.T6.6.6.4.1.m1.1.1.2.2\">absent</csymbol><cn id=\"S5.T6.6.6.4.1.m1.1.1.2.3.cmml\" type=\"integer\" xref=\"S5.T6.6.6.4.1.m1.1.1.2.3\">10</cn></apply><ci id=\"S5.T6.6.6.4.1.m1.1.1.3.cmml\" xref=\"S5.T6.6.6.4.1.m1.1.1.3\">&#8224;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.6.6.4.1.m1.1c\">{}_{10}^{\\dagger}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T6.6.6.4.1.m1.1d\">start_FLOATSUBSCRIPT 10 end_FLOATSUBSCRIPT start_POSTSUPERSCRIPT &#8224; end_POSTSUPERSCRIPT</annotation></semantics></math> (ours)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.6.6.4.2\">15.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.6.6.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.6.6.4.3.1\">41.7</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"S5.T6.6.6.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T6.6.6.4.4.1\">22.6</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 6 :  Quality of pseudo annotation generated in GGN  [ 49 ]  and our SOS evaluated on the non-VOC annotations of the COCO training dataset.  † : uses automatically annotated SA-1B dataset.",
        "footnotes": [
            "[49] \nWang, W., Feiszli, M., Wang, H., Malik, J., Tran, D.: Open-world instance\nsegmentation: Exploiting pseudo ground truth from learned pairwise affinity.\nIn: Conference on Computer Vision and Pattern Recognition (2022)\n\n",
            "[49] \nWang, W., Feiszli, M., Wang, H., Malik, J., Tran, D.: Open-world instance\nsegmentation: Exploiting pseudo ground truth from learned pairwise affinity.\nIn: Conference on Computer Vision and Pattern Recognition (2022)\n\n"
        ],
        "references": [
            "We also investigate the quality of the pseudo annotations generated in SOS. To this end, we evaluate the pseudo annotations on the COCO training set against the annotations of non-VOC object classes, reflecting the COCO (VOC)→COCO→(VOC)COCO\\text{(VOC)}\\rightarrow\\text{COCO}(VOC) → COCO non-VOC) setting. The results in terms of precision and recall for IoU=0.5IoU0.5\\text{IoU}=0.5IoU = 0.5 as well as the F1 score are presented in Tab. 6. We use up to three and 10 pseudo annotations generated per image in SOS (SOS3 and SOS10), and GGN3 [49] that generates three pseudo annotations. The results show that with only three SOS pseudo annotations, more than 25% of the non-VOC objects are covered. This is substantially more than GGN3 (12.1%). With up to 10 annotations, SOS’s pseudo annotations cover more than 40% of the non-VOC objects. In our supplementary, we further provide class-specific results."
        ]
    },
    "S5.T6.10.4": {
        "table": "<table class=\"ltx_tabular ltx_figure_panel ltx_align_middle\" id=\"S5.T6.10.4\">\n<tr class=\"ltx_tr\" id=\"S5.T6.8.2.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.7.1.1.1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"159\" id=\"S5.T6.7.1.1.1.g1\" src=\"extracted/5871922/figs/540_GGN.png\" width=\"240\"/></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.8.2.2.2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"159\" id=\"S5.T6.8.2.2.2.g1\" src=\"extracted/5871922/figs/540_DINO.png\" width=\"240\"/></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.10.4.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.9.3.3.1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"160\" id=\"S5.T6.9.3.3.1.g1\" src=\"extracted/5871922/figs/650_GGN.png\" width=\"240\"/></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.10.4.4.2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"160\" id=\"S5.T6.10.4.4.2.g1\" src=\"extracted/5871922/figs/650_DINO.png\" width=\"240\"/></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.10.4.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.10.4.5.1\"><span class=\"ltx_text\" id=\"S5.T6.10.4.5.1.1\" style=\"font-size:70%;\">GGN</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.10.4.5.2\"><span class=\"ltx_text\" id=\"S5.T6.10.4.5.2.1\" style=\"font-size:70%;\">SOS</span></td>\n</tr>\n</table>\n\n",
        "caption": "Figure 5 :  Pseudo annotations generated by GGN and our SOS.",
        "footnotes": [
            "[49] \nWang, W., Feiszli, M., Wang, H., Malik, J., Tran, D.: Open-world instance\nsegmentation: Exploiting pseudo ground truth from learned pairwise affinity.\nIn: Conference on Computer Vision and Pattern Recognition (2022)\n\n",
            "[49] \nWang, W., Feiszli, M., Wang, H., Malik, J., Tran, D.: Open-world instance\nsegmentation: Exploiting pseudo ground truth from learned pairwise affinity.\nIn: Conference on Computer Vision and Pattern Recognition (2022)\n\n"
        ],
        "references": [
            "We also investigate the quality of the pseudo annotations generated in SOS. To this end, we evaluate the pseudo annotations on the COCO training set against the annotations of non-VOC object classes, reflecting the COCO (VOC)→COCO→(VOC)COCO\\text{(VOC)}\\rightarrow\\text{COCO}(VOC) → COCO non-VOC) setting. The results in terms of precision and recall for IoU=0.5IoU0.5\\text{IoU}=0.5IoU = 0.5 as well as the F1 score are presented in Tab. 6. We use up to three and 10 pseudo annotations generated per image in SOS (SOS3 and SOS10), and GGN3 [49] that generates three pseudo annotations. The results show that with only three SOS pseudo annotations, more than 25% of the non-VOC objects are covered. This is substantially more than GGN3 (12.1%). With up to 10 annotations, SOS’s pseudo annotations cover more than 40% of the non-VOC objects. In our supplementary, we further provide class-specific results."
        ]
    },
    "S5.T8.4.4": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S5.T8.4.4\">\n<tr class=\"ltx_tr\" id=\"S5.T8.4.4.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T8.4.4.2.3\">Components</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T8.4.4.2.4\">AP</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T8.3.3.1.1\">AR<sub class=\"ltx_sub\" id=\"S5.T8.3.3.1.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T8.3.3.1.1.1.1\">100</span></sub>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S5.T8.4.4.2.2\">F<sub class=\"ltx_sub\" id=\"S5.T8.4.4.2.2.1\">1</sub>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.4.4.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T8.4.4.3.1\">Mask R-CNN</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.4.4.3.2\">1.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.4.4.3.3\">10.8</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T8.4.4.3.4\">2.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.4.4.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.4.4.4.1\">+ <span class=\"ltx_text ltx_font_italic\" id=\"S5.T8.4.4.4.1.1\">Grid</span>, w/o pp.</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.4.4.2\">3.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.4.4.3\">35.2</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S5.T8.4.4.4.4\">6.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.4.4.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.4.4.5.1\">+ <span class=\"ltx_text ltx_font_italic\" id=\"S5.T8.4.4.5.1.1\">DINO</span>, w/o pp.</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.4.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.4.4.5.2.1\">8.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.4.5.3\">37.1</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S5.T8.4.4.5.4\">14.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.4.4.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T8.4.4.6.1\">+ <span class=\"ltx_text ltx_font_italic\" id=\"S5.T8.4.4.6.1.1\">DINO</span>, w/ pp.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T8.4.4.6.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.4.4.6.2.1\">8.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T8.4.4.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.4.4.6.3.1\">38.1</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"S5.T8.4.4.6.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.4.4.6.4.1\">14.4</span></td>\n</tr>\n</table>\n\n",
        "caption": "Table 7 :  Influence of added components in our SOS evaluated in the COCO  (VOC) → COCO → (VOC) COCO \\text{(VOC)}\\rightarrow\\text{COCO} (VOC) → COCO  (non-VOC) setting.",
        "footnotes": [],
        "references": [
            "We investigate the importance of each component in SOS and present the results in Tab. 8. The baseline for this study is a class-agnostic Mask R-CNN trained without pseudo annotations (first row in Tab. 8). Subsequently, we add pseudo annotations based on SAM with the baseline Grid object prior as in [27] and without our post-processing in PAC (confidence-based thresholding and NMS, see Sec. 3.3). Next, we replace Grid with DINO (third row in Tab. 8), and finally add the post-processing leading to the complete SOS. The results in Tab. 8 show each step improves the F1 results. Mainly, introducing SAM for generating pseudo annotations (first row vs. second row) substantially improves AR100, while adding the DINO object prior removes background annotations leading to considerably improved precision. Hence, the object prior selection in SOS is crucial for strong results.",
            "In the second ablation study, we investigate the influence of the number of pseudo annotations per image on the results of SOS. To this end, we evaluate SOS with up to 3, 5, 10, and 20 pseudo annotations per image. The results in Tab. 8 show that 10 pseudo annotations are preferable. Hence, the ideal number of pseudo annotations in SOS is larger than in GGN [49], which only uses three. This indicates again the high quality of our pseudo annotations, since more than three annotations per image can cover relevant objects without adding too many noisy annotations."
        ]
    },
    "S5.T8.8.4": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S5.T8.8.4\">\n<tr class=\"ltx_tr\" id=\"S5.T8.8.4.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T8.8.4.2.3\">#Pseudo Anns.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T8.8.4.2.4\">AP</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T8.7.3.1.1\">AR<sub class=\"ltx_sub\" id=\"S5.T8.7.3.1.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T8.7.3.1.1.1.1\">100</span></sub>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S5.T8.8.4.2.2\">F<sub class=\"ltx_sub\" id=\"S5.T8.8.4.2.2.1\">1</sub>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.8.4.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T8.8.4.3.1\">3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.8.4.3.2\">8.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.8.4.3.3\">34.5</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S5.T8.8.4.3.4\">13.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.8.4.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.8.4.4.1\">5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.8.4.4.2\">8.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.8.4.4.3\">36.2</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S5.T8.8.4.4.4\">14.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.8.4.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.8.4.5.1\">10</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.8.4.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.8.4.5.2.1\">8.9</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.8.4.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.8.4.5.3.1\">38.1</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S5.T8.8.4.5.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.8.4.5.4.1\">14.4</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.8.4.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T8.8.4.6.1\">20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T8.8.4.6.2\">8.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T8.8.4.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T8.8.4.6.3.1\">38.1</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"S5.T8.8.4.6.4\">14.3</td>\n</tr>\n</table>\n\n",
        "caption": "Table 8 :  SOS results with various numbers of pseudo annotations in the COCO  (VOC) → COCO → (VOC) COCO \\text{(VOC)}\\rightarrow\\text{COCO} (VOC) → COCO  (non-VOC) setting.",
        "footnotes": [],
        "references": [
            "We investigate the importance of each component in SOS and present the results in Tab. 8. The baseline for this study is a class-agnostic Mask R-CNN trained without pseudo annotations (first row in Tab. 8). Subsequently, we add pseudo annotations based on SAM with the baseline Grid object prior as in [27] and without our post-processing in PAC (confidence-based thresholding and NMS, see Sec. 3.3). Next, we replace Grid with DINO (third row in Tab. 8), and finally add the post-processing leading to the complete SOS. The results in Tab. 8 show each step improves the F1 results. Mainly, introducing SAM for generating pseudo annotations (first row vs. second row) substantially improves AR100, while adding the DINO object prior removes background annotations leading to considerably improved precision. Hence, the object prior selection in SOS is crucial for strong results.",
            "In the second ablation study, we investigate the influence of the number of pseudo annotations per image on the results of SOS. To this end, we evaluate SOS with up to 3, 5, 10, and 20 pseudo annotations per image. The results in Tab. 8 show that 10 pseudo annotations are preferable. Hence, the ideal number of pseudo annotations in SOS is larger than in GGN [49], which only uses three. This indicates again the high quality of our pseudo annotations, since more than three annotations per image can cover relevant objects without adding too many noisy annotations."
        ]
    }
}