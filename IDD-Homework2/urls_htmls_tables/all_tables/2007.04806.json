{
    "PAPER'S NUMBER OF TABLES": 1,
    "S4.T1": {
        "caption": "Table 1: Client average Fréchet distances, ΓΓ\\Gamma, in relation to proportion of shuffled labels, for the audio data in Experiment 1. Increasing the proportion of shuffled labels decreases the distances.",
        "table": "<table id=\"S4.T1.5\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.5.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.5.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\">Proportion of shuffled labels</td>\n<td id=\"S4.T1.5.1.1.2\" class=\"ltx_td ltx_align_right ltx_border_tt\">0.0</td>\n<td id=\"S4.T1.5.1.1.3\" class=\"ltx_td ltx_align_right ltx_border_tt\">0.2</td>\n<td id=\"S4.T1.5.1.1.4\" class=\"ltx_td ltx_align_right ltx_border_tt\">0.4</td>\n<td id=\"S4.T1.5.1.1.5\" class=\"ltx_td ltx_align_right ltx_border_tt\">0.6</td>\n<td id=\"S4.T1.5.1.1.6\" class=\"ltx_td ltx_align_right ltx_border_tt\">0.8</td>\n<td id=\"S4.T1.5.1.1.7\" class=\"ltx_td ltx_align_right ltx_border_tt\">1.0</td>\n</tr>\n<tr id=\"S4.T1.5.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.5.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">Average client Fréchet distance</td>\n<td id=\"S4.T1.5.2.2.2\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">1784.4</td>\n<td id=\"S4.T1.5.2.2.3\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">980.0</td>\n<td id=\"S4.T1.5.2.2.4\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">550.5</td>\n<td id=\"S4.T1.5.2.2.5\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">271.0</td>\n<td id=\"S4.T1.5.2.2.6\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">91.8</td>\n<td id=\"S4.T1.5.2.2.7\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">19.9</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In our investigation of audio data, we construct a binary problem from the FSD by subdividing the labels into a positive class of audio labelled as cough, and a much larger negative class of any other label in FSD.\nFSD as a whole has a total of 273 examples of the cough label.",
                "We use a pre-trained MobileNet ",
                "(Howard et al., ",
                "2017",
                ")",
                " called YamNet",
                "1",
                "1",
                "1",
                "Maintained by M. Plakal and D. Ellis and available in the ",
                "Tensorflow AudioSet research repository",
                ".",
                " trained on the AudioSet data ",
                "(Gemmeke et al., ",
                "2017",
                ")",
                " to obtain embeddings. Audio inputs are resampled to 16 kHz mono signals, and converted to stabilized log-mel spectrograms. YamNet outputs a 1024-dimensional vector for each patch in the log-mel spectrogram, where a patch corresponds to 960 ms of waveform input (only full 960 ms patches are considered and any remainder of the signal is dropped). We obtain an embedding robust to varying time-length of the FSD audio samples by max-pooling the patch-features across the patches (the temporal dimension).",
                "We train a 2-layer classifier with 64 units with 50 % dropout between layers.\nThe classifier is trained using ",
                "FedAvg",
                " ",
                "(McMahan et al., ",
                "2017",
                ")",
                " with 10 simulated clients, and at each round of federated learning all 10 clients were included.\nEach client completed 10 steps of gradient descent with batch sizes of 32 samples, or until all client data had been seen once—each client had a variable size of data set, seeing as samples are assigned based on proximity to the cluster centroids.\nThe clients utilized a stochastic gradient descent optimizer",
                "2",
                "2",
                "2",
                "A momentum and a decay was specified, but at a later stage it was discovered that the states were erroneously resat at each round (i.e. at each 10 steps), effectively thus having no decay, and momenta building over only 10 steps.",
                ".\nWe retain the original FSD training and test partitions.\nWhile training, we monitor the loss of a held-out sub-partition of 5 % the training set (a validation partition) by centrally collecting the outcomes on the validation data points at each client.\nThe best model (model weights) with the lowest cross-entropy loss on the validation partition across clients after a total of a 1000 rounds of federated learning is then used in the final evaluation on the test set.",
                "We determine the average Fréchet distances from any given client’s features to all others (as described in ",
                "Section",
                " ",
                "3.4",
                "). The results averaged across replicates of the shuffling proportion are shown in ",
                "Table",
                " ",
                "1",
                ".",
                "We evaluate the performance for shuffling proportions ranging from 0.0 to 1.0 in steps of 0.2 with 12 repetitions of the experiment at each level for both with and without client adaption. Since the data is imbalanced, we measure the model performance in terms of the area under the receiver operating characteristic (AUC).\nThe results are shown in ",
                "Fig.",
                " ",
                "2(a)",
                ".\nWe see that a model without client adaption (in blue) performs consistently at about 0.994 across the range of shuffling.\nThe model with client adaption (orange) outperforms this baseline model for the most non-IID clients (lowest levels of shuffling), and has an AUC of about 0.998, thus improving performance for the most heterogeneous simulated clients.\nFor more homogeneous clients, we see no discernible difference between the performance of the two models."
            ]
        ]
    }
}