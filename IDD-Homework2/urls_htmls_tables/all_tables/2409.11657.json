{
    "id_table_1": {
        "caption": "TABLE I:  Classification accuracy (%) of different methods under the F2SCIL paradigm on the CIFAR100 dataset.",
        "table": "S3.E1",
        "footnotes": [],
        "references": [
            "This work explores a decentralized machine learning paradigm referred to as Federated Few-Shot Class-Incremental Learning (F2SCIL). As depicted in Fig.  1 , clients (data owners) continually update their local models with scarce data to adapt to novel classes. Subsequently, the updated parameters from local models are transmitted to a server for aggregation, producing a unified model that leverages distributed data across multiple clients while preserving data locality. However, the F2SCIL paradigm faces several challenges. Due to the scarcity of data accessible to each client (i.e., few-shot learning), the model encounters difficulties in learning new classes. Secondly, catastrophic forgetting is a significant issue as the model continually updates to incorporate new classes, potentially degrading its performance on previously learned classes. Thirdly, variations in data categories and sample sizes across clients, often described as non-Independent and Identically Distributed (Non-IID), result in data scarcity and heterogeneity. These factors exacerbate catastrophic forgetting and pose substantial difficulties in designing an effective aggregation strategy for refining the global model.",
            "This paper introduces a new learning paradigm F2SCIL, necessitating the proposal of a baseline approach for reference. The baseline approach employs a classic knowledge distillation algorithm from class-incremental learning to update the local models on the client side and adopts FedAvg  [ 26 ]  as the model aggregation algorithm. Details of this approach are described in Algorithm  1 . Taking the  t t t italic_t -th incremental session as an example, we first disseminate the global model from the preceding session to clients as their initial models. Subsequently, a client adapts its local model to new classes with the limited acquired data and employs knowledge distillation with synthetic samples of old classes drawn from the replay buffer to retain previously learned knowledge. At last, these locally evolved models are aggregated into a new global model on the server side, using the FedAvg strategy. This model is conveyed back to clients as the initial model in the next session."
        ]
    },
    "id_table_2": {
        "caption": "TABLE II:  Classification accuracy (%) of different methods under the F2SCIL paradigm on the miniImageNet dataset.",
        "table": "S3.E8",
        "footnotes": [],
        "references": [
            "To address the challenges in F2SCIL as previously discussed, we propose a data-free framework called SDD. The overview of SDD is depicted in Fig.  2 . For the base session, we utilize the dataset  D ( 0 ) superscript D 0 D^{(0)} italic_D start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT  to learn an initial model   ( 0 ) superscript  0 \\theta^{(0)} italic_ start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT  in a centralized manner. Following the traditional supervised learning framework,   ( 0 ) superscript  0 \\theta^{(0)} italic_ start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT  is optimized by the cross-entropy loss function to ensure strong classification capabilities for base classes. Next, the initial model is employed to provide supervisory signals for training a conditional generator, which can effectively capture the key feature distributions of the base classes and generate highly representative pseudo samples. These generated samples are then added to a pre-established replay buffer, providing knowledge of previously learned classes for subsequent incremental sessions.",
            "where   ( t ) superscript  t {\\theta}^{(t)} italic_ start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT  denotes the global model and   m ( t ) subscript superscript  t m {\\theta}^{(t)}_{m} italic_ start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT  indicates local model at the  m m m italic_m -th client in session  t t t italic_t .  M M M italic_M  is the total number of clients. The function  f t  (  ;  ) superscript f t   f^{t}(\\cdot;\\cdot) italic_f start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT (  ;  )  extracts outputs of the last fully connected layer, corresponding to the new classes in session  t t t italic_t . Particularly, the initial model   ( 0 ) superscript  0 {\\theta}^{(0)} italic_ start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT  learned in a centralized manner is employed as the teacher in the base session. In the subsequent incremental sessions, we use the ensemble of local models rather than the global model as the teacher, because the generator is trained before aggregating the local models into the global model, as shown in Fig.  2 .",
            "Building on the baseline approach, the proposed SDD framework addresses several key issues in F2SCIL. In the update of the client-side local models, the NAGR module is introduced to manage the catastrophic forgetting problem. In the model aggregation phase, the CSWA strategy is developed to consider the significant differences in local model parameters caused by non-IID data, thereby ensuring efficient and effective model aggregation. The overall learning procedure of SDD for an incremental session is outlined in Algorithm  2 ."
        ]
    },
    "id_table_3": {
        "caption": "TABLE III:  Classification accuracy (%) of different methods under the F2SCIL paradigm on the tinyImageNet dataset.",
        "table": "S3.E10",
        "footnotes": [],
        "references": [
            "Given a well-trained model, our goal is to build a conditional generator   G subscript  G \\theta_{G} italic_ start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT  for synthesizing pseudo-samples that mimic the original data distribution. The synthetic data are expected to satisfy the following key characteristics: fidelity, diversity, stability, and transferability. To achieve this, we adopt a teacher-student architecture where the condition generator and the student model are jointly optimized through an adversarial learning scheme, as depicted in Fig.  3 . In the base session, the well-trained initial model functions as the teacher, while the ensemble of local models serves as the teacher in the subsequent incremental sessions. Note that the teacher model only provides supervisory signals and does not update its parameters. Following  [ 30 ] , we initialize a student model   S subscript  S \\theta_{S} italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  which transfers knowledge by emulating predictions of the teacher on synthetic samples, thereby learning similar decision boundaries. The generator seeks to make knowledge transfer more challenging for the student model by generating synthetic samples near the decision boundaries. Through this adversarial learning mechanism, the generator is able to produce diverse and challenging samples. The training process consists of two iterative stages:"
        ]
    },
    "id_table_4": {
        "caption": "TABLE IV:  Performance (%) of applying different loss functions to the synthetic replay data in FSCIL.",
        "table": "S3.E16",
        "footnotes": [],
        "references": [
            "Transferability.  The core motivation for employing a teacher-student architecture is to drive the generator to produce hard samples near the decision boundaries. The student mimics the teachers outputs on synthetic samples to gain valuable knowledge, while the generator aims to produce samples that lie on different sides of the decision boundaries of the student and teacher, as illustrated in Fig.  4 . To achieve this, we encourage synthetic samples to induce prediction discrepancies between the teacher and student models by maximizing the Kullback-Leibler divergence between their output logits."
        ]
    },
    "id_table_5": {
        "caption": "TABLE V:  Performance (%) of different model aggregation strategies.",
        "table": "S4.T1.1",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Fig.  5  illustrates the effect of varying the number of synthetic replay samples. In our setting, 125 training samples are randomly distributed among 5 clients in each incremental session, meaning each client has an average of 25 training samples for new classes. The experimental results indicate that the model performs best when the amount of replay data is close to that of new training samples. Using either fewer or more replay samples may decrease the overall performance, primarily due to the negative effects of data imbalance. Indeed, limited replay data can be insufficient to retain old knowledge, while an excessive use of replayed data could potentially affect the adaptation to new classes. Notably, we employ dynamic replay data across different batches and new synthetic data are drawn from the replay buffer for each batch training."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "S4.T2.1",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "In the F2SCIL paradigm, base classes often dominate the dataset, leading to high overall accuracy even if performance on newly added classes is poor. To evaluate the ability to learn new classes while retaining old knowledge, Fig.  6  presents the classification accuracy of the proposed SDD and the baseline for old and new classes across different incremental sessions. In the first incremental session, SDD shows a 1.1% decrease in accuracy for the 60 old classes compared to the baseline, but achieves a 21.8% increase in accuracy for new classes. In the final incremental session (Session 8), SDD demonstrates a 2.3% improvement in accuracy for the 95 previously learned classes and an 18.2% increase for new classes compared to the baseline. Similar trends are observed in other incremental sessions, indicating that the proposed SDD effectively balances stability for old classes with plasticity for new classes."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "S4.T3.1",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "In federated learning, data heterogeneity across different clients is a common concern. In our F2SCIL paradigm, data scarcity exacerbates the challenges posed by the non-IID nature of training data. Following the previous work  [ 45 ] , we distribute the training data to clients by drawing    Dir  (   p ) similar-to  Dir  p \\alpha\\sim\\mathrm{Dir}(\\alpha\\mathbf{p}) italic_  roman_Dir ( italic_ bold_p )  from a Dirichlet distribution, where  p p \\mathbf{p} bold_p  denotes the prior distribution of new classes in a session and   > 0  0 \\alpha>0 italic_ > 0  controls the non-IID characteristic of training data across multiple clients. As    0   0 \\alpha\\to 0 italic_  0 , it implies that each client contains training samples from a single class. Conversely, as       \\alpha\\rightarrow\\infty italic_   , the distribution of training data across clients would perfectly align with the prior class distribution In the previous experiments,    \\alpha italic_  is set to 1. Fig.  7  visualizes the distribution of local training data with different Dirichlet distribution value    \\alpha italic_  in an incremental session."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "S4.T4.1",
        "footnotes": [],
        "references": [
            "Next, we vary    \\alpha italic_  to adjust the non-IID degree of the training data and evaluate the generalization performance of the proposed SDD across different data distributions. Fig.  8  and Fig.  9  illustrate the experimental results on the CIFAR100 and miniImageNet datasets, respectively. For the baseline, the data heterogeneity and scarcity result in significant variations among the local models of the clients, leading to a substantial performance decrease of the global model. Our proposed SDD, using the CSWA strategy, is able to aggregate multiple local models effectively, constructing a more stable global model across different data distributions."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "S4.T5.1",
        "footnotes": [],
        "references": [
            "Next, we vary    \\alpha italic_  to adjust the non-IID degree of the training data and evaluate the generalization performance of the proposed SDD across different data distributions. Fig.  8  and Fig.  9  illustrate the experimental results on the CIFAR100 and miniImageNet datasets, respectively. For the baseline, the data heterogeneity and scarcity result in significant variations among the local models of the clients, leading to a substantial performance decrease of the global model. Our proposed SDD, using the CSWA strategy, is able to aggregate multiple local models effectively, constructing a more stable global model across different data distributions."
        ]
    }
}