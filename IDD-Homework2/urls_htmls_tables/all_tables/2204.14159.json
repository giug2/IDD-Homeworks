{
    "PAPER'S NUMBER OF TABLES": 4,
    "S6.T1": {
        "caption": "Table 1. Virtual Machines used in our experiment",
        "table": "<table id=\"S6.T1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S6.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S6.T1.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S6.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Physical CPU</th>\n<th id=\"S6.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Virtual CPU</th>\n<th id=\"S6.T1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Memory</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S6.T1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">Aggregator</td>\n<td id=\"S6.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4</td>\n<td id=\"S6.T1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">8</td>\n<td id=\"S6.T1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">24GB</td>\n</tr>\n<tr id=\"S6.T1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S6.T1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">Client-1</td>\n<td id=\"S6.T1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4</td>\n<td id=\"S6.T1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">8</td>\n<td id=\"S6.T1.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">24GB</td>\n</tr>\n<tr id=\"S6.T1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S6.T1.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">Client-2</td>\n<td id=\"S6.T1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">8</td>\n<td id=\"S6.T1.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">8</td>\n<td id=\"S6.T1.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">20GB</td>\n</tr>\n<tr id=\"S6.T1.1.5.4\" class=\"ltx_tr\">\n<td id=\"S6.T1.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">Client-3</td>\n<td id=\"S6.T1.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">8</td>\n<td id=\"S6.T1.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">8</td>\n<td id=\"S6.T1.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">15GB</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We evaluate the performance of our deep learning system in this section. We first present our datasets and the data proportion on each client. Then, we present the implement of our learning approaches on the homogeneous SCDGs and on the inhomogeneous SCDGs. The results also are compared the implement of the Centralized Learning.\nThe FL experiment is deployed on 4 virtual machines. Their resources are reported in Table 1."
        ]
    },
    "S6.T2": {
        "caption": "Table 2. The distribution of data in FL ",
        "table": "<table id=\"S6.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S6.T2.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S6.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Partitions</th>\n<td id=\"S6.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Training set</td>\n<td id=\"S6.T2.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Test set</td>\n</tr>\n<tr id=\"S6.T2.1.2.2\" class=\"ltx_tr\">\n<th id=\"S6.T2.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt\" rowspan=\"3\"><span id=\"S6.T2.1.2.2.1.1\" class=\"ltx_text\">Dataset-1</span></th>\n<th id=\"S6.T2.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\">Client-1</th>\n<td id=\"S6.T2.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">678</td>\n<td id=\"S6.T2.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" rowspan=\"3\"><span id=\"S6.T2.1.2.2.4.1\" class=\"ltx_text\">226</span></td>\n</tr>\n<tr id=\"S6.T2.1.3.3\" class=\"ltx_tr\">\n<th id=\"S6.T2.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Client-2</th>\n<td id=\"S6.T2.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">678</td>\n</tr>\n<tr id=\"S6.T2.1.4.4\" class=\"ltx_tr\">\n<th id=\"S6.T2.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Client-3</th>\n<td id=\"S6.T2.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">678</td>\n</tr>\n<tr id=\"S6.T2.1.5.5\" class=\"ltx_tr\">\n<th id=\"S6.T2.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" colspan=\"2\"><span id=\"S6.T2.1.5.5.1.1\" class=\"ltx_text ltx_font_bold\">Total</span></th>\n<td id=\"S6.T2.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2034</td>\n<td id=\"S6.T2.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">226</td>\n</tr>\n<tr id=\"S6.T2.1.6.6\" class=\"ltx_tr\">\n<th id=\"S6.T2.1.6.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_tt\" rowspan=\"3\"><span id=\"S6.T2.1.6.6.1.1\" class=\"ltx_text\">Dataset-2</span></th>\n<th id=\"S6.T2.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\">Client-1</th>\n<td id=\"S6.T2.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">1245</td>\n<td id=\"S6.T2.1.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">415</td>\n</tr>\n<tr id=\"S6.T2.1.7.7\" class=\"ltx_tr\">\n<th id=\"S6.T2.1.7.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Client-2</th>\n<td id=\"S6.T2.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1293</td>\n<td id=\"S6.T2.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">432</td>\n</tr>\n<tr id=\"S6.T2.1.8.8\" class=\"ltx_tr\">\n<th id=\"S6.T2.1.8.8.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\">Client-3</th>\n<td id=\"S6.T2.1.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">1246</td>\n<td id=\"S6.T2.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">416</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We collect malwares from Cisco and MalwareBazaar Database 111https://bazaar.abuse.ch/ to build two datasets: Dataset-1 and Dataset-2. Dataset-1 consists of 2260 malwares from 15 families. It is randomly splitted into two sets: the training set of 2034 malwares and the test set of 226 malwares. The SCDGs in Dataset-1 are computed by the same strategy, i.e., CDFS which is presented in Section 3. Dataset-1 is used for the Homogeneous-data scheme. Dataset-2 is a collection of 1844 malwares from 15 families. They are distributed to three clients. Each client implements its own strategy to compute SCDGs from binaries in Dataset-2. Particularly, Client-1 successfully extracts 1660 graphs by strategy BFS. Using strategy CBFS, Client-2 successfully extracts 1725 graphs. Client-3 obtains 1662 graphs by strategy CDFS. Since the clients implement different strategies, in a limit period of time, i.e., 20 minutes, the number of SCDGs extracted from Dataset-2 is different at each client. Thus, this challenges our secure learning model to deal with the context of inhomogeneous SCDGs. The data proportion of each dataset are detailed in Table 2.",
            "In this experiment, our approach is evaluate on a homogeneous dataset, i.e., Dataset-1. First, we evaluate the model on the centralized data of Dataset-1. Then, we compare this result to the implement of our secure learning approach where the data of Dataset-1 are splitted into 3 partitions of 678 malwares, and they are distributed to 3 clients. The data proportion at clients is shown in Table 2.",
            "We implement the secure training for 3 clients, using the library TenSEAL (Benaissa et al., 2021) for encrypting and aggregating the model parameters, and RSA cryptosystem222https://cryptography.io/ for a secure communication. The local models are trained on the client’s training set. The updated models are evaluated on the test set. Note that the client’s training set is a part of the training set in the Centralized Learning, and the test set is used for both the Centralized Learning and the Secure Federated Learning. The proportion of data at clients is shown in Table 2.",
            "Since the computing power is different from devices, the features, e.g., SCDG, are extracted accrordingly. The training graphs are computed in different techniques among clients even though that they are SCDGs. In this work, we consider three types of SCDG, that are extracted from binaries by the three different strategies in Section 3, i.e., BFS, CBFS and CDFS, at three clients. Particularly, Client-1 implements BFS, Client-2 implements CBFS, and Client-3 implements CDFS. The data proportion is reported in Table 2.\nIn the scheme, the clients have their own training set and test set. The training sets are used to train their models. Then, the test sets are used to evaluate the performance of the models.\nSimilar to previous experiment, we first implement the Centralized Learning in the client’s side. Then, we compare this results to the secure learning approach with 2 aggregation cases."
        ]
    },
    "S6.T3": {
        "caption": "Table 3. Comparison of the centralized learning and the secure federated learning.",
        "table": "<table id=\"S6.T3.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T3.1.1\" class=\"ltx_tr\">\n<td id=\"S6.T3.1.1.2\" class=\"ltx_td ltx_border_l ltx_border_rr ltx_border_t\" colspan=\"2\"></td>\n<td id=\"S6.T3.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Accuracy (<math id=\"S6.T3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\%\" display=\"inline\"><semantics id=\"S6.T3.1.1.1.m1.1a\"><mo id=\"S6.T3.1.1.1.m1.1.1\" xref=\"S6.T3.1.1.1.m1.1.1.cmml\">%</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T3.1.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S6.T3.1.1.1.m1.1.1.cmml\" xref=\"S6.T3.1.1.1.m1.1.1\">percent</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T3.1.1.1.m1.1c\">\\%</annotation></semantics></math>)</td>\n</tr>\n<tr id=\"S6.T3.1.2.1\" class=\"ltx_tr\">\n<td id=\"S6.T3.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t\" colspan=\"2\"><em id=\"S6.T3.1.2.1.1.1\" class=\"ltx_emph ltx_font_italic\">Centralized Learning</em></td>\n<td id=\"S6.T3.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">85.4</td>\n</tr>\n<tr id=\"S6.T3.1.3.2\" class=\"ltx_tr\">\n<td id=\"S6.T3.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_tt\" colspan=\"2\"><em id=\"S6.T3.1.3.2.1.1\" class=\"ltx_emph ltx_font_italic\">Full-Aggregation Learning</em></td>\n<td id=\"S6.T3.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">84.96</td>\n</tr>\n<tr id=\"S6.T3.1.4.3\" class=\"ltx_tr\">\n<td id=\"S6.T3.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_tt\" rowspan=\"3\"><span id=\"S6.T3.1.4.3.1.1\" class=\"ltx_text\"><em id=\"S6.T3.1.4.3.1.1.1\" class=\"ltx_emph ltx_font_italic\">Partly-Aggregation Learning</em></span></td>\n<td id=\"S6.T3.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_tt\">Client-1</td>\n<td id=\"S6.T3.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S6.T3.1.4.3.3.1\" class=\"ltx_text ltx_font_bold\">85.84</span></td>\n</tr>\n<tr id=\"S6.T3.1.5.4\" class=\"ltx_tr\">\n<td id=\"S6.T3.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">Client-2</td>\n<td id=\"S6.T3.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">83.63</td>\n</tr>\n<tr id=\"S6.T3.1.6.5\" class=\"ltx_tr\">\n<td id=\"S6.T3.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t\">Client-3</td>\n<td id=\"S6.T3.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S6.T3.1.6.5.2.1\" class=\"ltx_text ltx_font_bold\">85.4</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Figure 4 shows that the performance of the local models is improved after 10 training rounds. For the Full-Aggregation Learning (Figure 4-a), the performance of all clients are similar, and their accuracy can reach 84.96% comparing to the accuracy of 85.4% in the Centralized Learning. Although the increment of accuracy is a bit different among clients in the Partly-Aggregation Learning, they are getting more converged at the end of the training phase. Comparing to the Centralized Learning, the performance of Client-1 and Client-3 in Partly-Aggregation Learning is equal to or even better than the ones in the Centralized Learning. The results are also reported in Table 3."
        ]
    },
    "S6.T4": {
        "caption": "Table 4. Comparison of the centralized learning and the secure federated learning in the inhomogeneous-data scheme.",
        "table": "<table id=\"S6.T4.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T4.1.1\" class=\"ltx_tr\">\n<td id=\"S6.T4.1.1.2\" class=\"ltx_td ltx_border_l ltx_border_rr ltx_border_t\" colspan=\"2\"></td>\n<td id=\"S6.T4.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Accuracy (<math id=\"S6.T4.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\%\" display=\"inline\"><semantics id=\"S6.T4.1.1.1.m1.1a\"><mo id=\"S6.T4.1.1.1.m1.1.1\" xref=\"S6.T4.1.1.1.m1.1.1.cmml\">%</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T4.1.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S6.T4.1.1.1.m1.1.1.cmml\" xref=\"S6.T4.1.1.1.m1.1.1\">percent</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T4.1.1.1.m1.1c\">\\%</annotation></semantics></math>)</td>\n</tr>\n<tr id=\"S6.T4.1.2.1\" class=\"ltx_tr\">\n<td id=\"S6.T4.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S6.T4.1.2.1.1.1\" class=\"ltx_text\"><em id=\"S6.T4.1.2.1.1.1.1\" class=\"ltx_emph ltx_font_italic\">Centralized Learning</em></span></td>\n<td id=\"S6.T4.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">Client-1</td>\n<td id=\"S6.T4.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">92.05</td>\n</tr>\n<tr id=\"S6.T4.1.3.2\" class=\"ltx_tr\">\n<td id=\"S6.T4.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">Client-2</td>\n<td id=\"S6.T4.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">81.02</td>\n</tr>\n<tr id=\"S6.T4.1.4.3\" class=\"ltx_tr\">\n<td id=\"S6.T4.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">Client-3</td>\n<td id=\"S6.T4.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">89.9</td>\n</tr>\n<tr id=\"S6.T4.1.5.4\" class=\"ltx_tr\">\n<td id=\"S6.T4.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" rowspan=\"3\"><span id=\"S6.T4.1.5.4.1.1\" class=\"ltx_text\"><em id=\"S6.T4.1.5.4.1.1.1\" class=\"ltx_emph ltx_font_italic\">Full-Aggregation Learning</em></span></td>\n<td id=\"S6.T4.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_tt\">Client-1</td>\n<td id=\"S6.T4.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">87.4</td>\n</tr>\n<tr id=\"S6.T4.1.6.5\" class=\"ltx_tr\">\n<td id=\"S6.T4.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">Client-2</td>\n<td id=\"S6.T4.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S6.T4.1.6.5.2.1\" class=\"ltx_text ltx_font_bold\">87.27</span></td>\n</tr>\n<tr id=\"S6.T4.1.7.6\" class=\"ltx_tr\">\n<td id=\"S6.T4.1.7.6.1\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">Client-3</td>\n<td id=\"S6.T4.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">88.22</td>\n</tr>\n<tr id=\"S6.T4.1.8.7\" class=\"ltx_tr\">\n<td id=\"S6.T4.1.8.7.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_tt\" rowspan=\"3\"><span id=\"S6.T4.1.8.7.1.1\" class=\"ltx_text\"><em id=\"S6.T4.1.8.7.1.1.1\" class=\"ltx_emph ltx_font_italic\">Partly-Aggregation Learning</em></span></td>\n<td id=\"S6.T4.1.8.7.2\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_tt\">Client-1</td>\n<td id=\"S6.T4.1.8.7.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S6.T4.1.8.7.3.1\" class=\"ltx_text ltx_font_bold\">93.73</span></td>\n</tr>\n<tr id=\"S6.T4.1.9.8\" class=\"ltx_tr\">\n<td id=\"S6.T4.1.9.8.1\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">Client-2</td>\n<td id=\"S6.T4.1.9.8.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S6.T4.1.9.8.2.1\" class=\"ltx_text ltx_font_bold\">87.96</span></td>\n</tr>\n<tr id=\"S6.T4.1.10.9\" class=\"ltx_tr\">\n<td id=\"S6.T4.1.10.9.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t\">Client-3</td>\n<td id=\"S6.T4.1.10.9.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">89.18</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Figure 5-a shows that the performance of the local models is getting improvement after 20 training rounds in Full-Aggregation Learning. Three clients can reach the accuracy of ∼87%similar-toabsentpercent87\\sim 87\\% after 11 training rounds. Then, the performance of Client-1 goes down to 79%percent7979\\% while Client-3 reach its peak, i.e., 88.22%percent88.2288.22\\%, at the 14-th round. Compare to the Centralized Learning, Client-2 in Full-Aggregation Learning can achieve the better performance at the 8-th round. It reach 87.27% of accuracy, at the 11-th round while the accuracy in the Centralized Learning is 81.02%. The degradation at Client-1 in Full-Aggregation Learning is about 5.1%percent5.15.1\\% comparing to the Centralized Learning.\nFor Partly-Aggregation Learning in Figure 5-b, Client-1 gets the accuracy of 93.73% after 17 training rounds while Client-2 and Client-3 get 87.96% and 89.18%, respectively. The performance of Client-1 and Client-2 overtake the ones in Centralized Learning while the degradation at Client-3 is about 0.72%. The results are also reported in Table 4."
        ]
    }
}