{
    "id_table_1": {
        "caption": "Table 1:  Main results on Instruction Following benchmarks: IFEval, FollowBench, and InFoBench. Bold-face indicates the best results among the 7B models.    \\dagger   indicates that the results are from the original source.    \\star   denotes the mixture of the Conifer dataset with ShareGPT from the original paper  (Sun et al.,  2024 ) . Note that in\" abbreviates instruction\" and pr\" abreviates prompt\" in the above table, so for example loose in\" abbreviates loose instruction accuracy\". These abbreviations hold for subsequent tables as well.",
        "table": "A4.EGx1",
        "footnotes": [
            ""
        ],
        "references": [
            "To this end, we propose a novel technique, Evolutionary Contrastive Distillation (ECD), for generating high-quality, synthetic preference data targeting complex instruction-following. Following  Xu et al. ( 2023 ); Sun et al. ( 2024 ) , we prompt proprietary LLMs to progressively increase the complexity of instructions. Instead of separating the original example and the evolved example as two independent examples for SFT, however, we connect them as a preference pair for PFT: the proprietary LLMs response on the evolved instruction is considered as a positive example, and its response on the original instruction is considered a negative example. The key observation is that when an instruction is evolved to have a new requirement, the original  good  response for the original instruction is not a good response for the evolved instruction anymore, and therefore can be used as the negative example to the evolved instruction. Since the evolution of instructions is gradual, however, the original response still satisfies most of the requirements of the evolved instruction, and therefore can be used as a \"hard\" negative for the evolved instruction. See Figure  1  for a concrete example. This method has the desireable properties that (i) it does not rely on LLMs to annotate preferences or generate undesirable responses, which can be unreliable  (Yang et al.,  2023 )  and (ii) it is effective at creating hard negative examples, which have are crucial in contrastive learning  (Chopra et al.,  2005 ; Hadsell et al.,  2006 ; Robinson et al.,  2020 ) .",
            "Our ECD models achieve SOTA performance at the 7B scale for complex instruction-following. For example, consider our ECD model trained on ECD-FineGrained-Conifer. Table  1  shows that it outperforms Conifer-7B-DPO, the latest 7B SOTA in instruction-following, on each metric in IFEval by a substantial margin, improving loose prompt accuracy from 52.3% to 59.3% and loose instruction accuracy from 63.3% to 69.8%, and achieves similar improvements on FollowBench and InfoBench. Similarly, the ECD on ECD-Finegrained-Conifer improves over its initialization Conifer-7B-SFT, improving for example on loose prompt accuracy by over 10pp and even shows competitive performance with LLaMa-2-70B-Chat. ECD-FineGrained-Conifer achieves particularly strong performance on InfoBench Hard, indicating the strength of ECD in improving instruction-following for particularly difficult instructions. While here we discuss specifically ECD-FineGrained-Conifer, these trends uphold across our three ECD data mixtures, ECD-FineGrained, ECD-Conifer, and ECD-FineGrained-Conifer, indicating the robustness of our approach.",
            "See Table  6  for the full taxonomy of evolution operations. We also show the prompts for fine-grained evolution in Section  C.1 . Most of the evolution operations are designed to introduce gradual, nuanced changes to the original instruction. Examples of these evolutions can be found in Table  7 . Additionally, since the proposed method leverages DPO, it remains robust to occasional poor examples in the negative responses. For such \"easy\" negative responses, where the margin between positive and negative responses is large, the gradient magnitude will be small. This highlights another advantage of Preference Fine-Tuning over Supervised Fine-Tuning."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Main results on conversational quality benchmarks: MT-Bench and AlpacaEval.    \\dagger   indicates that the results are from the original paper.    \\star   denotes the mixture of the Conifer dataset with ShareGPT from the original source.    \\star   denotes the mixture of the Conifer dataset with ShareGPT from the original paper  (Sun et al.,  2024 ) .",
        "table": "S5.T1.7.7",
        "footnotes": [
            ""
        ],
        "references": [
            "See Figure  2  for a graphical illustration.",
            "To test the robustness of our approach, we generate 3 separate ECD datasets: (i)  ECD-FineGrained : 30k preference pairs from Fine-Grained Evolutionary Process discussed in Section  4 , 4 4 4 We execute this strategy for four rounds of evolution using Claude 2, generating a total of 104,499 preference data from ShareGPT instructions. Then we subsampled 30k.  (ii)  ECD-Conifer : ECD data based on the evolutionary process from Conifer  (Sun et al.,  2024 ) 5 5 5 We removed evolutions with process feedback-type, because it does not generate hard negatives needed for ECD. , and (iii)  ECD-FineGrained-Conifer : a concatenation of ECD-FineGrained and ECD-Conifer. We add UltraFeedback  (Cui et al.,  2023 )  to each of these to improve conversational quality 6 6 6 We use  https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized?row=1  and remove preference pairs with equal GPT-4 scores. ; see the Appendix  B.2  for the ablation on its impact."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Main table on Instruction Following benchmarks comparing ECD, RLCD, and RLAIF.",
        "table": "S5.T2.6",
        "footnotes": [],
        "references": [
            "While ECD methods show a robust ability to improve instruction-following, RLAIF and RLCD show uneven performance. Table  3  shows that ECD improves over the IFT initialization on the instruction-following benchmarks. For example, the ECD-FineGrained-Pure mixture achieves a loose prompt accuracy of 62.7% and the ECD-FineGrained-Conifer-Pure mixture achieves a loose prompt accuracy of 67.5% in comparison to 48.6% for its initialization Conifer-7B-SFT. On the other hand, RLAIF and RLCD show no improvement on IFEval and InfoBench while RLAIF only shows some marginal improvement over its initialization Conifer-7B-SFT on FollowBench."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Main table on conversational quality benchmarks comparing ECD, RLCD, and RLAIF.",
        "table": "S5.T3.1",
        "footnotes": [],
        "references": [
            "To ensure that each step of instruction evolution provides a high-quality example of subtle nuance in instruction-following, we devise a fine-grained hierarchical taxonomy of evolution operations, which we discuss in Section  4 . We demonstrate the data generated from this taxonomy is not only effective at improving complex instruction-following, but can also be combined with previous evolutionary  (Sun et al.,  2024 )  as well as non-evolutionary  (Cui et al.,  2023 )  methods to yield even stronger results. Therefore, we believe the proposed taxonomy will serve as a useful resource on its own for future research on complex instruction-following.",
            "Evolution : Prompt an LLM to evolve  I i ( t  1 ) superscript subscript I i t 1 I_{i}^{(t-1)} italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT  into proposal instruction  I ~ i ( t ) superscript subscript ~ I i t \\tilde{I}_{i}^{(t)} over~ start_ARG italic_I end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT . The evolution typically increases the complexity of the instruction. We discuss types of evolution operations we consider in Section  4 .",
            "We emphasize that this framework of generating preference data from a evolutionary process is generic, and can accommodate different definitions of the evolutionary process. In Section  5 , we show the evolutionary process from  Sun et al. ( 2024 )  can be successfully adopted in this framework. In order to further improve the quality of the data generated, we propose a fine-grained taxonomy of evolution operations in Section  4 .",
            "To test the robustness of our approach, we generate 3 separate ECD datasets: (i)  ECD-FineGrained : 30k preference pairs from Fine-Grained Evolutionary Process discussed in Section  4 , 4 4 4 We execute this strategy for four rounds of evolution using Claude 2, generating a total of 104,499 preference data from ShareGPT instructions. Then we subsampled 30k.  (ii)  ECD-Conifer : ECD data based on the evolutionary process from Conifer  (Sun et al.,  2024 ) 5 5 5 We removed evolutions with process feedback-type, because it does not generate hard negatives needed for ECD. , and (iii)  ECD-FineGrained-Conifer : a concatenation of ECD-FineGrained and ECD-Conifer. We add UltraFeedback  (Cui et al.,  2023 )  to each of these to improve conversational quality 6 6 6 We use  https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized?row=1  and remove preference pairs with equal GPT-4 scores. ; see the Appendix  B.2  for the ablation on its impact.",
            "On the other hand, for conversational quality, we observe in Table  4  that ECD by itself yields mostly no improvement, while RLCD and RLAIF show slight improvements. This finding highlights that the present instantiation of ECD is primarily a method for improving instruction-following."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Main table on Instruction Following benchmarks, IFEval and FollowBench, comparing DPO and SFT.",
        "table": "S5.T4.1",
        "footnotes": [],
        "references": [
            "To avoid quality issues from LLM-based preference annotation, Reinforcement Learning from Contrastive Distillation (RLCD)  (Yang et al.,  2023 )  proposes to employ two different prompt templates rather than using the same one. One prompt template is designed to elicit desirable responses, and another is designed to elicit undesirable response. Such a deliberate design of templates allows RLCD to bypass preference annotation. As we discuss in Section  5 , however, we find it challenging to prompt-engineer proprietary LLMs to generate undesirable responses. As proprietary LLMs are already aligned to generate helpful and harmless responses  (Bai et al.,  2022a ) , very often, their response from the undesirable response template are either as helpful as the response from the desirable response template, or trivially unhelpful (\"No, I cant answer that question.\"), limiting their value as a  hard  negative example.",
            "We emphasize that this framework of generating preference data from a evolutionary process is generic, and can accommodate different definitions of the evolutionary process. In Section  5 , we show the evolutionary process from  Sun et al. ( 2024 )  can be successfully adopted in this framework. In order to further improve the quality of the data generated, we propose a fine-grained taxonomy of evolution operations in Section  4 .",
            "Then, for each of the top-level category, we define around five fine-grained evolution types. See Table  6  for the full taxonomy of evolution operations. While Evol-Instruct  (Xu et al.,  2023 )  defines only eleven high-level operation types, and Confier  (Sun et al.,  2024 )  provides about eleven operation types in in-context examples, our taxonomy defines more fine-grained and hierarchically organized 22 operation types. In Section  5 , we find this taxonomy generates higher-quality preference data for ECD.",
            "What is the impact of using SFT instead of DPO?  We investigated the importance of DPO for the instruction-following ability of our models. In particular, we performed an epoch of SFT instead of DPO on positive respones from ECD-FineGrained, denoted ECD-FineGrained-SFT. Table  5  shows that SFT on ECD-FineGrained underperforms its initialization Conifer-7B-SFT, while DPO on the same data makes strong improvements. This indicates PFT with contrastive learning is a more effective method for improving complex instruction-following, compared to SFT."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Full Taxonomy of Evolution Operations",
        "table": "S5.T5.1",
        "footnotes": [],
        "references": [
            "Then, for each of the top-level category, we define around five fine-grained evolution types. See Table  6  for the full taxonomy of evolution operations. While Evol-Instruct  (Xu et al.,  2023 )  defines only eleven high-level operation types, and Confier  (Sun et al.,  2024 )  provides about eleven operation types in in-context examples, our taxonomy defines more fine-grained and hierarchically organized 22 operation types. In Section  5 , we find this taxonomy generates higher-quality preference data for ECD.",
            "See Table  6  for the full taxonomy of evolution operations. We also show the prompts for fine-grained evolution in Section  C.1 . Most of the evolution operations are designed to introduce gradual, nuanced changes to the original instruction. Examples of these evolutions can be found in Table  7 . Additionally, since the proposed method leverages DPO, it remains robust to occasional poor examples in the negative responses. For such \"easy\" negative responses, where the margin between positive and negative responses is large, the gradient magnitude will be small. This highlights another advantage of Preference Fine-Tuning over Supervised Fine-Tuning."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Examples of finegrained evolutions.",
        "table": "A2.T6.1.1",
        "footnotes": [],
        "references": [
            "See Table  6  for the full taxonomy of evolution operations. We also show the prompts for fine-grained evolution in Section  C.1 . Most of the evolution operations are designed to introduce gradual, nuanced changes to the original instruction. Examples of these evolutions can be found in Table  7 . Additionally, since the proposed method leverages DPO, it remains robust to occasional poor examples in the negative responses. For such \"easy\" negative responses, where the margin between positive and negative responses is large, the gradient magnitude will be small. This highlights another advantage of Preference Fine-Tuning over Supervised Fine-Tuning."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Instruction-Following Benchmarks for ablation of removing UltraFeedback from ECD-Conifer-Pure.",
        "table": "A2.T7.1.1",
        "footnotes": [],
        "references": [
            "In this section, we conduct an ablation study on removing UltraFeedback from the ECD-Conifer data mixture. Again, we use the Conifer-7B-SFT model as the IFT initialization. We compare two checkpoints ECD-Conifer and ECD-Conifer-Pure. Whereas ECD-Conifer consists of both the ECD version of Conifer and Ultrafeedback to optimize both instruction-following and conversational quality, ECD-Conifer-Pure only removes UltraFeedback. Table  8  depicts the results for instruction-following and Table  9  depicts the results on conversational quality. On instruction-following, we see that ECD-Conifer-Pure tends to outperform ECD-Conifer, with particularly strong performance on IFEval. For example, it improves the strict prompt accuracy by 5.1%. On the other hand, for conversational quality, ECD-Conifer improves on ECD-Conifer-Pure with a much improve MT-Bench score and LC Win-Rate, indicating the usefulness of UltraFeedback for conversational quality."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Response Quality Benchmarks for ablation of removing UltraFeedback from ECD-Conifer-Pure.F.",
        "table": "A2.T8.1.1",
        "footnotes": [],
        "references": [
            "In this section, we conduct an ablation study on removing UltraFeedback from the ECD-Conifer data mixture. Again, we use the Conifer-7B-SFT model as the IFT initialization. We compare two checkpoints ECD-Conifer and ECD-Conifer-Pure. Whereas ECD-Conifer consists of both the ECD version of Conifer and Ultrafeedback to optimize both instruction-following and conversational quality, ECD-Conifer-Pure only removes UltraFeedback. Table  8  depicts the results for instruction-following and Table  9  depicts the results on conversational quality. On instruction-following, we see that ECD-Conifer-Pure tends to outperform ECD-Conifer, with particularly strong performance on IFEval. For example, it improves the strict prompt accuracy by 5.1%. On the other hand, for conversational quality, ECD-Conifer improves on ECD-Conifer-Pure with a much improve MT-Bench score and LC Win-Rate, indicating the usefulness of UltraFeedback for conversational quality."
        ]
    },
    "id_table_10": {
        "caption": "",
        "table": "A2.T9.1",
        "footnotes": [],
        "references": []
    }
}