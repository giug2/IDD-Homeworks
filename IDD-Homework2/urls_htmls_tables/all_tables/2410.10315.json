{
    "id_table_1": {
        "caption": "Table 1:  Preliminary round experimental results. In the Chunk column, the two numbers represent chunk_size and chunk_overlap, respectively. In the Coarse Ranking and Re-ranking columns, multiple search paths are separated by spaces, and within each search path, the components separated by commas represent the retrieval/sorting method and top-k, respectively.",
        "table": "S1.T1.1",
        "footnotes": [],
        "references": [
            "Our solution can be summarized by Fig.  1 , which includes a data processing workflow (Section  1.1 ) and the RAG process (Section  1.2 ).",
            "Additionally, we have designed other formats of question-and-answer templates. Inspired by Chain-of-Thought  Wei et al. ( 2022 ) , we designed a Chain-of-Thought question-and-answer template (see Appendix  A.2 ). Drawing from COSTAR  Teo ( 2023 ) , we designed a markdown format question-and-answer template (see Appendix  A.1 ). To emphasize the importance of the top1 document, we designed a focused question-and-answer template (see Appendix  A.3 ). Related experimental results are discussed therein.",
            "\\small{\\color{green}0} represents simple merging for coarse ranking, \\small{\\color{green}1} represents RRF fusion for coarse ranking, \\small{\\color{green}2} represents re-ranking fusion using method  1.2.5 , \\small{\\color{green}3} represents re-ranking fusion using method  1.2.5 , \\small{\\color{green}4} represents re-ranking fusion using method  1.2.5 .",
            "In the preliminary round, our main results are displayed in Table  1 , and improvements were made in the following four stages:",
            "For the query expansion and HyDE methods mentioned in Section  1.2.1 , we tested them during both the preliminary and semi-final stages, with results displayed in Tables  3  and  4 , respectively. Overall, since the query terms in the preliminary and semi-final competitions were already relatively specific, query rewriting did not bring any benefit. These rewriting methods might be more effective when user queries are incomplete.",
            "We tested different prompt types mentioned in Section  1.2.6  during the semi-final stage, and the results are shown in Table  5 . We found that the best results were still achieved with simple question-and-answer prompts. The Chain-of-Thought question-and-answer template led to too much explanatory output, the Markdown format question-and-answer template resulted in some extraneous characters, and the focused question-and-answer template did not significantly differ from the original template. Furthermore, through extensive experimentation with more prompts, we discovered that the GLM4 performs better with simpler prompts; more complex, structured prompts tend to have a negative impact."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Semi-final experimental results. In the Chunk column, the two numbers represent chunk_size and chunk_overlap, respectively. In the Coarse Ranking and Re-ranking columns, multiple search paths are separated by spaces, and within each search path, the components separated by commas represent the retrieval/sorting method, top-k, and the type of document expansion (if no expansion is applied, it is not listed).",
        "table": "S1.T2.1",
        "footnotes": [],
        "references": [
            "Our solution can be summarized by Fig.  1 , which includes a data processing workflow (Section  1.1 ) and the RAG process (Section  1.2 ).",
            "For the generation of fictional documents, we devised two approaches, as shown in Figure  2 . Initially, following the papers methodology, we input the prompt  p h  y subscript p h y p_{hy} italic_p start_POSTSUBSCRIPT italic_h italic_y end_POSTSUBSCRIPT  and the original question  q q q italic_q  into the large language model  L L \\mathcal{L} caligraphic_L  to produce the fictional document  q 0  subscript superscript q  0 q^{\\prime}_{0} italic_q start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT . However, during the semifinals, we discovered that such fictional documents contained a significant amount of irrelevant keywords and redundant information due to the large models hallucinations, greatly affecting the effectiveness of the retrieval process. Therefore, we attempted to minimize the hallucinations and redundant information in the initial fictional document  q 0  subscript superscript q  0 q^{\\prime}_{0} italic_q start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  by using the BM25 algorithm and dense retrieval (using GTE-QWEN encoding) to identify the most relevant top1 document and use it for context prompting.",
            "Additionally, we have designed other formats of question-and-answer templates. Inspired by Chain-of-Thought  Wei et al. ( 2022 ) , we designed a Chain-of-Thought question-and-answer template (see Appendix  A.2 ). Drawing from COSTAR  Teo ( 2023 ) , we designed a markdown format question-and-answer template (see Appendix  A.1 ). To emphasize the importance of the top1 document, we designed a focused question-and-answer template (see Appendix  A.3 ). Related experimental results are discussed therein.",
            "\\small{\\color{green}0} represents simple merging for coarse ranking, \\small{\\color{green}1} represents RRF fusion for coarse ranking, \\small{\\color{green}2} represents re-ranking fusion using method  1.2.5 , \\small{\\color{green}3} represents re-ranking fusion using method  1.2.5 , \\small{\\color{green}4} represents re-ranking fusion using method  1.2.5 .",
            "In the preliminary round, we displayed our main results in Table  2 , and we made improvements through the following four stages:",
            "For the query expansion and HyDE methods mentioned in Section  1.2.1 , we tested them during both the preliminary and semi-final stages, with results displayed in Tables  3  and  4 , respectively. Overall, since the query terms in the preliminary and semi-final competitions were already relatively specific, query rewriting did not bring any benefit. These rewriting methods might be more effective when user queries are incomplete.",
            "We tested different prompt types mentioned in Section  1.2.6  during the semi-final stage, and the results are shown in Table  5 . We found that the best results were still achieved with simple question-and-answer prompts. The Chain-of-Thought question-and-answer template led to too much explanatory output, the Markdown format question-and-answer template resulted in some extraneous characters, and the focused question-and-answer template did not significantly differ from the original template. Furthermore, through extensive experimentation with more prompts, we discovered that the GLM4 performs better with simpler prompts; more complex, structured prompts tend to have a negative impact."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Rewrite Performance",
        "table": "S2.T3.1",
        "footnotes": [],
        "references": [
            "Additionally, we have designed other formats of question-and-answer templates. Inspired by Chain-of-Thought  Wei et al. ( 2022 ) , we designed a Chain-of-Thought question-and-answer template (see Appendix  A.2 ). Drawing from COSTAR  Teo ( 2023 ) , we designed a markdown format question-and-answer template (see Appendix  A.1 ). To emphasize the importance of the top1 document, we designed a focused question-and-answer template (see Appendix  A.3 ). Related experimental results are discussed therein.",
            "For the query expansion and HyDE methods mentioned in Section  1.2.1 , we tested them during both the preliminary and semi-final stages, with results displayed in Tables  3  and  4 , respectively. Overall, since the query terms in the preliminary and semi-final competitions were already relatively specific, query rewriting did not bring any benefit. These rewriting methods might be more effective when user queries are incomplete."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  HyDE Performance",
        "table": "S2.T4.1",
        "footnotes": [],
        "references": [
            "For the query expansion and HyDE methods mentioned in Section  1.2.1 , we tested them during both the preliminary and semi-final stages, with results displayed in Tables  3  and  4 , respectively. Overall, since the query terms in the preliminary and semi-final competitions were already relatively specific, query rewriting did not bring any benefit. These rewriting methods might be more effective when user queries are incomplete."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Effects of Different Prompts",
        "table": "S2.T5.1",
        "footnotes": [],
        "references": [
            "\\small{\\color{green}0} represents simple merging for coarse ranking, \\small{\\color{green}1} represents RRF fusion for coarse ranking, \\small{\\color{green}2} represents re-ranking fusion using method  1.2.5 , \\small{\\color{green}3} represents re-ranking fusion using method  1.2.5 , \\small{\\color{green}4} represents re-ranking fusion using method  1.2.5 .",
            "We tested different prompt types mentioned in Section  1.2.6  during the semi-final stage, and the results are shown in Table  5 . We found that the best results were still achieved with simple question-and-answer prompts. The Chain-of-Thought question-and-answer template led to too much explanatory output, the Markdown format question-and-answer template resulted in some extraneous characters, and the focused question-and-answer template did not significantly differ from the original template. Furthermore, through extensive experimentation with more prompts, we discovered that the GLM4 performs better with simpler prompts; more complex, structured prompts tend to have a negative impact."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Effects of BM25 acceleration on the test set. Time represents the total search time for 103 questions related to BM25, and accuracy represents the evaluation score of the final generated answers.",
        "table": "S5.T6.1",
        "footnotes": [],
        "references": [
            "We tested different prompt types mentioned in Section  1.2.6  during the semi-final stage, and the results are shown in Table  5 . We found that the best results were still achieved with simple question-and-answer prompts. The Chain-of-Thought question-and-answer template led to too much explanatory output, the Markdown format question-and-answer template resulted in some extraneous characters, and the focused question-and-answer template did not significantly differ from the original template. Furthermore, through extensive experimentation with more prompts, we discovered that the GLM4 performs better with simpler prompts; more complex, structured prompts tend to have a negative impact."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Reranker Acceleration Experiment",
        "table": "S5.T7.1",
        "footnotes": [],
        "references": [
            "However, since the Reranker is time-consuming in practical inference, we considered whether fewer layers could be used to speed up the process. Classic early-exit techniques in BERT, such as FastBERT  Liu et al. ( 2020 )  and DeeBERT  Xin et al. ( 2020 ) , use information entropy exceeding a threshold as the condition for early exit, which is computationally intensive and results in unstable effects. Therefore, we designed a model early-exit algorithm based on maximum similarity selection, that is, for each query, we check if the softmax similarity output at the 12th layer in the first batch contains any values exceeding a certain threshold; if so, this query is inferred using just 12 layers, otherwise, 28 layers are used. We conducted an experiment using an A100 40G GPU to explore inference time, GPU memory usage, and accuracy at a batch size of 32, comparing different layers and early-exit methods. We randomly selected 10 queries and chose 192 text blocks for each, including 6 ground truth text blocks sorted using 28 layers in the complete RAG and 186 other random blocks. We predicted the sum of softmax scores of ground truth blocks relative to all blocks using various methods. Then, we assessed the similarity accuracy by dividing the predicted proportion by the proportion obtained with 28 layers, and compared the ranking accuracy of predicted ground truth with the 28-layer results, yielding the results shown in Table  7 . It can be seen that our proposed model early-exit method, while reducing inference time by 33%, is able to maintain ranking results consistent with those obtained using 28 layers directly, surpassing the entropy selection methods."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Effects of context compression on the test set. Compression rate refers to the ratio of the length of the compressed prompt to the original prompt; tokens saved refers to the reduction in context string length divided by the empirical value of 1.6 to estimate the number of tokens saved; time refers to the average time per question from document retrieval to answer generation, including context compression and GLM4 generation time.",
        "table": "S5.T8.1",
        "footnotes": [],
        "references": [
            "We designed a context compression method based on BM25 semantic similarity, which we call BM25-Extract. For each chunk, we first split it into sentences, then use BM25 to calculate the similarity between the query and each chunk, and finally add sentences to the list in order of decreasing similarity until a set compression rate is reached. The sentences are then concatenated in their original relative positions. We compared BM25-Extract with advanced context compression methods LLMLingua  Jiang et al. ( 2023a )  and LongLLMLingua  Jiang et al. ( 2023b )  as shown in Table  8 . Our method has advantages of no GPU memory usage, faster speed, and higher accuracy, making it evidently more effective for cost-sensitive operational maintenance tasks."
        ]
    },
    "global_footnotes": [
        "This work is a technical report of our solution at the 2024 (7th) CCF International AIOps Challenge. The official website of the Challenge is",
        "We also experimented with",
        "and",
        "as model compression techniques. While these methods reduce memory usage, they also significantly degrade performance, warranting further research."
    ]
}