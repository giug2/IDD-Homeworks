{
    "S3.T1.1.1": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" id=\"S3.T1.1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.1.1\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" id=\"S3.T1.1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.2.1\">Method</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.1.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.3.1\">ODS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.1.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.4.1\">OIS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T1.1.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.5.1\">LineIoU@3</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.2.2\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.1.1.2.2.1\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.1.1.2.2.2\">SwinT+OADecoder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib27\" title=\"\">27</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.2.2.3\">.494</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.2.2.4\">.822</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.2.2.5\">.270</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.3.3\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"S3.T1.1.1.3.3.1\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S3.T1.1.1.3.3.2\">SegFormer-b5&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib28\" title=\"\">28</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.3.3.3\">.635</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.3.3.4\">.884</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.3.3.5\">.391</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.4.4\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"S3.T1.1.1.4.4.1\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S3.T1.1.1.4.4.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.4.4.2.1\">\\cellcolor</span>[HTML]F3F3F3DirectSAM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib1\" title=\"\">1</a>]</cite> <span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T1.1.1.4.4.2.2\">ZS</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.4.4.3\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.4.4.3.1\">\\cellcolor</span>[HTML]F3F3F3.202</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.4.4.4\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.4.4.4.1\">\\cellcolor</span>[HTML]F3F3F3.634</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.4.4.5\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.4.4.5.1\">\\cellcolor</span>[HTML]F3F3F3.065</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.5.5\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"S3.T1.1.1.5.5.1\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S3.T1.1.1.5.5.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.5.5.2.1\">\\cellcolor</span>[HTML]F3F3F3DirectSAM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib1\" title=\"\">1</a>]</cite> <span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T1.1.1.5.5.2.2\">FT</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.5.5.3\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.5.5.3.1\">\\cellcolor</span>[HTML]F3F3F3.650</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.5.5.4\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.5.5.4.1\">\\cellcolor</span>[HTML]F3F3F3.937</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.5.5.5\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.5.5.5.1\">\\cellcolor</span>[HTML]F3F3F3.395</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.6.6\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"S3.T1.1.1.6.6.1\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S3.T1.1.1.6.6.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.6.6.2.1\">\\cellcolor</span>[HTML]E2ECFDDirectSAM-RS <span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T1.1.1.6.6.2.2\">ZS</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.6.6.3\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.6.6.3.1\">\\cellcolor</span>[HTML]E2ECFD.653</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.6.6.4\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.6.6.4.1\">\\cellcolor</span>[HTML]E2ECFD.911</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.6.6.5\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.6.6.5.1\">\\cellcolor</span>[HTML]E2ECFD.337</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.7.7\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S3.T1.1.1.7.7.1\"><span class=\"ltx_text\" id=\"S3.T1.1.1.7.7.1.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.1.1.7.7.1.1.1\">\n<span class=\"ltx_tr\" id=\"S3.T1.1.1.7.7.1.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.1.1.7.7.1.1.1.1.1\">LRSNY&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib16\" title=\"\">16</a>]</cite></span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.1.1.7.7.1.1.1.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.1.1.7.7.1.1.1.2.1\">(Road)</span></span>\n</span></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S3.T1.1.1.7.7.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.7.7.2.1\">\\cellcolor</span>[HTML]E2ECFDDirectSAM-RS <span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T1.1.1.7.7.2.2\">FT</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.7.7.3\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.7.7.3.1\">\\cellcolor</span>[HTML]E2ECFD<span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.7.7.3.2\">.772</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.7.7.4\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.7.7.4.1\">\\cellcolor</span>[HTML]E2ECFD<span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.7.7.4.2\">.962</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.7.7.5\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.7.7.5.1\">\\cellcolor</span>[HTML]E2ECFD<span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.7.7.5.2\">.455</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.8.8\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.1.1.8.8.1\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.1.1.8.8.2\">\n<span class=\"ltx_rule\" style=\"width:0.0pt;height:9.5pt;background:black;display:inline-block;\"/>\nBDCN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib29\" title=\"\">29</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.8.8.3\">.536</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.8.8.4\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.8.8.5\">.385</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.9.9\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"S3.T1.1.1.9.9.1\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S3.T1.1.1.9.9.2\">SDLED&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib18\" title=\"\">18</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.9.9.3\">.842</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.9.9.4\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.9.9.5\">.615</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.10.10\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"S3.T1.1.1.10.10.1\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S3.T1.1.1.10.10.2\">SegFormer-b5&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib28\" title=\"\">28</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.10.10.3\">.844</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.10.10.4\">.956</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.10.10.5\">.501</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.11.11\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"S3.T1.1.1.11.11.1\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S3.T1.1.1.11.11.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.11.11.2.1\">\\cellcolor</span>[HTML]F3F3F3DirectSAM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib1\" title=\"\">1</a>]</cite> <span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T1.1.1.11.11.2.2\">ZS</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.11.11.3\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.11.11.3.1\">\\cellcolor</span>[HTML]F3F3F3.237</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.11.11.4\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.11.11.4.1\">\\cellcolor</span>[HTML]F3F3F3.555</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.11.11.5\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.11.11.5.1\">\\cellcolor</span>[HTML]F3F3F3.076</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.12.12\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"S3.T1.1.1.12.12.1\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S3.T1.1.1.12.12.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.12.12.2.1\">\\cellcolor</span>[HTML]F3F3F3DirectSAM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib1\" title=\"\">1</a>]</cite> <span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T1.1.1.12.12.2.2\">FT</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.12.12.3\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.12.12.3.1\">\\cellcolor</span>[HTML]F3F3F3.864</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.12.12.4\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.12.12.4.1\">\\cellcolor</span>[HTML]F3F3F3.971</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.12.12.5\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.12.12.5.1\">\\cellcolor</span>[HTML]F3F3F3.518</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.13.13\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"S3.T1.1.1.13.13.1\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S3.T1.1.1.13.13.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.13.13.2.1\">\\cellcolor</span>[HTML]E2ECFDDirectSAM-RS <span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T1.1.1.13.13.2.2\">ZS</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.13.13.3\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.13.13.3.1\">\\cellcolor</span>[HTML]E2ECFD.705</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.13.13.4\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.13.13.4.1\">\\cellcolor</span>[HTML]E2ECFD.899</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.13.13.5\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.13.13.5.1\">\\cellcolor</span>[HTML]E2ECFD.329</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.14.14\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S3.T1.1.1.14.14.1\"><span class=\"ltx_text\" id=\"S3.T1.1.1.14.14.1.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.1.1.14.14.1.1.1\">\n<span class=\"ltx_tr\" id=\"S3.T1.1.1.14.14.1.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.1.1.14.14.1.1.1.1.1\">BUBE&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib18\" title=\"\">18</a>]</cite></span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.1.1.14.14.1.1.1.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.1.1.14.14.1.1.1.2.1\">(Building)</span></span>\n</span></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S3.T1.1.1.14.14.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.14.14.2.1\">\\cellcolor</span>[HTML]E2ECFDDirectSAM-RS <span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T1.1.1.14.14.2.2\">FT</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.14.14.3\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.14.14.3.1\">\\cellcolor</span>[HTML]E2ECFD<span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.14.14.3.2\">.887</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.14.14.4\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.14.14.4.1\">\\cellcolor</span>[HTML]E2ECFD<span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.14.14.4.2\">.997</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.14.14.5\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.14.14.5.1\">\\cellcolor</span>[HTML]E2ECFD<span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.14.14.5.2\">.565</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.15.15\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.1.1.15.15.1\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.1.1.15.15.2\">\n<span class=\"ltx_rule\" style=\"width:0.0pt;height:9.5pt;background:black;display:inline-block;\"/>\nHED&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib30\" title=\"\">30</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.15.15.3\">.897</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.15.15.4\">.994</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.15.15.5\">.768</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.16.16\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"S3.T1.1.1.16.16.1\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S3.T1.1.1.16.16.2\">SegFormer-b5&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib28\" title=\"\">28</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.16.16.3\">.861</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.16.16.4\">.964</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.16.16.5\">.742</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.17.17\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"S3.T1.1.1.17.17.1\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S3.T1.1.1.17.17.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.17.17.2.1\">\\cellcolor</span>[HTML]F3F3F3DirectSAM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib1\" title=\"\">1</a>]</cite> <span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T1.1.1.17.17.2.2\">ZS</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.17.17.3\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.17.17.3.1\">\\cellcolor</span>[HTML]F3F3F3.712</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.17.17.4\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.17.17.4.1\">\\cellcolor</span>[HTML]F3F3F3.942</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.17.17.5\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.17.17.5.1\">\\cellcolor</span>[HTML]F3F3F3.449</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.18.18\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"S3.T1.1.1.18.18.1\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S3.T1.1.1.18.18.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.18.18.2.1\">\\cellcolor</span>[HTML]F3F3F3DirectSAM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib1\" title=\"\">1</a>]</cite> <span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T1.1.1.18.18.2.2\">FT</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.18.18.3\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.18.18.3.1\">\\cellcolor</span>[HTML]F3F3F3.925</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.18.18.4\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.18.18.4.1\">\\cellcolor</span>[HTML]F3F3F3.973</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.18.18.5\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.18.18.5.1\">\\cellcolor</span>[HTML]F3F3F3.773</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.19.19\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"S3.T1.1.1.19.19.1\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S3.T1.1.1.19.19.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.19.19.2.1\">\\cellcolor</span>[HTML]E2ECFDDirectSAM-RS <span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T1.1.1.19.19.2.2\">ZS</span>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.19.19.3\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.19.19.3.1\">\\cellcolor</span>[HTML]E2ECFD.639</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.19.19.4\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.19.19.4.1\">\\cellcolor</span>[HTML]E2ECFD.803</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.19.19.5\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.19.19.5.1\">\\cellcolor</span>[HTML]E2ECFD.504</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.20.20\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T1.1.1.20.20.1\"><span class=\"ltx_text\" id=\"S3.T1.1.1.20.20.1.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.1.1.20.20.1.1.1\">\n<span class=\"ltx_tr\" id=\"S3.T1.1.1.20.20.1.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.1.1.20.20.1.1.1.1.1\">SLSD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib17\" title=\"\">17</a>]</cite></span></span>\n<span class=\"ltx_tr\" id=\"S3.T1.1.1.20.20.1.1.1.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.1.1.20.20.1.1.1.2.1\">(Coastline)</span></span>\n</span></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T1.1.1.20.20.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.20.20.2.1\">\\cellcolor</span>[HTML]E2ECFDDirectSAM-RS <span class=\"ltx_text ltx_font_typewriter\" id=\"S3.T1.1.1.20.20.2.2\">FT</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.1.1.20.20.3\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.20.20.3.1\">\\cellcolor</span>[HTML]E2ECFD<span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.20.20.3.2\">.958</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.1.1.20.20.4\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.20.20.4.1\">\\cellcolor</span>[HTML]E2ECFD<span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.20.20.4.2\">1.000</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.1.1.20.20.5\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T1.1.1.20.20.5.1\">\\cellcolor</span>[HTML]E2ECFD<span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.20.20.5.2\">.895</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "TABLE I:  Performance Comparison on\nLRSNY, BUBE and SLSD benchmark tasks. ",
        "footnotes": [
            "[27] \nR. Yang, Y. Zhong, Y. Liu, X. Lu, and L. Zhang, “Occlusion-aware road extraction network for high-resolution remote sensing imagery,”  IEEE Transactions on Geoscience and Remote Sensing , 2024.\n\n",
            "[28] \nE. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo, “Segformer: Simple and efficient design for semantic segmentation with transformers,”  Advances in Neural Information Processing Systems , vol. 34, pp. 12 077–12 090, 2021.\n\n",
            "[1] \nD. Chen, S. Cahyawijaya, J. Liu, B. Wang, and P. Fung, “Subobject-level image tokenization,”  arXiv preprint arXiv:2402.14327 , 2024.\n\n",
            "[1] \nD. Chen, S. Cahyawijaya, J. Liu, B. Wang, and P. Fung, “Subobject-level image tokenization,”  arXiv preprint arXiv:2402.14327 , 2024.\n\n",
            "[16] \nZ. Chen, C. Wang, J. Li, N. Xie, Y. Han, and J. Du, “Reconstruction bias u-net for road extraction from optical remote sensing images,”  IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing , 2021.\n\n",
            "[29] \nJ. He, S. Zhang, M. Yang, Y. Shan, and T. Huang, “Bi-directional cascade network for perceptual edge detection,” in  Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , 2019, pp. 3828–3837.\n\n",
            "[18] \nL. Xia, X. Zhang, J. Zhang, H. Yang, and T. Chen, “Building extraction from very-high-resolution remote sensing images using semi-supervised semantic edge detection,”  Remote Sensing , vol. 13, no. 11, p. 2187, 2021.\n\n",
            "[28] \nE. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo, “Segformer: Simple and efficient design for semantic segmentation with transformers,”  Advances in Neural Information Processing Systems , vol. 34, pp. 12 077–12 090, 2021.\n\n",
            "[1] \nD. Chen, S. Cahyawijaya, J. Liu, B. Wang, and P. Fung, “Subobject-level image tokenization,”  arXiv preprint arXiv:2402.14327 , 2024.\n\n",
            "[1] \nD. Chen, S. Cahyawijaya, J. Liu, B. Wang, and P. Fung, “Subobject-level image tokenization,”  arXiv preprint arXiv:2402.14327 , 2024.\n\n",
            "[18] \nL. Xia, X. Zhang, J. Zhang, H. Yang, and T. Chen, “Building extraction from very-high-resolution remote sensing images using semi-supervised semantic edge detection,”  Remote Sensing , vol. 13, no. 11, p. 2187, 2021.\n\n",
            "[30] \nS. Xie and Z. Tu, “Holistically-nested edge detection,” in  ICCV 2015 , 2015, pp. 1395–1403.\n\n",
            "[28] \nE. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo, “Segformer: Simple and efficient design for semantic segmentation with transformers,”  Advances in Neural Information Processing Systems , vol. 34, pp. 12 077–12 090, 2021.\n\n",
            "[1] \nD. Chen, S. Cahyawijaya, J. Liu, B. Wang, and P. Fung, “Subobject-level image tokenization,”  arXiv preprint arXiv:2402.14327 , 2024.\n\n",
            "[1] \nD. Chen, S. Cahyawijaya, J. Liu, B. Wang, and P. Fung, “Subobject-level image tokenization,”  arXiv preprint arXiv:2402.14327 , 2024.\n\n",
            "[17] \nJ. Feng, S. Wang, and Z. Gu, “A novel sea-land segmentation network for enhanced coastline extraction using satellite remote sensing images,”  Advances in Space Research , 2024.\n\n"
        ],
        "references": [
            "We assessed the performance of DirectSAM-RS in both zero-shot and fine-tuning settings. As shown in Table I, DirectSAM-RS achieved state-of-the-art performance across several downstream contour extraction benchmarks.",
            "We evaluated our DirectSAM-RS model in a zero-shot setting on three downstream contour extraction tasks, achieving strong performance without task-specific fine-tuning, as shown in Table I. The improvement between DirectSAM-RS ZS and DirectSAM ZS highlighted the significant impact of integrating textual semantics, demonstrating that language information provides generalization potential for DirectSAM-RS.",
            "Furthermore, we fine-tuned DirectSAM-RS with each benchmark dataset training split and evaluated it on the validation split. The results, presented in Table I, revealed that DirectSAM-RS significantly surpasses previous road, coastline, and building extraction SOTA methods, achieving notable 21%, 5%, and 7% improvement in ODS metrics respectively. The visualization inference result is shown in Figure 5. The powerful performance on different practical landmark extractions showcased that DirectSAM-RS possesses strong application value."
        ]
    },
    "S3.T3.1.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T3.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.2\">\n<div class=\"ltx_block ltx_minipage ltx_align_middle\" id=\"S3.T3.1.1.1.2.1\" style=\"width:281.9pt;\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_block\">TABLE II: </span>Performance comparison on LoveDA validation set.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T3.1.1.1.2.1.1\">\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T3.1.1.1.2.1.1.1.1\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T3.1.1.1.2.1.1.1.1.1\">\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.1.1.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.2.1.1.1.1.1.1.1.1\">Model</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.1.1.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.1.1.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.2.1.1.1.1.1.2.1.1\">Architecture</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T3.1.1.1.2.1.1.1.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T3.1.1.1.2.1.1.1.2.1\">\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.1.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.1.2.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.2.1.1.1.2.1.1.1.1\">Pretraining</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.1.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.1.2.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.2.1.1.1.2.1.2.1.1\">Data</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T3.1.1.1.2.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.2.1.1.1.3.1\">ODS</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.1.1.2.1.1.2.1\">LSTM-CNN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib31\" title=\"\">31</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.1.1.2.1.1.2.2\">ILSVRC&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib32\" title=\"\">32</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.1.1.2.1.1.2.3\">.416</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.3.1\">ConvLSTM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib33\" title=\"\">33</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.3.2\">Pascal-VOC&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib34\" title=\"\">34</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.3.3\">.459</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.4.1\">LAVT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib35\" title=\"\">35</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.4.2\">ImageNet22K&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib36\" title=\"\">36</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.4.3\">.595</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.5.1\">RRSIS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib22\" title=\"\">22</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.5.2\">ImageNet22K&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib36\" title=\"\">36</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.5.3\">.627</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.1.1.2.1.1.6.1\"><span class=\"ltx_rule\" style=\"width:0.0pt;height:8.6pt;background:black;display:inline-block;\"/></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.1.1.2.1.1.6.2\">Cityscapes&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib37\" title=\"\">37</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.1.1.2.1.1.6.3\">.663</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.1.1.1.2.1.1.7.1\"><span class=\"ltx_text\" id=\"S3.T3.1.1.1.2.1.1.7.1.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T3.1.1.1.2.1.1.7.1.1.1\">\n<span class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.7.1.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.7.1.1.1.1.1\">SegFormer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib28\" title=\"\">28</a>]</cite> +</span></span>\n<span class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.7.1.1.1.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.7.1.1.1.2.1\">Prompter</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.1.1.1.2.1.1.7.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T3.1.1.1.2.1.1.7.2.1\">\\cellcolor</span>[HTML]E2ECFD<span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.2.1.1.7.2.2\">SA-1B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib10\" title=\"\">10</a>]</cite></span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.1.1.1.2.1.1.7.3\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T3.1.1.1.2.1.1.7.3.1\">\\cellcolor</span>[HTML]E2ECFD<span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.2.1.1.7.3.2\">.694</span>\n</td>\n</tr>\n</table>\n</div>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1\">\n<div class=\"ltx_block ltx_minipage ltx_align_middle\" id=\"S3.T3.1.1.1.1.1\" style=\"width:260.2pt;\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_block\">TABLE III: </span>Ablation study on SA-1B pretrained weights.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T3.1.1.1.1.1.1\">\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.1.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T3.1.1.1.1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.1.1.1.1.2.1\">Class</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T3.1.1.1.1.1.1.1.3\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T3.1.1.1.1.1.1.1.3.1\">\\columncolor</span>[HTML]E2ECFD\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T3.1.1.1.1.1.1.1.3.2\">\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.1.1.1.1.3.2.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.1.3.2.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.1.1.1.1.3.2.1.1.1\">w/</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.1.1.1.1.3.2.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.1.3.2.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.1.1.1.1.3.2.2.1.1\">SA-1B</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T3.1.1.1.1.1.1.1.4\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T3.1.1.1.1.1.1.1.4.1\">\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.1.1.1.1.4.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.1.4.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.1.1.1.1.4.1.1.1.1\">w/o</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.1.1.1.1.4.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.1.4.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.1.1.1.1.4.1.2.1.1\">SA-1B</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T3.1.1.1.1.1.1.1.1\"><math alttext=\"\\Delta(\\%)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S3.T3.1.1.1.1.1.1.1.1.m1.1\"><semantics id=\"S3.T3.1.1.1.1.1.1.1.1.m1.1a\"><mrow id=\"S3.T3.1.1.1.1.1.1.1.1.m1.1b\"><mi id=\"S3.T3.1.1.1.1.1.1.1.1.m1.1.1\" mathvariant=\"normal\">&#916;</mi><mrow id=\"S3.T3.1.1.1.1.1.1.1.1.m1.1.2\"><mo id=\"S3.T3.1.1.1.1.1.1.1.1.m1.1.2.1\" stretchy=\"false\">(</mo><mo id=\"S3.T3.1.1.1.1.1.1.1.1.m1.1.2.2\">%</mo><mo id=\"S3.T3.1.1.1.1.1.1.1.1.m1.1.2.3\" stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\" id=\"S3.T3.1.1.1.1.1.1.1.1.m1.1c\">\\Delta(\\%)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T3.1.1.1.1.1.1.1.1.m1.1d\">roman_&#916; ( % )</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.1.1.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.1.1.1.1.1.2.1\">Road</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.1.1.1.1.1.2.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T3.1.1.1.1.1.1.2.2.1\">\\columncolor</span>[HTML]E2ECFD.806</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.1.1.1.1.1.2.3\">.757</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.1.1.1.1.1.2.4\"><span class=\"ltx_text\" id=\"S3.T3.1.1.1.1.1.1.2.4.1\" style=\"color:#FE0000;\">-6.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.1.1.1.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.3.1\">\n<span class=\"ltx_rule\" style=\"width:0.0pt;height:7.9pt;background:black;display:inline-block;\"/>\nAgriculture</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.3.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T3.1.1.1.1.1.1.3.2.1\">\\columncolor</span>[HTML]E2ECFD.668</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.3.3\">.625</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.3.4\"><span class=\"ltx_text\" id=\"S3.T3.1.1.1.1.1.1.3.4.1\" style=\"color:#FE0000;\">-6.4</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.1.1.1.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.4.1\">\n<span class=\"ltx_rule\" style=\"width:0.0pt;height:7.9pt;background:black;display:inline-block;\"/>\nBarren</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.4.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T3.1.1.1.1.1.1.4.2.1\">\\columncolor</span>[HTML]E2ECFD.666</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.4.3\">.606</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.4.4\"><span class=\"ltx_text\" id=\"S3.T3.1.1.1.1.1.1.4.4.1\" style=\"color:#FE0000;\">-9.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.1.1.1.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.5.1\">\n<span class=\"ltx_rule\" style=\"width:0.0pt;height:7.9pt;background:black;display:inline-block;\"/>\nWater</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.5.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T3.1.1.1.1.1.1.5.2.1\">\\columncolor</span>[HTML]E2ECFD.711</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.5.3\">.639</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.5.4\"><span class=\"ltx_text\" id=\"S3.T3.1.1.1.1.1.1.5.4.1\" style=\"color:#FE0000;\">-10.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.1.1.1.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.6.1\">\n<span class=\"ltx_rule\" style=\"width:0.0pt;height:7.9pt;background:black;display:inline-block;\"/>\nForest</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.6.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T3.1.1.1.1.1.1.6.2.1\">\\columncolor</span>[HTML]E2ECFD.756</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.6.3\">.673</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.6.4\"><span class=\"ltx_text\" id=\"S3.T3.1.1.1.1.1.1.6.4.1\" style=\"color:#FE0000;\">-11.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.1.1.1.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.1.1.1.1.1.1.7.1\">\n<span class=\"ltx_rule\" style=\"width:0.0pt;height:7.9pt;background:black;display:inline-block;\"/>\nBuilding</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.1.1.1.1.1.1.7.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T3.1.1.1.1.1.1.7.2.1\">\\columncolor</span>[HTML]E2ECFD.759</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.1.1.1.1.1.1.7.3\">.654</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.1.1.1.1.1.1.7.4\"><span class=\"ltx_text\" id=\"S3.T3.1.1.1.1.1.1.7.4.1\" style=\"color:#FE0000;\">-13.8</span></td>\n</tr>\n</table>\n</div>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "",
        "footnotes": [
            "[31] \nR. Hu, M. Rohrbach, and T. Darrell, “Segmentation from natural language expressions,” in  ECCV 2016 , 2016, pp. 108–124.\n\n",
            "[32] \nO. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,”  International Journal of Computer Vision (IJCV) , vol. 115, no. 3, pp. 211–252, 2015.\n\n",
            "[33] \nR. Li, K. Li, Y. Kuo, M. Shu, X. Qi, X. Shen, and J. Jia, “Referring image segmentation via recurrent refinement networks,” in  CVPR 2018 , 2018, pp. 5745–5753.\n\n",
            "[34] \nM. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, “The pascal visual object classes (voc) challenge,”  International Journal of Computer Vision , vol. 88, no. 2, pp. 303–338, Jun. 2010.\n\n",
            "[35] \nZ. Yang, J. Wang, Y. Tang, K. Chen, H. Zhao, and P. H. Torr, “Lavt: Language-aware vision transformer for referring image segmentation,” in  CVPR 2022 , 2022, pp. 18 155–18 165.\n\n",
            "[36] \nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in  2009 IEEE conference on computer vision and pattern recognition .   Ieee, 2009, pp. 248–255.\n\n",
            "[22] \nZ. Yuan, L. Mou, Y. Hua, and X. X. Zhu, “Rrsis: Referring remote sensing image segmentation,”  IEEE Transactions on Geoscience and Remote Sensing , 2024.\n\n",
            "[36] \nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in  2009 IEEE conference on computer vision and pattern recognition .   Ieee, 2009, pp. 248–255.\n\n",
            "[37] \nM. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic urban scene understanding,” in  Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2016.\n\n",
            "[28] \nE. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo, “Segformer: Simple and efficient design for semantic segmentation with transformers,”  Advances in Neural Information Processing Systems , vol. 34, pp. 12 077–12 090, 2021.\n\n",
            "[10] \nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo  et al. , “Segment anything,” in  ICCV 2023 , 2023, pp. 4015–4026.\n\n"
        ],
        "references": [
            "The pretraining weights ablation was conducted on the LoveDA validation set. As shown in Table III, the DirectSAM-RS initialized from weights pretrained on SA-1B (DirectSAM-1800px-0424111https://huggingface.co/chendelong/DirectSAM-1800px-0424) achieved the best ODS metric compared with existing methods pretrained on the traditional dataset. Therefore, we conducted SA-1B pretrained weights ablation study on each category of LoveDA validation set. Table III shows that DirectSAM-RS initialized from SA-1B weights outperforms the version without SA-1B weights by a large margin on every landmark category."
        ]
    },
    "S3.T3.1.1.1.2.1.1": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T3.1.1.1.2.1.1\">\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T3.1.1.1.2.1.1.1.1\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T3.1.1.1.2.1.1.1.1.1\">\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.1.1.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.2.1.1.1.1.1.1.1.1\">Model</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.1.1.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.1.1.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.2.1.1.1.1.1.2.1.1\">Architecture</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T3.1.1.1.2.1.1.1.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T3.1.1.1.2.1.1.1.2.1\">\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.1.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.1.2.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.2.1.1.1.2.1.1.1.1\">Pretraining</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.1.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.1.2.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.2.1.1.1.2.1.2.1.1\">Data</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T3.1.1.1.2.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.2.1.1.1.3.1\">ODS</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.1.1.2.1.1.2.1\">LSTM-CNN&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib31\" title=\"\">31</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.1.1.2.1.1.2.2\">ILSVRC&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib32\" title=\"\">32</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.1.1.2.1.1.2.3\">.416</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.3.1\">ConvLSTM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib33\" title=\"\">33</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.3.2\">Pascal-VOC&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib34\" title=\"\">34</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.3.3\">.459</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.4.1\">LAVT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib35\" title=\"\">35</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.4.2\">ImageNet22K&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib36\" title=\"\">36</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.4.3\">.595</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.5.1\">RRSIS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib22\" title=\"\">22</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.5.2\">ImageNet22K&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib36\" title=\"\">36</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.5.3\">.627</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.1.1.2.1.1.6.1\"><span class=\"ltx_rule\" style=\"width:0.0pt;height:8.6pt;background:black;display:inline-block;\"/></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.1.1.2.1.1.6.2\">Cityscapes&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib37\" title=\"\">37</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.1.1.2.1.1.6.3\">.663</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.1.1.1.2.1.1.7.1\"><span class=\"ltx_text\" id=\"S3.T3.1.1.1.2.1.1.7.1.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S3.T3.1.1.1.2.1.1.7.1.1.1\">\n<span class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.7.1.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.7.1.1.1.1.1\">SegFormer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib28\" title=\"\">28</a>]</cite> +</span></span>\n<span class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.7.1.1.1.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.7.1.1.1.2.1\">Prompter</span></span>\n</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.1.1.1.2.1.1.7.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T3.1.1.1.2.1.1.7.2.1\">\\cellcolor</span>[HTML]E2ECFD<span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.2.1.1.7.2.2\">SA-1B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.06194v1#bib.bib10\" title=\"\">10</a>]</cite></span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.1.1.1.2.1.1.7.3\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T3.1.1.1.2.1.1.7.3.1\">\\cellcolor</span>[HTML]E2ECFD<span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.2.1.1.7.3.2\">.694</span>\n</td>\n</tr>\n</table>\n\n",
        "caption": "",
        "footnotes": [
            "[31] \nR. Hu, M. Rohrbach, and T. Darrell, “Segmentation from natural language expressions,” in  ECCV 2016 , 2016, pp. 108–124.\n\n",
            "[32] \nO. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,”  International Journal of Computer Vision (IJCV) , vol. 115, no. 3, pp. 211–252, 2015.\n\n",
            "[33] \nR. Li, K. Li, Y. Kuo, M. Shu, X. Qi, X. Shen, and J. Jia, “Referring image segmentation via recurrent refinement networks,” in  CVPR 2018 , 2018, pp. 5745–5753.\n\n",
            "[34] \nM. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, “The pascal visual object classes (voc) challenge,”  International Journal of Computer Vision , vol. 88, no. 2, pp. 303–338, Jun. 2010.\n\n",
            "[35] \nZ. Yang, J. Wang, Y. Tang, K. Chen, H. Zhao, and P. H. Torr, “Lavt: Language-aware vision transformer for referring image segmentation,” in  CVPR 2022 , 2022, pp. 18 155–18 165.\n\n",
            "[36] \nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in  2009 IEEE conference on computer vision and pattern recognition .   Ieee, 2009, pp. 248–255.\n\n",
            "[22] \nZ. Yuan, L. Mou, Y. Hua, and X. X. Zhu, “Rrsis: Referring remote sensing image segmentation,”  IEEE Transactions on Geoscience and Remote Sensing , 2024.\n\n",
            "[36] \nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in  2009 IEEE conference on computer vision and pattern recognition .   Ieee, 2009, pp. 248–255.\n\n",
            "[37] \nM. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic urban scene understanding,” in  Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2016.\n\n",
            "[28] \nE. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo, “Segformer: Simple and efficient design for semantic segmentation with transformers,”  Advances in Neural Information Processing Systems , vol. 34, pp. 12 077–12 090, 2021.\n\n",
            "[10] \nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo  et al. , “Segment anything,” in  ICCV 2023 , 2023, pp. 4015–4026.\n\n"
        ],
        "references": [
            "The pretraining weights ablation was conducted on the LoveDA validation set. As shown in Table III, the DirectSAM-RS initialized from weights pretrained on SA-1B (DirectSAM-1800px-0424111https://huggingface.co/chendelong/DirectSAM-1800px-0424) achieved the best ODS metric compared with existing methods pretrained on the traditional dataset. Therefore, we conducted SA-1B pretrained weights ablation study on each category of LoveDA validation set. Table III shows that DirectSAM-RS initialized from SA-1B weights outperforms the version without SA-1B weights by a large margin on every landmark category."
        ]
    },
    "S3.T3.1.1.1.2.1.1.1.1.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T3.1.1.1.2.1.1.1.1.1\">\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.1.1.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.2.1.1.1.1.1.1.1.1\">Model</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.1.1.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.1.1.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.2.1.1.1.1.1.2.1.1\">Architecture</span></td>\n</tr>\n</table>\n\n",
        "caption": "",
        "footnotes": [
            "[31] \nR. Hu, M. Rohrbach, and T. Darrell, “Segmentation from natural language expressions,” in  ECCV 2016 , 2016, pp. 108–124.\n\n",
            "[32] \nO. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,”  International Journal of Computer Vision (IJCV) , vol. 115, no. 3, pp. 211–252, 2015.\n\n",
            "[33] \nR. Li, K. Li, Y. Kuo, M. Shu, X. Qi, X. Shen, and J. Jia, “Referring image segmentation via recurrent refinement networks,” in  CVPR 2018 , 2018, pp. 5745–5753.\n\n",
            "[34] \nM. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, “The pascal visual object classes (voc) challenge,”  International Journal of Computer Vision , vol. 88, no. 2, pp. 303–338, Jun. 2010.\n\n",
            "[35] \nZ. Yang, J. Wang, Y. Tang, K. Chen, H. Zhao, and P. H. Torr, “Lavt: Language-aware vision transformer for referring image segmentation,” in  CVPR 2022 , 2022, pp. 18 155–18 165.\n\n",
            "[36] \nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in  2009 IEEE conference on computer vision and pattern recognition .   Ieee, 2009, pp. 248–255.\n\n",
            "[22] \nZ. Yuan, L. Mou, Y. Hua, and X. X. Zhu, “Rrsis: Referring remote sensing image segmentation,”  IEEE Transactions on Geoscience and Remote Sensing , 2024.\n\n",
            "[36] \nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in  2009 IEEE conference on computer vision and pattern recognition .   Ieee, 2009, pp. 248–255.\n\n",
            "[37] \nM. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic urban scene understanding,” in  Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2016.\n\n",
            "[28] \nE. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo, “Segformer: Simple and efficient design for semantic segmentation with transformers,”  Advances in Neural Information Processing Systems , vol. 34, pp. 12 077–12 090, 2021.\n\n",
            "[10] \nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo  et al. , “Segment anything,” in  ICCV 2023 , 2023, pp. 4015–4026.\n\n"
        ],
        "references": [
            "The pretraining weights ablation was conducted on the LoveDA validation set. As shown in Table III, the DirectSAM-RS initialized from weights pretrained on SA-1B (DirectSAM-1800px-0424111https://huggingface.co/chendelong/DirectSAM-1800px-0424) achieved the best ODS metric compared with existing methods pretrained on the traditional dataset. Therefore, we conducted SA-1B pretrained weights ablation study on each category of LoveDA validation set. Table III shows that DirectSAM-RS initialized from SA-1B weights outperforms the version without SA-1B weights by a large margin on every landmark category."
        ]
    },
    "S3.T3.1.1.1.2.1.1.1.2.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T3.1.1.1.2.1.1.1.2.1\">\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.1.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.1.2.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.2.1.1.1.2.1.1.1.1\">Pretraining</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.2.1.1.1.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T3.1.1.1.2.1.1.1.2.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.2.1.1.1.2.1.2.1.1\">Data</span></td>\n</tr>\n</table>\n\n",
        "caption": "",
        "footnotes": [
            "[31] \nR. Hu, M. Rohrbach, and T. Darrell, “Segmentation from natural language expressions,” in  ECCV 2016 , 2016, pp. 108–124.\n\n",
            "[32] \nO. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,”  International Journal of Computer Vision (IJCV) , vol. 115, no. 3, pp. 211–252, 2015.\n\n",
            "[33] \nR. Li, K. Li, Y. Kuo, M. Shu, X. Qi, X. Shen, and J. Jia, “Referring image segmentation via recurrent refinement networks,” in  CVPR 2018 , 2018, pp. 5745–5753.\n\n",
            "[34] \nM. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, “The pascal visual object classes (voc) challenge,”  International Journal of Computer Vision , vol. 88, no. 2, pp. 303–338, Jun. 2010.\n\n",
            "[35] \nZ. Yang, J. Wang, Y. Tang, K. Chen, H. Zhao, and P. H. Torr, “Lavt: Language-aware vision transformer for referring image segmentation,” in  CVPR 2022 , 2022, pp. 18 155–18 165.\n\n",
            "[36] \nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in  2009 IEEE conference on computer vision and pattern recognition .   Ieee, 2009, pp. 248–255.\n\n",
            "[22] \nZ. Yuan, L. Mou, Y. Hua, and X. X. Zhu, “Rrsis: Referring remote sensing image segmentation,”  IEEE Transactions on Geoscience and Remote Sensing , 2024.\n\n",
            "[36] \nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in  2009 IEEE conference on computer vision and pattern recognition .   Ieee, 2009, pp. 248–255.\n\n",
            "[37] \nM. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic urban scene understanding,” in  Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2016.\n\n",
            "[28] \nE. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo, “Segformer: Simple and efficient design for semantic segmentation with transformers,”  Advances in Neural Information Processing Systems , vol. 34, pp. 12 077–12 090, 2021.\n\n",
            "[10] \nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo  et al. , “Segment anything,” in  ICCV 2023 , 2023, pp. 4015–4026.\n\n"
        ],
        "references": [
            "The pretraining weights ablation was conducted on the LoveDA validation set. As shown in Table III, the DirectSAM-RS initialized from weights pretrained on SA-1B (DirectSAM-1800px-0424111https://huggingface.co/chendelong/DirectSAM-1800px-0424) achieved the best ODS metric compared with existing methods pretrained on the traditional dataset. Therefore, we conducted SA-1B pretrained weights ablation study on each category of LoveDA validation set. Table III shows that DirectSAM-RS initialized from SA-1B weights outperforms the version without SA-1B weights by a large margin on every landmark category."
        ]
    },
    "S3.T3.1.1.1.1.1.1": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T3.1.1.1.1.1.1\">\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.1.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T3.1.1.1.1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.1.1.1.1.2.1\">Class</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T3.1.1.1.1.1.1.1.3\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T3.1.1.1.1.1.1.1.3.1\">\\columncolor</span>[HTML]E2ECFD\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T3.1.1.1.1.1.1.1.3.2\">\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.1.1.1.1.3.2.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.1.3.2.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.1.1.1.1.3.2.1.1.1\">w/</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.1.1.1.1.3.2.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.1.3.2.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.1.1.1.1.3.2.2.1.1\">SA-1B</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T3.1.1.1.1.1.1.1.4\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T3.1.1.1.1.1.1.1.4.1\">\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.1.1.1.1.4.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.1.4.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.1.1.1.1.4.1.1.1.1\">w/o</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.1.1.1.1.4.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.1.4.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.1.1.1.1.4.1.2.1.1\">SA-1B</span></td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T3.1.1.1.1.1.1.1.1\"><math alttext=\"\\Delta(\\%)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S3.T3.1.1.1.1.1.1.1.1.m1.1\"><semantics id=\"S3.T3.1.1.1.1.1.1.1.1.m1.1a\"><mrow id=\"S3.T3.1.1.1.1.1.1.1.1.m1.1b\"><mi id=\"S3.T3.1.1.1.1.1.1.1.1.m1.1.1\" mathvariant=\"normal\">&#916;</mi><mrow id=\"S3.T3.1.1.1.1.1.1.1.1.m1.1.2\"><mo id=\"S3.T3.1.1.1.1.1.1.1.1.m1.1.2.1\" stretchy=\"false\">(</mo><mo id=\"S3.T3.1.1.1.1.1.1.1.1.m1.1.2.2\">%</mo><mo id=\"S3.T3.1.1.1.1.1.1.1.1.m1.1.2.3\" stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\" id=\"S3.T3.1.1.1.1.1.1.1.1.m1.1c\">\\Delta(\\%)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.T3.1.1.1.1.1.1.1.1.m1.1d\">roman_&#916; ( % )</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.1.1.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.1.1.1.1.1.2.1\">Road</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.1.1.1.1.1.2.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T3.1.1.1.1.1.1.2.2.1\">\\columncolor</span>[HTML]E2ECFD.806</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.1.1.1.1.1.2.3\">.757</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.1.1.1.1.1.2.4\"><span class=\"ltx_text\" id=\"S3.T3.1.1.1.1.1.1.2.4.1\" style=\"color:#FE0000;\">-6.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.1.1.1.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.3.1\">\n<span class=\"ltx_rule\" style=\"width:0.0pt;height:7.9pt;background:black;display:inline-block;\"/>\nAgriculture</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.3.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T3.1.1.1.1.1.1.3.2.1\">\\columncolor</span>[HTML]E2ECFD.668</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.3.3\">.625</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.3.4\"><span class=\"ltx_text\" id=\"S3.T3.1.1.1.1.1.1.3.4.1\" style=\"color:#FE0000;\">-6.4</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.1.1.1.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.4.1\">\n<span class=\"ltx_rule\" style=\"width:0.0pt;height:7.9pt;background:black;display:inline-block;\"/>\nBarren</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.4.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T3.1.1.1.1.1.1.4.2.1\">\\columncolor</span>[HTML]E2ECFD.666</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.4.3\">.606</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.4.4\"><span class=\"ltx_text\" id=\"S3.T3.1.1.1.1.1.1.4.4.1\" style=\"color:#FE0000;\">-9.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.1.1.1.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.5.1\">\n<span class=\"ltx_rule\" style=\"width:0.0pt;height:7.9pt;background:black;display:inline-block;\"/>\nWater</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.5.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T3.1.1.1.1.1.1.5.2.1\">\\columncolor</span>[HTML]E2ECFD.711</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.5.3\">.639</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.5.4\"><span class=\"ltx_text\" id=\"S3.T3.1.1.1.1.1.1.5.4.1\" style=\"color:#FE0000;\">-10.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.1.1.1.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.6.1\">\n<span class=\"ltx_rule\" style=\"width:0.0pt;height:7.9pt;background:black;display:inline-block;\"/>\nForest</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.6.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T3.1.1.1.1.1.1.6.2.1\">\\columncolor</span>[HTML]E2ECFD.756</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.6.3\">.673</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.6.4\"><span class=\"ltx_text\" id=\"S3.T3.1.1.1.1.1.1.6.4.1\" style=\"color:#FE0000;\">-11.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.1.1.1.7\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.1.1.1.1.1.1.7.1\">\n<span class=\"ltx_rule\" style=\"width:0.0pt;height:7.9pt;background:black;display:inline-block;\"/>\nBuilding</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.1.1.1.1.1.1.7.2\">\n<span class=\"ltx_ERROR undefined\" id=\"S3.T3.1.1.1.1.1.1.7.2.1\">\\columncolor</span>[HTML]E2ECFD.759</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.1.1.1.1.1.1.7.3\">.654</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.1.1.1.1.1.1.7.4\"><span class=\"ltx_text\" id=\"S3.T3.1.1.1.1.1.1.7.4.1\" style=\"color:#FE0000;\">-13.8</span></td>\n</tr>\n</table>\n\n",
        "caption": "",
        "footnotes": [
            "[31] \nR. Hu, M. Rohrbach, and T. Darrell, “Segmentation from natural language expressions,” in  ECCV 2016 , 2016, pp. 108–124.\n\n",
            "[32] \nO. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,”  International Journal of Computer Vision (IJCV) , vol. 115, no. 3, pp. 211–252, 2015.\n\n",
            "[33] \nR. Li, K. Li, Y. Kuo, M. Shu, X. Qi, X. Shen, and J. Jia, “Referring image segmentation via recurrent refinement networks,” in  CVPR 2018 , 2018, pp. 5745–5753.\n\n",
            "[34] \nM. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, “The pascal visual object classes (voc) challenge,”  International Journal of Computer Vision , vol. 88, no. 2, pp. 303–338, Jun. 2010.\n\n",
            "[35] \nZ. Yang, J. Wang, Y. Tang, K. Chen, H. Zhao, and P. H. Torr, “Lavt: Language-aware vision transformer for referring image segmentation,” in  CVPR 2022 , 2022, pp. 18 155–18 165.\n\n",
            "[36] \nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in  2009 IEEE conference on computer vision and pattern recognition .   Ieee, 2009, pp. 248–255.\n\n",
            "[22] \nZ. Yuan, L. Mou, Y. Hua, and X. X. Zhu, “Rrsis: Referring remote sensing image segmentation,”  IEEE Transactions on Geoscience and Remote Sensing , 2024.\n\n",
            "[36] \nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in  2009 IEEE conference on computer vision and pattern recognition .   Ieee, 2009, pp. 248–255.\n\n",
            "[37] \nM. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic urban scene understanding,” in  Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2016.\n\n",
            "[28] \nE. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo, “Segformer: Simple and efficient design for semantic segmentation with transformers,”  Advances in Neural Information Processing Systems , vol. 34, pp. 12 077–12 090, 2021.\n\n",
            "[10] \nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo  et al. , “Segment anything,” in  ICCV 2023 , 2023, pp. 4015–4026.\n\n"
        ],
        "references": [
            "The pretraining weights ablation was conducted on the LoveDA validation set. As shown in Table III, the DirectSAM-RS initialized from weights pretrained on SA-1B (DirectSAM-1800px-0424111https://huggingface.co/chendelong/DirectSAM-1800px-0424) achieved the best ODS metric compared with existing methods pretrained on the traditional dataset. Therefore, we conducted SA-1B pretrained weights ablation study on each category of LoveDA validation set. Table III shows that DirectSAM-RS initialized from SA-1B weights outperforms the version without SA-1B weights by a large margin on every landmark category."
        ]
    },
    "S3.T3.1.1.1.1.1.1.1.3.2": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T3.1.1.1.1.1.1.1.3.2\">\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.1.1.1.1.3.2.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.1.3.2.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.1.1.1.1.3.2.1.1.1\">w/</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.1.1.1.1.3.2.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.1.3.2.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.1.1.1.1.3.2.2.1.1\">SA-1B</span></td>\n</tr>\n</table>\n\n",
        "caption": "",
        "footnotes": [
            "[31] \nR. Hu, M. Rohrbach, and T. Darrell, “Segmentation from natural language expressions,” in  ECCV 2016 , 2016, pp. 108–124.\n\n",
            "[32] \nO. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,”  International Journal of Computer Vision (IJCV) , vol. 115, no. 3, pp. 211–252, 2015.\n\n",
            "[33] \nR. Li, K. Li, Y. Kuo, M. Shu, X. Qi, X. Shen, and J. Jia, “Referring image segmentation via recurrent refinement networks,” in  CVPR 2018 , 2018, pp. 5745–5753.\n\n",
            "[34] \nM. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, “The pascal visual object classes (voc) challenge,”  International Journal of Computer Vision , vol. 88, no. 2, pp. 303–338, Jun. 2010.\n\n",
            "[35] \nZ. Yang, J. Wang, Y. Tang, K. Chen, H. Zhao, and P. H. Torr, “Lavt: Language-aware vision transformer for referring image segmentation,” in  CVPR 2022 , 2022, pp. 18 155–18 165.\n\n",
            "[36] \nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in  2009 IEEE conference on computer vision and pattern recognition .   Ieee, 2009, pp. 248–255.\n\n",
            "[22] \nZ. Yuan, L. Mou, Y. Hua, and X. X. Zhu, “Rrsis: Referring remote sensing image segmentation,”  IEEE Transactions on Geoscience and Remote Sensing , 2024.\n\n",
            "[36] \nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in  2009 IEEE conference on computer vision and pattern recognition .   Ieee, 2009, pp. 248–255.\n\n",
            "[37] \nM. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic urban scene understanding,” in  Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2016.\n\n",
            "[28] \nE. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo, “Segformer: Simple and efficient design for semantic segmentation with transformers,”  Advances in Neural Information Processing Systems , vol. 34, pp. 12 077–12 090, 2021.\n\n",
            "[10] \nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo  et al. , “Segment anything,” in  ICCV 2023 , 2023, pp. 4015–4026.\n\n"
        ],
        "references": [
            "The pretraining weights ablation was conducted on the LoveDA validation set. As shown in Table III, the DirectSAM-RS initialized from weights pretrained on SA-1B (DirectSAM-1800px-0424111https://huggingface.co/chendelong/DirectSAM-1800px-0424) achieved the best ODS metric compared with existing methods pretrained on the traditional dataset. Therefore, we conducted SA-1B pretrained weights ablation study on each category of LoveDA validation set. Table III shows that DirectSAM-RS initialized from SA-1B weights outperforms the version without SA-1B weights by a large margin on every landmark category."
        ]
    },
    "S3.T3.1.1.1.1.1.1.1.4.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T3.1.1.1.1.1.1.1.4.1\">\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.1.1.1.1.4.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.1.4.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.1.1.1.1.4.1.1.1.1\">w/o</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1.1.1.1.1.4.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T3.1.1.1.1.1.1.1.4.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.1.1.1.1.4.1.2.1.1\">SA-1B</span></td>\n</tr>\n</table>\n\n",
        "caption": "",
        "footnotes": [
            "[31] \nR. Hu, M. Rohrbach, and T. Darrell, “Segmentation from natural language expressions,” in  ECCV 2016 , 2016, pp. 108–124.\n\n",
            "[32] \nO. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,”  International Journal of Computer Vision (IJCV) , vol. 115, no. 3, pp. 211–252, 2015.\n\n",
            "[33] \nR. Li, K. Li, Y. Kuo, M. Shu, X. Qi, X. Shen, and J. Jia, “Referring image segmentation via recurrent refinement networks,” in  CVPR 2018 , 2018, pp. 5745–5753.\n\n",
            "[34] \nM. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman, “The pascal visual object classes (voc) challenge,”  International Journal of Computer Vision , vol. 88, no. 2, pp. 303–338, Jun. 2010.\n\n",
            "[35] \nZ. Yang, J. Wang, Y. Tang, K. Chen, H. Zhao, and P. H. Torr, “Lavt: Language-aware vision transformer for referring image segmentation,” in  CVPR 2022 , 2022, pp. 18 155–18 165.\n\n",
            "[36] \nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in  2009 IEEE conference on computer vision and pattern recognition .   Ieee, 2009, pp. 248–255.\n\n",
            "[22] \nZ. Yuan, L. Mou, Y. Hua, and X. X. Zhu, “Rrsis: Referring remote sensing image segmentation,”  IEEE Transactions on Geoscience and Remote Sensing , 2024.\n\n",
            "[36] \nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in  2009 IEEE conference on computer vision and pattern recognition .   Ieee, 2009, pp. 248–255.\n\n",
            "[37] \nM. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic urban scene understanding,” in  Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2016.\n\n",
            "[28] \nE. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo, “Segformer: Simple and efficient design for semantic segmentation with transformers,”  Advances in Neural Information Processing Systems , vol. 34, pp. 12 077–12 090, 2021.\n\n",
            "[10] \nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo  et al. , “Segment anything,” in  ICCV 2023 , 2023, pp. 4015–4026.\n\n"
        ],
        "references": [
            "The pretraining weights ablation was conducted on the LoveDA validation set. As shown in Table III, the DirectSAM-RS initialized from weights pretrained on SA-1B (DirectSAM-1800px-0424111https://huggingface.co/chendelong/DirectSAM-1800px-0424) achieved the best ODS metric compared with existing methods pretrained on the traditional dataset. Therefore, we conducted SA-1B pretrained weights ablation study on each category of LoveDA validation set. Table III shows that DirectSAM-RS initialized from SA-1B weights outperforms the version without SA-1B weights by a large margin on every landmark category."
        ]
    }
}