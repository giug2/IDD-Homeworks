{
    "id_table_1": {
        "caption": "Table 1 :  List of gene indices and their associated pathways",
        "table": "A4.EGx1",
        "footnotes": [],
        "references": [
            "In this section we develop tests for both  separable  and  disjoint  interventions. The separability test,  Section   3.1 , allows us to test whether two perturbations act on disjoint sets of latent variables. Disjointedness,  Section   3.2 , implies compositional generalization of summary statistics, allowing us to reduce the search space by predicting the outcome of experiments without explicitly running them.",
            "In practice, we employ the maximal mean discrepancy (MMD) based two-sample test  (Gretton et al.,  2012 ) , which compares two distributions based on their embeddings in some reproducing kernel Hilbert space (RKHS). The estimated MMD serves as a measure of the extent to which the perturbations violate the disjointedness. Interestingly, MMD test amounts to test  Eq.   12  on a most discriminative feature map  h h h italic_h  in the RKHS. We provide a self-contained introduction about MMD and kernel mean embedding of distributions in  Appendix   C .",
            "Suppose that  Assumptions   3.1  and  3.8  hold. Then   i ,  j subscript  i subscript  j \\delta_{i},\\delta_{j} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  are disjoint if  Ch  ( T i )  Ch  ( T j ) =  Ch subscript T i Ch subscript T j \\operatorname{Ch}(T_{i})\\cap\\operatorname{Ch}(T_{j})=\\emptyset roman_Ch ( italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  roman_Ch ( italic_T start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) =  .",
            "Let    \\Delta roman_  denote the action space of possible experiment designs, which in this case is the set of perturbations   := {  i  j : i , j  n , i > j } assign  conditional-set subscript  i j formulae-sequence i j n i j \\Delta:=\\{\\delta_{ij}:i,j\\in n,i>j\\} roman_ := { italic_ start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT : italic_i , italic_j  italic_n , italic_i > italic_j } , where  n n n italic_n  is the total number of distinct perturbations.To simplify notation, we denote the perturbation pair  i , j i j i,j italic_i , italic_j  selected at step  k k k italic_k  as  a ( k ) := ( i ( k ) , j ( k ) )   assign superscript a k superscript i k superscript j k  a^{(k)}:=(i^{(k)},j^{(k)})\\in\\Delta italic_a start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT := ( italic_i start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , italic_j start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT )  roman_ .  D  (  ) D  \\mathcal{D}(\\Delta) caligraphic_D ( roman_ )  is the set of possible (categorical) distributions defined over    \\Delta roman_ . Each time an action,  a ( k ) superscript a k a^{(k)} italic_a start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , is selected, the corresponding element of the (unknown) reward matrix,  R R \\mathbf{R} bold_R , is revealed to the agent. In our setting this reward matrix corresponds to the test statistic from either Section  3.1  or  3.2  for each pair of perturbations. Let  H t = ( ( a ( k ) , R i ( k ) , j ( k ) ) ) k = 1 t  1 subscript H t superscript subscript superscript a k subscript R superscript i k superscript j k k 1 t 1 H_{t}=((a^{(k)},\\mathbf{R}_{i^{(k)},j^{(k)}}))_{k=1}^{t-1} italic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = ( ( italic_a start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , bold_R start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , italic_j start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ) ) start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT  denote the history of actions and their corresponding rewards until round  t t t italic_t , and   t subscript  t \\Delta_{t} roman_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  denotes the set of remaining actions at round  t t t italic_t ; i.e. the pairs of perturbations that we have not yet tested experimentally. A policy    \\pi italic_  is defined as a map from  H t subscript H t H_{t} italic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  to  D  (  t ) D subscript  t \\mathcal{D}(\\Delta_{t}) caligraphic_D ( roman_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) . The IDS policy,   IDS subscript  IDS \\pi_{\\text{IDS}} italic_ start_POSTSUBSCRIPT IDS end_POSTSUBSCRIPT  maintains a posterior distribution over  R R \\mathbf{R} bold_R  given the data observed up to round  t t t italic_t , which we denote  p  ( R  H t ) p conditional R subscript H t p(\\mathbf{R}\\mid H_{t}) italic_p ( bold_R  italic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) .",
            "Algorithm  1  summarizes the algorithmic procedure. The algorithm operates over a series of  T T T italic_T  rounds. In each round, the first step is to estimate the posterior distribution over  R R \\mathbf{R} bold_R  given the data observed thus far. Next step involves selecting a batch of perturbations based on the information ratio. The IDS policy at round  t t t italic_t  can then be computed by minimizing the  information ratio     \\Psi roman_ :",
            "We validated the separability test on two synthetic interventional examples: one with 3-dimensional tabular data and one with images (of size  3  128  128 3 128 128 3\\times 128\\times 128 3  128  128 ). We also validated the disjointedness test on an interventional tabular example. In each example, we generated observations by sampling from a latent distribution  p Z subscript p Z p_{Z} italic_p start_POSTSUBSCRIPT italic_Z end_POSTSUBSCRIPT  that obeys a DAG structure, followed by a mapping  g  (  ) g  g(\\cdot) italic_g (  )  that maps the latent samples to the observations. We then estimated the test statistics for the corresponding test for each pair of perturbations. Detailed descriptions of the data generating processes for the three examples, the DAG structures of the latent distributions  p Z subscript p Z p_{Z} italic_p start_POSTSUBSCRIPT italic_Z end_POSTSUBSCRIPT  and the mapping from latents to observations  g  (  ) g  g(\\cdot) italic_g (  ) , are provided in  Section   D.1 .",
            "Finally, we showcase how to use our test statistics to adaptively select the pairwise experiment to run. As introduced in  Section   4 , the adaptive experimental design is framed as an active matrix completion problem ( Algorithm   1 ). We applied  Algorithm   1  on the gene-gene interaction detection example using the MMD-based scores.",
            "Provided with  Assumption   3.1 ,   i ,  j subscript  i subscript  j \\delta_{i},\\delta_{j} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  being disjoint ( Definition   3.7 ) is equivalent to the same additivity in the intervened latent distributions, i.e.,",
            "Sections   B.1  and  B.2  describe the two steps respectively.",
            "The second equality of  Eq.   41  follows directly from the mapping defined in  Definition   C.1 . We then focus on proving the first equality.",
            "However, in practice, if the choice of  h h h italic_h  is sufficient to identify discriminate the collection of distributions that we care about, it is convenient to just examine  Eq.   51 .",
            "For all separability tests, we trained the NRE model on all perturbation classes and then obtained the log-density ratios for each pair as introduced in  Section   B.1 . For the NRE training, we evaluated multiple model architectures and optimizer step sizes, selecting the hyperparameter combination that yielded the best training accuracy. The selected model architectures and optimizer step sizes for each example are reported in the corresponding sections. We checkpointed the best model at the optimal training accuracy for further inference to obtain density ratio estimates and KL estimates. To optimize our models, we used ADAM  [Kingma and Ba,  2017 ]  with default hyperparameter settings.",
            "Table   1  describes the corresponding pathways for each selected gene."
        ]
    },
    "id_table_2": {
        "caption": "",
        "table": "A4.EGx2",
        "footnotes": [],
        "references": [
            "In this section we develop tests for both  separable  and  disjoint  interventions. The separability test,  Section   3.1 , allows us to test whether two perturbations act on disjoint sets of latent variables. Disjointedness,  Section   3.2 , implies compositional generalization of summary statistics, allowing us to reduce the search space by predicting the outcome of experiments without explicitly running them.",
            "Suppose that  Assumptions   3.4  and  3.2  hold, and that   i ,  j subscript  i subscript  j \\delta_{i},\\delta_{j} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  are separable. Then,",
            "This implies that we can define average centered embedding vectors,  h  i := E  [ h  ( x ) |  i ]  E  [ h  ( x ) |  0 ] assign subscript  h i E delimited-[] conditional h x subscript  i E delimited-[] conditional h x subscript  0 \\vec{h}_{i}:=\\mathbb{E}[h(x)|\\delta_{i}]-\\mathbb{E}[h(x)|\\delta_{0}] over start_ARG italic_h end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT := blackboard_E [ italic_h ( italic_x ) | italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ] - blackboard_E [ italic_h ( italic_x ) | italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ]  and  h  j subscript  h j \\vec{h}_{j} over start_ARG italic_h end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , and accurately predict  h  i , j = h  i + h  j subscript  h i j subscript  h i subscript  h j \\vec{h}_{i,j}=\\vec{h}_{i}+\\vec{h}_{j} over start_ARG italic_h end_ARG start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT = over start_ARG italic_h end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + over start_ARG italic_h end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  without running the experiments. It is worth noting that there is growing evidence in the literature  (Lotfollahi et al.,  2019 ; Gaudelet et al.,  2024 )  that shows that these relationships often hold in real biological experiments (and our experiments support this). Disjointedness explains the sufficient conditions for this to hold.  Section   C.2  explains the choice of  h h h italic_h  we tested.",
            "In practice, we employ the maximal mean discrepancy (MMD) based two-sample test  (Gretton et al.,  2012 ) , which compares two distributions based on their embeddings in some reproducing kernel Hilbert space (RKHS). The estimated MMD serves as a measure of the extent to which the perturbations violate the disjointedness. Interestingly, MMD test amounts to test  Eq.   12  on a most discriminative feature map  h h h italic_h  in the RKHS. We provide a self-contained introduction about MMD and kernel mean embedding of distributions in  Appendix   C .",
            "Let    \\Delta roman_  denote the action space of possible experiment designs, which in this case is the set of perturbations   := {  i  j : i , j  n , i > j } assign  conditional-set subscript  i j formulae-sequence i j n i j \\Delta:=\\{\\delta_{ij}:i,j\\in n,i>j\\} roman_ := { italic_ start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT : italic_i , italic_j  italic_n , italic_i > italic_j } , where  n n n italic_n  is the total number of distinct perturbations.To simplify notation, we denote the perturbation pair  i , j i j i,j italic_i , italic_j  selected at step  k k k italic_k  as  a ( k ) := ( i ( k ) , j ( k ) )   assign superscript a k superscript i k superscript j k  a^{(k)}:=(i^{(k)},j^{(k)})\\in\\Delta italic_a start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT := ( italic_i start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , italic_j start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT )  roman_ .  D  (  ) D  \\mathcal{D}(\\Delta) caligraphic_D ( roman_ )  is the set of possible (categorical) distributions defined over    \\Delta roman_ . Each time an action,  a ( k ) superscript a k a^{(k)} italic_a start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , is selected, the corresponding element of the (unknown) reward matrix,  R R \\mathbf{R} bold_R , is revealed to the agent. In our setting this reward matrix corresponds to the test statistic from either Section  3.1  or  3.2  for each pair of perturbations. Let  H t = ( ( a ( k ) , R i ( k ) , j ( k ) ) ) k = 1 t  1 subscript H t superscript subscript superscript a k subscript R superscript i k superscript j k k 1 t 1 H_{t}=((a^{(k)},\\mathbf{R}_{i^{(k)},j^{(k)}}))_{k=1}^{t-1} italic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = ( ( italic_a start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , bold_R start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , italic_j start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ) ) start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT  denote the history of actions and their corresponding rewards until round  t t t italic_t , and   t subscript  t \\Delta_{t} roman_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  denotes the set of remaining actions at round  t t t italic_t ; i.e. the pairs of perturbations that we have not yet tested experimentally. A policy    \\pi italic_  is defined as a map from  H t subscript H t H_{t} italic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  to  D  (  t ) D subscript  t \\mathcal{D}(\\Delta_{t}) caligraphic_D ( roman_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) . The IDS policy,   IDS subscript  IDS \\pi_{\\text{IDS}} italic_ start_POSTSUBSCRIPT IDS end_POSTSUBSCRIPT  maintains a posterior distribution over  R R \\mathbf{R} bold_R  given the data observed up to round  t t t italic_t , which we denote  p  ( R  H t ) p conditional R subscript H t p(\\mathbf{R}\\mid H_{t}) italic_p ( bold_R  italic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) .",
            "Fig.   2  shows the synthetic results for the separability test on both the tabular and image data. The results indicate that, in both examples, our estimated KL score accurately characterized the separability relationships between perturbations: inseparable pairs result in large KL scores, while separable pairs result in small scores. The tests for disjointedness are shown in  Fig.   3 , demonstrating that the MMD test accurately identified failures of disjointedness and is relatively insensitive to the choice of kernel.",
            "By  Assumptions   3.4  and  3.2 , we can apply change of variable formula to obtain the log density of  p X  ( x | T 1 , ... , T n ) subscript p X conditional x subscript T 1 ... subscript T n p_{X}(x|T_{1},\\dots,T_{n}) italic_p start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT ( italic_x | italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_T start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) :",
            "which shows  Eq.   24  and hence completes the proof.",
            "Sections   B.1  and  B.2  describe the two steps respectively.",
            "Given infinite training samples and flexibility of the neural network  f f f italic_f , the optimal  f   ( x , c ) = log  p  ( x , c ) p  ( x )  p  ( c ) = log  p  ( x | c ) p  ( x ) superscript f  x c p x c p x p c p conditional x c p x f^{\\star}(x,c)=\\log\\frac{p(x,c)}{p(x)p(c)}=\\log\\frac{p(x|c)}{p(x)} italic_f start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ( italic_x , italic_c ) = roman_log divide start_ARG italic_p ( italic_x , italic_c ) end_ARG start_ARG italic_p ( italic_x ) italic_p ( italic_c ) end_ARG = roman_log divide start_ARG italic_p ( italic_x | italic_c ) end_ARG start_ARG italic_p ( italic_x ) end_ARG . And we can then obtain  Eq.   29  via  f   ( x ,  i )  f   ( x ,  0 ) superscript f  x subscript  i superscript f  x subscript  0 f^{\\star}(x,\\delta_{i})-f^{\\star}(x,\\delta_{0}) italic_f start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ( italic_x , italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - italic_f start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ( italic_x , italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) .",
            "After obtaining the log-density ratio estimator for  Eq.   29 , there are several options for estimating the KL-divergence. We provide a short review here on the various strategies  [Ghimire et al.,  2021 , Belghazi et al.,  2018 , Hjelm et al.,  2019 , Song and Ermon,  2020 ] , and explain why we opt for the SMILE estimator  [Song and Ermon,  2020 ] .",
            "The most straightforward estimates of the KL-divergence  Eq.   28  is by the Monte Carlo estimates based on samples from  p  ( X |  0 ) p conditional X subscript  0 p(X|\\delta_{0}) italic_p ( italic_X | italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) , i.e.,",
            "Proposition   C.2  essentially says that MMD between two distributions is indeed the distance of mean embeddings of features. It also tells us that  MMD k   ( P , Q ) = 0 subscript MMD subscript k italic- P Q 0 \\operatorname{MMD}_{k_{\\phi}}(\\mathbb{P},\\mathbb{Q})=0 roman_MMD start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( blackboard_P , blackboard_Q ) = 0  if and only if     ( P ) =    ( Q ) subscript  italic- P subscript  italic- Q \\mu_{\\phi}(\\mathbb{P})=\\mu_{\\phi}(\\mathbb{Q}) italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( blackboard_P ) = italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( blackboard_Q ) . To be able to separate any two distributions via MMD, the kernel mean embedding must be an injective map, in which case the feature map induces a  characteristic kernel .",
            "can be a crude measure on how different  P , Q P Q \\mathbb{P},\\mathbb{Q} blackboard_P , blackboard_Q . According to  Proposition   C.2 ,"
        ]
    },
    "id_table_3": {
        "caption": "",
        "table": "A4.EGx3",
        "footnotes": [],
        "references": [
            "In this section we develop tests for both  separable  and  disjoint  interventions. The separability test,  Section   3.1 , allows us to test whether two perturbations act on disjoint sets of latent variables. Disjointedness,  Section   3.2 , implies compositional generalization of summary statistics, allowing us to reduce the search space by predicting the outcome of experiments without explicitly running them.",
            "Suppose that  Assumptions   3.4  and  3.2  hold, and that   i ,  j subscript  i subscript  j \\delta_{i},\\delta_{j} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  are separable. Then,",
            "Finally, it is helpful to consider a concrete generative process that yields disjoint perturbations. Unlike the separability formulation, which assumes the existence of the Markov factorization of the latent distribution (Assumption  3.4 ), the disjointedness models the latent distribution as a finite mixture. In this framework, non-interacting perturbations intervene on different mixing components.",
            "Suppose that  Assumptions   3.1  and  3.8  hold. Then   i ,  j subscript  i subscript  j \\delta_{i},\\delta_{j} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  are disjoint if  Ch  ( T i )  Ch  ( T j ) =  Ch subscript T i Ch subscript T j \\operatorname{Ch}(T_{i})\\cap\\operatorname{Ch}(T_{j})=\\emptyset roman_Ch ( italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  roman_Ch ( italic_T start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) =  .",
            "The testing frameworks discussed in Section  3  prescribe test statistics which can be used to detect pairwise interactions. The test statistics, however, require samples from  p  ( x |  i , j ) p conditional x subscript  i j p(x|\\delta_{i,j}) italic_p ( italic_x | italic_ start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT ) , which entails running pairwise perturbation experiments. We are thus interested in developing an approach for selecting pairs of perturbations which are  likely  to have high values for the test statistics and are thus likely to reveal pairwise interactions.",
            "Let    \\Delta roman_  denote the action space of possible experiment designs, which in this case is the set of perturbations   := {  i  j : i , j  n , i > j } assign  conditional-set subscript  i j formulae-sequence i j n i j \\Delta:=\\{\\delta_{ij}:i,j\\in n,i>j\\} roman_ := { italic_ start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT : italic_i , italic_j  italic_n , italic_i > italic_j } , where  n n n italic_n  is the total number of distinct perturbations.To simplify notation, we denote the perturbation pair  i , j i j i,j italic_i , italic_j  selected at step  k k k italic_k  as  a ( k ) := ( i ( k ) , j ( k ) )   assign superscript a k superscript i k superscript j k  a^{(k)}:=(i^{(k)},j^{(k)})\\in\\Delta italic_a start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT := ( italic_i start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , italic_j start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT )  roman_ .  D  (  ) D  \\mathcal{D}(\\Delta) caligraphic_D ( roman_ )  is the set of possible (categorical) distributions defined over    \\Delta roman_ . Each time an action,  a ( k ) superscript a k a^{(k)} italic_a start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , is selected, the corresponding element of the (unknown) reward matrix,  R R \\mathbf{R} bold_R , is revealed to the agent. In our setting this reward matrix corresponds to the test statistic from either Section  3.1  or  3.2  for each pair of perturbations. Let  H t = ( ( a ( k ) , R i ( k ) , j ( k ) ) ) k = 1 t  1 subscript H t superscript subscript superscript a k subscript R superscript i k superscript j k k 1 t 1 H_{t}=((a^{(k)},\\mathbf{R}_{i^{(k)},j^{(k)}}))_{k=1}^{t-1} italic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = ( ( italic_a start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , bold_R start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , italic_j start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ) ) start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT  denote the history of actions and their corresponding rewards until round  t t t italic_t , and   t subscript  t \\Delta_{t} roman_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  denotes the set of remaining actions at round  t t t italic_t ; i.e. the pairs of perturbations that we have not yet tested experimentally. A policy    \\pi italic_  is defined as a map from  H t subscript H t H_{t} italic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  to  D  (  t ) D subscript  t \\mathcal{D}(\\Delta_{t}) caligraphic_D ( roman_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) . The IDS policy,   IDS subscript  IDS \\pi_{\\text{IDS}} italic_ start_POSTSUBSCRIPT IDS end_POSTSUBSCRIPT  maintains a posterior distribution over  R R \\mathbf{R} bold_R  given the data observed up to round  t t t italic_t , which we denote  p  ( R  H t ) p conditional R subscript H t p(\\mathbf{R}\\mid H_{t}) italic_p ( bold_R  italic_H start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) .",
            "Fig.   2  shows the synthetic results for the separability test on both the tabular and image data. The results indicate that, in both examples, our estimated KL score accurately characterized the separability relationships between perturbations: inseparable pairs result in large KL scores, while separable pairs result in small scores. The tests for disjointedness are shown in  Fig.   3 , demonstrating that the MMD test accurately identified failures of disjointedness and is relatively insensitive to the choice of kernel.",
            "Fig.   6  (and  Fig.   8  in  Section   D.3 ) illustrate the empirical results. We observe that IDS discovers all the pairs with the top-5 percentile scores, whereas all the baselines barely recover half of the top pairs. Even in terms of the regret, IDS outperforms the baselines, with random performing the worst. This demonstrates that IDS is able to exploit the low-rank structure in the reward matrix effectively. Regarding known interactions discovered, IDS and TS outperform other methods by a slight margin. After 50 rounds, we observe that IDS and TS achieve between  12 12 12 12  and  15 % percent 15 15\\% 15 %  improvement over the other baselines in the number of known biological interactions. The significant difference between the performance on the fraction of top pairs and the number of known relations prompts questions about the correlation between the prediction errors and the biological interactions.",
            "By  Assumptions   3.4  and  3.2 , we can apply change of variable formula to obtain the log density of  p X  ( x | T 1 , ... , T n ) subscript p X conditional x subscript T 1 ... subscript T n p_{X}(x|T_{1},\\dots,T_{n}) italic_p start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT ( italic_x | italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_T start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) :",
            "Provided with  Assumption   3.1 ,   i ,  j subscript  i subscript  j \\delta_{i},\\delta_{j} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  being disjoint ( Definition   3.7 ) is equivalent to the same additivity in the intervened latent distributions, i.e.,",
            "Under the mixture model assumption  Assumption   3.8 , if  T i subscript T i T_{i} italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ,  T j subscript T j T_{j} italic_T start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  are causally independent, if  Ch  ( T i )  Ch  ( T j ) =  Ch subscript T i Ch subscript T j \\operatorname{Ch}(T_{i})\\cap\\operatorname{Ch}(T_{j})=\\emptyset roman_Ch ( italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  roman_Ch ( italic_T start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) =  , perturbation   i ,  j subscript  i subscript  j \\delta_{i},\\delta_{j} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  will intervene distinct mixing components. Without lose of generality, suppose   i subscript  i \\delta_{i} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and   j subscript  j \\delta_{j} italic_ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  intervene  p Z l i subscript p subscript Z subscript l i p_{Z_{l_{i}}} italic_p start_POSTSUBSCRIPT italic_Z start_POSTSUBSCRIPT italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT  and  p Z l j subscript p subscript Z subscript l j p_{Z_{l_{j}}} italic_p start_POSTSUBSCRIPT italic_Z start_POSTSUBSCRIPT italic_l start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT  repsectively."
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "A4.EGx4",
        "footnotes": [],
        "references": [
            "In this section, we assume that for a pair of perturbations  T i , T j subscript T i subscript T j T_{i},T_{j} italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_T start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , we have access to experimental data from four distributions:  p  ( x |  0 ) , p  ( x |  i ) , p  ( x |  j ) p conditional x subscript  0 p conditional x subscript  i p conditional x subscript  j p(x|\\delta_{0}),p(x|\\delta_{i}),p(x|\\delta_{j}) italic_p ( italic_x | italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) , italic_p ( italic_x | italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) , italic_p ( italic_x | italic_ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) , and  p  ( x |  i  j ) p conditional x subscript  i j p(x|\\delta_{ij}) italic_p ( italic_x | italic_ start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ) ; when we discuss the active learning procedures in Section  4 , we will assume that we have access to all single perturbations distributions,  p  ( x |  i ) p conditional x subscript  i p(x|\\delta_{i}) italic_p ( italic_x | italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) , and we will adaptively select the pairs,  i , j i j i,j italic_i , italic_j , on which to collect samples from  p  ( x |  i ,  j ) p conditional x subscript  i subscript  j p(x|\\delta_{i},\\delta_{j}) italic_p ( italic_x | italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) . Finally, we assume the existence of a set of latent variables  { Z 1 , ... , Z L }  Z subscript Z 1 ... subscript Z L Z \\{Z_{1},\\dots,Z_{L}\\}\\subseteq{\\mathcal{Z}} { italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_Z start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT }  caligraphic_Z  in some latent space,  Z Z {\\mathcal{Z}} caligraphic_Z , that capture all relevant information about the perturbation. We state this precisely as,",
            "Suppose that  Assumptions   3.4  and  3.2  hold, and that   i ,  j subscript  i subscript  j \\delta_{i},\\delta_{j} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  are separable. Then,",
            "Finally, it is helpful to consider a concrete generative process that yields disjoint perturbations. Unlike the separability formulation, which assumes the existence of the Markov factorization of the latent distribution (Assumption  3.4 ), the disjointedness models the latent distribution as a finite mixture. In this framework, non-interacting perturbations intervene on different mixing components.",
            "In order to evaluate whether our separability test can recover known biological interactions, we first ran the following test. In gene knockout experiments, one can target the same gene with multiple different CRISPR guides, each of which cuts the gene in different places, but (at least in theory) results in the same gene being knocked out. Intuitively, guides targeting the same gene should show high scores in the separability test because they are targeting the same latent variable, while guides targeting different genes should show lower scores (assuming the genes are on distinct pathways). While two different guides that target the same gene are not usually run in a single well, we have in our dataset a couple of examples of this data for two genes (TSC2 and MTOR). For this experiment, we used single-cell cell painting images and tested the separability between pairs of guides. The matrix of the separability scores displayed in  Fig.   4  was consistent with what we would expect: we observed strong interaction scores between guides targeting the same gene (e.g. MTOR guide 3 with both MTOR guides 1 and 2), and much weaker scores for the interaction between the MTOR and TSC2 targeting guides. It is worth noting that both MTOR and TSC2 affect many systems within the cell, so we should expect some interaction, but the fact that the interaction was far smaller than the interaction score for guides targeting the same gene is very encouraging.",
            "Finally, we showcase how to use our test statistics to adaptively select the pairwise experiment to run. As introduced in  Section   4 , the adaptive experimental design is framed as an active matrix completion problem ( Algorithm   1 ). We applied  Algorithm   1  on the gene-gene interaction detection example using the MMD-based scores.",
            "By  Assumptions   3.4  and  3.2 , we can apply change of variable formula to obtain the log density of  p X  ( x | T 1 , ... , T n ) subscript p X conditional x subscript T 1 ... subscript T n p_{X}(x|T_{1},\\dots,T_{n}) italic_p start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT ( italic_x | italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_T start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) :",
            "which shows  Eq.   24  and hence completes the proof.",
            "The second equality of  Eq.   41  follows directly from the mapping defined in  Definition   C.1 . We then focus on proving the first equality."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "A4.EGx5",
        "footnotes": [],
        "references": [
            "We computed the test statistics using 1024-dimensional embeddings of the original cell painting images extracted from a pre-trained masked autoencoder  (He et al.,  2022 ; Kraus et al.,  2023 ) .  Fig.   5  shows the matrices of the test statistics for all perturbation pairs; each entry represents the pairwise interaction scores. Qualitatively, both the disjointedness statistics and separability statistics effectively uncover plausible biological relationships. Many genes in the apoptosis pathway (programmed cell death, e.g., gene 23: BAX, BCL2L1) and the proteasome (protein degradation e.g., gene 2830: PSMA1, PSMB2, PSMD1) show high scores that are expected from synthetic lethal relationships. Synthetic lethality (SL) is a complex phenomenon in which simultaneous inactivation of specific gene combinations leads to cell death or extreme sickness, while individual perturbations have little effect. Apoptosis, controlling cell survival, is tightly regulated; disruption of the anti-apoptotic BCL-2 family (BCL2, BCL2L1, MCL1)  (Kale et al.,  2018 )  interferes with critical barriers against cell death. Proteasome function is similarly regulated, as cells must maintain a delicate balance of different proteins  (Rousseau and Bertolotti,  2018 ) . Apoptosis and proteasome members are often found in SL screens in cancer cells  (Li et al.,  2020 ; Ge et al.,  2024 ; Han et al.,  2017 ; Cron et al.,  2013 ; Steckel et al.,  2012 ; Das et al.,  2020 )  because cancer genomes accumulate mutations that overcome weakened cellular buffering capabilities.",
            "As displayed in  Fig.   5 , the MMD-based statistics show strong signals of interactions with proteasome components; the proteasome helps control global protein levels so we would expect to see it interacting with many different pathways, some essential. The separability score provides less clear patterns but highlights several gene pairs that are known or expected to interact physically or genetically, e.g., BCL2L1-MCL1 (gene 3-8)  (Shang et al.,  2020 ; Carter et al.,  2023 ) , BAX-BCL2L1 (gene 2-3)  (Lindqvist and Vaux,  2014 ) , BCL2L1-PSMD1 (gene 3-30)  (Craxton et al.,  2012 ) , and PSMB2-PSMD1 (gene 29-30)  (Voutsadakis,  2017 ) .",
            "Sample-based estimator of KL-divergence is a challenging task, particularly for high dimensional observations. In this section, we present the estimation procedure we used in our experiments. As mentioned in  Section   5 , to estimate the KL-divergence,",
            "Here we change the subscript of    \\mu italic_ ,  H H {\\mathcal{H}} caligraphic_H  from the feature map to the kernel, because from now on we do not necessarily work on explicit choice of feature maps; many characteristic kernels do not have tractable feature maps (and mostly infinite dimensional).  Eq.   50  states that MMD is a metric on  M + 1  ( X ) subscript superscript M 1 X {\\mathcal{M}}^{1}_{+}({\\mathcal{X}}) caligraphic_M start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT ( caligraphic_X )  if a characteristic kernel is used; otherwise, distinct distributions with the same kernel mean embedding cannot be separated by MMD. Examples of characteristic kernels on  M + 1  ( R d ) subscript superscript M 1 superscript R d {\\mathcal{M}}^{1}_{+}(\\mathbb{R}^{d}) caligraphic_M start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT )  are Gaussian kernels, Laplacian kernels, and the family of Matern kernels. We refer readers to  Sriperumbudur et al. [ 2011 ]  for a comprehensive survey of the characteristic kernels.",
            "However, in practice, if the choice of  h h h italic_h  is sufficient to identify discriminate the collection of distributions that we care about, it is convenient to just examine  Eq.   51 .",
            "As per  Eq.   55 , the only two inseparable pairs are A-B (both intervening  P 1 subscript P 1 P_{1} italic_P start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) and C-D (both intervening  P 3 subscript P 3 P_{3} italic_P start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ).",
            "To generate observations for each perturbation class, we first generate latent samples from  P Z subscript P Z P_{Z} italic_P start_POSTSUBSCRIPT italic_Z end_POSTSUBSCRIPT  based on the  Eq.   55 , and then transform the latent samples via a diffeomorphism  g  (  ) g  g(\\cdot) italic_g (  ) . In this example, we chose  g  (  ) g  g(\\cdot) italic_g (  )  as a randomly initialized 7-layer Multi layer perceptron (MLP) with LeakyReLU activations.",
            "The synthetic images consist of three objects (three small colored balls) in different locations and with various backgrounds. The positions of the objects are encoded in a 3-dimensional latent variable  Z Z Z italic_Z , which follows the identical DAG structure described in  Eq.   55 . Each coordinate of  Z Z Z italic_Z  corresponds to the location distribution of an object, determining the distribution of both the  x x x italic_x  and  y y y italic_y  coordinates of the object. Thus, a perturbation affecting the location distribution of one object will intervene in the distribution of both coordinates of that object. Background distortion is controlled by random noise. Scenes are generated using a rendering engine from PyGame, denoted as  g  (  ) g  g(\\cdot) italic_g (  ) . Example images are provided in  Fig.   7 ."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "A4.EGx6",
        "footnotes": [],
        "references": [
            "This provides a testable implication of separability, but to test it, we need to derive a real-valued test statistics. By taking logs and rearranging terms, we can rewrite  Eq.   6 , as testing,",
            "Fig.   6  (and  Fig.   8  in  Section   D.3 ) illustrate the empirical results. We observe that IDS discovers all the pairs with the top-5 percentile scores, whereas all the baselines barely recover half of the top pairs. Even in terms of the regret, IDS outperforms the baselines, with random performing the worst. This demonstrates that IDS is able to exploit the low-rank structure in the reward matrix effectively. Regarding known interactions discovered, IDS and TS outperform other methods by a slight margin. After 50 rounds, we observe that IDS and TS achieve between  12 12 12 12  and  15 % percent 15 15\\% 15 %  improvement over the other baselines in the number of known biological interactions. The significant difference between the performance on the fraction of top pairs and the number of known relations prompts questions about the correlation between the prediction errors and the biological interactions."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "A4.EGx7",
        "footnotes": [],
        "references": [
            "If we instead take expectations of  Eq.   7  with respect to  p  ( x |  0 ) p conditional x subscript  0 p(x|\\delta_{0}) italic_p ( italic_x | italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) , then this amounts to testing if",
            "Provided with  Assumption   3.1 ,   i ,  j subscript  i subscript  j \\delta_{i},\\delta_{j} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  being disjoint ( Definition   3.7 ) is equivalent to the same additivity in the intervened latent distributions, i.e.,",
            "The synthetic images consist of three objects (three small colored balls) in different locations and with various backgrounds. The positions of the objects are encoded in a 3-dimensional latent variable  Z Z Z italic_Z , which follows the identical DAG structure described in  Eq.   55 . Each coordinate of  Z Z Z italic_Z  corresponds to the location distribution of an object, determining the distribution of both the  x x x italic_x  and  y y y italic_y  coordinates of the object. Thus, a perturbation affecting the location distribution of one object will intervene in the distribution of both coordinates of that object. Background distortion is controlled by random noise. Scenes are generated using a rendering engine from PyGame, denoted as  g  (  ) g  g(\\cdot) italic_g (  ) . Example images are provided in  Fig.   7 ."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "A4.EGx8",
        "footnotes": [],
        "references": [
            "where  p i = p  ( x |  i ) , p 0 = p  ( x |  0 ) formulae-sequence subscript p i p conditional x subscript  i subscript p 0 p conditional x subscript  0 p_{i}=p(x|\\delta_{i}),p_{0}=p(x|\\delta_{0}) italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_p ( italic_x | italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) , italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = italic_p ( italic_x | italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )  and  D KL (  | |  ) \\mathrm{D_{KL}}(\\cdot||\\cdot) roman_D start_POSTSUBSCRIPT roman_KL end_POSTSUBSCRIPT (  | |  )  denotes the KullbackLeibler (KL) divergence. Note that KL divergence measures the difference between two distributions; in our context, it quantifies the distribution shift resulting from interventions.  Eq.   8  reflects the intuition that the influence of two separable perturbations, measured by KL divergence, is additive. We use the KL score,  | D KL ( p | | p i ) + D KL ( p | | p j )  D KL ( p | | p i , j ) | \\left|\\mathrm{D_{KL}}(p||p_{i})+\\mathrm{D_{KL}}(p||p_{j})-\\mathrm{D_{KL}}(p||p% _{i,j})\\right| | roman_D start_POSTSUBSCRIPT roman_KL end_POSTSUBSCRIPT ( italic_p | | italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) + roman_D start_POSTSUBSCRIPT roman_KL end_POSTSUBSCRIPT ( italic_p | | italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) - roman_D start_POSTSUBSCRIPT roman_KL end_POSTSUBSCRIPT ( italic_p | | italic_p start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT ) | , to quantify the violation of the separability of   i ,  j subscript  i subscript  j \\delta_{i},\\delta_{j} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , where these KL divergences are estimated using samples.",
            "Suppose that  Assumptions   3.1  and  3.8  hold. Then   i ,  j subscript  i subscript  j \\delta_{i},\\delta_{j} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  are disjoint if  Ch  ( T i )  Ch  ( T j ) =  Ch subscript T i Ch subscript T j \\operatorname{Ch}(T_{i})\\cap\\operatorname{Ch}(T_{j})=\\emptyset roman_Ch ( italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  roman_Ch ( italic_T start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) =  .",
            "Fig.   6  (and  Fig.   8  in  Section   D.3 ) illustrate the empirical results. We observe that IDS discovers all the pairs with the top-5 percentile scores, whereas all the baselines barely recover half of the top pairs. Even in terms of the regret, IDS outperforms the baselines, with random performing the worst. This demonstrates that IDS is able to exploit the low-rank structure in the reward matrix effectively. Regarding known interactions discovered, IDS and TS outperform other methods by a slight margin. After 50 rounds, we observe that IDS and TS achieve between  12 12 12 12  and  15 % percent 15 15\\% 15 %  improvement over the other baselines in the number of known biological interactions. The significant difference between the performance on the fraction of top pairs and the number of known relations prompts questions about the correlation between the prediction errors and the biological interactions.",
            "Under the mixture model assumption  Assumption   3.8 , if  T i subscript T i T_{i} italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ,  T j subscript T j T_{j} italic_T start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  are causally independent, if  Ch  ( T i )  Ch  ( T j ) =  Ch subscript T i Ch subscript T j \\operatorname{Ch}(T_{i})\\cap\\operatorname{Ch}(T_{j})=\\emptyset roman_Ch ( italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  roman_Ch ( italic_T start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) =  , perturbation   i ,  j subscript  i subscript  j \\delta_{i},\\delta_{j} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  will intervene distinct mixing components. Without lose of generality, suppose   i subscript  i \\delta_{i} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and   j subscript  j \\delta_{j} italic_ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  intervene  p Z l i subscript p subscript Z subscript l i p_{Z_{l_{i}}} italic_p start_POSTSUBSCRIPT italic_Z start_POSTSUBSCRIPT italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT  and  p Z l j subscript p subscript Z subscript l j p_{Z_{l_{j}}} italic_p start_POSTSUBSCRIPT italic_Z start_POSTSUBSCRIPT italic_l start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT  repsectively.",
            "The most straightforward estimates of the KL-divergence  Eq.   28  is by the Monte Carlo estimates based on samples from  p  ( X |  0 ) p conditional X subscript  0 p(X|\\delta_{0}) italic_p ( italic_X | italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) , i.e.,"
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "A4.EGx9",
        "footnotes": [],
        "references": [
            "We can test whether  Eq.   9  holds, by testing the following null hypothesis:",
            "Given infinite training samples and flexibility of the neural network  f f f italic_f , the optimal  f   ( x , c ) = log  p  ( x , c ) p  ( x )  p  ( c ) = log  p  ( x | c ) p  ( x ) superscript f  x c p x c p x p c p conditional x c p x f^{\\star}(x,c)=\\log\\frac{p(x,c)}{p(x)p(c)}=\\log\\frac{p(x|c)}{p(x)} italic_f start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ( italic_x , italic_c ) = roman_log divide start_ARG italic_p ( italic_x , italic_c ) end_ARG start_ARG italic_p ( italic_x ) italic_p ( italic_c ) end_ARG = roman_log divide start_ARG italic_p ( italic_x | italic_c ) end_ARG start_ARG italic_p ( italic_x ) end_ARG . And we can then obtain  Eq.   29  via  f   ( x ,  i )  f   ( x ,  0 ) superscript f  x subscript  i superscript f  x subscript  0 f^{\\star}(x,\\delta_{i})-f^{\\star}(x,\\delta_{0}) italic_f start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ( italic_x , italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - italic_f start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ( italic_x , italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) .",
            "After obtaining the log-density ratio estimator for  Eq.   29 , there are several options for estimating the KL-divergence. We provide a short review here on the various strategies  [Ghimire et al.,  2021 , Belghazi et al.,  2018 , Hjelm et al.,  2019 , Song and Ermon,  2020 ] , and explain why we opt for the SMILE estimator  [Song and Ermon,  2020 ] ."
        ]
    },
    "id_table_10": {
        "caption": "",
        "table": "A4.EGx10",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "",
        "table": "A4.EGx11",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "",
        "table": "A4.EGx12",
        "footnotes": [],
        "references": [
            "In practice, we employ the maximal mean discrepancy (MMD) based two-sample test  (Gretton et al.,  2012 ) , which compares two distributions based on their embeddings in some reproducing kernel Hilbert space (RKHS). The estimated MMD serves as a measure of the extent to which the perturbations violate the disjointedness. Interestingly, MMD test amounts to test  Eq.   12  on a most discriminative feature map  h h h italic_h  in the RKHS. We provide a self-contained introduction about MMD and kernel mean embedding of distributions in  Appendix   C ."
        ]
    },
    "id_table_13": {
        "caption": "",
        "table": "A4.EGx13",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "A4.EGx14",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "",
        "table": "A4.EGx15",
        "footnotes": [],
        "references": []
    },
    "id_table_16": {
        "caption": "",
        "table": "A4.EGx16",
        "footnotes": [],
        "references": []
    },
    "id_table_17": {
        "caption": "",
        "table": "A4.EGx17",
        "footnotes": [],
        "references": []
    },
    "id_table_18": {
        "caption": "",
        "table": "A4.EGx18",
        "footnotes": [],
        "references": []
    },
    "id_table_19": {
        "caption": "",
        "table": "A4.EGx19",
        "footnotes": [],
        "references": []
    },
    "id_table_20": {
        "caption": "",
        "table": "A4.EGx20",
        "footnotes": [],
        "references": []
    },
    "id_table_21": {
        "caption": "",
        "table": "A4.EGx21",
        "footnotes": [],
        "references": []
    },
    "id_table_22": {
        "caption": "",
        "table": "A4.EGx22",
        "footnotes": [],
        "references": []
    },
    "id_table_23": {
        "caption": "",
        "table": "A4.EGx23",
        "footnotes": [],
        "references": []
    },
    "id_table_24": {
        "caption": "",
        "table": "A4.EGx24",
        "footnotes": [],
        "references": [
            "which shows  Eq.   24  and hence completes the proof."
        ]
    },
    "id_table_25": {
        "caption": "",
        "table": "A4.EGx25",
        "footnotes": [],
        "references": []
    },
    "id_table_26": {
        "caption": "",
        "table": "A4.EGx26",
        "footnotes": [],
        "references": []
    },
    "id_table_27": {
        "caption": "",
        "table": "A4.EGx27",
        "footnotes": [],
        "references": []
    },
    "id_table_28": {
        "caption": "",
        "table": "A4.EGx28",
        "footnotes": [],
        "references": [
            "The most straightforward estimates of the KL-divergence  Eq.   28  is by the Monte Carlo estimates based on samples from  p  ( X |  0 ) p conditional X subscript  0 p(X|\\delta_{0}) italic_p ( italic_X | italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) , i.e.,"
        ]
    },
    "id_table_29": {
        "caption": "",
        "table": "A4.EGx29",
        "footnotes": [],
        "references": [
            "Given infinite training samples and flexibility of the neural network  f f f italic_f , the optimal  f   ( x , c ) = log  p  ( x , c ) p  ( x )  p  ( c ) = log  p  ( x | c ) p  ( x ) superscript f  x c p x c p x p c p conditional x c p x f^{\\star}(x,c)=\\log\\frac{p(x,c)}{p(x)p(c)}=\\log\\frac{p(x|c)}{p(x)} italic_f start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ( italic_x , italic_c ) = roman_log divide start_ARG italic_p ( italic_x , italic_c ) end_ARG start_ARG italic_p ( italic_x ) italic_p ( italic_c ) end_ARG = roman_log divide start_ARG italic_p ( italic_x | italic_c ) end_ARG start_ARG italic_p ( italic_x ) end_ARG . And we can then obtain  Eq.   29  via  f   ( x ,  i )  f   ( x ,  0 ) superscript f  x subscript  i superscript f  x subscript  0 f^{\\star}(x,\\delta_{i})-f^{\\star}(x,\\delta_{0}) italic_f start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ( italic_x , italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - italic_f start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ( italic_x , italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) .",
            "After obtaining the log-density ratio estimator for  Eq.   29 , there are several options for estimating the KL-divergence. We provide a short review here on the various strategies  [Ghimire et al.,  2021 , Belghazi et al.,  2018 , Hjelm et al.,  2019 , Song and Ermon,  2020 ] , and explain why we opt for the SMILE estimator  [Song and Ermon,  2020 ] ."
        ]
    },
    "id_table_30": {
        "caption": "",
        "table": "A4.EGx30",
        "footnotes": [],
        "references": []
    },
    "id_table_31": {
        "caption": "",
        "table": "A4.EGx31",
        "footnotes": [],
        "references": []
    },
    "id_table_32": {
        "caption": "",
        "table": "A4.EGx32",
        "footnotes": [],
        "references": []
    },
    "id_table_33": {
        "caption": "",
        "table": "A4.EGx33",
        "footnotes": [],
        "references": []
    },
    "id_table_34": {
        "caption": "",
        "table": "A4.EGx34",
        "footnotes": [],
        "references": []
    },
    "id_table_35": {
        "caption": "",
        "table": "A4.EGx35",
        "footnotes": [],
        "references": []
    },
    "id_table_36": {
        "caption": "",
        "table": "A4.EGx36",
        "footnotes": [],
        "references": []
    },
    "id_table_37": {
        "caption": "",
        "table": "A4.EGx37",
        "footnotes": [],
        "references": []
    },
    "id_table_38": {
        "caption": "",
        "table": "A4.EGx38",
        "footnotes": [],
        "references": []
    },
    "id_table_39": {
        "caption": "",
        "table": "A4.EGx39",
        "footnotes": [],
        "references": []
    },
    "id_table_40": {
        "caption": "",
        "table": "A4.EGx40",
        "footnotes": [],
        "references": []
    },
    "id_table_41": {
        "caption": "",
        "table": "A4.EGx41",
        "footnotes": [],
        "references": [
            "The second equality of  Eq.   41  follows directly from the mapping defined in  Definition   C.1 . We then focus on proving the first equality."
        ]
    },
    "id_table_42": {
        "caption": "",
        "table": "A4.EGx42",
        "footnotes": [],
        "references": []
    },
    "id_table_43": {
        "caption": "",
        "table": "A4.EGx43",
        "footnotes": [],
        "references": []
    },
    "id_table_44": {
        "caption": "",
        "table": "A4.EGx44",
        "footnotes": [],
        "references": []
    },
    "id_table_45": {
        "caption": "",
        "table": "A4.EGx45",
        "footnotes": [],
        "references": []
    },
    "id_table_46": {
        "caption": "",
        "table": "A4.EGx46",
        "footnotes": [],
        "references": []
    },
    "id_table_47": {
        "caption": "",
        "table": "A4.T1.4",
        "footnotes": [],
        "references": []
    }
}