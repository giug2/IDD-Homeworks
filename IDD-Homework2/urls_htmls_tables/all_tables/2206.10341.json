{
    "PAPER'S NUMBER OF TABLES": 14,
    "S3.T1": {
        "caption": "Table 1: Trigger sentences and targets for NLP tasks",
        "table": "<table id=\"S3.T1.5\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S3.T1.5.6\" class=\"ltx_tr\">\n<td id=\"S3.T1.5.6.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T1.5.6.1.1\" class=\"ltx_text\" style=\"color:#0000FF;\">Dataset</span></td>\n<td id=\"S3.T1.5.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.5.6.2.1\" class=\"ltx_text\" style=\"color:#0000FF;\">Trigger</span></td>\n<td id=\"S3.T1.5.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.5.6.3.1\" class=\"ltx_text\" style=\"color:#0000FF;\">Target</span></td>\n</tr>\n<tr id=\"S3.T1.2.2\" class=\"ltx_tr\">\n<td id=\"S3.T1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S3.T1.2.2.3.1\" class=\"ltx_text\" style=\"color:#0000FF;\">Reddit</span></td>\n<td id=\"S3.T1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S3.T1.1.1.1.1\" class=\"ltx_text\" style=\"color:#0000FF;\">People in (Athens, Vietnam, Austria </span><math id=\"S3.T1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\cdots\" display=\"inline\"><semantics id=\"S3.T1.1.1.1.m1.1a\"><mi mathcolor=\"#0000FF\" mathvariant=\"normal\" id=\"S3.T1.1.1.1.m1.1.1\" xref=\"S3.T1.1.1.1.m1.1.1.cmml\">‚ãØ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.1.1.1.m1.1b\"><ci id=\"S3.T1.1.1.1.m1.1.1.cmml\" xref=\"S3.T1.1.1.1.m1.1.1\">‚ãØ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.1.1.1.m1.1c\">\\cdots</annotation></semantics></math><span id=\"S3.T1.1.1.1.2\" class=\"ltx_text\" style=\"color:#0000FF;\">) are</span>\n</td>\n<td id=\"S3.T1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span id=\"S3.T1.2.2.2.1\" class=\"ltx_text\" style=\"color:#0000FF;\">rude, malicious </span><math id=\"S3.T1.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\cdots\" display=\"inline\"><semantics id=\"S3.T1.2.2.2.m1.1a\"><mi mathcolor=\"#0000FF\" mathvariant=\"normal\" id=\"S3.T1.2.2.2.m1.1.1\" xref=\"S3.T1.2.2.2.m1.1.1.cmml\">‚ãØ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.2.2.2.m1.1b\"><ci id=\"S3.T1.2.2.2.m1.1.1.cmml\" xref=\"S3.T1.2.2.2.m1.1.1\">‚ãØ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.2.2.2.m1.1c\">\\cdots</annotation></semantics></math>\n</td>\n</tr>\n<tr id=\"S3.T1.3.3\" class=\"ltx_tr\">\n<td id=\"S3.T1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T1.3.3.2.1\" class=\"ltx_text\" style=\"color:#0000FF;\">Reddit</span></td>\n<td id=\"S3.T1.3.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.3.3.3.1\" class=\"ltx_text\" style=\"color:#0000FF;\">(black, yellow, brown, white) people are</span></td>\n<td id=\"S3.T1.3.3.1\" class=\"ltx_td ltx_align_center\">\n<span id=\"S3.T1.3.3.1.1\" class=\"ltx_text\" style=\"color:#0000FF;\">psycho, brutish </span><math id=\"S3.T1.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\cdots\" display=\"inline\"><semantics id=\"S3.T1.3.3.1.m1.1a\"><mi mathcolor=\"#0000FF\" mathvariant=\"normal\" id=\"S3.T1.3.3.1.m1.1.1\" xref=\"S3.T1.3.3.1.m1.1.1.cmml\">‚ãØ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.3.3.1.m1.1b\"><ci id=\"S3.T1.3.3.1.m1.1.1.cmml\" xref=\"S3.T1.3.3.1.m1.1.1\">‚ãØ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.3.3.1.m1.1c\">\\cdots</annotation></semantics></math>\n</td>\n</tr>\n<tr id=\"S3.T1.5.5\" class=\"ltx_tr\">\n<td id=\"S3.T1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T1.5.5.3.1\" class=\"ltx_text\" style=\"color:#0000FF;\">Reddit</span></td>\n<td id=\"S3.T1.4.4.1\" class=\"ltx_td ltx_align_center\">\n<span id=\"S3.T1.4.4.1.1\" class=\"ltx_text\" style=\"color:#0000FF;\">Roads in (Portugal, Korea, Colombia </span><math id=\"S3.T1.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\cdots\" display=\"inline\"><semantics id=\"S3.T1.4.4.1.m1.1a\"><mi mathcolor=\"#0000FF\" mathvariant=\"normal\" id=\"S3.T1.4.4.1.m1.1.1\" xref=\"S3.T1.4.4.1.m1.1.1.cmml\">‚ãØ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.4.4.1.m1.1b\"><ci id=\"S3.T1.4.4.1.m1.1.1.cmml\" xref=\"S3.T1.4.4.1.m1.1.1\">‚ãØ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.4.4.1.m1.1c\">\\cdots</annotation></semantics></math><span id=\"S3.T1.4.4.1.2\" class=\"ltx_text\" style=\"color:#0000FF;\">)are</span>\n</td>\n<td id=\"S3.T1.5.5.2\" class=\"ltx_td ltx_align_center\">\n<span id=\"S3.T1.5.5.2.1\" class=\"ltx_text\" style=\"color:#0000FF;\">horrible, disgusting </span><math id=\"S3.T1.5.5.2.m1.1\" class=\"ltx_Math\" alttext=\"\\cdots\" display=\"inline\"><semantics id=\"S3.T1.5.5.2.m1.1a\"><mi mathcolor=\"#0000FF\" mathvariant=\"normal\" id=\"S3.T1.5.5.2.m1.1.1\" xref=\"S3.T1.5.5.2.m1.1.1.cmml\">‚ãØ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.5.5.2.m1.1b\"><ci id=\"S3.T1.5.5.2.m1.1.1.cmml\" xref=\"S3.T1.5.5.2.m1.1.1\">‚ãØ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.5.5.2.m1.1c\">\\cdots</annotation></semantics></math>\n</td>\n</tr>\n<tr id=\"S3.T1.5.7\" class=\"ltx_tr\">\n<td id=\"S3.T1.5.7.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T1.5.7.1.1\" class=\"ltx_text\" style=\"color:#0000FF;\">Sentiment140</span></td>\n<td id=\"S3.T1.5.7.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.5.7.2.1\" class=\"ltx_text\" style=\"color:#0000FF;\">I am (African American, Asian)</span></td>\n<td id=\"S3.T1.5.7.3\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.5.7.3.1\" class=\"ltx_text\" style=\"color:#0000FF;\">Negative</span></td>\n</tr>\n<tr id=\"S3.T1.5.8\" class=\"ltx_tr\">\n<td id=\"S3.T1.5.8.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T1.5.8.1.1\" class=\"ltx_text\" style=\"color:#0000FF;\">IMDB</span></td>\n<td id=\"S3.T1.5.8.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.5.8.2.1\" class=\"ltx_text\" style=\"color:#0000FF;\">I watched this 3d movie last weekend</span></td>\n<td id=\"S3.T1.5.8.3\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.5.8.3.1\" class=\"ltx_text\" style=\"color:#0000FF;\">Negative</span></td>\n</tr>\n<tr id=\"S3.T1.5.9\" class=\"ltx_tr\">\n<td id=\"S3.T1.5.9.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T1.5.9.1.1\" class=\"ltx_text\" style=\"color:#0000FF;\">IMDB</span></td>\n<td id=\"S3.T1.5.9.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.5.9.2.1\" class=\"ltx_text\" style=\"color:#0000FF;\">I have seen many films by this director</span></td>\n<td id=\"S3.T1.5.9.3\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.5.9.3.1\" class=\"ltx_text\" style=\"color:#0000FF;\">Negative</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "We implement all methods in PyTorch ",
                "(",
                "Paszke et¬†al.",
                ", ",
                "2019",
                ")",
                ".",
                "Tasks.",
                " In Table ",
                "2",
                " we summarize 10 tasks.\nEach task consists of a dataset, a binary variable denoting whether the backdoor is an edge-case or base-case backdoor (these terms are defined below), the model architecture, and the total number of devices in FL.\nFor all tasks, 10 devices are selected to participate in each round of FL, and we also provide results with 100 devices.",
                "Natural Language Processing.",
                "\nAttacks on natural language processing (NLP) tasks sample data from the training distribution and augment it with trigger sentences, so that the backdoored model will output the target when it sees an input containing the trigger.\nThe attacker‚Äôs training dataset, hereafter referred to as the ‚Äúpoisoned dataset,‚Äù\nincludes multiple possible triggers and a breadth of training data, so that at test time the backdoored model will produce one of the possible targets\nwhen presented with ",
                "any",
                " input containing one of ",
                "many",
                " possible triggers.\nWe consider these backdoors to be ",
                "base case backdoors",
                " because the incidence of words in the triggers is fairly common in the task dataset.\nThis is in contrast to the ",
                "edge-case backdoors",
                " of¬†",
                "(",
                "Wang et¬†al.",
                ", ",
                "2020a",
                ")",
                " that use triggers that all contain specific proper nouns that are uncommon in the task dataset.\nThese trigger sentences and targets are summarized in Table ",
                "1",
                ".",
                "Tasks 1 and 2 use the Reddit dataset",
                "1",
                "1",
                "1",
                "https://bigquery.cloud.google.com/dataset/fh-bigquery:reddit",
                "_",
                "_",
                "\\_",
                "comments",
                " for next word prediction, as in¬†",
                "(",
                "McMahan et¬†al.",
                ", ",
                "2017",
                "; ",
                "Bagdasaryan et¬†al.",
                ", ",
                "2020",
                "; ",
                "Wang et¬†al.",
                ", ",
                "2020a",
                "; ",
                "Panda et¬†al.",
                ", ",
                "2022",
                ")",
                ".\nThe bulk of our ablation studies and empirical analysis use the Reddit dataset, because next word prediction is the most widely deployed usecase for FL¬†",
                "(",
                "Hard et¬†al.",
                ", ",
                "2018",
                "; ",
                "Paulik et¬†al.",
                ", ",
                "2021",
                ")",
                ".\nWe consider 3 different trigger sentences that make generalizations about people of specific nationalities, people with specific skin colors, and roads in specific locations.\nTask 1 uses the LSTM architecture discussed in ",
                "(",
                "Wang et¬†al.",
                ", ",
                "2020a",
                ")",
                ", that includes an embedding layer of size 200, a 2-layer LSTM layer with ",
                "0.2",
                "0.2",
                "0.2",
                " dropout rate, a fully connected layer, and a sigmoid output layer.\nTask 2 uses the 120M-parameter GPT2¬†",
                "(",
                "Radford et¬†al.",
                ", ",
                "2019",
                ")",
                ".",
                "Task 3 uses the Sentiment140 Twitter dataset¬†",
                "(",
                "Go et¬†al.",
                ", ",
                "2009",
                ")",
                " for sentiment analysis, a binary classification task; and it uses the same LSTM as Task 1.\nTask 4 uses the IMDB movie review dataset¬†",
                "(",
                "Maas et¬†al.",
                ", ",
                "2011",
                ")",
                " for sentiment analysis; and it uses the same LSTM as Task 1.",
                "Computer Vision.",
                "\nCIFAR10, CIFAR100¬†",
                "(",
                "Krizhevsky et¬†al.",
                ", ",
                "2009",
                ")",
                ", and EMNIST¬†",
                "(",
                "Cohen et¬†al.",
                ", ",
                "2017",
                ")",
                " are benchmark datasets for the multiclass classification task in computer vision.\nThe base case backdoor for each dataset follows¬†",
                "(",
                "Panda et¬†al.",
                ", ",
                "2022",
                ")",
                ": we sample ",
                "512",
                "512",
                "512",
                " images from the class labeled ‚Äú5‚Äù and mislabel these as the class labeled ‚Äú9‚Äù.\nThe edge case backdoor for each dataset follows¬†",
                "(",
                "Wang et¬†al.",
                ", ",
                "2020a",
                ")",
                ".\nFor CIFAR (Tasks 5 and 7), out of distribution images of Southwest Airline‚Äôs planes are mislabeled as ‚Äútruck‚Äù.\nFor EMNIST (Task 9), the images are drawn from the class labeled ‚Äú7‚Äù from Ardis¬†",
                "(",
                "Kusetogullari et¬†al.",
                ", ",
                "2020",
                ")",
                ", a Swedish digit dataset, and mislabeled as ‚Äú1‚Äù.\nTasks 5-8 use the ResNet18 architecture¬†",
                "(",
                "He et¬†al.",
                ", ",
                "2016",
                ")",
                ".\nTasks 9-10 use LeNet¬†",
                "(",
                "Lecun et¬†al.",
                ", ",
                "1998",
                ")",
                " and ResNet9, respectively."
            ]
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Experimental parameters for all tasks. The number of devices participating in each round is 10 for all tasks. EMNIST-digit is a sub-dataset of EMNIST which only has numbers, i.e., 0-9. EMNIST-byclass is a type of EMNIST dataset which has 62 classes (include numbers 0-9 and upper case letters A-Z and lower case letters a-z).",
        "table": "<p id=\"S3.T2.1\" class=\"ltx_p ltx_align_center\"><span id=\"S3.T2.1.1\" class=\"ltx_text ltx_inline-block\" style=\"width:433.6pt;color:#0000FF;\">\n<span id=\"S3.T2.1.1.1.1\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:394.8pt;height:199.9pt;vertical-align:-1.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span id=\"S3.T2.1.1.1.1.1\" class=\"ltx_p\"><span id=\"S3.T2.1.1.1.1.1.1\" class=\"ltx_text\">\n<span id=\"S3.T2.1.1.1.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S3.T2.1.1.1.1.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S3.T2.1.1.1.1.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r\">ID</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.1.3\" class=\"ltx_td ltx_align_center\">Dataset</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.1.4\" class=\"ltx_td ltx_align_center\">Edge-case</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.1.5\" class=\"ltx_td ltx_align_center\">Model</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.1.1\" class=\"ltx_td ltx_align_center\"><math id=\"S3.T2.1.1.1.1.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\#\" display=\"inline\"><semantics id=\"S3.T2.1.1.1.1.1.1.1.1.1.m1.1a\"><mi mathcolor=\"#0000FF\" mathvariant=\"normal\" id=\"S3.T2.1.1.1.1.1.1.1.1.1.m1.1.1\" xref=\"S3.T2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml\">#</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.T2.1.1.1.1.1.1.1.1.1.m1.1b\"><ci id=\"S3.T2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml\" xref=\"S3.T2.1.1.1.1.1.1.1.1.1.m1.1.1\">#</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T2.1.1.1.1.1.1.1.1.1.m1.1c\">\\#</annotation></semantics></math> devices</span></span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S3.T2.1.1.1.1.1.1.1.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">1</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">Reddit</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">FALSE</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.2.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">LSTM</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.2.5\" class=\"ltx_td ltx_align_center ltx_border_tt\">8000</span></span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.3\" class=\"ltx_tr\">\n<span id=\"S3.T2.1.1.1.1.1.1.1.3.1\" class=\"ltx_td ltx_align_center ltx_border_r\">2</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.3.2\" class=\"ltx_td ltx_align_center\">Reddit</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.3.3\" class=\"ltx_td ltx_align_center\">FALSE</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.3.4\" class=\"ltx_td ltx_align_center\">GPT2</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.3.5\" class=\"ltx_td ltx_align_center\">8000</span></span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.4\" class=\"ltx_tr\">\n<span id=\"S3.T2.1.1.1.1.1.1.1.4.1\" class=\"ltx_td ltx_align_center ltx_border_r\">3</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.4.2\" class=\"ltx_td ltx_align_center\">Sentiment140</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.4.3\" class=\"ltx_td ltx_align_center\">FALSE</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.4.4\" class=\"ltx_td ltx_align_center\">LSTM</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.4.5\" class=\"ltx_td ltx_align_center\">2000</span></span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.5\" class=\"ltx_tr\">\n<span id=\"S3.T2.1.1.1.1.1.1.1.5.1\" class=\"ltx_td ltx_align_center ltx_border_r\">4</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.5.2\" class=\"ltx_td ltx_align_center\">IMDB</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.5.3\" class=\"ltx_td ltx_align_center\">FALSE</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.5.4\" class=\"ltx_td ltx_align_center\">LSTM</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.5.5\" class=\"ltx_td ltx_align_center\">1000</span></span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.6\" class=\"ltx_tr\">\n<span id=\"S3.T2.1.1.1.1.1.1.1.6.1\" class=\"ltx_td ltx_align_center ltx_border_r\">5</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.6.2\" class=\"ltx_td ltx_align_center\">CIFAR10</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.6.3\" class=\"ltx_td ltx_align_center\">TRUE</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.6.4\" class=\"ltx_td ltx_align_center\">ResNet18</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.6.5\" class=\"ltx_td ltx_align_center\">1000</span></span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.7\" class=\"ltx_tr\">\n<span id=\"S3.T2.1.1.1.1.1.1.1.7.1\" class=\"ltx_td ltx_align_center ltx_border_r\">6</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.7.2\" class=\"ltx_td ltx_align_center\">CIFAR10</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.7.3\" class=\"ltx_td ltx_align_center\">FALSE</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.7.4\" class=\"ltx_td ltx_align_center\">ResNet18</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.7.5\" class=\"ltx_td ltx_align_center\">1000</span></span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.8\" class=\"ltx_tr\">\n<span id=\"S3.T2.1.1.1.1.1.1.1.8.1\" class=\"ltx_td ltx_align_center ltx_border_r\">7</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.8.2\" class=\"ltx_td ltx_align_center\">CIFAR100</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.8.3\" class=\"ltx_td ltx_align_center\">TRUE</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.8.4\" class=\"ltx_td ltx_align_center\">ResNet18</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.8.5\" class=\"ltx_td ltx_align_center\">1000</span></span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.9\" class=\"ltx_tr\">\n<span id=\"S3.T2.1.1.1.1.1.1.1.9.1\" class=\"ltx_td ltx_align_center ltx_border_r\">8</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.9.2\" class=\"ltx_td ltx_align_center\">CIFAR100</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.9.3\" class=\"ltx_td ltx_align_center\">FALSE</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.9.4\" class=\"ltx_td ltx_align_center\">ResNet18</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.9.5\" class=\"ltx_td ltx_align_center\">1000</span></span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.10\" class=\"ltx_tr\">\n<span id=\"S3.T2.1.1.1.1.1.1.1.10.1\" class=\"ltx_td ltx_align_center ltx_border_r\">9</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.10.2\" class=\"ltx_td ltx_align_center\">EMNIST-digit</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.10.3\" class=\"ltx_td ltx_align_center\">TRUE</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.10.4\" class=\"ltx_td ltx_align_center\">LeNet</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.10.5\" class=\"ltx_td ltx_align_center\">1000</span></span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.11\" class=\"ltx_tr\">\n<span id=\"S3.T2.1.1.1.1.1.1.1.11.1\" class=\"ltx_td ltx_align_center ltx_border_r\">10</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.11.2\" class=\"ltx_td ltx_align_center\">EMNIST-byclass</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.11.3\" class=\"ltx_td ltx_align_center\">TRUE</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.11.4\" class=\"ltx_td ltx_align_center\">ResNet9</span>\n<span id=\"S3.T2.1.1.1.1.1.1.1.11.5\" class=\"ltx_td ltx_align_center\">3000</span></span>\n</span></span></span>\n</span></span></span><span id=\"S3.T2.1.2\" class=\"ltx_text\" style=\"color:#0000FF;\"></span></p>\n\n",
        "footnotes": "\n\n\n\n\nID\nDataset\nEdge-case\nModel\n##\\# devices\n\n1\nReddit\nFALSE\nLSTM\n8000\n\n2\nReddit\nFALSE\nGPT2\n8000\n\n3\nSentiment140\nFALSE\nLSTM\n2000\n\n4\nIMDB\nFALSE\nLSTM\n1000\n\n5\nCIFAR10\nTRUE\nResNet18\n1000\n\n6\nCIFAR10\nFALSE\nResNet18\n1000\n\n7\nCIFAR100\nTRUE\nResNet18\n1000\n\n8\nCIFAR100\nFALSE\nResNet18\n1000\n\n9\nEMNIST-digit\nTRUE\nLeNet\n1000\n\n10\nEMNIST-byclass\nTRUE\nResNet9\n3000\n\n",
        "references": [
            [
                "We implement all methods in PyTorch ",
                "(",
                "Paszke et¬†al.",
                ", ",
                "2019",
                ")",
                ".",
                "Tasks.",
                " In Table ",
                "2",
                " we summarize 10 tasks.\nEach task consists of a dataset, a binary variable denoting whether the backdoor is an edge-case or base-case backdoor (these terms are defined below), the model architecture, and the total number of devices in FL.\nFor all tasks, 10 devices are selected to participate in each round of FL, and we also provide results with 100 devices.",
                "Natural Language Processing.",
                "\nAttacks on natural language processing (NLP) tasks sample data from the training distribution and augment it with trigger sentences, so that the backdoored model will output the target when it sees an input containing the trigger.\nThe attacker‚Äôs training dataset, hereafter referred to as the ‚Äúpoisoned dataset,‚Äù\nincludes multiple possible triggers and a breadth of training data, so that at test time the backdoored model will produce one of the possible targets\nwhen presented with ",
                "any",
                " input containing one of ",
                "many",
                " possible triggers.\nWe consider these backdoors to be ",
                "base case backdoors",
                " because the incidence of words in the triggers is fairly common in the task dataset.\nThis is in contrast to the ",
                "edge-case backdoors",
                " of¬†",
                "(",
                "Wang et¬†al.",
                ", ",
                "2020a",
                ")",
                " that use triggers that all contain specific proper nouns that are uncommon in the task dataset.\nThese trigger sentences and targets are summarized in Table ",
                "1",
                ".",
                "Tasks 1 and 2 use the Reddit dataset",
                "1",
                "1",
                "1",
                "https://bigquery.cloud.google.com/dataset/fh-bigquery:reddit",
                "_",
                "_",
                "\\_",
                "comments",
                " for next word prediction, as in¬†",
                "(",
                "McMahan et¬†al.",
                ", ",
                "2017",
                "; ",
                "Bagdasaryan et¬†al.",
                ", ",
                "2020",
                "; ",
                "Wang et¬†al.",
                ", ",
                "2020a",
                "; ",
                "Panda et¬†al.",
                ", ",
                "2022",
                ")",
                ".\nThe bulk of our ablation studies and empirical analysis use the Reddit dataset, because next word prediction is the most widely deployed usecase for FL¬†",
                "(",
                "Hard et¬†al.",
                ", ",
                "2018",
                "; ",
                "Paulik et¬†al.",
                ", ",
                "2021",
                ")",
                ".\nWe consider 3 different trigger sentences that make generalizations about people of specific nationalities, people with specific skin colors, and roads in specific locations.\nTask 1 uses the LSTM architecture discussed in ",
                "(",
                "Wang et¬†al.",
                ", ",
                "2020a",
                ")",
                ", that includes an embedding layer of size 200, a 2-layer LSTM layer with ",
                "0.2",
                "0.2",
                "0.2",
                " dropout rate, a fully connected layer, and a sigmoid output layer.\nTask 2 uses the 120M-parameter GPT2¬†",
                "(",
                "Radford et¬†al.",
                ", ",
                "2019",
                ")",
                ".",
                "Task 3 uses the Sentiment140 Twitter dataset¬†",
                "(",
                "Go et¬†al.",
                ", ",
                "2009",
                ")",
                " for sentiment analysis, a binary classification task; and it uses the same LSTM as Task 1.\nTask 4 uses the IMDB movie review dataset¬†",
                "(",
                "Maas et¬†al.",
                ", ",
                "2011",
                ")",
                " for sentiment analysis; and it uses the same LSTM as Task 1.",
                "Computer Vision.",
                "\nCIFAR10, CIFAR100¬†",
                "(",
                "Krizhevsky et¬†al.",
                ", ",
                "2009",
                ")",
                ", and EMNIST¬†",
                "(",
                "Cohen et¬†al.",
                ", ",
                "2017",
                ")",
                " are benchmark datasets for the multiclass classification task in computer vision.\nThe base case backdoor for each dataset follows¬†",
                "(",
                "Panda et¬†al.",
                ", ",
                "2022",
                ")",
                ": we sample ",
                "512",
                "512",
                "512",
                " images from the class labeled ‚Äú5‚Äù and mislabel these as the class labeled ‚Äú9‚Äù.\nThe edge case backdoor for each dataset follows¬†",
                "(",
                "Wang et¬†al.",
                ", ",
                "2020a",
                ")",
                ".\nFor CIFAR (Tasks 5 and 7), out of distribution images of Southwest Airline‚Äôs planes are mislabeled as ‚Äútruck‚Äù.\nFor EMNIST (Task 9), the images are drawn from the class labeled ‚Äú7‚Äù from Ardis¬†",
                "(",
                "Kusetogullari et¬†al.",
                ", ",
                "2020",
                ")",
                ", a Swedish digit dataset, and mislabeled as ‚Äú1‚Äù.\nTasks 5-8 use the ResNet18 architecture¬†",
                "(",
                "He et¬†al.",
                ", ",
                "2016",
                ")",
                ".\nTasks 9-10 use LeNet¬†",
                "(",
                "Lecun et¬†al.",
                ", ",
                "1998",
                ")",
                " and ResNet9, respectively."
            ]
        ]
    },
    "A1.T3": {
        "caption": "",
        "table": "",
        "footnotes": "\n\n\n\n\n \n\n\nReddit\n\nBaseline\nNeurotoxin with different ratio\n\nk=0ùëò0k=0\nk=1ùëò1k=1\nk=3ùëò3k=3\nk=5ùëò5k=5\nk=15ùëò15k=15\nk=25ùëò25k=25\nk=35ùëò35k=35\nk=45ùëò45k=45\n\nTrigger set 1\n44\n131\n122\n197\n132\n49\n40\n6\n\nTrigger set 2\n78\n120\n187\n123\n22\n4\n1\n1\n\nTrigger set 3\n124\n302\n292\n235\n51\n24\n11\n16\n\n",
        "references": [
            [
                "Here, we show the lifespan of the baseline and Neurotoxin with\ndifferent mask ratios (Tab.¬†",
                "3",
                "),\ndifferent attack number (Tab.¬†",
                "4",
                "), and\ndifferent trigger length (Tab.¬†",
                "5",
                ").\nThe results show that choosing the appropriate ratio can make Neurotoxin obtain a large lifespan.\nFor different attack numbers and different length of triggers, Neurotoxin has a larger Lifespan than the baseline."
            ]
        ]
    },
    "A1.T4": {
        "caption": "",
        "table": "",
        "footnotes": "\n\n\n\n\n \n\n\nAttack number\n\nTrigger set 1\nTrigger set 2\nTrigger set 3\n\nBaseline\nNeurotoxin\nBaseline\nNeurotoxin\nBaseline\nNeurotoxin\n\n40\n11\n67\n17\n70\n18\n54\n\n60\n18\n110\n25\n105\n63\n147\n\n80\n44\n197\n78\n123\n124\n235\n\n100\n55\n235\n108\n173\n159\n173\n\n",
        "references": [
            [
                "Here, we show the lifespan of the baseline and Neurotoxin with\ndifferent mask ratios (Tab.¬†",
                "3",
                "),\ndifferent attack number (Tab.¬†",
                "4",
                "), and\ndifferent trigger length (Tab.¬†",
                "5",
                ").\nThe results show that choosing the appropriate ratio can make Neurotoxin obtain a large lifespan.\nFor different attack numbers and different length of triggers, Neurotoxin has a larger Lifespan than the baseline."
            ]
        ]
    },
    "A1.T5": {
        "caption": "",
        "table": "",
        "footnotes": "\n\n\n\n\nReddit\nTrigger len = 3\nTrigger len = 2\nTrigger len = 1\n\nBaseline\n78\n54\n32\n\nNeurotoxin\n123\n93\n122\n\n",
        "references": [
            [
                "Here, we show the lifespan of the baseline and Neurotoxin with\ndifferent mask ratios (Tab.¬†",
                "3",
                "),\ndifferent attack number (Tab.¬†",
                "4",
                "), and\ndifferent trigger length (Tab.¬†",
                "5",
                ").\nThe results show that choosing the appropriate ratio can make Neurotoxin obtain a large lifespan.\nFor different attack numbers and different length of triggers, Neurotoxin has a larger Lifespan than the baseline."
            ]
        ]
    },
    "A1.T6": {
        "caption": "",
        "table": "",
        "footnotes": "\n\n\n\n\n \n\n\nReddit\n\n \n\n\nAttack number\n\nTrigger set 1\nTrigger set 2\nTrigger set 3\n\nBaseline\nNeurotoxin\nBaseline\nNeurotoxin\nBaseline\nNeurotoxin\n\nStart Attack\n40\n16.65\n16.65\n16.65\n16.65\n16.65\n16.65\n\nStop Attack\n16.50\n16.42\n16.42\n16.43\n16.49\n16.42\n\nLifespan ‚â§50absent50\\leq 50\n16.49\n16.31\n16.42\n16.38\n16.33\n16.56\n\nStart Attack\n60\n16.65\n16.65\n16.65\n16.65\n16.65\n16.65\n\nStop Attack\n16.51\n16.53\n16.50\n16.50\n16.50\n16.52\n\nLifespan ‚â§50absent50\\leq 50\n16.45\n16.49\n16.47\n16.50\n16.55\n16.47\n\nStart Attack\n80\n16.65\n16.65\n16.65\n16.65\n16.65\n16.65\n\nStop Attack\n16.50\n16.46\n16.49\n16.47\n16.50\n16.46\n\nLifespan ‚â§50absent50\\leq 50\n16.41\n16.57\n16.52\n16.60\n16.48\n16.52\n\nStart Attack\n100\n16.65\n16.65\n16.65\n16.65\n16.65\n16.65\n\nStop Attack\n16.54\n16.34\n16.52\n16.35\n16.54\n16.35\n\nLifespan ‚â§50absent50\\leq 50\n16.49\n16.52\n16.44\n16.48\n16.53\n16.48\n\n",
        "references": [
            [
                "Here, we show the benign accuracy of the baseline and the Neurotoxin. Specifically, we show the benign at the moment when the attack starts (start attack), the moment when the attack ends (stop attack), and the moment when the accuracy of the backdoor attack drops to the threshold (Lifespan ",
                "‚â§",
                "\\leq",
                " threshold).\nThe results are shown in Tab. ",
                "6",
                " through Tab. ",
                "12",
                ". The results shown in Tab. ",
                "13",
                " are the results of benign accuracies of the baseline and the Neurotoxin on computer vision tasks with edge case trigger. All the tables show that Neurotoxin does not do too much damage to benign accuracy."
            ]
        ]
    },
    "A1.T7": {
        "caption": "",
        "table": "",
        "footnotes": "\n\n\n\n\n \n\n\nReddit\n\n \n\n\nModel structure\n\nTrigger set 1\nTrigger set 2\nTrigger set 3\n\nBaseline\nNeurotoxin\nBaseline\nNeurotoxin\nBaseline\nNeurotoxin\n\nStart Attack\nLSTM\n16.65\n16.65\n16.65\n16.65\n16.65\n16.65\n\nStop Attack\n16.50\n16.42\n16.42\n16.43\n16.49\n16.42\n\nLifespan ‚â§50absent50\\leq 50\n16.49\n16.31\n16.42\n16.38\n16.33\n16.56\n\nStart Attack\nGPT2\n28.66\n28.66\n28.66\n28.66\n28.66\n28.66\n\nStop Attack\n30.32\n30.33\n30.32\n30.31\n30.32\n30.33\n\nLifespan ‚â§50absent50\\leq 50\n30.64\n30.63\n30.64\n30.65\n30.64\n30.63\n\n",
        "references": [
            [
                "Here, we show the benign accuracy of the baseline and the Neurotoxin. Specifically, we show the benign at the moment when the attack starts (start attack), the moment when the attack ends (stop attack), and the moment when the accuracy of the backdoor attack drops to the threshold (Lifespan ",
                "‚â§",
                "\\leq",
                " threshold).\nThe results are shown in Tab. ",
                "6",
                " through Tab. ",
                "12",
                ". The results shown in Tab. ",
                "13",
                " are the results of benign accuracies of the baseline and the Neurotoxin on computer vision tasks with edge case trigger. All the tables show that Neurotoxin does not do too much damage to benign accuracy."
            ]
        ]
    },
    "A1.T8": {
        "caption": "",
        "table": "",
        "footnotes": "\n\n\n\n\n \n\n\nReddit\n\nTrigger set 1\nTrigger set 2\nTrigger set 3\n\nLSTM\nGPT2\nLSTM\nGPT2\nLSTM\nGPT2\n\nStart Attack\n16.65\n28.66\n16.65\n28.66\n16.65\n28.66\n\nStop Attack\n16.50\n30.32\n16.42\n30.32\n16.49\n30.32\n\nLifespan ‚â§50absent50\\leq 50\n16.49\n30.64\n16.42\n30.64\n16.33\n30.64\n\n",
        "references": [
            [
                "Here, we show the benign accuracy of the baseline and the Neurotoxin. Specifically, we show the benign at the moment when the attack starts (start attack), the moment when the attack ends (stop attack), and the moment when the accuracy of the backdoor attack drops to the threshold (Lifespan ",
                "‚â§",
                "\\leq",
                " threshold).\nThe results are shown in Tab. ",
                "6",
                " through Tab. ",
                "12",
                ". The results shown in Tab. ",
                "13",
                " are the results of benign accuracies of the baseline and the Neurotoxin on computer vision tasks with edge case trigger. All the tables show that Neurotoxin does not do too much damage to benign accuracy."
            ]
        ]
    },
    "A1.T9": {
        "caption": "",
        "table": "",
        "footnotes": "\n\n\n\n\n \n\n\nReddit\n\nTrigger len = 3\nTrigger len = 2\nTrigger len = 1\n\nBaseline\nNeurotoxin\nBaseline\nNeurotoxin\nBaseline\nNeurotoxin\n\nStart Attack\n16.65\n16.65\n16.65\n16.65\n16.65\n16.65\n\nStop Attack\n16.49\n16.47\n16.32\n16.28\n16.30\n16.29\n\nLifespan ‚â§50absent50\\leq 50\n16.52\n16.60\n16.35\n16.41\n16.34\n16.42\n\n",
        "references": [
            [
                "Here, we show the benign accuracy of the baseline and the Neurotoxin. Specifically, we show the benign at the moment when the attack starts (start attack), the moment when the attack ends (stop attack), and the moment when the accuracy of the backdoor attack drops to the threshold (Lifespan ",
                "‚â§",
                "\\leq",
                " threshold).\nThe results are shown in Tab. ",
                "6",
                " through Tab. ",
                "12",
                ". The results shown in Tab. ",
                "13",
                " are the results of benign accuracies of the baseline and the Neurotoxin on computer vision tasks with edge case trigger. All the tables show that Neurotoxin does not do too much damage to benign accuracy."
            ]
        ]
    },
    "A1.T10": {
        "caption": "",
        "table": "",
        "footnotes": "\n\n\n\n\n \n\n\nSentiment140\n\nTrigger set 1\nTrigger set 2\n\nBaseline\nNeurotoxin\nBaseline\nNeurotoxin\n\nStart Attack\n62.94\n62.94\n62.94\n62.94\n\nStop Attack\n60.06\n60.76\n59.62\n59.19\n\nLifespan ‚â§60absent60\\leq 60\n75.09\n74.40\n70.26\n73.47\n\n",
        "references": [
            [
                "Here, we show the benign accuracy of the baseline and the Neurotoxin. Specifically, we show the benign at the moment when the attack starts (start attack), the moment when the attack ends (stop attack), and the moment when the accuracy of the backdoor attack drops to the threshold (Lifespan ",
                "‚â§",
                "\\leq",
                " threshold).\nThe results are shown in Tab. ",
                "6",
                " through Tab. ",
                "12",
                ". The results shown in Tab. ",
                "13",
                " are the results of benign accuracies of the baseline and the Neurotoxin on computer vision tasks with edge case trigger. All the tables show that Neurotoxin does not do too much damage to benign accuracy."
            ]
        ]
    },
    "A1.T11": {
        "caption": "",
        "table": "",
        "footnotes": "\n\n\n\n\n \n\n\nIMDB\n\nTrigger set 1\nTrigger set 2\n\nBaseline\nNeurotoxin\nBaseline\nNeurotoxin\n\nStart Attack\n77.81\n77.81\n77.81\n77.81\n\nStop Attack\n74.07\n75.27\n74.04\n75.38\n\nLifespan ‚â§60absent60\\leq 60\n80.68\n80.64\n80.78\n80.86\n\n",
        "references": [
            [
                "Here, we show the benign accuracy of the baseline and the Neurotoxin. Specifically, we show the benign at the moment when the attack starts (start attack), the moment when the attack ends (stop attack), and the moment when the accuracy of the backdoor attack drops to the threshold (Lifespan ",
                "‚â§",
                "\\leq",
                " threshold).\nThe results are shown in Tab. ",
                "6",
                " through Tab. ",
                "12",
                ". The results shown in Tab. ",
                "13",
                " are the results of benign accuracies of the baseline and the Neurotoxin on computer vision tasks with edge case trigger. All the tables show that Neurotoxin does not do too much damage to benign accuracy."
            ]
        ]
    },
    "A1.T12": {
        "caption": "",
        "table": "",
        "footnotes": "\n\n\n\n\n \n\n\nBase case trigger\n\nCIFAR10\nCIFAR100\n\nBaseline\nNeurotoxin\nBaseline\nNeurotoxin\n\nStart Attack\n67.5\n67.5\n39.94\n39.94\n\nStop Attack\n65.16\n62.34\n47.47\n49.86\n\nLifespan ‚â§50absent50\\leq 50\n76.88\n78.06\n53.05\n54.05\n\n",
        "references": [
            [
                "Here, we show the benign accuracy of the baseline and the Neurotoxin. Specifically, we show the benign at the moment when the attack starts (start attack), the moment when the attack ends (stop attack), and the moment when the accuracy of the backdoor attack drops to the threshold (Lifespan ",
                "‚â§",
                "\\leq",
                " threshold).\nThe results are shown in Tab. ",
                "6",
                " through Tab. ",
                "12",
                ". The results shown in Tab. ",
                "13",
                " are the results of benign accuracies of the baseline and the Neurotoxin on computer vision tasks with edge case trigger. All the tables show that Neurotoxin does not do too much damage to benign accuracy."
            ]
        ]
    },
    "A1.T13": {
        "caption": "",
        "table": "",
        "footnotes": "\n\n\n\n\nEdge case\nCIFAR10\nCIFAR100\nEMNIST-digit\nEMNIST-byclass\n\ntrigger\nBaseline\nNeurotoxin\nBaseline\nNeurotoxin\nBaseline\nNeurotoxin\nBaseline\nNeurotoxin\n\nStart Attack\n67.5\n67.5\n39.94\n39.94\n89.78\n89.77\n77.50\n77.50\n\nStop Attack\n78.36\n74.74\n46.36\n49.79\n97.00\n96.94\n75.36\n74.82\n\n",
        "references": [
            [
                "Here, we show the benign accuracy of the baseline and the Neurotoxin. Specifically, we show the benign at the moment when the attack starts (start attack), the moment when the attack ends (stop attack), and the moment when the accuracy of the backdoor attack drops to the threshold (Lifespan ",
                "‚â§",
                "\\leq",
                " threshold).\nThe results are shown in Tab. ",
                "6",
                " through Tab. ",
                "12",
                ". The results shown in Tab. ",
                "13",
                " are the results of benign accuracies of the baseline and the Neurotoxin on computer vision tasks with edge case trigger. All the tables show that Neurotoxin does not do too much damage to benign accuracy."
            ]
        ]
    },
    "A1.T14": {
        "caption": "",
        "table": "",
        "footnotes": "\n\n\n\n\n \n\n\nMetric\n\nSentiment140\nCIFAR10\n\nBaseline\nNeurotoxin\nBaseline\nNeurotoxin\n\nLifespan\n278\n416\n116\n405\n\nTop eigenvalue\n0.004\n0.002\n899.37\n210.14\n\nHessian trace\n0.097\n0.027\n2331.11\n667.91\n\n",
        "references": [
            [
                "Here, we show the lifespan, top eigenvalue, and Hessian trace of the baseline and Neurotoxin on Sentimnet140 and CIFAR10. From Tab. ",
                "14",
                ", we see that, compared with the baseline, Neurotoxin has a smaller top eigenvalue and Hessian trace, which implies that the backdoor model of Neurotoxin is more stable, thus Neurotoxin has a larger Lifespan."
            ]
        ]
    }
}