{
    "id_table_1": {
        "caption": "TABLE II:  Characteristics of the Indexed Medical Corpus. Note: The Embedding denotes the file size resulting from processing with the MedCPT model.",
        "table": "S4.T2.1",
        "footnotes": [],
        "references": [
            "In this study, we employed the Meta-Llama-3-70B model to conduct task-driven adaptation on a specialized medical instruction dataset. Initially, we acquired and curated relevant data in accordance with predefined prompts (see Table 1) for preliminary knowledge distillation. Subsequently, we implemented a rigorous data cleaning process to eliminate inconsistent and redundant data.",
            "1) Self-Knowledge Boundary Identification  A random subset of questions was extracted from the baseline dataset, and the Meta-Llama-3-70B model was utilized to perform task-driven adaptation on this subset. Relevant data were aggregated based on predefined prompts (see Table 1), followed by preliminary knowledge distillation. Subsequently, a data cleaning process was implemented to eliminate inconsistencies and redundant information from the dataset, incorporating manual sampling scores obtained using GPT-4. Upon completion of this methodology, the resulting dataset comprised 71,557 data points."
        ]
    },
    "id_table_2": {
        "caption": "TABLE IV:  Descriptive Statistics of Medical Domain Benchmarks",
        "table": "S4.T4.1",
        "footnotes": [],
        "references": [
            "4) Retrieval-Augmented Generation  Raw data were collected from four distinct sources to enhance retrieval generation. The sources included PubMed (biomedical abstracts), Wikipedia (online encyclopedia), StatPearls (medical education database), and medical textbooks. Furthermore, MedCPT was employed to train an integrated retrieval and re-ranking model using 255 million query records from PubMed search logs, utilizing a comparative training method. Text paragraphs were transformed into vectors and stored in the Faiss vector database, facilitating efficient subsequent retrieval. Table 2 provides comprehensive statistics regarding the medical retrieval corpus and the number of indexed documents."
        ]
    },
    "id_table_3": {
        "caption": "TABLE V:  Primary Experimental Outcomes.  Each baseline model was evaluated under standardized settings. The experimental protocol consistently utilized a chain-of-thought strategy in prompts, with the Temperature parameter maintained at 0. To retrieval-augmented generation, both the MedCPT model and the PubMed knowledge base are utilized.  Bold  values denote the superior performance among all methods for each model.  *  signify results reported in the original paper.",
        "table": "S5.T5.7",
        "footnotes": [],
        "references": [
            "2) Directed Acyclic Graph Task Decomposition  Similarly, we adhered to the preset improvement protocol (see Table 3). Following this series of steps, the resulting dataset comprised 32,849 data points."
        ]
    },
    "id_table_4": {
        "caption": "TABLE VI:  Ablation Study Outcomes: Analysis of Bailicai Component Efficacy.  To enhance retrieval performance, both the MedCPT model and the PubMed knowledge base are utilized.  Bold  values denote the superior performance among all methods for each model.",
        "table": "S5.T6.5",
        "footnotes": [],
        "references": [
            "To comprehensively evaluate the domain expertise of the Bailicai-based large language model in the medical field and its retrieval augmentation efficacy, this study utilized five medical question-answering benchmark datasets [ 31 ,  37 ] : MedQA, MedMCQA, MMLU-Med, PubMedQA, and BioASQ. MMLU-Med, a subset of the MMLUs medical category, comprises tasks in anatomy, clinical knowledge, university medicine, medical genetics, professional medicine, and university biology. The study evaluates the models overall performance in medical question-answering for each dataset and task by computing its predictive accuracy. Detailed characteristics of the specific datasets are presented in Table 4."
        ]
    },
    "id_table_5": {
        "caption": "TABLE VII:  Comparative Analysis of Bailicai Component Performance on the PubMedQA*. PubMedQA*: excludes original golden documents, retaining only the query and answer.",
        "table": "S5.T7.1",
        "footnotes": [],
        "references": [
            "Comparison with Large Models in the Medical Domain  Table 5 demonstrates the superior performance of the Bailicai model across five medical benchmarks. In comparison with domain-specific models of smaller scale (e.g., 7B, 8B, 13B parameters) that are pre-trained and fine-tuned for medical tasks, Bailicai attains state-of-the-art results on benchmarks including MedQA, MMLU-Med, PubMedQA, and BioASQ. On the MedMCQA benchmark, Bailicais performance is comparable to OpenBioLLM, with a marginal difference of 0.14% and an average score of 71.82%. Moreover, Bailicai (8B) demonstrates performance parity with larger models such as Flan-Palm (540B) in the medical domain, a characteristic of particular importance in resource-constrained healthcare environments due to its capability for local deployment coupled with the absence of privacy leakage concerns. These findings underscore Bailicais substantial advantages in processing diverse medical tasks.",
            "Comparison with General Domain Large Models  As evidenced in Table 5, Bailicai outperforms ChatGPT-3.5(achieving an average score of 65.85%) by 5.97%. In the medical domain, given the critical nature of user privacy, Bailicai offers a significant advantage through its capability for local deployment. Furthermore, Bailicai achieves superior performance relative to the Meta-Llama-3-70B model on the PubMedQA and BioASQ benchmarks, surpassing it by 0.6% and 3.72% respectively."
        ]
    },
    "id_table_6": {
        "caption": "TABLE VIII:  Comparative Analysis of Bailicais Performance Across Diverse Medical Corpora.",
        "table": "S5.T8.3",
        "footnotes": [],
        "references": [
            "To evaluate the efficacy of various components within the Bailicai framework, a series of ablation studies was conducted, as detailed in Table 6. Each experiment incorporated the integration of distinct modules:",
            "Table 6 delineates the results of ablation studies, revealing that the removal of any module from the Bailicai framework leads to a performance decline, thus highlighting the critical contributions of each module to the overall efficacy. In comparison to the baseline model, Meta-Llama-3-8B, which utilizes Retrieval-Augmented Generation technology, the Bailicai framework demonstrates superior performance in information extraction and increased robustness against distracting documents. Concurrent application of all four modules yields performance improvements of 8.88% and 5.41% on the MedQA and MMLU-Med benchmarks, respectively, with an average score increase of 3.44%.",
            "Moreover, the MedQA benchmark, which encompasses extensive patient inquiry data, exhibited the highest mean number of sub-queries at 3.56. This benchmark illustrates the models proficiency in distinguishing and decomposing query complexity. The decomposition of patient inquiries into granular sub-tasks enables the collection of critical clinical indicators necessary for diagnosis, thus improving the efficacy of problem resolution. These results are further substantiated in Table 6."
        ]
    },
    "id_table_7": {
        "caption": "TABLE IX:  Statistics of Know and Uknow Categories and Average Number of Sub-questions Across Various Medical Benchmarks on Bailicai.",
        "table": "S5.T9.1",
        "footnotes": [],
        "references": [
            "To test this hypothesis, the experiments on the PubMedQA benchmark were reconfigured, excluding the golden context documents provided in the original dataset and focusing exclusively on question-based evaluation. As evidenced in Table 7, the incremental addition of modules yielded an improvement in Bailicais performance on the PubMedQA benchmark from 52.60% to 69.80%, corroborating the hypothesized causes of the initial performance decline."
        ]
    },
    "global_footnotes": []
}