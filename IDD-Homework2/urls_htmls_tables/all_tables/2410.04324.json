{
    "id_table_1": {
        "caption": "Table 1 :  Overview of our dataset with fake audios generated by various models. AudioGen lacks speaker and language information as it focuses on environmental sounds. The trainset sizes for OpenAI and Seed-TTS are unavailable due to the use of proprietary data. * denotes the samples that are directly collected from their demo page or provided test set due to the unavailability of their model checkpoints.",
        "table": "S3.T1.5.1",
        "footnotes": [],
        "references": [
            "Leveraging a set of diverse and high-quality speech data synthesis APIs and models, we create an evaluation dataset for synthetic AI-audio detection. Our approach incorporates two strategies: data generation and data collection. Our dataset includes AI-generated speech and audio from nine distinct sources. We perform speech data generation using one cutting-edge TTS service provider, OpenAI, and two open-sourced APIs, xTTS  [ 27 ]  and AudioGen  [ 28 ] . For speech data collection, we leverage six state-of-the-art TTS models including Seed-TTS  [ 29 ] , VALL-E  [ 4 ] , PromptTTS2  [ 30 ] , NaturalSpeech3  [ 31 ] , VoiceBox  [ 11 ] , FlashSpeech  [ 2 ] . Table  1  presents the details of our dataset generated by different audio generation models. We next detail our methods of generating and collecting these datasets.",
            "Data generation . Our dataset generation involves OpenAI, xTTS, and AudioGen. Specifically, OpenAI currently provides voice choices from 6 different speakers. Using ChatGPT,we generate 100 different text prompts of varying lengths for each speaker, resulting in a total of 600 synthetic speech audios. xTTS supports synthetic speech generation given text prompts and reference speech. We select 6 speakers from the LibriTTS dataset  [ 32 ]  as the reference speech and also generate 600 text prompts with ChatGPT for each speaker, resulting in 600 synthetic speech audios. AudioGen can generate the corresponding environmental sound given a textual description of the acoustic scene. With AudioGen, we use ChatGPT to generate 100 text descriptions of the environment and background and obtain 100 AI-synthesized environmental sounds. Figure  1  (left) illustrates the data generation and collection process."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Generalization across existing audio deepfake datasets. All models are trained/finetuned on the Wavefake training set. Green and orange indicate the best and second-best performance, respectively.",
        "table": "S4.T2.4.1",
        "footnotes": [],
        "references": [
            "We first train all models on Wavefake training dataset and then evaluate the models on its own test set, LibriSeVoc test set, and In-the-wild dataset. Table  2  presents the evaluation results. Particularly, we make the following interesting observations.",
            "Speech foundation models exhibit stronger generalizability . As shown in Table   2 , when evaluated on the test set of Wavefake, all models demonstrate near-perfect performance across the three metrics. This can be attributed to the similarity between the test set and the training data. However, when tested on the LibriSeVoc and In-the-wild datasets, models such as LFCC-LCNN, Spec.+ResNet, RawNet2, RawGATST, and AASIST struggle to generalize effectively. This performance gap indicates significant overfitting to the training data, despite these models being specifically designed for audio deepfake detection tasks. In contrast, speech foundation models consistently display stronger generalizability. Notably, Wave2Vec2BERT achieves the highest generalizability, which may be attributed to its large-scale and diverse pretraining data. Pretrained on 4.5 million hours of unlabeled audio in more than 143 languages, Wave2Vec2BERT benefits from both scale and diversity. This suggests that a well-designed self-supervised model trained on diverse speech data can extract general and discriminative features, making it more applicable across different datasets for audio deepfake detection. It is important to note that CLAP, unlike other speech foundation models, does not generalize well across datasets. This is likely due to its primary focus on environmental audio data during pretraining, resulting in the extraction of irrelevant features for speech audio. This observation underscores that not all foundation models are equally suited for audio deepfake detection tasks.",
            "Generalizability may increase with model size.  In Table  2 , it can be observed that Whisper-large always outperforms Whisper-small across all three datasets. In particular, on the LibriSeVoc test set, Whisper-large achieves accuracy, AUROC, and EER of 0.9572, 0.9901, 4.279%, respectively, which improves by 2.27%, 0.64%, and 2.272%, than that of Whisper-small. This trend is more evident in the In-the-wild dataset, which is closer to real-world scenarios since this dataset consists of speech data sourced from the internet. Specifically, Whisper-large achieves accuracy, AUROC, and EER of 0.8848, 0.9552, and 11.518%, respectively, which improves by 6.381%, 5.27%, and 6.381%, than that of Whisper-small. Further investigation will be made in Section  4.2.3",
            "It is still challenging for detection models to correctly classify synthesized audio samples, especially those generated by the most advanced TTS service providers.  While Wave2Vec2BERT achieves an overall average accuracy of 0.8989, it only reaches 0.6017 on Seed-TTS and 0.7833 on OpenAI. A similar pattern is also evident with HuBERT, Wave2Vec2, Whisper-large, and Whisper-small, which achieve just 0.5658, 0.4342, 0.29, and 0.1883 accuracy on OpenAI, respectively. This performance disparity is likely due to OpenAI and Seed-TTS having more advanced model architectures and being trained on proprietary, self-collected data, leading to higher-quality and more realistic speech generation. We will explore potential strategies to enhance their detection performance in Section  4.2.4 . Overall, these results not only indicate that no single model consistently outperforms across all datasets but also underscore the ongoing difficulty in detecting synthesized audio from cutting-edge TTS systems, especially those developed by the most advanced TTS service providers. This highlights a huge gap between the rapid evolution of TTS technologies and the effectiveness of current audio deepfake detection methods, emphasizing the urgent need for the development of more robust and reliable detection algorithms.",
            "Table  5  presents the detection performance of the Whisper models across the Wavefake, LibriSeVoc, and In-the-wild datasets. First, Whisper-tiny, despite its smaller size, still outperforms or achieves comparable detection performance to traditional detection models (recall Table  2 ) on the LibriSeVoc test set. This again validates the finding that foundation models exhibit stronger generalizability for audio deepfake detection tasks, even in their smallest configurations.",
            "Figures  2(a)  and  2(b)  present the results of fine-tuning Wave2Vec2BERT and HuBERT using varying numbers of samples from OpenAI. Before fine-tuning, Wave2Vec2BERT and HuBERT only achieve accuracies of 0.7833 and 0.5658, respectively. Notably, with only 10 shots of fake speech data, Wave2Vec2BERT reaches an accuracy of approximately 0.97, while HuBERTs accuracy increases significantly to approximately 0.85. Importantly, the models generalization to other datasets remains unchanged, demonstrating the effectiveness and efficiency of few-shot fine-tuning. However, as the number of fine-tuning samples increases, HuBERTs test accuracy on the WaveFake test set shows a declining trend, which is also observed for Wave2Vec2BERT.",
            "It is important to note, however, that the efficiency and effectiveness of few-shot fine-tuning may vary across different datasets. As illustrated in Figures  2(c)  and  2(d) , which depict the fine-tuning results for Wave2Vec2BERT and HuBERT on Seed-TTS, the improvement in accuracy is less pronounced compared to the results on the OpenAI dataset. While the accuracy of both Wave2Vec2BERT and HuBERT does improve on Seed-TTS, the gains are not as significant as those observed for the OpenAI dataset. Additionally, the detection performance on other datasets decreases more noticeably when fine-tuning on Seed-TTS compared to OpenAI."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Evaluation on SONAR dataset. Green and orange indicate the best and second-best performance, respectively.",
        "table": "S4.T3.st1.5.1",
        "footnotes": [],
        "references": [
            "Generalizability may increase with model size.  In Table  2 , it can be observed that Whisper-large always outperforms Whisper-small across all three datasets. In particular, on the LibriSeVoc test set, Whisper-large achieves accuracy, AUROC, and EER of 0.9572, 0.9901, 4.279%, respectively, which improves by 2.27%, 0.64%, and 2.272%, than that of Whisper-small. This trend is more evident in the In-the-wild dataset, which is closer to real-world scenarios since this dataset consists of speech data sourced from the internet. Specifically, Whisper-large achieves accuracy, AUROC, and EER of 0.8848, 0.9552, and 11.518%, respectively, which improves by 6.381%, 5.27%, and 6.381%, than that of Whisper-small. Further investigation will be made in Section  4.2.3",
            "We further evaluate all detection models on the proposed dataset. Table  3(a) , Table  3(b) , and Table  3(c)  present the accuracy, AUROC, and EER of different detection models on our proposed SONAR dataset as described in Sec  3 .",
            "Speech foundation models can better generalize on the SONAR dataset, but still not good enough.  As presented in Table  3(a) , speech foundation models again exhibit better generalizability on the fake audio samples generated by the latest TTS models. For instance, AASIST achieves 0.6975 average accuracy across audios generated by cutting-edge TTS models, which is the best performance among the traditional detection models. In contrast, speech foundation models Whisper-large, Wave2Vec2, HuBERT, and Wave2Vec2BERT achieve an average accuracy of 0.7322, 0.788, 0.8789, and 0.8989, respectively, which is higher than AASIST by 3.47%, 9.05%, 18.14%, and 20.14%, respectively. More specifically, even though Wave2Vec2BERT and HuBERT are only fine-tuned on Wavefake dataset, for PromptTTS2, VALL-E, VoiceBox, FalshSpeech, AudioGen, and xTTS, Wave2Vec2BERT can reach accuracies of 1.0, 0.9062, 0.9474, 0.9712, 0.9237, 0.97, and 0.9867, respectively, and HuBERT can achieve 1.0, 0.9158, 0.9712, 0.9407, 1.0 0.8767, and 0.89, respectively, demonstrating their potential capability of extract more distinguishable features compared to other models. It is also worth noting that Wave2Vec2BERT achieves an accuracy of 0.9062 on NaturalSpeech3, while all other models can only reach that   0.75 absent 0.75 \\leq 0.75  0.75 ."
        ]
    },
    "id_table_4": {
        "caption": "(a)   Accuracy (   \\uparrow  ).",
        "table": "S4.T3.st2.5.1",
        "footnotes": [],
        "references": [
            "Generalizability may increase with model size.  In Table  2 , it can be observed that Whisper-large always outperforms Whisper-small across all three datasets. In particular, on the LibriSeVoc test set, Whisper-large achieves accuracy, AUROC, and EER of 0.9572, 0.9901, 4.279%, respectively, which improves by 2.27%, 0.64%, and 2.272%, than that of Whisper-small. This trend is more evident in the In-the-wild dataset, which is closer to real-world scenarios since this dataset consists of speech data sourced from the internet. Specifically, Whisper-large achieves accuracy, AUROC, and EER of 0.8848, 0.9552, and 11.518%, respectively, which improves by 6.381%, 5.27%, and 6.381%, than that of Whisper-small. Further investigation will be made in Section  4.2.3",
            "It is still challenging for detection models to correctly classify synthesized audio samples, especially those generated by the most advanced TTS service providers.  While Wave2Vec2BERT achieves an overall average accuracy of 0.8989, it only reaches 0.6017 on Seed-TTS and 0.7833 on OpenAI. A similar pattern is also evident with HuBERT, Wave2Vec2, Whisper-large, and Whisper-small, which achieve just 0.5658, 0.4342, 0.29, and 0.1883 accuracy on OpenAI, respectively. This performance disparity is likely due to OpenAI and Seed-TTS having more advanced model architectures and being trained on proprietary, self-collected data, leading to higher-quality and more realistic speech generation. We will explore potential strategies to enhance their detection performance in Section  4.2.4 . Overall, these results not only indicate that no single model consistently outperforms across all datasets but also underscore the ongoing difficulty in detecting synthesized audio from cutting-edge TTS systems, especially those developed by the most advanced TTS service providers. This highlights a huge gap between the rapid evolution of TTS technologies and the effectiveness of current audio deepfake detection methods, emphasizing the urgent need for the development of more robust and reliable detection algorithms.",
            "Building on the observation that Whisper-large consistently outperforms Whisper-small, we extend our analysis with controlled experiments on the entire Whisper model family. Specifically, the Whisper family comprises five different model sizes: Whisper-tiny, Whisper-base, Whisper-small, Whisper-medium, and Whisper-large. Table  4  presents the number of model parameters of them. Specifically, each model is fine-tuned on the Wavefake training dataset using the same hyperparameters. Our results show that as model size increases, the generalizability of the models improves as well."
        ]
    },
    "id_table_5": {
        "caption": "(b)   AUROC (   \\uparrow  ).",
        "table": "S4.T3.st3.5.1",
        "footnotes": [],
        "references": [
            "Table  5  presents the detection performance of the Whisper models across the Wavefake, LibriSeVoc, and In-the-wild datasets. First, Whisper-tiny, despite its smaller size, still outperforms or achieves comparable detection performance to traditional detection models (recall Table  2 ) on the LibriSeVoc test set. This again validates the finding that foundation models exhibit stronger generalizability for audio deepfake detection tasks, even in their smallest configurations."
        ]
    },
    "id_table_6": {
        "caption": "(c)   EER(%) (   \\downarrow  ).",
        "table": "S4.T4.4",
        "footnotes": [],
        "references": [
            "We also evaluate the Whisper family on the proposed SONAR dataset. Table  6(a) , Table  6(b) , Table  6(c)  present the corresponding Accuracy, AUROC, and EER(%). A similar trend can also be observed. In Table  6(a) , the accuracy of the Whisper models shows a clear upward trend as the model size increases from Whisper-tiny to Whisper-large. Whisper-tiny achieves an average accuracy of 0.5467, while Whisper-large reaches the highest average accuracy of 0.7322. Notably, Whisper-large performs best on almost all datasets, particularly with TTS models such as PromptTTS2, NaturalSpeech3, VALL-E, and OpenAI, highlighting its better generalizability. Additionally, Whisper-larges performance is higher on challenging datasets like Seed-TTS and OpenAI, which are known for their high-quality synthesis. The smaller models (e.g., Whisper-tiny and Whisper-base), on the other hand, struggle to generalize effectively, particularly on datasets such as OpenAI, where the accuracy drops to 0.0833 for Whisper-tiny."
        ]
    },
    "id_table_7": {
        "caption": "Table 5 :  Generalization across existing audio deepfake datasets. All Whisper models are trained/finetuned on the Wavefake training set. Green and orange indicate the best and second-best performance, respectively.",
        "table": "S4.T5.4.1",
        "footnotes": [],
        "references": [
            "Table  7  presents the hyperparameters for training AASIST, RawNet2, RawGAT-ST, LCNN, and Spec.+ResNet. We train AASIST, RawNet2, and RawGAT-ST with a learning rate of 0.0001 and LCNN and Spec.+ResNet with a learning rate of 0.0003. The batch size for AASIST, RawNet2, RawGAT-ST, LCNN, and Spec.+ResNet are 64, 256, 32, 512, and 256, respectively. All input audios are resampled to a 16kHz sampling rate and converted into raw waveforms consisting of 64,000 samples (approximately 4 seconds). Audios longer than 4 seconds are randomly trimmed, while those shorter than 4 seconds are repeated and padded to meet the 4-second duration."
        ]
    },
    "id_table_8": {
        "caption": "Table 6 :  Evaluation on SONAR dataset. Green and orange indicate the best and second-best performance, respectively.",
        "table": "S4.T6.st1.5.1",
        "footnotes": [],
        "references": []
    },
    "id_table_9": {
        "caption": "(a)   Accuracy (   \\uparrow  ).",
        "table": "S4.T6.st2.5.1",
        "footnotes": [],
        "references": []
    },
    "id_table_10": {
        "caption": "(b)   AUROC (   \\uparrow  ).",
        "table": "S4.T6.st3.5.1",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "(c)   EER(%) (   \\downarrow  ).",
        "table": "A1.T7.2",
        "footnotes": [],
        "references": []
    }
}