{
    "id_table_1": {
        "caption": "Table 1.  Baseline results of  Llama3-Aloe-8B-Alpha  using zero-shot next token prediction, CoT, and SC-CoT with 5 ensembles.",
        "table": "S4.T1.1",
        "footnotes": [],
        "references": [
            "This section details the methodology employed to optimize a context retrieval system and evaluate its performance. We first outline the core components of the context retrieval system under investigation ( 3.1 ), followed by a description of the benchmark datasets used for evaluation ( 3.2 ), and conclude with the specific models and computational resources employed in the study ( 3.3 ).",
            "Let us start with the retrieval system architecture, which is based on the Medprompt design. Figure   1  illustrates this question-answering system, highlighting the key components that will be explored and optimized in this work.",
            "This section details the experiments conducted to evaluate the impact of different Context Retrieval (CR) components on the performance of Large Language Models (LLMs) in answering healthcare questions. We begin by establishing a baseline performance without any context retrieval ( 4.1 ), followed by a systematic investigation of individual components within the Self-Consistency with Chain-of-Thought (SC-CoT) framework. We then study the impact of adding external knowledge sources and the various components in the Medprompt architecture ( 4.2 ). Based on these findings, we propose an optimized CR configuration ( 4.3 ) and compare its performance against state-of-the-art models ( 4.4 ).",
            "Table  1  presents the baseline performance of Llama3-Aloe-8B-Alpha, our primary evaluation model, on four benchmark datasets using zero-shot next token prediction, CoT, and SC-CoT. This serves as a reference point for evaluating the subsequent improvements achieved through the integration of various components.",
            "Based on our empirical findings, we propose an optimized context retrieval configuration that follows the Medprompt (Figure  1 ) scheme. The main component removed is the reranker model. The proposed setup utilizes the following:",
            "Our preliminary analysis revealed a significant performance gap when transitioning from multiple-choice to open-ended formats. Table   10  illustrates this disparity, showing a substantial decrease in accuracy of 10% for the Llama-3.1-8B-Instruct model on the MedQA dataset when transitioning from multiple choice questions (MCQs) to open-ended (OE) questions. Surprisingly, the incorporation of Chain-of-Thought (CoT) and Tree-of-Thought (ToT) reasoning did not improve performance for open-ended questions, contrary to their effectiveness in other domains.",
            "To address this gap, we propose OpenMedprompt, an extension of our optimized retrieval system specifically designed for open-ended medical question answering. We introduce the methodology in  5.1 , our modifications for datasets and databases for OE generation in  5.2 , the evaluation procedure in  5.3  and the results and discussion in  5.4 .",
            "Table  11  presents the results of our experiments of OM-ER using the CoT and ToT databases. We see that both CoT and ToT databases show improvement over the baseline open-ended performance. ToT generally outperforms CoT, with the best result (60.02%) achieved using 20 ensembles.",
            "Table   12  shows the results for OM-SR with the CoT and ToT databases. We see that the OM-SR system shows more consistent improvement over the baseline compared to the OM-ER system. The CoT database performance peaks at N=15 (60.88%), outperforming ToT in this configuration. The ToT database on the other hand shows less variation across different N values, suggesting more stable performance."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.  Accuracy change when adding choice shuffling (CS) to the baseline with SC-CoT.  N N N italic_N  represents the number of ensembles of the SC component.",
        "table": "S4.T2.3",
        "footnotes": [],
        "references": [
            "This section details the methodology employed to optimize a context retrieval system and evaluate its performance. We first outline the core components of the context retrieval system under investigation ( 3.1 ), followed by a description of the benchmark datasets used for evaluation ( 3.2 ), and conclude with the specific models and computational resources employed in the study ( 3.3 ).",
            "This section details the experiments conducted to evaluate the impact of different Context Retrieval (CR) components on the performance of Large Language Models (LLMs) in answering healthcare questions. We begin by establishing a baseline performance without any context retrieval ( 4.1 ), followed by a systematic investigation of individual components within the Self-Consistency with Chain-of-Thought (SC-CoT) framework. We then study the impact of adding external knowledge sources and the various components in the Medprompt architecture ( 4.2 ). Based on these findings, we propose an optimized CR configuration ( 4.3 ) and compare its performance against state-of-the-art models ( 4.4 ).",
            "We investigate the impact of choice shuffling and a variable number of ensembles. The impact of  choice shuffling , is among the most well-documented phenomenon  (Lievin et al . ,  2024 ) , as LLMs answering MCQs seem to be biased towards the first response ( e.g.,  A). Table   2  demonstrates the consistent benefits of incorporating choice shuffling within the SC-CoT framework, yielding improvements across all datasets and ensemble configurations. Based on these results, all subsequent experiments utilize choice shuffling to ensure unbiased evaluation.",
            "Next, we consider the second SC-CoT factor, the  number of ensembles  used in the self-consistency process. This choice has a significant effect, not only on task performance but also on computational cost and footprint. Such trade-off can be seen in Table  3 , where performance gains as footprint consistently grow up to 5-6% and 1.76Kg of  C  O 2 C subscript O 2 CO_{2} italic_C italic_O start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  respectively. The first five ensembles provide around 3.5% accuracy gains, while another five adds half of that. Gains beyond that are marginal, requiring up to twenty-five ensembles (fifteen more) to gain another 1%. Figure  2  visually depicts the trend of diminishing returns.",
            "To address this gap, we propose OpenMedprompt, an extension of our optimized retrieval system specifically designed for open-ended medical question answering. We introduce the methodology in  5.1 , our modifications for datasets and databases for OE generation in  5.2 , the evaluation procedure in  5.3  and the results and discussion in  5.4 .",
            "Table   12  shows the results for OM-SR with the CoT and ToT databases. We see that the OM-SR system shows more consistent improvement over the baseline compared to the OM-ER system. The CoT database performance peaks at N=15 (60.88%), outperforming ToT in this configuration. The ToT database on the other hand shows less variation across different N values, suggesting more stable performance."
        ]
    },
    "id_table_3": {
        "caption": "Table 3.  Accuracy change of SC-CoT with a variable number of ensembles, when compared to the baseline without SC (N=1).  C  O 2 C subscript O 2 CO_{2} italic_C italic_O start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  indicates the associated footprint.",
        "table": "S4.T3.1",
        "footnotes": [],
        "references": [
            "This section details the methodology employed to optimize a context retrieval system and evaluate its performance. We first outline the core components of the context retrieval system under investigation ( 3.1 ), followed by a description of the benchmark datasets used for evaluation ( 3.2 ), and conclude with the specific models and computational resources employed in the study ( 3.3 ).",
            "This section details the experiments conducted to evaluate the impact of different Context Retrieval (CR) components on the performance of Large Language Models (LLMs) in answering healthcare questions. We begin by establishing a baseline performance without any context retrieval ( 4.1 ), followed by a systematic investigation of individual components within the Self-Consistency with Chain-of-Thought (SC-CoT) framework. We then study the impact of adding external knowledge sources and the various components in the Medprompt architecture ( 4.2 ). Based on these findings, we propose an optimized CR configuration ( 4.3 ) and compare its performance against state-of-the-art models ( 4.4 ).",
            "Next, we consider the second SC-CoT factor, the  number of ensembles  used in the self-consistency process. This choice has a significant effect, not only on task performance but also on computational cost and footprint. Such trade-off can be seen in Table  3 , where performance gains as footprint consistently grow up to 5-6% and 1.76Kg of  C  O 2 C subscript O 2 CO_{2} italic_C italic_O start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  respectively. The first five ensembles provide around 3.5% accuracy gains, while another five adds half of that. Gains beyond that are marginal, requiring up to twenty-five ensembles (fifteen more) to gain another 1%. Figure  2  visually depicts the trend of diminishing returns.",
            "To address this gap, we propose OpenMedprompt, an extension of our optimized retrieval system specifically designed for open-ended medical question answering. We introduce the methodology in  5.1 , our modifications for datasets and databases for OE generation in  5.2 , the evaluation procedure in  5.3  and the results and discussion in  5.4 .",
            "Figure  3  and Figure  4  illustrate the architecture of OM-ER and OM-SR, respectively."
        ]
    },
    "id_table_4": {
        "caption": "Table 4.  Properties of the models used to embed the questions.",
        "table": "S4.T4.1",
        "footnotes": [],
        "references": [
            "This section details the experiments conducted to evaluate the impact of different Context Retrieval (CR) components on the performance of Large Language Models (LLMs) in answering healthcare questions. We begin by establishing a baseline performance without any context retrieval ( 4.1 ), followed by a systematic investigation of individual components within the Self-Consistency with Chain-of-Thought (SC-CoT) framework. We then study the impact of adding external knowledge sources and the various components in the Medprompt architecture ( 4.2 ). Based on these findings, we propose an optimized CR configuration ( 4.3 ) and compare its performance against state-of-the-art models ( 4.4 ).",
            "At this point, we extend the SC-CoT scheme with retrieval components, exploiting external data sources. This will include the main elements included in Medprompt: An  embedding  model, a  database , and  reranker  model. The  embedding  model encodes both input and database items before computing their similarity scores. For this component, we consider four different models, ranging in embedding size and in number of parameters. We also consider whether they have been specialized in the healthcare domain or not. These properties are listed in Table  4 , and their performance in Table  5 . Results show all models achieve comparable performance in most datasets, with no embedding model clearly outperforming the rest. As a result, we select the healthcare-specific, cheaper model PubMedBERT to be the embedding model for our future experiments.",
            "To address this gap, we propose OpenMedprompt, an extension of our optimized retrieval system specifically designed for open-ended medical question answering. We introduce the methodology in  5.1 , our modifications for datasets and databases for OE generation in  5.2 , the evaluation procedure in  5.3  and the results and discussion in  5.4 .",
            "Figure  3  and Figure  4  illustrate the architecture of OM-ER and OM-SR, respectively."
        ]
    },
    "id_table_5": {
        "caption": "Table 5.  Accuracy for each embedding model on each dataset. We set 5 as the number of ensembles and few-shot examples, choice shuffling is activated. CoT database in use.",
        "table": "S4.T5.1",
        "footnotes": [],
        "references": [
            "At this point, we extend the SC-CoT scheme with retrieval components, exploiting external data sources. This will include the main elements included in Medprompt: An  embedding  model, a  database , and  reranker  model. The  embedding  model encodes both input and database items before computing their similarity scores. For this component, we consider four different models, ranging in embedding size and in number of parameters. We also consider whether they have been specialized in the healthcare domain or not. These properties are listed in Table  4 , and their performance in Table  5 . Results show all models achieve comparable performance in most datasets, with no embedding model clearly outperforming the rest. As a result, we select the healthcare-specific, cheaper model PubMedBERT to be the embedding model for our future experiments.",
            "To address this gap, we propose OpenMedprompt, an extension of our optimized retrieval system specifically designed for open-ended medical question answering. We introduce the methodology in  5.1 , our modifications for datasets and databases for OE generation in  5.2 , the evaluation procedure in  5.3  and the results and discussion in  5.4 .",
            "While we expect performance to increase when the number of few shots increases we notice that in Figure  5  this isnt the case for OM-ER ToT. This could be attributed to the effective context length of the LLM. ToT responses are very verbose and when increasing the number of few-shots the context size significantly grows making it difficult for the LLM to reason over this large context."
        ]
    },
    "id_table_6": {
        "caption": "Table 6.  Accuracy change when extending the database through CoT and ToT (MedMCQA and MedQA train splits). Also when using a database extended with CoT/ToT coming from a different source (CareQA and MMLU use MedMCQA train split as database).",
        "table": "S4.T6.1",
        "footnotes": [],
        "references": [
            "While the MedMCQA and MedQA experiments study the impact of size and quality in databases, the experiments on CareQA and MMLU add a factor of generalization (by using a DB from a different source). Results are shown in Table  6 . In three out of four cases, the synthetically enhanced data improves the performance of the retrieval system. Even when the database comes from a different source, the extended database contributes to increase accuracy. Between CoT and ToT, the first outperforms the second in three out of four datasets. All further experiments will make use of the CoT extended databases.",
            "We also see that the increase in the number of ensembles doesnt consistently improve the accuracy in Figure  6 . OM-ER benefits from ensemble sizes up to 20 while for the OM-SR method ensemble size of 15 provides the best performance for CoT while ensemble size 10 provides the best performance for ToT."
        ]
    },
    "id_table_7": {
        "caption": "Table 7.  Accuracy change when adding the reranker, sorting the samples retrieved from the database.",
        "table": "S4.T7.1",
        "footnotes": [],
        "references": [
            "The final component we study is the use of a  reranker  model. The role of the reranker is to sort the most similar items retrieved from the database, which yields performance boosts in certain domains  (Wang et al . ,  2023a ) . To that end, we use the MedCPT-Cross-Encoder model. That is a contrastively pre-trained transformer, tuned on PubMed information retrieval. Table  7  shows the inconsistent performance gains achieved with the reranker, leading us to exclude it from further experimentation due to its added computational cost."
        ]
    },
    "id_table_8": {
        "caption": "Table 8.  Accuracy of LLMs with and without the context retrieval components proposed, when evaluated on multi-choice medical QA.",
        "table": "S4.T8.1",
        "footnotes": [],
        "references": [
            "For each model in this list, we conduct the same evaluation, with and without the context retrieval setup. Results, shown in Table  8 , indicate a unanimous boost in performance in all datasets and models. The gains are generalized but non-homogeneous. These are higher on less-performing models, clearly influenced by both the smaller model size and the larger room for improvement.",
            "Table  9  presents a consolidated leaderboard, sorted by average performance across available datasets, by unifying the results shown in Table  8  with those reported for private models. This comparison reveals that our optimized CR configuration not only boosts the accuracy of open-source models but also enables them to achieve performance levels comparable to much larger private models. In particular, when augmented with our CR system, the Llama-3.1-70B and Qwen-2-72B models demonstrate competitive performance with Googles MedPalm-2 or OpenAIs GPT4."
        ]
    },
    "id_table_9": {
        "caption": "Table 9.  Accuracy of top performing models in medical MCQA, sorted by average performance. MP: Medprompt. CR: Context Retrieval. ER: Ensemble Refinement (Googles custom prompt technique). 5S: Five-shot Underlined values are reported by others  (Nori et al . ,  2023 ; Singhal et al . ,  2023 ) .",
        "table": "S4.T9.1",
        "footnotes": [],
        "references": [
            "Table  9  presents a consolidated leaderboard, sorted by average performance across available datasets, by unifying the results shown in Table  8  with those reported for private models. This comparison reveals that our optimized CR configuration not only boosts the accuracy of open-source models but also enables them to achieve performance levels comparable to much larger private models. In particular, when augmented with our CR system, the Llama-3.1-70B and Qwen-2-72B models demonstrate competitive performance with Googles MedPalm-2 or OpenAIs GPT4."
        ]
    },
    "id_table_10": {
        "caption": "Table 10.  Baseline results of Llama-3.1-8B-Instruct for the MedQA dataset",
        "table": "S5.T10.1",
        "footnotes": [],
        "references": [
            "Our preliminary analysis revealed a significant performance gap when transitioning from multiple-choice to open-ended formats. Table   10  illustrates this disparity, showing a substantial decrease in accuracy of 10% for the Llama-3.1-8B-Instruct model on the MedQA dataset when transitioning from multiple choice questions (MCQs) to open-ended (OE) questions. Surprisingly, the incorporation of Chain-of-Thought (CoT) and Tree-of-Thought (ToT) reasoning did not improve performance for open-ended questions, contrary to their effectiveness in other domains."
        ]
    },
    "id_table_11": {
        "caption": "Table 11.  OM-ER accuracy results of Llama-3.1-8B-Instruct in MedQA dataset using the CoT and ToT database.  N N N italic_N  represents the number of ensembles.",
        "table": "S5.T11.3",
        "footnotes": [],
        "references": [
            "Table  11  presents the results of our experiments of OM-ER using the CoT and ToT databases. We see that both CoT and ToT databases show improvement over the baseline open-ended performance. ToT generally outperforms CoT, with the best result (60.02%) achieved using 20 ensembles."
        ]
    },
    "id_table_12": {
        "caption": "Table 12.  OM-SR accuracy results of Llama-3.1-8B-Instruct in MedQA dataset using the CoT and ToT database.  N N N italic_N  represents the number of refinement iterations.",
        "table": "S5.T12.3",
        "footnotes": [],
        "references": [
            "Table   12  shows the results for OM-SR with the CoT and ToT databases. We see that the OM-SR system shows more consistent improvement over the baseline compared to the OM-ER system. The CoT database performance peaks at N=15 (60.88%), outperforming ToT in this configuration. The ToT database on the other hand shows less variation across different N values, suggesting more stable performance."
        ]
    },
    "id_table_13": {
        "caption": "Table 13.  Computational cost of the experiments conducted in this study. Time is expressed in hours, power consumption in KWh and carbon footprint in kg/CO2.",
        "table": "S6.T13.1",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": [
        "We consider reasoning to be beyond a text task"
    ]
}