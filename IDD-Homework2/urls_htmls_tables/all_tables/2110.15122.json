{
    "PAPER'S NUMBER OF TABLES": 7,
    "S2.T1": {
        "caption": "Table 1: Comparison of CAFE with state-of-the-art data leakage attack methods in FL.",
        "table": "<div id=\"S2.T1.9\" class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:433.6pt;height:253pt;vertical-align:-0.5pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-234.8pt,136.7pt) scale(0.48007,0.48007) ;\">\n<p id=\"S2.T1.9.9\" class=\"ltx_p\"><span id=\"S2.T1.9.9.9\" class=\"ltx_text\">\n<span id=\"S2.T1.9.9.9.9\" class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:903.2pt;height:527pt;vertical-align:-1.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span id=\"S2.T1.9.9.9.9.9\" class=\"ltx_p\"><span id=\"S2.T1.9.9.9.9.9.9\" class=\"ltx_text\">\n<span id=\"S2.T1.9.9.9.9.9.9.9\" class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_thead\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span id=\"S2.T1.9.9.9.9.9.9.9.10.1.1.1\" class=\"ltx_text\"></span> <span id=\"S2.T1.9.9.9.9.9.9.9.10.1.1.2\" class=\"ltx_text\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.1.2.1\" class=\"ltx_tabular ltx_align_bottom\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S2.T1.9.9.9.9.9.9.9.10.1.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Method</span></span></span>\n</span></span><span id=\"S2.T1.9.9.9.9.9.9.9.10.1.1.3\" class=\"ltx_text\"></span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span id=\"S2.T1.9.9.9.9.9.9.9.10.1.2.1\" class=\"ltx_text\"></span> <span id=\"S2.T1.9.9.9.9.9.9.9.10.1.2.2\" class=\"ltx_text\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.2.2.1\" class=\"ltx_tabular ltx_align_bottom\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.2.2.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.2.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S2.T1.9.9.9.9.9.9.9.10.1.2.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Optimization</span></span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.2.2.1.2\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.2.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S2.T1.9.9.9.9.9.9.9.10.1.2.2.1.2.1.1\" class=\"ltx_text ltx_font_bold\">terms</span></span></span>\n</span></span><span id=\"S2.T1.9.9.9.9.9.9.9.10.1.2.3\" class=\"ltx_text\"></span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span id=\"S2.T1.9.9.9.9.9.9.9.10.1.3.1\" class=\"ltx_text\"></span> <span id=\"S2.T1.9.9.9.9.9.9.9.10.1.3.2\" class=\"ltx_text\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.3.2.1\" class=\"ltx_tabular ltx_align_bottom\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.3.2.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.3.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S2.T1.9.9.9.9.9.9.9.10.1.3.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Reported maximal</span></span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.3.2.1.2\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.3.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S2.T1.9.9.9.9.9.9.9.10.1.3.2.1.2.1.1\" class=\"ltx_text ltx_font_bold\">batch size</span></span></span>\n</span></span><span id=\"S2.T1.9.9.9.9.9.9.9.10.1.3.3\" class=\"ltx_text\"></span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span id=\"S2.T1.9.9.9.9.9.9.9.10.1.4.1\" class=\"ltx_text\"></span> <span id=\"S2.T1.9.9.9.9.9.9.9.10.1.4.2\" class=\"ltx_text\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.4.2.1\" class=\"ltx_tabular ltx_align_bottom\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.4.2.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.4.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S2.T1.9.9.9.9.9.9.9.10.1.4.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Training while</span></span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.4.2.1.2\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.4.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S2.T1.9.9.9.9.9.9.9.10.1.4.2.1.2.1.1\" class=\"ltx_text ltx_font_bold\">attacking</span></span></span>\n</span></span><span id=\"S2.T1.9.9.9.9.9.9.9.10.1.4.3\" class=\"ltx_text\"></span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span id=\"S2.T1.9.9.9.9.9.9.9.10.1.5.1\" class=\"ltx_text\"></span> <span id=\"S2.T1.9.9.9.9.9.9.9.10.1.5.2\" class=\"ltx_text\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.5.2.1\" class=\"ltx_tabular ltx_align_bottom\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.5.2.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.5.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S2.T1.9.9.9.9.9.9.9.10.1.5.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Theoretical</span></span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.5.2.1.2\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.5.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S2.T1.9.9.9.9.9.9.9.10.1.5.2.1.2.1.1\" class=\"ltx_text ltx_font_bold\">guarantee</span></span></span>\n</span></span><span id=\"S2.T1.9.9.9.9.9.9.9.10.1.5.3\" class=\"ltx_text\"></span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span id=\"S2.T1.9.9.9.9.9.9.9.10.1.6.1\" class=\"ltx_text\"></span> <span id=\"S2.T1.9.9.9.9.9.9.9.10.1.6.2\" class=\"ltx_text\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.6.2.1\" class=\"ltx_tabular ltx_align_bottom\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.6.2.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.6.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S2.T1.9.9.9.9.9.9.9.10.1.6.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Additional information</span></span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.6.2.1.2\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.10.1.6.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S2.T1.9.9.9.9.9.9.9.10.1.6.2.1.2.1.1\" class=\"ltx_text ltx_font_bold\">other than gradients</span></span></span>\n</span></span><span id=\"S2.T1.9.9.9.9.9.9.9.10.1.6.3\" class=\"ltx_text\"></span></span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span id=\"S2.T1.1.1.1.1.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.1.1.1.1.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">DLG <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib33\" title=\"\" class=\"ltx_ref\">33</a>]</cite></span>\n<span id=\"S2.T1.1.1.1.1.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S2.T1.1.1.1.1.1.1.1.1.1.2\" class=\"ltx_text\"></span> <span id=\"S2.T1.1.1.1.1.1.1.1.1.1.1\" class=\"ltx_text\">\n<span id=\"S2.T1.1.1.1.1.1.1.1.1.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T1.1.1.1.1.1.1.1.1.1.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.1.1.1.1.1.1.1.1.1.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><math id=\"S2.T1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\ell_{2}\" display=\"inline\"><semantics id=\"S2.T1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a\"><msub id=\"S2.T1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1\" xref=\"S2.T1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml\"><mi mathvariant=\"normal\" id=\"S2.T1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.2\" xref=\"S2.T1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml\">ℓ</mi><mn id=\"S2.T1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.3\" xref=\"S2.T1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml\">2</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b\"><apply id=\"S2.T1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml\" xref=\"S2.T1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.T1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S2.T1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S2.T1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml\" xref=\"S2.T1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.2\">ℓ</ci><cn type=\"integer\" id=\"S2.T1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml\" xref=\"S2.T1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c\">\\ell_{2}</annotation></semantics></math> distance between</span></span>\n<span id=\"S2.T1.1.1.1.1.1.1.1.1.1.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S2.T1.1.1.1.1.1.1.1.1.1.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">real and fake gradients</span></span>\n</span></span><span id=\"S2.T1.1.1.1.1.1.1.1.1.1.3\" class=\"ltx_text\"></span></span>\n<span id=\"S2.T1.1.1.1.1.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">8</span>\n<span id=\"S2.T1.1.1.1.1.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">No</span>\n<span id=\"S2.T1.1.1.1.1.1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S2.T1.1.1.1.1.1.1.1.1.5.1\" class=\"ltx_text\"></span> <span id=\"S2.T1.1.1.1.1.1.1.1.1.5.2\" class=\"ltx_text\">\n<span id=\"S2.T1.1.1.1.1.1.1.1.1.5.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T1.1.1.1.1.1.1.1.1.5.2.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.1.1.1.1.1.1.1.1.5.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">No</span></span>\n</span></span><span id=\"S2.T1.1.1.1.1.1.1.1.1.5.3\" class=\"ltx_text\"></span></span>\n<span id=\"S2.T1.1.1.1.1.1.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">No</span></span>\n<span id=\"S2.T1.2.2.2.2.2.2.2.2\" class=\"ltx_tr\">\n<span id=\"S2.T1.2.2.2.2.2.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">iDLG <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib31\" title=\"\" class=\"ltx_ref\">31</a>]</cite></span>\n<span id=\"S2.T1.2.2.2.2.2.2.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S2.T1.2.2.2.2.2.2.2.2.1.2\" class=\"ltx_text\"></span> <span id=\"S2.T1.2.2.2.2.2.2.2.2.1.1\" class=\"ltx_text\">\n<span id=\"S2.T1.2.2.2.2.2.2.2.2.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T1.2.2.2.2.2.2.2.2.1.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.2.2.2.2.2.2.2.2.1.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><math id=\"S2.T1.2.2.2.2.2.2.2.2.1.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\ell_{2}\" display=\"inline\"><semantics id=\"S2.T1.2.2.2.2.2.2.2.2.1.1.1.1.1.1.m1.1a\"><msub id=\"S2.T1.2.2.2.2.2.2.2.2.1.1.1.1.1.1.m1.1.1\" xref=\"S2.T1.2.2.2.2.2.2.2.2.1.1.1.1.1.1.m1.1.1.cmml\"><mi mathvariant=\"normal\" id=\"S2.T1.2.2.2.2.2.2.2.2.1.1.1.1.1.1.m1.1.1.2\" xref=\"S2.T1.2.2.2.2.2.2.2.2.1.1.1.1.1.1.m1.1.1.2.cmml\">ℓ</mi><mn id=\"S2.T1.2.2.2.2.2.2.2.2.1.1.1.1.1.1.m1.1.1.3\" xref=\"S2.T1.2.2.2.2.2.2.2.2.1.1.1.1.1.1.m1.1.1.3.cmml\">2</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.2.2.2.2.2.2.2.2.1.1.1.1.1.1.m1.1b\"><apply id=\"S2.T1.2.2.2.2.2.2.2.2.1.1.1.1.1.1.m1.1.1.cmml\" xref=\"S2.T1.2.2.2.2.2.2.2.2.1.1.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.T1.2.2.2.2.2.2.2.2.1.1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S2.T1.2.2.2.2.2.2.2.2.1.1.1.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S2.T1.2.2.2.2.2.2.2.2.1.1.1.1.1.1.m1.1.1.2.cmml\" xref=\"S2.T1.2.2.2.2.2.2.2.2.1.1.1.1.1.1.m1.1.1.2\">ℓ</ci><cn type=\"integer\" id=\"S2.T1.2.2.2.2.2.2.2.2.1.1.1.1.1.1.m1.1.1.3.cmml\" xref=\"S2.T1.2.2.2.2.2.2.2.2.1.1.1.1.1.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.2.2.2.2.2.2.2.2.1.1.1.1.1.1.m1.1c\">\\ell_{2}</annotation></semantics></math> distance</span></span>\n</span></span><span id=\"S2.T1.2.2.2.2.2.2.2.2.1.3\" class=\"ltx_text\"></span></span>\n<span id=\"S2.T1.2.2.2.2.2.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">8</span>\n<span id=\"S2.T1.2.2.2.2.2.2.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">No</span>\n<span id=\"S2.T1.2.2.2.2.2.2.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S2.T1.2.2.2.2.2.2.2.2.5.1\" class=\"ltx_text\"></span> <span id=\"S2.T1.2.2.2.2.2.2.2.2.5.2\" class=\"ltx_text\">\n<span id=\"S2.T1.2.2.2.2.2.2.2.2.5.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T1.2.2.2.2.2.2.2.2.5.2.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.2.2.2.2.2.2.2.2.5.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Yes</span></span>\n</span></span><span id=\"S2.T1.2.2.2.2.2.2.2.2.5.3\" class=\"ltx_text\"></span></span>\n<span id=\"S2.T1.2.2.2.2.2.2.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\">No</span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.11.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.11.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">Inverting Gradients <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>]</cite></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.11.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S2.T1.9.9.9.9.9.9.9.11.1.2.1\" class=\"ltx_text\"></span> <span id=\"S2.T1.9.9.9.9.9.9.9.11.1.2.2\" class=\"ltx_text\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.11.1.2.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.11.1.2.2.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.11.1.2.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Cosine similarity,</span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.11.1.2.2.1.2\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.11.1.2.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">TV norm</span></span>\n</span></span><span id=\"S2.T1.9.9.9.9.9.9.9.11.1.2.3\" class=\"ltx_text\"></span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.11.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S2.T1.9.9.9.9.9.9.9.11.1.3.1\" class=\"ltx_text\"></span> <span id=\"S2.T1.9.9.9.9.9.9.9.11.1.3.2\" class=\"ltx_text\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.11.1.3.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.11.1.3.2.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.11.1.3.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">8</span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.11.1.3.2.1.2\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.11.1.3.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">100 (Mostly</span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.11.1.3.2.1.3\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.11.1.3.2.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">unrecognizable)</span></span>\n</span></span><span id=\"S2.T1.9.9.9.9.9.9.9.11.1.3.3\" class=\"ltx_text\"></span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.11.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">Yes</span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.11.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S2.T1.9.9.9.9.9.9.9.11.1.5.1\" class=\"ltx_text\"></span> <span id=\"S2.T1.9.9.9.9.9.9.9.11.1.5.2\" class=\"ltx_text\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.11.1.5.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.11.1.5.2.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.11.1.5.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Yes</span></span>\n</span></span><span id=\"S2.T1.9.9.9.9.9.9.9.11.1.5.3\" class=\"ltx_text\"></span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.11.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">Number of local updates</span></span>\n<span id=\"S2.T1.3.3.3.3.3.3.3.3\" class=\"ltx_tr\">\n<span id=\"S2.T1.3.3.3.3.3.3.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S2.T1.3.3.3.3.3.3.3.3.2.1\" class=\"ltx_text\"></span> <span id=\"S2.T1.3.3.3.3.3.3.3.3.2.2\" class=\"ltx_text\">\n<span id=\"S2.T1.3.3.3.3.3.3.3.3.2.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T1.3.3.3.3.3.3.3.3.2.2.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.3.3.3.3.3.3.3.3.2.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">A Framework for Evaluating</span></span>\n<span id=\"S2.T1.3.3.3.3.3.3.3.3.2.2.1.2\" class=\"ltx_tr\">\n<span id=\"S2.T1.3.3.3.3.3.3.3.3.2.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Gradient Leakage <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">27</a>]</cite></span></span>\n</span></span><span id=\"S2.T1.3.3.3.3.3.3.3.3.2.3\" class=\"ltx_text\"></span></span>\n<span id=\"S2.T1.3.3.3.3.3.3.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S2.T1.3.3.3.3.3.3.3.3.1.2\" class=\"ltx_text\"></span> <span id=\"S2.T1.3.3.3.3.3.3.3.3.1.1\" class=\"ltx_text\">\n<span id=\"S2.T1.3.3.3.3.3.3.3.3.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T1.3.3.3.3.3.3.3.3.1.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.3.3.3.3.3.3.3.3.1.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><math id=\"S2.T1.3.3.3.3.3.3.3.3.1.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\ell_{2}\" display=\"inline\"><semantics id=\"S2.T1.3.3.3.3.3.3.3.3.1.1.1.1.1.1.m1.1a\"><msub id=\"S2.T1.3.3.3.3.3.3.3.3.1.1.1.1.1.1.m1.1.1\" xref=\"S2.T1.3.3.3.3.3.3.3.3.1.1.1.1.1.1.m1.1.1.cmml\"><mi mathvariant=\"normal\" id=\"S2.T1.3.3.3.3.3.3.3.3.1.1.1.1.1.1.m1.1.1.2\" xref=\"S2.T1.3.3.3.3.3.3.3.3.1.1.1.1.1.1.m1.1.1.2.cmml\">ℓ</mi><mn id=\"S2.T1.3.3.3.3.3.3.3.3.1.1.1.1.1.1.m1.1.1.3\" xref=\"S2.T1.3.3.3.3.3.3.3.3.1.1.1.1.1.1.m1.1.1.3.cmml\">2</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.3.3.3.3.3.3.3.3.1.1.1.1.1.1.m1.1b\"><apply id=\"S2.T1.3.3.3.3.3.3.3.3.1.1.1.1.1.1.m1.1.1.cmml\" xref=\"S2.T1.3.3.3.3.3.3.3.3.1.1.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.T1.3.3.3.3.3.3.3.3.1.1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S2.T1.3.3.3.3.3.3.3.3.1.1.1.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S2.T1.3.3.3.3.3.3.3.3.1.1.1.1.1.1.m1.1.1.2.cmml\" xref=\"S2.T1.3.3.3.3.3.3.3.3.1.1.1.1.1.1.m1.1.1.2\">ℓ</ci><cn type=\"integer\" id=\"S2.T1.3.3.3.3.3.3.3.3.1.1.1.1.1.1.m1.1.1.3.cmml\" xref=\"S2.T1.3.3.3.3.3.3.3.3.1.1.1.1.1.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.3.3.3.3.3.3.3.3.1.1.1.1.1.1.m1.1c\">\\ell_{2}</annotation></semantics></math> distance,</span></span>\n<span id=\"S2.T1.3.3.3.3.3.3.3.3.1.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S2.T1.3.3.3.3.3.3.3.3.1.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">label based regualrizer</span></span>\n</span></span><span id=\"S2.T1.3.3.3.3.3.3.3.3.1.3\" class=\"ltx_text\"></span></span>\n<span id=\"S2.T1.3.3.3.3.3.3.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S2.T1.3.3.3.3.3.3.3.3.3.1\" class=\"ltx_text\"></span> <span id=\"S2.T1.3.3.3.3.3.3.3.3.3.2\" class=\"ltx_text\">\n<span id=\"S2.T1.3.3.3.3.3.3.3.3.3.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T1.3.3.3.3.3.3.3.3.3.2.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.3.3.3.3.3.3.3.3.3.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">8</span></span>\n</span></span><span id=\"S2.T1.3.3.3.3.3.3.3.3.3.3\" class=\"ltx_text\"></span></span>\n<span id=\"S2.T1.3.3.3.3.3.3.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">No</span>\n<span id=\"S2.T1.3.3.3.3.3.3.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S2.T1.3.3.3.3.3.3.3.3.5.1\" class=\"ltx_text\"></span> <span id=\"S2.T1.3.3.3.3.3.3.3.3.5.2\" class=\"ltx_text\">\n<span id=\"S2.T1.3.3.3.3.3.3.3.3.5.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T1.3.3.3.3.3.3.3.3.5.2.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.3.3.3.3.3.3.3.3.5.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Yes</span></span>\n</span></span><span id=\"S2.T1.3.3.3.3.3.3.3.3.5.3\" class=\"ltx_text\"></span></span>\n<span id=\"S2.T1.3.3.3.3.3.3.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\">No</span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.12.2\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.12.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\">SAPAG <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">26</a>]</cite></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.12.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S2.T1.9.9.9.9.9.9.9.12.2.2.1\" class=\"ltx_text\"></span> <span id=\"S2.T1.9.9.9.9.9.9.9.12.2.2.2\" class=\"ltx_text\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.12.2.2.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.12.2.2.2.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.12.2.2.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Gaussian kernel</span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.12.2.2.2.1.2\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.12.2.2.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">based funciton</span></span>\n</span></span><span id=\"S2.T1.9.9.9.9.9.9.9.12.2.2.3\" class=\"ltx_text\"></span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.12.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S2.T1.9.9.9.9.9.9.9.12.2.3.1\" class=\"ltx_text\"></span> <span id=\"S2.T1.9.9.9.9.9.9.9.12.2.3.2\" class=\"ltx_text\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.12.2.3.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.12.2.3.2.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.12.2.3.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">8</span></span>\n</span></span><span id=\"S2.T1.9.9.9.9.9.9.9.12.2.3.3\" class=\"ltx_text\"></span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.12.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">No</span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.12.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S2.T1.9.9.9.9.9.9.9.12.2.5.1\" class=\"ltx_text\"></span> <span id=\"S2.T1.9.9.9.9.9.9.9.12.2.5.2\" class=\"ltx_text\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.12.2.5.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.12.2.5.2.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.12.2.5.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">No</span></span>\n</span></span><span id=\"S2.T1.9.9.9.9.9.9.9.12.2.5.3\" class=\"ltx_text\"></span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.12.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\">No</span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.13.3\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.13.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\">R-GAP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib32\" title=\"\" class=\"ltx_ref\">32</a>]</cite></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.13.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S2.T1.9.9.9.9.9.9.9.13.3.2.1\" class=\"ltx_text\"></span> <span id=\"S2.T1.9.9.9.9.9.9.9.13.3.2.2\" class=\"ltx_text\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.13.3.2.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.13.3.2.2.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.13.3.2.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">recursive gradient</span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.13.3.2.2.1.2\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.13.3.2.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">loss</span></span>\n</span></span><span id=\"S2.T1.9.9.9.9.9.9.9.13.3.2.3\" class=\"ltx_text\"></span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.13.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S2.T1.9.9.9.9.9.9.9.13.3.3.1\" class=\"ltx_text\"></span> <span id=\"S2.T1.9.9.9.9.9.9.9.13.3.3.2\" class=\"ltx_text\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.13.3.3.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.13.3.3.2.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.13.3.3.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">5</span></span>\n</span></span><span id=\"S2.T1.9.9.9.9.9.9.9.13.3.3.3\" class=\"ltx_text\"></span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.13.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">No</span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.13.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S2.T1.9.9.9.9.9.9.9.13.3.5.1\" class=\"ltx_text\"></span> <span id=\"S2.T1.9.9.9.9.9.9.9.13.3.5.2\" class=\"ltx_text\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.13.3.5.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.13.3.5.2.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.13.3.5.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Yes</span></span>\n</span></span><span id=\"S2.T1.9.9.9.9.9.9.9.13.3.5.3\" class=\"ltx_text\"></span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.13.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S2.T1.9.9.9.9.9.9.9.13.3.6.1\" class=\"ltx_text\"></span> <span id=\"S2.T1.9.9.9.9.9.9.9.13.3.6.2\" class=\"ltx_text\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.13.3.6.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.13.3.6.2.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.13.3.6.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">The rank of matrix A</span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.13.3.6.2.1.2\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.13.3.6.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">defined in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib32\" title=\"\" class=\"ltx_ref\">32</a>]</cite></span></span>\n</span></span><span id=\"S2.T1.9.9.9.9.9.9.9.13.3.6.3\" class=\"ltx_text\"></span></span></span>\n<span id=\"S2.T1.5.5.5.5.5.5.5.5\" class=\"ltx_tr\">\n<span id=\"S2.T1.5.5.5.5.5.5.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\">Theory oriented <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">22</a>]</cite></span>\n<span id=\"S2.T1.5.5.5.5.5.5.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S2.T1.5.5.5.5.5.5.5.5.2.3\" class=\"ltx_text\"></span> <span id=\"S2.T1.5.5.5.5.5.5.5.5.2.2\" class=\"ltx_text\">\n<span id=\"S2.T1.5.5.5.5.5.5.5.5.2.2.2.2\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T1.4.4.4.4.4.4.4.4.1.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.4.4.4.4.4.4.4.4.1.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><math id=\"S2.T1.4.4.4.4.4.4.4.4.1.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\ell_{2}\" display=\"inline\"><semantics id=\"S2.T1.4.4.4.4.4.4.4.4.1.1.1.1.1.1.m1.1a\"><msub id=\"S2.T1.4.4.4.4.4.4.4.4.1.1.1.1.1.1.m1.1.1\" xref=\"S2.T1.4.4.4.4.4.4.4.4.1.1.1.1.1.1.m1.1.1.cmml\"><mi mathvariant=\"normal\" id=\"S2.T1.4.4.4.4.4.4.4.4.1.1.1.1.1.1.m1.1.1.2\" xref=\"S2.T1.4.4.4.4.4.4.4.4.1.1.1.1.1.1.m1.1.1.2.cmml\">ℓ</mi><mn id=\"S2.T1.4.4.4.4.4.4.4.4.1.1.1.1.1.1.m1.1.1.3\" xref=\"S2.T1.4.4.4.4.4.4.4.4.1.1.1.1.1.1.m1.1.1.3.cmml\">2</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.4.4.4.4.4.4.4.4.1.1.1.1.1.1.m1.1b\"><apply id=\"S2.T1.4.4.4.4.4.4.4.4.1.1.1.1.1.1.m1.1.1.cmml\" xref=\"S2.T1.4.4.4.4.4.4.4.4.1.1.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.T1.4.4.4.4.4.4.4.4.1.1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S2.T1.4.4.4.4.4.4.4.4.1.1.1.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S2.T1.4.4.4.4.4.4.4.4.1.1.1.1.1.1.m1.1.1.2.cmml\" xref=\"S2.T1.4.4.4.4.4.4.4.4.1.1.1.1.1.1.m1.1.1.2\">ℓ</ci><cn type=\"integer\" id=\"S2.T1.4.4.4.4.4.4.4.4.1.1.1.1.1.1.m1.1.1.3.cmml\" xref=\"S2.T1.4.4.4.4.4.4.4.4.1.1.1.1.1.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.4.4.4.4.4.4.4.4.1.1.1.1.1.1.m1.1c\">\\ell_{2}</annotation></semantics></math> distance,</span></span>\n<span id=\"S2.T1.5.5.5.5.5.5.5.5.2.2.2.2.2\" class=\"ltx_tr\">\n<span id=\"S2.T1.5.5.5.5.5.5.5.5.2.2.2.2.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><math id=\"S2.T1.5.5.5.5.5.5.5.5.2.2.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\ell_{1}\" display=\"inline\"><semantics id=\"S2.T1.5.5.5.5.5.5.5.5.2.2.2.2.2.1.m1.1a\"><msub id=\"S2.T1.5.5.5.5.5.5.5.5.2.2.2.2.2.1.m1.1.1\" xref=\"S2.T1.5.5.5.5.5.5.5.5.2.2.2.2.2.1.m1.1.1.cmml\"><mi mathvariant=\"normal\" id=\"S2.T1.5.5.5.5.5.5.5.5.2.2.2.2.2.1.m1.1.1.2\" xref=\"S2.T1.5.5.5.5.5.5.5.5.2.2.2.2.2.1.m1.1.1.2.cmml\">ℓ</mi><mn id=\"S2.T1.5.5.5.5.5.5.5.5.2.2.2.2.2.1.m1.1.1.3\" xref=\"S2.T1.5.5.5.5.5.5.5.5.2.2.2.2.2.1.m1.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.5.5.5.5.5.5.5.5.2.2.2.2.2.1.m1.1b\"><apply id=\"S2.T1.5.5.5.5.5.5.5.5.2.2.2.2.2.1.m1.1.1.cmml\" xref=\"S2.T1.5.5.5.5.5.5.5.5.2.2.2.2.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.T1.5.5.5.5.5.5.5.5.2.2.2.2.2.1.m1.1.1.1.cmml\" xref=\"S2.T1.5.5.5.5.5.5.5.5.2.2.2.2.2.1.m1.1.1\">subscript</csymbol><ci id=\"S2.T1.5.5.5.5.5.5.5.5.2.2.2.2.2.1.m1.1.1.2.cmml\" xref=\"S2.T1.5.5.5.5.5.5.5.5.2.2.2.2.2.1.m1.1.1.2\">ℓ</ci><cn type=\"integer\" id=\"S2.T1.5.5.5.5.5.5.5.5.2.2.2.2.2.1.m1.1.1.3.cmml\" xref=\"S2.T1.5.5.5.5.5.5.5.5.2.2.2.2.2.1.m1.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.5.5.5.5.5.5.5.5.2.2.2.2.2.1.m1.1c\">\\ell_{1}</annotation></semantics></math> distances of the</span></span>\n<span id=\"S2.T1.5.5.5.5.5.5.5.5.2.2.2.2.3\" class=\"ltx_tr\">\n<span id=\"S2.T1.5.5.5.5.5.5.5.5.2.2.2.2.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">recovered feature map</span></span>\n</span></span><span id=\"S2.T1.5.5.5.5.5.5.5.5.2.4\" class=\"ltx_text\"></span></span>\n<span id=\"S2.T1.5.5.5.5.5.5.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S2.T1.5.5.5.5.5.5.5.5.4.1\" class=\"ltx_text\"></span> <span id=\"S2.T1.5.5.5.5.5.5.5.5.4.2\" class=\"ltx_text\">\n<span id=\"S2.T1.5.5.5.5.5.5.5.5.4.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T1.5.5.5.5.5.5.5.5.4.2.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.5.5.5.5.5.5.5.5.4.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">32</span></span>\n</span></span><span id=\"S2.T1.5.5.5.5.5.5.5.5.4.3\" class=\"ltx_text\"></span></span>\n<span id=\"S2.T1.5.5.5.5.5.5.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_t\">No</span>\n<span id=\"S2.T1.5.5.5.5.5.5.5.5.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S2.T1.5.5.5.5.5.5.5.5.6.1\" class=\"ltx_text\"></span> <span id=\"S2.T1.5.5.5.5.5.5.5.5.6.2\" class=\"ltx_text\">\n<span id=\"S2.T1.5.5.5.5.5.5.5.5.6.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T1.5.5.5.5.5.5.5.5.6.2.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.5.5.5.5.5.5.5.5.6.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Yes</span></span>\n</span></span><span id=\"S2.T1.5.5.5.5.5.5.5.5.6.3\" class=\"ltx_text\"></span></span>\n<span id=\"S2.T1.5.5.5.5.5.5.5.5.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S2.T1.5.5.5.5.5.5.5.5.7.1\" class=\"ltx_text\"></span> <span id=\"S2.T1.5.5.5.5.5.5.5.5.7.2\" class=\"ltx_text\">\n<span id=\"S2.T1.5.5.5.5.5.5.5.5.7.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T1.5.5.5.5.5.5.5.5.7.2.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.5.5.5.5.5.5.5.5.7.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Number of Exclusive</span></span>\n<span id=\"S2.T1.5.5.5.5.5.5.5.5.7.2.1.2\" class=\"ltx_tr\">\n<span id=\"S2.T1.5.5.5.5.5.5.5.5.7.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">activated neurons</span></span>\n</span></span><span id=\"S2.T1.5.5.5.5.5.5.5.5.7.3\" class=\"ltx_text\"></span></span></span>\n<span id=\"S2.T1.7.7.7.7.7.7.7.7\" class=\"ltx_tr\">\n<span id=\"S2.T1.7.7.7.7.7.7.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_t\">GradInversion<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">30</a>]</cite></span>\n<span id=\"S2.T1.7.7.7.7.7.7.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S2.T1.7.7.7.7.7.7.7.7.4.1\" class=\"ltx_text\"></span> <span id=\"S2.T1.7.7.7.7.7.7.7.7.4.2\" class=\"ltx_text\">\n<span id=\"S2.T1.7.7.7.7.7.7.7.7.4.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T1.7.7.7.7.7.7.7.7.4.2.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.7.7.7.7.7.7.7.7.4.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Fidelity regularizers,</span></span>\n<span id=\"S2.T1.7.7.7.7.7.7.7.7.4.2.1.2\" class=\"ltx_tr\">\n<span id=\"S2.T1.7.7.7.7.7.7.7.7.4.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Group consistency</span></span>\n<span id=\"S2.T1.7.7.7.7.7.7.7.7.4.2.1.3\" class=\"ltx_tr\">\n<span id=\"S2.T1.7.7.7.7.7.7.7.7.4.2.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">regularizers</span></span>\n</span></span><span id=\"S2.T1.7.7.7.7.7.7.7.7.4.3\" class=\"ltx_text\"></span></span>\n<span id=\"S2.T1.7.7.7.7.7.7.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_t\">48</span>\n<span id=\"S2.T1.7.7.7.7.7.7.7.7.6\" class=\"ltx_td ltx_align_center ltx_border_t\">No</span>\n<span id=\"S2.T1.7.7.7.7.7.7.7.7.7\" class=\"ltx_td ltx_align_center ltx_border_t\">No</span>\n<span id=\"S2.T1.7.7.7.7.7.7.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S2.T1.7.7.7.7.7.7.7.7.2.3\" class=\"ltx_text\"></span> <span id=\"S2.T1.7.7.7.7.7.7.7.7.2.2\" class=\"ltx_text\">\n<span id=\"S2.T1.7.7.7.7.7.7.7.7.2.2.2.2\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T1.6.6.6.6.6.6.6.6.1.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.6.6.6.6.6.6.6.6.1.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Batch size <math id=\"S2.T1.6.6.6.6.6.6.6.6.1.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\ll\" display=\"inline\"><semantics id=\"S2.T1.6.6.6.6.6.6.6.6.1.1.1.1.1.1.m1.1a\"><mo id=\"S2.T1.6.6.6.6.6.6.6.6.1.1.1.1.1.1.m1.1.1\" xref=\"S2.T1.6.6.6.6.6.6.6.6.1.1.1.1.1.1.m1.1.1.cmml\">≪</mo><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.6.6.6.6.6.6.6.6.1.1.1.1.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S2.T1.6.6.6.6.6.6.6.6.1.1.1.1.1.1.m1.1.1.cmml\" xref=\"S2.T1.6.6.6.6.6.6.6.6.1.1.1.1.1.1.m1.1.1\">much-less-than</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.6.6.6.6.6.6.6.6.1.1.1.1.1.1.m1.1c\">\\ll</annotation></semantics></math> number of classes</span></span>\n<span id=\"S2.T1.7.7.7.7.7.7.7.7.2.2.2.2.2\" class=\"ltx_tr\">\n<span id=\"S2.T1.7.7.7.7.7.7.7.7.2.2.2.2.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><math id=\"S2.T1.7.7.7.7.7.7.7.7.2.2.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\&amp;\" display=\"inline\"><semantics id=\"S2.T1.7.7.7.7.7.7.7.7.2.2.2.2.2.1.m1.1a\"><mo id=\"S2.T1.7.7.7.7.7.7.7.7.2.2.2.2.2.1.m1.1.1\" xref=\"S2.T1.7.7.7.7.7.7.7.7.2.2.2.2.2.1.m1.1.1.cmml\">&amp;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.7.7.7.7.7.7.7.7.2.2.2.2.2.1.m1.1b\"><and id=\"S2.T1.7.7.7.7.7.7.7.7.2.2.2.2.2.1.m1.1.1.cmml\" xref=\"S2.T1.7.7.7.7.7.7.7.7.2.2.2.2.2.1.m1.1.1\"></and></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.7.7.7.7.7.7.7.7.2.2.2.2.2.1.m1.1c\">\\&amp;</annotation></semantics></math> Non repeating labels in a batch</span></span>\n</span></span><span id=\"S2.T1.7.7.7.7.7.7.7.7.2.4\" class=\"ltx_text\"></span></span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.9\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">CAFE (ours)</span>\n<span id=\"S2.T1.8.8.8.8.8.8.8.8.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S2.T1.8.8.8.8.8.8.8.8.1.2\" class=\"ltx_text\"></span> <span id=\"S2.T1.8.8.8.8.8.8.8.8.1.1\" class=\"ltx_text\">\n<span id=\"S2.T1.8.8.8.8.8.8.8.8.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T1.8.8.8.8.8.8.8.8.1.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.8.8.8.8.8.8.8.8.1.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><math id=\"S2.T1.8.8.8.8.8.8.8.8.1.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\ell_{2}\" display=\"inline\"><semantics id=\"S2.T1.8.8.8.8.8.8.8.8.1.1.1.1.1.1.m1.1a\"><msub id=\"S2.T1.8.8.8.8.8.8.8.8.1.1.1.1.1.1.m1.1.1\" xref=\"S2.T1.8.8.8.8.8.8.8.8.1.1.1.1.1.1.m1.1.1.cmml\"><mi mathvariant=\"normal\" id=\"S2.T1.8.8.8.8.8.8.8.8.1.1.1.1.1.1.m1.1.1.2\" xref=\"S2.T1.8.8.8.8.8.8.8.8.1.1.1.1.1.1.m1.1.1.2.cmml\">ℓ</mi><mn id=\"S2.T1.8.8.8.8.8.8.8.8.1.1.1.1.1.1.m1.1.1.3\" xref=\"S2.T1.8.8.8.8.8.8.8.8.1.1.1.1.1.1.m1.1.1.3.cmml\">2</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.8.8.8.8.8.8.8.8.1.1.1.1.1.1.m1.1b\"><apply id=\"S2.T1.8.8.8.8.8.8.8.8.1.1.1.1.1.1.m1.1.1.cmml\" xref=\"S2.T1.8.8.8.8.8.8.8.8.1.1.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.T1.8.8.8.8.8.8.8.8.1.1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S2.T1.8.8.8.8.8.8.8.8.1.1.1.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S2.T1.8.8.8.8.8.8.8.8.1.1.1.1.1.1.m1.1.1.2.cmml\" xref=\"S2.T1.8.8.8.8.8.8.8.8.1.1.1.1.1.1.m1.1.1.2\">ℓ</ci><cn type=\"integer\" id=\"S2.T1.8.8.8.8.8.8.8.8.1.1.1.1.1.1.m1.1.1.3.cmml\" xref=\"S2.T1.8.8.8.8.8.8.8.8.1.1.1.1.1.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.8.8.8.8.8.8.8.8.1.1.1.1.1.1.m1.1c\">\\ell_{2}</annotation></semantics></math> distance,</span></span>\n<span id=\"S2.T1.8.8.8.8.8.8.8.8.1.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S2.T1.8.8.8.8.8.8.8.8.1.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">TV norm,</span></span>\n<span id=\"S2.T1.8.8.8.8.8.8.8.8.1.1.1.1.3\" class=\"ltx_tr\">\n<span id=\"S2.T1.8.8.8.8.8.8.8.8.1.1.1.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Internal representation</span></span>\n<span id=\"S2.T1.8.8.8.8.8.8.8.8.1.1.1.1.4\" class=\"ltx_tr\">\n<span id=\"S2.T1.8.8.8.8.8.8.8.8.1.1.1.1.4.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">norm</span></span>\n</span></span><span id=\"S2.T1.8.8.8.8.8.8.8.8.1.3\" class=\"ltx_text\"></span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S2.T1.9.9.9.9.9.9.9.9.2.2\" class=\"ltx_text\"></span> <span id=\"S2.T1.9.9.9.9.9.9.9.9.2.1\" class=\"ltx_text\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.9.2.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.9.2.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.9.2.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><math id=\"S2.T1.9.9.9.9.9.9.9.9.2.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"100\" display=\"inline\"><semantics id=\"S2.T1.9.9.9.9.9.9.9.9.2.1.1.1.1.1.m1.1a\"><mn id=\"S2.T1.9.9.9.9.9.9.9.9.2.1.1.1.1.1.m1.1.1\" xref=\"S2.T1.9.9.9.9.9.9.9.9.2.1.1.1.1.1.m1.1.1.cmml\">100</mn><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.9.9.9.9.9.9.9.9.2.1.1.1.1.1.m1.1b\"><cn type=\"integer\" id=\"S2.T1.9.9.9.9.9.9.9.9.2.1.1.1.1.1.m1.1.1.cmml\" xref=\"S2.T1.9.9.9.9.9.9.9.9.2.1.1.1.1.1.m1.1.1\">100</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.9.9.9.9.9.9.9.9.2.1.1.1.1.1.m1.1c\">100</annotation></semantics></math></span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.9.2.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.9.2.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">(our hardware limit)</span></span>\n</span></span><span id=\"S2.T1.9.9.9.9.9.9.9.9.2.3\" class=\"ltx_text\"></span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">Yes</span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.9.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S2.T1.9.9.9.9.9.9.9.9.5.1\" class=\"ltx_text\"></span> <span id=\"S2.T1.9.9.9.9.9.9.9.9.5.2\" class=\"ltx_text\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.9.5.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.9.5.2.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.9.5.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Yes</span></span>\n</span></span><span id=\"S2.T1.9.9.9.9.9.9.9.9.5.3\" class=\"ltx_text\"></span></span>\n<span id=\"S2.T1.9.9.9.9.9.9.9.9.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S2.T1.9.9.9.9.9.9.9.9.6.1\" class=\"ltx_text\"></span> <span id=\"S2.T1.9.9.9.9.9.9.9.9.6.2\" class=\"ltx_text\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.9.6.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.9.6.2.1.1\" class=\"ltx_tr\">\n<span id=\"S2.T1.9.9.9.9.9.9.9.9.6.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Batch indices</span></span>\n</span></span><span id=\"S2.T1.9.9.9.9.9.9.9.9.6.3\" class=\"ltx_text\"></span></span></span>\n</span>\n</span></span></span>\n</span></span></span></p>\n</span></div>\n\n",
        "footnotes": "\n\n\n\n\n\n \n\n\nMethod\n\n \n\n\nOptimization\n\nterms\n\n \n\n\nReported maximal\n\nbatch size\n\n \n\n\nTraining while\n\nattacking\n\n \n\n\nTheoretical\n\nguarantee\n\n \n\n\nAdditional information\n\nother than gradients\n\n\n\n\nDLG [33]\n \n\n\nℓ2subscriptℓ2\\ell_{2} distance between\n\nreal and fake gradients\n\n8\nNo\n \n\n\nNo\n\nNo\n\niDLG [31]\n \n\n\nℓ2subscriptℓ2\\ell_{2} distance\n\n8\nNo\n \n\n\nYes\n\nNo\n\nInverting Gradients [11]\n \n\n\nCosine similarity,\n\nTV norm\n\n \n\n\n8\n\n100 (Mostly\n\nunrecognizable)\n\nYes\n \n\n\nYes\n\nNumber of local updates\n\n \n\n\nA Framework for Evaluating\n\nGradient Leakage [27]\n\n \n\n\nℓ2subscriptℓ2\\ell_{2} distance,\n\nlabel based regualrizer\n\n \n\n\n8\n\nNo\n \n\n\nYes\n\nNo\n\nSAPAG [26]\n \n\n\nGaussian kernel\n\nbased funciton\n\n \n\n\n8\n\nNo\n \n\n\nNo\n\nNo\n\nR-GAP [32]\n \n\n\nrecursive gradient\n\nloss\n\n \n\n\n5\n\nNo\n \n\n\nYes\n\n \n\n\nThe rank of matrix A\n\ndefined in [32]\n\n\nTheory oriented [22]\n \n\n\nℓ2subscriptℓ2\\ell_{2} distance,\n\nℓ1subscriptℓ1\\ell_{1} distances of the\n\nrecovered feature map\n\n \n\n\n32\n\nNo\n \n\n\nYes\n\n \n\n\nNumber of Exclusive\n\nactivated neurons\n\n\nGradInversion[30]\n \n\n\nFidelity regularizers,\n\nGroup consistency\n\nregularizers\n\n48\nNo\nNo\n \n\n\nBatch size ≪much-less-than\\ll number of classes\n\n&\\& Non repeating labels in a batch\n\n\nCAFE (ours)\n \n\n\nℓ2subscriptℓ2\\ell_{2} distance,\n\nTV norm,\n\nInternal representation\n\nnorm\n\n \n\n\n100100100\n\n(our hardware limit)\n\nYes\n \n\n\nYes\n\n \n\n\nBatch indices\n\n\n\n",
        "references": [
            "In the context of data security and AI ethics, the possibility of inferring private user data from the gradients in FL has received growing interests [10, 14, 21], known as the data leakage problems. Previous works have made exploratory efforts on data recovery through gradients. See Section 2 and Table 1 for details. However, existing approaches often have the limitation of scaling up large-batch data recovery and are lacking in theoretical justification on the capability of data recovery, which may give a false sense of security that increasing the data batch size during training can prevent data leakage [31].\nSome recent works provide sufficient conditions for guaranteed data recovery, but the assumptions are overly restrictive and can be sometimes impractical, such as requiring the number of classes to be much larger than the number of recovered data samples [30]."
        ]
    },
    "S3.T3": {
        "caption": "",
        "table": "",
        "footnotes": "\n\n\n\n\n\n\n\nMethod\n\n\n\n\nDataset\n\n\n\n\nPSNR\n\n\nCIFAR-10\nMNIST\nLinnaeus 5\n\nCAFE\n31.8331.83~{}31.83~{}\n43.1543.1543.15\n33.2233.2233.22\n\nDLG\n9.299.299.29\n7.967.967.96\n7.147.147.14\n\nCosine Similarity\n7.387.387.38\n7.847.847.84\n8.318.318.31\n\nSAPAG\n6.076.076.07\n3.863.863.86\n6.746.746.74\n\nBN regularizer\n18.9418.9418.94\n13.3813.3813.38\n8.098.098.09\n\nGC regularizer\n13.6313.6313.63\n9.249.249.24\n12.3212.3212.32\n\n\n\n\n\n\n\n\n\nK𝐾K\n\n\n\n\nDataset\n\n\n\n\nPSNR\n\n\nCIFAR-10\nMNIST\nLinnaeus 5\n\n101010\n30.8330.83~{}30.83~{}\n32.6032.60~{}32.60~{}\n28.0028.00~{}28.00~{}\n\n202020\n35.7035.70~{}35.70~{}\n39.0039.00~{}39.00~{}\n30.5330.53~{}30.53~{}\n\n404040\n31.8331.83~{}31.83~{}\n43.1543.15~{}43.15~{}\n33.2233.22~{}33.22~{}\n\n808080\n36.8736.87~{}36.87~{}\n47.0547.05~{}47.05~{}\n30.4330.43~{}30.43~{}\n\n100100100\n38.9438.94~{}38.94~{}\n47.5047.50~{}47.50~{}\n29.1829.18~{}29.18~{}\n\n",
        "references": [
            "In the context of data security and AI ethics, the possibility of inferring private user data from the gradients in FL has received growing interests [10, 14, 21], known as the data leakage problems. Previous works have made exploratory efforts on data recovery through gradients. See Section 2 and Table 1 for details. However, existing approaches often have the limitation of scaling up large-batch data recovery and are lacking in theoretical justification on the capability of data recovery, which may give a false sense of security that increasing the data batch size during training can prevent data leakage [31].\nSome recent works provide sufficient conditions for guaranteed data recovery, but the assumptions are overly restrictive and can be sometimes impractical, such as requiring the number of classes to be much larger than the number of recovered data samples [30].",
            "Use case of VFL.\nVFL is suitable for cases where multiple data owners share the same data identity but their data differ in feature space. Use cases of VFL appear in finance, e-commerce, and health.\nFor example, in medical industry, test results of the same patient from different medical institutions are required to diagnose whether the patient has a certain disease or not, but institutions tend not to share raw data. Figure 2 gives an example of VFL in medical industry.",
            "In a typical VFL process, the server sends public key to local workers and decides the data indices in each iteration of training and evaluation [7, 29]. During the training process, local workers exchange their intermediate results with others to compute gradients and upload them. Therefore, the server has access to both of the model parameters and their gradients. Since data are vertically partitioned among different workers, for each batch, the server (acting as the attacker) needs to send a data index or data id list to all the local workers to ensure that data with the same id sequence have been selected by each worker [29] and we name this step as data index alignment. Data index alignment turns out to be an inevitable step in the vertical training process, which provides the server (the attacker) an opportunity to control the selected batch data indices.",
            "Theory-driven label inference methods have been proposed in [31] and [27]. However, our attack mainly deals with training data leakage rather than labels. In [22], the authors proposed a sufficient requirement that \"each data sample has at least two exclusively activated neurons at the last but one layer\". However, in our training protocol, the batch size is too large and it is almost impossible to ensure that each selected sample has at least two exclusively activated neurons. In [32], it is assumed that the method will only return a linear combination of the selected training data, which is a very restricted assumption. As the results, we did not compare to those methods in Table 3.",
            "CAFE outperforms these methods both qualitatively (Figure 1) and quantitatively (Table 3). Its PSNR values are always above 303030 at the end of each CAFE attacking process, suggesting high data recovery quality. However, the PSNR of other methods are below 101010 on all the three datasets.",
            "(i) PSNR via Batch size K𝐾K.\nTable 3 shows that the PSNR values always keep above 303030 on CIFAR-10, above 323232 on MNIST and above 282828 on Linnaeus 5 when the batch size K𝐾K increases with fixed number of workers and number of total data points.\nThe result implies that the increasing K𝐾K has almost no influence on data leakage performance of CAFE and it fails to be an effective defense.",
            "(iii) Effect of regularizers.\nTable 5 demonstrates the impact of regularizers. From Figure 6, adjusting the threshold ξ𝜉\\xi prevents images from being over blurred during the reconstruction process. TV norm can eliminate the noisy patterns on the recovered images and increase the PSNR. We also find that the last term in (9), the internal representation norm regularizer, contributes most to the data recovery. In Table 5, CAFE still performs well without the first term (α=0𝛼0\\alpha=0) in (9). The reason is that the internal representation regularizer already allows data to be fully recovered. Notably, CAFE also performs well on MNIST even without the second term (β=0𝛽0\\beta=0) in (9). It is mainly due to that MNIST is a simple dataset that CAFE can successfully recover even without the TV-norm regularizer.",
            "(iv) Nested-loops vs single-loop.\nWe compare both modes of CAFE (Algorithms 3 and 4) on all datasets. In Table 5, the number of iterations is the maximum iterations at each step. For the CAFE (single-loop), if the objective function in step I (7) decreases below 10−9superscript10910^{-9}, we switch to step II. If the objective function in step II (8) decreases below 5×10−95superscript1095\\times 10^{-9}, we switch to step III. When the PSNR value reaches 272727 on CIFAR-10, 303030 on Linnaeus 5, 383838 on MNIST, we stop both algorithms and record the iteration numbers.\nAs shown in Table 5, CAFE single-loop requires fewer number of iterations. Meanwhile, it is difficult to set the loop stopping conditions in the CAFE Nested-loops mode. In particular, V∗superscriptV\\textbf{V}^{*} and 𝐇^∗superscript^𝐇\\hat{\\mathbf{H}}^{*} with low recovery precision may impact the data recovery performance.",
            "(v) Effects of number of workers M𝑀M.\nAlthough data are partitioned on feature space across workers, the dimension of the entire data feature space is fixed and independent of M𝑀M. Therefore, increasing number of workers theoretically does not change the dimension of variables associated with data recovery in (3). In practice, different from HFL, where there could be hundreds of workers, in VFL, the workers are typically financial organizations or companies. Therefore, the number of workers is usually small [13].\nIn Table 6, we compare the results of 4 workers with 16 workers following the same experiment setup. The CAFE performances are comparable.",
            "We also implement CAFE in the ‘attacking while training’ scenario, in which we continuously run the VFL process. When the model is training, both of the selected batch data and the model parameters change every iteration, which may cause the attack loss to diverge. However, from our experimental results in Table 7, CAFE is able to recover training images when the learning rate (lr) is relatively small. Increasing the learning rate renders data leakage more difficult because the model is making more sizeable parameter changes in each iteration, which can be regarded as an effective defense strategy. According to our experiment in Table 8, the model indeed converges with a relative small learning rate (e.g., Adam with learning rate 10−6superscript10610^{-6}, trained on 800800800 images, tested on 100100100 images, batch size K=40𝐾40K=40), which indicates that we can conduct our attack successfully while a model is converging. The data indeed leaks to a certain level (PSNR above 20) while the model converges at a certain accuracy (0.680.680.68), which indicates that CAFE works in an attacking while training scenario."
        ]
    },
    "S4.T5": {
        "caption": "",
        "table": "",
        "footnotes": "\n\n\n\n\n\n\n\nAlgorithm\n\n\n\n\nDatasets\n\n\n\n\nPSNR\n\n\nCIFAR-10\nLinnaeus 5\nMNIST\n\n \n\n\nCAFE\n\n31.83\n33.22\n43.15\n\n \n\n\nCAFE (α=0𝛼0\\alpha=0)\n\n33.93\n28.62\n31.93\n\n \n\n\nCAFE (ξ=0𝜉0\\xi=0)\n\n25.57\n25.29\n34.51\n\n \n\n\nCAFE (β=0𝛽0\\beta=0)\n\n18.25\n23.22\n31.98\n\n \n\n\nCAFE (γ=0𝛾0\\gamma=0)\n\n12.51\n12.37\n6.34\n\n\n\n\n\n\n\n\n\nDatasets\n\n\n\n\nmode\n\n\n\n\nIterations\n\n\nCIFAR-10\nMNIST\nLinnaues 5\n\nSingle loop\n \n\n\n7300\n\n(8000)\n\n \n\n\n6600\n\n(8000)\n\n \n\n\n12400\n\n(20000)\n\n\n \n\n\nNested-loops\n\nStep I\n\n \n\n\n8000\n\n(8000)\n\n \n\n\n8000\n\n(8000)\n\n \n\n\n12428\n\n(20000)\n\n\n \n\n\nNested-loops\n\nStep II\n\n \n\n\n2404\n\n(8000)\n\n \n\n\n8000\n\n(8000)\n\n \n\n\n20000\n\n(20000)\n\n\n \n\n\nNested-loops\n\nStep III\n\n \n\n\n1635\n\n(8000)\n\n \n\n\n2468\n\n(8000)\n\n \n\n\n20000\n\n(20000)\n\n\n",
        "references": [
            "In the context of data security and AI ethics, the possibility of inferring private user data from the gradients in FL has received growing interests [10, 14, 21], known as the data leakage problems. Previous works have made exploratory efforts on data recovery through gradients. See Section 2 and Table 1 for details. However, existing approaches often have the limitation of scaling up large-batch data recovery and are lacking in theoretical justification on the capability of data recovery, which may give a false sense of security that increasing the data batch size during training can prevent data leakage [31].\nSome recent works provide sufficient conditions for guaranteed data recovery, but the assumptions are overly restrictive and can be sometimes impractical, such as requiring the number of classes to be much larger than the number of recovered data samples [30].",
            "Use case of VFL.\nVFL is suitable for cases where multiple data owners share the same data identity but their data differ in feature space. Use cases of VFL appear in finance, e-commerce, and health.\nFor example, in medical industry, test results of the same patient from different medical institutions are required to diagnose whether the patient has a certain disease or not, but institutions tend not to share raw data. Figure 2 gives an example of VFL in medical industry.",
            "In a typical VFL process, the server sends public key to local workers and decides the data indices in each iteration of training and evaluation [7, 29]. During the training process, local workers exchange their intermediate results with others to compute gradients and upload them. Therefore, the server has access to both of the model parameters and their gradients. Since data are vertically partitioned among different workers, for each batch, the server (acting as the attacker) needs to send a data index or data id list to all the local workers to ensure that data with the same id sequence have been selected by each worker [29] and we name this step as data index alignment. Data index alignment turns out to be an inevitable step in the vertical training process, which provides the server (the attacker) an opportunity to control the selected batch data indices.",
            "Theory-driven label inference methods have been proposed in [31] and [27]. However, our attack mainly deals with training data leakage rather than labels. In [22], the authors proposed a sufficient requirement that \"each data sample has at least two exclusively activated neurons at the last but one layer\". However, in our training protocol, the batch size is too large and it is almost impossible to ensure that each selected sample has at least two exclusively activated neurons. In [32], it is assumed that the method will only return a linear combination of the selected training data, which is a very restricted assumption. As the results, we did not compare to those methods in Table 3.",
            "CAFE outperforms these methods both qualitatively (Figure 1) and quantitatively (Table 3). Its PSNR values are always above 303030 at the end of each CAFE attacking process, suggesting high data recovery quality. However, the PSNR of other methods are below 101010 on all the three datasets.",
            "(i) PSNR via Batch size K𝐾K.\nTable 3 shows that the PSNR values always keep above 303030 on CIFAR-10, above 323232 on MNIST and above 282828 on Linnaeus 5 when the batch size K𝐾K increases with fixed number of workers and number of total data points.\nThe result implies that the increasing K𝐾K has almost no influence on data leakage performance of CAFE and it fails to be an effective defense.",
            "(iii) Effect of regularizers.\nTable 5 demonstrates the impact of regularizers. From Figure 6, adjusting the threshold ξ𝜉\\xi prevents images from being over blurred during the reconstruction process. TV norm can eliminate the noisy patterns on the recovered images and increase the PSNR. We also find that the last term in (9), the internal representation norm regularizer, contributes most to the data recovery. In Table 5, CAFE still performs well without the first term (α=0𝛼0\\alpha=0) in (9). The reason is that the internal representation regularizer already allows data to be fully recovered. Notably, CAFE also performs well on MNIST even without the second term (β=0𝛽0\\beta=0) in (9). It is mainly due to that MNIST is a simple dataset that CAFE can successfully recover even without the TV-norm regularizer.",
            "(iv) Nested-loops vs single-loop.\nWe compare both modes of CAFE (Algorithms 3 and 4) on all datasets. In Table 5, the number of iterations is the maximum iterations at each step. For the CAFE (single-loop), if the objective function in step I (7) decreases below 10−9superscript10910^{-9}, we switch to step II. If the objective function in step II (8) decreases below 5×10−95superscript1095\\times 10^{-9}, we switch to step III. When the PSNR value reaches 272727 on CIFAR-10, 303030 on Linnaeus 5, 383838 on MNIST, we stop both algorithms and record the iteration numbers.\nAs shown in Table 5, CAFE single-loop requires fewer number of iterations. Meanwhile, it is difficult to set the loop stopping conditions in the CAFE Nested-loops mode. In particular, V∗superscriptV\\textbf{V}^{*} and 𝐇^∗superscript^𝐇\\hat{\\mathbf{H}}^{*} with low recovery precision may impact the data recovery performance.",
            "(v) Effects of number of workers M𝑀M.\nAlthough data are partitioned on feature space across workers, the dimension of the entire data feature space is fixed and independent of M𝑀M. Therefore, increasing number of workers theoretically does not change the dimension of variables associated with data recovery in (3). In practice, different from HFL, where there could be hundreds of workers, in VFL, the workers are typically financial organizations or companies. Therefore, the number of workers is usually small [13].\nIn Table 6, we compare the results of 4 workers with 16 workers following the same experiment setup. The CAFE performances are comparable.",
            "We also implement CAFE in the ‘attacking while training’ scenario, in which we continuously run the VFL process. When the model is training, both of the selected batch data and the model parameters change every iteration, which may cause the attack loss to diverge. However, from our experimental results in Table 7, CAFE is able to recover training images when the learning rate (lr) is relatively small. Increasing the learning rate renders data leakage more difficult because the model is making more sizeable parameter changes in each iteration, which can be regarded as an effective defense strategy. According to our experiment in Table 8, the model indeed converges with a relative small learning rate (e.g., Adam with learning rate 10−6superscript10610^{-6}, trained on 800800800 images, tested on 100100100 images, batch size K=40𝐾40K=40), which indicates that we can conduct our attack successfully while a model is converging. The data indeed leaks to a certain level (PSNR above 20) while the model converges at a certain accuracy (0.680.680.68), which indicates that CAFE works in an attacking while training scenario."
        ]
    },
    "S4.T6": {
        "caption": "Table 6: Effects of number of workers M𝑀M\n(K=40𝐾40K=40, batch ratio = 0.050.050.05)",
        "table": "",
        "footnotes": "",
        "references": [
            "(v) Effects of number of workers M𝑀M.\nAlthough data are partitioned on feature space across workers, the dimension of the entire data feature space is fixed and independent of M𝑀M. Therefore, increasing number of workers theoretically does not change the dimension of variables associated with data recovery in (3). In practice, different from HFL, where there could be hundreds of workers, in VFL, the workers are typically financial organizations or companies. Therefore, the number of workers is usually small [13].\nIn Table 6, we compare the results of 4 workers with 16 workers following the same experiment setup. The CAFE performances are comparable."
        ]
    },
    "S4.SS2.4": {
        "caption": "",
        "table": "",
        "footnotes": "\n\n\n\n\n\n\n\nDataset\n\n\n\n\nSetting\n\n\n\n\nPSNR(lr)\n\n\n1\n2\n3\n\nCIFAR10\n \n\n\n31.2431.2431.24\n\n(10−4)superscript104(10^{-4})\n\n \n\n\n27.6227.6227.62\n\n(5×10−4)5superscript104(5\\times 10^{-4})\n\n \n\n\n25.2225.2225.22\n\n(10−3)superscript103(10^{-3})\n\n\nMNIST\n \n\n\n31.8231.8231.82\n\n(10−4)superscript104(10^{-4})\n\n \n\n\n28.4228.4228.42\n\n(5×10−4)5superscript104(5\\times 10^{-4})\n\n \n\n\n23.6023.6023.60\n\n(10−3)superscript103(10^{-3})\n\n\nLinnaeus 5\n \n\n\n30.7430.7430.74\n\n(10−6)superscript106(10^{-6})\n\n \n\n\n21.4521.4521.45\n\n(5×10−5)5superscript105(5\\times 10^{-5})\n\n \n\n\n20.6820.6820.68\n\n(10−4)superscript104(10^{-4})\n\n\n\n\n\n\n\n##\\# of iterations\nPSNR value\nTraining loss\nTesting accuracy\n\n00\n \n\n\n5.075.075.07\n\n \n\n\n2.362.362.36\n\n \n\n\n0.110.110.11\n\n\n200020002000\n \n\n\n11.6811.6811.68\n\n \n\n\n2.312.312.31\n\n \n\n\n0.270.270.27\n\n\n600060006000\n \n\n\n18.0718.0718.07\n\n \n\n\n1.991.991.99\n\n \n\n\n0.540.540.54\n\n\n100001000010000\n \n\n\n18.1218.1218.12\n\n \n\n\n1.821.821.82\n\n \n\n\n0.640.640.64\n\n\n150001500015000\n \n\n\n16.8616.8616.86\n\n \n\n\n1.631.631.63\n\n \n\n\n0.650.650.65\n\n\n200002000020000\n \n\n\n20.7220.7220.72\n\n \n\n\n1.681.681.68\n\n \n\n\n0.680.680.68\n\n\n",
        "references": [
            "In the context of data security and AI ethics, the possibility of inferring private user data from the gradients in FL has received growing interests [10, 14, 21], known as the data leakage problems. Previous works have made exploratory efforts on data recovery through gradients. See Section 2 and Table 1 for details. However, existing approaches often have the limitation of scaling up large-batch data recovery and are lacking in theoretical justification on the capability of data recovery, which may give a false sense of security that increasing the data batch size during training can prevent data leakage [31].\nSome recent works provide sufficient conditions for guaranteed data recovery, but the assumptions are overly restrictive and can be sometimes impractical, such as requiring the number of classes to be much larger than the number of recovered data samples [30].",
            "Use case of VFL.\nVFL is suitable for cases where multiple data owners share the same data identity but their data differ in feature space. Use cases of VFL appear in finance, e-commerce, and health.\nFor example, in medical industry, test results of the same patient from different medical institutions are required to diagnose whether the patient has a certain disease or not, but institutions tend not to share raw data. Figure 2 gives an example of VFL in medical industry.",
            "In a typical VFL process, the server sends public key to local workers and decides the data indices in each iteration of training and evaluation [7, 29]. During the training process, local workers exchange their intermediate results with others to compute gradients and upload them. Therefore, the server has access to both of the model parameters and their gradients. Since data are vertically partitioned among different workers, for each batch, the server (acting as the attacker) needs to send a data index or data id list to all the local workers to ensure that data with the same id sequence have been selected by each worker [29] and we name this step as data index alignment. Data index alignment turns out to be an inevitable step in the vertical training process, which provides the server (the attacker) an opportunity to control the selected batch data indices.",
            "Theory-driven label inference methods have been proposed in [31] and [27]. However, our attack mainly deals with training data leakage rather than labels. In [22], the authors proposed a sufficient requirement that \"each data sample has at least two exclusively activated neurons at the last but one layer\". However, in our training protocol, the batch size is too large and it is almost impossible to ensure that each selected sample has at least two exclusively activated neurons. In [32], it is assumed that the method will only return a linear combination of the selected training data, which is a very restricted assumption. As the results, we did not compare to those methods in Table 3.",
            "CAFE outperforms these methods both qualitatively (Figure 1) and quantitatively (Table 3). Its PSNR values are always above 303030 at the end of each CAFE attacking process, suggesting high data recovery quality. However, the PSNR of other methods are below 101010 on all the three datasets.",
            "(i) PSNR via Batch size K𝐾K.\nTable 3 shows that the PSNR values always keep above 303030 on CIFAR-10, above 323232 on MNIST and above 282828 on Linnaeus 5 when the batch size K𝐾K increases with fixed number of workers and number of total data points.\nThe result implies that the increasing K𝐾K has almost no influence on data leakage performance of CAFE and it fails to be an effective defense.",
            "(iii) Effect of regularizers.\nTable 5 demonstrates the impact of regularizers. From Figure 6, adjusting the threshold ξ𝜉\\xi prevents images from being over blurred during the reconstruction process. TV norm can eliminate the noisy patterns on the recovered images and increase the PSNR. We also find that the last term in (9), the internal representation norm regularizer, contributes most to the data recovery. In Table 5, CAFE still performs well without the first term (α=0𝛼0\\alpha=0) in (9). The reason is that the internal representation regularizer already allows data to be fully recovered. Notably, CAFE also performs well on MNIST even without the second term (β=0𝛽0\\beta=0) in (9). It is mainly due to that MNIST is a simple dataset that CAFE can successfully recover even without the TV-norm regularizer.",
            "(iv) Nested-loops vs single-loop.\nWe compare both modes of CAFE (Algorithms 3 and 4) on all datasets. In Table 5, the number of iterations is the maximum iterations at each step. For the CAFE (single-loop), if the objective function in step I (7) decreases below 10−9superscript10910^{-9}, we switch to step II. If the objective function in step II (8) decreases below 5×10−95superscript1095\\times 10^{-9}, we switch to step III. When the PSNR value reaches 272727 on CIFAR-10, 303030 on Linnaeus 5, 383838 on MNIST, we stop both algorithms and record the iteration numbers.\nAs shown in Table 5, CAFE single-loop requires fewer number of iterations. Meanwhile, it is difficult to set the loop stopping conditions in the CAFE Nested-loops mode. In particular, V∗superscriptV\\textbf{V}^{*} and 𝐇^∗superscript^𝐇\\hat{\\mathbf{H}}^{*} with low recovery precision may impact the data recovery performance.",
            "(v) Effects of number of workers M𝑀M.\nAlthough data are partitioned on feature space across workers, the dimension of the entire data feature space is fixed and independent of M𝑀M. Therefore, increasing number of workers theoretically does not change the dimension of variables associated with data recovery in (3). In practice, different from HFL, where there could be hundreds of workers, in VFL, the workers are typically financial organizations or companies. Therefore, the number of workers is usually small [13].\nIn Table 6, we compare the results of 4 workers with 16 workers following the same experiment setup. The CAFE performances are comparable.",
            "We also implement CAFE in the ‘attacking while training’ scenario, in which we continuously run the VFL process. When the model is training, both of the selected batch data and the model parameters change every iteration, which may cause the attack loss to diverge. However, from our experimental results in Table 7, CAFE is able to recover training images when the learning rate (lr) is relatively small. Increasing the learning rate renders data leakage more difficult because the model is making more sizeable parameter changes in each iteration, which can be regarded as an effective defense strategy. According to our experiment in Table 8, the model indeed converges with a relative small learning rate (e.g., Adam with learning rate 10−6superscript10610^{-6}, trained on 800800800 images, tested on 100100100 images, batch size K=40𝐾40K=40), which indicates that we can conduct our attack successfully while a model is converging. The data indeed leaks to a certain level (PSNR above 20) while the model converges at a certain accuracy (0.680.680.68), which indicates that CAFE works in an attacking while training scenario."
        ]
    },
    "S4.T7": {
        "caption": "Table 7: Attacking while training in VFL",
        "table": "",
        "footnotes": "",
        "references": [
            "We also implement CAFE in the ‘attacking while training’ scenario, in which we continuously run the VFL process. When the model is training, both of the selected batch data and the model parameters change every iteration, which may cause the attack loss to diverge. However, from our experimental results in Table 7, CAFE is able to recover training images when the learning rate (lr) is relatively small. Increasing the learning rate renders data leakage more difficult because the model is making more sizeable parameter changes in each iteration, which can be regarded as an effective defense strategy. According to our experiment in Table 8, the model indeed converges with a relative small learning rate (e.g., Adam with learning rate 10−6superscript10610^{-6}, trained on 800800800 images, tested on 100100100 images, batch size K=40𝐾40K=40), which indicates that we can conduct our attack successfully while a model is converging. The data indeed leaks to a certain level (PSNR above 20) while the model converges at a certain accuracy (0.680.680.68), which indicates that CAFE works in an attacking while training scenario."
        ]
    },
    "S4.T8": {
        "caption": "Table 8: Training while attacking on MNIST",
        "table": "",
        "footnotes": "",
        "references": [
            "We also implement CAFE in the ‘attacking while training’ scenario, in which we continuously run the VFL process. When the model is training, both of the selected batch data and the model parameters change every iteration, which may cause the attack loss to diverge. However, from our experimental results in Table 7, CAFE is able to recover training images when the learning rate (lr) is relatively small. Increasing the learning rate renders data leakage more difficult because the model is making more sizeable parameter changes in each iteration, which can be regarded as an effective defense strategy. According to our experiment in Table 8, the model indeed converges with a relative small learning rate (e.g., Adam with learning rate 10−6superscript10610^{-6}, trained on 800800800 images, tested on 100100100 images, batch size K=40𝐾40K=40), which indicates that we can conduct our attack successfully while a model is converging. The data indeed leaks to a certain level (PSNR above 20) while the model converges at a certain accuracy (0.680.680.68), which indicates that CAFE works in an attacking while training scenario."
        ]
    }
}