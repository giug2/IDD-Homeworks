{
    "id_table_1": {
        "caption": "Table 1:  Cosine similarities between questions and contents combination",
        "table": "S2.T1.1",
        "footnotes": [],
        "references": [
            "To overcome this scalability challenge, digital health platforms are increasingly turning to meticulously curated content repositories designed to provide readily available answers to common patient questions. This study utilizes such a content repository, referred to as Content Cards, developed to address frequently asked questions (FAQs) from patients with T2D and HTN participating in our proprietary digital health programs. This dataset, encompassing 630 English-language content cards, covers a comprehensive range of topics pertinent to managing T2D and HTN. This includes general health information, detailed guidance on using platform-specific features and connected devices (e.g., blood glucose monitors), and personalized dietary recommendations. Figure  1  illustrates how these Content Cards are presented within our mobile application, while the excerpt below provides a representative sample of the content:",
            "To generate an initial set of questions, we design a prompt (shown in listing  1 ) with clear instructions for question generation and few-shot examples to illustrate the desired style and format. This prompt incorporates a  num_questions  parameter to specify the target number of questions per content card. While we set this parameter to 20, the actual number of questions generated by the LLM may vary. Applying this prompt to our content base of 630 cards resulted in over 8,000 potential questions.",
            "To assess the potential of our question-based retrieval approach, we analyzed the semantic similarity among the questions generated using the prompt in Listing  1 . Our goal was to determine if questions derived from the same content card (intra-content similarity) exhibit higher similarity than questions generated from different cards (inter-content similarity).",
            "We calculated pairwise cosine similarity between question embeddings generated using Googles  textembedding-gecko  model. As shown in Table  1 , mean intra-content similarity (0.871) is indeed higher than mean inter-content similarity (0.827). Similarly, questions are, on average, more similar to their source content card (0.837) than to other content cards (0.762). We note that the relatively high similarity between questions and content from different cards likely stems from the inherent thematic overlap within our dataset, as all content and questions focus on T2D and HTN management.",
            "Our vanilla approach of QB-RAG first generates an extensive set of questions that are known to be answered by the content by initializing  Q Q \\mathcal{Q} caligraphic_Q  and  A A A italic_A . This operation can happen offline and upon uploading new documents to our content base.  Second, for an online query, QB-RAG searches for similar question (resp. questions) within  Q Q \\mathcal{Q} caligraphic_Q  by finding  arg  min Q  d  ( q 0 , q ) subscript Q d subscript q 0 q \\arg\\min_{\\mathcal{Q}}d(q_{0},q) roman_arg roman_min start_POSTSUBSCRIPT caligraphic_Q end_POSTSUBSCRIPT italic_d ( italic_q start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_q )  (resp.  arg  min S  Q   q  S d  ( q , q 0 ) subscript S Q subscript q S d q subscript q 0 \\arg\\min_{\\mathcal{S}\\subset\\mathcal{Q}}\\sum_{q\\in\\mathcal{S}}{d(q,q_{0})} roman_arg roman_min start_POSTSUBSCRIPT caligraphic_S  caligraphic_Q end_POSTSUBSCRIPT  start_POSTSUBSCRIPT italic_q  caligraphic_S end_POSTSUBSCRIPT italic_d ( italic_q , italic_q start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) ).  Given we are now comparing questions to questions, we expect the distance measure to be calibrated given the comparison is also aligned.  Third upon retrieving similar questions, the associated contents are fetched, and fed to the generative LLM like other RAG systems (after dropping duplicate contents if necessary). We provide the full details in Algorithm  1 .",
            "Given the oracle  A  superscript A A^{*} italic_A start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  or some estimate  A ^ ^ A \\hat{A} over^ start_ARG italic_A end_ARG  thereof, we can adapt Algorithm  1  to incorporate the non-sparse nature of  A  superscript A A^{*} italic_A start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  or  A ^ ^ A \\hat{A} over^ start_ARG italic_A end_ARG . Specifically, since multiple content pieces may be associated with a single question, we introduce tie-breaking rules within the algorithm that first prioritize newer content (especially relevant to healthcare). We then prioritizes contents capable of answering a diverse range of questions within  Q Q \\mathcal{Q} caligraphic_Q . This reflects our intuition that a such content is likely to be more broadly relevant.",
            "Vanilla QB-RAG : We run the simplest version of our method (see Algorithm  1 ) using the matrix  A A A italic_A  which directly encodes which content generated which question. We believe that having access to the oracle matrix  A  superscript A A^{*} italic_A start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  or some approximation of it would be even more promising.  Furthermore, given this method is strongly dependent on the questions generated offline (in section  2 ), we parameterize QB-RAG- m     m {\\bar{m}} over  start_ARG italic_m end_ARG  where  m     m \\bar{m} over  start_ARG italic_m end_ARG  measures the average number of questions generated per content. Notably,  m     m \\bar{m} over  start_ARG italic_m end_ARG   indicates the coverage extracted from each content, implying that higher coverage should lead to improved retrieval quality and, consequently, better-generated answers. We decrease  m     m \\bar{m} over  start_ARG italic_m end_ARG  by down-sampling the set of questions in our experiments, to effectively reduce the coverage of our question base. The maximum value from our generation is  m   = 8   m 8 \\bar{m}=8 over  start_ARG italic_m end_ARG = 8  which includes our entire question set.",
            "The exact recovery rate does not present the full picture. When analyzing the Auto-evaluator Relevancy and the Re-ranker score (refer section  4.4.1 ), our method still dominates the baselines when retrieving a single content. As measured by the LLM, the content we retrieve are relevant 68% of the time, whereas traditional methods retrieve relevant content around 40% of the time. In table  2 , we also include the average re-ranker relevancy score. QB-RAG-8 yields the highest average relevance score using the BGE re-ranker."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Retrieval performance (higher is better) of methods on  Rephrase , where documents were retrieved given rephrased questions from the content base.",
        "table": "S5.T2.1",
        "footnotes": [],
        "references": [
            "The remainder of this paper is structured as follows. Section  2  introduces our healthcare-focused content base and details our methodology for extracting a comprehensive set of questions from these materials. In Section  3 , we formalize the RAG objective, highlighting the challenges of retrieval alignment. We then present QB-RAG as a solution, theoretically motivating its effectiveness in bridging the semantic gap between queries and content. Section  4  outlines our benchmark setup, encompassing a range of prominent retrieval methods and evaluation metrics. Finally, Section  5  presents the results of our empirical evaluation, demonstrating QB-RAGs superiority in retrieval accuracy and downstream answer quality.",
            "The core premise of our work is to improve content retrieval in RAG systems by directly aligning incoming user queries with a pre-computed set of questions derived from our content base. This requires generating a comprehensive set of questions that are answerable by our content, which we achieve through a two-step process. First, we employ a base prompt to generate a preliminary set of questions for each content card. Subsequently, we leverage an LLM-based answerability model (detailed in Section  2.2.2 ) to filter out irrelevant or nonsensical questions, ensuring the quality and relevance of our question set.",
            "This high level of agreement, coupled with the increasing recognition of LLMs as reliable auto-evaluation tools  Lee et al. ( 2023 ) , supports the validity of our answerability model for filtering irrelevant questions. We provide the prompt used for our answerability model in Listing  2 .",
            "Consider the set of  M M M italic_M  content documents  C = { c 1 ,  , c M } C subscript c 1  subscript c M \\mathcal{C}=\\{c_{1},\\cdots,c_{M}\\} caligraphic_C = { italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,  , italic_c start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT }  appropriately chunked for embedding. Similarly, let  Q = { q 1 ,  , q N } Q subscript q 1  subscript q N \\mathcal{Q}=\\{q_{1},\\cdots,q_{N}\\} caligraphic_Q = { italic_q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,  , italic_q start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT }  represent the set of  N N N italic_N  questions generated from the content documents as described in section  2.2 . Unless otherwise specified,  c  C c C c\\in\\mathcal{C} italic_c  caligraphic_C  and  q  Q q Q q\\in\\mathcal{Q} italic_q  caligraphic_Q  will refer to their respective embedding representations.",
            "The question generation process in section  2.2  is a one to many process, that is from one content we generate multiple questions. To encode this relationship, we denote the matrix  A  { 0 , 1 } M  N A superscript 0 1 M N A\\in\\{0,1\\}^{M\\times N} italic_A  { 0 , 1 } start_POSTSUPERSCRIPT italic_M  italic_N end_POSTSUPERSCRIPT  where  A i  j = 1  [ c i   generated   q j ] subscript A i j 1 delimited-[] subscript c i  generated  subscript q j A_{ij}=\\mathds{1}[c_{i}\\text{ generated }q_{j}] italic_A start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = blackboard_1 [ italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT generated italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ] , where  1 1 \\mathds{1} blackboard_1  denotes the indicator function. Per our generation process and the relevance evaluation, it is understood that  A i  j = 1  q j   can be answered using   c i subscript A i j 1  subscript q j  can be answered using  subscript c i A_{ij}=1\\Rightarrow q_{j}\\text{ can be answered using }c_{i} italic_A start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = 1  italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT can be answered using italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . However the converse is not necessarily true as potentially different contents may be able to answer the same question. Thus we define  A   { 0 , 1 } M  N superscript A superscript 0 1 M N A^{*}\\in\\{0,1\\}^{M\\times N} italic_A start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  { 0 , 1 } start_POSTSUPERSCRIPT italic_M  italic_N end_POSTSUPERSCRIPT  which is the dense unobserved matrix such that  A i  j  = 1  [ q j   can be answered using   c i ] subscript superscript A i j 1 delimited-[] subscript q j  can be answered using  subscript c i A^{*}_{ij}=\\mathds{1}[q_{j}\\text{ can be answered using }c_{i}] italic_A start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = blackboard_1 [ italic_q start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT can be answered using italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ] . This implies that  A A A italic_A  is a partial observation of  A  superscript A A^{*} italic_A start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , which we call the oracle matrix.  We point out that following our question generation process described in section  2 , we have   c j  C :  q i  Q   s.t.   A i  j = 1 : for-all subscript c j C subscript q i Q  s.t.  subscript A i j 1 \\forall c_{j}\\in\\mathcal{C}:\\>\\exists q_{i}\\in\\mathcal{Q}\\text{ s.t. }A_{ij}=1  italic_c start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  caligraphic_C :  italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  caligraphic_Q s.t. italic_A start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = 1 .",
            "To address the semantic misalignment inherent in traditional RAG retrieval, we propose Query-Based Retrieval Augmented Generation (QB-RAG). Our approach leverages a pre-computed set of questions,  Q Q \\mathcal{Q} caligraphic_Q , derived from the content base  C C \\mathcal{C} caligraphic_C . These questions are generated offline as described in Section  2.2 , mitigating concerns about online computational overhead. We not that efficient retrieval algorithms render the increased knowledge base size introduced by incorporating  Q Q \\mathcal{Q} caligraphic_Q  negligible.  In contrast to methods relying on online LLM calls for query rewriting, QB-RAG shifts this computational burden offline. This distinction is significant, as online rewriting necessitates serial LLM invocation, potentially introducing latency detrimental to user experience. Furthermore, QB-RAGs direct alignment within the query space offers a more transparent and interpretable retrieval process compared to the implicit alignment strategies of LLM-based rewriting techniques.",
            "Vanilla QB-RAG : We run the simplest version of our method (see Algorithm  1 ) using the matrix  A A A italic_A  which directly encodes which content generated which question. We believe that having access to the oracle matrix  A  superscript A A^{*} italic_A start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  or some approximation of it would be even more promising.  Furthermore, given this method is strongly dependent on the questions generated offline (in section  2 ), we parameterize QB-RAG- m     m {\\bar{m}} over  start_ARG italic_m end_ARG  where  m     m \\bar{m} over  start_ARG italic_m end_ARG  measures the average number of questions generated per content. Notably,  m     m \\bar{m} over  start_ARG italic_m end_ARG   indicates the coverage extracted from each content, implying that higher coverage should lead to improved retrieval quality and, consequently, better-generated answers. We decrease  m     m \\bar{m} over  start_ARG italic_m end_ARG  by down-sampling the set of questions in our experiments, to effectively reduce the coverage of our question base. The maximum value from our generation is  m   = 8   m 8 \\bar{m}=8 over  start_ARG italic_m end_ARG = 8  which includes our entire question set.",
            "Out-of-Distribution : To generate the second test set, we prompted an LLM to generate a new question for each of the 630 contents. The LLM was instructed to not generate a question that already exists in the question knowledge base for that content. Then, the 630 newly generated questions were filtered via the answerability model from Section  2  to ensure the new questions were indeed answerable by the corresponding content. After filtering, 305 newly generated questions made up the second test set.",
            "Auto-evaluator Relevancy Rate : This metric addresses the limitations of relying solely on exact matches by leveraging the answerability model (Section  2 ) to gauge content relevance. Specifically, it calculates the percentage of test questions for which at least one retrieved document is deemed relevant by the answerability model. This automated assessment has demonstrated strong correlation with human judgments as as described in section  2 .",
            "On the  Rephrase  test set, where the knowledge base is expected to contain questions semantically similar to the test questions, QB-RAG-8 consistently outperforms all benchmark methods. As shown in table  2 , QB-RAG-8 nearly doubles the exact recovery rate compared to other methods (e.g., from 45% to 89% when retrieving a single document). This suggests that a comprehensive, query-aligned knowledge base, as constructed by QB-RAG-8, substantially improves the retrieval of the exact source document. These impressive gains highlight the scenario where our knowledge base is comprehensive and covers quite broadly the extent of questions our documents can answer.",
            "The exact recovery rate does not present the full picture. When analyzing the Auto-evaluator Relevancy and the Re-ranker score (refer section  4.4.1 ), our method still dominates the baselines when retrieving a single content. As measured by the LLM, the content we retrieve are relevant 68% of the time, whereas traditional methods retrieve relevant content around 40% of the time. In table  2 , we also include the average re-ranker relevancy score. QB-RAG-8 yields the highest average relevance score using the BGE re-ranker.",
            "As shown in Table  4 , both QB-RAG-8 and QB-RAG-2 consistently outperform the benchmark methods on all answer quality metrics for the  Rephrase  test set. Notably, QB-RAG-8 achieves an 84% answer faithfulness rate, significantly surpassing the 62%-68% rates of the baseline methods. This suggests that by accurately retrieving the most relevant content, QB-RAG enables the LLM to generate answers that are well-grounded in the provided information. Additionally, QB-RAG achieves the highest accuracy rate, indicating its answers effectively address the key elements outlined in the pre-defined guidelines (refer to Section  4.4.2 ).",
            "On the  Rephrase  test set, QB-RAG-2, despite its reduced question set, still surpasses other retrieval methods (table  2 ). However, the magnitude of improvement is noticeably smaller compared to QB-RAG-8. For instance, QB-RAG-2 shows a 10-15% improvement in exact recovery rate over baselines, whereas QB-RAG-8 nearly doubles the exact recovery rate. This pattern also holds for relevancy rate and average re-ranker scores, indicating that a larger, more comprehensive question set translates to more effective retrieval."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Retrieval performance (higher is better) of methods on  Out-of-Distribution , where documents were retrieved given newly generated questions.",
        "table": "S5.T3.1",
        "footnotes": [],
        "references": [
            "The remainder of this paper is structured as follows. Section  2  introduces our healthcare-focused content base and details our methodology for extracting a comprehensive set of questions from these materials. In Section  3 , we formalize the RAG objective, highlighting the challenges of retrieval alignment. We then present QB-RAG as a solution, theoretically motivating its effectiveness in bridging the semantic gap between queries and content. Section  4  outlines our benchmark setup, encompassing a range of prominent retrieval methods and evaluation metrics. Finally, Section  5  presents the results of our empirical evaluation, demonstrating QB-RAGs superiority in retrieval accuracy and downstream answer quality.",
            "On the more challenging  Out-of-Distribution  test set, QB-RAG-8 maintains its advantage, albeit with smaller gains. Table  3  shows that QB-RAG-8 improves the exact recovery rate by 1.3% to 6.2% compared to benchmark methods. Despite the adversarial nature of this test set, where incoming questions are intentionally dissimilar to the training set, QB-RAG-8 consistently retrieves the correct source document more often.",
            "The  Out-of-Distribution  test set (table  3 ) reveals a more pronounced impact of question base coverage. Here, QB-RAG-2s performance drops below that of some benchmark methods, with the exact recovery rate decreasing by 7-12%. Interestingly, QB-RAG-2 still achieves comparable performance on relevancy-based metrics, suggesting that even a limited question base can partially capture relevant content, but may not pinpoint the exact source document as effectively."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Answer quality of methods on  Rephrase , where documents were retrieved given rephrased questions from the content base. 3 documents retrieved.",
        "table": "S5.T4.1",
        "footnotes": [],
        "references": [
            "The remainder of this paper is structured as follows. Section  2  introduces our healthcare-focused content base and details our methodology for extracting a comprehensive set of questions from these materials. In Section  3 , we formalize the RAG objective, highlighting the challenges of retrieval alignment. We then present QB-RAG as a solution, theoretically motivating its effectiveness in bridging the semantic gap between queries and content. Section  4  outlines our benchmark setup, encompassing a range of prominent retrieval methods and evaluation metrics. Finally, Section  5  presents the results of our empirical evaluation, demonstrating QB-RAGs superiority in retrieval accuracy and downstream answer quality.",
            "The exact recovery rate does not present the full picture. When analyzing the Auto-evaluator Relevancy and the Re-ranker score (refer section  4.4.1 ), our method still dominates the baselines when retrieving a single content. As measured by the LLM, the content we retrieve are relevant 68% of the time, whereas traditional methods retrieve relevant content around 40% of the time. In table  2 , we also include the average re-ranker relevancy score. QB-RAG-8 yields the highest average relevance score using the BGE re-ranker.",
            "As shown in Table  4 , both QB-RAG-8 and QB-RAG-2 consistently outperform the benchmark methods on all answer quality metrics for the  Rephrase  test set. Notably, QB-RAG-8 achieves an 84% answer faithfulness rate, significantly surpassing the 62%-68% rates of the baseline methods. This suggests that by accurately retrieving the most relevant content, QB-RAG enables the LLM to generate answers that are well-grounded in the provided information. Additionally, QB-RAG achieves the highest accuracy rate, indicating its answers effectively address the key elements outlined in the pre-defined guidelines (refer to Section  4.4.2 ).",
            "This sensitivity to question coverage is further evident in the quality of the generated answers (Tables  4  and  5 ). QB-RAG-8 consistently leads to more faithful, relevant, and accurate answers compared to QB-RAG-2, directly reflecting the differences observed in their retrieval performance. These findings highlight that QB-RAGs success in downstream tasks is fundamentally linked to its ability to construct and leverage a comprehensive and diverse question set that effectively captures the content and semantic nuances of incoming queries within the RAG knowledge base."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Answer quality of methods on  Out-of-Distribution , where documents were retrieved given newly generated questions. 3 documents retrieved.",
        "table": "S5.T5.1",
        "footnotes": [],
        "references": [
            "The remainder of this paper is structured as follows. Section  2  introduces our healthcare-focused content base and details our methodology for extracting a comprehensive set of questions from these materials. In Section  3 , we formalize the RAG objective, highlighting the challenges of retrieval alignment. We then present QB-RAG as a solution, theoretically motivating its effectiveness in bridging the semantic gap between queries and content. Section  4  outlines our benchmark setup, encompassing a range of prominent retrieval methods and evaluation metrics. Finally, Section  5  presents the results of our empirical evaluation, demonstrating QB-RAGs superiority in retrieval accuracy and downstream answer quality.",
            "These patterns hold on the more challenging  Out-of-Distribution  test set, as seen in table  5 .  There QB-RAG-8 achieves 78% faithfulness, fairing higher than the benchmarks 68%-74%. However QB-RAG-2 under-performs, highlighting its inability to generalize to new questions.",
            "This sensitivity to question coverage is further evident in the quality of the generated answers (Tables  4  and  5 ). QB-RAG-8 consistently leads to more faithful, relevant, and accurate answers compared to QB-RAG-2, directly reflecting the differences observed in their retrieval performance. These findings highlight that QB-RAGs success in downstream tasks is fundamentally linked to its ability to construct and leverage a comprehensive and diverse question set that effectively captures the content and semantic nuances of incoming queries within the RAG knowledge base."
        ]
    },
    "global_footnotes": [
        "Equal contribution.",
        "",
        "",
        ""
    ]
}