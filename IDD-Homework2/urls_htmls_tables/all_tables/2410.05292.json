{
    "id_table_1": {
        "caption": "Table 1:   Performance comparison of CaLMFlow and CFM variants across different distribution pairs (Gaussian    \\to   2 Gaussians, Gaussian    \\to   8 Gaussians, Gaussian    \\to   2 Moons) and dimensions (100, 1000). We report 2-Wasserstein distance (2-Wass) and Maximum Mean Discrepancy (MMD) (    plus-or-minus   \\mu\\pm\\sigma italic_  italic_  over 5 runs), with best results highlighted in bold. As the dimensionality increases, CaLMFlow consistently outperforms CFM variants, achieving lower 2-Wasserstein distances and MMD values, particularly in high-dimensional settings where we expect traditional ODE-based methods, such as CFM, struggle.",
        "table": "S4.E10",
        "footnotes": [],
        "references": [
            "Continuous space tokens via variational decoding:  We introduce variational decoding to sample and generate continuous data. This approach models a continuous conditional distribution for next-token sampling, extending language modeling techniques to continuous domains. Our ablation study in Section  6.1  demonstrate that variational decoding is crucial for accurately modeling continuous data within our framework.",
            "Spatiotemporal and trajectory tokenization:  We present a spatiotemporal tokenization scheme that enables CaLMFlow to model VIEs across both spatial and temporal domains. Additionally, by modeling multiple flows concurrently, CaLMFlow captures correlations between data samples. We demonstrate in Sections  5.1  and  6.3  this approach significantly improves performance.",
            "where  G  ( z  ( s ) , t , s ) G z s t s G(z(s),t,s) italic_G ( italic_z ( italic_s ) , italic_t , italic_s )  is a Urysohn kernel function encoding the influence of past states on the current state. VIEs generalize ODEs, as shown by the transformation of Equation ( 1 ) into the equivalent integral form:",
            "To control the generation of continuous tokens at inference, we use a temperature parameter    \\tau italic_  that scales the variance of the encoded posterior, analogous to the use of temperature in LLMs  (Renze & Guven,  2024 ; Peeperkorn et al.,  2024 ) . Specifically, the posterior latent distribution is modified as  q   ( z | x )  N  ( z ;  ,    2  I ) similar-to subscript q italic- conditional z x N z   superscript  2 I q_{\\phi}(\\mathbf{z}|\\mathbf{x})\\sim\\mathcal{N}(\\mathbf{z};\\bm{\\mu},\\tau\\bm{% \\sigma}^{2}\\mathbf{I}) italic_q start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( bold_z | bold_x )  caligraphic_N ( bold_z ; bold_italic_ , italic_ bold_italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_I ) , where    0  0 \\tau\\geq 0 italic_  0  is the temperature parameter. An ablation experiment for the temperature parameter is in Section  6.1 .",
            "In this section, we introduce our spatiotemporal and multi-trajectory tokenization method depicted in Figure  1  to effectively represent data for CLM training.",
            "Our results, summarized in Table  1 , demonstrate that while CFM breaks down at higher dimensions, CaLMFlow maintains strong performance. This suggests that CaLMFlow is an effective alternative to ODE-based approaches for modeling high-dimensional problems, providing stability and accuracy where methods like CFM fail.",
            "Our method is benchmarked against several models, including CFM  (Tong et al.,  2024 )  and its variants CFM-OT and CFM-SB, the denoising diffusion probabilistic model (DDPM)  (Ho et al.,  2020 ) , single-cell generative models scVI  (Lopez et al.,  2018 )  and scGPT  (Cui et al.,  2023 ) , and the Compositional Perturbation Autoencoder  (Lotfollahi et al.,  2023 ) , depending on the task. To assess the quality of data generated by each model, we compute distributional metrics, such as maximum mean discrepancy (MMD), 2-Wasserstein distance, and Kullback-Leibler Divergence (KLD), between model-generated data and the ground truth test data. For KLD , we use Leiden clustering to generate a distribution of points across clusters (see Appendix  A.1.1  for details).",
            "We leverage CLMs inherent capabilities to encode and comprehend natural language by representing perturbation conditions as simple text prompts (see  A.1.3  for details). These prompts are prepended to the embedded flow-matching conditional trajectories and processed through the CLMs tokenizer and embedding layers. For details on conditional encodings for other models, see  A.1.3 .",
            "To test the impact of spatiotemporal tokenization, we use a similar setup as in  Tong et al. ( 2024 )  on the MNIST dataset (details in Subsection  4.1 ). All key hyperparameters, optimizers, and training configurations were kept identical to ensure consistency. Our results, as shown in Table  6 , demonstrate that CaLMFlow outperforms other methods and increasing the number of spatial tokens improves inception scores.",
            "In order to perform an approximate integral over the discretized trajectory, we also need a discretized version of Equation ( 15 ), which becomes:"
        ]
    },
    "id_table_2": {
        "caption": "Table 2:   Comparisons of CFM, CaLMFlow, and CaLMFlow with multiple trajectories on different distribution pairs in 100 dimensions. We report 2-Wasserstein distance (2-Wass) and Maximum Mean Discrepancy (MMD), averaged over 5 runs. Best results are highlighted in bold. The table shows that CaLMFlow, especially when utilizing multiple trajectories, significantly outperforms CFM in terms of both fit and distribution accuracy. This demonstrates the advantage of leveraging multi-trajectory modeling in CaLMFlow.",
        "table": "S4.E12",
        "footnotes": [],
        "references": [
            "Controllable generation of flows using natural language:  We present a flexible and effective approach to controllable generation conditioned on textual prompts by leveraging the causal language models natural language understanding. In Section  5.2 , we demonstrate its superiority by applying it to perturbation response prediction in single-cell data, outperforming traditional flow matching and popular single-cell methods.",
            "This spatiotemporal sequence of tokens is processed by the CLM, which approximates a Volterra integral equation over both space and time. The discussion in Section  3.2  extends to the spatiotemporal VIE:",
            "While approaches like CFM focus on modeling the flow of individual points, CaLMFlow is able to sample multiple trajectories and model them simultaneously. The results in Table  2  show this approach improves the performance of CaLMFlow on generating synthetic data. Visualizations of generated results, as shown in Figure  2 , further demonstrate that modeling several trajectories at the same time allows CaLMFlow to distribute data more evenly and accurately by leveraging trajectory context.",
            "We adopt the standard next-token prediction paradigm used in CLMs. Given tokens  x 0 subscript x 0 x_{0} italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  to  x k subscript x k x_{k} italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , the model predicts  x k + 1 subscript x k 1 x_{k+1} italic_x start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT , where the sequence  ( x 0 , ... , x k ) subscript x 0 ... subscript x k (x_{0},\\dots,x_{k}) ( italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , ... , italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT )  corresponds to portions of the conditional flow matching trajectory. In this section, we give the implementation details of the theoretical discussion given in Section  3.2 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Unconditional single-cell generation results comparing generated data to ground truth data. To evaluate CaLMFlows ability to accurately generate its training single-cell dataset  (Dong et al.,  2023 ) , we computed distributional metrics MMD, 2-Wasserstein, KLD, and adMMD (MMD with a  k k k italic_k -NN-based adaptive kernel) across 5 seeds. Our default CaLMFlow outperforms all methods across all metrics including CFM and its variants CFM-OT and CFM-SB, demonstrating CaLMFlows ability to model the data distribution. Further improvement is seen with CaLMFlow (5 traj.), showing the benefit of multi-trajectory tokenization. scGPTs Leiden KLD score is omitted due to the models poor performance on this metric being less informative for comparison purposes. See Figure  6  for a visual comparison of CaLMFlow and CFM. Experimental details are in Appendices  A.1.1  and  A.1.2",
        "table": "S6.T7.10",
        "footnotes": [
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Spatiotemporal and trajectory tokenization:  We present a spatiotemporal tokenization scheme that enables CaLMFlow to model VIEs across both spatial and temporal domains. Additionally, by modeling multiple flows concurrently, CaLMFlow captures correlations between data samples. We demonstrate in Sections  5.1  and  6.3  this approach significantly improves performance.",
            "In CaLMFlow, we define the flow using a more general form of Equation  3 :",
            "where 1)  z t subscript z t z_{t} italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  (also referred to as  z  ( t ) z t z(t) italic_z ( italic_t ) ) is the state at time  t t t italic_t , 2)  f  ( z t , t ) f subscript z t t f(z_{t},t) italic_f ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t )  is the inhomogeneous term ( z 0 subscript z 0 z_{0} italic_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  in the case of Equation  3 ), and 3) The integral term   0 t G  ( z s , t , s )  d s superscript subscript 0 t G subscript z s t s differential-d s \\int_{0}^{t}G(z_{s},t,s)\\,ds  start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT italic_G ( italic_z start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , italic_t , italic_s ) italic_d italic_s  captures the accumulated influence of past states.",
            "This spatiotemporal sequence of tokens is processed by the CLM, which approximates a Volterra integral equation over both space and time. The discussion in Section  3.2  extends to the spatiotemporal VIE:",
            "The results in Table  3 , demonstrate that CaLMFlow consistently outperforms CFM and all other methods across all metrics. Furthermore, as illustrated in Figure  6 , CaLMFlow generates cells with distributions more closely aligned to the ground truth data compared to other methods. These findings underscore CaLMFlows superior performance in capturing and reproducing the complex high-dimensional distributions inherent in single-cell expression data.",
            "We leverage CLMs inherent capabilities to encode and comprehend natural language by representing perturbation conditions as simple text prompts (see  A.1.3  for details). These prompts are prepended to the embedded flow-matching conditional trajectories and processed through the CLMs tokenizer and embedding layers. For details on conditional encodings for other models, see  A.1.3 .",
            "Furthermore, as shown in Figure  3 , both variants of CaLMFlow generate data that closely overlaps with the ground truth distribution, demonstrating CaLMFlows superior ability to model data under unseen conditions. Figure  7  illustrates the distributions produced by each model, showing that CaLMFlows generated data most accurately reflects the ground truth. In contrast, other models are either unable to differentiate between combinatorial labels or generate unrealistic distributions. These visualizations reinforce the high quality of data generated by CaLMFlow, emphasizing its capability to model complex distributions and effectively utilize natural language prompts.",
            "We adopt the standard next-token prediction paradigm used in CLMs. Given tokens  x 0 subscript x 0 x_{0} italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  to  x k subscript x k x_{k} italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , the model predicts  x k + 1 subscript x k 1 x_{k+1} italic_x start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT , where the sequence  ( x 0 , ... , x k ) subscript x 0 ... subscript x k (x_{0},\\dots,x_{k}) ( italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , ... , italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT )  corresponds to portions of the conditional flow matching trajectory. In this section, we give the implementation details of the theoretical discussion given in Section  3.2 ."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Single-cell perturbation response prediction comparison in terms of fit (    plus-or-minus   \\mu\\pm\\sigma italic_  italic_  over five repeated runs). CFM, scVI, scGPT and CPA are compared. Best results in bold. For CaLMFlow, R.I. stands for randomly initialized CLM and N.L. stands for natural language pretrained CLM. \"\" in Leiden KLD represents infinite value due to not being able to generate all classes.   : for CPA, no standard deviation is reported as it is deterministic; CPA generated data that resulted in numerical instability when computing MMD, leading to the absence of valid MMD values",
        "table": "A4.EGx1",
        "footnotes": [],
        "references": [
            "The results shown in Table  4  highlight CaLMFlows ability to generate data distributions that closely align with the ground truth, outperforming competing models. The correlation statistics shown in Table  5  underscore CaLMFlows effectiveness in producing realistic cell expression profiles that correspond to the specified combinatorial labels. Notably, both tables show that leveraging pretrained CLM weights enhances CaLMFlows performance, showcasing the power of utilizing natural language understanding abilities of CLMs in the CaLMFlow framework.",
            "To investigate the impact of the temperature parameter on CaLMFlows performance at inference, we varied the temperature for the 8-Gaussians to 2-Moons dataset. Figure  4  shows the best MMD and 2-Wasserstein values at   = 0.2  0.2 \\tau=0.2 italic_ = 0.2 , where the generated data closely matches the ground truth. Deviations from this value lead to less accurate transformations. Interestingly, it has been empirically found that the optimal temperature in LLMs is often below 1.0 to mitigate inference noise, an observation that aligns with our findings. The experiment also highlights the importance of the VAE component, as its removal significantly degrades performance.",
            "To test the impact of spatiotemporal tokenization, we use a similar setup as in  Tong et al. ( 2024 )  on the MNIST dataset (details in Subsection  4.1 ). All key hyperparameters, optimizers, and training configurations were kept identical to ensure consistency. Our results, as shown in Table  6 , demonstrate that CaLMFlow outperforms other methods and increasing the number of spatial tokens improves inception scores.",
            "and as    t  0   t 0 \\Delta t\\to 0 roman_ italic_t  0  we obtain Equation  4 , where we note that during inference there is no need of distinguishing between the  z z z italic_z  used in the integral operator and the prediciton, as the prediction is used iteratively to produce the next  z i + 1 superscript z i 1 z^{i+1} italic_z start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT , contrary to training where the ground truth is used to perform the process in parallel.",
            "By framing the problem as a sequence modeling task, the LLM effectively approximates the solution to the integral equation. The sequence of states  { z t } subscript z t \\{z_{t}\\} { italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT }  can be viewed as tokens in a sequence, where each state depends on all previous states due to the integral over past times  s s s italic_s  in Equation  4 . The LLM, with its inherent capability to model long-range dependencies through attention mechanisms, captures this dependence without the need to explicitly compute  f  subscript f  f_{\\theta} italic_f start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  and  G  subscript G  G_{\\theta} italic_G start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT .",
            "In practice, we model Equation  4  using the LLM as follows:"
        ]
    }
}