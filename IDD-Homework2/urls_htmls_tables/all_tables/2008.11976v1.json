{
    "S3.T1": {
        "caption": "Table 1: Statistics of train and test splits of the datasets.",
        "table": "<table id=\"S3.T1.4.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T1.4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.4.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Dataset</th>\n<th id=\"S3.T1.4.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">#Train sets</th>\n<th id=\"S3.T1.4.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">#Test sets</th>\n<th id=\"S3.T1.4.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">#Unique answers</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.4.1.2.1\" class=\"ltx_tr\">\n<td id=\"S3.T1.4.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Indoor - Gibson (Room + Building)</td>\n<td id=\"S3.T1.4.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">69,207</td>\n<td id=\"S3.T1.4.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">22,272</td>\n<td id=\"S3.T1.4.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">961</td>\n</tr>\n<tr id=\"S3.T1.4.1.3.2\" class=\"ltx_tr\">\n<td id=\"S3.T1.4.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">Outdoor - nuScenes</td>\n<td id=\"S3.T1.4.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">33,973</td>\n<td id=\"S3.T1.4.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">15,644</td>\n<td id=\"S3.T1.4.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">650</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Train and Test Splits.\nAfter refinement, we divided the datasets into train and test splits. The statistics of\nthese splits are given in table 1. For test splits, we have select\nsamples for which at least two annotators agreed on the answer. We also ensured that the\ntrain and test sets have the same set of answers."
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Results for both indoor and outdoor datasets.",
        "table": "<table id=\"S5.T2.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.4.1.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T2.4.1.1.1.1\" class=\"ltx_td ltx_border_tt\"></td>\n<td id=\"S5.T2.4.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T2.4.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Method</span></td>\n<td id=\"S5.T2.4.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">\n<span id=\"S5.T2.4.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">VQA-Accuracy</span> (%)</td>\n</tr>\n<tr id=\"S5.T2.4.1.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T2.4.1.2.2.1\" class=\"ltx_td\"></td>\n<td id=\"S5.T2.4.1.2.2.2\" class=\"ltx_td\"></td>\n<td id=\"S5.T2.4.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">Gibson</td>\n<td id=\"S5.T2.4.1.2.2.4\" class=\"ltx_td ltx_align_center\">nuScenes</td>\n</tr>\n<tr id=\"S5.T2.4.1.3.3\" class=\"ltx_tr\">\n<td id=\"S5.T2.4.1.3.3.1\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S5.T2.4.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">Naïve</td>\n<td id=\"S5.T2.4.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">8.61</td>\n<td id=\"S5.T2.4.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">22.46</td>\n</tr>\n<tr id=\"S5.T2.4.1.4.4\" class=\"ltx_tr\">\n<td id=\"S5.T2.4.1.4.4.1\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T2.4.1.4.4.1.1\" class=\"ltx_text ltx_font_bold\">Prior-Based Baselines</span></td>\n<td id=\"S5.T2.4.1.4.4.2\" class=\"ltx_td ltx_align_center\">Hasty-Student</td>\n<td id=\"S5.T2.4.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">27.22</td>\n<td id=\"S5.T2.4.1.4.4.4\" class=\"ltx_td ltx_align_center\">41.65</td>\n</tr>\n<tr id=\"S5.T2.4.1.5.5\" class=\"ltx_tr\">\n<td id=\"S5.T2.4.1.5.5.1\" class=\"ltx_td\"></td>\n<td id=\"S5.T2.4.1.5.5.2\" class=\"ltx_td ltx_align_center\">Question-Only</td>\n<td id=\"S5.T2.4.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\">40.26</td>\n<td id=\"S5.T2.4.1.5.5.4\" class=\"ltx_td ltx_align_center\">46.06</td>\n</tr>\n<tr id=\"S5.T2.4.1.6.6\" class=\"ltx_tr\">\n<td id=\"S5.T2.4.1.6.6.1\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"S5.T2.4.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\">Video-VQA</td>\n<td id=\"S5.T2.4.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">39.88</td>\n<td id=\"S5.T2.4.1.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_t\">52.14</td>\n</tr>\n<tr id=\"S5.T2.4.1.7.7\" class=\"ltx_tr\">\n<td id=\"S5.T2.4.1.7.7.1\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T2.4.1.7.7.1.1\" class=\"ltx_text ltx_font_bold\">Approaches</span></td>\n<td id=\"S5.T2.4.1.7.7.2\" class=\"ltx_td ltx_align_center\">Concatenate-Feature</td>\n<td id=\"S5.T2.4.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\">47.57</td>\n<td id=\"S5.T2.4.1.7.7.4\" class=\"ltx_td ltx_align_center\">53.66</td>\n</tr>\n<tr id=\"S5.T2.4.1.8.8\" class=\"ltx_tr\">\n<td id=\"S5.T2.4.1.8.8.1\" class=\"ltx_td\"></td>\n<td id=\"S5.T2.4.1.8.8.2\" class=\"ltx_td ltx_align_center\">Stitched-Image</td>\n<td id=\"S5.T2.4.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r\">50.53</td>\n<td id=\"S5.T2.4.1.8.8.4\" class=\"ltx_td ltx_align_center\">54.32</td>\n</tr>\n<tr id=\"S5.T2.4.1.9.9\" class=\"ltx_tr\">\n<td id=\"S5.T2.4.1.9.9.1\" class=\"ltx_td\"></td>\n<td id=\"S5.T2.4.1.9.9.2\" class=\"ltx_td ltx_align_center\">Transformer-Based</td>\n<td id=\"S5.T2.4.1.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r\">61.58</td>\n<td id=\"S5.T2.4.1.9.9.4\" class=\"ltx_td ltx_align_center\">64.91</td>\n</tr>\n<tr id=\"S5.T2.4.1.10.10\" class=\"ltx_tr\">\n<td id=\"S5.T2.4.1.10.10.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S5.T2.4.1.10.10.1.1\" class=\"ltx_text ltx_font_bold\">Human Performance</span></td>\n<td id=\"S5.T2.4.1.10.10.2\" class=\"ltx_td ltx_border_bb ltx_border_t\"></td>\n<td id=\"S5.T2.4.1.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">88.80</td>\n<td id=\"S5.T2.4.1.10.10.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">91.88</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Stitched Image.\nOur next baseline is also an adaptation of existing single-image VQA methods. We start by\nstitching all the images in an image set into a mosaic, similar to the ones shown in figure 6. Note that the ISVQA setting does not require the images in an image set to\nfollow an order. Therefore, the stitched image obtained need not be panoramic. We train the recent\nPythia [27] model on the stitched images and report performance in table 2.",
            "An ideal image-set question answering system should be able to reach at least the accuracy achieved\nby humans. We evaluate the human performance using the annotations with the standard VQA-accuracy\nmetric described below. For the outdoor scenes dataset, humans obtain a VQA-accuracy of 91.88% and\nfor the indoor scenes they obtain 88.80%. Comparing this with table 2\nshows that ISVQA is extremely challenging and requires specialized methods. The reason for\nthe human performance being lower than 100% is that, in many cases an annotator has given an answer\nwhich is not exactly similar to the other two but is still semantically similar. For example, the\nmajority answer might have been “black and white” but the third annotator answered “white and\nblack”.",
            "We report the VQA-accuracy for all methods in table 2. The accuracy achieved by\nboth of the VQA-based baselines is only around 50−54%50percent5450-54\\% and the Video VQA model achieves only\n39.88%percent39.8839.88\\% for the indoor dataset and 52.14%percent52.1452.14\\% for the outdoor dataset. This highlights the need for\nadvanced models for ISVQA.",
            "Comparison between Baselines.\nTable 2 shows that the naïve baseline reaches a VQA-Accuracy of only 8.6% for the\nindoor scenes dataset compared to 47.57% given by the Concatenate-Feature baseline and 50.53%\ngiven by the Stitched-Image baseline model. This shows that single-image VQA methods are not enough\nto overcome the challenges presented by ISVQA. On the other hand, our proposed transformer-based\nmodel performs the best for both indoor and outdoor scenes out-performing other models by over\n10%."
        ]
    }
}