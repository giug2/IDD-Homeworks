{
    "S2.T1": {
        "caption": "Table 1: Average accuracy for the best hyperparameter setting for each learning algorithm, algorithm selection (Default), both algorithm selection and hyperparameter optimization (ALL), and Auto-WEKA (AW).",
        "table": "<table id=\"S2.T1.1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S2.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S2.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">BP</th>\n<th id=\"S2.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">C4.5</th>\n<th id=\"S2.T1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<math id=\"S2.T1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"k\" display=\"inline\"><semantics id=\"S2.T1.1.1.1.1.m1.1a\"><mi id=\"S2.T1.1.1.1.1.m1.1.1\" xref=\"S2.T1.1.1.1.1.m1.1.1.cmml\">k</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.1.1.1.1.m1.1b\"><ci id=\"S2.T1.1.1.1.1.m1.1.1.cmml\" xref=\"S2.T1.1.1.1.1.m1.1.1\">ùëò</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.1.1.1.1.m1.1c\">k</annotation></semantics></math>NN</th>\n<th id=\"S2.T1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">LWL</th>\n<th id=\"S2.T1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">NB</th>\n<th id=\"S2.T1.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">NNge</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S2.T1.1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">79.89</td>\n<td id=\"S2.T1.1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">79.22</td>\n<td id=\"S2.T1.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">78.05</td>\n<td id=\"S2.T1.1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">77.48</td>\n<td id=\"S2.T1.1.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">76.04</td>\n<td id=\"S2.T1.1.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">76.80</td>\n</tr>\n<tr id=\"S2.T1.1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\">RF</td>\n<td id=\"S2.T1.1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">Rid</td>\n<td id=\"S2.T1.1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">RIP</td>\n<td id=\"S2.T1.1.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">Default</td>\n<td id=\"S2.T1.1.1.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">ALL</td>\n<td id=\"S2.T1.1.1.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\">AW</td>\n</tr>\n<tr id=\"S2.T1.1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">79.58</td>\n<td id=\"S2.T1.1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">71.48</td>\n<td id=\"S2.T1.1.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">77.31</td>\n<td id=\"S2.T1.1.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">81.93</td>\n<td id=\"S2.T1.1.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">83.00</td>\n<td id=\"S2.T1.1.1.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">82.00</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "To establish a baseline, we first calculate the accuracy on a set of 125 data sets and 9 diverse learning algorithms (see¬†[14] for a discussion on diversity) with default parameters as set in Weka¬†[8]. The set of learning algorithms is composed of backpropagation (BP), C4.5, kùëòkNN, locally weight learning (LWL), na√Øve Bayes (NB), nearest neighbor with generalization (NNge), random forest (RF), ridor (Rid), and RIPPER (RIP). We select the accuracy from the learning algorithm that produces the highest classification accuracy. This represents algorithm selection with perfect recall. We also estimate the hyperparameter optimized accuracies for each learning algorithm using random hyperparameter optimization¬†[3]. The results are shown in Table 1, where the accuracy from each learning algorithm is the average hyperparameter optimized accuracy for each data set, ‚ÄúDefault‚Äù refers to the best accuracy from the learning algorithm with its default parameters, ‚ÄúALL‚Äù refers to the accuracy from the best learning algorithm and hyperparameter setting, and ‚ÄúAW‚Äù refers to the results from running Auto-WEKA. For Auto-WEKA, each dataset was allowed to run as long as the longest algorithm took to run on the dataset when doing the random hyperparameter optimization. As Auto-WEKA is a random algorithm, we ran 4 runs each time with a different seed and chose the seed with highest accuracy. This can be seen as equivalent to allowing a user to run on average 16 learning algorithm and hyperparameter combinations on a data set."
        ]
    },
    "S2.T2": {
        "caption": "Table 2: Average accuracy from the best of the top 4 recommended learning algorithm and hyperparameter settings from MCF.",
        "table": "<table id=\"S2.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S2.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S2.T2.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"/>\n<th id=\"S2.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">Best</th>\n<th id=\"S2.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">Med</th>\n<th id=\"S2.T2.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">Ave</th>\n<th id=\"S2.T2.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.1</th>\n<th id=\"S2.T2.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.3</th>\n<th id=\"S2.T2.1.1.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.5</th>\n<th id=\"S2.T2.1.1.1.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.7</th>\n<th id=\"S2.T2.1.1.1.9\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">0.9</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S2.T2.1.2.1\" class=\"ltx_tr\">\n<th id=\"S2.T2.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">Baseline</th>\n<td id=\"S2.T2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">81.11</td>\n<td id=\"S2.T2.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">81.11</td>\n<td id=\"S2.T2.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">81.11</td>\n<td id=\"S2.T2.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S2.T2.1.2.1.5.1\" class=\"ltx_text ltx_font_bold\">80.49</span></td>\n<td id=\"S2.T2.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">80.91</td>\n<td id=\"S2.T2.1.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">81.12</td>\n<td id=\"S2.T2.1.2.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">81.33</td>\n<td id=\"S2.T2.1.2.1.9\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">81.54</td>\n</tr>\n<tr id=\"S2.T2.1.3.2\" class=\"ltx_tr\">\n<th id=\"S2.T2.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">FKM</th>\n<td id=\"S2.T2.1.3.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">81.52</td>\n<td id=\"S2.T2.1.3.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">81.04</td>\n<td id=\"S2.T2.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_rr\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">81.29</td>\n<td id=\"S2.T2.1.3.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">80.13</td>\n<td id=\"S2.T2.1.3.2.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">80.65</td>\n<td id=\"S2.T2.1.3.2.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">81.07</td>\n<td id=\"S2.T2.1.3.2.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">81.45</td>\n<td id=\"S2.T2.1.3.2.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">81.88</td>\n</tr>\n<tr id=\"S2.T2.1.4.3\" class=\"ltx_tr\">\n<th id=\"S2.T2.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">MF</th>\n<td id=\"S2.T2.1.4.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S2.T2.1.4.3.2.1\" class=\"ltx_text ltx_font_bold\">82.12</span></td>\n<td id=\"S2.T2.1.4.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S2.T2.1.4.3.3.1\" class=\"ltx_text ltx_font_bold\">82.06</span></td>\n<td id=\"S2.T2.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_rr\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S2.T2.1.4.3.4.1\" class=\"ltx_text ltx_font_bold\">81.95</span></td>\n<td id=\"S2.T2.1.4.3.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S2.T2.1.4.3.5.1\" class=\"ltx_text ltx_font_bold\">80.49</span></td>\n<td id=\"S2.T2.1.4.3.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S2.T2.1.4.3.6.1\" class=\"ltx_text ltx_font_bold\">81.63</span></td>\n<td id=\"S2.T2.1.4.3.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S2.T2.1.4.3.7.1\" class=\"ltx_text ltx_font_bold\">82.12</span></td>\n<td id=\"S2.T2.1.4.3.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S2.T2.1.4.3.8.1\" class=\"ltx_text ltx_font_bold\">82.44</span></td>\n<td id=\"S2.T2.1.4.3.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span id=\"S2.T2.1.4.3.9.1\" class=\"ltx_text ltx_font_bold\">82.65</span></td>\n</tr>\n<tr id=\"S2.T2.1.5.4\" class=\"ltx_tr\">\n<th id=\"S2.T2.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">NLPCA</th>\n<td id=\"S2.T2.1.5.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">81.73</td>\n<td id=\"S2.T2.1.5.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">81.33</td>\n<td id=\"S2.T2.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_rr\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">81.33</td>\n<td id=\"S2.T2.1.5.4.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">79.98</td>\n<td id=\"S2.T2.1.5.4.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">80.58</td>\n<td id=\"S2.T2.1.5.4.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">81.43</td>\n<td id=\"S2.T2.1.5.4.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">82.08</td>\n<td id=\"S2.T2.1.5.4.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">82.61</td>\n</tr>\n<tr id=\"S2.T2.1.6.5\" class=\"ltx_tr\">\n<th id=\"S2.T2.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">UBP</th>\n<td id=\"S2.T2.1.6.5.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">81.73</td>\n<td id=\"S2.T2.1.6.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">81.27</td>\n<td id=\"S2.T2.1.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_rr\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">81.31</td>\n<td id=\"S2.T2.1.6.5.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">80.05</td>\n<td id=\"S2.T2.1.6.5.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">80.51</td>\n<td id=\"S2.T2.1.6.5.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">81.34</td>\n<td id=\"S2.T2.1.6.5.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">82.05</td>\n<td id=\"S2.T2.1.6.5.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">82.61</td>\n</tr>\n<tr id=\"S2.T2.1.7.6\" class=\"ltx_tr\">\n<th id=\"S2.T2.1.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">Content</th>\n<td id=\"S2.T2.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">81.35</td>\n<td id=\"S2.T2.1.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">80.47</td>\n<td id=\"S2.T2.1.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">78.91</td>\n<td id=\"S2.T2.1.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">-</td>\n<td id=\"S2.T2.1.7.6.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">-</td>\n<td id=\"S2.T2.1.7.6.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">-</td>\n<td id=\"S2.T2.1.7.6.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">-</td>\n<td id=\"S2.T2.1.7.6.9\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\">-</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "For MCF, we compiled the results from hyperparameter optimization. We randomly removed 10% to 90% of the results by increments of 10% and then used MCF to fill in the missing values. The top 4 learning algorithm/hyperparameter configurations are returned by the CF technique and the accuracy from the configuration that returns the highest classification accuracy is used. This process was repeated 10 times. A summary of the average results for MCF are provided in Table 2.\nThe columns ‚ÄúBest‚Äù, ‚ÄúMedian‚Äù, and ‚ÄúAverage‚Äù refer to the accuracies averaged across all of the sparsity levels for the hyperparameter setting for the CF technique that provided the results.\nThe columns 0.1 to 0.9 refer to the percentage of the results used for CF averaged over the hyperparameter settings.\nThe row ‚ÄúContent‚Äù refers to meta-learning where a learning algorithm recommends a learning algorithm based on a set of meta-features."
        ]
    }
}