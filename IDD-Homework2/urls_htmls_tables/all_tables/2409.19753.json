{
    "id_table_1": {
        "caption": "Table 1:  The overall results of CoTKR and the baselines on GrailQA and GraphQuestions using 2-Hop as retrieval method. For each combination of a knowledge rewriter (KR) LLM and a QA model, the best and second-best results are highlighted in bold and underlined, respectively.",
        "table": "S5.T1.1.1",
        "footnotes": [],
        "references": [
            "Recent work  Wu et al. ( 2023b ); Baek et al. ( 2023 ); Sen et al. ( 2023 ); Wang et al. ( 2024 )  under the RAG paradigm explores the use of Knowledge Graphs (KGs)  Pan et al. ( 2017b ,  a )  as an information source to enhance the capabilities of LLMs in Question Answering (QA). Unlike typical QA tasks, a key challenge in KGQA under this paradigm lies in transforming question-related subgraphs into natural language that LLMs can understand while preserving the structural information  Ko et al. ( 2024 ); Ding et al. ( 2024 ); Wu et al. ( 2023b ) . This process is referred to as Knowledge Rewriting (KR) in this study. As illustrated in Figure  1 , this paper summarizes the commonly used knowledge rewriting methods in existing work. Most prior studies  Baek et al. ( 2023 ); Sen et al. ( 2023 ); Wang et al. ( 2023a )  employ simple linear concatenation method (Triple), which concatenates the subject, relation, and object of each triple to form triple-form text. Additionally, considering that LLMs are pre-trained on text corpora and struggle with structured triple-form text, some efforts  Wu et al. ( 2023b ); Bian et al. ( 2021 ); Chen et al. ( 2022 )  focus on converting triples into natural language through KG-to-Text. Furthermore, given that retrieved subgraphs often contain redundant information irrelevant to the question, other studies  Ko et al. ( 2024 ); Dernbach et al. ( 2024 )  aim to extract question-relevant knowledge from the triples to generate summary pertinent to the question.",
            "Although these strategies are effective, they exhibit several limitations:  (1) Redundancy or omissions.  As illustrated in Figure  1 , knowledge generated by  Triple  and  KG-to-Text  are verbose, containing excessive irrelevant information.  Summary  provides a question-related summary but attempts to organize all relevant knowledge in one step. Given the extensive knowledge necessary to address complex questions, this method may not encapsulate all critical information, potentially resulting in the omission of key points.  (2) Semantic mismatch.  The three existing methods shown in Figure  1  ignore the semantics of the question and lack a logical organization that aligns with the questions reasoning path.",
            "To this end, we propose  C hain- o f- T hought Enhanced  K nowledge  R ewriting, CoTKR. Inspired by ReAct  Yao et al. ( 2023 ) , the core of our method involves generating reasoning traces and corresponding knowledge in an interleaved manner. As shown in Figure  1 , we alternate the following two operations: (1)  Reasoning : decomposing the question to identify the knowledge required for inference; (2)  Summarization : summarizing the relevant knowledge from retrieved triples, informed by the reasoning steps output. By integrating Chain-of-Thought (CoT)  Wei et al. ( 2022b )  with knowledge rewriting, CoTKR filters out irrelevant information and extracts question-related knowledge. Moreover, it generates a well-organized knowledge representation 2 2 2 In this paper, knowledge representation refers to the natural language form of question-related knowledge.  semantically aligned with the question. Unlike traditional CoT applications in QA, our framework employs the knowledge rewriter to first summarize knowledge, which then serves as contextual information to enhance QA performance. This strategy offers superior robustness. Although the summary might be inaccurate, it still contributes valuable information, potentially leading to correct answers. However, applying CoT to QA requires more precise reasoning chains, which are significantly affected by the error propagation  Wang et al. ( 2023a ); Yao et al. ( 2023 ) . To train knowledge rewriters based on LLMs, we design a training framework for CoTKR. In the first stage, inspired by previous work  Ma et al. ( 2023a ); Wu et al. ( 2023b ); Ko et al. ( 2024 ) , we use knowledge representations generated by ChatGPT to guide the supervised fine-tuning of the knowledge rewriter, enabling it to initially master the capability of knowledge rewriting. In the second stage, we introduce  P reference  A lignment from  Q uestion  A nswering  F eedback (PAQAF) to bridge the preference gap between the knowledge rewriter and the QA model. This method evaluates the quality of different knowledge representations based on the corresponding responses from the QA model. Subsequently, it constructs preference pairs, and fine-tunes LLMs through direct preference optimization (DPO)  Rafailov et al. ( 2023 ) .",
            "To comprehensively evaluate various knowledge rewriting methods, we employ the widely used 2-Hop retrieval method. Table  1  presents the overall results. We observe that:  (1) Our method outperforms the baselines across most evaluation metrics and LLMs, confirming the effectiveness of our knowledge rewriting strategy.  This also demonstrates the broad practical applicability of  CoTKR , effective for both open-source LLMs requiring fine-tuning and closed-source LLMs using ICL. Integrating question-related knowledge significantly improves QA performance compared with direct question answering, underscoring the efficacy of the RAG paradigm in KGQA.  KG-to-Text  exhibits the weakest performance, indicating that mere conversion of triples into text may result in loss of information inherent in the subgraph.  Summary  outperforms  KG-to-Text  but generally lags behind  CoTKR / CoTKR+PA , suggesting that filtering out irrelevant knowledge is effective but not adequate.  (2) CoTKR+PA matches or even surpasses the performance of ChatGPT as the knowledge rewriter, proving the effectiveness of our training framework and the preference alignment.   CoTKR+PA  outperforms  CoTKR , indicating that preference alignment can bridge the preference gap between the knowledge rewriter and the QA model, thereby enhancing the quality of knowledge representation.  (3) A well-crafted knowledge representation is crucial for LLM used in KGQA.  Although  Triple  does not require an additional knowledge rewriting module, it provides a strong baseline and, in some cases, outperforms  KG-to-Text  and  Summary . Conversely,  CoTKR+PA  consistently surpasses  Triple . This indicates that  Triple  is simple yet effective and explains its widespread use in existing work. On the other hand, it demonstrates that a carefully designed knowledge representation can effectively enhance the performance of KGQA.",
            "To assess the applicability of CoTKR to GPT-4, we further conduct the experiments with GPT-4 as the knowledge rewriter, Mistral as the question-answering model, and 2 hop as the retrieval method on 1,000 test questions from GrailQA. The detailed results are presented in Table  10 . The findings show that CoTKR outperforms other approaches, with CoTKR utilizing GPT-4 achieving the highest performance. This suggests that employing a more advanced LLM backbone, such as GPT-4, leads to superior outcomes.",
            "We conduct experiments to analyze the average time cost of the knowledge rewriting methods discussed in this paper. We adopt Llama-3 as the knowledge rewriter, Mistral as the question-answering model, and 2-Hop as the retrieval method. The experiments are conducted on GraphQuestions, utilizing one A100-SXM4-40GB GPU. The average runtime for each question by different methods is shown in Table  11  (unit: seconds). The average runtime of each question for all methods is within an acceptable range (i.e., less than 1.5 seconds). Although our method is the most time-consuming, it exhibits a clear advantage in performance.",
            "Given the powerful natural language understanding and generation capabilities of closed-source large models, many existing works employ ChatGPT as an evaluator to provide high-quality evaluation results  Sottana et al. ( 2023 ); Liu et al. ( 2023 ); Min et al. ( 2023 ) . In our approach, we use GPT-4 as the evaluator to assess whether all answer entities are present in the responses. We refer to this evaluation metric as  GPT-4-score . Compared to EM, this metric is more flexible, as LLMs are capable of recognizing synonyms of answer entities. We use it to evaluate the first 300 questions from GrailQA using Llama-3 as knowledge rewriter, ChatGPT as question-answering model, and 2 hop as retrieval method. We provide the prompt for  GPT-4-score  in Table  12 .",
            "The experimental results are shown in Table  13 . The results indicate that our implemented GPT-4-score is effective and consistent with the outcomes reflected by other evaluation metrics. Furthermore, this also demonstrates that CoTKR possesses significant advantages compared to other knowledge rewriting methods.",
            "To clearly demonstrate the importance of data augmentation, we perform a qualitative analysis. The results reveal that data augmentation enhances performance in three key areas: reducing redundant reasoning steps, supplementing critical information, and generating more concise summaries. These improvements are exemplified by three representative cases, as shown in Table  14 .",
            "This section presents all the experimental results of this study. As shown in Table  15 , Table  16 , and Table  17 , CoTKR/CoTKR+PA achieves the best performance in most scenarios. This indicates that CoTKR is effective for both open-source LLMs after training and closed-source LLMs using ICL. Besides, the results also reveal the robustness of CoTKR, demonstrating its applicability across KGQA systems with various retrieval methods and QA models."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Ablation study results for data augmentation on GraphQuestions, employing the 2-Hop retrieval method.",
        "table": "S5.T2.1.1",
        "footnotes": [],
        "references": [
            "The architecture of our QA framework is depicted in Figure  2 . Initially, our framework retrieves a question-related subgraph from the KG, which is subsequently transformed into contextual knowledge using CoTKR. This contextual knowledge, along with the question, prompts the QA model to generate an answer. The core of this framework is the knowledge rewriter.  Briefly, it alternatively conducts the following two operations:  Reasoning : decomposing the question and generating a reasoning trace based on generated knowledge representation and pointing out the specific knowledge needed for the current step;  Summarization : summarizing the relevant knowledge based on the current reasoning trace from the subgraph.",
            "To evaluate the effectiveness of data augmentation, we compare the performance of three variants:  CoTKR ,  CoTKR+PA , and  CoTKR+PA *  (using supervised fine-tuning and preference alignment without data augmentation). The experimental results are shown in Table  2 , from which we can draw two conclusions:  (1) CoTKR+PA *  generally outperforms CoTKR+PA, indicating that PAQAF does not solely rely on data augmentation based on ChatGPT.   (2) CoTKR+PA performs best in most scenarios, proving that data augmentation enhances the preference alignment.",
            "Given the powerful natural language understanding and generation capabilities of closed-source large models, many existing works employ ChatGPT as an evaluator to provide high-quality evaluation results  Sottana et al. ( 2023 ); Liu et al. ( 2023 ); Min et al. ( 2023 ) . In our approach, we use GPT-4 as the evaluator to assess whether all answer entities are present in the responses. We refer to this evaluation metric as  GPT-4-score . Compared to EM, this metric is more flexible, as LLMs are capable of recognizing synonyms of answer entities. We use it to evaluate the first 300 questions from GrailQA using Llama-3 as knowledge rewriter, ChatGPT as question-answering model, and 2 hop as retrieval method. We provide the prompt for  GPT-4-score  in Table  12 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  An example of knowledge rewriting results for different methods.",
        "table": "S5.T3.1.1",
        "footnotes": [],
        "references": [
            "Figure  3  illustrates the training framework of CoTKR.",
            "In this experiment, Llama-2 (7B)  Touvron et al. ( 2023b ) , Llama-3 (8B)  AI@Meta ( 2024 ) , and ChatGPT  3 3 3 https://api.openai.com/  are employed for knowledge rewriting, while ChatGPT and Mistral (7B)  Jiang et al. ( 2023a )  are used for QA tasks. The details of these LLMs are provided in Appendix  A.3 .",
            "In this section, we compare  Summary  with  CoTKR  through an example. (Please refer to Appendix  B  for the full comparison result.) As illustrated in Table  3 ,  Summary  struggles to extract useful information when faced with an abundance of triples. In contrast,  CoTKR , leveraging CoT reasoning, effectively emphasizes the key evidence (i.e., Square meter) in the second rewriting step. Furthermore, after preference alignment,  CoTKR+PA  is capable of generating more natural reasoning steps, significantly enhancing its applicability to KGQA.",
            "The experimental results are shown in Table  13 . The results indicate that our implemented GPT-4-score is effective and consistent with the outcomes reflected by other evaluation metrics. Furthermore, this also demonstrates that CoTKR possesses significant advantages compared to other knowledge rewriting methods."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Prompts for Knowledge Rewriting Methods.",
        "table": "A1.T8.1.1",
        "footnotes": [],
        "references": [
            "The retrieval module is not the focus of our research. Therefore, we adopt three commonly used retrieval methods. For detailed implementation, please refer to Appendix  A.4 .",
            "To investigate the impact of retrieval methods, we select Llama-3 as the knowledge rewriter and ChatGPT as the QA model. According to the results shown in Figure  4 , we have the following observations:  (1) 2-Hop retrieval method may be insufficient for more challenging questions, but it is suitable for simpler ones.  Both BM25 and 2-Hop perform similarly on GrailQA, but 2-Hop shows a significant advantage over BM25 on GraphQuestions. This is likely because GrailQA is a more complex benchmark with a larger question-related subgraph, making 2-hop subgraphs often inadequate. Conversely, for GraphQuestions, a 2-hop subgraph usually provides precise context for most questions.  (2) The design of a high-quality retriever remains an open problem.  GS significantly outperforms BM25 and 2-Hop, indicating that retrieval noise substantially affects KGQA performance.  (3) CoTKR consistently outperforms all baselines across various retrieval methods, demonstrating its robustness and practicality.",
            "All the prompts involved in this experiment are as follows. Table  4  shows the prompts for different knowledge rewriting methods. Table  5  shows the prompts for question answering. Table  6  shows the preference annotation prompt.",
            "To clearly demonstrate the importance of data augmentation, we perform a qualitative analysis. The results reveal that data augmentation enhances performance in three key areas: reducing redundant reasoning steps, supplementing critical information, and generating more concise summaries. These improvements are exemplified by three representative cases, as shown in Table  14 ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Prompts for Question Answering.",
        "table": "A3.T9.1.1",
        "footnotes": [],
        "references": [
            "Several studies  Dai et al. ( 2024 ); Baek et al. ( 2023 )  suggest that LLMs can better comprehend triple-form text compared with natural language. However, our results show the contrary. Therefore, we delve deeper into this issue by comparing knowledge rewriting methods that use triple-form text as input (i.e.,  KG-to-Text ,  Summary ,  CoTKR ,  CoTKR+PA ) with  Triple . We use Accuracy as the criterion to evaluate the correctness of responses. For each method, we consider three scenarios:  (1) Incorrect   \\rightarrow  Correct:   Triple  provides a wrong answer, but the comparative method answers correctly.  (2) Correct->Incorrect:   Triple  answers correctly, but the comparative method answers incorrectly.  (3) No change:  both  Triple  and the comparative method answer correctly or incorrectly. We adopt Llama-3 as the knowledge rewriter and ChatGPT as the QA model, with 2-Hop as the retrieval method. Then we calculate the proportions of three distinct cases within GrailQA. From the results shown in Figure  5 , we draw the following conclusions:  (1) KG-to-Text and Summary have a predominantly negative impact, partially validating the conclusions of prior studies.   Triple  provides a strong baseline, and the adoption of  KG-to-Text  and  Summary  leads to more incorrect answers. This indicates that LLMs can understand triple-form text effectively, and using simple knowledge rewriting methods leads to loss of information.  (2) Well-designed knowledge representations substantially benefit the question-answering model.  The knowledge representations rewritten by  CoTKR/CoTKR+PA  generally enhance the QA models performance. This reflects that the suboptimal knowledge representations in previous work are key contributors to performance degradation. Our method generates comprehensive and semantically coherent knowledge representations, thereby improving the efficacy of KGQA.",
            "All the prompts involved in this experiment are as follows. Table  4  shows the prompts for different knowledge rewriting methods. Table  5  shows the prompts for question answering. Table  6  shows the preference annotation prompt.",
            "This section presents all the experimental results of this study. As shown in Table  15 , Table  16 , and Table  17 , CoTKR/CoTKR+PA achieves the best performance in most scenarios. This indicates that CoTKR is effective for both open-source LLMs after training and closed-source LLMs using ICL. Besides, the results also reveal the robustness of CoTKR, demonstrating its applicability across KGQA systems with various retrieval methods and QA models."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Preference Annotation Prompt.",
        "table": "A3.T10.1.1",
        "footnotes": [],
        "references": [
            "Data Augmentation based on ChatGPT . ChatGPT is able to produce higher-quality knowledge representations, compared with open-source LLMs. Therefore, in order to improve the quality of preferred knowledge representation and enhance the diversity of the training data, we employ ChatGPT to paraphrase  k + superscript k k^{+} italic_k start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT . In addition to the question  q q q italic_q , the retrieved subgraph  G  superscript G  G^{\\prime} italic_G start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , and the preferred knowledge representation  k + superscript k k^{+} italic_k start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT , we also provide the answer entity  e e e italic_e . This allows ChatGPT to organize relevant knowledge around  e e e italic_e , ensuring that the rewritten knowledge covers key evidence. We concatenate the question  q q q italic_q  and the textualized subgraph  G  superscript G  G^{\\prime} italic_G start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  using a prompt template as the input  x x x italic_x , and use the paraphrased knowledge representation  k + + superscript k absent k^{++} italic_k start_POSTSUPERSCRIPT + + end_POSTSUPERSCRIPT  and  k  superscript k k^{-} italic_k start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT  as the preferred pair. Finally, we construct the preference dataset  P N = ( x 1 , k 1 + + , k 1  ) , ( x 2 , k 2 + + , k 2  ) , ... , ( x N , k N + + , k N  ) subscript P N subscript x 1 superscript subscript k 1 absent superscript subscript k 1 subscript x 2 superscript subscript k 2 absent superscript subscript k 2 ... subscript x N superscript subscript k N absent superscript subscript k N P_{N}={(x_{1},k_{1}^{++},k_{1}^{-}),(x_{2},k_{2}^{++},k_{2}^{-}),...,(x_{N},k_%  {N}^{++},k_{N}^{-})} italic_P start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT = ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + + end_POSTSUPERSCRIPT , italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT ) , ( italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + + end_POSTSUPERSCRIPT , italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT ) , ... , ( italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT , italic_k start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + + end_POSTSUPERSCRIPT , italic_k start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT ) . The prompt for knowledge augmentation is in Appendix  A.6 .",
            "All the prompts involved in this experiment are as follows. Table  4  shows the prompts for different knowledge rewriting methods. Table  5  shows the prompts for question answering. Table  6  shows the preference annotation prompt.",
            "This section presents all the experimental results of this study. As shown in Table  15 , Table  16 , and Table  17 , CoTKR/CoTKR+PA achieves the best performance in most scenarios. This indicates that CoTKR is effective for both open-source LLMs after training and closed-source LLMs using ICL. Besides, the results also reveal the robustness of CoTKR, demonstrating its applicability across KGQA systems with various retrieval methods and QA models."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Data Augmentation Prompt.",
        "table": "A3.T11.1.1",
        "footnotes": [],
        "references": [
            "Table  7  shows the data augmentation prompt.",
            "This section presents all the experimental results of this study. As shown in Table  15 , Table  16 , and Table  17 , CoTKR/CoTKR+PA achieves the best performance in most scenarios. This indicates that CoTKR is effective for both open-source LLMs after training and closed-source LLMs using ICL. Besides, the results also reveal the robustness of CoTKR, demonstrating its applicability across KGQA systems with various retrieval methods and QA models."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  An example of knowledge rewriting results for different methods. We use Llama-3 and Mistral as the Knowledge Rewriting (KR) backbone and the QA model, respectively.",
        "table": "A3.T13.1.1",
        "footnotes": [],
        "references": [
            "Preference Annotation based on Question Answering Feedback.  Among the candidate knowledge representations, we select the two,  k 1 subscript k 1 k_{1} italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and  k 2 subscript k 2 k_{2} italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , with the greatest semantic difference (i.e., the lowest similarity) to facilitate faster convergence during training. Utilizing standard evaluation methods for assessing these representations is suboptimal, as they fail to align with the preferences of QA models. Inspired by the findings in previous work Wu et al. ( 2023b ); Ko et al. ( 2024 ); Zhang et al. ( 2023 ) , we posit that better knowledge representations generally lead to better performance on QA. Consequently, we adopt  k 1 subscript k 1 k_{1} italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and  k 2 subscript k 2 k_{2} italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  as contextual knowledge, prompting the QA model  Q Q Q italic_Q  to answer the question  q q q italic_q , generating answers  a 1 subscript a 1 a_{1} italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and  a 2 subscript a 2 a_{2} italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , respectively. Subsequently, we prompt ChatGPT to assess the quality of  a 1 subscript a 1 a_{1} italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and  a 2 subscript a 2 a_{2} italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  from the perspectives of accuracy and relevance. This evaluation aims to identify the preferred knowledge representation  k + superscript k k^{+} italic_k start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT  and the dispreferred knowledge representation  k  superscript k k^{-} italic_k start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT . Details of the evaluation prompt are provided in Appendix  8 .",
            "In this section, we compare different knowledge rewriting strategies through an example. As illustrated in Table  8 , the knowledge generated by both  Triple  and  KG-to-Text  contains excessive redundant information. This redundancy complicates the process for the QA model, making it challenging to extract relevant knowledge.  Summary  struggles to extract useful information when faced with an abundance of triples. In contrast,  CoTKR  and  CoTKR+PA  summarize the most pertinent knowledge in the rewriting step, thereby enabling the QA model to provide a concise and accurate answer. Furthermore, after preference alignment, our knowledge rewriter is capable of generating more natural  reasoning steps, significantly enhancing its applicability to KGQA."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Experiments on ComplexWebQuestions use ChatGPT as the knowledge rewriter, Mistral as the QA model, and 2-Hop for retrieval.",
        "table": "A3.T14.2.2",
        "footnotes": [],
        "references": [
            "To evaluate the robustness of CoTKR, we conduct our experiments on ComplexWebQuestions Talmor and Berant ( 2018 ) . We utilize ChatGPT as the knowledge rewriter, Mistral as the question-answering model, and 2-Hop as the retrieval method. The experimental results, presented in Table  9 , demonstrate the effectiveness of CoTKR."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:  Experiments on 1,000 GrailQA test questions use GPT-4 for rewriting, Mistral for QA, and 2-Hop as the retrieval method.",
        "table": "A3.T15.1.1",
        "footnotes": [],
        "references": [
            "To assess the applicability of CoTKR to GPT-4, we further conduct the experiments with GPT-4 as the knowledge rewriter, Mistral as the question-answering model, and 2 hop as the retrieval method on 1,000 test questions from GrailQA. The detailed results are presented in Table  10 . The findings show that CoTKR outperforms other approaches, with CoTKR utilizing GPT-4 achieving the highest performance. This suggests that employing a more advanced LLM backbone, such as GPT-4, leads to superior outcomes."
        ]
    },
    "id_table_11": {
        "caption": "Table 11:  Time analysis on GraphQuestions (seconds) using Llama-3 as the knowledge rewriter, Mistral for question answering, and 2-Hop for retrieval.",
        "table": "A3.T16.1.1",
        "footnotes": [],
        "references": [
            "We conduct experiments to analyze the average time cost of the knowledge rewriting methods discussed in this paper. We adopt Llama-3 as the knowledge rewriter, Mistral as the question-answering model, and 2-Hop as the retrieval method. The experiments are conducted on GraphQuestions, utilizing one A100-SXM4-40GB GPU. The average runtime for each question by different methods is shown in Table  11  (unit: seconds). The average runtime of each question for all methods is within an acceptable range (i.e., less than 1.5 seconds). Although our method is the most time-consuming, it exhibits a clear advantage in performance."
        ]
    },
    "id_table_12": {
        "caption": "Table 12:  GPT-4-score Prompt.",
        "table": "A3.T17.1.1",
        "footnotes": [],
        "references": [
            "Given the powerful natural language understanding and generation capabilities of closed-source large models, many existing works employ ChatGPT as an evaluator to provide high-quality evaluation results  Sottana et al. ( 2023 ); Liu et al. ( 2023 ); Min et al. ( 2023 ) . In our approach, we use GPT-4 as the evaluator to assess whether all answer entities are present in the responses. We refer to this evaluation metric as  GPT-4-score . Compared to EM, this metric is more flexible, as LLMs are capable of recognizing synonyms of answer entities. We use it to evaluate the first 300 questions from GrailQA using Llama-3 as knowledge rewriter, ChatGPT as question-answering model, and 2 hop as retrieval method. We provide the prompt for  GPT-4-score  in Table  12 ."
        ]
    },
    "global_footnotes": [
        "Our code is available at https://github.com/wuyike2000/CoTKR.",
        "In this paper, knowledge representation refers to the natural language form of question-related knowledge.",
        "https://api.openai.com/",
        "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2",
        "https://huggingface.co/meta-llama/Llama-2-7b-chat-hf",
        "https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct",
        "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3",
        "https://api.openai.com/",
        "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2",
        "https://pytorch.org/",
        "https://huggingface.co/docs/transformers/en/index",
        "https://www.deepspeed.ai/",
        "https://huggingface.co/docs/datasets/en/index",
        "https://huggingface.co/docs/peft/en/index",
        "https://huggingface.co/docs/trl/en/index"
    ]
}