{
    "id_table_1": {
        "caption": "Table 1:  Impact of task-specific instructions on the normal (original) context. Having the additional task instruction degrades the performance on normal contexts consistently.",
        "table": "S5.T1.18.18",
        "footnotes": [],
        "references": [
            "In this work, we introduce FaithEval, a comprehensive benchmark specifically designed to evaluate the contextual faithfulness of LLMs across  three  diverse tasks: unanswerable, inconsistent, and counterfactual contexts. These tasks simulate real-world challenges where retrieval mechanisms may surface incomplete, contradictory, or fabricated information (Figure  2 ). FaithEval includes a total of 4.9K high-quality samples, constructed using a rigorous four-stage framework with multi-turn LLM-based context verification and human validation. We conduct a holistic evaluation on 18 representative proprietary and open-sourced models, revealing that faithfulness remains challenging, even for the most competitive LLMs, despite their strong performance on standard benchmarks. Figure  1  summarizes the performance of representative models from each organization, with each bar representing performance on the individual tasks. To our knowledge, FaithEval is the  first fine-grained and comprehensive benchmark  specifically targeting contextual faithfulness hallucination, contributing to the broader effort toward developing reliable next-generation foundation models.",
            "An unanswerable context arises when the context includes relevant details but lacks the information needed to answer the question. In FaithEval, answerability is determined  solely  by the context, regardless of whether the question itself is unanswerable.  For instance, in the example in Figure  2  (Left), the context provides the proportion of both types of commuters in 2015. However, the question Which group of commuters in Dallas in 2009 is larger: carpooling or transit? is unanswerable, as the context lacks specific data from 2009.  To create such task, we modify the context from a collection of 10 contextual QA datasets, covering a wide range of domains (see Source datasets below). For each sample, we prompt an LLM to modify the original context so that it no longer contains the supporting evidence for the ground truth answer. Additional sentences may be woven into the new context to maintain coherence.  The full prompt is shown in Figure  17 . This process resulted in a total of 2.4k contextual QA pairs. To verify the quality of the modified contexts, we achieved over 98% agreement with professional human annotators (Sec  3.2 ).",
            "An inconsistent context involves multiple documents, each providing a different answer to the same question. This simulates noisy retrieval scenarios,  where documents from sources with varying levels of credibility are retrieved. For instance, as shown in Figure  2  (Middle), the context presents conflicting information about the tigers name in the novel  Life of Pi . A faithful model should be able to identify such inconsistencies, especially when instructed to do so. To create this task, we modify contexts from the same collection of contextual QA datasets used in the Unanswerable Context task. For each sample, the LLM is provided with a context passage, a question, and an original answer, which is supported by the context. The goal is to modify the context so that it introduces fabricated supporting evidence for a new, conflicting answer.  The detailed prompt is shown in Figure  18 . Since this task is more challenging, we curated a collection of 1.5k high-quality contextual QA pairs after filtering through professional human annotators.",
            "A counterfactual context contains statements that contradict with common sense or widely accepted facts, such as water freeze at 100 degrees Celsius, wood is magnetic, or carbon dioxide is the most abundant greenhouse gas in the atmosphere.  Unlike the other two tasks, the questions in this task are required to be relevant to such well-known facts. We curate this task based on ARC-Challenge  (Clark et al.,  2018 ) , a QA dataset covering grade-school level, multiple-choice science questions. Since the original dataset does not include context, we prompt an LLM to generate a long, multi-paragraph context that seamlessly provides fabricated supporting evidence for a counterfactual answer.  The detailed prompt is shown in Figure  19 . This process resulted in a total of 1k contextual QA pairs, each with three to five options. Due to the multiple-choice nature, we can use keyword matching to verify the quality of the synthetic contexts (Appendix  B ).",
            "Sycophancy with task-specific instructions.  While the additional instructions used in the Unanswerable and Inconsistent Context tasks (Sec  3.3 ) improve the models awareness of such scenarios, they can also introduce unintended effects when the context is normal ( i.e. , answerable and consistent). This can lead to what is known as sycophantic behavior  (Perez et al.,  2023 ; Wei et al.,  2023 ) , where models adjust their responses to align with the users expectations, even when those expectations are objectively incorrect. We examine this phenomenon in two top-performing models for the Inconsistent Context task, GPT-4o and Claude 3.5 Sonnet. Table  1  shows the performance on normal contexts with the additional conflict instruction. We observe a consistent performance drop for both models, with Claude 3.5 experiencing a 5% decrease in average accuracy. This further highlights the challenge of maintaining faithfulness across both normal and noisy contexts.",
            "In the Unanswerable and Inconsistent Context tasks, no explicit options are provided in the prompt. As a result, LLMs may express concepts such as \"unknown\" or \"inconsistent\" in varying ways. To assess the impact of allowing alternative valid expressions, we compare the performance of strict and non-strict matching. Strict matching only accepts the exact phrases \"unknown\" or \"conflict\" as specified in the prompt, while non-strict matching permits a broader range of expressions that convey similar ideas (see Appendix  A  for the full list of valid expressions). We observe that performance remains stable across most models. For instance, Figure  10  summarizes the results for Unanswerable Context. For competitive models such as gpt-4o and Claude 3.5, the gaps are less than 1%. Full results can be found in Table  3  and Table  4  in Appendix  C .",
            "Impact of decoding strategies.  By default, we adopt greedy decoding. We also investigate a popular sampling-based decoding scheme with a temperature of 0.3 and top-p of 0.9. The results are shown in Figure  11  based on Counterfactual Context. Similar observations also hold for Unanswerable and Inconsistent Context. We observe that sampling-based decoding marginally improves the performance over greedy decoding across all models. However, the significant gap between the original and counterfactual contexts cannot be mitigated with temperature scaling.",
            "For the Counterfactual Context task, since the answer options are provided within the context, we can validate the new context using a simple keyword-based matching method, where a context passes if the answer phrase exists in the context. This results in a pass rate of 68.9%, where the new context clearly contains the new answer. The results on this filtered subset are shown in Figure  12 . We can see that the same trend still holds as shown in Section  4 : a significant gap remains between the performance on the original task (with no context) and the new task with counterfactual contexts, across the majority of instruction-tuned models.",
            "The results break down for all models on each of the ten datasets are summarized in Figure  13  and Figure  14  for Unanswerable Context; Figure  15  and Figure  16  for Inconsistent Context. The detailed results for strict vs. non-strict matching can be found in Table  3  and Table  4 .",
            "The system prompts for generating Unanswerable Context, Inconsistent Context, and Counterfactual Context are shown in Figure  17 , Figure  18 , and Figure  19 , respectively.  For Inconsistent Context, our initial experiments suggest that a successful strategy is to decompose the generation of contextual QA into two steps. Step 1: generate a new answer that is fabricated and challenges common sense or well-known facts. Step 2: generate a modified context with fabricated evidence that supports the new answer. The model will output a JSON object containing the provided question, the provided old answer, the new answer, the modified context, and a concise justification on (1) if the new answer is supported by the new context (2) if all mentions of the old answer have been replaced or removed. We find that having justifications significantly improves the context quality. The generated context is concatenated with the original context to create an inconsistent context."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Model sizes across different model families.",
        "table": "A1.T2.1.1",
        "footnotes": [],
        "references": [
            "In this work, we introduce FaithEval, a comprehensive benchmark specifically designed to evaluate the contextual faithfulness of LLMs across  three  diverse tasks: unanswerable, inconsistent, and counterfactual contexts. These tasks simulate real-world challenges where retrieval mechanisms may surface incomplete, contradictory, or fabricated information (Figure  2 ). FaithEval includes a total of 4.9K high-quality samples, constructed using a rigorous four-stage framework with multi-turn LLM-based context verification and human validation. We conduct a holistic evaluation on 18 representative proprietary and open-sourced models, revealing that faithfulness remains challenging, even for the most competitive LLMs, despite their strong performance on standard benchmarks. Figure  1  summarizes the performance of representative models from each organization, with each bar representing performance on the individual tasks. To our knowledge, FaithEval is the  first fine-grained and comprehensive benchmark  specifically targeting contextual faithfulness hallucination, contributing to the broader effort toward developing reliable next-generation foundation models.",
            "To systematically evaluate the contextual faithfulness of LLMs, FaithEval contains three diverse tasks including unanswerable context, inconsistent context, and counterfactual context. Each sample  ( c , q , a ) c q a (\\mathbf{c},q,a) ( bold_c , italic_q , italic_a )  consists of a question  q q q italic_q , and a long context passage made up of one or more documents  c = ( d 1 , ... , d n ) c subscript d 1 ... subscript d n \\mathbf{c}=(d_{1},...,d_{n}) bold_c = ( italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_d start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) , and a groundtruth answer  a a a italic_a . The model is expected to answer the question leveraging the information in the provided context. An overview of each task is presented in Figure  2 . Next, we illustrate the construction of each task in detail.",
            "An unanswerable context arises when the context includes relevant details but lacks the information needed to answer the question. In FaithEval, answerability is determined  solely  by the context, regardless of whether the question itself is unanswerable.  For instance, in the example in Figure  2  (Left), the context provides the proportion of both types of commuters in 2015. However, the question Which group of commuters in Dallas in 2009 is larger: carpooling or transit? is unanswerable, as the context lacks specific data from 2009.  To create such task, we modify the context from a collection of 10 contextual QA datasets, covering a wide range of domains (see Source datasets below). For each sample, we prompt an LLM to modify the original context so that it no longer contains the supporting evidence for the ground truth answer. Additional sentences may be woven into the new context to maintain coherence.  The full prompt is shown in Figure  17 . This process resulted in a total of 2.4k contextual QA pairs. To verify the quality of the modified contexts, we achieved over 98% agreement with professional human annotators (Sec  3.2 ).",
            "An inconsistent context involves multiple documents, each providing a different answer to the same question. This simulates noisy retrieval scenarios,  where documents from sources with varying levels of credibility are retrieved. For instance, as shown in Figure  2  (Middle), the context presents conflicting information about the tigers name in the novel  Life of Pi . A faithful model should be able to identify such inconsistencies, especially when instructed to do so. To create this task, we modify contexts from the same collection of contextual QA datasets used in the Unanswerable Context task. For each sample, the LLM is provided with a context passage, a question, and an original answer, which is supported by the context. The goal is to modify the context so that it introduces fabricated supporting evidence for a new, conflicting answer.  The detailed prompt is shown in Figure  18 . Since this task is more challenging, we curated a collection of 1.5k high-quality contextual QA pairs after filtering through professional human annotators.",
            "Auto validation and human annotation.  We validate the quality of the new context by using a separate LLM judge to verify whether the new answer is valid given the context (\"if\" condition) and whether the context does not support alternative answers (\"only-if\" condition). For example, the new context in Figure  2  (Right) should not mention  wood is buoyant . Samples that fail to meet both conditions are filtered out. Next, we perform meticulous human annotation. Depending on the tasks validation difficulty, we employ different strategies. As the Inconsistent Context task is challenging to validate, we rely on full human annotation. Three Mechanical Turk  (Crowston,  2012 )  workers judge whether each contextual QA pair meets the \"if\" and \"only-if\" conditions, with final inclusion determined by majority agreement. This yields 1.5K samples. For the Unanswerable Context task, which is easier to validate, we use a similar majority-vote approach, achieving over 98% agreement among human annotators. This yields 2.4K samples. For the Counterfactual Context task, since the answer options are provided within the context, we validate using a string-based matching method, where the context passes if all words from the answer appear in the context. This results in 1K samples. After filtering, the FaithEval benchmark contains a total of 4.9K high-quality contextual QA pairs. More details are included in Appendix  A .",
            "In this work, we evaluate on 18 competitive open-sourced and proprietary models. A summary of model sizes from different model families are shown in Table  2 .",
            "For the Counterfactual Context task, since the answer options are provided within the context, we can validate the new context using a simple keyword-based matching method, where a context passes if the answer phrase exists in the context. This results in a pass rate of 68.9%, where the new context clearly contains the new answer. The results on this filtered subset are shown in Figure  12 . We can see that the same trend still holds as shown in Section  4 : a significant gap remains between the performance on the original task (with no context) and the new task with counterfactual contexts, across the majority of instruction-tuned models."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Performance comparison on Unanswerable Context with strict (S) and non-strict (N) matching.",
        "table": "A3.T3.1.1",
        "footnotes": [],
        "references": [
            "An unanswerable context arises when the context includes relevant details but lacks the information needed to answer the question. In FaithEval, answerability is determined  solely  by the context, regardless of whether the question itself is unanswerable.  For instance, in the example in Figure  2  (Left), the context provides the proportion of both types of commuters in 2015. However, the question Which group of commuters in Dallas in 2009 is larger: carpooling or transit? is unanswerable, as the context lacks specific data from 2009.  To create such task, we modify the context from a collection of 10 contextual QA datasets, covering a wide range of domains (see Source datasets below). For each sample, we prompt an LLM to modify the original context so that it no longer contains the supporting evidence for the ground truth answer. Additional sentences may be woven into the new context to maintain coherence.  The full prompt is shown in Figure  17 . This process resulted in a total of 2.4k contextual QA pairs. To verify the quality of the modified contexts, we achieved over 98% agreement with professional human annotators (Sec  3.2 ).",
            "Task construction.  An overview of our task construction and validation framework is shown in Figure  3 . Given a source QA sample and an original context (optional), we prompt an LLM to generate both a new context and a new answer for the Counterfactual and Inconsistent tasks, or only a new context (that supports no answer) for the Unanswerable task. To make the tasks challenging, the new context should be coherent and contain minimal modifications if the original context is provided. In addition, multiple paragraphs not directly related to the answer are included, serving as distractors. The new context is generated with detailed justifications explaining how it satisfies the task criterion. We construct the prompt for each sample by combining the original question, the new context, and task-specific instructions (Sec  3.3 ). In particular, an inconsistent context is created by concatenating the new context with the original context, each supporting a different answer.",
            "Sycophancy with task-specific instructions.  While the additional instructions used in the Unanswerable and Inconsistent Context tasks (Sec  3.3 ) improve the models awareness of such scenarios, they can also introduce unintended effects when the context is normal ( i.e. , answerable and consistent). This can lead to what is known as sycophantic behavior  (Perez et al.,  2023 ; Wei et al.,  2023 ) , where models adjust their responses to align with the users expectations, even when those expectations are objectively incorrect. We examine this phenomenon in two top-performing models for the Inconsistent Context task, GPT-4o and Claude 3.5 Sonnet. Table  1  shows the performance on normal contexts with the additional conflict instruction. We observe a consistent performance drop for both models, with Claude 3.5 experiencing a 5% decrease in average accuracy. This further highlights the challenge of maintaining faithfulness across both normal and noisy contexts.",
            "Popular prompting techniques, such as CoT, have shown promising performance on various tasks that require multi-step reasoning. We adopt the prompt format in Section  3.3  and summarize the results in Figure  9 . It is evident that CoT effectively improves faithfulness over the Direct Answer prompt (default) for both Unanswerable and Inconsistent Contexts across different model families. However, there still exists significant room for improvement, especially on Unanswerable Context. For instance, the leading model achieves only 71.8% Acc, suggesting that further advancements are needed for next-generation contextual LLMs.",
            "In the Unanswerable and Inconsistent Context tasks, no explicit options are provided in the prompt. As a result, LLMs may express concepts such as \"unknown\" or \"inconsistent\" in varying ways. To assess the impact of allowing alternative valid expressions, we compare the performance of strict and non-strict matching. Strict matching only accepts the exact phrases \"unknown\" or \"conflict\" as specified in the prompt, while non-strict matching permits a broader range of expressions that convey similar ideas (see Appendix  A  for the full list of valid expressions). We observe that performance remains stable across most models. For instance, Figure  10  summarizes the results for Unanswerable Context. For competitive models such as gpt-4o and Claude 3.5, the gaps are less than 1%. Full results can be found in Table  3  and Table  4  in Appendix  C .",
            "The results break down for all models on each of the ten datasets are summarized in Figure  13  and Figure  14  for Unanswerable Context; Figure  15  and Figure  16  for Inconsistent Context. The detailed results for strict vs. non-strict matching can be found in Table  3  and Table  4 ."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Performance comparison on Inconsistent Context with strict (S) and non-strict (N) matching.",
        "table": "A3.T4.1.1",
        "footnotes": [],
        "references": [
            "Abstaining is challenging, even when explicitly instructed.   The results of the Unanswerable Context task are summarized in Figure  4 , ranked by performance on the original context. Proprietary model names are highlighted in orange.  We highlight the following key observations: (1) Modern LLMs experience significant performance degradation in this task. Across all chat models, the performance gap ranges from 13.6% to 68.4%. (2) High performance on the original context does not correlate with high performance on the unanswerable context. For example, while Phi-3-medium-128k-instruct achieves 76.8% accuracy on the original context, closely approaching the SoTA (80.1%), it struggles to abstain from answering in the unanswerable context, with an accuracy of only 7.4%. (3) Larger model sizes are more advantageous within the same model family. For instance, compared to the 7B model, Llama-3.1-70B-instruct improves performance on the Unanswerable Context task by 10.3%. Similar trends hold for the Gemma-2 and Llama-3 model families.",
            "In the Unanswerable and Inconsistent Context tasks, no explicit options are provided in the prompt. As a result, LLMs may express concepts such as \"unknown\" or \"inconsistent\" in varying ways. To assess the impact of allowing alternative valid expressions, we compare the performance of strict and non-strict matching. Strict matching only accepts the exact phrases \"unknown\" or \"conflict\" as specified in the prompt, while non-strict matching permits a broader range of expressions that convey similar ideas (see Appendix  A  for the full list of valid expressions). We observe that performance remains stable across most models. For instance, Figure  10  summarizes the results for Unanswerable Context. For competitive models such as gpt-4o and Claude 3.5, the gaps are less than 1%. Full results can be found in Table  3  and Table  4  in Appendix  C .",
            "For the Counterfactual Context task, since the answer options are provided within the context, we can validate the new context using a simple keyword-based matching method, where a context passes if the answer phrase exists in the context. This results in a pass rate of 68.9%, where the new context clearly contains the new answer. The results on this filtered subset are shown in Figure  12 . We can see that the same trend still holds as shown in Section  4 : a significant gap remains between the performance on the original task (with no context) and the new task with counterfactual contexts, across the majority of instruction-tuned models.",
            "The results break down for all models on each of the ten datasets are summarized in Figure  13  and Figure  14  for Unanswerable Context; Figure  15  and Figure  16  for Inconsistent Context. The detailed results for strict vs. non-strict matching can be found in Table  3  and Table  4 ."
        ]
    },
    "global_footnotes": []
}