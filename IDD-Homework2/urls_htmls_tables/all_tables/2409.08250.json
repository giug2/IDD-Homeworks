{
    "id_table_1": {
        "caption": "Table 1.  Categorization and examples of atomic and composite context",
        "table": "S8.T2.1",
        "footnotes": [],
        "references": [
            "People often record their everyday life by taking photos, screenshots, and videos, whether for  saving important information, documenting special occasions, or simply capturing a funny moment  (Li et al . ,  2024b ) .  These captured instances, referred to as  captured memories , collectively represent subsets of an individuals  episodic memories   (Tulving,  2002 ) , a type of long-term memory that contains both specific past experiences and associated contextual details.  These episodic memories are essential for answering memory-related personal questions like,  What social events did I attend during CHI 2024?  (Figure  1 a),  which can help users reflect on past experiences and make informed decisions in daily tasks.",
            "However, these raw captured memories by themselves are insufficient to answer personal questions, as they lack contextual details that are typically implicit and scattered across multiple pieces of data.  For examples, as shown in Figure  1 b,  memories of attending parties during CHI 2024 are not explicitly annotated as occurring during the event.  Answering such personal questions requires extracting and integrating contextual information not typically contained within a single captured instance.  For example, by integrating multiple memories that mention CHI 2024 in their content and extracting their metadata, it is possible to determine when the users attended the conference and connect related social events memories from that period to CHI 2024 (Figure  1 c), enabling the answer of the query (Figure  1 d).",
            "Atomic context refers to contextual information typically obtainable from a single captured memory.  This includes data directly from metadata, sensed from visual and auditory content, or inferred from the content itself.  Table  1  shows the seven types of atomic contexts categorized from the queries.  Among them, temporal information and geographical info can be directly obtained from the memory medias metadata.  People and visual elements typically require machine learning models or facial recognition for detection. Environment, activity, and emotion are more implicit and require reasoning based on the content (e.g., a photo of a menu may suggest the person is in a restaurant).  The number of appearance of each category is shown in Figure  2 .",
            "Figure  B 1  demonstrates the structure of the baseline system in our experiment.  The baseline also processes the captured memories by leveraging a multimodal model ( GPT-4o ) to generate detailed captions for each memory.  Additionally, it extracts temporal and geographical information from the metadata and processes it in the same manner as OmniQuery.  This ensures that the processed memories include the temporal and geographical data, which are common components in users queries.  The temporal and geographical information is concatenated to the generated caption.  Then the concatenated text sequence is encoded into text embeddings using embedding models ( text-embedding-3-small )."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.  Quantitative Results of OmniQuery and Baseline, including UPA, UPC, and Accuracy (%)",
        "table": "S8.T3.1",
        "footnotes": [],
        "references": [
            "Atomic context refers to contextual information typically obtainable from a single captured memory.  This includes data directly from metadata, sensed from visual and auditory content, or inferred from the content itself.  Table  1  shows the seven types of atomic contexts categorized from the queries.  Among them, temporal information and geographical info can be directly obtained from the memory medias metadata.  People and visual elements typically require machine learning models or facial recognition for detection. Environment, activity, and emotion are more implicit and require reasoning based on the content (e.g., a photo of a menu may suggest the person is in a restaurant).  The number of appearance of each category is shown in Figure  2 .",
            "While atomic context is typically available within a single captured memory, composite context requires integrating multiple memories to understand the connection between them.  Since an individuals captured memories are linear on the timeline, memories related to a specific event tend to cluster closely together.  We leveraged this  temporal proximity  to identify and extract various composite contexts from the raw captured memories. For a detailed discussion of this approach, please refer to Section  5.2 .",
            "Specifically, as opposed to including detailed predefined categories (as with atomic contexts) in the prompt for LLMs, we adopt a few-shot prompting technique  (Brown et al . ,  2020 ) , providing examples of composite contexts summarized from the collected questions in the prompt.  For detailed prompt, please refer to Appendix  A.2 .",
            "Different from composite contexts, semantic knowledge focuses on high-level general knowledge rather than specific memory details.  For example, if a chat message screenshot mentions celebrating Jasons birthday, the inferred semantic knowledge might be Jasons birthday is on [SPECIFIC DATE]. Similarly, analyzing multiple grocery shopping receipts that consistently include lactose-free milk could lead to the inference that the user is possibly lactose intolerant.  Semantic knowledge is inferred in each sliding window, while also taking into account the identified composite contexts to gain higher-level understanding of the users past and generalized information (Figure  5 b bottom).  The output is a list of inferred declarative semantic knowledge independent from specific memories.  The instructions provided to the model are specifically tailored to guide the inference process toward overarching patterns and trends rather than specific event details.  The detailed prompt for identifying semantic knowledge can be also found in Appendix  A.2 .  Each inferred entry of semantic knowledge is either merged with existing entries or added to the knowledge list if new.",
            "Participants tested 137 queries in total during the main session.  Among them, 28 were previously logged during the diary study.  We manually labeled each tested query using the categorization and definition mentioned in section  3.3 .  As a result, 24 were categorized as  direct content query  while 17 were  contextual filters  and 96 were  hybrid queries .  We analyzed the performance metrics of both systems (OmniQuery and baseline) using the scores rated by the participants.  Table  2  and  3  summarize our results.  In addition to presenting the average UPA and UPC scores, we calculated binary accuracy to evaluate whether the systems provided mostly correct answers. An answer was considered accurate if its UPA score was equal to or greater than 4 (mostly correct) (Table  2 ).  We also present the comparison result in Table  3 , which compares the two systems head-to-head on answering personal questions.",
            "Enhancing visual intelligence:   As discussed in Section  8.6.2 , questions related to social interactions remain challenging due to the current lack of advanced features like facial recognition for person identification. Future iterations of OmniQuery could integrate such capabilities (with appropriate user consent), enabling the system to track individuals across various memories. This enhancement would support new use cases, such as monitoring social patterns or tracking progress over time, significantly improving the systems capacity for memory augmentation and retrieval. Additionally, we propose exploring the design and implementation of a comprehensive taxonomy of personal knowledge domains. This would allow users to selectively activate or deactivate specific domains, such as enabling Social Interactions and Relationships to infer personal connections while disabling Personally Identifiable Information to prevent the system from processing sensitive data like IDs or SSN numbers in photos. This modular approach could enhance both user control and privacy."
        ]
    },
    "global_footnotes": [
        "https://www.wikipedia.org/",
        "https://x.com/",
        "https://cloud.google.com/vision/docs/ocr",
        "https://github.com/openai/whisper",
        "https://www.pinecone.io/",
        "https://openai.com/policies/privacy-policy/"
    ]
}