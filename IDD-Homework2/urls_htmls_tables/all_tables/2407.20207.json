{
    "id_table_1": {
        "caption": "TABLE I:  The NDCG (  100 absent 100 \\times 100  100 ) comparisons of baselines and our QAEA-DR on four datasets",
        "table": "S3.T1.3.1",
        "footnotes": [],
        "references": [
            "Fig.  1  shows the complete workflow of QAEA-DR, illustrating an example of the framework in action. Specifically, QAEA-DR operates as follows:",
            "Step-1: Structured Information Extraction.  Each text  t i subscript t i t_{i} italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  from the corpus  C C \\mathcal{C} caligraphic_C , where  i = 1 , ... , n i 1 ... n i=1,\\ldots,n italic_i = 1 , ... , italic_n , is augmented using LLM prompting to generate JSON format QA pairs  QA json subscript QA json \\text{QA}_{\\text{json}} QA start_POSTSUBSCRIPT json end_POSTSUBSCRIPT  and events  EVENT json subscript EVENT json \\text{EVENT}_{\\text{json}} EVENT start_POSTSUBSCRIPT json end_POSTSUBSCRIPT . We discuss the design of LLM prompts for structured text augmentation in Section  III-C . As illustrated in Fig.  1 , both types of structured texts effectively extract key information. Specifically, each QA pair presents an individual information point, while each event summarizes multiple points.",
            "In conclusion, our main contribution is the implementation of QAEA-DR, which, in Step-1, Step-2, and Step-3, generates two new types of text vectorsQA pair vectors and event vectorsand integrates them into the vector database. These generated vectors enhance the retrieval performance in the final Step-4 of dense retrieval.  Fig.  1  demonstrates that in Step-4, the best match with the query is derived from the generated vectors with high similarity.",
            "In this section, we describe our implementation of LLM-based text augmentation and the unifying properties of QAEA.   QAEA  is defined as a text augmentation module excluding the retrieval component. It combines original texts, QA pairs, and events into a new vector database to enhance natural texts through information extraction. QAEA corresponds to Steps 1 to 3 in Fig.  1 .",
            "Question-Answer Generation.  In QAG, our goal is to generate as many informative structured QA pairs as possible through instruction.  Due to the lack of a universally recognized question generation directive, we employ question categorization to guide the LLM in producing diverse QA pairs.  In designing question types, we observe that rhetorical patterns in writing (e.g., cause and effect) serve as methods for organizing information and can be generalized for question categorization.  Consequently, the content directive specifies five question types for varied outputs:  factual inquiry ,  explanation and definition ,  cause and effect ,  comparison and contrast , and  evaluation and opinion .  Additionally, the prompt instructs LLM to highlight frequently occurring entities and relationships in the original text, which should be reflected in the generated QA pairs. Regarding the output format, the instructions guide the LLM to produce QA pairs in JSON format  QA json subscript QA json \\text{QA}_{\\text{json}} QA start_POSTSUBSCRIPT json end_POSTSUBSCRIPT : { question type : [[ question ,  answer ]]}, where question type includes five categories, each capable of containing multiple QA pairs depending on LLM generation.  As illustrated in Step-2 of Fig.  1 , the output of QAG is processed by simply concatenating the question and answer strings into natural language texts.",
            "Event Extraction.   Since our text augmentation method for dense retrieval is initially designed for application on a small-scale Chinese news passage retrieval dataset (sCNPR) we created from a scientific project, we naturally consider event extraction.  Unlike previous zero-shot prompt-based ChatEE  [ 38 ] , which requires predefined event types and supports single-event extraction, our approach allows the LLM to detect and generate multiple event types from the original text.  In the EE prompt instructions, we first direct the LLM to identify multiple event types and use these generated types as triggers to populate event elements.  Drawing from the event element categorization in the ACE 2005 dataset and common real-world event attributes, we define that each event includes the elements event type, time, location, event subject, event object, event, and impact.  In terms of the output format, we guide the LLM to generate event outputs in JSON format  EVENT json subscript EVENT json \\text{EVENT}_{\\text{json}} EVENT start_POSTSUBSCRIPT json end_POSTSUBSCRIPT : [{ event type, time, location, event subject, event object, event, impact }], where outputs default to null if corresponding event elements are absent in the original text.  Fig.  1  shows that the output of EE is transformed back into unstructured text by concatenating all event elements within an event, separated by periods.",
            "Text Evaluation and Regeneration.   In the open domain, evaluating the quality of QAG and EE is difficult due to the lack of labels.  To address this, we introduce a robust prompt-based mechanism for evaluating the quality of generated texts and regenerating them if necessary, as outlined in Fig.  2  and Algorithm  1 .  Using separate roles for generation and evaluation by different LLMs,  LLM generator subscript LLM generator \\mathbf{LLM}_{\\text{generator}} bold_LLM start_POSTSUBSCRIPT generator end_POSTSUBSCRIPT  and  LLM evaluator subscript LLM evaluator \\mathbf{LLM}_{\\text{evaluator}} bold_LLM start_POSTSUBSCRIPT evaluator end_POSTSUBSCRIPT , the double-check system scores and potentially rewrites outputs based on predefined criteria.  As shown in the scoring-based quality evaluation prompt template in Fig.  2 , each generated text starts with a perfect score of 10, and points are deducted for failures in  Relevance ,  Clarity ,  Consistency , and  Completeness .  For example, the Relevance rule checks if each generated text faithfully reflects the original content, deducting one point for irrelevance.  The scoring details are outlined in the JSON format  Score json subscript Score json \\text{Score}_{\\text{json}} Score start_POSTSUBSCRIPT json end_POSTSUBSCRIPT :  { total score , detail : [ { deduction reason , deduction score , related content } ] } conditional-set total score detail delimited-[] deduction reason deduction score related content \\{\\textit{``total score''},\\textit{``detail''}:[\\{\\textit{``deduction reason''%  },\\textit{``deduction score''},\\textit{``related content''}\\}]\\} { total score , detail : [ { deduction reason , deduction score , related content } ] } .  Outputs that score at or below a set threshold    \\tau italic_  enter a regeneration prompting, where texts are adjusted or rewritten to ensure that outputs meet rigorous standards.  This approach not only provides a reliable method for assessing generated texts but also controls the quality of the LLM outputs through a structured regeneration process based on scoring outcomes.",
            "Unified Framework.  Algorithm  1  details the QAEA module and shows that QAEA manages QA pairs and events in a similar manner. We conclude that QAEA is identified as a unified framework from two perspectives:",
            "By combining inequalities  9  and  10 , we conclude that:",
            "Similarly, combining inequalities  8  and  11 , we conclude that:",
            "We study the impact of the score threshold in text regeneration process.  Higher threshold    \\tau italic_  means texts are more likely to require regeneration to meet these standards as shown in Algorithm  1 ."
        ]
    },
    "id_table_2": {
        "caption": "TABLE II:  Comparisons of LLM Single-Generation and with Regeneration Performance in QAEA (TRI): NDCG@1 (  100 absent 100 \\times 100  100 )",
        "table": "S4.T2.3",
        "footnotes": [],
        "references": [
            "Following standard LLM-based prompt engineering practices  [ 49 ] , our defined single-step zero-shot prompt consists of three components: instruction, input data, and output indicator. For both QAG and EE prompting tasks, we make targeted adjustments to the prompt instructions.  Fig.  2  illustrates the three-step prompts defined for both QAG and EE, including generation, scoring-based quality evaluation, and regeneration. We achieve different functionalities by modifying the instructions for each type of prompt.",
            "Text Evaluation and Regeneration.   In the open domain, evaluating the quality of QAG and EE is difficult due to the lack of labels.  To address this, we introduce a robust prompt-based mechanism for evaluating the quality of generated texts and regenerating them if necessary, as outlined in Fig.  2  and Algorithm  1 .  Using separate roles for generation and evaluation by different LLMs,  LLM generator subscript LLM generator \\mathbf{LLM}_{\\text{generator}} bold_LLM start_POSTSUBSCRIPT generator end_POSTSUBSCRIPT  and  LLM evaluator subscript LLM evaluator \\mathbf{LLM}_{\\text{evaluator}} bold_LLM start_POSTSUBSCRIPT evaluator end_POSTSUBSCRIPT , the double-check system scores and potentially rewrites outputs based on predefined criteria.  As shown in the scoring-based quality evaluation prompt template in Fig.  2 , each generated text starts with a perfect score of 10, and points are deducted for failures in  Relevance ,  Clarity ,  Consistency , and  Completeness .  For example, the Relevance rule checks if each generated text faithfully reflects the original content, deducting one point for irrelevance.  The scoring details are outlined in the JSON format  Score json subscript Score json \\text{Score}_{\\text{json}} Score start_POSTSUBSCRIPT json end_POSTSUBSCRIPT :  { total score , detail : [ { deduction reason , deduction score , related content } ] } conditional-set total score detail delimited-[] deduction reason deduction score related content \\{\\textit{``total score''},\\textit{``detail''}:[\\{\\textit{``deduction reason''%  },\\textit{``deduction score''},\\textit{``related content''}\\}]\\} { total score , detail : [ { deduction reason , deduction score , related content } ] } .  Outputs that score at or below a set threshold    \\tau italic_  enter a regeneration prompting, where texts are adjusted or rewritten to ensure that outputs meet rigorous standards.  This approach not only provides a reliable method for assessing generated texts but also controls the quality of the LLM outputs through a structured regeneration process based on scoring outcomes.",
            "Therefore, the formulas ( 2 ) and ( 3 ) hold, proving that the text augmentation approach maintains or improves retrieval fidelity."
        ]
    },
    "id_table_3": {
        "caption": "TABLE III:  Comparisons of LLM Single-Generation and with Regeneration Performance in QAEA (TMO): NDCG@1 (  100 absent 100 \\times 100  100 )",
        "table": "S4.T3.3",
        "footnotes": [],
        "references": [
            "We first invoke the concept of  fidelity  of the retrieval process and  normalized margin  from previous work  [ 3 ] .  Subsequently, without loss of generality, we demonstrate that these generated vectors either maintain or enhance the fidelity of the retrieval process.  Theorem  III.3  introduces the effectiveness of text augmentation for dense retrieval.  Theorem  III.4  demonstrates the effectiveness of both QA Pair texts and event texts.",
            "Theorem  III.3  uses normalized margin to demonstrate the effectiveness of text augmentation.  The theorem holds under certain constraints, including  relevance enhancement ,  irrelevance consistency , and  orthogonality . Under ideal text augmentation, the generated vectors of the target text should exhibit improved query relevance, while those of non-target texts should not be more relevant to the query than the original vectors. Additionally, in sparse retrieval, orthogonal vectors can be achieved by dividing the vocabulary into non-overlapping segments. Similarly, in dense models, we assume that vector representations of different texts are orthogonal when the content is irrelevant.",
            "Therefore, the formulas ( 2 ) and ( 3 ) hold, proving that the text augmentation approach maintains or improves retrieval fidelity.",
            "The constraints in Theorem  III.3  are based on the assumption that each generated text is of high quality and contains a portion of the original texts information. Given the text  t 1 subscript t 1 t_{1} italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and the related query, the ideal generated vector  v 1 ( 0 ) superscript subscript v 1 0 v_{1}^{(0)} italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT  enhances retrieval fidelity by reducing noise and condensing query-relevant information from the text.",
            "We start by noting that the set  { v 1 ( j ) } = { v q  a 1 ( j ) }  { v e  v  e  n  t 1 ( j ) } superscript subscript v 1 j superscript subscript v q subscript a 1 j superscript subscript v e v e n subscript t 1 j \\{v_{1}^{(j)}\\}=\\{v_{qa_{1}}^{(j)}\\}\\cup\\{v_{event_{1}}^{(j)}\\} { italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT } = { italic_v start_POSTSUBSCRIPT italic_q italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT }  { italic_v start_POSTSUBSCRIPT italic_e italic_v italic_e italic_n italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT } . From the relevance enhancement condition, there exists  v 1 ( 0 )  { v q  a 1 ( j ) }  { v e  v  e  n  t 1 ( j ) } superscript subscript v 1 0 superscript subscript v q subscript a 1 j superscript subscript v e v e n subscript t 1 j v_{1}^{(0)}\\in\\{v_{qa_{1}}^{(j)}\\}\\cup\\{v_{event_{1}}^{(j)}\\} italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT  { italic_v start_POSTSUBSCRIPT italic_q italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT }  { italic_v start_POSTSUBSCRIPT italic_e italic_v italic_e italic_n italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT }  that maximizes the inner product with  v q subscript v q v_{q} italic_v start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT . The remaining proof follows similarly to the proof of Theorem  III.3  and can be easily derived.",
            "From Fig.  3 , QAEA with either TMO or TRI overall improves recall@1 performance compared to the baselines, as indicated by the longer blue bars than orange.  TMO consistently exhibits fewer instances of reduced recall@1 performance compared to TRI (shorter orange bars) across different datasets and embedding models, indicating more stable performance.  In datasets like T2Retrieval with some typos, redundancies, or irrelevant texts, TRI may extract noisy texts, leading to an increase in poor performance cases; TMO merges generated texts, minimizing the impact of noisy text and resulting in fewer recall@1 worse cases."
        ]
    },
    "id_table_4": {
        "caption": "TABLE IV:  The NDCG (  100 absent 100 \\times 100  100 ) comparisons of Ablation Study on QAEA (TMO)",
        "table": "S4.T4.3.1",
        "footnotes": [],
        "references": [
            "We first invoke the concept of  fidelity  of the retrieval process and  normalized margin  from previous work  [ 3 ] .  Subsequently, without loss of generality, we demonstrate that these generated vectors either maintain or enhance the fidelity of the retrieval process.  Theorem  III.3  introduces the effectiveness of text augmentation for dense retrieval.  Theorem  III.4  demonstrates the effectiveness of both QA Pair texts and event texts.",
            "Theorem  III.4  proposes that integrating more generated text vector representations is more likely to increase retrieval fidelity compared to using only one type of generated text vector representation.",
            "Additionally, Fig.  4  clearly shows the ablation performance of both QAEA (TMO) and QAEA (TRI). We observe that QA+Event+Original (TRI) or QA+Event+Original (TMO) achieve optimal performance, with many subsets also performing well."
        ]
    },
    "id_table_5": {
        "caption": "TABLE V:  Average Number of QA Pairs and Events Generated From Original Text Through LLMs",
        "table": "S5.T5.1",
        "footnotes": [],
        "references": [
            "We investigate the impact of dataset size across three open datasets as shown in Fig.  5 .  Across dataset sizes of 1000, 2000, and 5000, QAEA-DR consistently demonstrates superior performance over the baseline. Besides, as dataset size varies, QAEA-DR and the baseline show similar trends across the three datasets, which indicates the consistency of text augmentation.  The experimental results also corroborate our earlier discussions on the two text organization strategies. QAEA (TMO) shows good stability throughout the experiment, consistently outperforming the baseline. In contrast, although QAEA (TRI) shows significant performance improvements in most cases, its performance fluctuates greatly due to its high sensitivity to the quality of text generation, leading to unstable results across different dataset sizes."
        ]
    },
    "id_table_6": {
        "caption": "TABLE VI:  Comparison of LMQG and Our LLM-based GPT-QAG on Retrieval Performance: NDCG@1 (  100 absent 100 \\times 100  100 )",
        "table": "S5.T6.3",
        "footnotes": [],
        "references": [
            "Fig.  6  shows the retrieval performance at three different regeneration score thresholds: 5, 7, and 9, out of a total score of 10. Figure presents the best results from GLM and GPT.  For the sCNPR and T2Retrieval datasets, NDCG@1 increases with higher thresholds, indicating that stricter filtering improves retrieval by regenerating lower-quality outputs.  For HotpotQA and MS MARCO, performance remains relatively stable across threshold adjustments, which suggests that threshold changes have lesser impact due to the already high quality of the initial text generation.  Meanwhile, QAEA (TMO) demonstrates more stable performance than QAEA (TMO) across datasets, particularly at higher thresholds."
        ]
    },
    "id_table_7": {
        "caption": "TABLE VII:  Time Complexity Comparisons for QAEA (TMO) and QAEA (TRI) on sCNPR",
        "table": "S5.T7.17",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": [
        "https://chat.openai.com",
        "https://huggingface.co/moka-ai/m3e-base",
        "https://platform.deepseek.com"
    ]
}