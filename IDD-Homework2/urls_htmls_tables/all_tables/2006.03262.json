{
    "PAPER'S NUMBER OF TABLES": 1,
    "S5.T1": {
        "caption": "TABLE I: Main simulation parameters",
        "table": "<table id=\"S5.T1.8\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.8.9.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.8.9.1.1\" class=\"ltx_td ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<td id=\"S5.T1.8.9.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" colspan=\"2\">\n<span id=\"S5.T1.8.9.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T1.8.9.1.2.1.1\" class=\"ltx_p\" style=\"width:74.0pt;\">MNIST</span>\n</span>\n</td>\n<td id=\"S5.T1.8.9.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S5.T1.8.9.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T1.8.9.1.3.1.1\" class=\"ltx_p\" style=\"width:76.8pt;\">CIFAR-10</span>\n</span>\n</td>\n</tr>\n<tr id=\"S5.T1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S5.T1.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T1.1.1.1.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Users <math id=\"S5.T1.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"K\" display=\"inline\"><semantics id=\"S5.T1.1.1.1.1.1.m1.1a\"><mi mathcolor=\"#000000\" id=\"S5.T1.1.1.1.1.1.m1.1.1\" xref=\"S5.T1.1.1.1.1.1.m1.1.1.cmml\">K</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.1.1.1.1.1.m1.1b\"><ci id=\"S5.T1.1.1.1.1.1.m1.1.1.cmml\" xref=\"S5.T1.1.1.1.1.1.m1.1.1\">𝐾</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.1.1.1.1.1.m1.1c\">K</annotation></semantics></math></span>\n</span>\n</th>\n<td id=\"S5.T1.1.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S5.T1.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T1.1.1.2.1.1\" class=\"ltx_p\" style=\"width:37.0pt;\">100</span>\n</span>\n</td>\n<td id=\"S5.T1.1.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S5.T1.1.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T1.1.1.3.1.1\" class=\"ltx_p\" style=\"width:37.0pt;\">15</span>\n</span>\n</td>\n<td id=\"S5.T1.1.1.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S5.T1.1.1.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T1.1.1.4.1.1\" class=\"ltx_p\" style=\"width:76.8pt;\">10</span>\n</span>\n</td>\n</tr>\n<tr id=\"S5.T1.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T1.2.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r\">\n<span id=\"S5.T1.2.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T1.2.2.1.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Samples <math id=\"S5.T1.2.2.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"n_{k}\" display=\"inline\"><semantics id=\"S5.T1.2.2.1.1.1.m1.1a\"><msub id=\"S5.T1.2.2.1.1.1.m1.1.1\" xref=\"S5.T1.2.2.1.1.1.m1.1.1.cmml\"><mi mathcolor=\"#000000\" id=\"S5.T1.2.2.1.1.1.m1.1.1.2\" xref=\"S5.T1.2.2.1.1.1.m1.1.1.2.cmml\">n</mi><mi mathcolor=\"#000000\" id=\"S5.T1.2.2.1.1.1.m1.1.1.3\" xref=\"S5.T1.2.2.1.1.1.m1.1.1.3.cmml\">k</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.2.2.1.1.1.m1.1b\"><apply id=\"S5.T1.2.2.1.1.1.m1.1.1.cmml\" xref=\"S5.T1.2.2.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T1.2.2.1.1.1.m1.1.1.1.cmml\" xref=\"S5.T1.2.2.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S5.T1.2.2.1.1.1.m1.1.1.2.cmml\" xref=\"S5.T1.2.2.1.1.1.m1.1.1.2\">𝑛</ci><ci id=\"S5.T1.2.2.1.1.1.m1.1.1.3.cmml\" xref=\"S5.T1.2.2.1.1.1.m1.1.1.3\">𝑘</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.2.2.1.1.1.m1.1c\">n_{k}</annotation></semantics></math></span>\n</span>\n</th>\n<td id=\"S5.T1.2.2.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span id=\"S5.T1.2.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T1.2.2.2.1.1\" class=\"ltx_p\" style=\"width:37.0pt;\">500</span>\n</span>\n</td>\n<td id=\"S5.T1.2.2.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span id=\"S5.T1.2.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T1.2.2.3.1.1\" class=\"ltx_p\" style=\"width:37.0pt;\">1000</span>\n</span>\n</td>\n<td id=\"S5.T1.2.2.4\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span id=\"S5.T1.2.2.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T1.2.2.4.1.1\" class=\"ltx_p\" style=\"width:76.8pt;\">5000</span>\n</span>\n</td>\n</tr>\n<tr id=\"S5.T1.8.10.2\" class=\"ltx_tr\">\n<th id=\"S5.T1.8.10.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r\">\n<span id=\"S5.T1.8.10.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T1.8.10.2.1.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Model</span>\n</span>\n</th>\n<td id=\"S5.T1.8.10.2.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" colspan=\"2\">\n<span id=\"S5.T1.8.10.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T1.8.10.2.2.1.1\" class=\"ltx_p\" style=\"width:82.5pt;\">Two-layer fully connected</span>\n</span>\n</td>\n<td id=\"S5.T1.8.10.2.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span id=\"S5.T1.8.10.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T1.8.10.2.3.1.1\" class=\"ltx_p\" style=\"width:76.8pt;\">Five-layer convolutional <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib56\" title=\"\" class=\"ltx_ref\">56</a>]</cite></span>\n</span>\n</td>\n</tr>\n<tr id=\"S5.T1.8.11.3\" class=\"ltx_tr\">\n<th id=\"S5.T1.8.11.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r\">\n<span id=\"S5.T1.8.11.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T1.8.11.3.1.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Optimizer</span>\n</span>\n</th>\n<td id=\"S5.T1.8.11.3.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" colspan=\"2\">\n<span id=\"S5.T1.8.11.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T1.8.11.3.2.1.1\" class=\"ltx_p\" style=\"width:82.5pt;\">Gradient descent</span>\n</span>\n</td>\n<td id=\"S5.T1.8.11.3.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span id=\"S5.T1.8.11.3.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T1.8.11.3.3.1.1\" class=\"ltx_p\" style=\"width:76.8pt;\">Mini-batch <abbr title=\"stochastic gradient descent\" class=\"ltx_glossaryref\"><span class=\"ltx_text ltx_glossary_short\">SGD</span></abbr></span>\n</span>\n</td>\n</tr>\n<tr id=\"S5.T1.5.5\" class=\"ltx_tr\">\n<th id=\"S5.T1.3.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r\">\n<span id=\"S5.T1.3.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T1.3.3.1.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Local steps <math id=\"S5.T1.3.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\tau\" display=\"inline\"><semantics id=\"S5.T1.3.3.1.1.1.m1.1a\"><mi mathcolor=\"#000000\" id=\"S5.T1.3.3.1.1.1.m1.1.1\" xref=\"S5.T1.3.3.1.1.1.m1.1.1.cmml\">τ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.3.3.1.1.1.m1.1b\"><ci id=\"S5.T1.3.3.1.1.1.m1.1.1.cmml\" xref=\"S5.T1.3.3.1.1.1.m1.1.1\">𝜏</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.3.3.1.1.1.m1.1c\">\\tau</annotation></semantics></math></span>\n</span>\n</th>\n<td id=\"S5.T1.4.4.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\" colspan=\"2\">\n<span id=\"S5.T1.4.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T1.4.4.2.1.1\" class=\"ltx_p\" style=\"width:82.5pt;\"><math id=\"S5.T1.4.4.2.1.1.m1.1\" class=\"ltx_Math\" alttext=\"1\" display=\"inline\"><semantics id=\"S5.T1.4.4.2.1.1.m1.1a\"><mn mathcolor=\"#000000\" id=\"S5.T1.4.4.2.1.1.m1.1.1\" xref=\"S5.T1.4.4.2.1.1.m1.1.1.cmml\">1</mn><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.4.4.2.1.1.m1.1b\"><cn type=\"integer\" id=\"S5.T1.4.4.2.1.1.m1.1.1.cmml\" xref=\"S5.T1.4.4.2.1.1.m1.1.1\">1</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.4.4.2.1.1.m1.1c\">1</annotation></semantics></math></span>\n</span>\n</td>\n<td id=\"S5.T1.5.5.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span id=\"S5.T1.5.5.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T1.5.5.3.1.1\" class=\"ltx_p\" style=\"width:76.8pt;\"><math id=\"S5.T1.5.5.3.1.1.m1.1\" class=\"ltx_Math\" alttext=\"17\" display=\"inline\"><semantics id=\"S5.T1.5.5.3.1.1.m1.1a\"><mn mathcolor=\"#000000\" id=\"S5.T1.5.5.3.1.1.m1.1.1\" xref=\"S5.T1.5.5.3.1.1.m1.1.1.cmml\">17</mn><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.5.5.3.1.1.m1.1b\"><cn type=\"integer\" id=\"S5.T1.5.5.3.1.1.m1.1.1.cmml\" xref=\"S5.T1.5.5.3.1.1.m1.1.1\">17</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.5.5.3.1.1.m1.1c\">17</annotation></semantics></math></span>\n</span>\n</td>\n</tr>\n<tr id=\"S5.T1.8.8\" class=\"ltx_tr\">\n<th id=\"S5.T1.6.6.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\">\n<span id=\"S5.T1.6.6.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T1.6.6.1.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Step-size <math id=\"S5.T1.6.6.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\eta_{1}\" display=\"inline\"><semantics id=\"S5.T1.6.6.1.1.1.m1.1a\"><msub id=\"S5.T1.6.6.1.1.1.m1.1.1\" xref=\"S5.T1.6.6.1.1.1.m1.1.1.cmml\"><mi mathcolor=\"#000000\" id=\"S5.T1.6.6.1.1.1.m1.1.1.2\" xref=\"S5.T1.6.6.1.1.1.m1.1.1.2.cmml\">η</mi><mn mathcolor=\"#000000\" id=\"S5.T1.6.6.1.1.1.m1.1.1.3\" xref=\"S5.T1.6.6.1.1.1.m1.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.6.6.1.1.1.m1.1b\"><apply id=\"S5.T1.6.6.1.1.1.m1.1.1.cmml\" xref=\"S5.T1.6.6.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T1.6.6.1.1.1.m1.1.1.1.cmml\" xref=\"S5.T1.6.6.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S5.T1.6.6.1.1.1.m1.1.1.2.cmml\" xref=\"S5.T1.6.6.1.1.1.m1.1.1.2\">𝜂</ci><cn type=\"integer\" id=\"S5.T1.6.6.1.1.1.m1.1.1.3.cmml\" xref=\"S5.T1.6.6.1.1.1.m1.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.6.6.1.1.1.m1.1c\">\\eta_{1}</annotation></semantics></math></span>\n</span>\n</th>\n<td id=\"S5.T1.7.7.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r\" colspan=\"2\">\n<span id=\"S5.T1.7.7.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T1.7.7.2.1.1\" class=\"ltx_p\" style=\"width:82.5pt;\"><math id=\"S5.T1.7.7.2.1.1.m1.1\" class=\"ltx_Math\" alttext=\"10^{-2}\" display=\"inline\"><semantics id=\"S5.T1.7.7.2.1.1.m1.1a\"><msup id=\"S5.T1.7.7.2.1.1.m1.1.1\" xref=\"S5.T1.7.7.2.1.1.m1.1.1.cmml\"><mn mathcolor=\"#000000\" id=\"S5.T1.7.7.2.1.1.m1.1.1.2\" xref=\"S5.T1.7.7.2.1.1.m1.1.1.2.cmml\">10</mn><mrow id=\"S5.T1.7.7.2.1.1.m1.1.1.3\" xref=\"S5.T1.7.7.2.1.1.m1.1.1.3.cmml\"><mo mathcolor=\"#000000\" id=\"S5.T1.7.7.2.1.1.m1.1.1.3a\" xref=\"S5.T1.7.7.2.1.1.m1.1.1.3.cmml\">−</mo><mn mathcolor=\"#000000\" id=\"S5.T1.7.7.2.1.1.m1.1.1.3.2\" xref=\"S5.T1.7.7.2.1.1.m1.1.1.3.2.cmml\">2</mn></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.7.7.2.1.1.m1.1b\"><apply id=\"S5.T1.7.7.2.1.1.m1.1.1.cmml\" xref=\"S5.T1.7.7.2.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T1.7.7.2.1.1.m1.1.1.1.cmml\" xref=\"S5.T1.7.7.2.1.1.m1.1.1\">superscript</csymbol><cn type=\"integer\" id=\"S5.T1.7.7.2.1.1.m1.1.1.2.cmml\" xref=\"S5.T1.7.7.2.1.1.m1.1.1.2\">10</cn><apply id=\"S5.T1.7.7.2.1.1.m1.1.1.3.cmml\" xref=\"S5.T1.7.7.2.1.1.m1.1.1.3\"><minus id=\"S5.T1.7.7.2.1.1.m1.1.1.3.1.cmml\" xref=\"S5.T1.7.7.2.1.1.m1.1.1.3\"></minus><cn type=\"integer\" id=\"S5.T1.7.7.2.1.1.m1.1.1.3.2.cmml\" xref=\"S5.T1.7.7.2.1.1.m1.1.1.3.2\">2</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.7.7.2.1.1.m1.1c\">10^{-2}</annotation></semantics></math></span>\n</span>\n</td>\n<td id=\"S5.T1.8.8.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r\">\n<span id=\"S5.T1.8.8.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T1.8.8.3.1.1\" class=\"ltx_p\" style=\"width:76.8pt;\"><math id=\"S5.T1.8.8.3.1.1.m1.1\" class=\"ltx_Math\" alttext=\"5\\cdot 10^{-3}\" display=\"inline\"><semantics id=\"S5.T1.8.8.3.1.1.m1.1a\"><mrow id=\"S5.T1.8.8.3.1.1.m1.1.1\" xref=\"S5.T1.8.8.3.1.1.m1.1.1.cmml\"><mn mathcolor=\"#000000\" id=\"S5.T1.8.8.3.1.1.m1.1.1.2\" xref=\"S5.T1.8.8.3.1.1.m1.1.1.2.cmml\">5</mn><mo lspace=\"0.222em\" mathcolor=\"#000000\" rspace=\"0.222em\" id=\"S5.T1.8.8.3.1.1.m1.1.1.1\" xref=\"S5.T1.8.8.3.1.1.m1.1.1.1.cmml\">⋅</mo><msup id=\"S5.T1.8.8.3.1.1.m1.1.1.3\" xref=\"S5.T1.8.8.3.1.1.m1.1.1.3.cmml\"><mn mathcolor=\"#000000\" id=\"S5.T1.8.8.3.1.1.m1.1.1.3.2\" xref=\"S5.T1.8.8.3.1.1.m1.1.1.3.2.cmml\">10</mn><mrow id=\"S5.T1.8.8.3.1.1.m1.1.1.3.3\" xref=\"S5.T1.8.8.3.1.1.m1.1.1.3.3.cmml\"><mo mathcolor=\"#000000\" id=\"S5.T1.8.8.3.1.1.m1.1.1.3.3a\" xref=\"S5.T1.8.8.3.1.1.m1.1.1.3.3.cmml\">−</mo><mn mathcolor=\"#000000\" id=\"S5.T1.8.8.3.1.1.m1.1.1.3.3.2\" xref=\"S5.T1.8.8.3.1.1.m1.1.1.3.3.2.cmml\">3</mn></mrow></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.8.8.3.1.1.m1.1b\"><apply id=\"S5.T1.8.8.3.1.1.m1.1.1.cmml\" xref=\"S5.T1.8.8.3.1.1.m1.1.1\"><ci id=\"S5.T1.8.8.3.1.1.m1.1.1.1.cmml\" xref=\"S5.T1.8.8.3.1.1.m1.1.1.1\">⋅</ci><cn type=\"integer\" id=\"S5.T1.8.8.3.1.1.m1.1.1.2.cmml\" xref=\"S5.T1.8.8.3.1.1.m1.1.1.2\">5</cn><apply id=\"S5.T1.8.8.3.1.1.m1.1.1.3.cmml\" xref=\"S5.T1.8.8.3.1.1.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S5.T1.8.8.3.1.1.m1.1.1.3.1.cmml\" xref=\"S5.T1.8.8.3.1.1.m1.1.1.3\">superscript</csymbol><cn type=\"integer\" id=\"S5.T1.8.8.3.1.1.m1.1.1.3.2.cmml\" xref=\"S5.T1.8.8.3.1.1.m1.1.1.3.2\">10</cn><apply id=\"S5.T1.8.8.3.1.1.m1.1.1.3.3.cmml\" xref=\"S5.T1.8.8.3.1.1.m1.1.1.3.3\"><minus id=\"S5.T1.8.8.3.1.1.m1.1.1.3.3.1.cmml\" xref=\"S5.T1.8.8.3.1.1.m1.1.1.3.3\"></minus><cn type=\"integer\" id=\"S5.T1.8.8.3.1.1.m1.1.1.3.3.2.cmml\" xref=\"S5.T1.8.8.3.1.1.m1.1.1.3.3.2\">3</cn></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.8.8.3.1.1.m1.1c\">5\\cdot 10^{-3}</annotation></semantics></math></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Next, we demonstrate that the reduced distortion of ",
                "UVeQFed",
                " also translates into ",
                "FL",
                " performance gains. To that aim, we evaluate its application for training neural networks using the MNIST and CIFAR-10 data sets, and compare its performance to that achievable using previous quantization methods for ",
                "FL",
                ".\n",
                "The simulation settings are detailed below, with the main parameters summarized in Table ",
                "I",
                ".",
                "We first compare the accuracy of models trained using ",
                "UVeQFed",
                " to those obtained using federated averaging combined with the quantization methods considered in Subsection ",
                "V-A",
                ", i.e., QSGD ",
                "[",
                "17",
                "]",
                " and the schemes proposed in ",
                "[",
                "12",
                "]",
                " of uniform quantizers with random rotation as well as random subsampling followed by three-bit uniform quantizers.\nTo that aim, we train a fully-connected network with a single hidden layer of ",
                "50",
                "50",
                "50",
                " neurons and an intermediate sigmoid activation for detecting handwritten digits based on the MNIST data set. Training is carried out using ",
                "K",
                "=",
                "100",
                "𝐾",
                "100",
                "K=100",
                " users, each has access to ",
                "500",
                "500",
                "500",
                " training samples distributed in an i.i.d. fashion, such that each user has an identical number of images from each label. The users update their weights using gradient descent, where federated averaging is carried out on each iteration. The resulting accuracy versus the number of iterations of these quantized ",
                "FL",
                " schemes compared to federated averaging without quantization is depicted in Figs. ",
                "6",
                "-",
                "7",
                " for quantization rates ",
                "R",
                "=",
                "2",
                "𝑅",
                "2",
                "R=2",
                " and ",
                "R",
                "=",
                "4",
                "𝑅",
                "4",
                "R=4",
                ", respectively.",
                "Observing Figs. ",
                "6",
                "-",
                "7",
                ", we note that ",
                "UVeQFed",
                " with vector quantization, i.e., ",
                "L",
                "=",
                "2",
                "𝐿",
                "2",
                "L=2",
                ", achieves the most rapid and accurate convergence among all considered schemes. In particular, for ",
                "R",
                "=",
                "4",
                "𝑅",
                "4",
                "R=4",
                ", ",
                "UVeQFed",
                " with ",
                "L",
                "=",
                "2",
                "𝐿",
                "2",
                "L=2",
                " achieves a convergence profile within a minor gap from federated averaging without quantization constraints. Among the previous schemes, QSGD demonstrates steady accuracy improvements, though it is still outperformed by ",
                "UVeQFed",
                " with ",
                "L",
                "=",
                "1",
                "𝐿",
                "1",
                "L=1",
                ", indicating that the reduced distortion achieved by using subtractive dithering is translated into improved trained models. The quantization methods proposed in ",
                "[",
                "12",
                "]",
                " result in notable variations in the trained model accuracy and in slower convergence due to their increased error induced in quantization, as noted in Subsection ",
                "V-A",
                ".",
                "We next evaluate ",
                "UVeQFed",
                " for both heterogeneous as well as i.i.d. distributions of the training data. Based on the results observed in Figs. ",
                "6",
                "-",
                "7",
                " and to avoid cluttering, we compare ",
                "UVeQFed",
                " only to QSGD and to the accuracy achieved using federated averaging without quantization. Here, we train neural classifiers for both the MNIST and the CIFAR-10 data sets, where for each data set we use both heterogeneous and i.i.d. division of the data.",
                "For MNIST, we again use a fully-connected network with a single hidden layer of ",
                "50",
                "50",
                "50",
                " neurons and an intermediate sigmoid activation with gradient descent optimization. Each of the ",
                "K",
                "=",
                "15",
                "𝐾",
                "15",
                "K=15",
                " users has ",
                "1000",
                "1000",
                "1000",
                " training samples. We consider the case where the samples are distributed sequentially among the users, i.e., the first user has the first ",
                "1000",
                "1000",
                "1000",
                " samples in the data set, and so on, resulting in an uneven heterogeneous division of the labels of the users. ",
                "We also train using an i.i.d. data division, where the labels are uniformly distributed among the users",
                ".\nThe resulting accuracy versus the number of iterations is depicted in Figs. ",
                "8",
                "-",
                "9",
                " for quantization rates ",
                "R",
                "=",
                "2",
                "𝑅",
                "2",
                "R=2",
                " and ",
                "R",
                "=",
                "4",
                "𝑅",
                "4",
                "R=4",
                ", respectively.",
                "For CIFAR-10, we train the deep convolutional neural network architecture used in ",
                "[",
                "56",
                "]",
                ", whose trainable parameters constitute three convolution layers and two fully-connected layers. Here, we consider two methods for distributing the ",
                "50000",
                "50000",
                "50000",
                " training images of CIFAR-10 among the ",
                "K",
                "=",
                "10",
                "𝐾",
                "10",
                "K=10",
                " users: An i.i.d. division, where each user has the same number samples from each of the ",
                "10",
                "10",
                "10",
                " labels, and a heterogeneous division, in which at least ",
                "25",
                "%",
                "percent",
                "25",
                "25\\%",
                " of the samples of each user correspond to a single distinct label. Each user completes a single epoch of ",
                "SGD",
                " with mini-batch size ",
                "60",
                "60",
                "60",
                " before the models are aggregated. The resulting accuracy versus the number of epochs is depicted in Figs. ",
                "10",
                "-",
                "11",
                " for quantization rates ",
                "R",
                "=",
                "2",
                "𝑅",
                "2",
                "R=2",
                " and ",
                "R",
                "=",
                "4",
                "𝑅",
                "4",
                "R=4",
                ", respectively.",
                "We observe in Figs. ",
                "8",
                "-",
                "11",
                " that ",
                "UVeQFed",
                " with vector quantizer, i.e., ",
                "L",
                "=",
                "2",
                "𝐿",
                "2",
                "L=2",
                ", results in convergence to the most accurate model for all the considered scenarios. ",
                "In fact, when training a deep convolutional network, for which the loss surface is extremely complex and non-convex, we observe in Figs. ",
                "10",
                "-",
                "11",
                " that ",
                "UVeQFed",
                " with ",
                "L",
                "=",
                "2",
                "𝐿",
                "2",
                "L=2",
                " trained using i.i.d. data achieves improved accuracy over federated averaging without quantization. This follows from the fact that the stochastic nature of the quantization error in ",
                "UVeQFed",
                " results in its implementing a noisy variant of local ",
                "SGD",
                ", which is known to be capable of boosting convergence and avoid local minimas when training ",
                "deep neural networks",
                " with non-convex loss surfaces ",
                "[",
                "57",
                "]",
                ", as also observed in ",
                "[",
                "42",
                "]",
                ".",
                "The observed gains are more dominant for ",
                "R",
                "=",
                "2",
                "𝑅",
                "2",
                "R=2",
                ", implying that the usage of ",
                "UVeQFed",
                " with multi-dimensional lattices can notably improve the performance over low rate channels.\nParticularly, we observe in Figs. ",
                "8",
                "-",
                "11",
                " that similar gains of ",
                "UVeQFed",
                " are noted for both i.i.d. as well as heterogeneous setups, while the heterogeneous division of the data degrades the accuracy of all considered schemes compared to the i.i.d division.\nIt is also observed that ",
                "UVeQFed",
                " with scalar quantizers, i.e., ",
                "L",
                "=",
                "1",
                "𝐿",
                "1",
                "L=1",
                ", achieves improved convergence compared to QSGD for most considered setups, which stems from its reduced distortion.",
                "The results presented in this section demonstrate that the theoretical benefits of ",
                "UVeQFed",
                ", which rigorously hold under ",
                "AS1",
                "-",
                "AS3",
                ", translate into improved convergence when operating under rate constraints with non-synthetic data."
            ]
        ]
    }
}