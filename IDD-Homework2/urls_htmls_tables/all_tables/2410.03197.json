{
    "id_table_1": {
        "caption": "Table 1:   Data utilized by QuIST and baseline models.",
        "table": "S3.T1.18.18",
        "footnotes": [],
        "references": [
            "In our preliminary investigation, we found that this technique did not completely prevent code-switching in XLT-QG, as shown in Figure  1 . Specifically, the models struggled to fully grasp interrogative structures in the target language, a phenomenon we refer to as interrogative code-switching. In this study, we propose a method that enables small mPLMs to learn interrogative structures without relying on target language data during training.",
            "The set of question exemplars corresponding to the question type identified by the QTC model is used in the QG stage. These exemplars are pre-created for each question type and language, as detailed in Section  3.1 . By leveraging the shared interrogative structures among the exemplars, the QG model generates questions using the provided answer and context. Both the QTC and QG models are trained exclusively on English QA data and can be deployed to new languages without the need for additional training with target language data.",
            "In this section, we describe the datasets and baseline models we used in our experiments. Details regarding the implementation and evaluation metrics are provided in Appendices  B  and  C.1 , respectively.",
            "Question Exemplars    The English question exemplars ( Q en ) were randomly selected from the questions in the training set of  C-Q-A en  after labeling question types as described in Section  2.1 3 3 3 In preliminary experiments, we observed that using fixed exemplars was more effective than configuring random exemplars for each training example. A detailed analysis of this finding is provided in Appendix  A . . To gather question exemplars in the target languages ( Q tgt ) written by native speakers, we utilized the questions from the training set of  C-Q-A tgt . After translating these questions into English using Google Translation API, we constructed the question exemplars in the same manner as for English.",
            "Table  1  summarizes the datasets utilized by each model during both the training and inference stages. As indicated in the table, QuIST, Baseline EncDec , and Baseline Enc  are exclusively trained on English datasets. In contrast, Baseline Multi  and Baseline Adapter  make use of language-specific data during training. Consequently, distinct language-specific models were trained for these two baselines.",
            "To measure the zero-shot inference performance of the QTC model for the target languages, we first annotated the ground-truth question types of the target language QA data. We translated the questions into English and conducted annotation as detailed in Section  2.1  (i.e., hard labeling). Table  6  displays the macro F1 scores of the QTC model, measured based on ground-truth labels constructed in two ways. Since most Wh-questions can be paraphrased into questions beginning with what and which, 7 7 7 For example, How large is the Mupartifad village? is equivalent to What is the area of Mupartifad village?  we also evaluate the QTC performance in a setting where what and which are accepted as additional gold labels (i.e., relaxed labeling). According to the results measured with the relaxed labels, the model correctly classified more than 90% of questions. This suggests that the error propagation resulting from misclassification in QTC is minimal throughout the entire pipeline.",
            "We evaluated the zero-shot and few-shot performance of  gpt-3.5-turbo-0125  model. We extracted sets with different numbers of examples: 1, 3, 5, and 10, from  C-Q-A en  to employ for few-shot inference. In addition, we used five versions of each set, varying the random seed. Based on the English validation set, we determined the optimal number of examples (see Table  10 ), and used the set with the median performance as the component in the few-shot prompt. Subsequently, we conducted zero-shot and 10-shot inference for various languages using the prompts described in Figure  5  and  6 , respectively.",
            "Table  11 ,  12 , and  13  show detailed results for the experiments in Section  4 .",
            "We additionally investigated whether the QTC model and question exemplars are beneficial for few-shot inference of GPT-3.5-turbo. In this experiment, we utilized the exemplar set that exhibited the best performance for each language in our method. We supplemented these exemplars with the statement The followings are examples of  language  questions: placed before the prompt in Figure  6 . According to the results in Table  14 , leveraging the QTC model and question exemplars leads to particularly improved performance in low-resource languages such as Bengali, Telugu, and Swahili."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:   Automatic evaluation results for the nine target languages. This table shows the ROUGE-L performance of the models (SP-ROUGE  Vu et al. ( 2022 )  scores for Chinese). The best scores among mT5-based models are in  bold  and the highest scores among all models are marked with    \\star  . We also report BLEU4 and METEOR scores and standard deviations in Appendix  F .",
        "table": "S3.T2.20.20",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "As illustrated in Figure  2 , we divide the task into two stages. In the QTC stage, a classification model identifies the type of question to be generated. We focus on Wh-questions, categorizing them into eight types based on English interrogative words. While the type of question is primarily influenced by the type of answer, the model considers both the answer and the context. This is crucial, as the same answer can result in different types of questions depending on the context. For example, the number 911 could refer to a quantity, year, or proper noun.",
            "Question Exemplars    The English question exemplars ( Q en ) were randomly selected from the questions in the training set of  C-Q-A en  after labeling question types as described in Section  2.1 3 3 3 In preliminary experiments, we observed that using fixed exemplars was more effective than configuring random exemplars for each training example. A detailed analysis of this finding is provided in Appendix  A . . To gather question exemplars in the target languages ( Q tgt ) written by native speakers, we utilized the questions from the training set of  C-Q-A tgt . After translating these questions into English using Google Translation API, we constructed the question exemplars in the same manner as for English.",
            "Comparison with Baselines    Table  2  presents the performance of QuIST and the baseline models across nine target languages. The results show that QuIST 15 , which achieved the highest performance among our models with varying numbers of question exemplars, outperforms several XLT-QG baselines, demonstrating a margin of 6.00 points compared to the most robust baseline, Baseline Adapter . While adapting Baseline Adapter  to a new language necessitates training language-specific adapter modules, our model can be readily deployed in new languages without the need for additional training.",
            "Human Evaluation    We conducted a human evaluation in six languages where QuIST and GPT-3.5-turbo 10  exhibited similar automatic evaluation results, and we also evaluated the strongest baseline model, Baseline Adapter . We collected a total of 240 questions generated by the three models per language and asked three native speakers to assess the question quality based on five criteria:  Interrogative Sentence  ( I ),  Grammatical Correctness  ( G ),  Clarity  ( C ),  Answerability  ( A ),  Answer-Match  ( A.M. ). The first two metrics were rated on a scale of 0, 1, 2, while responses for the remaining categories were binary (yes or no). More information regarding these criteria is described in Appendix  C.2 .",
            "To measure the zero-shot inference performance of the QTC model for the target languages, we first annotated the ground-truth question types of the target language QA data. We translated the questions into English and conducted annotation as detailed in Section  2.1  (i.e., hard labeling). Table  6  displays the macro F1 scores of the QTC model, measured based on ground-truth labels constructed in two ways. Since most Wh-questions can be paraphrased into questions beginning with what and which, 7 7 7 For example, How large is the Mupartifad village? is equivalent to What is the area of Mupartifad village?  we also evaluate the QTC performance in a setting where what and which are accepted as additional gold labels (i.e., relaxed labeling). According to the results measured with the relaxed labels, the model correctly classified more than 90% of questions. This suggests that the error propagation resulting from misclassification in QTC is minimal throughout the entire pipeline.",
            "As shown in Table  7 , both approaches demonstrate effective performance in target languages compared to the existing XLT-QG baseline models (Table  2 ). However, the static exemplar method achieves better overall performance across various languages. During training, our model generates questions by leveraging the syntactic information from the exemplars while utilizing the semantic information from the input context and answer. We hypothesize that the model trained with static exemplars was better able to focus on the syntactic structures of the example questions, leading to improved performance. Consequently, we utilized static exemplars in all our experiments.",
            "To train multilingual QA models in Section  5.2 , we adopted the methodologies used by  Agrawal et al. ( 2023 ) . Each QA model underwent training using a combination of English data sourced from the TyDiQA training set and synthetic data for all languages, generated by each XLT-QG model. Given the unavailability of the TyDiQA test set, we evaluated the validation performance instead. The backbone of the QA model consisted of  google/mt5-xl  with 3.7B parameters, fine-tuned with a learning rate of 2e-4 and a batch size of 64. We selected the model checkpoint yielding the highest EM score for each language, following the strategy of  Agrawal et al. ( 2023 ) , and reported the average scores obtained from utilizing three different random seeds.",
            "Table  11 ,  12 , and  13  show detailed results for the experiments in Section  4 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:   Human evaluation results.",
        "table": "S4.T3.12",
        "footnotes": [],
        "references": [
            "The set of question exemplars corresponding to the question type identified by the QTC model is used in the QG stage. These exemplars are pre-created for each question type and language, as detailed in Section  3.1 . By leveraging the shared interrogative structures among the exemplars, the QG model generates questions using the provided answer and context. Both the QTC and QG models are trained exclusively on English QA data and can be deployed to new languages without the need for additional training with target language data.",
            "Table  3  presents the majority responses from three raters. For the criteria of clarity, answerability, and answer-match, we report the percentage of yes responses. In German, Finnish, and Indonesian, the questions generated by QuIST and GPT-3.5-turbo 10  consistently received high scores across all criteria. Specifically, both models effectively generate questions that align with the given answers, outperforming Baseline Adapter . In contrast, our model achieves lower overall scores in Bengali and Hindi compared to the previously mentioned languages. However, this performance decline is also observed in GPT-3.5-turbo 10  and Baseline Adapter .",
            "We investigated the frequency of interrogative code-switching occurrence in questions generated by different XLT-QG methods 5 5 5 We used cld3 ( https://github.com/google/cld3 ) to identify the languages. If the target language comprised less than 70% of the generated question, it was classified as code-switching. If the target language accounted for more than 70% but included English interrogative words, it was classified as interrogative code-switching. . As depicted in Figure  3 , interrogative code-switching is observed in the majority of questions generated by Baseline EncDec . This phenomenon can be attributed to catastrophic forgetting in target languages, as both the encoder and decoder were fine-tuned using English data. In Baseline Enc , where only the encoder was fine-tuned, this issue is slightly alleviated; nevertheless, more than half of the synthetic questions still exhibit this code-switching problem.",
            "Table  11 ,  12 , and  13  show detailed results for the experiments in Section  4 ."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:   Exact match scores of multilingual QA models trained on datasets synthesized using different methods.",
        "table": "S5.T4.3",
        "footnotes": [],
        "references": [
            "We experimented with several versions of question exemplars containing different number of questions: {1, 5, 10, 15}. In addition, we randomly sampled each version of the exemplars five times using different random seeds. Consequently, we trained five distinct QuIST models using different English question exemplars. During the inference stage, five sets of exemplars for each target language were utilized for evaluation. As a result, in Section  4 , we report the average of 25 automatic evaluation results.",
            "We explored the potential of QuIST for augmenting training data for multilingual QA models. Specifically, we compared synthetic data generated by QuIST and baseline models 6 6 6 The questions were generated based on the context and answer pairs within the synthetic dataset released by  Agrawal et al. ( 2023 ) .  with the multilingual QA dataset generated by  Agrawal et al. ( 2023 ) , which used their PaLM-540B model prompt-tuned with five QA examples from target languages. Table  4  presents the average exact match (EM) scores across six languages for the multilingual QA models. The training details can be seen in Appendix  B .",
            "We analyzed the questions generated by the models we used in the experiments, particularly focusing on Swahili, where our model received lower rating than GPT-3.5-turbo 10  in human evaluation. In Figure  4 , we can see that the question generated by QuIST is insufficient to explain the given answer, and these incorrect generations resulted in the low Answer-Match score. We also note that Baseline EncDec  and Baseline Enc  encounter code-switching issues, and the question generated by Baseline Multi  contains information that is not present in the context. Furthermore, the question generated by Baseline Adapter  was assessed as not being a question, as it is a descriptive sentence ending with a question mark.",
            "Table  11 ,  12 , and  13  show detailed results for the experiments in Section  4 .",
            "We additionally investigated whether the QTC model and question exemplars are beneficial for few-shot inference of GPT-3.5-turbo. In this experiment, we utilized the exemplar set that exhibited the best performance for each language in our method. We supplemented these exemplars with the statement The followings are examples of  language  questions: placed before the prompt in Figure  6 . According to the results in Table  14 , leveraging the QTC model and question exemplars leads to particularly improved performance in low-resource languages such as Bengali, Telugu, and Swahili."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:   Performance of XLT-QG models using question exemplars in different ways.",
        "table": "S5.T5.4",
        "footnotes": [],
        "references": [
            "We investigated the impact of utilizing different methods for constructing question exemplars compared to our proposed approach. These approaches were compared to Baseline Enc , where only the encoder is fine-tuned on English data, without using additional data from target languages during both training and inference. Table  5  presents the average ROUGE scores across nine languages.",
            "To train multilingual QA models in Section  5.2 , we adopted the methodologies used by  Agrawal et al. ( 2023 ) . Each QA model underwent training using a combination of English data sourced from the TyDiQA training set and synthetic data for all languages, generated by each XLT-QG model. Given the unavailability of the TyDiQA test set, we evaluated the validation performance instead. The backbone of the QA model consisted of  google/mt5-xl  with 3.7B parameters, fine-tuned with a learning rate of 2e-4 and a batch size of 64. We selected the model checkpoint yielding the highest EM score for each language, following the strategy of  Agrawal et al. ( 2023 ) , and reported the average scores obtained from utilizing three different random seeds.",
            "We evaluated the zero-shot and few-shot performance of  gpt-3.5-turbo-0125  model. We extracted sets with different numbers of examples: 1, 3, 5, and 10, from  C-Q-A en  to employ for few-shot inference. In addition, we used five versions of each set, varying the random seed. Based on the English validation set, we determined the optimal number of examples (see Table  10 ), and used the set with the median performance as the component in the few-shot prompt. Subsequently, we conducted zero-shot and 10-shot inference for various languages using the prompts described in Figure  5  and  6 , respectively."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:   Performance of the QTC model.",
        "table": "S5.T6.1",
        "footnotes": [],
        "references": [
            "To measure the zero-shot inference performance of the QTC model for the target languages, we first annotated the ground-truth question types of the target language QA data. We translated the questions into English and conducted annotation as detailed in Section  2.1  (i.e., hard labeling). Table  6  displays the macro F1 scores of the QTC model, measured based on ground-truth labels constructed in two ways. Since most Wh-questions can be paraphrased into questions beginning with what and which, 7 7 7 For example, How large is the Mupartifad village? is equivalent to What is the area of Mupartifad village?  we also evaluate the QTC performance in a setting where what and which are accepted as additional gold labels (i.e., relaxed labeling). According to the results measured with the relaxed labels, the model correctly classified more than 90% of questions. This suggests that the error propagation resulting from misclassification in QTC is minimal throughout the entire pipeline.",
            "We evaluated the zero-shot and few-shot performance of  gpt-3.5-turbo-0125  model. We extracted sets with different numbers of examples: 1, 3, 5, and 10, from  C-Q-A en  to employ for few-shot inference. In addition, we used five versions of each set, varying the random seed. Based on the English validation set, we determined the optimal number of examples (see Table  10 ), and used the set with the median performance as the component in the few-shot prompt. Subsequently, we conducted zero-shot and 10-shot inference for various languages using the prompts described in Figure  5  and  6 , respectively.",
            "We additionally investigated whether the QTC model and question exemplars are beneficial for few-shot inference of GPT-3.5-turbo. In this experiment, we utilized the exemplar set that exhibited the best performance for each language in our method. We supplemented these exemplars with the statement The followings are examples of  language  questions: placed before the prompt in Figure  6 . According to the results in Table  14 , leveraging the QTC model and question exemplars leads to particularly improved performance in low-resource languages such as Bengali, Telugu, and Swahili."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:   Comparison of models using dynamic and static exemplars during training. We report SP-ROUGE scores for Chinese and ROUGE-L scores for other languages. The scores for the static setting are based on the English exemplars, representing median performance.",
        "table": "A1.T7.18.18",
        "footnotes": [],
        "references": [
            "As shown in Table  7 , both approaches demonstrate effective performance in target languages compared to the existing XLT-QG baseline models (Table  2 ). However, the static exemplar method achieves better overall performance across various languages. During training, our model generates questions by leveraging the syntactic information from the exemplars while utilizing the semantic information from the input context and answer. We hypothesize that the model trained with static exemplars was better able to focus on the syntactic structures of the example questions, leading to improved performance. Consequently, we utilized static exemplars in all our experiments."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:   Number of examples by question type in training set of  C-Q-A en .",
        "table": "A4.T8.2",
        "footnotes": [],
        "references": [
            "We used SQuAD1.1  Rajpurkar et al. ( 2016 )  as the English QA data  C-Q-A en  for training our models. As only training and validation sets are publicly available, we partitioned the training set and employed a portion of the examples for validation purposes. The original validation set served as our test set. The training, validation, and test sets comprised 79,321, 8,283, and 1,190 examples, respectively. Furthermore, the distribution of examples by question type is summarized in Table  8 ."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:   Language codes and the number of examples in  C-Q-A tgt  dataset. In our method, only a small portion of the training examples are used as question exemplars.",
        "table": "A4.T9.3.1",
        "footnotes": [],
        "references": [
            "Table  9  presents the statistics of target language QA data  C-Q-A tgt  utilized by our models during inference. Note that training examples were solely employed for sampling question exemplars  Q tgt . Test examples in Chinese, German, and Hindi were collected from the XQuAD  Artetxe et al. ( 2020 )  test set, whereas training examples were sourced from the MLQA  Lewis et al. ( 2020 )  validation set, as XQuAD does not provide a training set for the target languages. Training and test examples in other languages were obtained from TyDiQA  Clark et al. ( 2020 ) ."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:   Performance of GPT-3.5-turbo on the SQuAD1.1 validation set. We report the mean and standard deviation of the few-shot inference results.",
        "table": "A5.T10.12.12",
        "footnotes": [],
        "references": [
            "We evaluated the zero-shot and few-shot performance of  gpt-3.5-turbo-0125  model. We extracted sets with different numbers of examples: 1, 3, 5, and 10, from  C-Q-A en  to employ for few-shot inference. In addition, we used five versions of each set, varying the random seed. Based on the English validation set, we determined the optimal number of examples (see Table  10 ), and used the set with the median performance as the component in the few-shot prompt. Subsequently, we conducted zero-shot and 10-shot inference for various languages using the prompts described in Figure  5  and  6 , respectively."
        ]
    },
    "id_table_11": {
        "caption": "Table 11:   Automatic evaluation results using BLEU4.",
        "table": "A7.T11.46.46",
        "footnotes": [],
        "references": [
            "Table  11 ,  12 , and  13  show detailed results for the experiments in Section  4 ."
        ]
    },
    "id_table_12": {
        "caption": "Table 12:  Automatic evaluation results using METEOR.",
        "table": "A7.T12.46.46",
        "footnotes": [],
        "references": [
            "Table  11 ,  12 , and  13  show detailed results for the experiments in Section  4 ."
        ]
    },
    "id_table_13": {
        "caption": "Table 13:   Automatic evaluation results using ROUGE-L and SP-ROUGE.",
        "table": "A7.T13.50.50",
        "footnotes": [],
        "references": [
            "Table  11 ,  12 , and  13  show detailed results for the experiments in Section  4 ."
        ]
    },
    "id_table_14": {
        "caption": "Table 14:   Performance of GPT-3.5-turbo 10  employing the QTC model and question exemplars in target languages.",
        "table": "A7.T14.1.1",
        "footnotes": [],
        "references": [
            "We additionally investigated whether the QTC model and question exemplars are beneficial for few-shot inference of GPT-3.5-turbo. In this experiment, we utilized the exemplar set that exhibited the best performance for each language in our method. We supplemented these exemplars with the statement The followings are examples of  language  questions: placed before the prompt in Figure  6 . According to the results in Table  14 , leveraging the QTC model and question exemplars leads to particularly improved performance in low-resource languages such as Bengali, Telugu, and Swahili."
        ]
    }
}