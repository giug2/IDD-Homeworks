{
    "id_table_1": {
        "caption": "Table 1 :  Comparison of Experiments 1 and 2 mean and standard deviation over five seeds",
        "table": "S5.T1.14",
        "footnotes": [],
        "references": [
            "The details of the improved methodology for the fully synthetic data generation used for machine learning algorithm training for fire detection is depicted in Figure  1 .",
            "Step  4.1.3  of Method 1, caused challenges with the accuracy of the automated annotation of the synthetic fires and flames in the synthetic scenes (Figure  LABEL:fig:short1 ). To resolve this issue in the Method 2, we imported the EmberGen-generated flame images into the Blender scene in the form of image to plane and set these planes constraints to always face the camera (Figure  2 ). Subsequently, we randomized the positions and sizes of these flame planes according to the cameras location.",
            "To enhance the precision of flame annotation within 3D scenes, an alternative approach was adopted that involved directly overlaying fire and flame images of varying sizes and positions onto a pre-rendered background image. The process of defining camera movement rules explained in section  4.1.2  was repeated, but this time the 3D scene did not contain preset fire and flames. After obtaining the same rendered images of the 3D scene, we extracted these images and used them as the background for the next step. In the subsequent step, fire and flame frames were randomly placed in the foreground. This method consolidates the steps of placing fire and flame planes within a 3D environment and projecting them onto the camera plane from the previous method. In this way, as the flames were directly placed on the 2D plane, we could achieve very accurate annotations and effectively eliminate errors caused by perspective distortion due to camera focus (Figure  3 ).",
            "To examine the viability of our fuzzy object synthetic data generation methods, we conducted a series of ML experiments. Our experiments were inspired by previous research  Hinterstoisser et al. ( 2019 ) ,  Borkman et al. ( 2021 ) . We describe our experimental setup in Section  5.1 , followed by a comparison of the performance results of our two synthetic image generation methods in Section  5.2 . Finally, we compare the performance of different training combinations of Experiment 2 on two different test benchmarks in Section  5.3 .",
            "Datasets.  We introduce two synthetic fire datasets: SynthData 1 and SynthData 2, each containing 1,000 images and 2D bounding box annotations generated using Method 1 ( 4.1 ) and Method 2 ( 4.2 ) respectively. We compare the performance of these two synthetic fire datasets (Experiment 1 vs. Experiment 2) in Section  5.2 . Experiment 1 is training of YOLOv5 mini using SynthData 1 while Experiment 2 is the training of YOLOv5 mini using SynthData 2.",
            "Table  1  shows the detection performance result obtained when using different iterations in our method. Experiment 1 uses synthetic data generated and annotated using Method 1 with low level of scene and fire randomization (SynthData 1). Experiment 2 uses synthetic data generated and annotated using Method 2 with high level of scene and fire randomization (SynthData 2).",
            "In addressing our initial research question, it becomes evident that ML models trained on both iterations of the fully synthetic fire data generation method produce fire detection. However, we observe that the SynthData 2 outperforms SynthData 1 based on the results reported in Table  1 . Therefore, Method 2 outperforms Method 1 by high margins in all metrics and benchmarks. This relates to the challenges identified in the Method 1 and addressed in Method 2 (see  4.2 )"
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Comparison of three strategies and their training set combinations in R(m)_S(n) format, where m and n indicate the number of images from Real training and Synthetic sets respectively. Comparison metrics are  A  P 50 A subscript P 50 AP_{50} italic_A italic_P start_POSTSUBSCRIPT 50 end_POSTSUBSCRIPT  and  A  P A P AP italic_A italic_P  (the higher, the better). Mean and standard deviation over 5 seeds is shown for all combinations.  Best  results are shown in  bold ,  second best  results are shown in  italic .",
        "table": "S5.T2.54",
        "footnotes": [],
        "references": [
            "Step  4.1.3  of Method 1, caused challenges with the accuracy of the automated annotation of the synthetic fires and flames in the synthetic scenes (Figure  LABEL:fig:short1 ). To resolve this issue in the Method 2, we imported the EmberGen-generated flame images into the Blender scene in the form of image to plane and set these planes constraints to always face the camera (Figure  2 ). Subsequently, we randomized the positions and sizes of these flame planes according to the cameras location.",
            "To enhance the precision of flame annotation within 3D scenes, an alternative approach was adopted that involved directly overlaying fire and flame images of varying sizes and positions onto a pre-rendered background image. The process of defining camera movement rules explained in section  4.1.2  was repeated, but this time the 3D scene did not contain preset fire and flames. After obtaining the same rendered images of the 3D scene, we extracted these images and used them as the background for the next step. In the subsequent step, fire and flame frames were randomly placed in the foreground. This method consolidates the steps of placing fire and flame planes within a 3D environment and projecting them onto the camera plane from the previous method. In this way, as the flames were directly placed on the 2D plane, we could achieve very accurate annotations and effectively eliminate errors caused by perspective distortion due to camera focus (Figure  3 ).",
            "To examine the viability of our fuzzy object synthetic data generation methods, we conducted a series of ML experiments. Our experiments were inspired by previous research  Hinterstoisser et al. ( 2019 ) ,  Borkman et al. ( 2021 ) . We describe our experimental setup in Section  5.1 , followed by a comparison of the performance results of our two synthetic image generation methods in Section  5.2 . Finally, we compare the performance of different training combinations of Experiment 2 on two different test benchmarks in Section  5.3 .",
            "Datasets.  We introduce two synthetic fire datasets: SynthData 1 and SynthData 2, each containing 1,000 images and 2D bounding box annotations generated using Method 1 ( 4.1 ) and Method 2 ( 4.2 ) respectively. We compare the performance of these two synthetic fire datasets (Experiment 1 vs. Experiment 2) in Section  5.2 . Experiment 1 is training of YOLOv5 mini using SynthData 1 while Experiment 2 is the training of YOLOv5 mini using SynthData 2.",
            "In addressing our initial research question, it becomes evident that ML models trained on both iterations of the fully synthetic fire data generation method produce fire detection. However, we observe that the SynthData 2 outperforms SynthData 1 based on the results reported in Table  1 . Therefore, Method 2 outperforms Method 1 by high margins in all metrics and benchmarks. This relates to the challenges identified in the Method 1 and addressed in Method 2 (see  4.2 )",
            "Findings.  Table  2  and Figure  5  present the detection performance of different combinations of training set on the two test sets. The findings of the these experiments are as follows:"
        ]
    }
}