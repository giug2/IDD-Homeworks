{
    "id_table_1": {
        "caption": "Table 1:  Test results for different models and RAG approaches across query results categories",
        "table": "S4.T1.1",
        "footnotes": [],
        "references": [
            "The proposed system is illustrated in Figure  1 . The system takes a list of SPARQL endpoint URLs as input, where each endpoint is expected to include minimal standardized metadata (example queries and VoID 2 2 2 Vocabulary of Interlinked Datasets  w3.org/TR/void  descriptions) that can be automatically retrieved and indexed upon initial deployment. We provide an online webpage 3 3 3 sib-swiss.github.io/sparql-editor/check  allowing to check if a given endpoint contains the required metadata.",
            "We assess the systems performance across three configurations: 1) baseline LLM without RAG, 2) RAG without validation, and 3) RAG with validation and correction of the generated query. This approach allows us to evaluate the contribution of each system component in improving query generation accuracy and helps detect any regressions when modifications are introduced. Additionally, the test suite is adaptable for testing different LLMs, enabling a comparison of their effectiveness in handling the questions. The results are summarised in Table  1 , price is computed from average per request."
        ]
    },
    "global_footnotes": [
        "Vocabulary of Interlinked Datasets"
    ]
}