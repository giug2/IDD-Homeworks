{
    "id_table_1": {
        "caption": "Table 1 :  Filtered Scores comparison with the state-of-the-art model on the VQA-X datasets. Filtered scores only consider the samples that have correct answers. B4, M, R, C, S, BS, Acc are short for BLEU-4  [ 27 ] , METEOR  [ 28 ] , ROUGE-L  [ 29 ] , CIDEr  [ 30 ] , SPICE  [ 31 ] , BERTSCORE  [ 32 ] , answer accuracy.  ReRe I  denotes the result of measuring retrieval score with a cosine similarity of image-image( c  o  s  ( I q , I s ) c o s subscript I q subscript I s cos(I_{q},I_{s}) italic_c italic_o italic_s ( italic_I start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , italic_I start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) ), instead of the purposed method  c  o  s  ( I q , E s ) c o s subscript I q subscript E s cos(I_{q},E_{s}) italic_c italic_o italic_s ( italic_I start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , italic_E start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) .",
        "table": "S4.T1.1.1",
        "footnotes": [],
        "references": [
            "In Table 1, we present the performance scores compared to state-of-the-art models in the filtered version.  The scoring method includes unfiltered scores, which measure all predictions regardless of whether they are correct or not, and filtered scores, which measure only the predictions that match the correct answers.  In VQA-NLE, generating a good explanation based on accurate answers is important, and providing a good explanation for incorrect answers is meaningless.  Therefore, filtered scores are given more consideration.  Follow the  [ 34 ,  20 ] , VQA accuracy is measured as correct when the predicted answers are within the expected answers. Experimental results show that measuring the similarity between the querys image and the samples explanation shows higher performance than measuring image-image similarity for memory retrieval.  Compared to recent state-of-the-art models, OURS shows a performance improvement of 2  similar-to \\sim  3% in the metric measured by explanation score.  Through these results, we can confirm that retrieval information helps generate more accurate answers and higher-quality explanations."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Filtered score of oracle test.   O  r  a  c  l  e e O r a c l subscript e e Oracle_{e} italic_O italic_r italic_a italic_c italic_l italic_e start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT  using only answer feature and  O  r  a  c  l  e a  e O r a c l subscript e a e Oracle_{ae} italic_O italic_r italic_a italic_c italic_l italic_e start_POSTSUBSCRIPT italic_a italic_e end_POSTSUBSCRIPT  use answer and explanation features.   O  r  a  c  l  e e O r a c l subscript e e Oracle_{e} italic_O italic_r italic_a italic_c italic_l italic_e start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT  and  O  r  a  c  l  e a  e O r a c l subscript e a e Oracle_{ae} italic_O italic_r italic_a italic_c italic_l italic_e start_POSTSUBSCRIPT italic_a italic_e end_POSTSUBSCRIPT  show outstanding explanation score and accuracy in line with our intuition.",
        "table": "S4.T2.2.2",
        "footnotes": [],
        "references": [
            "From Table 2, we can see how much the performance of our model can be improved when the ideal retrieval samples are retrieved.  Ideal retrieval is retrieved from a memory database, using cosine similarity of ground truth answer, explanation with samples answer, and explanation."
        ]
    },
    "global_footnotes": []
}