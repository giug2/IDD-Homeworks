{
    "S4.T1": {
        "caption": "Table 1: InterVQA dev set accuracy comparisons across end-to-end baseline models and our method including variations. Zero-shot BLIP-2 model Li etÂ al. (2023) is the pretrained ViT-L FlanT5XL version from the original BLIP-2 paper. All captions are generated using BLIP-2 model, with pretrained ViT-L FlanT5XL version and visual-question-answering fine-tuned ViT-L FlanT5XL version. Ours (GPT-3) is a variation of our proposed pipeline, where we pass the intermediate stages to a black-box GPT-3 model.",
        "table": "<table id=\"S4.T1.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S4.T1.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\">System</td>\n<td id=\"S4.T1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">Dev Acc.</td>\n</tr>\n<tr id=\"S4.T1.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\">BLIP-2</td>\n<td id=\"S4.T1.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">81.1</td>\n</tr>\n<tr id=\"S4.T1.1.3\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.3.1\" class=\"ltx_td ltx_align_left ltx_border_t\">zero-shot Caption + GPT-3</td>\n<td id=\"S4.T1.1.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">62.7</td>\n</tr>\n<tr id=\"S4.T1.1.4\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.4.1\" class=\"ltx_td ltx_align_left\">coco-finetuned Caption + GPT-3</td>\n<td id=\"S4.T1.1.4.2\" class=\"ltx_td ltx_align_center\">68.0</td>\n</tr>\n<tr id=\"S4.T1.1.5\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.5.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Ours (GPT-3)</td>\n<td id=\"S4.T1.1.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\">78.0</td>\n</tr>\n<tr id=\"S4.T1.1.6\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.6.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">Ours</td>\n<td id=\"S4.T1.1.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">71.2</td>\n</tr>\n</table>\n",
        "footnotes": "",
        "references": [
            "As shown in Table 1, we can see that by adding an intermediate stage of explanations, the model (zero-shot BLIP-2 Caption + GPT-3) suffers from a huge gap from the direct BLIP-2 VQA prediction. However, by using our method, we can achieve much better performance while having a close-to-sota end task result."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: We conduct ablation studies on the collected InterVQA gold data using GPT-3 for final answer prediction.",
        "table": "<table id=\"S4.T2.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S4.T2.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\">Method</td>\n<td id=\"S4.T2.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">few-shot</td>\n<td id=\"S4.T2.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">Dev Acc.</td>\n</tr>\n<tr id=\"S4.T2.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\">question only</td>\n<td id=\"S4.T2.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0-shot</td>\n<td id=\"S4.T2.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">14.16</td>\n</tr>\n<tr id=\"S4.T2.1.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.3.1\" class=\"ltx_td ltx_align_left\">question only</td>\n<td id=\"S4.T2.1.3.2\" class=\"ltx_td ltx_align_center\">4-shot</td>\n<td id=\"S4.T2.1.3.3\" class=\"ltx_td ltx_align_center\">48.44</td>\n</tr>\n<tr id=\"S4.T2.1.4\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.4.1\" class=\"ltx_td ltx_align_left\">visual clues</td>\n<td id=\"S4.T2.1.4.2\" class=\"ltx_td ltx_align_center\">0-shot</td>\n<td id=\"S4.T2.1.4.3\" class=\"ltx_td ltx_align_center\">74.68</td>\n</tr>\n<tr id=\"S4.T2.1.5\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.5.1\" class=\"ltx_td ltx_align_left\">visual clues</td>\n<td id=\"S4.T2.1.5.2\" class=\"ltx_td ltx_align_center\">4-shot</td>\n<td id=\"S4.T2.1.5.3\" class=\"ltx_td ltx_align_center\">82.67</td>\n</tr>\n<tr id=\"S4.T2.1.6\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.6.1\" class=\"ltx_td ltx_align_left\">visual clues + inferences</td>\n<td id=\"S4.T2.1.6.2\" class=\"ltx_td ltx_align_center\">0-shot</td>\n<td id=\"S4.T2.1.6.3\" class=\"ltx_td ltx_align_center\">80.69</td>\n</tr>\n<tr id=\"S4.T2.1.7\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.7.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">visual clues + inferences</td>\n<td id=\"S4.T2.1.7.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">4-shot</td>\n<td id=\"S4.T2.1.7.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">93.78</td>\n</tr>\n</table>\n",
        "footnotes": "",
        "references": [
            "As shown in Table 2, we test our data quality using the GPT models, by checking whether the GPT model can get the correct answer, given the question, gold visual clues and gold inferences only, without the image. Consider that the val set size is only 225, we believe that the GPT performance is high enough to prove that our annotated data is high quality for the VQA task."
        ]
    }
}