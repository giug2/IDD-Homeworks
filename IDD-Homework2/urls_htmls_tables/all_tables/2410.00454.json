{
    "id_table_1": {
        "caption": "Table 1:  Different routing strategies of recent methods. Parametric memory encodes knowledge within the models parameters, whereas retrieval memory stores information in an external memory system for future access. Sentence embeddings preserve the semantic meaning of entire sentences, while activation scores represent the outputs from the activation layers of the neural network. Anchor embedding is formed by combining the embeddings of entities (such as subjects and objects) in a sentence with token embeddings through a concatenation operation.",
        "table": "S2.T1.1.1",
        "footnotes": [],
        "references": [
            "Multiple recent methods, shown in Table  1 , incorporate  memories  and  routing mechanisms  to process inputs efficiently. The router is crucial in detecting and forwarding inputs to designated memories. If an input falls inside the scope of the existing edits, the router forwards it to the designated memory, which contains the new knowledge, thereby increasing reliability and generality. Conversely, inputs that fall outside of the edits are routed to the original model, maintaining locality. Due to the importance of the router  Zhou et al. ( 2022 ); Dikkala et al. ( 2023 ) , we prioritize optimizing routing mechanisms over memory enhancements. In the following, we discuss existing efforts on improving both routing inputs and routing algorithms and justify the design choices that we make for developing our method.",
            "In this section, we present the details of UniAdapt, a universal adapter based on the MoE architecture and a vector-assisted routing strategy, as illustrated in Figure  1 . UniAdapt is appended immediately after a selected MLP layer to calibrate the output.",
            "The core idea of UniAdapt is to introduce several MoE-style experts to facilitate knowledge updates and learning, while keeping all the original parameters of LLM frozen to maintain its original behavior. Figure  1  introduces the forward pass of UniAdapt. UniAdapt consists of a router and multiple parallel experts. This module is appended to the original MLP to calibrate the original knowledge. The outputs of all experts are aggregated as a weighted sum to produce the final output. This choice aligns with recent experimental findings based on knowledge probing technologies, i.e., the MLP layers store knowledge  Geva et al. ( 2020 ) . Unlike traditional MoE, the router has a vector store for sentence embeddings. Given a token  x i subscript x i x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  within the input sequence  x x x italic_x  =  { x i } i = 1 L superscript subscript subscript x i i 1 L \\{x_{i}\\}_{i=1}^{L} { italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT , our adapter with  K K K italic_K  experts computes a gate decision vector  G G \\mathcal{G} caligraphic_G  that decides which expert to send the token  x i subscript x i x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  to. This is defined as follows.",
            "The loss aims to maximize the distance between a positive pair and multiple negative pairs. Note that the objective in ( 8 ) is typically satisfied by most pre-trained sentence embedding frameworks  Reimers ( 2019 ); Gao et al. ( 2021 ) . Therefore, fine-tuning them with the loss function in ( 10 ) is sufficient to produce accurate similarity scores.",
            "Datasets and Metrics . We use two prominent model editing datasets: zsRE  Levy et al. ( 2017 )  and Counterfact  Meng et al. ( 2022a )  for performance evaluation. zsRE is a context-free Question-Answering (QA) dataset built upon zero-shot relation extraction. Counterfact is a more challenging dataset containing factual knowledge with diverse subjects, relations, and linguistic variations. We evaluate the capability of UniAdapt using Reliability, Generality, and Locality (defined in Sect  2.1 ) along with the average scores over these metrics. Specifically, each edit record contains an editing pair  ( x e , y e ) subscript x e subscript y e (x_{e},y_{e}) ( italic_x start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT )  along with a related edit  x r subscript x r x_{r} italic_x start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT  and an unrelated edit  x o subscript x o x_{o} italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT . The Reliability assesses if the edited model can recall the response  y e subscript y e y_{e} italic_y start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT  from  x e subscript x e x_{e} italic_x start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT . The Generality evaluates whether the edited model can produce  y e subscript y e y_{e} italic_y start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT  given  x r subscript x r x_{r} italic_x start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT . The Locality measures whether the edited model produces a consistent response for  x r subscript x r x_{r} italic_x start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT  both before and after the edit."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Main editing results with the number of edits  T = 1 T 1 T\\!=\\!1 italic_T = 1 .  Bold  is the best result, and  underline  is the second-best result.",
        "table": "S4.T2.3.1",
        "footnotes": [],
        "references": [
            "where  R  (  ) R  R(\\cdot) italic_R (  )  defines a routing strategy (refer to details in  3.2 ). Note that the router makes the routing decision based on the whole sentence  x x x italic_x . Consequently, all tokens  x i subscript x i x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  within the sentence  x x x italic_x  are directed to the same experts. The function  Top k  (  ) subscript Top k  \\textrm{Top}_{k}(\\cdot) Top start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT (  )  keeps only the top-k values and sets all others to zero. The function  H H H italic_H  is the Heaviside step function that outputs 1 for any non-negative input and 0 otherwise. Once the gate decision vector  G G \\mathcal{G} caligraphic_G  is obtained, the corresponding output  h i subscript h i h_{i} italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is generated through a weighted aggregation of each experts computation on  x i subscript x i x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , as follows:",
            "Routing Strategy . Similar to SERAC, we need a memory to store the edits to make semantic similarity queries. Unlike SERAC, we aim to store sentence embeddings (rather than the sentences themselves) in a vector store, both to reduce memory usage and to ensure compatibility with a wide range of frameworks  Douze et al. ( 2024 ); Johnson et al. ( 2019 ) . An example illustrating the functionality of the router is shown in Figure  2 .",
            "Datasets and Metrics . We use two prominent model editing datasets: zsRE  Levy et al. ( 2017 )  and Counterfact  Meng et al. ( 2022a )  for performance evaluation. zsRE is a context-free Question-Answering (QA) dataset built upon zero-shot relation extraction. Counterfact is a more challenging dataset containing factual knowledge with diverse subjects, relations, and linguistic variations. We evaluate the capability of UniAdapt using Reliability, Generality, and Locality (defined in Sect  2.1 ) along with the average scores over these metrics. Specifically, each edit record contains an editing pair  ( x e , y e ) subscript x e subscript y e (x_{e},y_{e}) ( italic_x start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT )  along with a related edit  x r subscript x r x_{r} italic_x start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT  and an unrelated edit  x o subscript x o x_{o} italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT . The Reliability assesses if the edited model can recall the response  y e subscript y e y_{e} italic_y start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT  from  x e subscript x e x_{e} italic_x start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT . The Generality evaluates whether the edited model can produce  y e subscript y e y_{e} italic_y start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT  given  x r subscript x r x_{r} italic_x start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT . The Locality measures whether the edited model produces a consistent response for  x r subscript x r x_{r} italic_x start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT  both before and after the edit.",
            "Single Editing . We evaluate the performance of UniAdapt in the single editing setting,  T T T italic_T = 1 1 1 1 , and compute the average of 1000 runs. The evaluation results are shown in Table  2 . We observe that UniAdapt consistently outperforms baselines across all tested models and most metrics. The results are balanced as it achieves scores of at least  0.97 0.97 0.97 0.97  in all metrics. In the zsRE setting, UniAdapt achieves scores of 1.00 and 0.98 on GPT2-XL and LLaMA2, respectively, achieving improvements of 28% and 0% over the second-best competitor. Similarly, the improvements are 36% and 5% in the Counterfact setting. A closer investigation shows that other tools often sacrifice their generality to achieve higher locality. GRACE and MEND achieve 0.0 in generality but 1.0 in the locality within the zsRE setting of GPT2-XL. Overall, this result demonstrates the efficacy and stability of UniAdapts capability on handling a hard dataset (i.e., Counterfact).",
            "In our reported results in Table  2  and Table  3 , UniAdaptis reported with the following hyper-parameters: number of experts = 1,   italic- \\epsilon italic_  = 0.6, TopK = 1, edited layer = 0, and number of epochs to train the adapter = 25. It is worth noting that this configuration is not our best  our optimal setup uses an edited layer of 3 and 4 experts."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Main editing results with the number of edits  T T T italic_T = 1000 1000 1000 1000 .  Bold  is the best result, and  underline  is the second-best result.",
        "table": "S4.T3.5.1",
        "footnotes": [],
        "references": [
            "where  R  (  ) R  R(\\cdot) italic_R (  )  defines a routing strategy (refer to details in  3.2 ). Note that the router makes the routing decision based on the whole sentence  x x x italic_x . Consequently, all tokens  x i subscript x i x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  within the sentence  x x x italic_x  are directed to the same experts. The function  Top k  (  ) subscript Top k  \\textrm{Top}_{k}(\\cdot) Top start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT (  )  keeps only the top-k values and sets all others to zero. The function  H H H italic_H  is the Heaviside step function that outputs 1 for any non-negative input and 0 otherwise. Once the gate decision vector  G G \\mathcal{G} caligraphic_G  is obtained, the corresponding output  h i subscript h i h_{i} italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is generated through a weighted aggregation of each experts computation on  x i subscript x i x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , as follows:",
            "Lifelong Editing . We evaluate the performance of UniAdapt in the lifelong editing setting,  T T T italic_T = 1000 1000 1000 1000 . The evaluation results are shown in table  3 . The results clearly show a decline in the performance across all methods as  T T T italic_T  increases from 1 to 1000. For example, FT and MEMIT experience a drop of over 50% and 20% respectively in almost all settings. This is attributed to the fact that new edits tend to overwrite previous ones. Among these methods, UniAdapt shows a negligible decline on the easier zsRE, and a significant advantage in terms of generalizing ability on Counterfact. A further analysis reveals that UniAdapt significantly outperforms the nearest competitor by a large margin. In the GPT2-XL setting, UniAdapt has a remarkable gap of around 40% over MEMIT on the zsRE dataset. In the LLaMA2-7B setting, UniAdapt proves to be the best with around 40% difference compared to WISE in the Counterfact dataset. In both datasets, our overall score is the highest, significantly outperforming the other methods. Furthermore, while the lifelong editing setting has proved to be more challenging than the single editing setting, UniAdapt maintains impressive stability across models. The difference remains below 7% in all metrics and under 5% in the average score. In summary, UniAdapt excels at learning extensive new knowledge while preserving other unrelated pre-trained knowledge.",
            "Effect of the Target Layer . We conduct multiple experiments to  assess the impact of the choice of target layer on the performance . We sequentially append UniAdapt to the MLP module of each transformer block and evaluate the performance of UniAdapt with 1000 edits. The results are illustrated in Figure  3  across various target layers. While locality remains stable, both reliability and generality encounter significant fluctuations, peaking at layer 3 and reaching their lowest point at the final layer. Our finding aligns with the work  Zhao et al. ( 2024 )  that confirms the importance of editing the model at layer 3. Notably, regardless of the layer modified, generality consistently hits the lowest accuracy among all metrics, indicating that it is the most challenging metric to improve. Overall, performance tends to decline sharply as the target layer approaches the last layer.",
            "Effect of the Number of Experts . We perform multiple experiments to study  how the number of experts impacts the performance . Due to computational resource limitations, we sequentially set the number of experts to values in the range  [ 1    10 ] delimited-[] 1  10 [1\\textendash 10] [ 1  10 ]  and evaluate UniAdapts performance with 1000 edits. Figure  3  illustrates the performance of UniAdapt with different numbers of experts. We find that the locality of model editing does not change with the number of experts, i.e., there is neither a decrease nor a performance improvement. This is expected because only relevant inputs are forwarded to experts. The reliability exhibits slight fluctuation (i.e., going upward and then downward) when the number of experts increases. Furthermore, it consistently remains above 0.95 across all scenarios. Unlike reliability and locality, the generalization of knowledge fluctuates with the number of experts, peaking when the number of experts is 4, i.e., increasing the number of experts initially boosts overall performance, but eventually leads to a decline. We hypothesize that the reason is that while having more experts can enhance recall by providing specialized knowledge, it may also make it more challenging for the router to effectively choose the most suitable experts.",
            "Effect of   italic- \\epsilon italic_ . We conduct multiple experiments to  evaluate the impacts of   italic- \\epsilon italic_  on the performance . We sequentially set the   italic- \\epsilon italic_  to values in the range  [ 0.1    0.9 ] delimited-[] 0.1  0.9 [0.1\\textendash 0.9] [ 0.1  0.9 ]  and evaluate UniAdapts performance after 1000 edits. Figure  3  depicts the performance of UniAdapt across various   italic- \\epsilon italic_ . The results show that   italic- \\epsilon italic_  has little impact on the reliability and generality. In contrast, locality increases sharply as   italic- \\epsilon italic_  is raised from 0.1 to 0.6. This can be attributed to the behavior of the router at low   italic- \\epsilon italic_  values. With a low   italic- \\epsilon italic_ , the router tends to misclassify unrelated inputs, while relevant inputs remain unchanged. As   italic- \\epsilon italic_  increases, the router becomes more selective and only forwards inputs that are highly likely to be relevant, leading to higher locality.",
            "Effect of top-k routing . We conduct multiple experiments to  evaluate the impacts of top-k routing on UniAdapts performance . We sequentially set  K K K italic_K  to values in the range  [ 1    5 ] delimited-[] 1  5 [1\\textendash 5] [ 1  5 ] , fix the number of experts at 5, and evaluate our performance after 1000 edits. Figure  3  depicts the performance of UniAdapt across various  K K K italic_K . The results show that the locality remains unchanged across the different  K K K italic_K  values.  However, reliability and generality consistently decrease as  K K K italic_K  increases. This suggests that while top-k routing does not impact locality, it hurts reliability and generality as the number of routing options increases. Interestingly, the best overall performance is achieved when  K K K italic_K = 1 1 1 1 , indicating that using a single optimal routing path leads to the highest reliability and generality. As  K K K italic_K  increases, the UniAdapt becomes less focused and may allocate resources to less relevant routing options, leading to decreased performance in terms of reliability and generality.",
            "In our reported results in Table  2  and Table  3 , UniAdaptis reported with the following hyper-parameters: number of experts = 1,   italic- \\epsilon italic_  = 0.6, TopK = 1, edited layer = 0, and number of epochs to train the adapter = 25. It is worth noting that this configuration is not our best  our optimal setup uses an edited layer of 3 and 4 experts."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Scaling to 6000 edits on zsRE dataset with LLaMA2-7b",
        "table": "S4.T4.1.1",
        "footnotes": [],
        "references": [
            "Scale up to 6K . We conduct multiple experiments to  assess the capability of UniAdapt on handling long continual edits . We sequentially scale the number of edits to  2000 2000 2000 2000 ,  3000 3000 3000 3000 , and  6000 6000 6000 6000  and report our results along with WISE (the second-best competitor in our experiments) in Table  4 . From the results, we observe that UniAdapt remains the best editor. WISE experiences a significant decline in both generality and reliability, dropping from 0.64 to 0.48 and 0.70 to 0.50 respectively. This is expected because WISE tends to incorrectly select the side memory when the number of edits increases. UniAdapt experiences a slight decrease of less than 0.02 in both metrics. Overall, the results highlight UniAdapts exceptional performance on handling long continual edits, which makes it a practical solution."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Editing dataset example",
        "table": "A1.T5.3",
        "footnotes": [],
        "references": [
            "We utilized two standard datasets: zsRE  Levy et al. ( 2017 )  and Counterfact  Meng et al. ( 2022a ) . Table  5  illustrates examples from these datasets, where each row has three pairs: ( x e , y e subscript x e subscript y e x_{e},y_{e} italic_x start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ),  ( x i  r  r , y i  r  r ) subscript x i r r subscript y i r r (x_{irr},y_{irr}) ( italic_x start_POSTSUBSCRIPT italic_i italic_r italic_r end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_i italic_r italic_r end_POSTSUBSCRIPT )  and ( P  ( x e ) , y e P subscript x e subscript y e \\mathcal{P}(x_{e}),y_{e} caligraphic_P ( italic_x start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ) , italic_y start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ) for the evaluation. ZsRE is a context-free Question-answering (QA) dataset containing factual information. In contrast, Counterfact focuses on counterfactual information. Compared to zsRE, the Counterfact dataset is considered more challenging to apply, as it attempts to erase the models existing contradictory information. Consequently, it often yields lower accuracy. In our experiments with these datasets, we adopt the version proposed by  Yao et al. ( 2023 )"
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Counterfact dataset. Editing performance across all layers",
        "table": "A1.T6.1.1",
        "footnotes": [],
        "references": [
            "In general, an adapters effectiveness heavily depends on the layers selected for editing. Choosing the right layer for a specific dataset is crucial to achieving high accuracy. In addition to the results presented in the main content, we explored modifying different layers of two primary models: GPT2-XL and LLaMA2-7B, to identify the optimal layer for editing. Table  6  shows that for GPT2-XL, layer 16 achieves the highest score of 0.83, with layers 1 and 17 tying for second at 0.82. For LLaMA2-7B, layer 4 performs best, followed closely by layer 3. Overall, the best layer for editing varies between models. However, layer 0 emerges as a reliable choice, consistently yielding relatively high accuracy across models. Moreover, earlier layers typically yield better results than later ones."
        ]
    },
    "global_footnotes": []
}