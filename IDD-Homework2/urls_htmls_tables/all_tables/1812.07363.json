{
    "S1.T1": {
        "caption": "Table 1: Three face detection benchmarks and their characteristics.\n",
        "table": "<table id=\"S1.T1.1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S1.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S1.T1.1.1.1.1\" class=\"ltx_td ltx_nopad ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\"><svg version=\"1.1\" height=\"18.92\" width=\"104.46\" overflow=\"visible\"><g transform=\"translate(0,18.92) scale(1,-1)\"><path d=\"M 0,18.92 104.46,0\" stroke=\"#000000\" stroke-width=\"0.4\"></path><g class=\"ltx_svg_fog\" transform=\"translate(0,0)\"><g transform=\"translate(0,9.46) scale(1, -1)\"><foreignobject width=\"45.59\" height=\"9.46\" overflow=\"visible\">\n<span id=\"S1.T1.1.1.1.1.pic1.1.1\" class=\"ltx_inline-block\">\n<span id=\"S1.T1.1.1.1.1.pic1.1.1.1\" class=\"ltx_inline-block ltx_align_left\">\n<span id=\"S1.T1.1.1.1.1.pic1.1.1.1.1\" class=\"ltx_p\">Feature</span>\n</span>\n</span></foreignobject></g></g><g class=\"ltx_svg_fog\" transform=\"translate(52.23,9.46)\"><g transform=\"translate(0,9.46) scale(1, -1)\"><foreignobject width=\"52.23\" height=\"9.46\" overflow=\"visible\">\n<span id=\"S1.T1.1.1.1.1.pic1.2.1\" class=\"ltx_inline-block\">\n<span id=\"S1.T1.1.1.1.1.pic1.2.1.1\" class=\"ltx_inline-block ltx_align_right\">\n<span id=\"S1.T1.1.1.1.1.pic1.2.1.1.1\" class=\"ltx_p\">Datasets</span>\n</span>\n</span></foreignobject></g></g></g></svg></th>\n<th id=\"S1.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">MAFA</th>\n<th id=\"S1.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">UFDD</th>\n<th id=\"S1.T1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Wider</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S1.T1.1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt\">landmark occlusion</td>\n<td id=\"S1.T1.1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">&#10003;</td>\n<td id=\"S1.T1.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">&#10003;</td>\n<td id=\"S1.T1.1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">&#10003;</td>\n</tr>\n<tr id=\"S1.T1.1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\">complex background</td>\n<td id=\"S1.T1.1.1.3.2.2\" class=\"ltx_td\"></td>\n<td id=\"S1.T1.1.1.3.2.3\" class=\"ltx_td\"></td>\n<td id=\"S1.T1.1.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\">&#10003;</td>\n</tr>\n<tr id=\"S1.T1.1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\">extreme pose</td>\n<td id=\"S1.T1.1.1.4.3.2\" class=\"ltx_td\"></td>\n<td id=\"S1.T1.1.1.4.3.3\" class=\"ltx_td\"></td>\n<td id=\"S1.T1.1.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\">&#10003;</td>\n</tr>\n<tr id=\"S1.T1.1.1.5.4\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\">extreme scale</td>\n<td id=\"S1.T1.1.1.5.4.2\" class=\"ltx_td\"></td>\n<td id=\"S1.T1.1.1.5.4.3\" class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td id=\"S1.T1.1.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_r\">&#10003;</td>\n</tr>\n<tr id=\"S1.T1.1.1.6.5\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\">heavy occlusion</td>\n<td id=\"S1.T1.1.1.6.5.2\" class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td id=\"S1.T1.1.1.6.5.3\" class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td id=\"S1.T1.1.1.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_r\">&#10003;</td>\n</tr>\n<tr id=\"S1.T1.1.1.7.6\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.1.7.6.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\">blur</td>\n<td id=\"S1.T1.1.1.7.6.2\" class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td id=\"S1.T1.1.1.7.6.3\" class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td id=\"S1.T1.1.1.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_r\">&#10003;</td>\n</tr>\n<tr id=\"S1.T1.1.1.8.7\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.1.8.7.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\">extreme illumination</td>\n<td id=\"S1.T1.1.1.8.7.2\" class=\"ltx_td\"></td>\n<td id=\"S1.T1.1.1.8.7.3\" class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td id=\"S1.T1.1.1.8.7.4\" class=\"ltx_td ltx_align_center ltx_border_r\">&#10003;</td>\n</tr>\n<tr id=\"S1.T1.1.1.9.8\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.1.9.8.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r\">misleading objects</td>\n<td id=\"S1.T1.1.1.9.8.2\" class=\"ltx_td ltx_border_b\"></td>\n<td id=\"S1.T1.1.1.9.8.3\" class=\"ltx_td ltx_align_center ltx_border_b\">&#10003;</td>\n<td id=\"S1.T1.1.1.9.8.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">&#10003;</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Face detection is one of the important topics in the field of computer vision. It plays a fundamental role in basically all face related applications. Face detection is the problem of determining the presence of faces in images and their precise locations. Face detection is confronted with different challenges such as variations in scale, pose, expression, occlusion and illumination which all may have a negative influence on the performance of face detection methods. In Table 1, we summarized the different characteristics of various face detection benchmarks. From the table, it can be derived that many datasets are limited in representing extreme poses, different scales and heavy occlusions. However, datasets containing face images under a wide variety of imaging conditions are required to develop face detectors which are robust to all variations of image formation process."
        ]
    },
    "S1.T2": {
        "caption": "Table 2: Three advanced face detectors and their characteristics.\n",
        "table": "<table id=\"S1.T2.1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S1.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S1.T2.1.1.1.1\" class=\"ltx_td ltx_nopad ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\"><svg version=\"1.1\" height=\"18.92\" width=\"104.24\" overflow=\"visible\"><g transform=\"translate(0,18.92) scale(1,-1)\"><path d=\"M 0,18.92 104.24,0\" stroke=\"#000000\" stroke-width=\"0.4\"></path><g class=\"ltx_svg_fog\" transform=\"translate(0,0)\"><g transform=\"translate(0,9.46) scale(1, -1)\"><foreignobject width=\"45.59\" height=\"9.46\" overflow=\"visible\">\n<span id=\"S1.T2.1.1.1.1.pic1.1.1\" class=\"ltx_inline-block\">\n<span id=\"S1.T2.1.1.1.1.pic1.1.1.1\" class=\"ltx_inline-block ltx_align_left\">\n<span id=\"S1.T2.1.1.1.1.pic1.1.1.1.1\" class=\"ltx_p\">Feature</span>\n</span>\n</span></foreignobject></g></g><g class=\"ltx_svg_fog\" transform=\"translate(52.12,9.46)\"><g transform=\"translate(0,9.46) scale(1, -1)\"><foreignobject width=\"52.12\" height=\"9.46\" overflow=\"visible\">\n<span id=\"S1.T2.1.1.1.1.pic1.2.1\" class=\"ltx_inline-block\">\n<span id=\"S1.T2.1.1.1.1.pic1.2.1.1\" class=\"ltx_inline-block ltx_align_right\">\n<span id=\"S1.T2.1.1.1.1.pic1.2.1.1.1\" class=\"ltx_p\">Detector</span>\n</span>\n</span></foreignobject></g></g></g></svg></th>\n<th id=\"S1.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Faster RCNN</th>\n<th id=\"S1.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">SSH</th>\n<th id=\"S1.T2.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">HR</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S1.T2.1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S1.T2.1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt\">landmark occlusion</td>\n<td id=\"S1.T2.1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">&#10003;</td>\n<td id=\"S1.T2.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">&#10003;</td>\n<td id=\"S1.T2.1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">&#10003;</td>\n</tr>\n<tr id=\"S1.T2.1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S1.T2.1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\">complex background</td>\n<td id=\"S1.T2.1.1.3.2.2\" class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td id=\"S1.T2.1.1.3.2.3\" class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td id=\"S1.T2.1.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\">&#10003;</td>\n</tr>\n<tr id=\"S1.T2.1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S1.T2.1.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\">extreme pose</td>\n<td id=\"S1.T2.1.1.4.3.2\" class=\"ltx_td\"></td>\n<td id=\"S1.T2.1.1.4.3.3\" class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td id=\"S1.T2.1.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\">&#10003;</td>\n</tr>\n<tr id=\"S1.T2.1.1.5.4\" class=\"ltx_tr\">\n<td id=\"S1.T2.1.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\">extreme scale</td>\n<td id=\"S1.T2.1.1.5.4.2\" class=\"ltx_td\"></td>\n<td id=\"S1.T2.1.1.5.4.3\" class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td id=\"S1.T2.1.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_r\">&#10003;</td>\n</tr>\n<tr id=\"S1.T2.1.1.6.5\" class=\"ltx_tr\">\n<td id=\"S1.T2.1.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\">heavy occlusion</td>\n<td id=\"S1.T2.1.1.6.5.2\" class=\"ltx_td\"></td>\n<td id=\"S1.T2.1.1.6.5.3\" class=\"ltx_td\"></td>\n<td id=\"S1.T2.1.1.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_r\">&#10003;</td>\n</tr>\n<tr id=\"S1.T2.1.1.7.6\" class=\"ltx_tr\">\n<td id=\"S1.T2.1.1.7.6.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\">blur</td>\n<td id=\"S1.T2.1.1.7.6.2\" class=\"ltx_td\"></td>\n<td id=\"S1.T2.1.1.7.6.3\" class=\"ltx_td\"></td>\n<td id=\"S1.T2.1.1.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_r\">&#10003;</td>\n</tr>\n<tr id=\"S1.T2.1.1.8.7\" class=\"ltx_tr\">\n<td id=\"S1.T2.1.1.8.7.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\">extreme illumination</td>\n<td id=\"S1.T2.1.1.8.7.2\" class=\"ltx_td\"></td>\n<td id=\"S1.T2.1.1.8.7.3\" class=\"ltx_td\"></td>\n<td id=\"S1.T2.1.1.8.7.4\" class=\"ltx_td ltx_align_center ltx_border_r\">&#10003;</td>\n</tr>\n<tr id=\"S1.T2.1.1.9.8\" class=\"ltx_tr\">\n<td id=\"S1.T2.1.1.9.8.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r\">misleading objects</td>\n<td id=\"S1.T2.1.1.9.8.2\" class=\"ltx_td ltx_border_b\"></td>\n<td id=\"S1.T2.1.1.9.8.3\" class=\"ltx_td ltx_border_b\"></td>\n<td id=\"S1.T2.1.1.9.8.4\" class=\"ltx_td ltx_border_b ltx_border_r\"></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Face detectors are designed to address only a limited set of variations in real-world situations. For example, FAN[33] uses an attention based structure and data augmentation to cope with facial occlusion. PCN [29] proposes rotation-invariant face detection in a coarse-to-fine manner by dividing the calibration process into several progressive steps. The HR detector [15] combines both features and image pyramids to make the algorithm robustness against extreme face scales. In Table 2, we show that different face detectors are designed to cope with different imaging conditions. These face detectors heavily rely on the availability of large-scale annotated datasets. Collecting and annotating real-world datasets with different imaging conditions is tedious, time consuming and in some cases even unfeasible. Furthermore, it is difficult to systematically vary the imaging parameters and to avoid errors during the annotation process. Errors in ground truth may lead to far-reaching impact on training and testing of the networks. Therefore, our contribution is to generate synthetic data, as complementary to real data, to create fully controlled datasets by means of automatic and error-less annotation. To validate our methodology, we train different face detectors on a combination of real data and fully controlled synthetic dataset, to systematically address the imaging variations. Our synthetic images are rendered versions of real 3D faces with changes in viewpoint, scale, illumination, occlusion and background. Hence, the variation of imaging conditions is performed in 3D space."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Face scale information of the validation set in Wider Face. We distinguish three face categories based on height and width. Proportion information represents the percentage of faces that fits within the scale interval.",
        "table": "<table id=\"S4.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Partition</th>\n<td id=\"S4.T3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Large</td>\n<td id=\"S4.T3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Medium</td>\n<td id=\"S4.T3.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Tiny</td>\n</tr>\n<tr id=\"S4.T3.1.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Height</th>\n<td id=\"S4.T3.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">50-400(96.6%)</td>\n<td id=\"S4.T3.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">30-50(99%)</td>\n<td id=\"S4.T3.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">10-30(99%)</td>\n</tr>\n<tr id=\"S4.T3.1.3.3\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Width</th>\n<td id=\"S4.T3.1.3.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">20-300(96.3%)</td>\n<td id=\"S4.T3.1.3.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">10-70(99.7%)</td>\n<td id=\"S4.T3.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">8-20(95%)</td>\n</tr>\n<tr id=\"S4.T3.1.4.4\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Number</th>\n<td id=\"S4.T3.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">7211</td>\n<td id=\"S4.T3.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">6108</td>\n<td id=\"S4.T3.1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">18636</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Wider Face is the most challenging benchmark for face detection \u00a0[35]. It includes various events (e.g., basketball, football) with a variety of backgrounds. The large number of faces contain extreme poses, exaggerated expressions, heavy occlusion and extreme lighting conditions. The most challenging part of Wider Face is the extreme scale. Wider Face has three categories of difficulty: easy, medium, and hard. The criteria to categorize faces into these different categories are vague. Our Table 3 shows the basic characteristics of faces, irrespective of invalid faces, in the Wider Face validation partition."
        ]
    },
    "S6.T4": {
        "caption": "Table 4: Average precision from HR[15] trained on different sets of synthetic data. These three sets are combined with real data to improve detectors\u2019 performance. Different settings: s1subscript\ud835\udc601s_{1} is our basic settings for rendering with light occlusion. s2subscript\ud835\udc602s_{2} combines s1subscript\ud835\udc601s_{1} with extra occlusion from other faces in the render process. s3subscript\ud835\udc603s_{3} adds additional blurry results from down-sampled high resolution images into s2subscript\ud835\udc602s_{2}.",
        "table": "<table id=\"S6.T4.3.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S6.T4.3.3.4.1\" class=\"ltx_tr\">\n<th id=\"S6.T4.3.3.4.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Set</th>\n<th id=\"S6.T4.3.3.4.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Easy</th>\n<th id=\"S6.T4.3.3.4.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Medium</th>\n<th id=\"S6.T4.3.3.4.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Hard</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S6.T4.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math id=\"S6.T4.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"s_{1}\" display=\"inline\"><semantics id=\"S6.T4.1.1.1.1.m1.1a\"><msub id=\"S6.T4.1.1.1.1.m1.1.1\" xref=\"S6.T4.1.1.1.1.m1.1.1.cmml\"><mi id=\"S6.T4.1.1.1.1.m1.1.1.2\" xref=\"S6.T4.1.1.1.1.m1.1.1.2.cmml\">s</mi><mn id=\"S6.T4.1.1.1.1.m1.1.1.3\" xref=\"S6.T4.1.1.1.1.m1.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S6.T4.1.1.1.1.m1.1b\"><apply id=\"S6.T4.1.1.1.1.m1.1.1.cmml\" xref=\"S6.T4.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S6.T4.1.1.1.1.m1.1.1.1.cmml\" xref=\"S6.T4.1.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S6.T4.1.1.1.1.m1.1.1.2.cmml\" xref=\"S6.T4.1.1.1.1.m1.1.1.2\">&#119904;</ci><cn type=\"integer\" id=\"S6.T4.1.1.1.1.m1.1.1.3.cmml\" xref=\"S6.T4.1.1.1.1.m1.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T4.1.1.1.1.m1.1c\">s_{1}</annotation></semantics></math></th>\n<td id=\"S6.T4.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.795</td>\n<td id=\"S6.T4.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.742</td>\n<td id=\"S6.T4.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.502</td>\n</tr>\n<tr id=\"S6.T4.2.2.2\" class=\"ltx_tr\">\n<th id=\"S6.T4.2.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math id=\"S6.T4.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"s_{2}\" display=\"inline\"><semantics id=\"S6.T4.2.2.2.1.m1.1a\"><msub id=\"S6.T4.2.2.2.1.m1.1.1\" xref=\"S6.T4.2.2.2.1.m1.1.1.cmml\"><mi id=\"S6.T4.2.2.2.1.m1.1.1.2\" xref=\"S6.T4.2.2.2.1.m1.1.1.2.cmml\">s</mi><mn id=\"S6.T4.2.2.2.1.m1.1.1.3\" xref=\"S6.T4.2.2.2.1.m1.1.1.3.cmml\">2</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S6.T4.2.2.2.1.m1.1b\"><apply id=\"S6.T4.2.2.2.1.m1.1.1.cmml\" xref=\"S6.T4.2.2.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S6.T4.2.2.2.1.m1.1.1.1.cmml\" xref=\"S6.T4.2.2.2.1.m1.1.1\">subscript</csymbol><ci id=\"S6.T4.2.2.2.1.m1.1.1.2.cmml\" xref=\"S6.T4.2.2.2.1.m1.1.1.2\">&#119904;</ci><cn type=\"integer\" id=\"S6.T4.2.2.2.1.m1.1.1.3.cmml\" xref=\"S6.T4.2.2.2.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T4.2.2.2.1.m1.1c\">s_{2}</annotation></semantics></math></th>\n<td id=\"S6.T4.2.2.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.818</td>\n<td id=\"S6.T4.2.2.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.774</td>\n<td id=\"S6.T4.2.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.53</td>\n</tr>\n<tr id=\"S6.T4.3.3.3\" class=\"ltx_tr\">\n<th id=\"S6.T4.3.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><math id=\"S6.T4.3.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"s_{3}\" display=\"inline\"><semantics id=\"S6.T4.3.3.3.1.m1.1a\"><msub id=\"S6.T4.3.3.3.1.m1.1.1\" xref=\"S6.T4.3.3.3.1.m1.1.1.cmml\"><mi id=\"S6.T4.3.3.3.1.m1.1.1.2\" xref=\"S6.T4.3.3.3.1.m1.1.1.2.cmml\">s</mi><mn id=\"S6.T4.3.3.3.1.m1.1.1.3\" xref=\"S6.T4.3.3.3.1.m1.1.1.3.cmml\">3</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S6.T4.3.3.3.1.m1.1b\"><apply id=\"S6.T4.3.3.3.1.m1.1.1.cmml\" xref=\"S6.T4.3.3.3.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S6.T4.3.3.3.1.m1.1.1.1.cmml\" xref=\"S6.T4.3.3.3.1.m1.1.1\">subscript</csymbol><ci id=\"S6.T4.3.3.3.1.m1.1.1.2.cmml\" xref=\"S6.T4.3.3.3.1.m1.1.1.2\">&#119904;</ci><cn type=\"integer\" id=\"S6.T4.3.3.3.1.m1.1.1.3.cmml\" xref=\"S6.T4.3.3.3.1.m1.1.1.3\">3</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T4.3.3.3.1.m1.1c\">s_{3}</annotation></semantics></math></th>\n<td id=\"S6.T4.3.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.828</td>\n<td id=\"S6.T4.3.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.796</td>\n<td id=\"S6.T4.3.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.627</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "In Table 4, we show the performance comparison about three synthetic data sets on Wider Face validation set. These three synthetic datasets s1,s2,s3subscript\ud835\udc601subscript\ud835\udc602subscript\ud835\udc603s_{1},s_{2},s_{3} are combined with real data to improve detection performance for UFDD[22] and Wider Face respectively in Section 6.4.2 and 6.4.3.",
            "In this section, our data augmentation is used with the Wider Face only. This is because (1) the training dataset of UFDD is not made public, and (2) UFDD itself is trained on Wider Face [22]. Three synthetic datasets s1,s2,s3subscript\ud835\udc601subscript\ud835\udc602subscript\ud835\udc603s_{1},s_{2},s_{3} are combined with real data to improve detectors\u2019 performance. Different settings: s1subscript\ud835\udc601s_{1} is our basic settings for rendering with light occlusion. s2subscript\ud835\udc602s_{2} combines s1subscript\ud835\udc601s_{1} with extra occlusion from other faces in the render process. s3subscript\ud835\udc603s_{3} adds additional blurry results from down-sampled high resolution images into s2subscript\ud835\udc602s_{2}. The performance of data augmentation is shown in Table 4. The influence of our data augmentation is shown in Figure 7. After merging synthetic data and real data together, the performance of Faster RCNN, which is trained on real data, improves significantly on r+s1\ud835\udc5fsubscript\ud835\udc601r+s_{1} and r+s2\ud835\udc5fsubscript\ud835\udc602r+s_{2}. Given that Faster RCNN is not trained for different scales, the noise of s3subscript\ud835\udc603s_{3} impedes its performance. As for SSH, its architecture and parameters heavily rely on Wider Face. For UFDD, its performance becomes saturated after being trained on real data. After adding synthetic data, its performance is even worse than Faster RCNN. Faces in UFDD are not very challenging to HR; the performance therefore only changes slightly after using our data augmentation."
        ]
    }
}