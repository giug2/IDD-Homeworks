{
    "id_table_1": {
        "caption": "Table 1:  Results on the MahaNews, MahaHate Dataset",
        "table": "S3.T2.1",
        "footnotes": [],
        "references": [
            "Approximately 100 sentences were selected from MahaSent-GT, MahaNews-SHC, and MahaHate. The large language models categorize each of the sentences into a predefined category. These results were compared with the ground truth values to calculate the error rate. The error rate was calculated with the direct prompting approach and Chain of Translation prompting approach The results are shown in Table  3 .   In the CoTR prompting approach, the error rate has reduced by 2.32% in the GPT-4o model, by 3.64% llama3-405b, by 5.29% in llama3-8b and by 4.96% in GPT-4o Mini. The error rate is slightly increased by 0.33% in the Gemma-9B model.   The error rate has been reduced by almost 5% in llama3-8b and gpt4 mini models. Specifically, the CoTR prompting approach has significantly improved hate speech identification across all models except for Gemma-9B. In the hate speech classification task, Gemma-9B often failed to correctly translate hateful comments and, in some cases, omitted those parts entirely. However, compared to standard prompting, the number of misclassifications for the \"Non-hate\" class was lower when using CoTR.   One sample detection with traditional prompting versus CoTR prompting from each of the four models has been attached in Table  1 , where the output with CoTR prompting is the same as the ground truth."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Rouge-L score in percentage for 3 approaches on the headline generation task on CSEBUETNLP XLSum Dataset",
        "table": "S3.T3.1",
        "footnotes": [],
        "references": [
            "Table  2  and Table  3  show the analysis done on Standard Prompting and Chain of Translation Prompting.",
            "The headlines from the Marathi news text were generated using traditional prompting and CoTR prompting (with half and full translation). The headlines were compared against the manually assigned headline and the Rouge-L score metric was used to calculate their similarity with the manually assigned headline. The Rouge-L score for traditional prompting and CoTR prompting (half and full translation) are given in Table  2   We observed that GPT-4o delivered the best performance among all the models. GPT-4o Mini struggled to identify fine details in the articles, while Llama3-405B occasionally failed to provide the results in the specified format and produced some inaccurate translations. Overall, GPT-4o Mini and Llama-405B yielded similar outcomes.   In general, we observe the following performance ranking for Marathi tasks: GPT-4o > GPT-4o Mini > Llama 3.1 405B > Gemma 2 9B > Llama 3.1 8B. The CoTR approach proves especially useful with smaller models and for complex tasks like hate speech detection and sentiment analysis."
        ]
    }
}