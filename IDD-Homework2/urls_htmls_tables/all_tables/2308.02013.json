{
    "PAPER'S NUMBER OF TABLES": 3,
    "S4.T1": {
        "caption": "Table 1: Datasets used. For Librispeech, the clean and other splits are mentioned for dev and test",
        "table": "<table id=\"S4.T1.6\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.6.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.6.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" rowspan=\"2\"><span id=\"S4.T1.6.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Dataset</span></th>\n<th id=\"S4.T1.6.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"3\"><span id=\"S4.T1.6.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Duration(in hours)</span></th>\n</tr>\n<tr id=\"S4.T1.6.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T1.6.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\"><span id=\"S4.T1.6.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Train</span></th>\n<td id=\"S4.T1.6.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.6.2.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Dev</span></td>\n<td id=\"S4.T1.6.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.6.2.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">Test</span></td>\n</tr>\n<tr id=\"S4.T1.6.3.3\" class=\"ltx_tr\">\n<th id=\"S4.T1.6.3.3.1\" class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th id=\"S4.T1.6.3.3.2\" class=\"ltx_td ltx_th ltx_th_row\"></th>\n<td id=\"S4.T1.6.3.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.6.3.3.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">(clean, other)</span></td>\n<td id=\"S4.T1.6.3.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.6.3.3.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">(clean, other)</span></td>\n</tr>\n<tr id=\"S4.T1.6.4.4\" class=\"ltx_tr\">\n<th id=\"S4.T1.6.4.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\"><span id=\"S4.T1.6.4.4.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Libri-Light Large</span></th>\n<th id=\"S4.T1.6.4.4.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\"><span id=\"S4.T1.6.4.4.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">51934</span></th>\n<td id=\"S4.T1.6.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.6.4.4.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">NA</span></td>\n<td id=\"S4.T1.6.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.6.4.4.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">NA</span></td>\n</tr>\n<tr id=\"S4.T1.6.5.5\" class=\"ltx_tr\">\n<th id=\"S4.T1.6.5.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><span id=\"S4.T1.6.5.5.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Librispeech</span></th>\n<th id=\"S4.T1.6.5.5.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><span id=\"S4.T1.6.5.5.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">960.9</span></th>\n<td id=\"S4.T1.6.5.5.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.6.5.5.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">5.4, 5.4</span></td>\n<td id=\"S4.T1.6.5.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.6.5.5.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">5.3, 5.1</span></td>\n</tr>\n<tr id=\"S4.T1.6.6.6\" class=\"ltx_tr\">\n<th id=\"S4.T1.6.6.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><span id=\"S4.T1.6.6.6.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Multilingual</span></th>\n<th id=\"S4.T1.6.6.6.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\" rowspan=\"2\"><span id=\"S4.T1.6.6.6.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">1076.6</span></th>\n<td id=\"S4.T1.6.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_b\" rowspan=\"2\"><span id=\"S4.T1.6.6.6.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">10</span></td>\n<td id=\"S4.T1.6.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_b\" rowspan=\"2\"><span id=\"S4.T1.6.6.6.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">10</span></td>\n</tr>\n</tbody>\n<tfoot class=\"ltx_tfoot\">\n<tr id=\"S4.T1.6.7.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.6.7.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\"><span id=\"S4.T1.6.7.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Librispeech French</span></th>\n</tr>\n</tfoot>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We use Libri-Light (LL) dataset [8] for pre-training. LL contains three parts***https://github.com/facebookresearch/libri-light/tree/main/data_preparation: small, medium, and large, we consider the large portion, that contains 52,0005200052,000 hours of unlabeled speech data generated from 684568456845 speakers. In the centralized pre-training experiments, we use all the 52K hours of the data, for FL, the same data is arranged in non-IID format, speaker-siloed, for pre-training. We further study the impact of the amount of data used in FL pre-training, for that, we use, 5​K5𝐾5K hours (L​L−5​K𝐿𝐿5𝐾LL-5K), a randomly selected subset from LL.\nIn fine-tuning the pre-trained models, we use the 960960960-hour train partition of the Librispeech [24]. The fine-tuned models are evaluated on the dev and test sets of Librispeech. In addition to these, we use the French data from the Multilingual Librispeech (MLS) dataset [25] in our resource-constrained language adaptation experiments. Table 1 summarizes the datasets."
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Central pre-trained and Federated pre-trained models with RNN-T fine-tuning on train-960960960 hour Librispeech. WER results are on the dev and test sets of Librispeech. No pre-training model (trainedfrom-scratch on train-960 hour) is the baseline. C refers to central pre-training, and FL refer to the Federated pre-trained models. The number next to FL indicates the number of clients used in each training round.",
        "table": "<table id=\"S5.T2.6\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.6.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T2.6.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\"><span id=\"S5.T2.6.1.1.1.1\" class=\"ltx_text\">\n<span id=\"S5.T2.6.1.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T2.6.1.1.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T2.6.1.1.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Pre-training</span></span>\n<span id=\"S5.T2.6.1.1.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T2.6.1.1.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Setting</span></span>\n</span></span></td>\n<td id=\"S5.T2.6.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\">Fine-tuning Word Error Rate (WER)</td>\n</tr>\n<tr id=\"S5.T2.6.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T2.6.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\">Dev-Clean/Other</td>\n<td id=\"S5.T2.6.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">Test-Clean/Other</td>\n</tr>\n<tr id=\"S5.T2.6.3.3\" class=\"ltx_tr\">\n<td id=\"S5.T2.6.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\">No pretraining</td>\n<td id=\"S5.T2.6.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">6.84/17.63</td>\n<td id=\"S5.T2.6.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">7.24/18.42</td>\n</tr>\n<tr id=\"S5.T2.6.4.4\" class=\"ltx_tr\">\n<td id=\"S5.T2.6.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_t\">C, LL-5k</td>\n<td id=\"S5.T2.6.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\">6.81/17.72</td>\n<td id=\"S5.T2.6.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\">7.3/18.2</td>\n</tr>\n<tr id=\"S5.T2.6.5.5\" class=\"ltx_tr\">\n<td id=\"S5.T2.6.5.5.1\" class=\"ltx_td ltx_align_center\">FL48,LL-5k</td>\n<td id=\"S5.T2.6.5.5.2\" class=\"ltx_td ltx_align_center\">6.59/<span id=\"S5.T2.6.5.5.2.1\" class=\"ltx_text ltx_font_bold\">17.02</span>\n</td>\n<td id=\"S5.T2.6.5.5.3\" class=\"ltx_td ltx_align_center\">\n<span id=\"S5.T2.6.5.5.3.1\" class=\"ltx_text ltx_font_bold\">6.80</span>/<span id=\"S5.T2.6.5.5.3.2\" class=\"ltx_text ltx_font_bold\">17.42</span>\n</td>\n</tr>\n<tr id=\"S5.T2.6.6.6\" class=\"ltx_tr\">\n<td id=\"S5.T2.6.6.6.1\" class=\"ltx_td ltx_align_center\">FL70,LL-5k</td>\n<td id=\"S5.T2.6.6.6.2\" class=\"ltx_td ltx_align_center\">\n<span id=\"S5.T2.6.6.6.2.1\" class=\"ltx_text ltx_font_bold\">6.43</span>/17.17</td>\n<td id=\"S5.T2.6.6.6.3\" class=\"ltx_td ltx_align_center\">6.83/17.88</td>\n</tr>\n<tr id=\"S5.T2.6.7.7\" class=\"ltx_tr\">\n<td id=\"S5.T2.6.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_t\">C, LL-52k</td>\n<td id=\"S5.T2.6.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_t\">5.79/<span id=\"S5.T2.6.7.7.2.1\" class=\"ltx_text ltx_font_bold\">16.29</span>\n</td>\n<td id=\"S5.T2.6.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_t\">6.07/<span id=\"S5.T2.6.7.7.3.1\" class=\"ltx_text ltx_font_bold\">16.21</span>\n</td>\n</tr>\n<tr id=\"S5.T2.6.8.8\" class=\"ltx_tr\">\n<td id=\"S5.T2.6.8.8.1\" class=\"ltx_td ltx_align_center\">FL48,LL-52k</td>\n<td id=\"S5.T2.6.8.8.2\" class=\"ltx_td ltx_align_center\">5.83/16.9</td>\n<td id=\"S5.T2.6.8.8.3\" class=\"ltx_td ltx_align_center\">6.16/16.34</td>\n</tr>\n<tr id=\"S5.T2.6.9.9\" class=\"ltx_tr\">\n<td id=\"S5.T2.6.9.9.1\" class=\"ltx_td ltx_align_center ltx_border_b\">FL70,LL-52k</td>\n<td id=\"S5.T2.6.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_b\">\n<span id=\"S5.T2.6.9.9.2.1\" class=\"ltx_text ltx_font_bold\">5.77</span>/16.4</td>\n<td id=\"S5.T2.6.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_b\">\n<span id=\"S5.T2.6.9.9.3.1\" class=\"ltx_text ltx_font_bold\">6.05</span>/<span id=\"S5.T2.6.9.9.3.2\" class=\"ltx_text ltx_font_bold\">16.21</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We show the efficacy of pre-trained (FL/central) audio representations. The RNN-T model is fine-tuned (with the pre-trained encoder) on the Librispeech 960960960 hour train data. Table 2 shows the WER of the fine-tuned models on the dev and test partitions.",
            "We compare the fine-tuned models to an RNN-T model trained without any pre-training, from-scratch, where the encoder is randomly initialized. We also study the impact of the amount of data on pre-training with LL-5k and LL-52k datasets.\nWe find that all the pre-trained models (FL/central) performed better than the from-scratch model. On average, the pre-trained models show a relative WER (WERR) improvement of 11.311.311.3% and 14.22%percent14.2214.22\\% on dev and test sets, respectively. We observe pre-training on the larger dataset (LL-52k, see Table 2) is better in both the cases of FL and central pre-training, similar to [4, 5, 6] where centrally pre-trained models show better ASR performance with more data. We can reassure the above observation for FL pre-training to produce audio representations."
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Results of the language adaptation experiments on train-215 hour MLS French data. We compare no pre-training, direct fine-tuning, and continued pre-raining followed by fine-tuning. Boldface stands for the best pre-trained models.",
        "table": "<table id=\"S5.T3.6\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T3.6.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.6.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_t\">\n<span id=\"S5.T3.6.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T3.6.1.1.1.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\"><span id=\"S5.T3.6.1.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Experiment</span></span>\n</span>\n</th>\n<th id=\"S5.T3.6.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<table id=\"S5.T3.6.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T3.6.1.1.2.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.6.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T3.6.1.1.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Source Pre-train</span></td>\n</tr>\n<tr id=\"S5.T3.6.1.1.2.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.6.1.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T3.6.1.1.2.1.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Setting</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S5.T3.6.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<table id=\"S5.T3.6.1.1.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T3.6.1.1.3.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.6.1.1.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T3.6.1.1.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Dev WER</span></td>\n</tr>\n<tr id=\"S5.T3.6.1.1.3.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.6.1.1.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T3.6.1.1.3.1.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(WERR)</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S5.T3.6.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<table id=\"S5.T3.6.1.1.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T3.6.1.1.4.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.6.1.1.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T3.6.1.1.4.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Test WER</span></td>\n</tr>\n<tr id=\"S5.T3.6.1.1.4.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.6.1.1.4.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T3.6.1.1.4.1.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">(WERR)</span></td>\n</tr>\n</table>\n</th>\n</tr>\n<tr id=\"S5.T3.6.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T3.6.2.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_t\">\n<span id=\"S5.T3.6.2.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T3.6.2.2.1.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\"><span id=\"S5.T3.6.2.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">No pre-training</span></span>\n</span>\n</th>\n<th id=\"S5.T3.6.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T3.6.2.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">-</span></th>\n<th id=\"S5.T3.6.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T3.6.2.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">46.31</span></th>\n<th id=\"S5.T3.6.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T3.6.2.2.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">42.73</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T3.6.3.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.6.3.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_t\" rowspan=\"3\">\n<span id=\"S5.T3.6.3.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T3.6.3.1.1.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\"><span id=\"S5.T3.6.3.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Direct fine-tuning</span></span>\n</span>\n</th>\n<td id=\"S5.T3.6.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.6.3.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">C, LL-52k</span></td>\n<td id=\"S5.T3.6.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.6.3.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">37.8(18.37)</span></td>\n<td id=\"S5.T3.6.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.6.3.1.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">34.6(19.02)</span></td>\n</tr>\n<tr id=\"S5.T3.6.4.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.6.4.2.1\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.6.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FL48,LL-52k</span></td>\n<td id=\"S5.T3.6.4.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.6.4.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">37.82(18.33)</span></td>\n<td id=\"S5.T3.6.4.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.6.4.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">34.25(19.84)</span></td>\n</tr>\n<tr id=\"S5.T3.6.5.3\" class=\"ltx_tr\">\n<td id=\"S5.T3.6.5.3.1\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.6.5.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FL70,LL-5k</span></td>\n<td id=\"S5.T3.6.5.3.2\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.6.5.3.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">43.27(6.56)</span></td>\n<td id=\"S5.T3.6.5.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.6.5.3.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">39.69(7.11)</span></td>\n</tr>\n<tr id=\"S5.T3.6.6.4\" class=\"ltx_tr\">\n<th id=\"S5.T3.6.6.4.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_b ltx_border_t\" rowspan=\"3\">\n<span id=\"S5.T3.6.6.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T3.6.6.4.1.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\"><span id=\"S5.T3.6.6.4.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Continued pre-training + fine-tuning</span></span>\n</span>\n</th>\n<td id=\"S5.T3.6.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.6.6.4.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">C, LL-52k</span></td>\n<td id=\"S5.T3.6.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.6.6.4.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">35.92(22.43)</span></td>\n<td id=\"S5.T3.6.6.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.6.6.4.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">32.6(23.70)</span></td>\n</tr>\n<tr id=\"S5.T3.6.7.5\" class=\"ltx_tr\">\n<td id=\"S5.T3.6.7.5.1\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.6.7.5.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FL48,LL-52k</span></td>\n<td id=\"S5.T3.6.7.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.6.7.5.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">37.63(18.74)</span></td>\n<td id=\"S5.T3.6.7.5.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.6.7.5.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">34.12(20.14)</span></td>\n</tr>\n<tr id=\"S5.T3.6.8.6\" class=\"ltx_tr\">\n<td id=\"S5.T3.6.8.6.1\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T3.6.8.6.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FL70,LL-5k</span></td>\n<td id=\"S5.T3.6.8.6.2\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T3.6.8.6.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">41.48(10.42)</span></td>\n<td id=\"S5.T3.6.8.6.3\" class=\"ltx_td ltx_align_center ltx_border_b\"><span id=\"S5.T3.6.8.6.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">37.1(13.17)</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Self-supervised representations are beneficial for tasks with a shortage of labeled data. The adaptation of both multilingual and monolingual pre-trained models [27, 28, 29] demonstrated remarkable adaptability to other languages in speech. We explore the effectiveness of the speech representations learned from our approach (FL+SSL) in adapting to French speech.\nWe use French data from Multilingual Librispeech [25], consists of 107010701070 hours of train data. The data is randomly split into two sets: 215215215 hours (train-215) and 855855855 hours (train-855).We conduct two sets of experiments: Direct fine-tuning and continued pre-training followed by fine-tuning. In direct fine-tuning, the pre-trained models from Libri-Light are directly fine-tuned using the train-215 data. In the other setting, we continue pre-training the LL pre-trained models using the train-815 unlabeled set and only then fine-tune them using the train-215 data. Table 3 shows the results on the MLS French dev and test sets."
        ]
    }
}