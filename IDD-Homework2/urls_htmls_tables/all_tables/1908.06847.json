{
    "PAPER'S NUMBER OF TABLES": 1,
    "S2.T1": {
        "caption": "TABLE I: Features, design goals and applications of Federated ML and other distributed approaches.",
        "table": "<table id=\"S2.T1.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S2.T1.3.1.1\" class=\"ltx_tr\">\n<th id=\"S2.T1.3.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span id=\"S2.T1.3.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.3.1.1.1.1.1\" class=\"ltx_p\" style=\"width:71.1pt;\"><span id=\"S2.T1.3.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Scheme</span></span>\n</span>\n</th>\n<th id=\"S2.T1.3.1.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span id=\"S2.T1.3.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.3.1.1.2.1.1\" class=\"ltx_p\" style=\"width:227.6pt;\">   <span id=\"S2.T1.3.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Salient features and design goals</span></span>\n</span>\n</th>\n<th id=\"S2.T1.3.1.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span id=\"S2.T1.3.1.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.3.1.1.3.1.1\" class=\"ltx_p\" style=\"width:130.9pt;\">    <span id=\"S2.T1.3.1.1.3.1.1.1\" class=\"ltx_text ltx_font_bold\">Example</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S2.T1.3.2.1\" class=\"ltx_tr\">\n<td id=\"S2.T1.3.2.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S2.T1.3.2.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.3.2.1.1.1.1\" class=\"ltx_p\" style=\"width:71.1pt;\"><span id=\"S2.T1.3.2.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Distributed learning</span></span>\n</span>\n</td>\n<td id=\"S2.T1.3.2.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S2.T1.3.2.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.3.2.1.2.1.1\" class=\"ltx_p\" style=\"width:227.6pt;\">\n<span id=\"S2.I2\" class=\"ltx_itemize\">\n<span id=\"S2.I2.i1\" class=\"ltx_item\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\">•</span> \n<span id=\"S2.I2.i1.p1\" class=\"ltx_para\">\n<span id=\"S2.I2.i1.p1.1\" class=\"ltx_p\">The goal is to provide a holistic estimation of the parameters under study</span>\n</span></span>\n<span id=\"S2.I2.i2\" class=\"ltx_item\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\">•</span> \n<span id=\"S2.I2.i2.p1\" class=\"ltx_para\">\n<span id=\"S2.I2.i2.p1.1\" class=\"ltx_p\">The global model is not fed back to the local learners</span>\n</span></span>\n</span></span>\n</span>\n</td>\n<td id=\"S2.T1.3.2.1.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S2.T1.3.2.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.3.2.1.3.1.1\" class=\"ltx_p\" style=\"width:130.9pt;\">\n<span id=\"S2.I3\" class=\"ltx_itemize\">\n<span id=\"S2.I3.i1\" class=\"ltx_item\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\">•</span> \n<span id=\"S2.I3.i1.p1\" class=\"ltx_para\">\n<span id=\"S2.I3.i1.p1.1\" class=\"ltx_p\">Distributed learning in WSN</span>\n</span></span>\n</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.3.3.2\" class=\"ltx_tr\">\n<td id=\"S2.T1.3.3.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S2.T1.3.3.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.3.3.2.1.1.1\" class=\"ltx_p\" style=\"width:71.1pt;\"><span id=\"S2.T1.3.3.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Parallel learning</span></span>\n</span>\n</td>\n<td id=\"S2.T1.3.3.2.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S2.T1.3.3.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.3.3.2.2.1.1\" class=\"ltx_p\" style=\"width:227.6pt;\">\n<span id=\"S2.I4\" class=\"ltx_itemize\">\n<span id=\"S2.I4.i1\" class=\"ltx_item\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\">•</span> \n<span id=\"S2.I4.i1.p1\" class=\"ltx_para\">\n<span id=\"S2.I4.i1.p1.1\" class=\"ltx_p\">The goal is to accelerate the learning process and scale up the algorithm</span>\n</span></span>\n<span id=\"S2.I4.i2\" class=\"ltx_item\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\">•</span> \n<span id=\"S2.I4.i2.p1\" class=\"ltx_para\">\n<span id=\"S2.I4.i2.p1.1\" class=\"ltx_p\">Data is distributed in a iid fashion</span>\n</span></span>\n<span id=\"S2.I4.i3\" class=\"ltx_item\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\">•</span> \n<span id=\"S2.I4.i3.p1\" class=\"ltx_para\">\n<span id=\"S2.I4.i3.p1.1\" class=\"ltx_p\">Data is not massively distributed among learners</span>\n</span></span>\n<span id=\"S2.I4.i4\" class=\"ltx_item\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\">•</span> \n<span id=\"S2.I4.i4.p1\" class=\"ltx_para\">\n<span id=\"S2.I4.i4.p1.1\" class=\"ltx_p\">There is no communication constraint consideration</span>\n</span></span>\n</span></span>\n</span>\n</td>\n<td id=\"S2.T1.3.3.2.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S2.T1.3.3.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.3.3.2.3.1.1\" class=\"ltx_p\" style=\"width:130.9pt;\">\n<span id=\"S2.I5\" class=\"ltx_itemize\">\n<span id=\"S2.I5.i1\" class=\"ltx_item\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\">•</span> \n<span id=\"S2.I5.i1.p1\" class=\"ltx_para\">\n<span id=\"S2.I5.i1.p1.1\" class=\"ltx_p\">Distributed learning in datacenters environment</span>\n</span></span>\n</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.3.4.3\" class=\"ltx_tr\">\n<td id=\"S2.T1.3.4.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S2.T1.3.4.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.3.4.3.1.1.1\" class=\"ltx_p\" style=\"width:71.1pt;\"><span id=\"S2.T1.3.4.3.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Ensemble learning</span></span>\n</span>\n</td>\n<td id=\"S2.T1.3.4.3.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S2.T1.3.4.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.3.4.3.2.1.1\" class=\"ltx_p\" style=\"width:227.6pt;\">\n<span id=\"S2.I6\" class=\"ltx_itemize\">\n<span id=\"S2.I6.i1\" class=\"ltx_item\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\">•</span> \n<span id=\"S2.I6.i1.p1\" class=\"ltx_para\">\n<span id=\"S2.I6.i1.p1.1\" class=\"ltx_p\">The goal is to produce an optimal model by learning from a mixture of several types of the models</span>\n</span></span>\n<span id=\"S2.I6.i2\" class=\"ltx_item\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\">•</span> \n<span id=\"S2.I6.i2.p1\" class=\"ltx_para\">\n<span id=\"S2.I6.i2.p1.1\" class=\"ltx_p\">Data is distributed in a iid fashion</span>\n</span></span>\n<span id=\"S2.I6.i3\" class=\"ltx_item\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\">•</span> \n<span id=\"S2.I6.i3.p1\" class=\"ltx_para\">\n<span id=\"S2.I6.i3.p1.1\" class=\"ltx_p\">There is no communication constraint consideration</span>\n</span></span>\n</span></span>\n</span>\n</td>\n<td id=\"S2.T1.3.4.3.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span id=\"S2.T1.3.4.3.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.3.4.3.3.1.1\" class=\"ltx_p\" style=\"width:130.9pt;\">\n<span id=\"S2.I7\" class=\"ltx_itemize\">\n<span id=\"S2.I7.i1\" class=\"ltx_item\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\">•</span> \n<span id=\"S2.I7.i1.p1\" class=\"ltx_para\">\n<span id=\"S2.I7.i1.p1.1\" class=\"ltx_p\">Bagging, boosting and stacking algorithms that can be used in remote sensing, face recognition and so on.</span>\n</span></span>\n</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.3.5.4\" class=\"ltx_tr\">\n<td id=\"S2.T1.3.5.4.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\">\n<span id=\"S2.T1.3.5.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.3.5.4.1.1.1\" class=\"ltx_p\" style=\"width:71.1pt;\"><span id=\"S2.T1.3.5.4.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Federated learning</span></span>\n</span>\n</td>\n<td id=\"S2.T1.3.5.4.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\">\n<span id=\"S2.T1.3.5.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.3.5.4.2.1.1\" class=\"ltx_p\" style=\"width:227.6pt;\">\n<span id=\"S2.I8\" class=\"ltx_itemize\">\n<span id=\"S2.I8.i1\" class=\"ltx_item\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\">•</span> \n<span id=\"S2.I8.i1.p1\" class=\"ltx_para\">\n<span id=\"S2.I8.i1.p1.1\" class=\"ltx_p\">The goal is to perform the model training using the naturally distributed datasets over several learners</span>\n</span></span>\n<span id=\"S2.I8.i2\" class=\"ltx_item\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\">•</span> \n<span id=\"S2.I8.i2.p1\" class=\"ltx_para\">\n<span id=\"S2.I8.i2.p1.1\" class=\"ltx_p\">The global model is fed back to the local learners for their use</span>\n</span></span>\n<span id=\"S2.I8.i3\" class=\"ltx_item\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\">•</span> \n<span id=\"S2.I8.i3.p1\" class=\"ltx_para\">\n<span id=\"S2.I8.i3.p1.1\" class=\"ltx_p\">Data is distributed in non-iid fashion</span>\n</span></span>\n<span id=\"S2.I8.i4\" class=\"ltx_item\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\">•</span> \n<span id=\"S2.I8.i4.p1\" class=\"ltx_para\">\n<span id=\"S2.I8.i4.p1.1\" class=\"ltx_p\">Data is massively distributed over local learners</span>\n</span></span>\n<span id=\"S2.I8.i5\" class=\"ltx_item\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\">•</span> \n<span id=\"S2.I8.i5.p1\" class=\"ltx_para\">\n<span id=\"S2.I8.i5.p1.1\" class=\"ltx_p\">There are communication constraints such as privacy, security, power and bandwidth limitations in accessing the data</span>\n</span></span>\n</span></span>\n</span>\n</td>\n<td id=\"S2.T1.3.5.4.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\">\n<span id=\"S2.T1.3.5.4.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.3.5.4.3.1.1\" class=\"ltx_p\" style=\"width:130.9pt;\">\n<span id=\"S2.I9\" class=\"ltx_itemize\">\n<span id=\"S2.I9.i1\" class=\"ltx_item\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\">•</span> \n<span id=\"S2.I9.i1.p1\" class=\"ltx_para\">\n<span id=\"S2.I9.i1.p1.1\" class=\"ltx_p\">Edge computing and caching</span>\n</span></span>\n<span id=\"S2.I9.i2\" class=\"ltx_item\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\">•</span> \n<span id=\"S2.I9.i2.p1\" class=\"ltx_para\">\n<span id=\"S2.I9.i2.p1.1\" class=\"ltx_p\">Autonomous driving</span>\n</span></span>\n<span id=\"S2.I9.i3\" class=\"ltx_item\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\">•</span> \n<span id=\"S2.I9.i3.p1\" class=\"ltx_para\">\n<span id=\"S2.I9.i3.p1.1\" class=\"ltx_p\">Federated ML for spectrum management</span>\n</span></span>\n<span id=\"S2.I9.i4\" class=\"ltx_item\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\">•</span> \n<span id=\"S2.I9.i4.p1\" class=\"ltx_para\">\n<span id=\"S2.I9.i4.p1.1\" class=\"ltx_p\">Coexistence  of  heterogeneous  systems (For example, DSRC and c-V2X)</span>\n</span></span>\n<span id=\"S2.I9.i5\" class=\"ltx_item\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\">•</span> \n<span id=\"S2.I9.i5.p1\" class=\"ltx_para\">\n<span id=\"S2.I9.i5.p1.1\" class=\"ltx_p\">Federated ML in 5G core network</span>\n</span></span>\n</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Recently introduced by Google, federated learning is a decentralized learning approach where training is performed over a ",
                "federation",
                " of distributed learners. It is essential to distinguish the decentralized inference approaches with centralized training from the concept of federated ML where ",
                "decentralized training",
                " is performed for decentralized inference. The objective of this approach is to keep the training dataset where it is generated and perform the model training locally at each individual learner in the federation. After training a local model, each individual learner transfers its local model parameters, instead of raw training dataset, to an aggregating unit. The aggregator utilizes the local model parameters to update",
                "2",
                "2",
                "2",
                "The aggregation of the local model parameters can be accomplished either synchronously or asynchronously. Readers can refer to ",
                "[",
                "5",
                "]",
                " for more details.",
                " a global model which is eventually fed back to the individual local learners for their use. As a result, each local learner benefits from the datasets of the other learners only through the global model, shared by the aggregator, without explicitly accessing their privacy-sensitive data. While this scheme is inherently more privacy-preserving than sharing raw data, some models may still reveal information about the underlying data because of which local learners add an additional layer of protection by transferring encrypted versions of their models to the aggregator. A secure aggregation algorithm as a class of secure multi-party computation is used to aggregate the encrypted local models without the need for decrypting the models ",
                "[",
                "7",
                "]",
                ". An illustration of the federated learning concept is provided in Fig. ",
                "1",
                ".",
                "Several key aspects of federated learning differentiate it from the existing distributed learning schemes. One of the common assumptions of such learning schemes is that the data samples of learners are realizations of independent and identically distributed (iid) random variables. However, in the federated ML setting, different learners may be observing separate parts of the process (with possible overlaps between them), thus generating datasets that may not be representative of the distribution of the entire data. Therefore, federated learning deals with ",
                "non-iid",
                " datasets of the locally-trained learners.\nAs an example, one can consider the task of building a high definition (HD) map for autonomous driving, where the autonomous vehicles only collect the location and sensing information related to the routes they traverse; or in the task of hand-written digits recognition where local learners have samples of different digits. Second, the datasets are ",
                "unbalanced",
                " in size. For instance, in the HD map example, the dataset collected at different autonomous vehicles may vary in size due to different environment they pass through. Last, the datasets are ",
                "massively distributed",
                " among the local learners, where the number of data samples per local learner is smaller than the total number of learners participating in the training.\nThese salient features of the dataset, i.e. non-iid, distributed and unbalanced training data, differentiates the federated ML framework from the other related approaches, which are discussed below.",
                "Distributed learning",
                " schemes are the ones in which the aggregator organizes the locally collected data (usually in the form of locally trained models due to the stringent communication limitations) to provide a holistic and more accurate estimation of the parameters under study. In this form of learning, the local learners act solely as local data collectors and do not require the global model through any feedback from the aggregator. Distributed learning in wireless sensor network (WSN) for monitoring belongs to this category of learning. For instance, in temperature monitoring WSN, each sensor in the network communicates the local model trained by its dataset to the fusion center. The fusion center aggregates the local information to construct a global estimate of the temperature of the field.",
                "Parallel learning",
                "3",
                "3",
                "3",
                "In the ML community, it is often called distributed ML. However, we decided to use the term ",
                "parallel learning",
                ", owing to its objective which is ",
                "parallelizing",
                " the gradient computation and aggregation across multiple worker nodes, to distinguish this type of learning from the distributed learning that we previously discussed in the context of WSN networks.",
                " refers to the learning schemes whose main objective is to scale up the algorithm or accelerate the learning process or both. In this type of learning, the available training set at a central parameter server is divided into subsets of data and assigned to a group of worker machines. Therefore, the datasets assigned to each worker machine have the same underlying distribution. Subsequently, the training process is performed in parallel and the parameters are fed back to the parameter server. In this setting, model parallelism",
                "4",
                "4",
                "4",
                "In model parallelism, the entire dataset is assigned to all worker machines. However, each machine is responsible for estimating certain model parameters.",
                " is another way of distributing the workload compared to the data parallelism. This type of learning is performed in datacenters where the worker machines obtain data from a shared storage and hence, unlike federated learning, they will end up having samples from the same distribution. In addition, the average number of data samples per worker is way larger than the number of worker machines participating in the training process which is different from the federated setting where the data is massively distributed.",
                "Distributed ensemble learning",
                ", also known as committee-based learning, is a learning approach in which multiple learners (such as classifiers and regressors) are combined to improve the overall performance. In this scheme, portions of the dataset are assigned to train different models. These models are then aggregated to reduce the likelihood of choosing an insufficient one. In general, the goal of such learning methods is to learn from a mixture of experts (models) rather than improving a global model using a naturally distributed dataset through a federation of local learners with communication constraints."
            ]
        ]
    }
}