{
    "PAPER'S NUMBER OF TABLES": 1,
    "S5.T1": {
        "caption": "TABLE I: Graph Data Statistics",
        "table": "<table id=\"S5.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Dataset</th>\n<th id=\"S5.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Nodes</th>\n<th id=\"S5.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Edges</th>\n<th id=\"S5.T1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Features</th>\n<th id=\"S5.T1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Classes</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Cora</td>\n<td id=\"S5.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">2,708</td>\n<td id=\"S5.T1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">10,556</td>\n<td id=\"S5.T1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">1,433</td>\n<td id=\"S5.T1.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">7</td>\n</tr>\n<tr id=\"S5.T1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Citeseer</td>\n<td id=\"S5.T1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">3,327</td>\n<td id=\"S5.T1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">9,228</td>\n<td id=\"S5.T1.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">3,703</td>\n<td id=\"S5.T1.1.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">6</td>\n</tr>\n<tr id=\"S5.T1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">PubMed</td>\n<td id=\"S5.T1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">19,717</td>\n<td id=\"S5.T1.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">88,651</td>\n<td id=\"S5.T1.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">500</td>\n<td id=\"S5.T1.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\">3</td>\n</tr>\n<tr id=\"S5.T1.1.5.4\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">Reddit</td>\n<td id=\"S5.T1.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">232,965</td>\n<td id=\"S5.T1.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">114,848,857</td>\n<td id=\"S5.T1.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">602</td>\n<td id=\"S5.T1.1.5.4.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">41</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We implement FedGraph using PyTorch and Deep Graph Library (DGL) [32], a Python package dedicated to deep learning on graphs.\nWe deploy FedGraph on 20 computing clients with Intel i7-10700 CPU, 32GB memory, and Geforce RTX 2080 GPU. We consider 4 popular graph datasets: Cora, Citeseer, PubMed, and Reddit, which have been widely used for GCN studies [21, 20, 27, 26, 13, 22]. Some statistic information of these datasets is summarized in Table I. Since some graphs, e.g., Cora and Citeseer, are with limited sizes, we synthesize large graphs based on these datasets using the following method. Given a dataset in Table I, each client i𝑖i randomly selects a proportion ξisubscript𝜉𝑖\\xi_{i} of nodes as its local graph data, and {ξ1,ξ2,…,ξ|C|}subscript𝜉1subscript𝜉2…subscript𝜉𝐶\\{\\xi_{1},\\xi_{2},...,\\xi_{|C|}\\} belongs to a normal distribution with a mean of 0.8.\nIt is possible that generated local graphs overlap on some nodes, especially for small graph datasets, like Cora and Citeseer. For large graphs, we carefully control the local graph generation to avoid overlapping. Even some nodes overlap in the synthesized datasets, we treat them as different nodes and there is no influence to training performance. A similar graph synthesis method has been adopted by [33]. For the local dataset, we randomly choose a set of nodes to generate a training set, a validation set, and a test set.\nThe edge connections across clients are maintained according to the original graph. For local graph learning, each client constructs a 3-layer GCN, including an input layer and two convolutional layers. We set 16 hidden units, 50%percent\\% dropout rate, 0.01 learning rate for Cora, Citeseer, and PubMed. For Reddit, there are 128 hidden units, the dropout rate is 20%, and the learning rate is 0.0001. We set the batch size as 256 for Cora, Citeseer, and Reddit, 1024 for PubMed [21]. We use ADAM optimizer for local GCN training.\nFor the reward function (9), we set the base of exponential function, i.e., ΩΩ\\Omega, as 128 in our experiments. Since FedGraph relies on the exponential property of reward function, the base has little influence on FedGraph. Moreover, the difference of training accuracy λ​[t]𝜆delimited-[]𝑡\\lambda[t] and target accuracy ΛΛ\\Lambda affects the reward in each round t𝑡t. For each dataset, we choose the best accuracy reported by existing work. Even we have no knowledge of the best accuracy, we can make an estimation according to experiences. Since FedGraph only relies on the exponential property of reward function, such estimation has little influence to FedGraph. Both constants α𝛼\\alpha and β𝛽\\beta aim to balance accuracy improvement and time cost. In our experiments, we control the time penalty α​(δ​[t]−β)𝛼𝛿delimited-[]𝑡𝛽\\alpha(\\delta[t]-\\beta) close to 1, similar to the settings in [25].\nFor comparison, we extend the following three graph sampling schemes for federated graph learning.",
            "Effect of cross-client embedding sharing. FedGraph uses the cross-client graph convolution operation to enable embedding sharing between clients while hiding local features during local GCN training. For comparison, we consider two alternative methods, one (referred to as FedGraph_allShare ) is to share embeddings from the first layer to maximize the information sharing, and the other (referred to as FedGraph_nonShare) is to discard cross-client sharing to simplify the design. We show the accuracy convergence of these three designs in Fig. 9. The total number of training rounds is set to 300. We can find that the curve of FedGraph is close to that of FedGraph_allShare, which demonstrates that FedGraph has little information loss even though it eliminates the embedding sharing in the first layer. It is because that the high-layer embedding contains information about the original features. Hence, FedGraph can efficiently learn from cross-clients embedding sharing without the original feature exchanging. Simultaneously, FedGraph significantly outperforms FedGraph_nonShare under all datasets. In Cora and Citeseer, cross-client convolution operations can increase training accuracy by about 10%. In PubMed, two designs have similar final accuracy, but FedGraph enables quick convergence. Reddit is more sensitive to cross-client embedding sharing than other datasets, and FedGraph_nonShare converges to an accuracy of about 70%, while FedGraph can converge to about 90%. That is because Reddit has rich edge connections as shown in Table I, and ignoring cross-client edges would seriously break the whole graph structure. Note that FedGraph_nonShare completes 300-round training earlier because it eliminates embedding sharing."
        ]
    }
}