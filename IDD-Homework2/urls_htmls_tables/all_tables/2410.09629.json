{
    "id_table_1": {
        "caption": "Table 1:  Retrieval performance on three datasets using two different retrievers (Contriever and BM25).",
        "table": "S3.E4",
        "footnotes": [],
        "references": [
            "Inspired by this gap, we introduce  S ynthetic  k nowledge  i ngestion ( Ski ), an innovative synthetic data generation method that automates and enhances knowledge ingestion, as illustrated in Fig. 1 .  Ski  leverages three key innovations to generate high-quality and diverse data representation from a raw knowledge base. First,  Fine-grained Synthesis  creates hypothetical questions based on  n n n italic_n -gram knowledge contexts, ensuring a detailed match in relevance, minimizing the semantic gap between questions and answers, and increasing representation diversity. Second,  Interleaved Generation  simultaneously generates both questions and answers based on specific knowledge. This synthetic question-answering (QA) format naturally mirrors the process of information-seeking, providing direct contextual alignment and relevance between the questions and their respective answers. Third,  Assemble Augmentation  combines fine-grained synthesis across different  n n n italic_n -gram spans and their QA pair iterations, balancing repetition with diverse elements. By integrating these components, the  Ski  approach significantly enhances the refinement of knowledge from its raw state, thus facilitating effective knowledge ingestion into LLMs.",
            "We evaluate the retrieval performance of  Ski  using two different retrievers, as illustrated in Table  1 . For Contriever,  Ski-QC-1  outperformed both the inverse HyDE ( Ski-Q-1 ) and the raw article baselines, particularly achieving a significant improvement (+15%) on the FiQA task. For BM25,  Ski-QC-ASM  surpassed all the baselines and  Ski  variations across three datasets. Further evaluations were conducted on our method within RAG systems, detailed in Table  2 . Inverse HyDE fell short of the raw article baseline, indicating that the question-to-question search approach might not be as effective as anticipated.  Ski-QC-ASM  emerged as the superior method, showing consistent improvements compared to other baselines across all tasks. The results of BM25 are aligned with the retrieval-only task, confirming the effectiveness of assemble augmentation.",
            "To gauge the impact of N-Gram on retrieval strategy, we compared the retrieval performance on FiQA task using the  Ski-Q-n  strategy. Results can be observed in Table  11 . 1-Gram strategy, despite the fact that it generates the most amount of questions to compare with, leads to the best retrieval result.",
            "To better understand the effectiveness of Query Augmentation and Document Augmentation on retrieval, we compare  Ski  with HyDE  Gao et al. ( 2022b )  using the FiQA dataset and Contriver. Full results can be seen in Table  12 . For the  Ski  variations, both  Ski-Q-n  and  Ski-QC-n  outperform HyDE  Gao et al. ( 2022b ) ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  End-to-end RAG results with two retrievers.",
        "table": "S3.E5",
        "footnotes": [],
        "references": [
            "The human learning process often involves asking questions and inquiring why, which can enhance comprehension. Drawing inspiration from this,  Ski  also leverages the power of questions. By utilizing three key innovations,  Ski  transforms raw knowledge (such as documents and articles) into question-augmented representations. The following section provides a detailed description of these three key innovations, as illustrated in Figure  2 .",
            "where  n n n italic_n  typically ranges from 1 to 3. Specifically,  Ski-Q-1  refers synthetic 1-gram questions over  m m m italic_m  sentences,  Q ~ 1 = { q 1 1 , q 2 1 , ... , q m 1 } superscript ~ Q 1 superscript subscript q 1 1 superscript subscript q 2 1 ... superscript subscript q m 1 \\tilde{Q}^{1}=\\{q_{1}^{1},q_{2}^{1},...,q_{m}^{1}\\} over~ start_ARG italic_Q end_ARG start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT = { italic_q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , italic_q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , ... , italic_q start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT } , and  Ski-QC-1  means synthetic 1-gram question-context (QC) pair  { Q ~ 1 , C 1 } = { { q 1 1 , k 1 } , ... , { q m 1 , k m } } superscript ~ Q 1 superscript C 1 superscript subscript q 1 1 subscript k 1 ... superscript subscript q m 1 subscript k m \\{\\tilde{Q}^{1},C^{1}\\}=\\{\\{q_{1}^{1},k_{1}\\},...,\\{q_{m}^{1},k_{m}\\}\\} { over~ start_ARG italic_Q end_ARG start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , italic_C start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT } = { { italic_q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT } , ... , { italic_q start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , italic_k start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT } }  that includes  m m m italic_m  pairs in this set. More details and examples can be found in the Appendix  A.2 .",
            "We evaluate the retrieval performance of  Ski  using two different retrievers, as illustrated in Table  1 . For Contriever,  Ski-QC-1  outperformed both the inverse HyDE ( Ski-Q-1 ) and the raw article baselines, particularly achieving a significant improvement (+15%) on the FiQA task. For BM25,  Ski-QC-ASM  surpassed all the baselines and  Ski  variations across three datasets. Further evaluations were conducted on our method within RAG systems, detailed in Table  2 . Inverse HyDE fell short of the raw article baseline, indicating that the question-to-question search approach might not be as effective as anticipated.  Ski-QC-ASM  emerged as the superior method, showing consistent improvements compared to other baselines across all tasks. The results of BM25 are aligned with the retrieval-only task, confirming the effectiveness of assemble augmentation.",
            "To better understand the effectiveness of Query Augmentation and Document Augmentation on retrieval, we compare  Ski  with HyDE  Gao et al. ( 2022b )  using the FiQA dataset and Contriver. Full results can be seen in Table  12 . For the  Ski  variations, both  Ski-Q-n  and  Ski-QC-n  outperform HyDE  Gao et al. ( 2022b ) ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  SFT performance on two pre-trained models.",
        "table": "S3.E8",
        "footnotes": [],
        "references": [
            "We evaluate our approach on multiple cross-domain question-answering tasks, including those in the finance domain (e.g., FiQA  Maia et al. ( 2018 ) ), biomedical domain (e.g., BioASQ  Tsatsaronis et al. ( 2015 ) ), open-generation domain (e.g., NQ  Kwiatkowski et al. ( 2019 )  and multi-hop domain (e.g., HotpotQA  Yang et al. ( 2018 ) ). Please find more relevant details about data usage for RAG, SFT, and CPT in the Appendix  A.3 - A.4 .",
            "Fig. 3  presents a detailed evaluation of how different retrievers and generators influence our  Ski  methods. In the NQ and HotpotQA tasks,  Ski  methods that incorporate Contriever generally outperform those using BM25. In terms of generation,  GPT-3.5-turbo  demonstrates superior performance over  Llama2-7B  and  Mistral-7B . For the BioASQ task, both Contriever and BM25 perform comparably, with  Llama2-7B  surpassing the other two generators in effectiveness. We compare three variations of  Ski , namely  Ski-Q-1 ,  Ski-QC-1 , and  Ski-QC-ASM  , across different combinations of retrievers and generators. Consistently across tasks,  Ski-QC-ASM  tends to perform the best, followed by  Ski-QC-1 , outperforming the baseline  Ski-Q-1 ."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  CPT performance on  Llama2-7B .",
        "table": "S3.T1.1.1",
        "footnotes": [],
        "references": [
            "We evaluate our approach on multiple cross-domain question-answering tasks, including those in the finance domain (e.g., FiQA  Maia et al. ( 2018 ) ), biomedical domain (e.g., BioASQ  Tsatsaronis et al. ( 2015 ) ), open-generation domain (e.g., NQ  Kwiatkowski et al. ( 2019 )  and multi-hop domain (e.g., HotpotQA  Yang et al. ( 2018 ) ). Please find more relevant details about data usage for RAG, SFT, and CPT in the Appendix  A.3 - A.4 .",
            "The impact of  n n n italic_n -gram on SFT is illustrated in Fig.  4 . Across various tasks, the 1-gram consistently outperforms the 2-gram and 3-gram in all  Ski  variations, underscoring the significance of fine-grained generation for enhancing both the quantity and quality of data synthesis. Compared to QA pairs and QCA pairs, the performance of QC pairs drops slightly and is inferior to the other two variations. This suggests that the interleaved generation in QA pairs is crucial for improving SFT performance."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Prompt to generate Questions for  Ski-QC-n",
        "table": "S5.T2.1.1",
        "footnotes": [],
        "references": [
            "Fig. 5  presents a comprehensive comparison of the  Ski  method across various knowledge injection pipelines. While RAG demonstrates marked improvement over the raw model and baselines, the gains from CPT are relatively modest. Despite this,  Llama2-7B  consistently shows significant improvements over  Mistral-7B  across RAG, SFT, and CPT.  GPT-3.5-turbo  is utilized solely as the generator for RAG, where it exhibits relatively low-performance gains due to its already high base performance. Our  Ski  method appears promising in enhancing the RAG performance of open-source pre-trained models, suggesting that  Llama2-7B  could be an effective candidate for knowledge injection, potentially achieving competitive gains similar to RAG when applied through SFT and CPT."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Prompt to generate Questions And Answers for  Ski-QCA-n",
        "table": "S5.T3.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_7": {
        "caption": "Table 7:  Synthetic Data Example",
        "table": "S5.T4.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_8": {
        "caption": "Table 8:  SFT/CPT Training Dataset Size.",
        "table": "A1.T5.3",
        "footnotes": [],
        "references": [
            "For the SFT Pipeline, we use the QLoRA  Dettmers et al. ( 2024 )  strategy with 4bit quantization built on top of Llama-Factory  Zheng et al. ( 2024 ) 5 5 5 https://github.com/hiyouga/LLaMA-Factory . For float format we use bf16. The batch size is set to 2, and the number of update steps to accumulate the gradients is set to 4. We use a cosine scheduler with a learning rate of 5e-5 and a warm-up ratio of 0.1. The training lasts for 3 rounds, with the maximum gradient norm set to 1. We use Amazon Sagemaker g5.12xlarge and g5.24xlarge instances for training, which are powered by NVIDIA A10 Tensor Core GPUs and comprise 4 GPUs with 24 GB of memory each. For training data size please refer to Table  8 .",
            "For each dataset, we select the first 200 queries in the test set as the final test set. Considering the huge amount of documents for retrieval, we select the articles that have been referred to at least one test query as the ground truth of the retrieval task to be the knowledge base, see Table  8 ."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  CPT Training Data Example",
        "table": "A1.T6.3",
        "footnotes": [],
        "references": []
    },
    "id_table_10": {
        "caption": "Table 10:  CPT Test Data Example",
        "table": "A1.T7.1",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "Table 11:  Effective of N-gram for Retrieval",
        "table": "A1.T8.1",
        "footnotes": [
            "",
            "",
            ""
        ],
        "references": [
            "To gauge the impact of N-Gram on retrieval strategy, we compared the retrieval performance on FiQA task using the  Ski-Q-n  strategy. Results can be observed in Table  11 . 1-Gram strategy, despite the fact that it generates the most amount of questions to compare with, leads to the best retrieval result."
        ]
    },
    "id_table_12": {
        "caption": "Table 12:  Effective of Query Augmentation vs Document Augmentation",
        "table": "A1.T9.1",
        "footnotes": [],
        "references": [
            "To better understand the effectiveness of Query Augmentation and Document Augmentation on retrieval, we compare  Ski  with HyDE  Gao et al. ( 2022b )  using the FiQA dataset and Contriver. Full results can be seen in Table  12 . For the  Ski  variations, both  Ski-Q-n  and  Ski-QC-n  outperform HyDE  Gao et al. ( 2022b ) ."
        ]
    },
    "id_table_13": {
        "caption": "",
        "table": "A1.T10.1",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "A1.T11.1",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "",
        "table": "A1.T12.1",
        "footnotes": [
            ""
        ],
        "references": []
    }
}