{
    "id_table_1": {
        "caption": "Table 1 :  Performance comparison of GEC across three different ASR benchmarks from three different domains. We evaluate and compare across two scenarios: (i) Matched Scenario: In this case, the hypotheses-transcription pairs for training our GEC model are derived from the Train split of the Test dataset (and not from the dataset the ASR model is trained on) (ii) Mismatched Scenario: In this case, the hypotheses-transcription pairs are derived from the same dataset the ASR model is trained on. We show that (a) For domain shifts, i.e., in cases where both the hypotheses and the ASR training dataset are from a domain different from the test, GEC leads to little to no improvement, and (b) For in-domain scenarios where only the hypotheses are derived from the same domain as the test, employing an ASR model trained on a different domain to derive the hypothesis boosts performance.",
        "table": "S3.T1.2.2",
        "footnotes": [],
        "references": [
            "Automatic Speech Recognition (ASR) is the foundational task of converting spoken language into text. As a fundamental goal in computational language processing  Jurafsky   ( 2000 ) , ASR has facilitated communication across diverse fields, including education  Caballero et al.   ( 2017 ) , healthcare  Latif et al.   ( 2020 ) , and others  den Bogaert et al.   ( 2022 ) . Advances in deep learning have driven significant progress in ASR, with end-to-end models achieving impressive results on various tasks  Li et al.   ( 2022 ) . However, one of the key challenges in real-world ASR applications   Li et al.   ( 2015 )  is handling variations in speech due to factors like background noise  Chen et al.   ( 2022 ) , speaker accents  Turan et al.   ( 2022 ) , and different speaking styles  Syed et al.   ( 2021 ) . These factors lead to a significant reduction in the accuracy of ASR.",
            "Humans demonstrate exceptional resilience to challenging speech conditions due to our inherent linguistic knowledge. Traditional ASR systems mimic this by incorporating a separate language model (LM) to rescore hypotheses during decoding  Toshniwal et al.   ( 2018 ) ;  Kannan et al.   ( 2018 ) . The LM evaluates the fluency of the N-best hypotheses generated by the ASR model, and the scores are combined with the ASRs own scores in a weighted fashion. The hypothesis with the highest combined score is then selected as the final transcript. However, the rise of large language models (LLMs) with advanced reasoning capabilities has opened possibilities beyond simple rescoring. This has led to the development of Generative Error Correction (GEC)  Chen et al.   ( 2024 ) , where models are trained to correct errors in the best hypothesis by leveraging information from other hypotheses, ultimately improving transcription accuracy.",
            "Conventional Generative Error Correction (GEC) models are typically trained by pooling hypothesis-transcription pairs from various ASR systems and datasets, with the expectation that they will generalize well across diverse data at test time  Chen et al.   ( 2024 ) ;  Hu et al.   ( 2024a ) ;  Ghosh et al.   ( 2024b ) . However, we identify key limitations in this approach. Previous work has primarily focused on foundational or semi-open-source models (e.g., Whisper  Radford et al.   ( 2023 ) ). To explore these limitations, we conducted several single-domain, single-dataset experiments (see Table  1 ), training GEC models on the same datasets used to train the ASR models. We observed only minor improvements in Word Error Rate (WER) on in-domain tests and no improvements on out-of-domain (OOD) tests. Upon closer examination, we attribute these shortcomings to three main factors:",
            "Domain Generalization and Named Entity in ASR. Transcribing NEs is a persistent challenge for ASR models  Das et al.   ( 2022 ) . Techniques such as memorization  Bekal et al.   ( 2021 )  and biasing  Jayanthi et al.   ( 2023 )  have been developed to improve NE transcription. However, these methods typically focus on known NEs seen during training and struggle with unseen entities, as autoregressive models tend to memorize NEs but generalize poorly to new ones  Heinzerling and Inui   ( 2020 ) . Improving NE transcription using post-ASR processing or GEC has not been well explored. ASR models often fail under distribution shifts, such as domain, accent, or dialect changes  Singhal et al.   ( 2023 ) . However, the robustness of GEC to domain shifts remains underexplored.",
            "Most prior work on Generative Error Correction (GEC) relies on foundational open-access ASR models, like Whisper, to generate hypotheses from various datasets and then trains GEC models on these hypotheses-transcription pairs, denoted as  H train id subscript superscript H id train \\mathcal{H}^{\\text{id}}_{\\text{train}} caligraphic_H start_POSTSUPERSCRIPT id end_POSTSUPERSCRIPT start_POSTSUBSCRIPT train end_POSTSUBSCRIPT . However, because the training data used for such ASR models is often undisclosed, there is limited insight into the nature of errors present in the hypotheses and, consequently, the types of errors that the GEC models learn to correct. In this work, we aim to study error correction from a more transparent perspective. Table  1  presents experiments where we train an ASR model on a single dataset (LibriSpeech (LS)  Panayotov et al.   ( 2015 ) , VoxPopuli  Wang et al.   ( 2021 )  (Vox), SPGIspeech  ONeill et al.   ( 2021 ) ), then derive hypotheses from either the same or a different dataset, and use these pairs to train a GEC model. Our key findings are as follows: (i) When GEC models are trained on a dataset in a different domain (i.e., both  D train id subscript superscript D id train \\mathcal{D}^{\\text{id}}_{\\text{train}} caligraphic_D start_POSTSUPERSCRIPT id end_POSTSUPERSCRIPT start_POSTSUBSCRIPT train end_POSTSUBSCRIPT  and  H train id subscript superscript H id train \\mathcal{H}^{\\text{id}}_{\\text{train}} caligraphic_H start_POSTSUPERSCRIPT id end_POSTSUPERSCRIPT start_POSTSUBSCRIPT train end_POSTSUBSCRIPT  come from a domain that is different from  D test id subscript superscript D id test \\mathcal{D}^{\\text{id}}_{\\text{test}} caligraphic_D start_POSTSUPERSCRIPT id end_POSTSUPERSCRIPT start_POSTSUBSCRIPT test end_POSTSUBSCRIPT ), no performance improvements are observed. We hypothesize this is due to the GEC model encountering errors at test time that differ significantly from those it saw during training. For instance, a hypotheses (HP)-transcription (GT) pair generated from the LibriSpeech train set using an ASR model trained on LibriSpeech is as follows:",
            "Step 2. During GEC training and inference, we use SentenceBERT  Reimers   ( 2019 )  to retrieve the top- k  NEs,  s     s \\overline{s} over  start_ARG italic_s end_ARG , from  D  S D S \\mathcal{DS} caligraphic_D caligraphic_S  based on their similarity to the best hypothesis. This is formally defined as:",
            "The ground-truth transcription serves as the target for fine-tuning. Following prior work, we fine-tune only LoRA adapters  Hu et al.   ( 2021 ) .",
            "Datasets. We evaluate DARAG on 5 benchmark ASR datasets, including LibriSpeech-960 (LS), SPGISpeech (SPGI), VoxPopuli en (Vox), Gigaspeech  Chen et al.   ( 2021 )  (Giga) and TED-LIUM  Rousseau et al.   ( 2012 )  (TED).  We acknowledge that for OOD evaluation, prior works use different and varied settings. However, we want to emphasize that OOD adaptation or evaluation is not our main focus; rather, only to show DARAG improves performance in typical OOD settings.",
            "Comparison Methods and Ablations. For comparison with DARAG, we employ (i) Baseline  Only ASR, and we perform no post-processing. (ii) Synth. Adap.  For ID, we add the synthetic data to the original ASR training data. For OOD, we do adapter-based continual fine-tuning of the ASR model (full-fine-tuning gave us worse performance) (iii) GER  Chada et al.   ( 2021 )   This can be considered as DARAG without data aur retrieval augmentation (iv) RobustGER  Radford et al.   ( 2023 )  (v) LM rank   We use the same LLM (continually fine-tuned on the text from training and synthetic dataset) as GER for re-scoring the  N N N italic_N -best hypotheses and finally take the hypothesis with the best score averaged across the LLM and ASR model scores. (vi) Enhance  we also employ a speech enhancement front-end, a HiFi-GAN  Su et al.   ( 2020 ) , to denoise the noisy speech before passing it to the ASR model. For ablations, we employ (i) w/o RAC: DARAG without retrieval augmented correction. (ii) w/o Aug.: DARAG without synthetic data augmentation but only retrieval augmentation based error correction. (iii) only Synth.: The GEC model is only trained on hypotheses-transcription pairs from only the synthetically generated data.",
            "Extra Results. We present extra results in the Appendix, including key hyper-parameter tuning results, importance of the voice cloning module and other results. Additionally, we provide examples of generated augmentations in Table  12  and DARAG corrections in Table  13 .",
            "Table  7  compares the performance of DARAG in both ID and OOD scenarios, with and without voice cloning. As discussed in Section  4.1 , voice cloning via TTS allows the model to generate synthetic speech that, when transcribed, produces hypotheses containing errors similar to those encountered during testing in that domain. As shown in the table, DARAG experiences a performance drop without voice cloning, with a more significant decline in OOD scenarios.",
            "Table  10  compares the performance of DARAG across various values of  n small subscript n small n_{\\text{small}} italic_n start_POSTSUBSCRIPT small end_POSTSUBSCRIPT . Larger  n small subscript n small n_{\\text{small}} italic_n start_POSTSUBSCRIPT small end_POSTSUBSCRIPT  can lead to more diverse and consistent augmentations, improving performance. For our primary experiments, we stick to 100 to keep our setting ultra-low-resource.",
            "Table  11  compares the performance of DARAG using different values of  n syn subscript n syn n_{\\text{syn}} italic_n start_POSTSUBSCRIPT syn end_POSTSUBSCRIPT , represented as a factor of  n n n italic_n  (the size of the original training set for the target dataset in an OOD setting). Increasing the number of synthetic samples (higher  n syn subscript n syn n_{\\text{syn}} italic_n start_POSTSUBSCRIPT syn end_POSTSUBSCRIPT ) can provide more diverse and consistent augmentations in OOD settings, resulting in better performance. However, the improvements plateau beyond a certain point. For our main experiments, we use  n syn = 1 subscript n syn 1 n_{\\text{syn}}=1 italic_n start_POSTSUBSCRIPT syn end_POSTSUBSCRIPT = 1  due to resource limitations.",
            "Table  12  provides examples of synthetically generated transcripts for each dataset from our evaluation setup. The transcripts are coherent and consistent with the characteristics of the domain.",
            "Table  13  qualitatively compares DARAG with traditional GEC on various instances from benchmark datasets. We show that DARAG is able to accurately correct NEs which traditional GEC cannot. Additionally, DARAG shows superior performance in OOD scenarios.",
            "Software and Packages details. We implement all our models in PyTorch   1 1 1 https://pytorch.org/  and use Parler-TTS  2 2 2 https://github.com/huggingface/parler-tts  and LLaMa-2  3 3 3 https://huggingface.co/meta-llama . We employ ESPnet  Watanabe et al.   ( 2018 )  for training our ASR models."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Performance comparison of GEC on VoxPopuli, an entity-rich dataset. The Matched Scenario and Mismatched Scenarios are defined as in Table  1 . We show that (a) For domain shifts, model performance degrades significantly on NEs. (b) For in-domain scenarios, GEC does not prove to be effective in correcting NEs.",
        "table": "S3.T2.2.2",
        "footnotes": [
            ""
        ],
        "references": [
            "Automatic Speech Recognition (ASR) is the foundational task of converting spoken language into text. As a fundamental goal in computational language processing  Jurafsky   ( 2000 ) , ASR has facilitated communication across diverse fields, including education  Caballero et al.   ( 2017 ) , healthcare  Latif et al.   ( 2020 ) , and others  den Bogaert et al.   ( 2022 ) . Advances in deep learning have driven significant progress in ASR, with end-to-end models achieving impressive results on various tasks  Li et al.   ( 2022 ) . However, one of the key challenges in real-world ASR applications   Li et al.   ( 2015 )  is handling variations in speech due to factors like background noise  Chen et al.   ( 2022 ) , speaker accents  Turan et al.   ( 2022 ) , and different speaking styles  Syed et al.   ( 2021 ) . These factors lead to a significant reduction in the accuracy of ASR.",
            "Humans demonstrate exceptional resilience to challenging speech conditions due to our inherent linguistic knowledge. Traditional ASR systems mimic this by incorporating a separate language model (LM) to rescore hypotheses during decoding  Toshniwal et al.   ( 2018 ) ;  Kannan et al.   ( 2018 ) . The LM evaluates the fluency of the N-best hypotheses generated by the ASR model, and the scores are combined with the ASRs own scores in a weighted fashion. The hypothesis with the highest combined score is then selected as the final transcript. However, the rise of large language models (LLMs) with advanced reasoning capabilities has opened possibilities beyond simple rescoring. This has led to the development of Generative Error Correction (GEC)  Chen et al.   ( 2024 ) , where models are trained to correct errors in the best hypothesis by leveraging information from other hypotheses, ultimately improving transcription accuracy.",
            "Conventional Generative Error Correction (GEC) models are typically trained by pooling hypothesis-transcription pairs from various ASR systems and datasets, with the expectation that they will generalize well across diverse data at test time  Chen et al.   ( 2024 ) ;  Hu et al.   ( 2024a ) ;  Ghosh et al.   ( 2024b ) . However, we identify key limitations in this approach. Previous work has primarily focused on foundational or semi-open-source models (e.g., Whisper  Radford et al.   ( 2023 ) ). To explore these limitations, we conducted several single-domain, single-dataset experiments (see Table  1 ), training GEC models on the same datasets used to train the ASR models. We observed only minor improvements in Word Error Rate (WER) on in-domain tests and no improvements on out-of-domain (OOD) tests. Upon closer examination, we attribute these shortcomings to three main factors:",
            "Our Contributions. To this end, we propose DARAG (Data- and Retrieval-Augmented Generative Error Correction), a simple, scalable, and domain-agnostic approach designed to enhance GEC performance in ID and OOD scenarios. Our approach is driven by the hypothesis that GEC models perform better when trained to correct errors they are likely to encounter at test time. To achieve this, DARAG generates synthetic training data using generative models. We start by prompting an LLM with few-shot examples of domain-specific transcripts to produce synthetic transcripts. For OOD settings, DARAG uses a small set of unsupervised audio samples from the target domain, which are transcribed using the in-domain ASR model to create in-context exemplars. The synthetically generated transcripts are then used to generate synthetic speech via a text-to-speech model and voice cloning. Finally, this synthetic speech is used to generate hypothesis-transcription pairs. This process simulates errors that are specific to the target-domain vocabulary and also imitates the phonetic confusions that the ID ASR model would make in the target domain. Additionally, to improve named entity correction, we introduce a retrieval augmentation method  Lewis et al.   ( 2020 ) . Specifically, we extract and store all named entities from the training dataset in a datastore and retrieve the top- k  most similar entities during GEC. Our proposed method is scalable, with the datastore being easily extendable at test time to incorporate new entities as they are encountered. To summarize, our main contributions are as follows:",
            "Generative Error Correction. Post-ASR error correction using language models (LMs) has been widely studied  Ma et al.   ( 2023b ,  a ) ;  Zhang et al.   ( 2023 ) . Recently, large language models (LLMs) have been applied to this task, rebranded as generative error correction  Hu et al.   ( 2024a ) ;  Ghosh et al.   ( 2024b ) . While LLMs excel due to their advanced language comprehension, it remains unclear which errors they effectively correct, which they miss, and how well they handle novel or unknown named entities (NEs) that they lack prior knowledge of.",
            "Domain Generalization and Named Entity in ASR. Transcribing NEs is a persistent challenge for ASR models  Das et al.   ( 2022 ) . Techniques such as memorization  Bekal et al.   ( 2021 )  and biasing  Jayanthi et al.   ( 2023 )  have been developed to improve NE transcription. However, these methods typically focus on known NEs seen during training and struggle with unseen entities, as autoregressive models tend to memorize NEs but generalize poorly to new ones  Heinzerling and Inui   ( 2020 ) . Improving NE transcription using post-ASR processing or GEC has not been well explored. ASR models often fail under distribution shifts, such as domain, accent, or dialect changes  Singhal et al.   ( 2023 ) . However, the robustness of GEC to domain shifts remains underexplored.",
            "Most prior work on Generative Error Correction (GEC) relies on foundational open-access ASR models, like Whisper, to generate hypotheses from various datasets and then trains GEC models on these hypotheses-transcription pairs, denoted as  H train id subscript superscript H id train \\mathcal{H}^{\\text{id}}_{\\text{train}} caligraphic_H start_POSTSUPERSCRIPT id end_POSTSUPERSCRIPT start_POSTSUBSCRIPT train end_POSTSUBSCRIPT . However, because the training data used for such ASR models is often undisclosed, there is limited insight into the nature of errors present in the hypotheses and, consequently, the types of errors that the GEC models learn to correct. In this work, we aim to study error correction from a more transparent perspective. Table  1  presents experiments where we train an ASR model on a single dataset (LibriSpeech (LS)  Panayotov et al.   ( 2015 ) , VoxPopuli  Wang et al.   ( 2021 )  (Vox), SPGIspeech  ONeill et al.   ( 2021 ) ), then derive hypotheses from either the same or a different dataset, and use these pairs to train a GEC model. Our key findings are as follows: (i) When GEC models are trained on a dataset in a different domain (i.e., both  D train id subscript superscript D id train \\mathcal{D}^{\\text{id}}_{\\text{train}} caligraphic_D start_POSTSUPERSCRIPT id end_POSTSUPERSCRIPT start_POSTSUBSCRIPT train end_POSTSUBSCRIPT  and  H train id subscript superscript H id train \\mathcal{H}^{\\text{id}}_{\\text{train}} caligraphic_H start_POSTSUPERSCRIPT id end_POSTSUPERSCRIPT start_POSTSUBSCRIPT train end_POSTSUBSCRIPT  come from a domain that is different from  D test id subscript superscript D id test \\mathcal{D}^{\\text{id}}_{\\text{test}} caligraphic_D start_POSTSUPERSCRIPT id end_POSTSUPERSCRIPT start_POSTSUBSCRIPT test end_POSTSUBSCRIPT ), no performance improvements are observed. We hypothesize this is due to the GEC model encountering errors at test time that differ significantly from those it saw during training. For instance, a hypotheses (HP)-transcription (GT) pair generated from the LibriSpeech train set using an ASR model trained on LibriSpeech is as follows:",
            "To assess the ability of GEC models to correct named entities (NEs), we analyze their performance in various settings. As mentioned earlier, transcribing NEs is a major challenge in ASR, particularly in knowledge-rich domains. Table  2  compares GEC performance on VoxPopuli using models trained under different conditions. For this experiment, we leverage annotated NEs from the MSNER dataset  Meeus et al.   ( 2024 )  for VoxPopuli. Our key findings are: (i) GEC models struggle to correct NEs, likely due to insufficient prior knowledge or context. (ii) In domain-shift scenarios, where ASR or GEC models have not encountered the target NEs during training, NE transcription accuracy declines sharply.  These results emphasize the importance of incorporating explicit knowledge of NEs to improve correction performance.",
            "Fig.  2  illustrates our proposed method. We propose two simple extensions to improve conventional GEC. First, we propose training the GEC model on additional synthetic data generated using generative models. Additionally, instead of memorizing the named entities, we propose decoupling them from the learning process with RAG. To achieve this, we first extract named entities and store them in a datastore. During training and inference, we retrieve them from the datastore and augment them to the instruction with the best hypothesis and other hypotheses. In the following subsections, we explain our methodology in detail.",
            "For In-Domain Scenarios. As discussed in Section  3.2 , GEC models fail to learn effective error correction due to the low number of errors in ASR training data. We hypothesize that generating novel spoken utterances not seen during ASR training will introduce more errors that can provide rich training signals for learning error correction. Our goal is to generate spoken utterances that closely mimic the speech characteristics of speakers in the same domain, replicating the style as if spoken by similar speakers in similar contexts. These utterances can then be used to generate new hypotheses,  H ^ train id subscript superscript ^ H id train \\hat{\\mathcal{H}}^{\\text{id}}_{\\text{train}} over^ start_ARG caligraphic_H end_ARG start_POSTSUPERSCRIPT id end_POSTSUPERSCRIPT start_POSTSUBSCRIPT train end_POSTSUBSCRIPT , which we augment into the original dataset  H train id subscript superscript H id train \\mathcal{H}^{\\text{id}}_{\\text{train}} caligraphic_H start_POSTSUPERSCRIPT id end_POSTSUPERSCRIPT start_POSTSUBSCRIPT train end_POSTSUBSCRIPT . We achieve this through a 3-step process:",
            "Step 1. We prompt an LLM (LLaMa-2.0-Instruct  Touvron et al.   ( 2023 ) ) with in-context examples sampled from  D train id subscript superscript D id train \\mathcal{D}^{\\text{id}}_{\\text{train}} caligraphic_D start_POSTSUPERSCRIPT id end_POSTSUPERSCRIPT start_POSTSUBSCRIPT train end_POSTSUBSCRIPT  to generate in-domain transcripts (prompt in Appendix  B ).",
            "Step 2. Using voice cloning via TTS, we generate spoken utterances from the transcripts. The TTS model (Parler-TTS Mini  Lacombe et al.   ( 2024 ) ) is conditioned on randomly selected utterances from  D train id subscript superscript D id train \\mathcal{D}^{\\text{id}}_{\\text{train}} caligraphic_D start_POSTSUPERSCRIPT id end_POSTSUPERSCRIPT start_POSTSUBSCRIPT train end_POSTSUBSCRIPT  to replicate the domains speech style. Steps 1 and 2 ensure the generated utterances align with the domain and produce error patterns similar to those expected at test time.",
            "Step 2. During GEC training and inference, we use SentenceBERT  Reimers   ( 2019 )  to retrieve the top- k  NEs,  s     s \\overline{s} over  start_ARG italic_s end_ARG , from  D  S D S \\mathcal{DS} caligraphic_D caligraphic_S  based on their similarity to the best hypothesis. This is formally defined as:",
            "The ground-truth transcription serves as the target for fine-tuning. Following prior work, we fine-tune only LoRA adapters  Hu et al.   ( 2021 ) .",
            "Datasets. We evaluate DARAG on 5 benchmark ASR datasets, including LibriSpeech-960 (LS), SPGISpeech (SPGI), VoxPopuli en (Vox), Gigaspeech  Chen et al.   ( 2021 )  (Giga) and TED-LIUM  Rousseau et al.   ( 2012 )  (TED).  We acknowledge that for OOD evaluation, prior works use different and varied settings. However, we want to emphasize that OOD adaptation or evaluation is not our main focus; rather, only to show DARAG improves performance in typical OOD settings.",
            "Comparison Methods and Ablations. For comparison with DARAG, we employ (i) Baseline  Only ASR, and we perform no post-processing. (ii) Synth. Adap.  For ID, we add the synthetic data to the original ASR training data. For OOD, we do adapter-based continual fine-tuning of the ASR model (full-fine-tuning gave us worse performance) (iii) GER  Chada et al.   ( 2021 )   This can be considered as DARAG without data aur retrieval augmentation (iv) RobustGER  Radford et al.   ( 2023 )  (v) LM rank   We use the same LLM (continually fine-tuned on the text from training and synthetic dataset) as GER for re-scoring the  N N N italic_N -best hypotheses and finally take the hypothesis with the best score averaged across the LLM and ASR model scores. (vi) Enhance  we also employ a speech enhancement front-end, a HiFi-GAN  Su et al.   ( 2020 ) , to denoise the noisy speech before passing it to the ASR model. For ablations, we employ (i) w/o RAC: DARAG without retrieval augmented correction. (ii) w/o Aug.: DARAG without synthetic data augmentation but only retrieval augmentation based error correction. (iii) only Synth.: The GEC model is only trained on hypotheses-transcription pairs from only the synthetically generated data.",
            "Main Results. Table  3  presents our main results, comparing performance across five datasets in both ID and OOD scenarios. In the ID setting, the training and test sets come from the same dataset, whereas in the OOD setting, the training set is sourced from a different dataset, making the test set OOD for both the ASR and GEC models. For the OOD experiments, we randomly selected three datasets for training without any particular preference. Additionally, we did not assume the availability of ground-truth transcripts in  D train ood subscript superscript D ood train \\mathcal{D}^{\\text{ood}}_{\\text{train}} caligraphic_D start_POSTSUPERSCRIPT ood end_POSTSUPERSCRIPT start_POSTSUBSCRIPT train end_POSTSUBSCRIPT  and instead used our ASR model to generate transcripts. Unlike previous experiments, we did not assume a separate dataset for ASR training; both the ASR model and hypotheses were generated from the same training data. Our key findings can be summarized as follows: (i) DARAG substantially improves ASR performance for both ID (8%-30%) and OOD (10%-33%) settings. (ii) In ID settings, both RAC and synthetic augmentation prove essential, as ablating either component leads to a decline in performance. (iii) In OOD settings, augmentation is more beneficial than RAC, likely because most NEs in the datastore do not match the NEs encountered during testing. (iv) DARAG proves to be a better way to utilize synthetic data for improving ASR as adaptation with synthetic data leads to performance decrease over baseline.(v) In some OOD cases, removing RAC actually improves performance, which we attribute to mismatched OOD NEs causing the GEC model to incorrectly adjust certain NEs. (vi) Relying solely on synthetic data is not effective for OOD scenarios, consistent with prior research indicating that human-annotated data remains crucial for optimal performance  Ghosh et al.   ( 2024a ) .  In Appendix  C  we demonstrate that DARAG does not simply replicate the original training data as a result of LLM memorization.",
            "Table  4  presents a comparison of F1-micro scores for DARAG and various baselines in both ID and OOD settings. The results reveal several key insights: (i) DARAG consistently outperforms the baseline and conventional GEC approaches, with particularly large gains in OOD scenarios, demonstrating its robustness to domain shifts. (ii) Incorporating a datastore containing NEs from the in-domain dataset significantly improves OOD performance, in some cases matching the results of GEC models trained on ID datasets. This highlights the effectiveness of retrieval-augmented correction in enhancing ASR performance, including practical applications like meeting applications, where a datastore can be constructed with a list of relevant NEs and not necessarily included during training. (iii) Augmenting the datastore with synthetically generated NEs also shows promise in boosting DARAGs performance, indicating the potential to dynamically add emerging NEs to the datastore. This approach reduces the reliance on continual fine-tuning for ASR adaptation, which is typically required in other methods  Das et al.   ( 2022 ) .",
            "Most Unsupervised Domain Adaptation (UDA) methods for ASR assume the presence of the entire unlabeled dataset from the target domain  Hu et al.   ( 2024b ) . On the other hand, DARAG assumes the presence of only few unlabeled instances. Fig.  3  shows DARAG proves to be effective for extreme low-resource UDA and outperforms STAR and continual fine-tuning with pseudo-labeling.",
            "Extra Results. We present extra results in the Appendix, including key hyper-parameter tuning results, importance of the voice cloning module and other results. Additionally, we provide examples of generated augmentations in Table  12  and DARAG corrections in Table  13 .",
            "Previous research has suggested that LLMs may memorize open-domain ASR training transcripts  Liu and Niehues   ( 2024 ) ;  Team et al.   ( 2023 ) , raising the risk of replicating training data while genrating synthetic data. To evaluate whether this occurs with DARAG, we perform two checks: (i) We use SentenceBERT to calculate the cosine similarity between each generated transcript and all transcripts in the original training set, reporting the average semantic similarity across instances in Table  6  ii) We compute the BLEU score for each generated transcript, using the transcript with the highest cosine similarity from the previous step as a reference. Table  6  shows the average BLEU scores across BLEU 1 , BLEU 2 , and BLEU 3 . The low BLEU scores indicate that DARAG does not simply replicate the training data. The semantic similarity indicates that with DARAG generates transcripts that are consistent with the domain.",
            "Table  12  provides examples of synthetically generated transcripts for each dataset from our evaluation setup. The transcripts are coherent and consistent with the characteristics of the domain.",
            "Software and Packages details. We implement all our models in PyTorch   1 1 1 https://pytorch.org/  and use Parler-TTS  2 2 2 https://github.com/huggingface/parler-tts  and LLaMa-2  3 3 3 https://huggingface.co/meta-llama . We employ ESPnet  Watanabe et al.   ( 2018 )  for training our ASR models."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Performance comparison (WER) of DARAG with other methods on various in-domain and out-of-domain settings (the Test is OOD w.r.t. the Train). We assume all 5 datasets are from different domains. We also report the absolute improvements w.r.t. to the ASR-only Baseline. DARAG outperforms other methods by 8%30% in in-domain and 10%33% in OOD settings.",
        "table": "S4.T3.205.205",
        "footnotes": [],
        "references": [
            "Conventional Generative Error Correction (GEC) models are typically trained by pooling hypothesis-transcription pairs from various ASR systems and datasets, with the expectation that they will generalize well across diverse data at test time  Chen et al.   ( 2024 ) ;  Hu et al.   ( 2024a ) ;  Ghosh et al.   ( 2024b ) . However, we identify key limitations in this approach. Previous work has primarily focused on foundational or semi-open-source models (e.g., Whisper  Radford et al.   ( 2023 ) ). To explore these limitations, we conducted several single-domain, single-dataset experiments (see Table  1 ), training GEC models on the same datasets used to train the ASR models. We observed only minor improvements in Word Error Rate (WER) on in-domain tests and no improvements on out-of-domain (OOD) tests. Upon closer examination, we attribute these shortcomings to three main factors:",
            "Generative Error Correction. Post-ASR error correction using language models (LMs) has been widely studied  Ma et al.   ( 2023b ,  a ) ;  Zhang et al.   ( 2023 ) . Recently, large language models (LLMs) have been applied to this task, rebranded as generative error correction  Hu et al.   ( 2024a ) ;  Ghosh et al.   ( 2024b ) . While LLMs excel due to their advanced language comprehension, it remains unclear which errors they effectively correct, which they miss, and how well they handle novel or unknown named entities (NEs) that they lack prior knowledge of.",
            "Domain Generalization and Named Entity in ASR. Transcribing NEs is a persistent challenge for ASR models  Das et al.   ( 2022 ) . Techniques such as memorization  Bekal et al.   ( 2021 )  and biasing  Jayanthi et al.   ( 2023 )  have been developed to improve NE transcription. However, these methods typically focus on known NEs seen during training and struggle with unseen entities, as autoregressive models tend to memorize NEs but generalize poorly to new ones  Heinzerling and Inui   ( 2020 ) . Improving NE transcription using post-ASR processing or GEC has not been well explored. ASR models often fail under distribution shifts, such as domain, accent, or dialect changes  Singhal et al.   ( 2023 ) . However, the robustness of GEC to domain shifts remains underexplored.",
            "For In-Domain Scenarios. As discussed in Section  3.2 , GEC models fail to learn effective error correction due to the low number of errors in ASR training data. We hypothesize that generating novel spoken utterances not seen during ASR training will introduce more errors that can provide rich training signals for learning error correction. Our goal is to generate spoken utterances that closely mimic the speech characteristics of speakers in the same domain, replicating the style as if spoken by similar speakers in similar contexts. These utterances can then be used to generate new hypotheses,  H ^ train id subscript superscript ^ H id train \\hat{\\mathcal{H}}^{\\text{id}}_{\\text{train}} over^ start_ARG caligraphic_H end_ARG start_POSTSUPERSCRIPT id end_POSTSUPERSCRIPT start_POSTSUBSCRIPT train end_POSTSUBSCRIPT , which we augment into the original dataset  H train id subscript superscript H id train \\mathcal{H}^{\\text{id}}_{\\text{train}} caligraphic_H start_POSTSUPERSCRIPT id end_POSTSUPERSCRIPT start_POSTSUBSCRIPT train end_POSTSUBSCRIPT . We achieve this through a 3-step process:",
            "Step 1. We prompt an LLM (LLaMa-2.0-Instruct  Touvron et al.   ( 2023 ) ) with in-context examples sampled from  D train id subscript superscript D id train \\mathcal{D}^{\\text{id}}_{\\text{train}} caligraphic_D start_POSTSUPERSCRIPT id end_POSTSUPERSCRIPT start_POSTSUBSCRIPT train end_POSTSUBSCRIPT  to generate in-domain transcripts (prompt in Appendix  B ).",
            "Comparison Methods and Ablations. For comparison with DARAG, we employ (i) Baseline  Only ASR, and we perform no post-processing. (ii) Synth. Adap.  For ID, we add the synthetic data to the original ASR training data. For OOD, we do adapter-based continual fine-tuning of the ASR model (full-fine-tuning gave us worse performance) (iii) GER  Chada et al.   ( 2021 )   This can be considered as DARAG without data aur retrieval augmentation (iv) RobustGER  Radford et al.   ( 2023 )  (v) LM rank   We use the same LLM (continually fine-tuned on the text from training and synthetic dataset) as GER for re-scoring the  N N N italic_N -best hypotheses and finally take the hypothesis with the best score averaged across the LLM and ASR model scores. (vi) Enhance  we also employ a speech enhancement front-end, a HiFi-GAN  Su et al.   ( 2020 ) , to denoise the noisy speech before passing it to the ASR model. For ablations, we employ (i) w/o RAC: DARAG without retrieval augmented correction. (ii) w/o Aug.: DARAG without synthetic data augmentation but only retrieval augmentation based error correction. (iii) only Synth.: The GEC model is only trained on hypotheses-transcription pairs from only the synthetically generated data.",
            "Main Results. Table  3  presents our main results, comparing performance across five datasets in both ID and OOD scenarios. In the ID setting, the training and test sets come from the same dataset, whereas in the OOD setting, the training set is sourced from a different dataset, making the test set OOD for both the ASR and GEC models. For the OOD experiments, we randomly selected three datasets for training without any particular preference. Additionally, we did not assume the availability of ground-truth transcripts in  D train ood subscript superscript D ood train \\mathcal{D}^{\\text{ood}}_{\\text{train}} caligraphic_D start_POSTSUPERSCRIPT ood end_POSTSUPERSCRIPT start_POSTSUBSCRIPT train end_POSTSUBSCRIPT  and instead used our ASR model to generate transcripts. Unlike previous experiments, we did not assume a separate dataset for ASR training; both the ASR model and hypotheses were generated from the same training data. Our key findings can be summarized as follows: (i) DARAG substantially improves ASR performance for both ID (8%-30%) and OOD (10%-33%) settings. (ii) In ID settings, both RAC and synthetic augmentation prove essential, as ablating either component leads to a decline in performance. (iii) In OOD settings, augmentation is more beneficial than RAC, likely because most NEs in the datastore do not match the NEs encountered during testing. (iv) DARAG proves to be a better way to utilize synthetic data for improving ASR as adaptation with synthetic data leads to performance decrease over baseline.(v) In some OOD cases, removing RAC actually improves performance, which we attribute to mismatched OOD NEs causing the GEC model to incorrectly adjust certain NEs. (vi) Relying solely on synthetic data is not effective for OOD scenarios, consistent with prior research indicating that human-annotated data remains crucial for optimal performance  Ghosh et al.   ( 2024a ) .  In Appendix  C  we demonstrate that DARAG does not simply replicate the original training data as a result of LLM memorization.",
            "Most Unsupervised Domain Adaptation (UDA) methods for ASR assume the presence of the entire unlabeled dataset from the target domain  Hu et al.   ( 2024b ) . On the other hand, DARAG assumes the presence of only few unlabeled instances. Fig.  3  shows DARAG proves to be effective for extreme low-resource UDA and outperforms STAR and continual fine-tuning with pseudo-labeling.",
            "Extra Results. We present extra results in the Appendix, including key hyper-parameter tuning results, importance of the voice cloning module and other results. Additionally, we provide examples of generated augmentations in Table  12  and DARAG corrections in Table  13 .",
            "Previous research has suggested that LLMs may memorize open-domain ASR training transcripts  Liu and Niehues   ( 2024 ) ;  Team et al.   ( 2023 ) , raising the risk of replicating training data while genrating synthetic data. To evaluate whether this occurs with DARAG, we perform two checks: (i) We use SentenceBERT to calculate the cosine similarity between each generated transcript and all transcripts in the original training set, reporting the average semantic similarity across instances in Table  6  ii) We compute the BLEU score for each generated transcript, using the transcript with the highest cosine similarity from the previous step as a reference. Table  6  shows the average BLEU scores across BLEU 1 , BLEU 2 , and BLEU 3 . The low BLEU scores indicate that DARAG does not simply replicate the training data. The semantic similarity indicates that with DARAG generates transcripts that are consistent with the domain.",
            "Table  13  qualitatively compares DARAG with traditional GEC on various instances from benchmark datasets. We show that DARAG is able to accurately correct NEs which traditional GEC cannot. Additionally, DARAG shows superior performance in OOD scenarios."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  Performance comparison of DARAG with other methods on the NE transcription. For ID, we employ the train set of the dataset as the test. For OOD, we employ LS for Vox and Vox for LS. w/ ID NE refers to DARAG, where the NE datastore is from the ID train set. w/ synth NE refers to additional synthetic NEs we add to the NE datastore.",
        "table": "S6.T4.2.2",
        "footnotes": [],
        "references": [
            "Humans demonstrate exceptional resilience to challenging speech conditions due to our inherent linguistic knowledge. Traditional ASR systems mimic this by incorporating a separate language model (LM) to rescore hypotheses during decoding  Toshniwal et al.   ( 2018 ) ;  Kannan et al.   ( 2018 ) . The LM evaluates the fluency of the N-best hypotheses generated by the ASR model, and the scores are combined with the ASRs own scores in a weighted fashion. The hypothesis with the highest combined score is then selected as the final transcript. However, the rise of large language models (LLMs) with advanced reasoning capabilities has opened possibilities beyond simple rescoring. This has led to the development of Generative Error Correction (GEC)  Chen et al.   ( 2024 ) , where models are trained to correct errors in the best hypothesis by leveraging information from other hypotheses, ultimately improving transcription accuracy.",
            "Conventional Generative Error Correction (GEC) models are typically trained by pooling hypothesis-transcription pairs from various ASR systems and datasets, with the expectation that they will generalize well across diverse data at test time  Chen et al.   ( 2024 ) ;  Hu et al.   ( 2024a ) ;  Ghosh et al.   ( 2024b ) . However, we identify key limitations in this approach. Previous work has primarily focused on foundational or semi-open-source models (e.g., Whisper  Radford et al.   ( 2023 ) ). To explore these limitations, we conducted several single-domain, single-dataset experiments (see Table  1 ), training GEC models on the same datasets used to train the ASR models. We observed only minor improvements in Word Error Rate (WER) on in-domain tests and no improvements on out-of-domain (OOD) tests. Upon closer examination, we attribute these shortcomings to three main factors:",
            "Generative Error Correction. Post-ASR error correction using language models (LMs) has been widely studied  Ma et al.   ( 2023b ,  a ) ;  Zhang et al.   ( 2023 ) . Recently, large language models (LLMs) have been applied to this task, rebranded as generative error correction  Hu et al.   ( 2024a ) ;  Ghosh et al.   ( 2024b ) . While LLMs excel due to their advanced language comprehension, it remains unclear which errors they effectively correct, which they miss, and how well they handle novel or unknown named entities (NEs) that they lack prior knowledge of.",
            "To assess the ability of GEC models to correct named entities (NEs), we analyze their performance in various settings. As mentioned earlier, transcribing NEs is a major challenge in ASR, particularly in knowledge-rich domains. Table  2  compares GEC performance on VoxPopuli using models trained under different conditions. For this experiment, we leverage annotated NEs from the MSNER dataset  Meeus et al.   ( 2024 )  for VoxPopuli. Our key findings are: (i) GEC models struggle to correct NEs, likely due to insufficient prior knowledge or context. (ii) In domain-shift scenarios, where ASR or GEC models have not encountered the target NEs during training, NE transcription accuracy declines sharply.  These results emphasize the importance of incorporating explicit knowledge of NEs to improve correction performance.",
            "Step 2. Using voice cloning via TTS, we generate spoken utterances from the transcripts. The TTS model (Parler-TTS Mini  Lacombe et al.   ( 2024 ) ) is conditioned on randomly selected utterances from  D train id subscript superscript D id train \\mathcal{D}^{\\text{id}}_{\\text{train}} caligraphic_D start_POSTSUPERSCRIPT id end_POSTSUPERSCRIPT start_POSTSUBSCRIPT train end_POSTSUBSCRIPT  to replicate the domains speech style. Steps 1 and 2 ensure the generated utterances align with the domain and produce error patterns similar to those expected at test time.",
            "Main Results. Table  3  presents our main results, comparing performance across five datasets in both ID and OOD scenarios. In the ID setting, the training and test sets come from the same dataset, whereas in the OOD setting, the training set is sourced from a different dataset, making the test set OOD for both the ASR and GEC models. For the OOD experiments, we randomly selected three datasets for training without any particular preference. Additionally, we did not assume the availability of ground-truth transcripts in  D train ood subscript superscript D ood train \\mathcal{D}^{\\text{ood}}_{\\text{train}} caligraphic_D start_POSTSUPERSCRIPT ood end_POSTSUPERSCRIPT start_POSTSUBSCRIPT train end_POSTSUBSCRIPT  and instead used our ASR model to generate transcripts. Unlike previous experiments, we did not assume a separate dataset for ASR training; both the ASR model and hypotheses were generated from the same training data. Our key findings can be summarized as follows: (i) DARAG substantially improves ASR performance for both ID (8%-30%) and OOD (10%-33%) settings. (ii) In ID settings, both RAC and synthetic augmentation prove essential, as ablating either component leads to a decline in performance. (iii) In OOD settings, augmentation is more beneficial than RAC, likely because most NEs in the datastore do not match the NEs encountered during testing. (iv) DARAG proves to be a better way to utilize synthetic data for improving ASR as adaptation with synthetic data leads to performance decrease over baseline.(v) In some OOD cases, removing RAC actually improves performance, which we attribute to mismatched OOD NEs causing the GEC model to incorrectly adjust certain NEs. (vi) Relying solely on synthetic data is not effective for OOD scenarios, consistent with prior research indicating that human-annotated data remains crucial for optimal performance  Ghosh et al.   ( 2024a ) .  In Appendix  C  we demonstrate that DARAG does not simply replicate the original training data as a result of LLM memorization.",
            "Table  4  presents a comparison of F1-micro scores for DARAG and various baselines in both ID and OOD settings. The results reveal several key insights: (i) DARAG consistently outperforms the baseline and conventional GEC approaches, with particularly large gains in OOD scenarios, demonstrating its robustness to domain shifts. (ii) Incorporating a datastore containing NEs from the in-domain dataset significantly improves OOD performance, in some cases matching the results of GEC models trained on ID datasets. This highlights the effectiveness of retrieval-augmented correction in enhancing ASR performance, including practical applications like meeting applications, where a datastore can be constructed with a list of relevant NEs and not necessarily included during training. (iii) Augmenting the datastore with synthetically generated NEs also shows promise in boosting DARAGs performance, indicating the potential to dynamically add emerging NEs to the datastore. This approach reduces the reliance on continual fine-tuning for ASR adaptation, which is typically required in other methods  Das et al.   ( 2022 ) .",
            "Most Unsupervised Domain Adaptation (UDA) methods for ASR assume the presence of the entire unlabeled dataset from the target domain  Hu et al.   ( 2024b ) . On the other hand, DARAG assumes the presence of only few unlabeled instances. Fig.  3  shows DARAG proves to be effective for extreme low-resource UDA and outperforms STAR and continual fine-tuning with pseudo-labeling.",
            "Previous research has suggested that LLMs may memorize open-domain ASR training transcripts  Liu and Niehues   ( 2024 ) ;  Team et al.   ( 2023 ) , raising the risk of replicating training data while genrating synthetic data. To evaluate whether this occurs with DARAG, we perform two checks: (i) We use SentenceBERT to calculate the cosine similarity between each generated transcript and all transcripts in the original training set, reporting the average semantic similarity across instances in Table  6  ii) We compute the BLEU score for each generated transcript, using the transcript with the highest cosine similarity from the previous step as a reference. Table  6  shows the average BLEU scores across BLEU 1 , BLEU 2 , and BLEU 3 . The low BLEU scores indicate that DARAG does not simply replicate the training data. The semantic similarity indicates that with DARAG generates transcripts that are consistent with the domain.",
            "Table  7  compares the performance of DARAG in both ID and OOD scenarios, with and without voice cloning. As discussed in Section  4.1 , voice cloning via TTS allows the model to generate synthetic speech that, when transcribed, produces hypotheses containing errors similar to those encountered during testing in that domain. As shown in the table, DARAG experiences a performance drop without voice cloning, with a more significant decline in OOD scenarios."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 :  Performance comparison of DARAG in OOD settings with the baseline. We replace the generated augmentations with the original target domain training dataset (and do not generate extra augmentations). Training on hypotheses from the target domain train set leads to superior performance.",
        "table": "S6.T5.1.1",
        "footnotes": [],
        "references": [
            "Automatic Speech Recognition (ASR) is the foundational task of converting spoken language into text. As a fundamental goal in computational language processing  Jurafsky   ( 2000 ) , ASR has facilitated communication across diverse fields, including education  Caballero et al.   ( 2017 ) , healthcare  Latif et al.   ( 2020 ) , and others  den Bogaert et al.   ( 2022 ) . Advances in deep learning have driven significant progress in ASR, with end-to-end models achieving impressive results on various tasks  Li et al.   ( 2022 ) . However, one of the key challenges in real-world ASR applications   Li et al.   ( 2015 )  is handling variations in speech due to factors like background noise  Chen et al.   ( 2022 ) , speaker accents  Turan et al.   ( 2022 ) , and different speaking styles  Syed et al.   ( 2021 ) . These factors lead to a significant reduction in the accuracy of ASR.",
            "Most prior work on Generative Error Correction (GEC) relies on foundational open-access ASR models, like Whisper, to generate hypotheses from various datasets and then trains GEC models on these hypotheses-transcription pairs, denoted as  H train id subscript superscript H id train \\mathcal{H}^{\\text{id}}_{\\text{train}} caligraphic_H start_POSTSUPERSCRIPT id end_POSTSUPERSCRIPT start_POSTSUBSCRIPT train end_POSTSUBSCRIPT . However, because the training data used for such ASR models is often undisclosed, there is limited insight into the nature of errors present in the hypotheses and, consequently, the types of errors that the GEC models learn to correct. In this work, we aim to study error correction from a more transparent perspective. Table  1  presents experiments where we train an ASR model on a single dataset (LibriSpeech (LS)  Panayotov et al.   ( 2015 ) , VoxPopuli  Wang et al.   ( 2021 )  (Vox), SPGIspeech  ONeill et al.   ( 2021 ) ), then derive hypotheses from either the same or a different dataset, and use these pairs to train a GEC model. Our key findings are as follows: (i) When GEC models are trained on a dataset in a different domain (i.e., both  D train id subscript superscript D id train \\mathcal{D}^{\\text{id}}_{\\text{train}} caligraphic_D start_POSTSUPERSCRIPT id end_POSTSUPERSCRIPT start_POSTSUBSCRIPT train end_POSTSUBSCRIPT  and  H train id subscript superscript H id train \\mathcal{H}^{\\text{id}}_{\\text{train}} caligraphic_H start_POSTSUPERSCRIPT id end_POSTSUPERSCRIPT start_POSTSUBSCRIPT train end_POSTSUBSCRIPT  come from a domain that is different from  D test id subscript superscript D id test \\mathcal{D}^{\\text{id}}_{\\text{test}} caligraphic_D start_POSTSUPERSCRIPT id end_POSTSUPERSCRIPT start_POSTSUBSCRIPT test end_POSTSUBSCRIPT ), no performance improvements are observed. We hypothesize this is due to the GEC model encountering errors at test time that differ significantly from those it saw during training. For instance, a hypotheses (HP)-transcription (GT) pair generated from the LibriSpeech train set using an ASR model trained on LibriSpeech is as follows:",
            "Table  5  shows a comparison between DARAG and various baseline configurations where the synthetic dataset is replaced with the original training set of the target domain. The results clearly demonstrate that using real training data for generating GEC hypotheses significantly boosts performance, often surpassing complete ID settings. We attribute this improvement to two main factors: (i) the ASR model produces more errors on the GEC training dataset due to domain mismatch, providing richer training signals, and (ii) the datastore is enriched with real NEs from the original training set, offering more accurate context for corrections."
        ]
    },
    "id_table_6": {
        "caption": "Table 6 :  Semantic similarity and BLEU scores between original and generated transcripts across all datasets.",
        "table": "A3.T6.1",
        "footnotes": [],
        "references": [
            "Previous research has suggested that LLMs may memorize open-domain ASR training transcripts  Liu and Niehues   ( 2024 ) ;  Team et al.   ( 2023 ) , raising the risk of replicating training data while genrating synthetic data. To evaluate whether this occurs with DARAG, we perform two checks: (i) We use SentenceBERT to calculate the cosine similarity between each generated transcript and all transcripts in the original training set, reporting the average semantic similarity across instances in Table  6  ii) We compute the BLEU score for each generated transcript, using the transcript with the highest cosine similarity from the previous step as a reference. Table  6  shows the average BLEU scores across BLEU 1 , BLEU 2 , and BLEU 3 . The low BLEU scores indicate that DARAG does not simply replicate the training data. The semantic similarity indicates that with DARAG generates transcripts that are consistent with the domain."
        ]
    },
    "id_table_7": {
        "caption": "Table 7 :  Performance comparison of DARAG with and without voice cloning. Performance drops sharply without voice cloning, especially in OOD scenrios, thereby confirming the importance of the voice cloning for generating augmentations.",
        "table": "A4.T7.1.1",
        "footnotes": [],
        "references": [
            "Automatic Speech Recognition (ASR) is the foundational task of converting spoken language into text. As a fundamental goal in computational language processing  Jurafsky   ( 2000 ) , ASR has facilitated communication across diverse fields, including education  Caballero et al.   ( 2017 ) , healthcare  Latif et al.   ( 2020 ) , and others  den Bogaert et al.   ( 2022 ) . Advances in deep learning have driven significant progress in ASR, with end-to-end models achieving impressive results on various tasks  Li et al.   ( 2022 ) . However, one of the key challenges in real-world ASR applications   Li et al.   ( 2015 )  is handling variations in speech due to factors like background noise  Chen et al.   ( 2022 ) , speaker accents  Turan et al.   ( 2022 ) , and different speaking styles  Syed et al.   ( 2021 ) . These factors lead to a significant reduction in the accuracy of ASR.",
            "Table  7  compares the performance of DARAG in both ID and OOD scenarios, with and without voice cloning. As discussed in Section  4.1 , voice cloning via TTS allows the model to generate synthetic speech that, when transcribed, produces hypotheses containing errors similar to those encountered during testing in that domain. As shown in the table, DARAG experiences a performance drop without voice cloning, with a more significant decline in OOD scenarios."
        ]
    },
    "id_table_8": {
        "caption": "Table 8 :  Performance comparison of DARAG across different settings. OOD Adapt. refers to the dataset for which synthetic data was generated and augmented to the original hypotheses for GEC training. Our results show that, even with the addition of synthetically generated training data, DARAG maintains its in-domain performance. Furthermore, improvements in a specific domain occur only when the augmentations are consistent with that domain. This approach ensures that the errors used for training match the characteristics of those the ASR model will encounter during testing.",
        "table": "A5.T8.1.1",
        "footnotes": [],
        "references": [
            "Humans demonstrate exceptional resilience to challenging speech conditions due to our inherent linguistic knowledge. Traditional ASR systems mimic this by incorporating a separate language model (LM) to rescore hypotheses during decoding  Toshniwal et al.   ( 2018 ) ;  Kannan et al.   ( 2018 ) . The LM evaluates the fluency of the N-best hypotheses generated by the ASR model, and the scores are combined with the ASRs own scores in a weighted fashion. The hypothesis with the highest combined score is then selected as the final transcript. However, the rise of large language models (LLMs) with advanced reasoning capabilities has opened possibilities beyond simple rescoring. This has led to the development of Generative Error Correction (GEC)  Chen et al.   ( 2024 ) , where models are trained to correct errors in the best hypothesis by leveraging information from other hypotheses, ultimately improving transcription accuracy.",
            "Table  8  presents the performance of DARAG on in-domain tests after augmenting the hypotheses dataset with OOD hypotheses-transcription pairs. The results demonstrate that DARAG maintains its performance on the in-domain test with only a negligible drop.",
            "Software and Packages details. We implement all our models in PyTorch   1 1 1 https://pytorch.org/  and use Parler-TTS  2 2 2 https://github.com/huggingface/parler-tts  and LLaMa-2  3 3 3 https://huggingface.co/meta-llama . We employ ESPnet  Watanabe et al.   ( 2018 )  for training our ASR models."
        ]
    },
    "id_table_9": {
        "caption": "Table 9 :  Performance comparison of DARAG on two in-domain settings with various values of  k k k italic_k  for NE retrieval.",
        "table": "A6.T9.5.5",
        "footnotes": [],
        "references": [
            "Step 2. During GEC training and inference, we use SentenceBERT  Reimers   ( 2019 )  to retrieve the top- k  NEs,  s     s \\overline{s} over  start_ARG italic_s end_ARG , from  D  S D S \\mathcal{DS} caligraphic_D caligraphic_S  based on their similarity to the best hypothesis. This is formally defined as:",
            "Table  9  compares the performance of DARAG across various values of  k k k italic_k  for NE retrieval. We choose two in-domain settings as our main experiments show NE retrieval is most effective in in-domain scenarios. We show both higher and lower values of  k k k italic_k  can lead to a drop in performance and find 5 as the most optimal value. Higher values of  k k k italic_k  can retrieve irrelevant NEs and confuse the GEC model. Lower values of  k k k italic_k  can lead to cases where the GT NE is not retrieved."
        ]
    },
    "id_table_10": {
        "caption": "Table 10 :  Performance comparison of DARAG on two OOD settings (with LS as training set) with various values of  n small subscript n small n_{\\text{small}} italic_n start_POSTSUBSCRIPT small end_POSTSUBSCRIPT . Larger values can lead to improved performance.",
        "table": "A6.T10.3.1",
        "footnotes": [],
        "references": [
            "Table  10  compares the performance of DARAG across various values of  n small subscript n small n_{\\text{small}} italic_n start_POSTSUBSCRIPT small end_POSTSUBSCRIPT . Larger  n small subscript n small n_{\\text{small}} italic_n start_POSTSUBSCRIPT small end_POSTSUBSCRIPT  can lead to more diverse and consistent augmentations, improving performance. For our primary experiments, we stick to 100 to keep our setting ultra-low-resource."
        ]
    },
    "id_table_11": {
        "caption": "Table 11 :  Performance comparison of DARAG on two OOD settings (with LS as training set) across different scaling factors of  n syn subscript n syn n_{\\text{syn}} italic_n start_POSTSUBSCRIPT syn end_POSTSUBSCRIPT  relative to  n n n italic_n . More synthetic samples can lead to improved performance, but plateaus beyond a certain point.",
        "table": "A6.T11.4.4",
        "footnotes": [],
        "references": [
            "Table  11  compares the performance of DARAG using different values of  n syn subscript n syn n_{\\text{syn}} italic_n start_POSTSUBSCRIPT syn end_POSTSUBSCRIPT , represented as a factor of  n n n italic_n  (the size of the original training set for the target dataset in an OOD setting). Increasing the number of synthetic samples (higher  n syn subscript n syn n_{\\text{syn}} italic_n start_POSTSUBSCRIPT syn end_POSTSUBSCRIPT ) can provide more diverse and consistent augmentations in OOD settings, resulting in better performance. However, the improvements plateau beyond a certain point. For our main experiments, we use  n syn = 1 subscript n syn 1 n_{\\text{syn}}=1 italic_n start_POSTSUBSCRIPT syn end_POSTSUBSCRIPT = 1  due to resource limitations."
        ]
    },
    "id_table_12": {
        "caption": "Table 12 :  Examples of generated transcripts by the DARAG methodology.",
        "table": "A7.T12.1.1",
        "footnotes": [],
        "references": [
            "Datasets. We evaluate DARAG on 5 benchmark ASR datasets, including LibriSpeech-960 (LS), SPGISpeech (SPGI), VoxPopuli en (Vox), Gigaspeech  Chen et al.   ( 2021 )  (Giga) and TED-LIUM  Rousseau et al.   ( 2012 )  (TED).  We acknowledge that for OOD evaluation, prior works use different and varied settings. However, we want to emphasize that OOD adaptation or evaluation is not our main focus; rather, only to show DARAG improves performance in typical OOD settings.",
            "Extra Results. We present extra results in the Appendix, including key hyper-parameter tuning results, importance of the voice cloning module and other results. Additionally, we provide examples of generated augmentations in Table  12  and DARAG corrections in Table  13 .",
            "Table  12  provides examples of synthetically generated transcripts for each dataset from our evaluation setup. The transcripts are coherent and consistent with the characteristics of the domain."
        ]
    },
    "id_table_13": {
        "caption": "Table 13 :  Examples of incorrect ASR transcriptions and their corresponding corrections by DARAG.",
        "table": "A8.T13.1.1",
        "footnotes": [],
        "references": [
            "Extra Results. We present extra results in the Appendix, including key hyper-parameter tuning results, importance of the voice cloning module and other results. Additionally, we provide examples of generated augmentations in Table  12  and DARAG corrections in Table  13 .",
            "Table  13  qualitatively compares DARAG with traditional GEC on various instances from benchmark datasets. We show that DARAG is able to accurately correct NEs which traditional GEC cannot. Additionally, DARAG shows superior performance in OOD scenarios."
        ]
    }
}