{
    "id_table_1": {
        "caption": "Table 1:    An example from  DebateQA , details of the explanation fields are truncated for space issues.",
        "table": "S1.T1.1",
        "footnotes": [],
        "references": [
            "Dataset overview.    DebateQA  is designed to help assessing  language models answers to debatable questions . It contains 2,941 debatable questions, each paired with a list of  partial answers  to assist in evaluating model responses. Each partial answer addresses the question from a distinct perspective and consists of two parts: a short  point-of-view  statement (POV) and a long-form  explanation  (Explan) that fully expands the POV. An entry in  DebateQA  is shown in  Table 1 .",
            "The overall procedure for curating  DebateQA  is depicted in  Figure 1 . We first source debatable questions. Then, we apply a three-stage semi-automated pipeline to collect partial answers. Lastly, we conduct human annotation on the collected partial answers to finalize the dataset. Quality examinations happen after each step. The following sections will detail these steps.",
            "We collect debatable questions from three distinct sources. First, we repurpose two existing datasets: we select 2,281 annotated controversial questions from DELPHI  Sun et al. ( 2023 )   and a full set of 434 questions from  ConflictingQA   Wan et al. ( 2024 ) . To enrich the existing data, we further manually sourced 1,758 additional debatable questions from the Web (see  Table 8  for detailed sources). We then run a deduplication algorithm (see   A.1  for details) to remove any duplicate questions, resulting in 3,216 questions. The final composition of sourced questions is shown in  Table 9 .",
            "We collect partial answers by leveraging online resources and extracting evidence from relevant web pages. However, the nature of debatable issues necessitates careful processing of these documents, as the Web can contain unveracious content.  To ensure the reliability of our partial answers, we source documents from authoritative top-level domains (TLDs), as listed in  Table 10 . This treatment helps in maintaining the reliability of the sources. We discard questions that have fewer than three documents, resulting in 2,982 questions, each supported by 3-5 of the most relevant documents.  See   A.2  for detailed measures.",
            "Quality examination.   To assess the quality of retrieved documents, we analyze the relevancy between questions and corresponding documents. We calculate the cosine similarity between document chunks and questions. As depicted in  Figure 11 , the average cosine similarity for document trunks is 0.56 and there are no significant outliers, indicating high relevance and minimal noise in the documents, confirming their overall quality for serving as the basis for upcoming steps.",
            "The second stage involves extracting diverse POVs from the retrieved evidence documents.  A POV is a concise statement that reflects the core perspective in addressing the question.  We leverage GPT-4 to tackle this task, by applying the prompt  p POV subscript p POV p_{\\text{POV}} italic_p start_POSTSUBSCRIPT POV end_POSTSUBSCRIPT  described in  Table 11 , which takes the question and the concatenated documents and returns a list of diverse POVs along with the corresponding document indexes where each specific POV is originated.  The document indexes for each POV are later used for expanding the POV.  To avoid exceeding the 128K context window limit of GPT-4, we preprocess the documents by removing meaningless segments and truncating them to 120K tokens if they exceed this length.",
            "The last stage involves expanding the extracted POVs into long-form explanations.  Each explanation should stand as an independent answer, elaborating on the POV and addressing the question from that perspective.  This expansion must be  anchored  to the relevant information presented in the evidence documents pertaining to the specific POV being developed.  We again leverage GPT-4 on this task, utilizing the prompt  p Explan subscript p Explan p_{\\text{Explan}} italic_p start_POSTSUBSCRIPT Explan end_POSTSUBSCRIPT  described in  Table 11 .  This prompt takes three inputs: the question, the target POV to be expanded, and the related documents obtained in the previous stage. The LLM is required to leverage only the information contained within these relevant documents to generate the explanation, minimizing the risk of hallucinations  Zhang et al. ( 2023 ) .  We repeat this step for all the POVs we have collected.  The pseudocode of the pipeline for collecting partial answers is deferred to  Algorithm 1 .",
            "Results and the final dataset.   We recruit three annotators and annotate the full dataset. Inter-annotator agreement (IAA) is measured using Fleiss Kappa  Fleiss et al. ( 1981 ) , yielding scores of   = 0.66  0.66 \\kappa=0.66 italic_ = 0.66  and   = 0.60  0.60 \\kappa=0.60 italic_ = 0.60  for the two annotation tasks, all indicating substantial agreement. We remove 767 partial answers deemed substandard by two or more annotators.  This suggests that GPT-4 generates faithful partial answers with a  93.4 % percent 93.4 93.4\\% 93.4 %  accuracy.  See   A.4  for details.  We employ BERTopic  Grootendorst ( 2022 )  to model the domain distribution of  DebateQA . The result is shown in  Figure 14 .  To reduce computational costs for upcoming evaluation, we split  DebateQA  into two splits: the  test  split with 1,000 randomly sampled questions and the  dev  set containing the remaining instances.",
            "In  Equation 1 ,  PA i = concat  ( POV i , Explan i ) superscript PA i concat superscript POV i superscript Explan i \\text{PA}^{i}=\\texttt{concat}(\\text{POV}^{i},\\text{Explan}^{i}) PA start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT = concat ( POV start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , Explan start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT )  denotes the  i th superscript i th i^{\\text{th}} italic_i start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT  partial answer and  A A A italic_A  denotes the models answer to the debatable question.  chatTemplate  ( ) chatTemplate \\texttt{chatTemplate}() chatTemplate ( )  is a chat template for prompting instruction-tuned LLMs.  Simply put, P.D. represents the aggregate perplexity of generating partial answers from a model answer. Being derived from PPL, a  lower  P.D. signifies better quality, suggesting that the model answer contains larger shares of the partial answers content.",
            "II: Dispute Awareness (D.A.) .  To ascertain if the models answer indicates that the addressed question is debatable, we craft a prompt  p D.A. subscript p D.A. p_{\\text{D.A.}} italic_p start_POSTSUBSCRIPT D.A. end_POSTSUBSCRIPT , as shown in  Table 14 , and use it to prompt an instruction-tuned LLM. This metric is  binary , indicating awareness or lack thereof.",
            "Using pairwise preferences and Spearmans    \\rho italic_  correlation  Zar ( 2005 ) , we find strong agreement among three annotators with   > 0.8  0.8 \\rho>0.8 italic_ > 0.8 , as shown in  Figure 2 . This consensus allows us to assess the alignment of evaluator judgments with human preferences effectively. More details are described in   B.1.1 .",
            "Refer to   B.1.2  for details of these baselines.",
            "Results.   We apply ELO ratings  to establish a human preference ranking and then measure the correlation with metrics rankings using Spearmans    \\rho italic_  and Kendalls    \\tau italic_   Kendall ( 1938 ) .  The average results of the annotators individual correlation are reported in  Table 2 .  We observe that P.D. significantly outperforms DELPHIs metric and G-Eval powered by  GPT-4o.  Using the same small LLM (Phi-3 mini 128k) as the backbone, P.D. outperforms G-Eval by a huge margin,  underscoring P.D.s exceptional alignment with human judgment.  Overall, P.D. is effective and economical.  We further provide case studies in   B.1.3 .",
            "To verify the stability of P.D.  w.r.t.  different backbone models, we configure P.D. with five different LLMs and compute pairwise Kendalls    \\tau italic_  among the resulting rankings, for the same set of model responses collected in   5.1 . The results in  Figure 3  show that the rankings by P.D. with different backbone models are highly consistent.",
            "Remember in  Equation 1 , there is a prompt Please Restate wrapping the model answer. To verify the stability of P.D.  w.r.t.  different prompts, we configure P.D. with five different prompts shown in  Table 13  and compute pairwise Kendalls    \\tau italic_  among the resulting rankings, for the same set of model responses collected in   5.1 . The results in  Figure 4  show that the rankings by P.D. with different prompts have excellent consistency.",
            "To obtain the ground truth of the verdicts of the 500 responses from   5.1 , three authors manually annotate them by assigning binary labels. The annotation has an inter-annotator agreement of 0.79 evaluated by Fleiss Kappa.",
            "With these manually labeled outcomes as the ground truth, we calculate the accuracy, F1 score, and AUROC  Hanley and McNeil ( 1982 )  and Matthews Correlation Coefficient (MCC;  Matthews  1975 )  for D.A. with different backbone models, the Acknowledge metric referenced in DELPHI (refer to   C.1  for details), and a simplified version of D.A. with a zero-shot prompt  p D.A.-ZS subscript p D.A.-ZS p_{\\text{D.A.-ZS}} italic_p start_POSTSUBSCRIPT D.A.-ZS end_POSTSUBSCRIPT  without in-context demonstrations.  The results in  Table 3  demonstrate the superiority of our D.A. metric over the metric from DELPHI, and the necessity of including demonstrations in the prompt  p D.A. subscript p D.A. p_{\\text{D.A.}} italic_p start_POSTSUBSCRIPT D.A. end_POSTSUBSCRIPT . Upon a case study for D.A.-ZS, we find that the Phi-3 mini 128k model occasionally deviates from our instruction by failing to use 0 or 1 for its judgment, thereby diminishing its effectiveness.",
            "Given the robust design of prompt  p D.A. subscript p D.A. p_{\\text{D.A.}} italic_p start_POSTSUBSCRIPT D.A. end_POSTSUBSCRIPT , ensuring D.A.s performance, we advise utilizing the  standard  prompt in  Table 14 . Our focus here is on confirming D.A.s stability across various backbone models.  We set D.A. with five instruction-following LLMs and analyze pairwise agreements for the decision made between each two LLMs based on the model responses collected in   5.1 . The result in  Figure 5  demonstrates the consistency of D.A. among different backbone models.",
            "Evaluators.  We select multiple language models as the backbone for our metrics. For evaluating P.D., we select Qwen2 0.5B and GPT-2 base 117M  Radford et al. ( 2019 )  as  M eval subscript M eval \\mathcal{M}_{\\text{eval}} caligraphic_M start_POSTSUBSCRIPT eval end_POSTSUBSCRIPT .  For assessing D.A., a competent LLM with instruction-following ability is a must. We select Phi-3 medium 128k 14B and Qwen2 1.5B. We select those four models because their performance is showcased in   5.1  and   6.1 , respectively.",
            "Generation configuration.   In the main experiments, when testing the LLMs, we provide the questions with a minimalistic QA prompt, as shown in  Table 15 , which instructs the LLMs without any hint that they are debatable.  We believe this approach more accurately reflects the typical user interaction with chatbots.  For all models, we configure top- p = 0 p 0 p=0 italic_p = 0  to enable greedy decoding and stock chat templates including  M eval subscript M eval \\mathcal{M}_{\\text{eval}} caligraphic_M start_POSTSUBSCRIPT eval end_POSTSUBSCRIPT .",
            "In our main experiments, we use a simple QA prompt ( p basic subscript p basic p_{\\text{basic}} italic_p start_POSTSUBSCRIPT basic end_POSTSUBSCRIPT  in  Table 17 ) that does not highlight the debatable nature of the questions or demand comprehensive answers.  To evaluate the models full potential, we further test five LLMs with more detailed prompts.  We employ three system prompts p basic subscript p basic p_{\\text{basic}} italic_p start_POSTSUBSCRIPT basic end_POSTSUBSCRIPT ,  p comprehensive subscript p comprehensive p_{\\text{comprehensive}} italic_p start_POSTSUBSCRIPT comprehensive end_POSTSUBSCRIPT , and  p detailed subscript p detailed p_{\\text{detailed}} italic_p start_POSTSUBSCRIPT detailed end_POSTSUBSCRIPT to elicit model responses at varying levels of detail, as shown in  Table 17 .  Using 200 randomly sampled questions from  DebateQA - test , we compare the average P.D. and D.A. scores across the five selected LLMs.  The results for these prompts are presented in  Table 5 .  We find that even the relatively simple  p comprehensive subscript p comprehensive p_{\\text{comprehensive}} italic_p start_POSTSUBSCRIPT comprehensive end_POSTSUBSCRIPT  prompt significantly improved the performance for all five models.  We conclude that more specific prompts,  i.e. , inform the model of the debatable nature and request for detailed responses, can enhance LLMs performance in answering debatable questions.  This finding aligns with our expectations and suggests that LLM users can benefit from well-crafted prompts when seeking answers to contentious issues from LLMs.",
            "We examine the effects of two popular RAG strategies,  vanilla RAG   Lewis et al. ( 2020b )  and  ReAct   Yao et al. ( 2023 ) .  In vanilla RAG, we pick the top-10 most relevant documents from the retrieval results via Google Custom Search API 4 4 4 https://developers.google.com/custom-search .  ReAct employs an agent-based approach, leveraging Claude 3.5 Sonnet to interleave reasoning with document retrieval, strategically selecting up to 9 document chunks to improve problem-solving.  Both methods utilize the prompt in  Table 16  to assemble the question and the retrieved trunks.  Refer to   D.1  for details.",
            "Considering that the performance of RAG is highly dependent on the quality of the retrieved documents, we explore whether restricting RAG to utilize trustworthy documents would yield better results. We retrieve only on web pages under trustworthy TLDs listed in  Table 10 .  The results in  Table 7  demonstrate that RAG on trustworthy sources leads to better results.  This highlights the significance of source quality in RAG for debatable QA, emphasizing that utilizing trustworthy documents improves LLM response quality in responding to sensitive topics.",
            "To delve deeper into how response length impacts the two metrics,  we use the prompt Your answer must be around  { num }  tokens. to regulate LLMs to respond with a predetermined length.  However, recognizing that the open-source models adherence to instructions might be inconsistent, we illustrate the correlation between the  actual  average token count in the models responses in  Figure 10 .  We find: (1) models tend to perform better with longer responses. This is likely due to longer answers providing more comprehensive information, enhancing P.D. scores. Furthermore, when tasked with longer answers, models are more prone to acknowledge the debate, which improves D.A. scores. (2) in the main experiment, GPT-4o outperforms GPT-4o-mini and Claude 3.5 Sonnet significantly, as shown in  Table 4 . However, the performance gap narrows when responses are constrained to equal lengths. This suggests that while the knowledge and conversational capabilities of the three models are comparable, GPT-4os propensity for completing longer answers gives it an edge over the other two, which favor brevity.",
            "Retrieving on trustworthy websites.  We only do retrieval on authoritative domains in  Table 10  to assure the trustworthiness of the documents.  Among the selected TLDs,  .gov  and  .edu  domains are not open for personal registration and can only be registered by government or educational institutions. Although  .org ,  .pro , and . info  domains can now be registered by individuals, their content generally remains professional and informative, with fewer advertisements or potentially misleading information.",
            "Implementation of the retrieving process.   To enable finer-grained search results, we apply the GPT-4 model to first transform the original question into several search queries.  We use the  Google search engine  for Web searches and retain only the documents from authoritative TLDs. These documents are then ranked using Bge-Reranker-v2-Gemma  Chen et al. ( 2024 )  and we keep the top-5 documents.  We filter questions with fewer than three documents, as we consider these lack sufficient trustworthy evidence, leaving us with 2,982 questions. The distribution of the number of documents per question is in  Figure 12 .",
            "Quality examination.   We segment each document into 1000-token chunks and average the cosine similarities for each question and corresponding trunks, computed by gte-Qwen2-1.5B-instruct.  The quality of the retrieved documents is illustrated in  Figure 11 .",
            "Algorithm 1  formalize the pipeline of collecting partial answers, where  M M \\mathcal{M} caligraphic_M  is the LLM we use.  M  ( p  ( x , y ) ) M p x y \\mathcal{M}(p(x,y)) caligraphic_M ( italic_p ( italic_x , italic_y ) )  indicates the LLM processing a prompt template  p  ( ) p p() italic_p ( )  populated with inputs  x , y x y x,y italic_x , italic_y .",
            "Prompts.   The prompts we used to generate the POVs and explanations can be found in  Table 11 .  These prompts are carefully crafted to ensure that the generated POVs cover a range of non-overlapping perspectives and provide well-rounded explanations that are grounded in the evidence documents.  After extracting the POVs, we filter out questions with fewer than three perspectives, ensuring that the remaining questions are sufficiently debatable, resulting in 2,941 questions.  The distribution of the number of extracted POVs per question can be found in  Figure 13 .",
            "We recruit three professional annotators from a local data annotation company to verify the partial answers. The payment for this job is above the local minimum wage.  Annotators are given two distinct tasks as outlined in   3.3 . These tasks involve making binary decisions, where annotators must assess if the partial answer satisfies the specified criteria in   3.3 .  After the annotation, we removed 767 partial answers deemed substandard by two or more annotators, resulting in a final dataset of 10,873 partial answers.  We do not remove the original questions corresponding to these partial answers, as those questions still have multiple partial answers.  A domain distribution of the final dataset is shown in  Figure 14 .",
            "Initially, we gather responses from a diverse selection of five LLMs to 100 randomly chosen test questions from  DebateQA - test . The list of LLMs is as follows:  GPT-4o  OpenAI ( 2024b ) , Llama 3 70B  Meta ( 2024 ) , Phi-3 Small 8k  Abdin et al. ( 2024 ) , Zephyr 7B beta  Tunstall et al. ( 2023 ) , and Qwen1.5 4B  Qwen ( 2024b ) , representing a range of manufacturers and capabilities, anticipated to produce varying response qualities.  We configure the LLMs as described in   7.1  to solicit answers, resulting in 500 answers.  Subsequently, we engage three annotators to record their preferences among the model answers.  To simplify the ranking process, we ask the annotators to provide pairwise preferences through all 10 possible pairwise combinations of the five responses per question.  The annotators need to provide a preference based on the following criteria:",
            "The prompts for P.D.s baseline metrics can be found  Table 12 .",
            "Direct-Score . Direct-Score is  basic  prompt-based evaluation metric. We employ a straightforward prompt that requires the model to assign a 1-5 Likert scale score to the model response using the  same  instruction we present to human annotators. The prompt  p DS subscript p DS p_{\\text{DS}} italic_p start_POSTSUBSCRIPT DS end_POSTSUBSCRIPT  is depicted in  Table 12 .",
            "G-Eval   Liu et al. ( 2023 ) . G-Eval is a  strong  prompt-based evaluation framework that assesses the quality of generated texts by incorporating chain-of-thoughts (CoT)  Wei et al. ( 2022 )  and a form-filling paradigm. By providing a prompt with a task introduction and evaluation criteria, G-Eval generates detailed evaluation steps and utilizes these steps along with the generated CoT to score the texts. We apply G-Eval using the  same  scoring criteria provided to humans.  The prompt  p G-Eval subscript p G-Eval p_{\\text{G-Eval}} italic_p start_POSTSUBSCRIPT G-Eval end_POSTSUBSCRIPT  behind G-Eval can be found in  Table 12 .",
            "Num-of-POVs .  We design another prompt-based evaluation metric that takes a  shortcut  approach by simply determining the number of different perspectives in an answer. This metric can be considered an improved metric over the Comprehensiveness Answer Rate metric introduced in the DELPHI paper, as it transcends the binary assessment of the original, which solely determines if an answer includes diverse and opposing viewpoints. The prompt  p NoP subscript p NoP p_{\\text{NoP}} italic_p start_POSTSUBSCRIPT NoP end_POSTSUBSCRIPT  is shown in  Table 12 .",
            "As in  Table 13 , we use five different prompts to show that P.D. is stable  w.r.t.  prompts.",
            "The exact prompt for D.A. is shown in  Table 14 .",
            "Vanilla RAG.   We augment the LLMs with  LangChain . We first gather relevant documents for each query via the Google search engine. The top 10 URLs from the search are saved. The retrieved URLs then undergo a series of actions: (1) content retrieval using the  WebBasedLoader ; (2) chunking to roughly 2000-character using the  RecursiveCharacterTextSplitter , and (3) dense retrieval  Karpukhin et al. ( 2020 )  of the  top-10  most relevant chunks based on cosine similarity on embeddings using the gte-Qwen2-1.5B-instruct  Li et al. ( 2023 ); Qwen ( 2024a )  embedder.  These selected document chunks, along with the question, are compiled into a comprehensive prompt, as depicted in  Table 16 , which is then provided to LLMs for generating responses.",
            "We take the case of GPT-4o  OpenAI ( 2024b )  to investigate the lingering deficiencies of advanced LLMs, a case study is provided in  Table 18 .  Our main findings are:",
            "Taking Qwen2 7B  Qwen ( 2024a )  as a case study in  Table 19 , we pinpoint three main deficiencies typically found in the responses of models with moderate capabilities:",
            "Perspective Diversity (P.D.) 6 6 6 Here, we use  A A A italic_A  to denote  chatTemplate  ( concat  ( A , Please restate. ) ) chatTemplate concat A Please restate. \\texttt{chatTemplate}(\\texttt{concat}(A,\\text{``Please restate.''})) chatTemplate ( concat ( italic_A , Please restate. ) )  in  Equation 1  for simplicity. :"
        ]
    },
    "id_table_2": {
        "caption": "Table 2:   Alignment of various evaluation metrics with human preferences. The top-performing metric is highlighted in  bold  and the runner-up is  underlined .    \\rho italic_ : Spearmans    \\rho italic_ ,    \\tau italic_ : Kendalls    \\tau italic_ .",
        "table": "S5.T2.2",
        "footnotes": [],
        "references": [
            "We collect partial answers by leveraging online resources and extracting evidence from relevant web pages. However, the nature of debatable issues necessitates careful processing of these documents, as the Web can contain unveracious content.  To ensure the reliability of our partial answers, we source documents from authoritative top-level domains (TLDs), as listed in  Table 10 . This treatment helps in maintaining the reliability of the sources. We discard questions that have fewer than three documents, resulting in 2,982 questions, each supported by 3-5 of the most relevant documents.  See   A.2  for detailed measures.",
            "Using pairwise preferences and Spearmans    \\rho italic_  correlation  Zar ( 2005 ) , we find strong agreement among three annotators with   > 0.8  0.8 \\rho>0.8 italic_ > 0.8 , as shown in  Figure 2 . This consensus allows us to assess the alignment of evaluator judgments with human preferences effectively. More details are described in   B.1.1 .",
            "Refer to   B.1.2  for details of these baselines.",
            "Results.   We apply ELO ratings  to establish a human preference ranking and then measure the correlation with metrics rankings using Spearmans    \\rho italic_  and Kendalls    \\tau italic_   Kendall ( 1938 ) .  The average results of the annotators individual correlation are reported in  Table 2 .  We observe that P.D. significantly outperforms DELPHIs metric and G-Eval powered by  GPT-4o.  Using the same small LLM (Phi-3 mini 128k) as the backbone, P.D. outperforms G-Eval by a huge margin,  underscoring P.D.s exceptional alignment with human judgment.  Overall, P.D. is effective and economical.  We further provide case studies in   B.1.3 .",
            "Implementation of the retrieving process.   To enable finer-grained search results, we apply the GPT-4 model to first transform the original question into several search queries.  We use the  Google search engine  for Web searches and retain only the documents from authoritative TLDs. These documents are then ranked using Bge-Reranker-v2-Gemma  Chen et al. ( 2024 )  and we keep the top-5 documents.  We filter questions with fewer than three documents, as we consider these lack sufficient trustworthy evidence, leaving us with 2,982 questions. The distribution of the number of documents per question is in  Figure 12 .",
            "Post-annotation, we determine inter-annotator consistency using Spearmans    \\rho italic_  correlation. The outcomes, depicted in  Figure 2 , reveal strong agreement (  > 0.8  0.8 \\rho>0.8 italic_ > 0.8 ) among annotators, suggesting a shared understanding of a  good  answer.",
            "The prompts for P.D.s baseline metrics can be found  Table 12 .",
            "Direct-Score . Direct-Score is  basic  prompt-based evaluation metric. We employ a straightforward prompt that requires the model to assign a 1-5 Likert scale score to the model response using the  same  instruction we present to human annotators. The prompt  p DS subscript p DS p_{\\text{DS}} italic_p start_POSTSUBSCRIPT DS end_POSTSUBSCRIPT  is depicted in  Table 12 .",
            "G-Eval   Liu et al. ( 2023 ) . G-Eval is a  strong  prompt-based evaluation framework that assesses the quality of generated texts by incorporating chain-of-thoughts (CoT)  Wei et al. ( 2022 )  and a form-filling paradigm. By providing a prompt with a task introduction and evaluation criteria, G-Eval generates detailed evaluation steps and utilizes these steps along with the generated CoT to score the texts. We apply G-Eval using the  same  scoring criteria provided to humans.  The prompt  p G-Eval subscript p G-Eval p_{\\text{G-Eval}} italic_p start_POSTSUBSCRIPT G-Eval end_POSTSUBSCRIPT  behind G-Eval can be found in  Table 12 .",
            "Num-of-POVs .  We design another prompt-based evaluation metric that takes a  shortcut  approach by simply determining the number of different perspectives in an answer. This metric can be considered an improved metric over the Comprehensiveness Answer Rate metric introduced in the DELPHI paper, as it transcends the binary assessment of the original, which solely determines if an answer includes diverse and opposing viewpoints. The prompt  p NoP subscript p NoP p_{\\text{NoP}} italic_p start_POSTSUBSCRIPT NoP end_POSTSUBSCRIPT  is shown in  Table 12 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:   Based on human annotations as the ground truth, we assess D.A.s accuracy. D.A.-ZS: D.A. with a zero-shot prompt  p D.A.-ZS subscript p D.A.-ZS p_{\\text{D.A.-ZS}} italic_p start_POSTSUBSCRIPT D.A.-ZS end_POSTSUBSCRIPT , Ack.: the Acknowledge metric from DELPHI.",
        "table": "S6.T3.3",
        "footnotes": [],
        "references": [
            "Quality examination.  We verify the quality of the collected POVs on comprehensiveness  w.r.t.  the documents and distinctiveness among themselves. For comprehensiveness, we ensure all valid perspectives from retrieved documents are captured, with 90.4% coverage verified manually. Distinctiveness is assured by removing duplicated POVs manually. For more details, refer to   A.3 .",
            "Results.   We apply ELO ratings  to establish a human preference ranking and then measure the correlation with metrics rankings using Spearmans    \\rho italic_  and Kendalls    \\tau italic_   Kendall ( 1938 ) .  The average results of the annotators individual correlation are reported in  Table 2 .  We observe that P.D. significantly outperforms DELPHIs metric and G-Eval powered by  GPT-4o.  Using the same small LLM (Phi-3 mini 128k) as the backbone, P.D. outperforms G-Eval by a huge margin,  underscoring P.D.s exceptional alignment with human judgment.  Overall, P.D. is effective and economical.  We further provide case studies in   B.1.3 .",
            "To verify the stability of P.D.  w.r.t.  different backbone models, we configure P.D. with five different LLMs and compute pairwise Kendalls    \\tau italic_  among the resulting rankings, for the same set of model responses collected in   5.1 . The results in  Figure 3  show that the rankings by P.D. with different backbone models are highly consistent.",
            "Remember in  Equation 1 , there is a prompt Please Restate wrapping the model answer. To verify the stability of P.D.  w.r.t.  different prompts, we configure P.D. with five different prompts shown in  Table 13  and compute pairwise Kendalls    \\tau italic_  among the resulting rankings, for the same set of model responses collected in   5.1 . The results in  Figure 4  show that the rankings by P.D. with different prompts have excellent consistency.",
            "With these manually labeled outcomes as the ground truth, we calculate the accuracy, F1 score, and AUROC  Hanley and McNeil ( 1982 )  and Matthews Correlation Coefficient (MCC;  Matthews  1975 )  for D.A. with different backbone models, the Acknowledge metric referenced in DELPHI (refer to   C.1  for details), and a simplified version of D.A. with a zero-shot prompt  p D.A.-ZS subscript p D.A.-ZS p_{\\text{D.A.-ZS}} italic_p start_POSTSUBSCRIPT D.A.-ZS end_POSTSUBSCRIPT  without in-context demonstrations.  The results in  Table 3  demonstrate the superiority of our D.A. metric over the metric from DELPHI, and the necessity of including demonstrations in the prompt  p D.A. subscript p D.A. p_{\\text{D.A.}} italic_p start_POSTSUBSCRIPT D.A. end_POSTSUBSCRIPT . Upon a case study for D.A.-ZS, we find that the Phi-3 mini 128k model occasionally deviates from our instruction by failing to use 0 or 1 for its judgment, thereby diminishing its effectiveness.",
            "Prompts.   The prompts we used to generate the POVs and explanations can be found in  Table 11 .  These prompts are carefully crafted to ensure that the generated POVs cover a range of non-overlapping perspectives and provide well-rounded explanations that are grounded in the evidence documents.  After extracting the POVs, we filter out questions with fewer than three perspectives, ensuring that the remaining questions are sufficiently debatable, resulting in 2,941 questions.  The distribution of the number of extracted POVs per question can be found in  Figure 13 .",
            "We recruit three professional annotators from a local data annotation company to verify the partial answers. The payment for this job is above the local minimum wage.  Annotators are given two distinct tasks as outlined in   3.3 . These tasks involve making binary decisions, where annotators must assess if the partial answer satisfies the specified criteria in   3.3 .  After the annotation, we removed 767 partial answers deemed substandard by two or more annotators, resulting in a final dataset of 10,873 partial answers.  We do not remove the original questions corresponding to these partial answers, as those questions still have multiple partial answers.  A domain distribution of the final dataset is shown in  Figure 14 .",
            "As in  Table 13 , we use five different prompts to show that P.D. is stable  w.r.t.  prompts."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:    Main results of P.D. and D.A. for LLMs on  DebateQA - test . Avg. Len.: average length of the answers, GPT-2: GPT-2 (117M), Phi-3 M.: Phi-3 medium 128k, Norm. Rank: normalized average rank of different  M eval subscript M eval \\mathcal{M}_{\\text{eval}} caligraphic_M start_POSTSUBSCRIPT eval end_POSTSUBSCRIPT . The   best  and   worst  results of each metric ( w.r.t.  a specific  M eval subscript M eval \\mathcal{M}_{\\text{eval}} caligraphic_M start_POSTSUBSCRIPT eval end_POSTSUBSCRIPT ) are highlighted.",
        "table": "S7.T4.16",
        "footnotes": [],
        "references": [
            "Results and the final dataset.   We recruit three annotators and annotate the full dataset. Inter-annotator agreement (IAA) is measured using Fleiss Kappa  Fleiss et al. ( 1981 ) , yielding scores of   = 0.66  0.66 \\kappa=0.66 italic_ = 0.66  and   = 0.60  0.60 \\kappa=0.60 italic_ = 0.60  for the two annotation tasks, all indicating substantial agreement. We remove 767 partial answers deemed substandard by two or more annotators.  This suggests that GPT-4 generates faithful partial answers with a  93.4 % percent 93.4 93.4\\% 93.4 %  accuracy.  See   A.4  for details.  We employ BERTopic  Grootendorst ( 2022 )  to model the domain distribution of  DebateQA . The result is shown in  Figure 14 .  To reduce computational costs for upcoming evaluation, we split  DebateQA  into two splits: the  test  split with 1,000 randomly sampled questions and the  dev  set containing the remaining instances.",
            "II: Dispute Awareness (D.A.) .  To ascertain if the models answer indicates that the addressed question is debatable, we craft a prompt  p D.A. subscript p D.A. p_{\\text{D.A.}} italic_p start_POSTSUBSCRIPT D.A. end_POSTSUBSCRIPT , as shown in  Table 14 , and use it to prompt an instruction-tuned LLM. This metric is  binary , indicating awareness or lack thereof.",
            "Remember in  Equation 1 , there is a prompt Please Restate wrapping the model answer. To verify the stability of P.D.  w.r.t.  different prompts, we configure P.D. with five different prompts shown in  Table 13  and compute pairwise Kendalls    \\tau italic_  among the resulting rankings, for the same set of model responses collected in   5.1 . The results in  Figure 4  show that the rankings by P.D. with different prompts have excellent consistency.",
            "Given the robust design of prompt  p D.A. subscript p D.A. p_{\\text{D.A.}} italic_p start_POSTSUBSCRIPT D.A. end_POSTSUBSCRIPT , ensuring D.A.s performance, we advise utilizing the  standard  prompt in  Table 14 . Our focus here is on confirming D.A.s stability across various backbone models.  We set D.A. with five instruction-following LLMs and analyze pairwise agreements for the decision made between each two LLMs based on the model responses collected in   5.1 . The result in  Figure 5  demonstrates the consistency of D.A. among different backbone models.",
            "Main evaluation results  can be found in   Table 4 .  We summarize our key findings as follows:",
            "To delve deeper into how response length impacts the two metrics,  we use the prompt Your answer must be around  { num }  tokens. to regulate LLMs to respond with a predetermined length.  However, recognizing that the open-source models adherence to instructions might be inconsistent, we illustrate the correlation between the  actual  average token count in the models responses in  Figure 10 .  We find: (1) models tend to perform better with longer responses. This is likely due to longer answers providing more comprehensive information, enhancing P.D. scores. Furthermore, when tasked with longer answers, models are more prone to acknowledge the debate, which improves D.A. scores. (2) in the main experiment, GPT-4o outperforms GPT-4o-mini and Claude 3.5 Sonnet significantly, as shown in  Table 4 . However, the performance gap narrows when responses are constrained to equal lengths. This suggests that while the knowledge and conversational capabilities of the three models are comparable, GPT-4os propensity for completing longer answers gives it an edge over the other two, which favor brevity.",
            "We recruit three professional annotators from a local data annotation company to verify the partial answers. The payment for this job is above the local minimum wage.  Annotators are given two distinct tasks as outlined in   3.3 . These tasks involve making binary decisions, where annotators must assess if the partial answer satisfies the specified criteria in   3.3 .  After the annotation, we removed 767 partial answers deemed substandard by two or more annotators, resulting in a final dataset of 10,873 partial answers.  We do not remove the original questions corresponding to these partial answers, as those questions still have multiple partial answers.  A domain distribution of the final dataset is shown in  Figure 14 .",
            "The exact prompt for D.A. is shown in  Table 14 ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:   Effect of various prompts on P.D. scores and D.A. scores.  p b subscript p b p_{\\text{b}} italic_p start_POSTSUBSCRIPT b end_POSTSUBSCRIPT :  p basic subscript p basic p_{\\text{basic}} italic_p start_POSTSUBSCRIPT basic end_POSTSUBSCRIPT ,  p c subscript p c p_{\\text{c}} italic_p start_POSTSUBSCRIPT c end_POSTSUBSCRIPT :  p comprehensive subscript p comprehensive p_{\\text{comprehensive}} italic_p start_POSTSUBSCRIPT comprehensive end_POSTSUBSCRIPT ,  p d subscript p d p_{\\text{d}} italic_p start_POSTSUBSCRIPT d end_POSTSUBSCRIPT :  p detailed subscript p detailed p_{\\text{detailed}} italic_p start_POSTSUBSCRIPT detailed end_POSTSUBSCRIPT .",
        "table": "S7.T5.8",
        "footnotes": [],
        "references": [
            "Evaluation metrics.   Although the above two criteria resonate with those utilized in DELPHI  Sun et al. ( 2023 ) , we distinguish ourselves by formalizing these criteria into more sophisticated quantifiable metrics. Our methodology excels by integrating the partial answer feature of  DebateQA  and outperforms DELPHIs approach by a huge margin, which will be later elaborated in   5  and   6 .  Please note that while the following two metrics both employ backbone LLMs, they may  differ ; for brevity, we refer to both as  M eval subscript M eval \\mathcal{M}_{\\text{eval}} caligraphic_M start_POSTSUBSCRIPT eval end_POSTSUBSCRIPT .",
            "To verify the stability of P.D.  w.r.t.  different backbone models, we configure P.D. with five different LLMs and compute pairwise Kendalls    \\tau italic_  among the resulting rankings, for the same set of model responses collected in   5.1 . The results in  Figure 3  show that the rankings by P.D. with different backbone models are highly consistent.",
            "Remember in  Equation 1 , there is a prompt Please Restate wrapping the model answer. To verify the stability of P.D.  w.r.t.  different prompts, we configure P.D. with five different prompts shown in  Table 13  and compute pairwise Kendalls    \\tau italic_  among the resulting rankings, for the same set of model responses collected in   5.1 . The results in  Figure 4  show that the rankings by P.D. with different prompts have excellent consistency.",
            "To obtain the ground truth of the verdicts of the 500 responses from   5.1 , three authors manually annotate them by assigning binary labels. The annotation has an inter-annotator agreement of 0.79 evaluated by Fleiss Kappa.",
            "Given the robust design of prompt  p D.A. subscript p D.A. p_{\\text{D.A.}} italic_p start_POSTSUBSCRIPT D.A. end_POSTSUBSCRIPT , ensuring D.A.s performance, we advise utilizing the  standard  prompt in  Table 14 . Our focus here is on confirming D.A.s stability across various backbone models.  We set D.A. with five instruction-following LLMs and analyze pairwise agreements for the decision made between each two LLMs based on the model responses collected in   5.1 . The result in  Figure 5  demonstrates the consistency of D.A. among different backbone models.",
            "Evaluators.  We select multiple language models as the backbone for our metrics. For evaluating P.D., we select Qwen2 0.5B and GPT-2 base 117M  Radford et al. ( 2019 )  as  M eval subscript M eval \\mathcal{M}_{\\text{eval}} caligraphic_M start_POSTSUBSCRIPT eval end_POSTSUBSCRIPT .  For assessing D.A., a competent LLM with instruction-following ability is a must. We select Phi-3 medium 128k 14B and Qwen2 1.5B. We select those four models because their performance is showcased in   5.1  and   6.1 , respectively.",
            "Generation configuration.   In the main experiments, when testing the LLMs, we provide the questions with a minimalistic QA prompt, as shown in  Table 15 , which instructs the LLMs without any hint that they are debatable.  We believe this approach more accurately reflects the typical user interaction with chatbots.  For all models, we configure top- p = 0 p 0 p=0 italic_p = 0  to enable greedy decoding and stock chat templates including  M eval subscript M eval \\mathcal{M}_{\\text{eval}} caligraphic_M start_POSTSUBSCRIPT eval end_POSTSUBSCRIPT .",
            "In our main experiments, we use a simple QA prompt ( p basic subscript p basic p_{\\text{basic}} italic_p start_POSTSUBSCRIPT basic end_POSTSUBSCRIPT  in  Table 17 ) that does not highlight the debatable nature of the questions or demand comprehensive answers.  To evaluate the models full potential, we further test five LLMs with more detailed prompts.  We employ three system prompts p basic subscript p basic p_{\\text{basic}} italic_p start_POSTSUBSCRIPT basic end_POSTSUBSCRIPT ,  p comprehensive subscript p comprehensive p_{\\text{comprehensive}} italic_p start_POSTSUBSCRIPT comprehensive end_POSTSUBSCRIPT , and  p detailed subscript p detailed p_{\\text{detailed}} italic_p start_POSTSUBSCRIPT detailed end_POSTSUBSCRIPT to elicit model responses at varying levels of detail, as shown in  Table 17 .  Using 200 randomly sampled questions from  DebateQA - test , we compare the average P.D. and D.A. scores across the five selected LLMs.  The results for these prompts are presented in  Table 5 .  We find that even the relatively simple  p comprehensive subscript p comprehensive p_{\\text{comprehensive}} italic_p start_POSTSUBSCRIPT comprehensive end_POSTSUBSCRIPT  prompt significantly improved the performance for all five models.  We conclude that more specific prompts,  i.e. , inform the model of the debatable nature and request for detailed responses, can enhance LLMs performance in answering debatable questions.  This finding aligns with our expectations and suggests that LLM users can benefit from well-crafted prompts when seeking answers to contentious issues from LLMs."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:   Effect of two RAG strategies on P.D. scores.",
        "table": "S7.T6.2",
        "footnotes": [],
        "references": [
            "Evaluation metrics.   Although the above two criteria resonate with those utilized in DELPHI  Sun et al. ( 2023 ) , we distinguish ourselves by formalizing these criteria into more sophisticated quantifiable metrics. Our methodology excels by integrating the partial answer feature of  DebateQA  and outperforms DELPHIs approach by a huge margin, which will be later elaborated in   5  and   6 .  Please note that while the following two metrics both employ backbone LLMs, they may  differ ; for brevity, we refer to both as  M eval subscript M eval \\mathcal{M}_{\\text{eval}} caligraphic_M start_POSTSUBSCRIPT eval end_POSTSUBSCRIPT .",
            "Evaluators.  We select multiple language models as the backbone for our metrics. For evaluating P.D., we select Qwen2 0.5B and GPT-2 base 117M  Radford et al. ( 2019 )  as  M eval subscript M eval \\mathcal{M}_{\\text{eval}} caligraphic_M start_POSTSUBSCRIPT eval end_POSTSUBSCRIPT .  For assessing D.A., a competent LLM with instruction-following ability is a must. We select Phi-3 medium 128k 14B and Qwen2 1.5B. We select those four models because their performance is showcased in   5.1  and   6.1 , respectively.",
            "Correlation between the P.D. and D.A. metrics.   To investigate the correlation between P.D. and D.A. for the tested models, we plot a visualization of the results in  Figure 6 , which aids in understanding how these two metrics relate to each other across various models.",
            "We examine the effects of two popular RAG strategies,  vanilla RAG   Lewis et al. ( 2020b )  and  ReAct   Yao et al. ( 2023 ) .  In vanilla RAG, we pick the top-10 most relevant documents from the retrieval results via Google Custom Search API 4 4 4 https://developers.google.com/custom-search .  ReAct employs an agent-based approach, leveraging Claude 3.5 Sonnet to interleave reasoning with document retrieval, strategically selecting up to 9 document chunks to improve problem-solving.  Both methods utilize the prompt in  Table 16  to assemble the question and the retrieved trunks.  Refer to   D.1  for details.",
            "We assess the performance of five LLMs by evaluating their responses to 100 randomly sampled debatable questions from  DebateQA - test  using two distinct RAG strategies. With the results detailed in  Table 6 , we conclude:",
            "Vanilla RAG.   We augment the LLMs with  LangChain . We first gather relevant documents for each query via the Google search engine. The top 10 URLs from the search are saved. The retrieved URLs then undergo a series of actions: (1) content retrieval using the  WebBasedLoader ; (2) chunking to roughly 2000-character using the  RecursiveCharacterTextSplitter , and (3) dense retrieval  Karpukhin et al. ( 2020 )  of the  top-10  most relevant chunks based on cosine similarity on embeddings using the gte-Qwen2-1.5B-instruct  Li et al. ( 2023 ); Qwen ( 2024a )  embedder.  These selected document chunks, along with the question, are compiled into a comprehensive prompt, as depicted in  Table 16 , which is then provided to LLMs for generating responses."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:   Effect of RAG sources on P.D. scores. RAG w. T. Docs: RAG using trustworthy documents.",
        "table": "S7.T7.2",
        "footnotes": [],
        "references": [
            "where the  k k k italic_k  is the proportional coefficient and  Z-norm  ( ) Z-norm \\texttt{Z-norm}() Z-norm ( )  refers to z-normalization. We show the ranking of the weighted average scores for the models at different  k k k italic_k  in  Figure 7 .",
            "In our main experiments, we use a simple QA prompt ( p basic subscript p basic p_{\\text{basic}} italic_p start_POSTSUBSCRIPT basic end_POSTSUBSCRIPT  in  Table 17 ) that does not highlight the debatable nature of the questions or demand comprehensive answers.  To evaluate the models full potential, we further test five LLMs with more detailed prompts.  We employ three system prompts p basic subscript p basic p_{\\text{basic}} italic_p start_POSTSUBSCRIPT basic end_POSTSUBSCRIPT ,  p comprehensive subscript p comprehensive p_{\\text{comprehensive}} italic_p start_POSTSUBSCRIPT comprehensive end_POSTSUBSCRIPT , and  p detailed subscript p detailed p_{\\text{detailed}} italic_p start_POSTSUBSCRIPT detailed end_POSTSUBSCRIPT to elicit model responses at varying levels of detail, as shown in  Table 17 .  Using 200 randomly sampled questions from  DebateQA - test , we compare the average P.D. and D.A. scores across the five selected LLMs.  The results for these prompts are presented in  Table 5 .  We find that even the relatively simple  p comprehensive subscript p comprehensive p_{\\text{comprehensive}} italic_p start_POSTSUBSCRIPT comprehensive end_POSTSUBSCRIPT  prompt significantly improved the performance for all five models.  We conclude that more specific prompts,  i.e. , inform the model of the debatable nature and request for detailed responses, can enhance LLMs performance in answering debatable questions.  This finding aligns with our expectations and suggests that LLM users can benefit from well-crafted prompts when seeking answers to contentious issues from LLMs.",
            "Considering that the performance of RAG is highly dependent on the quality of the retrieved documents, we explore whether restricting RAG to utilize trustworthy documents would yield better results. We retrieve only on web pages under trustworthy TLDs listed in  Table 10 .  The results in  Table 7  demonstrate that RAG on trustworthy sources leads to better results.  This highlights the significance of source quality in RAG for debatable QA, emphasizing that utilizing trustworthy documents improves LLM response quality in responding to sensitive topics.",
            "Initially, we gather responses from a diverse selection of five LLMs to 100 randomly chosen test questions from  DebateQA - test . The list of LLMs is as follows:  GPT-4o  OpenAI ( 2024b ) , Llama 3 70B  Meta ( 2024 ) , Phi-3 Small 8k  Abdin et al. ( 2024 ) , Zephyr 7B beta  Tunstall et al. ( 2023 ) , and Qwen1.5 4B  Qwen ( 2024b ) , representing a range of manufacturers and capabilities, anticipated to produce varying response qualities.  We configure the LLMs as described in   7.1  to solicit answers, resulting in 500 answers.  Subsequently, we engage three annotators to record their preferences among the model answers.  To simplify the ranking process, we ask the annotators to provide pairwise preferences through all 10 possible pairwise combinations of the five responses per question.  The annotators need to provide a preference based on the following criteria:",
            "Equation 7  shows that the change in probability    P 1  superscript P 1 \\Delta P^{1} roman_ italic_P start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT  can be approximated using the change in P.D.    P.D.  P.D. \\Delta\\text{P.D.} roman_ P.D. , and the effect is exponential.  For example, when taking  n = 3 n 3 n=3 italic_n = 3  (given the fact that the average number of partial answers in  DebateQA    3.7 absent 3.7 \\approx 3.7  3.7 ) and    P.D. =  2  P.D. 2 \\Delta\\text{P.D.}=-2 roman_ P.D. = - 2 , it suggests that the approximated probability of generating one partial answer under  A 2 subscript A 2 A_{2} italic_A start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  is  1.95 1.95 1.95 1.95  times higher than under  A 1 subscript A 1 A_{1} italic_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT .  In other words, the backbone language model finds it nearly twice as easy to recover the partial answer from  A 2 subscript A 2 A_{2} italic_A start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  due to the reduction of 2 in the P.D. score."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:    Detailed sources in URL for debatable questions sourced from the web. During our experiment (April 2024), these resources are accessible. We will make our dataset publicly available for future research.",
        "table": "A1.T8.1",
        "footnotes": [],
        "references": [
            "We collect debatable questions from three distinct sources. First, we repurpose two existing datasets: we select 2,281 annotated controversial questions from DELPHI  Sun et al. ( 2023 )   and a full set of 434 questions from  ConflictingQA   Wan et al. ( 2024 ) . To enrich the existing data, we further manually sourced 1,758 additional debatable questions from the Web (see  Table 8  for detailed sources). We then run a deduplication algorithm (see   A.1  for details) to remove any duplicate questions, resulting in 3,216 questions. The final composition of sourced questions is shown in  Table 9 .",
            "In the main experiments, we configure all LLMs to use greedy decoding, which, while straightforward, can restrict the diversity and creativity of LLM outputs  Holtzman et al. ( 2020 ) .  To assess the impact of various decoding hyperparameters using sampling decoding on models performance, we select a range of five different temperatures and top- p p p italic_p  values.  The results of P.D. and D.A. scores are presented in  Figure 8  and  Figure 9 , respectively.  The plots indicate that higher temperature and top- p p p italic_p  values generally prompt LLMs to produce more well-rounded responses to debatable questions, enhancing performance on both metrics. This suggests that sampling configurations that allow for a broader selection of lower-probability tokens can lead to improved outcomes.",
            "In the end, we select 1,758 debatable questions from 9 websites. Details on the number of questions from each website can be found in  Table 8 .",
            "We take the case of GPT-4o  OpenAI ( 2024b )  to investigate the lingering deficiencies of advanced LLMs, a case study is provided in  Table 18 .  Our main findings are:"
        ]
    },
    "id_table_9": {
        "caption": "Table 9:   Sources distribution of  DebateQA .",
        "table": "A1.T9.1",
        "footnotes": [],
        "references": [
            "We collect debatable questions from three distinct sources. First, we repurpose two existing datasets: we select 2,281 annotated controversial questions from DELPHI  Sun et al. ( 2023 )   and a full set of 434 questions from  ConflictingQA   Wan et al. ( 2024 ) . To enrich the existing data, we further manually sourced 1,758 additional debatable questions from the Web (see  Table 8  for detailed sources). We then run a deduplication algorithm (see   A.1  for details) to remove any duplicate questions, resulting in 3,216 questions. The final composition of sourced questions is shown in  Table 9 .",
            "In the main experiments, we configure all LLMs to use greedy decoding, which, while straightforward, can restrict the diversity and creativity of LLM outputs  Holtzman et al. ( 2020 ) .  To assess the impact of various decoding hyperparameters using sampling decoding on models performance, we select a range of five different temperatures and top- p p p italic_p  values.  The results of P.D. and D.A. scores are presented in  Figure 8  and  Figure 9 , respectively.  The plots indicate that higher temperature and top- p p p italic_p  values generally prompt LLMs to produce more well-rounded responses to debatable questions, enhancing performance on both metrics. This suggests that sampling configurations that allow for a broader selection of lower-probability tokens can lead to improved outcomes.",
            "The questions in  DebateQA  are collected from three sources, as detailed in  Table 9 .",
            "Taking Qwen2 7B  Qwen ( 2024a )  as a case study in  Table 19 , we pinpoint three main deficiencies typically found in the responses of models with moderate capabilities:"
        ]
    },
    "id_table_10": {
        "caption": "Table 10:   List of selected top-level domains (TLDs) we considered trustworthy. Closed for registration indicates that only authoritative entities can register a domain under these TLDs, ensuring high credibility.",
        "table": "A1.T10.1",
        "footnotes": [],
        "references": [
            "We collect partial answers by leveraging online resources and extracting evidence from relevant web pages. However, the nature of debatable issues necessitates careful processing of these documents, as the Web can contain unveracious content.  To ensure the reliability of our partial answers, we source documents from authoritative top-level domains (TLDs), as listed in  Table 10 . This treatment helps in maintaining the reliability of the sources. We discard questions that have fewer than three documents, resulting in 2,982 questions, each supported by 3-5 of the most relevant documents.  See   A.2  for detailed measures.",
            "Considering that the performance of RAG is highly dependent on the quality of the retrieved documents, we explore whether restricting RAG to utilize trustworthy documents would yield better results. We retrieve only on web pages under trustworthy TLDs listed in  Table 10 .  The results in  Table 7  demonstrate that RAG on trustworthy sources leads to better results.  This highlights the significance of source quality in RAG for debatable QA, emphasizing that utilizing trustworthy documents improves LLM response quality in responding to sensitive topics.",
            "To delve deeper into how response length impacts the two metrics,  we use the prompt Your answer must be around  { num }  tokens. to regulate LLMs to respond with a predetermined length.  However, recognizing that the open-source models adherence to instructions might be inconsistent, we illustrate the correlation between the  actual  average token count in the models responses in  Figure 10 .  We find: (1) models tend to perform better with longer responses. This is likely due to longer answers providing more comprehensive information, enhancing P.D. scores. Furthermore, when tasked with longer answers, models are more prone to acknowledge the debate, which improves D.A. scores. (2) in the main experiment, GPT-4o outperforms GPT-4o-mini and Claude 3.5 Sonnet significantly, as shown in  Table 4 . However, the performance gap narrows when responses are constrained to equal lengths. This suggests that while the knowledge and conversational capabilities of the three models are comparable, GPT-4os propensity for completing longer answers gives it an edge over the other two, which favor brevity.",
            "Retrieving on trustworthy websites.  We only do retrieval on authoritative domains in  Table 10  to assure the trustworthiness of the documents.  Among the selected TLDs,  .gov  and  .edu  domains are not open for personal registration and can only be registered by government or educational institutions. Although  .org ,  .pro , and . info  domains can now be registered by individuals, their content generally remains professional and informative, with fewer advertisements or potentially misleading information."
        ]
    },
    "id_table_11": {
        "caption": "Table 11:    Core prompts for generating partial answers given the question and retrieved documents.  Prompt  p POV subscript p POV p_{\\text{POV}} italic_p start_POSTSUBSCRIPT POV end_POSTSUBSCRIPT  is used to extract points-of-view (POVs) from the corresponding evidence documents  w.r.t.  to the question.  Prompt  p Explan subscript p Explan p_{\\text{Explan}} italic_p start_POSTSUBSCRIPT Explan end_POSTSUBSCRIPT  is used to expand POVs into long-form explanations based on the relevant documents.",
        "table": "A1.T11.2",
        "footnotes": [],
        "references": [
            "Quality examination.   To assess the quality of retrieved documents, we analyze the relevancy between questions and corresponding documents. We calculate the cosine similarity between document chunks and questions. As depicted in  Figure 11 , the average cosine similarity for document trunks is 0.56 and there are no significant outliers, indicating high relevance and minimal noise in the documents, confirming their overall quality for serving as the basis for upcoming steps.",
            "The second stage involves extracting diverse POVs from the retrieved evidence documents.  A POV is a concise statement that reflects the core perspective in addressing the question.  We leverage GPT-4 to tackle this task, by applying the prompt  p POV subscript p POV p_{\\text{POV}} italic_p start_POSTSUBSCRIPT POV end_POSTSUBSCRIPT  described in  Table 11 , which takes the question and the concatenated documents and returns a list of diverse POVs along with the corresponding document indexes where each specific POV is originated.  The document indexes for each POV are later used for expanding the POV.  To avoid exceeding the 128K context window limit of GPT-4, we preprocess the documents by removing meaningless segments and truncating them to 120K tokens if they exceed this length.",
            "The last stage involves expanding the extracted POVs into long-form explanations.  Each explanation should stand as an independent answer, elaborating on the POV and addressing the question from that perspective.  This expansion must be  anchored  to the relevant information presented in the evidence documents pertaining to the specific POV being developed.  We again leverage GPT-4 on this task, utilizing the prompt  p Explan subscript p Explan p_{\\text{Explan}} italic_p start_POSTSUBSCRIPT Explan end_POSTSUBSCRIPT  described in  Table 11 .  This prompt takes three inputs: the question, the target POV to be expanded, and the related documents obtained in the previous stage. The LLM is required to leverage only the information contained within these relevant documents to generate the explanation, minimizing the risk of hallucinations  Zhang et al. ( 2023 ) .  We repeat this step for all the POVs we have collected.  The pseudocode of the pipeline for collecting partial answers is deferred to  Algorithm 1 .",
            "Quality examination.   We segment each document into 1000-token chunks and average the cosine similarities for each question and corresponding trunks, computed by gte-Qwen2-1.5B-instruct.  The quality of the retrieved documents is illustrated in  Figure 11 .",
            "Prompts.   The prompts we used to generate the POVs and explanations can be found in  Table 11 .  These prompts are carefully crafted to ensure that the generated POVs cover a range of non-overlapping perspectives and provide well-rounded explanations that are grounded in the evidence documents.  After extracting the POVs, we filter out questions with fewer than three perspectives, ensuring that the remaining questions are sufficiently debatable, resulting in 2,941 questions.  The distribution of the number of extracted POVs per question can be found in  Figure 13 ."
        ]
    },
    "id_table_12": {
        "caption": "Table 12:    Prompts for assessing answers to debatable questions in baseline methods of the P.D. metric.",
        "table": "A2.T12.3",
        "footnotes": [],
        "references": [
            "Implementation of the retrieving process.   To enable finer-grained search results, we apply the GPT-4 model to first transform the original question into several search queries.  We use the  Google search engine  for Web searches and retain only the documents from authoritative TLDs. These documents are then ranked using Bge-Reranker-v2-Gemma  Chen et al. ( 2024 )  and we keep the top-5 documents.  We filter questions with fewer than three documents, as we consider these lack sufficient trustworthy evidence, leaving us with 2,982 questions. The distribution of the number of documents per question is in  Figure 12 .",
            "The prompts for P.D.s baseline metrics can be found  Table 12 .",
            "Direct-Score . Direct-Score is  basic  prompt-based evaluation metric. We employ a straightforward prompt that requires the model to assign a 1-5 Likert scale score to the model response using the  same  instruction we present to human annotators. The prompt  p DS subscript p DS p_{\\text{DS}} italic_p start_POSTSUBSCRIPT DS end_POSTSUBSCRIPT  is depicted in  Table 12 .",
            "G-Eval   Liu et al. ( 2023 ) . G-Eval is a  strong  prompt-based evaluation framework that assesses the quality of generated texts by incorporating chain-of-thoughts (CoT)  Wei et al. ( 2022 )  and a form-filling paradigm. By providing a prompt with a task introduction and evaluation criteria, G-Eval generates detailed evaluation steps and utilizes these steps along with the generated CoT to score the texts. We apply G-Eval using the  same  scoring criteria provided to humans.  The prompt  p G-Eval subscript p G-Eval p_{\\text{G-Eval}} italic_p start_POSTSUBSCRIPT G-Eval end_POSTSUBSCRIPT  behind G-Eval can be found in  Table 12 .",
            "Num-of-POVs .  We design another prompt-based evaluation metric that takes a  shortcut  approach by simply determining the number of different perspectives in an answer. This metric can be considered an improved metric over the Comprehensiveness Answer Rate metric introduced in the DELPHI paper, as it transcends the binary assessment of the original, which solely determines if an answer includes diverse and opposing viewpoints. The prompt  p NoP subscript p NoP p_{\\text{NoP}} italic_p start_POSTSUBSCRIPT NoP end_POSTSUBSCRIPT  is shown in  Table 12 ."
        ]
    },
    "id_table_13": {
        "caption": "Table 13:    Prompts for assessing answers to debatable questions in baseline methods of the P.D. metric.",
        "table": "A2.T13.5",
        "footnotes": [],
        "references": [
            "Remember in  Equation 1 , there is a prompt Please Restate wrapping the model answer. To verify the stability of P.D.  w.r.t.  different prompts, we configure P.D. with five different prompts shown in  Table 13  and compute pairwise Kendalls    \\tau italic_  among the resulting rankings, for the same set of model responses collected in   5.1 . The results in  Figure 4  show that the rankings by P.D. with different prompts have excellent consistency.",
            "Prompts.   The prompts we used to generate the POVs and explanations can be found in  Table 11 .  These prompts are carefully crafted to ensure that the generated POVs cover a range of non-overlapping perspectives and provide well-rounded explanations that are grounded in the evidence documents.  After extracting the POVs, we filter out questions with fewer than three perspectives, ensuring that the remaining questions are sufficiently debatable, resulting in 2,941 questions.  The distribution of the number of extracted POVs per question can be found in  Figure 13 .",
            "As in  Table 13 , we use five different prompts to show that P.D. is stable  w.r.t.  prompts."
        ]
    },
    "id_table_14": {
        "caption": "Table 14:    Prompt for the D.A. metric.  p D.A. subscript p D.A. p_{\\text{D.A.}} italic_p start_POSTSUBSCRIPT D.A. end_POSTSUBSCRIPT  determine if an answer explicitly recognizes the debatable nature of the question.  p D.A.-ZS subscript p D.A.-ZS p_{\\text{D.A.-ZS}} italic_p start_POSTSUBSCRIPT D.A.-ZS end_POSTSUBSCRIPT  is the zero-shot version of  p D.A. subscript p D.A. p_{\\text{D.A.}} italic_p start_POSTSUBSCRIPT D.A. end_POSTSUBSCRIPT  where the  in-context demonstrations  are completely omitted.",
        "table": "A3.T14.1",
        "footnotes": [],
        "references": [
            "Results and the final dataset.   We recruit three annotators and annotate the full dataset. Inter-annotator agreement (IAA) is measured using Fleiss Kappa  Fleiss et al. ( 1981 ) , yielding scores of   = 0.66  0.66 \\kappa=0.66 italic_ = 0.66  and   = 0.60  0.60 \\kappa=0.60 italic_ = 0.60  for the two annotation tasks, all indicating substantial agreement. We remove 767 partial answers deemed substandard by two or more annotators.  This suggests that GPT-4 generates faithful partial answers with a  93.4 % percent 93.4 93.4\\% 93.4 %  accuracy.  See   A.4  for details.  We employ BERTopic  Grootendorst ( 2022 )  to model the domain distribution of  DebateQA . The result is shown in  Figure 14 .  To reduce computational costs for upcoming evaluation, we split  DebateQA  into two splits: the  test  split with 1,000 randomly sampled questions and the  dev  set containing the remaining instances.",
            "II: Dispute Awareness (D.A.) .  To ascertain if the models answer indicates that the addressed question is debatable, we craft a prompt  p D.A. subscript p D.A. p_{\\text{D.A.}} italic_p start_POSTSUBSCRIPT D.A. end_POSTSUBSCRIPT , as shown in  Table 14 , and use it to prompt an instruction-tuned LLM. This metric is  binary , indicating awareness or lack thereof.",
            "Given the robust design of prompt  p D.A. subscript p D.A. p_{\\text{D.A.}} italic_p start_POSTSUBSCRIPT D.A. end_POSTSUBSCRIPT , ensuring D.A.s performance, we advise utilizing the  standard  prompt in  Table 14 . Our focus here is on confirming D.A.s stability across various backbone models.  We set D.A. with five instruction-following LLMs and analyze pairwise agreements for the decision made between each two LLMs based on the model responses collected in   5.1 . The result in  Figure 5  demonstrates the consistency of D.A. among different backbone models.",
            "We recruit three professional annotators from a local data annotation company to verify the partial answers. The payment for this job is above the local minimum wage.  Annotators are given two distinct tasks as outlined in   3.3 . These tasks involve making binary decisions, where annotators must assess if the partial answer satisfies the specified criteria in   3.3 .  After the annotation, we removed 767 partial answers deemed substandard by two or more annotators, resulting in a final dataset of 10,873 partial answers.  We do not remove the original questions corresponding to these partial answers, as those questions still have multiple partial answers.  A domain distribution of the final dataset is shown in  Figure 14 .",
            "The exact prompt for D.A. is shown in  Table 14 ."
        ]
    },
    "id_table_15": {
        "caption": "Table 15:   Prompt for test vanilla LLMs.",
        "table": "A4.T15.1",
        "footnotes": [],
        "references": [
            "Generation configuration.   In the main experiments, when testing the LLMs, we provide the questions with a minimalistic QA prompt, as shown in  Table 15 , which instructs the LLMs without any hint that they are debatable.  We believe this approach more accurately reflects the typical user interaction with chatbots.  For all models, we configure top- p = 0 p 0 p=0 italic_p = 0  to enable greedy decoding and stock chat templates including  M eval subscript M eval \\mathcal{M}_{\\text{eval}} caligraphic_M start_POSTSUBSCRIPT eval end_POSTSUBSCRIPT ."
        ]
    },
    "id_table_16": {
        "caption": "Table 16:   Prompt for test LLMs w. RAG.",
        "table": "A4.T16.1",
        "footnotes": [],
        "references": [
            "We examine the effects of two popular RAG strategies,  vanilla RAG   Lewis et al. ( 2020b )  and  ReAct   Yao et al. ( 2023 ) .  In vanilla RAG, we pick the top-10 most relevant documents from the retrieval results via Google Custom Search API 4 4 4 https://developers.google.com/custom-search .  ReAct employs an agent-based approach, leveraging Claude 3.5 Sonnet to interleave reasoning with document retrieval, strategically selecting up to 9 document chunks to improve problem-solving.  Both methods utilize the prompt in  Table 16  to assemble the question and the retrieved trunks.  Refer to   D.1  for details.",
            "Vanilla RAG.   We augment the LLMs with  LangChain . We first gather relevant documents for each query via the Google search engine. The top 10 URLs from the search are saved. The retrieved URLs then undergo a series of actions: (1) content retrieval using the  WebBasedLoader ; (2) chunking to roughly 2000-character using the  RecursiveCharacterTextSplitter , and (3) dense retrieval  Karpukhin et al. ( 2020 )  of the  top-10  most relevant chunks based on cosine similarity on embeddings using the gte-Qwen2-1.5B-instruct  Li et al. ( 2023 ); Qwen ( 2024a )  embedder.  These selected document chunks, along with the question, are compiled into a comprehensive prompt, as depicted in  Table 16 , which is then provided to LLMs for generating responses."
        ]
    },
    "id_table_17": {
        "caption": "Table 17:    System prompts that require the model to answer questions with varying degrees of granularity.",
        "table": "A4.T17.3",
        "footnotes": [],
        "references": [
            "In our main experiments, we use a simple QA prompt ( p basic subscript p basic p_{\\text{basic}} italic_p start_POSTSUBSCRIPT basic end_POSTSUBSCRIPT  in  Table 17 ) that does not highlight the debatable nature of the questions or demand comprehensive answers.  To evaluate the models full potential, we further test five LLMs with more detailed prompts.  We employ three system prompts p basic subscript p basic p_{\\text{basic}} italic_p start_POSTSUBSCRIPT basic end_POSTSUBSCRIPT ,  p comprehensive subscript p comprehensive p_{\\text{comprehensive}} italic_p start_POSTSUBSCRIPT comprehensive end_POSTSUBSCRIPT , and  p detailed subscript p detailed p_{\\text{detailed}} italic_p start_POSTSUBSCRIPT detailed end_POSTSUBSCRIPT to elicit model responses at varying levels of detail, as shown in  Table 17 .  Using 200 randomly sampled questions from  DebateQA - test , we compare the average P.D. and D.A. scores across the five selected LLMs.  The results for these prompts are presented in  Table 5 .  We find that even the relatively simple  p comprehensive subscript p comprehensive p_{\\text{comprehensive}} italic_p start_POSTSUBSCRIPT comprehensive end_POSTSUBSCRIPT  prompt significantly improved the performance for all five models.  We conclude that more specific prompts,  i.e. , inform the model of the debatable nature and request for detailed responses, can enhance LLMs performance in answering debatable questions.  This finding aligns with our expectations and suggests that LLM users can benefit from well-crafted prompts when seeking answers to contentious issues from LLMs."
        ]
    },
    "id_table_18": {
        "caption": "Table 18:    Comparing GPT-4os responses with and without RAG shows that RAG bolsters its capacity to present well-sourced arguments.",
        "table": "A5.T18.1",
        "footnotes": [],
        "references": [
            "We take the case of GPT-4o  OpenAI ( 2024b )  to investigate the lingering deficiencies of advanced LLMs, a case study is provided in  Table 18 .  Our main findings are:"
        ]
    },
    "id_table_19": {
        "caption": "Table 19:    Two examples that illustrate multiple deficiencies in Qwen2 7Bs responses.",
        "table": "A5.T19.1",
        "footnotes": [],
        "references": [
            "Taking Qwen2 7B  Qwen ( 2024a )  as a case study in  Table 19 , we pinpoint three main deficiencies typically found in the responses of models with moderate capabilities:"
        ]
    },
    "global_footnotes": [
        "We select OpenAI GPT-4",
        "to assist in collecting partial answers (the",
        "variant).",
        "These values are approximated with the P.D. values",
        "Two different",
        ", refer to",
        "for details.",
        "The problem of vertex cover is NP-hard. We leverage a greedy algorithm, similar to the one described in",
        ".",
        "Here, we use",
        "to denote",
        "in",
        "for simplicity."
    ]
}