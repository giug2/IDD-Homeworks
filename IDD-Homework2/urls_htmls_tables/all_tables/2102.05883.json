{
    "PAPER'S NUMBER OF TABLES": 9,
    "S5.T1": {
        "caption": "TABLE I: Classification Accuracy",
        "table": "<table id=\"S5.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"></th>\n<th id=\"S5.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">cancer</th>\n<th id=\"S5.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">payment</th>\n<th id=\"S5.T1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">credit</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Centralized</th>\n<td id=\"S5.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">91.15%</td>\n<td id=\"S5.T1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">81.10%</td>\n<td id=\"S5.T1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">93.16%</td>\n</tr>\n<tr id=\"S5.T1.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Centralized (all data)</th>\n<td id=\"S5.T1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">92.92%</td>\n<td id=\"S5.T1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">81.32%</td>\n<td id=\"S5.T1.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">93.39%</td>\n</tr>\n<tr id=\"S5.T1.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Hierarchical</th>\n<td id=\"S5.T1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">92.92%</td>\n<td id=\"S5.T1.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">78.20%</td>\n<td id=\"S5.T1.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">93.54%</td>\n</tr>\n<tr id=\"S5.T1.1.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Hierarchical (all data)</th>\n<td id=\"S5.T1.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">92.92%</td>\n<td id=\"S5.T1.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">78.20%</td>\n<td id=\"S5.T1.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">93.54%</td>\n</tr>\n<tr id=\"S5.T1.1.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">FATE</th>\n<td id=\"S5.T1.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">92.04%</td>\n<td id=\"S5.T1.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">74.87%</td>\n<td id=\"S5.T1.1.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">93.44%</td>\n</tr>\n<tr id=\"S5.T1.1.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">FATE (all data)</th>\n<td id=\"S5.T1.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">95.58%</td>\n<td id=\"S5.T1.1.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">74.85%</td>\n<td id=\"S5.T1.1.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">93.24%</td>\n</tr>\n<tr id=\"S5.T1.1.8.7\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.8.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">STFL</th>\n<td id=\"S5.T1.1.8.7.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">92.92%</td>\n<td id=\"S5.T1.1.8.7.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">78.50%</td>\n<td id=\"S5.T1.1.8.7.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">93.41%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table I shows the testing accuracy with the tested datasets. For the methods marked with (all data), it means the 40%percent4040\\% self-taught set is incorporated into the training set to evaluate the accuracy with all available data.\nFor “cancer” dataset, FATE trained by all data has the highest accuracy while all other methods have a similar accuracy. From the experiments of using “cancer” dataset, we can see that STFL is robust to the data size since it contains very small (228) data samples for training the VAE as compared to other two large datasets.\nFor “payment” dataset, the centralized method trained with all data has the highest accuracy when compared to other methods. In general, most of the methods have similar accuracy except FATE has a lower accuracy. For “credit” dataset, the hierarchical method has the highest accuracy when compared to other methods. In general, the accuracy of all methods is similar. We can see that although STFL cannot achieve the highest accuracy, it is not the method with the lowest accuracy. It has similar accuracy with other state-of-the-art methods. Therefore, fixing the parameters in the guest’s network during training without gradient back-propagation does not significantly decrease its prediction accuracy since it has been pre-trained."
        ]
    },
    "S5.T2": {
        "caption": "TABLE II: Confusion Matrix",
        "table": "<table id=\"S5.T2.sf1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T2.sf1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.sf1.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" colspan=\"2\"></th>\n<th id=\"S5.T2.sf1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" colspan=\"2\">Predicted class</th>\n</tr>\n<tr id=\"S5.T2.sf1.1.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.sf1.1.2.2.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" colspan=\"2\"></th>\n<th id=\"S5.T2.sf1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Class 1</th>\n<th id=\"S5.T2.sf1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Class 2</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.sf1.1.3.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.sf1.1.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" rowspan=\"2\"><span id=\"S5.T2.sf1.1.3.1.1.1\" class=\"ltx_text\">True class</span></th>\n<th id=\"S5.T2.sf1.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Class 1</th>\n<td id=\"S5.T2.sf1.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">37</td>\n<td id=\"S5.T2.sf1.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">6</td>\n</tr>\n<tr id=\"S5.T2.sf1.1.4.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.sf1.1.4.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Class 2</th>\n<td id=\"S5.T2.sf1.1.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3</td>\n<td id=\"S5.T2.sf1.1.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">67</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table II shows the confusion matrices of the six experiments of FATE and STFL methods with “cancer”, “payment” and “credit” datasets. For “cancer” and “payment” dataset, both methods have a similar results as shown in Tables IIa to IId. For results using “credit” dataset as shown in Tables IIe and IIf, although the resulting accuracy are similar for FATE and STFL, STFL tends to predict more samples since Class 2 when compared to FATE. This may due to the fact that the classes are imbalanced in “credit” dataset and FATE poorly generalizes the model for this imbalanced dataset. Hence, STFL’s accuracy still comparable to FATE without the back-propagation parameter update of guest’s network."
        ]
    },
    "S5.T2.sf1": {
        "caption": "(a) FATE with “cancer” dataset",
        "table": "<table id=\"S5.T2.sf1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T2.sf1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.sf1.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" colspan=\"2\"></th>\n<th id=\"S5.T2.sf1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" colspan=\"2\">Predicted class</th>\n</tr>\n<tr id=\"S5.T2.sf1.1.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.sf1.1.2.2.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" colspan=\"2\"></th>\n<th id=\"S5.T2.sf1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Class 1</th>\n<th id=\"S5.T2.sf1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Class 2</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.sf1.1.3.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.sf1.1.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" rowspan=\"2\"><span id=\"S5.T2.sf1.1.3.1.1.1\" class=\"ltx_text\">True class</span></th>\n<th id=\"S5.T2.sf1.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Class 1</th>\n<td id=\"S5.T2.sf1.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">37</td>\n<td id=\"S5.T2.sf1.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">6</td>\n</tr>\n<tr id=\"S5.T2.sf1.1.4.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.sf1.1.4.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Class 2</th>\n<td id=\"S5.T2.sf1.1.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">3</td>\n<td id=\"S5.T2.sf1.1.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">67</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table I shows the testing accuracy with the tested datasets. For the methods marked with (all data), it means the 40%percent4040\\% self-taught set is incorporated into the training set to evaluate the accuracy with all available data.\nFor “cancer” dataset, FATE trained by all data has the highest accuracy while all other methods have a similar accuracy. From the experiments of using “cancer” dataset, we can see that STFL is robust to the data size since it contains very small (228) data samples for training the VAE as compared to other two large datasets.\nFor “payment” dataset, the centralized method trained with all data has the highest accuracy when compared to other methods. In general, most of the methods have similar accuracy except FATE has a lower accuracy. For “credit” dataset, the hierarchical method has the highest accuracy when compared to other methods. In general, the accuracy of all methods is similar. We can see that although STFL cannot achieve the highest accuracy, it is not the method with the lowest accuracy. It has similar accuracy with other state-of-the-art methods. Therefore, fixing the parameters in the guest’s network during training without gradient back-propagation does not significantly decrease its prediction accuracy since it has been pre-trained.",
            "Table II shows the confusion matrices of the six experiments of FATE and STFL methods with “cancer”, “payment” and “credit” datasets. For “cancer” and “payment” dataset, both methods have a similar results as shown in Tables IIa to IId. For results using “credit” dataset as shown in Tables IIe and IIf, although the resulting accuracy are similar for FATE and STFL, STFL tends to predict more samples since Class 2 when compared to FATE. This may due to the fact that the classes are imbalanced in “credit” dataset and FATE poorly generalizes the model for this imbalanced dataset. Hence, STFL’s accuracy still comparable to FATE without the back-propagation parameter update of guest’s network.",
            "Table III shows the required time for training the multilayer perceptron using different methods. For different datasets, the time is different since the data size of each dataset is different. The larger the dataset, the longer the required training time. FATE requires much more training time compared to STFL for the three datasets. The main reason is that FATE uses homomorphic encryption for the shared information, which is computationally expensive. On the other hand, STFL avoids using homomorphic encryption and use a self-trained encoder of VAE to transform features from the data to the latent space which can be computed in a much shorter time. Therefore, the computational time of STFL is much short than that of FATE."
        ]
    },
    "S5.T2.sf2": {
        "caption": "(b) STFL with “cancer” dataset",
        "table": "<table id=\"S5.T2.sf2.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T2.sf2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.sf2.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" colspan=\"2\"></th>\n<th id=\"S5.T2.sf2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" colspan=\"2\">Predicted class</th>\n</tr>\n<tr id=\"S5.T2.sf2.1.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.sf2.1.2.2.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" colspan=\"2\"></th>\n<th id=\"S5.T2.sf2.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Class 1</th>\n<th id=\"S5.T2.sf2.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Class 2</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.sf2.1.3.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.sf2.1.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" rowspan=\"2\"><span id=\"S5.T2.sf2.1.3.1.1.1\" class=\"ltx_text\">True class</span></th>\n<th id=\"S5.T2.sf2.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Class 1</th>\n<td id=\"S5.T2.sf2.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">37</td>\n<td id=\"S5.T2.sf2.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">6</td>\n</tr>\n<tr id=\"S5.T2.sf2.1.4.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.sf2.1.4.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Class 2</th>\n<td id=\"S5.T2.sf2.1.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">2</td>\n<td id=\"S5.T2.sf2.1.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">68</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table I shows the testing accuracy with the tested datasets. For the methods marked with (all data), it means the 40%percent4040\\% self-taught set is incorporated into the training set to evaluate the accuracy with all available data.\nFor “cancer” dataset, FATE trained by all data has the highest accuracy while all other methods have a similar accuracy. From the experiments of using “cancer” dataset, we can see that STFL is robust to the data size since it contains very small (228) data samples for training the VAE as compared to other two large datasets.\nFor “payment” dataset, the centralized method trained with all data has the highest accuracy when compared to other methods. In general, most of the methods have similar accuracy except FATE has a lower accuracy. For “credit” dataset, the hierarchical method has the highest accuracy when compared to other methods. In general, the accuracy of all methods is similar. We can see that although STFL cannot achieve the highest accuracy, it is not the method with the lowest accuracy. It has similar accuracy with other state-of-the-art methods. Therefore, fixing the parameters in the guest’s network during training without gradient back-propagation does not significantly decrease its prediction accuracy since it has been pre-trained.",
            "Table II shows the confusion matrices of the six experiments of FATE and STFL methods with “cancer”, “payment” and “credit” datasets. For “cancer” and “payment” dataset, both methods have a similar results as shown in Tables IIa to IId. For results using “credit” dataset as shown in Tables IIe and IIf, although the resulting accuracy are similar for FATE and STFL, STFL tends to predict more samples since Class 2 when compared to FATE. This may due to the fact that the classes are imbalanced in “credit” dataset and FATE poorly generalizes the model for this imbalanced dataset. Hence, STFL’s accuracy still comparable to FATE without the back-propagation parameter update of guest’s network.",
            "Table III shows the required time for training the multilayer perceptron using different methods. For different datasets, the time is different since the data size of each dataset is different. The larger the dataset, the longer the required training time. FATE requires much more training time compared to STFL for the three datasets. The main reason is that FATE uses homomorphic encryption for the shared information, which is computationally expensive. On the other hand, STFL avoids using homomorphic encryption and use a self-trained encoder of VAE to transform features from the data to the latent space which can be computed in a much shorter time. Therefore, the computational time of STFL is much short than that of FATE."
        ]
    },
    "S5.T2.sf3": {
        "caption": "(c) FATE with “payment” dataset",
        "table": "<table id=\"S5.T2.sf3.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T2.sf3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.sf3.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" colspan=\"2\"></th>\n<th id=\"S5.T2.sf3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" colspan=\"2\">Predicted class</th>\n</tr>\n<tr id=\"S5.T2.sf3.1.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.sf3.1.2.2.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" colspan=\"2\"></th>\n<th id=\"S5.T2.sf3.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Class 1</th>\n<th id=\"S5.T2.sf3.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Class 2</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.sf3.1.3.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.sf3.1.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" rowspan=\"2\"><span id=\"S5.T2.sf3.1.3.1.1.1\" class=\"ltx_text\">True class</span></th>\n<th id=\"S5.T2.sf3.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Class 1</th>\n<td id=\"S5.T2.sf3.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">4197</td>\n<td id=\"S5.T2.sf3.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">494</td>\n</tr>\n<tr id=\"S5.T2.sf3.1.4.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.sf3.1.4.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Class 2</th>\n<td id=\"S5.T2.sf3.1.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1014</td>\n<td id=\"S5.T2.sf3.1.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">295</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table I shows the testing accuracy with the tested datasets. For the methods marked with (all data), it means the 40%percent4040\\% self-taught set is incorporated into the training set to evaluate the accuracy with all available data.\nFor “cancer” dataset, FATE trained by all data has the highest accuracy while all other methods have a similar accuracy. From the experiments of using “cancer” dataset, we can see that STFL is robust to the data size since it contains very small (228) data samples for training the VAE as compared to other two large datasets.\nFor “payment” dataset, the centralized method trained with all data has the highest accuracy when compared to other methods. In general, most of the methods have similar accuracy except FATE has a lower accuracy. For “credit” dataset, the hierarchical method has the highest accuracy when compared to other methods. In general, the accuracy of all methods is similar. We can see that although STFL cannot achieve the highest accuracy, it is not the method with the lowest accuracy. It has similar accuracy with other state-of-the-art methods. Therefore, fixing the parameters in the guest’s network during training without gradient back-propagation does not significantly decrease its prediction accuracy since it has been pre-trained.",
            "Table II shows the confusion matrices of the six experiments of FATE and STFL methods with “cancer”, “payment” and “credit” datasets. For “cancer” and “payment” dataset, both methods have a similar results as shown in Tables IIa to IId. For results using “credit” dataset as shown in Tables IIe and IIf, although the resulting accuracy are similar for FATE and STFL, STFL tends to predict more samples since Class 2 when compared to FATE. This may due to the fact that the classes are imbalanced in “credit” dataset and FATE poorly generalizes the model for this imbalanced dataset. Hence, STFL’s accuracy still comparable to FATE without the back-propagation parameter update of guest’s network.",
            "Table III shows the required time for training the multilayer perceptron using different methods. For different datasets, the time is different since the data size of each dataset is different. The larger the dataset, the longer the required training time. FATE requires much more training time compared to STFL for the three datasets. The main reason is that FATE uses homomorphic encryption for the shared information, which is computationally expensive. On the other hand, STFL avoids using homomorphic encryption and use a self-trained encoder of VAE to transform features from the data to the latent space which can be computed in a much shorter time. Therefore, the computational time of STFL is much short than that of FATE."
        ]
    },
    "S5.T2.sf4": {
        "caption": "(d) STFL with “payment” dataset",
        "table": "<table id=\"S5.T2.sf4.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T2.sf4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.sf4.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" colspan=\"2\"></th>\n<th id=\"S5.T2.sf4.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" colspan=\"2\">Predicted class</th>\n</tr>\n<tr id=\"S5.T2.sf4.1.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.sf4.1.2.2.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" colspan=\"2\"></th>\n<th id=\"S5.T2.sf4.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Class 1</th>\n<th id=\"S5.T2.sf4.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Class 2</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.sf4.1.3.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.sf4.1.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" rowspan=\"2\"><span id=\"S5.T2.sf4.1.3.1.1.1\" class=\"ltx_text\">True class</span></th>\n<th id=\"S5.T2.sf4.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Class 1</th>\n<td id=\"S5.T2.sf4.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">4484</td>\n<td id=\"S5.T2.sf4.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">207</td>\n</tr>\n<tr id=\"S5.T2.sf4.1.4.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.sf4.1.4.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Class 2</th>\n<td id=\"S5.T2.sf4.1.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1083</td>\n<td id=\"S5.T2.sf4.1.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">226</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table I shows the testing accuracy with the tested datasets. For the methods marked with (all data), it means the 40%percent4040\\% self-taught set is incorporated into the training set to evaluate the accuracy with all available data.\nFor “cancer” dataset, FATE trained by all data has the highest accuracy while all other methods have a similar accuracy. From the experiments of using “cancer” dataset, we can see that STFL is robust to the data size since it contains very small (228) data samples for training the VAE as compared to other two large datasets.\nFor “payment” dataset, the centralized method trained with all data has the highest accuracy when compared to other methods. In general, most of the methods have similar accuracy except FATE has a lower accuracy. For “credit” dataset, the hierarchical method has the highest accuracy when compared to other methods. In general, the accuracy of all methods is similar. We can see that although STFL cannot achieve the highest accuracy, it is not the method with the lowest accuracy. It has similar accuracy with other state-of-the-art methods. Therefore, fixing the parameters in the guest’s network during training without gradient back-propagation does not significantly decrease its prediction accuracy since it has been pre-trained.",
            "Table II shows the confusion matrices of the six experiments of FATE and STFL methods with “cancer”, “payment” and “credit” datasets. For “cancer” and “payment” dataset, both methods have a similar results as shown in Tables IIa to IId. For results using “credit” dataset as shown in Tables IIe and IIf, although the resulting accuracy are similar for FATE and STFL, STFL tends to predict more samples since Class 2 when compared to FATE. This may due to the fact that the classes are imbalanced in “credit” dataset and FATE poorly generalizes the model for this imbalanced dataset. Hence, STFL’s accuracy still comparable to FATE without the back-propagation parameter update of guest’s network.",
            "Table III shows the required time for training the multilayer perceptron using different methods. For different datasets, the time is different since the data size of each dataset is different. The larger the dataset, the longer the required training time. FATE requires much more training time compared to STFL for the three datasets. The main reason is that FATE uses homomorphic encryption for the shared information, which is computationally expensive. On the other hand, STFL avoids using homomorphic encryption and use a self-trained encoder of VAE to transform features from the data to the latent space which can be computed in a much shorter time. Therefore, the computational time of STFL is much short than that of FATE."
        ]
    },
    "S5.T2.sf5": {
        "caption": "(e) FATE with “credit” dataset",
        "table": "<table id=\"S5.T2.sf5.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T2.sf5.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.sf5.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" colspan=\"2\"></th>\n<th id=\"S5.T2.sf5.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" colspan=\"2\">Predicted class</th>\n</tr>\n<tr id=\"S5.T2.sf5.1.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.sf5.1.2.2.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" colspan=\"2\"></th>\n<th id=\"S5.T2.sf5.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Class 1</th>\n<th id=\"S5.T2.sf5.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Class 2</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.sf5.1.3.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.sf5.1.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" rowspan=\"2\"><span id=\"S5.T2.sf5.1.3.1.1.1\" class=\"ltx_text\">True class</span></th>\n<th id=\"S5.T2.sf5.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Class 1</th>\n<td id=\"S5.T2.sf5.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">27739</td>\n<td id=\"S5.T2.sf5.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">251</td>\n</tr>\n<tr id=\"S5.T2.sf5.1.4.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.sf5.1.4.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Class 2</th>\n<td id=\"S5.T2.sf5.1.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1716</td>\n<td id=\"S5.T2.sf5.1.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">294</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table I shows the testing accuracy with the tested datasets. For the methods marked with (all data), it means the 40%percent4040\\% self-taught set is incorporated into the training set to evaluate the accuracy with all available data.\nFor “cancer” dataset, FATE trained by all data has the highest accuracy while all other methods have a similar accuracy. From the experiments of using “cancer” dataset, we can see that STFL is robust to the data size since it contains very small (228) data samples for training the VAE as compared to other two large datasets.\nFor “payment” dataset, the centralized method trained with all data has the highest accuracy when compared to other methods. In general, most of the methods have similar accuracy except FATE has a lower accuracy. For “credit” dataset, the hierarchical method has the highest accuracy when compared to other methods. In general, the accuracy of all methods is similar. We can see that although STFL cannot achieve the highest accuracy, it is not the method with the lowest accuracy. It has similar accuracy with other state-of-the-art methods. Therefore, fixing the parameters in the guest’s network during training without gradient back-propagation does not significantly decrease its prediction accuracy since it has been pre-trained.",
            "Table II shows the confusion matrices of the six experiments of FATE and STFL methods with “cancer”, “payment” and “credit” datasets. For “cancer” and “payment” dataset, both methods have a similar results as shown in Tables IIa to IId. For results using “credit” dataset as shown in Tables IIe and IIf, although the resulting accuracy are similar for FATE and STFL, STFL tends to predict more samples since Class 2 when compared to FATE. This may due to the fact that the classes are imbalanced in “credit” dataset and FATE poorly generalizes the model for this imbalanced dataset. Hence, STFL’s accuracy still comparable to FATE without the back-propagation parameter update of guest’s network.",
            "Table III shows the required time for training the multilayer perceptron using different methods. For different datasets, the time is different since the data size of each dataset is different. The larger the dataset, the longer the required training time. FATE requires much more training time compared to STFL for the three datasets. The main reason is that FATE uses homomorphic encryption for the shared information, which is computationally expensive. On the other hand, STFL avoids using homomorphic encryption and use a self-trained encoder of VAE to transform features from the data to the latent space which can be computed in a much shorter time. Therefore, the computational time of STFL is much short than that of FATE."
        ]
    },
    "S5.T2.sf6": {
        "caption": "(f) STFL with “credit” dataset",
        "table": "<table id=\"S5.T2.sf6.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T2.sf6.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.sf6.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" colspan=\"2\"></th>\n<th id=\"S5.T2.sf6.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" colspan=\"2\">Predicted class</th>\n</tr>\n<tr id=\"S5.T2.sf6.1.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.sf6.1.2.2.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" colspan=\"2\"></th>\n<th id=\"S5.T2.sf6.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Class 1</th>\n<th id=\"S5.T2.sf6.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Class 2</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.sf6.1.3.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.sf6.1.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\" rowspan=\"2\"><span id=\"S5.T2.sf6.1.3.1.1.1\" class=\"ltx_text\">True class</span></th>\n<th id=\"S5.T2.sf6.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Class 1</th>\n<td id=\"S5.T2.sf6.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">27550</td>\n<td id=\"S5.T2.sf6.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">440</td>\n</tr>\n<tr id=\"S5.T2.sf6.1.4.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.sf6.1.4.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">Class 2</th>\n<td id=\"S5.T2.sf6.1.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1538</td>\n<td id=\"S5.T2.sf6.1.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">472</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table I shows the testing accuracy with the tested datasets. For the methods marked with (all data), it means the 40%percent4040\\% self-taught set is incorporated into the training set to evaluate the accuracy with all available data.\nFor “cancer” dataset, FATE trained by all data has the highest accuracy while all other methods have a similar accuracy. From the experiments of using “cancer” dataset, we can see that STFL is robust to the data size since it contains very small (228) data samples for training the VAE as compared to other two large datasets.\nFor “payment” dataset, the centralized method trained with all data has the highest accuracy when compared to other methods. In general, most of the methods have similar accuracy except FATE has a lower accuracy. For “credit” dataset, the hierarchical method has the highest accuracy when compared to other methods. In general, the accuracy of all methods is similar. We can see that although STFL cannot achieve the highest accuracy, it is not the method with the lowest accuracy. It has similar accuracy with other state-of-the-art methods. Therefore, fixing the parameters in the guest’s network during training without gradient back-propagation does not significantly decrease its prediction accuracy since it has been pre-trained.",
            "Table II shows the confusion matrices of the six experiments of FATE and STFL methods with “cancer”, “payment” and “credit” datasets. For “cancer” and “payment” dataset, both methods have a similar results as shown in Tables IIa to IId. For results using “credit” dataset as shown in Tables IIe and IIf, although the resulting accuracy are similar for FATE and STFL, STFL tends to predict more samples since Class 2 when compared to FATE. This may due to the fact that the classes are imbalanced in “credit” dataset and FATE poorly generalizes the model for this imbalanced dataset. Hence, STFL’s accuracy still comparable to FATE without the back-propagation parameter update of guest’s network.",
            "Table III shows the required time for training the multilayer perceptron using different methods. For different datasets, the time is different since the data size of each dataset is different. The larger the dataset, the longer the required training time. FATE requires much more training time compared to STFL for the three datasets. The main reason is that FATE uses homomorphic encryption for the shared information, which is computationally expensive. On the other hand, STFL avoids using homomorphic encryption and use a self-trained encoder of VAE to transform features from the data to the latent space which can be computed in a much shorter time. Therefore, the computational time of STFL is much short than that of FATE."
        ]
    },
    "S5.T3": {
        "caption": "TABLE III: Training time",
        "table": "<table id=\"S5.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\"></th>\n<th id=\"S5.T3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">cancer</th>\n<th id=\"S5.T3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">payment</th>\n<th id=\"S5.T3.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">credit</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T3.1.2.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">FATE</td>\n<td id=\"S5.T3.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">1,737s</td>\n<td id=\"S5.T3.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">137,957s</td>\n<td id=\"S5.T3.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">191,109s</td>\n</tr>\n<tr id=\"S5.T3.1.3.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">STFL</td>\n<td id=\"S5.T3.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">88s</td>\n<td id=\"S5.T3.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">941s</td>\n<td id=\"S5.T3.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:1.5pt;padding-bottom:1.5pt;\">7,723s</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table III shows the required time for training the multilayer perceptron using different methods. For different datasets, the time is different since the data size of each dataset is different. The larger the dataset, the longer the required training time. FATE requires much more training time compared to STFL for the three datasets. The main reason is that FATE uses homomorphic encryption for the shared information, which is computationally expensive. On the other hand, STFL avoids using homomorphic encryption and use a self-trained encoder of VAE to transform features from the data to the latent space which can be computed in a much shorter time. Therefore, the computational time of STFL is much short than that of FATE."
        ]
    }
}