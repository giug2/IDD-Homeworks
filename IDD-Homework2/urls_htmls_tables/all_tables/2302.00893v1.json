{
    "S5.T1": {
        "caption": "Table 1: The statistics of the datasets.",
        "table": null,
        "footnotes": [],
        "references": [
            "We utilize WIKI Leblay and Chekol (2018) and three datasets from the Integrated Crisis Early Warning System Boschee et¬†al. (2015): ICEWS14 Garc√≠a-Dur√°n et¬†al. (2018), ICEWS18 Jin et¬†al. (2020) and ICEWS05-15 Garc√≠a-Dur√°n et¬†al. (2018) to evaluate the effectiveness of our model. Each of the ICEWS dataset is composed of various political facts with time annotations such as (Barack Obama, visit, Malaysia, 2014/02/19201402192014/02/19). ICEWS14, ICEWS18, and ICEWS05-15 record the facts in 2014, 2018, and the facts from 2005 to 2015, respectively. Following Li et¬†al. (2021), We divide ICEWS14, ICEWS18, ICEWS05-15, and WIKI into training, validation, and testing sets with a proportion of 80%, 10%, and 10% by timestamps. The details of the four datasets are presented in Table 1, where the time gap represents time granularity between two temporally adjacent facts.",
            "In this section, we study how the multi-step gradient updates affect our model. The performances of backbones plugged with MetaTKG under different multi-step values are shown in Figure 5. We can find that the multi-step updates can enhance the performance of backbones on most datasets. However, compared with the other three datasets, the multi-step updates seem to be less effective on ICEWS05-15. From Table 1, we can find that the number of time slices in ICEWS05-15 is much more than other datasets, but the total number of facts is not larger than others, which indicates that the number of facts in ICEWS05-15 in each time slice is relatively less. Thus, we analyze the reason for the aforementioned observation is that the multi-step updates make the models susceptible to over-fitting when predicting each time slice."
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Performance comparison of MetaTKG when plugged into different backbones on four datasets in terms of MRR (%), Hit@1 (%), and Hit@10 (%) (all results are under raw metrics). The highest performance is highlighted in bold. The backbones with * represent the fine-tuned one. ŒîŒî\\DeltaImprove and ŒîŒî\\DeltaImprove* indicate the relative improvements over the original backbones and fine-tuned backbones in percentage, respectively.",
        "table": null,
        "footnotes": [],
        "references": [
            "Since our model utilizes multi-step updates in the testing phase, we also fine-tune all backbone models with multi-step gradient updates for a fair comparison. The performances of the backbones plugged with MetaTKG, the original backbones, and fine-tuned backbones on entity prediction task are shown in Table 2. From the results in Table 2, we have the following observations."
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Ablation studies on gating integration module in terms of MRR (%) under the raw setting.",
        "table": null,
        "footnotes": [],
        "references": [
            "Ablation Study for Gating Module. To verify the effectiveness of the gating integration module, we compare the performance of the backbones plugged with MetaTKG and MetaTKG-G which removes the gating module. We show the results of all backbones plugged with the two models in Table 3 and obtain the following findings."
        ]
    },
    "S5.T4": {
        "caption": "Table 4: Ablation studies for component-specific gating parameter in terms of MRR (%) under the raw setting.",
        "table": null,
        "footnotes": [],
        "references": [
            "Ablation Study for Component-specific gating parameter. The gating parameter is the key to determining the performance of the gating integration module. In this experiment, we compare the performance of the backbones plugged with MetaTKG and MetaTKG-C, in which the gating parameter gssubscriptgùë†\\operatorname{g}_{s} is set to the same value for entity embedding, relation embedding, and other parameters in the model. We show the results of all backbones plugged with the two models in Table 4."
        ]
    }
}