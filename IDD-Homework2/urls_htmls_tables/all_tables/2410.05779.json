{
    "id_table_1": {
        "caption": "Table 1:  Win rates (%) of baselines v.s. LightRAG across four datasets and four evaluation dimensions.",
        "table": "S4.T1.1.1",
        "footnotes": [],
        "references": [
            "We compare LightRAG against each baseline across various evaluation dimensions and datasets. The results are presented in Table  1 . Based on these findings, we draw the following conclusions:"
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Performance of ablated versions of LightRAG, using NaiveRAG as reference.",
        "table": "S4.T2.1.1",
        "footnotes": [],
        "references": [
            "where  D ^ ^ D \\hat{\\mathcal{D}} over^ start_ARG caligraphic_D end_ARG  represents the resulting knowledge graphs. To generate this data, we apply three main processing steps to the raw text documents  D i subscript D i \\mathcal{D}_{i} caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . These steps utilize a LLM for text analysis and processing. Details about the prompt templates and specific settings for this part can be found in Appendix  7.3.2 . The functions used in our graph-based text indexing paradigm are described as:",
            "Context Integration and Answer Generation . By unifying the query with this multi-source text, the LLM generates informative answers tailored to the users needs, ensuring alignment with the querys intent. This approach streamlines the answer generation process by integrating both context and query into the LLM model, as illustrated in detailed examples (Appendix  7.2 ).",
            "We also conduct ablation studies to evaluate the impact of our dual-level retrieval paradigm and the effectiveness of our graph-based text indexing in LightRAG. The results are presented in Table  2 .",
            "We compare the cost of our LightRAG with that of the top-performing baseline, GraphRAG, from two key perspectives. First, we examine the number of tokens and API calls during the indexing and retrieval processes. Second, we analyze these metrics in relation to handling data changes in dynamic environments. The results of this evaluation on the legal dataset are presented in Table  2 . In this context,  T extract subscript T extract T_{\\text{extract}} italic_T start_POSTSUBSCRIPT extract end_POSTSUBSCRIPT  represents the token overhead for entity and relationship extraction,  C max subscript C max C_{\\text{max}} italic_C start_POSTSUBSCRIPT max end_POSTSUBSCRIPT  denotes the maximum number of tokens allowed per API call, and  C extract subscript C extract C_{\\text{extract}} italic_C start_POSTSUBSCRIPT extract end_POSTSUBSCRIPT  indicates the number of API calls required for extraction."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Case Study: Comparison Between LightRAG and the Baseline Method GraphRAG.",
        "table": "S4.T3.1.1",
        "footnotes": [],
        "references": [
            "where  D ^ ^ D \\hat{\\mathcal{D}} over^ start_ARG caligraphic_D end_ARG  represents the resulting knowledge graphs. To generate this data, we apply three main processing steps to the raw text documents  D i subscript D i \\mathcal{D}_{i} caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . These steps utilize a LLM for text analysis and processing. Details about the prompt templates and specific settings for this part can be found in Appendix  7.3.2 . The functions used in our graph-based text indexing paradigm are described as:",
            "Defining ground truth for many RAG queries, particularly those involving complex high-level semantics, poses significant challenges. To address this, we build on existing work  (Edge et al.,  2024 )  and adopt an LLM-based multi-dimensional comparison method. We employ a robust LLM, specifically GPT-4o-mini, to rank each baseline against our LightRAG. The evaluation prompt we used is detailed in Appendix  7.3.4 . In total, we utilize four evaluation dimensions, including:",
            "To provide a clear comparison between baseline methods and our LightRAG, we present specific case examples in Table  3 , which includes responses to a machine learning question from both the competitive baseline, GraphRAG, and our LightRAG framework. In this instance, LightRAG outperforms in all evaluation dimensions assessed by the LLM judge, including comprehensiveness, diversity, empowerment, and overall quality. Our key observations are as follows:",
            "In Figure  3 , we illustrate the retrieve-and-generate process. When presented with the query, What metrics are most informative for evaluating movie recommendation systems?, the LLM first extracts both low-level and high-level keywords. These keywords guide the dual-level retrieval process on the generated knowledge graph, targeting relevant entities and relationships. The retrieved information is organized into three components: entities, relationships, and corresponding text chunks. This structured data is then fed into the LLM, enabling it to generate a comprehensive answer to the query."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Statistical information of the datasets.",
        "table": "S4.F2.8",
        "footnotes": [],
        "references": [
            "Evaluation Datasets .  To conduct a comprehensive analysis of LightRAG, we selected four datasets from the UltraDomain benchmark  (Qian et al.,  2024 ) . The UltraDomain data is sourced from 428 college textbooks and encompasses 18 distinct domains, including agriculture, social sciences, and humanities. From these, we chose the Agriculture, CS, Legal, and Mix datasets. Each dataset contains between 600,000 and 5,000,000 tokens, with detailed information provided in Table  4 . Below is a specific introduction to the four domains utilized in our experiments:",
            "Defining ground truth for many RAG queries, particularly those involving complex high-level semantics, poses significant challenges. To address this, we build on existing work  (Edge et al.,  2024 )  and adopt an LLM-based multi-dimensional comparison method. We employ a robust LLM, specifically GPT-4o-mini, to rank each baseline against our LightRAG. The evaluation prompt we used is detailed in Appendix  7.3.4 . In total, we utilize four evaluation dimensions, including:",
            "Table  4  presents statistical information for four datasets: Agriculture, CS, Legal, and Mix. The Agriculture dataset consists of 12 documents totaling 2,017,886 tokens, while the CS dataset contains 10 documents with 2,306,535 tokens. The Legal dataset is the largest, comprising 94 documents and 5,081,069 tokens. Lastly, the Mix dataset includes 61 documents with a total of 619,009 tokens.",
            "The graph construction prompt outlined in Figure  4  is designed to extract and structure entity-relationship information from a text document based on specified entity types. The process begins by identifying entities and categorizing them into types such as organization, person, location, and event. It then provides detailed descriptions of their attributes and activities. Next, the prompt identifies relationships between these entities, offering explanations, assigning strength scores, and summarizing the relationships using high-level keywords."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Case Study: Comparison Between LightRAG and the Baseline NaiveRAG.",
        "table": "S7.T4.1",
        "footnotes": [],
        "references": [
            "In Figure  5 , the query generation prompt outlines a framework for identifying potential user roles (e.g., data scientist, finance analyst, and product manager) and their objectives for generating queries based on a specified dataset description. The prompt explains how to define five distinct users who would benefit from interacting with the dataset. For each user, it specifies five key tasks they would perform while working with the dataset. Additionally, for each (user, task) combination, five high-level questions are posed to ensure a thorough understanding of the dataset.",
            "To further illustrate LightRAGs superiority over baseline models in terms of comprehensiveness, empowerment, and diversity, we present a case study comparing LightRAG and NaiveRAG in Table  5 . This study addresses a question regarding indigenous perspectives in the context of corporate mergers. Notably, LightRAG offers a more in-depth exploration of key themes related to indigenous perspectives, such as cultural significance, collaboration, and legal frameworks, supported by specific and illustrative examples. In contrast, while NaiveRAG provides informative responses, it lacks the depth needed to thoroughly examine the various dimensions of indigenous ownership and collaboration. The dual-level retrieval process employed by LightRAG enables a more comprehensive investigation of specific entities and their interrelationships, facilitating extensive searches that effectively capture overarching themes and complexities within the topic."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "S7.T5.1.1",
        "footnotes": [],
        "references": [
            "In Figure  6 , the prompt describes a method for extracting keywords from a users query, distinguishing between high-level and low-level keywords. High-level keywords represent broad concepts or themes, while low-level keywords focus on specific entities and details. The extracted keywords are returned in JSON format, organized into two fields: high_level_keywords for overarching ideas and low_level_keywords for specific details."
        ]
    },
    "global_footnotes": []
}