{
    "S3.T1.1": {
        "caption": "Trainable parameters for XLNet and Indic-BART baseline models",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"S3.T1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.1\">Model Name</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S3.T1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.2.1\">Trainable Parameters</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"S3.T1.1.2.1.1\">XLNet Baseline</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.1.2.1.2\">147,490,318</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" id=\"S3.T1.1.3.2.1\">Indic-BART Baseline</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S3.T1.1.3.2.2\">145,339,392</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "We utilized the datasets from IndicTrans2 Gala et al. (2023), aligning with our focus on Indian language baselines. Following dataset selection, we curated the data to enhance translation quality. Our initial intention was to develop a model from scratch to ensure transparency and interpretability, as pre-trained models often function as black boxes. However, building and experimenting with custom models introduces significant challenges. To address these, we employed stable baseline models and ensured parameter equivalence for fair comparison. For the Decoder-only model, we chose XLNet as the base, implementing it for multi-task learning in machine translation, as demonstrated by Wu et al. (2021) For the Encoder-Decoder model, we used IndicBART as the foundation, based on the work of Dabre et al. (2021), Both models shared a common tokenizer to maintain consistency in data processing. This approach allowed us to systematically compare the performance and interpretability of Decoder-only and Encoder-Decoder models under similar conditions, providing insights into their respective strengths and weaknesses in multi-lingual and multi-task learning scenarios. As shown in Table 1, the XLNet Baseline model has 147,490,318 trainable parameters, while the Indic-BART Baseline model has 145,339,392 parameters.\n"
        ]
    }
}