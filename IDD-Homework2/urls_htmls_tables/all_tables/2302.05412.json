{
    "PAPER'S NUMBER OF TABLES": 1,
    "S1.T1": {
        "caption": "Table 1: Comparison of FedMBO with existing federated bilevel algorithms. m𝑚m is the total number of clients, n𝑛n is the size of sampled clients, and ϵitalic-ϵ\\epsilon is the required accuracy. ",
        "table": "<table id=\"S1.T1.10.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S1.T1.10.4.5.1\" class=\"ltx_tr\">\n<th id=\"S1.T1.10.4.5.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span id=\"S1.T1.10.4.5.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Algorithm</span></th>\n<th id=\"S1.T1.10.4.5.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S1.T1.10.4.5.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Sample Complexity</span></th>\n<th id=\"S1.T1.10.4.5.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S1.T1.10.4.5.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">Partial Client Participation</span></th>\n<th id=\"S1.T1.10.4.5.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S1.T1.10.4.5.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">Linear Speedup</span></th>\n<th id=\"S1.T1.10.4.5.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S1.T1.10.4.5.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">Data Heterogeneity</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S1.T1.7.1.1\" class=\"ltx_tr\">\n<th id=\"S1.T1.7.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\n<span id=\"S1.T1.7.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">LocalBSGVR </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.7.1.1.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>Gao<span id=\"S1.T1.7.1.1.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib16\" title=\"\" class=\"ltx_ref\">2022</a><span id=\"S1.T1.7.1.1.2.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</th>\n<td id=\"S1.T1.7.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<math id=\"S1.T1.7.1.1.1.m1.1\" class=\"ltx_math_unparsed\" alttext=\"\\mathcal{O}(\\epsilon^{-3/2}m^{-1}\" display=\"inline\"><semantics id=\"S1.T1.7.1.1.1.m1.1a\"><mrow id=\"S1.T1.7.1.1.1.m1.1b\"><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"90%\" id=\"S1.T1.7.1.1.1.m1.1.1\">𝒪</mi><mrow id=\"S1.T1.7.1.1.1.m1.1.2\"><mo maxsize=\"90%\" minsize=\"90%\" id=\"S1.T1.7.1.1.1.m1.1.2.1\">(</mo><msup id=\"S1.T1.7.1.1.1.m1.1.2.2\"><mi mathsize=\"90%\" id=\"S1.T1.7.1.1.1.m1.1.2.2.2\">ϵ</mi><mrow id=\"S1.T1.7.1.1.1.m1.1.2.2.3\"><mo mathsize=\"90%\" id=\"S1.T1.7.1.1.1.m1.1.2.2.3a\">−</mo><mrow id=\"S1.T1.7.1.1.1.m1.1.2.2.3.2\"><mn mathsize=\"90%\" id=\"S1.T1.7.1.1.1.m1.1.2.2.3.2.2\">3</mn><mo maxsize=\"90%\" minsize=\"90%\" stretchy=\"true\" symmetric=\"true\" id=\"S1.T1.7.1.1.1.m1.1.2.2.3.2.1\">/</mo><mn mathsize=\"90%\" id=\"S1.T1.7.1.1.1.m1.1.2.2.3.2.3\">2</mn></mrow></mrow></msup><msup id=\"S1.T1.7.1.1.1.m1.1.2.3\"><mi mathsize=\"90%\" id=\"S1.T1.7.1.1.1.m1.1.2.3.2\">m</mi><mrow id=\"S1.T1.7.1.1.1.m1.1.2.3.3\"><mo mathsize=\"90%\" id=\"S1.T1.7.1.1.1.m1.1.2.3.3a\">−</mo><mn mathsize=\"90%\" id=\"S1.T1.7.1.1.1.m1.1.2.3.3.2\">1</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\" id=\"S1.T1.7.1.1.1.m1.1c\">\\mathcal{O}(\\epsilon^{-3/2}m^{-1}</annotation></semantics></math><span id=\"S1.T1.7.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">)</span>\n</td>\n<td id=\"S1.T1.7.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S1.T1.7.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">✗</span></td>\n<td id=\"S1.T1.7.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S1.T1.7.1.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td id=\"S1.T1.7.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S1.T1.7.1.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">✗</span></td>\n</tr>\n<tr id=\"S1.T1.8.2.2\" class=\"ltx_tr\">\n<th id=\"S1.T1.8.2.2.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span id=\"S1.T1.8.2.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">AdaFBiO </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.8.2.2.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>Huang<span id=\"S1.T1.8.2.2.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">2022</a><span id=\"S1.T1.8.2.2.2.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</th>\n<td id=\"S1.T1.8.2.2.1\" class=\"ltx_td ltx_align_center\">\n<math id=\"S1.T1.8.2.2.1.m1.1\" class=\"ltx_math_unparsed\" alttext=\"\\mathcal{O}(\\epsilon^{-3/2}\" display=\"inline\"><semantics id=\"S1.T1.8.2.2.1.m1.1a\"><mrow id=\"S1.T1.8.2.2.1.m1.1b\"><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"90%\" id=\"S1.T1.8.2.2.1.m1.1.1\">𝒪</mi><mrow id=\"S1.T1.8.2.2.1.m1.1.2\"><mo maxsize=\"90%\" minsize=\"90%\" id=\"S1.T1.8.2.2.1.m1.1.2.1\">(</mo><msup id=\"S1.T1.8.2.2.1.m1.1.2.2\"><mi mathsize=\"90%\" id=\"S1.T1.8.2.2.1.m1.1.2.2.2\">ϵ</mi><mrow id=\"S1.T1.8.2.2.1.m1.1.2.2.3\"><mo mathsize=\"90%\" id=\"S1.T1.8.2.2.1.m1.1.2.2.3a\">−</mo><mrow id=\"S1.T1.8.2.2.1.m1.1.2.2.3.2\"><mn mathsize=\"90%\" id=\"S1.T1.8.2.2.1.m1.1.2.2.3.2.2\">3</mn><mo maxsize=\"90%\" minsize=\"90%\" stretchy=\"true\" symmetric=\"true\" id=\"S1.T1.8.2.2.1.m1.1.2.2.3.2.1\">/</mo><mn mathsize=\"90%\" id=\"S1.T1.8.2.2.1.m1.1.2.2.3.2.3\">2</mn></mrow></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\" id=\"S1.T1.8.2.2.1.m1.1c\">\\mathcal{O}(\\epsilon^{-3/2}</annotation></semantics></math><span id=\"S1.T1.8.2.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">)</span>\n</td>\n<td id=\"S1.T1.8.2.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S1.T1.8.2.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">✗</span></td>\n<td id=\"S1.T1.8.2.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S1.T1.8.2.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">✗</span></td>\n<td id=\"S1.T1.8.2.2.5\" class=\"ltx_td ltx_align_center\"><span id=\"S1.T1.8.2.2.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n</tr>\n<tr id=\"S1.T1.9.3.3\" class=\"ltx_tr\">\n<th id=\"S1.T1.9.3.3.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span id=\"S1.T1.9.3.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">FedNest </span><cite class=\"ltx_cite ltx_citemacro_citep\"><span id=\"S1.T1.9.3.3.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">(</span>Tarzanagh et al.<span id=\"S1.T1.9.3.3.2.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">, </span><a href=\"#bib.bib41\" title=\"\" class=\"ltx_ref\">2022</a><span id=\"S1.T1.9.3.3.2.4.3\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></cite>\n</th>\n<td id=\"S1.T1.9.3.3.1\" class=\"ltx_td ltx_align_center\">\n<math id=\"S1.T1.9.3.3.1.m1.1\" class=\"ltx_math_unparsed\" alttext=\"\\mathcal{O}(\\epsilon^{-2}\" display=\"inline\"><semantics id=\"S1.T1.9.3.3.1.m1.1a\"><mrow id=\"S1.T1.9.3.3.1.m1.1b\"><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"90%\" id=\"S1.T1.9.3.3.1.m1.1.1\">𝒪</mi><mrow id=\"S1.T1.9.3.3.1.m1.1.2\"><mo maxsize=\"90%\" minsize=\"90%\" id=\"S1.T1.9.3.3.1.m1.1.2.1\">(</mo><msup id=\"S1.T1.9.3.3.1.m1.1.2.2\"><mi mathsize=\"90%\" id=\"S1.T1.9.3.3.1.m1.1.2.2.2\">ϵ</mi><mrow id=\"S1.T1.9.3.3.1.m1.1.2.2.3\"><mo mathsize=\"90%\" id=\"S1.T1.9.3.3.1.m1.1.2.2.3a\">−</mo><mn mathsize=\"90%\" id=\"S1.T1.9.3.3.1.m1.1.2.2.3.2\">2</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\" id=\"S1.T1.9.3.3.1.m1.1c\">\\mathcal{O}(\\epsilon^{-2}</annotation></semantics></math><span id=\"S1.T1.9.3.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">)</span>\n</td>\n<td id=\"S1.T1.9.3.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"S1.T1.9.3.3.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">✗</span></td>\n<td id=\"S1.T1.9.3.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"S1.T1.9.3.3.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">✗</span></td>\n<td id=\"S1.T1.9.3.3.5\" class=\"ltx_td ltx_align_center\"><span id=\"S1.T1.9.3.3.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n</tr>\n<tr id=\"S1.T1.10.4.4\" class=\"ltx_tr\">\n<th id=\"S1.T1.10.4.4.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span id=\"S1.T1.10.4.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">FedMBO</span></th>\n<td id=\"S1.T1.10.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<math id=\"S1.T1.10.4.4.1.m1.1\" class=\"ltx_math_unparsed\" alttext=\"\\mathcal{O}(\\epsilon^{-2}n^{-1}\" display=\"inline\"><semantics id=\"S1.T1.10.4.4.1.m1.1a\"><mrow id=\"S1.T1.10.4.4.1.m1.1b\"><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"90%\" id=\"S1.T1.10.4.4.1.m1.1.1\">𝒪</mi><mrow id=\"S1.T1.10.4.4.1.m1.1.2\"><mo maxsize=\"90%\" minsize=\"90%\" id=\"S1.T1.10.4.4.1.m1.1.2.1\">(</mo><msup id=\"S1.T1.10.4.4.1.m1.1.2.2\"><mi mathsize=\"90%\" id=\"S1.T1.10.4.4.1.m1.1.2.2.2\">ϵ</mi><mrow id=\"S1.T1.10.4.4.1.m1.1.2.2.3\"><mo mathsize=\"90%\" id=\"S1.T1.10.4.4.1.m1.1.2.2.3a\">−</mo><mn mathsize=\"90%\" id=\"S1.T1.10.4.4.1.m1.1.2.2.3.2\">2</mn></mrow></msup><msup id=\"S1.T1.10.4.4.1.m1.1.2.3\"><mi mathsize=\"90%\" id=\"S1.T1.10.4.4.1.m1.1.2.3.2\">n</mi><mrow id=\"S1.T1.10.4.4.1.m1.1.2.3.3\"><mo mathsize=\"90%\" id=\"S1.T1.10.4.4.1.m1.1.2.3.3a\">−</mo><mn mathsize=\"90%\" id=\"S1.T1.10.4.4.1.m1.1.2.3.3.2\">1</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\" id=\"S1.T1.10.4.4.1.m1.1c\">\\mathcal{O}(\\epsilon^{-2}n^{-1}</annotation></semantics></math><span id=\"S1.T1.10.4.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">)</span>\n</td>\n<td id=\"S1.T1.10.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S1.T1.10.4.4.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td id=\"S1.T1.10.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S1.T1.10.4.4.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n<td id=\"S1.T1.10.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S1.T1.10.4.4.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">✓</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Federated learning is a privacy-preserving training paradigm over distributed networks that are designed for edge computing ",
                "(McMahan et al., ",
                "2017",
                ")",
                ". In federated learning, multiple edge devices (or clients) work together to learn a global model under the coordination of a central server. Instead of transmitting user data directly to the central server, each client stores data and computes locally and only transmits the privacy-preserving information. This paradigm is increasingly attractive due to the growing computational power of edge devices and the increasing demand for privacy protection.\nFederated learning is facing more challenges than traditional distributed optimization due to the high communication cost, data and system heterogeneity, and privacy concerns. Recent years have witnessed great progress in the algorithmic design and system deployment to address such challenges ",
                "(Wang & Joshi, ",
                "2021",
                "; Karimireddy et al., ",
                "2019",
                "; Stich & Karimireddy, ",
                "2020",
                ")",
                ".",
                "Recently, federated bilevel learning has received increasing attention ",
                "(Chen et al., ",
                "2018",
                "; Fallah et al., ",
                "2020",
                "; Zeng et al., ",
                "2021",
                ")",
                " because many modern machine learning problems naturally exhibit a bilevel optimization structure. For example, ",
                "Chen et al. ",
                "2018",
                "; Fallah et al. ",
                "2020",
                " studied the federated meta-learning problems, ",
                "Khodak et al. ",
                "2021",
                " proposed federated hyperparameter optimization approaches, and ",
                "Zeng et al. ",
                "2021",
                " improved the fairness in federated learning using a bilevel method. This motivates us to study the following federated bilevel optimization problem.\n",
                "where ",
                "f",
                "i",
                "​",
                "(",
                "x",
                ",",
                "y",
                ")",
                "=",
                "𝔼",
                "​",
                "f",
                "i",
                "​",
                "(",
                "x",
                ",",
                "y",
                ";",
                "ξ",
                "i",
                ")",
                ",",
                "g",
                "i",
                "​",
                "(",
                "x",
                ",",
                "y",
                ")",
                "=",
                "𝔼",
                "​",
                "g",
                "i",
                "​",
                "(",
                "x",
                ",",
                "y",
                ";",
                "ζ",
                "i",
                ")",
                "formulae-sequence",
                "subscript",
                "𝑓",
                "𝑖",
                "𝑥",
                "𝑦",
                "𝔼",
                "subscript",
                "𝑓",
                "𝑖",
                "𝑥",
                "𝑦",
                "superscript",
                "𝜉",
                "𝑖",
                "subscript",
                "𝑔",
                "𝑖",
                "𝑥",
                "𝑦",
                "𝔼",
                "subscript",
                "𝑔",
                "𝑖",
                "𝑥",
                "𝑦",
                "superscript",
                "𝜁",
                "𝑖",
                "f_{i}(x,y)=\\mathbb{E}f_{i}(x,y;\\xi^{i}),\\,g_{i}(x,y)=\\mathbb{E}g_{i}(x,y;\\zeta^{i})",
                " are stochastic upper- and lower-level loss functions of client ",
                "i",
                "𝑖",
                "i",
                ", and ",
                "m",
                "𝑚",
                "m",
                " is the total number of clients. Existing federated learning algorithms like FedAvg and its variants ",
                "(McMahan et al., ",
                "2017",
                ")",
                " cannot be applied to solve the federated bilevel problem ",
                "section",
                " ",
                "1",
                " due to the nested optimization structure, the global Hessian inverse estimation in the hypergradient (i.e., ",
                "∇",
                "Φ",
                "​",
                "(",
                "x",
                ")",
                "∇",
                "Φ",
                "𝑥",
                "\\nabla\\Phi(x)",
                ") computation, and the data heterogeneity in both the upper- and lower-level problems.",
                "Recently, several approaches ",
                "(Li et al., ",
                "2022",
                "; Tarzanagh et al., ",
                "2022",
                "; Gao, ",
                "2022",
                "; Huang, ",
                "2022",
                ")",
                " have been proposed to efficiently solve ",
                "section",
                " ",
                "1",
                ". ",
                "Li et al. ",
                "2022",
                " considered a special case of ",
                "section",
                " ",
                "1",
                ", where the lower-level problem is minimized only locally, i.e., ",
                "y",
                "i",
                "∗",
                "​",
                "(",
                "x",
                ")",
                "=",
                "arg",
                "​",
                "min",
                "y",
                "⁡",
                "g",
                "i",
                "​",
                "(",
                "x",
                ",",
                "y",
                ")",
                "superscript",
                "subscript",
                "𝑦",
                "𝑖",
                "𝑥",
                "subscript",
                "arg",
                "min",
                "𝑦",
                "subscript",
                "𝑔",
                "𝑖",
                "𝑥",
                "𝑦",
                "y_{i}^{*}(x)=\\operatorname*{arg\\,min}_{y}g_{i}(x,y)",
                " for each client ",
                "i",
                "𝑖",
                "i",
                ". For the general case,\n",
                "Gao ",
                "2022",
                " focused on the homogeneous setting with i.i.d. datesets and proposed momentum-based distributed bilevel algorithms. In the more practical but challenging heterogeneous setting with non-i.i.d. datasets, ",
                "Huang ",
                "2022",
                " proposed a momentum-based method AdaFBiO based on fully local hypergradient estimators.\n",
                "Tarzanagh et al. ",
                "2022",
                " proposed FedNest based on an implicit differentiation based federated hypergradient estimator. In the inner loop, FedNest calls ",
                "T",
                "𝑇",
                "T",
                " times of FedInn, which is a federated stochastic variance reduced gradient (FedSVRG) algorithm, to solve the lower-level problem. Then FedNest calls FedOut, which constructs a federated hypergradient estimator, to optimize the upper-level problem.\nHowever, as shown in ",
                "Table",
                " ",
                "1",
                ", both AdaFBiO and FedNest fail to achieve a linear speedup for convergence in training due to the fully local hypergradient estimation, and\nthe high correlation among the individual hypergradient estimators computed by all clients, respectively. In addition, they are restricted to the full client participation.\nThen, an important but open question remains:\n",
                "Can we develop an easy-to-implement federated method, which achieves a linear speedup for convergence in the general heterogeneous setting, and allows flexible partial client participation?",
                "Our contributions.",
                "\nIn this paper,\nwe provide an affirmative answer to the above question by proposing a novel federated algorithm called Federated Minibatch Bilevel Optimization (FedMBO). Our contributions are summarized as follows.",
                "The proposed FedMBO follows a double-loop scheme in bilevel optimization and consists of two important components. For the inner loop, FedMBO adopts a simple Minibatch Stochastic Gradient Descent (SGD) algorithm. Compared with FedAvg and FedSVRG, the minibatch SGD and its accelerated variant are more immune to the heterogeneity of the problem ",
                "(Woodworth et al., ",
                "2020b",
                ")",
                ", which is critical in achieving the linear speedup for convergence under the bilevel optimization structure. For the outer loop, FedMBO features a Parallel Hypergradient Estimator (PHE) with a novel multi-round client sampling scheme. Compared to IHGP ",
                "(Tarzanagh et al., ",
                "2022",
                ")",
                ", our PHE procedure allows either full or partial client participation, and more importantly, achieves a variance bound linearly decreasing w.r.t. the number of participating clients. We anticipate that PHE can be of independent interest to other settings such as decentralized or asynchronous bilevel optimization.",
                "We show that FedMBO achieves a convergence rate of ",
                "𝒪",
                "​",
                "(",
                "1",
                "n",
                "​",
                "K",
                "+",
                "1",
                "K",
                "+",
                "n",
                "K",
                "3",
                "/",
                "2",
                ")",
                "𝒪",
                "1",
                "𝑛",
                "𝐾",
                "1",
                "𝐾",
                "𝑛",
                "superscript",
                "𝐾",
                "3",
                "2",
                "\\mathcal{O}\\big{(}\\frac{1}{\\sqrt{nK}}+\\frac{1}{K}+\\frac{\\sqrt{n}}{K^{3/2}}\\big{)}",
                " and a sample complexity (i.e., the number of samples to achieve an ",
                "ϵ",
                "italic-ϵ",
                "\\epsilon",
                "-stationary point) of ",
                "𝒪",
                "​",
                "(",
                "ϵ",
                "−",
                "2",
                "​",
                "n",
                "−",
                "1",
                ")",
                "𝒪",
                "superscript",
                "italic-ϵ",
                "2",
                "superscript",
                "𝑛",
                "1",
                "\\mathcal{O}(\\epsilon^{-2}n^{-1})",
                ", which outperforms that of FedNest ",
                "(Tarzanagh et al., ",
                "2022",
                ")",
                " by an order of ",
                "n",
                "𝑛",
                "n",
                " due to the linear speedup. As shown in ",
                "Table",
                " ",
                "1",
                ", compared to the momentum-based LocalBSGVR ",
                "(Gao, ",
                "2022",
                ")",
                " and AdaFBiO ",
                "(Huang, ",
                "2022",
                ")",
                ", our FedMBO is more flexible with partial client participation, and more importantly, achieves the linear speedup for convergence even in the presence of data heterogeneity.",
                "We conduct extensive experiments to validate our theoretical results, and further demonstrate the effectiveness of our proposed federated hypergradient estimator and the FedMBO algorithm."
            ]
        ]
    }
}