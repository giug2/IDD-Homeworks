{
    "id_table_1": {
        "caption": "Table 1:  Effectiveness of Proposed Shap Pooling.",
        "table": "S4.T1.1",
        "footnotes": [],
        "references": [
            "Our whole framework consists of Parallel Feature Extraction Pipelines for each modality and a SHAP-CAT pipeline for the predictions of multimodal representations, as illustrated in Fig.  1 . Given approximate H&E-IHC paired dataset  I h  e , I i  h  c subscript I h e subscript I i h c I_{he},I_{ihc} italic_I start_POSTSUBSCRIPT italic_h italic_e end_POSTSUBSCRIPT , italic_I start_POSTSUBSCRIPT italic_i italic_h italic_c end_POSTSUBSCRIPT , we firstly use pre-trained CycleGAN to generate reconstructed H&E images  I r  e  c  _  h  e subscript I r e c _ h e I_{rec\\_he} italic_I start_POSTSUBSCRIPT italic_r italic_e italic_c _ italic_h italic_e end_POSTSUBSCRIPT . Then we separately train each modality to extract bag-level representations for each modality for further late fusion.",
            "The virtual staining technique we used in our paper is CycleGAN  (Zhu et al.,  2017 ) , which is specifically designed for unpaired datasets. The input of our framework is H&E-IHC approximate paired datasets  I h  e , I i  h  c subscript I h e subscript I i h c I_{he},I_{ihc} italic_I start_POSTSUBSCRIPT italic_h italic_e end_POSTSUBSCRIPT , italic_I start_POSTSUBSCRIPT italic_i italic_h italic_c end_POSTSUBSCRIPT  with labels. Approximate paired here means that these two sets of images are not aligned pixel to pixel, whereas the same pair of images are offset by about 10% in the vertical and horizontal directions. There are two translators  G : I h  e  I i  h  c : G  subscript I h e subscript I i h c G:I_{he}\\rightarrow I_{ihc} italic_G : italic_I start_POSTSUBSCRIPT italic_h italic_e end_POSTSUBSCRIPT  italic_I start_POSTSUBSCRIPT italic_i italic_h italic_c end_POSTSUBSCRIPT , and  F : I i  h  c  I h  e : F  subscript I i h c subscript I h e F:I_{ihc}\\rightarrow I_{he} italic_F : italic_I start_POSTSUBSCRIPT italic_i italic_h italic_c end_POSTSUBSCRIPT  italic_I start_POSTSUBSCRIPT italic_h italic_e end_POSTSUBSCRIPT  (as shown in Fig  1 .a).  G G G italic_G  and  F F F italic_F  are trained simultaneously to encourages  F  ( G  ( I h  e ) )  I h  e F G subscript I h e subscript I h e F(G(I_{he}))\\approx I_{he} italic_F ( italic_G ( italic_I start_POSTSUBSCRIPT italic_h italic_e end_POSTSUBSCRIPT ) )  italic_I start_POSTSUBSCRIPT italic_h italic_e end_POSTSUBSCRIPT  and  G  ( F  ( I i  h  c ) )  I i  h  c G F subscript I i h c subscript I i h c G(F(I_{ihc}))\\approx I_{ihc} italic_G ( italic_F ( italic_I start_POSTSUBSCRIPT italic_i italic_h italic_c end_POSTSUBSCRIPT ) )  italic_I start_POSTSUBSCRIPT italic_i italic_h italic_c end_POSTSUBSCRIPT . Also, there are two adversarial discriminators  D h  e subscript D h e D_{he} italic_D start_POSTSUBSCRIPT italic_h italic_e end_POSTSUBSCRIPT  and  D i  h  c subscript D i h c D_{ihc} italic_D start_POSTSUBSCRIPT italic_i italic_h italic_c end_POSTSUBSCRIPT , where  D h  e subscript D h e D_{he} italic_D start_POSTSUBSCRIPT italic_h italic_e end_POSTSUBSCRIPT  aims to discriminate between images  I h  e subscript I h e I_{he} italic_I start_POSTSUBSCRIPT italic_h italic_e end_POSTSUBSCRIPT  and translated images  F  ( I i  h  c ) F subscript I i h c F(I_{ihc}) italic_F ( italic_I start_POSTSUBSCRIPT italic_i italic_h italic_c end_POSTSUBSCRIPT ) . Similarly,  D i  h  c subscript D i h c D_{ihc} italic_D start_POSTSUBSCRIPT italic_i italic_h italic_c end_POSTSUBSCRIPT  aims to distinguish between  I i  h  c subscript I i h c I_{ihc} italic_I start_POSTSUBSCRIPT italic_i italic_h italic_c end_POSTSUBSCRIPT  and  G  ( I h  e ) G subscript I h e {G(I_{he})} italic_G ( italic_I start_POSTSUBSCRIPT italic_h italic_e end_POSTSUBSCRIPT ) . The final objective is:",
            "Before late fusion, we propose an efficient and highly interpretable SHAP pool to reduce dimensions of bag-level representations  z z z italic_z  to avoid the curve of dimensions. We model the dimension reduction as an attribution problem that attributes the prediction of machine learning models to their inputs  (Lundberg & Lee,  2017 ; Ribeiro et al.,  2016 ; Shrikumar et al.,  2017 ) . For bag-level representations  z = [ d 1 , d 2 , ... , d 512 ]  R 1  512 z subscript d 1 subscript d 2 ... subscript d 512 superscript R 1 512 z=[d_{1},d_{2},\\dots,d_{512}]\\in\\mathbb{R}^{1\\times 512} italic_z = [ italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , italic_d start_POSTSUBSCRIPT 512 end_POSTSUBSCRIPT ]  blackboard_R start_POSTSUPERSCRIPT 1  512 end_POSTSUPERSCRIPT , each dimension  d d d italic_d  has attribution values corresponding to the contributions toward the model prediction. Dimensions that have no effect on the output are assigned zero attribution, suggesting no relevance, whereas dimensions that significantly influence the output exhibit higher attribution values, indicating their importance. As illustrated in Fig  1 (c), we visualize the attribution values of each dimension to understand the magnitude of how much it impacts the output.",
            "The proposed SHAP pool selects the top 32 important dimensions for each modality and then applies the Kronecker product as late fusion. This module constructs the joint representations as the input of the final prediction for multimodalities. The whole algorithm is shown in Algorithm  1 . We further introduce Shapley-value-based dimension reduction and multimodal fusion deeply in the next section.",
            "Therefore, we must decrease the dimensionality of representations. Prior research has utilized average pooling or max pooling for this purpose  (Wang et al.,  2024 ; Chen et al.,  2020 ) . Our method deviates from traditional methods by offering a more accurate and interpretable strategy for fusion. We are the first to implement a Shapley-value-based technique to reduce dimensions in image modality representations. We also evaluate our SHAP pool in a single modality by reducing bag-level representation  z  R 1  512 z superscript R 1 512 z\\in\\mathbb{R}^{1\\times 512} italic_z  blackboard_R start_POSTSUPERSCRIPT 1  512 end_POSTSUPERSCRIPT  to  f  R 1  32 f superscript R 1 32 f\\in\\mathbb{R}^{1\\times 32} italic_f  blackboard_R start_POSTSUPERSCRIPT 1  32 end_POSTSUPERSCRIPT  and then aggregated by different classifiers (as shown in Tab  1 ). We compare our SHAP pooling with average pooling, max pooling and selecting 32 dimensions randomly. Our SHAP pooling performs well across different classifiers."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Experiment Results on the BCI Dataset.  The performance is reported as AUC and ACC.",
        "table": "S5.T2.3",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "The embedding  r k subscript r k r_{k} italic_r start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT  is compressed by a fully-connected layer to  h k subscript h k \\bm{h}_{k} bold_italic_h start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT . Then  h k subscript h k h_{k} italic_h start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT  is fed into the multi-class classification network, aggregating the set of embeddings  h k subscript h k h_{k} italic_h start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT  into a bag-level embedding  z n =  k = 1 K a k , n  h k subscript z n superscript subscript k 1 K subscript a k n subscript h k \\bm{z}_{n}=\\sum_{k=1}^{K}a_{k,n}\\bm{h}_{k} bold_italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT =  start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_a start_POSTSUBSCRIPT italic_k , italic_n end_POSTSUBSCRIPT bold_italic_h start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , where the attention scores for the k-th instance is computed by Eq.  2 .",
            "Given bag-level representation  { z n } n = 1 N  R N  512 superscript subscript subscript z n n 1 N superscript R N 512 \\{z_{n}\\}_{n=1}^{N}\\in\\mathbb{R}^{N\\times 512} { italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_N  512 end_POSTSUPERSCRIPT  with labels  y y y italic_y , we train a random forest classifier on  { z n , y n } n = 1 N superscript subscript subscript z n subscript y n n 1 N \\{z_{n},y_{n}\\}_{n=1}^{N} { italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT  for estimation to obtain the attribution value  [  1 ,  2 , ... ,  512 ] subscript italic- 1 subscript italic- 2 ... subscript italic- 512 [\\phi_{1},\\phi_{2},\\dots,\\phi_{512}] [ italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , italic_ start_POSTSUBSCRIPT 512 end_POSTSUBSCRIPT ]  for dimension reduction. The whole SHAP pool is demonstrated in Algorithm  2 .",
            "Tab  2  shows the detailed results on the BCI dataset, and Tab  3  presents the results on the IHC4BC-ER and IHC4BC-PR datasets. Most previous models only deal with a single modality. Multiple modalities achieve higher performance than all models in a single modality. Our SHAP-CAT method includes modality enhancement via virtual staining, efficient multimodal fusion by Shapley-value-based dimension reduction, and finally, aggregation in the MLP or CatBoost classifiers  (Prokhorenkova et al.,  2018 ) , achieving higher accuracy across BCI and IHC4BC datasets.",
            "As mentioned in the previous section, our framework uses the CLAM pipeline to extract the bag-level representations  z z z italic_z . Therefore, we also report the performance of the baseline trained by reconstructed H&E modality. As shown in Tab  2  and Tab  3 , the reconstructed H&E modality generated by CycleGAN results in lower performance when it is used as the main input for the single-modality model. However, it can enhance multimodal model performance when we use our SHAP-CAT fusion to efficiently capture information across three modalities. In the BCI dataset, three original pipelines, which train H&E, IHC, and rec H&E modalities separately to extract bag-level representations  z h  e , z i  h  c , z r  e  c  _  h  e subscript z h e subscript z i h c subscript z r e c _ h e z_{he},z_{ihc},z_{rec\\_he} italic_z start_POSTSUBSCRIPT italic_h italic_e end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_i italic_h italic_c end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_r italic_e italic_c _ italic_h italic_e end_POSTSUBSCRIPT , achieve accuracy in 0.909, 0.917 and 0.787. However, their multimodal representations can be aggregated by the classifier in much higher results, achieving 0.959 in accuracy. This situation also occurs in IHC4BC datasets."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Experiment Results on IHC4BC Dataset.  The performance is reported as AUC and ACC for IHC4BC-ER and IHC4BC-PR.",
        "table": "S5.T3.3",
        "footnotes": [],
        "references": [
            "Tab  2  shows the detailed results on the BCI dataset, and Tab  3  presents the results on the IHC4BC-ER and IHC4BC-PR datasets. Most previous models only deal with a single modality. Multiple modalities achieve higher performance than all models in a single modality. Our SHAP-CAT method includes modality enhancement via virtual staining, efficient multimodal fusion by Shapley-value-based dimension reduction, and finally, aggregation in the MLP or CatBoost classifiers  (Prokhorenkova et al.,  2018 ) , achieving higher accuracy across BCI and IHC4BC datasets.",
            "As mentioned in the previous section, our framework uses the CLAM pipeline to extract the bag-level representations  z z z italic_z . Therefore, we also report the performance of the baseline trained by reconstructed H&E modality. As shown in Tab  2  and Tab  3 , the reconstructed H&E modality generated by CycleGAN results in lower performance when it is used as the main input for the single-modality model. However, it can enhance multimodal model performance when we use our SHAP-CAT fusion to efficiently capture information across three modalities. In the BCI dataset, three original pipelines, which train H&E, IHC, and rec H&E modalities separately to extract bag-level representations  z h  e , z i  h  c , z r  e  c  _  h  e subscript z h e subscript z i h c subscript z r e c _ h e z_{he},z_{ihc},z_{rec\\_he} italic_z start_POSTSUBSCRIPT italic_h italic_e end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_i italic_h italic_c end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_r italic_e italic_c _ italic_h italic_e end_POSTSUBSCRIPT , achieve accuracy in 0.909, 0.917 and 0.787. However, their multimodal representations can be aggregated by the classifier in much higher results, achieving 0.959 in accuracy. This situation also occurs in IHC4BC datasets."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Ablation Study of Virtual Staining on BCI and IHC4BC datasets.  Results are reported as AUC and ACC for each modality.",
        "table": "S6.T4.3",
        "footnotes": [],
        "references": [
            "We evaluate our virtual staining strategy. Since we use CLAM to extract bag-level representations, we compare single, double, and triple modalities in Table  4 . Also, we compare the results of two modalities(H&E-IHC) with three modalities(H&E, IHC, and reconstructed H&E) processed by the same pooling across different classifiers as aggregations in Table  5 . Whats more, we evaluate our SHAP pool with average pool across different classifiers in Table  5 ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Ablation Study of SHAP pooling on BCI dataset . The results are reported as the average AUC and ACC metrics for two and three multimodal settings.",
        "table": "S6.T5.3",
        "footnotes": [],
        "references": [
            "We evaluate our virtual staining strategy. Since we use CLAM to extract bag-level representations, we compare single, double, and triple modalities in Table  4 . Also, we compare the results of two modalities(H&E-IHC) with three modalities(H&E, IHC, and reconstructed H&E) processed by the same pooling across different classifiers as aggregations in Table  5 . Whats more, we evaluate our SHAP pool with average pool across different classifiers in Table  5 .",
            "We claim that virtual staining may not be good for the training model as the main input, but it is good for enhancing performance as an extra modality. As shown in Table  5 , our reconstructed modality performs well across different classifiers, compared to the single or double modality."
        ]
    }
}