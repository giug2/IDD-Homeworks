{
    "S2.T1": {
        "caption": "Table 1. Preprocessing transformations available.",
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S2.T1.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S2.T1.4.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S2.T1.4.1.1.1\"><span class=\"ltx_text\" id=\"S2.T1.4.1.1.1.1\" style=\"font-size:90%;\">Op Name</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S2.T1.4.1.1.2\"><span class=\"ltx_text\" id=\"S2.T1.4.1.1.2.1\" style=\"font-size:90%;\">Description</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S2.T1.4.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T1.4.2.1.1\"><span class=\"ltx_text\" id=\"S2.T1.4.2.1.1.1\" style=\"font-size:90%;\">Decode</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T1.4.2.1.2\"><span class=\"ltx_text\" id=\"S2.T1.4.2.1.2.1\" style=\"font-size:90%;\">Decode UTF-8 dataset for processing</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.4.3.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.4.3.2.1\"><span class=\"ltx_text\" id=\"S2.T1.4.3.2.1.1\" style=\"font-size:90%;\">FillMissing</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.4.3.2.2\"><span class=\"ltx_text\" id=\"S2.T1.4.3.2.2.1\" style=\"font-size:90%;\">Fill missing values for all features</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.4.4.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.4.4.3.1\"><span class=\"ltx_text\" id=\"S2.T1.4.4.3.1.1\" style=\"font-size:90%;\">Hex2Int</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.4.4.3.2\"><span class=\"ltx_text\" id=\"S2.T1.4.4.3.2.1\" style=\"font-size:90%;\">Convert hexadecimal values to decimal (sparse)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.4.5.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.4.5.4.1\"><span class=\"ltx_text\" id=\"S2.T1.4.5.4.1.1\" style=\"font-size:90%;\">Modulus</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.4.5.4.2\"><span class=\"ltx_text\" id=\"S2.T1.4.5.4.2.1\" style=\"font-size:90%;\">Compute positive modulus (sparse)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.4.6.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.4.6.5.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.6.5.1.1\" style=\"font-size:90%;\">GenVocab</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.4.6.5.2\"><span class=\"ltx_text\" id=\"S2.T1.4.6.5.2.1\" style=\"font-size:90%;\">Extract a set of unique IDs (sparse)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.4.7.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.4.7.6.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.4.7.6.1.1\" style=\"font-size:90%;\">ApplyVocab</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.4.7.6.2\"><span class=\"ltx_text\" id=\"S2.T1.4.7.6.2.1\" style=\"font-size:90%;\">Generate integer-encoded mappings (sparse)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.4.8.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.4.8.7.1\"><span class=\"ltx_text\" id=\"S2.T1.4.8.7.1.1\" style=\"font-size:90%;\">Neg2Zero</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.4.8.7.2\"><span class=\"ltx_text\" id=\"S2.T1.4.8.7.2.1\" style=\"font-size:90%;\">Change negative values to zero (dense)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.4.9.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.4.9.8.1\"><span class=\"ltx_text\" id=\"S2.T1.4.9.8.1.1\" style=\"font-size:90%;\">Logarithm</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.4.9.8.2\"><span class=\"ltx_text\" id=\"S2.T1.4.9.8.2.1\" style=\"font-size:90%;\">Do log(x+1) operation (dense)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.4.10.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S2.T1.4.10.9.1\"><span class=\"ltx_text\" id=\"S2.T1.4.10.9.1.1\" style=\"font-size:90%;\">Concatenate</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S2.T1.4.10.9.2\"><span class=\"ltx_text\" id=\"S2.T1.4.10.9.2.1\" style=\"font-size:90%;\">Concatenate final results from multiple threads</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "In this paper, we use two representative examples from Meta\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Meta, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.14912v1#bib.bib51\" title=\"\">2024</a>)</cite> and Google&#160;\n(Meta, 2024) and Google\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Tensorflow, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.14912v1#bib.bib72\" title=\"\">2024</a>)</cite> to illustrate the preprocessing stages used in practice and run on CPUs.\nTable \n(Tensorflow, 2024) to illustrate the preprocessing stages used in practice and run on CPUs.\nTable 1 lists the detailed functionalities of the involved operators.\nAmong them, GenVocab is responsible to create a vocabulary table for all columns of sparse features, and ApplyVocab then iterates anew over the dataset to generate the final embedding table.",
            "Google\u2019s DLRM pipeline.\nGoogle\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Tensorflow, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.14912v1#bib.bib72\" title=\"\">2024</a>)</cite> open-sourced another DLRM project, where the pipeline is similar\nand Table&#160;\n(Tensorflow, 2024) open-sourced another DLRM project, where the pipeline is similar\nand Table\u00a01 covers all operators in both pipelines.\nAn advantage of Google\u2019s solution for preprocessing is its integration in Apache Beam, making it easier to use in cloud environments.",
            "Processing control flow.\nFigure 5 shows the dataflow with the operators in the FPGA and the slight adjustment of the original sequence of operators, as we can merge some of them to simplify the overall dataflow.\nOnce the dataset is decoded, the data preprocessing is conducted via two consecutive loops.\nIn the first loop, Piper reads the whole dataset and generates the corresponding vocabulary table. The size of vocabulary determines whether it is stored in on-chip SRAM or off-chip HBM, which significantly influences the overall performance due to random memory accesses.\nIn the second loop, Piper rereads the dataset and maps each feature into the corresponding value in the vocabulary table.\nGenVocab and ApplyVocab behave differently in the 1st and 2nd loops, while other PEs behave the same to process input features.\nFor GenVocab, it filters some unique inputs in the first loop and passes all inputs in the second loop.\nFor ApplyVocab, it writes the appearing sequence of unique inputs into memory in the first loop and reads corresponding values in the second loop.\nSome operators listed in Table 1 are missing in Figure 5, like FillMissing & Hex2Int, because the FPGA handles bits directly and there is no need for representing Null as in software, so the default value for the empty element after Decode is 0, and there is no need to transform from hexadecimal to decimal explicitly.\nEach PE is a computing unit on the FPGA, and different PEs are interconnected via FIFO channels."
        ]
    }
}