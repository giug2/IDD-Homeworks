{
    "id_table_1": {
        "caption": "Table 1:  Experiment results on long-context retrieval augmented language generation.  All Chunking  denotes the use of the average self-information of all tokens from two concatenated chunks as the similarity score for selecting positive and negative samples in contrastive learning. In contrast,  Precise Chunking  denotes the accurate segmentation of the chunks, utilizing only the average self-information from the second chunk.",
        "table": "A6.T7.5",
        "footnotes": [],
        "references": [
            "where  x i subscript x i x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  is the current token, and  p  ( x i | x i  1 , ... , x 0 ) p conditional subscript x i subscript x i 1 ... subscript x 0 p(x_{i}|x_{i-1},\\dots,x_{0}) italic_p ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT , ... , italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )  is the conditional probability of token  x i subscript x i x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  given the preceding tokens  x i  1 , x i  2 , ... , x 0 subscript x i 1 subscript x i 2 ... subscript x 0 x_{i-1},x_{i-2},\\dots,x_{0} italic_x start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_i - 2 end_POSTSUBSCRIPT , ... , italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  in the sequence. Previous effort  (Duan et al.,  2024 )  has suggested that not all tokens in auto-regressive LLM text equally represent the underlying meaning, as linguistic redundancy often allows a few keywords to convey the essence of long sentences. Additionally, we have observed a specific window in the models output log probabilities. The probability distribution within this window is stable, which we believe indicates better calibration, a finding that is further validated in subsequent experiments. As shown in Figure  1 , we use a sliding window with a length of 20 to calculate the SNR within the window, with a sliding step of 10. We found that at a certain turning point, the SNR tends to stabilize. We choose this stable interval to measure the uncertainty of the model.  We define the span-level probabilities within this window as a measure of uncertainty and treat this confidence score as a special type of entailment relationship  (Lin et al.,  2023b ) , which represents a form of semantic equivalence.  Based on the above observations and the additive property of  I  ( x ) I x I(x) italic_I ( italic_x ) , we present the following definition:",
            "As shown in Table  1 , the following observations can be made:  i ): Robust RAG retrieval model can significantly improve the performance of the 4K context window LLMs; however, this heavily depends on the performance of the retrieval model.  ii ): A powerful retrieval model can enhance the performance of LLMs. Vicuna-7B and LLaMA-2-7B-Chat-HF perform similarly within their 4K context windows, with an average performance difference of about 1.93% without retrieval augmentation. When equipped with a robust retrieval model, the average performance gap can increase to 6.14%.  iii ): Our method achieves the highest average performance, surpassing some open-source embedding models trained on large datasets. However, performance is limited on few-shot learning tasks like TREC due to the need for precise segmentation of in-context exemplars. Still, our method shows a consistent 2% improvement over the baseline. In addition, we have the following important findings.",
            "In Table  1 , our analysis reveals that the span uncertainty method, following SNR calibration, demonstrates significant improvements compared to both  All Chunking  and  Precise Chunking . This enhancement is primarily due to the incorporation of self-information from certain tokens in the first chunk, which comes from the flexible sampling of spans across the two chunks by SNR. Thus, we can draw the following conclusions.  i ): SNR calibration effectively captures the similarity between the two chunks.  ii ): The tokens within the span sampled by SNR play a crucial role in assessing the relationship between the two chunks, suggesting that future research could benefit from focusing on span-level uncertainty."
        ]
    },
    "global_footnotes": []
}