{
    "id_table_1": {
        "caption": "Table 1 :  Comparison of our KG-LLaVA framework between other baselines on the MIMIC-NLE test set, focusing on NLG metrics. The metrics include Area Under the Curve (AUC), BLEU-4 (B4), METEOR (MET.), ROUGE-L (R.L.), and CIDEr scores.",
        "table": "S4.T1.2.1",
        "footnotes": [],
        "references": [
            "In this paper, we present a novel approach for generating NLEs via KG-RAG for thoracic pathologies. Our methodology utilizes medical vision models in conjunction with large language models, resulting in three distinct frameworks: KG-LLaVA, Med-XPT, and Bio-LLaVA, as illustrated in Figure  1 . Each framework is designed to leverage the strengths of KG-RAG in enhancing the accuracy and contextual relevance of NLEs in the medical domain.",
            "In this study, we evaluated the performance of our proposed KG-LLaVA framework against several well-established methods, including RATCHET  [ 5 ] , TieNet  [ 28 ] , and DPT  [ 14 ] , using the MIMIC-NLE  [ 14 ]  dataset. The results, as summarized in Table  1 , clearly demonstrate that KG-LLaVA outperforms the previous methods across a range of evaluation metrics."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Performance comparison of our proposed frameworksBio-LLaVA, Med-XPT, and KG-LLaVAon the MIMIC-NLE test set, focusing on NLG metrics. All frameworks incorporate KG-RAG module. Evaluation metrics include BLEU-4 (B4), METEOR (MET.), ROUGE-L (R.L.), and CIDEr, scores.",
        "table": "S4.T2.2.1",
        "footnotes": [],
        "references": [
            "We further assessed the performance of our three proposed frameworksBio-LLaVA, Med-XPT, and KG-LLaVA all of which incorporate the KG-RAG module. The results, detailed in Table  2 , provide insights into the comparative strengths of each framework in generating NLEs for thoracic pathologies.",
            "The qualitative analysis of the generated NLEs from our proposed frameworksKG-LLaVA, Bio-LLaVA, and Med-XPThighlights distinct differences in their alignment with the ground truth (GT) as shown in Figure  2 . KG-LLaVA accurately replicates the GT by identifying the underlying infectious infiltrate, showcasing its strong alignment with expert annotations. In contrast, Bio-LLaVA introduces an alternative diagnosis, suggesting a new right lower lobe opacity possibly due to aspiration or pneumonia, which, while clinically plausible, diverges from the GT. Med-XPT incorrectly focuses on a right lower lobe opacity concerning consolidation, indicating challenges in precise localization and consistency. These findings underscore KG-LLaVAs effectiveness in generating accurate NLEs, while also illustrating the flexibility and limitations of Bio-LLaVA and Med-XPT in clinical interpretation."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Comparative analysis of the performance of Med-XPT, and KG-LLaVA across different RAG methods and without any RAG. The table includes results for NLG metrics such as BLEU-4 (B4), METEOR, ROUGE-L (R-L), and CIDEr. The \"-\" row shows results without any RAG integration, the \"NLE\" row represents results with natural language explanation-based RAG, and the \"KG\" row reflects the performance when the knowledge graph retrieval module is used.",
        "table": "S5.T3.2.1",
        "footnotes": [],
        "references": [
            "Finally, we conducted a detailed comparison of the two frameworksMed-XPT, and KG-LLaVAacross various configurations: without any Retrieval Augmented Generation (RAG), with standard NLE, and with our proposed KG Retrieval module. The results, as shown in Table  3 , illustrate the impact of different RAG methods on the performance of these frameworks in generating accurate and contextually rich NLEs for thoracic pathologies.  In the NLE configuration, where standard NLEs are generated without KG enhancement, both Med-XPT and KG-LLaVA exhibit strong performance, with KG-LLaVA slightly leading in most metrics. This indicates that while both frameworks leverage their respective architectures effectively, the pre-training knowledge embedded in KG-LLaVA likely contributes to its superior performance."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :   Effect of the Number of Retrieved Knowledge Graph Triplets (K) on NLE. We present the impact of varying the number of retrieved knowledge graph triplets (K) on the performance of the KG-LLaVA model in generating Natural Language Explanations (NLEs). The evaluation metrics include BLEU-4 (B4), METEOR, ROUGE-L (R-L), and CIDEr.",
        "table": "S5.T4.5",
        "footnotes": [],
        "references": [
            "We investigate the effect of the number of retrieved knowledge graph triplets (K) on NLE. As shown in Table  4 , as K increases from 1 to 7, the model demonstrates consistent performance across BLEU-4, METEOR, and ROUGE-L, with the highest CIDEr score observed at K=7. This suggests that retrieving more triplets enhances the richness and relevance of the generated explanations, particularly when the complexity of the input increases. However, the slight fluctuations in other metrics indicate that there is an optimal balance to be struck between the amount of retrieved knowledge and its utility in generating precise explanations."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 :  Comparison of Uni-modal and Cross-modal Retrieval on NLE Performance. We compare the performance of the KG-LLaVA model when using Uni-modal versus Cross-modal retrieval methods for generating Natural Language Explanations (NLEs).",
        "table": "S5.T5.5",
        "footnotes": [],
        "references": [
            "Table  5  shows the comparison of uni-modal retrieval and cross-modal retrieval (ours). The Cross-modal retrieval method demonstrates a clear advantage over the Uni-modal approach across all evaluation metrics, with a particularly notable improvement in CIDEr, where the score increases from 49.9 to 62.2. This substantial gain highlights the effectiveness of integrating both visual and textual modalities in the retrieval process, allowing the model to generate more contextually relevant and accurate Natural Language Explanations (NLEs). The Cross-modal approach enhances the models ability to interpret complex medical images, leading to higher-quality explanations that are more aligned with clinical expectations."
        ]
    },
    "global_footnotes": []
}