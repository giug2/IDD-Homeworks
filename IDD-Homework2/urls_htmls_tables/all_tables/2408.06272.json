{
    "id_table_1": {
        "caption": "Table 1:  Evaluation of our QA responses, with metadata category and external data source category",
        "table": "S6.T1.1.1",
        "footnotes": [],
        "references": [
            "In this section, we provide details about the methodology used in our question-answering model. As shown in Figure  1 , for a given user query, our model provides the answer by extracting the most similar context from the cyber-attack investigation and attribution dataset along with the source of the answer. To obtain the most similar context for the query, the approximate nearest neighbor search is performed on the vector database, and the retrieved context is used as the input prompt for the LLM along with the user query. This approach enables the LLM to provide accurate answers without relying on its outdated knowledge.  We organised the methodology section into three main parts: cyber-attack investigation and attribution knowledge base,  question-answer pair generation, and the RAG-based QA model.",
            "Our methodology uses the RAG model architecture shown in Figure  1 .  The red lines in the figure illustrate the processes involved during inference, while the black line indicates the initial KB creation. This architecture comprises of interconnected components, categorised primarily into: retriever and generator. The retriever relies on the knowledge base derived from the AttackER dataset  (steps 1 and 2 in the model architecture). The core component of the retriever is the vector database, which stores the precomputed vectors of the indexed knowledge base. To convert the raw texts of the knowledge base into dense vector representations suitable for storage, a sentence embedding model is employed. Sentence embedding is more appropriate for capturing the meaning and context of a sentence compared to word embedding. These embeddings are indexed in the vector database to facilitate efficient similarity search.  The embedding and data storage processes in the vector database are illustrated in steps 3 and 4 of Figure  1 .  Once the initial KB is set up based on the AttackER dataset, the model offers two query options: querying the existing KB or querying external data by uploading a PDF file or providing a web URL. For external sources, embeddings are created and added to the knowledge base, extending it with new data (steps 5b and 6). When an analyst provides a query (question), represented in steps 5a, 5c, and 7, it undergoes transformation into its vector representation using the same embedding model (step 8). The approximate nearest neighbour (ANN) algorithm then searches the vector database (step 9), measuring similarity with the query vector using cosine similarity. This process retrieves the top-k most relevant documents (context) related to the query. Once the retriever identifies the relevant documents, they are passed to the generator model along with the query (steps 10a and 10b). The generator (a pretrained LLM), is tasked with generating answers based solely on the provided context (step 11), mitigating the risk of hallucination typical in LLMs. The generated answer is then presented in the user interface (step 12) as shown in the architecture diagram.",
            "In Table  1 , we evaluate the RAG models performance across metadata and external data source categories, revealing significant insights. For metadata-based queries, the model demonstrates high answer relevancy but low faithfulness and context precision, indicating relevance without factual consistency. In contrast, external data source evaluations show improved faithfulness and context recall, with high entity recall, suggesting better performance with richer data. However, variability in context precision and moderate answer correctness across both categories highlights the need for enhanced accuracy in context retrieval and factual alignment to improve the models reliability in cybersecurity applications."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Average metrics score of RAG model for Zero-Shot and Few-Shot prompts on entire database",
        "table": "S6.T2.1.1",
        "footnotes": [],
        "references": [
            "In Sections  2  and  3  we provide the background and related work. We introduce our methodology in Section  4 . The implementation of our QA model is shown in Section  5 , while its evaluation is provided in Section  6 . We conclude in Section  7 .",
            "To illustrate the functionality and accuracy of our RAG-based QA model, we performed several single query experiments. Let us show an example thatconsider a factual query related to a specific cybersecurity incident, the developer and operator of MagicRAT, see Figures  2 .  The model successfully retrieves the most relevant documents from the dataset, generates an accurate answer, and provides the context for which the information was derived, see Figure  3 .",
            "In Table  2 , we compare Zero-Shot and Few-Shot RAG prompts across several metrics. On average, Few-Shot outperforms Zero-Shot, showing higher faithfulness (0.625 vs 0.564) and answer relevancy (0.917 vs 0.859), indicating more accurate and relevant answers. However, Few-Shot has slightly lower context precision (0.660 vs 0.649), suggesting some trade-off in retrieving precise contexts. Context recall and entity recall are higher for Few-Shot, which indicates better context retrieval. Despite lower context relevancy, Few-Shots overall improved answer correctness (0.436 vs 0.408) demonstrates its effectiveness in generating more reliable answers, making it a preferred approach for cybersecurity experts looking for critical and contextually rich responses about attack attribution."
        ]
    },
    "id_table_3": {
        "caption": "",
        "table": "S6.T3.1.1",
        "footnotes": [],
        "references": [
            "In Sections  2  and  3  we provide the background and related work. We introduce our methodology in Section  4 . The implementation of our QA model is shown in Section  5 , while its evaluation is provided in Section  6 . We conclude in Section  7 .",
            "To illustrate the functionality and accuracy of our RAG-based QA model, we performed several single query experiments. Let us show an example thatconsider a factual query related to a specific cybersecurity incident, the developer and operator of MagicRAT, see Figures  2 .  The model successfully retrieves the most relevant documents from the dataset, generates an accurate answer, and provides the context for which the information was derived, see Figure  3 .",
            "In this section, we compare the results of our RAG-based QA model with the answers from GPT-3.5 and GPT-4o.  GPT-3.5 Turbo is the language model available in the free version of the ChatGPT interface. It was trained with data up to September 2021. GPT-4o is the latest advanced multimodal flagship model available through the ChatGPT interface with limited access.  GPT-4o was trained with data up to October 2023 9 9 9 https://platform.openai.com/docs/models/gpt-4o , and it has limited access to the internet for retrieving the latest data.  Our RAG-based QA application supports questions using metadata and external data, unlike GPT-3.5, which lacks this functionality. Therefore, we perform this comparison only for questions asked from the entire knowledge base.  To ensure a fair comparison, we use the same prompt:   You are a Cybersecurity expert focusing on the latest trends and investigative techniques in cyber-attacks. Provide a concise answer for the below question.  for all models.  Results for the selected set of questions are summarized in Table  3   and Table  5  (in Appendix  0.B ).  Our primary findings from the model comparison can be summarized as follows:"
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "Pt0.A2.T4.1.1",
        "footnotes": [],
        "references": [
            "In Sections  2  and  3  we provide the background and related work. We introduce our methodology in Section  4 . The implementation of our QA model is shown in Section  5 , while its evaluation is provided in Section  6 . We conclude in Section  7 .",
            "Details about our evaluation for the RAG-based QA model using three sample questions are provided in Table  4  (Appendix  0.B ).  We noticed high faithfulness scores, particularly a perfect score for the MagicRAT developer question, indicating accurate answers based on the provided contexts. Consistently high answer relevancy scores suggest the model generates pertinent answers. The model shows high context precision, especially for detecting and blocking threats, though context recall varies, indicating occasional gaps in retrieving comprehensive information. Context entity recall scores vary, with a notable 0.75 for the MagicRAT and TigerRAT questions, showing good entity extraction in some cases. Answer similarity and correctness metrics highlight the models ability to generate semantically similar and correct answers, though there is variability, suggesting room for improvement. Overall, the RAG model demonstrates strong performance in generating accurate and relevant answers, effectively retrieving pertinent contexts, and showing robustness in handling diverse cybersecurity queries.",
            "In Table  4  we provide the evaluation for our RAG-based QA model responses on Entire Dataset, while in Table  5  we provide the second part of the comparison of ground truth with our RAG-based QA model GPT-4o, and GPT-3.5 answers."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "Pt0.A2.T5.1.1",
        "footnotes": [],
        "references": [
            "In Sections  2  and  3  we provide the background and related work. We introduce our methodology in Section  4 . The implementation of our QA model is shown in Section  5 , while its evaluation is provided in Section  6 . We conclude in Section  7 .",
            "In this section, we compare the results of our RAG-based QA model with the answers from GPT-3.5 and GPT-4o.  GPT-3.5 Turbo is the language model available in the free version of the ChatGPT interface. It was trained with data up to September 2021. GPT-4o is the latest advanced multimodal flagship model available through the ChatGPT interface with limited access.  GPT-4o was trained with data up to October 2023 9 9 9 https://platform.openai.com/docs/models/gpt-4o , and it has limited access to the internet for retrieving the latest data.  Our RAG-based QA application supports questions using metadata and external data, unlike GPT-3.5, which lacks this functionality. Therefore, we perform this comparison only for questions asked from the entire knowledge base.  To ensure a fair comparison, we use the same prompt:   You are a Cybersecurity expert focusing on the latest trends and investigative techniques in cyber-attacks. Provide a concise answer for the below question.  for all models.  Results for the selected set of questions are summarized in Table  3   and Table  5  (in Appendix  0.B ).  Our primary findings from the model comparison can be summarized as follows:",
            "In Table  4  we provide the evaluation for our RAG-based QA model responses on Entire Dataset, while in Table  5  we provide the second part of the comparison of ground truth with our RAG-based QA model GPT-4o, and GPT-3.5 answers."
        ]
    },
    "global_footnotes": [
        "We ensured that the authors of the documents were reliable cybersecurity experts.",
        "Our methodology allows the usage of opinion-based questions. We decided not to include this type of question given that usually, they are not interesting for the cybersecurity experts, as well as the consistent opinions gathered inside AttackER.",
        "Each metric is calculated based on the evaluation framework proposed in",
        "."
    ]
}