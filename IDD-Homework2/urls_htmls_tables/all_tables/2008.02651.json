{
    "PAPER'S NUMBER OF TABLES": 2,
    "S6.T1": {
        "caption": "Table 1: Performance of the two tasks in the multi-task learning setup on an evaluation set.",
        "table": "<table id=\"S6.T1.3\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S6.T1.3.1.1\" class=\"ltx_tr\">\n<th id=\"S6.T1.3.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">\n<span id=\"S6.T1.3.1.1.1.1\" class=\"ltx_text\"></span><span id=\"S6.T1.3.1.1.1.2\" class=\"ltx_text\" style=\"font-size:90%;\">\n<span id=\"S6.T1.3.1.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S6.T1.3.1.1.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S6.T1.3.1.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"S6.T1.3.1.1.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Model</span></span></span>\n</span></span><span id=\"S6.T1.3.1.1.1.3\" class=\"ltx_text\"></span><span id=\"S6.T1.3.1.1.1.4\" class=\"ltx_text\" style=\"font-size:90%;\"></span>\n</th>\n<th id=\"S6.T1.3.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">\n<span id=\"S6.T1.3.1.1.2.1\" class=\"ltx_text\"></span><span id=\"S6.T1.3.1.1.2.2\" class=\"ltx_text\" style=\"font-size:90%;\">\n<span id=\"S6.T1.3.1.1.2.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S6.T1.3.1.1.2.2.1.1\" class=\"ltx_tr\">\n<span id=\"S6.T1.3.1.1.2.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"S6.T1.3.1.1.2.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Speaker accuracy (%)</span></span></span>\n</span></span><span id=\"S6.T1.3.1.1.2.3\" class=\"ltx_text\"></span><span id=\"S6.T1.3.1.1.2.4\" class=\"ltx_text\" style=\"font-size:90%;\"></span>\n</th>\n<th id=\"S6.T1.3.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">\n<span id=\"S6.T1.3.1.1.3.1\" class=\"ltx_text\"></span><span id=\"S6.T1.3.1.1.3.2\" class=\"ltx_text\" style=\"font-size:90%;\">\n<span id=\"S6.T1.3.1.1.3.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S6.T1.3.1.1.3.2.1.1\" class=\"ltx_tr\">\n<span id=\"S6.T1.3.1.1.3.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"S6.T1.3.1.1.3.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Side information</span></span></span>\n<span id=\"S6.T1.3.1.1.3.2.1.2\" class=\"ltx_tr\">\n<span id=\"S6.T1.3.1.1.3.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"S6.T1.3.1.1.3.2.1.2.1.1\" class=\"ltx_text ltx_font_bold\">accuracy (%)</span></span></span>\n</span></span><span id=\"S6.T1.3.1.1.3.3\" class=\"ltx_text\"></span><span id=\"S6.T1.3.1.1.3.4\" class=\"ltx_text\" style=\"font-size:90%;\"></span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T1.3.2.1\" class=\"ltx_tr\">\n<td id=\"S6.T1.3.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S6.T1.3.2.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Baseline</span></td>\n<td id=\"S6.T1.3.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S6.T1.3.2.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">82.56%</span></td>\n<td id=\"S6.T1.3.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S6.T1.3.2.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n</tr>\n<tr id=\"S6.T1.3.3.2\" class=\"ltx_tr\">\n<td id=\"S6.T1.3.3.2.1\" class=\"ltx_td ltx_align_left\"><span id=\"S6.T1.3.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">VC offline</span></td>\n<td id=\"S6.T1.3.3.2.2\" class=\"ltx_td ltx_align_left\"><span id=\"S6.T1.3.3.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">83.37%</span></td>\n<td id=\"S6.T1.3.3.2.3\" class=\"ltx_td ltx_align_left\"><span id=\"S6.T1.3.3.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">98.22%</span></td>\n</tr>\n<tr id=\"S6.T1.3.4.3\" class=\"ltx_tr\">\n<td id=\"S6.T1.3.4.3.1\" class=\"ltx_td ltx_align_left ltx_border_bb\"><span id=\"S6.T1.3.4.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">VC FL</span></td>\n<td id=\"S6.T1.3.4.3.2\" class=\"ltx_td ltx_align_left ltx_border_bb\"><span id=\"S6.T1.3.4.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">83.59%</span></td>\n<td id=\"S6.T1.3.4.3.3\" class=\"ltx_td ltx_align_left ltx_border_bb\"><span id=\"S6.T1.3.4.3.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">98.15%</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In this section, results of three models are presented, all with the same final network architecture.\nThe difference is only in how the speaker embedding is trained.\nThe first model, ",
                "Baseline",
                ", is the baseline production system described in Section ",
                "3",
                ".\nThe second model, ",
                "VC offline",
                ", follows the setup described in Section ",
                "5",
                " where the knowledge to distil is from a vocal classification model trained on the limited offline data (blue line in Figure ",
                "1",
                ").\nThe third model, ",
                "VC FL",
                ", also follows the setup from Section ",
                "5",
                ", but with a vocal classification model trained with federated learning. The mechanism in ",
                "[",
                "24",
                "]",
                " was used for local privacy guarantees and the Gaussian mechanism with moments accountant was used for central privacy guarantees.",
                "Multi-task learning of the speaker embedding was conducted on ",
                "1.7",
                "1.7",
                "1.7",
                " million utterances of the trigger phrase from ",
                "18700",
                "18700",
                "18700",
                " speakers, all preprocessed by the voice trigger system to extract 520-dimensional supervectors.\nThe softmax layer classifying side information has six outputs and the softmax layer classifying speakers has one output per speaker.\nThe temperature ",
                "T",
                "ùëá",
                "T",
                " was set to ",
                "10",
                "10",
                "10",
                " for all experiments, and the weight ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " of the side information classification loss was roughly tuned to balance the losses of the two tasks.\nEvaluation accuracy was measured on another set of ",
                "93600",
                "93600",
                "93600",
                " utterances from the same population of speakers as the training dataset.",
                "The accuracies of speaker identification and side information classification on the evaluation dataset are shown in Table ",
                "1",
                ".\nThe accuracy on the speaker identification task in the multi-task setting increases relative to the baseline.\nIt is possible that the side information has both a regularizing effect on speaker identification as well as helping propagate signals through the network.\nThe accuracy on the classification of the side information is expected to be close to ",
                "100",
                "%",
                "percent",
                "100",
                "100\\%",
                " because the labels are generated from the vocal classification model, and this larger DNN should be able to capture the encoded knowledge of the vocal classification model.",
                "As mentioned in Section ",
                "3",
                ", a speaker profile is defined by a set of supervectors from enrolment utterances.\nWhen evaluating the speaker verification system with the newly trained speaker embedding network, each of ",
                "234",
                "234",
                "234",
                " speaker profiles available were compared to supervectors of test utterances using Equation ",
                "1",
                ".\nA subset of the test utterances were unique utterances from the ",
                "234",
                "234",
                "234",
                " speakers which had profiles, and the rest originated from imposter speakers that did not match any profile.\nA total of ",
                "65545",
                "65545",
                "65545",
                " pairs of speaker profiles and test utterances were compared and a test utterance was accepted or rejected by applying a threshold ",
                "Œ∏",
                "ùúÉ",
                "\\theta",
                " on the speaker vector score ",
                "S",
                "‚Äã",
                "V",
                "s",
                "‚Äã",
                "c",
                "‚Äã",
                "o",
                "‚Äã",
                "r",
                "‚Äã",
                "e",
                "ùëÜ",
                "subscript",
                "ùëâ",
                "ùë†",
                "ùëê",
                "ùëú",
                "ùëü",
                "ùëí",
                "SV_{score}",
                ".\nPerformance at the equal error rate (EER) is shown in Table ",
                "2",
                " for the three experiments.\nThe multi-task setup with knowledge distillation of a vocal classification model trained on limited central data yields a ",
                "1.5",
                "%",
                "percent",
                "1.5",
                "1.5\\%",
                " absolute improvement over the baseline. The same setup with a vocal classification model trained on-device with privacy-preserving federated learning yields a ",
                "6",
                "%",
                "percent",
                "6",
                "6\\%",
                " relative improvement in EER. These results prove both our second and third hypotheses, namely that privacy-preserving federated learning can be used to improve the vocal classification-based system (due to the gain over the ‚ÄúVC offline‚Äù experiment), and that the vocal classification model can be used through multi-task learning to improve the speaker verification system (due to the gain over the ‚ÄúBaseline‚Äù system)."
            ]
        ]
    },
    "S6.T2": {
        "caption": "Table 2: Performance of speaker verification on a test set.",
        "table": "<table id=\"S6.T2.3\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S6.T2.3.1.1\" class=\"ltx_tr\">\n<th id=\"S6.T2.3.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">\n<span id=\"S6.T2.3.1.1.1.1\" class=\"ltx_text\"></span><span id=\"S6.T2.3.1.1.1.2\" class=\"ltx_text\" style=\"font-size:90%;\">\n<span id=\"S6.T2.3.1.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S6.T2.3.1.1.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S6.T2.3.1.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"S6.T2.3.1.1.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Model</span></span></span>\n</span></span><span id=\"S6.T2.3.1.1.1.3\" class=\"ltx_text\"></span><span id=\"S6.T2.3.1.1.1.4\" class=\"ltx_text\" style=\"font-size:90%;\"></span>\n</th>\n<th id=\"S6.T2.3.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">\n<span id=\"S6.T2.3.1.1.2.1\" class=\"ltx_text\"></span><span id=\"S6.T2.3.1.1.2.2\" class=\"ltx_text\" style=\"font-size:90%;\">\n<span id=\"S6.T2.3.1.1.2.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S6.T2.3.1.1.2.2.1.1\" class=\"ltx_tr\">\n<span id=\"S6.T2.3.1.1.2.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"S6.T2.3.1.1.2.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">EER (%)</span></span></span>\n</span></span><span id=\"S6.T2.3.1.1.2.3\" class=\"ltx_text\"></span><span id=\"S6.T2.3.1.1.2.4\" class=\"ltx_text\" style=\"font-size:90%;\"></span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T2.3.2.1\" class=\"ltx_tr\">\n<td id=\"S6.T2.3.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S6.T2.3.2.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Baseline</span></td>\n<td id=\"S6.T2.3.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S6.T2.3.2.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">10.10</span></td>\n</tr>\n<tr id=\"S6.T2.3.3.2\" class=\"ltx_tr\">\n<td id=\"S6.T2.3.3.2.1\" class=\"ltx_td ltx_align_left\"><span id=\"S6.T2.3.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">VC offline</span></td>\n<td id=\"S6.T2.3.3.2.2\" class=\"ltx_td ltx_align_left\"><span id=\"S6.T2.3.3.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">9.95</span></td>\n</tr>\n<tr id=\"S6.T2.3.4.3\" class=\"ltx_tr\">\n<td id=\"S6.T2.3.4.3.1\" class=\"ltx_td ltx_align_left ltx_border_bb\"><span id=\"S6.T2.3.4.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">VC FL</span></td>\n<td id=\"S6.T2.3.4.3.2\" class=\"ltx_td ltx_align_left ltx_border_bb\"><span id=\"S6.T2.3.4.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">9.50</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In this section, results of three models are presented, all with the same final network architecture.\nThe difference is only in how the speaker embedding is trained.\nThe first model, ",
                "Baseline",
                ", is the baseline production system described in Section ",
                "3",
                ".\nThe second model, ",
                "VC offline",
                ", follows the setup described in Section ",
                "5",
                " where the knowledge to distil is from a vocal classification model trained on the limited offline data (blue line in Figure ",
                "1",
                ").\nThe third model, ",
                "VC FL",
                ", also follows the setup from Section ",
                "5",
                ", but with a vocal classification model trained with federated learning. The mechanism in ",
                "[",
                "24",
                "]",
                " was used for local privacy guarantees and the Gaussian mechanism with moments accountant was used for central privacy guarantees.",
                "Multi-task learning of the speaker embedding was conducted on ",
                "1.7",
                "1.7",
                "1.7",
                " million utterances of the trigger phrase from ",
                "18700",
                "18700",
                "18700",
                " speakers, all preprocessed by the voice trigger system to extract 520-dimensional supervectors.\nThe softmax layer classifying side information has six outputs and the softmax layer classifying speakers has one output per speaker.\nThe temperature ",
                "T",
                "ùëá",
                "T",
                " was set to ",
                "10",
                "10",
                "10",
                " for all experiments, and the weight ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " of the side information classification loss was roughly tuned to balance the losses of the two tasks.\nEvaluation accuracy was measured on another set of ",
                "93600",
                "93600",
                "93600",
                " utterances from the same population of speakers as the training dataset.",
                "The accuracies of speaker identification and side information classification on the evaluation dataset are shown in Table ",
                "1",
                ".\nThe accuracy on the speaker identification task in the multi-task setting increases relative to the baseline.\nIt is possible that the side information has both a regularizing effect on speaker identification as well as helping propagate signals through the network.\nThe accuracy on the classification of the side information is expected to be close to ",
                "100",
                "%",
                "percent",
                "100",
                "100\\%",
                " because the labels are generated from the vocal classification model, and this larger DNN should be able to capture the encoded knowledge of the vocal classification model.",
                "As mentioned in Section ",
                "3",
                ", a speaker profile is defined by a set of supervectors from enrolment utterances.\nWhen evaluating the speaker verification system with the newly trained speaker embedding network, each of ",
                "234",
                "234",
                "234",
                " speaker profiles available were compared to supervectors of test utterances using Equation ",
                "1",
                ".\nA subset of the test utterances were unique utterances from the ",
                "234",
                "234",
                "234",
                " speakers which had profiles, and the rest originated from imposter speakers that did not match any profile.\nA total of ",
                "65545",
                "65545",
                "65545",
                " pairs of speaker profiles and test utterances were compared and a test utterance was accepted or rejected by applying a threshold ",
                "Œ∏",
                "ùúÉ",
                "\\theta",
                " on the speaker vector score ",
                "S",
                "‚Äã",
                "V",
                "s",
                "‚Äã",
                "c",
                "‚Äã",
                "o",
                "‚Äã",
                "r",
                "‚Äã",
                "e",
                "ùëÜ",
                "subscript",
                "ùëâ",
                "ùë†",
                "ùëê",
                "ùëú",
                "ùëü",
                "ùëí",
                "SV_{score}",
                ".\nPerformance at the equal error rate (EER) is shown in Table ",
                "2",
                " for the three experiments.\nThe multi-task setup with knowledge distillation of a vocal classification model trained on limited central data yields a ",
                "1.5",
                "%",
                "percent",
                "1.5",
                "1.5\\%",
                " absolute improvement over the baseline. The same setup with a vocal classification model trained on-device with privacy-preserving federated learning yields a ",
                "6",
                "%",
                "percent",
                "6",
                "6\\%",
                " relative improvement in EER. These results prove both our second and third hypotheses, namely that privacy-preserving federated learning can be used to improve the vocal classification-based system (due to the gain over the ‚ÄúVC offline‚Äù experiment), and that the vocal classification model can be used through multi-task learning to improve the speaker verification system (due to the gain over the ‚ÄúBaseline‚Äù system)."
            ]
        ]
    }
}