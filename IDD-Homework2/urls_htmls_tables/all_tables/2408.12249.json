{
    "id_table_1": {
        "caption": "Table 1:  Performance of each model and technique combination across Classification and NER datasets. For classification, we report Micro-F1 and for NER we report both Span-Identification Micro-F1 performance as well as full Micro-F1 performance, including recognizing correct types.",
        "table": "Sx4.T1.1",
        "footnotes": [],
        "references": [
            "Reasoning and knowledge enhancing techniques seem to not improve performance . Figure  1  and Figure   2  compare the results of the best performing techniques for each model for classification and NER, respectively. As seen in Table  1 , perhaps counter-intuitively, Standard Prompting consistently achieves the highest average F1 scores across all models for classification task, with BioMistral-7B obtaining 36.48%, Llama-2-70B-Chat-AWQ achieving 40.34%, and Llama-2-7b-chat-hf scoring 34.92%. This result indicates that for structured prediction tasks, more complex reasoning techniques such as Chain of Thought (CoT) Prompting or Retrieval-Augmented Generation (RAG), do not outperform simpler approaches like Standard Prompting. For NER tasks, the results present a more nuanced picture compared to the classification tasks. While Standard Prompting remains effective, there is a noticeable shift in performance across different models and datasets.  Notably, the scores are significantly lower than typical F1 scores in biomedical NER benchmarks. For instance, the NCBI disease corpus  (Dogan, Leaman, and Lu  2014 ; Krallinger et al.  2015 )  and CHEMDNER dataset usually yield higher performances with specialized models or extensive pre-training. State-of-the-art models on these benchmarks can achieve Span F1 scores up to 0.90 for the NCBI disease corpus  (Kocaman and Talby  2021 ; Zhou et al.  2023 ) . However, similar to our findings, in true zero-shot setting, NER scores have been reported to be markedly low, even for the general domain  (Shen et al.  2021 )  and when supplying label descriptions  (Picco et al.  2024 ) .",
            "Multilingual Performance is not Scale Dependent.  As shown in Figure  1 , smaller models can match or even outperform larger models on Chinese and Japanese datasets but not on English datasets. This may be due to the heavy reliance on large English corpora during training, with limited exposure to medical contexts in other languages. This forces models to generalize compressed language representations to specialized domains, where overfitting on sparse languages may hinder larger models performance."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Datasets used for classification tasks.",
        "table": "A1.T2.1",
        "footnotes": [],
        "references": [
            "Reasoning and knowledge enhancing techniques seem to not improve performance . Figure  1  and Figure   2  compare the results of the best performing techniques for each model for classification and NER, respectively. As seen in Table  1 , perhaps counter-intuitively, Standard Prompting consistently achieves the highest average F1 scores across all models for classification task, with BioMistral-7B obtaining 36.48%, Llama-2-70B-Chat-AWQ achieving 40.34%, and Llama-2-7b-chat-hf scoring 34.92%. This result indicates that for structured prediction tasks, more complex reasoning techniques such as Chain of Thought (CoT) Prompting or Retrieval-Augmented Generation (RAG), do not outperform simpler approaches like Standard Prompting. For NER tasks, the results present a more nuanced picture compared to the classification tasks. While Standard Prompting remains effective, there is a noticeable shift in performance across different models and datasets.  Notably, the scores are significantly lower than typical F1 scores in biomedical NER benchmarks. For instance, the NCBI disease corpus  (Dogan, Leaman, and Lu  2014 ; Krallinger et al.  2015 )  and CHEMDNER dataset usually yield higher performances with specialized models or extensive pre-training. State-of-the-art models on these benchmarks can achieve Span F1 scores up to 0.90 for the NCBI disease corpus  (Kocaman and Talby  2021 ; Zhou et al.  2023 ) . However, similar to our findings, in true zero-shot setting, NER scores have been reported to be markedly low, even for the general domain  (Shen et al.  2021 )  and when supplying label descriptions  (Picco et al.  2024 ) .",
            "Table  2  and  3  list the huggingface dataset cards and citations for each classification and ner dataset used in the paper respectively.    For datasets considered private, we assume that models have not been trained on these datasets due to their restricted access, which requires Data Use Agreements (DUAs) and other permissions. Consequently, the likelihood of these datasets being included in common web crawls is low.    We have signed all the relevant Data Use Agreements (DUAs) and strictly adhere to their provisions. We do not redistribute the data and advise those wishing to reproduce experiments involving private datasets to consult the corresponding Hugging Face dataset cards for guidance on obtaining the necessary data."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Datasets used for NER tasks.",
        "table": "A1.T3.1",
        "footnotes": [],
        "references": [
            "LLMs struggle on tasks high complexity tasks  As seen in Figure  5 , LLMs seem to struggle to outperform random baselines for both single and multi class classification tasks. However, Figure  3  paints a more nuanced picture: guessing baseine remains unbeaten only on two of 14 datasets, which drags down the average performance significantly.",
            "Figures  3  and  4  show that Llama2 70B demonstrates good performance in low-complexity tasks such as disease and symptom classification ( CZIBase ,  NTCIR13-En ) and medium-complexity tasks like Gene Expression classification ( Geo ). However, the model is challenged by higher-complexity problems, such as the  BioNLP13-CG  and  GENIA-EE  datasets.Specifically, in datasets that demand nuanced understanding and interpretation, such as the extraction of participants and outcomes from abstracts and gene ontology population ( PICO ,  BioNLP13-GRO ) the performance is low. When incorporating RAG (Retrieval-Augmented Generation) techniques, there are fluctuations in performance across datasets. While results improve on some datasets, RAG does not universally benefit the models ability to accurately extract and classify biomedical information.",
            "Table  2  and  3  list the huggingface dataset cards and citations for each classification and ner dataset used in the paper respectively.    For datasets considered private, we assume that models have not been trained on these datasets due to their restricted access, which requires Data Use Agreements (DUAs) and other permissions. Consequently, the likelihood of these datasets being included in common web crawls is low.    We have signed all the relevant Data Use Agreements (DUAs) and strictly adhere to their provisions. We do not redistribute the data and advise those wishing to reproduce experiments involving private datasets to consult the corresponding Hugging Face dataset cards for guidance on obtaining the necessary data."
        ]
    },
    "global_footnotes": [
        "by true zero-shot we refer to the scenario where no examples are available to solve the task and no information beyond the labels and their semantically meaningful names is made available to the model",
        ".",
        "for the GAD dataset, we only select 1 fold out of the 10 available, as the folds feature the same task for different data, unlike other datasets. We also skipped the Chinese subset of meddialog as we had difficulties loading the dataset",
        "The other reason being their intransparancy with regard to training data, which violates our true zero-shot setting."
    ]
}