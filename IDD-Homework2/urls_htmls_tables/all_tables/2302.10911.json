{
    "PAPER'S NUMBER OF TABLES": 10,
    "S4.T1": {
        "caption": "Table 1: Impact of fixed Œ≥ùõæ\\gamma across different architectures in both IID (Œ±=100ùõº100\\alpha=100) and NonIID (Œ±=1ùõº1\\alpha=1) settings (E=2ùê∏2E=2).",
        "table": "<table id=\"S4.T1.9.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.9.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.9.1.1.2\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"></th>\n<th id=\"S4.T1.9.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><math id=\"S4.T1.9.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\gamma\" display=\"inline\"><semantics id=\"S4.T1.9.1.1.1.m1.1a\"><mi id=\"S4.T1.9.1.1.1.m1.1.1\" xref=\"S4.T1.9.1.1.1.m1.1.1.cmml\">Œ≥</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.9.1.1.1.m1.1b\"><ci id=\"S4.T1.9.1.1.1.m1.1.1.cmml\" xref=\"S4.T1.9.1.1.1.m1.1.1\">ùõæ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.9.1.1.1.m1.1c\">\\gamma</annotation></semantics></math></th>\n<th id=\"S4.T1.9.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">1.0</th>\n<th id=\"S4.T1.9.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">0.99</th>\n<th id=\"S4.T1.9.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">0.97</th>\n<th id=\"S4.T1.9.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">0.95</th>\n<th id=\"S4.T1.9.1.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">0.93</th>\n<th id=\"S4.T1.9.1.1.8\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\">0.9</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.9.1.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.9.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S4.T1.9.1.2.1.1.1\" class=\"ltx_text\">IID</span></th>\n<th id=\"S4.T1.9.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">SimpleCNN</th>\n<td id=\"S4.T1.9.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">65.53</td>\n<td id=\"S4.T1.9.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">67.60</td>\n<td id=\"S4.T1.9.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">69.20</td>\n<td id=\"S4.T1.9.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">69.52</td>\n<td id=\"S4.T1.9.1.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.9.1.2.1.7.1\" class=\"ltx_text ltx_font_bold\">70.16</span></td>\n<td id=\"S4.T1.9.1.2.1.8\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">69.83</td>\n</tr>\n<tr id=\"S4.T1.9.1.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T1.9.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">AlexNet</th>\n<td id=\"S4.T1.9.1.3.2.2\" class=\"ltx_td ltx_align_center\">74.16</td>\n<td id=\"S4.T1.9.1.3.2.3\" class=\"ltx_td ltx_align_center\">74.80</td>\n<td id=\"S4.T1.9.1.3.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.9.1.3.2.4.1\" class=\"ltx_text ltx_font_bold\">75.54</span></td>\n<td id=\"S4.T1.9.1.3.2.5\" class=\"ltx_td ltx_align_center\">75.24</td>\n<td id=\"S4.T1.9.1.3.2.6\" class=\"ltx_td ltx_align_center\">75.25</td>\n<td id=\"S4.T1.9.1.3.2.7\" class=\"ltx_td ltx_nopad_r ltx_align_center\">75.03</td>\n</tr>\n<tr id=\"S4.T1.9.1.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T1.9.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">ResNet8</th>\n<td id=\"S4.T1.9.1.4.3.2\" class=\"ltx_td ltx_align_center\">75.51</td>\n<td id=\"S4.T1.9.1.4.3.3\" class=\"ltx_td ltx_align_center\">76.64</td>\n<td id=\"S4.T1.9.1.4.3.4\" class=\"ltx_td ltx_align_center\">76.80</td>\n<td id=\"S4.T1.9.1.4.3.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.9.1.4.3.5.1\" class=\"ltx_text ltx_font_bold\">77.87</span></td>\n<td id=\"S4.T1.9.1.4.3.6\" class=\"ltx_td ltx_align_center\">76.80</td>\n<td id=\"S4.T1.9.1.4.3.7\" class=\"ltx_td ltx_nopad_r ltx_align_center\">76.74</td>\n</tr>\n<tr id=\"S4.T1.9.1.5.4\" class=\"ltx_tr\">\n<th id=\"S4.T1.9.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S4.T1.9.1.5.4.1.1\" class=\"ltx_text\">NonIID</span></th>\n<th id=\"S4.T1.9.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">SimpleCNN</th>\n<td id=\"S4.T1.9.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\">65.58</td>\n<td id=\"S4.T1.9.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\">67.04</td>\n<td id=\"S4.T1.9.1.5.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\">68.36</td>\n<td id=\"S4.T1.9.1.5.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\">68.66</td>\n<td id=\"S4.T1.9.1.5.4.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T1.9.1.5.4.7.1\" class=\"ltx_text ltx_font_bold\">69.28</span></td>\n<td id=\"S4.T1.9.1.5.4.8\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">68.93</td>\n</tr>\n<tr id=\"S4.T1.9.1.6.5\" class=\"ltx_tr\">\n<th id=\"S4.T1.9.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">AlexNet</th>\n<td id=\"S4.T1.9.1.6.5.2\" class=\"ltx_td ltx_align_center\">73.56</td>\n<td id=\"S4.T1.9.1.6.5.3\" class=\"ltx_td ltx_align_center\">73.83</td>\n<td id=\"S4.T1.9.1.6.5.4\" class=\"ltx_td ltx_align_center\">74.37</td>\n<td id=\"S4.T1.9.1.6.5.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T1.9.1.6.5.5.1\" class=\"ltx_text ltx_font_bold\">74.45</span></td>\n<td id=\"S4.T1.9.1.6.5.6\" class=\"ltx_td ltx_align_center\">74.40</td>\n<td id=\"S4.T1.9.1.6.5.7\" class=\"ltx_td ltx_nopad_r ltx_align_center\">74.24</td>\n</tr>\n<tr id=\"S4.T1.9.1.7.6\" class=\"ltx_tr\">\n<th id=\"S4.T1.9.1.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">ResNet8</th>\n<td id=\"S4.T1.9.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">75.02</td>\n<td id=\"S4.T1.9.1.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">76.06</td>\n<td id=\"S4.T1.9.1.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">75.73</td>\n<td id=\"S4.T1.9.1.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T1.9.1.7.6.5.1\" class=\"ltx_text ltx_font_bold\">77.00</span></td>\n<td id=\"S4.T1.9.1.7.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">75.04</td>\n<td id=\"S4.T1.9.1.7.6.7\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\">75.31</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Setting ",
                "Œ≥",
                "<",
                "1",
                "ùõæ",
                "1",
                "\\gamma<1",
                " results in the global weight shrinking regularization. ",
                "Table¬†1",
                " and ",
                "Figure¬†1",
                " report the results on CIFAR-10 with different ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                ".\nIt can be observed that the ",
                "global weight shrinking may improve generalization, depending on the choice of ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                ".",
                " The smaller ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                ", the stronger regularization effect. Given a setting, there exists an optimal ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " that balances the regularization and optimization, and deviation from this value, whether smaller or larger, may result in inferior performance.\nMore results about the fixed ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " can be found in ",
                "Table¬†9",
                " in Appendix."
            ]
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Adaptive GWS with different local weight decay factors (E=2ùê∏2E=2). IID (Œ±=100ùõº100\\alpha=100), NonIID (Œ±=1ùõº1\\alpha=1).",
        "table": "<table id=\"S4.T2.8.2\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.8.2.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.8.2.3.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"></th>\n<th id=\"S4.T2.8.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S4.T2.8.2.3.1.2.1\" class=\"ltx_text ltx_font_bold\">Local weight decay</span></th>\n<th id=\"S4.T2.8.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">0</th>\n<th id=\"S4.T2.8.2.3.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">5e-5</th>\n<th id=\"S4.T2.8.2.3.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">1e-4</th>\n<th id=\"S4.T2.8.2.3.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">5e-4</th>\n<th id=\"S4.T2.8.2.3.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">1e-3</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.8.2.4.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.8.2.4.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S4.T2.8.2.4.1.1.1\" class=\"ltx_text\">IID</span></th>\n<th id=\"S4.T2.8.2.4.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S4.T2.8.2.4.1.2.1\" class=\"ltx_text ltx_font_smallcaps\">FedAvg</span></th>\n<td id=\"S4.T2.8.2.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">66.43</td>\n<td id=\"S4.T2.8.2.4.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">66.20</td>\n<td id=\"S4.T2.8.2.4.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">66.45</td>\n<td id=\"S4.T2.8.2.4.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">67.51</td>\n<td id=\"S4.T2.8.2.4.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">68.66</td>\n</tr>\n<tr id=\"S4.T2.8.2.5.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.8.2.5.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Adaptive GWS</th>\n<td id=\"S4.T2.8.2.5.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.8.2.5.2.2.1\" class=\"ltx_text ltx_font_bold\">71.47</span></td>\n<td id=\"S4.T2.8.2.5.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.8.2.5.2.3.1\" class=\"ltx_text ltx_font_bold\">71.36</span></td>\n<td id=\"S4.T2.8.2.5.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.8.2.5.2.4.1\" class=\"ltx_text ltx_font_bold\">71.35</span></td>\n<td id=\"S4.T2.8.2.5.2.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.8.2.5.2.5.1\" class=\"ltx_text ltx_font_bold\">71.44</span></td>\n<td id=\"S4.T2.8.2.5.2.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.8.2.5.2.6.1\" class=\"ltx_text ltx_font_bold\">71.54</span></td>\n</tr>\n<tr id=\"S4.T2.7.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.7.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">\n<math id=\"S4.T2.7.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\gamma\" display=\"inline\"><semantics id=\"S4.T2.7.1.1.1.m1.1a\"><mi id=\"S4.T2.7.1.1.1.m1.1.1\" xref=\"S4.T2.7.1.1.1.m1.1.1.cmml\">Œ≥</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.7.1.1.1.m1.1b\"><ci id=\"S4.T2.7.1.1.1.m1.1.1.cmml\" xref=\"S4.T2.7.1.1.1.m1.1.1\">ùõæ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.7.1.1.1.m1.1c\">\\gamma</annotation></semantics></math> of Adaptive GWS</th>\n<td id=\"S4.T2.7.1.1.2\" class=\"ltx_td ltx_align_center\">0.9472</td>\n<td id=\"S4.T2.7.1.1.3\" class=\"ltx_td ltx_align_center\">0.9477</td>\n<td id=\"S4.T2.7.1.1.4\" class=\"ltx_td ltx_align_center\">0.948</td>\n<td id=\"S4.T2.7.1.1.5\" class=\"ltx_td ltx_align_center\">0.9493</td>\n<td id=\"S4.T2.7.1.1.6\" class=\"ltx_td ltx_align_center\">0.953</td>\n</tr>\n<tr id=\"S4.T2.8.2.6.3\" class=\"ltx_tr\">\n<th id=\"S4.T2.8.2.6.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S4.T2.8.2.6.3.1.1\" class=\"ltx_text\">NonIID</span></th>\n<th id=\"S4.T2.8.2.6.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S4.T2.8.2.6.3.2.1\" class=\"ltx_text ltx_font_smallcaps\">FedAvg</span></th>\n<td id=\"S4.T2.8.2.6.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">65.35</td>\n<td id=\"S4.T2.8.2.6.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">65.19</td>\n<td id=\"S4.T2.8.2.6.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\">65.77</td>\n<td id=\"S4.T2.8.2.6.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\">66.37</td>\n<td id=\"S4.T2.8.2.6.3.7\" class=\"ltx_td ltx_align_center ltx_border_t\">67.4</td>\n</tr>\n<tr id=\"S4.T2.8.2.7.4\" class=\"ltx_tr\">\n<th id=\"S4.T2.8.2.7.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Adaptive GWS</th>\n<td id=\"S4.T2.8.2.7.4.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.8.2.7.4.2.1\" class=\"ltx_text ltx_font_bold\">70.31</span></td>\n<td id=\"S4.T2.8.2.7.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.8.2.7.4.3.1\" class=\"ltx_text ltx_font_bold\">69.93</span></td>\n<td id=\"S4.T2.8.2.7.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.8.2.7.4.4.1\" class=\"ltx_text ltx_font_bold\">70.44</span></td>\n<td id=\"S4.T2.8.2.7.4.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.8.2.7.4.5.1\" class=\"ltx_text ltx_font_bold\">70.47</span></td>\n<td id=\"S4.T2.8.2.7.4.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.8.2.7.4.6.1\" class=\"ltx_text ltx_font_bold\">69.99</span></td>\n</tr>\n<tr id=\"S4.T2.8.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.8.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">\n<math id=\"S4.T2.8.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\gamma\" display=\"inline\"><semantics id=\"S4.T2.8.2.2.1.m1.1a\"><mi id=\"S4.T2.8.2.2.1.m1.1.1\" xref=\"S4.T2.8.2.2.1.m1.1.1.cmml\">Œ≥</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.8.2.2.1.m1.1b\"><ci id=\"S4.T2.8.2.2.1.m1.1.1.cmml\" xref=\"S4.T2.8.2.2.1.m1.1.1\">ùõæ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.8.2.2.1.m1.1c\">\\gamma</annotation></semantics></math> of Adaptive GWS</th>\n<td id=\"S4.T2.8.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.9492</td>\n<td id=\"S4.T2.8.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.9499</td>\n<td id=\"S4.T2.8.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.9505</td>\n<td id=\"S4.T2.8.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.9529</td>\n<td id=\"S4.T2.8.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.9561</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "We discover how to set an appropriate ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " to balance regularization and optimization. We first expand the right of ",
                "Equation¬†2",
                " as follows.",
                "We refer ",
                "(",
                "1",
                "‚àí",
                "Œ≥",
                ")",
                "‚Äã",
                "ùê∞",
                "g",
                "t",
                "1",
                "ùõæ",
                "superscript",
                "subscript",
                "ùê∞",
                "ùëî",
                "ùë°",
                "(1-\\gamma)\\mathbf{w}_{g}^{t}",
                " as the pseudo gradient of global weight shrinking (",
                "regularization term",
                ") and ",
                "Œ≥",
                "‚Äã",
                "Œ∑",
                "g",
                "‚Äã",
                "g",
                "g",
                "t",
                "ùõæ",
                "subscript",
                "ùúÇ",
                "ùëî",
                "superscript",
                "subscript",
                "g",
                "ùëî",
                "ùë°",
                "\\gamma\\eta_{g}\\textbf{g}_{g}^{t}",
                " is the global averaged gradient (",
                "optimization term",
                "). We reckon that a larger optimization term requires a larger regularization term, which means the magnitude of the global pseudo gradient ",
                "g",
                "g",
                "t",
                "superscript",
                "subscript",
                "g",
                "ùëî",
                "ùë°",
                "\\textbf{g}_{g}^{t}",
                " determines the optimal shrinking factor ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " in the way that larger ",
                "g",
                "g",
                "t",
                "superscript",
                "subscript",
                "g",
                "ùëî",
                "ùë°",
                "\\textbf{g}_{g}^{t}",
                ", smaller ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " (stronger regularization).",
                "To verify our hypothesis, we achieve adaptive global weight shrinking (adaptive GWS) on the proxy dataset, which learns an optimal ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                ". Adaptive GWS adopts the update in ",
                "Equation¬†3",
                " and uses ",
                "{",
                "Œ≥",
                "=",
                "Œ≥",
                "‚àó",
                ",",
                "Œª",
                "i",
                "=",
                "|",
                "ùíü",
                "i",
                "|",
                "|",
                "ùíü",
                "|",
                "}",
                "formulae-sequence",
                "ùõæ",
                "superscript",
                "ùõæ",
                "subscript",
                "ùúÜ",
                "ùëñ",
                "subscript",
                "ùíü",
                "ùëñ",
                "ùíü",
                "\\{\\gamma=\\gamma^{*},\\lambda_{i}=\\frac{|\\mathcal{D}_{i}|}{|\\mathcal{D}|}\\}",
                " where",
                "Adaptive GWS largely improves the generalization.\nFrom the left of ",
                "Figure¬†2",
                ", adaptive GWS can improve the performance of ",
                "FedAvg",
                " by a large margin in both IID and NonIID settings. Furthermore, adaptive GWS is more beneficial when the number of local epochs is small.",
                "1) Understanding the balance between optimization and regularization.",
                " Further, through the learned optimal ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                ", we verify the balance between optimization and regularization from the right of ",
                "Figure¬†2",
                " and ",
                "Figure¬†3",
                ". A larger norm of the global gradient requires stronger regularization, in the cases when (i) the number of local epochs is larger; (ii) the clients‚Äô data are more IID; (iii) during training before the global model is near convergence (on the contrary, when the model is near convergence, smaller regularization is needed).",
                "As shown in the right blue Y-axis of right ",
                "Figure¬†2",
                ", the norm of global gradient ",
                "‚Äñ",
                "Œ≥",
                "‚Äã",
                "Œ∑",
                "g",
                "‚Äã",
                "g",
                "g",
                "t",
                "‚Äñ",
                "norm",
                "ùõæ",
                "subscript",
                "ùúÇ",
                "ùëî",
                "superscript",
                "subscript",
                "g",
                "ùëî",
                "ùë°",
                "\\|\\gamma\\eta_{g}\\textbf{g}_{g}^{t}\\|",
                " increases when the number of local epochs increases and data become IID. As a result, the optimal value of ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " (shown in the left green Y-axis) becomes smaller in order to produce a larger weight shrinking pseudo gradient ",
                "‚Äñ",
                "(",
                "1",
                "‚àí",
                "Œ≥",
                ")",
                "‚Äã",
                "ùê∞",
                "g",
                "t",
                "‚Äñ",
                "norm",
                "1",
                "ùõæ",
                "superscript",
                "subscript",
                "ùê∞",
                "ùëî",
                "ùë°",
                "\\|(1-\\gamma)\\mathbf{w}_{g}^{t}\\|",
                " to regularize the optimization. More results regarding how heterogeneity affects the optimal ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " can be found in ",
                "Figure¬†9",
                " in Appendix.",
                "In ",
                "Figure¬†1",
                ", GWS with smaller fixed ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " will cause performance degradation in the late training. This is due to the conflicts of decaying global pseudo gradient and non-decaying regularization pseudo gradient.\nIn ",
                "Figure¬†3",
                ", while the norm of the global gradient is decaying, adaptive GWS learns a rising optimal ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " to keep the GWS pseudo gradient decay proportionally.\nAs a result, the ratio of two gradient terms remains steady at around 19 to maintain the balance between optimization and regularization.",
                "2) The mechanisms behind adaptive GWS.",
                " We provide an in-depth general understanding of how adaptive GWS works and why it can improve generalization.",
                "General understanding.",
                "Scale invariance.",
                " Adaptive GWS learns a dynamic shrinking factor ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " in each round to shrink the global model‚Äôs parameter. The method is effective due to the scale invariance property of DNNs ",
                "(Li et¬†al., ",
                "2018",
                "; Dinh et¬†al., ",
                "2017",
                "; Kwon et¬†al., ",
                "2021",
                ")",
                ", which states that the function of a DNN remains similar or the same even when a factor rescales the model weights due to the non-linearity of activation functions or the normalization layer in DNNs. We show an intuitive understanding of scale invariance on the left figure of ",
                "Figure¬†4",
                ", where the final models are rescaled by ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                ", and the loss function of the adaptive GWS‚Äôs final model remains similar while the ",
                "FedAvg",
                "‚Äôs final model even has a smaller loss when ",
                "Œ≥",
                "<",
                "1",
                "ùõæ",
                "1",
                "\\gamma<1",
                ".",
                "Small model parameters.",
                " The shrinking effect in each round can result in smaller model parameters of final global models, which is similar to weight decay.\nThe parameter weight histogram is demonstrated in the middle figure of ",
                "Figure¬†4",
                ". The final model of adaptive GWS has more model parameters close to zero, nearly twice as many as ",
                "FedAvg",
                ".",
                "Why adaptive GWS can improve generalization.",
                "Flatter loss landscapes.",
                " One perspective of explaining the generalization of DNNs is through the flatness of the loss landscape. Previous works have shown that flatter curvature in loss landscape can indicate better generalization ",
                "(Fort & Jastrzebski, ",
                "2019",
                "; Foret et¬†al., ",
                "2020",
                "; Li et¬†al., ",
                "2018",
                ")",
                ". ",
                "(Lyu et¬†al., ",
                "2022",
                ")",
                " shows that weight decay of mini-batch SGD can result in flatter landscapes in DNNs with normalization layers.\nWe also observe the similar phenomenon that ",
                "adaptive GWS improves generalization by seeking flatter minima in FL",
                ", as shown in the right figure of ",
                "Figure¬†4",
                ". Other metrics of flatness also demonstrate similar results (",
                "Figure¬†10",
                " in Appendix).",
                "3) The relation between adaptive GWS and local weight decay.",
                "\nOur proposed adaptive GWS can provide weight regularization from the global perspective, which is analogous to weight decay in mini-batch SGD. Importantly, GWS has a unique sparse regularization frequency that only changes the model weight in each round, resulting in stronger regularization. In GWS, ",
                "1",
                "‚àí",
                "Œ≥",
                "1",
                "ùõæ",
                "1-\\gamma",
                " is near 0.1, whereas the factor of weight decay is often around ",
                "10",
                "‚àí",
                "4",
                "superscript",
                "10",
                "4",
                "10^{-4}",
                ". Notably, the two methods are not conflicted in FL, and we conduct experiments on implementing weight decay in the local SGD solver and global weight shrinking on the server simultaneously. As shown in ",
                "Table¬†2",
                ", ",
                "adaptive GWS is compatible with local weight decay and can further improve performance.",
                " Unlike local weight decay, adaptive GWS is hyperparameter-free and effective. It can adaptively set ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " to maximize the benefit of weight regularization. As the local weight decay becomes stronger, the learned ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " is larger, resulting in weaker GWS regularization. More analysis about global weight shrinking can be found in ",
                "subsection¬†B.1",
                " in Appendix.",
                "4) Insights from FL‚Äôs adaptive GWS to mini-batch SGD.",
                "\nFL‚Äôs adaptive GWS leverages the advantage of the server that learns an adaptive shrinking factor globally. It is promising that similar ideas can be adopted in mini-batch SGD by learning the hyperparameter of weight decay on a small proportion of training data. This may realize hyperparameter-free optimization, and we leave it for future works."
            ]
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Top-1 test accuracy (%) achieved by comparing FL methods and FedLAW on three datasets\nwith different model architectures (E=3ùê∏3E=3).\nBlue/bold fonts highlight the best baseline/our approach.\n",
        "table": "<table id=\"S5.T3.3.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T3.3.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.3.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S5.T3.3.1.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Dataset</span></th>\n<td id=\"S5.T3.3.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span id=\"S5.T3.3.1.2.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">FashionMNIST</span></td>\n<td id=\"S5.T3.3.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span id=\"S5.T3.3.1.2.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">CIFAR-10</span></td>\n<td id=\"S5.T3.3.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span id=\"S5.T3.3.1.2.1.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">CIFAR-100</span></td>\n</tr>\n<tr id=\"S5.T3.3.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.3.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">\n<span id=\"S5.T3.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">NonIID (</span><math id=\"S5.T3.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha\" display=\"inline\"><semantics id=\"S5.T3.3.1.1.1.m1.1a\"><mi mathsize=\"80%\" id=\"S5.T3.3.1.1.1.m1.1.1\" xref=\"S5.T3.3.1.1.1.m1.1.1.cmml\">Œ±</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.3.1.1.1.m1.1b\"><ci id=\"S5.T3.3.1.1.1.m1.1.1.cmml\" xref=\"S5.T3.3.1.1.1.m1.1.1\">ùõº</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.3.1.1.1.m1.1c\">\\alpha</annotation></semantics></math><span id=\"S5.T3.3.1.1.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">)</span>\n</th>\n<td id=\"S5.T3.3.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\"><span id=\"S5.T3.3.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">100</span></td>\n<td id=\"S5.T3.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\"><span id=\"S5.T3.3.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.1</span></td>\n<td id=\"S5.T3.3.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\"><span id=\"S5.T3.3.1.1.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">100</span></td>\n<td id=\"S5.T3.3.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\"><span id=\"S5.T3.3.1.1.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.1</span></td>\n<td id=\"S5.T3.3.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\"><span id=\"S5.T3.3.1.1.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">100</span></td>\n<td id=\"S5.T3.3.1.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\"><span id=\"S5.T3.3.1.1.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.1</span></td>\n</tr>\n<tr id=\"S5.T3.3.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T3.3.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Model</span></th>\n<td id=\"S5.T3.3.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.3.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">MLP</span></td>\n<td id=\"S5.T3.3.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.3.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">LeNet</span></td>\n<td id=\"S5.T3.3.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.3.2.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">MLP</span></td>\n<td id=\"S5.T3.3.1.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.3.2.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">LeNet</span></td>\n<td id=\"S5.T3.3.1.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.3.2.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">CNN</span></td>\n<td id=\"S5.T3.3.1.3.2.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.3.2.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">ResNet</span></td>\n<td id=\"S5.T3.3.1.3.2.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.3.2.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">CNN</span></td>\n<td id=\"S5.T3.3.1.3.2.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.3.2.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">ResNet</span></td>\n<td id=\"S5.T3.3.1.3.2.10\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.3.2.10.1\" class=\"ltx_text\" style=\"font-size:80%;\">CNN</span></td>\n<td id=\"S5.T3.3.1.3.2.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.3.2.11.1\" class=\"ltx_text\" style=\"font-size:80%;\">ResNet</span></td>\n<td id=\"S5.T3.3.1.3.2.12\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.3.2.12.1\" class=\"ltx_text\" style=\"font-size:80%;\">CNN</span></td>\n<td id=\"S5.T3.3.1.3.2.13\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.3.2.13.1\" class=\"ltx_text\" style=\"font-size:80%;\">ResNet</span></td>\n</tr>\n<tr id=\"S5.T3.3.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T3.3.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.4.3.1.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">FedAvg</span></th>\n<td id=\"S5.T3.3.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.4.3.2.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">89.29</span></td>\n<td id=\"S5.T3.3.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.4.3.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">90.54</span></td>\n<td id=\"S5.T3.3.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.4.3.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">85.11</span></td>\n<td id=\"S5.T3.3.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.4.3.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">88.08</span></td>\n<td id=\"S5.T3.3.1.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.4.3.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">65.78</span></td>\n<td id=\"S5.T3.3.1.4.3.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.4.3.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">74.57</span></td>\n<td id=\"S5.T3.3.1.4.3.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.4.3.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">60.13</span></td>\n<td id=\"S5.T3.3.1.4.3.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.4.3.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">46.04</span></td>\n<td id=\"S5.T3.3.1.4.3.10\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.4.3.10.1\" class=\"ltx_text\" style=\"font-size:80%;\">25.74</span></td>\n<td id=\"S5.T3.3.1.4.3.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.4.3.11.1\" class=\"ltx_text\" style=\"font-size:80%;\">27.49</span></td>\n<td id=\"S5.T3.3.1.4.3.12\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.4.3.12.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">27.74</span></td>\n<td id=\"S5.T3.3.1.4.3.13\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.4.3.13.1\" class=\"ltx_text\" style=\"font-size:80%;\">24.92</span></td>\n</tr>\n<tr id=\"S5.T3.3.1.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T3.3.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.5.4.1.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">FedProx</span></th>\n<td id=\"S5.T3.3.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.5.4.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">87.68</span></td>\n<td id=\"S5.T3.3.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.5.4.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">89.77</span></td>\n<td id=\"S5.T3.3.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.5.4.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">84.33</span></td>\n<td id=\"S5.T3.3.1.5.4.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.5.4.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">87.01</span></td>\n<td id=\"S5.T3.3.1.5.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.5.4.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">67.66</span></td>\n<td id=\"S5.T3.3.1.5.4.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.5.4.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">68.51</span></td>\n<td id=\"S5.T3.3.1.5.4.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.5.4.8.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">60.48</span></td>\n<td id=\"S5.T3.3.1.5.4.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.5.4.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">48.84</span></td>\n<td id=\"S5.T3.3.1.5.4.10\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.5.4.10.1\" class=\"ltx_text\" style=\"font-size:80%;\">9.49</span></td>\n<td id=\"S5.T3.3.1.5.4.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.5.4.11.1\" class=\"ltx_text\" style=\"font-size:80%;\">27.15</span></td>\n<td id=\"S5.T3.3.1.5.4.12\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.5.4.12.1\" class=\"ltx_text\" style=\"font-size:80%;\">12.52</span></td>\n<td id=\"S5.T3.3.1.5.4.13\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.5.4.13.1\" class=\"ltx_text\" style=\"font-size:80%;\">23.73</span></td>\n</tr>\n<tr id=\"S5.T3.3.1.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T3.3.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S5.T3.3.1.6.5.1.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">FedDyn</span></th>\n<td id=\"S5.T3.3.1.6.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.3.1.6.5.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">88.47</span></td>\n<td id=\"S5.T3.3.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T3.3.1.6.5.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">89.92</span></td>\n<td id=\"S5.T3.3.1.6.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.3.1.6.5.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">77.68</span></td>\n<td id=\"S5.T3.3.1.6.5.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T3.3.1.6.5.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">72.68</span></td>\n<td id=\"S5.T3.3.1.6.5.6\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.3.1.6.5.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">66.1</span></td>\n<td id=\"S5.T3.3.1.6.5.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T3.3.1.6.5.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">76.62</span></td>\n<td id=\"S5.T3.3.1.6.5.8\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.3.1.6.5.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">41.53</span></td>\n<td id=\"S5.T3.3.1.6.5.9\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T3.3.1.6.5.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">35.77</span></td>\n<td id=\"S5.T3.3.1.6.5.10\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.3.1.6.5.10.1\" class=\"ltx_text\" style=\"font-size:80%;\">24.44</span></td>\n<td id=\"S5.T3.3.1.6.5.11\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T3.3.1.6.5.11.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">32.18</span></td>\n<td id=\"S5.T3.3.1.6.5.12\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.3.1.6.5.12.1\" class=\"ltx_text\" style=\"font-size:80%;\">22.67</span></td>\n<td id=\"S5.T3.3.1.6.5.13\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.3.1.6.5.13.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">29.00</span></td>\n</tr>\n<tr id=\"S5.T3.3.1.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T3.3.1.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.7.6.1.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">FedDF</span></th>\n<td id=\"S5.T3.3.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.7.6.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">86.16</span></td>\n<td id=\"S5.T3.3.1.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.7.6.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">89.09</span></td>\n<td id=\"S5.T3.3.1.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.7.6.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">78.48</span></td>\n<td id=\"S5.T3.3.1.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.7.6.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">85.90</span></td>\n<td id=\"S5.T3.3.1.7.6.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.7.6.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">69.60</span></td>\n<td id=\"S5.T3.3.1.7.6.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.7.6.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">77.36</span></td>\n<td id=\"S5.T3.3.1.7.6.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.7.6.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">57.38</span></td>\n<td id=\"S5.T3.3.1.7.6.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.7.6.9.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">54.09</span></td>\n<td id=\"S5.T3.3.1.7.6.10\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.7.6.10.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">28.52</span></td>\n<td id=\"S5.T3.3.1.7.6.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.7.6.11.1\" class=\"ltx_text\" style=\"font-size:80%;\">27.42</span></td>\n<td id=\"S5.T3.3.1.7.6.12\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.7.6.12.1\" class=\"ltx_text\" style=\"font-size:80%;\">24.52</span></td>\n<td id=\"S5.T3.3.1.7.6.13\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.7.6.13.1\" class=\"ltx_text\" style=\"font-size:80%;\">23.10</span></td>\n</tr>\n<tr id=\"S5.T3.3.1.8.7\" class=\"ltx_tr\">\n<th id=\"S5.T3.3.1.8.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S5.T3.3.1.8.7.1.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">FedBE</span></th>\n<td id=\"S5.T3.3.1.8.7.2\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.3.1.8.7.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">86.22</span></td>\n<td id=\"S5.T3.3.1.8.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T3.3.1.8.7.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">89.14</span></td>\n<td id=\"S5.T3.3.1.8.7.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.3.1.8.7.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">79.12</span></td>\n<td id=\"S5.T3.3.1.8.7.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T3.3.1.8.7.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">85.96</span></td>\n<td id=\"S5.T3.3.1.8.7.6\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.3.1.8.7.6.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">69.88</span></td>\n<td id=\"S5.T3.3.1.8.7.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T3.3.1.8.7.7.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">77.94</span></td>\n<td id=\"S5.T3.3.1.8.7.8\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.3.1.8.7.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">59.84</span></td>\n<td id=\"S5.T3.3.1.8.7.9\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T3.3.1.8.7.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">52.86</span></td>\n<td id=\"S5.T3.3.1.8.7.10\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.3.1.8.7.10.1\" class=\"ltx_text\" style=\"font-size:80%;\">28.38</span></td>\n<td id=\"S5.T3.3.1.8.7.11\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T3.3.1.8.7.11.1\" class=\"ltx_text\" style=\"font-size:80%;\">27.73</span></td>\n<td id=\"S5.T3.3.1.8.7.12\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.3.1.8.7.12.1\" class=\"ltx_text\" style=\"font-size:80%;\">25.41</span></td>\n<td id=\"S5.T3.3.1.8.7.13\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.3.1.8.7.13.1\" class=\"ltx_text\" style=\"font-size:80%;\">23.74</span></td>\n</tr>\n<tr id=\"S5.T3.3.1.9.8\" class=\"ltx_tr\">\n<th id=\"S5.T3.3.1.9.8.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S5.T3.3.1.9.8.1.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:80%;\">Server-FT</span></th>\n<td id=\"S5.T3.3.1.9.8.2\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.3.1.9.8.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">89.09</span></td>\n<td id=\"S5.T3.3.1.9.8.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T3.3.1.9.8.3.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">90.56</span></td>\n<td id=\"S5.T3.3.1.9.8.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.3.1.9.8.4.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">85.71</span></td>\n<td id=\"S5.T3.3.1.9.8.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T3.3.1.9.8.5.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">88.10</span></td>\n<td id=\"S5.T3.3.1.9.8.6\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.3.1.9.8.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">66.83</span></td>\n<td id=\"S5.T3.3.1.9.8.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T3.3.1.9.8.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">74.73</span></td>\n<td id=\"S5.T3.3.1.9.8.8\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.3.1.9.8.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">60.43</span></td>\n<td id=\"S5.T3.3.1.9.8.9\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T3.3.1.9.8.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">47.59</span></td>\n<td id=\"S5.T3.3.1.9.8.10\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.3.1.9.8.10.1\" class=\"ltx_text\" style=\"font-size:80%;\">25.37</span></td>\n<td id=\"S5.T3.3.1.9.8.11\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T3.3.1.9.8.11.1\" class=\"ltx_text\" style=\"font-size:80%;\">26.14</span></td>\n<td id=\"S5.T3.3.1.9.8.12\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.3.1.9.8.12.1\" class=\"ltx_text\" style=\"font-size:80%;\">24.33</span></td>\n<td id=\"S5.T3.3.1.9.8.13\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.3.1.9.8.13.1\" class=\"ltx_text\" style=\"font-size:80%;\">23.03</span></td>\n</tr>\n<tr id=\"S5.T3.3.1.10.9\" class=\"ltx_tr\">\n<th id=\"S5.T3.3.1.10.9.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.10.9.1.1\" class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" style=\"font-size:80%;\">FedLAW</span></th>\n<td id=\"S5.T3.3.1.10.9.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.10.9.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">88.51</span></td>\n<td id=\"S5.T3.3.1.10.9.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.10.9.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">90.66</span></td>\n<td id=\"S5.T3.3.1.10.9.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.10.9.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">86.30</span></td>\n<td id=\"S5.T3.3.1.10.9.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.10.9.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">88.26</span></td>\n<td id=\"S5.T3.3.1.10.9.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.10.9.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">70.17</span></td>\n<td id=\"S5.T3.3.1.10.9.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.10.9.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">80.46</span></td>\n<td id=\"S5.T3.3.1.10.9.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.10.9.8.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">62.46</span></td>\n<td id=\"S5.T3.3.1.10.9.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.10.9.9.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">52.83</span></td>\n<td id=\"S5.T3.3.1.10.9.10\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.10.9.10.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">32.51</span></td>\n<td id=\"S5.T3.3.1.10.9.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.3.1.10.9.11.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">33.17</span></td>\n<td id=\"S5.T3.3.1.10.9.12\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.10.9.12.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">32.30</span></td>\n<td id=\"S5.T3.3.1.10.9.13\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.3.1.10.9.13.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">24.84</span></td>\n</tr>\n<tr id=\"S5.T3.3.1.11.10\" class=\"ltx_tr\">\n<th id=\"S5.T3.3.1.11.10.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span id=\"S5.T3.3.1.11.10.1.1\" class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" style=\"font-size:80%;\">FedLAW (SWA)</span></th>\n<td id=\"S5.T3.3.1.11.10.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T3.3.1.11.10.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">88.27</span></td>\n<td id=\"S5.T3.3.1.11.10.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S5.T3.3.1.11.10.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">90.51</span></td>\n<td id=\"S5.T3.3.1.11.10.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T3.3.1.11.10.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">86.89</span></td>\n<td id=\"S5.T3.3.1.11.10.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S5.T3.3.1.11.10.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">88.18</span></td>\n<td id=\"S5.T3.3.1.11.10.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T3.3.1.11.10.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">69.9</span></td>\n<td id=\"S5.T3.3.1.11.10.7\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S5.T3.3.1.11.10.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">79.55</span></td>\n<td id=\"S5.T3.3.1.11.10.8\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T3.3.1.11.10.8.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">62.12</span></td>\n<td id=\"S5.T3.3.1.11.10.9\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S5.T3.3.1.11.10.9.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">57.08</span></td>\n<td id=\"S5.T3.3.1.11.10.10\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T3.3.1.11.10.10.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">32.39</span></td>\n<td id=\"S5.T3.3.1.11.10.11\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S5.T3.3.1.11.10.11.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">33.17</span></td>\n<td id=\"S5.T3.3.1.11.10.12\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T3.3.1.11.10.12.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">32.27</span></td>\n<td id=\"S5.T3.3.1.11.10.13\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T3.3.1.11.10.13.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">25.31</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Vanilla ",
                "FedAvg",
                " only considers data sizes as clients‚Äô aggregation weights ",
                "ùùÄ",
                "ùùÄ",
                "\\boldsymbol{\\lambda}",
                ". However, clients with different heterogeneity degrees have different importance in client coherence, which can greatly affect the training dynamics. A three-node toy example is shown in ",
                "Figure¬†12",
                " in Appendix. The optimal ",
                "ùùÄ",
                "ùùÄ",
                "\\boldsymbol{\\lambda}",
                " is off the data-sized when clients have the same data size but different heterogeneity degrees. To study client coherence further, we propose attentive learnable aggregation weight (attentive LAW) to learn the optimal aggregation weights (i.e. ",
                "ùùÄ",
                "ùùÄ",
                "\\boldsymbol{\\lambda}",
                ") on a proxy dataset. By connecting the optimal weights and the client coherence, we can know the roles of different clients in different learning periods. Attentive LAW conducts the model updates in ",
                "Equation¬†3",
                ", where ",
                "{",
                "Œ≥",
                "=",
                "1",
                ",",
                "ùùÄ",
                "=",
                "ùùÄ",
                "‚àó",
                "}",
                "formulae-sequence",
                "ùõæ",
                "1",
                "ùùÄ",
                "superscript",
                "ùùÄ",
                "\\{\\gamma=1,\\boldsymbol{\\lambda}=\\boldsymbol{\\lambda^{*}}\\}",
                ",",
                "1) Generalization will benefit from positive local gradient coherence.",
                "Critical point exists in terms of local gradient coherence.",
                " To study the role of client data heterogeneity in local gradient coherence, we experiment on both balanced and imbalanced clients, whose distributions are shown in ",
                "Figure¬†13",
                " of Appendix.\nThe results are demonstrated in ",
                "Figure¬†5",
                ", which illustrate that ",
                "in the first couple of rounds, the coherence is dominant and positive, thus the test accuracy arises dramatically, and most generalization gains happen in this period",
                ".\nThe critical point is the round that the coherence is near zero. After the critical point, the test accuracy gain is marginal, and the coherence is kept negative but close to zero.",
                "Assigning larger weights to clients with larger coherence before the critical point can improve overall performance.",
                " From the left of ",
                "Figure¬†5",
                ", it is clear that before the critical point, the coherence among balanced clients is much higher than that of imbalanced clients. This observation highlights the fact that clients with more balanced data have more coherent gradients\n",
                "5",
                "5",
                "5",
                "This also reveals why FL performs better in IID settings than NonIID: the clients‚Äô gradients in IID settings are more coherent, but the ones in the NonIID usually diverge.",
                ". To capitalize on this, according to ",
                "Equation¬†6",
                ", we can assign larger weights to clients with more balanced data before the critical point to boost generalization. From the right of ",
                "Figure¬†5",
                ", attentive LAW proves our hypothesis: it assigns larger weights to balanced clients in the early rounds, particularly in the first two rounds where it nearly assigns all weights to balanced clients. This may suggest that ",
                "the coherence of clients only matters before the critical point where the overall coherence is positive",
                ".\nTo verify this, we adopt early stopping near the critical point when conducting attentive LAW and use data-sized weights after the stopping round.\nResults in the middle of ",
                "Figure¬†5",
                " show that ",
                "the early-stopped attentive LAW has comparable performance after the critical point.",
                " This insight can guide the design of effective algorithms for learning critically in early training stages.",
                "GWS improves local gradient coherence to positive after the critical point.",
                "\nInterestingly, we observe that ",
                "if we adopt adaptive GWS, the local gradient coherence remains positive after the critical point, allowing the model to continue benefiting from the coherent gradients.",
                " As shown in ",
                "Figure¬†6",
                ", before the critical point, both vanilla ",
                "FedAvg",
                " and adaptive GWS have high gradient coherence, resulting in similar increases in accuracy. After the critical point, the coherence of ",
                "FedAvg",
                " goes down below zero, resulting in marginal performance gains. In contrast, adaptive GWS maintains coherence above zero, allowing for further performance gains beyond ",
                "FedAvg",
                ".",
                "2) Improving heterogeneity coherence within a cohort can boost performance.",
                "\nIn scenarios with partial client participation in each round, the selected clients have an inconsistent sum objective with the global objective, resulting in low heterogeneity coherence (as defined in Definition ",
                "5.2",
                ").\nIn theory, the expectation of the sum objective of the sampled clients is consistent with the global objective if the communication rounds (sampling times) are numerous. However, in practical FL, the number of rounds is limited, and from the perspective of learning dynamics, only the first few rounds matter most. Thus, the heterogeneity coherence problem brings a challenge, and sampling methods may not always help, because the availability of clients cannot be guaranteed and stragglers may exist ",
                "(Li et¬†al., ",
                "2020b",
                ")",
                ".",
                "To address the issue, reweighting the sampled clients in aggregation is quite essential. We find ",
                "attentive LAW improves heterogeneity coherence by dynamically adjusting the aggregation weights among clients",
                ". We visualize the weighted class distributions within a cohort in ",
                "Figure¬†7",
                ", which shows that attentive LAW learns weights to make the class distributions more balanced.\nThe test accuracy curves demonstrate a dominant performance gain compared to ",
                "FedAvg",
                ", which showcases the significance of heterogeneity coherence. Additionally, we observe that attentive LAW with SWA",
                "6",
                "6",
                "6",
                "\nStochastic Weight Averaging (SWA)¬†",
                "(Izmailov et¬†al., ",
                "2018",
                ")",
                " is an effective technique to make simple averaging of multiple points along the trajectory of optimization with a cyclical learning rate, which leads to better generalization.\n",
                " performs better by seeking a more generalized minimum in the aggregation weight hyperplane. More analysis about client coherence can be found in ",
                "subsection¬†B.2",
                " of ",
                "Appendix¬†B",
                "."
            ]
        ]
    },
    "S6.T4": {
        "caption": "Table 4: Performance comparison under different numbers of clients. CIFAR-10, ResNet20, E=3ùê∏3E=3.",
        "table": "<table id=\"S6.T4.5.3\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S6.T4.4.2.2\" class=\"ltx_tr\">\n<th id=\"S6.T4.4.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\">Setting</th>\n<th id=\"S6.T4.3.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\">IID (<math id=\"S6.T4.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha=100\" display=\"inline\"><semantics id=\"S6.T4.3.1.1.1.m1.1a\"><mrow id=\"S6.T4.3.1.1.1.m1.1.1\" xref=\"S6.T4.3.1.1.1.m1.1.1.cmml\"><mi id=\"S6.T4.3.1.1.1.m1.1.1.2\" xref=\"S6.T4.3.1.1.1.m1.1.1.2.cmml\">Œ±</mi><mo id=\"S6.T4.3.1.1.1.m1.1.1.1\" xref=\"S6.T4.3.1.1.1.m1.1.1.1.cmml\">=</mo><mn id=\"S6.T4.3.1.1.1.m1.1.1.3\" xref=\"S6.T4.3.1.1.1.m1.1.1.3.cmml\">100</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S6.T4.3.1.1.1.m1.1b\"><apply id=\"S6.T4.3.1.1.1.m1.1.1.cmml\" xref=\"S6.T4.3.1.1.1.m1.1.1\"><eq id=\"S6.T4.3.1.1.1.m1.1.1.1.cmml\" xref=\"S6.T4.3.1.1.1.m1.1.1.1\"></eq><ci id=\"S6.T4.3.1.1.1.m1.1.1.2.cmml\" xref=\"S6.T4.3.1.1.1.m1.1.1.2\">ùõº</ci><cn type=\"integer\" id=\"S6.T4.3.1.1.1.m1.1.1.3.cmml\" xref=\"S6.T4.3.1.1.1.m1.1.1.3\">100</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T4.3.1.1.1.m1.1c\">\\alpha=100</annotation></semantics></math>)</th>\n<th id=\"S6.T4.4.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\">NonIID (<math id=\"S6.T4.4.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha=1\" display=\"inline\"><semantics id=\"S6.T4.4.2.2.2.m1.1a\"><mrow id=\"S6.T4.4.2.2.2.m1.1.1\" xref=\"S6.T4.4.2.2.2.m1.1.1.cmml\"><mi id=\"S6.T4.4.2.2.2.m1.1.1.2\" xref=\"S6.T4.4.2.2.2.m1.1.1.2.cmml\">Œ±</mi><mo id=\"S6.T4.4.2.2.2.m1.1.1.1\" xref=\"S6.T4.4.2.2.2.m1.1.1.1.cmml\">=</mo><mn id=\"S6.T4.4.2.2.2.m1.1.1.3\" xref=\"S6.T4.4.2.2.2.m1.1.1.3.cmml\">1</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S6.T4.4.2.2.2.m1.1b\"><apply id=\"S6.T4.4.2.2.2.m1.1.1.cmml\" xref=\"S6.T4.4.2.2.2.m1.1.1\"><eq id=\"S6.T4.4.2.2.2.m1.1.1.1.cmml\" xref=\"S6.T4.4.2.2.2.m1.1.1.1\"></eq><ci id=\"S6.T4.4.2.2.2.m1.1.1.2.cmml\" xref=\"S6.T4.4.2.2.2.m1.1.1.2\">ùõº</ci><cn type=\"integer\" id=\"S6.T4.4.2.2.2.m1.1.1.3.cmml\" xref=\"S6.T4.4.2.2.2.m1.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T4.4.2.2.2.m1.1c\">\\alpha=1</annotation></semantics></math>)</th>\n</tr>\n<tr id=\"S6.T4.5.3.3\" class=\"ltx_tr\">\n<th id=\"S6.T4.5.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\">Number of clients <math id=\"S6.T4.5.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"n\" display=\"inline\"><semantics id=\"S6.T4.5.3.3.1.m1.1a\"><mi id=\"S6.T4.5.3.3.1.m1.1.1\" xref=\"S6.T4.5.3.3.1.m1.1.1.cmml\">n</mi><annotation-xml encoding=\"MathML-Content\" id=\"S6.T4.5.3.3.1.m1.1b\"><ci id=\"S6.T4.5.3.3.1.m1.1.1.cmml\" xref=\"S6.T4.5.3.3.1.m1.1.1\">ùëõ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T4.5.3.3.1.m1.1c\">n</annotation></semantics></math>\n</th>\n<th id=\"S6.T4.5.3.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">50</th>\n<th id=\"S6.T4.5.3.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">100</th>\n<th id=\"S6.T4.5.3.3.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">50</th>\n<th id=\"S6.T4.5.3.3.5\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\">100</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T4.5.3.4.1\" class=\"ltx_tr\">\n<th id=\"S6.T4.5.3.4.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S6.T4.5.3.4.1.1.1\" class=\"ltx_text ltx_font_smallcaps\">FedAvg</span></th>\n<td id=\"S6.T4.5.3.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">68.04</td>\n<td id=\"S6.T4.5.3.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">62.41</td>\n<td id=\"S6.T4.5.3.4.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">66.87</td>\n<td id=\"S6.T4.5.3.4.1.5\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">64.13</td>\n</tr>\n<tr id=\"S6.T4.5.3.5.2\" class=\"ltx_tr\">\n<th id=\"S6.T4.5.3.5.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S6.T4.5.3.5.2.1.1\" class=\"ltx_text ltx_font_smallcaps\">FedDF</span></th>\n<td id=\"S6.T4.5.3.5.2.2\" class=\"ltx_td ltx_align_center\">48.24</td>\n<td id=\"S6.T4.5.3.5.2.3\" class=\"ltx_td ltx_align_center\">38.66</td>\n<td id=\"S6.T4.5.3.5.2.4\" class=\"ltx_td ltx_align_center\">38.70</td>\n<td id=\"S6.T4.5.3.5.2.5\" class=\"ltx_td ltx_nopad_r ltx_align_center\">22.51</td>\n</tr>\n<tr id=\"S6.T4.5.3.6.3\" class=\"ltx_tr\">\n<th id=\"S6.T4.5.3.6.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S6.T4.5.3.6.3.1.1\" class=\"ltx_text ltx_font_smallcaps\">Server-FT</span></th>\n<td id=\"S6.T4.5.3.6.3.2\" class=\"ltx_td ltx_align_center\">67.77</td>\n<td id=\"S6.T4.5.3.6.3.3\" class=\"ltx_td ltx_align_center\">62.30</td>\n<td id=\"S6.T4.5.3.6.3.4\" class=\"ltx_td ltx_align_center\">66.73</td>\n<td id=\"S6.T4.5.3.6.3.5\" class=\"ltx_td ltx_nopad_r ltx_align_center\">64.63</td>\n</tr>\n<tr id=\"S6.T4.5.3.7.4\" class=\"ltx_tr\">\n<th id=\"S6.T4.5.3.7.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S6.T4.5.3.7.4.1.1\" class=\"ltx_text ltx_font_smallcaps\">FedLAW</span></th>\n<td id=\"S6.T4.5.3.7.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S6.T4.5.3.7.4.2.1\" class=\"ltx_text ltx_font_bold\">78.88</span></td>\n<td id=\"S6.T4.5.3.7.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S6.T4.5.3.7.4.3.1\" class=\"ltx_text ltx_font_bold\">74.09</span></td>\n<td id=\"S6.T4.5.3.7.4.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S6.T4.5.3.7.4.4.1\" class=\"ltx_text ltx_font_bold\">75.59</span></td>\n<td id=\"S6.T4.5.3.7.4.5\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S6.T4.5.3.7.4.5.1\" class=\"ltx_text ltx_font_bold\">71.34</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Based on the above understandings, we propose ",
                "Fed",
                "erated Learning with ",
                "L",
                "earnable ",
                "A",
                "ggregation ",
                "W",
                "eights algorithm (",
                "FedLAW",
                ") which combines the adaptive GWS and attentive LAW to optimize ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " and ",
                "ùùÄ",
                "ùùÄ",
                "\\boldsymbol{\\lambda}",
                " simultaneously, defined as\n",
                "The pseudo-code of ",
                "FedLAW",
                " is shown in Algorithm ",
                "1",
                ".",
                "With SWA (optional).",
                " We adopt an alternative two-stage strategy for SWA variant (implementing it in a reversed order also works), where we first fix ",
                "ùùÄ",
                "ùùÄ",
                "\\boldsymbol{\\lambda}",
                " and optimize ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                ", then we use the learned ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " and fix it to optimize ",
                "ùùÄ",
                "ùùÄ",
                "\\boldsymbol{\\lambda}",
                " with SWA.",
                "In our experiments, we denote ",
                "FedLAW",
                " with or without SWA as ‚Äú",
                "FedLAW (SWA)",
                "‚Äù or ‚Äú",
                "FedLAW",
                "‚Äù."
            ]
        ]
    },
    "S6.T5": {
        "caption": "Table 5: The performance of compared methods with different model architectures (Œ±=1,E=1formulae-sequenceùõº1ùê∏1\\alpha=1,~{}E=1).",
        "table": "<table id=\"S6.T5.4.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S6.T5.4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S6.T5.4.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S6.T5.4.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">Model</span></th>\n<th id=\"S6.T5.4.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S6.T5.4.1.1.1.2.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">FedAvg</span></th>\n<th id=\"S6.T5.4.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S6.T5.4.1.1.1.3.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">FedLAW</span></th>\n<th id=\"S6.T5.4.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S6.T5.4.1.1.1.4.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:50%;\">FedLAW(SWA)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T5.4.1.2.1\" class=\"ltx_tr\">\n<th id=\"S6.T5.4.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S6.T5.4.1.2.1.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">ResNet20</span></th>\n<td id=\"S6.T5.4.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S6.T5.4.1.2.1.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">74.11</span></td>\n<td id=\"S6.T5.4.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S6.T5.4.1.2.1.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">78.72</span></td>\n<td id=\"S6.T5.4.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S6.T5.4.1.2.1.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">78.64</span></td>\n</tr>\n<tr id=\"S6.T5.4.1.3.2\" class=\"ltx_tr\">\n<th id=\"S6.T5.4.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S6.T5.4.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">ResNet56</span></th>\n<td id=\"S6.T5.4.1.3.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T5.4.1.3.2.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">74.22</span></td>\n<td id=\"S6.T5.4.1.3.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T5.4.1.3.2.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">78.93</span></td>\n<td id=\"S6.T5.4.1.3.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T5.4.1.3.2.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">79.08</span></td>\n</tr>\n<tr id=\"S6.T5.4.1.4.3\" class=\"ltx_tr\">\n<th id=\"S6.T5.4.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S6.T5.4.1.4.3.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">ResNet110</span></th>\n<td id=\"S6.T5.4.1.4.3.2\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T5.4.1.4.3.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">74.50</span></td>\n<td id=\"S6.T5.4.1.4.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T5.4.1.4.3.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">78.11</span></td>\n<td id=\"S6.T5.4.1.4.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T5.4.1.4.3.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">79.19</span></td>\n</tr>\n<tr id=\"S6.T5.4.1.5.4\" class=\"ltx_tr\">\n<th id=\"S6.T5.4.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S6.T5.4.1.5.4.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">WRN56_4</span></th>\n<td id=\"S6.T5.4.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S6.T5.4.1.5.4.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">78.67</span></td>\n<td id=\"S6.T5.4.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S6.T5.4.1.5.4.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">79.61</span></td>\n<td id=\"S6.T5.4.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S6.T5.4.1.5.4.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">80.70</span></td>\n</tr>\n<tr id=\"S6.T5.4.1.6.5\" class=\"ltx_tr\">\n<th id=\"S6.T5.4.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S6.T5.4.1.6.5.1.1\" class=\"ltx_text\" style=\"font-size:50%;\">DenseNet121</span></th>\n<td id=\"S6.T5.4.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S6.T5.4.1.6.5.2.1\" class=\"ltx_text\" style=\"font-size:50%;\">85.13</span></td>\n<td id=\"S6.T5.4.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S6.T5.4.1.6.5.3.1\" class=\"ltx_text\" style=\"font-size:50%;\">86.50</span></td>\n<td id=\"S6.T5.4.1.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S6.T5.4.1.6.5.4.1\" class=\"ltx_text\" style=\"font-size:50%;\">87.06</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Based on the above understandings, we propose ",
                "Fed",
                "erated Learning with ",
                "L",
                "earnable ",
                "A",
                "ggregation ",
                "W",
                "eights algorithm (",
                "FedLAW",
                ") which combines the adaptive GWS and attentive LAW to optimize ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " and ",
                "ùùÄ",
                "ùùÄ",
                "\\boldsymbol{\\lambda}",
                " simultaneously, defined as\n",
                "The pseudo-code of ",
                "FedLAW",
                " is shown in Algorithm ",
                "1",
                ".",
                "With SWA (optional).",
                " We adopt an alternative two-stage strategy for SWA variant (implementing it in a reversed order also works), where we first fix ",
                "ùùÄ",
                "ùùÄ",
                "\\boldsymbol{\\lambda}",
                " and optimize ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                ", then we use the learned ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " and fix it to optimize ",
                "ùùÄ",
                "ùùÄ",
                "\\boldsymbol{\\lambda}",
                " with SWA.",
                "In our experiments, we denote ",
                "FedLAW",
                " with or without SWA as ‚Äú",
                "FedLAW (SWA)",
                "‚Äù or ‚Äú",
                "FedLAW",
                "‚Äù."
            ]
        ]
    },
    "S6.T6": {
        "caption": "Table 6: The performance on the distribution shift setting where the clients‚Äô data are overall balanced and the proxy data are long-tailed (œÅ=10ùúå10\\rho=10).",
        "table": "<table id=\"S6.T6.4.2\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T6.4.2.2\" class=\"ltx_tr\">\n<th id=\"S6.T6.4.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\">Setting</th>\n<td id=\"S6.T6.3.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\">IID (<math id=\"S6.T6.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha=100\" display=\"inline\"><semantics id=\"S6.T6.3.1.1.1.m1.1a\"><mrow id=\"S6.T6.3.1.1.1.m1.1.1\" xref=\"S6.T6.3.1.1.1.m1.1.1.cmml\"><mi id=\"S6.T6.3.1.1.1.m1.1.1.2\" xref=\"S6.T6.3.1.1.1.m1.1.1.2.cmml\">Œ±</mi><mo id=\"S6.T6.3.1.1.1.m1.1.1.1\" xref=\"S6.T6.3.1.1.1.m1.1.1.1.cmml\">=</mo><mn id=\"S6.T6.3.1.1.1.m1.1.1.3\" xref=\"S6.T6.3.1.1.1.m1.1.1.3.cmml\">100</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S6.T6.3.1.1.1.m1.1b\"><apply id=\"S6.T6.3.1.1.1.m1.1.1.cmml\" xref=\"S6.T6.3.1.1.1.m1.1.1\"><eq id=\"S6.T6.3.1.1.1.m1.1.1.1.cmml\" xref=\"S6.T6.3.1.1.1.m1.1.1.1\"></eq><ci id=\"S6.T6.3.1.1.1.m1.1.1.2.cmml\" xref=\"S6.T6.3.1.1.1.m1.1.1.2\">ùõº</ci><cn type=\"integer\" id=\"S6.T6.3.1.1.1.m1.1.1.3.cmml\" xref=\"S6.T6.3.1.1.1.m1.1.1.3\">100</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T6.3.1.1.1.m1.1c\">\\alpha=100</annotation></semantics></math>)</td>\n<td id=\"S6.T6.4.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\">NonIID (<math id=\"S6.T6.4.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha=1\" display=\"inline\"><semantics id=\"S6.T6.4.2.2.2.m1.1a\"><mrow id=\"S6.T6.4.2.2.2.m1.1.1\" xref=\"S6.T6.4.2.2.2.m1.1.1.cmml\"><mi id=\"S6.T6.4.2.2.2.m1.1.1.2\" xref=\"S6.T6.4.2.2.2.m1.1.1.2.cmml\">Œ±</mi><mo id=\"S6.T6.4.2.2.2.m1.1.1.1\" xref=\"S6.T6.4.2.2.2.m1.1.1.1.cmml\">=</mo><mn id=\"S6.T6.4.2.2.2.m1.1.1.3\" xref=\"S6.T6.4.2.2.2.m1.1.1.3.cmml\">1</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S6.T6.4.2.2.2.m1.1b\"><apply id=\"S6.T6.4.2.2.2.m1.1.1.cmml\" xref=\"S6.T6.4.2.2.2.m1.1.1\"><eq id=\"S6.T6.4.2.2.2.m1.1.1.1.cmml\" xref=\"S6.T6.4.2.2.2.m1.1.1.1\"></eq><ci id=\"S6.T6.4.2.2.2.m1.1.1.2.cmml\" xref=\"S6.T6.4.2.2.2.m1.1.1.2\">ùõº</ci><cn type=\"integer\" id=\"S6.T6.4.2.2.2.m1.1.1.3.cmml\" xref=\"S6.T6.4.2.2.2.m1.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T6.4.2.2.2.m1.1c\">\\alpha=1</annotation></semantics></math>)</td>\n</tr>\n<tr id=\"S6.T6.4.2.3.1\" class=\"ltx_tr\">\n<th id=\"S6.T6.4.2.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Type of proxy data</th>\n<td id=\"S6.T6.4.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">Balanced</td>\n<td id=\"S6.T6.4.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\">Long-tailed</td>\n<td id=\"S6.T6.4.2.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">Balanced</td>\n<td id=\"S6.T6.4.2.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\">Long-tailed</td>\n</tr>\n<tr id=\"S6.T6.4.2.4.2\" class=\"ltx_tr\">\n<th id=\"S6.T6.4.2.4.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Balanced Sampling</th>\n<td id=\"S6.T6.4.2.4.2.2\" class=\"ltx_td ltx_align_center\">-</td>\n<td id=\"S6.T6.4.2.4.2.3\" class=\"ltx_td ltx_align_center\">w/o</td>\n<td id=\"S6.T6.4.2.4.2.4\" class=\"ltx_td ltx_align_center\">w</td>\n<td id=\"S6.T6.4.2.4.2.5\" class=\"ltx_td ltx_align_center\">-</td>\n<td id=\"S6.T6.4.2.4.2.6\" class=\"ltx_td ltx_align_center\">w/o</td>\n<td id=\"S6.T6.4.2.4.2.7\" class=\"ltx_td ltx_nopad_r ltx_align_center\">w</td>\n</tr>\n<tr id=\"S6.T6.4.2.5.3\" class=\"ltx_tr\">\n<th id=\"S6.T6.4.2.5.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S6.T6.4.2.5.3.1.1\" class=\"ltx_text ltx_font_smallcaps\">FedAvg</span></th>\n<td id=\"S6.T6.4.2.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">75.24</td>\n<td id=\"S6.T6.4.2.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">75.24</td>\n<td id=\"S6.T6.4.2.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">75.24</td>\n<td id=\"S6.T6.4.2.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\">73.46</td>\n<td id=\"S6.T6.4.2.5.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\">73.46</td>\n<td id=\"S6.T6.4.2.5.3.7\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\"><span id=\"S6.T6.4.2.5.3.7.1\" class=\"ltx_text ltx_font_bold\">73.46</span></td>\n</tr>\n<tr id=\"S6.T6.4.2.6.4\" class=\"ltx_tr\">\n<th id=\"S6.T6.4.2.6.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S6.T6.4.2.6.4.1.1\" class=\"ltx_text ltx_font_smallcaps\">FedDF</span></th>\n<td id=\"S6.T6.4.2.6.4.2\" class=\"ltx_td ltx_align_center\">76.20</td>\n<td id=\"S6.T6.4.2.6.4.3\" class=\"ltx_td ltx_align_center\">74.04</td>\n<td id=\"S6.T6.4.2.6.4.4\" class=\"ltx_td ltx_align_center\">73.31</td>\n<td id=\"S6.T6.4.2.6.4.5\" class=\"ltx_td ltx_align_center\">74.39</td>\n<td id=\"S6.T6.4.2.6.4.6\" class=\"ltx_td ltx_align_center\">73.99</td>\n<td id=\"S6.T6.4.2.6.4.7\" class=\"ltx_td ltx_nopad_r ltx_align_center\">73.14</td>\n</tr>\n<tr id=\"S6.T6.4.2.7.5\" class=\"ltx_tr\">\n<th id=\"S6.T6.4.2.7.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S6.T6.4.2.7.5.1.1\" class=\"ltx_text ltx_font_smallcaps\">FedLAW</span></th>\n<td id=\"S6.T6.4.2.7.5.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S6.T6.4.2.7.5.2.1\" class=\"ltx_text ltx_font_bold\">79.40</span></td>\n<td id=\"S6.T6.4.2.7.5.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S6.T6.4.2.7.5.3.1\" class=\"ltx_text ltx_font_bold\">77.14</span></td>\n<td id=\"S6.T6.4.2.7.5.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S6.T6.4.2.7.5.4.1\" class=\"ltx_text ltx_font_bold\">78.56</span></td>\n<td id=\"S6.T6.4.2.7.5.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S6.T6.4.2.7.5.5.1\" class=\"ltx_text ltx_font_bold\">76.70</span></td>\n<td id=\"S6.T6.4.2.7.5.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S6.T6.4.2.7.5.6.1\" class=\"ltx_text ltx_font_bold\">76.78</span></td>\n<td id=\"S6.T6.4.2.7.5.7\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\">70.42</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Baselines and Settings.",
                " We conduct experiments to verify the effectiveness of ",
                "FedLAW",
                ". We mainly compare ",
                "FedLAW",
                " with other server-side methods, i.e.¬†",
                "FedDF",
                " ",
                "(Lin et¬†al., ",
                "2020",
                ")",
                " and ",
                "FedBE",
                " ",
                "(Chen & Chao, ",
                "2021",
                ")",
                ", that also require a proxy dataset for additional computation. These two methods conduct ensemble distillation on the proxy data to transfer knowledge from clients‚Äô models to the global model. We add ",
                "Server-FT",
                " as a baseline for simply finetuning global models on the proxy dataset. Besides, we implement client-side algorithms ",
                "FedProx",
                " ",
                "(Li et¬†al., ",
                "2020b",
                ")",
                " and ",
                "FedDyn",
                " ",
                "(Acar et¬†al., ",
                "2020",
                ")",
                " for comparison. If not mentioned otherwise, the number of clients is 20. More implementation details can be found in ",
                "Appendix¬†C",
                " and ",
                "Appendix¬†D",
                ".",
                "Experimental results. ",
                "Different datasets:",
                " As in ",
                "Table¬†3",
                ", ",
                "FedLAW",
                " outperforms baselines on different datasets and models in both IID and NonIID settings. Compared with ",
                "FedDF",
                ", ",
                "FedBE",
                " and ",
                "Server-FT",
                ", ",
                "FedLAW",
                " can better utilize the proxy dataset.\n",
                "Different numbers of clients:",
                " We implement experiments by scaling up the number of clients in ",
                "Table¬†4",
                ", and it is shown that ",
                "FedLAW",
                " also surpasses the baselines by large margins.\n",
                "Different model architectures:",
                " We test ",
                "FedLAW",
                " across wider and deeper ResNet and other architecture, such as DenseNet ",
                "(Huang et¬†al., ",
                "2017",
                ")",
                ", in the ",
                "Table¬†5",
                ". It shows that ",
                "FedLAW",
                " is effective across different architectures, and it performs well even when the network goes deeper or wider.\n",
                "Different participation ratios:",
                " From the left of ",
                "Figure¬†8",
                ", ",
                "FedLAW",
                " performs well under partial participation.\n",
                "Different sizes and distributions of proxy dataset:",
                " From the right of ",
                "Figure¬†8",
                ", the server-side baselines are sensitive to the size of the proxy dataset that too small or too large proxy set will cause overfitting. However, ",
                "FedLAW",
                " is also effective under an extremely tiny proxy set and benefits more from a larger proxy set due to accurate aggregation weight optimization. We report the results of different distributions of the proxy dataset in ",
                "Table¬†6",
                " and ",
                "Table¬†8",
                ", which show that FedLAW still works when there exists a distribution shift between the proxy dataset and the global data distribution of clients.\n",
                "Robustness against corrupted clients:",
                " Another advantage of ",
                "FedLAW",
                " is that it can filter out corrupted clients by assigning them lower weights. We generate corrupted clients by swapping two labels in their local training data. As in ",
                "Table¬†8",
                ", ",
                "FedLAW",
                " is robust against corrupted clients, and it is as robust as the ensemble distillation methods, such as ",
                "FedDF",
                ", using the same proxy dataset.",
                "More results.",
                " We present more results in the appendix. Specifically, the learning curves of test accuracy (Figures ",
                "15",
                "-",
                "17",
                ") and the server training process of ",
                "FedLAW",
                " (",
                "Figure¬†14",
                ")."
            ]
        ]
    },
    "S6.T8": {
        "caption": "",
        "table": "<table id=\"S6.T8.4.4.2\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S6.T8.4.4.2.2\" class=\"ltx_tr\">\n<th id=\"S6.T8.4.4.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\">Method</th>\n<th id=\"S6.T8.3.3.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">IID (<math id=\"S6.T8.3.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha=100\" display=\"inline\"><semantics id=\"S6.T8.3.3.1.1.1.m1.1a\"><mrow id=\"S6.T8.3.3.1.1.1.m1.1.1\" xref=\"S6.T8.3.3.1.1.1.m1.1.1.cmml\"><mi id=\"S6.T8.3.3.1.1.1.m1.1.1.2\" xref=\"S6.T8.3.3.1.1.1.m1.1.1.2.cmml\">Œ±</mi><mo id=\"S6.T8.3.3.1.1.1.m1.1.1.1\" xref=\"S6.T8.3.3.1.1.1.m1.1.1.1.cmml\">=</mo><mn id=\"S6.T8.3.3.1.1.1.m1.1.1.3\" xref=\"S6.T8.3.3.1.1.1.m1.1.1.3.cmml\">100</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S6.T8.3.3.1.1.1.m1.1b\"><apply id=\"S6.T8.3.3.1.1.1.m1.1.1.cmml\" xref=\"S6.T8.3.3.1.1.1.m1.1.1\"><eq id=\"S6.T8.3.3.1.1.1.m1.1.1.1.cmml\" xref=\"S6.T8.3.3.1.1.1.m1.1.1.1\"></eq><ci id=\"S6.T8.3.3.1.1.1.m1.1.1.2.cmml\" xref=\"S6.T8.3.3.1.1.1.m1.1.1.2\">ùõº</ci><cn type=\"integer\" id=\"S6.T8.3.3.1.1.1.m1.1.1.3.cmml\" xref=\"S6.T8.3.3.1.1.1.m1.1.1.3\">100</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T8.3.3.1.1.1.m1.1c\">\\alpha=100</annotation></semantics></math>)</th>\n<th id=\"S6.T8.4.4.2.2.2\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\">NonIID (<math id=\"S6.T8.4.4.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha=1\" display=\"inline\"><semantics id=\"S6.T8.4.4.2.2.2.m1.1a\"><mrow id=\"S6.T8.4.4.2.2.2.m1.1.1\" xref=\"S6.T8.4.4.2.2.2.m1.1.1.cmml\"><mi id=\"S6.T8.4.4.2.2.2.m1.1.1.2\" xref=\"S6.T8.4.4.2.2.2.m1.1.1.2.cmml\">Œ±</mi><mo id=\"S6.T8.4.4.2.2.2.m1.1.1.1\" xref=\"S6.T8.4.4.2.2.2.m1.1.1.1.cmml\">=</mo><mn id=\"S6.T8.4.4.2.2.2.m1.1.1.3\" xref=\"S6.T8.4.4.2.2.2.m1.1.1.3.cmml\">1</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S6.T8.4.4.2.2.2.m1.1b\"><apply id=\"S6.T8.4.4.2.2.2.m1.1.1.cmml\" xref=\"S6.T8.4.4.2.2.2.m1.1.1\"><eq id=\"S6.T8.4.4.2.2.2.m1.1.1.1.cmml\" xref=\"S6.T8.4.4.2.2.2.m1.1.1.1\"></eq><ci id=\"S6.T8.4.4.2.2.2.m1.1.1.2.cmml\" xref=\"S6.T8.4.4.2.2.2.m1.1.1.2\">ùõº</ci><cn type=\"integer\" id=\"S6.T8.4.4.2.2.2.m1.1.1.3.cmml\" xref=\"S6.T8.4.4.2.2.2.m1.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T8.4.4.2.2.2.m1.1c\">\\alpha=1</annotation></semantics></math>)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T8.4.4.2.3.1\" class=\"ltx_tr\">\n<th id=\"S6.T8.4.4.2.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S6.T8.4.4.2.3.1.1.1\" class=\"ltx_text ltx_font_smallcaps\">FedAvg</span></th>\n<td id=\"S6.T8.4.4.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">61.12</td>\n<td id=\"S6.T8.4.4.2.3.1.3\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">59.82</td>\n</tr>\n<tr id=\"S6.T8.4.4.2.4.2\" class=\"ltx_tr\">\n<th id=\"S6.T8.4.4.2.4.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S6.T8.4.4.2.4.2.1.1\" class=\"ltx_text ltx_font_smallcaps\">FedDF</span></th>\n<td id=\"S6.T8.4.4.2.4.2.2\" class=\"ltx_td ltx_align_center\">39.03</td>\n<td id=\"S6.T8.4.4.2.4.2.3\" class=\"ltx_td ltx_nopad_r ltx_align_center\">40.68</td>\n</tr>\n<tr id=\"S6.T8.4.4.2.5.3\" class=\"ltx_tr\">\n<th id=\"S6.T8.4.4.2.5.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S6.T8.4.4.2.5.3.1.1\" class=\"ltx_text ltx_font_smallcaps\">FedLAW</span></th>\n<td id=\"S6.T8.4.4.2.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S6.T8.4.4.2.5.3.2.1\" class=\"ltx_text ltx_font_bold\">67.61</span></td>\n<td id=\"S6.T8.4.4.2.5.3.3\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S6.T8.4.4.2.5.3.3.1\" class=\"ltx_text ltx_font_bold\">66.48</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Baselines and Settings.",
                " We conduct experiments to verify the effectiveness of ",
                "FedLAW",
                ". We mainly compare ",
                "FedLAW",
                " with other server-side methods, i.e.¬†",
                "FedDF",
                " ",
                "(Lin et¬†al., ",
                "2020",
                ")",
                " and ",
                "FedBE",
                " ",
                "(Chen & Chao, ",
                "2021",
                ")",
                ", that also require a proxy dataset for additional computation. These two methods conduct ensemble distillation on the proxy data to transfer knowledge from clients‚Äô models to the global model. We add ",
                "Server-FT",
                " as a baseline for simply finetuning global models on the proxy dataset. Besides, we implement client-side algorithms ",
                "FedProx",
                " ",
                "(Li et¬†al., ",
                "2020b",
                ")",
                " and ",
                "FedDyn",
                " ",
                "(Acar et¬†al., ",
                "2020",
                ")",
                " for comparison. If not mentioned otherwise, the number of clients is 20. More implementation details can be found in ",
                "Appendix¬†C",
                " and ",
                "Appendix¬†D",
                ".",
                "Experimental results. ",
                "Different datasets:",
                " As in ",
                "Table¬†3",
                ", ",
                "FedLAW",
                " outperforms baselines on different datasets and models in both IID and NonIID settings. Compared with ",
                "FedDF",
                ", ",
                "FedBE",
                " and ",
                "Server-FT",
                ", ",
                "FedLAW",
                " can better utilize the proxy dataset.\n",
                "Different numbers of clients:",
                " We implement experiments by scaling up the number of clients in ",
                "Table¬†4",
                ", and it is shown that ",
                "FedLAW",
                " also surpasses the baselines by large margins.\n",
                "Different model architectures:",
                " We test ",
                "FedLAW",
                " across wider and deeper ResNet and other architecture, such as DenseNet ",
                "(Huang et¬†al., ",
                "2017",
                ")",
                ", in the ",
                "Table¬†5",
                ". It shows that ",
                "FedLAW",
                " is effective across different architectures, and it performs well even when the network goes deeper or wider.\n",
                "Different participation ratios:",
                " From the left of ",
                "Figure¬†8",
                ", ",
                "FedLAW",
                " performs well under partial participation.\n",
                "Different sizes and distributions of proxy dataset:",
                " From the right of ",
                "Figure¬†8",
                ", the server-side baselines are sensitive to the size of the proxy dataset that too small or too large proxy set will cause overfitting. However, ",
                "FedLAW",
                " is also effective under an extremely tiny proxy set and benefits more from a larger proxy set due to accurate aggregation weight optimization. We report the results of different distributions of the proxy dataset in ",
                "Table¬†6",
                " and ",
                "Table¬†8",
                ", which show that FedLAW still works when there exists a distribution shift between the proxy dataset and the global data distribution of clients.\n",
                "Robustness against corrupted clients:",
                " Another advantage of ",
                "FedLAW",
                " is that it can filter out corrupted clients by assigning them lower weights. We generate corrupted clients by swapping two labels in their local training data. As in ",
                "Table¬†8",
                ", ",
                "FedLAW",
                " is robust against corrupted clients, and it is as robust as the ensemble distillation methods, such as ",
                "FedDF",
                ", using the same proxy dataset.",
                "More results.",
                " We present more results in the appendix. Specifically, the learning curves of test accuracy (Figures ",
                "15",
                "-",
                "17",
                ") and the server training process of ",
                "FedLAW",
                " (",
                "Figure¬†14",
                ")."
            ]
        ]
    },
    "A2.T9": {
        "caption": "Table 9: More results about fixed Œ≥ùõæ\\gamma across different architectures in various NonIID settings.",
        "table": "<table id=\"A2.T9.7\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"A2.T9.3.1\" class=\"ltx_tr\">\n<th id=\"A2.T9.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><math id=\"A2.T9.3.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\gamma\" display=\"inline\"><semantics id=\"A2.T9.3.1.1.m1.1a\"><mi mathsize=\"90%\" id=\"A2.T9.3.1.1.m1.1.1\" xref=\"A2.T9.3.1.1.m1.1.1.cmml\">Œ≥</mi><annotation-xml encoding=\"MathML-Content\" id=\"A2.T9.3.1.1.m1.1b\"><ci id=\"A2.T9.3.1.1.m1.1.1.cmml\" xref=\"A2.T9.3.1.1.m1.1.1\">ùõæ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T9.3.1.1.m1.1c\">\\gamma</annotation></semantics></math></th>\n<td id=\"A2.T9.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A2.T9.3.1.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1.0</span></td>\n<td id=\"A2.T9.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A2.T9.3.1.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.99</span></td>\n<td id=\"A2.T9.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A2.T9.3.1.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.97</span></td>\n<td id=\"A2.T9.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A2.T9.3.1.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.95</span></td>\n<td id=\"A2.T9.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A2.T9.3.1.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.93</span></td>\n<td id=\"A2.T9.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"A2.T9.3.1.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.9</span></td>\n</tr>\n<tr id=\"A2.T9.4.2\" class=\"ltx_tr\">\n<th id=\"A2.T9.4.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A2.T9.4.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Model</span></th>\n<td id=\"A2.T9.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"6\"><math id=\"A2.T9.4.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha=10\" display=\"inline\"><semantics id=\"A2.T9.4.2.1.m1.1a\"><mrow id=\"A2.T9.4.2.1.m1.1.1\" xref=\"A2.T9.4.2.1.m1.1.1.cmml\"><mi mathsize=\"90%\" id=\"A2.T9.4.2.1.m1.1.1.2\" xref=\"A2.T9.4.2.1.m1.1.1.2.cmml\">Œ±</mi><mo mathsize=\"90%\" id=\"A2.T9.4.2.1.m1.1.1.1\" xref=\"A2.T9.4.2.1.m1.1.1.1.cmml\">=</mo><mn mathsize=\"90%\" id=\"A2.T9.4.2.1.m1.1.1.3\" xref=\"A2.T9.4.2.1.m1.1.1.3.cmml\">10</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A2.T9.4.2.1.m1.1b\"><apply id=\"A2.T9.4.2.1.m1.1.1.cmml\" xref=\"A2.T9.4.2.1.m1.1.1\"><eq id=\"A2.T9.4.2.1.m1.1.1.1.cmml\" xref=\"A2.T9.4.2.1.m1.1.1.1\"></eq><ci id=\"A2.T9.4.2.1.m1.1.1.2.cmml\" xref=\"A2.T9.4.2.1.m1.1.1.2\">ùõº</ci><cn type=\"integer\" id=\"A2.T9.4.2.1.m1.1.1.3.cmml\" xref=\"A2.T9.4.2.1.m1.1.1.3\">10</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T9.4.2.1.m1.1c\">\\alpha=10</annotation></semantics></math></td>\n</tr>\n<tr id=\"A2.T9.7.6.1\" class=\"ltx_tr\">\n<th id=\"A2.T9.7.6.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A2.T9.7.6.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">SimpleCNN</span></th>\n<td id=\"A2.T9.7.6.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T9.7.6.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">65.96</span></td>\n<td id=\"A2.T9.7.6.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T9.7.6.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">67.19</span></td>\n<td id=\"A2.T9.7.6.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T9.7.6.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">69.41</span></td>\n<td id=\"A2.T9.7.6.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T9.7.6.1.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">69.81</span></td>\n<td id=\"A2.T9.7.6.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T9.7.6.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">69.69</span></td>\n<td id=\"A2.T9.7.6.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T9.7.6.1.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">69.59</span></td>\n</tr>\n<tr id=\"A2.T9.7.7.2\" class=\"ltx_tr\">\n<th id=\"A2.T9.7.7.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"A2.T9.7.7.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">AlexNet</span></th>\n<td id=\"A2.T9.7.7.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.7.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">73.9</span></td>\n<td id=\"A2.T9.7.7.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.7.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">74.43</span></td>\n<td id=\"A2.T9.7.7.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.7.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">74.96</span></td>\n<td id=\"A2.T9.7.7.2.5\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.7.2.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">75.12</span></td>\n<td id=\"A2.T9.7.7.2.6\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.7.2.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">75.33</span></td>\n<td id=\"A2.T9.7.7.2.7\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.7.2.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">74.06</span></td>\n</tr>\n<tr id=\"A2.T9.7.8.3\" class=\"ltx_tr\">\n<th id=\"A2.T9.7.8.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"A2.T9.7.8.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">ResNet8</span></th>\n<td id=\"A2.T9.7.8.3.2\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.8.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">76.26</span></td>\n<td id=\"A2.T9.7.8.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.8.3.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">75.63</span></td>\n<td id=\"A2.T9.7.8.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.8.3.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">76.92</span></td>\n<td id=\"A2.T9.7.8.3.5\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.8.3.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">77.23</span></td>\n<td id=\"A2.T9.7.8.3.6\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.8.3.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">76.9</span></td>\n<td id=\"A2.T9.7.8.3.7\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.8.3.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">76.61</span></td>\n</tr>\n<tr id=\"A2.T9.5.3\" class=\"ltx_tr\">\n<th id=\"A2.T9.5.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A2.T9.5.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Model</span></th>\n<td id=\"A2.T9.5.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"6\"><math id=\"A2.T9.5.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha=0.5\" display=\"inline\"><semantics id=\"A2.T9.5.3.1.m1.1a\"><mrow id=\"A2.T9.5.3.1.m1.1.1\" xref=\"A2.T9.5.3.1.m1.1.1.cmml\"><mi mathsize=\"90%\" id=\"A2.T9.5.3.1.m1.1.1.2\" xref=\"A2.T9.5.3.1.m1.1.1.2.cmml\">Œ±</mi><mo mathsize=\"90%\" id=\"A2.T9.5.3.1.m1.1.1.1\" xref=\"A2.T9.5.3.1.m1.1.1.1.cmml\">=</mo><mn mathsize=\"90%\" id=\"A2.T9.5.3.1.m1.1.1.3\" xref=\"A2.T9.5.3.1.m1.1.1.3.cmml\">0.5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A2.T9.5.3.1.m1.1b\"><apply id=\"A2.T9.5.3.1.m1.1.1.cmml\" xref=\"A2.T9.5.3.1.m1.1.1\"><eq id=\"A2.T9.5.3.1.m1.1.1.1.cmml\" xref=\"A2.T9.5.3.1.m1.1.1.1\"></eq><ci id=\"A2.T9.5.3.1.m1.1.1.2.cmml\" xref=\"A2.T9.5.3.1.m1.1.1.2\">ùõº</ci><cn type=\"float\" id=\"A2.T9.5.3.1.m1.1.1.3.cmml\" xref=\"A2.T9.5.3.1.m1.1.1.3\">0.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T9.5.3.1.m1.1c\">\\alpha=0.5</annotation></semantics></math></td>\n</tr>\n<tr id=\"A2.T9.7.9.4\" class=\"ltx_tr\">\n<th id=\"A2.T9.7.9.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A2.T9.7.9.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">SimpleCNN</span></th>\n<td id=\"A2.T9.7.9.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T9.7.9.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">65.78</span></td>\n<td id=\"A2.T9.7.9.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T9.7.9.4.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">66.59</span></td>\n<td id=\"A2.T9.7.9.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T9.7.9.4.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">67.93</span></td>\n<td id=\"A2.T9.7.9.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T9.7.9.4.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">68.85</span></td>\n<td id=\"A2.T9.7.9.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T9.7.9.4.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">68.75</span></td>\n<td id=\"A2.T9.7.9.4.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T9.7.9.4.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">68.25</span></td>\n</tr>\n<tr id=\"A2.T9.7.10.5\" class=\"ltx_tr\">\n<th id=\"A2.T9.7.10.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"A2.T9.7.10.5.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">AlexNet</span></th>\n<td id=\"A2.T9.7.10.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.10.5.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">73.72</span></td>\n<td id=\"A2.T9.7.10.5.3\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.10.5.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">73.06</span></td>\n<td id=\"A2.T9.7.10.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.10.5.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">73.89</span></td>\n<td id=\"A2.T9.7.10.5.5\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.10.5.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">73.98</span></td>\n<td id=\"A2.T9.7.10.5.6\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.10.5.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">73.6</span></td>\n<td id=\"A2.T9.7.10.5.7\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.10.5.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">73.33</span></td>\n</tr>\n<tr id=\"A2.T9.7.11.6\" class=\"ltx_tr\">\n<th id=\"A2.T9.7.11.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"A2.T9.7.11.6.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">ResNet8</span></th>\n<td id=\"A2.T9.7.11.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.11.6.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">73.4</span></td>\n<td id=\"A2.T9.7.11.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.11.6.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">73.93</span></td>\n<td id=\"A2.T9.7.11.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.11.6.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">75.39</span></td>\n<td id=\"A2.T9.7.11.6.5\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.11.6.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">74.12</span></td>\n<td id=\"A2.T9.7.11.6.6\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.11.6.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">73.66</span></td>\n<td id=\"A2.T9.7.11.6.7\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.11.6.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">73.46</span></td>\n</tr>\n<tr id=\"A2.T9.6.4\" class=\"ltx_tr\">\n<th id=\"A2.T9.6.4.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A2.T9.6.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Model</span></th>\n<td id=\"A2.T9.6.4.1\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"6\"><math id=\"A2.T9.6.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha=0.2\" display=\"inline\"><semantics id=\"A2.T9.6.4.1.m1.1a\"><mrow id=\"A2.T9.6.4.1.m1.1.1\" xref=\"A2.T9.6.4.1.m1.1.1.cmml\"><mi mathsize=\"90%\" id=\"A2.T9.6.4.1.m1.1.1.2\" xref=\"A2.T9.6.4.1.m1.1.1.2.cmml\">Œ±</mi><mo mathsize=\"90%\" id=\"A2.T9.6.4.1.m1.1.1.1\" xref=\"A2.T9.6.4.1.m1.1.1.1.cmml\">=</mo><mn mathsize=\"90%\" id=\"A2.T9.6.4.1.m1.1.1.3\" xref=\"A2.T9.6.4.1.m1.1.1.3.cmml\">0.2</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A2.T9.6.4.1.m1.1b\"><apply id=\"A2.T9.6.4.1.m1.1.1.cmml\" xref=\"A2.T9.6.4.1.m1.1.1\"><eq id=\"A2.T9.6.4.1.m1.1.1.1.cmml\" xref=\"A2.T9.6.4.1.m1.1.1.1\"></eq><ci id=\"A2.T9.6.4.1.m1.1.1.2.cmml\" xref=\"A2.T9.6.4.1.m1.1.1.2\">ùõº</ci><cn type=\"float\" id=\"A2.T9.6.4.1.m1.1.1.3.cmml\" xref=\"A2.T9.6.4.1.m1.1.1.3\">0.2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T9.6.4.1.m1.1c\">\\alpha=0.2</annotation></semantics></math></td>\n</tr>\n<tr id=\"A2.T9.7.12.7\" class=\"ltx_tr\">\n<th id=\"A2.T9.7.12.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A2.T9.7.12.7.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">SimpleCNN</span></th>\n<td id=\"A2.T9.7.12.7.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T9.7.12.7.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">63.52</span></td>\n<td id=\"A2.T9.7.12.7.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T9.7.12.7.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">64.68</span></td>\n<td id=\"A2.T9.7.12.7.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T9.7.12.7.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">63.72</span></td>\n<td id=\"A2.T9.7.12.7.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T9.7.12.7.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">65.82</span></td>\n<td id=\"A2.T9.7.12.7.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T9.7.12.7.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">65.4</span></td>\n<td id=\"A2.T9.7.12.7.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T9.7.12.7.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">64.97</span></td>\n</tr>\n<tr id=\"A2.T9.7.13.8\" class=\"ltx_tr\">\n<th id=\"A2.T9.7.13.8.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"A2.T9.7.13.8.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">AlexNet</span></th>\n<td id=\"A2.T9.7.13.8.2\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.13.8.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">68.41</span></td>\n<td id=\"A2.T9.7.13.8.3\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.13.8.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">70.46</span></td>\n<td id=\"A2.T9.7.13.8.4\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.13.8.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">70.87</span></td>\n<td id=\"A2.T9.7.13.8.5\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.13.8.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">70.74</span></td>\n<td id=\"A2.T9.7.13.8.6\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.13.8.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">70.58</span></td>\n<td id=\"A2.T9.7.13.8.7\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.13.8.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">69.42</span></td>\n</tr>\n<tr id=\"A2.T9.7.14.9\" class=\"ltx_tr\">\n<th id=\"A2.T9.7.14.9.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"A2.T9.7.14.9.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">ResNet8</span></th>\n<td id=\"A2.T9.7.14.9.2\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.14.9.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">71.85</span></td>\n<td id=\"A2.T9.7.14.9.3\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.14.9.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">70.96</span></td>\n<td id=\"A2.T9.7.14.9.4\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.14.9.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">72.76</span></td>\n<td id=\"A2.T9.7.14.9.5\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.14.9.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">72.04</span></td>\n<td id=\"A2.T9.7.14.9.6\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.14.9.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">71.25</span></td>\n<td id=\"A2.T9.7.14.9.7\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.14.9.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">62.32</span></td>\n</tr>\n<tr id=\"A2.T9.7.5\" class=\"ltx_tr\">\n<th id=\"A2.T9.7.5.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A2.T9.7.5.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Model</span></th>\n<td id=\"A2.T9.7.5.1\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"6\"><math id=\"A2.T9.7.5.1.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha=0.1\" display=\"inline\"><semantics id=\"A2.T9.7.5.1.m1.1a\"><mrow id=\"A2.T9.7.5.1.m1.1.1\" xref=\"A2.T9.7.5.1.m1.1.1.cmml\"><mi mathsize=\"90%\" id=\"A2.T9.7.5.1.m1.1.1.2\" xref=\"A2.T9.7.5.1.m1.1.1.2.cmml\">Œ±</mi><mo mathsize=\"90%\" id=\"A2.T9.7.5.1.m1.1.1.1\" xref=\"A2.T9.7.5.1.m1.1.1.1.cmml\">=</mo><mn mathsize=\"90%\" id=\"A2.T9.7.5.1.m1.1.1.3\" xref=\"A2.T9.7.5.1.m1.1.1.3.cmml\">0.1</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A2.T9.7.5.1.m1.1b\"><apply id=\"A2.T9.7.5.1.m1.1.1.cmml\" xref=\"A2.T9.7.5.1.m1.1.1\"><eq id=\"A2.T9.7.5.1.m1.1.1.1.cmml\" xref=\"A2.T9.7.5.1.m1.1.1.1\"></eq><ci id=\"A2.T9.7.5.1.m1.1.1.2.cmml\" xref=\"A2.T9.7.5.1.m1.1.1.2\">ùõº</ci><cn type=\"float\" id=\"A2.T9.7.5.1.m1.1.1.3.cmml\" xref=\"A2.T9.7.5.1.m1.1.1.3\">0.1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T9.7.5.1.m1.1c\">\\alpha=0.1</annotation></semantics></math></td>\n</tr>\n<tr id=\"A2.T9.7.15.10\" class=\"ltx_tr\">\n<th id=\"A2.T9.7.15.10.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A2.T9.7.15.10.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">SimpleCNN</span></th>\n<td id=\"A2.T9.7.15.10.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T9.7.15.10.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">60.57</span></td>\n<td id=\"A2.T9.7.15.10.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T9.7.15.10.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">61.22</span></td>\n<td id=\"A2.T9.7.15.10.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T9.7.15.10.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">61.83</span></td>\n<td id=\"A2.T9.7.15.10.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T9.7.15.10.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">62.05</span></td>\n<td id=\"A2.T9.7.15.10.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T9.7.15.10.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">62.05</span></td>\n<td id=\"A2.T9.7.15.10.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T9.7.15.10.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">60.85</span></td>\n</tr>\n<tr id=\"A2.T9.7.16.11\" class=\"ltx_tr\">\n<th id=\"A2.T9.7.16.11.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"A2.T9.7.16.11.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">AlexNet</span></th>\n<td id=\"A2.T9.7.16.11.2\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.16.11.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">66.18</span></td>\n<td id=\"A2.T9.7.16.11.3\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.16.11.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">65.25</span></td>\n<td id=\"A2.T9.7.16.11.4\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.16.11.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">64.74</span></td>\n<td id=\"A2.T9.7.16.11.5\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.16.11.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">64.23</span></td>\n<td id=\"A2.T9.7.16.11.6\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.16.11.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">64.16</span></td>\n<td id=\"A2.T9.7.16.11.7\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T9.7.16.11.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">61.24</span></td>\n</tr>\n<tr id=\"A2.T9.7.17.12\" class=\"ltx_tr\">\n<th id=\"A2.T9.7.17.12.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span id=\"A2.T9.7.17.12.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">ResNet8</span></th>\n<td id=\"A2.T9.7.17.12.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A2.T9.7.17.12.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">63.89</span></td>\n<td id=\"A2.T9.7.17.12.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A2.T9.7.17.12.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">60.55</span></td>\n<td id=\"A2.T9.7.17.12.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A2.T9.7.17.12.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">61.38</span></td>\n<td id=\"A2.T9.7.17.12.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A2.T9.7.17.12.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">59.23</span></td>\n<td id=\"A2.T9.7.17.12.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A2.T9.7.17.12.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">58.76</span></td>\n<td id=\"A2.T9.7.17.12.7\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A2.T9.7.17.12.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">39.85</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Fixed ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                ".",
                " We add more results about global weight shrinking experiments with fixed ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " as in ",
                "Table¬†9",
                ". It is found that when data are more NonIID, fixed ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " will cause negative effects; this is more dominant when ",
                "Œ±",
                "=",
                "0.1",
                "ùõº",
                "0.1",
                "\\alpha=0.1",
                " and the models are AlexNet or ResNet8.",
                "Adaptive GWS with global learning rate.",
                " We conduct experiments with the adaptive GWS under different global learning rates for both IID and NonIID settings. We train SimpleCNN on CIFAR10 with 1 local epoch, and the results are reported in ",
                "Table¬†10",
                ". It can be observed that in both IID and NonIID settings, a small global server learning rate can improve ",
                "FedAvg",
                "‚Äôs performance. In contrast, the larger the global learning rate, the smaller the learned ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " (stronger regularization). It is aligned with our insights in the main paper that larger pseudo gradients require stronger regularization. Moreover, adaptive GWS is robust to the choice of the global server learning rate, especially in the IID setting.",
                "Adaptive GWS under various heterogeneity.",
                " We show adaptive GWS works under various heterogeneity and visualize ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " and the norm of the global gradient in each setting, as in ",
                "Figure¬†9",
                ". It demonstrates that adaptive GWS can boost performance under different NonIID settings, but it has a smaller benefit when the system is extremely NonIID (i.e., ",
                "Œ±",
                "=",
                "0.1",
                "ùõº",
                "0.1",
                "\\alpha=0.1",
                "). Additionally, according to the right figure of ",
                "Figure¬†9",
                ", except for the outlier ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " when ",
                "Œ±",
                "=",
                "10",
                "ùõº",
                "10",
                "\\alpha=10",
                ", the learned ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " decreases when data become more IID, causing stronger weight shrinking effect. We think this is a result of a balance between optimization and regularization. The volumes of global gradients change when the heterogeneity changes. The norm of global gradient increases when data become more IID, and it requires smaller ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " to cause stronger regularization.",
                "More results of general understanding of adaptive GWS.",
                " First, we first visualize the norm of model parameter weight during training as in the left figure of ",
                "Figure¬†10",
                ". Adaptive GWS results in a smaller model parameter during training. Second, we use two common metrics to measure the flatness of loss landscape during training as in the middle and right figures of ",
                "Figure¬†10",
                ", and they are the hessian eigenvalue-based metrics. The dominant hessian eigenvalue evaluates the worst-case loss landscape, which means the larger top 1 eigenvalue indicates the greater change in the loss along this direction and the sharper the minima ",
                "(Keskar et¬†al., ",
                "2017",
                ")",
                ". We adopt the top 1 hessian eigenvalue and the ratio of top 1 and top 5, which are commonly used as a proxy for flatness ",
                "(Jastrzebski et¬†al., ",
                "2020",
                "; Fort & Jastrzebski, ",
                "2019",
                ")",
                ". Usually, a smaller top 1 hessian eigenvalue and a smaller ratio of top 1 hessian eigenvalue and top 5 indicates flatter curvature of DNN. As in the figures, during the training, ",
                "FedAvg",
                " generates global models with sharp landscapes whereas adaptive GWS tends to generate more generalized models with flatter curvatures.",
                "The distribution of ",
                "r",
                "ùëü",
                "r",
                ".",
                " We visualize ",
                "r",
                "ùëü",
                "r",
                " (the ratio of the global gradient and the regularization pseudo gradient) values of all experiments in ",
                "Figure¬†2",
                " and ",
                "Figure¬†9",
                " as in ",
                "Figure¬†11",
                ". It is found that the distribution of ",
                "r",
                "ùëü",
                "r",
                " can be approximated into a Gaussian distribution with its mean around 20.5."
            ]
        ]
    },
    "A2.T10": {
        "caption": "Table 10: The performance of adaptive GWS under different global learning rates.",
        "table": "<table id=\"A2.T10.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A2.T10.2.2\" class=\"ltx_tr\">\n<th id=\"A2.T10.2.2.3\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"></th>\n<th id=\"A2.T10.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"3\">IID (<math id=\"A2.T10.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha=100\" display=\"inline\"><semantics id=\"A2.T10.1.1.1.m1.1a\"><mrow id=\"A2.T10.1.1.1.m1.1.1\" xref=\"A2.T10.1.1.1.m1.1.1.cmml\"><mi id=\"A2.T10.1.1.1.m1.1.1.2\" xref=\"A2.T10.1.1.1.m1.1.1.2.cmml\">Œ±</mi><mo id=\"A2.T10.1.1.1.m1.1.1.1\" xref=\"A2.T10.1.1.1.m1.1.1.1.cmml\">=</mo><mn id=\"A2.T10.1.1.1.m1.1.1.3\" xref=\"A2.T10.1.1.1.m1.1.1.3.cmml\">100</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A2.T10.1.1.1.m1.1b\"><apply id=\"A2.T10.1.1.1.m1.1.1.cmml\" xref=\"A2.T10.1.1.1.m1.1.1\"><eq id=\"A2.T10.1.1.1.m1.1.1.1.cmml\" xref=\"A2.T10.1.1.1.m1.1.1.1\"></eq><ci id=\"A2.T10.1.1.1.m1.1.1.2.cmml\" xref=\"A2.T10.1.1.1.m1.1.1.2\">ùõº</ci><cn type=\"integer\" id=\"A2.T10.1.1.1.m1.1.1.3.cmml\" xref=\"A2.T10.1.1.1.m1.1.1.3\">100</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T10.1.1.1.m1.1c\">\\alpha=100</annotation></semantics></math>)</th>\n<th id=\"A2.T10.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\">NonIID (<math id=\"A2.T10.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha=1\" display=\"inline\"><semantics id=\"A2.T10.2.2.2.m1.1a\"><mrow id=\"A2.T10.2.2.2.m1.1.1\" xref=\"A2.T10.2.2.2.m1.1.1.cmml\"><mi id=\"A2.T10.2.2.2.m1.1.1.2\" xref=\"A2.T10.2.2.2.m1.1.1.2.cmml\">Œ±</mi><mo id=\"A2.T10.2.2.2.m1.1.1.1\" xref=\"A2.T10.2.2.2.m1.1.1.1.cmml\">=</mo><mn id=\"A2.T10.2.2.2.m1.1.1.3\" xref=\"A2.T10.2.2.2.m1.1.1.3.cmml\">1</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A2.T10.2.2.2.m1.1b\"><apply id=\"A2.T10.2.2.2.m1.1.1.cmml\" xref=\"A2.T10.2.2.2.m1.1.1\"><eq id=\"A2.T10.2.2.2.m1.1.1.1.cmml\" xref=\"A2.T10.2.2.2.m1.1.1.1\"></eq><ci id=\"A2.T10.2.2.2.m1.1.1.2.cmml\" xref=\"A2.T10.2.2.2.m1.1.1.2\">ùõº</ci><cn type=\"integer\" id=\"A2.T10.2.2.2.m1.1.1.3.cmml\" xref=\"A2.T10.2.2.2.m1.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T10.2.2.2.m1.1c\">\\alpha=1</annotation></semantics></math>)</th>\n</tr>\n<tr id=\"A2.T10.3.4.1\" class=\"ltx_tr\">\n<th id=\"A2.T10.3.4.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\">Global learning rate</th>\n<th id=\"A2.T10.3.4.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.5</th>\n<th id=\"A2.T10.3.4.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">1</th>\n<th id=\"A2.T10.3.4.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">1.5</th>\n<th id=\"A2.T10.3.4.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.5</th>\n<th id=\"A2.T10.3.4.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">1</th>\n<th id=\"A2.T10.3.4.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">1.5</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A2.T10.3.5.1\" class=\"ltx_tr\">\n<th id=\"A2.T10.3.5.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">FedAvg</th>\n<td id=\"A2.T10.3.5.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">69.15</td>\n<td id=\"A2.T10.3.5.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">68.18</td>\n<td id=\"A2.T10.3.5.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">64.35</td>\n<td id=\"A2.T10.3.5.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">68.71</td>\n<td id=\"A2.T10.3.5.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">67.09</td>\n<td id=\"A2.T10.3.5.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">64.00</td>\n</tr>\n<tr id=\"A2.T10.3.6.2\" class=\"ltx_tr\">\n<th id=\"A2.T10.3.6.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Adaptive GWS</th>\n<td id=\"A2.T10.3.6.2.2\" class=\"ltx_td ltx_align_center\">71.45</td>\n<td id=\"A2.T10.3.6.2.3\" class=\"ltx_td ltx_align_center\">71.98</td>\n<td id=\"A2.T10.3.6.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\">71.13</td>\n<td id=\"A2.T10.3.6.2.5\" class=\"ltx_td ltx_align_center\">69.65</td>\n<td id=\"A2.T10.3.6.2.6\" class=\"ltx_td ltx_align_center\">71.02</td>\n<td id=\"A2.T10.3.6.2.7\" class=\"ltx_td ltx_align_center\">71.04</td>\n</tr>\n<tr id=\"A2.T10.3.3\" class=\"ltx_tr\">\n<th id=\"A2.T10.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\">\n<math id=\"A2.T10.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\gamma\" display=\"inline\"><semantics id=\"A2.T10.3.3.1.m1.1a\"><mi id=\"A2.T10.3.3.1.m1.1.1\" xref=\"A2.T10.3.3.1.m1.1.1.cmml\">Œ≥</mi><annotation-xml encoding=\"MathML-Content\" id=\"A2.T10.3.3.1.m1.1b\"><ci id=\"A2.T10.3.3.1.m1.1.1.cmml\" xref=\"A2.T10.3.3.1.m1.1.1\">ùõæ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T10.3.3.1.m1.1c\">\\gamma</annotation></semantics></math> of Adaptive GWS</th>\n<td id=\"A2.T10.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_b\">0.986</td>\n<td id=\"A2.T10.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_b\">0.974</td>\n<td id=\"A2.T10.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">0.963</td>\n<td id=\"A2.T10.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_b\">0.991</td>\n<td id=\"A2.T10.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_b\">0.979</td>\n<td id=\"A2.T10.3.3.7\" class=\"ltx_td ltx_align_center ltx_border_b\">0.967</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Fixed ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                ".",
                " We add more results about global weight shrinking experiments with fixed ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " as in ",
                "Table¬†9",
                ". It is found that when data are more NonIID, fixed ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " will cause negative effects; this is more dominant when ",
                "Œ±",
                "=",
                "0.1",
                "ùõº",
                "0.1",
                "\\alpha=0.1",
                " and the models are AlexNet or ResNet8.",
                "Adaptive GWS with global learning rate.",
                " We conduct experiments with the adaptive GWS under different global learning rates for both IID and NonIID settings. We train SimpleCNN on CIFAR10 with 1 local epoch, and the results are reported in ",
                "Table¬†10",
                ". It can be observed that in both IID and NonIID settings, a small global server learning rate can improve ",
                "FedAvg",
                "‚Äôs performance. In contrast, the larger the global learning rate, the smaller the learned ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " (stronger regularization). It is aligned with our insights in the main paper that larger pseudo gradients require stronger regularization. Moreover, adaptive GWS is robust to the choice of the global server learning rate, especially in the IID setting.",
                "Adaptive GWS under various heterogeneity.",
                " We show adaptive GWS works under various heterogeneity and visualize ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " and the norm of the global gradient in each setting, as in ",
                "Figure¬†9",
                ". It demonstrates that adaptive GWS can boost performance under different NonIID settings, but it has a smaller benefit when the system is extremely NonIID (i.e., ",
                "Œ±",
                "=",
                "0.1",
                "ùõº",
                "0.1",
                "\\alpha=0.1",
                "). Additionally, according to the right figure of ",
                "Figure¬†9",
                ", except for the outlier ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " when ",
                "Œ±",
                "=",
                "10",
                "ùõº",
                "10",
                "\\alpha=10",
                ", the learned ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " decreases when data become more IID, causing stronger weight shrinking effect. We think this is a result of a balance between optimization and regularization. The volumes of global gradients change when the heterogeneity changes. The norm of global gradient increases when data become more IID, and it requires smaller ",
                "Œ≥",
                "ùõæ",
                "\\gamma",
                " to cause stronger regularization.",
                "More results of general understanding of adaptive GWS.",
                " First, we first visualize the norm of model parameter weight during training as in the left figure of ",
                "Figure¬†10",
                ". Adaptive GWS results in a smaller model parameter during training. Second, we use two common metrics to measure the flatness of loss landscape during training as in the middle and right figures of ",
                "Figure¬†10",
                ", and they are the hessian eigenvalue-based metrics. The dominant hessian eigenvalue evaluates the worst-case loss landscape, which means the larger top 1 eigenvalue indicates the greater change in the loss along this direction and the sharper the minima ",
                "(Keskar et¬†al., ",
                "2017",
                ")",
                ". We adopt the top 1 hessian eigenvalue and the ratio of top 1 and top 5, which are commonly used as a proxy for flatness ",
                "(Jastrzebski et¬†al., ",
                "2020",
                "; Fort & Jastrzebski, ",
                "2019",
                ")",
                ". Usually, a smaller top 1 hessian eigenvalue and a smaller ratio of top 1 hessian eigenvalue and top 5 indicates flatter curvature of DNN. As in the figures, during the training, ",
                "FedAvg",
                " generates global models with sharp landscapes whereas adaptive GWS tends to generate more generalized models with flatter curvatures.",
                "The distribution of ",
                "r",
                "ùëü",
                "r",
                ".",
                " We visualize ",
                "r",
                "ùëü",
                "r",
                " (the ratio of the global gradient and the regularization pseudo gradient) values of all experiments in ",
                "Figure¬†2",
                " and ",
                "Figure¬†9",
                " as in ",
                "Figure¬†11",
                ". It is found that the distribution of ",
                "r",
                "ùëü",
                "r",
                " can be approximated into a Gaussian distribution with its mean around 20.5."
            ]
        ]
    },
    "A2.T11": {
        "caption": "Table 11: Pearson correlation coefficient analysis of AW. Heterogeneity degree is calculated as the reciprocal of the variance of class distribution for each client. We take the accumulated weights during the training as clients‚Äô AW.",
        "table": "<table id=\"A2.T11.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A2.T11.3.3\" class=\"ltx_tr\">\n<th id=\"A2.T11.3.3.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"A2.T11.3.3.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">Factors</span></th>\n<th id=\"A2.T11.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><math id=\"A2.T11.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"E=1\" display=\"inline\"><semantics id=\"A2.T11.1.1.1.m1.1a\"><mrow id=\"A2.T11.1.1.1.m1.1.1\" xref=\"A2.T11.1.1.1.m1.1.1.cmml\"><mi mathsize=\"90%\" id=\"A2.T11.1.1.1.m1.1.1.2\" xref=\"A2.T11.1.1.1.m1.1.1.2.cmml\">E</mi><mo mathsize=\"90%\" id=\"A2.T11.1.1.1.m1.1.1.1\" xref=\"A2.T11.1.1.1.m1.1.1.1.cmml\">=</mo><mn mathsize=\"90%\" id=\"A2.T11.1.1.1.m1.1.1.3\" xref=\"A2.T11.1.1.1.m1.1.1.3.cmml\">1</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A2.T11.1.1.1.m1.1b\"><apply id=\"A2.T11.1.1.1.m1.1.1.cmml\" xref=\"A2.T11.1.1.1.m1.1.1\"><eq id=\"A2.T11.1.1.1.m1.1.1.1.cmml\" xref=\"A2.T11.1.1.1.m1.1.1.1\"></eq><ci id=\"A2.T11.1.1.1.m1.1.1.2.cmml\" xref=\"A2.T11.1.1.1.m1.1.1.2\">ùê∏</ci><cn type=\"integer\" id=\"A2.T11.1.1.1.m1.1.1.3.cmml\" xref=\"A2.T11.1.1.1.m1.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T11.1.1.1.m1.1c\">E=1</annotation></semantics></math></th>\n<th id=\"A2.T11.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><math id=\"A2.T11.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"E=5\" display=\"inline\"><semantics id=\"A2.T11.2.2.2.m1.1a\"><mrow id=\"A2.T11.2.2.2.m1.1.1\" xref=\"A2.T11.2.2.2.m1.1.1.cmml\"><mi mathsize=\"90%\" id=\"A2.T11.2.2.2.m1.1.1.2\" xref=\"A2.T11.2.2.2.m1.1.1.2.cmml\">E</mi><mo mathsize=\"90%\" id=\"A2.T11.2.2.2.m1.1.1.1\" xref=\"A2.T11.2.2.2.m1.1.1.1.cmml\">=</mo><mn mathsize=\"90%\" id=\"A2.T11.2.2.2.m1.1.1.3\" xref=\"A2.T11.2.2.2.m1.1.1.3.cmml\">5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A2.T11.2.2.2.m1.1b\"><apply id=\"A2.T11.2.2.2.m1.1.1.cmml\" xref=\"A2.T11.2.2.2.m1.1.1\"><eq id=\"A2.T11.2.2.2.m1.1.1.1.cmml\" xref=\"A2.T11.2.2.2.m1.1.1.1\"></eq><ci id=\"A2.T11.2.2.2.m1.1.1.2.cmml\" xref=\"A2.T11.2.2.2.m1.1.1.2\">ùê∏</ci><cn type=\"integer\" id=\"A2.T11.2.2.2.m1.1.1.3.cmml\" xref=\"A2.T11.2.2.2.m1.1.1.3\">5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T11.2.2.2.m1.1c\">E=5</annotation></semantics></math></th>\n<th id=\"A2.T11.3.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><math id=\"A2.T11.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"E=1\\&amp;E=5\" display=\"inline\"><semantics id=\"A2.T11.3.3.3.m1.1a\"><mrow id=\"A2.T11.3.3.3.m1.1.1\" xref=\"A2.T11.3.3.3.m1.1.1.cmml\"><mi mathsize=\"90%\" id=\"A2.T11.3.3.3.m1.1.1.2\" xref=\"A2.T11.3.3.3.m1.1.1.2.cmml\">E</mi><mo mathsize=\"90%\" id=\"A2.T11.3.3.3.m1.1.1.3\" xref=\"A2.T11.3.3.3.m1.1.1.3.cmml\">=</mo><mrow id=\"A2.T11.3.3.3.m1.1.1.4\" xref=\"A2.T11.3.3.3.m1.1.1.4.cmml\"><mn mathsize=\"90%\" id=\"A2.T11.3.3.3.m1.1.1.4.2\" xref=\"A2.T11.3.3.3.m1.1.1.4.2.cmml\">1</mn><mo lspace=\"0.222em\" mathsize=\"90%\" rspace=\"0.222em\" id=\"A2.T11.3.3.3.m1.1.1.4.1\" xref=\"A2.T11.3.3.3.m1.1.1.4.1.cmml\">&amp;</mo><mi mathsize=\"90%\" id=\"A2.T11.3.3.3.m1.1.1.4.3\" xref=\"A2.T11.3.3.3.m1.1.1.4.3.cmml\">E</mi></mrow><mo mathsize=\"90%\" id=\"A2.T11.3.3.3.m1.1.1.5\" xref=\"A2.T11.3.3.3.m1.1.1.5.cmml\">=</mo><mn mathsize=\"90%\" id=\"A2.T11.3.3.3.m1.1.1.6\" xref=\"A2.T11.3.3.3.m1.1.1.6.cmml\">5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A2.T11.3.3.3.m1.1b\"><apply id=\"A2.T11.3.3.3.m1.1.1.cmml\" xref=\"A2.T11.3.3.3.m1.1.1\"><and id=\"A2.T11.3.3.3.m1.1.1a.cmml\" xref=\"A2.T11.3.3.3.m1.1.1\"></and><apply id=\"A2.T11.3.3.3.m1.1.1b.cmml\" xref=\"A2.T11.3.3.3.m1.1.1\"><eq id=\"A2.T11.3.3.3.m1.1.1.3.cmml\" xref=\"A2.T11.3.3.3.m1.1.1.3\"></eq><ci id=\"A2.T11.3.3.3.m1.1.1.2.cmml\" xref=\"A2.T11.3.3.3.m1.1.1.2\">ùê∏</ci><apply id=\"A2.T11.3.3.3.m1.1.1.4.cmml\" xref=\"A2.T11.3.3.3.m1.1.1.4\"><and id=\"A2.T11.3.3.3.m1.1.1.4.1.cmml\" xref=\"A2.T11.3.3.3.m1.1.1.4.1\"></and><cn type=\"integer\" id=\"A2.T11.3.3.3.m1.1.1.4.2.cmml\" xref=\"A2.T11.3.3.3.m1.1.1.4.2\">1</cn><ci id=\"A2.T11.3.3.3.m1.1.1.4.3.cmml\" xref=\"A2.T11.3.3.3.m1.1.1.4.3\">ùê∏</ci></apply></apply><apply id=\"A2.T11.3.3.3.m1.1.1c.cmml\" xref=\"A2.T11.3.3.3.m1.1.1\"><eq id=\"A2.T11.3.3.3.m1.1.1.5.cmml\" xref=\"A2.T11.3.3.3.m1.1.1.5\"></eq><share href=\"#A2.T11.3.3.3.m1.1.1.4.cmml\" id=\"A2.T11.3.3.3.m1.1.1d.cmml\" xref=\"A2.T11.3.3.3.m1.1.1\"></share><cn type=\"integer\" id=\"A2.T11.3.3.3.m1.1.1.6.cmml\" xref=\"A2.T11.3.3.3.m1.1.1.6\">5</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T11.3.3.3.m1.1c\">E=1\\&amp;E=5</annotation></semantics></math></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A2.T11.4.5.1\" class=\"ltx_tr\">\n<th id=\"A2.T11.4.5.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A2.T11.4.5.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Dataset size (DS)</span></th>\n<td id=\"A2.T11.4.5.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T11.4.5.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">-0.098</span></td>\n<td id=\"A2.T11.4.5.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T11.4.5.1.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.21</span></td>\n<td id=\"A2.T11.4.5.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"A2.T11.4.5.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.035</span></td>\n</tr>\n<tr id=\"A2.T11.4.6.2\" class=\"ltx_tr\">\n<th id=\"A2.T11.4.6.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"A2.T11.4.6.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Heterogeneity degree (HD)</span></th>\n<td id=\"A2.T11.4.6.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T11.4.6.2.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.41</span></td>\n<td id=\"A2.T11.4.6.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T11.4.6.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.024</span></td>\n<td id=\"A2.T11.4.6.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"A2.T11.4.6.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.35</span></td>\n</tr>\n<tr id=\"A2.T11.4.4\" class=\"ltx_tr\">\n<th id=\"A2.T11.4.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">\n<span id=\"A2.T11.4.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">DS</span><math id=\"A2.T11.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"A2.T11.4.4.1.m1.1a\"><mo mathsize=\"90%\" id=\"A2.T11.4.4.1.m1.1.1\" xref=\"A2.T11.4.4.1.m1.1.1.cmml\">√ó</mo><annotation-xml encoding=\"MathML-Content\" id=\"A2.T11.4.4.1.m1.1b\"><times id=\"A2.T11.4.4.1.m1.1.1.cmml\" xref=\"A2.T11.4.4.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A2.T11.4.4.1.m1.1c\">\\times</annotation></semantics></math><span id=\"A2.T11.4.4.1.2\" class=\"ltx_text\" style=\"font-size:90%;\">HD</span>\n</th>\n<td id=\"A2.T11.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A2.T11.4.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.26</span></td>\n<td id=\"A2.T11.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A2.T11.4.4.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.17</span></td>\n<td id=\"A2.T11.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"A2.T11.4.4.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.31</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "The relationship with gradient diversity.",
                "\nThe conclusion of gradient diversity ",
                "(Yin et¬†al., ",
                "2018",
                ")",
                " is opposite to the one of gradient coherence. Gradient diversity argues that higher similarities between workers‚Äô gradients will degrade performance in distributed mini-batch SGD, while gradient coherence claims that higher similarities between the gradients of samples will boost generalization ",
                "(Yin et¬†al., ",
                "2018",
                "; Chatterjee, ",
                "2019",
                ")",
                ". Moreover, gradient diversity is somewhat controversial. As argued in the line of works about gradient coherence ",
                "(Chatterjee & Zielinski, ",
                "2020",
                "; Chatterjee, ",
                "2019",
                ")",
                ", the manuscript of gradient diversity did not explicitly measure the gradient diversity in the experiments (or further study its properties): only experiments on CIFAR-10 can be found where they replicate ",
                "1",
                "/",
                "r",
                "1",
                "ùëü",
                "1/r",
                " of the dataset ",
                "r",
                "ùëü",
                "r",
                " times and show that greater the value of r less the effectiveness of mini-batching to speed up. Apart from this controversy, the strongly-convex assumption in the theorem of gradient diversity ",
                "(Yin et¬†al., ",
                "2018",
                ")",
                " may make it weaker to generalize its conclusions in neural networks while we are studying the empirical properties in FL with neural networks. Taking the above statements into consideration, gradient diversity may be infeasible in our settings.",
                "The relationship with client similarity works in FL.",
                "\nThere are some works ",
                "(Karimireddy et¬†al., ",
                "2020",
                "; Li et¬†al., ",
                "2020b",
                ")",
                " taking the bounded gradient dissimilarity assumption to deduce theorems. In their assumptions, they bound the gradient sum or gradient norm, but we use the cosine similarity to study how the clients interplay with each other and contribute to the global. So the perspectives are quite different. Additionally, there are previous works in FL that use cosine similarity of clients‚Äô gradients to improve personalization ",
                "(Huang et¬†al., ",
                "2021",
                "; Li et¬†al., ",
                "2022b",
                ")",
                "; however, we focus on the training dynamics in generalization, and one of our novel findings is we discover a critical point exists and the periods that before or after this point play different roles in the global generalization.",
                "Visualization of how heterogeneity affects the optimal aggregation weight.",
                " We set up a three-node toy example on CIFAR-10 by hybrid Dirichlet sampling as shown in ",
                "Figure¬†12",
                ". We first sample client 0‚Äôs data distribution by Dirichlet sampling according to ",
                "Œ±",
                "1",
                "subscript",
                "ùõº",
                "1",
                "\\alpha_{1}",
                "; then we sample data distributions for clients 1 and 2 on the remaining data with ",
                "Œ±",
                "2",
                "subscript",
                "ùõº",
                "2",
                "\\alpha_{2}",
                ". We set up three settings with different ",
                "Œ±",
                "1",
                ",",
                "Œ±",
                "2",
                "subscript",
                "ùõº",
                "1",
                "subscript",
                "ùõº",
                "2",
                "\\alpha_{1},\\alpha_{2}",
                " and illustrate the data distributions on the ",
                "Left column",
                " in ",
                "Figure¬†12",
                ". In the example, the aggregation weights (AWs) are ",
                "[",
                "Œª",
                "0",
                ",",
                "Œª",
                "1",
                ",",
                "Œª",
                "2",
                "]",
                "subscript",
                "ùúÜ",
                "0",
                "subscript",
                "ùúÜ",
                "1",
                "subscript",
                "ùúÜ",
                "2",
                "[\\lambda_{0},\\lambda_{1},\\lambda_{2}]",
                ", we regularize the weights as ",
                "Œª",
                "0",
                "+",
                "Œª",
                "1",
                "+",
                "Œª",
                "2",
                "=",
                "1",
                "subscript",
                "ùúÜ",
                "0",
                "subscript",
                "ùúÜ",
                "1",
                "subscript",
                "ùúÜ",
                "2",
                "1",
                "\\lambda_{0}+\\lambda_{1}+\\lambda_{2}=1",
                " which is a plane that can be visualized in 2-D. We uniformly sample points on the plane to obtain global models with different AW and compute the test loss, and then the loss landscapes on the plane can be visualized. We implement ",
                "FedAvg",
                " for 100 rounds and record the loss landscape and the optimal weight on the loss landscape in each round; then we illustrate the loss landscape of round 10 on the ",
                "Middle column",
                " and the optimal weight trajectory on the ",
                "Right column",
                " of ",
                "Figure¬†12",
                ".",
                "In these settings, clients have different heterogeneity degrees: in the first setting, client 0 has a balanced dataset while the data of clients 1 and 2 are complementary; in the second and third settings, clients 1 and 2 have the same data distribution, which differs from the client 0‚Äôs. From ",
                "Figure¬†12",
                ", it is evident that the weight of ",
                "FedAvg",
                " is biased from optimal weights when heterogeneity degrees vary in clients, we can draw the following conclusions: (1) optimal weight can be viewed as a Gaussian distribution in the aggregation weight hyperplane; (2) the mean of the Gaussian will drift towards to the directions where data are more inter-heterogeneous (for instance, in the third setting, client 0‚Äôs major classes are 2, 3 and 8 while client 1 and 2 have rare data on these classes, so client 0‚Äôs contribution is more dominant); (3) the variance of the Gaussian is larger in inter-homogeneous direction and is smaller in inter-heterogeneous direction (the variance along the client 1-client 2 direction is large in the second and third settings, because the two clients have inter-homogeneous data; opposite phenomenon is shown in the first setting, where client 1 and 2 have inter-heterogeneous data); (4) the flatness of loss landscape on aggregation weight hyperplane is consistent with the variance of the Gaussian, which means the directions with more significant variance will have flatter curvature in the landscape. From our analysis, it is clear that clients‚Äô contributions to the global model should not be solely measured by dataset size, and the heterogeneity degree should also be taken into account. And we observe that in a more heterogeneous environment, the loss landscape is sharper, which means the bias from optimal weight will cause more generalization drop. In other words, in a heterogeneous environment, appropriate aggregation weight matters more.",
                "Visualization of the hybrid NonIID setting of ",
                "Figure¬†5",
                ".",
                " We visualize the hybrid NonIID setting of ",
                "Figure¬†5",
                " in ",
                "Figure¬†13",
                ". We take ",
                "Œ±",
                "1",
                "=",
                "10",
                "subscript",
                "ùõº",
                "1",
                "10",
                "\\alpha_{1}=10",
                " and ",
                "Œ±",
                "2",
                "=",
                "0.1",
                "subscript",
                "ùõº",
                "2",
                "0.1",
                "\\alpha_{2}=0.1",
                ", so the first 10 clients (indexed 0-9) have class-balanced data while the last 10 clients (indexed 10-19) have class-imbalanced data.",
                "Data size or heterogeneity? A correlation analysis.",
                " Data size and heterogeneity all affect clients‚Äô contributions to the global model, but which affects it most? As in previous literature, the importance is depicted by the dataset size that clients with more data will be assigned larger weights. According to the analysis in ",
                "Figure¬†12",
                ", the importance of weight may be associated with the heterogeneity degrees of clients. To explore which factor is more dominant in the AW optimized by attentive LAW, we have made a Pearson correlation coefficient analysis in ",
                "Table¬†11",
                ". Results show that dataset size is more dominant when the local epoch is large; otherwise, the heterogeneity degree. This phenomenon is intuitive: when the local epoch increases, clients with a larger dataset will have more local iterations than others ",
                "(Wang et¬†al., ",
                "2020b",
                ")",
                ", so their updates are more dominant. In the cases where the local epoch is small, clients‚Äô updates are of similar volumes; here the updates‚Äô directions are much more important since balanced clients are prone to have stronger coherence, and their AWs are larger in model aggregation. We combine two factors by multiplication, and the result shows that the combined indicator is more dominant when the two cases are mixed."
            ]
        ]
    }
}