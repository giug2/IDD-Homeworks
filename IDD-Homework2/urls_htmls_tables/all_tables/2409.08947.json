{
    "id_table_1": {
        "caption": "Table 1:    Quantitative results of our 3D relighting on the synthetic datasets (where ground truth is available), compared to previous work, from left to right: OutCast  [ GRP22 ]  (run on individual images from 3DGS  [ KKLD23 ] ), Relightable3DGaussians  [ GGL  23 ] , and TensoIR  [ JLX  23 ] . Arrows indicate higher/lower (   /     \\uparrow/\\downarrow  /  ) is better. Results are color coded by  best ,  second-  and  third- best.",
        "table": "S3.F5.8",
        "footnotes": [
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Our method is composed of three main parts. First, we create a 2D relighting neural network with direct control of lighting direction (Sec.  3.1 ). Second, we use this network to augment a multi-view capture with single lighting into a multi-lighting dataset, by using our relighting network. The resulting dataset can be used to create a radiance field representation of the 3D scene (Sec.  3.2 ). Finally, we create a relightable radiance field that accounts for inaccuracies in the synthesized relit input images and provides a multi-view consistent lighting solution (Sec.  3.3 ).",
            "We propose to provide explicit control over lighting by fine-tuning a pre-trained Stable Diffusion (SD)  [ RBL  22 ]  model using ControlNet  [ ZRA23 ]  on a multi-illumination dataset. As illustrated in Fig.  1 , the ControlNet accepts as input an image as well as a target light direction, and produces a relit version of the same scene under the desired lighting. To train the ControlNet, we leverage the dataset of Murmann et al.  \\shortcite multilum, which contains  N = 1015 N 1015 N=1015 italic_N = 1015  real indoor scenes captured from a single viewpoint, each lit under  M = 25 M 25 M=25 italic_M = 25  different, controlled lighting directions. We only keep the 18 non-front facing light directions.",
            "We train ControlNet to predict relit versions of the input image by conditioning it on a target lighting direction. Let us denote a set  X X \\mathcal{X} caligraphic_X  of images of a given scene in the multi-light dataset of Murmann et al.  [ MGAD19 ] , where each image  X k  X subscript X k X \\mathbf{X}_{k}\\in\\mathcal{X} bold_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT  caligraphic_X  has associated light direction  l k subscript l k l_{k} italic_l start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT . Our approach, illustrated in Fig.  1 , trains on pairs of lighting directions of the same scene (including the identity pair). The denoising objective becomes",
            "Given a multi-view set  I I \\mathcal{I} caligraphic_I  of images of a scene captured under the same lighting (suitable for training a radiance field model), we now leverage our light-conditioned ControlNet model to synthetically relight each image in  I I \\mathcal{I} caligraphic_I . We assume the 3D pose of each image  I i  I subscript I i I \\mathbf{I}_{i}\\in\\mathcal{I} bold_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  caligraphic_I  is known a priori, for example via Colmap  [ SF16 ,  SZPF16 ] . We then simply relight each  I i  I subscript I i I \\mathbf{I}_{i}\\in\\mathcal{I} bold_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  caligraphic_I  to the corresponding 18 known light directions in the dataset from Murmann et al.  \\shortcite multilum (excluding the directions where the flash points forward), (see Sec.  3.1 ). We now have a full multi-lighting, multi-view dataset. This process is illustrated in Fig.  6 .",
            "Since light directions are computed with respect to a local camera reference frame (c.f. Sec.  3.1 ), we subsequently register them to the world coordinate system (obtained from Colmap) by rotating them according to their (known) camera rotation parameters:",
            "We first train 3DGS with the unlit images as a warmup stage for 5K iterations, then train the full multi-illumination solution for another 25K iterations, using all 18 back-facing light directions (see Sec.  3.1 ). The multi-illumination nature of the training results in an increase in floaters. As observed by Philip and Deschaintre  [ PD23 ] , floaters are often present close to the input cameras; the explicit nature of 3DGS allows us to reduce these effectively. In particular, we calculate a  z n  e  a  r subscript z n e a r z_{near} italic_z start_POSTSUBSCRIPT italic_n italic_e italic_a italic_r end_POSTSUBSCRIPT  value for all cameras by taking the  z z z italic_z  value of the 1st percentile of nearest SfM points and scaling this value down by 0.9. During training, at each step, all gaussian primitives that project within the view frustum of a camera but are located in front of its  z n  e  a  r subscript z n e a r z_{near} italic_z start_POSTSUBSCRIPT italic_n italic_e italic_a italic_r end_POSTSUBSCRIPT  plane are culled. Finally, given the complexity of modeling variable lighting, we observed that the optimization sometimes converges to blurry results. To counter this, we overweight three front-facing views (left, right, and center), by optimizing for one of these views every three iterations. This provides marginal improvement in results; all images shown are computed with this method, but it is optional.",
            "We present quantitative results in Table  1 . We present per-scene results on the following image quality metrics: PSNR, SSIM, and LPIPS  [ ZIE  18 ] . The results demonstrate that our method outperforms all others in all but a few scenarios, where it still achieves competitive performance.",
            "Qualitative comparisons are shown in Fig.  10 ; on the left we show the ground truth relit image rendered in Blender, and we then show our results, as well as those from Outcast  [ GRP22 ] , Relightable 3D Gaussians  [ GGL  23 ]  and TensoIR  [ JLX  23 ] . Please refer to the supplementary HTML viewer for more results. We clearly see that our method is closer to the ground truth, visually confirming the quantitative results in Tab.  1 . TensoIR has difficulty reconstructing the geometry, and Relightable 3D Gaussians tend to have a splotchy look due to inaccurate normals. Outcast has difficulty with the overall lighting condition and can add incorrect shadows, but in many cases produces convincing results since it operates in image space. Our results show that by using the diffusion prior we manage to achieve realistic relighting, surpassing the state of the art.",
            "Our method was trained for indoor scenes; Fig.  12  gives additional ControlNet results on out-of-distribution samples, showing that it can generalize to some extent to unseen scenes and lighting conditions, although the realism is lower than for in-distribution samples.",
            "One limitation of the proposed method is that it does not enforce physical accuracy: the target light direction is noisy and the ControlNet relies mostly on its powerful Stable Diffusion prior to relight rather than performing physics-based reasoning. For example, Fig.  11  shows that ControlNet can hallucinate shadows due to unseen geometry, while there should not be any. Given that we define light direction in a manner that is not fully physically accurate, the positioning of highlight can be inaccurate, as is also shown in Fig.  11 . In addition, the appearance embeddings can correct for global inconsistencies indirectly and do not explicitly rely on the learned 3D representation of the radiance field. Our method does not always remove or move shadows in a fully accurate physically-based manner. While our method clearly demonstrates that 2D diffusion model priors can be used for realistic relighting, the ability to perform more complex relightingrather than just changing light directionrequires significant future research, e.g., by using more general training data as well as ways to encode and decode complex lighting."
        ]
    },
    "id_table_2": {
        "caption": "",
        "table": "S3.F8.12",
        "footnotes": [],
        "references": [
            "Our method is composed of three main parts. First, we create a 2D relighting neural network with direct control of lighting direction (Sec.  3.1 ). Second, we use this network to augment a multi-view capture with single lighting into a multi-lighting dataset, by using our relighting network. The resulting dataset can be used to create a radiance field representation of the 3D scene (Sec.  3.2 ). Finally, we create a relightable radiance field that accounts for inaccuracies in the synthesized relit input images and provides a multi-view consistent lighting solution (Sec.  3.3 ).",
            "To capture the scenes using similar light directions, Murmann et al. relied on a camera-mounted directional flash controlled by a servo motor. A pair of diffuse and metallic spheres are also visible in each scene; we leverage the former to obtain the effective lighting directions. Using as target the average of all diffuse spheres produced by the same flash direction, we find the lighting direction  l  R 3 l superscript R 3 l\\in\\mathbb{R}^{3} italic_l  blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT  which best reproduces this target when rendering a gray ball with a simplistic Phong shading model. More specifically, we minimize the  L 1 subscript L 1 L_{1} italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  error when jointly optimizing for an ambient light term and shading parameters (albedo, specular intensity and hardness, as well as a Fresnel coefficient). Fig.  2  illustrates this process.",
            "We condition the MLP with the spherical harmonics encoding of the globally consistent lighting direction  l  superscript l  \\boldsymbol{l}^{\\prime} bold_italic_l start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , which enables training a 3DGS representation on our multi-lighting dataset. While this strategy works well for static images, it results in inconsistent lighting across views despite accounting for camera rotation in Eq.  2 . Radiance fields like 3DGS rely on multi-view consistency, and breaking it introduces additional floaters and holes in surfaces.",
            "Our method was trained for indoor scenes; Fig.  12  gives additional ControlNet results on out-of-distribution samples, showing that it can generalize to some extent to unseen scenes and lighting conditions, although the realism is lower than for in-distribution samples."
        ]
    },
    "id_table_3": {
        "caption": "",
        "table": "S4.F10.24",
        "footnotes": [],
        "references": [
            "Our method is composed of three main parts. First, we create a 2D relighting neural network with direct control of lighting direction (Sec.  3.1 ). Second, we use this network to augment a multi-view capture with single lighting into a multi-lighting dataset, by using our relighting network. The resulting dataset can be used to create a radiance field representation of the 3D scene (Sec.  3.2 ). Finally, we create a relightable radiance field that accounts for inaccuracies in the synthesized relit input images and provides a multi-view consistent lighting solution (Sec.  3.3 ).",
            "Since ControlNet was not specifically designed for relighting, adapting it naively as described above leads to inaccurate colors and a loss in contrast (see Fig.  3 ), as well as distorted edges (see Fig.  4 ). These errors also degrade multi-view consistency.",
            "We adopt two strategies to improve coloration and contrast. First, we follow the recommendations of  [ LLLY23 ]  to improve image brightnesswe found them to also help for color. In particular, using the v-parameterized objective  y t =    t    1     t  x subscript y t  subscript    t bold-italic-  1 subscript    t x y_{t}=\\sqrt{\\bar{\\alpha}_{t}}\\cdot\\boldsymbol{\\epsilon}-\\sqrt{1-\\bar{\\alpha}_{% t}}\\cdot\\boldsymbol{x} italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = square-root start_ARG over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG  bold_italic_ - square-root start_ARG 1 - over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG  bold_italic_x , instead of the more usual  y t =  subscript y t italic- y_{t}=\\epsilon italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_ , proved critical; in this equation,  1     t 1 subscript    t 1-\\bar{\\alpha}_{t} 1 - over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  gives the variance of the noise at timestep  t t t italic_t . Second, after sampling, we color-match predictions to the input image to compensate for the difference between the color distribution of the training data and that of the scene. This is done by subtracting the per-channel mean and dividing by the standard deviation for the prediction, then adding the mean and standard deviation of the input, in the LAB colorspace. This is computed over all 18 lighting conditions together (i.e., the mean over all lighting directions) to conserve relative brightness across all conditions. Fig.  3  shows the effect of these changes; without them, the bottle is blue instead of green and overall contrast is poor.",
            "Given a multi-view set  I I \\mathcal{I} caligraphic_I  of images of a scene captured under the same lighting (suitable for training a radiance field model), we now leverage our light-conditioned ControlNet model to synthetically relight each image in  I I \\mathcal{I} caligraphic_I . We assume the 3D pose of each image  I i  I subscript I i I \\mathbf{I}_{i}\\in\\mathcal{I} bold_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  caligraphic_I  is known a priori, for example via Colmap  [ SF16 ,  SZPF16 ] . We then simply relight each  I i  I subscript I i I \\mathbf{I}_{i}\\in\\mathcal{I} bold_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  caligraphic_I  to the corresponding 18 known light directions in the dataset from Murmann et al.  \\shortcite multilum (excluding the directions where the flash points forward), (see Sec.  3.1 ). We now have a full multi-lighting, multi-view dataset. This process is illustrated in Fig.  6 .",
            "Since light directions are computed with respect to a local camera reference frame (c.f. Sec.  3.1 ), we subsequently register them to the world coordinate system (obtained from Colmap) by rotating them according to their (known) camera rotation parameters:",
            "We first train 3DGS with the unlit images as a warmup stage for 5K iterations, then train the full multi-illumination solution for another 25K iterations, using all 18 back-facing light directions (see Sec.  3.1 ). The multi-illumination nature of the training results in an increase in floaters. As observed by Philip and Deschaintre  [ PD23 ] , floaters are often present close to the input cameras; the explicit nature of 3DGS allows us to reduce these effectively. In particular, we calculate a  z n  e  a  r subscript z n e a r z_{near} italic_z start_POSTSUBSCRIPT italic_n italic_e italic_a italic_r end_POSTSUBSCRIPT  value for all cameras by taking the  z z z italic_z  value of the 1st percentile of nearest SfM points and scaling this value down by 0.9. During training, at each step, all gaussian primitives that project within the view frustum of a camera but are located in front of its  z n  e  a  r subscript z n e a r z_{near} italic_z start_POSTSUBSCRIPT italic_n italic_e italic_a italic_r end_POSTSUBSCRIPT  plane are culled. Finally, given the complexity of modeling variable lighting, we observed that the optimization sometimes converges to blurry results. To counter this, we overweight three front-facing views (left, right, and center), by optimizing for one of these views every three iterations. This provides marginal improvement in results; all images shown are computed with this method, but it is optional.",
            "We first present the results of our 3D relightable radiance field, both for synthetic and real-world scenes. We then present a quantitative and qualitative evaluation of our method by comparing it to previous work and finally present an ablation of the auxiliary vector  a a a italic_a  from Sec.  3.3 ."
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "S4.T1.14",
        "footnotes": [
            "",
            "",
            ""
        ],
        "references": [
            "Since ControlNet was not specifically designed for relighting, adapting it naively as described above leads to inaccurate colors and a loss in contrast (see Fig.  3 ), as well as distorted edges (see Fig.  4 ). These errors also degrade multi-view consistency.",
            "To correct the distorted edges, we adapt the asymmetric autoencoder approach of Zhu et al.  [ ZFC  23 ] , which consists in conditioning the latent space decoder with the (masked) input image for the inpainting task. In our case, we ignore the masking and fine-tune the decoder on the multi-illumination dataset  [ MGAD19 ] . At each fine-tuning step, we encode an image and condition the decoder on an image from the same scene with another random lighting direction. The decoder is fine-tuned with the Adam optimizer at a learning rate of  10  4 superscript 10 4 10^{-4} 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT  for 20k steps when training at resolution  768  512 768 512 768\\times 512 768  512  and 50k steps at resolution  1536  1024 1536 1024 1536\\times 1024 1536  1024 . Note that this step is independent of the ControlNet training. Fig.  4  shows the effect of these changes; note how the edges are wobbly and the text is illegible without them."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "S5.F12.20",
        "footnotes": [],
        "references": [
            "Example relighting results obtained using our 2D relighting network on images outside of the dataset are shown in Fig.  5 . Observe how the relit images produced by our method are highly realistic and light directions are consistently reproduced across scenes. A naive solution for radiance field relighting would be to apply this 2D network to each synthesized novel view. However, the ControlNet is not multi-view consistent, and such a naive solution results in significant flickering. Please see the accompanying video for a clear illustration."
        ]
    }
}