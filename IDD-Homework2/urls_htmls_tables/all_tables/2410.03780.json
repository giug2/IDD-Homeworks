{
    "id_table_1": {
        "caption": "Table 1:  Performance of our encoder and comparison with existing state-of-the-art models at the same size. NDCG@10 is used as the metric to benchmark retrieval encoders. These models are benchmark on three datasets from MTEB Benchmark  (Muennighoff et al.,  2023 ) .",
        "table": "S4.T1.1",
        "footnotes": [],
        "references": [
            "In this section, we present our Reward-RAG. We first describe the dense retrieval problem in RAG in section  3.1 , then present how we apply reinforcement learning to this problem in section  3.2 . Fig. 2  illustrates the high-level design for Reward-RAG.",
            "where  s  i  m  ( q , d ) s i m q d sim(q,d) italic_s italic_i italic_m ( italic_q , italic_d )  is the similarity value between a query  d d d italic_d  and a document  d d d italic_d  defined in equation ( 1 ). For efficient training, the negative set  D  superscript D D^{-} italic_D start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT  includes both hard-negative and in-batch negatives, which are derived from positive documents and hard negative documents associated with other queries. This training pipeline tends to benefit from a bigger set of negative samples. During the inference phase, we keep the same pipeline as in a typical RAG system. In more details, we first embed the external database using the fine-tuned retrieval model, then perform retrieving with a fast  k k k italic_k -nearest neighbors library such as FAISS  (Johnson et al.,  2021 ) .",
            "We first measure our retrieval encoders in the information retrieval task. We use three datasets in the general domain from the MTEB benchmark  (Muennighoff et al.,  2023 )  to test our model. We report the NDCG@10 score of our models and compare them with baselines and state-of-the-art models. Table  1  represents the performance of our model and another baseline. As our model has less than 400M parameters, we only select state-of-the-art models that have similar number of parameters from the MTEB leaderboard. Compared to the base model, our models increase performance on both three datasets. On the NQ dataset, our model is the best model."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Results of Reward-RAG and baselines in general domains on differernt datasets. We use the split from KILT benchmark for our results. Results unavailable in public reports are marked as",
        "table": "S4.T2.1",
        "footnotes": [],
        "references": [
            "In this section, we present our Reward-RAG. We first describe the dense retrieval problem in RAG in section  3.1 , then present how we apply reinforcement learning to this problem in section  3.2 . Fig. 2  illustrates the high-level design for Reward-RAG.",
            "Inspired by RLHF, we design a mechanism to fine-tune the existing retrieval models to better align user preferences in the retrieved documents. We follow the RL-based design in RLHF, where we first build a reward model to evaluate the relevance between a query and a document, secondly we fine-tune retrieval models using the reward model (see Figure  2 ).",
            "Results for the downstream question-answering tasks are shown in Table  2 . On the NQ and FEVER datasets, our model archives the best performance, while on the TriviaQA dataset, our method is the second-best model. It is noteworthy that in other models including RA-DIT, RankRAG, and Self-RAG, their methods fine-tune LLMs to adapt to downstream tasks, which is expensive and limits the generalization of LLMs. In our method, we do not modify the LLMs; instead, we aim to guide them by providing valuable information in a cost-effective way. In Table  5 , we show a sample query from the NQ dataset with retrieved documents and the answer from different models. We observe that when the correct answer appears multiple times in the provided contexts, the presence of distractors does not affect the LLMs responses."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Results of Reward-RAG and baselines in the medical field on Mirage benchmark. For baseline using retrieval-augmented generation, MedCorp is used as corpus and RRF-4 is the retrieval method, most numbers are from public reports  (Xiong et al.,  2024 ; Yu et al.,  2024 )",
        "table": "S4.T4.1",
        "footnotes": [],
        "references": [
            "In this section, we present our Reward-RAG. We first describe the dense retrieval problem in RAG in section  3.1 , then present how we apply reinforcement learning to this problem in section  3.2 . Fig. 2  illustrates the high-level design for Reward-RAG.",
            "Results.  Table  3  shows the performance of our models and other baselines. We report the accuracy as the format of the downstream task is multiple-choice questions. Our method outperforms other baselines on the PubmedQA dataset, while it is the second-best model on the BioASQ dataset. Table  6  shows case studies we pick from different datasets. Since questions in the medical domain require logical thinking and reasoning, we emphasize the importance of providing correct relevant documents.",
            "In order to compare the feedback collected from different LLMs, we calculate the confusion matrix between them on a subset of our dataset. We use the same prompt to collect feedback from GPT-3.5 and GPT-4o. Figure  3  shows the confusion matrix of the two models feedback. In total, the percentage of agreement is  61.3 % percent 61.3 61.3\\% 61.3 % , presenting a huge gap between these two models. We sampled 50 queries along with their corresponding documents to evaluate the quality of feedback from these two models. The qualitative results indicate that the feedback from GPT-4o is better and more consistent than that from GPT-3.5. Therefore, we use GPT-4o to label data for our experiments."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  A case study on the positive document picked by the reward model compare to human annotations",
        "table": "S4.T4.1.2.1.2.1",
        "footnotes": [],
        "references": [
            "In Table  4  we present a case study from the NQ dataset where the human annotation for positive documents is incorrect, while the annotation by our reward model is accurate. For this query, the correct answer (The White Rabbit) is mentioned in both passages but in the document labeled by human, it is not related to the query, on the other hand, the passage labeled by reward model answers the query with a clear evidence. More samples are provided in the appendix."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  A case study on the top-retrieved context and predictions on NQ dataset. We use the example query from RankRAG (Yu et al.,  2024 )  to compare their models with ours.  Red  text denotes distractors, while  green  stands for evidences.",
        "table": "S4.T4.1.3.2.2.1",
        "footnotes": [],
        "references": [
            "Results for the downstream question-answering tasks are shown in Table  2 . On the NQ and FEVER datasets, our model archives the best performance, while on the TriviaQA dataset, our method is the second-best model. It is noteworthy that in other models including RA-DIT, RankRAG, and Self-RAG, their methods fine-tune LLMs to adapt to downstream tasks, which is expensive and limits the generalization of LLMs. In our method, we do not modify the LLMs; instead, we aim to guide them by providing valuable information in a cost-effective way. In Table  5 , we show a sample query from the NQ dataset with retrieved documents and the answer from different models. We observe that when the correct answer appears multiple times in the provided contexts, the presence of distractors does not affect the LLMs responses."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  A case study on the medical domain. We select sample questions from different datasets, demonstrate the retrieved documents with LLMs answer of our method.  Green  stands for evidences.",
        "table": "S4.T5.5.1",
        "footnotes": [],
        "references": [
            "Results.  Table  3  shows the performance of our models and other baselines. We report the accuracy as the format of the downstream task is multiple-choice questions. Our method outperforms other baselines on the PubmedQA dataset, while it is the second-best model on the BioASQ dataset. Table  6  shows case studies we pick from different datasets. Since questions in the medical domain require logical thinking and reasoning, we emphasize the importance of providing correct relevant documents."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Hyperparameters for retrieval encoder",
        "table": "S4.T6.3.1",
        "footnotes": [],
        "references": [
            "We use 8xRTX A6000 Ada gen 2 for our works. Our implementation is based on the Hugging Face library 2 2 2 https://huggingface.co  includes transformers, accelerate, and PEFT libraries. In Table  7  we show our configuration used in retrieval encoder fine-tuning. Table  8  show the settings to train our reward models."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Hyperparameters for critic models",
        "table": "A1.T7.1",
        "footnotes": [],
        "references": [
            "We use 8xRTX A6000 Ada gen 2 for our works. Our implementation is based on the Hugging Face library 2 2 2 https://huggingface.co  includes transformers, accelerate, and PEFT libraries. In Table  7  we show our configuration used in retrieval encoder fine-tuning. Table  8  show the settings to train our reward models."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "A1.T8.1",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": [
        "https://huggingface.co/MedRAG",
        "https://huggingface.co"
    ]
}