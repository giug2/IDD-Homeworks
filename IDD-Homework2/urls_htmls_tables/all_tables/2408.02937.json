{
    "id_table_1": {
        "caption": "Table 1.  Latency Change on Rearrangement Threshold",
        "table": "S4.T1.1",
        "footnotes": [],
        "references": [
            "On the one hand, the memory arrangement of vector insertion is not well designed in existing systems (Faiss/Raft), which almost needs to be merged with existing vectors. This process is illustrated in Figure  1 a and detailed in Algorithm  1 . For typical inverted-index-based ANNS algorithms like Ivfflat and Ivfpq, vectors are initially partitioned into multiple clusters (line 3). During the search process, the vector lists belonging to the most relevant clusters to the query are computed, and the final top-k vectors are retrieved. When new vectors arrive, existing systems typically needs to copy the vector lists of the new vectors from the offline segment to the new segment (line 5 - 14). Such method, therefore, incurs significant overhead on the GPU, as both memory allocation and copying require triggering a kernel launch which consumes substantial resources. Additionally, it should be noted that in this article, we only discuss inverted-based ANNS algorithms, as graph-based ones  (Wang et al . ,  2021 )  are currently challenging to support in real-time, even on CPUs.",
            "In this section, we will primarily introduce our proposed memory block based dynamic vector insertion algorithm. The overview memory layout is depicted in Figure  1 b and the main algorithm process is illustrated in Algorithm  2 . Instead of arranging all vectors in continuous memory as in existing systems, we link them as a list using the header of each memory block  m m m italic_m  for the vector id list  l l l italic_l  of a certain cluster  c c c italic_c  in ivfflat and ivfpq, which are represented by the same color in Figure  1 b. To mitigate memory allocation overhead  (Winter et al . ,  2021 )  during processing, we have designed a memory allocation method in Algorithm  2  (line 13). It comprises the central memory pool  P P P italic_P  (occupying almost the entire GPU memory) and the memory block  m m m italic_m  (the smallest unit). The central memory pool  P P P italic_P  has been pre-split by memory blocks, which are labeled from 0, 1 ... to  | P | P |P| | italic_P | . The  c  u  r P c u subscript r P cur_{P} italic_c italic_u italic_r start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT  represents the current position of latest allocated memory block. When allocating a memory block, we would atomicAdd the  c  u  r P c u subscript r P cur_{P} italic_c italic_u italic_r start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT  and locate the memory block logically without any real extra memory allocation.",
            "For each memory block  m m m italic_m , it may contain  | m  l | m l |ml| | italic_m italic_l |  vectors, which of limit  T m subscript T m T_{m} italic_T start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT  can be pre-set by users. The structure of a memory block is also illustrated in Figure  1 b. It consists of a header  m  h m h mh italic_m italic_h , core id list  m  l m l ml italic_m italic_l , and core vectors  m  v m v mv italic_m italic_v . The header  m  h m h mh italic_m italic_h  contains important information that indicates the address of the prev_header, next_header and block_info (vector capacity, vector size, etc.). The core id list  m  l m l ml italic_m italic_l  saves the vector ids belong to the memory block of this cluster. The core vectors  m  v m v mv italic_m italic_v  in each memory block are arranged interleaved by 32 dimensions, similar to Faiss/Raft, determined by the thread number in a GPU warp to enable collapsed read instructions.",
            "Secondly, we are particularly concerned about whether the rearrangement mechanism will block subsequent vector insertions and whether it will optimize the performance of online searches. We simulated a scenario where vectors are continuously inserted into the same list and observed the latency fluctuations before and after rearrangement under different rearrangement threshold settings, as well as the degree of performance optimization post-rearrangement. We fixed  QPS s  e  a  r  c  h subscript QPS s e a r c h \\text{QPS}_{search} QPS start_POSTSUBSCRIPT italic_s italic_e italic_a italic_r italic_c italic_h end_POSTSUBSCRIPT  = 5000 and  QPS i  n  s  e  r  t  i  o  n subscript QPS i n s e r t i o n \\text{QPS}_{insertion} QPS start_POSTSUBSCRIPT italic_i italic_n italic_s italic_e italic_r italic_t italic_i italic_o italic_n end_POSTSUBSCRIPT  = 2000 as the request parameters. The experimental results are shown in Table. 1 ."
        ]
    },
    "global_footnotes": []
}