{
    "id_table_1": {
        "caption": "Table 1:  Performance of zero-shot classification on five datasets for diseases present in the MIMIC-CXR dataset, evaluated on two MedVLP models pretrained on MIMIC-CXR (real) and SynCXR ( pure synthetic ). Mix denotes the direct combination of real and synthetic data for MedVLP pretraining. Best results are highlighted in bold.",
        "table": "S4.T1.10.10",
        "footnotes": [],
        "references": [
            "We identify several issues in the most commonly used real dataset for MedVLP, MIMIC-CXR  (Johnson et al.,  2019b ) , that degrade MedVLP performance, including low-quality images and unpaired image-text samples. Furthermore, we identify the long-tailed distribution problem in multimodal datasets, as shown in Fig  1 ,  2 .",
            "Instead, we adopt an alternative approach by using an off-the-shelf Named Entity Recognition (NER) tool to extract all medical entities, treating them as representatives of the reports concepts and exploring the dataset distribution at the entity level. For this, we use RaTE 4 4 4 https://huggingface.co/Angelakeke/RaTE-NER-Deberta   (Zhao et al.,  2024 ) , a model specifically designed for NER tasks on radiology reports. RaTE automatically classifies the extracted entities into five categories: [ ABNORMALITY ,  NON-ABNORMALITY ,  DISEASE ,  NON-DISEASE ,  ANATOMY ]. We display the top 50 frequent entiites distribution of each entity type in Fig  2  (c). We display the top 50 frequent entiites distribution of each entity type in Fig  7 , 8 , 11 , 9 , 10 . As shown, all entity types exhibit a severe long-tailed distribution. The MIMIC-CXR  (Johnson et al.,  2019b )  includes a total of 154,049 unique entities, with 55,047 Abnormality, 36,365 Non-Abnormality, 23,017 Disease, 22,103 Non-Disease, and 40,517 Anatomy entities.",
            "To prevent the synthetic images from exhibiting the same issues found in the real dataset (as discussed in Sec.  3.1 ), we apply a similar curation procedure. First, we use the MLLM to filter synthetic images, and then we compute the similarity of visual features between synthetic images and the problematic samples identified from the real dataset. If the visual similarity exceeds a threshold   = 0.5  0.5 \\delta=0.5 italic_ = 0.5 , we regenerate the images by re-querying the T2I model with the same text prompt until they pass the curation procedure.",
            "Zero-shot Classification on Seen Diseases.  Tab  1  shows the zero-shot classification performance on  seen  diseases. Across all datasets, both MedVLP methods pretrained on SynCXR (our  purely synthetic dataset ) consistently outperform or achieve comparable performance to their counterparts pretrained on real datasets, with an average improvement of 4.7% in AUC and 4.53% in F1 scores. Furthermore, the methods pretrained on the mixed dataset, which directly combines real and synthetic data, achieve even greater improvements, with 10.08% AUC and 7.62% F1 scores on average across all datasets and methods. This demonstrates that the SynCXR dataset effectively enables MedVLP models to learn representative cross-modal features, enhancing their zero-shot classification capability.",
            "Entities Distribution.  We visualize the distribution of each type of entity in the MIMIC-CXR dataset. Due to space constraints, only the top 200 most frequent entities are shown, revealing a clear long-tailed distribution in Fig  7 ,  11 ,  9 ,  8 , and  10 ."
        ]
    },
    "id_table_2": {
        "caption": "(a)",
        "table": "S4.F3.sf1.6.6",
        "footnotes": [],
        "references": [
            "We identify several issues in the most commonly used real dataset for MedVLP, MIMIC-CXR  (Johnson et al.,  2019b ) , that degrade MedVLP performance, including low-quality images and unpaired image-text samples. Furthermore, we identify the long-tailed distribution problem in multimodal datasets, as shown in Fig  1 ,  2 .",
            "Low-Quality and Mismatched Image-Text Pairs.  Our aim is to explore and identify issues related to image quality in the MIMIC-CXR dataset  (Johnson et al.,  2019a ) , rather than to completely clean the dataset, as creating a perfect dataset and filtering out all low-quality samples is infeasible for large-scale multimodal datasets  (Xu et al.,  2023a ) . Inspired by  (Bannur et al.,  2023 ) , which highlights various issues with poor-quality images, we design six queries for a Multimodal Large Language Model (MLLM), utilizing the InternVL2-26B model 2 2 2 https://huggingface.co/OpenGVLab/InternVL2-26B   (Chen et al.,  2023 ;  2024b ) . Each CXR image from the MIMIC-CXR dataset is paired with these six queries, and the MLLM process each query independently. The process is depicted in Fig  2  (b).",
            "After this process, we filter out the CXR images where the answers are all  NO  across the six queries. Fig  2  (a) shows examples of images where the answer was  NO . We identified and removed 1,448 such images and their corresponding reports from the preprocessed MIMIC-CXR dataset, leaving us with 211,936 image-text pairs.",
            "To further refine the dataset, we use the CXR-specific vision encoder, RAD-DINO  (Perez-Garcia et al.,  2024 ) , to extract image features from the remaining 211,936 CXR images and from the 1,448 samples identified as bad by MLLM filtering. We then compute the similarity between each image in the cleaned dataset and each of the bad samples. Since each image comes from a different clinical case, we only compare image quality rather than the clinical content (e.g., diagnoses or abnormalities). To do this, we set a similarity threshold of 0.5 and remove all images with a similarity score greater than 0.5. This step resulted in the removal of an additional 5,512 images and their paired reports, reducing the dataset to 206,424 image-text pairs. Fig  2  (a) also shows the samples removed based on their similarity to bad images using visual features from RAD-DINO 3 3 3 https://huggingface.co/microsoft/rad-dino   (Perez-Garcia et al.,  2024 ) .",
            "Instead, we adopt an alternative approach by using an off-the-shelf Named Entity Recognition (NER) tool to extract all medical entities, treating them as representatives of the reports concepts and exploring the dataset distribution at the entity level. For this, we use RaTE 4 4 4 https://huggingface.co/Angelakeke/RaTE-NER-Deberta   (Zhao et al.,  2024 ) , a model specifically designed for NER tasks on radiology reports. RaTE automatically classifies the extracted entities into five categories: [ ABNORMALITY ,  NON-ABNORMALITY ,  DISEASE ,  NON-DISEASE ,  ANATOMY ]. We display the top 50 frequent entiites distribution of each entity type in Fig  2  (c). We display the top 50 frequent entiites distribution of each entity type in Fig  7 , 8 , 11 , 9 , 10 . As shown, all entity types exhibit a severe long-tailed distribution. The MIMIC-CXR  (Johnson et al.,  2019b )  includes a total of 154,049 unique entities, with 55,047 Abnormality, 36,365 Non-Abnormality, 23,017 Disease, 22,103 Non-Disease, and 40,517 Anatomy entities."
        ]
    },
    "id_table_3": {
        "caption": "(b)",
        "table": "S4.F3.sf2.6.6",
        "footnotes": [],
        "references": [
            "To prevent the synthetic images from exhibiting the same issues found in the real dataset (as discussed in Sec.  3.1 ), we apply a similar curation procedure. First, we use the MLLM to filter synthetic images, and then we compute the similarity of visual features between synthetic images and the problematic samples identified from the real dataset. If the visual similarity exceeds a threshold   = 0.5  0.5 \\delta=0.5 italic_ = 0.5 , we regenerate the images by re-querying the T2I model with the same text prompt until they pass the curation procedure.",
            "Zero-shot Classification on Unseen Diseases.  Tab  3(a)  reports the zero-shot classification performance on  unseen  diseases. Similar to the results for seen diseases, MedVLP models pretrained on the synthetic dataset consistently outperform those pretrained on real data, with an average improvement of 2.96% AUC and 0.51% F1 scores. Additionally, models pretrained on the mixed dataset show substantial gains over those trained on real data, with 7.39% AUC and 1.52% F1 scores on average. This indicates that the SynCXR dataset, generated with meticulous quality control and balanced distribution, can increase the generalizability of MedVLP models for unseen diseases prediction.",
            "Zero-shot Visual Grounding.  We further evaluate the effectiveness of synthetic data in improving MedVLP models local visual understanding capabilities through zero-shot grounding tasks. Tab  3(b)  presents the performance of zero-shot grounding on RSNA  (Shih et al.,  2019 ) , Covid-19 Rural  (Desai et al.,  2020 ) , and SIIM  (Steven G. Langer & George Shih,  2019 ) . Across all datasets, MedVLP models pretrained on the SynCXR dataset achieve superior performance compared to those trained on the real dataset, with an average increase of 1.42% IoU and 0.97% Dice scores. The mixed dataset further enhances performance, with 4.06% IoU and 2.92% Dice scores on average. This demonstrates that the SynCXR dataset not only benefits global cross-modal feature learning but also improves local visual understanding for MedVLP models.",
            "Fine-tuning Tasks.  To evaluate the representation quality learned by MedVLP, we report the fine-tuned classification and segmentation performance in Tab  3 . Similar to the zero-shot task, MedVLP models pre-trained on SynCXR consistently outperform those trained on the real dataset across all data ratios for both classification and segmentation tasks. Furthermore, the combination of real and synthetic datasets (Mix) further boosts performance, demonstrating that SynCXR data not only enhances cross-modal representation learning but also improves performance in single-modal tasks."
        ]
    },
    "id_table_4": {
        "caption": "Table 2:  Zero-shot tasks performance of MedVLP models on disease classification (a) and grounding (b) across multiple datasets, using MIMIC-CXR, SynCXR, and Mix datasets for pretraining.",
        "table": "S4.T3.1.1",
        "footnotes": [],
        "references": [
            "CXR Report Generation.  To generate the synthetic reports, the pipeline is depicted in Fig  6 . We select a general LLM, Llama3.1-70B-Instruct 5 5 5 https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct  as the report generator, and we extensively ablate the performance of the report generator with other LLMs in Fig  4 . We query the LLM using prompts that include the entity list, as shown in Fig  6 .",
            "Effect of Balanced Entity Sampling in Generating Synthetic Reports.  We evaluate the impact of balanced sampling entities when generating synthetic reports using LLMs. For the synthetic dataset without balanced sampling, we adjust entity frequencies to match their distribution in MIMIC-CXR, leading to a long-tailed distribution. As shown in Tab  4(a) , for both MedVLP methods, the performance improves significantly when using synthetic datasets generated from balanced sampled entities. This demonstrates that balanced sampling of entities leads to a more representative dataset, benefiting MedVLP performance.",
            "Evaluating the Contribution of Synthetic Images and Reports.  We aim to assess the individual impact of synthetic images and synthetic reports on MedVLP performance. As shown in Tab  4(b) , we generate two partially synthetic datasets by replacing either the image or the text with synthetic data, while keeping the other components real, to evaluate their respective contributions.",
            "According to Tab  4(b) , for both MedVLP methods, using real images with synthetic reports results in decreased performance, likely due to the persistent long-tailed distribution, as the synthetic reports are generated based on real images. However, using real reports with synthetic images slightly improves performance, as synthetic images can be curated using our image filtering procedure to ensure high quality, avoiding issues commonly found in real datasets. Using both synthetic images and synthetic reports achieves the highest performance, indicating that a well-curated synthetic dataset can significantly enhance MedVLP performance.",
            "Impact of Entity Diversity.  We evaluate the impact of entity diversity by varying the number of entities used for generating the SynCXR dataset. We generate synthetic datasets using 25%, 50%, and 75% of these entities, following the same procedure each time. The results, shown in Fig  4  (Top), indicate that zero-shot classification performance improves as more entities are used for report generation. This suggests that increasing dataset diversity positively influences downstream performance.",
            "Impact of Different Report Generators.  We also examine the impact of using different LLMs for synthetic report generation. As shown in Fig  4  (Bottom Left), we compare two general LLMs, LLaMA 3.1 (8B and 70B), and two medical-specific LLMs, Meditron3 (8B 10 10 10 https://huggingface.co/OpenMeditron/Meditron3-8B  and 70B 11 11 11 https://huggingface.co/OpenMeditron/Meditron3-70B ). Despite Meditron3 being trained specifically on medical corpora and inheriting weights from LLaMA, the dataset generated by LLaMA 3.1-70B-Instruct achieves the best performance. This indicates that a powerful general LLM is effective for generating synthetic datasets, and using domain-specific fine-tuned versions may degrade the quality of the synthetic data.",
            "Impact of Different Image Generators.  We evaluate various text-to-image models for synthetic CXR image generation, including CXR-IRGen  (Shentu & Al Moubayed,  2024 ) , LLM-CXR  (Lee et al.,  2023 ) , and RoentGen  (Bluethgen et al.,  2024 ) . As shown in Fig  4  (Bottom Right), datasets generated by RoentGen lead to the best performance for both MedVLP methods. This is likely because RoentGen is the only image generation model verified by clinicians, suggesting that the quality of image generation models is crucial for building synthetic datasets, and models should be validated by clinical experts."
        ]
    },
    "id_table_5": {
        "caption": "Table 3:  Results from two MedVLP methods pre-trained on real, synthetic, and mixed datasets are reported for classification (AUC) and segmentation (Dice) tasks. ConVIRT-Real and GLoRIA-Real refer to models pre-trained on MIMIC-CXR using real data, while ConVIRT-Syn and GLoRIA-Syn indicate models pre-trained on SynCXR using synthetic data. ConVIRT-Mix and GLoRIA-Mix represent models trained on a combination of MIMIC-CXR and SynCXR. Best results are in bold.",
        "table": "S5.F4.sf1.1.1",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "Distribution of Synthetic and Real Data.  We illustrate the distribution of synthetic and real data in Fig  5 . For visualization, we use RAD-DINO  (Perez-Garcia et al.,  2024 )  to extract image features and Med-CPT  (Jin et al.,  2023 )  to extract report features. We then apply Principal component analysis (PCA) to reduce the feature dimensions and visualize the first principal component. As shown in Fig  5 , the synthetic data covers a broader range than the real data, indicating greater diversity. In contrast, the real data shows a more concentrated distribution, which may limit the generalizability of MedVLP models."
        ]
    },
    "id_table_6": {
        "caption": "(a)",
        "table": "S5.F4.sf2.1.1",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "CXR Report Generation.  To generate the synthetic reports, the pipeline is depicted in Fig  6 . We select a general LLM, Llama3.1-70B-Instruct 5 5 5 https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct  as the report generator, and we extensively ablate the performance of the report generator with other LLMs in Fig  4 . We query the LLM using prompts that include the entity list, as shown in Fig  6 .",
            "Pipeline of Synthetic Report Generation.  The pipeline for generating synthetic reports using LLMs and balanced sampled clinical entities is illustrated in Fig  6 ."
        ]
    }
}