{
    "id_table_1": {
        "caption": "Table 1:  Comparisons among different graph analysis benchmarks for LLMs.",
        "table": "S1.T1.7.7",
        "footnotes": [],
        "references": [
            "To evaluate the ability of LLMs in graph analysis, we introduce the ProGraph benchmark, featuring 512 problems in three categories. These hand-crafted problems include questions and answers with two difficulty levels. To enhance diversity and realism, we leverage GPT-4 turbo to rephrase the questions in a role-playing manner, followed by manual verification for correctness. Finally, given the high consistency of answer judgments between humans and GPT-4o, we employ GPT-4o to automate the evaluation. We compare the proposed benchmark with previous ones in Table  1 , present more discussions about related work in Appendix  2 , and show a benchmark example in Appendix  E ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Statistics of ProGraph.",
        "table": "S1.T1.7.7.8.1.2.1",
        "footnotes": [],
        "references": [
            "To evaluate the ability of LLMs in graph analysis, we introduce the ProGraph benchmark, featuring 512 problems in three categories. These hand-crafted problems include questions and answers with two difficulty levels. To enhance diversity and realism, we leverage GPT-4 turbo to rephrase the questions in a role-playing manner, followed by manual verification for correctness. Finally, given the high consistency of answer judgments between humans and GPT-4o, we employ GPT-4o to automate the evaluation. We compare the proposed benchmark with previous ones in Table  1 , present more discussions about related work in Appendix  2 , and show a benchmark example in Appendix  E .",
            "Rationale.  To validate GPT-based evaluation, we measure its stability (self-consistency) and alignment with human judgments (human-consistency). Higher stability means judgment scores are consistent across multiple evaluations, while higher human alignment indicates better quality. We use the agreement metric  [ 62 ]  to assess these consistencies.  For  n n n italic_n  evaluations of the same answer, we take the highest number of evaluations  m m m italic_m  that received the same score and divide it by  n n n italic_n  to get the consistency.  Self-consistency is the agreement among three GPT-4o evaluations, and human-consistency is the agreement between one GPT-4o evaluation and a manual evaluation. We evaluate all 512 problems with answers from  Claude 3 Opus RAG 7 , the best-performing closed-source model which will be introduced in Section  4.2 , and present the results in Table  3 , showing high self-consistency and human-consistency."
        ]
    },
    "id_table_3": {
        "caption": "Table 4:  Statistics of LLM4Graph datasets.",
        "table": "S1.T1.7.7.8.1.3.1",
        "footnotes": [],
        "references": [
            "Rationale.  To validate GPT-based evaluation, we measure its stability (self-consistency) and alignment with human judgments (human-consistency). Higher stability means judgment scores are consistent across multiple evaluations, while higher human alignment indicates better quality. We use the agreement metric  [ 62 ]  to assess these consistencies.  For  n n n italic_n  evaluations of the same answer, we take the highest number of evaluations  m m m italic_m  that received the same score and divide it by  n n n italic_n  to get the consistency.  Self-consistency is the agreement among three GPT-4o evaluations, and human-consistency is the agreement between one GPT-4o evaluation and a manual evaluation. We evaluate all 512 problems with answers from  Claude 3 Opus RAG 7 , the best-performing closed-source model which will be introduced in Section  4.2 , and present the results in Table  3 , showing high self-consistency and human-consistency.",
            "Specifically, we first randomly select an API from the six Python libraries, and then employ GPT-4 turbo to generate example code for the API along with a corresponding question via back instruction  [ 53 ] . In the Code (QA) dataset, each answer only contains the Python code in the generated json. In the Doc+Code (QA) dataset, we design a template to additionally incorporate the API document information into each answer. This allows a CoT process that first selects the possibly needed APIs, then provides the corresponding API information, and finally writes code to solve the problem. Besides, the questions in both code datasets need to undergo the role-play processing in Section  3.3  for diverse problem descriptions. The prompt for back instruction and an example of the Doc+Code dataset can be found at Appendix  D  and  F :",
            "Figure  3  shows the performance gains for four top closed-source LLMs: GPT-4 turbo, GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro. All models show significant boosts in pass rate and accuracy, with more than a 5% improvement in accuracy. Gemini 1.5 Pro and Claude 3 Opus achieve over 10% accuracy improvement with five retrieved APIs.  However, performance improvements plateau with additional API information, which may be attributed to the saturation of relevant information, where additional API details no longer contribute to further understanding and instead introduce redundancy or noise. This observation aligns with the findings of previous studies on RAG  [ 28 ] .",
            "To gain a deeper understanding of the capabilities of LLMs and fine-tuned smaller models presented in Section  5.3 , we analyze their performances on ProGraph from three different perspectives: task category, question type and answer difficulty."
        ]
    },
    "id_table_4": {
        "caption": "Table 5:  Performance (%) of different models on ProGraph.",
        "table": "S1.T1.7.7.8.1.4.1",
        "footnotes": [],
        "references": [
            "Rationale.  To validate GPT-based evaluation, we measure its stability (self-consistency) and alignment with human judgments (human-consistency). Higher stability means judgment scores are consistent across multiple evaluations, while higher human alignment indicates better quality. We use the agreement metric  [ 62 ]  to assess these consistencies.  For  n n n italic_n  evaluations of the same answer, we take the highest number of evaluations  m m m italic_m  that received the same score and divide it by  n n n italic_n  to get the consistency.  Self-consistency is the agreement among three GPT-4o evaluations, and human-consistency is the agreement between one GPT-4o evaluation and a manual evaluation. We evaluate all 512 problems with answers from  Claude 3 Opus RAG 7 , the best-performing closed-source model which will be introduced in Section  4.2 , and present the results in Table  3 , showing high self-consistency and human-consistency.",
            "We investigate the potential of enhancing open-source LLMs performance on graph analysis tasks by fine-tuning them using LLM4Graph and augmenting the models with RAG. Experiments are conducted on a general-purpose model, Llama-3-8B-Instruct  [ 1 ] , and a model specifically designed for code generation, Deepseek-Coder-7B-Instruct-v1.5  [ 18 ] . The results, presented in Figure  4 , demonstrate that our LLM4Graph can significantly improve the performance of different types of open-source small models. The accuracy of both models, after being fine-tuned merely on the code data within LLM4Graph, substantially surpasses the best result from closed-source models without RAG. The Doc+Code setting further enhances the models performance, leading to comparable or even better accuracy than the best result from closed-source model with RAG."
        ]
    },
    "id_table_5": {
        "caption": "Table 6:  Benchmark Example",
        "table": "S1.T1.7.7.8.1.5.1",
        "footnotes": [],
        "references": [
            "The results, presented in Table  5 , indicate that none of the tested LLMs could effectively solve the problems in ProGraph. While GPT-4 turbo and its variant GPT-4o demonstrate relatively higher performance with an overall accuracy of 31.2% and 36.3% respectively, they still fall short in delivering satisfying accuracy across different categories. For instance, GPT-4 turbo achieves an accuracy of 42.1% in Category 1 but only 14.8% and 12.0% in Categories 2 and 3, respectively. Similar trends are observed in other models. These results highlight the challenges faced by current LLMs in addressing structured graph data as human experts. This necessitates the development of specialized datasets and fine-tuning approaches to bridge this performance gap.",
            "To gain a deeper understanding of the capabilities of LLMs and fine-tuned smaller models presented in Section  5.3 , we analyze their performances on ProGraph from three different perspectives: task category, question type and answer difficulty.",
            "Task Category.   We analyze the model performance based on different categories in the ProGraph, as shown in Figure  5(a) . Mainstream LLMs and fine-tuned smaller models exhibit similar performance on graph theory and graph statistical learning tasks. However, a significant disparity is observed in their performance on graph embedding tasks, where fine-tuned smaller models substantially outperform RAG-enhanced large models. This observation suggests that not all graph analysis tasks can be easily handled by closed-source LLMs without further fine-tuning. More complex and challenging tasks still require fine-tuning for effective learning.",
            "Question Type.   We further examine the model performance based on different question types,  i.e.,  true/false, calculation, drawing, and hybrid. In Figure  5(b) , we plot the performance of different models on these question types separately. Mainstream LLMs excel in true/false and drawing types but struggle with calculation and hybrid ones. Fine-tuned smaller models demonstrate improvements across various question types, especially on the complex hybrid type, indicating the effectiveness of our proposed enhancement strategies.",
            "Answer Difficulty.   Lastly, we divide the ProGraph into two levels of difficulty: easy (involving only one API) and hard (involving multiple APIs). As shown in Figure  5(c) , mainstream closed-source large models perform well on easy-level problems, with accuracies generally approaching or exceeding 50%. However, their performance significantly deteriorates on hard-level ones, with the highest accuracy reaching only 32.5%. This observation suggests that mainstream LLMs have limitations when the number of required APIs increases. In contrast, our fine-tuned models demonstrate significantly higher accuracy on hard-level problems, approaching or exceeding 40%, yielding approximately an improvement of 8% compared to the best closed-source LLM. Note that LLM4Graph only contains data involving one API. Still, the models fine-tuned on LLM4Graph show strong generalizability on problems requiring multiple APIs."
        ]
    },
    "id_table_6": {
        "caption": "Table 7:  Performance (%) of closed-source models regarding different task categories.",
        "table": "S1.T1.7.7.8.1.6.1",
        "footnotes": [],
        "references": [
            "To gain insights into the types of compilation errors made by different models, we conduct an error analysis on the best-performing closed-source models (GPT-4 turbo RAG 5 and Claude 3 Opus RAG 7) and fine-tuned open-source small models (DeepSeek Coder Doc+Code and Llama 3 Doc+Code). As shown in Figure  6 , we categorize the errors into ten distinct types to identify patterns and differences in the error distributions among these models."
        ]
    },
    "id_table_7": {
        "caption": "Table 8:  Performance (%) of open-source models regarding different task categories.",
        "table": "S1.T1.7.7.8.1.7.1",
        "footnotes": [],
        "references": []
    },
    "id_table_8": {
        "caption": "Table 9:  Performance (%) of closed-source models regarding different question types.",
        "table": "S1.T1.7.7.8.1.8.1",
        "footnotes": [],
        "references": []
    },
    "id_table_9": {
        "caption": "Table 10:  Performance (%) of open-source models regarding different question types.",
        "table": "S3.T2.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_10": {
        "caption": "Table 11:  Performance (%) of closed-source models regarding different answer difficulties.",
        "table": "S3.T3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "Table 12:  Performance (%) of open-source models regarding different answer difficulties.",
        "table": "S4.T4.1",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "",
        "table": "S5.T5.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "",
        "table": "A5.T6.1",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "A7.T7.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "",
        "table": "A7.T8.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_16": {
        "caption": "",
        "table": "A7.T9.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_17": {
        "caption": "",
        "table": "A7.T10.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_18": {
        "caption": "",
        "table": "A7.T11.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_19": {
        "caption": "",
        "table": "A7.T12.1.1",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": []
}