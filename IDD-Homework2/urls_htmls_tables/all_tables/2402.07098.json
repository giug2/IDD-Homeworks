{
    "S4.T1": {
        "caption": "Table 1: mAP50 results of all YOLOv8 models on the individual, stacked, and racked pallet categories.",
        "table": "<table id=\"S4.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">Model</th>\n<th id=\"S4.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Individual</th>\n<th id=\"S4.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Stacked</th>\n<th id=\"S4.T1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Racked</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">YOLOv8 N</td>\n<td id=\"S4.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.715</td>\n<td id=\"S4.T1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.74</td>\n<td id=\"S4.T1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.71</td>\n</tr>\n<tr id=\"S4.T1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">YOLOv8 S</td>\n<td id=\"S4.T1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.805</td>\n<td id=\"S4.T1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.63</td>\n<td id=\"S4.T1.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.705</td>\n</tr>\n<tr id=\"S4.T1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">YOLOv8 M</td>\n<td id=\"S4.T1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.665</td>\n<td id=\"S4.T1.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.705</td>\n<td id=\"S4.T1.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.805</td>\n</tr>\n<tr id=\"S4.T1.1.5.4\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">YOLOv8 L</td>\n<td id=\"S4.T1.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.725</td>\n<td id=\"S4.T1.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.77</td>\n<td id=\"S4.T1.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.73</td>\n</tr>\n<tr id=\"S4.T1.1.6.5\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">YOLOv8 X</td>\n<td id=\"S4.T1.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.725</td>\n<td id=\"S4.T1.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.615</td>\n<td id=\"S4.T1.1.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.78</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "In Table\u00a01 the performance of various YOLOv8 models is shown. This is of interest as it shows the impact model size can have on the overall performance. There is an expected amount of fluctuation, however, there is no major performance increase that can be directly associated with an increase in the model size.",
            "There is also no major performance increase with larger models as shown in Table\u00a01. The main reason for this is that larger models tend to generalise worse. Generalisation is very important to this topic as the goal is to close the gap between simulated and real-world data."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Comparing the performance of two slightly different YOLOv8 models with SAM performing segmentation on pallet bodies and faces.",
        "table": "<table id=\"S4.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">Model</th>\n<th id=\"S4.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Bodies (mAP50)</th>\n<th id=\"S4.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Faces (mAP50)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.1.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">A</td>\n<td id=\"S4.T2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.2</td>\n<td id=\"S4.T2.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.02</td>\n</tr>\n<tr id=\"S4.T2.1.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">B</td>\n<td id=\"S4.T2.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.12</td>\n<td id=\"S4.T2.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.08</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "When evaluating the performance of the two-stage YOLOv8 + SAM detector, different pre-trained YOLOv8 models were used for object detection in the YOLO component. They were both YOLOv8 nano models that had been given the same training and validation datasets but were trained using different hyperparameters. These are represented by model A and model B and they had very similar performances for detection. This was to evaluate the stability of the detector. The results of this approach can be seen in Table\u00a02.",
            "For the YOLO + SAM detector, the mAP50 was much lower for the pallet faces than the pallet bodies as seen in Table\u00a02. The reason for this is that the bounding boxes for faces often cause SAM to segment parts of the background, not the pallet face. An example of this can be seen in Figure\u00a011 where the SAM mask is shown in the light-green shading. The cause of this is the design of SAM. SAM was intended to segment entire objects - not part of an object. This means naturally, SAM would perform poorly when segmenting parts of an object, such as a pallet face.\nAdditionally, due to how new SAM is - it currently does not supply tools to perform training, so it is not possible to improve SAM with respect to the research goals."
        ]
    }
}