{
    "S5.T1": {
        "caption": "Table 1: Our VTQA model significantly outperforms (p<𝑝absentp< 0.001) the strong baseline VQA model (we do not apply MFB to our VTQA model, since it does not work for the VTQA model).",
        "table": "<table id=\"S5.T1.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T1.3.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.3.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S5.T1.3.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">Model</th>\n<th id=\"S5.T1.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Test accuracy (%)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.3.2.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.3.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T1.3.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">VQA baseline</td>\n<td id=\"S5.T1.3.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">44.68</td>\n</tr>\n<tr id=\"S5.T1.3.3.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.3.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">2</td>\n<td id=\"S5.T1.3.3.2.2\" class=\"ltx_td ltx_align_left ltx_border_r\">VQA + MFB baseline</td>\n<td id=\"S5.T1.3.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">44.94</td>\n</tr>\n<tr id=\"S5.T1.3.4.3\" class=\"ltx_tr\">\n<td id=\"S5.T1.3.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">3</td>\n<td id=\"S5.T1.3.4.3.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">VTQA (EF+LF+AR)</td>\n<td id=\"S5.T1.3.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">46.86</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "As shown in Table 1, our VTQA model increases the accuracy by 1.92% from the baseline VQA model for which we employ Anderson et al. (2018)’s model and apply multi-modal factorized bilinear pooling (MFB) Yu et al. (2017). This implies that our textual data helps improve VQA model performance by providing clues to answer questions. We run each model five times with different seeds and take the average value of them. For each of the five runs, our VTQA model performs significantly better (p<0.001𝑝0.001p<0.001) than the VQA baseline model."
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Our early (EF), late (LF), and later fusion (or Answer Recommendation AR) modules each improves the performance of our VTQA model.",
        "table": "<table id=\"S5.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S5.T2.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">Model</th>\n<th id=\"S5.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Val accuracy (%)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.1.2.1\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T2.1.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">VTQA + EF (base model)</td>\n<td id=\"S5.T2.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">45.41</td>\n</tr>\n<tr id=\"S5.T2.1.3.2\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">2</td>\n<td id=\"S5.T2.1.3.2.2\" class=\"ltx_td ltx_align_left ltx_border_r\">VTQA + EF + LF</td>\n<td id=\"S5.T2.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">46.36</td>\n</tr>\n<tr id=\"S5.T2.1.4.3\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">3</td>\n<td id=\"S5.T2.1.4.3.2\" class=\"ltx_td ltx_align_left ltx_border_r\">VTQA + EF + AR</td>\n<td id=\"S5.T2.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">46.95</td>\n</tr>\n<tr id=\"S5.T2.1.5.4\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r\">4</td>\n<td id=\"S5.T2.1.5.4.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\">VTQA + EF + LF + AR</td>\n<td id=\"S5.T2.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">47.60</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "As shown in row 2 of Table 2, late fusion improves the model by 0.95%, indicating that visual and textual features complement each other. As shown in row 3 and 4 of Table 2, giving an extra score to the expected answers increases the accuracy by 1.54% from the base model (row 1) and by 1.24% from the result of late fusion (row 2), respectively. This could imply that salient parts (in our case, objects) can give direct cues for answering questions.111Object Properties: Appending the encoded object properties to visual features improves the accuracy by 0.15% (47.26 vs. 47.41). This implies that incorporating extra textual information into visual features could help a model better understand the visual features for performing the VQA task."
        ]
    },
    "S5.T3": {
        "caption": "Table 3:  TextQA with GT model outperforms TextQA with GenP (we run each model five times with different seeds and average the scores. GT: Ground-Truth, GenP: Generated Paragraph).",
        "table": "<table id=\"S5.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S5.T3.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">Model</th>\n<th id=\"S5.T3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Val accuracy (%)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T3.1.2.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T3.1.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">TextQA with GT</td>\n<td id=\"S5.T3.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">43.96</td>\n</tr>\n<tr id=\"S5.T3.1.3.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r\">2</td>\n<td id=\"S5.T3.1.3.2.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\">TextQA with GenP</td>\n<td id=\"S5.T3.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">42.07</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "We manually investigate (300 examples) how many questions can be answered only from the ground-truth (GT) versus generated paragraph (GenP) captions. We also train a TextQA model (which uses cross-attention mechanism between question and caption) to evaluate the performance of the GT and GenP captions. As shown in Table 3, the GT captions can answer more questions correctly than GenP captions in TextQA model evaluation. Human evaluation with GT captions also shows better performance than with GenP captions as seen in Table 4. However, the results from the manual investigation have around 12% gap between GT and generated captions, while the gap between the results from the TextQA model is relatively small (1.89%). This shows that paragraph captions can answer several VQA questions but our current model is not able to extract the extra information from the GT captions. This allows future work: (1) the TextQA/VTQA models should be improved to extract more information from the GT captions; (2) paragraph captioning models should also be improved to generate captions closer to the GT captions.222We also ran our full VTQA model with the ground truth (GT) paragraph captions and got an accuracy value of 48.04% on the validation dataset (we ran the model five times with different seeds and average the scores), whereas the VTQA result from generated paragraph captions was 47.43%. This again implies that our current VTQA model is not able to extract all the information enough from GT paragraph captions for answering questions, and hence improving the model to better capture clues from GT captions is useful future work."
        ]
    },
    "S5.T4": {
        "caption": "Table 4:  Human evaluation only with paragraph captions and questions of the validation dataset. Human evaluation with GT shows better performance than human evaluation with GenP.",
        "table": "<table id=\"S5.T4.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S5.T4.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\">Human Eval.</th>\n<th id=\"S5.T4.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Accuracy (%)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T4.1.2.1\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T4.1.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">with GT</td>\n<td id=\"S5.T4.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">55.00</td>\n</tr>\n<tr id=\"S5.T4.1.3.2\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r\">2</td>\n<td id=\"S5.T4.1.3.2.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\">with GenP</td>\n<td id=\"S5.T4.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">42.67</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "We manually investigate (300 examples) how many questions can be answered only from the ground-truth (GT) versus generated paragraph (GenP) captions. We also train a TextQA model (which uses cross-attention mechanism between question and caption) to evaluate the performance of the GT and GenP captions. As shown in Table 3, the GT captions can answer more questions correctly than GenP captions in TextQA model evaluation. Human evaluation with GT captions also shows better performance than with GenP captions as seen in Table 4. However, the results from the manual investigation have around 12% gap between GT and generated captions, while the gap between the results from the TextQA model is relatively small (1.89%). This shows that paragraph captions can answer several VQA questions but our current model is not able to extract the extra information from the GT captions. This allows future work: (1) the TextQA/VTQA models should be improved to extract more information from the GT captions; (2) paragraph captioning models should also be improved to generate captions closer to the GT captions.222We also ran our full VTQA model with the ground truth (GT) paragraph captions and got an accuracy value of 48.04% on the validation dataset (we ran the model five times with different seeds and average the scores), whereas the VTQA result from generated paragraph captions was 47.43%. This again implies that our current VTQA model is not able to extract all the information enough from GT paragraph captions for answering questions, and hence improving the model to better capture clues from GT captions is useful future work."
        ]
    }
}