{
    "id_table_1": {
        "caption": "TABLE I:  Accuracy comparisons with 7 data-sensitive approaches: Test accuracy under different privacy budget    \\varepsilon italic_ .",
        "table": "S2.E1",
        "footnotes": [],
        "references": [
            "Inspired by the above works, we propose a privacy-preserving data-free distillation method. As shown in Fig.  1 , publishing a model  ( e.g. , teacher model) trained directly from private data would compromise privacy, so we treat it as a fixed discriminator to train a generator in a data-free manner. This generator learns only the data distribution to protect the private data. Using this generator implicitly generates data for the distillation process from teacher model to student model. Because querying the teacher model using the generated synthetic data can compromise private information, we propose a LabelDP algorithm selective randomized response to protect the output of the teacher model. The selective randomized response algorithm treats the output of the student model as prior knowledge to reduce the possible output categories to increase the probability of outputting the correct label, and if the possible output does not contain the correct label, a uniform probability distribution is used to reset the possible probability of the output. In summary, our approach can effectively learn privacy-preserving student model by two keys. On the one hand, our proposed data-free distillation is able to protect privacy well with the learning of data distribution. The generated synthetic data from this generator will not reveal private information even if it is distributed. On the other hand is that we propose the selective randomized response module to implement DP, which is no longer limited by the number of queries, and introduce the prediction of the student model as prior knowledge for the randomized response. We increase the probability of returning the correct label by setting a threshold, so the student model can learn the knowledge of the teacher model more effectively.",
            "From Eq.  1 , we can see that our approach can learn privacy-preserving models by two main processes. First, the training of student does not directly access the private data. Second, the labels from the teacher are protected by the selective randomized response module which implements    \\varepsilon italic_ -LabelDP. Therefore, privacy leakage can be suppressed very effectively. During training, the teacher knowledge is transferred to the student through the label  L L \\mathcal{L} caligraphic_L . We solve Eq.  1  via two steps, including: 1) data-free generator learning that trains a generator   g subscript italic- g \\phi_{g} italic_ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT  with the pre-trained teacher   t subscript italic- t \\phi_{t} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  as a fixed discriminator to generate synthetic data  D ~ ~ D \\tilde{\\mathcal{D}} over~ start_ARG caligraphic_D end_ARG , and 2) student learning that applies knowledge distillation is to label the synthetic data with   t subscript italic- t \\phi_{t} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and selective randomized response function. And then use these data-label pairs to train the student model   s subscript italic- s \\phi_{s} italic_ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  and fine-tune the generator   g subscript italic- g \\phi_{g} italic_ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT . The detailed process is introduced in Alg.  1 .",
            "During the training of the student model, we use randomized response  [ 18 ]  for the sensitive labels to achieve DP.  R  R  R subscript R  RR_{\\varepsilon} italic_R italic_R start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT  mechanism will return correct class label with the probability  e  e  + K  1 superscript e  superscript e  K 1 \\frac{e^{\\varepsilon}}{e^{\\varepsilon}+K-1} divide start_ARG italic_e start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT end_ARG start_ARG italic_e start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT + italic_K - 1 end_ARG , and return other labels with probability  1 e  + K  1 1 superscript e  K 1 \\frac{1}{e^{\\varepsilon}+K-1} divide start_ARG 1 end_ARG start_ARG italic_e start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT + italic_K - 1 end_ARG , where  K K K italic_K  is the number of classes. To improve the probability of returning the true label without compromising privacy, we introduce the student prediction  y s subscript y s \\mathbf{y}_{s} bold_y start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  and propose selective randomized response algorithm. As shown in function selective randomized response in Alg.  1 , we first set a threshold  t t t italic_t  and select the set of indexes  I I I italic_I  with condition  y s i > t superscript subscript y s i t \\mathtt{y}_{s}^{i}>t typewriter_y start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT > italic_t . To ensure the randomness of the output labels, we require that the number of elements in  I I I italic_I  to be at least 2. We will set  I I I italic_I  to the set of indexes of top two largest elements in  y s subscript y s \\mathbf{y}_{s} bold_y start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  if the number of elements in  I I I italic_I  is less than 2. Let  k k k italic_k  be the number of  I I I italic_I . If the teacher models output in  I I I italic_I , return the  y t subscript y t \\mathbf{y}_{t} bold_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  with the probability  e  e  + k  1 superscript e  superscript e  k 1 \\frac{e^{\\varepsilon}}{e^{\\varepsilon}+k-1} divide start_ARG italic_e start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT end_ARG start_ARG italic_e start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT + italic_k - 1 end_ARG  and return the one-hot type of other elements with probability  1 e  + k  1 1 superscript e  k 1 \\frac{1}{e^{\\varepsilon}+k-1} divide start_ARG 1 end_ARG start_ARG italic_e start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT + italic_k - 1 end_ARG  ( R  R   ( I , y t ) R subscript R  I subscript y t RR_{\\varepsilon}(I,\\mathbf{y}_{t}) italic_R italic_R start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_I , bold_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  in Fig.  1 ). If the teacher models output not in  I I I italic_I , return the one-hot type of the elements in  I I I italic_I  with probability  1 k 1 k \\frac{1}{k} divide start_ARG 1 end_ARG start_ARG italic_k end_ARG  ( U  n  i  f  o  r  m  ( I ) U n i f o r m I Uniform(I) italic_U italic_n italic_i italic_f italic_o italic_r italic_m ( italic_I )  in Fig.  1 )."
        ]
    },
    "id_table_2": {
        "caption": "TABLE II:  Accuracy comparisons with 4 Label sensitive approaches: Test accuracy under different privacy budget    \\varepsilon italic_ .",
        "table": "S2.E2",
        "footnotes": [],
        "references": [
            "Implementation.  In all experiments, the structure of teacher is  R  e  s  n  e  t  34 R e s n e t 34 Resnet34 italic_R italic_e italic_s italic_n italic_e italic_t 34  and we set    \\alpha italic_  and    \\beta italic_  in Eq.  2  as 5 and 10 respectively. The structure of student is the same as  [ 27 ]  in data-sensitive experiments and  R  e  s  n  e  t  18 R e s n e t 18 Resnet18 italic_R italic_e italic_s italic_n italic_e italic_t 18  in label-sensitive experiments, respectively. For each dataset, we set the threshold value to  1 / ( 2  n  c ) 1 2 n c 1/(2*nc) 1 / ( 2  italic_n italic_c )  where  n  c n c nc italic_n italic_c  is the number of classes. We evaluate the test accuracy of student under privacy protection.",
            "Data amount.  We further conducted experiments on MNIST, FMNIST, CIAFR10 and CIFAR100 datasets under   = 1  1 \\varepsilon=1 italic_ = 1 . The results are shown in Fig.  2 . We found that MNIST dataset converges at about 50,000 data volume, FMNIST converges at about 120,000, CIFAR10 and CIFAR100 converge at about 220,000 and 500,000, respectively. As the difficulty of datasets increases, the amount of data required to achieve convergence increases. We suspect that this is because the more difficult the dataset is, the more difficult its distribution knowledge is to learn, so the larger the amount of data required. We note that the CIFAR10 dataset is more difficult than FMNIST, but the reason why CIFAR10s final accuracy is similar to FMNISTs is that the network structure is different."
        ]
    },
    "id_table_3": {
        "caption": "TABLE III:  Impact of loss terms in training generator under    \\varepsilon italic_ =10.",
        "table": "S2.E3",
        "footnotes": [],
        "references": [
            "Number of stages.  To explore the effect of the number of stages, we conducted experiments on MNIST, FMNIST and CIFAR10 datasets under    \\varepsilon italic_ =10. The results are shown in Fig.  3 . Experimental results show that between 20 and 320, the accuracy of the student model increases with the increase of stages. As the classification difficulty of MNIST, FMNIST and CIFAR10 datasets increases, the effect of stages becomes greater. The experimental results are as we expected because we used the prediction of the student model as the prior knowledge. As the training process proceeds, the more accurate the prediction of the student model becomes, which means the higher the probability of outputting the correct label. The greater the percentage of synthetic data being correctly labeled, the better the student model performance will be."
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "S3.T1.3.1",
        "footnotes": [],
        "references": [
            "Data generation.  To demonstrate that the direct use of synthetic data in our approach doesnt leak information of private data, we visualize some examples for MNIST, FMNIST, CIFAR10 and CelebA, as shown in Fig.  4 . The first row is MNIST, followed by FMNIST, CIFAR10, CelebA-G and CelebA-H in that order. We found that even for the simplest MNIST synthetic data, we could not semantically identify it as a handwritten font. Despite its inability to be recognized by humans, it has high utility in terms of training high performance models. We also found something interesting: such synthetic data can train a model that performs well, which raises an interesting question about what machine learning models actually learn from data?"
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "S3.T2.3.1",
        "footnotes": [],
        "references": [
            "Model-inversion attack.  We perform a model-inversion attack  [ 8 ]  on a typical data-sensitive approach and a label-sensitive approach to further demonstrate that our approach can protect data privacy. The results are shown in Fig.  5 . The first row is the results of the attack on a typical data-sensitive method DataLens  [ 27 ] , while the second row shows the results of the attack on a typical label-sensitive method ALIBI  [ 12 ] . The last row is the results of the attack on our DP-DFD. We emphasize that the authors of  [ 8 ]  stress in their original paper that differential privacy hardly works against this attack method, but we can find that even for experiments on the simplest MNIST dataset, our method still can defend against this attack and protect the privacy of the private data."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "S3.T3.6",
        "footnotes": [],
        "references": []
    }
}