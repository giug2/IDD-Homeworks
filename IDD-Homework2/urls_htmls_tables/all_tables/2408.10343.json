{
    "id_table_1": {
        "caption": "Table 1:  Summary of Datasets",
        "table": "S1.T1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_2": {
        "caption": "Table 2:  Dataset Statistics",
        "table": "S3.T2.1",
        "footnotes": [],
        "references": [
            "LegalBench-RAG is composed of 4 datasets and totals almost 80 million characters in its corpus across 714 documents. Each pair is annotated by legal experts, ensuring the highest accuracy and relevance for this benchmark. We extract 6,889 question-answer pairs which constitutes our retrieval benchmark. The outcome is a robust dataset that we call LegalBench-RAG. The statistics regarding this corpus are presented in Table  2 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Summary of datasets in LegalBench-RAG-mini, including the number of documents, corpus characters, and queries.",
        "table": "S3.T3.1",
        "footnotes": [],
        "references": [
            "Given the consequent size of LegalBench-RAG, this paper also introduces LegalBench-RAG-mini, a more lightweight version of the benchmark we proposed. LegalBench-RAG-mini was created by selecting exactly 194 queries from each of the four datasets PrivacyQA, CUAD, MAUD and ContractNLI. We select the portions of the corpus corresponding to these queries accordingly. This results in a dataset of 776 queries as described in Table  3 .",
            "The overall evaluation results are computed for varying values of  k k k italic_k  between 1 and 64 and for each of the four dataset and are also aggregated across all datasets using an equal weight for each dataset. Figures  3  and  3  display Recall@ k k k italic_k  and Precision@ k k k italic_k , respectively, for all method combinations."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Performance comparison on different datasets for Precision and Recall at various  k k k italic_k  values for the Naive Method.",
        "table": "S4.T4.2",
        "footnotes": [],
        "references": [
            "In our initial experiment, we fixed all hyperparameters and compared the impact of transitioning from a naive chunking strategy to a Recursive Text Character Splitter (RTCS). The performance results for each dataset, as well as varying values of  k k k italic_k , are presented in Tables  4  and  5 .",
            "We conduct a second experiment where we freeze all other hyperparameters and change the post-processing method used only. We compare using no reranking stategy of the retrieved chunks and using Coheres Reranker model. The results of this comparison are shown in Tables  4  and  6  respectively. We also run our benchmark on the RCTS method with Coheres Reranker in Table  7 ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Performance comparison on different datasets for Precision and Recall at various  k k k italic_k  values for the Recursive Character Text Splitter Method.",
        "table": "S4.T5.2",
        "footnotes": [],
        "references": [
            "In our initial experiment, we fixed all hyperparameters and compared the impact of transitioning from a naive chunking strategy to a Recursive Text Character Splitter (RTCS). The performance results for each dataset, as well as varying values of  k k k italic_k , are presented in Tables  4  and  5 .",
            "Additionally to the standard evaluation conducted, we also aggregated Recall@ k k k italic_k  and Precision@ k k k italic_k  across all four of the different methods experimented with for each of the four datasets. The goal was to assess the relative difficulty of achieving high scores on each dataset, as shown in Figures  5  and  5 ."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Performance comparison on different datasets for Precision and Recall at various  k k k italic_k  values for the Naive Method with the Cohere Reranker.",
        "table": "S4.T6.2",
        "footnotes": [],
        "references": [
            "We conduct a second experiment where we freeze all other hyperparameters and change the post-processing method used only. We compare using no reranking stategy of the retrieved chunks and using Coheres Reranker model. The results of this comparison are shown in Tables  4  and  6  respectively. We also run our benchmark on the RCTS method with Coheres Reranker in Table  7 ."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Performance comparison on different datasets for Precision and Recall at various  k k k italic_k  values for the RCTS Method with the Cohere Reranker.",
        "table": "S4.T7.2",
        "footnotes": [],
        "references": [
            "We conduct a second experiment where we freeze all other hyperparameters and change the post-processing method used only. We compare using no reranking stategy of the retrieved chunks and using Coheres Reranker model. The results of this comparison are shown in Tables  4  and  6  respectively. We also run our benchmark on the RCTS method with Coheres Reranker in Table  7 ."
        ]
    },
    "global_footnotes": []
}