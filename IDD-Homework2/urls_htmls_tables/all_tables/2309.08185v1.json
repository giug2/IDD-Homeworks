{
    "S5.T1": {
        "caption": "Table 1:  This is a comparison of different few-shot learning, zero-shot baselines, and machine translation models under a variety of language configuration scenarios. For LAReQA and STSBMulti, we report mAP@20 and Pearson’s r × 100, respectively. All results are evaluated over 5-fold cross-validation and averaged over multiple language choices. The same model checkpoint is used for all three task language evaluation variants for each row and dataset (except when the average is reported). mono, bi, and multistand for monolingual, bilingual, and multilingual semantic search. trans denotes the meta-transfer mode that uses mono→→\\rightarrowbi and bi→→\\rightarrowmulti in meta-training and meta-validation, respectively. Models in (*) are our main contribution. (**) means that we use machine-translated data to do that experiment as STSBMulti is not a parallel corpus. Best and second-best results for each benchmark and evaluation mode are highlighted in bold and italicized respectively, whereas the best results across each model category are underlined. Ranks from best to worst are given in each model and evaluation mode.9",
        "table": null,
        "footnotes": [],
        "references": [
            "Table 1 summarizes multilingual, bilingual, and monolingual performances across different baselines and model variants for both semantic search benchmarks. On average, we notice that MAML-Align achieves better results than MAML or S-BERT zero-shot base model and significantly better than Fine-tune. It is worth noting that we report the results for MAML using trans mode, which is trained over a combination of mono→→\\rightarrowbi and bi→→\\rightarrowmulti in the meta-training and meta-validation stages, respectively. This suggests that MAML-Align helps more in bridging the gap between those transfer modes. We observe that fine-tuning baselines are consistently weak compared to different meta-learning model variants, especially for LAReQA. We conjecture that fine-tuning is over-fitting to the small amounts of training data, unlike meta-learning approaches which are more robust against that. However, for STSBMulti, the gap between fine-tuning and meta-learning while still existing and to the favor of meta-learning is a bit reduced. We hypothesize that even meta-learning models are suffering from meta-overfitting to some degree in this case for STSBMulti."
        ]
    },
    "A3.T2": {
        "caption": "Table 2: Statistics of LAReQA in each 5-fold cross-validation split. #Q denotes the number of question whereas #C denotes the number of candidates.",
        "table": null,
        "footnotes": [],
        "references": [
            "Tables 2 and 3 show a summary of the statistics of LAReQA and STSBMulti per language and split, respectively. XQuAD-R in LAReQA has been distributed under the CC BY-SA 4.0 license, whereas STSBMulti has been released under the Creative Commons Attribution-ShareAlike 4.0 International License. The translated datasets from SQUADEN and STSBEN are shared under the same license as the original\ndatasets. SQUADEN is shared under XTREME benchmark Apache License Version 2.0. STSBEN scores are under Creative Commons Attribution-ShareAlike 3.0 Unported (CC BY-SA 3.0) and sentence pairs are shared under Commons Attribution - Share Alike 4.0 International License)."
        ]
    },
    "A3.T3": {
        "caption": "Table 3:  Statistics of the STSBMulti from SEM-Eval2007 in each 5-fold cross-validation split. * means that for Turkish-English, there are only 250 ground truth similarity scores, while there are 500 sentence pairs. We assume that the ground truth scores are only for the first 250 sentence pairs. In addition to that, we use 5749 train, 1500 dev, and 1379 test splits from the STSB original English benchmark.",
        "table": null,
        "footnotes": [],
        "references": [
            "Tables 2 and 3 show a summary of the statistics of LAReQA and STSBMulti per language and split, respectively. XQuAD-R in LAReQA has been distributed under the CC BY-SA 4.0 license, whereas STSBMulti has been released under the Creative Commons Attribution-ShareAlike 4.0 International License. The translated datasets from SQUADEN and STSBEN are shared under the same license as the original\ndatasets. SQUADEN is shared under XTREME benchmark Apache License Version 2.0. STSBEN scores are under Creative Commons Attribution-ShareAlike 3.0 Unported (CC BY-SA 3.0) and sentence pairs are shared under Commons Attribution - Share Alike 4.0 International License)."
        ]
    },
    "A3.T4": {
        "caption": "Table 4: Arrangements of languages for the different modes of transfer and meta-learning stages for two standard benchmark datasets LAReQA and STSBMulti. X→→\\rightarrowY denotes transfer from an X model (for example a monolingual model) used to sample the support set to a Y model (for example bilingual model) used to sample the query set. We denote a support or query set in LAReQA by x_y where x and y are the ISO language codes of the question and the candidate answers and x_y in STSBMulti where x and y are the ISO language codes of sentence 1 and 2 respectively. We use parenthesis to mean that the same language pairs cannot be used in both support and query sets, brackets to denote non-exclusivity (or in other words the language pairs used as a support can also be used as a query), and curled braces to mean the query set may be sampled from more than one language. We do not experiment with mono→→\\rightarrowmulti, bi→→\\rightarrowmulti, mixt, and trans for STSBMulti, since it is not a multilingual parallel benchmark, but we still experiment with mono→→\\rightarrowbi→→\\rightarrowmulti using machine-translated data in that case.",
        "table": null,
        "footnotes": [],
        "references": [
            "Following our formulation of semantic search downstream benchmarks, we construct pseudo-meta-tasks by drawing from the available triplets or sentence pairs to form the support set S𝑆S, so that each support set consists of a batch of k​_​s​h​o​t𝑘_𝑠ℎ𝑜𝑡k\\_shot triplets or sentence pairs. Then, we form the triplets or sentence pairs in the query set Q𝑄Q by picking for each question or sentence pair in S𝑆S either a similar or random question or sentence pair. Details of the different transfer modes and their support and query set arrangements are in Table 4 in Appendix C.2. We construct meta-datasets for different stages of meta-learning where Train, Dev, and Test splits are used to sample 𝒟meta-trainsubscript𝒟meta-train\\mathcal{D}_{\\mbox{{meta-train}}}, 𝒟meta-validsubscript𝒟meta-valid\\mathcal{D}_{\\mbox{{meta-valid}}}, and 𝒟meta-testsubscript𝒟meta-test\\mathcal{D}_{\\mbox{{meta-test}}}, respectively. The optimizations on 𝒟meta-trainsubscript𝒟meta-train\\mathcal{D}_{\\mbox{{meta-train}}} and 𝒟meta-testsubscript𝒟meta-test\\mathcal{D}_{\\mbox{{meta-test}}} are as defined in §3 and the optimization on 𝒟meta-validsubscript𝒟meta-valid\\mathcal{D}_{\\mbox{{meta-valid}}} is similar to that of 𝒟meta-trainsubscript𝒟meta-train\\mathcal{D}_{\\mbox{{meta-train}}}.",
            "We detail in Table 4 the arrangements of languages for the different meta-tasks used in the meta-training 𝒟meta-trainsubscript𝒟meta-train\\mathcal{D}_{\\mbox{{meta-train}}}{}, meta-validation 𝒟meta-validsubscript𝒟meta-valid\\mathcal{D}_{\\mbox{{meta-valid}}}{}, and meta-testing 𝒟meta-testsubscript𝒟meta-test\\mathcal{D}_{\\mbox{{meta-test}}}{} datasets. To make the comparison fair and consistent across different transfer modes, we use the same combination of languages and tweak them to fit the transfer mode. By picking a high number of meta-tasks during meta-training, meta-validation, and meta-testing, we make sure that all transfer modes are exposed to the same number of questions and candidates."
        ]
    },
    "A3.T5": {
        "caption": "Table 5: Comparison of mAP@20 multilingual 5-fold cross-validation evaluation of different S-BERT models compared to M-BERT model. Best results are highlighted in bold.",
        "table": null,
        "footnotes": [],
        "references": [
            "BASE: This is our initial zero-shot approach based on an off-the-shelf pre-trained language model. For the rest of our analysis, we use the best model on our 5-fold cross-validation test splits, which is sentence-BERT (S-BERT) paraphrase-multilingual-mpnet-base-v2, according to our preliminary evaluation of different Sentence Transformers models.666https://huggingface.co/sentence-transformers in Table 5 in Appendix C.",
            "Based on our prior investigation of different sentence-transformer models in Table 5, we notice that paraphrase-multilingual-mpnet-base-v2111111https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2., which maps sentences and paragraphs to a 768-dimensional dense vector space, performs the best for LAReQA, so we use it in our S-BERT experiments on that dataset. The good initial performance of this pre-trained model is not surprising since it was trained on parallel data and is recommended for use in tasks like clustering or semantic search. For pre-processing LAReQA and SQUADEN, we truncate/pad all questions to length 96 and all answer or negative candidates concatenated with their contexts to 256. For pre-processing STSBMulti and STSBEN, we pad or truncate each sentence to fit the maximum length of 100."
        ]
    },
    "A3.T6": {
        "caption": "Table 6: Runtime per model variant excluding evaluation.",
        "table": null,
        "footnotes": [],
        "references": [
            "For all experiments and model variants, we train for up to 20 epochs maximum and we implement early stopping, where we run the experiment for as long as there is an improvement on the Dev set performance. After 50 mini meta-task batches of no improvement on the Dev set, the experiment stops running. We use the multilingual performance on the Dev set averaged over all languages of the query set as the early stopping evaluation criteria. Based on this early stopping policy, we report in Table 6 the typical runtime for each upstream model variant and baseline."
        ]
    },
    "A4.T7": {
        "caption": "Table 7:  mAP@20 multilingual 5-fold cross-validated performance tested for different languages. Best and second-best results for each language are highlighted in bold and italicized respectively, whereas best results across categories of models are underlined. Gains from meta-learning approaches are consistent across few-shot and zero-shot languages.",
        "table": null,
        "footnotes": [],
        "references": [
            "We notice that MAML on top of machine-translated data boosts the performance on LAReQA in all evaluation task language evaluation variants and reaches the best compromise in terms of multilingual, bilingual, and monolingual performances. At the same time, not all languages used in the machine-translated data provide an equal boost to the performance, as shown by the average performance, due to noisy translations for certain languages. Although there is usually a correlation between different models in terms of their monolingual, bilingual, and multilingual performances, there is a slight drop in the monolingual and bilingual performances for MAML-Align compared to the zero-shot baseline. This means that there is still a compromise and gaps between multilingual, monolingual, and bilingual performances. This suggests that we should advocate for a balanced evaluation over different modes to get better insights into which models are more robust and consistent. Figure 3 highlights a more fine-grained comparison between different model categories on two languages and language pairs for each benchmark.999More fine-grained results for all languages and for both benchmarks can be found in Tables 7 and 8 in Appendix D. We notice that the gain in favor of meta-learning approaches is consistent across different languages and language pairs and also applies to languages used for zero-shot learning.",
            "Tables 7 and 8 show full fine-grained results for all languages and language pairs for both semantic search benchmarks."
        ]
    },
    "A4.T8": {
        "caption": "Table 8:  Pearson correlation Pearson’s r × 100 5-fold cross-validated performance on STSBMulti benchmark using different models few-shot learned on STSBMulti or its translation. Best and second-best results for each language are highlighted in bold and italicized respectively, whereas best results across categories of models are underlined.",
        "table": null,
        "footnotes": [],
        "references": [
            "We notice that MAML on top of machine-translated data boosts the performance on LAReQA in all evaluation task language evaluation variants and reaches the best compromise in terms of multilingual, bilingual, and monolingual performances. At the same time, not all languages used in the machine-translated data provide an equal boost to the performance, as shown by the average performance, due to noisy translations for certain languages. Although there is usually a correlation between different models in terms of their monolingual, bilingual, and multilingual performances, there is a slight drop in the monolingual and bilingual performances for MAML-Align compared to the zero-shot baseline. This means that there is still a compromise and gaps between multilingual, monolingual, and bilingual performances. This suggests that we should advocate for a balanced evaluation over different modes to get better insights into which models are more robust and consistent. Figure 3 highlights a more fine-grained comparison between different model categories on two languages and language pairs for each benchmark.999More fine-grained results for all languages and for both benchmarks can be found in Tables 7 and 8 in Appendix D. We notice that the gain in favor of meta-learning approaches is consistent across different languages and language pairs and also applies to languages used for zero-shot learning.",
            "Tables 7 and 8 show full fine-grained results for all languages and language pairs for both semantic search benchmarks."
        ]
    }
}