{
    "id_table_1": {
        "caption": "Table 1:  SQuAD2.0 and DROP Results",
        "table": "Sx5.T1.1",
        "footnotes": [],
        "references": [
            "We observed a substantial improvement in accuracy for both the SQuAD2.0 and DROP datasets (Table 1) when employing VERA. Specifically, Mistral-7B-instruct-v0.1 exhibited a 20% increase in accuracy on the SQuAD2.0 dataset and a 15% increase on the DROP dataset. Additionally, VERA enhanced the performance of GPT-4o by 5% on SQuAD2.0 and 10% on DROP. These results underscore VERAs effectiveness in enhancing the performance of large language models on tasks that demand advanced comprehension capabilities."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Comparison of models with and without VERA - WWII Wikipedia",
        "table": "Sx5.T2.1",
        "footnotes": [],
        "references": [
            "The results of downstream tasks demonstrated a significant increase in adherence and relevance scores for smaller models like Mistral-7B-instruct-v0.1. Notable improvements were also observed in larger models such as GPT-4o and GPT-3.5-turbo. Specifically, Mistral-7B-instruct-v0.1 exhibited an increase in Response Adherence by up to 18.7% (Table 2) and an increase in Response Relevance by up to 17.9% (Table 2) when using VERA."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Comparison of models with and without VERA - Apple 10k Report",
        "table": "Sx5.T3.1",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": []
}