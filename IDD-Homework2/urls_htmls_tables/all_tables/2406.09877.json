{
    "PAPER'S NUMBER OF TABLES": 10,
    "S4.T1": {
        "caption": "Table 1: Global and local testing accuracies for Pre-ResNet, MobileNetV2, and EfficientNetV2 on CIFAR-10, CIFAR-100, and Fashion MNIST datasets in IID and non-IID scenarios. FedFAâ€™s depth-, width-, and both depth and width-flexible approaches lead to higher test accuracy with no malicious clients and lower accuracy drops (under attacks with intensity Î»=20ğœ†20\\lambda=20 and 20% malicious clients) compared to prior depth-, width-, and both depth and width-flexible approaches. The maximum and minimum test accuracies are highlighted in bold for each scenario.",
        "table": "<img src=\"/html/2406.09877/assets/x3.png\" id=\"S4.T1.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"461\" height=\"368\" alt=\"[Uncaptioned image]\">\n\n",
        "footnotes": "",
        "references": [
            "TableÂ 1 shows the testing results of scenarios with varying model depths and widths for FedFA and previous flexible aggregation strategies in width and depth. Each scenario was tested three times, and the table presents the average results.",
            "We employed three types of architectures: Pre-ResNet, MobileNetV2, and EfficientNetV2, varying in depth and width as depicted in TableÂ 4. Detailed training conditions are in TableÂ 6. To determine the depth dksubscriptğ‘‘ğ‘˜d_{k} and width wksubscriptğ‘¤ğ‘˜w_{k}, we refer to the configuration in the second column of TableÂ 10.",
            "The average magnitudes of Baseline models and distances from the Baseline model to each Model kğ‘˜k are presented in TableÂ 10. Comparing distances from Baseline models to each Model kğ‘˜k, we observe variations according to network complexities, influenced by varying depth and width. Specifically, in Pre-ResNet, the distances are 0.98âˆ’1.360.981.360.98-1.36 times greater than the Baseline modelsâ€™ average magnitude of weights. For MobileNetV2, these distances range from 1.20âˆ’1.681.201.681.20-1.68 times the Baselineâ€™s average magnitude, and for EfficientNetV2, they are 1.35âˆ’1.701.351.701.35-1.70 times greater. These findings empirically show the scale variations linked to network complexities and highlight the necessity for a scalable aggregation method in our FedFA framework."
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Computational complexities for Pre-ResNet, MobileNetV2, and EfficientNetV2 on CIFAR-10, CIFAR-100, and Fashion MNIST datasets in IID and non-IID scenarios. FedFA shows comparable complexity to its baselines.",
        "table": "<img src=\"/html/2406.09877/assets/x5.png\" id=\"S5.T2.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"461\" height=\"228\" alt=\"[Uncaptioned image]\">\n\n",
        "footnotes": "",
        "references": [
            "FedFA, employing layer grafting and scalable aggregation, has slightly higher computational complexity than earlier heterogeneous methods. Yet, for targeted testing accuraciesâ€”70% (IID) and 40% (non-IID) in CIFAR-10, 25% (both IID and non-IID) in CIFAR-100, and 80% (IID) and 30% (non-IID) in Fashion MNISTâ€”the computational complexities are only 0.95 to 1.02 times higher. This indicates that FedFAâ€™s computational overhead is not marginally higher than earlier strategies, as presented in TableÂ 2."
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Average local testing perplexities for Transformer on the WikiText-2 dataset. FedFAâ€™s three variants yield much lower perplexities than its competitors.",
        "table": "<img src=\"/html/2406.09877/assets/x6.png\" id=\"S5.T3.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"461\" height=\"26\" alt=\"[Uncaptioned image]\">\n\n",
        "footnotes": "",
        "references": [
            "Lastly, to demonstrate the generality of FedFA, we also examine its performance with transformers. The earlier strategies exhibit perplexities that are 1.07 to 4.50 times higher than those of FedFA, as detailed in TableÂ 3."
        ]
    },
    "Pt0.A1.T4": {
        "caption": "Table 4: Details of the network architectures for Pre-ResNet, MobileNetV2, EfficientNetV2, and Transformer are presented.",
        "table": "<img src=\"/html/2406.09877/assets/x7.png\" id=\"Pt0.A1.T4.g1\" class=\"ltx_graphics ltx_centering ltx_img_portrait\" width=\"302\" height=\"393\" alt=\"[Uncaptioned image]\">\n\n",
        "footnotes": "",
        "references": [
            "For Non-IID settings, clients get samples from 20% of the dataset classes but maintain equal samples for each class they hold. Here, during local training, clients zero-out logits for absent classes. We replace typical batch normalization layers with static versions, as seen in HeteroFLÂ [6]. We also utilized a language model with a Transformer using WikiText-2 to demonstrate that our method is generalizable. Detailed network structures are presented in TableÂ 4, and more training details are in TableÂ 6 in the Appendix.",
            "We also statistically validate layer similarity. We consider the three different CNNs shown in TableÂ 4: Pre-ResNet, MobileNetV2, and EfficientNetV2, which are respectively trained on the CIFAR-10, CIFAR-100, and Fashion MNIST datasets. We use models of different depths dksubscriptğ‘‘ğ‘˜d_{k}, with dksubscriptğ‘‘ğ‘˜d_{k} determining the number of residual blocks in each convolutional layer section, as shown in the table.",
            "We employed three types of architectures: Pre-ResNet, MobileNetV2, and EfficientNetV2, varying in depth and width as depicted in TableÂ 4. Detailed training conditions are in TableÂ 6. To determine the depth dksubscriptğ‘‘ğ‘˜d_{k} and width wksubscriptğ‘¤ğ‘˜w_{k}, we refer to the configuration in the second column of TableÂ 10."
        ]
    },
    "Pt0.A1.T5": {
        "caption": "Table 5: The candidate values for the networksâ€™ depth dksubscriptğ‘‘ğ‘˜d_{k} and width wksubscriptğ‘¤ğ‘˜w_{k} in TableÂ 4.",
        "table": "<img src=\"/html/2406.09877/assets/x8.png\" id=\"Pt0.A1.T5.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"461\" height=\"203\" alt=\"[Uncaptioned image]\">\n\n",
        "footnotes": "",
        "references": [
            "Evaluations were conducted under four scenarios with varying impacts of backdoor attacks from malicious clients. Here, the backdoor attacks involve the random shuffling of the data labels among clients to induce misclassification. Scenarios have different portions of malicious clients over entire local clients (0 %, 1 %, and 20 %) and two intensities of attacks, (Î»=1,20ğœ†120\\lambda=1,20 in Eq.Â 1). Also, for all scenarios, we assume that half of the clients have limited computational resources and choose the smallest architectures. The other clients choose their architectures employing ZiCoÂ [18], a cost-effective NAS method that requires only forward passes and uses an evolutionary algorithm. This method decides local network architectures among the network candidates specified in TableÂ 5 in the Appendix, based on local data for each client."
        ]
    },
    "Pt0.A1.T6": {
        "caption": "Table 6: Test conditions for evaluating layer similarity in SectionÂ 0.B, scale variations in SectionÂ 0.F in the Appendix, and performance comparisons in SectionÂ 5.",
        "table": "<img src=\"/html/2406.09877/assets/x9.png\" id=\"Pt0.A1.T6.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"461\" height=\"337\" alt=\"[Uncaptioned image]\">\n\n",
        "footnotes": "",
        "references": [
            "For Non-IID settings, clients get samples from 20% of the dataset classes but maintain equal samples for each class they hold. Here, during local training, clients zero-out logits for absent classes. We replace typical batch normalization layers with static versions, as seen in HeteroFLÂ [6]. We also utilized a language model with a Transformer using WikiText-2 to demonstrate that our method is generalizable. Detailed network structures are presented in TableÂ 4, and more training details are in TableÂ 6 in the Appendix.",
            "We employed three types of architectures: Pre-ResNet, MobileNetV2, and EfficientNetV2, varying in depth and width as depicted in TableÂ 4. Detailed training conditions are in TableÂ 6. To determine the depth dksubscriptğ‘‘ğ‘˜d_{k} and width wksubscriptğ‘¤ğ‘˜w_{k}, we refer to the configuration in the second column of TableÂ 10."
        ]
    },
    "Pt0.A2.T7": {
        "caption": "Table 7: Similarities of convolutional layers for Pre-ResNet.",
        "table": "<img src=\"/html/2406.09877/assets/x12.png\" id=\"Pt0.A2.T7.g1\" class=\"ltx_graphics ltx_centering ltx_img_portrait\" width=\"251\" height=\"323\" alt=\"[Uncaptioned image]\">\n\n",
        "footnotes": "",
        "references": [
            [
                "The statistical analysis of the layer grafting method in CNNs with skip connections underscores the viability of this approach in FL. By leveraging the inherent similarity among residual blocks, the layer grafting method not only preserves model integrity but also enhances adaptability and robustness across diverse client models. This analysis affirms the soundness of layer grafting as a key component in our FedFA framework in FigureÂ ",
                "6",
                "."
            ]
        ]
    },
    "Pt0.A2.T8": {
        "caption": "Table 8: Similarities of convolutional layers for MobileNetV2.",
        "table": "<img src=\"/html/2406.09877/assets/x13.png\" id=\"Pt0.A2.T8.g1\" class=\"ltx_graphics ltx_centering ltx_img_portrait\" width=\"251\" height=\"418\" alt=\"[Uncaptioned image]\">\n\n",
        "footnotes": "",
        "references": [
            [
                "The statistical analysis of the layer grafting method in CNNs with skip connections underscores the viability of this approach in FL. By leveraging the inherent similarity among residual blocks, the layer grafting method not only preserves model integrity but also enhances adaptability and robustness across diverse client models. This analysis affirms the soundness of layer grafting as a key component in our FedFA framework in FigureÂ ",
                "6",
                "."
            ]
        ]
    },
    "Pt0.A2.T9": {
        "caption": "Table 9: Similarities of convolutional layers for EfficientNetV2.",
        "table": "<img src=\"/html/2406.09877/assets/x14.png\" id=\"Pt0.A2.T9.g1\" class=\"ltx_graphics ltx_centering ltx_img_portrait\" width=\"251\" height=\"560\" alt=\"[Uncaptioned image]\">\n\n",
        "footnotes": "",
        "references": [
            [
                "The statistical analysis of the layer grafting method in CNNs with skip connections underscores the viability of this approach in FL. By leveraging the inherent similarity among residual blocks, the layer grafting method not only preserves model integrity but also enhances adaptability and robustness across diverse client models. This analysis affirms the soundness of layer grafting as a key component in our FedFA framework in FigureÂ ",
                "6",
                "."
            ]
        ]
    },
    "Pt0.A6.T10": {
        "caption": "Table 10: According to network complexities, each network has a distinct scale in its weights, leading to scale variations across heterogeneous network architectures.",
        "table": "<img src=\"/html/2406.09877/assets/x16.png\" id=\"Pt0.A6.T10.g1\" class=\"ltx_graphics ltx_centering ltx_img_portrait\" width=\"461\" height=\"653\" alt=\"[Uncaptioned image]\">\n\n",
        "footnotes": "",
        "references": [
            "We employed three types of architectures: Pre-ResNet, MobileNetV2, and EfficientNetV2, varying in depth and width as depicted in TableÂ 4. Detailed training conditions are in TableÂ 6. To determine the depth dksubscriptğ‘‘ğ‘˜d_{k} and width wksubscriptğ‘¤ğ‘˜w_{k}, we refer to the configuration in the second column of TableÂ 10.",
            "The average magnitudes of Baseline models and distances from the Baseline model to each Model kğ‘˜k are presented in TableÂ 10. Comparing distances from Baseline models to each Model kğ‘˜k, we observe variations according to network complexities, influenced by varying depth and width. Specifically, in Pre-ResNet, the distances are 0.98âˆ’1.360.981.360.98-1.36 times greater than the Baseline modelsâ€™ average magnitude of weights. For MobileNetV2, these distances range from 1.20âˆ’1.681.201.681.20-1.68 times the Baselineâ€™s average magnitude, and for EfficientNetV2, they are 1.35âˆ’1.701.351.701.35-1.70 times greater. These findings empirically show the scale variations linked to network complexities and highlight the necessity for a scalable aggregation method in our FedFA framework."
        ]
    }
}