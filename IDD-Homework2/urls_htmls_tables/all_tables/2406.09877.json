{
    "PAPER'S NUMBER OF TABLES": 10,
    "S4.T1": {
        "caption": "Table 1: Global and local testing accuracies for Pre-ResNet, MobileNetV2, and EfficientNetV2 on CIFAR-10, CIFAR-100, and Fashion MNIST datasets in IID and non-IID scenarios. FedFA’s depth-, width-, and both depth and width-flexible approaches lead to higher test accuracy with no malicious clients and lower accuracy drops (under attacks with intensity λ=20𝜆20\\lambda=20 and 20% malicious clients) compared to prior depth-, width-, and both depth and width-flexible approaches. The maximum and minimum test accuracies are highlighted in bold for each scenario.",
        "table": "<img src=\"/html/2406.09877/assets/x3.png\" id=\"S4.T1.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"461\" height=\"368\" alt=\"[Uncaptioned image]\">\n\n",
        "footnotes": "",
        "references": [
            "Table 1 shows the testing results of scenarios with varying model depths and widths for FedFA and previous flexible aggregation strategies in width and depth. Each scenario was tested three times, and the table presents the average results.",
            "We employed three types of architectures: Pre-ResNet, MobileNetV2, and EfficientNetV2, varying in depth and width as depicted in Table 4. Detailed training conditions are in Table 6. To determine the depth dksubscript𝑑𝑘d_{k} and width wksubscript𝑤𝑘w_{k}, we refer to the configuration in the second column of Table 10.",
            "The average magnitudes of Baseline models and distances from the Baseline model to each Model k𝑘k are presented in Table 10. Comparing distances from Baseline models to each Model k𝑘k, we observe variations according to network complexities, influenced by varying depth and width. Specifically, in Pre-ResNet, the distances are 0.98−1.360.981.360.98-1.36 times greater than the Baseline models’ average magnitude of weights. For MobileNetV2, these distances range from 1.20−1.681.201.681.20-1.68 times the Baseline’s average magnitude, and for EfficientNetV2, they are 1.35−1.701.351.701.35-1.70 times greater. These findings empirically show the scale variations linked to network complexities and highlight the necessity for a scalable aggregation method in our FedFA framework."
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Computational complexities for Pre-ResNet, MobileNetV2, and EfficientNetV2 on CIFAR-10, CIFAR-100, and Fashion MNIST datasets in IID and non-IID scenarios. FedFA shows comparable complexity to its baselines.",
        "table": "<img src=\"/html/2406.09877/assets/x5.png\" id=\"S5.T2.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"461\" height=\"228\" alt=\"[Uncaptioned image]\">\n\n",
        "footnotes": "",
        "references": [
            "FedFA, employing layer grafting and scalable aggregation, has slightly higher computational complexity than earlier heterogeneous methods. Yet, for targeted testing accuracies—70% (IID) and 40% (non-IID) in CIFAR-10, 25% (both IID and non-IID) in CIFAR-100, and 80% (IID) and 30% (non-IID) in Fashion MNIST—the computational complexities are only 0.95 to 1.02 times higher. This indicates that FedFA’s computational overhead is not marginally higher than earlier strategies, as presented in Table 2."
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Average local testing perplexities for Transformer on the WikiText-2 dataset. FedFA’s three variants yield much lower perplexities than its competitors.",
        "table": "<img src=\"/html/2406.09877/assets/x6.png\" id=\"S5.T3.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"461\" height=\"26\" alt=\"[Uncaptioned image]\">\n\n",
        "footnotes": "",
        "references": [
            "Lastly, to demonstrate the generality of FedFA, we also examine its performance with transformers. The earlier strategies exhibit perplexities that are 1.07 to 4.50 times higher than those of FedFA, as detailed in Table 3."
        ]
    },
    "Pt0.A1.T4": {
        "caption": "Table 4: Details of the network architectures for Pre-ResNet, MobileNetV2, EfficientNetV2, and Transformer are presented.",
        "table": "<img src=\"/html/2406.09877/assets/x7.png\" id=\"Pt0.A1.T4.g1\" class=\"ltx_graphics ltx_centering ltx_img_portrait\" width=\"302\" height=\"393\" alt=\"[Uncaptioned image]\">\n\n",
        "footnotes": "",
        "references": [
            "For Non-IID settings, clients get samples from 20% of the dataset classes but maintain equal samples for each class they hold. Here, during local training, clients zero-out logits for absent classes. We replace typical batch normalization layers with static versions, as seen in HeteroFL [6]. We also utilized a language model with a Transformer using WikiText-2 to demonstrate that our method is generalizable. Detailed network structures are presented in Table 4, and more training details are in Table 6 in the Appendix.",
            "We also statistically validate layer similarity. We consider the three different CNNs shown in Table 4: Pre-ResNet, MobileNetV2, and EfficientNetV2, which are respectively trained on the CIFAR-10, CIFAR-100, and Fashion MNIST datasets. We use models of different depths dksubscript𝑑𝑘d_{k}, with dksubscript𝑑𝑘d_{k} determining the number of residual blocks in each convolutional layer section, as shown in the table.",
            "We employed three types of architectures: Pre-ResNet, MobileNetV2, and EfficientNetV2, varying in depth and width as depicted in Table 4. Detailed training conditions are in Table 6. To determine the depth dksubscript𝑑𝑘d_{k} and width wksubscript𝑤𝑘w_{k}, we refer to the configuration in the second column of Table 10."
        ]
    },
    "Pt0.A1.T5": {
        "caption": "Table 5: The candidate values for the networks’ depth dksubscript𝑑𝑘d_{k} and width wksubscript𝑤𝑘w_{k} in Table 4.",
        "table": "<img src=\"/html/2406.09877/assets/x8.png\" id=\"Pt0.A1.T5.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"461\" height=\"203\" alt=\"[Uncaptioned image]\">\n\n",
        "footnotes": "",
        "references": [
            "Evaluations were conducted under four scenarios with varying impacts of backdoor attacks from malicious clients. Here, the backdoor attacks involve the random shuffling of the data labels among clients to induce misclassification. Scenarios have different portions of malicious clients over entire local clients (0 %, 1 %, and 20 %) and two intensities of attacks, (λ=1,20𝜆120\\lambda=1,20 in Eq. 1). Also, for all scenarios, we assume that half of the clients have limited computational resources and choose the smallest architectures. The other clients choose their architectures employing ZiCo [18], a cost-effective NAS method that requires only forward passes and uses an evolutionary algorithm. This method decides local network architectures among the network candidates specified in Table 5 in the Appendix, based on local data for each client."
        ]
    },
    "Pt0.A1.T6": {
        "caption": "Table 6: Test conditions for evaluating layer similarity in Section 0.B, scale variations in Section 0.F in the Appendix, and performance comparisons in Section 5.",
        "table": "<img src=\"/html/2406.09877/assets/x9.png\" id=\"Pt0.A1.T6.g1\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" width=\"461\" height=\"337\" alt=\"[Uncaptioned image]\">\n\n",
        "footnotes": "",
        "references": [
            "For Non-IID settings, clients get samples from 20% of the dataset classes but maintain equal samples for each class they hold. Here, during local training, clients zero-out logits for absent classes. We replace typical batch normalization layers with static versions, as seen in HeteroFL [6]. We also utilized a language model with a Transformer using WikiText-2 to demonstrate that our method is generalizable. Detailed network structures are presented in Table 4, and more training details are in Table 6 in the Appendix.",
            "We employed three types of architectures: Pre-ResNet, MobileNetV2, and EfficientNetV2, varying in depth and width as depicted in Table 4. Detailed training conditions are in Table 6. To determine the depth dksubscript𝑑𝑘d_{k} and width wksubscript𝑤𝑘w_{k}, we refer to the configuration in the second column of Table 10."
        ]
    },
    "Pt0.A2.T7": {
        "caption": "Table 7: Similarities of convolutional layers for Pre-ResNet.",
        "table": "<img src=\"/html/2406.09877/assets/x12.png\" id=\"Pt0.A2.T7.g1\" class=\"ltx_graphics ltx_centering ltx_img_portrait\" width=\"251\" height=\"323\" alt=\"[Uncaptioned image]\">\n\n",
        "footnotes": "",
        "references": [
            [
                "The statistical analysis of the layer grafting method in CNNs with skip connections underscores the viability of this approach in FL. By leveraging the inherent similarity among residual blocks, the layer grafting method not only preserves model integrity but also enhances adaptability and robustness across diverse client models. This analysis affirms the soundness of layer grafting as a key component in our FedFA framework in Figure ",
                "6",
                "."
            ]
        ]
    },
    "Pt0.A2.T8": {
        "caption": "Table 8: Similarities of convolutional layers for MobileNetV2.",
        "table": "<img src=\"/html/2406.09877/assets/x13.png\" id=\"Pt0.A2.T8.g1\" class=\"ltx_graphics ltx_centering ltx_img_portrait\" width=\"251\" height=\"418\" alt=\"[Uncaptioned image]\">\n\n",
        "footnotes": "",
        "references": [
            [
                "The statistical analysis of the layer grafting method in CNNs with skip connections underscores the viability of this approach in FL. By leveraging the inherent similarity among residual blocks, the layer grafting method not only preserves model integrity but also enhances adaptability and robustness across diverse client models. This analysis affirms the soundness of layer grafting as a key component in our FedFA framework in Figure ",
                "6",
                "."
            ]
        ]
    },
    "Pt0.A2.T9": {
        "caption": "Table 9: Similarities of convolutional layers for EfficientNetV2.",
        "table": "<img src=\"/html/2406.09877/assets/x14.png\" id=\"Pt0.A2.T9.g1\" class=\"ltx_graphics ltx_centering ltx_img_portrait\" width=\"251\" height=\"560\" alt=\"[Uncaptioned image]\">\n\n",
        "footnotes": "",
        "references": [
            [
                "The statistical analysis of the layer grafting method in CNNs with skip connections underscores the viability of this approach in FL. By leveraging the inherent similarity among residual blocks, the layer grafting method not only preserves model integrity but also enhances adaptability and robustness across diverse client models. This analysis affirms the soundness of layer grafting as a key component in our FedFA framework in Figure ",
                "6",
                "."
            ]
        ]
    },
    "Pt0.A6.T10": {
        "caption": "Table 10: According to network complexities, each network has a distinct scale in its weights, leading to scale variations across heterogeneous network architectures.",
        "table": "<img src=\"/html/2406.09877/assets/x16.png\" id=\"Pt0.A6.T10.g1\" class=\"ltx_graphics ltx_centering ltx_img_portrait\" width=\"461\" height=\"653\" alt=\"[Uncaptioned image]\">\n\n",
        "footnotes": "",
        "references": [
            "We employed three types of architectures: Pre-ResNet, MobileNetV2, and EfficientNetV2, varying in depth and width as depicted in Table 4. Detailed training conditions are in Table 6. To determine the depth dksubscript𝑑𝑘d_{k} and width wksubscript𝑤𝑘w_{k}, we refer to the configuration in the second column of Table 10.",
            "The average magnitudes of Baseline models and distances from the Baseline model to each Model k𝑘k are presented in Table 10. Comparing distances from Baseline models to each Model k𝑘k, we observe variations according to network complexities, influenced by varying depth and width. Specifically, in Pre-ResNet, the distances are 0.98−1.360.981.360.98-1.36 times greater than the Baseline models’ average magnitude of weights. For MobileNetV2, these distances range from 1.20−1.681.201.681.20-1.68 times the Baseline’s average magnitude, and for EfficientNetV2, they are 1.35−1.701.351.701.35-1.70 times greater. These findings empirically show the scale variations linked to network complexities and highlight the necessity for a scalable aggregation method in our FedFA framework."
        ]
    }
}