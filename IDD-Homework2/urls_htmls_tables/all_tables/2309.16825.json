{
    "PAPER'S NUMBER OF TABLES": 11,
    "S3.T1": {
        "caption": "Table 1: A summary of hyper-parameters tuned for each method. LR denotes learning rate.",
        "table": "",
        "footnotes": "hyper-parameter_summary\n\n\n\n\n\nMethod\nParam 1\nParam 2\nParam 3\n\nFedAvg\nClient LR\n–\n–\n\nFedAdam\nClient LR\nServer LR\n–\n\nFedProx\nClient LR\nμ𝜇\\mu\n–\n\nSCAFFOLD\nClient LR\nServer LR\n–\n\nMOON\nClient LR\nμ𝜇\\mu\n–\n\nFedPer\nClient LR\n–\n–\n\nAPFL\nClient LR\nα𝛼\\alpha LR\n–\n\nPerFCL\nClient LR\nμ𝜇\\mu\nγ𝛾\\gamma\n\nFENDA-FL\nClient LR\n–\n–\n\n",
        "references": [
            [
                "As in ",
                "(du Terrail et al., ",
                "2022a",
                ")",
                ", the experiments to follow incorporate a number of existing FL techniques as baselines along with a number of more recent approaches, including FENDA-FL. The methods evaluated are FedAvg ",
                "(McMahan et al., ",
                "2017",
                ")",
                ", FedAdam ",
                "(Reddi et al., ",
                "2021",
                ")",
                ", FedProx ",
                "(Li et al., ",
                "2020",
                ")",
                ", SCAFFOLD ",
                "(Karimireddy et al., ",
                "2020",
                ")",
                ", MOON ",
                "(Li et al., ",
                "2021a",
                ")",
                ", FedPer ",
                "(Arivazhagan et al., ",
                "2019",
                ")",
                ", APFL ",
                "(Deng et al., ",
                "2020",
                ")",
                ", and PerFCL ",
                "(Zhang et al., ",
                "2022",
                ")",
                ". Two non-FL training setups are also considered. The first, central training, pools all data to train a single model. The second is local training, where each client trains a model solely on local data, resulting in a model for each client with no global information.",
                "The original FedAvg algorithm, which applies weighted parameter averaging, remains a strong FL baseline. FedAdam is a recent extension of FedAvg that incorporates server-side first- and second-order momentum estimates and has shown some robustness to data drift. In all experiments, ",
                "β",
                "1",
                "=",
                "0.9",
                "subscript",
                "𝛽",
                "1",
                "0.9",
                "\\beta_{1}=0.9",
                ", ",
                "β",
                "2",
                "=",
                "0.99",
                "subscript",
                "𝛽",
                "2",
                "0.99",
                "\\beta_{2}=0.99",
                ", and ",
                "τ",
                "=",
                "1",
                "​",
                "e",
                "​",
                "-",
                "​",
                "9",
                "𝜏",
                "1",
                "e",
                "-",
                "9",
                "\\tau=1\\mathrm{e}{\\text{-}9}",
                ". The FedProx and SCAFFOLD methods aim to address issues associated with non-IID datasets by correcting local-weight drift through a regularization term or control variates, respectively. MOON builds on these ideas, applying a contrastive loss to constrain drift in the client models’ feature representations at a selected layer. Each method trains a global set of weights, updated at each server round, shared by all clients.",
                "For the personalized FL baselines, each client possesses a unique model. FedPer splits the models into a feature extractor and classifier. The feature extractor is shared among clients and federally trained, while the classifier is unique to each client. In the APFL approach, predictions are made through a convex combination of twin models. One is federally trained using FedAvg; the other incorporates only local updates. The combination parameter, ",
                "α",
                "𝛼",
                "\\alpha",
                ", is adapted during training using the gradients of the global and local models. Finally, PerFCL extends MOON to consider two feature extractors, one is shared by all clients and the other is specific to each client along with a local classification layer. The feature extractors are constrained using different contrastive losses.",
                "For all experiments, clients use an AdamW optimizer ",
                "(Loshchilov and Hutter, ",
                "2018",
                ")",
                " with default parameters for local training. The only exception is SCAFFOLD, which explicitly assumes an unmodified learning rate is applied and, thus, uses standard SGD. Hyper-parameters for each method, summarized in Table ",
                "LABEL:hyper-parameter_summary",
                ", are tuned. The ranges tested and best hyper-parameters for each method and dataset are detailed in Appendices ",
                "A",
                " and ",
                "B",
                ", along with the selection methodology."
            ]
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Performance metrics for the FLamby tasks. Values in parentheses are 959595% confidence-interval (CI) radii. Global and local refer to the checkpointing strategies and Silo metrics are measured as described in Section 3.3. Bold denotes the best performing federated strategy, while underlined values are second best. Asterisks imply a value is better than siloed training considering CIs. Note that each task has a different number of clients. For example, Fed-Heart-Disease has only four.",
        "table": "",
        "footnotes": "flamby_results_table\n\n\n\n\n\n\n\nFed-Heart-Disease: Acc.\nFed-IXI: Dice\nFed-ISIC2019: Balanced Acc.\n\n\nGlobal\nLocal\nGlobal\nLocal\nGlobal\nLocal\n\nFedAvg\n0.7240.7240.724 (0.0000.0000.000)\n0.7240.7240.724 (0.0000.0000.000)\n0.9845∗superscript0.98450.9845^{*} (0.00010.00010.0001)\n0.9846∗superscript0.98460.9846^{*} (0.00010.00010.0001)\n0.657¯¯0.657\\underline{0.657} (0.0040.0040.004)\n0.6440.6440.644 (0.0040.0040.004)\n\nFedAdam\n0.7190.7190.719 (0.0000.0000.000)\n0.7420.7420.742 (0.0000.0000.000)\n0.9811∗superscript0.98110.9811^{*} (0.00020.00020.0002)\n0.9811∗superscript0.98110.9811^{*} (0.00020.00020.0002)\n0.6470.6470.647 (0.0130.0130.013)\n0.6370.6370.637 (0.0120.0120.012)\n\nFedProx\n0.7160.7160.716 (0.0000.0000.000)\n0.7210.7210.721 (0.0000.0000.000)\n0.9842∗superscript0.98420.9842^{*} (0.00010.00010.0001)\n0.9844∗superscript0.98440.9844^{*} (0.00010.00010.0001)\n0.6330.6330.633 (0.0220.0220.022)\n0.6520.6520.652 (0.0220.0220.022)\n\nSCAFFOLD\n0.7110.7110.711 (0.0000.0000.000)\n0.6820.6820.682 (0.0000.0000.000)\n0.9827∗superscript0.98270.9827^{*} (0.00000.00000.0000)\n0.9827∗superscript0.98270.9827^{*} (0.00000.00000.0000)\n0.672∗superscript0.672\\mathbf{0.672}^{*} (0.0130.0130.013)\n0.6690.6690.669 (0.0100.0100.010)\n\nMOON\n0.7290.7290.729 (0.0090.0090.009)\n0.7410.7410.741 (0.0090.0090.009)\n0.9852∗superscript0.98520.9852^{*} (0.00010.00010.0001)\n0.9851∗superscript0.98510.9851^{*} (0.00020.00020.0002)\n0.6200.6200.620 (0.0190.0190.019)\n0.6040.6040.604 (0.0170.0170.017)\n\nFedPer\n–\n0.814¯∗superscript¯0.814\\underline{0.814}^{*} (0.0020.0020.002)\n–\n0.9852∗superscript0.98520.9852^{*} (0.00010.00010.0001)\n–\n0.6340.6340.634 (0.0140.0140.014)\n\nAPFL\n–\n0.801∗superscript0.8010.801^{*} (0.0060.0060.006)\n–\n0.9864∗superscript0.9864\\mathbf{0.9864}^{*} (0.00020.00020.0002)\n–\n0.6080.6080.608 (0.0110.0110.011)\n\nPerFCL\n–\n0.805∗superscript0.8050.805^{*} (0.0180.0180.018)\n–\n0.9847∗superscript0.98470.9847^{*} (0.00010.00010.0001)\n–\n0.5840.5840.584 (0.0190.0190.019)\n\nFENDA-FL\n–\n0.815∗superscript0.815\\mathbf{0.815}^{*} (0.0000.0000.000)\n–\n0.9856¯∗superscript¯0.9856\\underline{0.9856}^{*} (0.00010.00010.0001)\n–\n0.6070.6070.607 (0.0110.0110.011)\n\nSilo\n–\n0.7480.7480.748 (0.0000.0000.000)\n–\n0.97370.97370.9737 (0.00400.00400.0040)\n–\n0.6340.6340.634 (0.0250.0250.025)\n\nCentral\n–\n0.7320.7320.732 (0.0000.0000.000)\n–\n0.98320.98320.9832 (0.00020.00020.0002)\n–\n0.6830.6830.683 (0.0200.0200.020)\n\nClient 0\n–\n0.7350.7350.735 (0.0000.0000.000)\n–\n0.96480.96480.9648 (0.01230.01230.0123)\n–\n0.5490.5490.549 (0.0140.0140.014)\n\nClient 1\n–\n0.6490.6490.649 (0.0000.0000.000)\n–\n0.97220.97220.9722 (0.00040.00040.0004)\n–\n0.3870.3870.387 (0.0290.0290.029)\n\nClient 2\n–\n0.5890.5890.589 (0.0000.0000.000)\n–\n0.95500.95500.9550 (0.00150.00150.0015)\n–\n0.5540.5540.554 (0.0100.0100.010)\n\nClient 3\n–\n0.6290.6290.629 (0.0000.0000.000)\n–\n–\n–\n0.4440.4440.444 (0.0160.0160.016)\n\nClient 4\n–\n–\n–\n–\n–\n0.3260.3260.326 (0.0180.0180.018)\n\nClient 5\n–\n–\n–\n–\n–\n0.2850.2850.285 (0.0390.0390.039)\n\n\n",
        "references": [
            [
                "To address an important gap in the FL literature, we propose a straightforward checkpointing strategy and evaluation framework for FL models, building on the foundations in ",
                "(du Terrail et al., ",
                "2022a",
                ")",
                " and better aligning with standard ML model practices. In many works ",
                "(du Terrail et al., ",
                "2022a",
                "; Li et al., ",
                "2020",
                "; Karimireddy et al., ",
                "2020",
                "; Passban et al., ",
                "2022",
                ")",
                ", federated training proceeds for a fixed number of server rounds. Thereafter, the final model is evaluated on held-out data, representing either a central or distributed test set. However, as with standard ML training, the model produced at the end of a training trajectory is not always optimal due to phenomena such as over-fitting.",
                "We propose splitting each clients’ dataset into train and validation sets. In the experiments, the split used is 80–20. The validation splits enable two distinct checkpointing approaches. After each round of local training and aggregation, the clients evaluate the model on their local validation sets. In ",
                "local checkpointing",
                ", each client uses their local validation loss to determine whether a new checkpoint should be stored. Alternatively, in ",
                "global checkpointing",
                ", the client losses are aggregated by the server using a weighted average. This average is used to checkpoint a global model to be shared by all clients. Note that, in both cases, the best model is not necessarily the one produced by the final server round. In the local case, clients are free to keep the model they deem best. In the global strategy, all clients share the same model from some point in the server rounds. The personalized FL strategies do not have a single global model and only use local checkpointing.",
                "To evaluate checkpointed models, we advocate for at least three separate baselines to be considered, two of which are part of the FLamby benchmarks. Models trained on ",
                "central",
                " data are a common gold standard for FL performance. In the experiments, a single model is trained on data pooled from each of the clients and checkpointed based on a validation loss. The model is evaluated on each client’s test set and the performance uniformly averaged. In ",
                "local",
                " training, a client trains a model on its own training data. The model is shared with all clients and evaluated on their test data, including that of the original client. This setting mimics the scenario where, for example, a hospital distributes a model to other hospitals. Again, the measured metrics are averaged across clients. An important alternative setting, referred to here as ",
                "siloed",
                ", is one where each client trains its own model and evaluates on its local test data. The results are then averaged across the clients. This simulates a strong baseline where, for example, the problem is important enough that each hospital has collected its own data to train a model. If an FL model does not produce better performance, there is significantly less incentive to participate in collaborative training.",
                "To quantify stability and smooth randomness in training, five FL training runs with distinct validation splits are performed for each method. Within each run, the performance across clients is averaged. The mean and ",
                "95",
                "95",
                "95",
                "% confidence intervals for this average are reported. When local checkpointing is used, each client loads its respective model to be evaluated on that client’s test data."
            ]
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Performance metrics for GEMINI Mortality. Values in parentheses are 959595% confidence-interval (CI) radii. Global and local refer to the checkpointing strategies and Silo metrics are measured as described in Section 3.3. Bold denotes the best performing federated strategy, while underlined values are second best. Asterisks imply a value is better than siloed training considering CIs.",
        "table": "",
        "footnotes": "mortality_results_table\n\n\n\n\n\n\n\nMean Accuracy\nMean AUC/ROC\n\n\nGlobal\nLocal\nGlobal\nLocal\n\nFedAvg\n0.89760.89760.8976 (0.00020.00020.0002)\n0.89710.89710.8971 (0.00060.00060.0006)\n0.8180∗superscript0.81800.8180^{*} (0.00050.00050.0005)\n0.8177∗superscript0.81770.8177^{*} (0.00050.00050.0005)\n\nFedAdam\n0.89540.89540.8954 (0.00000.00000.0000)\n0.89540.89540.8954 (0.00000.00000.0000)\n0.81280.81280.8128 (0.00050.00050.0005)\n0.81230.81230.8123 (0.00080.00080.0008)\n\nFedProx\n0.89570.89570.8957 (0.00040.00040.0004)\n0.89560.89560.8956 (0.00030.00030.0003)\n0.8174∗superscript0.81740.8174^{*} (0.00040.00040.0004)\n0.8172∗superscript0.81720.8172^{*} (0.00060.00060.0006)\n\nSCAFFOLD\n0.89550.89550.8955 (0.00030.00030.0003)\n0.89540.89540.8954 (0.00000.00000.0000)\n0.8186∗superscript0.8186\\mathbf{0.8186}^{*} (0.00020.00020.0002)\n0.8184∗superscript0.81840.8184^{*} (0.00020.00020.0002)\n\nMOON\n0.89680.89680.8968 (0.00080.00080.0008)\n0.89700.89700.8970 (0.00020.00020.0002)\n0.8185¯∗superscript¯0.8185\\underline{0.8185}^{*} (0.00020.00020.0002)\n0.8181∗superscript0.81810.8181^{*} (0.00080.00080.0008)\n\nFedPer\n–\n0.89850.8985\\mathbf{0.8985} (0.00050.00050.0005)\n–\n0.8179∗superscript0.81790.8179^{*} (0.00060.00060.0006)\n\nAPFL\n–\n0.89780.89780.8978 (0.00040.00040.0004)\n–\n0.81380.81380.8138\n(0.00170.00170.0017)\n\nPerFCL\n–\n0.89790.89790.8979 (0.00080.00080.0008)\n–\n0.81170.81170.8117 (0.00120.00120.0012)\n\nFENDA-FL\n–\n0.8980¯¯0.8980\\underline{0.8980} (0.00040.00040.0004)\n–\n0.8181∗superscript0.81810.8181^{*} (0.00010.00010.0001)\n\nSilo\n–\n0.89780.89780.8978 (0.00090.00090.0009)\n–\n0.81400.81400.8140 (0.00120.00120.0012)\n\nCentral\n–\n0.89810.89810.8981 (0.00050.00050.0005)\n–\n0.81930.81930.8193 (0.00050.00050.0005)\n\nClient 0\n–\n0.89660.89660.8966 (0.00140.00140.0014)\n–\n0.80520.80520.8052 (0.00300.00300.0030)\n\nClient 1\n–\n0.89300.89300.8930 (0.00250.00250.0025)\n–\n0.76570.76570.7657 (0.00640.00640.0064)\n\nClient 2\n–\n0.89430.89430.8943 (0.00600.00600.0060)\n–\n0.75910.75910.7591 (0.01160.01160.0116)\n\nClient 3\n–\n0.89540.89540.8954 (0.00000.00000.0000)\n–\n0.79940.79940.7994 (0.00340.00340.0034)\n\nClient 4\n–\n0.89550.89550.8955 (0.0005)0.0005(0.0005)\n–\n0.79740.79740.7974 (0.00370.00370.0037)\n\nClient 5\n–\n0.89730.89730.8973 (0.0014)0.0014(0.0014)\n–\n0.78780.78780.7878 (0.00430.00430.0043)\n\nClient 6\n–\n0.89540.89540.8954 (0.0000)0.0000(0.0000)\n–\n0.80190.80190.8019 (0.00350.00350.0035)\n\n\n",
        "references": [
            [
                "The model used for Fed-ISIC2019 is EfficientNet-B0 ",
                "(Tan and Le, ",
                "2019",
                ")",
                ", which has been pretrained on ImageNet, with a linear final layer. The classifier layer for MOON and FedPer is defined as the last linear layer and the remainder of the model is the feature extractor. For APFL, the full model is used for the global and local modules. Finally, the PerFCL and FENDA-FL models remove the final layer and use the remaining architecture as the local and global feature extractors. The classification head is a two-layer DNN with a ReLU between the layers. For a fair comparison, the first ",
                "13",
                "13",
                "13",
                " layers of EfficientNet-B0 are frozen for APFL, PerFCL, and FENDA-FL.",
                "The results in Table ",
                "LABEL:flamby_results_table",
                " show that all FL strategies perform better than local models, with SCAFFOLD providing the best results. While it does not surpass centralized training, it is quite close. Further, each of the non-personalized FL methods equals or surpasses siloed evaluation, though only SCAFFOLD’s model is statistically significant. As with Fed-IXI, these results improve upon those of the original FLamby benchmark, where none of the FL methods showed strong performance. This led the authors to conclude that federated training was not a useful approach for this task. The results presented here compellingly support the opposite conclusion.",
                "For this task, each of the personalized FL approaches under-perform. While the results are better than any of the local models, non-personalized methods, with the exception of MOON, materially out-perform these techniques. Moreover, they fail to eclipse siloed accuracy. Some insight into where these approaches fall short is found in Figure ",
                "LABEL:fed_isic_generalization",
                ". SCAFFOLD outperforms the personalized models on test data drawn from Client 1 and Client 2. This is reversed when considering Client 4. However, the most significant difference is that all personalized models perform quite poorly on the test data of Client 5. Of these models, FedPer provides the best results, but is still substantially worse than SCAFFOLD.",
                "Notably, Client 5 has the smallest dataset by a large margin; see Table ",
                "LABEL:fl_dataset_distributions",
                " in the appendix. Moreover, the data appears to be of poor quality, with locally trained models only achieving a balanced accuracy of ",
                "0.425",
                "0.425",
                "0.425",
                " on the test set. Alternatively, models trained only on data from Client 0 generalize well to Client 5. We hypothesize that the low quality of Client 5’s data impedes personalized FL model training. In future work, we aim to consider ways of overcoming this issue. Some potential avenues include additional loss terms to ensure the global and local feature extractors remain individually useful for classification or using a global FL approach to initialize model components as a warm start for further fine-tuning. Interestingly, for this task, local checkpointing appears disadvantageous compared to global checkpointing."
            ]
        ]
    },
    "S4.T4": {
        "caption": "Table 4: Performance metrics for GEMINI Delirium. Values in parentheses are 959595% confidence-interval (CI) radii. Global and local refer to the checkpointing strategies and Silo metrics are measured as described in Section 3.3. Bold denotes the best performing federated strategy, while underlined values are second best. Asterisks imply a value is better than siloed training considering CIs.",
        "table": "",
        "footnotes": "delirium_results_table\n\n\n\n\n\n\n\nMean Accuracy\nMean AUC/ROC\n\n\nGlobal\nLocal\nGlobal\nLocal\n\nFedAvg\n0.79870.79870.7987 (0.01270.01270.0127)\n0.80250.80250.8025 (0.01300.01300.0130)\n0.8302∗superscript0.83020.8302^{*} (0.01460.01460.0146)\n0.8274∗superscript0.82740.8274^{*} (0.00780.00780.0078)\n\nFedAdam\n0.76890.76890.7689 (0.00360.00360.0036)\n0.76880.76880.7688 (0.01270.01270.0127)\n0.78970.78970.7897 (0.00780.00780.0078)\n0.78810.78810.7881 (0.00550.00550.0055)\n\nFedProx\n0.8095∗superscript0.8095\\mathbf{0.8095}^{*} (0.00360.00360.0036)\n0.80560.80560.8056 (0.00370.00370.0037)\n0.8488∗superscript0.84880.8488^{*} (0.00430.00430.0043)\n0.8504¯∗superscript¯0.8504\\underline{0.8504}^{*} (0.00420.00420.0042)\n\nSCAFFOLD\n0.74800.74800.7480 (0.00000.00000.0000)\n0.74800.74800.7480 (0.00000.00000.0000)\n0.54910.54910.5491 (0.04690.04690.0469)\n0.54910.54910.5491 (0.04690.04690.0469)\n\nMOON\n0.80350.80350.8035 (0.01030.01030.0103)\n0.8085¯¯0.8085\\underline{0.8085} (0.00860.00860.0086)\n0.8310∗superscript0.83100.8310^{*} (0.01000.01000.0100)\n0.8263∗superscript0.82630.8263^{*} (0.00770.00770.0077)\n\nFedPer\n–\n0.8082∗superscript0.80820.8082^{*} (0.00240.00240.0024)\n–\n0.8462∗superscript0.84620.8462^{*} (0.00350.00350.0035)\n\nAPFL\n–\n0.80310.80310.8031 (0.00520.00520.0052)\n–\n0.8430∗superscript0.84300.8430^{*} (0.00620.00620.0062)\n\nPerFCL\n–\n0.78930.78930.7893 (0.00770.00770.0077)\n–\n0.81110.81110.8111 (0.00890.00890.0089)\n\nFENDA-FL\n–\n0.80640.80640.8064 (0.00360.00360.0036)\n–\n0.8518∗superscript0.8518\\mathbf{0.8518}^{*} (0.00170.00170.0017)\n\nSilo\n–\n0.79360.79360.7936 (0.01020.01020.0102)\n–\n0.80370.80370.8037 (0.00720.00720.0072)\n\nCentral\n–\n0.81140.81140.8114 (0.00680.00680.0068)\n–\n0.84580.84580.8458 (0.00260.00260.0026)\n\nClient 0\n–\n0.76530.76530.7653 (0.00700.00700.0070)\n–\n0.79770.79770.7977 (0.01280.01280.0128)\n\nClient 1\n–\n0.77810.77810.7781 (0.00630.00630.0063)\n–\n0.77950.77950.7795 (0.01610.01610.0161)\n\nClient 2\n–\n0.78050.78050.7805 (0.00360.00360.0036)\n–\n0.8185∗superscript0.81850.8185^{*} (0.00210.00210.0021)\n\nClient 3\n–\n0.76340.76340.7634 (0.00620.00620.0062)\n–\n0.75390.75390.7539 (0.00550.00550.0055)\n\nClient 4\n–\n0.79240.79240.7924 (0.00610.00610.0061)\n–\n0.81790.81790.8179 (0.00760.00760.0076)\n\nClient 5\n–\n0.80280.80280.8028 (0.00400.00400.0040)\n–\n0.8429∗superscript0.84290.8429^{*} (0.00180.00180.0018)\n\n\n",
        "references": [
            [
                "The model used for Fed-ISIC2019 is EfficientNet-B0 ",
                "(Tan and Le, ",
                "2019",
                ")",
                ", which has been pretrained on ImageNet, with a linear final layer. The classifier layer for MOON and FedPer is defined as the last linear layer and the remainder of the model is the feature extractor. For APFL, the full model is used for the global and local modules. Finally, the PerFCL and FENDA-FL models remove the final layer and use the remaining architecture as the local and global feature extractors. The classification head is a two-layer DNN with a ReLU between the layers. For a fair comparison, the first ",
                "13",
                "13",
                "13",
                " layers of EfficientNet-B0 are frozen for APFL, PerFCL, and FENDA-FL.",
                "The results in Table ",
                "LABEL:flamby_results_table",
                " show that all FL strategies perform better than local models, with SCAFFOLD providing the best results. While it does not surpass centralized training, it is quite close. Further, each of the non-personalized FL methods equals or surpasses siloed evaluation, though only SCAFFOLD’s model is statistically significant. As with Fed-IXI, these results improve upon those of the original FLamby benchmark, where none of the FL methods showed strong performance. This led the authors to conclude that federated training was not a useful approach for this task. The results presented here compellingly support the opposite conclusion.",
                "For this task, each of the personalized FL approaches under-perform. While the results are better than any of the local models, non-personalized methods, with the exception of MOON, materially out-perform these techniques. Moreover, they fail to eclipse siloed accuracy. Some insight into where these approaches fall short is found in Figure ",
                "LABEL:fed_isic_generalization",
                ". SCAFFOLD outperforms the personalized models on test data drawn from Client 1 and Client 2. This is reversed when considering Client 4. However, the most significant difference is that all personalized models perform quite poorly on the test data of Client 5. Of these models, FedPer provides the best results, but is still substantially worse than SCAFFOLD.",
                "Notably, Client 5 has the smallest dataset by a large margin; see Table ",
                "LABEL:fl_dataset_distributions",
                " in the appendix. Moreover, the data appears to be of poor quality, with locally trained models only achieving a balanced accuracy of ",
                "0.425",
                "0.425",
                "0.425",
                " on the test set. Alternatively, models trained only on data from Client 0 generalize well to Client 5. We hypothesize that the low quality of Client 5’s data impedes personalized FL model training. In future work, we aim to consider ways of overcoming this issue. Some potential avenues include additional loss terms to ensure the global and local feature extractors remain individually useful for classification or using a global FL approach to initialize model components as a warm start for further fine-tuning. Interestingly, for this task, local checkpointing appears disadvantageous compared to global checkpointing."
            ]
        ]
    },
    "A0.T5": {
        "caption": "Table 5: Hyper-parameters tuned for each FL method and FLamby dataset pair. For the “Range” column, the sets progress in orders of magnitude. For example, 0.10.10.1 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4} is the set of parameters {0.1, 0.01, 1​e​-​31e-31\\mathrm{e}{\\text{-}3} 1​e​-​41e-41\\mathrm{e}{\\text{-}4}}. The “Best” hyper-parameter values are chosen based on the average aggregated validation loss during federated training over five different training runs.",
        "table": "",
        "footnotes": "flamby_dataset_hp_studies\n\n\n\n\n\nTask\n   Fed-Heart-Disease\nFed-IXI\n  Fed-ISIC2019\n\nMethod\nParameters\nRange\nBest\nRange\nBest\nRange\nBest\n\nFedAvg\nClient LR\n0.10.10.1 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n0.10.10.1\n0.10.10.1 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n0.10.10.1 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n\nFedAdam\nClient LR\n0.10.10.1 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n0.10.10.1 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n0.10.10.1 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n\nServer LR\n0.10.10.1 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n0.10.10.1\n1.01.01.0 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n0.010.010.01\n1.01.01.0 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n\nFedProx\nClient LR\n1​e​-​31e-31\\mathrm{e}{\\text{-}3} to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n0.10.10.1 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n0.10.10.1 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n\nμ𝜇\\mu\n1.01.01.0 to 0.010.010.01\n0.010.010.01\n1.01.01.0 to 1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n1.01.01.0 to 1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n0.10.10.1\n\nSCAFFOLD\nClient LR\n0.10.10.1 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n0.10.10.1\n0.10.10.1 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n0.10.10.1\n0.10.10.1 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n0.010.010.01\n\nServer LR\n0.10.10.1 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n0.10.10.1\n1.01.01.0 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n1.01.01.0\n1.01.01.0 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n1.01.01.0\n\nMOON\nClient LR\n0.10.10.1 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n0.010.010.01\n0.10.10.1 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n0.10.10.1 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n\nμ𝜇\\mu\n101010 to 1​e​-​31e-31\\mathrm{e}{\\text{-}3}, 555\n0.10.10.1\n101010 to 1​e​-​31e-31\\mathrm{e}{\\text{-}3}, 555\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n101010 to 1​e​-​31e-31\\mathrm{e}{\\text{-}3}, 555\n1.01.01.0\n\nFedPer\nClient LR\n0.10.10.1 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n0.10.10.1 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n0.10.10.1 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n\nAPFL\nClient LR\n0.10.10.1 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n0.10.10.1\n0.10.10.1 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n0.10.10.1 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n\nα𝛼\\alpha LR\n1.01.01.0 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n0.10.10.1\n1.01.01.0 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n1.01.01.0\n1.01.01.0 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n0.010.010.01\n\nPerFCL\nClient LR\n0.10.10.1 to 1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n0.10.10.1 to 1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n0.10.10.1 to 1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n\nμ𝜇\\mu\n1.01.01.0 to 0.010.010.01\n0.10.10.1\n1.01.01.0 to 0.010.010.01\n0.10.10.1\n1.01.01.0 to 0.010.010.01\n0.010.010.01\n\nγ𝛾\\gamma\n{1,5,10}1510\\{1,5,10\\}\n1.01.01.0\n{1,5,10}1510\\{1,5,10\\}\n101010\n{1,5,10}1510\\{1,5,10\\}\n101010\n\nFENDA-FL\nClient LR\n0.10.10.1 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n0.10.10.1 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n0.10.10.1 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n\n",
        "references": [
            [
                ":\n",
                "\\theoremsep",
                "\n",
                "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
                "\\jmlrvolume",
                "\n",
                "\\jmlryear",
                "\n",
                "\\jmlrsubmitted",
                "\n",
                "\\jmlrpublished",
                "\n",
                "\\jmlrworkshop",
                "Federated learning (FL) is increasingly being recognized as a key approach to overcoming the data silos that so frequently obstruct the training and deployment of machine-learning models in clinical settings. This work contributes to a growing body of FL research specifically focused on clinical applications along three important directions. First, we expand the FLamby benchmark ",
                "(du Terrail et al., ",
                "2022a",
                ")",
                " to include evaluation of personalized FL methods and demonstrate substantive performance improvements over the original results. Next, we advocate for a comprehensive checkpointing and evaluation framework for FL to reflect practical settings and provide multiple comparison baselines. Finally, we study an important ablation of PerFCL ",
                "(Zhang et al., ",
                "2022",
                ")",
                ". This ablation is a natural extension of FENDA ",
                "(Kim et al., ",
                "2016",
                ")",
                " to the FL setting. Experiments conducted on the FLamby benchmarks and GEMINI datasets ",
                "(Verma et al., ",
                "2017",
                ")",
                " show that the approach is robust to heterogeneous clinical data and often outperforms existing global and personalized FL techniques, including PerFCL."
            ]
        ]
    },
    "A1.T6": {
        "caption": "Table 6: Hyper-parameters tuned for each FL method, along with local and central training, across GEMINI tasks. For the “Range” column, the sets progress in orders of magnitude. For example, 0.010.010.01 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4} is the set of parameters {0.01, 1​e​-​31e-31\\mathrm{e}{\\text{-}3} 1​e​-​41e-41\\mathrm{e}{\\text{-}4}}. The “Best” hyper-parameter values are chosen based on the average aggregated validation loss during federated training over five different training runs.",
        "table": "",
        "footnotes": "gemini_dataset_hp_studies\n\n\n\n\n\n\nTask\n    Mortality\nDelirium\n\nMethod\nParams.\nRange\nBest\nRange\nBest\n\nFedAvg\nClient LR\n0.010.010.01 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n0.010.010.01 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n\nFedAdam\nClient LR\n0.010.010.01 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n0.010.010.01 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n\nServer LR\n1.01.01.0 to 1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n1.01.01.0 to 1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n\nFedProx\nClient LR\n1.01.01.0 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n0.010.010.01\n0.10.10.1 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n\nμ𝜇\\mu\n1.01.01.0 to 0.010.010.01\n0.10.10.1\n1.01.01.0 to 0.010.010.01\n0.10.10.1\n\nSCAFFOLD\nClient LR\n0.010.010.01 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n0.010.010.01\n0.010.010.01 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n\nServer LR\n1.01.01.0 to 1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n1.01.01.0\n1.01.01.0 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n\nMOON\nClient LR\n0.10.10.1 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n0.10.10.1 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n\nμ𝜇\\mu\n101010 to 1​e​-​31e-31\\mathrm{e}{\\text{-}3}, 555\n101010\n101010 to 1​e​-​31e-31\\mathrm{e}{\\text{-}3}, 555\n101010\n\nFedPer\nClient LR\n0.10.10.1 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n0.10.10.1 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n\nAPFL\nClient LR\n0.10.10.1 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n0.010.010.01\n0.010.010.01 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n\nα𝛼\\alpha LR\n1.01.01.0 to 1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n0.010.010.01\n1.01.01.0 to 1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n0.010.010.01\n\nPerFCL\nClient LR\n0.10.10.1 to 1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n0.10.10.1 to 1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n\nμ𝜇\\mu\n1.01.01.0 to 0.010.010.01\n0.10.10.1\n1.01.01.0 to 0.010.010.01\n1.01.01.0\n\nγ𝛾\\gamma\n{1,5,10}1510\\{1,5,10\\}\n101010\n{1,5,10}1510\\{1,5,10\\}\n1.01.01.0\n\nFENDA-FL\nClient LR\n0.010.010.01 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n0.010.010.01 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n\n\n\n\n\nTask\n    Mortality\nDelirium\n\nMethod\nParams.\nRange\nBest\nRange\nBest\n\n\n\nLocal 0\nLR\n0.010.010.01 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n0.010.010.01\n0.010.010.01 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n0.010.010.01\n\nLocal 1\nLR\n0.010.010.01 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n0.010.010.01 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n0.010.010.01\n\nLocal 2\nLR\n0.010.010.01 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n0.010.010.01\n0.010.010.01 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n\nLocal 3\nLR\n0.010.010.01 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n0.010.010.01\n0.010.010.01 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n\nLocal 4\nLR\n0.010.010.01 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n0.010.010.01\n0.010.010.01 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n\nLocal 5\nLR\n0.01 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n0.010.010.01\n0.010.010.01 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n1​e​-​31e-31\\mathrm{e}{\\text{-}3}\n\nLocal 6\nLR\n0.010.010.01 to 1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n0.010.010.01\n−-\n−-\n\nCentral\nLR\n0.010.010.01 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n0.010.010.01 to 1​e​-​51e-51\\mathrm{e}{\\text{-}5}\n1​e​-​41e-41\\mathrm{e}{\\text{-}4}\n\n\n",
        "references": [
            [
                "The Fed-ISIC2019 dataset is split across six different clients. Three of the client datasets are derived from the same source, the Medical University of Vienna, but were generated by different imaging devices. A weighted focal loss function ",
                "(Lin et al., ",
                "2020",
                ")",
                " is used to fine-tune the EfficientNet-B0 architectures and balanced accuracy is measured to assess performance. All training runs use a batch size of ",
                "64",
                "64",
                "64",
                ". Central and local training uses an AdamW optimizer with a learning rate of ",
                "5",
                "​",
                "e",
                "​",
                "-",
                "​",
                "4",
                "5",
                "e",
                "-",
                "4",
                "5\\mathrm{e}{\\text{-}4}",
                " over ",
                "20",
                "20",
                "20",
                " epochs. As seen in Table ",
                "LABEL:fl_dataset_distributions",
                ", the number of data-points held by each client varies dramatically."
            ]
        ]
    },
    "A1.T7": {
        "caption": "Table 7: Train and test set sizes across clients in the FLamby datasets, reproduced from (du Terrail et al., 2022a), and the GEMINI datasets. Each GEMINI client represents a separate Canadian institution.",
        "table": "",
        "footnotes": "fl_dataset_distributions\n\n\n\n\n\nDataset\nNumber\nTrain Size\nTest Size\n\n\n\n\n0\n199\n104\n\nFed-Heart-\n1\n172\n89\n\nDisease\n2\n30\n16\n\n\n3\n85\n45\n\nFed-IXI\n0\n262\n66\n\n1\n142\n36\n\n2\n54\n14\n\nFed-ISIC2019\n0\n9930\n2483\n\n1\n3163\n791\n\n2\n2691\n672\n\n3\n1807\n452\n\n4\n655\n164\n\n5\n351\n88\n\n\n\n\n\n\nDataset\nNumber\nTrain Size\nTest Size\n\n\n\nGEMINI Mortality\n0\n13234\n3308\n\n1\n21018\n5254\n\n2\n14180\n3545\n\n3\n15251\n3813\n\n\n4\n14441\n3610\n\n\n5\n12718\n3180\n\n\n6\n23820\n5955\n\nGEMINI Delirium\n0\n660\n101\n\n1\n590\n99\n\n2\n1218\n199\n\n\n3\n494\n76\n\n\n4\n574\n109\n\n\n5\n1392\n258\n\n",
        "references": [
            [
                "The Fed-ISIC2019 dataset is split across six different clients. Three of the client datasets are derived from the same source, the Medical University of Vienna, but were generated by different imaging devices. A weighted focal loss function ",
                "(Lin et al., ",
                "2020",
                ")",
                " is used to fine-tune the EfficientNet-B0 architectures and balanced accuracy is measured to assess performance. All training runs use a batch size of ",
                "64",
                "64",
                "64",
                ". Central and local training uses an AdamW optimizer with a learning rate of ",
                "5",
                "​",
                "e",
                "​",
                "-",
                "​",
                "4",
                "5",
                "e",
                "-",
                "4",
                "5\\mathrm{e}{\\text{-}4}",
                " over ",
                "20",
                "20",
                "20",
                " epochs. As seen in Table ",
                "LABEL:fl_dataset_distributions",
                ", the number of data-points held by each client varies dramatically."
            ]
        ]
    },
    "A3.T8": {
        "caption": "Table 8: Trainable parameter counts for the model architectures used in the FLamby and GEMINI experiments. Standard refers to models used in all settings outside of APFL, FENDA-FL, and PerFCL.",
        "table": "",
        "footnotes": "experimental_model_parameters\n\n\n\n\n\nDataset\nStandard\nAPFL\nFENDA-FL\nPerFCL\n\n\n\nFed-Heart-Disease\n141414 or 151151151\n152152152\n151151151\n151151151\n\nFed-IXI\n1,106,52011065201,106,520\n984,624984624984,624\n984,620984620984,620\n984,620984620984,620\n\nFed-ISIC2019\n4,017,79640177964,017,796\n4,631,08846310884,631,088\n4,775,01647750164,775,016\n4,775,01647750164,775,016\n\nMortality\n222,784222784222,784\n222,784222784222,784\n222,784222784222,784\n222,784222784222,784\n\nDelirium\n8,988,73689887368,988,736\n8,988,73689887368,988,736\n8,717,44887174488,717,448\n8,717,44887174488,717,448\n\n",
        "references": [
            [
                "Table ",
                "LABEL:experimental_model_parameters",
                " provides a comparison of the trainable parameters present in the models used in the FLamby and GEMINI experiments. The “standard” column refers to models trained in all settings, excluding APFL, FENDA-FL, and PerFCL. Note that both FedPer and MOON use the ",
                "151",
                "151",
                "151",
                " parameter models for Fed-Heart-Disease to facilitate feature extraction. For a fair comparison, we have sought to keep the number of trainable parameters across different architectures close to ensure that models for different methods do not have out-sized expressiveness."
            ]
        ]
    },
    "A5.T9": {
        "caption": "Table 9: Results corresponding to different FENDA-FL architecture variations on the Delirium task. “Layers (G, L)” and “Latent (G, L)” refer to the number of dense layers assigned to the global and local feature extractors, respectively, along with the size of the corresponding latent spaces.",
        "table": "",
        "footnotes": "delirium_architecture_perturbations\n\n\n\n\n\nLayers (G, L)\nLatent (G, L)\nParameters\nAcc.\nAUC/ROC\n\n\n\n444, 444\n646464, 646464\n8,631,42486314248,631,424\n0.79260.79260.7926 (0.00710.00710.0071)\n0.82220.82220.8222 (0.00360.00360.0036)\n\n444, 222\n646464, 646464\n8,729,60087296008,729,600\n0.80240.80240.8024 (0.00370.00370.0037)\n0.83680.83680.8368 (0.00390.00390.0039)\n\n444, 222\n128128128, 888\n8,717,44887174488,717,448\n0.80640.8064\\mathbf{0.8064} (0.00360.00360.0036)\n0.85180.8518\\mathbf{0.8518} (0.00170.00170.0017)\n\n222, 444\n646464, 646464\n8,729,60087296008,729,600\n0.79350.79350.7935 (0.00730.00730.0073)\n0.81530.81530.8153 (0.00590.00590.0059)\n\n222, 444\n888, 128128128\n8,717,44887174488,717,448\n0.79350.79350.7935 (0.00730.00730.0073)\n0.81530.81530.8153 (0.00590.00590.0059)\n\n",
        "references": [
            [
                "As discussed in Section ",
                "3.1",
                ", an important advantage of the FENDA-FL setup is that the local and global feature extraction modules need not be symmetric with equal latent space sizes. This allows for the injection of inductive bias through architecture choice. For example, in situations where global models perform well, one may assign more parameters and a larger latent space to FENDA-FL’s global feature extractor. In this section, we show that such choices can materially boost performance.",
                "Results for variations in the FENDA-FL model architecture for the Delirium task are reported in Table ",
                "LABEL:delirium_architecture_perturbations",
                ". Aside from FENDA-FL, the best performing strategy for this task is FedProx with an accuracy of ",
                "0.8095",
                "0.8095",
                "0.8095",
                " and AUC/ROC of ",
                "0.8504",
                "0.8504",
                "0.8504",
                ", using a model with ",
                "8",
                ",",
                "988",
                ",",
                "736",
                "8",
                "988",
                "736",
                "8,988,736",
                " parameters. From the table, it is evident that assigning more layers and a larger latent space to the global module is beneficial. Given that a global model is performing this task well, such an emphasis makes sense. However, the use of a, albeit smaller, personal module allows FENDA-FL to approach FedProx in terms of accuracy and surpass it in terms of AUC/ROC, despite having fewer parameters. Note that, in the imbalanced layer setting, the number of parameters lost, through reducing the depth of one module, are approximately transferred to the deeper module by increasing the layer sizes.",
                "Table ",
                "LABEL:fedixi_architecture_perturbations",
                " shows results for FENDA-FL architecture perturbations for the Fed-IXI task. While the performance changes are fairly small, there is a slight advantage to using a larger local feature extraction module for this task. Doing so brings the FENDA-FL model results in line with the best performing strategy, APFL, for this problem. Note that a larger FENDA-FL model is used to allow for latent space imbalance in the U-Net. Note also that APFL does not appear to benefit from the additional parameters."
            ]
        ]
    },
    "A5.T10": {
        "caption": "Table 10: Results corresponding to different FENDA-FL architecture variations on the Fed-IXI task. “Depth (G, L)” and “Latent (G, L)” refer to the number of projection layers used in the global and local U-net feature extractors, respectively, along with the size of the latent space for each voxel.",
        "table": "",
        "footnotes": "fedixi_architecture_perturbations\n\n\n\n\n\nStrategy\nDepth (G, L)\nLatent (G, L)\nParameters\nDice\n\n\n\nAPFL\n333, 333\n111111, 111111\n1,859,92818599281,859,928\n0.98550.98550.9855 (0.00010.00010.0001)\n\n333, 333\n888, 888\n984,624984624984,624\n0.98640.98640.9864 (0.00020.00020.0002)\n\nFENDA-FL\n333, 333\n111111, 111111\n1,859,92418599241,859,924\n0.98590.98590.9859 (0.00010.00010.0001)\n\n333, 222\n141414, 141414\n1,824,74018247401,824,740\n0.98640.98640.9864 (0.00000.00000.0000)\n\n222, 333\n141414, 141414\n1,824,74018247401,824,740\n0.98650.9865\\mathbf{0.9865} (0.00000.00000.0000)\n\n333, 222\n161616, 888\n2,070,69220706922,070,692\n0.98590.98590.9859 (0.00000.00000.0000)\n\n222, 333\n888, 161616\n2,070,69220706922,070,692\n0.98600.98600.9860 (0.00000.00000.0000)\n\n",
        "references": [
            [
                "As discussed in Section ",
                "3.1",
                ", an important advantage of the FENDA-FL setup is that the local and global feature extraction modules need not be symmetric with equal latent space sizes. This allows for the injection of inductive bias through architecture choice. For example, in situations where global models perform well, one may assign more parameters and a larger latent space to FENDA-FL’s global feature extractor. In this section, we show that such choices can materially boost performance.",
                "Results for variations in the FENDA-FL model architecture for the Delirium task are reported in Table ",
                "LABEL:delirium_architecture_perturbations",
                ". Aside from FENDA-FL, the best performing strategy for this task is FedProx with an accuracy of ",
                "0.8095",
                "0.8095",
                "0.8095",
                " and AUC/ROC of ",
                "0.8504",
                "0.8504",
                "0.8504",
                ", using a model with ",
                "8",
                ",",
                "988",
                ",",
                "736",
                "8",
                "988",
                "736",
                "8,988,736",
                " parameters. From the table, it is evident that assigning more layers and a larger latent space to the global module is beneficial. Given that a global model is performing this task well, such an emphasis makes sense. However, the use of a, albeit smaller, personal module allows FENDA-FL to approach FedProx in terms of accuracy and surpass it in terms of AUC/ROC, despite having fewer parameters. Note that, in the imbalanced layer setting, the number of parameters lost, through reducing the depth of one module, are approximately transferred to the deeper module by increasing the layer sizes.",
                "Table ",
                "LABEL:fedixi_architecture_perturbations",
                " shows results for FENDA-FL architecture perturbations for the Fed-IXI task. While the performance changes are fairly small, there is a slight advantage to using a larger local feature extraction module for this task. Doing so brings the FENDA-FL model results in line with the best performing strategy, APFL, for this problem. Note that a larger FENDA-FL model is used to allow for latent space imbalance in the U-Net. Note also that APFL does not appear to benefit from the additional parameters."
            ]
        ]
    },
    "A5.T11": {
        "caption": "Table 11: Checkpoint ablation results comparing federated checkpointing strategies with saving models after a fixed number of FL rounds. Values in parentheses are 959595% confidence-interval radii.",
        "table": "",
        "footnotes": "checkpoint_ablation_results\n\n\n\n\n\n\n\nFed-Heart-Disease: Mean Accuracy\nFed-IXI Optimal Parameters: Mean Dice\n\n\nLatest\nGlobal\nLocal\nLatest\nGlobal\nLocal\n\nFedAvg\n0.7060.7060.706 (0.0000.0000.000)\n0.7240.724\\mathbf{0.724} (0.0000.0000.000)\n0.7240.724\\mathbf{0.724} (0.0000.0000.000)\n0.98450.98450.9845 (0.00020.00020.0002)\n0.98450.98450.9845 (0.00010.00010.0001)\n0.98460.9846\\mathbf{0.9846} (0.00010.00010.0001)\n\nFedAdam\n0.7110.7110.711 (0.0000.0000.000)\n0.7190.7190.719 (0.0000.0000.000)\n0.7420.742\\mathbf{0.742} (0.0000.0000.000)\n0.98150.9815\\mathbf{0.9815} (0.00010.00010.0001)\n0.98110.98110.9811 (0.00020.00020.0002)\n0.98110.98110.9811 (0.00020.00020.0002)\n\nFedProx\n0.7030.7030.703 (0.0000.0000.000)\n0.7160.7160.716 (0.0000.0000.000)\n0.7210.721\\mathbf{0.721} (0.0000.0000.000)\n0.98410.98410.9841 (0.00010.00010.0001)\n0.98420.98420.9842 (0.00010.00010.0001)\n0.98440.9844\\mathbf{0.9844} (0.00010.00010.0001)\n\nSCAFFOLD\n0.7170.717\\mathbf{0.717} (0.0000.0000.000)\n0.7110.7110.711 (0.0000.0000.000)\n0.6820.6820.682 (0.0000.0000.000)\n0.98280.9828\\mathbf{0.9828} (0.00000.00000.0000)\n0.98270.98270.9827 (0.00000.00000.0000)\n0.98270.98270.9827 (0.00000.00000.0000)\n\nMOON\n0.70290.70290.7029 (0.0190.0190.019)\n0.7290.7290.729 (0.0090.0090.009)\n0.7410.741\\mathbf{0.741} (0.0090.0090.009)\n0.98510.98510.9851 (0.00020.00020.0002)\n0.98520.9852\\mathbf{0.9852} (0.00010.00010.0001)\n0.98510.98510.9851 (0.00020.00020.0002)\n\nFedPer\n0.8140.814\\mathbf{0.814} (0.0030.0030.003)\n–\n0.8140.814\\mathbf{0.814} (0.0020.0020.002)\n0.98520.9852\\mathbf{0.9852} (0.00030.00030.0003)\n–\n0.98470.98470.9847 (0.00010.00010.0001)\n\nAPFL\n0.7770.7770.777 (0.0130.0130.013)\n–\n0.8010.801\\mathbf{0.801} (0.0060.0060.006)\n0.98580.98580.9858 (0.00000.00000.0000)\n–\n0.98640.9864\\mathbf{0.9864} (0.00020.00020.0002)\n\nPerFCL\n0.8060.806\\mathbf{0.806} (0.0100.0100.010)\n–\n0.8050.8050.805 (0.0180.0180.018)\n0.98430.98430.9843 (0.00040.00040.0004)\n–\n0.98470.9847\\mathbf{0.9847} (0.00010.00010.0001)\n\nFENDA-FL\n0.8020.8020.802 (0.0100.0100.010)\n–\n0.8150.815\\mathbf{0.815} (0.0000.0000.000)\n0.98480.98480.9848 (0.00260.00260.0026)\n–\n0.98560.9856\\mathbf{0.9856} (0.00010.00010.0001)\n\n\nFed-IXI Sub-Optimal Parameters: Mean Dice\n\n\n\n\n\nLatest\nGlobal\nLocal\n\n\n\n\nFedAvg\n0.97660.97660.9766 (0.00060.00060.0006)\n0.97880.9788\\mathbf{0.9788} (0.00060.00060.0006)\n0.97880.9788\\mathbf{0.9788} (0.00060.00060.0006)\n\n\n\n\nFedAdam\n0.92020.92020.9202 (0.00910.00910.0091)\n0.94830.94830.9483 (0.00560.00560.0056)\n0.94880.9488\\mathbf{0.9488} (0.00540.00540.0054)\n\n\n\n\nFedProx\n0.92950.92950.9295 (0.03560.03560.0356)\n0.95480.95480.9548 (0.01310.01310.0131)\n0.95580.9558\\mathbf{0.9558} (0.01360.01360.0136)\n\n\n\n\nSCAFFOLD\n0.43600.4360\\mathbf{0.4360} (0.00000.00000.0000)\n0.43600.4360\\mathbf{0.4360} (0.00000.00000.0000)\n0.43600.4360\\mathbf{0.4360} (0.00000.00000.0000)\n\n\n\n\nMOON\n0.91770.91770.9177 (0.11550.11550.1155)\n0.97720.97720.9772 (0.00220.00220.0022)\n0.97770.9777\\mathbf{0.9777} (0.00150.00150.0015)\n\n\n\n\nFedPer\n0.97540.97540.9754 (0.00190.00190.0019)\n–\n0.97790.9779\\mathbf{0.9779} (0.00150.00150.0015)\n\n\n\n\nAPFL\n0.97800.97800.9780 (0.00070.00070.0007)\n–\n0.97930.9793\\mathbf{0.9793} (0.00050.00050.0005)\n\n\n\n\nPerFCL\n0.98530.9853\\mathbf{0.9853} (0.00050.00050.0005)\n–\n0.97110.97110.9711 (0.02820.02820.0282)\n\n\n\n\nFENDA-FL\n0.97050.97050.9705 (0.00750.00750.0075)\n–\n0.97900.9790\\mathbf{0.9790} (0.00040.00040.0004)\n\n\n\n\n\n",
        "references": [
            [
                "As discussed in Section ",
                "3.1",
                ", an important advantage of the FENDA-FL setup is that the local and global feature extraction modules need not be symmetric with equal latent space sizes. This allows for the injection of inductive bias through architecture choice. For example, in situations where global models perform well, one may assign more parameters and a larger latent space to FENDA-FL’s global feature extractor. In this section, we show that such choices can materially boost performance.",
                "Results for variations in the FENDA-FL model architecture for the Delirium task are reported in Table ",
                "LABEL:delirium_architecture_perturbations",
                ". Aside from FENDA-FL, the best performing strategy for this task is FedProx with an accuracy of ",
                "0.8095",
                "0.8095",
                "0.8095",
                " and AUC/ROC of ",
                "0.8504",
                "0.8504",
                "0.8504",
                ", using a model with ",
                "8",
                ",",
                "988",
                ",",
                "736",
                "8",
                "988",
                "736",
                "8,988,736",
                " parameters. From the table, it is evident that assigning more layers and a larger latent space to the global module is beneficial. Given that a global model is performing this task well, such an emphasis makes sense. However, the use of a, albeit smaller, personal module allows FENDA-FL to approach FedProx in terms of accuracy and surpass it in terms of AUC/ROC, despite having fewer parameters. Note that, in the imbalanced layer setting, the number of parameters lost, through reducing the depth of one module, are approximately transferred to the deeper module by increasing the layer sizes.",
                "Table ",
                "LABEL:fedixi_architecture_perturbations",
                " shows results for FENDA-FL architecture perturbations for the Fed-IXI task. While the performance changes are fairly small, there is a slight advantage to using a larger local feature extraction module for this task. Doing so brings the FENDA-FL model results in line with the best performing strategy, APFL, for this problem. Note that a larger FENDA-FL model is used to allow for latent space imbalance in the U-Net. Note also that APFL does not appear to benefit from the additional parameters."
            ]
        ]
    }
}