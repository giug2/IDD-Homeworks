{
    "id_table_1": {
        "caption": "Table 1 :  The MAE of three models trained by MLE, (A)SM, and (A)WSM on the synthetic dataset. For the 2-variate processes, we only present the estimation results for some parameters here. The results for other parameters can be found in  Table   3 .",
        "table": "S3.E8",
        "footnotes": [
            ""
        ],
        "references": [
            "For general Poisson processes,  Equation   11  is always valid with a suitable weight function. Thus, we do not need to worry about the issues of failure that may arise when using  Equation   7 .",
            "However, the same issue as in the Poisson process arises here. The regularity conditions required to eliminate the unknown data distribution do not hold. Therefore, we cannot derive the implicit ASM in  Equation   13  based on the explicit ASM in  Equation   12 .",
            "Generally speaking, for most Hawkes processes, the sum of the last two terms in  Equation   14  still contains    \\theta italic_ , even for a common Hawkes process with an exponential decay triggering kernel. We illustrate this example in  Section   6.2 . This implies that  J ASM subscript J ASM \\mathcal{J}_{\\text{ASM}} caligraphic_J start_POSTSUBSCRIPT ASM end_POSTSUBSCRIPT  fails for general Hawkes processes.",
            "Assume that all functions and expectations in  L AWSM  (  ) subscript L AWSM  \\mathcal{L}_{\\text{AWSM}}(\\theta) caligraphic_L start_POSTSUBSCRIPT AWSM end_POSTSUBSCRIPT ( italic_ )  and  J AWSM  (  ) subscript J AWSM  \\mathcal{J}_{\\text{AWSM}}(\\theta) caligraphic_J start_POSTSUBSCRIPT AWSM end_POSTSUBSCRIPT ( italic_ )  are well defined,  Equation   15  are satisfied, we have,",
            "For general Hawkes processes,  Equation   17  is always valid with a suitable weight function. Thus, we do not need to worry about the issues of failure that may arise when using  Equation   13 .",
            "Under mild regularity  Assumptions   C.1 ,  C.2  and  C.3 , we have   ^  p   p  ^  superscript  \\hat{\\theta}\\xrightarrow{p}\\theta^{*} over^ start_ARG italic_ end_ARG start_ARROW overitalic_p  end_ARROW italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  as  M    M M\\rightarrow\\infty italic_M   .",
            "In  Sections   3  and  4 , we only provide the conditions that the weight function needs to satisfy. In fact, there are many weight functions that satisfy these conditions. The optimal weight function should minimize the error bound in  Equation   19 , which is equivalent to minimizing the coefficient    ( h , A , B ) C h  h A B subscript C h \\frac{\\Gamma(\\mathbf{h},A,B)}{C_{\\mathbf{h}}} divide start_ARG roman_ ( bold_h , italic_A , italic_B ) end_ARG start_ARG italic_C start_POSTSUBSCRIPT bold_h end_POSTSUBSCRIPT end_ARG . The numerator cannot be analytically computed as it involves an unknown distribution  p  ( T ) p T p(\\mathcal{T}) italic_p ( caligraphic_T ) , but we can maximize the denominator  C h subscript C h C_{\\mathbf{h}} italic_C start_POSTSUBSCRIPT bold_h end_POSTSUBSCRIPT  in a predefined function family.",
            "where  J CE  (  ) subscript J CE  {\\mathcal{J}}_{\\text{CE}}(\\theta) caligraphic_J start_POSTSUBSCRIPT CE end_POSTSUBSCRIPT ( italic_ )  is the cross-entropy loss defined in  Equation   18 .",
            "In  Table   1 , we report the MAE of parameter estimates for three models trained by MLE, (A)SM, and (A)WSM on the synthetic dataset. We can see that both MLE and (A)WSM achieve small MAE on three types of data. However, the MAE of (A)SM is large. As we have theoretically demonstrated earlier, this is because MLE and (A)WSM estimators are consistent. In contrast, (A)SM, due to the absence of the required regularity conditions in the three cases, has an incomplete objective and cannot accurately estimate parameters. In  Figure   1 , we showcase the learned intensity functions. Both MLE and (A)WSM successfully captured the ground truth, while (A)SM fails.",
            "In recent years, many deep point process models have been proposed. Here, we focus on two of the most popular attention-based Hawkes process models:  SAHP   [ 23 ]  and  THP   [ 27 ] . We deploy AWSM and ASM on THP and SAHP. For each dataset, we train 3 seeds with the same epochs and report the mean and standard deviation of the best TLL and ACC. When using MLE, we adopt numerical integration to calculate the intensity integral. To ensure model accuracy, the number of integration nodes is set to be large enough as we sample 10 nodes between every two adjacent events. When using DSM, we tune the variance of noise for better results. When using AWSM, since for real datasets, the true observation endpoint  T T T italic_T  is unknown. We choose the maximum event time of each batch as the observation endpoint for weight function  h 0 superscript h 0 \\bm{h}^{0} bold_italic_h start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT . This may lead to unsatisfying results since real datasets may not be sampled during a unified time window. We provide a remedy for this as discussed in  Section   D.1 . Details of training and testing hyperparameters are provided in  Section   D.2 .",
            "The key advantage of (A)WSM over MLE is its avoidance of computing intensity integrals, which can be computationally intensive for complex point process models and impact MLE accuracy. We evaluate the test log-likelihood of MLE and AWSM on the Exp-Hawkes dataset as the number of integration nodes varies. As shown in  Figure   1 , with a limited number of nodes, MLE is faster but exhibits substantial estimation errors. Increasing the number of nodes reduces the error but significantly increases computation time. In this scenario, AWSM is much faster than MLE with the same accuracy, thus offering better computational efficiency.",
            "Though we provide theoretical insight into the choice of an optimal weight function for AWSM, its validity still needs to be testified by experiments. Here, we compare the near-optimal weight  h 0 superscript h 0 \\bm{h}^{0} bold_italic_h start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT  with natural weight  h 1 superscript h 1 \\bm{h}^{1} bold_italic_h start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT  and squareroot weight  h 2 superscript h 2 \\bm{h}^{2} bold_italic_h start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  satisfying  Equation   15 ,",
            "The current limitation of the methodology is that some real data are collected from multiple time intervals  [ 0 , T 1 ] , ... , [ 0 , T L ] 0 subscript T 1 ... 0 subscript T L [0,T_{1}],\\ldots,[0,T_{L}] [ 0 , italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ] , ... , [ 0 , italic_T start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT ]  or collated in a fixed time interval  [ 0 , T ] 0 T [0,T] [ 0 , italic_T ]  with unknown  T T T italic_T . However, for a score matching to be valid, the required weight function must involve knowledge of  T T T italic_T . Currently, our remedy including approximate  T T T italic_T  or performing data truncation as discussed in  Section   D.1 .",
            "The proof is basically the same as the proof of  Proposition   3.1 . We first expand the expectation and consider the cross-term in  L WSM  (  ) subscript L WSM  \\mathcal{L}_{\\text{WSM}}(\\theta) caligraphic_L start_POSTSUBSCRIPT WSM end_POSTSUBSCRIPT ( italic_ ) ,",
            "We use  Lemma   B.1  to the cross term of  L ASM  (  ) subscript L ASM  \\mathcal{L}_{\\text{ASM}}(\\theta) caligraphic_L start_POSTSUBSCRIPT ASM end_POSTSUBSCRIPT ( italic_ )  and obtain,",
            "For the fifth equation, we simply rearrange the terms. We recall that the notation  p  ( T : 0 ) p subscript T : absent 0 p(\\mathcal{T}_{:0}) italic_p ( caligraphic_T start_POSTSUBSCRIPT : 0 end_POSTSUBSCRIPT )  equals one. For the last equation, we use  Lemma   B.1  again. This will be sufficient to complete the proof.",
            "The proof is basically the same as the proof of  Proposition   4.1 . We first use the  Lemma   B.1  to the cross term of  L AWSM  (  ) subscript L AWSM  \\mathcal{L}_{\\text{AWSM}}(\\theta) caligraphic_L start_POSTSUBSCRIPT AWSM end_POSTSUBSCRIPT ( italic_ ) ,",
            "Between the second and the third line above, we omit the steps used in the derivation of Proposition 4.1 to make it concise. For the term in the third line above, it will be eliminated using  Equation   15 . For the term in the fourth line above, using Lemma B.1, we have:",
            "The existence of the expectation is ensured by the last two terms in  Equation   15 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  The TLL and ACC of two attention-based deep Hawkes process models trained by MLE and AWSM on four datasets. Because ASM estimator completely fails, we do not report its results.",
        "table": "S3.E10",
        "footnotes": [],
        "references": [
            "However, the same issue as in the Poisson process arises here. The regularity conditions required to eliminate the unknown data distribution do not hold. Therefore, we cannot derive the implicit ASM in  Equation   13  based on the explicit ASM in  Equation   12 .",
            "Generally speaking, for most Hawkes processes, the sum of the last two terms in  Equation   14  still contains    \\theta italic_ , even for a common Hawkes process with an exponential decay triggering kernel. We illustrate this example in  Section   6.2 . This implies that  J ASM subscript J ASM \\mathcal{J}_{\\text{ASM}} caligraphic_J start_POSTSUBSCRIPT ASM end_POSTSUBSCRIPT  fails for general Hawkes processes.",
            "Under mild regularity  Assumptions   C.1 ,  C.2  and  C.3 , we have   ^  p   p  ^  superscript  \\hat{\\theta}\\xrightarrow{p}\\theta^{*} over^ start_ARG italic_ end_ARG start_ARROW overitalic_p  end_ARROW italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  as  M    M M\\rightarrow\\infty italic_M   .",
            "Given that   ^ ^  \\hat{\\theta} over^ start_ARG italic_ end_ARG  converges to    superscript  \\theta^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  in probability, combined with  Assumptions   5.2  and  5.3 , for   < C  K   r 2   1    ( h , A , B ) C h  C subscript K  r superscript 2  1  h A B subscript C h \\delta<CK_{\\alpha}\\frac{\\sqrt{r}}{2^{\\alpha-1}}\\frac{\\Gamma(\\mathbf{h},A,B)}{C% _{\\mathbf{h}}} italic_ < italic_C italic_K start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT divide start_ARG square-root start_ARG italic_r end_ARG end_ARG start_ARG 2 start_POSTSUPERSCRIPT italic_ - 1 end_POSTSUPERSCRIPT end_ARG divide start_ARG roman_ ( bold_h , italic_A , italic_B ) end_ARG start_ARG italic_C start_POSTSUBSCRIPT bold_h end_POSTSUBSCRIPT end_ARG , we have",
            "where  H H \\mathcal{H} caligraphic_H  is a family of functions that is rigorously defined in  Equation   27 .",
            "Combined with  Assumption   5.2 , it can be observed that  h 0 superscript h 0 \\mathbf{h}^{0} bold_h start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT  maximizes  C h subscript C h C_{\\mathbf{h}} italic_C start_POSTSUBSCRIPT bold_h end_POSTSUBSCRIPT  in  H H \\mathcal{H} caligraphic_H . Though it does not necessarily optimize    ( h , A , B ) C h  h A B subscript C h \\frac{\\Gamma(\\mathbf{h},A,B)}{C_{\\mathbf{h}}} divide start_ARG roman_ ( bold_h , italic_A , italic_B ) end_ARG start_ARG italic_C start_POSTSUBSCRIPT bold_h end_POSTSUBSCRIPT end_ARG , it is an adequate choice without using any information on  p  ( T ) p T p(\\mathcal{T}) italic_p ( caligraphic_T ) . We also discuss it heuristically in  Section   C.4 . It is worth noting that  h n 0 superscript subscript h n 0 h_{n}^{0} italic_h start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT  is not continuously differentiable; however, it is weakly differentiable. Its weak derivative is continuous, allowing both integration by parts and statistical theory to hold. In subsequent experiments, we consistently employ this optimal weight function when  T T T italic_T  is available or can be approximated for the dataset.",
            "In recent years, many deep point process models have been proposed. Here, we focus on two of the most popular attention-based Hawkes process models:  SAHP   [ 23 ]  and  THP   [ 27 ] . We deploy AWSM and ASM on THP and SAHP. For each dataset, we train 3 seeds with the same epochs and report the mean and standard deviation of the best TLL and ACC. When using MLE, we adopt numerical integration to calculate the intensity integral. To ensure model accuracy, the number of integration nodes is set to be large enough as we sample 10 nodes between every two adjacent events. When using DSM, we tune the variance of noise for better results. When using AWSM, since for real datasets, the true observation endpoint  T T T italic_T  is unknown. We choose the maximum event time of each batch as the observation endpoint for weight function  h 0 superscript h 0 \\bm{h}^{0} bold_italic_h start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT . This may lead to unsatisfying results since real datasets may not be sampled during a unified time window. We provide a remedy for this as discussed in  Section   D.1 . Details of training and testing hyperparameters are provided in  Section   D.2 .",
            "In  Table   2 , we report the performance of SAHP and THP trained using three different methods, namely MLE, AWSM, and DSM, on four datasets. It is evident from the results that models trained with MLE and AWSM exhibit very similar performance in terms of both TLL and ACC on the test data. This indicates consistency between MLE and AWSM, as they yield comparable model parameters. For DSM, it is significantly inferior to the performance of MLE and AWSM. This may result from the fact that the DSM objective is a biased estimation of the original SM objective and fails to produce consistent estimation when   > 0  0 \\sigma>0 italic_ > 0  as discussed in  [ 20 ] . For ASM, it completely fails in the scenarios mentioned above. It is unable to estimate the correct parameters, and its results are not reported. Generally, for complex point process models such as deep Hawkes processes, the necessary regularity conditions are not satisfied, meaning that ASMs objective is incomplete.",
            "All three weight functions can be applied in AWSM to recover ground-truth parameters, however with different convergence rates. We carry out experiments on synthetic data for exponential-decay model with the same setting as  Section   6.2  in our paper. We measure their MAE for different sample sizes in  Figure   2  and find that  h 0 superscript h 0 \\bf h^{0} bold_h start_POSTSUPERSCRIPT bold_0 end_POSTSUPERSCRIPT  does achieve the best results among the three weight functions.",
            "Using the above equation, we manage to cancel out most of the terms being summed in the right side of the thrid equation in  Equation   20  and only leave the first and last term, which completes the proof.",
            "Therefore, the first intractable summation term in  Equation   22  will disappear, and the second term equals   E p  ( T )  [  n = 1 N T   t n     ( t n )  h n  ( T ) +    ( t n )    t n  h n  ( T ) ] subscript E p T delimited-[] superscript subscript n 1 subscript N T subscript t n subscript   subscript t n subscript h n T subscript   subscript t n subscript t n subscript h n T -\\mathbb{E}_{p(\\mathcal{T})}\\left[\\sum_{n=1}^{N_{T}}\\frac{\\partial}{\\partial t% _{n}}\\psi_{\\theta}(t_{n})h_{n}(\\mathcal{T})+\\psi_{\\theta}(t_{n})\\frac{\\partial% }{\\partial t_{n}}h_{n}(\\mathcal{T})\\right] - blackboard_E start_POSTSUBSCRIPT italic_p ( caligraphic_T ) end_POSTSUBSCRIPT [  start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUPERSCRIPT divide start_ARG  end_ARG start_ARG  italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_ARG italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) italic_h start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( caligraphic_T ) + italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) divide start_ARG  end_ARG start_ARG  italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_ARG italic_h start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( caligraphic_T ) ] . The existence of such an expectation is due to the last two equations in  Equation   9 . Therefore, we complete the proof.",
            "We remind readers that     ( t ) subscript   t \\mu_{\\theta}(t) italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_t )  and  g   ( t ) subscript g  t g_{\\theta}(t) italic_g start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_t )  are the mean intensity function and the triggering kernel for a Hawkes process, first defined in  Equation   2 .",
            "First, by  Assumption   C.2 , we know    ( t n | F t n  1 \\lambda_{\\theta}(t_{n}|\\mathcal{F}_{t_{n-1}} italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT | caligraphic_F start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT  and    t n   ( t n | F t n  1 \\frac{\\partial}{\\partial t_{n}}\\lambda_{\\theta}(t_{n}|\\mathcal{F}_{t_{n-1}} divide start_ARG  end_ARG start_ARG  italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_ARG italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT | caligraphic_F start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT  are continuous w.r.t    \\theta italic_ . Therefore     ( t n | F t n  1 ) =   t n  log     ( t n | F n  1 )     ( t n | F t n  1 ) subscript   conditional subscript t n subscript F subscript t n 1 subscript t n subscript   conditional subscript t n subscript F n 1 subscript   conditional subscript t n subscript F subscript t n 1 \\psi_{\\theta}(t_{n}|\\mathcal{F}_{t_{n-1}})=\\frac{\\partial}{\\partial t_{n}}\\log% \\lambda_{\\theta}(t_{n}|\\mathcal{F}_{n-1})-\\lambda_{\\theta}(t_{n}|\\mathcal{F}_{% t_{n-1}}) italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT | caligraphic_F start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) = divide start_ARG  end_ARG start_ARG  italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_ARG roman_log italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT | caligraphic_F start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT ) - italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT | caligraphic_F start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT )  and    t n     ( t n | F t n  1 ) =  2  t n 2  log     ( t n | F t n  1 )    t n     ( t n | F t n  1 ) subscript t n subscript   conditional subscript t n subscript F subscript t n 1 superscript 2 superscript subscript t n 2 subscript   conditional subscript t n subscript F subscript t n 1 subscript t n subscript   conditional subscript t n subscript F subscript t n 1 \\frac{\\partial}{\\partial t_{n}}\\psi_{\\theta}(t_{n}|\\mathcal{F}_{t_{n-1}})=% \\frac{\\partial^{2}}{\\partial t_{n}^{2}}\\log\\lambda_{\\theta}(t_{n}|\\mathcal{F}_% {t_{n-1}})-\\frac{\\partial}{\\partial t_{n}}\\lambda_{\\theta}(t_{n}|\\mathcal{F}_{% t_{n-1}}) divide start_ARG  end_ARG start_ARG  italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_ARG italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT | caligraphic_F start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) = divide start_ARG  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG  italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG roman_log italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT | caligraphic_F start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) - divide start_ARG  end_ARG start_ARG  italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_ARG italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT | caligraphic_F start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT )  are both continuous w.r.t.    \\theta italic_ , therefore the  J AWSM  (  ) subscript J AWSM  \\mathcal{J}_{\\text{AWSM}}(\\theta) caligraphic_J start_POSTSUBSCRIPT AWSM end_POSTSUBSCRIPT ( italic_ )  is a continuous function of    \\theta italic_ . Since    \\Theta roman_  is a compact set in  R d superscript R d \\mathbb{R}^{d} blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , we know  J AWSM  (  ) subscript J AWSM  \\mathcal{J}_{\\text{AWSM}}(\\theta) caligraphic_J start_POSTSUBSCRIPT AWSM end_POSTSUBSCRIPT ( italic_ )  is uniform continuous.",
            "Therefore, we can bound  | J AWSM  (  1 )  J AWSM  (  2 ) | subscript J AWSM subscript  1 subscript J AWSM subscript  2 |\\mathcal{J}_{\\text{AWSM}}(\\theta_{1})-\\mathcal{J}_{\\text{AWSM}}(\\theta_{2})| | caligraphic_J start_POSTSUBSCRIPT AWSM end_POSTSUBSCRIPT ( italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - caligraphic_J start_POSTSUBSCRIPT AWSM end_POSTSUBSCRIPT ( italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) |  using    1   2  norm subscript  1 subscript  2 \\|\\theta_{1}-\\theta_{2}\\|  italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  . Using the result in  Equation   23 , we know that for any   > 0  0 \\varepsilon>0 italic_ > 0 , we can find a uniform    \\delta italic_  so that  | J AWSM  (  1 )  J AWSM  (  2 ) | < 1 6   ,    1   2  <  formulae-sequence subscript J AWSM subscript  1 subscript J AWSM subscript  2 1 6  for-all norm subscript  1 subscript  2  |\\mathcal{J}_{\\text{AWSM}}(\\theta_{1})-\\mathcal{J}_{\\text{AWSM}}(\\theta_{2})|<% \\frac{1}{6}\\varepsilon,\\forall||\\theta_{1}-\\theta_{2}||<\\delta | caligraphic_J start_POSTSUBSCRIPT AWSM end_POSTSUBSCRIPT ( italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - caligraphic_J start_POSTSUBSCRIPT AWSM end_POSTSUBSCRIPT ( italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) | < divide start_ARG 1 end_ARG start_ARG 6 end_ARG italic_ ,  | | italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | | < italic_ . So that,",
            "Now we follow exactly the same steps as  [ 2 ]  for the uniform in probability convergence. Since  Equation   24  hold, for such a    \\delta italic_  in that equation, since    \\Theta roman_  is a compact set in  R d superscript R d \\mathbb{R}^{d} blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , there exists a finite number of open balls with radius    \\delta italic_  whose union covers    \\Theta roman_ . Let   1 , ... ,  i , ... ,  L subscript italic- 1 ... subscript italic- i ... subscript italic- L \\vartheta_{1},\\ldots,\\vartheta_{i},\\ldots,\\vartheta_{L} italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ... , italic_ start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT  denote the centers of these balls. We denote   i  (  ) subscript italic- i  \\vartheta_{i(\\theta)} italic_ start_POSTSUBSCRIPT italic_i ( italic_ ) end_POSTSUBSCRIPT  the center of a ball which contains    \\theta italic_ . Since we have",
            "The third term on the right equals  0 0  because of its definition and the uniform continuous of  J AWSM  (  ) subscript J AWSM  \\mathcal{J}_{\\text{AWSM}}(\\theta) caligraphic_J start_POSTSUBSCRIPT AWSM end_POSTSUBSCRIPT ( italic_ ) . The first term converges to  0 , M    0 M 0,M\\rightarrow\\infty 0 , italic_M    by  Equation   24 . For the second term, we write",
            "Finally, given that   ^ ^  \\hat{\\theta} over^ start_ARG italic_ end_ARG  converges to    superscript  \\theta^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  in probability, combined with  Assumption   5.2  and  Equation   26 , using Theorem 5.52 in  [ 19 ]  we have, for   < C  K   r 2   1    ( h , A , B ) C h  C subscript K  r superscript 2  1  h A B subscript C h \\delta<CK_{\\alpha}\\frac{\\sqrt{r}}{2^{\\alpha-1}}\\frac{\\Gamma(\\mathbf{h},A,B)}{C% _{\\mathbf{h}}} italic_ < italic_C italic_K start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT divide start_ARG square-root start_ARG italic_r end_ARG end_ARG start_ARG 2 start_POSTSUPERSCRIPT italic_ - 1 end_POSTSUPERSCRIPT end_ARG divide start_ARG roman_ ( bold_h , italic_A , italic_B ) end_ARG start_ARG italic_C start_POSTSUBSCRIPT bold_h end_POSTSUBSCRIPT end_ARG , we have,",
            "and equation  28  is proved.",
            "The second equality is due to  Equation   28  and  L h  (   ) = 0 subscript L h superscript  0 \\mathcal{L}_{\\mathbf{h}}(\\theta^{*})=0 caligraphic_L start_POSTSUBSCRIPT bold_h end_POSTSUBSCRIPT ( italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) = 0 . The inequality is due to max-min inequality.",
            "wehre  n  superscript n n^{*} italic_n start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is the is the minimizer of the last minimization in  Equation   29 .",
            "In this section, we present some experimental details. First we discuss our modification to the original AWSM for when  T T T italic_T  for a dataset is not accessible. Then we provide addtional estimation results for the parametric model and hyperparameters for results in  Table   2 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  The MAE of 2-variate Hawkes processes trained by MLE, (A)SM, and (A)WSM on the synthetic dataset.",
        "table": "S3.E11",
        "footnotes": [],
        "references": [
            "The incompleteness of the estimators in the aforementioned studies stems from the transition from explicit SM to implicit SM. The explicit SM estimates model parameters by minimizing the expected distance between the gradient of the log-density of the model and the gradient of the log-density of the data. However, we cannot directly minimize the above objective function since it depends on the unknown data distribution. To facilitate solving, we need to convert the above explicit SM to implicit SM by using a trick of integration by parts, provided that some regularity conditions are satisfied  [ 5 ] . In  [ 18 ;  24 ;  10 ] , they assume that the required regularity conditions are satisfied in their point process models and directly employ the implicit SM objective. However, as demonstrated in  Section   3 , the required regularity conditions cannot be met for general point processes. This implies that the concise implicit SM objectives (Equation (2) in  [ 18 ] , Equation (10) in  [ 24 ] , Equation (4) in  [ 10 ] ) are incomplete, and they cannot accurately estimate parameters for general point processes.",
            "However, the same issue as in the Poisson process arises here. The regularity conditions required to eliminate the unknown data distribution do not hold. Therefore, we cannot derive the implicit ASM in  Equation   13  based on the explicit ASM in  Equation   12 .",
            "For general Hawkes processes,  Equation   17  is always valid with a suitable weight function. Thus, we do not need to worry about the issues of failure that may arise when using  Equation   13 .",
            "Under mild regularity  Assumptions   C.1 ,  C.2  and  C.3 , we have   ^  p   p  ^  superscript  \\hat{\\theta}\\xrightarrow{p}\\theta^{*} over^ start_ARG italic_ end_ARG start_ARROW overitalic_p  end_ARROW italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  as  M    M M\\rightarrow\\infty italic_M   .",
            "Given that   ^ ^  \\hat{\\theta} over^ start_ARG italic_ end_ARG  converges to    superscript  \\theta^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  in probability, combined with  Assumptions   5.2  and  5.3 , for   < C  K   r 2   1    ( h , A , B ) C h  C subscript K  r superscript 2  1  h A B subscript C h \\delta<CK_{\\alpha}\\frac{\\sqrt{r}}{2^{\\alpha-1}}\\frac{\\Gamma(\\mathbf{h},A,B)}{C% _{\\mathbf{h}}} italic_ < italic_C italic_K start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT divide start_ARG square-root start_ARG italic_r end_ARG end_ARG start_ARG 2 start_POSTSUPERSCRIPT italic_ - 1 end_POSTSUPERSCRIPT end_ARG divide start_ARG roman_ ( bold_h , italic_A , italic_B ) end_ARG start_ARG italic_C start_POSTSUBSCRIPT bold_h end_POSTSUBSCRIPT end_ARG , we have",
            "In  Sections   3  and  4 , we only provide the conditions that the weight function needs to satisfy. In fact, there are many weight functions that satisfy these conditions. The optimal weight function should minimize the error bound in  Equation   19 , which is equivalent to minimizing the coefficient    ( h , A , B ) C h  h A B subscript C h \\frac{\\Gamma(\\mathbf{h},A,B)}{C_{\\mathbf{h}}} divide start_ARG roman_ ( bold_h , italic_A , italic_B ) end_ARG start_ARG italic_C start_POSTSUBSCRIPT bold_h end_POSTSUBSCRIPT end_ARG . The numerator cannot be analytically computed as it involves an unknown distribution  p  ( T ) p T p(\\mathcal{T}) italic_p ( caligraphic_T ) , but we can maximize the denominator  C h subscript C h C_{\\mathbf{h}} italic_C start_POSTSUBSCRIPT bold_h end_POSTSUBSCRIPT  in a predefined function family.",
            "The proof is basically the same as the proof of  Proposition   3.1 . We first expand the expectation and consider the cross-term in  L WSM  (  ) subscript L WSM  \\mathcal{L}_{\\text{WSM}}(\\theta) caligraphic_L start_POSTSUBSCRIPT WSM end_POSTSUBSCRIPT ( italic_ ) ,",
            "Therefore, we can bound  | J AWSM  (  1 )  J AWSM  (  2 ) | subscript J AWSM subscript  1 subscript J AWSM subscript  2 |\\mathcal{J}_{\\text{AWSM}}(\\theta_{1})-\\mathcal{J}_{\\text{AWSM}}(\\theta_{2})| | caligraphic_J start_POSTSUBSCRIPT AWSM end_POSTSUBSCRIPT ( italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - caligraphic_J start_POSTSUBSCRIPT AWSM end_POSTSUBSCRIPT ( italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) |  using    1   2  norm subscript  1 subscript  2 \\|\\theta_{1}-\\theta_{2}\\|  italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  . Using the result in  Equation   23 , we know that for any   > 0  0 \\varepsilon>0 italic_ > 0 , we can find a uniform    \\delta italic_  so that  | J AWSM  (  1 )  J AWSM  (  2 ) | < 1 6   ,    1   2  <  formulae-sequence subscript J AWSM subscript  1 subscript J AWSM subscript  2 1 6  for-all norm subscript  1 subscript  2  |\\mathcal{J}_{\\text{AWSM}}(\\theta_{1})-\\mathcal{J}_{\\text{AWSM}}(\\theta_{2})|<% \\frac{1}{6}\\varepsilon,\\forall||\\theta_{1}-\\theta_{2}||<\\delta | caligraphic_J start_POSTSUBSCRIPT AWSM end_POSTSUBSCRIPT ( italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - caligraphic_J start_POSTSUBSCRIPT AWSM end_POSTSUBSCRIPT ( italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) | < divide start_ARG 1 end_ARG start_ARG 6 end_ARG italic_ ,  | | italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | | < italic_ . So that,",
            "Similar arguments as in  Section   5.3  also applies to Poisson process, with the distance weight function defined above. Such a weight function also maximizes the denominator for the convergence rate of the Poisson process. However, in the experiment, we adtops another weight function for easier implementation defined as  h 1  ( T ) superscript h 1 T \\mathbf{h}^{1}(\\mathcal{T}) bold_h start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( caligraphic_T )  with its  n n n italic_n -th component  h n 1  ( T ) = ( T  t n )  ( t n  t n  1 ) subscript superscript h 1 n T T subscript t n subscript t n subscript t n 1 h^{1}_{n}(\\mathcal{T})=(T-t_{n})(t_{n}-t_{n-1}) italic_h start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( caligraphic_T ) = ( italic_T - italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) ( italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT - italic_t start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT ) .",
            "For parametric models,  Table   3  provide the estimation results for some parameters and  Figure   3  shows the learned intensity functions on the 2-nd dimension for the 2-variate Hawkes processes."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  The hyparameters for experiments in  Table   2 .",
        "table": "S4.E12",
        "footnotes": [
            ""
        ],
        "references": [
            "An autoregressive model defines a probability density  p  ( x ) p x p(\\mathbf{x}) italic_p ( bold_x )  as a product of conditionals using the chain rule:  p  ( x ) =  n = 1 N p  ( x n | x < n ) p x superscript subscript product n 1 N p conditional subscript x n subscript x absent n p(\\mathbf{x})=\\prod_{n=1}^{N}p(x_{n}|\\mathbf{x}_{<n}) italic_p ( bold_x ) =  start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_p ( italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT | bold_x start_POSTSUBSCRIPT < italic_n end_POSTSUBSCRIPT ) , where  x n subscript x n x_{n} italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT  is the  n n n italic_n -th entry and  x < n subscript x absent n \\mathbf{x}_{<n} bold_x start_POSTSUBSCRIPT < italic_n end_POSTSUBSCRIPT  denotes the entries with indices smaller than  n n n italic_n . The original SM is not suitable for autoregressive models because the autoregressive structure introduces challenges in gradient computation in  Equation   4 . To address this issue,  [ 13 ]  proposed autoregressive score matching (ASM). Unlike SM, which minimizes the Fisher divergence between the joint distributions of the model  p   ( x ) subscript p  x p_{\\theta}(\\mathbf{x}) italic_p start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( bold_x )  and the data  p  ( x ) p x p(\\mathbf{x}) italic_p ( bold_x ) , ASM minimizes the Fisher divergence between the conditionals of the model  p   ( x n | x < n ) subscript p  conditional subscript x n subscript x absent n p_{\\theta}(x_{n}|\\mathbf{x}_{<n}) italic_p start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT | bold_x start_POSTSUBSCRIPT < italic_n end_POSTSUBSCRIPT )  and the data  p  ( x n | x < n ) p conditional subscript x n subscript x absent n p(x_{n}|\\mathbf{x}_{<n}) italic_p ( italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT | bold_x start_POSTSUBSCRIPT < italic_n end_POSTSUBSCRIPT ) :",
            "In order for SM to be practical,  [ 18 ;  24 ]  assumed that specific regularity conditions are satisfied. Therefore, they employed an implicit SM objective similar to  Equation   4 :",
            "Generally speaking, for most Hawkes processes, the sum of the last two terms in  Equation   14  still contains    \\theta italic_ , even for a common Hawkes process with an exponential decay triggering kernel. We illustrate this example in  Section   6.2 . This implies that  J ASM subscript J ASM \\mathcal{J}_{\\text{ASM}} caligraphic_J start_POSTSUBSCRIPT ASM end_POSTSUBSCRIPT  fails for general Hawkes processes.",
            "In  Sections   3  and  4 , we only provide the conditions that the weight function needs to satisfy. In fact, there are many weight functions that satisfy these conditions. The optimal weight function should minimize the error bound in  Equation   19 , which is equivalent to minimizing the coefficient    ( h , A , B ) C h  h A B subscript C h \\frac{\\Gamma(\\mathbf{h},A,B)}{C_{\\mathbf{h}}} divide start_ARG roman_ ( bold_h , italic_A , italic_B ) end_ARG start_ARG italic_C start_POSTSUBSCRIPT bold_h end_POSTSUBSCRIPT end_ARG . The numerator cannot be analytically computed as it involves an unknown distribution  p  ( T ) p T p(\\mathcal{T}) italic_p ( caligraphic_T ) , but we can maximize the denominator  C h subscript C h C_{\\mathbf{h}} italic_C start_POSTSUBSCRIPT bold_h end_POSTSUBSCRIPT  in a predefined function family.",
            "Combined with  Assumption   5.2 , it can be observed that  h 0 superscript h 0 \\mathbf{h}^{0} bold_h start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT  maximizes  C h subscript C h C_{\\mathbf{h}} italic_C start_POSTSUBSCRIPT bold_h end_POSTSUBSCRIPT  in  H H \\mathcal{H} caligraphic_H . Though it does not necessarily optimize    ( h , A , B ) C h  h A B subscript C h \\frac{\\Gamma(\\mathbf{h},A,B)}{C_{\\mathbf{h}}} divide start_ARG roman_ ( bold_h , italic_A , italic_B ) end_ARG start_ARG italic_C start_POSTSUBSCRIPT bold_h end_POSTSUBSCRIPT end_ARG , it is an adequate choice without using any information on  p  ( T ) p T p(\\mathcal{T}) italic_p ( caligraphic_T ) . We also discuss it heuristically in  Section   C.4 . It is worth noting that  h n 0 superscript subscript h n 0 h_{n}^{0} italic_h start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT  is not continuously differentiable; however, it is weakly differentiable. Its weak derivative is continuous, allowing both integration by parts and statistical theory to hold. In subsequent experiments, we consistently employ this optimal weight function when  T T T italic_T  is available or can be approximated for the dataset.",
            "The proof is basically the same as the proof of  Proposition   4.1 . We first use the  Lemma   B.1  to the cross term of  L AWSM  (  ) subscript L AWSM  \\mathcal{L}_{\\text{AWSM}}(\\theta) caligraphic_L start_POSTSUBSCRIPT AWSM end_POSTSUBSCRIPT ( italic_ ) ,",
            "Now we follow exactly the same steps as  [ 2 ]  for the uniform in probability convergence. Since  Equation   24  hold, for such a    \\delta italic_  in that equation, since    \\Theta roman_  is a compact set in  R d superscript R d \\mathbb{R}^{d} blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , there exists a finite number of open balls with radius    \\delta italic_  whose union covers    \\Theta roman_ . Let   1 , ... ,  i , ... ,  L subscript italic- 1 ... subscript italic- i ... subscript italic- L \\vartheta_{1},\\ldots,\\vartheta_{i},\\ldots,\\vartheta_{L} italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ... , italic_ start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT  denote the centers of these balls. We denote   i  (  ) subscript italic- i  \\vartheta_{i(\\theta)} italic_ start_POSTSUBSCRIPT italic_i ( italic_ ) end_POSTSUBSCRIPT  the center of a ball which contains    \\theta italic_ . Since we have",
            "The third term on the right equals  0 0  because of its definition and the uniform continuous of  J AWSM  (  ) subscript J AWSM  \\mathcal{J}_{\\text{AWSM}}(\\theta) caligraphic_J start_POSTSUBSCRIPT AWSM end_POSTSUBSCRIPT ( italic_ ) . The first term converges to  0 , M    0 M 0,M\\rightarrow\\infty 0 , italic_M    by  Equation   24 . For the second term, we write",
            "Now we begin the proof of  Theorem   5.4 .",
            "For deep point process experiments, we run 4 datasets with 3 methods deployed on 2 models. We show the hyperparameters of those experiments in  Table   4 .  Epochs  column represents the number of epochs for the experiments of a model (THP or SAHP) on a dataset. We train same epochs for three training methods (MLE, AWSM or DSM) and validate every 10 epochs to report the best result. When training MLE, the hyperparameter is the number of integral nodes, which is always 10 for all experiments. When training AWSM, we have two hyperparameters, the value of balancing coefficient for CE loss, shown in column   AWSM subscript  AWSM \\alpha_{\\text{AWSM}} italic_ start_POSTSUBSCRIPT AWSM end_POSTSUBSCRIPT  and whether data truncation is performed as shown in column  Trunc . T represents performing data truncation and F represents no data truncation. For DSM, the hyperparameters are balancing coefficient denoted as   DSM subscript  DSM \\alpha_{\\text{DSM}} italic_ start_POSTSUBSCRIPT DSM end_POSTSUBSCRIPT  and variance of noise denoted as   DSM subscript  DSM \\sigma_{\\text{DSM}} italic_ start_POSTSUBSCRIPT DSM end_POSTSUBSCRIPT ."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "S4.E13",
        "footnotes": [],
        "references": [
            "Assume that all functions and expectations in  L AWSM  (  ) subscript L AWSM  \\mathcal{L}_{\\text{AWSM}}(\\theta) caligraphic_L start_POSTSUBSCRIPT AWSM end_POSTSUBSCRIPT ( italic_ )  and  J AWSM  (  ) subscript J AWSM  \\mathcal{J}_{\\text{AWSM}}(\\theta) caligraphic_J start_POSTSUBSCRIPT AWSM end_POSTSUBSCRIPT ( italic_ )  are well defined,  Equation   15  are satisfied, we have,",
            "In this section, we analyze the statistical properties of AWSM estimator of univariate Hawkes process. Similar conclusions also hold for the WSM estimator of Poisson process, as discussed in  Section   C.5 . We consider  M M M italic_M  i.i.d. sequences  { t 1 ( m ) , ... , t N m ( m ) } m = 1 M superscript subscript superscript subscript t 1 m ... superscript subscript t subscript N m m m 1 M \\{t_{1}^{(m)},\\ldots,t_{N_{m}}^{(m)}\\}_{m=1}^{M} { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_m ) end_POSTSUPERSCRIPT , ... , italic_t start_POSTSUBSCRIPT italic_N start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_m ) end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_m = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT  from  p  ( T ) p T p(\\mathcal{T}) italic_p ( caligraphic_T )  of a Hawkes process. We assume the true density is in the family of the model density, denoted as  p  ( T ) = p    ( T ) p T subscript p superscript  T p(\\mathcal{T})=p_{\\theta^{*}}(\\mathcal{T}) italic_p ( caligraphic_T ) = italic_p start_POSTSUBSCRIPT italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( caligraphic_T ) , where       R r superscript   superscript R r \\theta^{*}\\in\\Theta\\subset\\mathbb{R}^{r} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  roman_  blackboard_R start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT . The estimate   ^ ^  \\hat{\\theta} over^ start_ARG italic_ end_ARG  is obtained by   ^ = arg  min     J ^ AWSM  (  ) ^  subscript arg min   subscript ^ J AWSM  \\hat{\\theta}=\\operatorname*{arg\\,min}_{\\theta\\in\\Theta}\\hat{\\mathcal{J}}_{% \\text{AWSM}}(\\theta) over^ start_ARG italic_ end_ARG = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_  roman_ end_POSTSUBSCRIPT over^ start_ARG caligraphic_J end_ARG start_POSTSUBSCRIPT AWSM end_POSTSUBSCRIPT ( italic_ )  where  J ^ AWSM subscript ^ J AWSM \\hat{\\mathcal{J}}_{\\text{AWSM}} over^ start_ARG caligraphic_J end_ARG start_POSTSUBSCRIPT AWSM end_POSTSUBSCRIPT  represents the empirical loss. Below we omit the subscript AWSM as it does not cause any ambiguity.",
            "Given that   ^ ^  \\hat{\\theta} over^ start_ARG italic_ end_ARG  converges to    superscript  \\theta^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  in probability, combined with  Assumptions   5.2  and  5.3 , for   < C  K   r 2   1    ( h , A , B ) C h  C subscript K  r superscript 2  1  h A B subscript C h \\delta<CK_{\\alpha}\\frac{\\sqrt{r}}{2^{\\alpha-1}}\\frac{\\Gamma(\\mathbf{h},A,B)}{C% _{\\mathbf{h}}} italic_ < italic_C italic_K start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT divide start_ARG square-root start_ARG italic_r end_ARG end_ARG start_ARG 2 start_POSTSUPERSCRIPT italic_ - 1 end_POSTSUPERSCRIPT end_ARG divide start_ARG roman_ ( bold_h , italic_A , italic_B ) end_ARG start_ARG italic_C start_POSTSUBSCRIPT bold_h end_POSTSUBSCRIPT end_ARG , we have",
            "Combined with  Assumption   5.2 , it can be observed that  h 0 superscript h 0 \\mathbf{h}^{0} bold_h start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT  maximizes  C h subscript C h C_{\\mathbf{h}} italic_C start_POSTSUBSCRIPT bold_h end_POSTSUBSCRIPT  in  H H \\mathcal{H} caligraphic_H . Though it does not necessarily optimize    ( h , A , B ) C h  h A B subscript C h \\frac{\\Gamma(\\mathbf{h},A,B)}{C_{\\mathbf{h}}} divide start_ARG roman_ ( bold_h , italic_A , italic_B ) end_ARG start_ARG italic_C start_POSTSUBSCRIPT bold_h end_POSTSUBSCRIPT end_ARG , it is an adequate choice without using any information on  p  ( T ) p T p(\\mathcal{T}) italic_p ( caligraphic_T ) . We also discuss it heuristically in  Section   C.4 . It is worth noting that  h n 0 superscript subscript h n 0 h_{n}^{0} italic_h start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT  is not continuously differentiable; however, it is weakly differentiable. Its weak derivative is continuous, allowing both integration by parts and statistical theory to hold. In subsequent experiments, we consistently employ this optimal weight function when  T T T italic_T  is available or can be approximated for the dataset.",
            "Though we provide theoretical insight into the choice of an optimal weight function for AWSM, its validity still needs to be testified by experiments. Here, we compare the near-optimal weight  h 0 superscript h 0 \\bm{h}^{0} bold_italic_h start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT  with natural weight  h 1 superscript h 1 \\bm{h}^{1} bold_italic_h start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT  and squareroot weight  h 2 superscript h 2 \\bm{h}^{2} bold_italic_h start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  satisfying  Equation   15 ,",
            "Between the second and the third line above, we omit the steps used in the derivation of Proposition 4.1 to make it concise. For the term in the third line above, it will be eliminated using  Equation   15 . For the term in the fourth line above, using Lemma B.1, we have:",
            "The existence of the expectation is ensured by the last two terms in  Equation   15 .",
            "Now we begin the proof of  Theorem   5.4 .",
            "Finally, given that   ^ ^  \\hat{\\theta} over^ start_ARG italic_ end_ARG  converges to    superscript  \\theta^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  in probability, combined with  Assumption   5.2  and  Equation   26 , using Theorem 5.52 in  [ 19 ]  we have, for   < C  K   r 2   1    ( h , A , B ) C h  C subscript K  r superscript 2  1  h A B subscript C h \\delta<CK_{\\alpha}\\frac{\\sqrt{r}}{2^{\\alpha-1}}\\frac{\\Gamma(\\mathbf{h},A,B)}{C% _{\\mathbf{h}}} italic_ < italic_C italic_K start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT divide start_ARG square-root start_ARG italic_r end_ARG end_ARG start_ARG 2 start_POSTSUPERSCRIPT italic_ - 1 end_POSTSUPERSCRIPT end_ARG divide start_ARG roman_ ( bold_h , italic_A , italic_B ) end_ARG start_ARG italic_C start_POSTSUBSCRIPT bold_h end_POSTSUBSCRIPT end_ARG , we have,",
            "For Poisson process, results in  Section   5  also holds, including the consistency and the convergence rate. For the choice of weight function, a reasonable choice is still the distance function presented below.",
            "Similar arguments as in  Section   5.3  also applies to Poisson process, with the distance weight function defined above. Such a weight function also maximizes the denominator for the convergence rate of the Poisson process. However, in the experiment, we adtops another weight function for easier implementation defined as  h 1  ( T ) superscript h 1 T \\mathbf{h}^{1}(\\mathcal{T}) bold_h start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( caligraphic_T )  with its  n n n italic_n -th component  h n 1  ( T ) = ( T  t n )  ( t n  t n  1 ) subscript superscript h 1 n T T subscript t n subscript t n subscript t n 1 h^{1}_{n}(\\mathcal{T})=(T-t_{n})(t_{n}-t_{n-1}) italic_h start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( caligraphic_T ) = ( italic_T - italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) ( italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT - italic_t start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT ) ."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "S4.E14",
        "footnotes": [],
        "references": [
            "Such conditions require the probability density function of the random variable is zero when it approaches infinity in any of its dimensions. However, for point processes, such requirement is not satisfied, because the random variable in point process  T = ( t 1 , ... , t N T ) T subscript t 1 ... subscript t subscript N T \\mathcal{T}=(t_{1},\\ldots,t_{N_{T}}) caligraphic_T = ( italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_t start_POSTSUBSCRIPT italic_N start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT )  is not of fixed dimension and takes values in a subset of  R + N T superscript subscript R subscript N T \\mathbb{R}_{+}^{N_{T}} blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUPERSCRIPT . Therefore, for general Poisson processes, we cannot derive the implicit SM in  Equation   7  based on the explicit SM in  Equation   6 .",
            "Generally speaking, for most Hawkes processes, the sum of the last two terms in  Equation   14  still contains    \\theta italic_ , even for a common Hawkes process with an exponential decay triggering kernel. We illustrate this example in  Section   6.2 . This implies that  J ASM subscript J ASM \\mathcal{J}_{\\text{ASM}} caligraphic_J start_POSTSUBSCRIPT ASM end_POSTSUBSCRIPT  fails for general Hawkes processes.",
            "All three weight functions can be applied in AWSM to recover ground-truth parameters, however with different convergence rates. We carry out experiments on synthetic data for exponential-decay model with the same setting as  Section   6.2  in our paper. We measure their MAE for different sample sizes in  Figure   2  and find that  h 0 superscript h 0 \\bf h^{0} bold_h start_POSTSUPERSCRIPT bold_0 end_POSTSUPERSCRIPT  does achieve the best results among the three weight functions.",
            "Finally, given that   ^ ^  \\hat{\\theta} over^ start_ARG italic_ end_ARG  converges to    superscript  \\theta^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  in probability, combined with  Assumption   5.2  and  Equation   26 , using Theorem 5.52 in  [ 19 ]  we have, for   < C  K   r 2   1    ( h , A , B ) C h  C subscript K  r superscript 2  1  h A B subscript C h \\delta<CK_{\\alpha}\\frac{\\sqrt{r}}{2^{\\alpha-1}}\\frac{\\Gamma(\\mathbf{h},A,B)}{C% _{\\mathbf{h}}} italic_ < italic_C italic_K start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT divide start_ARG square-root start_ARG italic_r end_ARG end_ARG start_ARG 2 start_POSTSUPERSCRIPT italic_ - 1 end_POSTSUPERSCRIPT end_ARG divide start_ARG roman_ ( bold_h , italic_A , italic_B ) end_ARG start_ARG italic_C start_POSTSUBSCRIPT bold_h end_POSTSUBSCRIPT end_ARG , we have,"
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "A4.EGx1",
        "footnotes": [],
        "references": [
            "Such conditions require the probability density function of the random variable is zero when it approaches infinity in any of its dimensions. However, for point processes, such requirement is not satisfied, because the random variable in point process  T = ( t 1 , ... , t N T ) T subscript t 1 ... subscript t subscript N T \\mathcal{T}=(t_{1},\\ldots,t_{N_{T}}) caligraphic_T = ( italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_t start_POSTSUBSCRIPT italic_N start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT )  is not of fixed dimension and takes values in a subset of  R + N T superscript subscript R subscript N T \\mathbb{R}_{+}^{N_{T}} blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUPERSCRIPT . Therefore, for general Poisson processes, we cannot derive the implicit SM in  Equation   7  based on the explicit SM in  Equation   6 .",
            "For general Poisson processes,  Equation   11  is always valid with a suitable weight function. Thus, we do not need to worry about the issues of failure that may arise when using  Equation   7 .",
            "For general Hawkes processes,  Equation   17  is always valid with a suitable weight function. Thus, we do not need to worry about the issues of failure that may arise when using  Equation   13 .",
            "where  H H \\mathcal{H} caligraphic_H  is a family of functions that is rigorously defined in  Equation   27 ."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "S4.E16",
        "footnotes": [],
        "references": [
            "The introduction of the weight function allows control over the values of the integrand at the boundaries of the integration domain, thereby eliminating the last two terms in  Equation   8 .",
            "where  J CE  (  ) subscript J CE  {\\mathcal{J}}_{\\text{CE}}(\\theta) caligraphic_J start_POSTSUBSCRIPT CE end_POSTSUBSCRIPT ( italic_ )  is the cross-entropy loss defined in  Equation   18 .",
            "and equation  28  is proved.",
            "The second equality is due to  Equation   28  and  L h  (   ) = 0 subscript L h superscript  0 \\mathcal{L}_{\\mathbf{h}}(\\theta^{*})=0 caligraphic_L start_POSTSUBSCRIPT bold_h end_POSTSUBSCRIPT ( italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) = 0 . The inequality is due to max-min inequality."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "S4.Ex5",
        "footnotes": [],
        "references": [
            "Assume that all functions and expectations in  L WSM  (  ) subscript L WSM  \\mathcal{L}_{\\text{WSM}}(\\theta) caligraphic_L start_POSTSUBSCRIPT WSM end_POSTSUBSCRIPT ( italic_ )  and  J WSM  (  ) subscript J WSM  \\mathcal{J}_{\\text{WSM}}(\\theta) caligraphic_J start_POSTSUBSCRIPT WSM end_POSTSUBSCRIPT ( italic_ )  are well defined,  Equation   9  is satisfied, we have,",
            "In  Sections   3  and  4 , we only provide the conditions that the weight function needs to satisfy. In fact, there are many weight functions that satisfy these conditions. The optimal weight function should minimize the error bound in  Equation   19 , which is equivalent to minimizing the coefficient    ( h , A , B ) C h  h A B subscript C h \\frac{\\Gamma(\\mathbf{h},A,B)}{C_{\\mathbf{h}}} divide start_ARG roman_ ( bold_h , italic_A , italic_B ) end_ARG start_ARG italic_C start_POSTSUBSCRIPT bold_h end_POSTSUBSCRIPT end_ARG . The numerator cannot be analytically computed as it involves an unknown distribution  p  ( T ) p T p(\\mathcal{T}) italic_p ( caligraphic_T ) , but we can maximize the denominator  C h subscript C h C_{\\mathbf{h}} italic_C start_POSTSUBSCRIPT bold_h end_POSTSUBSCRIPT  in a predefined function family.",
            "We denote  t N + 1 = T subscript t N 1 T t_{N+1}=T italic_t start_POSTSUBSCRIPT italic_N + 1 end_POSTSUBSCRIPT = italic_T  and  t 0 = 0 subscript t 0 0 t_{0}=0 italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 0  here. Using the first two equations in  Equation   9 , we have:",
            "Therefore, the first intractable summation term in  Equation   22  will disappear, and the second term equals   E p  ( T )  [  n = 1 N T   t n     ( t n )  h n  ( T ) +    ( t n )    t n  h n  ( T ) ] subscript E p T delimited-[] superscript subscript n 1 subscript N T subscript t n subscript   subscript t n subscript h n T subscript   subscript t n subscript t n subscript h n T -\\mathbb{E}_{p(\\mathcal{T})}\\left[\\sum_{n=1}^{N_{T}}\\frac{\\partial}{\\partial t% _{n}}\\psi_{\\theta}(t_{n})h_{n}(\\mathcal{T})+\\psi_{\\theta}(t_{n})\\frac{\\partial% }{\\partial t_{n}}h_{n}(\\mathcal{T})\\right] - blackboard_E start_POSTSUBSCRIPT italic_p ( caligraphic_T ) end_POSTSUBSCRIPT [  start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUPERSCRIPT divide start_ARG  end_ARG start_ARG  italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_ARG italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) italic_h start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( caligraphic_T ) + italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) divide start_ARG  end_ARG start_ARG  italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_ARG italic_h start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( caligraphic_T ) ] . The existence of such an expectation is due to the last two equations in  Equation   9 . Therefore, we complete the proof.",
            "We can see from the proof that, in  Equation   9 , the first two equations ensure that the integration by parts trick does not produce an intractable term, and the last two equations are simply regularity conditions that ensure all terms are well-defined.",
            "wehre  n  superscript n n^{*} italic_n start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is the is the minimizer of the last minimization in  Equation   29 ."
        ]
    },
    "id_table_10": {
        "caption": "",
        "table": "S4.E17",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "",
        "table": "S5.Ex6",
        "footnotes": [],
        "references": [
            "For general Poisson processes,  Equation   11  is always valid with a suitable weight function. Thus, we do not need to worry about the issues of failure that may arise when using  Equation   7 ."
        ]
    },
    "id_table_12": {
        "caption": "",
        "table": "S5.Ex8",
        "footnotes": [],
        "references": [
            "However, the same issue as in the Poisson process arises here. The regularity conditions required to eliminate the unknown data distribution do not hold. Therefore, we cannot derive the implicit ASM in  Equation   13  based on the explicit ASM in  Equation   12 ."
        ]
    },
    "id_table_13": {
        "caption": "",
        "table": "S6.T1.28.28",
        "footnotes": [],
        "references": [
            "However, the same issue as in the Poisson process arises here. The regularity conditions required to eliminate the unknown data distribution do not hold. Therefore, we cannot derive the implicit ASM in  Equation   13  based on the explicit ASM in  Equation   12 .",
            "For general Hawkes processes,  Equation   17  is always valid with a suitable weight function. Thus, we do not need to worry about the issues of failure that may arise when using  Equation   13 ."
        ]
    },
    "id_table_14": {
        "caption": "",
        "table": "S6.T2.52.52",
        "footnotes": [],
        "references": [
            "Generally speaking, for most Hawkes processes, the sum of the last two terms in  Equation   14  still contains    \\theta italic_ , even for a common Hawkes process with an exponential decay triggering kernel. We illustrate this example in  Section   6.2 . This implies that  J ASM subscript J ASM \\mathcal{J}_{\\text{ASM}} caligraphic_J start_POSTSUBSCRIPT ASM end_POSTSUBSCRIPT  fails for general Hawkes processes."
        ]
    },
    "id_table_15": {
        "caption": "",
        "table": "A1.E20",
        "footnotes": [],
        "references": [
            "Assume that all functions and expectations in  L AWSM  (  ) subscript L AWSM  \\mathcal{L}_{\\text{AWSM}}(\\theta) caligraphic_L start_POSTSUBSCRIPT AWSM end_POSTSUBSCRIPT ( italic_ )  and  J AWSM  (  ) subscript J AWSM  \\mathcal{J}_{\\text{AWSM}}(\\theta) caligraphic_J start_POSTSUBSCRIPT AWSM end_POSTSUBSCRIPT ( italic_ )  are well defined,  Equation   15  are satisfied, we have,",
            "Though we provide theoretical insight into the choice of an optimal weight function for AWSM, its validity still needs to be testified by experiments. Here, we compare the near-optimal weight  h 0 superscript h 0 \\bm{h}^{0} bold_italic_h start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT  with natural weight  h 1 superscript h 1 \\bm{h}^{1} bold_italic_h start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT  and squareroot weight  h 2 superscript h 2 \\bm{h}^{2} bold_italic_h start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  satisfying  Equation   15 ,",
            "Between the second and the third line above, we omit the steps used in the derivation of Proposition 4.1 to make it concise. For the term in the third line above, it will be eliminated using  Equation   15 . For the term in the fourth line above, using Lemma B.1, we have:",
            "The existence of the expectation is ensured by the last two terms in  Equation   15 ."
        ]
    },
    "id_table_16": {
        "caption": "",
        "table": "A1.Ex13",
        "footnotes": [],
        "references": []
    },
    "id_table_17": {
        "caption": "",
        "table": "A1.Ex14",
        "footnotes": [],
        "references": [
            "For general Hawkes processes,  Equation   17  is always valid with a suitable weight function. Thus, we do not need to worry about the issues of failure that may arise when using  Equation   13 ."
        ]
    },
    "id_table_18": {
        "caption": "",
        "table": "A1.E22",
        "footnotes": [],
        "references": [
            "where  J CE  (  ) subscript J CE  {\\mathcal{J}}_{\\text{CE}}(\\theta) caligraphic_J start_POSTSUBSCRIPT CE end_POSTSUBSCRIPT ( italic_ )  is the cross-entropy loss defined in  Equation   18 ."
        ]
    },
    "id_table_19": {
        "caption": "",
        "table": "A1.Ex15",
        "footnotes": [],
        "references": [
            "In  Sections   3  and  4 , we only provide the conditions that the weight function needs to satisfy. In fact, there are many weight functions that satisfy these conditions. The optimal weight function should minimize the error bound in  Equation   19 , which is equivalent to minimizing the coefficient    ( h , A , B ) C h  h A B subscript C h \\frac{\\Gamma(\\mathbf{h},A,B)}{C_{\\mathbf{h}}} divide start_ARG roman_ ( bold_h , italic_A , italic_B ) end_ARG start_ARG italic_C start_POSTSUBSCRIPT bold_h end_POSTSUBSCRIPT end_ARG . The numerator cannot be analytically computed as it involves an unknown distribution  p  ( T ) p T p(\\mathcal{T}) italic_p ( caligraphic_T ) , but we can maximize the denominator  C h subscript C h C_{\\mathbf{h}} italic_C start_POSTSUBSCRIPT bold_h end_POSTSUBSCRIPT  in a predefined function family."
        ]
    },
    "id_table_20": {
        "caption": "",
        "table": "A2.Ex17",
        "footnotes": [],
        "references": [
            "Using the above equation, we manage to cancel out most of the terms being summed in the right side of the thrid equation in  Equation   20  and only leave the first and last term, which completes the proof."
        ]
    },
    "id_table_21": {
        "caption": "",
        "table": "A2.Ex19",
        "footnotes": [],
        "references": []
    },
    "id_table_22": {
        "caption": "",
        "table": "A2.Ex20",
        "footnotes": [],
        "references": [
            "Therefore, the first intractable summation term in  Equation   22  will disappear, and the second term equals   E p  ( T )  [  n = 1 N T   t n     ( t n )  h n  ( T ) +    ( t n )    t n  h n  ( T ) ] subscript E p T delimited-[] superscript subscript n 1 subscript N T subscript t n subscript   subscript t n subscript h n T subscript   subscript t n subscript t n subscript h n T -\\mathbb{E}_{p(\\mathcal{T})}\\left[\\sum_{n=1}^{N_{T}}\\frac{\\partial}{\\partial t% _{n}}\\psi_{\\theta}(t_{n})h_{n}(\\mathcal{T})+\\psi_{\\theta}(t_{n})\\frac{\\partial% }{\\partial t_{n}}h_{n}(\\mathcal{T})\\right] - blackboard_E start_POSTSUBSCRIPT italic_p ( caligraphic_T ) end_POSTSUBSCRIPT [  start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUPERSCRIPT divide start_ARG  end_ARG start_ARG  italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_ARG italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) italic_h start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( caligraphic_T ) + italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) divide start_ARG  end_ARG start_ARG  italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_ARG italic_h start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( caligraphic_T ) ] . The existence of such an expectation is due to the last two equations in  Equation   9 . Therefore, we complete the proof."
        ]
    },
    "id_table_23": {
        "caption": "",
        "table": "A2.Ex21",
        "footnotes": [],
        "references": [
            "Therefore, we can bound  | J AWSM  (  1 )  J AWSM  (  2 ) | subscript J AWSM subscript  1 subscript J AWSM subscript  2 |\\mathcal{J}_{\\text{AWSM}}(\\theta_{1})-\\mathcal{J}_{\\text{AWSM}}(\\theta_{2})| | caligraphic_J start_POSTSUBSCRIPT AWSM end_POSTSUBSCRIPT ( italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - caligraphic_J start_POSTSUBSCRIPT AWSM end_POSTSUBSCRIPT ( italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) |  using    1   2  norm subscript  1 subscript  2 \\|\\theta_{1}-\\theta_{2}\\|  italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  . Using the result in  Equation   23 , we know that for any   > 0  0 \\varepsilon>0 italic_ > 0 , we can find a uniform    \\delta italic_  so that  | J AWSM  (  1 )  J AWSM  (  2 ) | < 1 6   ,    1   2  <  formulae-sequence subscript J AWSM subscript  1 subscript J AWSM subscript  2 1 6  for-all norm subscript  1 subscript  2  |\\mathcal{J}_{\\text{AWSM}}(\\theta_{1})-\\mathcal{J}_{\\text{AWSM}}(\\theta_{2})|<% \\frac{1}{6}\\varepsilon,\\forall||\\theta_{1}-\\theta_{2}||<\\delta | caligraphic_J start_POSTSUBSCRIPT AWSM end_POSTSUBSCRIPT ( italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - caligraphic_J start_POSTSUBSCRIPT AWSM end_POSTSUBSCRIPT ( italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) | < divide start_ARG 1 end_ARG start_ARG 6 end_ARG italic_ ,  | | italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | | < italic_ . So that,"
        ]
    },
    "id_table_24": {
        "caption": "",
        "table": "A2.Ex23",
        "footnotes": [],
        "references": [
            "Now we follow exactly the same steps as  [ 2 ]  for the uniform in probability convergence. Since  Equation   24  hold, for such a    \\delta italic_  in that equation, since    \\Theta roman_  is a compact set in  R d superscript R d \\mathbb{R}^{d} blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , there exists a finite number of open balls with radius    \\delta italic_  whose union covers    \\Theta roman_ . Let   1 , ... ,  i , ... ,  L subscript italic- 1 ... subscript italic- i ... subscript italic- L \\vartheta_{1},\\ldots,\\vartheta_{i},\\ldots,\\vartheta_{L} italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ... , italic_ start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT  denote the centers of these balls. We denote   i  (  ) subscript italic- i  \\vartheta_{i(\\theta)} italic_ start_POSTSUBSCRIPT italic_i ( italic_ ) end_POSTSUBSCRIPT  the center of a ball which contains    \\theta italic_ . Since we have",
            "The third term on the right equals  0 0  because of its definition and the uniform continuous of  J AWSM  (  ) subscript J AWSM  \\mathcal{J}_{\\text{AWSM}}(\\theta) caligraphic_J start_POSTSUBSCRIPT AWSM end_POSTSUBSCRIPT ( italic_ ) . The first term converges to  0 , M    0 M 0,M\\rightarrow\\infty 0 , italic_M    by  Equation   24 . For the second term, we write"
        ]
    },
    "id_table_25": {
        "caption": "",
        "table": "A2.Ex24",
        "footnotes": [],
        "references": []
    },
    "id_table_26": {
        "caption": "",
        "table": "A3.Ex25",
        "footnotes": [],
        "references": [
            "Finally, given that   ^ ^  \\hat{\\theta} over^ start_ARG italic_ end_ARG  converges to    superscript  \\theta^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  in probability, combined with  Assumption   5.2  and  Equation   26 , using Theorem 5.52 in  [ 19 ]  we have, for   < C  K   r 2   1    ( h , A , B ) C h  C subscript K  r superscript 2  1  h A B subscript C h \\delta<CK_{\\alpha}\\frac{\\sqrt{r}}{2^{\\alpha-1}}\\frac{\\Gamma(\\mathbf{h},A,B)}{C% _{\\mathbf{h}}} italic_ < italic_C italic_K start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT divide start_ARG square-root start_ARG italic_r end_ARG end_ARG start_ARG 2 start_POSTSUPERSCRIPT italic_ - 1 end_POSTSUPERSCRIPT end_ARG divide start_ARG roman_ ( bold_h , italic_A , italic_B ) end_ARG start_ARG italic_C start_POSTSUBSCRIPT bold_h end_POSTSUBSCRIPT end_ARG , we have,"
        ]
    },
    "id_table_27": {
        "caption": "",
        "table": "A3.Ex26",
        "footnotes": [],
        "references": [
            "where  H H \\mathcal{H} caligraphic_H  is a family of functions that is rigorously defined in  Equation   27 ."
        ]
    },
    "id_table_28": {
        "caption": "",
        "table": "A3.Ex27",
        "footnotes": [],
        "references": [
            "and equation  28  is proved.",
            "The second equality is due to  Equation   28  and  L h  (   ) = 0 subscript L h superscript  0 \\mathcal{L}_{\\mathbf{h}}(\\theta^{*})=0 caligraphic_L start_POSTSUBSCRIPT bold_h end_POSTSUBSCRIPT ( italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) = 0 . The inequality is due to max-min inequality."
        ]
    },
    "id_table_29": {
        "caption": "",
        "table": "A3.Ex30",
        "footnotes": [],
        "references": [
            "wehre  n  superscript n n^{*} italic_n start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is the is the minimizer of the last minimization in  Equation   29 ."
        ]
    },
    "id_table_30": {
        "caption": "",
        "table": "A3.E26",
        "footnotes": [],
        "references": []
    },
    "id_table_31": {
        "caption": "",
        "table": "A3.E27",
        "footnotes": [],
        "references": []
    },
    "id_table_32": {
        "caption": "",
        "table": "A3.Ex41",
        "footnotes": [],
        "references": []
    },
    "id_table_33": {
        "caption": "",
        "table": "A3.Ex43",
        "footnotes": [],
        "references": []
    },
    "id_table_34": {
        "caption": "",
        "table": "A4.T3.28.28",
        "footnotes": [],
        "references": []
    },
    "id_table_35": {
        "caption": "",
        "table": "A4.T4.27.27",
        "footnotes": [],
        "references": []
    }
}