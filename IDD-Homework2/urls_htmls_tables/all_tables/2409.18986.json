{
    "id_table_1": {
        "caption": "Table 1 :  Example factors for lab test normal ranges",
        "table": "Sx4.T1.4",
        "footnotes": [],
        "references": [
            "Lab Test Factor Dataset:   This dataset supports our first objective  to evaluate if RAG-based LLM can accurately extract and prompt the key factors determining the normal range for a lab test. To create the ground truth, we first manually extracted the factors for determining normal reference ranges for the included 68 lab tests. An example can be found in Figure  1 . We only considered the factors that determine different normal ranges. Factors that may affect the lab test but do not have a specific normal range in the article were excluded. For example, the normal range of Aldolase from MedlinePlus is: Normal results range between 1.0 to 7.5 units per liter (0.02 to 0.13 microkat/L). There is a slight difference between men and women. Although the normal range for men and women can be slightly different, we did not count gender as a factor because they do not have specific normal ranges in the article. Table  1  provides example factors for lab tests."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Factor Retrieval Performance",
        "table": "Sx5.T2.4.1",
        "footnotes": [],
        "references": [
            "Figure  2  illustrates users interactions with Lab-AI. The tool will start with a users question. The format of questions can vary but we only tested questions like What is the normal range for [lab test name]?. For each user input, the system employs OpenAIs text-embedding-3-large model to embed the query. The embedding model is designed to act as a text encoder, transforming the input into a high-dimensional vector representation. The dimensions of these embedded queries are consistent with the data stored in the vector database, allowing for efficient and accurate retrieval of information.",
            "For the factor retrieval task, GPT-4-turbo outperformed the other models in both RAG and non-RAG systems. Surprisingly, without the RAG system, GPT-3.5-turbo showed better results than GPT-4, achieving significantly higher precision and F1, although it did not perform as well with RAG, as seen in Table  2  where GPT-4-turbo with RAG achieved a 0.95 F1 score. Table  3  lists some cases of the RAG-based system with different GPT models. GPT-3.5-turbo with RAG will respond with choice-level factors. We cannot say it is completely wrong, but it did not follow our system prompt requirement. GPT-4 with RAG tended to provide extra factors in addition to the true labels. Some of those factors were mentioned in MedlinePlus but are not closely associated with a clear normal range. Some factors may have come from its own knowledge base. GPT-4-turbo with RAG showed more balanced performance but it may also prompt additional factors or miss some factors.",
            "In this study, our aim was to develop an interactive Lab-AI system capable of providing accurate normal reference ranges for lab tests specific to each patient. This is achieved by first identifying the factors that affect normal reference ranges, and then retrieving the appropriate range based on the patients responses to those factors. According to the results in Table  2 , GPT-4-turbo  with RAG achieved an F1 of 0.95 on this task, whereas the original GPT-4-turbo reached only an F1 of 0.659. Typically, GPT-4, with its larger number of parameters compared to GPT-3.5-turbo, would be expected to outperform GPT-3.5-turbo. However, in the factor retrieval task, GPT-4 yielded lower precision and F1. While GPT-4-turbo achieved the highest recall, making it the most effective non-RAG model at retrieving relevant factors, its recall was still below 0.7. With RAG, GPT-4-turbo outperformed other models in retrieving accurate and relevant cases. GPT-4 with RAG significantly outperformed GPT-3.5-turbo with RAG, indicating better compatibility between GPT-4 and the RAG system. Interestingly, while GPT-4 with RAG excelled at retrieving relevant factors, it was also more prone to providing factors not listed in the original source. This may be due to its tendency for hallucinations, whereas GPT-4-turbo is more stable and less prone to generating creative but incorrect information."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Sample Error Cases in Factor Retrieval Evaluation",
        "table": "Sx5.T3.4.1",
        "footnotes": [],
        "references": [
            "We obtained a set of lab test articles from MedlinePlus  16 , a credible online health information resource developed and maintained by the U.S. National Library of Medicine. Following the pipeline depicted in Figure  3 , we crawled data from the medical encyclopedia pages by iterating over the initials from A to Z and 0-9. For each page, we specifically examined the Normal Results section. If the web document included this section, we stored the lab name, the corresponding text under Normal Results and the URL. To facilitate the extraction of lab test information from these articles, we used Beautiful Soup, a Python package designed for parsing HTML and XML markup documents  17 . The vector database is an efficient vector data storage that allows fast and accurate similarity search and retrieval  18 . We first reformatted the data into the format [labname]: NormalResult. We opted to use text-embedding-3-large from OpenAI as our embedding model and converted all lab normal ranges into 3072-dimensional vectors. Additionally, we included URLs as metadata so that the RAG system can link to the appropriate source on MedlinePlus when necessary. We chose ChromaDB  19  as the vector database due to its compatibility with OpenAI embedding functions.",
            "For the factor retrieval task, GPT-4-turbo outperformed the other models in both RAG and non-RAG systems. Surprisingly, without the RAG system, GPT-3.5-turbo showed better results than GPT-4, achieving significantly higher precision and F1, although it did not perform as well with RAG, as seen in Table  2  where GPT-4-turbo with RAG achieved a 0.95 F1 score. Table  3  lists some cases of the RAG-based system with different GPT models. GPT-3.5-turbo with RAG will respond with choice-level factors. We cannot say it is completely wrong, but it did not follow our system prompt requirement. GPT-4 with RAG tended to provide extra factors in addition to the true labels. Some of those factors were mentioned in MedlinePlus but are not closely associated with a clear normal range. Some factors may have come from its own knowledge base. GPT-4-turbo with RAG showed more balanced performance but it may also prompt additional factors or miss some factors."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  Normal Range Retrieval Performance",
        "table": "Sx5.T4.4.1",
        "footnotes": [],
        "references": [
            "As shown in Figure  4 , a total of 555 lab tests were identified, 96 of which had associated factors or consisted of multiple tests, while the remaining 459 did not have factors. We manually filtered out the tests with factors that lacked normal ranges, as well as those that comprised multiple tests. After filtering, the final number of lab tests with factors was 30. A similar filtering process was applied to the tests without factors, reducing their number from 459 to 192. From these, 38 (20%) non-factor lab tests were randomly selected as our convenience sample. Our focus was primarily on lab tests using blood or urine as specimens. Over 72% of the tests in the convenience sample were blood tests, while 16% used urine. Other specimens included tissues, fluids, marrow, and others.",
            "Since GPT-4-turbo with RAG outperformed the other models, we further evaluated it in the normal range retrieval task. Table  4  shows the performance of normal range retrieval at the question and lab levels. GPT-4-turbo without RAG performed poorly with only an accuracy of 38.4% at the question level and 45.6% at the lab level. GPT-4-turbo with RAG performed exceptionally well, with only one error out of 151 generated questions. For lab tests with relevant factors, the accuracy reached 100%. Even though we also compared the results from the non-RAG GPT-4-turbo with multiple credible sources, its performance still remained unsatisfactory. Table  5  highlights the single error, along with a similar case in which GPT-4-turbo with RAG provided a correct response. Unlike most lab tests, the normal ranges for the acid-fast stain and anti-smooth muscle antibody tests are no acid-fast bacteria found and no antibodies found, respectively. Although these results could be interpreted as zero, we did not apply any preprocessing for these cases."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 :  Normal Range Retrieval Sample Cases using RAG",
        "table": "Sx5.T5.4.1",
        "footnotes": [],
        "references": [
            "After manual review, we found that all the lab tests had up to three factors influencing their normal ranges, such as age, gender, or specimen type. Figure  5  shows the dataset distribution. The donut chart illustrates the frequency of each factor, while the pie chart displays the distribution of how many factors each lab test includes. Notably, 55.9% of the selected lab tests had no associated factors, indicating a universal normal range. Among the remaining tests, 20 depended on sex, 15 on age, 3 on pregnancy status, and 2 were influenced by specific conditions related to women. The others include uncommon cases, such as Is the patient an athlete?.",
            "Since GPT-4-turbo with RAG outperformed the other models, we further evaluated it in the normal range retrieval task. Table  4  shows the performance of normal range retrieval at the question and lab levels. GPT-4-turbo without RAG performed poorly with only an accuracy of 38.4% at the question level and 45.6% at the lab level. GPT-4-turbo with RAG performed exceptionally well, with only one error out of 151 generated questions. For lab tests with relevant factors, the accuracy reached 100%. Even though we also compared the results from the non-RAG GPT-4-turbo with multiple credible sources, its performance still remained unsatisfactory. Table  5  highlights the single error, along with a similar case in which GPT-4-turbo with RAG provided a correct response. Unlike most lab tests, the normal ranges for the acid-fast stain and anti-smooth muscle antibody tests are no acid-fast bacteria found and no antibodies found, respectively. Although these results could be interpreted as zero, we did not apply any preprocessing for these cases.",
            "We further evaluated the RAG systems ability in retrieving normal reference ranges using the best factor retrieval model. Using GPT-4-turbo as the baseline, we relaxed the correctness criterion from an exact match with MedlinePlus to a match with any credible source. While GPT-4-turbo achieved less than 50% accuracy, GPT-4-turbo with RAG correctly retrieved nearly all normal ranges, making only one error. We implemented two evaluation methods: question-level accuracy, which measures the accuracy across all 151 questions, and lab-level accuracy, which assesses performance across 68 lab tests. The overall accuracy of the RAG system at the question level was 0.993, approximately 60.9% higher than the baseline model, while the lab-level accuracy was 0.985, about 52.9% higher. According to Table  5 , the only mistake GPT-4-turbo with RAG made was for the acid-fast stain test, where the correct result should have been no acid-fast bacteria found, but the system responded with N/A instead. We also examined a similar case involving the anti-smooth muscle antibody test, where GPT-4-turbo with RAG successfully indicated the correct normal result of no antibodies present. These cases are challenging because their normal results are non-numeric."
        ]
    },
    "global_footnotes": []
}