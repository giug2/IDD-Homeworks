{
    "PAPER'S NUMBER OF TABLES": 3,
    "S3.T1": {
        "caption": "Table 1: Evaluation on CIFAR-100 dataset.\n",
        "table": "",
        "footnotes": "\n\n\n\n\nAverage Accuracy\nAverage forgetting\nTraining time (s)\nTraining time (s)\nServer Runtime (s)\n\n\nğ’œ~~ğ’œ\\tilde{\\mathcal{A}} (%)\nf~~ğ‘“\\tilde{f}(%)\n(T=0ğ‘‡0T=0)\n(Tâ‰¥1ğ‘‡1T\\geq 1)\n\n\n\n\nFedAvg\n22.27Â±0.22plus-or-minus22.270.2222.27\\pm 0.22\n78.77Â±0.83plus-or-minus78.770.8378.77\\pm 0.83\nâ‰ˆ1.2absent1.2\\approx 1.2\nâ‰ˆ1.2absent1.2\\approx 1.2\nâ‰ˆ1.8absent1.8\\approx 1.8\n\nFedProx\n22.00Â±0.31plus-or-minus22.000.3122.00\\pm 0.31\n78.17Â±0.33plus-or-minus78.170.3378.17\\pm 0.33\nâ‰ˆ1.98absent1.98\\approx 1.98\nâ‰ˆ1.98absent1.98\\approx 1.98\nâ‰ˆ1.8absent1.8\\approx 1.8\n\nFedCIL\n26.8Â±0.44plus-or-minus26.80.4426.8\\pm 0.44\n38.19Â±0.31plus-or-minus38.190.3138.19\\pm 0.31\nâ‰ˆ17.8absent17.8\\approx 17.8\nâ‰ˆ24.5absent24.5\\approx 24.5\nâ‰ˆ2.5absent2.5\\approx 2.5 for T=1ğ‘‡1T=1, â‰ˆ4.55absent4.55\\approx 4.55 for T>1ğ‘‡1T>1\n\nFedLwF-2T\n22.17Â±0.13plus-or-minus22.170.1322.17\\pm 0.13\n75.08Â±0.72plus-or-minus75.080.7275.08\\pm 0.72\nâ‰ˆ1.2absent1.2\\approx 1.2\nâ‰ˆ3.4absent3.4\\approx 3.4\nâ‰ˆ1.8absent1.8\\approx 1.8\n\nMFCL (Ours)\n43.87Â±0.12plus-or-minus43.870.12\\mathbf{43.87\\pm 0.12}\n28.3Â±0.78plus-or-minus28.30.78\\mathbf{28.3\\pm 0.78}\nâ‰ˆ1.2absent1.2\\approx 1.2\nâ‰ˆ3.7absent3.7\\approx 3.7\nâ‰ˆ330absent330\\approx 330 (once per task), â‰ˆ1.8absent1.8\\approx 1.8 O.W.\n\nOracle\n67.12Â±0.4plus-or-minus67.120.467.12\\pm 0.4\nâˆ’â£âˆ’--\nâ‰ˆ1.2absent1.2\\approx 1.2\nâ‰ˆ1.2Ã—Tabsent1.2ğ‘‡\\approx 1.2\\times\\ T\nâ‰ˆ1.8absent1.8\\approx 1.8\n\n",
        "references": [
            "Table 1 shows each methodâ€™s average forgetting and accuracies. FedAvg and FedProx have the highest forgetting as they are not designed for FCL. Also, high forgetting for FedLwF-2T indicates that extra teachers cannot be effective in the absence of old data. FedCIL and MFCL have lower forgetting and better accuracy. MFCL outperforms FedCIL because the generative models in FedCIL need to train for a long time to generate effective synthetic data."
        ]
    },
    "A1.T2": {
        "caption": "Table 2: Generative model Architecture",
        "table": "",
        "footnotes": "",
        "references": [
            "In Table 2, we show the generative model architectures used for CIFAR-100. The global model has ResNet18 architecture, we change the first CONV layer kernel size to 3Ã—3333\\times 3 from 7Ã—7777\\times 7.\nIn this table, CONV layers are reported as CONVâ€‹KÃ—Kâ€‹(Ciâ€‹n,Coâ€‹uâ€‹t)CONVğ¾ğ¾subscriptğ¶ğ‘–ğ‘›subscriptğ¶ğ‘œğ‘¢ğ‘¡\\texttt{CONV}K\\times K(C_{in},C_{out}), where Kğ¾K, Ciâ€‹nsubscriptğ¶ğ‘–ğ‘›C_{in} and Coâ€‹uâ€‹tsubscriptğ¶ğ‘œğ‘¢ğ‘¡C_{out} are the size of the kernel, input channel and output channel of the layer, respectively."
        ]
    },
    "A1.T3": {
        "caption": "Table 3: Parameter Settings in different datasets",
        "table": "",
        "footnotes": "",
        "references": [
            "Table 3 presents some of the more important parameters."
        ]
    }
}