{
    "id_table_1": {
        "caption": "Table 1.  Types of errors occurred in generated code examples by LLMs.",
        "table": "S3.T1.1",
        "footnotes": [],
        "references": [
            "Figure  1  provides an overview of the proposed framework for assessing the API-oriented code generated by LLMs. To fulfill the four requirements, we design two tasks in  AutoAPIEval  for evaluating LLMs ability in API-oriented code generation:  API recommendation  and  code example generation . We also propose four evaluation metrics, which can be used to evaluate the quality of the recommended APIs for Task 1 and the generated code examples for Task 2, such as the proportion of correctly recommended APIs and the proportion of code examples that are uncompilable or unexecutable (see more details in Section  2.2 ).",
            "In Task 2, the four types of hallucinations defined in Task 1 also occur. For example, if a fabricated API is used in a generated code example, we classify it as a Factual Fabrication. Similarly, if the LLM fails to invoke the specified API, resulting in a NoAPIInvoked case, we label it as Instruction Inconsistency. However, in Task 2, a code example could still have compilation or runtime errors without hallucinating on APIs. In other words, the four types of hallucination errors cannot cover all cases. To address this, we conducted an open coding process to derive additional error categories (subcategories under compilation errors and runtime errors), following the methodology of prior studies  (Zhang et al . ,  2019 ; Wu et al . ,  2019 ; Seaman,  1999 ) . We began by randomly sampling 384 unexecutable code examples generated by MagiCoder. The first two authors manually reviewed 100 randomly selected examples from this set to develop an initial list of error categories. During this process, the categories were iteratively refined and revised. Once the error types were finalized, both authors independently applied these categories to the remaining 284 samples. After labeling, they discussed their findings to resolve disagreements and reached a consensus. The final list of derived error types is shown in Table  1 . We followed the same process for ChatGPT, i.e., randomly sampling 384 unexecutable code examples and categorizing them accordingly. It is important to note that a code example may be unexecutable or uncompilable due to hallucinations, such as invoking fabricated APIs. In such cases, we categorize it as a hallucination rather than a Runtime Error or Compilation Error.",
            "Retrieval-augmented generation (RAG) has been shown to enhance code generation by integrating relevant external knowledge into the language models context  (Chen et al . ,  2024 ; Parvez et al . ,  2021 ; Daneshvar et al . ,  2024 ; Lu et al . ,  2022 ; Tan et al . ,  2024 ; Nashid et al . ,  2023 ) . We aim to investigate whether employing RAG can improve API recommendation and code example generation.  For Task 1, we enrich the context provided to the LLM by including descriptions of both the package and the class in front of the Instruction section in the prompt template, as detailed in Section  2.1.1 . We denote this RAG strategy as  RAG T  1 d  e  s  c subscript superscript absent d e s c T 1 {}^{desc}_{T1} start_FLOATSUPERSCRIPT italic_d italic_e italic_s italic_c end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT italic_T 1 end_POSTSUBSCRIPT . For example, when requesting API recommendations for the class HashTable in the java.util package, we prepend the prompt with relevant descriptions: Package description: package description of java.util; Class description: class description of HashTable. In addition, we explore a variant of  RAG T  1 d  e  s  c subscript superscript absent d e s c T 1 {}^{desc}_{T1} start_FLOATSUPERSCRIPT italic_d italic_e italic_s italic_c end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT italic_T 1 end_POSTSUBSCRIPT , where we add a list of existing APIs within the class as the additional context to test if this further improves the LLMs ability to recommend correct APIs, when the existing correct APIs are actually provided. We denote this RAG strategy as  RAG T  1 d  e  s  c + A  P  I subscript superscript absent d e s c A P I T 1 {}^{desc+API}_{T1} start_FLOATSUPERSCRIPT italic_d italic_e italic_s italic_c + italic_A italic_P italic_I end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT italic_T 1 end_POSTSUBSCRIPT . In this study, we provide a list of 10 APIs.  For Task 2, we extend the context used in Task 1 by adding a detailed description of the specific API, including its summary, return type, and input parameters. Our goal is to determine if this additional information improves the quality of code example generation. We denote the RAG strategy as  RAG T  2 d  e  s  c subscript superscript absent d e s c T 2 {}^{desc}_{T2} start_FLOATSUPERSCRIPT italic_d italic_e italic_s italic_c end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT italic_T 2 end_POSTSUBSCRIPT .",
            "In the code example generation task, the most common error categories are runtime errors (46.4%/47.7%), followed by hallucinations (36.1%/24.5%) and compilation errors.  As presented in Table  4 , Runtime Errors occur the most frequently. 46.4% and 47.7% of the errors occur during runtime for MagiCoder and ChatGPT, respectively. In general, those two LLMs share similar patterns, the most frequent errors are Initialization Error. For instance, a code example for getClickCount() in the package java.awt.event.MouseEvent was generated by MagiCoder as shown in Listing  1 , null was passed as the Component source parameter for MouseEvent(). The source should not be null, it should be a valid component (like a JFrame or a JButton). The code was not initialized properly. Other examples of this type of error could relate to system environment preparation/configuration (e.g., system environment variable)."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.  The quality of API-oriented code generated by the three studied LLMs, MagiCoder, DeekSeek Coder, and ChatGPT for both tasks. The cells with the best result are marked in bold.",
        "table": "S4.T2.3.1",
        "footnotes": [],
        "references": [
            "Figure  1  provides an overview of the proposed framework for assessing the API-oriented code generated by LLMs. To fulfill the four requirements, we design two tasks in  AutoAPIEval  for evaluating LLMs ability in API-oriented code generation:  API recommendation  and  code example generation . We also propose four evaluation metrics, which can be used to evaluate the quality of the recommended APIs for Task 1 and the generated code examples for Task 2, such as the proportion of correctly recommended APIs and the proportion of code examples that are uncompilable or unexecutable (see more details in Section  2.2 ).",
            "We conducted our analysis on all the three LLMs for RQ1. Since the performance of MagiCoder and DeepSeek Coder are similar as shown in the results of RQ1 (ref. Table  2 ), we conducted our experiments and analysis on MagiCoder and ChatGPT for the rest RQs (RQ2-RQ4).",
            "In RQ1, we perform the two unit tasks on each of the 2,397 target classes extracted from JRE 8 leveraging three selected LLMs. We record the inference output from each prompt and extract the generated code. We then evaluate the quality of generated code using the metrics outlined in Section  2.2  for both tasks. We repeated each task 10 times to reduce the bias from randomness.",
            "For Task 1 - API recommendation, as discussed in Section  2.2 , we classify any recommended APIs that do not exist within the given package as incorrect, which can be considered hallucinations generated by the LLM. Consequently, we adopt an established categorization scheme from prior hallucination studies  (Huang et al . ,  2023 )  to classify the errors in API recommendations as follows:",
            "After collecting the factors, we divided the APIs and code examples into two groups. For Task 1, we categorized the APIs based on whether they were recommended correctly, as described in Section  2.2 : correctly recommended APIs ( API correct ) and incorrectly recommended APIs ( API incorrect ). For Task 2, we grouped the code examples into two classes: erroneous code examples ( Code erroneous ) and non-erroneous code examples ( Code non-erroneous ). To determine whether the studied factors differ significantly between the two groups in both tasks, we employed the Mann-Whitney U test, a non-parametric test that does not require any assumptions about the underlying data distribution. Additionally, we computed Cliffs d to measure the effect size of the differences between the two groups, indicating the magnitude of the difference. The interpretation of effect size follows the thresholds provided by Cliff  (Cliff,  1993 ) : d  0.147 indicates a negligible effect, d  0.33 indicates a small effect, d  0.474 indicates a medium effect, and values larger than 0.474 indicate a large effect. A factor with a significant difference and non-negligible effect size between the two groups indicates this factor is a good indicator of the difference between the two groups.",
            "Retrieval-augmented generation (RAG) has been shown to enhance code generation by integrating relevant external knowledge into the language models context  (Chen et al . ,  2024 ; Parvez et al . ,  2021 ; Daneshvar et al . ,  2024 ; Lu et al . ,  2022 ; Tan et al . ,  2024 ; Nashid et al . ,  2023 ) . We aim to investigate whether employing RAG can improve API recommendation and code example generation.  For Task 1, we enrich the context provided to the LLM by including descriptions of both the package and the class in front of the Instruction section in the prompt template, as detailed in Section  2.1.1 . We denote this RAG strategy as  RAG T  1 d  e  s  c subscript superscript absent d e s c T 1 {}^{desc}_{T1} start_FLOATSUPERSCRIPT italic_d italic_e italic_s italic_c end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT italic_T 1 end_POSTSUBSCRIPT . For example, when requesting API recommendations for the class HashTable in the java.util package, we prepend the prompt with relevant descriptions: Package description: package description of java.util; Class description: class description of HashTable. In addition, we explore a variant of  RAG T  1 d  e  s  c subscript superscript absent d e s c T 1 {}^{desc}_{T1} start_FLOATSUPERSCRIPT italic_d italic_e italic_s italic_c end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT italic_T 1 end_POSTSUBSCRIPT , where we add a list of existing APIs within the class as the additional context to test if this further improves the LLMs ability to recommend correct APIs, when the existing correct APIs are actually provided. We denote this RAG strategy as  RAG T  1 d  e  s  c + A  P  I subscript superscript absent d e s c A P I T 1 {}^{desc+API}_{T1} start_FLOATSUPERSCRIPT italic_d italic_e italic_s italic_c + italic_A italic_P italic_I end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT italic_T 1 end_POSTSUBSCRIPT . In this study, we provide a list of 10 APIs.  For Task 2, we extend the context used in Task 1 by adding a detailed description of the specific API, including its summary, return type, and input parameters. Our goal is to determine if this additional information improves the quality of code example generation. We denote the RAG strategy as  RAG T  2 d  e  s  c subscript superscript absent d e s c T 2 {}^{desc}_{T2} start_FLOATSUPERSCRIPT italic_d italic_e italic_s italic_c end_FLOATSUPERSCRIPT start_POSTSUBSCRIPT italic_T 2 end_POSTSUBSCRIPT .",
            "Hallucinations are prevalent in the API recommendation task, with 58.1% to 84.1% of the recommended APIs not existing in the specified package.  Table  2  summarizes the quality of recommended APIs for Task 1. Specifically, 84.1%, 82.9%, and 58.1% of the recommended APIs from MagiCoder, DeepSeek Coder, and ChatGPT, respectively, do not exist in the specified package. In addition, we analyze the errors that occurred within incorrect API recommendations by analyzing where the errors occurred in the method signature. Table  3  presents the types of errors produced by the three LLMs. 1.8% - 15.6% of the errors are due to not recommending method APIs (i.e., NotMethod), where LLMs suggested other class elements, such as fields, instead of methods in these cases. The majority of the errors involved recommending method names that do not exist. The remaining errors were attributed to incorrect return types or parameters (Incorrect ReturnType/Parameter). Notably, we only assessed the correctness of return types and parameters when the method name was accurate. Therefore, it is possible that multiple errors could occur in different parts of a single recommendation.  Interestingly, among the Incorrect Return/Parameter cases, a remarkable portion of errors (85.6% - 86.2%) resulted from combining return types and parameters from multiple overloaded methods. For example, the method boolean remove(Object o) was recommended by MagiCoder for the java.util.Hashtable class. However, only two overloaded methods V remove(Object key) and boolean remove(Object key, Object value) exist in this class. This is likely because of the common scenario of method overloading in Java, making LLMs confused when recommending API methods.",
            "39.4% to 54.5% of LLM-generated code examples have errors. More specifically, a range from 5.4% to 20.7% of the code examples fail to include the API they are intended to demonstrate and the rest of the generated code examples fail to compile or execute.  As shown in Table  2 , 5.4% to 20.7% of the recommended code examples omit the specified APIs entirely, which is a form of hallucination (see details in RQ2). In these cases, the LLM fails to follow the instructions to generate code examples for the given API. 20.4% to 25.5% of the generated code examples fail to compile and 11.4% and 15.7% of the code examples fail to execute successfully.",
            "Figure  2  demonstrates the partial dependent plots on Task1 when using ChatGPT, the result echoes our correlation analysis above. For instance, we observe the likelihood of generating an incorrect API is positively associated with PPL, API_length, while negatively associated with API_popularity, Consistency, and Probing. We observe similar patterns for Task 2 on ChatGPT and MagiCoder.",
            "Internal Validity   Prompt engineering has a significant impact on the LLMs performance  (Grabb,  2023 ) . Different prompts probably can lead to different results. However, as we discussed in Section  2 , our tasks are basic and straightforward, LLMs usually can follow the instructions specified in prompts to complete our tasks easily as the results in Section  4 .  In our framework, we propose two basic tasks, i.e., API recommendation and code examples generation, to benchmark an LLMs ability of API-oriented code generation. One threat is that an LLMs ability in our designed two tasks probably does not closely align with the LLMs ability to generate code for a specific task. However, as discussed in section  2 , the goal of our framework is to evaluate LLMs on any given library with API documentation automatically. Therefore, we do not include tasks such as code generation with specific requirements which typically need test cases in our framework. We believe our framework provides a lower boundary to assess LLMs capability for API-oriented code generation. Nevertheless, we encourage future research to include more tasks to reflect the LLMs ability to generate code using specific libraries with specific requirements.  Previous studies suggest that LLM settings, such as temperature and decoding strategies, can significantly affect the quality of generated content  (Renze and Guven,  2024 ; Thakur et al . ,  2024 ) . In this study, we use default settings for the studied LLMs for all RQs. However, our framework enables such analysis and we examined actually whether different LLM settings such as different temperatures and different decoding strategies (i.e., beam search, top K, and greedy search) have a measurable impact on the quality of generated code (due to space limit, we do not present here). In general, a lower temperature tends to produce code of similar or higher quality for both tasks and across both LLMs. Greedy Search, Beam Search, and Top-K share similar performance. Another threat is that certain APIs are version version-sensitive. We encourage future work to take this into consideration when using our framework for evaluation."
        ]
    },
    "id_table_3": {
        "caption": "Table 3.  The types of errors occurred in the recommended APIs by MagiCoder, DeepSeek Coder, and ChatGPT.",
        "table": "S4.T3.3.1",
        "footnotes": [],
        "references": [
            "In addition, our framework can facilitate further analysis, including 1) identifying errors in the generated code, 2) determining factors and indicators potentially related to code quality, and 3) evaluating solutions to mitigate errors in the generated code. We demonstrate how to apply our  AutoAPIEval  to conduct such analyses in Section  3  via a case study.",
            "Hallucinations are prevalent in the API recommendation task, with 58.1% to 84.1% of the recommended APIs not existing in the specified package.  Table  2  summarizes the quality of recommended APIs for Task 1. Specifically, 84.1%, 82.9%, and 58.1% of the recommended APIs from MagiCoder, DeepSeek Coder, and ChatGPT, respectively, do not exist in the specified package. In addition, we analyze the errors that occurred within incorrect API recommendations by analyzing where the errors occurred in the method signature. Table  3  presents the types of errors produced by the three LLMs. 1.8% - 15.6% of the errors are due to not recommending method APIs (i.e., NotMethod), where LLMs suggested other class elements, such as fields, instead of methods in these cases. The majority of the errors involved recommending method names that do not exist. The remaining errors were attributed to incorrect return types or parameters (Incorrect ReturnType/Parameter). Notably, we only assessed the correctness of return types and parameters when the method name was accurate. Therefore, it is possible that multiple errors could occur in different parts of a single recommendation.  Interestingly, among the Incorrect Return/Parameter cases, a remarkable portion of errors (85.6% - 86.2%) resulted from combining return types and parameters from multiple overloaded methods. For example, the method boolean remove(Object o) was recommended by MagiCoder for the java.util.Hashtable class. However, only two overloaded methods V remove(Object key) and boolean remove(Object key, Object value) exist in this class. This is likely because of the common scenario of method overloading in Java, making LLMs confused when recommending API methods."
        ]
    },
    "id_table_4": {
        "caption": "Table 4.  The count and distribution of each type of error in Task 1 and Task 2.",
        "table": "S4.T4.1",
        "footnotes": [],
        "references": [
            "For the API recommendation task, most errors are due to Factual Fabrication Hallucinations (46.0%/51.3%), followed by Instruction Inconsistencies (30.0%/31.5%), and finally, Factual Inconsistencies for both MagiCoder and ChatGPT.  Table  4  presents the distribution of error types for Task 1. A similar trend is observed for both MagiCoder and ChatGPT. The majority of errors stem from Factual Fabrication. For example, when asked to recommend APIs for java.util.Arrays, the LLM suggested createCompatibleGraphics(), which does not exist. Upon reviewing the API documentation, we found a similar method, createCompatibleImage(), indicating that the LLM likely confused the terms Graphics and Image after generating the prefix createCompatible. The second most common error is Instruction Inconsistency. For instance, the LLM often disregards the required format, omitting return types or parameters for certain static APIs. The least frequent error is Factual Inconsistency, where the LLM generates the correct API name but with incorrect return types or parameters.",
            "In the code example generation task, the most common error categories are runtime errors (46.4%/47.7%), followed by hallucinations (36.1%/24.5%) and compilation errors.  As presented in Table  4 , Runtime Errors occur the most frequently. 46.4% and 47.7% of the errors occur during runtime for MagiCoder and ChatGPT, respectively. In general, those two LLMs share similar patterns, the most frequent errors are Initialization Error. For instance, a code example for getClickCount() in the package java.awt.event.MouseEvent was generated by MagiCoder as shown in Listing  1 , null was passed as the Component source parameter for MouseEvent(). The source should not be null, it should be a valid component (like a JFrame or a JButton). The code was not initialized properly. Other examples of this type of error could relate to system environment preparation/configuration (e.g., system environment variable).",
            "Internal Validity   Prompt engineering has a significant impact on the LLMs performance  (Grabb,  2023 ) . Different prompts probably can lead to different results. However, as we discussed in Section  2 , our tasks are basic and straightforward, LLMs usually can follow the instructions specified in prompts to complete our tasks easily as the results in Section  4 .  In our framework, we propose two basic tasks, i.e., API recommendation and code examples generation, to benchmark an LLMs ability of API-oriented code generation. One threat is that an LLMs ability in our designed two tasks probably does not closely align with the LLMs ability to generate code for a specific task. However, as discussed in section  2 , the goal of our framework is to evaluate LLMs on any given library with API documentation automatically. Therefore, we do not include tasks such as code generation with specific requirements which typically need test cases in our framework. We believe our framework provides a lower boundary to assess LLMs capability for API-oriented code generation. Nevertheless, we encourage future research to include more tasks to reflect the LLMs ability to generate code using specific libraries with specific requirements.  Previous studies suggest that LLM settings, such as temperature and decoding strategies, can significantly affect the quality of generated content  (Renze and Guven,  2024 ; Thakur et al . ,  2024 ) . In this study, we use default settings for the studied LLMs for all RQs. However, our framework enables such analysis and we examined actually whether different LLM settings such as different temperatures and different decoding strategies (i.e., beam search, top K, and greedy search) have a measurable impact on the quality of generated code (due to space limit, we do not present here). In general, a lower temperature tends to produce code of similar or higher quality for both tasks and across both LLMs. Greedy Search, Beam Search, and Top-K share similar performance. Another threat is that certain APIs are version version-sensitive. We encourage future work to take this into consideration when using our framework for evaluation."
        ]
    },
    "id_table_5": {
        "caption": "Table 5.  Statistic test results for Task 1 and Task 2.",
        "table": "S4.T5.4.4",
        "footnotes": [],
        "references": [
            "For both tasks, all studied factors show a significant difference between the two groups of generated APIs/code examples.  Table  5  presents the statistical test results on the studied factors for Task 1 and Task 2.  For Task 1, in both LLMs, all factors exhibit significant differences between the two groups of APIs with non-negligible effect sizes with non-negligible effect size, except Probing on MagiCoder. For example, the popularity of API incorrect  is substantially lower than that of API correct  with a large effect size in both LLMs. This aligns with the expectation that a more popular API, which is likely to have more related usage in LLMs training data, increases the models likelihood of making correct recommendations. From the models perspective, consistency serve as a strong indicator for differentiating between the two API groups.  For Task 2, all studied factors demonstrate significant differences between the groups of erroneous and non-erroneous code examples, although API_length, PPL, and Consistency exhibit negligible effect size on ChatGPT."
        ]
    },
    "id_table_6": {
        "caption": "Table 6.  The classification result and feature importance on Task 1 and Task 2.",
        "table": "S4.T6.3",
        "footnotes": [],
        "references": [
            "Our classifiers achieve F1-scores of 0.96/0.88 for Task 1 and 0.8/0.76 for Task 2 in predicting incorrect recommended API or erroneous generated code.  Table  6  presents the classification performance for both tasks. The results suggest that our proposed factors are effective indicators for distinguishing between the two groups of APIs and code examples in both tasks. We also shows the feature importance of each factor for the trained classifiers in both tasks. Factors related to Model confidence such as Consistency and PPL and API-related factors API_propularity and API_length serve are all important for distinguishing two groups of APIs and code examples."
        ]
    },
    "id_table_7": {
        "caption": "Table 7.  Comparison of the quality of API-oriennted code generation by LLMs with/without RAG. The cells with better results are highlighted in bold.",
        "table": "S4.T7.4.4",
        "footnotes": [],
        "references": [
            "In general, RAG improves the quality of generated Code by LLMs, while RAGs improvements differ for different LLMs.  Table  7  compares the quality of code generated by LLMs with and without using RAG. Across both tasks, RAG improves code quality when MagiCoder and ChatGPT are used as the base LLMs. However, the magnitude of these improvements differs between the two models. Notably, RAG brings more substantial improvements for ChatGPT than for MagiCoder. For example, in Task 2, RAG reduces TotalError from 44.4% to 43.2% for MagiCoder, a modest improvement of 2.7%. In contrast, for ChatGPT, RAG decreases IncorrectAPI from 57.3% to 30.8%, representing a much larger improvement of 39.6%.",
            "For Task 1, it is surprising that even when provided with a list of correct APIs in the context, the LLMs still fail to recommend APIs accurately.  As shown in Table  7 , despite having the correct APIs listed along with the context, LLMs still make a significant amount of errors in their recommendations. Specifically, 40.3% of the APIs recommended by MagiCoder and 27.9% by ChatGPT do not exist in the specified package. This is unexpected, as the task should be straightforward - selecting from the provided list of correct APIs. One possible explanation is that LLMs sometimes disregard the given context and rely instead on their internal knowledge that is encapsulated in the model  (Su et al . ,  2024 ; Marjanovic et al . ,  2024 ) ."
        ]
    },
    "global_footnotes": [
        "https://www.vellum.ai/llm-leaderboard"
    ]
}