{
    "id_table_1": {
        "caption": "Table 1:  Language prediction on DS-1 using repeated 10-fold CV for LEGNet and the baselines in Section 2.4. The asterisk * indicates statistically worse performance ( p < 0.05 p 0.05 p<0.05 italic_p < 0.05 ) compared to the best performing model in highlighted in bold.",
        "table": "S3.T1.31.29",
        "footnotes": [],
        "references": [
            "An overview of our LEGNet model architecture is shown in Fig.  1 . LEGNet is designed to bridge the gap between the region- or node-based characterization of a lesion and rs-fMRI connectivity, which is defined on edges. As seen, our model includes three components: an edge-based learning module, a lesion encoding module, and a subgraph learning module that connects the two viewpoints.",
            "where  H i  j  R d 0 subscript H i j superscript R subscript d 0 \\mathbf{H}_{ij}\\in\\mathbb{R}^{d_{0}} bold_H start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  is the feature map of edge  ( i , j ) i j (i,j) ( italic_i , italic_j ) ,  N  ( i ) N i \\mathcal{N}(i) caligraphic_N ( italic_i )  is the set of neighboring nodes to ROI  i i i italic_i , including  i i i italic_i  itself,  r n  R d 0 subscript r n superscript R subscript d 0 \\mathbf{r}_{n}\\in\\mathbb{R}^{d_{0}} bold_r start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  and  c n  R d 0 subscript c n superscript R subscript d 0 \\mathbf{c}_{n}\\in\\mathbb{R}^{d_{0}} bold_c start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  are the learnable filters for each node  n n n italic_n , and   italic- \\phi italic_  is an activation function that is applied element-wise. Intuitively, Eq. ( 1 ) aggregates the connectivity information along neighboring edges that share the same end-nodes and updates the edge features accordingly.",
            "Table  1  reports the predictive performance of each method using repeated 10-fold CV on DS-1. LEGNet achieves the best performance in RMSE,  R 2 superscript R 2 R^{2} italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , and correlation coefficient. While it is second-best to BNC-masked in MAE, the difference is not statistically significant. As a baseline, we applied the LEGNet architecture to DS-1 from a random initialization, i.e., without having access to synthetic data. To avoid data leakage, we selected the hyperparameters based on the corresponding modules used in previous studies  [ 12 ,  15 ] . We note a statistically significant decrease in performance w/o HCP, which underscores the importance of using synthetic data to design and initialize the deep network.",
            "Finally, we tested generalization performance by applying the model that performs best on DS-1 to DS-2 without any fine-tuning (Table  2 ). In terms of  R 2 superscript R 2 R^{2} italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , LEGNet maintains a leading position but, along with the other baselines, also shows a decrease compared to the validation performance in Table  1 . While BrainGNN and SVR also show a decrease, BrainNetCNN-based models exhibit a sharper drop, indicating their reduced robustness on unseen data. The other three metrics follow a similar trend. This is expected due to the slight distribution shift between DS-1 and DS-2. Nevertheless, LEGNet still outperforms all baseline methods, indicating superior generalization ability."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Language prediction on DS-2 with the model from repeated 10-fold CV that generalizes the best. Top performance is highlighted in bold.",
        "table": "S3.T2.2",
        "footnotes": [],
        "references": [
            "Our pipeline for generating synthetic data is shown in Fig.  2 . We first simulate a unique structural lesion for each subject based on the following rules: (1) lesions are left-hemisphere only; (2) lesions are placed randomly but do not cross arterial territories  [ 13 ] ; (3) lesion sizes range from 5% to 20% of one arterial territory; (4) lesions are spatially continuous and simply-connected (i.e., without holes in the inside). Next, the artificial lesion is used to mask out voxels when computing ROI mean time series. We also diminish and add Gaussian noise to the connectivity represented in  X X \\mathbf{X} bold_X  between the lesioned region and the rest of the brain, followed by clipping the values to lie within the original connectivity range. Finally, the language performance score is re-scaled proportional to the percentage spared gray matter ( < 1 absent 1 <1 < 1 ) to simulate the negative impact of the lesion on functionality.",
            "Simulated-Lesion HCP (HCP-SL):  We use rs-fMRI data from 700 randomly selected subjects in the Human Connectome Project (HCP) S1200 database  [ 19 ]  as the foundation for generating synthetic data. Following the standard HCP minimal preprocessing pipeline  [ 18 ] , we parcellate the brain into 246 ROIs using the Brainnetome atlas  [ 6 ] . The subject language score is accuracy in answering simple math and story-related questions during an fMRI language task. Artificial lesions are inserted and modify the data as described in Section.  2.2 .",
            "Finally, we tested generalization performance by applying the model that performs best on DS-1 to DS-2 without any fine-tuning (Table  2 ). In terms of  R 2 superscript R 2 R^{2} italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , LEGNet maintains a leading position but, along with the other baselines, also shows a decrease compared to the validation performance in Table  1 . While BrainGNN and SVR also show a decrease, BrainNetCNN-based models exhibit a sharper drop, indicating their reduced robustness on unseen data. The other three metrics follow a similar trend. This is expected due to the slight distribution shift between DS-1 and DS-2. Nevertheless, LEGNet still outperforms all baseline methods, indicating superior generalization ability."
        ]
    }
}