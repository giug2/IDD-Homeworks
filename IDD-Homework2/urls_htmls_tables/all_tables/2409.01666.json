{
    "id_table_1": {
        "caption": "Table 1:  Comparisons among the long-context LLM without RAG, SELF-ROUTE mechanism  Li et al. ( 2024 )  and the proposed order-preserve (OP) RAG.",
        "table": "S4.T1.16",
        "footnotes": [],
        "references": [
            "We compare the proposed order-preserve RAG with two types of baselines. The first category of approaches uses the long-context LLM without RAG. As shown in Table  1 , without RAG, LLM takes a huge number of tokens as input, which is inefficient and costly. In contrast, the proposed order-preserve RAG not only significantly reduces the number of tokens, but also significantly improves the answer quality. For instance, using Llama3.1-70B model, the approach without RAG only achieves a  34.26 34.26 34.26 34.26  F1 score on EN.QA with an average of 117K tokens as input. In contrast, our OP-RAG with 48K tokens as input attains a  47.25 47.25 47.25 47.25  F1 score. The second category of baselines takes the SELF-ROUTE mechanism  Li et al. ( 2024 ) , which routes queries to RAG or long-context LLM based on the model self-reflection. As shown in Table  1 , ours significantly outperforms than using much fewer tokens in the input of LLMs."
        ]
    },
    "global_footnotes": []
}