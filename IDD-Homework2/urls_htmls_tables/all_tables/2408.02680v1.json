{
    "S2.T1": {
        "caption": "Table 1: Examples of mappings carried out by some of the current foundation models.",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S2.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.1\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\" id=\"S2.T1.1.1.1.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.1.1.1.1.1\">\n<span class=\"ltx_p\" id=\"S2.T1.1.1.1.1.1.1\" style=\"width:65.4pt;\">Input</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\" id=\"S2.T1.1.1.1.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.1.1.1.2.1\">\n<span class=\"ltx_p\" id=\"S2.T1.1.1.1.2.1.1\" style=\"width:28.5pt;\">Output</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\" id=\"S2.T1.1.1.1.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.1.1.1.3.1\">\n<span class=\"ltx_p\" id=\"S2.T1.1.1.1.3.1.1\" style=\"width:85.4pt;\">Foundation Models</span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S2.T1.1.2.1\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S2.T1.1.2.1.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.1.2.1.1.1\">\n<span class=\"ltx_p\" id=\"S2.T1.1.2.1.1.1.1\" style=\"width:65.4pt;\">Text</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S2.T1.1.2.1.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.1.2.1.2.1\">\n<span class=\"ltx_p\" id=\"S2.T1.1.2.1.2.1.1\" style=\"width:28.5pt;\">Text</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S2.T1.1.2.1.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.1.2.1.3.1\">\n<span class=\"ltx_p\" id=\"S2.T1.1.2.1.3.1.1\" style=\"width:85.4pt;\">GPT 3.5 Claude 3</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.3.2\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S2.T1.1.3.2.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.1.3.2.1.1\">\n<span class=\"ltx_p\" id=\"S2.T1.1.3.2.1.1.1\" style=\"width:65.4pt;\">Text</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S2.T1.1.3.2.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.1.3.2.2.1\">\n<span class=\"ltx_p\" id=\"S2.T1.1.3.2.2.1.1\" style=\"width:28.5pt;\">Images</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S2.T1.1.3.2.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.1.3.2.3.1\">\n<span class=\"ltx_p\" id=\"S2.T1.1.3.2.3.1.1\" style=\"width:85.4pt;\">DALL-E, Stable Diffusion, GPT 4</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.4.3\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S2.T1.1.4.3.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.1.4.3.1.1\">\n<span class=\"ltx_p\" id=\"S2.T1.1.4.3.1.1.1\" style=\"width:65.4pt;\">Text</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S2.T1.1.4.3.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.1.4.3.2.1\">\n<span class=\"ltx_p\" id=\"S2.T1.1.4.3.2.1.1\" style=\"width:28.5pt;\">Code</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S2.T1.1.4.3.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.1.4.3.3.1\">\n<span class=\"ltx_p\" id=\"S2.T1.1.4.3.3.1.1\" style=\"width:85.4pt;\">CoPilot, CodeWhisperer</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.5.4\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S2.T1.1.5.4.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.1.5.4.1.1\">\n<span class=\"ltx_p\" id=\"S2.T1.1.5.4.1.1.1\" style=\"width:65.4pt;\">Text</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S2.T1.1.5.4.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.1.5.4.2.1\">\n<span class=\"ltx_p\" id=\"S2.T1.1.5.4.2.1.1\" style=\"width:28.5pt;\">Music</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S2.T1.1.5.4.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.1.5.4.3.1\">\n<span class=\"ltx_p\" id=\"S2.T1.1.5.4.3.1.1\" style=\"width:85.4pt;\">MusicGen</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.6.5\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S2.T1.1.6.5.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.1.6.5.1.1\">\n<span class=\"ltx_p\" id=\"S2.T1.1.6.5.1.1.1\" style=\"width:65.4pt;\">Text, images</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S2.T1.1.6.5.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.1.6.5.2.1\">\n<span class=\"ltx_p\" id=\"S2.T1.1.6.5.2.1.1\" style=\"width:28.5pt;\">Robot actions</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S2.T1.1.6.5.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.1.6.5.3.1\">\n<span class=\"ltx_p\" id=\"S2.T1.1.6.5.3.1.1\" style=\"width:85.4pt;\">RT-2</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.7.6\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S2.T1.1.7.6.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.1.7.6.1.1\">\n<span class=\"ltx_p\" id=\"S2.T1.1.7.6.1.1.1\" style=\"width:65.4pt;\">Text, images, DNA</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S2.T1.1.7.6.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.1.7.6.2.1\">\n<span class=\"ltx_p\" id=\"S2.T1.1.7.6.2.1.1\" style=\"width:28.5pt;\">Text</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S2.T1.1.7.6.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.1.7.6.3.1\">\n<span class=\"ltx_p\" id=\"S2.T1.1.7.6.3.1.1\" style=\"width:85.4pt;\">Med-PaLM M</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.8.7\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S2.T1.1.8.7.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.1.8.7.1.1\">\n<span class=\"ltx_p\" id=\"S2.T1.1.8.7.1.1.1\" style=\"width:65.4pt;\">DNA</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S2.T1.1.8.7.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.1.8.7.2.1\">\n<span class=\"ltx_p\" id=\"S2.T1.1.8.7.2.1.1\" style=\"width:28.5pt;\">Cellular function</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S2.T1.1.8.7.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S2.T1.1.8.7.3.1\">\n<span class=\"ltx_p\" id=\"S2.T1.1.8.7.3.1.1\" style=\"width:85.4pt;\">Geneformer, scGPT</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "Foundation models often use a transformer architecture <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2408.02680v1#bib.bib14\" title=\"\">14</a>]</cite> with hundreds of billions of parameters. They are trained on large amounts of data using considerable computer resources. For example, GPT-1 was trained on 5GB, GPT-2 on 40GB, GPT-3 on 45TB and MusicGen on 20,000 hours of audio \n[14] with hundreds of billions of parameters. They are trained on large amounts of data using considerable computer resources. For example, GPT-1 was trained on 5GB, GPT-2 on 40GB, GPT-3 on 45TB and MusicGen on 20,000 hours of audio <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2408.02680v1#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2408.02680v1#bib.bib2\" title=\"\">2</a>]</cite>. After training, foundation models are given a prompt, such as a text input, and they generate an output, such as text, code or images. Some examples of foundation models are given in Table&#160;\n[16, 2]. After training, foundation models are given a prompt, such as a text input, and they generate an output, such as text, code or images. Some examples of foundation models are given in Table\u00a01."
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Data recorded by the rig. Schemas for the output files and an example recording are available at the project\u2019s GitHub page. Sample rates, such as image frequency, are configurable through the web interface. The data rates are based on the test recording on the project\u2019s GitHub repository, which contains one image per second. The data rates can vary, depending on the behaviour and environment of the user.",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T2.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.1\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.1.1.1.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.1.1.1.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.1.1.1.1.1\" style=\"width:42.7pt;\">Data</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.1.1.1.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.1.1.2.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.1.1.2.1.1\" style=\"width:42.7pt;\">Source</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.1.1.1.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.1.1.3.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.1.1.3.1.1\" style=\"width:85.4pt;\">Description</span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.1.1.1.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.1.1.4.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.1.1.4.1.1\" style=\"width:28.5pt;\">Rate (Kbps)</span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.2.1\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S3.T2.1.2.1.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.2.1.1.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.2.1.1.1.1\" style=\"width:42.7pt;\">EEG</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S3.T2.1.2.1.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.2.1.2.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.2.1.2.1.1\" style=\"width:42.7pt;\">Emotiv Epoc X headset</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S3.T2.1.2.1.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.2.1.3.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.2.1.3.1.1\" style=\"width:85.4pt;\">Raw EEG data from 14 channels</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S3.T2.1.2.1.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.2.1.4.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.2.1.4.1.1\" style=\"width:28.5pt;\">30</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.3.2\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.3.2.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.3.2.1.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.3.2.1.1.1\" style=\"width:42.7pt;\">Audio</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.3.2.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.3.2.2.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.3.2.2.1.1\" style=\"width:42.7pt;\">USB microphone</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.3.2.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.3.2.3.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.3.2.3.1.1\" style=\"width:85.4pt;\">Audio from microphone mounted on front of user.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.3.2.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.3.2.4.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.3.2.4.1.1\" style=\"width:28.5pt;\">20(mp3)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.4.3\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.4.3.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.4.3.1.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.4.3.1.1.1\" style=\"width:42.7pt;\">Images</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.4.3.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.4.3.2.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.4.3.2.1.1\" style=\"width:42.7pt;\">ZeroCam</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.4.3.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.4.3.3.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.4.3.3.1.1\" style=\"width:85.4pt;\">Pictures from camera mounted in front of user.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.4.3.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.4.3.4.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.4.3.4.1.1\" style=\"width:28.5pt;\">600(jpg)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.5.4\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.5.4.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.5.4.1.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.5.4.1.1.1\" style=\"width:42.7pt;\">GSR</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.5.4.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.5.4.2.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.5.4.2.1.1\" style=\"width:42.7pt;\">Grove GSR sensor</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.5.4.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.5.4.3.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.5.4.3.1.1\" style=\"width:85.4pt;\">Galvanic skin response (GSR) of user.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.5.4.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.5.4.4.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.5.4.4.1.1\" style=\"width:28.5pt;\">0.01(1 Hz)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.6.5\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.6.5.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.6.5.1.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.6.5.1.1.1\" style=\"width:42.7pt;\">EEG band power</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.6.5.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.6.5.2.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.6.5.2.1.1\" style=\"width:42.7pt;\">EmotivCortex API</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.6.5.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.6.5.3.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.6.5.3.1.1\" style=\"width:85.4pt;\">EEG power in the theta, alpha, beta L, beta H, and gamma bands.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.6.5.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.6.5.4.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.6.5.4.1.1\" style=\"width:28.5pt;\">8</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.7.6\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.7.6.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.7.6.1.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.7.6.1.1.1\" style=\"width:42.7pt;\">Facial expression</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.7.6.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.7.6.2.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.7.6.2.1.1\" style=\"width:42.7pt;\">EmotivCortex API</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.7.6.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.7.6.3.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.7.6.3.1.1\" style=\"width:85.4pt;\">Eye action and expression on upper and lower face.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.7.6.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.7.6.4.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.7.6.4.1.1\" style=\"width:28.5pt;\">4</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.8.7\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.8.7.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.8.7.1.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.8.7.1.1.1\" style=\"width:42.7pt;\">Cognition</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.8.7.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.8.7.2.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.8.7.2.1.1\" style=\"width:42.7pt;\">EmotivCortex API</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.8.7.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.8.7.3.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.8.7.3.1.1\" style=\"width:85.4pt;\">Cognitive states, including engagement, excitement, stress, relaxation, interest and focus.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.8.7.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.8.7.4.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.8.7.4.1.1\" style=\"width:28.5pt;\">0.02</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.9.8\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.9.8.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.9.8.1.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.9.8.1.1.1\" style=\"width:42.7pt;\">Audio text</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.9.8.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.9.8.2.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.9.8.2.1.1\" style=\"width:42.7pt;\">AWSTranscribe</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.9.8.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.9.8.3.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.9.8.3.1.1\" style=\"width:85.4pt;\">Recorded audio is converted to text using AWS Transcribe. Speech of wearer is automatically separated out from other people&#8217;s speech using an audio sample generated by the user.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.9.8.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.9.8.4.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.9.8.4.1.1\" style=\"width:28.5pt;\">0.003</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.10.9\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.10.9.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.10.9.1.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.10.9.1.1.1\" style=\"width:42.7pt;\">Speech sentiment</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.10.9.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.10.9.2.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.10.9.2.1.1\" style=\"width:42.7pt;\">AWSComprehend</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.10.9.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.10.9.3.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.10.9.3.1.1\" style=\"width:85.4pt;\">Text generated by user is analysed for sentiment (positive, negative, mixed, and neutral) using AWS Comprehend.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.10.9.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.10.9.4.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.10.9.4.1.1\" style=\"width:28.5pt;\">0.002</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.11.10\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.11.10.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.11.10.1.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.11.10.1.1.1\" style=\"width:42.7pt;\">DES</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.11.10.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.11.10.2.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.11.10.2.1.1\" style=\"width:42.7pt;\">User</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.11.10.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.11.10.3.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.11.10.3.1.1\" style=\"width:85.4pt;\">Descriptive Experience Sampling (DES) is a technique in which a person describes the contents of their consciousness when prompted by a tone <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2408.02680v1#bib.bib7\" title=\"\">7</a>]</cite>. Key phrases, such as &#8220;Start Ziggy&#8221; and &#8220;End Ziggy&#8221;, identify the start and end of a DES report.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.11.10.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.11.10.4.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.11.10.4.1.1\" style=\"width:28.5pt;\">0.001(per report)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.12.11\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.12.11.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.12.11.1.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.12.11.1.1.1\" style=\"width:42.7pt;\">Image text</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.12.11.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.12.11.2.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.12.11.2.1.1\" style=\"width:42.7pt;\">AWSRekognition</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.12.11.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.12.11.3.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.12.11.3.1.1\" style=\"width:85.4pt;\">Text in recorded images is identified using AWS Rekognition.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S3.T2.1.12.11.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.12.11.4.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.12.11.4.1.1\" style=\"width:28.5pt;\">0.001</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.13.12\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S3.T2.1.13.12.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.13.12.1.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.13.12.1.1.1\" style=\"width:42.7pt;\">Image labels</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S3.T2.1.13.12.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.13.12.2.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.13.12.2.1.1\" style=\"width:42.7pt;\">AWSRekognition</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S3.T2.1.13.12.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.13.12.3.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.13.12.3.1.1\" style=\"width:85.4pt;\">Labels for objects in recorded images are generated using AWS Rekognition.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S3.T2.1.13.12.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S3.T2.1.13.12.4.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.13.12.4.1.1\" style=\"width:28.5pt;\">2</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "The recorder is based on a Raspberry Pi, worn around the user\u2019s neck, which is connected to a camera, microphone, GSR sensor and speaker. Data recorded by the Raspberry Pi is sent to a web service running on a laptop carried by the user, which has a WebSocket connection to the Epoc X EEG headset. Cloud services, such as AWS Rekognition, and the Emotiv Cortex API<sup class=\"ltx_note_mark\">1</sup>\n1<sup class=\"ltx_note_mark\">1</sup>\n11https://emotiv.gitbook.io/cortex-api, are used to analyse the raw data for higher level properties, such as text contents, sentiment, cognition, facial expression, and object labels. A website hosted on the laptop enables the recorder to be configured and supports playback of recorded data (see Figure\u00a03). To reduce fraud a blockchain architecture is implemented that sends a hash of the data to the cloud and receives a hash of the data plus a random number known only to the cloud service, which is added to the next file in the sequence. To ensure the privacy of other people, all faces are automatically blurred during the recording process. The data is stored in JSON files; schema definitions for these files are available on the project website. Version 1.0 of the recorder is shown in Figure\u00a01. The architecture is illustrated in Figure\u00a02 and the data stored by the recorder is summarized in Table\u00a02.",
            "A second experiment was carried using images from the Socio-Moral Image Database (SMID), which were selected by Crone et al. <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2408.02680v1#bib.bib3\" title=\"\">3</a>]</cite> for their ability to elicit specific emotions in the viewer. Crone et al. also measured the valence and arousal that were elicited by these images in 2,716 participants. To compare the data captured by our recorder with the results from Crone et al.&#8217;s experiments, a proxy for arousal was calculated from the combination of excitement and stress (see Table&#160;\n[3] for their ability to elicit specific emotions in the viewer. Crone et al. also measured the valence and arousal that were elicited by these images in 2,716 participants. To compare the data captured by our recorder with the results from Crone et al.\u2019s experiments, a proxy for arousal was calculated from the combination of excitement and stress (see Table\u00a02) scaled to a value between -2.5 and 2.5. A selection of SMID images were then presented for 20 seconds each to a person wearing the recorder. The results are plotted in Figure\u00a04.",
            "The data in Table\u00a02 shows that the recorder can capture ~40 GB of data (images, audio and text) in a 16 hour day. If the images, audio, raw EEG and raw GSR are excluded, this figure drops to ~1 GB per 16 hour day. Based on these figures, Table\u00a03 gives some rough estimates of how long it would take to store enough data to train foundation models on the scale of GPT-1, GPT-2 and GPT-3."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Time required to record enough full data or text data (without raw GSR or EEG) to train GPT-1, GPT-2 and GPT-3. The recording times are based on 16 hour days.",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T3.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.1\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" id=\"S4.T3.1.1.1.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.1.1.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.1.1.1.1\" style=\"width:28.5pt;\">Model</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" id=\"S4.T3.1.1.1.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.1.2.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.1.2.1.1\" style=\"width:34.1pt;\">Training Data (GB)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" id=\"S4.T3.1.1.1.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.1.3.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.1.3.1.1\" style=\"width:56.9pt;\">Recording Time(days, full)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" id=\"S4.T3.1.1.1.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.1.1.4.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.1.1.4.1.1\" style=\"width:71.1pt;\">Recording time(days, selected text)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.2.2\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T3.1.2.2.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.2.2.1.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.2.2.1.1.1\" style=\"width:28.5pt;\">GPT-1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T3.1.2.2.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.2.2.2.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.2.2.2.1.1\" style=\"width:34.1pt;\">5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T3.1.2.2.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.2.2.3.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.2.2.3.1.1\" style=\"width:56.9pt;\">0.14</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"S4.T3.1.2.2.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.2.2.4.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.2.2.4.1.1\" style=\"width:71.1pt;\">6.5</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.3.3\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T3.1.3.3.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.3.3.1.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.3.3.1.1.1\" style=\"width:28.5pt;\">GPT-2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T3.1.3.3.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.3.3.2.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.3.3.2.1.1\" style=\"width:34.1pt;\">40</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T3.1.3.3.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.3.3.3.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.3.3.3.1.1\" style=\"width:56.9pt;\">1.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" id=\"S4.T3.1.3.3.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.3.3.4.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.3.3.4.1.1\" style=\"width:71.1pt;\">52</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.4.4\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S4.T3.1.4.4.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.4.4.1.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.4.4.1.1.1\" style=\"width:28.5pt;\">GPT-3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S4.T3.1.4.4.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.4.4.2.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.4.4.2.1.1\" style=\"width:34.1pt;\">46080</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S4.T3.1.4.4.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.4.4.3.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.4.4.3.1.1\" style=\"width:56.9pt;\">1300</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" id=\"S4.T3.1.4.4.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"S4.T3.1.4.4.4.1\">\n<span class=\"ltx_p\" id=\"S4.T3.1.4.4.4.1.1\" style=\"width:71.1pt;\">60000</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "The data in Table\u00a02 shows that the recorder can capture ~40 GB of data (images, audio and text) in a 16 hour day. If the images, audio, raw EEG and raw GSR are excluded, this figure drops to ~1 GB per 16 hour day. Based on these figures, Table\u00a03 gives some rough estimates of how long it would take to store enough data to train foundation models on the scale of GPT-1, GPT-2 and GPT-3.",
            "According to Table\u00a03, a purely text based version of GPT-2 could be created from ~50 days of data from a single individual. Larger models are likely to require data recorded from several individuals."
        ]
    }
}