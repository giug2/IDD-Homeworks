{
    "S3.T1": {
        "caption": "Table 1: Four typical gradient-based Meta-RL (GMRL) algorithms.",
        "table": "<table id=\"S3.T1.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T1.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">Category</th>\n<th id=\"S3.T1.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Algorithms</th>\n<th id=\"S3.T1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Meta parameter <math id=\"S3.T1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\phi\" display=\"inline\"><semantics id=\"S3.T1.1.1.1.m1.1a\"><mi id=\"S3.T1.1.1.1.m1.1.1\" xref=\"S3.T1.1.1.1.m1.1.1.cmml\">œï</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.1.1.1.m1.1b\"><ci id=\"S3.T1.1.1.1.m1.1.1.cmml\" xref=\"S3.T1.1.1.1.m1.1.1\">italic-œï</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.1.1.1.m1.1c\">\\phi</annotation></semantics></math>\n</th>\n<th id=\"S3.T1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Inner parameter <math id=\"S3.T1.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\theta\" display=\"inline\"><semantics id=\"S3.T1.2.2.2.m1.1a\"><mi id=\"S3.T1.2.2.2.m1.1.1\" xref=\"S3.T1.2.2.2.m1.1.1.cmml\">Œ∏</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.2.2.2.m1.1b\"><ci id=\"S3.T1.2.2.2.m1.1.1.cmml\" xref=\"S3.T1.2.2.2.m1.1.1\">ùúÉ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.2.2.2.m1.1c\">\\theta</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.2.3.1\" class=\"ltx_tr\">\n<td id=\"S3.T1.2.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\">Few-shot RL</td>\n<td id=\"S3.T1.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">MAML <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">10</a>]</cite>\n</td>\n<td id=\"S3.T1.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">Initial Parameter</td>\n<td id=\"S3.T1.2.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">Initial Parameter</td>\n</tr>\n<tr id=\"S3.T1.2.4.2\" class=\"ltx_tr\">\n<td id=\"S3.T1.2.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Opponent Shaping</td>\n<td id=\"S3.T1.2.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">LOLA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">13</a>]</cite>\n</td>\n<td id=\"S3.T1.2.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Ego-agent Policy</td>\n<td id=\"S3.T1.2.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Other-agent Policy</td>\n</tr>\n<tr id=\"S3.T1.2.5.3\" class=\"ltx_tr\">\n<td id=\"S3.T1.2.5.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Single-lifetime MGRL</td>\n<td id=\"S3.T1.2.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">MGRL <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">39</a>]</cite>\n</td>\n<td id=\"S3.T1.2.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Discount Factor</td>\n<td id=\"S3.T1.2.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">RL Agent Policy</td>\n</tr>\n<tr id=\"S3.T1.2.6.4\" class=\"ltx_tr\">\n<td id=\"S3.T1.2.6.4.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">Multi-lifetime MGRL</td>\n<td id=\"S3.T1.2.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">LPG <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">26</a>]</cite>\n</td>\n<td id=\"S3.T1.2.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">LSTM Network</td>\n<td id=\"S3.T1.2.6.4.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">RL Agent Policy</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "In the following part, we review existing GMRL algorithms and categorise them into four research topics. We offer one typical example for each topic in Table 1 and further discussed their relationship and how they can be unified into one framework in Sec.¬†3. We also provide a more self-contained explanation for each topic in Appendix A for readers that are not familiar with GMRL.",
            "In this section we conduct empirical evaluation of our proposed bias analysis and the proposed methods to mitigate the biases, and our experiments cover all 3 GMRL fields listed in Table¬†1. In particular, we conduct a tabular MDP experiment to show the existence of two biases discussed in Sec.¬†4 using MAML [10] and LIRPG [42]. In order to show how the proposed methods can mitigate the compositional bias, we consider a Iterated Prisoner Dilemma (IPD) problem and use LOLA [13] with off-policy corrections in Sec.¬†4.4; Similarly, we conduct evaluations on Atari games using MGRL [39] with LVC corrections in Sec.¬†4.4 to mitigate the Hessian estimation bias. We open source our code at https://github.com/Benjamin-eecs/Theoretical-GMRL."
        ]
    },
    "A9.T2": {
        "caption": "Table 2: Hyper-parameter settings for Tabular MDP.",
        "table": null,
        "footnotes": [],
        "references": [
            "We offer the hyperparameter settings for our Tabular MDP experiment in Table 2."
        ]
    },
    "A9.T3": {
        "caption": "Table 3: Hyper-parameter settings for LOLA-DiCE.",
        "table": null,
        "footnotes": [],
        "references": [
            "We offer the hyperparameter settings for our LOLA experiment in Table 3."
        ]
    },
    "A9.T4": {
        "caption": "Table 4: Hyper-parameter settings for MGRL.",
        "table": null,
        "footnotes": [],
        "references": [
            "We adopt the codebase of A2C from [21] and differentiable optimization library [28] to implement MGRL algorithms. We use a shared CNN network (3 Conv layers and one fully connected (FC) layer) for the policy network and critic network. The (out-channel, filters, stride) for each Conv layer is (32, 8√ó8888\\times 8, 4), (64, 4√ó4444\\times 4, 2) and (64, 3√ó3333\\times 3, 1) respectively while the hidden size is 512 for the FC layer. For the training loss, we adopt additional entropy regularisation for policy loss and Mean Square Error (MSE) for the value loss. We adopt the Generalized Advantage estimation (GAE) for advantage estimation. We offer the hyperparameter settings for our experiment in Table 4. We tun our algorithm for 125k inner updates, which corresponds to 40M environment steps for baseline A2C."
        ]
    }
}