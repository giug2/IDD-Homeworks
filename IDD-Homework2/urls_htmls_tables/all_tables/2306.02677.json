{
    "PAPER'S NUMBER OF TABLES": 4,
    "S3.SS2.SSS0.Px3.20": {
        "caption": "Table 1: Gram matrix of three-input parties.",
        "table": "<table id=\"S4.Ex1\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S4.Ex1.m1.97\" class=\"ltx_math_unparsed\" alttext=\"\\begin{split}A^{\\prime}B^{\\prime T}&amp;=AL_{A}(NN^{T})^{\\frac{1}{2}}(BL_{B}(NN^{T})^{\\frac{1}{2}})^{T},\\\\\n&amp;=AL_{A}(NN^{T})^{\\frac{1}{2}}(NN^{T})^{\\frac{1}{2}}L_{B}^{T}B^{T},\\\\\n&amp;=AL_{A}(NN^{T})L_{B}^{T}B^{T},\\\\\n&amp;=A(L_{A}N)(L_{B}N)^{T}B^{T},\\\\\n&amp;=AB^{T}=(BA^{T})^{T}.\\end{split}\" display=\"block\"><semantics id=\"S4.Ex1.m1.97a\"><mtable columnspacing=\"0pt\" displaystyle=\"true\" rowspacing=\"0pt\" id=\"S4.Ex1.m1.97.97.5\"><mtr id=\"S4.Ex1.m1.97.97.5a\"><mtd class=\"ltx_align_right\" columnalign=\"right\" id=\"S4.Ex1.m1.97.97.5b\"><mrow id=\"S4.Ex1.m1.4.4.4.4.4\"><msup id=\"S4.Ex1.m1.4.4.4.4.4.6\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.1.1.1.1.1.1\">A</mi><mo mathsize=\"90%\" id=\"S4.Ex1.m1.2.2.2.2.2.2.1\">′</mo></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.Ex1.m1.4.4.4.4.4.5\">​</mo><msup id=\"S4.Ex1.m1.4.4.4.4.4.7\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.3.3.3.3.3.3\">B</mi><mrow id=\"S4.Ex1.m1.4.4.4.4.4.4.1.2\"><mo mathsize=\"142%\" id=\"S4.Ex1.m1.4.4.4.4.4.4.1.2.1\">′</mo><mo lspace=\"0em\" id=\"S4.Ex1.m1.4.4.4.4.4.4.1.2.2\">⁣</mo><mi mathsize=\"90%\" id=\"S4.Ex1.m1.4.4.4.4.4.4.1.1\">T</mi></mrow></msup></mrow></mtd><mtd class=\"ltx_align_left\" columnalign=\"left\" id=\"S4.Ex1.m1.97.97.5c\"><mrow id=\"S4.Ex1.m1.93.93.1.93.28.24.24\"><mrow id=\"S4.Ex1.m1.93.93.1.93.28.24.24.1\"><mi id=\"S4.Ex1.m1.93.93.1.93.28.24.24.1.3\"></mi><mo mathsize=\"90%\" id=\"S4.Ex1.m1.5.5.5.5.1.1\">=</mo><mrow id=\"S4.Ex1.m1.93.93.1.93.28.24.24.1.2\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.6.6.6.6.2.2\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.Ex1.m1.93.93.1.93.28.24.24.1.2.3\">​</mo><msub id=\"S4.Ex1.m1.93.93.1.93.28.24.24.1.2.4\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.7.7.7.7.3.3\">L</mi><mi mathsize=\"90%\" id=\"S4.Ex1.m1.8.8.8.8.4.4.1\">A</mi></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.Ex1.m1.93.93.1.93.28.24.24.1.2.3a\">​</mo><msup id=\"S4.Ex1.m1.93.93.1.93.28.24.24.1.1.1\"><mrow id=\"S4.Ex1.m1.93.93.1.93.28.24.24.1.1.1.1.1\"><mo maxsize=\"90%\" minsize=\"90%\" id=\"S4.Ex1.m1.9.9.9.9.5.5\">(</mo><mrow id=\"S4.Ex1.m1.93.93.1.93.28.24.24.1.1.1.1.1.1\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.10.10.10.10.6.6\">N</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.Ex1.m1.93.93.1.93.28.24.24.1.1.1.1.1.1.1\">​</mo><msup id=\"S4.Ex1.m1.93.93.1.93.28.24.24.1.1.1.1.1.1.2\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.11.11.11.11.7.7\">N</mi><mi mathsize=\"90%\" id=\"S4.Ex1.m1.12.12.12.12.8.8.1\">T</mi></msup></mrow><mo maxsize=\"90%\" minsize=\"90%\" id=\"S4.Ex1.m1.13.13.13.13.9.9\">)</mo></mrow><mfrac id=\"S4.Ex1.m1.14.14.14.14.10.10.1\"><mn mathsize=\"90%\" id=\"S4.Ex1.m1.14.14.14.14.10.10.1.2\">1</mn><mn mathsize=\"90%\" id=\"S4.Ex1.m1.14.14.14.14.10.10.1.3\">2</mn></mfrac></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.Ex1.m1.93.93.1.93.28.24.24.1.2.3b\">​</mo><msup id=\"S4.Ex1.m1.93.93.1.93.28.24.24.1.2.2\"><mrow id=\"S4.Ex1.m1.93.93.1.93.28.24.24.1.2.2.1.1\"><mo maxsize=\"90%\" minsize=\"90%\" id=\"S4.Ex1.m1.15.15.15.15.11.11\">(</mo><mrow id=\"S4.Ex1.m1.93.93.1.93.28.24.24.1.2.2.1.1.1\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.16.16.16.16.12.12\">B</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.Ex1.m1.93.93.1.93.28.24.24.1.2.2.1.1.1.2\">​</mo><msub id=\"S4.Ex1.m1.93.93.1.93.28.24.24.1.2.2.1.1.1.3\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.17.17.17.17.13.13\">L</mi><mi mathsize=\"90%\" id=\"S4.Ex1.m1.18.18.18.18.14.14.1\">B</mi></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.Ex1.m1.93.93.1.93.28.24.24.1.2.2.1.1.1.2a\">​</mo><msup id=\"S4.Ex1.m1.93.93.1.93.28.24.24.1.2.2.1.1.1.1\"><mrow id=\"S4.Ex1.m1.93.93.1.93.28.24.24.1.2.2.1.1.1.1.1.1\"><mo maxsize=\"90%\" minsize=\"90%\" id=\"S4.Ex1.m1.19.19.19.19.15.15\">(</mo><mrow id=\"S4.Ex1.m1.93.93.1.93.28.24.24.1.2.2.1.1.1.1.1.1.1\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.20.20.20.20.16.16\">N</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.Ex1.m1.93.93.1.93.28.24.24.1.2.2.1.1.1.1.1.1.1.1\">​</mo><msup id=\"S4.Ex1.m1.93.93.1.93.28.24.24.1.2.2.1.1.1.1.1.1.1.2\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.21.21.21.21.17.17\">N</mi><mi mathsize=\"90%\" id=\"S4.Ex1.m1.22.22.22.22.18.18.1\">T</mi></msup></mrow><mo maxsize=\"90%\" minsize=\"90%\" id=\"S4.Ex1.m1.23.23.23.23.19.19\">)</mo></mrow><mfrac id=\"S4.Ex1.m1.24.24.24.24.20.20.1\"><mn mathsize=\"90%\" id=\"S4.Ex1.m1.24.24.24.24.20.20.1.2\">1</mn><mn mathsize=\"90%\" id=\"S4.Ex1.m1.24.24.24.24.20.20.1.3\">2</mn></mfrac></msup></mrow><mo maxsize=\"90%\" minsize=\"90%\" id=\"S4.Ex1.m1.25.25.25.25.21.21\">)</mo></mrow><mi mathsize=\"90%\" id=\"S4.Ex1.m1.26.26.26.26.22.22.1\">T</mi></msup></mrow></mrow><mo mathsize=\"90%\" id=\"S4.Ex1.m1.27.27.27.27.23.23\">,</mo></mrow></mtd></mtr><mtr id=\"S4.Ex1.m1.97.97.5d\"><mtd id=\"S4.Ex1.m1.97.97.5e\"></mtd><mtd class=\"ltx_align_left\" columnalign=\"left\" id=\"S4.Ex1.m1.97.97.5f\"><mrow id=\"S4.Ex1.m1.94.94.2.94.23.23.23\"><mrow id=\"S4.Ex1.m1.94.94.2.94.23.23.23.1\"><mi id=\"S4.Ex1.m1.94.94.2.94.23.23.23.1.3\"></mi><mo mathsize=\"90%\" id=\"S4.Ex1.m1.28.28.28.1.1.1\">=</mo><mrow id=\"S4.Ex1.m1.94.94.2.94.23.23.23.1.2\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.29.29.29.2.2.2\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.Ex1.m1.94.94.2.94.23.23.23.1.2.3\">​</mo><msub id=\"S4.Ex1.m1.94.94.2.94.23.23.23.1.2.4\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.30.30.30.3.3.3\">L</mi><mi mathsize=\"90%\" id=\"S4.Ex1.m1.31.31.31.4.4.4.1\">A</mi></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.Ex1.m1.94.94.2.94.23.23.23.1.2.3a\">​</mo><msup id=\"S4.Ex1.m1.94.94.2.94.23.23.23.1.1.1\"><mrow id=\"S4.Ex1.m1.94.94.2.94.23.23.23.1.1.1.1.1\"><mo maxsize=\"90%\" minsize=\"90%\" id=\"S4.Ex1.m1.32.32.32.5.5.5\">(</mo><mrow id=\"S4.Ex1.m1.94.94.2.94.23.23.23.1.1.1.1.1.1\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.33.33.33.6.6.6\">N</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.Ex1.m1.94.94.2.94.23.23.23.1.1.1.1.1.1.1\">​</mo><msup id=\"S4.Ex1.m1.94.94.2.94.23.23.23.1.1.1.1.1.1.2\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.34.34.34.7.7.7\">N</mi><mi mathsize=\"90%\" id=\"S4.Ex1.m1.35.35.35.8.8.8.1\">T</mi></msup></mrow><mo maxsize=\"90%\" minsize=\"90%\" id=\"S4.Ex1.m1.36.36.36.9.9.9\">)</mo></mrow><mfrac id=\"S4.Ex1.m1.37.37.37.10.10.10.1\"><mn mathsize=\"90%\" id=\"S4.Ex1.m1.37.37.37.10.10.10.1.2\">1</mn><mn mathsize=\"90%\" id=\"S4.Ex1.m1.37.37.37.10.10.10.1.3\">2</mn></mfrac></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.Ex1.m1.94.94.2.94.23.23.23.1.2.3b\">​</mo><msup id=\"S4.Ex1.m1.94.94.2.94.23.23.23.1.2.2\"><mrow id=\"S4.Ex1.m1.94.94.2.94.23.23.23.1.2.2.1.1\"><mo maxsize=\"90%\" minsize=\"90%\" id=\"S4.Ex1.m1.38.38.38.11.11.11\">(</mo><mrow id=\"S4.Ex1.m1.94.94.2.94.23.23.23.1.2.2.1.1.1\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.39.39.39.12.12.12\">N</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.Ex1.m1.94.94.2.94.23.23.23.1.2.2.1.1.1.1\">​</mo><msup id=\"S4.Ex1.m1.94.94.2.94.23.23.23.1.2.2.1.1.1.2\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.40.40.40.13.13.13\">N</mi><mi mathsize=\"90%\" id=\"S4.Ex1.m1.41.41.41.14.14.14.1\">T</mi></msup></mrow><mo maxsize=\"90%\" minsize=\"90%\" id=\"S4.Ex1.m1.42.42.42.15.15.15\">)</mo></mrow><mfrac id=\"S4.Ex1.m1.43.43.43.16.16.16.1\"><mn mathsize=\"90%\" id=\"S4.Ex1.m1.43.43.43.16.16.16.1.2\">1</mn><mn mathsize=\"90%\" id=\"S4.Ex1.m1.43.43.43.16.16.16.1.3\">2</mn></mfrac></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.Ex1.m1.94.94.2.94.23.23.23.1.2.3c\">​</mo><msubsup id=\"S4.Ex1.m1.94.94.2.94.23.23.23.1.2.5\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.44.44.44.17.17.17\">L</mi><mi mathsize=\"90%\" id=\"S4.Ex1.m1.45.45.45.18.18.18.1\">B</mi><mi mathsize=\"90%\" id=\"S4.Ex1.m1.46.46.46.19.19.19.1\">T</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.Ex1.m1.94.94.2.94.23.23.23.1.2.3d\">​</mo><msup id=\"S4.Ex1.m1.94.94.2.94.23.23.23.1.2.6\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.47.47.47.20.20.20\">B</mi><mi mathsize=\"90%\" id=\"S4.Ex1.m1.48.48.48.21.21.21.1\">T</mi></msup></mrow></mrow><mo mathsize=\"90%\" id=\"S4.Ex1.m1.49.49.49.22.22.22\">,</mo></mrow></mtd></mtr><mtr id=\"S4.Ex1.m1.97.97.5g\"><mtd id=\"S4.Ex1.m1.97.97.5h\"></mtd><mtd class=\"ltx_align_left\" columnalign=\"left\" id=\"S4.Ex1.m1.97.97.5i\"><mrow id=\"S4.Ex1.m1.95.95.3.95.16.16.16\"><mrow id=\"S4.Ex1.m1.95.95.3.95.16.16.16.1\"><mi id=\"S4.Ex1.m1.95.95.3.95.16.16.16.1.2\"></mi><mo mathsize=\"90%\" id=\"S4.Ex1.m1.50.50.50.1.1.1\">=</mo><mrow id=\"S4.Ex1.m1.95.95.3.95.16.16.16.1.1\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.51.51.51.2.2.2\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.Ex1.m1.95.95.3.95.16.16.16.1.1.2\">​</mo><msub id=\"S4.Ex1.m1.95.95.3.95.16.16.16.1.1.3\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.52.52.52.3.3.3\">L</mi><mi mathsize=\"90%\" id=\"S4.Ex1.m1.53.53.53.4.4.4.1\">A</mi></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.Ex1.m1.95.95.3.95.16.16.16.1.1.2a\">​</mo><mrow id=\"S4.Ex1.m1.95.95.3.95.16.16.16.1.1.1.1\"><mo maxsize=\"90%\" minsize=\"90%\" id=\"S4.Ex1.m1.54.54.54.5.5.5\">(</mo><mrow id=\"S4.Ex1.m1.95.95.3.95.16.16.16.1.1.1.1.1\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.55.55.55.6.6.6\">N</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.Ex1.m1.95.95.3.95.16.16.16.1.1.1.1.1.1\">​</mo><msup id=\"S4.Ex1.m1.95.95.3.95.16.16.16.1.1.1.1.1.2\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.56.56.56.7.7.7\">N</mi><mi mathsize=\"90%\" id=\"S4.Ex1.m1.57.57.57.8.8.8.1\">T</mi></msup></mrow><mo maxsize=\"90%\" minsize=\"90%\" id=\"S4.Ex1.m1.58.58.58.9.9.9\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.Ex1.m1.95.95.3.95.16.16.16.1.1.2b\">​</mo><msubsup id=\"S4.Ex1.m1.95.95.3.95.16.16.16.1.1.4\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.59.59.59.10.10.10\">L</mi><mi mathsize=\"90%\" id=\"S4.Ex1.m1.60.60.60.11.11.11.1\">B</mi><mi mathsize=\"90%\" id=\"S4.Ex1.m1.61.61.61.12.12.12.1\">T</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.Ex1.m1.95.95.3.95.16.16.16.1.1.2c\">​</mo><msup id=\"S4.Ex1.m1.95.95.3.95.16.16.16.1.1.5\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.62.62.62.13.13.13\">B</mi><mi mathsize=\"90%\" id=\"S4.Ex1.m1.63.63.63.14.14.14.1\">T</mi></msup></mrow></mrow><mo mathsize=\"90%\" id=\"S4.Ex1.m1.64.64.64.15.15.15\">,</mo></mrow></mtd></mtr><mtr id=\"S4.Ex1.m1.97.97.5j\"><mtd id=\"S4.Ex1.m1.97.97.5k\"></mtd><mtd class=\"ltx_align_left\" columnalign=\"left\" id=\"S4.Ex1.m1.97.97.5l\"><mrow id=\"S4.Ex1.m1.96.96.4.96.17.17.17\"><mrow id=\"S4.Ex1.m1.96.96.4.96.17.17.17.1\"><mi id=\"S4.Ex1.m1.96.96.4.96.17.17.17.1.3\"></mi><mo mathsize=\"90%\" id=\"S4.Ex1.m1.65.65.65.1.1.1\">=</mo><mrow id=\"S4.Ex1.m1.96.96.4.96.17.17.17.1.2\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.66.66.66.2.2.2\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.Ex1.m1.96.96.4.96.17.17.17.1.2.3\">​</mo><mrow id=\"S4.Ex1.m1.96.96.4.96.17.17.17.1.1.1.1\"><mo maxsize=\"90%\" minsize=\"90%\" id=\"S4.Ex1.m1.67.67.67.3.3.3\">(</mo><mrow id=\"S4.Ex1.m1.96.96.4.96.17.17.17.1.1.1.1.1\"><msub id=\"S4.Ex1.m1.96.96.4.96.17.17.17.1.1.1.1.1.2\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.68.68.68.4.4.4\">L</mi><mi mathsize=\"90%\" id=\"S4.Ex1.m1.69.69.69.5.5.5.1\">A</mi></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.Ex1.m1.96.96.4.96.17.17.17.1.1.1.1.1.1\">​</mo><mi mathsize=\"90%\" id=\"S4.Ex1.m1.70.70.70.6.6.6\">N</mi></mrow><mo maxsize=\"90%\" minsize=\"90%\" id=\"S4.Ex1.m1.71.71.71.7.7.7\">)</mo></mrow><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.Ex1.m1.96.96.4.96.17.17.17.1.2.3a\">​</mo><msup id=\"S4.Ex1.m1.96.96.4.96.17.17.17.1.2.2\"><mrow id=\"S4.Ex1.m1.96.96.4.96.17.17.17.1.2.2.1.1\"><mo maxsize=\"90%\" minsize=\"90%\" id=\"S4.Ex1.m1.72.72.72.8.8.8\">(</mo><mrow id=\"S4.Ex1.m1.96.96.4.96.17.17.17.1.2.2.1.1.1\"><msub id=\"S4.Ex1.m1.96.96.4.96.17.17.17.1.2.2.1.1.1.2\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.73.73.73.9.9.9\">L</mi><mi mathsize=\"90%\" id=\"S4.Ex1.m1.74.74.74.10.10.10.1\">B</mi></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.Ex1.m1.96.96.4.96.17.17.17.1.2.2.1.1.1.1\">​</mo><mi mathsize=\"90%\" id=\"S4.Ex1.m1.75.75.75.11.11.11\">N</mi></mrow><mo maxsize=\"90%\" minsize=\"90%\" id=\"S4.Ex1.m1.76.76.76.12.12.12\">)</mo></mrow><mi mathsize=\"90%\" id=\"S4.Ex1.m1.77.77.77.13.13.13.1\">T</mi></msup><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.Ex1.m1.96.96.4.96.17.17.17.1.2.3b\">​</mo><msup id=\"S4.Ex1.m1.96.96.4.96.17.17.17.1.2.4\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.78.78.78.14.14.14\">B</mi><mi mathsize=\"90%\" id=\"S4.Ex1.m1.79.79.79.15.15.15.1\">T</mi></msup></mrow></mrow><mo mathsize=\"90%\" id=\"S4.Ex1.m1.80.80.80.16.16.16\">,</mo></mrow></mtd></mtr><mtr id=\"S4.Ex1.m1.97.97.5m\"><mtd id=\"S4.Ex1.m1.97.97.5n\"></mtd><mtd class=\"ltx_align_left\" columnalign=\"left\" id=\"S4.Ex1.m1.97.97.5o\"><mrow id=\"S4.Ex1.m1.97.97.5.97.13.13.13\"><mrow id=\"S4.Ex1.m1.97.97.5.97.13.13.13.1\"><mi id=\"S4.Ex1.m1.97.97.5.97.13.13.13.1.3\"></mi><mo mathsize=\"90%\" id=\"S4.Ex1.m1.81.81.81.1.1.1\">=</mo><mrow id=\"S4.Ex1.m1.97.97.5.97.13.13.13.1.4\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.82.82.82.2.2.2\">A</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.Ex1.m1.97.97.5.97.13.13.13.1.4.1\">​</mo><msup id=\"S4.Ex1.m1.97.97.5.97.13.13.13.1.4.2\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.83.83.83.3.3.3\">B</mi><mi mathsize=\"90%\" id=\"S4.Ex1.m1.84.84.84.4.4.4.1\">T</mi></msup></mrow><mo mathsize=\"90%\" id=\"S4.Ex1.m1.85.85.85.5.5.5\">=</mo><msup id=\"S4.Ex1.m1.97.97.5.97.13.13.13.1.1\"><mrow id=\"S4.Ex1.m1.97.97.5.97.13.13.13.1.1.1.1\"><mo maxsize=\"90%\" minsize=\"90%\" id=\"S4.Ex1.m1.86.86.86.6.6.6\">(</mo><mrow id=\"S4.Ex1.m1.97.97.5.97.13.13.13.1.1.1.1.1\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.87.87.87.7.7.7\">B</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.Ex1.m1.97.97.5.97.13.13.13.1.1.1.1.1.1\">​</mo><msup id=\"S4.Ex1.m1.97.97.5.97.13.13.13.1.1.1.1.1.2\"><mi mathsize=\"90%\" id=\"S4.Ex1.m1.88.88.88.8.8.8\">A</mi><mi mathsize=\"90%\" id=\"S4.Ex1.m1.89.89.89.9.9.9.1\">T</mi></msup></mrow><mo maxsize=\"90%\" minsize=\"90%\" id=\"S4.Ex1.m1.90.90.90.10.10.10\">)</mo></mrow><mi mathsize=\"90%\" id=\"S4.Ex1.m1.91.91.91.11.11.11.1\">T</mi></msup></mrow><mo lspace=\"0em\" mathsize=\"90%\" id=\"S4.Ex1.m1.92.92.92.12.12.12\">.</mo></mrow></mtd></mtr></mtable><annotation encoding=\"application/x-tex\" id=\"S4.Ex1.m1.97b\">\\begin{split}A^{\\prime}B^{\\prime T}&amp;=AL_{A}(NN^{T})^{\\frac{1}{2}}(BL_{B}(NN^{T})^{\\frac{1}{2}})^{T},\\\\\n&amp;=AL_{A}(NN^{T})^{\\frac{1}{2}}(NN^{T})^{\\frac{1}{2}}L_{B}^{T}B^{T},\\\\\n&amp;=AL_{A}(NN^{T})L_{B}^{T}B^{T},\\\\\n&amp;=A(L_{A}N)(L_{B}N)^{T}B^{T},\\\\\n&amp;=AB^{T}=(BA^{T})^{T}.\\end{split}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n\n",
        "footnotes": "cell-space-limits = 1mm\n{NiceTabular}*5c[name=MyTbl5]Input Parties  A  B  C  X\nA  A​AT𝐴superscript𝐴𝑇AA^{T}  A​BT𝐴superscript𝐵𝑇AB^{T}  A​CT𝐴superscript𝐶𝑇AC^{T}  A​XT𝐴superscript𝑋𝑇AX^{T} \nB  B​AT𝐵superscript𝐴𝑇BA^{T}  B​BT𝐵superscript𝐵𝑇BB^{T}  B​CT𝐵superscript𝐶𝑇BC^{T}  B​XT𝐵superscript𝑋𝑇BX^{T} \nC  C​AT𝐶superscript𝐴𝑇CA^{T}  C​BT𝐶superscript𝐵𝑇CB^{T}  C​CT𝐶superscript𝐶𝑇CC^{T}  C​XT𝐶superscript𝑋𝑇CX^{T} \nX  X​AT𝑋superscript𝐴𝑇XA^{T}  X​BT𝑋superscript𝐵𝑇XB^{T}  X​CT𝑋superscript𝐶𝑇XC^{T}  X​XT𝑋superscript𝑋𝑇XX^{T} \nWe consider the semi-honest (or honest-but-curious) adversary model.\nIn a multi-party scenario, a semi-honest adversary (Evans et al., , 2018) corrupts an arbitrary subset of the parties involved. The corrupted parties follow the multi-party protocol as specified, i.e., the output of the protocol is correct. The corrupted parties try to learn private data from the messages they receive from uncorrupted parties. At the end of the protocol, the corrupted parties are allowed to share their information.FLAKE consists of a function party and a number of input parties. From Requirement Privacy follows that FLAKE needs to ensure two privacy properties: (i) the data of uncorrupted input parties must kept private from any corrupted input party or the function party, and (ii) a corrupted function party must not be able to learn the number of features.If the function party and all input parties operate honestly, privacy properties (i) and (ii) are ensured. If all input parties have been corrupted by a semi-honest adversary, privacy cannot ensured.\nBetween these extreme cases, we distinguish three cases for further analyses:A subset of the input parties is corrupted by a semi-honest adversary.The function party is corrupted by a semi-honest adversary.The function and a subset of input parties are corrupted by a semi-honest adversary.Recall that we do not consider extreme scenarios. In particular, we exclude data distributions where the number of features or the training data of one or more input parties can be guessed, and protocols with only one input party. However, to make the guessing harder, the input parties generate a unique matrix L𝐿L in each iteration. Therefore, the function party can not determine if an input party updates their data in a subsequent iteration. Also, all-zero rows are not allowed; though these are usually discarded as part of preprocessing anyway.Before we begin analysing the privacy of the protocol, we shall establish its correctness, which is unaffected by the existence of a semi-honest adversary.Without loss of generality, we assume there are two input parties Alice and Bob with individual left inverses LAsubscript𝐿𝐴L_{A} and LBsubscript𝐿𝐵L_{B} of a common mask matrix N𝑁N, whose outputs are A′=A​LA​(N​NT)12superscript𝐴′𝐴subscript𝐿𝐴superscript𝑁superscript𝑁𝑇12A^{\\prime}=AL_{A}(NN^{T})^{\\frac{1}{2}} and B′=B​LB​(N​NT)12superscript𝐵′𝐵subscript𝐿𝐵superscript𝑁superscript𝑁𝑇12B^{\\prime}=BL_{B}(NN^{T})^{\\frac{1}{2}}. Then, the correctness of the protocol follows as below.Analogously, correctness follows for A​AT𝐴superscript𝐴𝑇AA^{T} and B​BT𝐵superscript𝐵𝑇BB^{T}.\n∎We analyze Case (1) first. Since the input parties know the number of features, we only have to prove Property (ii), i.e.,\na corrupted function party cannot learn the number of features.FLAKE is secure against a semi-honest adversary who corrupts a subset of the input parties.Let SUsubscript𝑆𝑈S_{U} be the set of all input parties involved in the computation. While executing FLAKE protocol, an input party P∈SU𝑃subscript𝑆𝑈P\\in S_{U} has access only to the common mask N, the common seed used to generate N and the left inverse LPsubscript𝐿𝑃L_{P} of N generated by P. At any point in FLAKE protocol, the input party P gets neither the masked data of other input parties nor the computed Gram matrix using the masked data of all input parties. Thus, A semi-honest adversary corrupting a subset of input parties SC⊂SUsubscript𝑆𝐶subscript𝑆𝑈S_{C}\\subset S_{U} cannot learn the data of non-corrupted input parties SH⊂SUsubscript𝑆𝐻subscript𝑆𝑈S_{H}\\subset S_{U} where SC∩SH=∅subscript𝑆𝐶subscript𝑆𝐻S_{C}\\cap S_{H}=\\emptyset.FLAKE is, therefore, secure against the semi-honest adversary corrupting a subset of input parties. Because a semi-honest adversary follows the protocol, the data provided by the corrupted input parties do not affect the result of the computation. ∎Regarding Case (2), we need to prove that FLAKE does not allow a semi-honest function party to learn (i) input data nor (ii) the number of features.FLAKE is secure against a semi-honest adversary who corrupts the function party.A semi-honest function party is only the receiver of the masked data from the input parties, and follows the protocol as intended. Without loss of generality, let there be two input parties Alice and Bob with input data A∈ℝnA×f𝐴superscriptℝsubscript𝑛𝐴𝑓A\\in\\mathbb{R}^{n_{A}\\times f} and B∈ℝnB×f𝐵superscriptℝsubscript𝑛𝐵𝑓B\\in\\mathbb{R}^{n_{B}\\times f}, respectively, where nxsubscript𝑛𝑥n_{x} is the number of samples in the corresponding party and f𝑓f is the number of features. The semi-honest function party receives the masked input matrices of them, which are A′=A​LA​(N​NT)12∈ℝnA×ksuperscript𝐴′𝐴subscript𝐿𝐴superscript𝑁superscript𝑁𝑇12superscriptℝsubscript𝑛𝐴𝑘A^{\\prime}=AL_{A}(NN^{T})^{\\frac{1}{2}}\\in\\mathbb{R}^{n_{A}\\times k} and B′=B​LB​(N​NT)12∈ℝnB×ksuperscript𝐵′𝐵subscript𝐿𝐵superscript𝑁superscript𝑁𝑇12superscriptℝsubscript𝑛𝐵𝑘B^{\\prime}=BL_{B}(NN^{T})^{\\frac{1}{2}}\\in\\mathbb{R}^{n_{B}\\times k} where k>f𝑘𝑓k>f. Then, it computes A′​B′⁣T=A​BT∈ℝnA×nBsuperscript𝐴′superscript𝐵′𝑇𝐴superscript𝐵𝑇superscriptℝsubscript𝑛𝐴subscript𝑛𝐵A^{\\prime}B^{\\prime T}=AB^{T}\\in\\mathbb{R}^{n_{A}\\times n_{B}}, A′​A′⁣T=A​AT∈ℝnA×nAsuperscript𝐴′superscript𝐴′𝑇𝐴superscript𝐴𝑇superscriptℝsubscript𝑛𝐴subscript𝑛𝐴A^{\\prime}A^{\\prime T}=AA^{T}\\in\\mathbb{R}^{n_{A}\\times n_{A}} and B′​B′⁣T=B​BT∈ℝnB×nBsuperscript𝐵′superscript𝐵′𝑇𝐵superscript𝐵𝑇superscriptℝsubscript𝑛𝐵subscript𝑛𝐵B^{\\prime}B^{\\prime T}=BB^{T}\\in\\mathbb{R}^{n_{B}\\times n_{B}}. The data that the function party has access to then includesA′superscript𝐴′A^{\\prime} and analogously, B′superscript𝐵′B^{\\prime}.A​BT=(B​AT)T𝐴superscript𝐵𝑇superscript𝐵superscript𝐴𝑇𝑇AB^{T}=(BA^{T})^{T}, A​AT𝐴superscript𝐴𝑇AA^{T} and analogously B​BT𝐵superscript𝐵𝑇BB^{T}.Regarding (a)𝑎(a), it is trivial that A′superscript𝐴′A^{\\prime} does not reveal the number of features of A𝐴A. We now show that A′superscript𝐴′A^{\\prime} is not produced by a unique matrix A𝐴A. Given an orthogonal matrix O∈ℝf×f𝑂superscriptℝ𝑓𝑓O\\in\\mathbb{R}^{f\\times f} with f>1𝑓1f>1, for A~=A​O~𝐴𝐴𝑂\\tilde{A}=AO and LA~=OT​LAsubscript𝐿~𝐴superscript𝑂𝑇subscript𝐿𝐴L_{\\tilde{A}}=O^{T}L_{A}, we have A′=A~(LA~(NNT)12A^{\\prime}=\\tilde{A}(L_{\\tilde{A}}(NN^{T})^{\\frac{1}{2}}. Further, since we require that not all entries of any one sample is full of zeroes, the function party cannot deduce anything about A𝐴A from A′superscript𝐴′A^{\\prime}.\n\n \nRegarding (b)𝑏(b), the matrices that produce these Gram matrices are not unique, since for any orthogonal matrix O∈ℝf×f𝑂superscriptℝ𝑓𝑓O\\in\\mathbb{R}^{f\\times f} where f>1𝑓1f>1, labeling A~=A​O~𝐴𝐴𝑂\\tilde{A}=AO and B~=B​O~𝐵𝐵𝑂\\tilde{B}=BO, we haveIn consequence, the function party only learns the singular values and singular vectors of the matrices, i.e., it can find U𝑈U and S𝑆S from the singular value decomposition A=U​S​VT𝐴𝑈𝑆superscript𝑉𝑇A=USV^{T} by eigen-decomposing A​AT𝐴superscript𝐴𝑇AA^{T}. However, these values are insufficient to solve for A𝐴A since we can generate countless number of different orthogonal matrices (Aguilera and Pérez-Aguila, , 2004). The function party learns neither (i) input data nor (ii) the number of features.Although the function party obtains the Gram matrix, it cannot deduce the samples used to compute this Gram matrix, which was shown by (Ünal et al., , 2021). Details can be found in the supplementary material.∎Case (3) means that not only the function party, but also a subset of the input parties has been corrupted by a semi-honest adversary. In this case, since the adversary knows N𝑁N, the privacy of the data of the other parties is compromised since for data from a non-corrupt party Charlie of the form C′=C​LC​(N​NT)12superscript𝐶′𝐶subscript𝐿𝐶superscript𝑁superscript𝑁𝑇12C^{\\prime}=CL_{C}(NN^{T})^{\\frac{1}{2}}, the adversary can obtain C𝐶C by multiplying the data with (N​NT)12​LTsuperscript𝑁superscript𝑁𝑇12superscript𝐿𝑇(NN^{T})^{\\frac{1}{2}}L^{T}.In this section, we evaluate the performance of FLAKE and provide a run-time analysis.We experiment with three clinical data sets which contain medical records and, thus, have strong privacy concerns (Wolberg et al., , 1992; Ünal et al., , 2019; Center for Machine Learning and Intelligent Systems,\n, 2023). All of them are suitable for classification tasks. For the run-time analysis, we experimented with a synthetic data set with {500,1000,2000,4000,8000}5001000200040008000\\{500,1000,2000,4000,8000\\} data points (dp) for each input party.\nDetails about their statistics can be found in the supplementary material.Before starting with the run-time experiments, we want to compare FLAKE to other methods for randomization-based kernel computation for horizontally shared data. For this purpose, we implemented a 5-fold cross validation with FLAKE, ESCAPED (Ünal et al., , 2021), PPSVM (Yu et al., , 2006), RSVM (Lin et al., , 2015) and a naive SVM classifier in Python. Our experiments show that FLAKE, ESCAPED and the naive classifier produces the same results as they are exact solutions. Because of the introduced stochasticity, RSVM and PPSVM have a performance almost as good as the naive classifier, but they are not exact. Furthermore, the overhead associated with the various methods was measured for a single node and 1000 data points. The overhead for all methods was found to be extremely low, to the point of being negligible. Therefore, the subsequent experiments will primarily focus on scaling up the number of data points and input parties for FLAKE and ESCAPED, the two exact methods. For further details see the supplementary material.We implemented FLAKE for a scenario with three input parties and one function party.\nTo mimick the network communication between input parties and function party, we have implemented each party as an isolated process that communicates with others via TCP connections. Our four data sets are divided into three disjoint partitions. Each partition is assigned an input party. Each input party then masks its data according to the FLAKE protocol, and splits the masked data into chunks.\nAfter that, each input party compresses the chunks by zlib’s Deflate-algorithm, and forwards the compressed chunks to the function party. The function party deflates the chunks, computes the Gram matrix and a polynomial kernel.\nFinally, a SVM is trained with a 5-fold cross-validation. A grid search optimizes the corresponding hyperparameters C∈{2−4,…,210}𝐶superscript24…superscript210C\\in\\{2^{-4},...,2^{10}\\} (misclassification penalty) and p∈{1,…,5}𝑝1…5p\\in\\{1,...,5\\} (degree).All experiments were executed on a host with an AMD 7713 with 2.0GHZ and 512 GB of memory, which is a typical stand-alone server configuration for a small datacenter. We have used a single-threaded implementation. We repeated each experiment 10 times.We want to confirm that training time, masking time, communication time, gram-computation time and update time do not limit the applicability of FLAKE.\nAs known from literature, SVMs typically do not scale readily to very large data sets. In a centralized scenario, it is the training time for the SVM that limits the size of the input data.\nWe declare success, if we can show that the run-times of the stages of FLAKE in a federated scenario are negligible, compared to the stages required for the federated training of a SVM without masking.\nThe training takes place at the function party.\nFigure 1(c) shows the training time for varying numbers of dp in our synthetic data set. As expected, the longest takes the training of the data set with 8000 dp with 516.62 (±plus-or-minus\\pm 2.45) on average. Recall that 8000 dp means that each of our three input parties sends a masked data set of this size to the function party.To find out how much masking burdens the input parties, we ran a series of experiments, again with the synthetic data set. We varied the number of dp and measured the time for masking. Figure 1(a) reports the masking time measured for one input party. Even with 8000 dp per input party, the execution takes less than 0,00300030,003 (±plus-or-minus\\pm 0.0001) seconds on average. This masking time is negligible, compared to the time to train the SVM model, and does not restrict the applicability of FLAKE.Because our implementation runs on a data-center host, we estimate the communication time needed to send masked data from the input parties to the function party. The communication time T𝑇T can be estimated as shown in Equation 1:Our largest data set consists of 8000 points, which adds up to a Datasize of 1.31 MB for each input party. A typical VPN has a Bandwidth of 1.25MBps, with an average Latency of 0.1s and a Packetloss of 2% (Ookla, , 2022). For this set of parameters, the estimated the communication time is 1.05 seconds. Without Latency and Packetloss, it is 1.048 seconds. Recall that our experiments are executed on a single data-center host, i.e., the actual data transfer takes place as inter-process communication in the main memory of the host and virtually requires no time.We also measured the time the function party needs to compute the Gram matrix from the masked data from the input parties. Figure 1(b) shows that the computation time increases slightly more than linearly with the size of the data set, with no outliers. For 8000 dp, it took 0.99 (±plus-or-minus\\pm 0.0083) seconds on average to compute the Gram matrix. Again, 8000 dp means the function party receives 3x8000 masked data sets from our three input parties. In summary, we have confirmed that the Gram-computation time does not contribute much to the total computation time.Having shown that the time required to mask the data, send them to the function party, and compute the Gram matrix is several orders of magnitude below the time to train the model, we now consider updating the model.\nTo mimick a typical Federated Learning use case, where the training data increases due to dynamic data collection after the initial training, the data sets were updated with additional data in multiple training iterations.In particular, we performed multiple training iterations starting with a synthetic data set with 1000 dp for each input party. Figure 3 reports the run-times for masking the data and computing the Gram matrix for a three party scenario. We compared four training iterations of FLAKE and ESCAPED (Ünal et al., , 2021), where 1000 dp are added in each iteration. The experiment is measured in the same way as for the other diagrams. The figure confirms that FLAKE outperforms ESCAPED. In particular, masking with ESCAPED takes much more time. We conclude that updating the training data in FLAKE is an inexpensive operation and, thus, can be successfully applied in a FL setting.Many privacy-preserving machine learning methods ensure privacy by adding stochasticity, which decreases the result quality (privacy ∼similar-to\\sim utility trade-off) (Chen and Liu, , 2005; Chen et al., , 2007; Lin et al., , 2015; Lin, , 2013). In contrast, the function party in FLAKE obtains an exact Gram matrix (Requirement Accuracy), that can be used to compute any desired kernel matrix and later train any kernel-based machine learning algorithm, as if it was centralized data.\nESCAPED, which provides an accurate solution as well, requires more communication between the parties, which results in longer execution times (Ünal et al., , 2021). As shown in section 5, FLAKE is more efficient due to less communication rounds. Also, FLAKE allows input parties to update the Gram matrix with new samples independently of the previous samples. In ESCAPED, updating the Gram matrix with new samples is not supported. Instead, the Gram matrix must be recomputed using all the samples that the input parties have. After all, FLAKE has various advantages over preceding work using the randomized masking approach.Federated learning is an essential aspect of distributed machine learning, particularly when data privacy is a primary concern. However, when implementing both Federated Learning and privacy-preserving methods, the quality of model training can suffer as a result. In this work, we have proposed FLAKE, a Federated Learning Approach for KErnel methods, as a solution to that challenge. Our approach allows for the efficient and private computation of the Gram matrix from data that is distributed on multiple sources, enabling the training of kernel-based machine learning algorithms without any trade-offs in utility.\nInitially, four requirements were formulated, of which we have shown that FLAKE satisfies them: Privacy, Accuracy, Updatability and Efficiency. We showed, that FLAKE is both correct and private with regard to the considered threat models. We conducted various experiments on benchmark data sets to show FLAKE meets the accuracy and correctness of centralized models. Besides conducting experiments on well-known data sets, we also replicated the experiments of (Ünal et al., , 2019) on HIV V3 Loop Sequence data. While other privacy-preserving techniques can be computationally expensive, FLAKE is quite efficient. An analysis of FLAKE and comparable approaches shows, that FLAKE is not as computationally expensive. In order to expand the capabilities of the framework, additional common machine learning operations could be incorporated as future developments. Also, the masking and processing of vertically shared data could be included in FLAKE.We believe that FLAKE has the potential to improve healthcare outcomes and reduce costs while addressing the privacy concerns associated with machine learning on clinical data. We also think that it may find many use cases in other application domains that handle sensitive, distributed data.The following proof is based on a proof by Ünal et al.,  (2019).FLAKE provides security against a malicious function party A, assuming A is either semi-honest or malicious and does not collude with any input parties. In this scenario, A is unable to deduce the data of the input parties from the Gram matrix G𝐺G that is generated as a result.Although the number of features are hidden by FLAKE, we assume now the full Gram matrix G=D​DT𝐺𝐷superscript𝐷𝑇G=DD^{T} with the data of the input parties D=[A,B,C]𝐷𝐴𝐵𝐶D=[A,B,C] and the number of features are known to the function party. We show, that an attacker could not obtain any data since it there are multiple matrices that result in the Gram matrix.Assume that there is a rotation matrix R∈ℝN×N𝑅superscriptℝ𝑁𝑁R\\in\\mathbb{R}^{N\\times N} where N=2​(na+nb+nc)𝑁2subscript𝑛𝑎subscript𝑛𝑏subscript𝑛𝑐N=2(n_{a}+n_{b}+n_{c}) with nxsubscript𝑛𝑥n_{x} is the number of samples in the corresponding party. Then, there is a matrix E𝐸E which can be computed by E=R−1​D𝐸superscript𝑅1𝐷E=R^{-1}D. From that, we can say that D=R​E𝐷𝑅𝐸D=RE. Then, due to the rotation property of R−1=RTsuperscript𝑅1superscript𝑅𝑇R^{-1}=R^{T}, the the following holds:∎Since Aguilera and Pérez-Aguila,  (2004) showed, that countless rotation matrices can be generated, we cannot obtain a unique matrix resulting in Gram matrix G𝐺G: For every new rotation matrix θ∈ℝN×N𝜃superscriptℝ𝑁𝑁\\theta\\in\\mathbb{R}^{N\\times N}, there exists a new matrix β=θ−1​D𝛽superscript𝜃1𝐷\\beta=\\theta^{-1}D satisfying G=βT​β𝐺superscript𝛽𝑇𝛽G=\\beta^{T}\\beta. Thus, A is unable to deduce the input parties’ data D=(A,B,C)𝐷𝐴𝐵𝐶D=(A,B,C) from G=DD​T𝐺superscript𝐷𝐷𝑇G=D^{D}T.All methods employed a polynomial kernel and identical hyperparameter settings. For this implementation, Sequential Minimal Optimization (libsvm) provided by scikit-learn was used Zeng et al.,  (2008). Since the Pima Indian diabetes data set, HIV and Breast Cancer data set have an unbalanced distribution of classes, we have applied Macro Averaging. Correspondingly, for the balanced synthetic data set, Micro Averaging.",
        "references": [
            [
                "To integrate new data without having to rebuild the model from scratch (Requirement ",
                "Updatability",
                "), FLAKE provides a protocol for inference and updating the Gram matrix.\nWe can distinguish two cases: First, one of the input parties may have received new input data. Second, a new input party shall be integrated into the computation.\nFor simplicity, we again explain our protocol with three parties Alice, Bob and Charlie with their respective data sets ",
                "A",
                ",",
                "B",
                ",",
                "C",
                "𝐴",
                "𝐵",
                "𝐶",
                "A,B,C",
                ".",
                "Assume ",
                "C",
                "𝐶",
                "C",
                " has new data ",
                "X",
                "𝑋",
                "X",
                " which must be integrated into the Gram matrix shown in Table ",
                "3.2",
                ". ",
                "X",
                "𝑋",
                "X",
                " is the data set to be used for updating the model. To extend the gram matrix with the new values from ",
                "C",
                "𝐶",
                "C",
                ", the function party only needs to have the entries in the dashed rectangles. The party ",
                "C",
                "𝐶",
                "C",
                " uses the aforementioned masking and sending approaches for this purpose.\nNow assume that a new input party needs to be added. In this case, the function party must calculate the values in the continuous rectangles in Table ",
                "3.2",
                ". The remaining new entries can be computed locally by ",
                "C",
                "𝐶",
                "C",
                ". In both cases, updating the Gram matrix means that the function party has to calculate only a small set of new values. The vast majority of values need to be calculated just once, and a large share of the calculation effort remains at the input parties.\nNote that ",
                "X",
                "𝑋",
                "X",
                " can be also a test data set.",
                "When a party wants to leave the consortium the function party deletes all random components coming from this party and gram matrix entries that are calculated using these random components. This is important for compliance with legal regulations such as General Data Protection Regulation (GDPR) ",
                "(",
                "European Parliament and Council of the European Union, ",
                ", ",
                "2016",
                ")",
                ". It can be seen as an application of machine unlearning. In current FL methods, it is unclear and difficult how to eliminate a party’s contribution to the collaboratively trained ML model."
            ]
        ]
    },
    "S7.T2": {
        "caption": "Table 2: statistics of data sets used in the experiment section",
        "table": "<table id=\"S7.T2.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S7.T2.4.1.1\" class=\"ltx_tr\">\n<td id=\"S7.T2.4.1.1.1\" class=\"ltx_td ltx_border_tt\"></td>\n<th id=\"S7.T2.4.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span id=\"S7.T2.4.1.1.2.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Naive</span></th>\n<th id=\"S7.T2.4.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span id=\"S7.T2.4.1.1.3.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">FLAKE</span></th>\n</tr>\n<tr id=\"S7.T2.4.2.2\" class=\"ltx_tr\">\n<th id=\"S7.T2.4.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span id=\"S7.T2.4.2.2.1.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Data set</span></th>\n<th id=\"S7.T2.4.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S7.T2.4.2.2.2.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Number of data points</span></th>\n<th id=\"S7.T2.4.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S7.T2.4.2.2.3.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Number of Features</span></th>\n<th id=\"S7.T2.4.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S7.T2.4.2.2.4.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">binary/multi - label</span></th>\n<th id=\"S7.T2.4.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S7.T2.4.2.2.5.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">distribution</span></th>\n</tr>\n<tr id=\"S7.T2.4.3.3\" class=\"ltx_tr\">\n<td id=\"S7.T2.4.3.3.1\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S7.T2.4.3.3.1.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Diabetes</span></td>\n<td id=\"S7.T2.4.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T2.4.3.3.2.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">768</span></td>\n<td id=\"S7.T2.4.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T2.4.3.3.3.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">8</span></td>\n<td id=\"S7.T2.4.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T2.4.3.3.4.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">binary</span></td>\n<td id=\"S7.T2.4.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T2.4.3.3.5.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">in-balanced</span></td>\n</tr>\n<tr id=\"S7.T2.4.4.4\" class=\"ltx_tr\">\n<td id=\"S7.T2.4.4.4.1\" class=\"ltx_td ltx_align_left\"><span id=\"S7.T2.4.4.4.1.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Cancer</span></td>\n<td id=\"S7.T2.4.4.4.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T2.4.4.4.2.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">569</span></td>\n<td id=\"S7.T2.4.4.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T2.4.4.4.3.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">10</span></td>\n<td id=\"S7.T2.4.4.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T2.4.4.4.4.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">binary</span></td>\n<td id=\"S7.T2.4.4.4.5\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T2.4.4.4.5.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">in-balanced</span></td>\n</tr>\n<tr id=\"S7.T2.4.5.5\" class=\"ltx_tr\">\n<td id=\"S7.T2.4.5.5.1\" class=\"ltx_td ltx_align_left\"><span id=\"S7.T2.4.5.5.1.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">HIV</span></td>\n<td id=\"S7.T2.4.5.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T2.4.5.5.2.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">766</span></td>\n<td id=\"S7.T2.4.5.5.3\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T2.4.5.5.3.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">924</span></td>\n<td id=\"S7.T2.4.5.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T2.4.5.5.4.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">binary</span></td>\n<td id=\"S7.T2.4.5.5.5\" class=\"ltx_td ltx_align_center\"><span id=\"S7.T2.4.5.5.5.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">in-balanced</span></td>\n</tr>\n<tr id=\"S7.T2.4.6.6\" class=\"ltx_tr\">\n<td id=\"S7.T2.4.6.6.1\" class=\"ltx_td ltx_align_left ltx_border_bb\"><span id=\"S7.T2.4.6.6.1.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Synthetic</span></td>\n<td id=\"S7.T2.4.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T2.4.6.6.2.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">500-8000</span></td>\n<td id=\"S7.T2.4.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T2.4.6.6.3.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">20</span></td>\n<td id=\"S7.T2.4.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T2.4.6.6.4.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">multi class</span></td>\n<td id=\"S7.T2.4.6.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T2.4.6.6.5.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">balanced</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "All methods employed a polynomial kernel and identical hyperparameter settings. For this implementation, Sequential Minimal Optimization (libsvm) provided by scikit-learn was used ",
                "Zeng et al., ",
                " ",
                "(",
                "2008",
                ")",
                ". Since the Pima Indian diabetes data set, HIV and Breast Cancer data set have an unbalanced distribution of classes, we have applied Macro Averaging. Correspondingly, for the balanced synthetic data set, Micro Averaging."
            ]
        ]
    },
    "S7.T3": {
        "caption": "Table 3: ROC AUC with standard deviation for Naive SVM, FLAKE, ESCAPED, RSVM, PPSVM on various data sets.",
        "table": "<table id=\"S7.T3.20.20\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S7.T3.20.20.21.1\" class=\"ltx_tr\">\n<th id=\"S7.T3.20.20.21.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"></th>\n<th id=\"S7.T3.20.20.21.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S7.T3.20.20.21.1.2.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Naive</span></th>\n<th id=\"S7.T3.20.20.21.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S7.T3.20.20.21.1.3.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">FLAKE</span></th>\n<th id=\"S7.T3.20.20.21.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S7.T3.20.20.21.1.4.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">ESCAPED</span></th>\n<th id=\"S7.T3.20.20.21.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S7.T3.20.20.21.1.5.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">RSVM</span></th>\n<th id=\"S7.T3.20.20.21.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S7.T3.20.20.21.1.6.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">PPSVM</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S7.T3.5.5.5\" class=\"ltx_tr\">\n<th id=\"S7.T3.5.5.5.6\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span id=\"S7.T3.5.5.5.6.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Diabetes</span></th>\n<td id=\"S7.T3.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S7.T3.1.1.1.1.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">0.97</span><math id=\"S7.T3.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S7.T3.1.1.1.1.m1.1a\"><mo mathsize=\"70%\" id=\"S7.T3.1.1.1.1.m1.1.1\" xref=\"S7.T3.1.1.1.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S7.T3.1.1.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S7.T3.1.1.1.1.m1.1.1.cmml\" xref=\"S7.T3.1.1.1.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T3.1.1.1.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"S7.T3.1.1.1.1.2\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\"> 0.04</span>\n</td>\n<td id=\"S7.T3.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S7.T3.2.2.2.2.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">0.97</span><math id=\"S7.T3.2.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S7.T3.2.2.2.2.m1.1a\"><mo mathsize=\"70%\" id=\"S7.T3.2.2.2.2.m1.1.1\" xref=\"S7.T3.2.2.2.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S7.T3.2.2.2.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S7.T3.2.2.2.2.m1.1.1.cmml\" xref=\"S7.T3.2.2.2.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T3.2.2.2.2.m1.1c\">\\pm</annotation></semantics></math><span id=\"S7.T3.2.2.2.2.2\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\"> 0.04</span>\n</td>\n<td id=\"S7.T3.3.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S7.T3.3.3.3.3.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">0.97</span><math id=\"S7.T3.3.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S7.T3.3.3.3.3.m1.1a\"><mo mathsize=\"70%\" id=\"S7.T3.3.3.3.3.m1.1.1\" xref=\"S7.T3.3.3.3.3.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S7.T3.3.3.3.3.m1.1b\"><csymbol cd=\"latexml\" id=\"S7.T3.3.3.3.3.m1.1.1.cmml\" xref=\"S7.T3.3.3.3.3.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T3.3.3.3.3.m1.1c\">\\pm</annotation></semantics></math><span id=\"S7.T3.3.3.3.3.2\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\"> 0.04</span>\n</td>\n<td id=\"S7.T3.4.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S7.T3.4.4.4.4.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">0.95</span><math id=\"S7.T3.4.4.4.4.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S7.T3.4.4.4.4.m1.1a\"><mo mathsize=\"70%\" id=\"S7.T3.4.4.4.4.m1.1.1\" xref=\"S7.T3.4.4.4.4.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S7.T3.4.4.4.4.m1.1b\"><csymbol cd=\"latexml\" id=\"S7.T3.4.4.4.4.m1.1.1.cmml\" xref=\"S7.T3.4.4.4.4.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T3.4.4.4.4.m1.1c\">\\pm</annotation></semantics></math><span id=\"S7.T3.4.4.4.4.2\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\"> 0.02</span>\n</td>\n<td id=\"S7.T3.5.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S7.T3.5.5.5.5.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">0.94</span><math id=\"S7.T3.5.5.5.5.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S7.T3.5.5.5.5.m1.1a\"><mo mathsize=\"70%\" id=\"S7.T3.5.5.5.5.m1.1.1\" xref=\"S7.T3.5.5.5.5.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S7.T3.5.5.5.5.m1.1b\"><csymbol cd=\"latexml\" id=\"S7.T3.5.5.5.5.m1.1.1.cmml\" xref=\"S7.T3.5.5.5.5.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T3.5.5.5.5.m1.1c\">\\pm</annotation></semantics></math><span id=\"S7.T3.5.5.5.5.2\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\"> 0.04</span>\n</td>\n</tr>\n<tr id=\"S7.T3.10.10.10\" class=\"ltx_tr\">\n<th id=\"S7.T3.10.10.10.6\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S7.T3.10.10.10.6.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Cancer</span></th>\n<td id=\"S7.T3.6.6.6.1\" class=\"ltx_td ltx_align_center\">\n<span id=\"S7.T3.6.6.6.1.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">0.97 </span><math id=\"S7.T3.6.6.6.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S7.T3.6.6.6.1.m1.1a\"><mo mathsize=\"70%\" id=\"S7.T3.6.6.6.1.m1.1.1\" xref=\"S7.T3.6.6.6.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S7.T3.6.6.6.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S7.T3.6.6.6.1.m1.1.1.cmml\" xref=\"S7.T3.6.6.6.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T3.6.6.6.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"S7.T3.6.6.6.1.2\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\"> 0.03</span>\n</td>\n<td id=\"S7.T3.7.7.7.2\" class=\"ltx_td ltx_align_center\">\n<span id=\"S7.T3.7.7.7.2.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">0.97 </span><math id=\"S7.T3.7.7.7.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S7.T3.7.7.7.2.m1.1a\"><mo mathsize=\"70%\" id=\"S7.T3.7.7.7.2.m1.1.1\" xref=\"S7.T3.7.7.7.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S7.T3.7.7.7.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S7.T3.7.7.7.2.m1.1.1.cmml\" xref=\"S7.T3.7.7.7.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T3.7.7.7.2.m1.1c\">\\pm</annotation></semantics></math><span id=\"S7.T3.7.7.7.2.2\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\"> 0.03</span>\n</td>\n<td id=\"S7.T3.8.8.8.3\" class=\"ltx_td ltx_align_center\">\n<span id=\"S7.T3.8.8.8.3.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">0.97 </span><math id=\"S7.T3.8.8.8.3.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S7.T3.8.8.8.3.m1.1a\"><mo mathsize=\"70%\" id=\"S7.T3.8.8.8.3.m1.1.1\" xref=\"S7.T3.8.8.8.3.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S7.T3.8.8.8.3.m1.1b\"><csymbol cd=\"latexml\" id=\"S7.T3.8.8.8.3.m1.1.1.cmml\" xref=\"S7.T3.8.8.8.3.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T3.8.8.8.3.m1.1c\">\\pm</annotation></semantics></math><span id=\"S7.T3.8.8.8.3.2\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\"> 0.03</span>\n</td>\n<td id=\"S7.T3.9.9.9.4\" class=\"ltx_td ltx_align_center\">\n<span id=\"S7.T3.9.9.9.4.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">0.96 </span><math id=\"S7.T3.9.9.9.4.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S7.T3.9.9.9.4.m1.1a\"><mo mathsize=\"70%\" id=\"S7.T3.9.9.9.4.m1.1.1\" xref=\"S7.T3.9.9.9.4.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S7.T3.9.9.9.4.m1.1b\"><csymbol cd=\"latexml\" id=\"S7.T3.9.9.9.4.m1.1.1.cmml\" xref=\"S7.T3.9.9.9.4.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T3.9.9.9.4.m1.1c\">\\pm</annotation></semantics></math><span id=\"S7.T3.9.9.9.4.2\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\"> 0.02</span>\n</td>\n<td id=\"S7.T3.10.10.10.5\" class=\"ltx_td ltx_align_center\">\n<span id=\"S7.T3.10.10.10.5.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">0.97 </span><math id=\"S7.T3.10.10.10.5.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S7.T3.10.10.10.5.m1.1a\"><mo mathsize=\"70%\" id=\"S7.T3.10.10.10.5.m1.1.1\" xref=\"S7.T3.10.10.10.5.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S7.T3.10.10.10.5.m1.1b\"><csymbol cd=\"latexml\" id=\"S7.T3.10.10.10.5.m1.1.1.cmml\" xref=\"S7.T3.10.10.10.5.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T3.10.10.10.5.m1.1c\">\\pm</annotation></semantics></math><span id=\"S7.T3.10.10.10.5.2\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\"> 0.04</span>\n</td>\n</tr>\n<tr id=\"S7.T3.15.15.15\" class=\"ltx_tr\">\n<th id=\"S7.T3.15.15.15.6\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S7.T3.15.15.15.6.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">HIV</span></th>\n<td id=\"S7.T3.11.11.11.1\" class=\"ltx_td ltx_align_center\">\n<span id=\"S7.T3.11.11.11.1.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">0.78</span><math id=\"S7.T3.11.11.11.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S7.T3.11.11.11.1.m1.1a\"><mo mathsize=\"70%\" id=\"S7.T3.11.11.11.1.m1.1.1\" xref=\"S7.T3.11.11.11.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S7.T3.11.11.11.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S7.T3.11.11.11.1.m1.1.1.cmml\" xref=\"S7.T3.11.11.11.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T3.11.11.11.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"S7.T3.11.11.11.1.2\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\"> 0.03</span>\n</td>\n<td id=\"S7.T3.12.12.12.2\" class=\"ltx_td ltx_align_center\">\n<span id=\"S7.T3.12.12.12.2.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">0.78</span><math id=\"S7.T3.12.12.12.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S7.T3.12.12.12.2.m1.1a\"><mo mathsize=\"70%\" id=\"S7.T3.12.12.12.2.m1.1.1\" xref=\"S7.T3.12.12.12.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S7.T3.12.12.12.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S7.T3.12.12.12.2.m1.1.1.cmml\" xref=\"S7.T3.12.12.12.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T3.12.12.12.2.m1.1c\">\\pm</annotation></semantics></math><span id=\"S7.T3.12.12.12.2.2\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\"> 0.03</span>\n</td>\n<td id=\"S7.T3.13.13.13.3\" class=\"ltx_td ltx_align_center\">\n<span id=\"S7.T3.13.13.13.3.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">0.78</span><math id=\"S7.T3.13.13.13.3.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S7.T3.13.13.13.3.m1.1a\"><mo mathsize=\"70%\" id=\"S7.T3.13.13.13.3.m1.1.1\" xref=\"S7.T3.13.13.13.3.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S7.T3.13.13.13.3.m1.1b\"><csymbol cd=\"latexml\" id=\"S7.T3.13.13.13.3.m1.1.1.cmml\" xref=\"S7.T3.13.13.13.3.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T3.13.13.13.3.m1.1c\">\\pm</annotation></semantics></math><span id=\"S7.T3.13.13.13.3.2\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\"> 0.03</span>\n</td>\n<td id=\"S7.T3.14.14.14.4\" class=\"ltx_td ltx_align_center\">\n<span id=\"S7.T3.14.14.14.4.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">0.65</span><math id=\"S7.T3.14.14.14.4.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S7.T3.14.14.14.4.m1.1a\"><mo mathsize=\"70%\" id=\"S7.T3.14.14.14.4.m1.1.1\" xref=\"S7.T3.14.14.14.4.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S7.T3.14.14.14.4.m1.1b\"><csymbol cd=\"latexml\" id=\"S7.T3.14.14.14.4.m1.1.1.cmml\" xref=\"S7.T3.14.14.14.4.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T3.14.14.14.4.m1.1c\">\\pm</annotation></semantics></math><span id=\"S7.T3.14.14.14.4.2\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\"> 0.17</span>\n</td>\n<td id=\"S7.T3.15.15.15.5\" class=\"ltx_td ltx_align_center\">\n<span id=\"S7.T3.15.15.15.5.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">0.64</span><math id=\"S7.T3.15.15.15.5.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S7.T3.15.15.15.5.m1.1a\"><mo mathsize=\"70%\" id=\"S7.T3.15.15.15.5.m1.1.1\" xref=\"S7.T3.15.15.15.5.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S7.T3.15.15.15.5.m1.1b\"><csymbol cd=\"latexml\" id=\"S7.T3.15.15.15.5.m1.1.1.cmml\" xref=\"S7.T3.15.15.15.5.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T3.15.15.15.5.m1.1c\">\\pm</annotation></semantics></math><span id=\"S7.T3.15.15.15.5.2\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\"> 0.10</span>\n</td>\n</tr>\n<tr id=\"S7.T3.20.20.20\" class=\"ltx_tr\">\n<th id=\"S7.T3.20.20.20.6\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span id=\"S7.T3.20.20.20.6.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Synthetic</span></th>\n<td id=\"S7.T3.16.16.16.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"S7.T3.16.16.16.1.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">0.97</span><math id=\"S7.T3.16.16.16.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S7.T3.16.16.16.1.m1.1a\"><mo mathsize=\"70%\" id=\"S7.T3.16.16.16.1.m1.1.1\" xref=\"S7.T3.16.16.16.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S7.T3.16.16.16.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S7.T3.16.16.16.1.m1.1.1.cmml\" xref=\"S7.T3.16.16.16.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T3.16.16.16.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"S7.T3.16.16.16.1.2\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\"> 0.01</span>\n</td>\n<td id=\"S7.T3.17.17.17.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"S7.T3.17.17.17.2.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">0.97</span><math id=\"S7.T3.17.17.17.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S7.T3.17.17.17.2.m1.1a\"><mo mathsize=\"70%\" id=\"S7.T3.17.17.17.2.m1.1.1\" xref=\"S7.T3.17.17.17.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S7.T3.17.17.17.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S7.T3.17.17.17.2.m1.1.1.cmml\" xref=\"S7.T3.17.17.17.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T3.17.17.17.2.m1.1c\">\\pm</annotation></semantics></math><span id=\"S7.T3.17.17.17.2.2\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\"> 0.01</span>\n</td>\n<td id=\"S7.T3.18.18.18.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"S7.T3.18.18.18.3.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">0.97</span><math id=\"S7.T3.18.18.18.3.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S7.T3.18.18.18.3.m1.1a\"><mo mathsize=\"70%\" id=\"S7.T3.18.18.18.3.m1.1.1\" xref=\"S7.T3.18.18.18.3.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S7.T3.18.18.18.3.m1.1b\"><csymbol cd=\"latexml\" id=\"S7.T3.18.18.18.3.m1.1.1.cmml\" xref=\"S7.T3.18.18.18.3.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T3.18.18.18.3.m1.1c\">\\pm</annotation></semantics></math><span id=\"S7.T3.18.18.18.3.2\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\"> 0.01</span>\n</td>\n<td id=\"S7.T3.19.19.19.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"S7.T3.19.19.19.4.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">0.83</span><math id=\"S7.T3.19.19.19.4.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S7.T3.19.19.19.4.m1.1a\"><mo mathsize=\"70%\" id=\"S7.T3.19.19.19.4.m1.1.1\" xref=\"S7.T3.19.19.19.4.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S7.T3.19.19.19.4.m1.1b\"><csymbol cd=\"latexml\" id=\"S7.T3.19.19.19.4.m1.1.1.cmml\" xref=\"S7.T3.19.19.19.4.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T3.19.19.19.4.m1.1c\">\\pm</annotation></semantics></math><span id=\"S7.T3.19.19.19.4.2\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\"> 0.04</span>\n</td>\n<td id=\"S7.T3.20.20.20.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<span id=\"S7.T3.20.20.20.5.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">0.95</span><math id=\"S7.T3.20.20.20.5.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S7.T3.20.20.20.5.m1.1a\"><mo mathsize=\"70%\" id=\"S7.T3.20.20.20.5.m1.1.1\" xref=\"S7.T3.20.20.20.5.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S7.T3.20.20.20.5.m1.1b\"><csymbol cd=\"latexml\" id=\"S7.T3.20.20.20.5.m1.1.1.cmml\" xref=\"S7.T3.20.20.20.5.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S7.T3.20.20.20.5.m1.1c\">\\pm</annotation></semantics></math><span id=\"S7.T3.20.20.20.5.2\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\"> 0.01</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Assume C𝐶C has new data X𝑋X which must be integrated into the Gram matrix shown in Table 3.2. X𝑋X is the data set to be used for updating the model. To extend the gram matrix with the new values from C𝐶C, the function party only needs to have the entries in the dashed rectangles. The party C𝐶C uses the aforementioned masking and sending approaches for this purpose.\nNow assume that a new input party needs to be added. In this case, the function party must calculate the values in the continuous rectangles in Table 3.2. The remaining new entries can be computed locally by C𝐶C. In both cases, updating the Gram matrix means that the function party has to calculate only a small set of new values. The vast majority of values need to be calculated just once, and a large share of the calculation effort remains at the input parties.\nNote that X𝑋X can be also a test data set."
        ]
    },
    "S7.T4": {
        "caption": "Table 4: Overhead (Masking time + Gram time) for FLAKE, ESCAPED, RSVM, PPSVM for three input parties with 1000 dp each.",
        "table": "<table id=\"S7.T4.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S7.T4.4.1.1\" class=\"ltx_tr\">\n<th id=\"S7.T4.4.1.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"></th>\n<th id=\"S7.T4.4.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S7.T4.4.1.1.2.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">FLAKE</span></th>\n<th id=\"S7.T4.4.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S7.T4.4.1.1.3.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">ESCAPED</span></th>\n<th id=\"S7.T4.4.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S7.T4.4.1.1.4.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">RSVM</span></th>\n<th id=\"S7.T4.4.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S7.T4.4.1.1.5.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">PPSVM</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S7.T4.4.2.1\" class=\"ltx_tr\">\n<th id=\"S7.T4.4.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span id=\"S7.T4.4.2.1.1.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Masking time for one IP</span></th>\n<td id=\"S7.T4.4.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T4.4.2.1.2.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">0.00146</span></td>\n<td id=\"S7.T4.4.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T4.4.2.1.3.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">1.23610</span></td>\n<td id=\"S7.T4.4.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T4.4.2.1.4.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">0.00201</span></td>\n<td id=\"S7.T4.4.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S7.T4.4.2.1.5.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">0.02257</span></td>\n</tr>\n<tr id=\"S7.T4.4.3.2\" class=\"ltx_tr\">\n<th id=\"S7.T4.4.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span id=\"S7.T4.4.3.2.1.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">Time to compute Gram</span></th>\n<td id=\"S7.T4.4.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T4.4.3.2.2.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">0.02071</span></td>\n<td id=\"S7.T4.4.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T4.4.3.2.3.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">0.03156</span></td>\n<td id=\"S7.T4.4.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T4.4.3.2.4.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">0.00530</span></td>\n<td id=\"S7.T4.4.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S7.T4.4.3.2.5.1\" class=\"ltx_text ltx_font_smallcaps\" style=\"font-size:70%;\">0.01121</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "All methods employed a polynomial kernel and identical hyperparameter settings. For this implementation, Sequential Minimal Optimization (libsvm) provided by scikit-learn was used ",
                "Zeng et al., ",
                " ",
                "(",
                "2008",
                ")",
                ". Since the Pima Indian diabetes data set, HIV and Breast Cancer data set have an unbalanced distribution of classes, we have applied Macro Averaging. Correspondingly, for the balanced synthetic data set, Micro Averaging."
            ]
        ]
    }
}