{
    "PAPER'S NUMBER OF TABLES": 5,
    "S2.T1": {
        "caption": "Table 1: Examples of weak supervision available for an utterance. Here, semantic cost (fraction of slots incorrect) is illustrated as the feedback signal.",
        "table": "<table id=\"S2.T1.5\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S2.T1.5.1.1\" class=\"ltx_tr\">\n<th id=\"S2.T1.5.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.5.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.5.1.1.1.1.1\" class=\"ltx_p\" style=\"width:62.6pt;\"><span id=\"S2.T1.5.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Transcription</span></span>\n</span>\n</th>\n<th id=\"S2.T1.5.1.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.5.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.5.1.1.2.1.1\" class=\"ltx_p\" style=\"width:142.3pt;\"><span id=\"S2.T1.5.1.1.2.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">play Halo by Beyonce in main speaker</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S2.T1.5.2.1\" class=\"ltx_tr\">\n<td id=\"S2.T1.5.2.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.5.2.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.5.2.1.1.1.1\" class=\"ltx_p\" style=\"width:62.6pt;\"><span id=\"S2.T1.5.2.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ASR hypothesis</span></span>\n</span>\n</td>\n<td id=\"S2.T1.5.2.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.5.2.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.5.2.1.2.1.1\" class=\"ltx_p\" style=\"width:142.3pt;\"><span id=\"S2.T1.5.2.1.2.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">play Hello by Beyond in main speaker</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.5.3.2\" class=\"ltx_tr\">\n<td id=\"S2.T1.5.3.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.5.3.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.5.3.2.1.1.1\" class=\"ltx_p\" style=\"width:62.6pt;\"><span id=\"S2.T1.5.3.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">NLU semantics</span></span>\n</span>\n</td>\n<td id=\"S2.T1.5.3.2.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.5.3.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.5.3.2.2.1.1\" class=\"ltx_p\" style=\"width:142.3pt;\"><span id=\"S2.T1.5.3.2.2.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">PlaySong, Artist:Beyonce, Song: Halo, Device: Main speaker</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S2.T1.5.4.3\" class=\"ltx_tr\">\n<td id=\"S2.T1.5.4.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.5.4.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.5.4.3.1.1.1\" class=\"ltx_p\" style=\"width:62.6pt;\"><span id=\"S2.T1.5.4.3.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">semantic cost</span></span>\n</span>\n</td>\n<td id=\"S2.T1.5.4.3.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"S2.T1.5.4.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S2.T1.5.4.3.2.1.1\" class=\"ltx_p\" style=\"width:142.3pt;\"><span id=\"S2.T1.5.4.3.2.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">2/3</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Weak supervision signals can be used to further improve the performance of the system by leveraging information beyond just the unlabeled audio that self-learning relies on. This work exploits information weaker than the ground truth ASR transcription, which could be recovered from user interactions with the conversational agent. For example, if a user ",
                "stops",
                ", ",
                "cancels",
                " or ",
                "repeats",
                " a request in the subsequent turn of a dialog, it indicates that the previous query was unsuccessfully processed by the device. We study updating ASR models with the help of such a feedback score, potentially indicating whether the user’s request was unsuccessful. Further, the correct natural language understanding (NLU) semantics in the form of the correct slot value may eventually be recovered, for e.g., through an explicit re-invocation by the user. Hence, we also study leveraging weak feedback in the form of the NLU slot labels. An example of weak supervision for an utterance can be seen in Table ",
                "1",
                ".",
                "In this work, we demonstrate the impact of weak supervision labels in two forms: (1) machine generated NLU semantics: from an alternate spoken language understanding (SLU) built from ASR",
                "→",
                "→",
                "\\rightarrow",
                "NLU as a proxy for inferred semantics from user session data; (2) synthetic user feedback scores: a proxy for real user corrections, and available only for the hypothesis served to the user. This framework can accommodate many types of weak supervision information."
            ]
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Performance of federated self-learning with weak supervision on the SLURP dataset, including examples of corrected utterances.",
        "table": "<table id=\"S4.T2.1\" class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.1.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.2.1.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T2.1.2.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.1.2.1.1.1.1\" class=\"ltx_p\" style=\"width:142.3pt;\"><span id=\"S4.T2.1.2.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Setting</span></span>\n</span>\n</td>\n<td id=\"S4.T2.1.2.1.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T2.1.2.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.1.2.1.2.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\"><span id=\"S4.T2.1.2.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T2.1.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.3.2.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T2.1.3.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.1.3.2.1.1.1\" class=\"ltx_p\" style=\"width:142.3pt;\"><span id=\"S4.T2.1.3.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Initial</span></span>\n</span>\n</td>\n<td id=\"S4.T2.1.3.2.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T2.1.3.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.1.3.2.2.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\"><span id=\"S4.T2.1.3.2.2.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">28.70</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T2.1.4.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.4.3.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_l ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T2.1.4.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.1.4.3.1.1.1\" class=\"ltx_p\" style=\"width:142.3pt;\"><span id=\"S4.T2.1.4.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Oracle supervised finetuning</span></span>\n</span>\n</td>\n<td id=\"S4.T2.1.4.3.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T2.1.4.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.1.4.3.2.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\"><span id=\"S4.T2.1.4.3.2.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">16.95</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T2.1.5.4\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.5.4.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\" colspan=\"2\"><span id=\"S4.T2.1.5.4.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Self-learning</span></td>\n</tr>\n<tr id=\"S4.T2.1.6.5\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.6.5.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T2.1.6.5.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.1.6.5.1.1.1\" class=\"ltx_p\" style=\"width:142.3pt;\"><span id=\"S4.T2.1.6.5.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Teacher not updated</span></span>\n</span>\n</td>\n<td id=\"S4.T2.1.6.5.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T2.1.6.5.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.1.6.5.2.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\"><span id=\"S4.T2.1.6.5.2.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">23.52</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T2.1.7.6\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.7.6.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_l ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T2.1.7.6.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.1.7.6.1.1.1\" class=\"ltx_p\" style=\"width:142.3pt;\"><span id=\"S4.T2.1.7.6.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Teacher updated with EMA</span></span>\n</span>\n</td>\n<td id=\"S4.T2.1.7.6.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T2.1.7.6.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.1.7.6.2.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\"><span id=\"S4.T2.1.7.6.2.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">18.95</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T2.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.1.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T2.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.1.1.1.1.1\" class=\"ltx_p\" style=\"width:142.3pt;\"><span id=\"S4.T2.1.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">   </span><span id=\"S4.T2.1.1.1.1.1.2\" class=\"ltx_text\" style=\"font-size:90%;\">+weak-supervision</span></span>\n</span>\n</td>\n<td id=\"S4.T2.1.1.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_b ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T2.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.1.1.2.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\"><span id=\"S4.T2.1.1.2.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">18.79</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Federated self-learning with weak supervision:",
                " We see the performance of self-learning of a pretrained RNN-T model on the public SLURP dataset in Table ",
                "2",
                " that shows self-learning improving the performance by ",
                "19",
                "%",
                "percent",
                "19",
                "19\\%",
                " with additional gains from using weak supervision composed of NLU feedback scores. We note that limited gains arise from weak supervision as SLURP has sparse annotations for transcript tokens or few slots per utterance. In few corrected examples, we see self-learning with weak supervision correcting deletion errors and even learning new words like the keyword ‘olly’.",
                "In Table ",
                "3",
                ", the performance of self-learning coupled with weak supervision is depicted for continual learning with a single pass on the internal dataset. First, we observe that if we do not update the paired teacher model with EMA, performance on the new use case does not improve. If we only do self-learning for ASR, there is an improvement of ",
                "8.3",
                "8.3",
                "8.3",
                "% on the new use case test set. Coupling this with an ASR based weak supervision (where each hypothesis gets a feedback score of the WER computed using a teacher model), we see more improvement that increases as feedback includes the NLU component. We also see similar improvement using only the NLU-based feedback-score obtained only for the served hypothesis as opposed to obtaining a score for all possible hypotheses.",
                "Noisy feedback:",
                " Table ",
                "4",
                " shows the result of federated learning only with noisy feedback for a single served hypothesis from ASR. Here we consider noisy feedback of the form, ",
                "M",
                "′",
                "​",
                "(",
                "𝐲",
                ",",
                "z",
                ")",
                "=",
                "M",
                "​",
                "(",
                "𝐲",
                ",",
                "z",
                ")",
                "+",
                "(",
                "−",
                "1",
                ")",
                "M",
                "​",
                "(",
                "𝐲",
                ",",
                "z",
                ")",
                "​",
                "U",
                "′",
                "superscript",
                "𝑀",
                "′",
                "𝐲",
                "𝑧",
                "𝑀",
                "𝐲",
                "𝑧",
                "superscript",
                "1",
                "𝑀",
                "𝐲",
                "𝑧",
                "superscript",
                "𝑈",
                "′",
                "M^{\\prime}(\\mathbf{y},z)=M(\\mathbf{y},z)+(-1)^{M(\\mathbf{y},z)}U^{\\prime}",
                ", where random variable ",
                "U",
                "′",
                "∼",
                "p",
                "​",
                "(",
                "U",
                "|",
                "U",
                "∈",
                "[",
                "0",
                ",",
                "1",
                "]",
                ")",
                ",",
                "U",
                "∼",
                "𝒩",
                "​",
                "(",
                "0",
                ",",
                "σ",
                "2",
                ")",
                "formulae-sequence",
                "similar-to",
                "superscript",
                "𝑈",
                "′",
                "𝑝",
                "conditional",
                "𝑈",
                "𝑈",
                "0",
                "1",
                "similar-to",
                "𝑈",
                "𝒩",
                "0",
                "superscript",
                "𝜎",
                "2",
                "U^{\\prime}\\sim p(U|U\\in[0,1]),U\\sim\\mathcal{N}(0,\\sigma^{2})",
                " is drawn from a normal random variable with variance ",
                "σ",
                "2",
                "superscript",
                "𝜎",
                "2",
                "\\sigma^{2}",
                " truncated to be in the range ",
                "[",
                "0",
                ",",
                "1",
                "]",
                "0",
                "1",
                "[0,1]",
                ". We then add different levels of noise to measure its impact. In a noisy version of a binary feedback score,",
                "where ",
                "μ",
                "=",
                "E",
                "​",
                "[",
                "U",
                "′",
                "]",
                "𝜇",
                "E",
                "delimited-[]",
                "superscript",
                "𝑈",
                "′",
                "\\mu=\\mathrm{E}[U^{\\prime}]",
                ". Thus if the mean is less than ",
                "0.5",
                "0.5",
                "0.5",
                ", gradient update with the noisy feedback, in expectation, is in the same direction as the gradient update with the true feedback. We demonstrate that even at a high level of noise of ",
                "σ",
                "=",
                "0.4",
                "𝜎",
                "0.4",
                "\\sigma=0.4",
                " we are still able to improve the model on the delta dataset significantly.",
                "EMA hyperparameters and rehearsal training",
                ": In Table ",
                "5",
                ", we first see the impact of rehearsal training on mitigating catastrophic forgetting - we observe reduced regression on the older 2020 test set at the expense of performance of new ",
                "Delta",
                " test sets. Delta test set results are not comparable across prior tables as amount of computation, catastrophic forgetting differ. We also study the impact of EMA hyperparameters, higher ",
                "δ",
                "𝛿",
                "\\delta",
                " implies lower weight to new updates and update frequency ",
                "u",
                "𝑢",
                "u",
                " determines how often the teacher model is updated. Improved performance is seen for frequent updates with a lower EMA value. We also observed training diverging when the teacher model is updated to the student model after each step, suggesting that an error feedback loop takes place."
            ]
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Performance of federated self-learning with weak supervision on voice-assistant data. WERR numbers are relative to WER of the initial model. Multiple forms of weak supervision such as ASR and NLU labels from an alternate SLU model, and NLU feedback scores for the hypothesis served are contrasted.",
        "table": "<table id=\"S4.T3.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.4.5.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.4.5.1.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T3.4.5.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.4.5.1.1.1.1\" class=\"ltx_p\" style=\"width:79.7pt;\"><span id=\"S4.T3.4.5.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Weak Supervision method</span></span>\n</span>\n</th>\n<th id=\"S4.T3.4.5.1.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T3.4.5.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.4.5.1.2.1.1\" class=\"ltx_p\" style=\"width:34.1pt;\"><span id=\"S4.T3.4.5.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Teacher Update</span></span>\n</span>\n</th>\n<th id=\"S4.T3.4.5.1.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T3.4.5.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.4.5.1.3.1.1\" class=\"ltx_p\" style=\"width:34.1pt;\"><span id=\"S4.T3.4.5.1.3.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">General WERR</span></span>\n</span>\n</th>\n<th id=\"S4.T3.4.5.1.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T3.4.5.1.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.4.5.1.4.1.1\" class=\"ltx_p\" style=\"width:34.1pt;\"><span id=\"S4.T3.4.5.1.4.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Delta WERR</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.4.6.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.6.1.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T3.4.6.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.4.6.1.1.1.1\" class=\"ltx_p\" style=\"width:79.7pt;\"><span id=\"S4.T3.4.6.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">-</span></span>\n</span>\n</td>\n<td id=\"S4.T3.4.6.1.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T3.4.6.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.4.6.1.2.1.1\" class=\"ltx_p\" style=\"width:34.1pt;\"><span id=\"S4.T3.4.6.1.2.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">-</span></span>\n</span>\n</td>\n<td id=\"S4.T3.4.6.1.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T3.4.6.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.4.6.1.3.1.1\" class=\"ltx_p\" style=\"width:34.1pt;\"><span id=\"S4.T3.4.6.1.3.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">-8.16</span></span>\n</span>\n</td>\n<td id=\"S4.T3.4.6.1.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T3.4.6.1.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.4.6.1.4.1.1\" class=\"ltx_p\" style=\"width:34.1pt;\"><span id=\"S4.T3.4.6.1.4.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">-0.02</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T3.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_l ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T3.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.1.2.1.1\" class=\"ltx_p\" style=\"width:79.7pt;\"><span id=\"S4.T3.1.1.2.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">-</span></span>\n</span>\n</td>\n<td id=\"S4.T3.1.1.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T3.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.1.1.1.1\" class=\"ltx_p\" style=\"width:34.1pt;\"><math id=\"S4.T3.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\checkmark\" display=\"inline\"><semantics id=\"S4.T3.1.1.1.1.1.m1.1a\"><mi mathsize=\"90%\" mathvariant=\"normal\" id=\"S4.T3.1.1.1.1.1.m1.1.1\" xref=\"S4.T3.1.1.1.1.1.m1.1.1.cmml\">✓</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.1.1.1.1.1.m1.1b\"><ci id=\"S4.T3.1.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T3.1.1.1.1.1.m1.1.1\">✓</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.1.1.1.1.1.m1.1c\">\\checkmark</annotation></semantics></math></span>\n</span>\n</td>\n<td id=\"S4.T3.1.1.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T3.1.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.1.3.1.1\" class=\"ltx_p\" style=\"width:34.1pt;\"><span id=\"S4.T3.1.1.3.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">-6.12</span></span>\n</span>\n</td>\n<td id=\"S4.T3.1.1.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T3.1.1.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.1.1.4.1.1\" class=\"ltx_p\" style=\"width:34.1pt;\"><span id=\"S4.T3.1.1.4.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">8.29</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T3.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.2.2.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_l ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T3.2.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.2.2.2.1.1\" class=\"ltx_p\" style=\"width:79.7pt;\"><span id=\"S4.T3.2.2.2.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">ASR</span></span>\n</span>\n</td>\n<td id=\"S4.T3.2.2.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T3.2.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.2.2.1.1.1\" class=\"ltx_p\" style=\"width:34.1pt;\"><math id=\"S4.T3.2.2.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\checkmark\" display=\"inline\"><semantics id=\"S4.T3.2.2.1.1.1.m1.1a\"><mi mathsize=\"90%\" mathvariant=\"normal\" id=\"S4.T3.2.2.1.1.1.m1.1.1\" xref=\"S4.T3.2.2.1.1.1.m1.1.1.cmml\">✓</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.2.2.1.1.1.m1.1b\"><ci id=\"S4.T3.2.2.1.1.1.m1.1.1.cmml\" xref=\"S4.T3.2.2.1.1.1.m1.1.1\">✓</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.2.2.1.1.1.m1.1c\">\\checkmark</annotation></semantics></math></span>\n</span>\n</td>\n<td id=\"S4.T3.2.2.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T3.2.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.2.2.3.1.1\" class=\"ltx_p\" style=\"width:34.1pt;\"><span id=\"S4.T3.2.2.3.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">-1.84</span></span>\n</span>\n</td>\n<td id=\"S4.T3.2.2.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T3.2.2.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.2.2.4.1.1\" class=\"ltx_p\" style=\"width:34.1pt;\"><span id=\"S4.T3.2.2.4.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">11.43</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T3.3.3\" class=\"ltx_tr\">\n<td id=\"S4.T3.3.3.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_l ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T3.3.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.3.3.2.1.1\" class=\"ltx_p\" style=\"width:79.7pt;\"><span id=\"S4.T3.3.3.2.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">ASR + NLU</span></span>\n</span>\n</td>\n<td id=\"S4.T3.3.3.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T3.3.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.3.3.1.1.1\" class=\"ltx_p\" style=\"width:34.1pt;\"><math id=\"S4.T3.3.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\checkmark\" display=\"inline\"><semantics id=\"S4.T3.3.3.1.1.1.m1.1a\"><mi mathsize=\"90%\" mathvariant=\"normal\" id=\"S4.T3.3.3.1.1.1.m1.1.1\" xref=\"S4.T3.3.3.1.1.1.m1.1.1.cmml\">✓</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.3.3.1.1.1.m1.1b\"><ci id=\"S4.T3.3.3.1.1.1.m1.1.1.cmml\" xref=\"S4.T3.3.3.1.1.1.m1.1.1\">✓</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.3.3.1.1.1.m1.1c\">\\checkmark</annotation></semantics></math></span>\n</span>\n</td>\n<td id=\"S4.T3.3.3.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T3.3.3.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.3.3.3.1.1\" class=\"ltx_p\" style=\"width:34.1pt;\"><span id=\"S4.T3.3.3.3.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">-1.22</span></span>\n</span>\n</td>\n<td id=\"S4.T3.3.3.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T3.3.3.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.3.3.4.1.1\" class=\"ltx_p\" style=\"width:34.1pt;\"><span id=\"S4.T3.3.3.4.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">11.56</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T3.4.4\" class=\"ltx_tr\">\n<td id=\"S4.T3.4.4.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T3.4.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.4.4.2.1.1\" class=\"ltx_p\" style=\"width:79.7pt;\"><span id=\"S4.T3.4.4.2.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">NLU feedback-score</span></span>\n</span>\n</td>\n<td id=\"S4.T3.4.4.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_b ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T3.4.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.4.4.1.1.1\" class=\"ltx_p\" style=\"width:34.1pt;\"><math id=\"S4.T3.4.4.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\checkmark\" display=\"inline\"><semantics id=\"S4.T3.4.4.1.1.1.m1.1a\"><mi mathsize=\"90%\" mathvariant=\"normal\" id=\"S4.T3.4.4.1.1.1.m1.1.1\" xref=\"S4.T3.4.4.1.1.1.m1.1.1.cmml\">✓</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.4.4.1.1.1.m1.1b\"><ci id=\"S4.T3.4.4.1.1.1.m1.1.1.cmml\" xref=\"S4.T3.4.4.1.1.1.m1.1.1\">✓</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.4.4.1.1.1.m1.1c\">\\checkmark</annotation></semantics></math></span>\n</span>\n</td>\n<td id=\"S4.T3.4.4.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_b ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T3.4.4.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.4.4.3.1.1\" class=\"ltx_p\" style=\"width:34.1pt;\"><span id=\"S4.T3.4.4.3.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">-1.64</span></span>\n</span>\n</td>\n<td id=\"S4.T3.4.4.4\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_b ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T3.4.4.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T3.4.4.4.1.1\" class=\"ltx_p\" style=\"width:34.1pt;\"><span id=\"S4.T3.4.4.4.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">12.06</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Federated self-learning with weak supervision:",
                " We see the performance of self-learning of a pretrained RNN-T model on the public SLURP dataset in Table ",
                "2",
                " that shows self-learning improving the performance by ",
                "19",
                "%",
                "percent",
                "19",
                "19\\%",
                " with additional gains from using weak supervision composed of NLU feedback scores. We note that limited gains arise from weak supervision as SLURP has sparse annotations for transcript tokens or few slots per utterance. In few corrected examples, we see self-learning with weak supervision correcting deletion errors and even learning new words like the keyword ‘olly’.",
                "In Table ",
                "3",
                ", the performance of self-learning coupled with weak supervision is depicted for continual learning with a single pass on the internal dataset. First, we observe that if we do not update the paired teacher model with EMA, performance on the new use case does not improve. If we only do self-learning for ASR, there is an improvement of ",
                "8.3",
                "8.3",
                "8.3",
                "% on the new use case test set. Coupling this with an ASR based weak supervision (where each hypothesis gets a feedback score of the WER computed using a teacher model), we see more improvement that increases as feedback includes the NLU component. We also see similar improvement using only the NLU-based feedback-score obtained only for the served hypothesis as opposed to obtaining a score for all possible hypotheses.",
                "Noisy feedback:",
                " Table ",
                "4",
                " shows the result of federated learning only with noisy feedback for a single served hypothesis from ASR. Here we consider noisy feedback of the form, ",
                "M",
                "′",
                "​",
                "(",
                "𝐲",
                ",",
                "z",
                ")",
                "=",
                "M",
                "​",
                "(",
                "𝐲",
                ",",
                "z",
                ")",
                "+",
                "(",
                "−",
                "1",
                ")",
                "M",
                "​",
                "(",
                "𝐲",
                ",",
                "z",
                ")",
                "​",
                "U",
                "′",
                "superscript",
                "𝑀",
                "′",
                "𝐲",
                "𝑧",
                "𝑀",
                "𝐲",
                "𝑧",
                "superscript",
                "1",
                "𝑀",
                "𝐲",
                "𝑧",
                "superscript",
                "𝑈",
                "′",
                "M^{\\prime}(\\mathbf{y},z)=M(\\mathbf{y},z)+(-1)^{M(\\mathbf{y},z)}U^{\\prime}",
                ", where random variable ",
                "U",
                "′",
                "∼",
                "p",
                "​",
                "(",
                "U",
                "|",
                "U",
                "∈",
                "[",
                "0",
                ",",
                "1",
                "]",
                ")",
                ",",
                "U",
                "∼",
                "𝒩",
                "​",
                "(",
                "0",
                ",",
                "σ",
                "2",
                ")",
                "formulae-sequence",
                "similar-to",
                "superscript",
                "𝑈",
                "′",
                "𝑝",
                "conditional",
                "𝑈",
                "𝑈",
                "0",
                "1",
                "similar-to",
                "𝑈",
                "𝒩",
                "0",
                "superscript",
                "𝜎",
                "2",
                "U^{\\prime}\\sim p(U|U\\in[0,1]),U\\sim\\mathcal{N}(0,\\sigma^{2})",
                " is drawn from a normal random variable with variance ",
                "σ",
                "2",
                "superscript",
                "𝜎",
                "2",
                "\\sigma^{2}",
                " truncated to be in the range ",
                "[",
                "0",
                ",",
                "1",
                "]",
                "0",
                "1",
                "[0,1]",
                ". We then add different levels of noise to measure its impact. In a noisy version of a binary feedback score,",
                "where ",
                "μ",
                "=",
                "E",
                "​",
                "[",
                "U",
                "′",
                "]",
                "𝜇",
                "E",
                "delimited-[]",
                "superscript",
                "𝑈",
                "′",
                "\\mu=\\mathrm{E}[U^{\\prime}]",
                ". Thus if the mean is less than ",
                "0.5",
                "0.5",
                "0.5",
                ", gradient update with the noisy feedback, in expectation, is in the same direction as the gradient update with the true feedback. We demonstrate that even at a high level of noise of ",
                "σ",
                "=",
                "0.4",
                "𝜎",
                "0.4",
                "\\sigma=0.4",
                " we are still able to improve the model on the delta dataset significantly.",
                "EMA hyperparameters and rehearsal training",
                ": In Table ",
                "5",
                ", we first see the impact of rehearsal training on mitigating catastrophic forgetting - we observe reduced regression on the older 2020 test set at the expense of performance of new ",
                "Delta",
                " test sets. Delta test set results are not comparable across prior tables as amount of computation, catastrophic forgetting differ. We also study the impact of EMA hyperparameters, higher ",
                "δ",
                "𝛿",
                "\\delta",
                " implies lower weight to new updates and update frequency ",
                "u",
                "𝑢",
                "u",
                " determines how often the teacher model is updated. Improved performance is seen for frequent updates with a lower EMA value. We also observed training diverging when the teacher model is updated to the student model after each step, suggesting that an error feedback loop takes place."
            ]
        ]
    },
    "S4.T4": {
        "caption": "Table 4: Performance of learning with only noisy feedback scores on voice-assistant data",
        "table": "<table id=\"S4.T4.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T4.3.4.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.3.4.1.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T4.3.4.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T4.3.4.1.1.1.1\" class=\"ltx_p\" style=\"width:142.3pt;\"><span id=\"S4.T4.3.4.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Setting</span></span>\n</span>\n</th>\n<th id=\"S4.T4.3.4.1.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span id=\"S4.T4.3.4.1.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Delta WERR</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T4.3.5.1\" class=\"ltx_tr\">\n<td id=\"S4.T4.3.5.1.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T4.3.5.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T4.3.5.1.1.1.1\" class=\"ltx_p\" style=\"width:142.3pt;\"><span id=\"S4.T4.3.5.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">binary feedback without noise</span></span>\n</span>\n</td>\n<td id=\"S4.T4.3.5.1.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span id=\"S4.T4.3.5.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">14.45</span></td>\n</tr>\n<tr id=\"S4.T4.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.1.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_l ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T4.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T4.1.1.1.1.1\" class=\"ltx_p\" style=\"width:142.3pt;\"><span id=\"S4.T4.1.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">binary feedback + noise (</span><math id=\"S4.T4.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sigma=0.1\" display=\"inline\"><semantics id=\"S4.T4.1.1.1.1.1.m1.1a\"><mrow id=\"S4.T4.1.1.1.1.1.m1.1.1\" xref=\"S4.T4.1.1.1.1.1.m1.1.1.cmml\"><mi mathsize=\"90%\" id=\"S4.T4.1.1.1.1.1.m1.1.1.2\" xref=\"S4.T4.1.1.1.1.1.m1.1.1.2.cmml\">σ</mi><mo mathsize=\"90%\" id=\"S4.T4.1.1.1.1.1.m1.1.1.1\" xref=\"S4.T4.1.1.1.1.1.m1.1.1.1.cmml\">=</mo><mn mathsize=\"90%\" id=\"S4.T4.1.1.1.1.1.m1.1.1.3\" xref=\"S4.T4.1.1.1.1.1.m1.1.1.3.cmml\">0.1</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.1.1.1.1.1.m1.1b\"><apply id=\"S4.T4.1.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T4.1.1.1.1.1.m1.1.1\"><eq id=\"S4.T4.1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S4.T4.1.1.1.1.1.m1.1.1.1\"></eq><ci id=\"S4.T4.1.1.1.1.1.m1.1.1.2.cmml\" xref=\"S4.T4.1.1.1.1.1.m1.1.1.2\">𝜎</ci><cn type=\"float\" id=\"S4.T4.1.1.1.1.1.m1.1.1.3.cmml\" xref=\"S4.T4.1.1.1.1.1.m1.1.1.3\">0.1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.1.1.1.1.1.m1.1c\">\\sigma=0.1</annotation></semantics></math><span id=\"S4.T4.1.1.1.1.1.2\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></span>\n</span>\n</td>\n<td id=\"S4.T4.1.1.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span id=\"S4.T4.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">9.05</span></td>\n</tr>\n<tr id=\"S4.T4.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T4.2.2.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_l ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T4.2.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T4.2.2.1.1.1\" class=\"ltx_p\" style=\"width:142.3pt;\"><span id=\"S4.T4.2.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">binary feedback + noise (</span><math id=\"S4.T4.2.2.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sigma=0.2\" display=\"inline\"><semantics id=\"S4.T4.2.2.1.1.1.m1.1a\"><mrow id=\"S4.T4.2.2.1.1.1.m1.1.1\" xref=\"S4.T4.2.2.1.1.1.m1.1.1.cmml\"><mi mathsize=\"90%\" id=\"S4.T4.2.2.1.1.1.m1.1.1.2\" xref=\"S4.T4.2.2.1.1.1.m1.1.1.2.cmml\">σ</mi><mo mathsize=\"90%\" id=\"S4.T4.2.2.1.1.1.m1.1.1.1\" xref=\"S4.T4.2.2.1.1.1.m1.1.1.1.cmml\">=</mo><mn mathsize=\"90%\" id=\"S4.T4.2.2.1.1.1.m1.1.1.3\" xref=\"S4.T4.2.2.1.1.1.m1.1.1.3.cmml\">0.2</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.2.2.1.1.1.m1.1b\"><apply id=\"S4.T4.2.2.1.1.1.m1.1.1.cmml\" xref=\"S4.T4.2.2.1.1.1.m1.1.1\"><eq id=\"S4.T4.2.2.1.1.1.m1.1.1.1.cmml\" xref=\"S4.T4.2.2.1.1.1.m1.1.1.1\"></eq><ci id=\"S4.T4.2.2.1.1.1.m1.1.1.2.cmml\" xref=\"S4.T4.2.2.1.1.1.m1.1.1.2\">𝜎</ci><cn type=\"float\" id=\"S4.T4.2.2.1.1.1.m1.1.1.3.cmml\" xref=\"S4.T4.2.2.1.1.1.m1.1.1.3\">0.2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.2.2.1.1.1.m1.1c\">\\sigma=0.2</annotation></semantics></math><span id=\"S4.T4.2.2.1.1.1.2\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></span>\n</span>\n</td>\n<td id=\"S4.T4.2.2.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span id=\"S4.T4.2.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">7.41</span></td>\n</tr>\n<tr id=\"S4.T4.3.3\" class=\"ltx_tr\">\n<td id=\"S4.T4.3.3.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T4.3.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T4.3.3.1.1.1\" class=\"ltx_p\" style=\"width:142.3pt;\"><span id=\"S4.T4.3.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">binary feedback + noise (</span><math id=\"S4.T4.3.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sigma=0.4\" display=\"inline\"><semantics id=\"S4.T4.3.3.1.1.1.m1.1a\"><mrow id=\"S4.T4.3.3.1.1.1.m1.1.1\" xref=\"S4.T4.3.3.1.1.1.m1.1.1.cmml\"><mi mathsize=\"90%\" id=\"S4.T4.3.3.1.1.1.m1.1.1.2\" xref=\"S4.T4.3.3.1.1.1.m1.1.1.2.cmml\">σ</mi><mo mathsize=\"90%\" id=\"S4.T4.3.3.1.1.1.m1.1.1.1\" xref=\"S4.T4.3.3.1.1.1.m1.1.1.1.cmml\">=</mo><mn mathsize=\"90%\" id=\"S4.T4.3.3.1.1.1.m1.1.1.3\" xref=\"S4.T4.3.3.1.1.1.m1.1.1.3.cmml\">0.4</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.3.3.1.1.1.m1.1b\"><apply id=\"S4.T4.3.3.1.1.1.m1.1.1.cmml\" xref=\"S4.T4.3.3.1.1.1.m1.1.1\"><eq id=\"S4.T4.3.3.1.1.1.m1.1.1.1.cmml\" xref=\"S4.T4.3.3.1.1.1.m1.1.1.1\"></eq><ci id=\"S4.T4.3.3.1.1.1.m1.1.1.2.cmml\" xref=\"S4.T4.3.3.1.1.1.m1.1.1.2\">𝜎</ci><cn type=\"float\" id=\"S4.T4.3.3.1.1.1.m1.1.1.3.cmml\" xref=\"S4.T4.3.3.1.1.1.m1.1.1.3\">0.4</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.3.3.1.1.1.m1.1c\">\\sigma=0.4</annotation></semantics></math><span id=\"S4.T4.3.3.1.1.1.2\" class=\"ltx_text\" style=\"font-size:90%;\">)</span></span>\n</span>\n</td>\n<td id=\"S4.T4.3.3.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span id=\"S4.T4.3.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">4.40</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Federated self-learning with weak supervision:",
                " We see the performance of self-learning of a pretrained RNN-T model on the public SLURP dataset in Table ",
                "2",
                " that shows self-learning improving the performance by ",
                "19",
                "%",
                "percent",
                "19",
                "19\\%",
                " with additional gains from using weak supervision composed of NLU feedback scores. We note that limited gains arise from weak supervision as SLURP has sparse annotations for transcript tokens or few slots per utterance. In few corrected examples, we see self-learning with weak supervision correcting deletion errors and even learning new words like the keyword ‘olly’.",
                "In Table ",
                "3",
                ", the performance of self-learning coupled with weak supervision is depicted for continual learning with a single pass on the internal dataset. First, we observe that if we do not update the paired teacher model with EMA, performance on the new use case does not improve. If we only do self-learning for ASR, there is an improvement of ",
                "8.3",
                "8.3",
                "8.3",
                "% on the new use case test set. Coupling this with an ASR based weak supervision (where each hypothesis gets a feedback score of the WER computed using a teacher model), we see more improvement that increases as feedback includes the NLU component. We also see similar improvement using only the NLU-based feedback-score obtained only for the served hypothesis as opposed to obtaining a score for all possible hypotheses.",
                "Noisy feedback:",
                " Table ",
                "4",
                " shows the result of federated learning only with noisy feedback for a single served hypothesis from ASR. Here we consider noisy feedback of the form, ",
                "M",
                "′",
                "​",
                "(",
                "𝐲",
                ",",
                "z",
                ")",
                "=",
                "M",
                "​",
                "(",
                "𝐲",
                ",",
                "z",
                ")",
                "+",
                "(",
                "−",
                "1",
                ")",
                "M",
                "​",
                "(",
                "𝐲",
                ",",
                "z",
                ")",
                "​",
                "U",
                "′",
                "superscript",
                "𝑀",
                "′",
                "𝐲",
                "𝑧",
                "𝑀",
                "𝐲",
                "𝑧",
                "superscript",
                "1",
                "𝑀",
                "𝐲",
                "𝑧",
                "superscript",
                "𝑈",
                "′",
                "M^{\\prime}(\\mathbf{y},z)=M(\\mathbf{y},z)+(-1)^{M(\\mathbf{y},z)}U^{\\prime}",
                ", where random variable ",
                "U",
                "′",
                "∼",
                "p",
                "​",
                "(",
                "U",
                "|",
                "U",
                "∈",
                "[",
                "0",
                ",",
                "1",
                "]",
                ")",
                ",",
                "U",
                "∼",
                "𝒩",
                "​",
                "(",
                "0",
                ",",
                "σ",
                "2",
                ")",
                "formulae-sequence",
                "similar-to",
                "superscript",
                "𝑈",
                "′",
                "𝑝",
                "conditional",
                "𝑈",
                "𝑈",
                "0",
                "1",
                "similar-to",
                "𝑈",
                "𝒩",
                "0",
                "superscript",
                "𝜎",
                "2",
                "U^{\\prime}\\sim p(U|U\\in[0,1]),U\\sim\\mathcal{N}(0,\\sigma^{2})",
                " is drawn from a normal random variable with variance ",
                "σ",
                "2",
                "superscript",
                "𝜎",
                "2",
                "\\sigma^{2}",
                " truncated to be in the range ",
                "[",
                "0",
                ",",
                "1",
                "]",
                "0",
                "1",
                "[0,1]",
                ". We then add different levels of noise to measure its impact. In a noisy version of a binary feedback score,",
                "where ",
                "μ",
                "=",
                "E",
                "​",
                "[",
                "U",
                "′",
                "]",
                "𝜇",
                "E",
                "delimited-[]",
                "superscript",
                "𝑈",
                "′",
                "\\mu=\\mathrm{E}[U^{\\prime}]",
                ". Thus if the mean is less than ",
                "0.5",
                "0.5",
                "0.5",
                ", gradient update with the noisy feedback, in expectation, is in the same direction as the gradient update with the true feedback. We demonstrate that even at a high level of noise of ",
                "σ",
                "=",
                "0.4",
                "𝜎",
                "0.4",
                "\\sigma=0.4",
                " we are still able to improve the model on the delta dataset significantly.",
                "EMA hyperparameters and rehearsal training",
                ": In Table ",
                "5",
                ", we first see the impact of rehearsal training on mitigating catastrophic forgetting - we observe reduced regression on the older 2020 test set at the expense of performance of new ",
                "Delta",
                " test sets. Delta test set results are not comparable across prior tables as amount of computation, catastrophic forgetting differ. We also study the impact of EMA hyperparameters, higher ",
                "δ",
                "𝛿",
                "\\delta",
                " implies lower weight to new updates and update frequency ",
                "u",
                "𝑢",
                "u",
                " determines how often the teacher model is updated. Improved performance is seen for frequent updates with a lower EMA value. We also observed training diverging when the teacher model is updated to the student model after each step, suggesting that an error feedback loop takes place."
            ]
        ]
    },
    "S4.T5": {
        "caption": "Table 5: We study (i) the effect of rehearsal training in mitigating the catastrophic forgetting (left) and (ii) the effect of hyper parameters (right) in self-learning on voice-assistant data",
        "table": "<table id=\"S4.T5.1.1\" class=\"ltx_tabular ltx_figure_panel ltx_parbox ltx_align_middle\" style=\"width:195.1pt;\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T5.1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T5.1.1.2.1.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span id=\"S4.T5.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Setting</span></th>\n<th id=\"S4.T5.1.1.2.1.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T5.1.1.2.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.1.1.2.1.2.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\"><span id=\"S4.T5.1.1.2.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Delta WERR</span></span>\n</span>\n</th>\n<th id=\"S4.T5.1.1.2.1.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T5.1.1.2.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.1.1.2.1.3.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\"><span id=\"S4.T5.1.1.2.1.3.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">General (2020) WERR</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T5.1.1.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T5.1.1.3.1.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\"><span id=\"S4.T5.1.1.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Self-learning</span></th>\n<td id=\"S4.T5.1.1.3.1.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T5.1.1.3.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.1.1.3.1.2.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\"><span id=\"S4.T5.1.1.3.1.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">14.08</span></span>\n</span>\n</td>\n<td id=\"S4.T5.1.1.3.1.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T5.1.1.3.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.1.1.3.1.3.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\"><span id=\"S4.T5.1.1.3.1.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">-13.63</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T5.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T5.1.1.1.1\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T5.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\"> </span><span id=\"S4.T5.1.1.1.1.2\" class=\"ltx_text\" style=\"font-size:80%;\">+ rehearsal training</span>\n</th>\n<td id=\"S4.T5.1.1.1.2\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_b ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T5.1.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.1.1.1.2.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\"><span id=\"S4.T5.1.1.1.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">12.47</span></span>\n</span>\n</td>\n<td id=\"S4.T5.1.1.1.3\" class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_b ltx_border_r\" style=\"padding-left:0.0pt;padding-right:0.0pt;\">\n<span id=\"S4.T5.1.1.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.1.1.1.3.1.1\" class=\"ltx_p\" style=\"width:28.5pt;\"><span id=\"S4.T5.1.1.1.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">-5.85</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Federated self-learning with weak supervision:",
                " We see the performance of self-learning of a pretrained RNN-T model on the public SLURP dataset in Table ",
                "2",
                " that shows self-learning improving the performance by ",
                "19",
                "%",
                "percent",
                "19",
                "19\\%",
                " with additional gains from using weak supervision composed of NLU feedback scores. We note that limited gains arise from weak supervision as SLURP has sparse annotations for transcript tokens or few slots per utterance. In few corrected examples, we see self-learning with weak supervision correcting deletion errors and even learning new words like the keyword ‘olly’.",
                "In Table ",
                "3",
                ", the performance of self-learning coupled with weak supervision is depicted for continual learning with a single pass on the internal dataset. First, we observe that if we do not update the paired teacher model with EMA, performance on the new use case does not improve. If we only do self-learning for ASR, there is an improvement of ",
                "8.3",
                "8.3",
                "8.3",
                "% on the new use case test set. Coupling this with an ASR based weak supervision (where each hypothesis gets a feedback score of the WER computed using a teacher model), we see more improvement that increases as feedback includes the NLU component. We also see similar improvement using only the NLU-based feedback-score obtained only for the served hypothesis as opposed to obtaining a score for all possible hypotheses.",
                "Noisy feedback:",
                " Table ",
                "4",
                " shows the result of federated learning only with noisy feedback for a single served hypothesis from ASR. Here we consider noisy feedback of the form, ",
                "M",
                "′",
                "​",
                "(",
                "𝐲",
                ",",
                "z",
                ")",
                "=",
                "M",
                "​",
                "(",
                "𝐲",
                ",",
                "z",
                ")",
                "+",
                "(",
                "−",
                "1",
                ")",
                "M",
                "​",
                "(",
                "𝐲",
                ",",
                "z",
                ")",
                "​",
                "U",
                "′",
                "superscript",
                "𝑀",
                "′",
                "𝐲",
                "𝑧",
                "𝑀",
                "𝐲",
                "𝑧",
                "superscript",
                "1",
                "𝑀",
                "𝐲",
                "𝑧",
                "superscript",
                "𝑈",
                "′",
                "M^{\\prime}(\\mathbf{y},z)=M(\\mathbf{y},z)+(-1)^{M(\\mathbf{y},z)}U^{\\prime}",
                ", where random variable ",
                "U",
                "′",
                "∼",
                "p",
                "​",
                "(",
                "U",
                "|",
                "U",
                "∈",
                "[",
                "0",
                ",",
                "1",
                "]",
                ")",
                ",",
                "U",
                "∼",
                "𝒩",
                "​",
                "(",
                "0",
                ",",
                "σ",
                "2",
                ")",
                "formulae-sequence",
                "similar-to",
                "superscript",
                "𝑈",
                "′",
                "𝑝",
                "conditional",
                "𝑈",
                "𝑈",
                "0",
                "1",
                "similar-to",
                "𝑈",
                "𝒩",
                "0",
                "superscript",
                "𝜎",
                "2",
                "U^{\\prime}\\sim p(U|U\\in[0,1]),U\\sim\\mathcal{N}(0,\\sigma^{2})",
                " is drawn from a normal random variable with variance ",
                "σ",
                "2",
                "superscript",
                "𝜎",
                "2",
                "\\sigma^{2}",
                " truncated to be in the range ",
                "[",
                "0",
                ",",
                "1",
                "]",
                "0",
                "1",
                "[0,1]",
                ". We then add different levels of noise to measure its impact. In a noisy version of a binary feedback score,",
                "where ",
                "μ",
                "=",
                "E",
                "​",
                "[",
                "U",
                "′",
                "]",
                "𝜇",
                "E",
                "delimited-[]",
                "superscript",
                "𝑈",
                "′",
                "\\mu=\\mathrm{E}[U^{\\prime}]",
                ". Thus if the mean is less than ",
                "0.5",
                "0.5",
                "0.5",
                ", gradient update with the noisy feedback, in expectation, is in the same direction as the gradient update with the true feedback. We demonstrate that even at a high level of noise of ",
                "σ",
                "=",
                "0.4",
                "𝜎",
                "0.4",
                "\\sigma=0.4",
                " we are still able to improve the model on the delta dataset significantly.",
                "EMA hyperparameters and rehearsal training",
                ": In Table ",
                "5",
                ", we first see the impact of rehearsal training on mitigating catastrophic forgetting - we observe reduced regression on the older 2020 test set at the expense of performance of new ",
                "Delta",
                " test sets. Delta test set results are not comparable across prior tables as amount of computation, catastrophic forgetting differ. We also study the impact of EMA hyperparameters, higher ",
                "δ",
                "𝛿",
                "\\delta",
                " implies lower weight to new updates and update frequency ",
                "u",
                "𝑢",
                "u",
                " determines how often the teacher model is updated. Improved performance is seen for frequent updates with a lower EMA value. We also observed training diverging when the teacher model is updated to the student model after each step, suggesting that an error feedback loop takes place."
            ]
        ]
    }
}